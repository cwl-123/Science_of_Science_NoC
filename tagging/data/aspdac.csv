year,title,abstract,full_text
2003,Energy-aware mapping for tile-based NoC architectures under performance constraints.,,"Energy-Aware Mapping for Tile-based NoC Architectures Under Performance Constraints (cid:3) Jingcao Hu Radu Marculescu Department of Electrical and Computer Engineering Carnegie Mellon University Pittsburgh, PA 15213-3890, USA e-mail: fjingcao, radumg@ece.cmu.edu Abstract — In this paper, we present an algorithm which automatically maps the IPs/cores onto a generic regular Network on Chip (NoC) architecture such that the total communication energy is minimized. At the same time, the performance of the mapped system is guaranteed to satisfy the speci ﬁed constraints through bandwidth reservation. As the main contribution, we ﬁrst formulate the problem of energy-aware mapping, in a topological sense, and then propose an efﬁcient branch-and-bound algorithm to solve it. Experimental results show that the proposed algorithm is very fast and robust, and signi ﬁcant energy savings can be achieved. For instance, for a complex video/audio SoC design, on average, 60.4% energy savings have been observed compared to an ad-hoc implementation. I . IN TRODUCT ION With the advance of the semiconductor technology, the enormous number of transistors available on a single chip allows designers to integrate dozens of IP blocks together with large amounts of embedded memory. These IPs can be CPU or DSP cores, video stream processors, high-bandwidth I/O, etc[1]. The richness of the computational resources places tremendous demands on the communication resources as well. Additionally, the shrinking feature size in the deep-sub-micron (DSM) era is continuously pushing interconnection delay and power consumption as the dominant factors in the optimization of modern systems. Another consequence of the DSM effects is the difﬁculty in optimizing the interconnection because of the ensued worsening effects such as crosstalk, etc. To mitigate these problems, Dally and Towles [2] have recently proposed a regular tile-based architecture where communication can be efﬁciently realized using an on-chip network (Fig. 1)1 . As shown in the left part of Fig. 1, the chip is divided into regular tiles where each tile can be a general-purpose processor, a DSP, a memory subsystem, etc. A router is embedded within each tile with the objective of connecting it to its neighboring tiles. Thus, instead of routing design-speciﬁc global wires, the inter-tile communication can (cid:3)Research supported by NSF CCR-00-93104 and DARPA/Marco Gigascale Research Center (GSRC), and SRC 2001-HJ-898. 1 This implementation is slightly different from the example implementation given in [2], where a torus topology is adopted. Tile (cid:13) Network(cid:13) Logic(cid:13) (3,0)(cid:13) (3,1)(cid:13) (3,2)(cid:13) (3,3)(cid:13) (2,0)(cid:13) (2,1)(cid:13) (2,2)(cid:13) (2,3)(cid:13) (1,0)(cid:13) (1,1)(cid:13) (1,2)(cid:13) (1,3)(cid:13) (0,0)(cid:13) (0,1)(cid:13) (0,2)(cid:13) (0,3)(cid:13) Mapping (cid:13) ASIC1(cid:13) DSP1(cid:13) DSP3(cid:13) DSP2(cid:13) ASIC2(cid:13) CPU1(cid:13) Tile-based Architecture(cid:13) Communication Task Graph(cid:13) Fig. 1. Tile-based architecture and the mapping problem be achieved by routing packets via these embedded routers. Three key concepts come together to make this tile-based architecture very promising: structured network wiring, modularity and standard interfaces. More precisely, since the network wires are structured and wired beforehand, their electrical parameters can be very well controlled and optimized. In turn, these controlled electrical parameters make possible to use aggressively signaling circuits which reduce power dissipation and propagation delay signi ﬁcantly . Modularity and standard network interfaces facilitate re-usability and interoperability of the modules. Moreover, since the network platform can be designed in advance and later used for many applications, it makes sense to highly optimize this platform as its development cost can be amortized across many applications. To exploit this regular tile-based architecture, the design ﬂo w needs the following three steps: First, the application needs to be divided into a graph of concurrent tasks. Second, using a set of available IPs, the application tasks are assigned and scheduled. Finally, the designer needs to decide to which tile each selected IP should be mapped such that the metrics of interest are optimized. More precisely, given the assigned/scheduled task graph which has been generated from previous two steps, this last phase determines the topological placement of these IPs onto different tiles. For instance, referring to Fig. 1, this step determines onto which tile (e.g. (3,0), (2,1), (1,3) etc.) each IP (e.g. ASIC2, DSP3, CPU1, etc.) should be placed. The ﬁrst two steps described above are not new to the CAD community, as they have been addressed in the area of hardware/software co-design and IP-reuse [3]. However, the mapping phase (that is, the topological placement of the IPs onto the on-chip tiles) represents a new problem, especially in the context of the regular tile-based architecture, as it signiﬁcantly 233 impacts the energy and performance metrics of the system. In this paper, we address this very issue. To this end, we ﬁrst formulate the mapping problem and show the impact of different mappings on the communication energy consumption of a given system. An efﬁcient branch-and-bound algorithm is then proposed to solve this problem under tight performance constraints. Experimental results show that signiﬁcant energy savings can be achieved, while guaranteeing the speciﬁed system performance. Compared to a simulated annealing algorithm, our algorithm is orders of magnitude faster, while the energy consumption of the solution is almost the same (less than 10% difference). The paper is organized as follows: Section II brieﬂy introduces the related work. The platform of the targeted system and its associated power model are described in Section III. Sections IV and V illustrate the energy-aware mapping algorithm. Experimental results are shown in Section VI. Finally, Section VII summarizes our contribution and outlines some directions for future work. I I . R E LAT ED WORK In their paper [2], Dally et al. suggest using the on-chip interconnection networks instead of ad-hoc global wiring to structure the top-level wires on a chip and facilitate modular design. In [4], Hemani et al. present a honeycomb structure in which each processing core (resource) is located on a regular hexagonal node connected to three switches while these switches are directly linked to their next nearest neighbors. In [5], Kumar et al. describe a physical NoC architecture implemented by a direct layout of a 2D mesh of switches and resources. Although different in topology and some other aspects, all the above papers essentially advocate the advantages of using NoCs and regularity as effective means to design high performance SoCs. While these papers mostly focus on the concept of regular NoC architecture (discussing the overall advantages and challenges), to the best of our knowledge, our work is the ﬁr st to address the mapping problem for tile-based architecture and provide an efﬁcient way to solve it. I I I . P LAT FORM D E SCR I P T ION In this section, we describe the regular tile-based architecture and the power model associated to the communication network. A.TheArchitecture The chip under consideration in this paper is composed of n (cid:2) n tiles which are inter-connected by a 2D mesh network2 . Fig. 2 shows an abstract view of a tile in this architecture. As shown in Fig. 2, each tile is composed of a processing core and a router. The router embedded onto each tile One(cid:13) tile(cid:13) (cid:13)h t r o N t u p n I r (cid:13)e f f u b (cid:13)h (cid:13)t r o N t u (cid:13)p u t O Processing(cid:13) Core(cid:13) Router (cid:13) West(cid:13) Input(cid:13) buffer(cid:13) West(cid:13) Output(cid:13) Crossbar (cid:13) Switch(cid:13) buffer(cid:13) East(cid:13) Input(cid:13) East(cid:13) Output(cid:13) b(cid:13)u(cid:13)f(cid:13)f(cid:13)e(cid:13)r(cid:13) P(cid:13)r(cid:13)o(cid:13)c(cid:13).(cid:13) I(cid:13)n(cid:13)p(cid:13)u(cid:13)t(cid:13) P(cid:13)r(cid:13)o(cid:13)c(cid:13).(cid:13) O(cid:13)u(cid:13)t(cid:13)p(cid:13)u(cid:13)t(cid:13) t u (cid:13)p u t O r e (cid:13)f f u b h t u o (cid:13) S t u p n I h t u (cid:13)o S Fig. 2. The typical structure of a tile is connected to the four neighboring tiles and its local processing core via channels. Each channel consists of two onedirectional point-to-point links between two routers or a router and a local processing core. Compared to typical macro-networks, an on-chip network is by far more resource limited. To minimize the implementation cost, the on-chip network should be implemented with very little area overhead. This is especially important for those architectures composed of tiles with ﬁne-le vel granularity. Thus, instead of having huge memories (e.g. SRAM or DRAM) as buffer space for those routers/switches in the macro-network, it’s more reasonable to use registers as buffers for on-chip routers3 . For the architecture in Fig. 2, a 5 (cid:2) 5 crossbar switch is used as the switching fabric because of its nice cost/performance trade-offs for switches with small number of ports. To be able to direct the information appropriately, a tilebased architecture requires a method of routing the data packets through the network. There are quite a few routing algorithms proposed so far. In general, they can be divided into two categories: static routing and adaptive routing [6]. For the tile-based architecture, we believe that the static routing is more suitable than the adaptive routing because: 1. Compared to static routers, implementing adaptive routers requires by far more resources because of their complexity. 2. Since in adaptive routing packets may arrive out of order, huge buffering space is needed to reorder them. This, together with the protocol overhead, leads to prohibitive cost overhead, extra delay and jitter. Based on the above considerations, static XY routing is assumed for the on-chip network. In a few words, for 2D mesh networks, XY routing ﬁrst routes packets along the X-axis. Once it reaches the column wherein lies the destination tile, the packet is then routed along the Y-axis. Obviously, XY routing is a minimal path routing algorithm and is free of deadlock and livelock [6]. 2We use the 2D mesh network simply because it naturally ﬁts the tile-based architecture. However, our algorithm can be extended for other topologies. 3As we will see later, this leads to a much simpler power model compared to its macro-network peer. 234 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) B.TheEnergyModel In [7], Ye et al. propose a new model for evaluating the power consumption of switch fabrics in network routers. To this end, the bit energy (Ebit ) metric is deﬁned as the energy consumed when one bit of data is transported through the router. Ebit can be calculated by the following equation: Ebit = ESbit + EBbit + EWbit (1) where ESbit , EBbit and EWbit represent the energy consumed by switch, buffering and interconnection wires, respectively. (Note that the authors in [7] assume the buffers are implemented in SRAM or DRAM.) Although the above power model is targeted for network routers where the entire chip is occupied by just one router, it can be adapted to the tile-based architecture with the following modiﬁcations: (cid:15) First, in [7], EBbit becomes dominant when congestion happens since accessing and refreshing the memory are very expensive in terms of power consumption. This is no longer true for on-chip networks where the buffers are implemented using regular registers. (cid:15) Second, in [7], EWbit is the energy consumed on the wires inside the switch fabric. For the on-chip network, the energy consumed on the links between tiles should also be included; in the following this is denoted by ELbit . Thus, the average energy consumed in sending one bit of data from a tile to its neighboring tile can be calculated as: Ebit = ESbit + EBbit + EWbit + ELbit (2) Since the link between each pair of nodes is typically in the order of mm, the energy consumed by buffering and internal wires is negligible4 compared to ELbit (EBbit + EWbit (cid:28) ELbit ). Thus, Eq. (2) reduces to: (3) Consequently, the average energy consumption of sending one bit of data from tile ti to tile tj can be calculated as: Ebit = ESbit + ELbit E ti ;tj bit = nhops (cid:2) ESbit + (nhops (cid:0) 1) (cid:2) ELbit (4) where nhops is the number of routers the bit passes on its way along a path. Eq. (4) gives the energy model for the regular tile-based NoC architecture. Without loss of generality, in what follows, we focus on 2D mesh network. Note that, for the 2D mesh network with XY routing, Eq. (4) shows that the average energy consumption of sending one bit of data from tile ti to tile tj is determined by the Manhattan distance between these two tiles. IV. TH E PROBL EM O F EN ERGY-AWARE MA P P ING A.ProblemFormulation Simply stated, our objective is to ﬁgure out, after the designer has selected a set of IPs and assigned/scheduled the 4We implemented a 4 (cid:2) 4 crossbar switch and then evaluated its power consumption with Synopsys design compiler for a 0.18(cid:22)m technology. The results show that EBbit = 0:075pJ , which is indeed negligible compared to ELbit (typically in the order of pJ ). tasks onto these IPs, how to map these IPs onto different tiles such that the total communication energy consumption is minimized, while guaranteeing the performance of the system. To formulate this problem in a more formal way, we need to ﬁrst introduce the following two new concepts: Deﬁnition 1 An Application Characterization Graph (APCG) G = G(C; A) is a directed graph, where each vertex ci represents one selected IP/core, and each directed arc ai;j represents the communication from ci to cj . The following quantities are associated with each ai;j as arc properties: (cid:15) v(ai;j ): arc volume from vertex ci to cj , which stands for the communication volume (bits) from ci to cj . (cid:15) b(ai;j ): arc bandwidth requirement from vertex ci to cj , which stands for the minimum bandwidth (bits/sec.) that should be allocated by the communication network. Deﬁnition 2 An Architecture Characterization Graph (ARCG) G 0 = G(T ; P ) is a directed graph, where each vertex ti represents one tile in the architecture, and each directed arc pi;j represents the routing path from ti to tj 5 . The following quantities are associated with each pi;j as arc properties: (cid:15) e(pi;j ): arc cost from vertex ti to tj , which represents the average energy consumption (joule) of sending one bit of data from tile ti to tj , i. e., E . (cid:15) L(pi;j ): the set of links that make up the path pi;j . ti ;tj bit Using the above graph representations, the problem of minimizing the communication energy consumption under performance constraints can be formulated as: Given an APCG and an ARCG that satisfy size(AP CG) (cid:20) size(ARCG) (5) ﬁnd a mapping function map() from APCG to ARCG which minimizes: minfEnergy = X8ai;j such that: v(ai;j ) (cid:2) e(pmap(ci );map(cj ) )g 8ci 2 C; map(ci ) 2 T 8ci 6= cj 2 C; map(ci ) 6= map(cj ) 8 link lk ; B (lk ) (cid:21) X8ai;j b(ai;j ) (cid:2) f (lk ; pmap(ci );map(cj ) ) where B (lk ) is the bandwidth of link lk , and: (6) (7) (8) (9) f (lk ; pm;n ) = (cid:26) 0 1 : : lk 62 L(pm;n ) lk 2 L(pm;n ) Conditions (7) and (8) mean that each IP should be mapped to exactly one tile and no tile can host more than one IP. Eq. (9) guarantees that the load of any link will not exceed its bandwidth. 5 For 2D mesh network with static routing, this suggests a complete connected graph with exactly one arc from each vertex to any other vertex. 235 B.Signiﬁcance of theProblem To prove that the mapping heavily affects the communication energy consumption, we carried out the following experiment. A series of task graphs are generated using TGFF [8]. Then the output graph is randomly assigned to a given number of IPs, with the computational times and communication volumes randomly generated according to the speci ﬁed distribution. Our tool is then used to pre-process and annotate these task graphs and build the Communication Task Graphs (CTG), which characterizes the partitioning, task assignment, scheduling, communication patterns, task execution time, of the application. Also, the bandwidth requirements between any communicating IP pairs are calculated. The number of IPs used in the experiment ranges from 3 (cid:2) 3 to 13 (cid:2) 13. For each benchmark, we randomly generate 3000 mapping con ﬁgurations and the corresponding energy consumption values are calculated. In parallel, an optimizer using simulated annealing (SA) was also developed with the goal of ﬁnding a legal mapping which consumes the least amount of communication energy. The resulting energy ratios are plotted in Fig. 3. 4.5 4 o i t Random_min/SA_sol Random_med/SA_sol a r 3.5 n o i t p m 3 u s n o c 2.5 2 y g r e n E 1.5 1 0 50 100 Number of tiles 150 200 Fig. 3. The impact of mapping on energy consumption The dashed line in Fig. 3 shows the energy consumption ratio of the best solution among the 3000 random mappings (Random min) to the solution found by the simulated annealing (SA sol). The solid line shows the ratio of the median solution among the 3000 random mappings (Random med) to SA sol. As we can see, although the simulated annealing optimizer does not necessarily ﬁnd the optimal solution, it still saves around 50% energy compared to the median solution for the system with 3 (cid:2) 3 tiles. Moreover, the savings increase as the system size scales up. For systems with 13 (cid:2) 13 tiles, the savings can be as high as 75%. Another observation is that the best solution among the 3000 random mappings is far from satisfactory, even with a system as small as 3 (cid:2) 3 tiles. Unfortunately, the mapping problem is an instance of constrained quadratic assignment problem which is known to be NP-hard [9]. The search space of the problem increases factorially with the system size. Even for a system with 4 (cid:2) 4 tiles, there can be 16! mappings which are already impossible to enumerate, not to mention systems with 10(cid:2) 10 tiles that are anticipated in ﬁ ve years or less [5]. In the following section, we propose an efﬁcient branch-and-bound algorithm which can be used to ﬁnd nearly optimal solutions in reasonable run times. V. TH E A LGOR I THM O F EN ERGY-AWARE MA P P ING A.TheDataStructure Our approach is based on a branch-and-bound algorithm. The algorithm is used to efﬁciently walk through the search tree which represents the whole searching space. Fig. 4 shows an example of the searching tree for mapping an application with 4 IPs onto a 2 (cid:2) 2 tile architecture. To keep the ﬁgure simple, we do not show all the nodes. Root Node xxxx 0xxx 1xxx 2xxx 3xxx Internal Node 01xx 02xx 03xx 20xx 21xx 23xx Leaf Node 031x 032x 0312 0321 230x 231x 2301 2310 Fig. 4. An example search tree In Fig. 4, each node belongs to one of the three categories: root node, internal node, and leaf node. The root node corresponds to the state where no IP has been mapped. Each internal node represents a partial mapping which is tagged by a label. Each number in the label represents which tile the corresponding IP is mapped to. For example, the node with the label “ 23xx” represents a partial mapping where I P0 and I P1 are mapped to T ile2 and T ile3 respectively, while I P2 and I P3 are still unmapped. Each leaf node represents a complete mapping of the IPs to the tiles. To explain how our algorithm works, the following terms need to be deﬁned: Deﬁnition 3 The cost of a node is the energy consumed by the communication among those IPs that have already been mapped. For instance, the cost of the node labeled “ 23xx” can be calculated as v(a0;1 ) (cid:2) e(p2;3 ) + v(a1;0 ) (cid:2) e(p3;2 ). We can infer from deﬁnition 3 that any child node’s cost is no less than its parent node’s cost. This property will later be used in the algorithm to trim away unqualiﬁed sub-trees. Deﬁnition 4 Let M be the set of vertices in the APCG that have already been mapped. A node is called a legal node if and only if, for any link lk , it satisﬁes the following condition: B (lk ) (cid:21) X8ai;j ;ci cj 2M b(ai;j ) (cid:2) f (lk ; pmap(ci );map(cj ) ) (10) Eq. (10) guarantees that all the bandwidth requirements between the currently mapped IPs are satisﬁed. Also, if a node is illegal, then all of its descendant nodes are illegal. Deﬁnition 5 The Upper Bound Cost (UBC) of a node is deﬁned as a value that is no less than the minimum cost of its legal, descendant leaf nodes. 236     Deﬁnition 6 The Lower Bound Cost (LBC) of a node is deﬁned to be the lowest cost that its descendant leaf nodes can possibly achieve. Differently stated, this means that if a node has the LBC equal to x, then each of its descendant leaf nodes has at least a cost of x. B.TheBranch-and-BoundAlgorithm Given the above deﬁnitions, ﬁnding the optimal mapping is equivalent to ﬁnding the legal leaf node which has the least cost6 . To achieve this, our algorithm searches the optimal solution by alternating the following two steps: Branch: In this step, an unexpanded node is selected from the tree, the next unmapped IP is enumeratively assigned to the set of remaining unoccupied tiles and then the corresponding new child nodes are generated. Bound: Each of the newly generated child nodes are inspected to see if it is possible to generate the best leaf nodes later. A node can be trimmed away without further expansion if either its cost or its LBC is higher than the lowest UBC that has been found during the searching (since it is guaranteed that other nodes will eventually lead to a better solution). Obviously, the calculation of the UBC and LBC signi ﬁcantly impacts the speed of the algorithm. Primarily, we want to have tight UBC and LBC for each node so that more nonpromising nodes can be detected and trimmed away early on during the search. Unfortunately, calculating a tight UBC or LBC usually demands more computational time. Next, we describe our method for computing UBC and LBC, which offers a satisfactory trade-off between the average time for processing one node and the number of nodes that need to be processed. (cid:15) UBC calculation By deﬁnition 5, the cost of any legal descendant leaf node can be used as the UBC of that node. Since we want to select the legal descendant leaf node with the smallest cost, we choose the descendant leaf node using a greedy method for mapping the remaining unmapped IPs to the unoccupied tiles. For each step in the greedy mapping procedure, the next unmapped IP ck with the highest communication demand is selected and its ideal topological location (x; y) on the chip is calculated as: i x = P8ci2M (v(ak;i ) + v(ai;k )) (cid:2) cx P8ci2M (v(ak;i ) + v(ai;k )) y = P8ci2M (v(ak;i ) + v(ai;k )) (cid:2) cy P8ci2M (v(ak;i ) + v(ai;k )) (11) (12) i where cx i and cy i represent the row id and column id of the tile that ci is mapped onto, respectively, and M is the set of mapped IPs which is updated at each step. ck is then mapped to an unoccupied tile whose topological location has the smallest Manhattan distance to (x; y). 6 The performance constraints are guaranteed to be satisﬁed by the legality of the node. This step is repeated until all IPs have been mapped. This leads to a complete mapping and thus identi ﬁes a single descendant leaf node. If this leaf node is illegal, then the UBC of the node under inspection is set to be in ﬁnitely large; otherwise, it is set to be the cost of that descendant leaf node. (cid:15) LBC calculation The LBC cost of a node n can be decomposed into three components, as shown in Eq. (13): LBC = Cm;m + Cu;u + Cm;u (13) Cm;m is the cost of the intercommunication among mapped IPs. Since the location of these IPs is known, Cm;m can be calculated exactly. Cu;u is the cost of the intercommunication among unmapped IPs. Eq. (14) is used to calculate Cu;u , where (cid:22)M stands for the set of unmapped IPs and (cid:22)O stands for the set of tiles that have not been occupied yet. Cu;u = 1 2 (cid:2) X8ci2 (cid:22)M X8cj 2 (cid:22)M v(ai;j ) (cid:2) min 8tm tn2 (cid:22)O e(pm;n ) (14) The last item Cm;u stands for the cost of the intercommunication between the mapped IPs and the unmapped IPs. Let M, (cid:22)M and (cid:22)O be the sets of mapped IPs, unmapped IPs and unoccupied tiles, respectively. Cm;u can be derived by: Cm;u = X8ci2M X8cj 2 (cid:22)M v(ai;j ) (cid:2) min 8tk 2 (cid:22)O e(pmap(ci );k ) + X8ci2 (cid:22)M X8cj 2M v(ai;j ) (cid:2) min 8tk 2 (cid:22)O e(pk;map(cj ) ) (15) C.Speed-upTechniques In order to speed up the searching process, it is critical to trim away as many non-promising nodes as possible, as early as possible during the search process. We propose the following techniques for this purpose. (cid:15) IP ordering: We can sort the IPs according to their communication demand7 so that the IPs with higher demand will be mapped earlier. Since the positions of the IPs with higher demand generally have a larger impact on the overall communication energy consumption than those of IPs with lower demand, ﬁxing their positions earlier helps exposing those nonpromising internal nodes at earlier times in the searching; this reduces the number of nodes to be expanded. As most applications have non-uniform trafﬁc patterns, this heuristic is quite useful in practice. (cid:15) Priority queue (PQ): A priority queue is used to sort those nodes that are waiting to be branched based on their cost. The lower the cost of the node, the higher the priority the node has for branching. Intuitively, expanding a node with lower cost will more likely decrease the minimum UBC so that more non-promising nodes may be detected. (cid:15) Symmetry Exploitation: To further speed up our algorithm, the symmetry property of the architecture is exploited. Considering the system with 16 tiles and nodes of depth 1 in the search tree as an example, we only need to investigate those nodes 7 For IP ci , this is calculated as P8j 6=i fv(ai;j ) + v(aj;i )g 237 which map the ﬁrst IP to the tiles denoted by (0; 0), (0; 1) and (1; 1) (see Fig. 1), as the other nodes are just mirrors of these nodes. D.ThePseudocode Fig. 5 gives the pseudo code of our algorithm which also shows how the above speed-up techniques are employed. p u d e e p S 100 50 0 o i t a r y g r e n E 1.5 1 0.5 0 1 2 3 4 5 6 7 8 9 Benchmark 1 2 3 4 5 6 7 8 9 Benchmark Sort the IPs by communication demand root node = new node(NULL) min UBC = +1, best mapping cost = +1 PQ.Insert(root node) while(!PQ.Empty()) f cur node = PQ.Next() for each unoccupied tile ti f generate child node nnew if(nnew ’s mirror node exists in the PQ) continue if(nnew .LBC>min UBC) continue if(nnew .isLeafNode) f if(nnew .cost < best mapping cost) f best mapping cost = nnew .cost best mapping = nnew gg else f if(nnew .UBC<min UBC) min UBC = nnew .UBC PQ.insert(nnew ) gg g Fig. 6. Comparison between SA and our algorithm for category III benchmarks Fig. 6 shows the comparison between our algorithm and simulated annealing for the benchmarks in category III. The left ﬁgure gives the speed up ratios of our algorithm over the simulated annealing algorithm. The right ﬁgure shows the energy ratios of the solutions provided by our algorithm to that generated using simulated annealing. Note that although we have 10 benchmarks for category III, we only show the results for 9 benchmarks here since neither of them can ﬁnd a mapping solution which meets the speciﬁed performance constraints for one of the benchmarks. As it can be seen, our algorithm runs much faster (72 times on average) over SA with very competitive solutions (the difference of the communication energy consumption between the solutions generated by these two algorithms are within 6%, on average). Fig. 5. The pseudo code of the algorithm Energy ratio vs. system size 1 0.5 ) A S / g a l r u o ( o i t a r y g r e n E 238 Obviously, the code shown in Fig. 5 will always ﬁnd the optimal solution. However, as the system size scales up, the run time of this algorithm will also increase drastically. Thus, the following heuristic needs to be used to trade-off the solution quality with run time. The length of the PQ is monitored during the process. When it reaches a threshold value, strict criteria are applied to select the child nodes for insertion into the queue. Suppose we are currently expanding node n. If n is the node in the PQ which has the minimal UBC, then all of its child nodes will be evaluated by the code in Fig. 5 for insertion into the PQ. Otherwise, only the child with the lowest cost among its siblings and the child generated by the greedy mapping will be evaluated for insertion. V I . EX P ER IM EN TA L R E SU LT S A.EvaluationExperiments We ﬁrst compare the run-time and quality of the solution generated by our algorithm to a simulated annealing optimizer (SA)8 . Four categories of benchmarks are generated by the technique described in subsection IV.B. Categories I, II, III and IV contain 10 applications with 9, 16, 25 and 36 IPs, respectively. These need to be mapped onto architectures with the same number of tiles. 8 To make the comparison fair, SA was optimized by carefully selecting parameters such as number of moves per temperature, cooling schedule, etc. Speedup ratio vs. system size 100 50 ) A S / g a l r u o ( o i t a r p u d e e p S 40 0 0 20 0 0 20 40 System size(number of tiles) System size(number of tiles) Fig. 7. Comparison between SA and our algorithm with system size scales up Fig. 7 shows how our algorithm performs compared to SA as the system size scales up. For benchmark applications using 36 tiles, our algorithm runs 127 times faster than SA, on average. Meanwhile, the solutions produced by our algorithm remain very competitive compared to those generated by SA. On average, the energy consumption of the solution generated by our algorithm is only 3%, 6% and 10% for category II, III and IV, respectively. For category I, our algorithm can even ﬁnd better solutions than SA because it can in general walk through the whole search tree due to the small size of the problem. B.AVideo/AudioApplication To evaluate the potential of our algorithm for real applications, we applied this algorithm to a generic MultiMedia System (MMS). MMS is an integrated video/audio system which includes an h263 video encoder, an h263 video decoder, an mp3 audio encoder and an mp3 audio decoder. We partitioned the application into 40 distinct tasks and then these tasks were             assigned and scheduled onto 25 selected IPs [10]. These IPs range from DSPs, generic processors, embedded DRAMs to customized ASICs. Real video and audio clips are then used as inputs to derive the communication patterns among these IPs. Fig. 8 shows the derived CTG of this system based on the simulation result. ME(cid:13) ASIC1(cid:13) Q(cid:13) DSP2(cid:13) Filter (cid:13) DSP6(cid:13) FFT(cid:13) DSP5(cid:13) FP(cid:13) DSP4(cid:13) FS0(cid:13) MEM1(cid:13) FP(cid:13) DSP3(cid:13) VLE(cid:13) ASIC2(cid:13) DCT(cid:13) DSP1(cid:13) MC(cid:13) CPU1(cid:13) IDCT(cid:13) 1(cid:13)6(cid:13)6(cid:13)9(cid:13)1(cid:13) IQ(cid:13) ADD(cid:13) FS1(cid:13) FS2(cid:13) Iterative Encoding1(cid:13) CPU2(cid:13) Iterative Encoding2(cid:13) Bit reservoir1(cid:13) ASIC3(cid:13) 2(cid:13)5(cid:13) Bit reservoir 2(cid:13) PsychoAccoustic Model(cid:13) 2(cid:13)6(cid:13)9(cid:13)2(cid:13)4(cid:13) MDCT(cid:13) 3(cid:13)8(cid:13)0(cid:13)1(cid:13)6(cid:13) 3(cid:13) 6(cid:13)8(cid:13)7(cid:13) 1(cid:13)1(cid:13) 1 9 7 (cid:13) (cid:13) (cid:13) 3(cid:13)3(cid:13)8(cid:13)4(cid:13)8(cid:13) 3(cid:13)3(cid:13)8(cid:13)4(cid:13)8(cid:13) 3(cid:13)3(cid:13)8(cid:13)4(cid:13)8(cid:13) 1(cid:13)6(cid:13)6(cid:13)9(cid:13)1(cid:13) 7(cid:13)5(cid:13)2(cid:13)0(cid:13)5(cid:13) 3 (cid:13) 8 (cid:13) 0 (cid:13) 1 (cid:13) 6 (cid:13) 7(cid:13)0(cid:13)6(cid:13)1(cid:13)7 0 6 1 (cid:13) (cid:13) (cid:13) (cid:13) 2(cid:13)8(cid:13)2(cid:13)4(cid:13)8(cid:13) 8(cid:13) 0(cid:13) Synchronziation(cid:13) ASIC2(cid:13) Multiplexing(cid:13) IDCT(cid:13) DSP6(cid:13) IQ(cid:13) DSP5(cid:13) VLD(cid:13) DSP4(cid:13) MC(cid:13) CPU2(cid:13) ADD(cid:13) FS4(cid:13) MEM2(cid:13) FS5(cid:13) IMDCT(cid:13) DSP6(cid:13) Bit reservoir 1(cid:13) DSP5(cid:13) Huffman(cid:13) Decoding 1 (cid:13) 6(cid:13)4(cid:13)1(cid:13) DSP4(cid:13) 7(cid:13)0(cid:13)6(cid:13)5(cid:13) Buffering(cid:13) Mem4(cid:13) Huffman(cid:13) Decoding 2(cid:13) Bit reservoir 2(cid:13) SUM(cid:13) 3 (cid:13) 6 (cid:13) 7 (cid:13) 2 (cid:13) 3(cid:13)6(cid:13)7(cid:13)2(cid:13) 1 (cid:13) 9 (cid:13) 7 (cid:13) 3(cid:13)6(cid:13)7(cid:13)2(cid:13) 8 5 5 7 4 3(cid:13)8(cid:13)0(cid:13)1(cid:13)6(cid:13) 3(cid:13) 8(cid:13) 0(cid:13) 1(cid:13) 6(cid:13) 1(cid:13)4(cid:13)4(cid:13) 8(cid:13) 0(cid:13) 2(cid:13)5(cid:13) 2(cid:13)8(cid:13)2(cid:13)6(cid:13)5(cid:13) Demulplexi(cid:13) ng(cid:13) ASIC2(cid:13) 7(cid:13)6(cid:13) 4(cid:13) Synchronziation(cid:13) ASIC5(cid:13) 7(cid:13)0(cid:13)6(cid:13)5(cid:13) H263 (cid:13) Encoder(cid:13) MP3 (cid:13) Encoder(cid:13) Mem2(cid:13) Buffering(cid:13) 6(cid:13)4(cid:13)0(cid:13) 6 (cid:13) (cid:13) (cid:13) 4 0 H263 (cid:13) Decoder(cid:13) MP3 (cid:13) Decoder(cid:13) Fig. 8. Communication Task Graph for MMS Applying our algorithm to MMS, the solution is found in less than 7 seconds CPU time. An ad-hoc implementation9 was also developed to serve as reference. The results are shown in Table I. TABLE I POW ER COM PAR I SON B E TW E EN AD -HOC SO LU T ION AND OUR SO LU T ION Movie clips Ad-hoc(mW ) Our alg(mW ) box/hand 374.5 148.3 akiyo/cup 440.5 187.3 man/phone 327.5 120 Savings 60.4% 57.5% 63.4% In Table I, each row represents the power consumption of using two movie clips as simulation inputs, with one clip for the video/audio encoder and the other for the video/audio decoder. Compared to the ad-hoc solution, we observe around 60.4% energy savings, on average, which demonstrates the effectiveness of our algorithm. Simulated annealing is also applied to MMS, and the power consumption of the solution and the run time are shown in Table II together with the results of our algorithm for comparison. TABLE II COM PAR I SON B E TW E EN SA AND OUR A LGOR I THM U S ING MMS SA 181.67 148.8 Our alg. 6.52 148.3 Ratio 27.864 0.997 Run Time (sec) Power (mW ) As shown in Table II, for this complex application, our algorithm generates a better solution with signiﬁcantly less run time compared to the simulated annealing. We should point 9We randomly generated 3000 solutions, from which the one that consumes median energy consumption was chosen as the ad-hoc implementation. out that although for this system which has only 5 (cid:2) 5 tiles, the solving time of SA is affordable, the run time of SA increases dramatically as the system size scales up. For systems with 7 (cid:2) 7 tiles, the average run time of SA increases to 2.2 hours, on average. For systems with 10(cid:2) 10 tiles (which will be available in the near future), our algorithm needs just a few minutes to complete while the run time of SA becomes prohibitive (in our experiments on systems with 10 (cid:2) 10 tiles, the SA did not ﬁnish in 40 hours of CPU time). V I I . CONCLU S ION AND FU TURE WORK In this paper, we addressed the mapping problem for regular tile-based architectures. An efﬁcient algorithm which automatically maps the IPs to the tiles so that the total communication energy consumption is minimized under speciﬁed performance constraints was proposed. By using simulated annealing as reference, we have shown that we can generate high quality solutions with signi ﬁcantly less computational time. Although in this paper we focus on the tile-based architecture interconnected by 2D mesh network with XY routing, our algorithm can be adapted to other regular architectures with different network topologies and different static routing schemes. We plan to advance this research in several directions. Due to the short run time of our algorithm, one possible extension is to combine the IP selection and the task partitioning/scheduling into our framework such that the computation and communication energy consumption can be optimized at the same time. ACKNOW L EDGM EN T S The authors would like to thank Dr. John Darringer and Dr. Youngsoo Shin of IBM T.J. Watson Research Center for insightful comments on this work. "
2003,Multi-parametric improvements for embedded systems using code-placement and address bus coding.,"Code placement techniques for instruction code have shown to increase an SoC's performance mostly due to the increased cache hit ratios and as such those techniques can be a major optimization strategy for embedded systems. Little has been investigated on the interdependencies between code placement techniques and interconnect traffic (e.g. bus traffic) and optimization techniques combining both. In this paper we show as the first approach of its kind that a carefully designed known code placement strategy combined and adapted to a known interconnect encoding scheme does not only lead to a performance increase but it does also lead to a significant reduction of interconnect-related energy consumption. This becomes especially interesting since future SoC bus systems (or more general: ""networks on a chip"") are predicted to be a dominant energy consumer of an SoC. We show that a high-level optimization strategy like code placement and a lower-level optimization strategy like interconnect encoding are NOT orthogonal. Specifically, we report cache miss reduction ratios of 32% in average combined with bus related energy savings of 50.4% in average (with a maximum of up to 95.7%) by means of our combined optimization strategy. The results have been verified by means of diverse real-world SoC applications.","Multi-Parametric Improvements for Embedded Systems using Code-Placement and Address Bus Coding Sri Parameswaran J ¨org Henkel Haris Lekastas Dept. of Comp. Science & Eng. The University of New South Wales Kensington, NSW 2052 sridevan@cse.unsw.edu.au NEC USA Inc. Princeton, NJ henkel@nec-lab.com NEC USA Inc. Princeton, NJ lekatsas@nec-lab.com Abstract Code placement techniques for instruction code have shown to increase an SOCs performance mostly due to the increased cache hit ratios and as such those techniques can be a major optimization strategy for embedded systems. Little has been investigated on the interdependencies between code placement techniques and interconnect trafﬁc (e.g. bus trafﬁc) and optimization techniques combining both. In this paper we show as the ﬁrst approach of its kind that a carefully designed known code placement strategy combined and adapted to a known interconnect encoding scheme does not only lead to a performance increase but it does also lead to a signiﬁcant reduction of interconnect-related energy consumption. This becomes especially interesting since future SOC bus systems (or more general: ”networks on a chip”) are predicted to be a dominant energy consumer of an SOC. We show that a high-level optimization strategy like code placement and a lower-level optimization strategy like interconnect encoding are NOT orthogonal. Speciﬁcally, we report cache miss reduction ratios of 32% in average combined with bus related energy savings of 50.4% in average (with a maximum of up to 95.7%) by means of our combined optimization strategy. The results have been veri ﬁed by means of diverse real-world SOC applications. 1 Introduction The advent of silicon technologies will lead to System on Chips (SOC) that will reach one billion transistor designs within the next few years. A major reason preventing the integration of several hundred million transistors on a single chip (indeed, this would be possible through today ’s mainstream 0.13µ silicon technologies and wafer technologies) is the energy dissipation problem. The per- square-mm and per-time generated heat energy can hardly be carried off-chip without substantial (i.e. costly) effort and thus prevent those designs ﬁnding their way into mainstream consumer products. An additional constraint is implied by mobile computing/communication/entertainment devices which draw their current from capacity-limited batteries. The problem has been addressed at various levels of abstraction starting from new silicon technologies, through gate-level, RT-level, architecturallevel and eventually to system-level approaches. However, it can be observed that many proposed approaches are orthogonal (at least as far as many methods within a certain abstractionlevel are concerned). In other words, power saving/reducing methods are often designed, without having complementary methods in mind and thus complicating or even preventing the effective implementation of one or more other power saving methods. There is currently promising evidence by new research at the system-level that the tuning of parameters of various system parts can lead to substantial power savings. Work in this direction is typically carried out at a high-level of abstraction and thus cannot capture subtle architectural characteristics. In this paper we present the ﬁrst approach that combines, 1 15 Benchmark Compress Mpeg Rstest Before 527685559 420232997 415164716 After 535616238 401858247 410877294 Table 1: Average number of transitions before and after code placement for three benchmarks adapts and optimizes two methods for signiﬁcantly reducing the power consumption of communication dominated systems. We present a code placement strategy that, among others, reduces the communication overhead between CPU and the memory hierarchy. Code placement reduces the power by reducing memory accesses and by reducing the number of wait cycles in the processor. However, code placement does not reduce the average number of bus transitions (see Table 1). Thus a bus encoding mechanism is needed to amplify the power savings in the processor and memory. This encoding leads to ultra-low bus power consumption combined with an effectively increased bus bandwidth that yields a higher system performance as well (we will later on explain why bus power and interconnect power in general will be a major contributor to the system power consumption in future silicon technologies). We achieve interconnect energy savings of 50.4% compared to the case where a single method is applied and thus report energy savings that top all other known stand-alone energy saving techniques addressing SOC interconnect. The paper is structured as follows: the next Section 1.1 gives an introduction to the related work in both code placement and bus encoding. Our techniques for code placement and bus encoding are then introduced in detail in Sections 2 and 3, respectively. The validation environment is given in Section 4 and the experimental part follows in Section 5 and ﬁnally conclusions arrived at in Section 6. 1.1 Related work In the following we report on the most relevant work on the two areas of code placement techniques and bus encoding techniques. Both areas are crucial to our approach which is the ﬁrst one to combine, adapt and optimize two previously separately treated low energy methods to achieve an ultra-low energy dissipating interconnect for SOC designs. Several articles have appeared in recent literature about reducing cache misses by reorganizing data or instructions in the cache. The work on cache misses has predominantly concentrated on data cache optimisations [4][13][12]. Hwu et al, in [1], McFarling in [2] and [3], Chow in [7], Tomiyama and Yasuura in [11], Kirovski et al in [10], Kirk in [8] and [9], Li and Wolf in [5], and Parameswaran in [15] have given various methods to reduce instruction cache misses in microprocessor based systems. All of these systems are at the function level. In [27], algorithms were presented in order to reduce the total cache misses at the assembler block level. In recent work it has been recognized that inter-wire capacitances increasingly contribute to the power consumed on a bus system. Various approaches have tried to address this problem and bus-related power consumption in general. Initial work on bus encoding has been conducted by Stan/Burleson [16]. The basic idea is to transfer an inverted word through the bus whenever it can reduce the Hamming distance between a word and its previous word. Later in [17], they introduced Limited-Weight Codes (LWC) for low power encoding and provided optimal statistic performance for random data. The above schemes belong to the class of space-time redundant encoding, where bus sizes are augmented. While the above encodings were developed for random input, researchers started to address data source properties. Panda/Dutt [20] developed a scheme to map arrays in memory for reducing energy on address bus. Exploiting the characteristic that consecutive memory accesses tend to have a consecutive addresses, Mehta et al. introduced Gray code for address bus [21]. To further reduce the energy on an address bus, Benini et al. proposed a prediction scheme taking high regularity of data on address buses into consideration [23]. E. Musoll et al. [18] proposed the WZE (WorkingZone Encoding) scheme to exploit locality of memory reference. Meanwhile, theoretical approaches for bus encoding were developed. In [25], Ramprasad et al. started to use a general communication model to analyze bus-encoding schemes giving lower bounds on average signal transition activities. The work introduced so far is focussed on reducing transition activities on a bus based on the assumption that the inter-wire parasitic capacitances are negligible. However, with the advent of deep sub-micron technology, inter-wire parasitic capacitances become a major issue. Sotiriadis/Chandrakasan stated in [22] that simply minimizing transition might not lead to optimal power reduction. They developed a model to incorporate the inter-wire capacitances of a bus and search the code space to ﬁnd the best codes for inputs based on their bus power model. In the work conducted by Kim et al. [24], two new schemes have been introduced for low power buses. Schemes addressing data properties for deep sub-micron technologies are proposed by Henkel/Lekatsas [19] through re-arranging bus lines and then applying local bus invert. Macchiarulo et al. [26] have shown that the layout of an address bus can be arranged for low power consumption. 1.2 Motivation and Focus Deep sub-micron silicon designs of 0.10u and beyond lead to a shift in optimization strategies for SOCs for several reasons: 1. due to the small feature sizes, inter-wire capacitances of, for instance, bus lines become dominating (compared to intrinsic bus line capacitances). Hence, the relative share of energy consumption of the SOC buses compared to all other components will increase by up to around 30% in future designs. 2. architectural optimizations like improved code placement techniques do not only increase the performance (as shown in [27], they may also dramatically change the extension of the trafﬁc on the buses involved. For example, an improved code placement technique might lead to a higher cache hit ratio and thus a) reduce the number of related bus transactions between the cache and the main memory b) shift the bus trafﬁc to the processor-to-cache bus instead and c) decrease the amount of total related bus transactions. This leads to interdependencies that were previously not considered to be orthogonal. Now, with deep sub-micron designs emerging, code placement (and other, higher level optimization techniques for SOCs) has a direct impact on the busrelated energy consumption and as such in ﬂuences the energy consumption of the whole SOC. Previously considered relevant was the energy savings that come with the reduced execution time (improved performance) of an application with a more efﬁcient code placement. Now, it does matter in which way a code placement technique implicitly shifts transactions from one bus to another and it does matter how efﬁciently these buses make use of bus encoding schemes to reduce the energy consumption. In this context it also does matter how long (physically) those buses are inducing a direct relationship between a code placement technique and physical parameters. This work focuses on these relationships and presents, as the ﬁrst approach, a quantiﬁcation and optimization of interdependencies between: a) code placement on the one side and processor-tocache and cache-to-main memory bus lengths on the other side b) code placement and energy-saving bus encoding schemes We will show that these interdependencies and their exploitation lead to a reduction of 50.4% in average (maximum of 95.7%) of the address bus energy consumption of a whole SOC. 1.3 Assumptions The following assumptions hold for the approach introduced later in this section: 1) The systems considered are single microprocessor systems, with memory and instruction cache which is conﬁgured for a single application. This is quite common in embedded systems. 2) The size of code block placed is no bigger than the size of the cache. This assumption is quite valid in embedded systems where the basic blocks are usually small enough to ﬁt into small cache sizes. If the task is too large for the cache it is possible to break up the task into smaller granules such that each granule will ﬁt into the cache. 3) Only Level-1 caches are available for use. Once again in an embedded system, where frequently there is no cache at all, it is unlikely that more than a single level of cache is going to be available for use. 4) The caches are direct mapped. High-speed systems frequently use direct mapped systems in order to speed up the system as much as possible. This assumption makes it easier to analyse due to the deterministic mapping to cache from memory. 5) The problem is sufﬁciently large so that the total size of the instructions (in bytes) are several times larger the size of cache. This is a reasonable assumption in a realistic system. 2 Allocation of Assembly Level Basic Blocks in Cache and Memory This section details the code placement methodology. Note here that the methodology looks at the code at the assembler level. The basic blocks here are blocks of assembler instructions which are executed together. This methodology contains an algorithm with two parts. The ﬁrst part places basic blocks in the cache so that basic blocks with high frequency are swapped out as little as possible. The second part of the algorithm takes the placed basic blocks and maps them into main memory. This algorithm is performed as a preprocessing step, taking the application ’s original instructions in memory and re-mapping them to different locations. 16 n a i m y r o m e m e h c a c I 1 2 3 4 5 6 1 2 3 4 5 6 Figure 1: The memory allocation example In order to re-map instructions, it was necessary to identify basic blocks. After the identiﬁcation of basic blocks, we had to identify which blocks executed consecutively. We identiﬁed them by running the application through an instruction set simulator and ﬁnding blocks of instructions which were always executed together. The number of basic blocks within applications under consideration varied from 100 - 900. A more comprehensive study of the approach is given in [27]. 2.1 Part 1: Cache Allocation The methodology used for ordering basic blocks in the cache is as follows. All the loops containing a particular basic block are grouped into a single super loop. Thus a loop will be only a member of one super loop. Each super loop’s execution frequency (fsl ) is deﬁned as the addition of all the execution frequencies of the component loops. The super loops are ordered in descending order of frequency. The ordered super loop list is given as sl1 , sl2 , ...slp . For super loop sl 1 , the basic blocks within it are taken in order (from highest to lowest frequency of execution of basic blocks - fb ) and these basic blocks (only whole basic blocks are allowed) are allocated to the cache from the lowest address to the highest until the cache is completely ﬁlled or there are no remaining basic blocks within that super loop. Once this step is ﬁnished, and if there are any remaining basic blocks, we ﬁnd the largest basic block from the remaining basic blocks of sl1 . This large basic block is allocated to the bottom of the cache, say with starting address A ls , and ending at the end of the cache. After this we take the next largest basic block and allocate its starting address in the cache to A ls . The ending address will be less than the ﬁnal address of the cache. Then if another unallocated basic block can be found which can go into the space (below the basic block we just allocated, and above the last cache address), we allocate that basic block into the available space. We keep doing this until we reach the end of the cache. We take the next largest unallocated basic block, and start it at address Als and we repeat the process until all basic blocks are allocated. This is then repeated for all the other super loops in the ordered list. 2.2 Part 2: Memory Allocation The memory allocation part of the algorithm takes the already placed basic blocks in the cache and directly maps them to the memory. Figure 1 shows an example of how the basic blocks are mapped to memory from the cache. In this ﬁgure blocks 1 to 5 are mapped directly on to the memory, but the block 6 is mapped to some memory locations further away, such that the mapped block will end up in the desired position in the cache. Thus if a basic block is mapped to the location from t x to ex in cache of the processor, then the basic block can be placed tx + i ∗ S to ex + i ∗ S , where i is a positive integer and S in memory in any one of the address ranges from addresses is the size of the cache. However, since basic blocks in the cache will wrap around the cache, an offset Z r , can be added to each basic block allocated from super loop sl r , and the basic block can be placed from memory location t x + Zr + i ∗ S to memory location e x + Zr + i ∗ S . This introduction of the offset 17 allows the reduction in size of the total memory needed for the system. 2.2.1 Algorithm Part 1. ordering basic blocks in cache For Each super loop in ordered list { Until Cache is ﬁlled { Allocate basic blocks to the cache in descending order of frequency fb until no more blocks can be allocated } Reorder Unallocated basic blocks in order of size and place in list BBu (BBu = bbu1 , bbu2 , bbu3 ...bbuy , where y is the number of unallocated basic blocks for that super loop) Allocate bbu1 from address Als to end of cache (where Als = S - sizeof (bbu1 )) Remove bbu1 from BBu Repeat until all basic blocks are allocated { Find the next largest unallocated Task bbup from the list BBu Allocate bbup from address Als to Ale where Ale = Als + sizeof (bbup ) Mark bbup from list BBu as allocated Move along the list BBu and place as many basic blocks as possible between Ale and S Mark placed basic blocks as allocated }} Part 2. Memory Allocation Let us assume that the super loops used in the system are sl1 , sl2 , sl3 ...slk . The associated cache is C . For each super loop sl r , the basic blocks to be executed in that super loop are as follows: bb 1 , bb2 , bb3 ......bbm where m is the number of basic blocks. Reorder super loops from largest sum to smallest sum of total basic block size and name them sla , slb , slc ... For all basic blocks ordered in the cache allocation order in sla do { While basic block is not allocated do { If memory locations tx + i ∗ S to ex + i ∗ S is free then Map basic block to address tx + i ∗ S to ex + i ∗ S i = 0 Else i + + } } loop until the end of the super loop list do { For all basic blocks ordered in descending order of size in the next super Allocate largest basic block in the ﬁrst available contiguous memory block (Mx to My ), which will hold the basic block Calculate Zr = (MxmodS ) − tx , where tx is the address in which the basic block being allocated starts in the cache at address 0 and slr is the present super loop under consideration While basic block not allocated do { If memory locations tx + Zr + i ∗ S to ex + Zr + i ∗ S is free then Map basic block to address tx + Zr + i ∗ S to ex + Zr + i ∗ S Else i++ }} 3 Enhancing Code Placement’s Efﬁciency through Adaption of Bus Coding The code placement algorithm introduced in Section 2 reduces the number of cache misses. Hence, the trafﬁc on the CPU-tocache bus and the cache-to-main-memory bus is signiﬁcantly reduced, leading to a higher performance of the whole system and a decrease of the interconnect energy dissipation. The aim of this Section is to adapt a bus encoding scheme that ampliﬁes these two effects even further and thus leads to an ultra-low bus power consumption combined with an effectively increased bus-bandwidth that yields a higher system performance as well. Since we target sub-0.10µ technologies it is necessary to also provide means for cross-talk reduction since signal integrity is another major concern. In the following we introduce the bus encoding scheme to address these problems. Normalized coupling value 00 01 10 11 i, j t i, j t+1 00 0 0 0 0 01 1 0 2 1 10 1 2 0 1 11 0 0 0 0 s e n i l s u b Table 2: Normalized coupling capacitance for all transition combinations of two adjacent bus lines i and j 3.1 Buses in Deep-submicron Designs The closer geometrical proximity of adjacent bus lines in sub0.10µ technologies leads to effects that are almost negligible in technologies not as advanced as 0.10µ and below: two adjacent bus lines form a parasitic capacitance between them. This effect does not only lead to cross-talk and delay effects, it also leads to an increased power consumption since the parasitic capacitance is charged and discharged when there is a voltage swing between two or more bus lines. Thus, each bus line’s capacitance can be represented as Ci = CB + CC,lef t + CC,right (1) where CB is the base (or intrinsic) capacitance (capacitance between bus line and metal layers) and C C,lef t , CC,right are the left and right coupling capacitance between bus line I and it’s left and right neighbor (if any) respectively. Table 2 shows the normalized coupling capacitance C C between a bus line i and one of its neighbors j according to the values the bus lines take at time T1 = t and T2 = t + 1. Obviously, the coupling effect is highest when both lines are subject to a transition in the opposite direction. We have measured the absolute capacitances CB and CC for a 0.10µ technology: CB = 42.22pF /m CC = 35.89pF /m According to Equation 1 and the table above, the maximum capacitance for a bus line I we can expect is: Ci,max = (42.22+2×35.89+2×35.89)pF = 185pF /m (2) Compared to the case where the inter-wire capacitances are negligible (i.e. C i,max = CC = 42.22pF /m this is 4.4 times higher. This is why inter-wire effects have to be taken into consideration. There are several means to diminish or at least reduce the problem of inter-wire capacitances: a) Widen the distance between bus lines: this is typically not preferred since the total area of the bus systems grows too large. b) Use P&R place & route tools that avoid side-by-side routing of bus lines. This is what is actually done in the newest generation of P&R tools. However, the interconnect complexity of one billion transistor SOCs with multiple bus hierarchies and long buses with many cores connected to them will prevent a satisfying solution at a feasible routing time (complexity of the routing problem). c) Change the geometrical shape of bus lines: the bus lines themselves can be re-shaped. For example, the cross-sectional shape can be made narrower such that the distance between two bus lines increases without sacriﬁcing space for the whole bus. However, the main disadvantage of this approach is that the cross-sectional area of a bus line is ﬁxed, since the current-per-area ratio is ﬁxed for any certain technology. That typically leads to solutions where the bus line is buried deeper into the substrate with the height 18 4 = w w . . . '0' . . . . . . '1' '0' '1' '1' . . . h=x =x +3 h l i=l l=x l T=t -1 0 T=t 0 t Figure 2: a) a32-bit bus partitioned in 8 windows of 4bit each b) and c) examples for calculating the TA measure for two cases within a window being larger than the width of a bus line. However, even though the inter-wire capacitance decreases due to a decreasing distance between bus lines, it does increase due to the increased ﬂank area of two opposing bus lines. In conclusion: what is won through a wider distance has to be, at least partly, given up through the effect of larger ﬂank areas. d) Bus encoding techniques that take inter-wire capacitances into consideration when words are transmitted via a bus system. Within this section, we focus on the latter technique, namely on ﬁnding a bus encoding technique that compliments the code placement technique introduced earlier and thus leads to an ultra-low bus power consumption combined with an effectively increased bus-bandwidth that yields a higher system performance 3.2 Reducing Power and Increasing Signal Integrity In the following we introduce an encoding method that solves the problems discussed in sub-section 3.1 and that serves as an enhancement to further optimize the advantages achieved through the code placement techniques from Section2. Let us ﬁrst deﬁne what we call a window: wl,h (ww) = {l, h| h − l = ww − 1, h > l, 0 ≤ (h, l) ≤ bw − 1} (3) with l , h being the lower and upper border bit positions of the window, respectively, ww the window size in bits and bw the bus size in bits. Now, let us ﬁrst deﬁne what we call the Transition Activity TA for a window w l,h (ww). In order to make the formula easier readable we simply use w to denote the window. Furthermore, let us assume that bx is the x ’th bit within a window with B x being the value of that bit (i.e. B x ∈ {0, 1}). Thus, we can deﬁne the TA measure as follows: + Bi ⊕ B−1 i (cid:3) · (cid:1) (Bi ⊕ Bj )) (cid:1) (cid:2) (cid:3) (cid:2) Bi ⊕ B−1 i TA(w) = ∀bi∈w ( ∀bj ∈w,bj (cid:3)=bi (4) i Thereby B −1 gives the value of bit b i at time t − 1 i.e. the temporal predecessing value. Thus, B i ⊕ B−1 determines whether bit bi has a high/low or low/high transition (=1) or not (=0). Accordingly this speci ﬁc bit will contribute to the TA measure or not. Figure 2 gives an idea on how TA is measured using an example. There, the portion of the TA measure contributed by i = a+ 1 is demonstrated. The dotted line shows the scope that is important for the calculation of the respective TA portion. It equals to 2. It is very important to note that TA as shown does NOT violate i is because the bus word referring to time T = t − 1 is stored the causality principle as it might seem from the Figure 2. This in a register. But even the bus word for time t is stored in a register since the word is not yet put on the bus (it is just in the I/O register of a device) and thus TA does work as intended by Equation 4. According to Equation 4 every value of a bit different to the bit under review is contributing 1 or 0 to the value of TA depending on whether it is different in value or not. That each contribution is equally sized (1 or 0 with no other values allowed) is justiﬁed by our capacitance measure that gives us values of base capacitance compared to coupling capacitances of the closest neighbors (a maximum of three left or right neighbors in a 4-bit window) that are approximately the same and thus contribute the same to the power/energy consumption. Window sizes larger than 4 bit yielded lower energy savings since such a model would assume that inter-wire capacitances reach far beyond the closest neighbor (which is actually not the case). Window sizes less than 4 bits on the other side might be more beneﬁcial in terms of power savings (3 would be ideal since it exactly reﬂects the physical relationship of adjacent bus lines) but the additional hardware effort cannot be justiﬁed. In the next step we use the TA as a measure to determine whether we should invert the information in the window or not. Please note that the TA scheme is able to measure the impact of coupling capacitances. A Hamming Distance measure, as used for regular invert schemes would not lead to a reasonable improvement in power/energy consumption. It would only reduce the number of transitions. But the number of transitions does not necessarily reﬂect the amount of power/energy that is consumed. Our whole scheme works according to the following procedure: For all windows the TA measure is calculated Strategy of the Scheme 1) For All windows wi ∈ W 2) determine TA(wi ) 3) If 4) Then 5) 6) 7) 8) 9) 10) 11) done. If Then For All windows wi ∈ W hi ta > (#windows)/2 TA(wi ) > T Amax (ww)/2 hi ta+ = 1 invert(wi ) Figure 3: The strategy of the Scheme (lines 1-2). If the measure exceeds half of the maximum value (dependent on the window size ww) then it is counted (lines 3-5). After all TA measures are calculated, it is determined whether more than half of the windows have a high TA value (Equation 4) If that is the case the information in the windows is transmitted inverted. Please note that decoding can be done inversely. Only 1 extra bit line is used for that since all windows will be inverted or not (majority vote). Also, note that this code explains only the strategy. It does not in any way reﬂect the implementation that, of course, is in hardware. Hardware related issues The design bus encoding interface including an encode/decode pair uses approximately 400 gates; it does not incur an additional clock cycle; the critical path is between 2-3ns)).The whole encoding scheme has been designed with signal integrity in mind since this is another major issue in sub-0.10µ designs. As explained earlier, the scheme aims to minimize the switching activities within a certain window as the TA measure (Equation 4) shows. That means that the probability of switching within a window is being reduced and thus reduces 19 Power Models Performance Data Power Data ISS CPU Bus I$ D$ Main Memory Dinero QPT Core Placement Bus Lengths + Coding Code Placement Executable Code Figure 4: The whole set-up for power and performance estimation and including the focus of this work i.e. code placement strategy adapted to bus encoding and bus length determination. the probability of violating the signal integrity through, for example, crosstalk between two adjacent bus lines. Bus lines located at the border of a window may still interfere with bus lines located at the border of adjacent windows. But note that due to the scheme, this effect is not any larger than in the nonencoded case. Minimizing the remaining border-to-border effects could be achieved by increasing the window size. However, this would decrease the efﬁciency of the encoding scheme and thus it is contrary to the low power goal. 4 Validation Environment We explain the experiments in Section 5, but in prelude to it we brieﬂy introduce our validation environment. It is the main goal of this work to show the efﬁciency of combining code placement and bus coding for an ultra-low power bus/interconnect for an SOC. These two methodologies are highlighted in a dashed box in Fig. 4. The bus lengths of the involved buses (i.e. buses between instruction cache and main memory and instruction cache and CPU) are crucial parameters for the power consumption. The lengths are determined by the results of the core placement (memory, cache and CPU). The results of the code placement and the bus coding scheme are fed into the power models of instruction cache and the bus system, respectively. Further power models in the environment are a CPU power model, a data cache power model and a main memory power model. All models plus the code placement and bus encoding mechanisms are fed by instruction traces through the ”QPT” /”Dinero” tool set sequence [6]. The output of the environment is power and performance data. For more detailed information please refer to [14]. 5 Experiments and Results The target system the experiments were conducted on is shown in Fig. 5: it shows a chip layout with the interesting parts magI $ ”), and the main ni ﬁed: the CPU, the instruction cache (” memory banks. The buses that are affected by our code placement and bus encoding methodologies are buses ”Bus1” and ”Bus2 ” . The length and/or the ratio of the lengths of these buses varies with the placement and relative size of all cores comprised within this SOC. Hence, the bus power consumption will not only depend on our methodologies (see Sections 2 Peripherals ASIC Graph Ctr Memory Banks CPU I$ D$ Memory Banks Bus2 Bus1 . r i D CPU I$ D$ t n e m e c a l P Placement Dir. Figure 5: Chip-layout and buses ”Bus1” and ”Bus2” that are subject to extension/contraction according to the placement of the involved cores. and 3) but also on the geometrical characteristics of ”Bus1 ” and ”Bus2” . This is one of the parameters that will be investigated in this section. The experiments were conducted with the evaluation environment shown in Fig. 4. Here are the main steps: 1) Placing the instruction code according to Section 2. 2) Generating traces for the new code allocation. 3) Running the traces through the bus encoding scheme (Section 3). 4) Measuring power and performance with the evaluation environment. 5) Varying instruction cache sizes. 6) Varying the ratios of bus lengths ”Bus1” to ”Bus2 ” (see Fig. 5) according to different placement scenarios of the affected cores. 7) Repeating steps 1)-4) for all applied combinations of parameters. We performed experiments on a set of ﬁve applications. The applications have been chosen with as much variety in characteristics as possible in order to show the wide application area of the methodology. Thus, the applications varied in size (8k to 200k), application area (video, animation, algorithmic etc.) and application domain (data dominated or control dominated). The applications used were: a complete MPEGII video encoder mpeg, a video trick animation algorithm trick1, the Whetston benchmark sequences whetston, the unix command compress compress, and a chromakey video mixer as part of a digital video studio equipment. The results achieved are shown in Table 5. The ﬁrst column gives the application name and the number of instructions executed for that application. The second column gives the cache sizes. The third column gives the cache miss rates before code placement and the fourth column gives the miss rates after code placement. The next ﬁve columns (columns 5-9) are results which have been obtained by simulating with bus lengths of 0.2mm for the cpu-cache bus and 3.8mm for the cachememory bus. The ﬁfth column gives the energy expended for a system without optimization. Column six shows the the energy expended with address coding only, and the seventh column shows energy consumption in the busses if only code placement was performed and ﬁnally in column eight we show the energy consumption when both address coding and code placement methodologies are applied. Column nine shows the percentage improvement between column eight and column ﬁve. Columns 10-14 are in a similar format to Columns 5-9 except that they are results of a simulation with 0.5mm and 3.5 mm for the respective bus lengths. Likewise, Columns 15-19 are results of a simulation with 0.8mm and 3.2 mm for the respective bus lengths. 20 Maximum Energy savings for each of the bu 0.2/3.8mm 0.5/3.5mm 0.8/3.2mm g n i v a  S 120.0% 100.0% 80.0% 60.0% 40.0% 20.0% 0.0% mpeg trick1 whetston compress rstest Application Figure 6: The graph of Max Savings As can be seen from the ﬁgures, the energy savings of the bus system are quite substantial. The maximum savings achieved for each of the applications with differing bus lengths are given in Figure 6. Even though the energy consumption of the buses using current mainstream silicon technologies (i.e. 0.18µ, 0.13µ) is only around 5% to 20% of the whole SOC, it is anticipated that in sub-0.10µ technologies this portion raises to 20% to 30% [28]. On average, for the applications investigated in our experiments, the energy consumption of the bus system is reduced by 54.6% for the 0.2/3.8 mm (”Bus1 ” / ”Bus2 ” ) bus case, 50.4% for the 0.5/3.5 mm bus case and 46.6% for the 0.8/3.2 mm bus case. Another factor is the relative lengths of the busses: at some point the relative length of the busses are going to be of importance. If the two busses were of equal length, then the energy consumption might have even increased. At that point the designer will have to decide upon the relative merits of energy saving, crosstalk reduction and performance. Typically, ”Bus1 ” will be much shorter than ”Bus2 ” since this is the preferable outcome of place&route according to the sizes of the involved cores (instruction cache, main memory banks, CPU). Our methodology favors this tendency as we can observe that results are best for a smaller ”Bus1 ” to ”Bus2” ratios for reasonable cache misses. As the cache misses increase beyond 50% the energy dissipation does not always favor smaller bus ratios. Note that the best results were achieved for ”feasible” instruction cache sizes i.e. cache sizes that are neither too small nor too large as to ﬁt the entire code in the instruction cache anyway. These are design points chosen by designers as the best compromise between effort (i.e. costs) and results (e.g. performance, power). In those cases (bold highlighted) we can always achieve a large drop in cache misses. The implications are a higher performance and a lower energy consumption. Also, the performance of the entire system increases substantially because of the reduction in the cache misses (due to the code placement strategy) as can be seen from columns 3 and 4. Thus we can state that the two combined methods should be used in conjunction to produce ultra low power systems with increased performance and reduced crosstalk. For certain applications it can be seen that address coding alone will produce superior results to that of the combined scheme (see Compress with a cache size of 64 in Table 2). This is due to the fact that the code placement algorithms has a number of jump instructions which are further away than in the noncode-placed algorithm and this is causing it to increase the bus activity. Note that for this application there was little reduction Bus length ratio 0.2/3.8 0.5/3.5 0.8/3.2 Appl. # Instr I-$ Size Miss Ratio Code-placed Miss Rat. No-opt 105J A Only 105J C only 105J A&C 105J % Imp. No-opt 105J A Only 105J C only 105J A & C 105J % Imp. No-opt 105J A Only 105J C only 105J A & C 105J % Imp. 128 256 512 1024 2048 72.94% 56.20% 47.85% 32.93% 2.60% 71.34% 51.26% 36.35% 20.77% 1.39% 6.63 5.22 4.51 3.25 0.69 4.00 3.15 2.72 1.96 0.42 6.61 4.72 3.62 2.19 0.58 4.33 3.03 2.52 1.53 0.44 34.7% 41.9% 44.1% 53.0% 36.3% 6.31 5.13 4.54 3.49 1.36 3.81 3.10 2.74 2.11 0.82 6.30 4.71 3.80 2.60 1.27 4.13 3.02 2.65 1.81 0.96 34.6% 41.2% 41.7% 48.1% 29.6% 5.98 5.04 4.57 3.73 2.02 3.61 3.04 2.76 2.25 1.22 6.00 4.69 3.98 3.00 1.95 3.93 3.01 2.77 2.09 1.47 34.4% 40.4% 39.3% 43.9% 27.3% mpeg 22406459 128 256 512 1024 2048 100.00% 99.58% 87.70% 71.17% 17.70% 100.00% 97.16% 62.15% 0.04% 0.00% 37.91 37.76 33.49 27.56 8.35 22.45 22.36 19.83 16.31 4.49 38.52 30.67 22.91 2.13 1.93 22.31 15.31 14.89 1.19 2.09 41.2% 59.5% 55.5% 95.7% 75.0% 34.92 34.79 31.24 26.29 10.29 20.67 20.60 18.49 15.56 6.09 35.48 28.32 22.23 5.30 4.82 20.55 14.14 14.45 2.96 3.64 41.2% 59.4% 53.8% 88.7% 64.6% 31.93 31.83 28.98 25.02 12.22 18.90 18.84 17.16 14.81 7.23 32.44 25.98 21.54 8.47 7.70 18.79 12.97 14.00 4.73 5.82 41.2% 59.3% 51.7% 81.1% 52.4% trick1 1.03E+08 128 256 512 1024 97.25% 69.46% 6.18% 0.06% 93.51% 47.81% 6.10% 0.06% 0.48 0.35 0.05 0.03 0.33 0.24 0.04 0.02 0.47 0.24 0.05 0.02 0.32 0.17 0.04 0.02 33.4% 50.9% 31.1% 30.9% 0.44 0.34 0.09 0.07 0.30 0.23 0.06 0.04 0.43 0.24 0.09 0.06 0.30 0.17 0.06 0.04 33.2% 48.3% 30.9% 30.9% 0.41 0.32 0.12 0.10 0.28 0.22 0.08 0.07 0.40 0.25 0.12 0.10 0.27 0.17 0.09 0.07 32.9% 45.5% 30.8% 30.9% whetston 1749402 64 128 256 512 1024 2048 89.79% 74.49% 54.73% 13.33% 2.39% 0.83% 89.79% 53.55% 19.69% 2.92% 0.48% 0.10% 12.52 10.51 7.91 2.48 1.04 0.84 8.11 6.81 5.13 1.61 0.68 0.54 12.57 7.77 3.10 1.06 0.77 0.70 8.30 5.15 2.18 0.77 0.55 0.54 33.6% 51.0% 72.5% 68.8% 47.4% 35.9% 11.65 9.97 7.81 3.28 2.08 1.91 7.55 6.46 5.06 2.13 1.35 1.24 11.69 7.69 3.75 2.05 1.81 1.73 7.73 5.10 2.64 1.49 1.30 1.33 33.6% 48.8% 66.2% 54.6% 37.7% 30.6% 10.78 9.44 7.71 4.08 3.13 2.99 6.98 6.11 4.99 2.65 2.03 1.94 10.82 7.62 4.40 3.03 2.86 2.76 7.15 5.05 3.10 2.20 2.05 2.12 33.6% 46.5% 59.8% 46.0% 34.4% 29.1% compress 53280973 128 256 512 1024 2048 86.86% 49.23% 43.79% 23.57% 23.52% 84.16% 16.30% 4.35% 1.30% 0.01% 7.79 4.62 4.16 2.45 2.45 4.44 2.63 2.37 1.40 1.40 7.72 1.88 0.79 0.44 0.47 4.26 1.13 0.52 0.30 0.28 45.4% 75.6% 87.5% 87.7% 88.4% 7.27 4.63 4.25 2.83 2.82 4.14 2.64 2.42 1.61 1.61 7.23 2.36 1.39 1.09 1.19 3.99 1.41 0.92 0.75 0.71 45.2% 69.5% 78.3% 73.6% 74.9% 6.75 4.64 4.33 3.20 3.19 3.85 2.64 2.47 1.82 1.82 6.74 2.84 2.00 1.74 1.90 3.72 1.70 1.32 1.19 1.14 45.0% 63.3% 69.5% 62.8% 64.5% rstest 28235416 Table 3: Table of results in cache misses. We mentioned earlier that the signal integrity has been a concern in this work even though performance and power consumption were the main goals: a) The code placement method leads to a decreased trafﬁc on the CPU-to-cache bus and the cache-to-mainmemory bus. This reduced trafﬁc is not traded against an increased trafﬁc elsewhere. Consequently, there is a lower vulnerability through crosstalk just through the minimized trafﬁc on the buses. b) In a second step, the data on these buses is encoded to increase signal integrity (see Section 3). 6 Conclusions In this work we have expoited the interdependencies between a high-level optimization technique, namely code placement, and a lower-level optimization technique, namely bus encoding. It could be shown that these previously orthogonally handled techniques are in fact interdependent on each other due to the increasing inﬂuence of deep sub-micron effects. As a result we have achieved much higher interconnect energy savings than any of these methods can achieve when applied solely (according to related research): the average SOC interconnect energy savings are 50% with a maximum of 95.7% The performance improvements are shown by large reductions in cache misses.. We have validated the results by means of real-world SOC applications that range in size between 8k and 200k lines of code. As an added beneﬁt, the probability of crosstalk effects is reduced by both code placement and bus encoding techniques. "
2005,System-level communication modeling for network-on-chip synthesis.,"As we are entering the network-on-chip era and system communication is becoming a dominating factor, communication abstraction and synthesis are becoming the integral part of system design flows. The key to the success of any design flow are well defined abstraction levels and models, which enable automation of early validation, synthesis and verification. In this paper, we define system communication abstraction layers and corresponding design models that support successive, stepwise refinement from abstract message passing down to a cycle accurate, bus-functional implementation. Experimental results show the benefits of our definitions and design flow.","System-Level Communication Modeling for Network-on-Chip Synthesis Andreas Gerstlauer, Dongwan Shin, Rainer D ¤omer, Daniel D. Gajski Center for Embedded Computer Systems University of California, Irvine, USA fgerstl,dongwans,doemer,gajskig@cecs.uci.edu Abstract(cid:151) As we are entering the network-on-chip era and system communication is becoming a dominating factor, communication abstraction and synthesis are becoming the integral part of system design (cid:3)ows. The key to the success of any design (cid:3)ow are well-de(cid:2)ned abstraction levels and models, which enable automation of early validation, synthesis and veri(cid:2)cation. In this paper, we de(cid:2)ne system communication abstraction layers and corresponding design models that support successive, stepwise re(cid:2)nement from abstract message-passing down to a cycleaccurate, bus-functional implementation. Experimental results show the bene(cid:2)ts of our de(cid:2)nitions and design (cid:3)ow. I . IN TRODUCT ION As SoCs grow in complexity and size, on-chip communication is becoming increasingly important. Furthermore, new classes of optimization problems arise as communication delays and latencies across the chip start dominating computation delays. In other words, simple (e.g. bus based) communication architectures are not suf(cid:2)cient any more. Therefore, as we enter the network-on-chip (NoC) era, new network-based communication architectures and design (cid:3)ows are needed. Communication design for SoCs poses unique challenges in order to cover a wide range of architectures while at the same time offering new opportunities for optimizations based on the application-speci(cid:2)c nature of system designs. The goal is therefore, to develop a corresponding NoC communication design (cid:3)ow that enables rapid design space exploration through design automation in order to achieve the required productivity gains while supporting a wide range of implementations. In order to automate the NoC design process, a well-de(cid:2)ned design (cid:3)ow with clear and unambiguous abstraction levels, models, and transformations is required. The key to the success of this approach are properly de(cid:2)ned design models. Arbitrary models without clear semantics do not enable synthesis and veri(cid:2)cation. For example, only subsets of hardware description languages such as VHDL or Verilog are synthesizable or veri(cid:2)able. In addition, synthesis requires clear de(cid:2)nitions of the target architecture and the set of synthesis steps to transform the input model into the target model. In this work, we aim to de(cid:2)ne such models, design steps, and corresponding model transformations that are necessary for an automated network-on-chip design (cid:3)ow. Note that due to space limitations, this paper can only provide an overview of the approach. Details can be found in [8]. A.CommunicationDesignFlow Fig. 1 shows the proposed communication design (cid:3)ow. Communication design starts with a virtual architecture model of the system in which processing elements (PEs) communicate via abstract channels with untimed synchronous or asynchronous message-passing semantics. In a (cid:2)rst network deGU IGU I Architecture mode l Architecture mode l Network Des ign Network Des ign Network Network protoco ls protoco ls L ink mode l L ink mode l Comm. L ink Des ign Comm. L ink Des ign Med ia Med ia protoco ls protoco ls MAC mode l MAC mode l Protoco l mode l Protoco l mode l Phys ica l mode l Phys ica l mode l Fig. 1. Communication design (cid:3)ow. sign task, the global system network is designed and end-toend communication between PEs is mapped into point-to-point communication between stations of the network architecture. The result of the network design step is a re(cid:2)ned link model of the system. In the link model, PEs and other network stations communicate via logical link channels that carry streams of packets between directly connected components. In the second communication link design task, logical links between adjacent stations are then grouped and implemented over an actual communication medium where each group of links can be implemented separately. As a result of the communication design process, a physical model of the system is generated. The physical model is a fully structural model in which stations are connected via pins and wires and communicate in a cycle-accurate manner based on media protocol timing speci(cid:2)cations. In the backend process, behavioral descriptions of computation and communication in each component of the physical model are then synthesized into targeted hardware or software implementations. Apart from the physical model, the communication design (cid:3)ow can produce transaction-level models (TLMs) which abstract the pin-level communication in the physical model to the level of media access or individual protocol word/frame transactions. Depending on the parameters of the implementation, automatically generated TLMs can be used to trade off accuracy and model complexity for simulation speed, for example. B.RelatedWork There is a wealth of system-level design languages (SLDL) like SystemC [1] or SpecC [2] available for modeling and describing systems at different levels of abstraction. However, the languages itself do not de(cid:2)ne any details of actual concrete design (cid:3)ows. More recently, SLDLs have been proposed as vehicles for so-called transaction-level modeling (TLM) for communication abstraction [4]. However, no speci(cid:2)c de(cid:2)nition of the level of abstraction and the semantics of transactions in such models have been given. Furthermore, TLM proposals so far focus on simulation only and they lack the path to vertical integration of models for implementation and synthesis.  45 Layer Application Presentation Interface semantics N/A PE-to-PE, typed, named messages (cid:15) v1.send(struct myData) Session Transport Network Link Stream Media Access Protocol Physical PE-to-PE, untyped, named messages (cid:15) v1.send(void*, unsigned len) PE-to-PE streams of untyped messages (cid:15) strm1.send(void*, unsigned len) PE-to-PE streams of packets (cid:15) strm1.send(struct Packet) Station-to-station logical links (cid:15) link1.send(void*, unsigned len) Station-to-station control and data streams (cid:15) ctrl1.receive() (cid:15) data1.write(void*, unsigned len) Shared medium byte streams (cid:15) bus.write(int addr, void*, unsigned len) Unregulated word/frame media transmission (cid:15) bus.writeWord(bit[] addr, bit[] data) Pins, wires (cid:15) ADDR.drive(0) (cid:15) DATA.sample() TABLE I. COMMUN ICAT ION LAY ER S . Functionality (cid:15) Computation Impl. Application OSI 7 (cid:15) Data formatting Application (cid:15) Synchronization (cid:15) Multiplexing (cid:15) Packeting (cid:15) Flow control (cid:15) Error correction OS kernel OS kernel (cid:15) Routing OS kernel (cid:15) Station typing (cid:15) Synchronization (cid:15) Multiplexing (cid:15) Addressing (cid:15) Data slicing (cid:15) Arbitration Driver Driver HAL (cid:15) Protocol timing Hardware 6 5 4 3 2b 2b 2a 2a (cid:15) Driving, sampling Interconnect 1 There are several approaches dealing with automatic generation, synthesis and re(cid:2)nement of communication [3, 7]. None of these approaches, however, provide intermediate models breaking the design gap into smaller steps required for rapid, early exploration of critical design issues. Furthermore, to our knowledge, there is no approach that deals with methodical and automated implementation of communication over networkoriented, non-traditional communication structures. In [6], the authors show an approach for modeling of communication at different levels of abstraction with automatic translation between levels based on message composition rules. However, they do not describe an actual design (cid:3)ow that includes support for arbitration and interrupt handling in traditional bus-based architectures. I I . COMMUN ICAT ION LAY ER S The communication design (cid:3)ow is structured along a layering of communication functionality within each task of the design (cid:3)ow. The implementation of SoC communication is divided into several layers based on separation of concerns, grouping of common functionality, dependencies across layers, and early validation of critical issues for rapid and ef(cid:2)cient design space exploration through humans or automated tools. Table I summarizes the layers for SoC communication by listing for each layer its interface of services offered to the layer above, its functionality, and the level where it will be implemented through the backend tools (software, operating system kernel, device driver, hardware abstraction layer (HAL), hardware). Layering is based on the ISO OSI reference model [9]. However, due to the unique features and characteristics of SoC communication, layers have been tailored speci(cid:2)cally to network-on-chip requirements. Furthermore, note that layers only serve as a speci(cid:2)cation of the desired implementation. As part of communication synthesis within each tool, layers may be merged for cross-optimizations. A.NetworkDesign Network design implements presentation, session, transport, and network layers. The presentation layer is responsible for data formatting. It converts abstract data types in the application to untyped data blocks as de(cid:2)ned by the canonical network byte layout. The session layer implements end-to-end synchronization for synchronous communication and multiplexing of channels into a set of end-to-end message streams. The transport layer splits messages into packets (e.g. to reduce required intermediate buffer sizes) and optionally implements end-to-end (cid:3)ow control and error correction. Finally, the network layer is responsible for routing and multiplexing of endto-end paths over individual point-to-point links. As part of the network layer, additional communication stations are introduced as necessary, e.g. to create and bridge subnets, splitting the system of connected PEs into several segments. B.LinkDesign Link design implements link, stream, media access, and protocol layers. The link layer determines interface types (e.g. master/slave) and implements any necessary synchronization over underlying control and data streams. The stream layer multiplexes control and data streams over shared media by separating them in space (but not time) through addressing and polling. The media access layer is responsible for slicing data packets into protocol transactions and for regulating and separating simultaneous accesses in time (e.g. through arbitration, possibly introducing additional arbiter components). Finally, the protocol layer implements the timing- and pin-accurate driving and sampling of wires. I I I . IM P L EM EN TAT ION We have implemented network and communication re(cid:2)nement tools that can generate design models corresponding to various communication layers automatically [10]. Given design decisions, the tools will take a virtual architecture model of the system down to its bus-functional, physical model. A.Experiments In order to demonstrate the modeling concepts, we applied the communication design (cid:3)ow to the example design of a mobile phone baseband platform. For additional examples,  46 RcvData Co-process SpchOut DCT stripe[] Decoder JPEG Coder OSModel SerOut SpchIn SerIn DSP CF_OS Mem DMA HW DCT_IP BI BO SO SI Ctrl ColdFire DSP_OS p a d A T C D t e r Vocoder Fig. 2. Architecture model example. CF_OS ColdFire DMA DMA_HW DCT DCT_IP M S A M D DSP_OS DSP OSModel HW SI SI_HW BI BI_HW SO SO_HW BO BO_HW Bridge M S linkBri l C D T A p a d t e r i l k n linkBri linkBI linkHW linkSI linkBO linkSO Mem Fig. 3. Link model example. CF_BF ISR l PIC CF_HAL CF_OS CF_HW R D D A ColdFire DMA DMA_BF R D D A R D D A Mem Mem_BF R D D A DCT DCT_IP Arbi ter T_BF P S D _ B F PIC DSP_OS _ P S D _ P S L D A H H W DSP l R D D A OSModel ISR HW R D D A HW_BF SI SI_BF BI ADDR, POLL_ADDR BI_BF SO SO_BF BO BO_BF Bridge R D D A R D D A ADDR, POLL_ADDR ADDR, POLL_ADDR ADDR, POLL_ADDR Fig. 4. Physical model example. including application of the design (cid:3)ow to non-traditional, network-oriented communication architectures, see [8]. The virtual architecture model of the system at the input of communication design is shown in Fig. 2. The design consists of two subsystems: a ColdFire subsystem running JPEG encoding and a DSP subsystem for voice encoding/decoding (vocoder). The ColdFire processor is running the JPEG encoder in software assisted by a hardware IP component for DCT (DCT IP). Under control of the processor, a DMA component receives pixel stripes from the camera and puts them in the shared memory (Mem). The DSP is running concurrent encoding and decoding tasks. Tasks are dynamically scheduled under the control of an operating system model [5] that sits in an additional OS layer DSP OS of the DSP processor. The encoder on the DSP is assisted by a custom hardware coprocessor (HW ) for the codebook search. Furthermore, four custom hardware I/O processors perform buffering and framing of the vocoder speech and bit streams. In the architecture model, hardware and software processors communicate via asynchronous message-passing channels. As a result of the network design process, the network is partitioned into one segment per subsystem with a Bridge connecting the two segments (Fig. 3). Individual point-to-point logical links connect each pair of stations in the resulting link model. Application channels are routed statically over these links where the Ctrl channel spanning the two subsystems is routed over two links via the intermediate bridge. In the resulting link model, presentation layers are instantiated inside each system component. The presentation layer for communication with the DCT IP is inlined from the wrapper into the ColdFire processor. The memory component is replaced with a model describing the memory byte layout and presentation layers accessing the memory perform the necessary conversions of variables into memory bytes. Session, transport, and network layers are not implemented and presentation layers are routed over links through proper connectivity. During link design, links in each subsystem are implemented over its shared medium. The native ColdFire and DSP processor busses are selected as communication media. Within each segment, unique bus addresses and interrupts for synchronization are assigned to each link and memory. In the resulting physical model (Fig. 4), link, stream, media access and protocol layers are instantiated inside the OS and hardware layers of each station. Inside the processors, interrupt handlers that communicate with link layer adapters through semaphores are created. Interrupt service routines (ISR) together with models of programmable interrupt controllers (PIC) model the processor’s interrupt behavior and invoke the corresponding handlers when triggered. Components are connected via pins and wires driven by the protocol layer adapters. On the ColdFire side, an additional arbiter component regulates bus accesses between the two masters, DMA BF and CF BF. Finally, a transducer T BF is inserted to translate between the DCT IP and ColdFire bus protocols.  47 Model Application Link Stream Media Access Protocol Physical ColdFire subsystem Lines of Simulation Comm. code time delays 3,729 0.29 s 0 ms 3,978 0.30 s 0 ms 4,099 0.62 s 0.28 ms 4,337 0.99 s 0.40 ms 5,313 8.66 s 1.18 ms 5,906 20.6 s 1.50 ms Lines of code 12,528 12,480 12,558 12,782 12,966 13,245 DSP subsystem Simulation time 17.8 s 18.7 s 18.8 s 25.2 s 56.1 s 178 s Comm. delays 0 ms 0 ms 0.29 ms 0.57 ms 0.79 ms 0.92 ms System Lines of Simulation code time 14,363 34.1 s 14,535 35.2 s 14,754 58.4 s 15,244 90.5 s 16,436 544 s 17,335 1,824 s TABLE II. EX P ER IM EN TA L R E SU LT S . System DSP CF s y a l e d . m m o c d e z i l a m r o N 1.2 1 0.8 0.6 0.4 0.2 0 Transcoding JPEG 100.0 e m i t n o i t l a u m i s d e z i l a m r o N 10.0 1.0 App Link Stream MAC Protocol Physical App Link Stream MAC Protocol Physical Fig. 5. Simulation performance. Fig. 6. Simulated communication overhead. B.Results Table II summarizes the results for the example design. Using the re(cid:2)nement tools, models of the example design were automatically generated within seconds. A testbench common to all models was created which exercises the design by simultaneously encoding and decoding 163 frames of speech on the vocoder side while performing JPEG encoding of 30 pictures with 116x96 pixels. Models of the whole system and each subsystem were simulated on a 360 MHz Sun Ultra 5 workstation using the QuickThreads version of the SpecC simulator. Fig. 5 plots simulation times normalized against the architecture model times. Contributions of communication overhead to the simulated overall transcoding (back-to-back encoding and decoding) and encoding delays in the vocoder and JPEG encoder, respectively, are shown in Fig. 6. Delays are normalized against the overhead in the (cid:2)nal physical model. Results show that with increasing implementation detail at lower levels of abstraction, accuracy improves linearily while model complexities grow exponentially. Results con(cid:2)rm the choice of the link model as the intermediate model in the design (cid:3)ow that allows fast validation of the overall network topology. By de(cid:2)nition, all models above the physical model are TLMs in which communication is abstracted away from pins and wires. The results show that depending on the architecture, MAC or protocol TLMs return accurate results at much higher simulation speeds. If there is no bus contention, the MAC model provides fast and accurate feedback. However, in the presence of arbitration, slicing of data into bus words/frames needs to be modeled in order to get accurate results that include effects of interleaved media accesses at the protocol level. In these cases, only the protocol model can provide correct delays with signi(cid:2)cantly reduced simulation speeds. Finally, at the communication level, pin- and timingaccurate results are available at the expense of huge runtimes. IV. SUMMARY & CONCLU S ION S In this paper, we presented a communication design (cid:3)ow with well-de(cid:2)ned design steps and design models. Starting from a virtual architecture model with abstract messagepassing communication, a design is brought down to a busfunctional implementation through network and link design tasks. Using an industrial-strength example, the feasibility and bene(cid:2)ts of the approach have been demonstrated. Out of all possible models, intermediate models have been de(cid:2)ned based on accuracy vs. simulation speed tradeoffs allowing early validation of critical design decisions. In between design tasks, the link model de(cid:2)nes the implementation of the end-to-end network on top of point-to-point logical links. Furthermore, two transaction-level models have been identi(cid:2)ed for providing accurate results above the pin level. In general, models at various levels of abstraction have been de(cid:2)ned such that they can be automatically generated through successive re(cid:2)nement. Therefore, the (cid:3)ow supports high-level communication abstractions for fast feedback and early simulation together with an automated path to implementation. In conclusion, the models are the enabler for rapid, early design space exploration and signi(cid:2)cant productivity gains. Future work includes adding algorithms for decision making to provide a completely automated synthesis process. Furthermore, we plan to extend design tasks and re(cid:2)nement tools to implement error-correction, (cid:3)ow control, and dynamic routing for long-latency, error-prone network communication media.  [1] T. Gr ¤otker et al. System Design with SystemC. Kluwer, 2002. [2] A. Gerstlauer et al. System Design: A Practical Guide with SpecC. Kluwer, 2001. [3] W. O. Ces ·ario et al. (cid:147)Multiprocessor SoC platforms: A componentbased design approach.(cid:148) IEEE D&T, 19(6), November/December 2002. [4] M. Coppola et al. (cid:147)IPSIM: SystemC 3.0 enhancements for communication re(cid:2)nement.(cid:148) In DATE, 2003. [5] A. Gerstlauer et al. (cid:147)RTOS Modeling for System Level Design.(cid:148) In DATE 2003. [6] R. Siegmund and D. M ¤uller. (cid:147)SystemCSV : An extension of SystemC for mixed multi-level communication modeling and interface-based system design.(cid:148) In DATE, 2001. [7] K. van Rompaey et al. (cid:147)CoWare: A design environment for heterogeneous hardware/software systems.(cid:148) In Euro-DAC, 1996. [8] A. Gerstlauer. (cid:147)Communication Abstractions for System-Level Design and Synthesis.(cid:148) Technical Report CECS-TR-03-30, UC Irvine, 2003. [9] International Organization for Standardization. "
2005,Communication-driven task binding for multiprocessor with latency insensitive network-on-chip.,"Network-on-chip is a new design paradigm for designing core based system-on-chip. It features high degree of reusability and scalability. In this paper, we propose a switch which employs the latency insensitive concepts and applies the round-robin scheduling techniques to achieve high communication resource utilization. Based on the assumptions of the 2D-mesh network topology constructed by the switch, this work not only models the communication and the contention effect of the network, but develops a communication-driven task binding algorithm that employs the divide and conquer strategy to map applications onto the multiprocessor system-on-chip. The algorithm attempts to derive a binding of tasks such that the overall system throughput is maximized. To compare with the task binding without consideration of communication and contention effect, the experimental results demonstrate that the overall improvement of the system throughput is 20% for 844 test cases.","Communication-driven Task Binding for Multiprocessor with Latency Insensitive  Network-on-Chip  Liang-Yu Lin, Cheng-Yeh Wang, Pao-Jui Huang, Chih-Chieh Chou and Jing-Yang Jou  Dept. of Electronics Engineering  National Chiao Tung University  Hsinchu, Taiwan, ROC  {lylin,cywang,pjhuang,ccchou,jyjou}@eda.ee.nctu.edu.tw Abstract - Network-on-Chip is a new design paradigm for  designing core based System-on-Chip. It features high degree of  reusability and scalability. In this paper, we propose a switch  which employs the latency insensitive concepts and applies the  round-robin  scheduling  techniques  to  achieve  high  communication resource utilization. Based on the assumptions  of the 2D-mesh network topology constructed by the switch,  this work not only models the communication and the  contention  effect  of  the network, but develops  a  communication-driven task binding algorithm that employs the  divide and conquer strategy to map applications onto the  multiprocessor system-on-chip. The algorithm attempts to  derive a binding of tasks such that the overall system  throughput is maximized. To compare with the task binding  without consideration of communication and contention effect,  the experimental results demonstrate  that  the overall  improvement of the system throughput is 20% for 844 test  cases.  I. Introduction  As silicon technology scales, people today are able to  integrate billions of  transistors on  a  chip. The  Multiprocessor System-on-Chip (MPSOC) is the trend of  confronting the pressure of design productivity. However, as  more and more components are integrated, several problems  emerge. First of all, the shared buses can efficiently handle 3  to 10 communication partners, but they do not scale to  higher numbers [1][2][3]. Secondly, since  technology  scaling works better for transistors than for interconnecting  wires, wire delay is no longer negligible. Global synchrony  is hard to maintain and the long and global wires make  system performance unpredictable [1][3][4]. Consequently,  designer spent much effort on system integration such that  the design productivity is difficult to elevate and the product  development cycles are expanded.  Recently, Networks-on-Chip design methodology  is  proposed to provide alternative way to design the on-chip  communication [1][5]. A typical NoC architecture provides  a scalable communication infrastructure for interconnected  components,  and Globally Asynchronous Locally  Synchronous (GALS) [6] style of large chip implementation  is supported. Each component could be implemented as a  separate clock domain and could communicate to other  components using asynchronous communication through  switches.   By managing communication channels properly, data  transmission can coexist peacefully such that the network  has the capability to integrate the arbitrary components. On  the other hand, each component can be designed and verified  independently owing to each one is guaranteed to comply  with the network protocol. By reusing the pre-developed  components, a new system can be quickly built-up and much  verification effort can be saved.  The related NoC research topics are described as  following. First, the design issues of the NoC architecture,  [8][9][10][11][12] discuss  the switch design, network  topology and protocol. [8] exploits circuit switching  technique  to guarantee communication bandwidth. [9]  proposes a data-transfer method called Black-Bus, which  utilizes the local address to save up to 75% of routing tags  compared to the global addressing scheme used in traditional  packet network. [10] presents a communication protocol  stack that can guarantee traffic bandwidth. [11][12] suggest  a latency-insensitive protocol that controls communication  among components of a synchronous system such that the  functionality of the system depends only on the order of  each signal’s events and not on their exact timing. The idea  is to pipeline the long wire by inserting the relay stations  which are the special memory elements [12].  Second, for synthesis issues, [13] proposes a two-step  genetic algorithm that maps an application, described by a  parameterized task graph, onto 2D-mesh NoC architecture.  The simple communication delay model based on  packet-switching is also presented.   In our work, the latency insensitive switch design is  propounded. We believe that the communication and  contention issues affect the effectiveness of how to bind the  tasks onto processors. Based on the ideas, we propose the  communication-driven task binding algorithm to determine a  task binding so that the overall system throughput is  maximized.  The rest of the paper is organized as follows. Section II  introduces our Multiprocessor System-on-Chip platform and  the switch design. In section III, The methodology of task  binding is described. Then, experimental results are given  and discussed in section IV. Finally, the conclusion is made  in section V.   39                   II. Architecture and Switch Design  A. Architecture  As shown in Fig. 1, there are two components in our  platform: processors and switches. Each processor contains  local memory and connects to the local switch. Each switch  connects to the four neighboring switches and the local  processor.  Switch Switch Switch Processor Processor Processor Switch Switch Switch Processor Processor Processor Fig. 1. Our platform  The topology of the network is 2D-mesh For three  reasons. The following describes the reasons. First, because  of the simple connection and easy routing provided by  adjacency, it is widely used in parallel computing platforms  [14]. Second, the interconnect length between nodes is  uniform, which ensures the uniformity of the performance  and overall scalability of the network. And last, it meets the  inherent constraint of IC manufacturing technology because  of the flat topology.  B. Switch Design  Packet-switching and circuit-switching are designed for  different application domains. The real-time applications  typically apply the circuit-switching technique because of  the  requirement of performance guarantee.  In  this  application domain, the performance is usually predictable  and the variation of communication latency is relatively  small once the path is setup. One of the drawbacks of this  switching is the low utilization.  On  the other hand, packet-switching  is normally  employed  for general applications without  real-time  constraints. The three kinds of the switching methods are the  store-and-forward  switching,  the virtual  cut-through  switching and the wormhole switching. Wormhole switching  is one of the best candidates for on-chip communication due  to the little memory usage. But wormhole switching has  unpredictable latency under heavy loads. Designers can  obtain good average performance but can not predict the  worst case condition.  Our switch design is based on latency insensitive  concepts [11][12] and applied round-robin shared bus  technology to achieve high utilization. In Fig. 2, relay  stations [12] are replaced by our switches. Our switches use  two-port SRAM instead of registers when the number of  virtual channel in a physical channel is large. To improve  the low utilization of the dedicated peer-to-peer connection,  we substitute  them by  the virtual channels. In  this  architecture, designers should consider the communication  scheduling to achieve high performance in system designs.   PE PE PE RS RS RS RS RS RS RS RS Physical Channel S S S S PE PE PE PE Virtual Channel PE Fig. 2. Replace the relay station with our switch  Our switch has five important features to adapt the  on-chip communication. First, our switch is based on circuit  switching due to (1) limited on-chip memory because  memory on-chip is expensive and the circuit-switching does  not require much memory. (2) the behavior is predictable  and (3) the support of real-time applications is also the  important property.  Second, the switch exploits the virtual channel flow  control [15]. If the parts of data are buffered at the input or  output of each physical channel, once a message occupies  the buffer, no other messages can ever access that channel  until it is released.   By dividing a physical channel into several virtual  channels, messages can make progress rather than being  blocked. For example, Fig. 3 shows two messages crossing  the physical channel between switch 1 and switch 2. Since  the time required for a message to wait until it is transferred  is reduced, the average latency is decreased. As a result, the  physical channel utilization rate is higher and the network  throughput is increased. By utilizing virtual channels, the  overall message latency and network throughput can be  improved further and the cost is larger buffer size and  complex multiplexer.  Fig. 3. Messages make progress rather than being blocked  Third, a detail data exchange protocol between two  switches or between switch and processor network interface  is executed in four cycles.   40               Fig. 4 shows an example of interface transaction between  two neighboring switches. The local address technique  which is similar to [9] is applied and the address mapping  table records mapping address of output stage of East port of  switch 1.   Buffer A Buffer A Buffer B Buffer B Buffer C Buffer C Fig. 4. An example of interface transaction between two  switches  At cycle 1, buffer E1 of switch 1 has data inside, and  controller grants channel privilege to it. At cycle 2, buffer  E1 of switch 1 sends address E1, indicating this transaction  trying to send data into buffer E1 of switch 2, by the  Address-line. At cycle 3, buffer E1 of switch 2 sends  acknowledge signal (false/true) according to its buffer status  (full/available) by Ack-line. At the same time, buffer E1  sends data by Data-line. Buffer E1 of Switch 2 stores data in  Data-line if it still has space or discards the data. At cycle 4,  according to the acknowledge signal in Ack-line, buffer E1  of switch 1 decides whether to keep the data or flush the data  it holds.  Fourth, although the bandwidth of a physical channel can  be equally shared among all virtual channels, this is  generally not a good idea. As shown in Fig. 5, instead of  using time-division style to share bandwidth equally, this  switch design uses a round-robin scheduler to grant rights to  each virtual channel. Only when messages are to be  transmitted over some virtual channels, our round-robin  scheduler gives rights to them. Those virtual channels  without data transmission cannot, and do not have to, access  the physical channel.  The last, for traditional networks, the number of nodes is  not known and the behavior of communication is not  predetermined. This is not true for on-chip networks because  the number of nodes and the behavior of communication can  be known before run-time. Therefore,  the dedicated  connection paths can be established in advance by reserving  the  corresponding  virtual  channels. With  proper  configuration, our switching method acts just like circuit  switching. Due to this feature, our switch performs the  similar behavior with the circuit switching to support  real-time applications. The detail of the path assignment  process is described in the next section.  Fig. 5. Virtual channels share the bandwidth of a physical  channel using round-robin scheduler  III. The Task Binding  A.  The Task Binding Problem  The flow of task binding is shown in Fig. 6. First, a task  graph containing  the computation and communication  information for all tasks is given. Next, this approach  attempts to utilize the placement techniques used in FPGA  to map tasks onto processors [16]. If the traffic loading of  any two tasks is heavy, they are allocated as neighbor as  possible. After the task mapping, the all shortest path  technique is performed to configure the connection paths for  these tasks. The next step, this approach performs the  simulation to obtain the profile of communication in time  domain and calculates the contention parameters which are  explained in the subsection III-C. The last, the profile feeds  back to the task mapping process and the path assignment  process employs the profile to obtain a better assignment.  Designers can repeat this flow iteratively to enhance the  system performance. The flow is also well known as the  profile driven optimization.  Fig. 6. Task binding flow  We assume that (1) given applications  A ~1 kA  which  are modeled by task graph. (2) Each vertex representing a  task to run on one processor, and the weight of the vertex is  the amount of computation. (3) Each edge indicating data  transmission along the arrow, and the weight of the edge is  the amount of communication.  Our goal is to bind each vertex onto a processor such that  the total routing resource requirement and communication  contention are minimized  and the system throughput is  tend to close the ideal condition.   41               Because of the problem complexity, the process is  divided into two steps: task mapping and connection path  assignment.  B.  The Task Mapping  We solve the task mapping problem by utilizing the  simulated annealing technique in our mapping tool. Due to  the optimization goals of the on-chip network may change  from architecture to architecture and designers may explore  different architectures. Thus, the mapping tool should be  more easily adapted to new optimization goals. Therefore,  simulated-annealing technique is applied to map tasks onto  processors.  In task mapping problem, our goal is to minimize the  total communication resource usage. The cost function used  in the mapping process is shown as the equation (1) .  cost = ∑ processors paired distance 1(* + communicat ion amount communicat ion amount max   (1)  ) The distance in the function is Manhattan distance  between source node and destination node. The first term of  the cost function: distance*1 describes the resource usage of  the virtual channels. This term is also the traditional metric  in the FPGA placement algorithm. In our approach, we  hypothesize that not only the distance has the impact on the  system performance, but  the communication amount  between the tasks. To model the communication effect, we  use  the  second  term:  distance*(communication  amount/communication  amountmax)  to describe  the  occurrence of physical channel which is a normalized value.   C. The Connection Path Assignment  Once processors for all the tasks have been chosen, a router  tries to assign connection paths between any pair of  interconnected processors. Here, the routing algorithm like  the one proposed in [17] is performed to solve the problem.  This router is essentially a variant of maze router [18]. It  runs Dijkstra’s algorithm [19] to find the lowest cost path  between a sender and a receiver processor. The Pathfinder  algorithm [16] then performs multiple routing iterations to  rip up some or all nets and reroute them by different paths in  case there is a competition for routing resources that makes  the routing illegal. Please note that ripping up and rerouting  these nets only affect the net ordering. These nets are all  routed by the same maze routing algorithm.  The contention effect  is considered  in our path  assignment algorithm. Our strategy  is  to divide  the  contention into two categories: time domain and spatial  domain.  Pa th A Pa th B T ime line In Fig. 7. The contention of path A and path B in time  domain is defined as the degree of the two communication  paths overlapped in the time line.  We term the contention  as density. The density of the path A which is caused by  path B is shown as the equation (2).  Density =>− A B Time of overlap ( BA , ) total communicat ion time The Aof  (2)  We apply  the system simulation  to derive  the  communication profile in time domain and calculate the  density of each communication path pairs. The contention of  spatial domain  is defined as  the distance of  two  communication paths which are using the same physical  channel.  Sw itch Sw itch Sw itch Sw itch Wrapper W B Wrapper Z Wrapper X A C Sw itch Sw itch Wrapper Y Sw itch Sw itch Sw itch Fig. 8. One possible path assignment for path A  Here the cost of contention effect is defined as following.  The cost of contention for a path A is influenced by paths  which are overlapped with path A in time domain and spatial  domain. In Fig. 8, for example, if path B and path C are  assigned previously, now we choose a path assignment: path  A, which is from processor Y to processor W. The cost of  path A is  cost = distance 1(* + communicat ion amount 1( communicat ion amount ∑+ density )  (3)  ) max The cost function of the path assignment inherits the  concept from the cost function of the task mapping. The two  differences are (1) the distance is the real routing length  which differs from the Manhattan distance in task mapping  and (2) adding the contention effect which is modeled by the  contention density contributed by other paths. According to  the cost function, an applicable path for path A can be  obtained.  The path assignment algorithm is summarized as follows:  Fig. 7. The contention in time domain  Fig. 9. The pseudo code of the path assignment   42                                                          IV. Experimental Results  This section consists of two parts. First, to evaluate the  functionality and  the performance of our network  infrastructure, we use a 2D-mesh with 4-by-4 nodes to make  experiments. Because  this approach only considers  evaluating the traffic information of our platform, individual  processing element only provides randomly generated traffic  generating function here.  The second part is the experimental results of the task  binding algorithm and the network infrastructure model  comes from the first part.  A. The Results of Network Infrastructure  Our switch is designed in both Verilog HDL and cycle  accuracy C++ model. Verilog version is for traditional  cell-based design flow implementation, and C++ model is  for system design flow and platform evaluation. The  synthesis result shows that our switch design can work at  185MHZ with TSMC 0.25um technology in typical case.  To evaluate traffic performance of the platform, we use  the C++ model of switch which constructs the model of  network, and write a random pattern generating model to  replace  the original processing element. This pattern  generator can generate packets with random length from  random source to random destination.   There are several definitions. Latency is the time elapsed  from when the packet transmission is initiated until the  packet is received at the destination node [20]. Maximum  Latency is defined as the predicted worst case latency.  Normalized Latency is defined as the latency divided by the  maximum latency. Normalized latency indicates the average  performance. Injection rate is defined as the needed  bandwidth of generated traffic divided by the guarantee  bandwidth of this communication path. By changing the  value of injection rate, we can evaluate our platform under  different communication loads.  Fig. 10. Histograms of normalized latency under different  injection rate   43 The first thing that we need to do is to show that our  platform can guarantee the minimum bandwidth for each  transmission. In this experiment, we provide each virtual  channel with a 2–word buffer in the intermediate switch and  show results in Fig. 10. Even under high injection rate  (injection rate = 1) packet is still delivered to destination  under maximum latency (normalized latency < 1).   With this property, we can implement a real-time system  and improve the predictability of our platform. In Fig. 10,  the normalized latency approaches to zero as injection rate  decreases. It indicates that the average latency decreases as  injection rate decreases.  Fig. 11. Histograms of normalized latency under different  buffer size of virtual channel  As the need of different system requirements to various  applications, we may need different platforms. By deciding  appropriate buffer size of virtual channel in our switch, we  can provide better communication performance. Fig. 11  shows a trend that the average latency decreases as the  buffer size increases.  B. The Results of Task Binding Algorithm  The Task Graph For Free  (TGFF)  [21],  a  user-controllable, general-purpose, pseudorandom  task  graph generator, is utilized to generate the random cases.  Then, the information of generated cases is fed in our task  binding tool. After each task is mapped onto a processor and  each connection path is assigned, we incorporate the routing  information with our network  infrastructure and run  simulation.   For each task graph we generated, 60 to 100 tasks are  contained at least. The maximum number of inputs/outputs  of each task is from 7 to 10.  In this stage, two models are used: (1) the 2D-mesh  network infrastructure constructed by the switch which is  mentioned previously and (2) the processor model. In our  processor model, we assume that a processor begins  operating only when all input data are available and the  output buffer size is enough for data that will be later  generated.   We use the overall throughput as the metric of the system  performance. Given a fixed time period, we simulate the                chip for hard real time embedded systems,” in Proceedings of the  International Parallel and Distributed Processing Symposium, pp.  78 – 85, 2003  [9] Kenichiro Anjo, Yutaka Yamada, Michihiro Koibuchi, Akiya  Jouraku and Hideharu Amano, ""BLACK-BUS: a new data-transfer  technique using  local address on networks-on-chips,""  in  Proceedings of the 18th International Parallel and Distributed  Processing Symposium, pp. 10 – 17, 2004.  [10] Mikael Millberg, Erland Nilsson, Rikard Thid, Shashi Kumar  and Axel Jantsch, ""The Nostrum backbone - a communication  protocol stack for networks on chip,"" in Proceedings of the 17th  International Conference on VLSI Design, pp. 693 – 696, 2004.  [11] Luca P. Carloni, Kenneth L. McMillan, Alexander Saldanha  and Alberto L. Sangiovanni-Vincentelli, ”A Methodology for  Correct-by-construction  Latency  Insensitive  Design,”  Computer-Aided Design, Digest of Technical Papers. IEEE/ACM  International Conference on , pp. 309-315, 1999  [12] Luca P. Carloni and Alberto L. Sangiovanni-Vincentelli,  “Coping with latency in SOC design,” Micro, IEEE , Volume: 22 ,  Issue: 5 , Sept.-Oct. pp. 24 – 35 , 2002.  [13] Tang Lei and Shashi Kumar, “A two-step genetic algorithm for  mapping task graphs to a network on chip architecture,” in  Proceedings of the Euromicro Symposium on Digital System  Design, pp. 180 – 187, 2003.  [14] William J. Dally, “Performance analysis of a k-ary n-cube  interconnect networks,” in IEEE Transactions on Computers, pp.  775 – 785, 1990.  [15] William J. Dally, “Virtual-channel  flow control,”  in  Proceedings of the 17th Annual International Symposium on  Computer Architecture, pp. 60 – 68, 1990.  [16] A. Marquardt, V. Betz and J. Rose, “Timing-driven Placement  for FPGAs,” in ACM Symposium on FPGAs, pp. 203-213, 2000.  [17] Carl Ebeling, Lary McMurchie, Scott A. Hauck and Steven  Burns, “Placement and routing tools for the Triptych FPGA,” in  IEEE Transactions on Very Large Scale Integration Systems, pp.  473 – 482, 1995.  [18] C. Y. Lee, “An algorithm for path connections and its  applications,” in IRE Transactions Electron Computing, volume EC  10, pp. 346 – 365, 1961.  [19] E. Dijkstra, “A Note on Two Problems in Connexion with  Graphs,” in Numerical Math, volume 1, pp. 269 – 271, 1959.  [20] Jose Duato, Sudhakar Yalamanchili, and Lionel Ni,  “Interconnection Networks: an Engineering Approach,” Morgan  Kaufmann, 2003.  [21] Robert P. Dick, David L. Rhodes and Wayne Wolf , “TGFF:  Task Graphs for Free,” in Proceedings of the 6th International  Workshop on Hardware/Software Codesign, pp. 97 – 101, 1998.  system with four different conditions. When the simulation  is finished, our tool reports the system throughput.  TABLE I  The experimental results of 844 test cases  Without  communication  and contention  effect  0%  Improvement  of  Throughput  Only mapping  with  communication  6.26%  Only path  assignment  with  contention  16.30%  Both  20.94%  We experiment on 844 cases and summarize in TABLE I.  The table demonstrates that our 2-step task binding is  effective and increase the system throughput by 6%, 16%  and 20% for only consideration of communication in  mapping process, only path assignment process and  cascaded both effect respectively.  V. Conclusions  This paper proposes the switch design which is based on  circuit-switching with additional features and the latency  insensitivity concept. The communication-driven  task  binding algorithm is also presented. The algorithm not only  employs the iterative profile driven optimization technique,  but also models the communication contention to achieve  high system throughput.  The experimental results indicate that the iterative 2-step  approach increases system utilization effectively. They also  show that the improvement of the system throughput is 20%  on average.  "
2005,Feasibility analysis of messages for on-chip networks using wormhole routing.,"The feasibility of a message in a network concerns if its timing property can be satisfied without jeopardizing any messages already in the network to meet their timing properties. We present a novel feasibility analysis for real-time (RT) and nonrealtime (NT) messages in wormhole-routed networks on chip. For RT messages, we formulate a contention tree that captures contentions in the network. For coexisting RT and NT messages, we propose a simple bandwidth partitioning method that allows us to analyze their feasibility independently.","Feasibility Analysis of Messages for On-chip Networks Using Wormhole Routing Zhonghai Lu, Axel Jantsch and Ingo Sander Royal Institute of Technology, Stockholm, 16440 Kista, Sweden fzhonghai,axel,ingog@imit.kth.se Abstract —The feasibility of a message in a network concerns if its timing property can be satis ﬁed without jeopardizing any messages already in the network to meet their timing properties. We present a novel feasibility analysis for real-time (RT) and nonrealtime (NT) messages in wormhole-routed networks on chip. For RT messages, we formulate a contention tree that captures contentions in the network. For coexisting RT and NT messages, we propose a simple bandwidth partitioning method that allows us to analyze their feasibility independently. I . INT RODUCT ION Network-on-Chip (NoC) [3, 4, 10] design starts with a system speci ﬁcation which can be expressed as a set or sets of communicating tasks. The second step is to map these tasks onto the nodes of a NoC instance. With a mapping, application tasks running on these nodes load the network with messages, and impose timing requirements. Timely delivery of messages is essential for performance and predictability. However, routing messages in a network is inherently nondeterministic because messages experience various contention scenarios which stem from sharing buffers at routers and links between the routers. These contentions cause indeterminate delay and jitter, leading to possibly the violation of the timing constraints of the messages. It is therefore important to conduct an analysis on messages to determine their feasibility. Given a set of already scheduled messages, a message is termed feasible if its own timing property is satisﬁed irrespective of any arrival orders of the messages in the set, and it does not prevent any message in the set from meeting its timing property [2]. In general, on-chip messages can be categorized as real-time (RT) and nonreal-time (NT) messages [10]. Messages with a deterministic bound, which must be delivered predictably even under worst case scenarios, are RT messages. Messages with a probabilistic bound, which request an average response time, are NT messages. Wormhole ﬂow control with lanes (virtual channels) is being advocated for NoCs due to its shorter latency, greater throughput and smaller buffering requirement [3, 10]. However, few studies have been performed to analyze the message feasibility for wormhole-routed networks. For real-time messages, the lumped link model [2, 5] is a path-based model in which all the links along a message i ’s path are lumped into a single link. The message is scheduled on this link together with other competing messages. The feasibility test algorithms based on this model are efﬁcient [2, 5]. However, due to lumping, all the competing messages must be scheduled in sequence. As a result, direct and indirect contentions are treated in the same way. Also, no concurrent use of the links on i ’s path can be taken into account. In [6], Kim et al. used a blocking dependency graph to express the contentions a message may meet and derived the message’s delivery upper bound. However, this graph does not re ﬂect the possible concurrent use of links, too. In the paper, we present a novel feasibility analysis for both RT and NT messages on wormhole-routed networks on chip. Section II describes the communication models delivering the RT and NT messages. In Section III, we ﬁrst classify messages according to the type of performance bound and timing requirements on delay or jitter. Then, for the RT messages, we formulate a contention tree that can accurately re ﬂect contentions and link usage. Speciﬁcally, it can distinguish direct and indirect contentions and captures concurrent use of links. Finally, we use a bandwidth partitioning method to test the feasibility of RT and NT messages coexisting in the network. The experiments are described in Section IV, followed by conclusions in Section V. I I . THE COMMUN ICAT ION MODE L S A.TheNonreal-timeCommunicationModel In wormhole routing, a message is divided into a number of ﬂits ( ﬂow control units) for transmission 1 . The head ﬂit carrying routing and sequencing information governs the route. As the head ﬂit advances, the remaining ﬂits follow in a pipeline fashion. The message transmission is complete when its last ﬂit is delivered to the destination. When required resources are unavailable, the messages are blocked in place. Wormhole routing manages two types of resources: the lanes and the physical link bandwidth. In conventional wormhole routers, the shared lanes are arbitrated on First-Come-FirstServe (FCFS), and they are multiplexed over the shared link bandwidth on demand [9]. This model is fair and produces good average-case latency results. But there is no guarantee that the messages are delivered before deadline. Therefore this communication model is suitable for the delivery of NT messages. With this NT model, the average network latency T  1 The effect of packetization is not considered in this study.  960 of delivering a message with  ﬂits is calculated by [1]: T  = =B    R  ! = a  ! (1) where B is the minimum link bandwidth allocated to the message along its route;   denotes the number of hops the message passes; R is the routing delay per hop. The ﬁrst two terms represent the non-contentional or base latency a, which is the lower bound on T  ; ! is the average contention delay due to the message being unable to access the shared lanes and link bandwidth. which is the sum of the latency due to the resource node Tde and the network T , we focus on the network latency T . The effects of Tde can be straightforwardly incorporated into the delay constraint resulting in a more stringent deadline. Depending on the type of performance bound (deterministic or probabilistic) and that of timing requirement (delay or jitter), we de ﬁne the Quality Class ( 	C ) of a message, which can be viewed as an index representing the Quality of Service (QoS) requirement(s) of the message. For a probabilistic bound, we refer to constrain the bound to be an average response time. We de ﬁne four quality classes as follows: 	C1 : jitter constrained, D   j (cid:20) T (cid:20) D. 	C2 : delay constrained, T (cid:20) D, j = D. 	C3 : average jitter constrained, D   j (cid:20) Tavg (cid:20) D. 	C4 : average delay constrained, Tavg (cid:20) D, j = D. 	C1 and 	C2 messages are RT trafﬁc while 	C3 and 	C4 are NT trafﬁc. Also, 	C2 and 	C4 messages can be regarded as a special case of 	C1 and 	C3 messages when j = D, respectively. B.Real-TimeMessages According to Equation (2), a feasible real-time (RT) message i satisﬁes its timing constraint: 8 i 2 	C1 Di   ji (cid:20) ci  (cid:28)i (cid:20) Di 8 i 2 	C2 ci  (cid:28)i (cid:20) Di (3) To estimate the worst-case latency of an RT message i , we must ﬁrst determine all the contentions the message may meet. In ﬂit-buffered networks, the ﬂits of a message i are pipelined along its routing path. The message advances when it receives the bandwidth of all the links along the path. The message may directly and/or indirectly contend with other messages for shared lanes and link bandwidth. i has a higher priority set Si that consists of a direct contention set SDi and an indirect contention set Si , Si = SDi  Si . SDi includes the higher priority messages that share at least one link with i . Messages in SDi directly contend with i . Si includes the higher priority messages that do not share a link with i , but share at least one link with a message in SDi , and Si B.TheReal-timeCommunicationModel Real-time messages must be served in such a way that the message delivery is predictable and guaranteed. Li and Mutka [7] developed a range of ﬂow control schemes for real-time messages concerning priority mapping strategies, priority adjustment methods, and arbitration functions. In [2], based on a global priority, Preemptive Pipelined Circuit Switching for Real-Time (PPCS-RT) decouples the message delivery into two phases: path establishment and data delivery, where the path setup is preemptable. In [11], a ﬂit-level preemption ﬂow control is developed to resolve the priority inversion problem, i.e., a higher priority message is blocked by a lower priority message occupying shared resources. These real-time models complicate wormhole router design. We assume a real-time (RT) message delivery model without a complicated router architecture and without a special service. All messages are globally prioritized (priority ties are resolved arbitrarily). This model arbitrates shared lanes and link bandwidth by priority. The priority, which may be assigned according to rate, deadline or laxity [5, 7], takes a small number of ﬂits. With this RT model, assuming the same routing delay R for the head ﬂit and other ﬂits, the worst-case latency delivering a message with  ﬂits is given by : T  of T  =   i =B     R  (cid:28) = c  (cid:28) (2) where B  is the minimum link bandwidth allocated to the RT message along its route; i is the number of ﬂits taken by the message priority. The ﬁrst term counts for the transmission time of all the message ﬂits including that occupied by the priority; the sum of the ﬁrst two terms is the non-contentional latency c, which is the lower bound on T  ; the last term (cid:28) is the worst-case blocking time due to contentions. I I I . FEA S IB IL ITY ANALY S I S A.TheMessageModelandQualityClasses We consider messages or message streams that can be characterized by four parameters  = S; ; D; j , where S denotes the maximum size of all the message instances;  is the message period meaning that all the inter-arrival times of the message instances are never less than ; D is the end-toend delay constraint; j is the jitter constraint. Though the delay D is a constraint on the end-to-end communication latency, M1 D A (a) (b) M3 M1 M3 M2 M4 M3 M4 B C M3 M2 E13 E23 E34 Fig. 1. Network Contentions and Contention Tree   E . A message i is a node i in the tree, and vice versa. An edge Eij (i < j  directs from node i to node j , representing the direct contention between i and j . i is called parent, j child. Given a set  of RT messages, after mapping to the target network, we can build a contention tree with the following three steps: Step 1. Sort the message set in descending priority sequence with a chosen priority assignment policy. Step 2. Determine the routing path for each of the messages. Step 3. Form a tree. If i shares at least one link with j where i < j (cid:20) , an edge Eij is created between them. Each tree node only maintains a list of its parent nodes. In a contention tree, a direct contention is represented by a directed edge while an indirect contention is implied by a “walk ” via parent node(s). A walk is a path following directed edges in the tree. The contention tree for Fig. 1a is shown in Fig. 1b, where the three direct contentions are represented by the three edges E13 , E23 and E34 , and the two indirect contentions for 4 are implied by the two walks E13 ! E34 and E23 ! E34 via 4 ’s parent node 3 . Since knowing the routing path is a priori, creating a contention tree is more suitable for deterministic routing. For adaptive routing, it is difﬁcult to ﬁgure out the worst-case routing path. TABLE I M E S S AG E PA RAM E T E R S AND LAT EN CY BOUND S Message Period  10 15 30 30 Deadline D 10 15 30 30 Base latency c 7 3 5 8 Lat. bound 7 3 20 28 1 2 3 4 Table I shows the message parameters for Fig. 1, where the priority is assigned by rate, and deadline D equals period . The worst-case schedules2 for the three links are illustrated separately in Fig. 2a. The latency bounds for the four messages are also listed in Table I. We can see that all the four messages are feasible. Looking into the schedules, we can observe that (1) 1 and 2 are scheduled in parallel. This concurrency is in fact re ﬂected by the disjoint nodes in the tree. We call two nodes disjoint if no single walk can pass through both nodes. For instance, 1 and 2 in Fig. 1b are disjoint, 2A schedule is a timing sequence where a time slot is occupied by a message or left empty. therefore their schedules do not interfere with each other; (2) 3 is scheduled on the overlapped empty time slots [8, 10] and [19, 20] left after scheduling 1 and 2 . The competed slots [1,7] and [11,18] are occupied by 1 or 2 . This is implied in the tree where 3 has two parents, 1 and 2 ; (3) 4 is scheduled only after 3 completes transmission at time 20. The indirect contentions from 1 and 2 , which are re ﬂected via slots [1,7] and [11,18], propagate via its parent node 3 . For 3 , these slots are directly competed slots. For 4 , they become indirectly competed slots. The four message schedules are individually depicted in Fig. 2b. If the concurrent use of the two links, AB by 1 and BC by 2 , was not captured, 3 and 4 would be considered infeasible since 2 would occupy the slots [8, 10] and [18, 20], levaving only three empty slots before slot 30 for 3 and 4 .                                        (a) Link schedules of the messages 10 M2 15 0 M2 20 M3 18 20 M3 10 15 0 M1 25 5 M1 15 10 20 0 M1 M3 10 M3 10 M3 20 M3 10 37 7 (b) Global schedules of the messages Message fires 25 5 18 7 M4 M3 M2 M1 Directly Indirectly Competed slots Unroll twice Unroll 3 times M4 30 20 15 10 28 0 30 30 30 M4 Message fires 37 M1 25 M1 M1,M3 M1,M3 5 30 M1 15 M3 18 20 28 M1 0 M1 M1 link AB         7 M3,M4 30 15 28 M3,M4 link CD 0 M3 20 Message fires link BC 37 M2,M3 M2 30 M2 15 0 M2 M2,M3 M2 28 Fig. 2. Message Scheduling In a contention tree, all levels of indirect contentions propagate via the intermediate node(s). This might be pessimistic since many of them are not likely to occur at the same time. If the number of shared lanes increases, the indirect contentions due to lane unavailability decrease. Also, a lower priority message can use the link bandwidth if a competing message with a higher priority is blocked elsewhere. To balance this pessimism, we have neglected priority inversion. As discussed in [2, 5], this problem can be alleviated by packetization. C.Nonreal-TimeMessages According to Equation (1), a feasible nonreal-time (NT) message i satisﬁes its timing constraint: 8 i 2 	C3 Di   ji (cid:20) ai  !i (cid:20) Di 8 i 2 	C4 ai  !i (cid:20) Di (4)  962 To analytically estimate the average contention delay !i is a difﬁcult task because it is dependent on the network characteristics such as topology, routing algorithm, ﬂow control, as well as the network communication patterns. Since this estimation is not the focus of this paper, we consider only special cases. To this end we use the closed form of contention delay [1] that Agarwal developed for random trafﬁc k -ary d-cubes using dimension-order wormhole routing and unbounded internal buffers. For a 2D mesh network, !i is roughly calculated by: !i = 3 , where (cid:26) is the network utilization calculated by (cid:26) = i  i   1i=C , where C is the network capacity measured in the total number of network links; i is the probability of a network request a cycle. Scheduling a new NT message leads to an increase in (cid:26). The timing constraints of the already scheduled messages must be met with the new (cid:26). Otherwise, the new message is infeasible. 1 (cid:26) :  i 1 2  i  i (cid:26) B  D.Real-TimeandNonreal-TimeMessages In a network supporting both RT and NT messages, estimating the values of worst-case blocking time (cid:28) and average blocking time ! becomes more complicated due to the possible interactions while delivering both classes of messages. For example, with respect to ! , if the NT messages are allowed to use the unused bandwidth reserved by the RT messages, the RT messages may suffer from severe priority inversion problems, i.e., they may be blocked by the NT messages for an uncertain amount of time; with respect to (cid:28) , the portion of the shared resources available to the RT messages may be dynamically changing, leading to intractability. This dynamic network behavior is not in accordance with our static analysis approach. In fact, such dynamic resource sharing schemes complicate the router design; for instance, it becomes too costly for the scheduler to adjust the allocated bandwidth. Therefore we have chosen to isolate the RT and NT trafﬁc into two disjoint virtual networks. Such a nonwork-conserving service discipline has been discussed in [12]. Lanes B Link bandwidth RT lanes NT lane Brt Bnt Fig. 3. Bandwidth Partitioning Suppose the link bandwidth B is normalized to 1, then each class of trafﬁc has a weighted portion of B , as shown in Fig. 3. Let B and B  be the bandwidth assigned to the NT trafﬁc and RT trafﬁc, respectively, B  B  = 1. As a result, the link bandwidth is arbitrated by weighted round robin where the weights (B and B  ) can be chosen a priori based on all types of trafﬁc the router is designed to carry [8]. Concerning a network with uniform trafﬁc, the same weights may be selected for all the routers. We can then apply our analysis method in Section III.B and III.C to the RT and NT trafﬁc, respectively. IV. EX PE R IMENT S We have implemented a feasibility test algorithm based on the contention tree for RT messages and the bandwidth partitioning scheme for coexisting RT and NT messages. Then we conducted feasibility tests on messages in a 2D 8 X 8 mesh NoC with bidirectional links (the network capacity C is 4  8  8   1 = 224). The network uses wormhole ﬂow control with dimension-order X-Y routing, which is a deterministic and deadlock-free algorithm. Lower dimension networks and deterministic routing algorithms are bene ﬁcial for NoCs in order to reduce the control complexity of the routers [4]. The purposes of our experiments are two-fold. First, we investigate how messages with a different Quality Class (	C ) affect the NoC performance. Second, we examine the impact of a bandwidth partitioning on the system performance. A message with the four parameters S; ; D; j  is randomly generated between a pair of nodes. The message size S including protocol overhead randomly takes a value from 32, 64, 128, and 512 in ﬂits. For each of the message sizes, the period  takes a random value from 50(cid:21), 100(cid:21), 200(cid:21), and 800(cid:21), where (cid:21) 2 f1; 2; 3g, respectively, and  = D. In this way, a longer message is likely to have a longer period. The routing delay per hop R is chosen to be 2. The amount of trafﬁc is generated given a threshold (cid:15) from 0:1 to 1 (normalized with the network capacity) with a step length of 0:1. For any message generated, we must ensure that the link capacity is not violated. Let the probability of a network request of an RT and an NT message i on any given cycle be  , respectively. With a period of i , i = i =i . Let ij be the link bandwidth requirement of i on link j , ij = i . For a link j with  RT and k NT messages, the link constraint is: i and  i = i  i;i =i and   i 8j  X i=1  ij  k X i=1  ij (cid:20) B   B = 1 (5) If a new message generated does not lead to violate Inequality 5, the message is offered into the network; otherwise, it is discarded. By our trafﬁc generation method, the offered trafﬁc, which is the input of the feasibility test, is up to 62 of the generated trafﬁc as illustrated by the dashed line in Fig. 4. Also, we treat infeasible RT and NT trafﬁc differently. If an RT message fails the feasibility test, it will not be considered any more. In contrast, all the offered NT messages are always involved. This is because a feasibility test needs to be conducted before admitting an RT message into the network while such a test is usually not necessary for an NT message. For each (cid:15), the simulation runs 50 times to steady states and reports average results of pass ratio, i.e. the percentage of the messages that pass the feasibility test, and of the network link utilization of these feasible messages. In general, the more messages that ful ﬁll their timing constraints, the higher the performance of the system. A higher utilization may imply a lower design cost while a lower utilization may imply an over-designed network. We designed three groups of experiments. The ﬁrst two groups consider delay-constrained messages. The ﬁrst (Fig. 4)  963 concerns only delay-constrained RT trafﬁc ( 	C2 ), and B  = 1 and B = 0. An RT message with a shorter period has a higher priority. The overhead due to the priority is two ﬂits. The second one (Fig. 5) concerns both delay-constrained RT (	C2 ) and delay-constrained NT (	C4 ) trafﬁc with various values of bandwidth partitioning. The last one (Fig. 6) considers jitter-constrained trafﬁc, i.e., 	C1 and 	C3 messages. The jitter j is set to be 0:15; thus the network latency of a feasible message falls in the region [0:85; ]. 0 0.1 0.1 0.2 0.2 0.3 0.3 0.4 0.4 0.5 0.5 0.6 0.6 Generated Traffic 0.7 0.7 0.8 0.8 0.9 0.9 0 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 s s a P R a i t o ‹Pass ratio 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 N e t w o r k U i l i t a z i t n o Offered trafficﬁ ‹Feasible messages Fig. 4. Delay-constrained RT Trafﬁc ( 	C2 ) In Fig. 4, as the generated trafﬁc increases, the pass ratio decreases but the network utilization increases up to around 0:37. Closing to this point, the network is near to saturation where the network latency increases exponentially but the throughput does not improve any more [1]. Therefore the gap between the offered trafﬁc and the feasible trafﬁc increases rapidly. Also, the pass ratio with this uniform trafﬁc pattern is always below 1. For a hard real-time system that requires 100 pass ratio, this means we need to ﬁnd an application-speci ﬁc mapping and our feasibility assessment can support such a mapping. In Fig. 5, 	C2 and 	C4 messages are randomly generated; thus the number and message sizes of the RT and NT trafﬁc have equal probability. With the value of B : B  increasing, the network tends to achieve higher pass ratio and utilization. In Fig. 6, 	C1 and 	C3 messages are also randomly generated. Comparing with Fig. 5, the corresponding pass ratio and network utilization are reduced. This is because a jitter constraint adds another condition (D   j (cid:20) T ) besides the deadline constraint (T (cid:20) D), leading to fewer messages that pass the feasibility test. V. CONCLU S ION We have presented a feasibility analysis of messages in wormhole-routed networks on chip which is a crucial step in a NoC design ﬂow. The contention tree we formulate can accurately re ﬂect the network contentions but relies on deterministic routing. The static bandwidth partitioning method for coexisting RT and NT messages is simple but can illustrate some non-obvious results. From the experiments conducted, we can see that the feasibility analysis is useful for performance/cost tradeoff analysis of mapping messages with different QoS requirements on a NoC. 0 0.1 0.1 0.2 0.2 0.3 0.3 0.4 0.4 0.5 0.5 0.6 0.6 Generated Traffic 0.7 0.7 Bnt=0.1,Brt=0.9 Bnt=0.5,Brt=0.5 Bnt=0.9,Brt=0.1 0.8 0.8 0.9 0.9 0 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 s s a P R a i t o 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 N e t w o r k U i l i t a z i t n o Fig. 5. Delay-constrained Trafﬁc ( 	C2 -	C4 ) with Bandwidth Partitioning 0 0.1 0.1 0.2 0.2 0.3 0.3 0.4 0.4 0.5 0.5 0.6 0.6 Generated Traffic 0.7 0.7 Bnt=0.1,Brt=0.9 Bnt=0.5,Brt=0.5 Bnt=0.9,Brt=0.1 0.8 0.8 0.9 0.9 0 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 s s a P R a i t o 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 N e t w o r k U i l i t a z i t n o Fig. 6. Jitter-constrained Trafﬁc ( 	C1 -	C3 ) with Bandwidth Partitioning Future work will investigate methods to enhance the pass ratio and/or network utilization by combining the feasibility assessment with task-to-node mappings. "
2005,Mapping and physical planning of networks-on-chip architectures with quality-of-service guarantees.,"Networks on chips (NoCs) have evolved as the communication design paradigm of future systems on chips (SoCs). In this work we target the NoC design of complex SoCs with heterogeneous processor/memory cores, providing quality-of-service (QoS) for the application. We present an integrated approach to mapping of cores onto NoC topologies and physical planning of NoCs, where the position and size of the cores and network components are computed. Our design methodology automates NoC mapping, physical planning, topology selection, topology optimization and instantiation, bridging an important design gap in building application specific NoCs. We also present a methodology to guarantee QoS for the application during the mapping-physical planning process by satisfying the delay/jitter constraints and real-time constraints of the traffic streams. Experimental studies show large area savings (up to 2/spl times/), bandwidth savings (up to 5/spl times/) and network component savings (up to 2.2/spl times/ in buffer count, 3.8/spl times/ in number of wires, 1.6/spl times/ in switch ports) compared to traditional design approaches.","Mapping and Physical Planning of Networks-on-Chip Architectures with Quality-of-Service Guarantees Srinivasan Murali CSL, Stanford University Stanford, CA-94305 smurali@stanford.edu Luca Benini Giovanni De Micheli DEIS, Univ. of Bologna CSL, Stanford University Bologna, Italy Stanford, CA-94305, USA lbenini@deis.unibo.it nanni@stanford.edu Abstract — Networks on Chips (NoCs) have evolved as the communication design paradigm of future Systems on Chips (SoCs). In this work we target the NoC design of complex SoCs with heterogeneous processor/memory cores, providing Qualityof-Service (QoS) for the application. We present an integrated approach to mapping of cores onto NoC topologies and physical planning of NoCs, where the position and size of the cores and network components are computed. Our design methodology automates NoC mapping, physical planning, topology selection, topology optimization and instantiation, bridging an important design gap in building application speci ﬁc NoCs. We also pre sent a methodology to guarantee QoS for the application during the mapping-physical planning process by satisfying the delay/jitter constraints and real-time constraints of the traf ﬁc stream s. Experimental studies show large area savings (up to 2×), bandwidth savings (up to 5×) and network component savings (up to 2.2× in buffer count, 3.8× in number of wires, 1.6× in switch ports) compared to traditional design approaches. Keywords: Networks on Chips, Systems on Chips, Mapping, Physical Planning, QoS, Optimization. I . INT RODUCT ION With scaling of transistor sizes, the number of processor and memory cores on Systems on Chips (SoCs) and their speed of operation is increasing. Future SoCs will have many different applications such as speech recognition, ambient intelligence, high-end video processing capabilities, 3D gaming, etc on the same device. In such complex systems, communication between the cores will become a major bottleneck as current busbased communication architectures (either single bus or the state-of-the art multiple buses) will be inefﬁcient in term s of throughput, latency and power consumption [2], [10]. Scalable interconnection networks are needed to interconnect the cores and the resulting communication centric design paradigm is called as Networks on Chip (NoC) [2]. SoCs are aggressively designed to meet the performance requirements of diverse applications that need to be supported. In most cases the cores in the SoC are heterogeneous in nature with each core performing a set of specialized functions in order to maximize performance and satisfy design constraints such as Quality-of-Service (QoS) for the applications. As an example, consider an efﬁcient design of an MPEG4 decoder shown in Figure 1(a) [12]. In this design, there are several processors (for e.g. RISC), several hardware cores (e.g. Upsampler) and memory cores (e.g. SDRAM). Each core has different functionality, size and communication requirements. Some of the cores are hard cores, with size ﬁxed during design (e.g. RISC) and some of the cores are soft cores, whose size can be varied with some restrictions on the aspect ratios (e.g. Upsampler). Figure 1(b) shows the design area for the best mappings of the MPEG4 onto a mesh topology for two schemes: in the ﬁrst scheme the mapping of the cores is done logically (without considering the physical planning of au vu ra st sram bab SDRAM samp spad up dsp sram mcpu RISC Area (sq mm) 36.0 20.25 Scheme   1. Mapping Then Physical Planning Integrated Mapping & Physical Planning 2. (a) MPEG4 (b) Mapping Schemes Cores Cores 0 1 2 3 4 5 6 7 Stag1 Stag2 Stag3 0 1 2 3 0 1 2 0 1 2 3 3 0 1 2 3 4 5 6 7 (c) Butterﬂ y Network Fig. 1. MPEG4 mapping schemes and example butterﬂ y topology the cores) followed by a separate physical planning phase and in the second scheme the mapping and physical planning are done together, so that the mapping process takes the physical planning information, i.e., the position of the cores and network components (e.g. switches, links) and the size of soft cores and switches in the 2-D plane. There is signiﬁ cant area improvement in the second scheme where mapping and physical planning are integrated together. This improvement will be even more pronounced for indirect topologies such as the butterﬂ y network shown in Figure 1(c). In a butter ﬂ y topology, logically, the switches are arranged as stages with the switches in the ﬁ rst and last stages connected to the cores. Ideally we would like to distribute the switches around the cores so that performance of the NoC is maximized and mappings onto the butter ﬂ y should take this physical planning information into account. Another important design consideration for SoCs is to guarantee Quality-of-Service (QoS) for the application. As an example, in many video applications, data should be communicated in such a way that the system supports a pre-determined frame rate (e.g. 30 frames/s in many video displays). The network should support the QoS requirements of the applications satisfying the delay constraints of the trafﬁ c streams. It should also provide support for real-time communication. These QoS guarantees need to be considered during the mapping process. Moreover the burstiness in the trafﬁ c streams (that makes providing QoS guarantees harder) needs to be considered. In this work, we propose a design methodology for building complex application-speciﬁ c NoCs. We provide an integrated approach to mapping and physical planning, where we determine the 2-D position of the cores and network components and the size of soft cores and switches during the mapping process. The physical planning phase also automatically computes the switch buffers needed to support the application trafﬁ c and integrates this in the switch size computation. We also present a method to provide QoS guarantees for the application during the mapping-physical planning phase. For QoS guarantees, we consider the burstiness in the application traf 27 Phase 2 Phase 3 Appn Co−Design Simulation Mapping Physical Planning comm graph constr −aints Regula −tor design Topology Library Routing Library phase 1 Fig. 2. Design Methology NoC  Optimization xpipesCompiler SystemC Design Simulation ﬁc, delay/jitter constraints of the individual trafﬁc stre ams and provide support for real-time communication. The additional power-area overhead in obtaining the QoS guarantees is negligible. We integrate all these features into our tool presented in [21]. The mapping and physical planning of the cores is applied to several topologies de ﬁned in a topology library a nd the best topology for the application is automatically selected. In the resulting topology, the switches and links are optimized for the trafﬁc characteristics, followed by automatic inst antiation of the topology. Thus our integrated design methodology automates mapping, physical planning, topology selection, optimization and instantiation for an application providing QoS guarantees, thereby bridging an important design gap in building application-speciﬁc NoCs. I I . PREV IOU S WORK The use of NoCs to replace global wiring is presented in [1], [2]. The use of an Octagon topology for Network Processor design is presented in [3]. In recent years a large body of research focuses on the design methodologies for NoCs such as the Nostrum [4], Scalable Programmable Interconnection Network (SPIN) [5], aSoC [8], etc. We refer the reader to [10] for details of several NoC design methodologies. Several research works, such as [18], [19], [9], [7] have been presented for automatically instantiating software/RTL design of network components (such as network interfaces, switches and links) of an NoC. QoS guarantees to applications is provided by efﬁcient router design in the Aethereal NoC [6]. In [17], performance analysis of communication architectures is presented. Stochastic analysis of on-chip MPEG trafﬁc is presented in [22]. Topology design of NoCs for well-behaved trafﬁc patterns is explored in [13], [14], [21], [20]. Mapping of cores onto NoC topologies is presented in [15], [16], [21],[20], [13]. In all these works, the mapping process is based on the average communication demands of the cores. A physical planner for homogeneous NoCs is presented in [23]. As the main contributions of this paper, we present an integrated approach to mapping and physical planning, guaranteeing QoS for applications by using a trafﬁc regulator base d design and integrate the mapping/physical planning phase with our existing tools to automate the complex design ﬂow of NoCs. During the mapping-physical planning process we consider the burstiness, criticality and delay constraints of the trafﬁc streams and ensure that QoS is guaranteed. We integrate these features to [21], so that our mapping and physical planning phases are followed by automatic topology selection and instantiation (of the SystemC design) of the NoC. The resulting NoC can be simulated at cycle-accurate level. I I I . D E S IGN M E THODOLOGY The design methodology of the mapping and physical planning is illustrated in Figure 2. We assume that the parallel Mem ory ARM 100 100 Filter 50 50 50 50 50 50 IFFT FFT Disp lay Avg BW in MB/S (a) DSP Application Filter             ARM 500 cycles delay constraint ... ... 100 cycles burst B1 900 cycles silence period ARM              Filter critical stream ... (b) Trafﬁc Flow Fig. 3. DSP Filter application and trafﬁc ﬂow between ARM & Filter cores Phase 4 kernels in the application are mapped onto hardware/software cores using existing tools (such as [11]). In the ﬁrst phase, an initial simulation is carried out, from which the trafﬁc characteristics (such as the burstiness, d elay/jitter constraints and criticality of the trafﬁc strea ms) are obtained. Based on the trafﬁc characteristics of the applic ation, graphs representing the trafﬁc ﬂow between cores and bandwidth constraints required for the various ﬂows are generat ed. At this step, trafﬁc regulators for ensuring QoS guarantees [25] are developed. These steps are explained in detail in sections IV and V. In the next phase, the cores are mapped onto the topologies that are de ﬁned in the topology library. The mapping process is integrated with the physical planning process, where the positions and sizes of the various cores and switches are generated. The mapping/physical planning considers several design objectives such as minimizing design area, power or hop delay, satisfying the QoS constraints of the application (such as criticality constraints, delay/jitter constraints). As in [21], several routing functions de ﬁned in a library are also considered. T his phase is explained in sections VI and VII. In the third phase, the best network topology for the application (the topology that minimizes the design objective and satisﬁes the design constraints) is selected and a postoptimization of the network resources is carried out. In this step, the redundant switches, ports and links are removed and the bit-width (or frequency) of the links is adjusted to match the application needs. In the last phase, the SystemC design of the resulting NoC is automatically generated using ×pipesCompiler [18]. The ×pipesCompiler instantiates SystemC network components (switches, links and network interfaces) for the NoC and automatically integrates them with the cores. The resulting design is then simulated at cycle-accurate level. IV. ON -CH I P TRA FFIC MODE L ING In this section, we develop trafﬁc models to characterize th e application trafﬁc, providing QoS guarantees for the appli cation. As an example, consider the trafﬁc ﬂowing between the Filter core and the ARM core in a DSP Filter application (refer Figure 3). Without loss of generality assume that the packet size is such that a packet is sent in one cycle, although the following discussion also applies when a packet is sent over multiple cycles (i.e. when a packet has multiple ﬂits). Ther e are three important features to be noted from Figure 3(b). • Bursty Trafﬁc Flows: The application trafﬁc from Filter to ARM core is bursty in nature, with a burst period of 100 cycles followed by 900-cycles of silence period. The peak-bandwidth of the trafﬁc (100 packtes/100 cycles) is an order of magnitude higher than the average bandwidth (100 packets/1000 cycles). • Delay/Jitter Constraints: Each burst from the Filter core has a delay constraint by which it should reach the ARM core. In this example, we assume that the burst B1 has to reach the ARM core by 500 cycles, which is obtained from the application characteristics.  28 TABLE I L I NK IM P L EM EN TAT I ON Scheme 1. Avg 2. Peak 3. Opt BW (pk/cy) 100 1000 200 Delay (cyles) 1000 100 500 Increment Signal (given every 1/rho  cycles) Packets from core Comparator Saturating Counter  (sigma) Decrement Signal (sigma,rho) CP 50 v1 v2 100 50 1000 v3 50 50 50 50 v4 critical stream wieghted by 10 regulated traffic v5 v6 Fig. 4. A (σ ,ρ) regulator Fig. 5. Weighted core graph v1 v4 100 100 v5 v2 200 200 100 100 100 v6 v3 100 Fig. 6. Constraint Graph:BW in MB/S • Real Time Constraints: The ARM core issues a control stream to the Filter which is assumed to be critical and needs to reach the Filter as quickly as possible. These real-time requirements need to be satisﬁed by the network. by using a weighted communication graph (called as weighted core graph), where the weights are a function of the criticality. In the next section, we explain this in more detail, where we present mathematical models for modeling the ρ value for the regulators. Consider three implementations of the communication link (refer Table I) between the Filter core and ARM core (for illustrative purposes assume other cores don’t send trafﬁc on thi s link). In the ﬁrst case, the link is designed to support the av erage bandwidth of trafﬁc ﬂowing between Filter and ARM. As seen from Table I, the delay incurred in this scheme for the burst B1 violates the delay constraint for the stream. In the second case, the link is designed for the peak bandwidth requirements and the delay constraints are met. However, the link is over-designed with 5× the capacity that is needed to support the delay constraints of the burst. In the third case, the link is optimally designed to support the burst without violating the delay constraints. From this example, it is clear that the communication links should be designed optimally in a way such that they support the trafﬁc ﬂowing through them, satisfying the delay/j itter constraints of the trafﬁc streams. Moreover, there should b e a mechanism that ensures that each core sends trafﬁc so that the links can support the trafﬁc and the delay constraints ar e met. Clearly these two objectives complement each other and to ensure that the objectives are met we propose the use of trafﬁc regulators for NoCs. Trafﬁc regulators are widely used in ATM networks to guarantee QoS to applications [25]. A trafﬁc regulator can be abstracted as a hardware block with two parameters: σ and ρ. The parameter ρ represents the bandwidth required to support the trafﬁc streams so that the delay constraints are met and the parameter σ represents the variations permitted over the ρ value. Such a regulator is also called as a (σ ,ρ) regulator [25]. The trafﬁc ﬂow between each sourcedestination is represented by a (σ , ρ) value. As an example, the Filter to ARM communication is represented by (0,0.2), which means that one packet can be be sent every 5 cycles (i.e. one packet can be sent every 1/ρ = 1/0.2 = 5 cycles) and no variations over the required rate is permitted (as the σ value is 0). A (1,0.2) regulator would allow a burst of one packet over the required packet rate. In the rest of this paper, we assume that the σ value is chosen to be equal to 0, so that no variation is permitted over the required rate. To ensure that each core sends data according to the regulator values, we need to add small hardware to each core (or to the Network Interface connecting the core to the network), which is shown in Figure 4. The additional hardware consists of a saturating credit counter and a comparator. The saturating counter is incremented at rate ρ and saturates when it reaches a count of (1 + σ). A packet is transmitted only if the credit counter is non-zero and when a packet is transmitted the counter is decremented by 1. This counter ensures that the amount of trafﬁc transmitted by the source matches the rate for which the links are designed to handle. For trafﬁc streams to different destinations, differe nt sets of (σ, ρ) values are used in the regulator. Note that power-area overhead of such a regulator is negligible as it’s just a counter and a comparator. For supporting real-time constraints, we assume tight latency bounds for the real-time stream and during the mapping process we consider the criticality of the stream V. PROBL EM FORMULAT ION The communication between the cores of the SoC is represented by the weighted core graph: De ﬁnition 1 The weighted core graph is a directed graph, G(V , E ) with each vertex vi ∈ V representing a core and the directed edge (vi , vj ), denoted as ei,j ∈ E , representing the communication between the cores vi and vj . The weight of the edge ei,j , denoted by commi,j , represents the average bandwidth of the communication from vi to vj weighted by the criticality of the communication. As an example, the weighted core graph of the Filter application is given in Figure 5. The edge weights are a function of the criticality of the stream (which depends on the application characteristics) and the amount of trafﬁc communicated in t he stream. The value of the weights depends on how critical are the streams and on the number of classes of streams. In this work, we assume two classes of streams: non-critical and critical and weigh the critical streams by a factor of 10 compared to non-critical streams. Other approaches such as weighing a stream based on the amount of slack permitted for the stream (as used in [17]) can also be used. De ﬁnition 2 In G(V , E ), the trafﬁc ﬂow from each source to each destination vj , ∀i, j ∈ V is represented by the set Ti,j . Each Ti,j comprises of Mi,j bursts, with each burst bi,j,k , ∀k ∈ Mi,j , having a burst length of bleni,j,k cycles and a latency window of blati,j,k cycles. vi In the above example, the trafﬁc ﬂow between the Filter and the ARM (v3 and v2 ) is represented by the set T3,2 . The set T3,2 consists of 1 burst (we assume such a small sampling window for illustrative purposes), with M3,2 equal to 1, blen3,2,1 equal to 100 cycles and blat3,2,1 equal to 400 cycles. The latency window, blati,j,k is the deadline (or slack) that is permissible for the burst, which is obtained from the initial simulation of the application and the application characteristics. The ρi,j values of the regulator for each source vi to destination vj is obtained by: ρi,j = max∀k∈Mi,j (cid:18) bleni,j,k bleni,j,k + blati,j,k (cid:19) ∀i, j s.t. ei,j ∈ |E | (1) De ﬁnition 3 The bandwidth constraint graph CG(Q, R) is a directed graph where the vertex and edge sets are equal to the vertex and edge sets of G(V , E ) but with edge weights ri,j equal to ρi,j × P acketS iz e/C ycletime, ∀i, j ∈ s.t. ri,j ∈ |R|.  29 6 7 8 Mapping_and_physicalplanning(G,CG,P) {       obtain an initial greedy mapping of G onto P; 1        obtain greedy mappings of higher dimensional 2        topologies onto 2D plane; 3 In each iteration of the robust tabu search perform 4 { for all i and j in U of topology graph { generate current move by swapping the cores 5 in positions i and j  ; if current move is not tabu        compute routes  based on routing function; { Simultaneously perform following steps in physical planning using MILP: Opimize design area, power or hop delay; Compute the core and switch sizes and positions; Compute number of buffers; Check whether QoS constraints are satisfied; Update tabu list and check whether aspiration condition is met; if current iteration’s cost is better than best cost update the best cost and best solution; 9 10 11 } } } } Fig. 7. Mapping and Physical Planning Algorithm The bandwidth constraint graph for the above example is given in Figure 6. The edge weights in the graph are ρ × packetsiz e/C ycletime values for the corresponding trafﬁc ﬂows. When calculating the ρ values, we neglect the network latency as it is of the order of tens of cycles (refer section VIIIB), while burst lengths and latency windows are of the order of hundreds of cycles. The NoC topology is de ﬁned by the adjacency information of nodes in the topology and by the capacity of the links. Formally the NoC topology is de ﬁned as: De ﬁnition 4 The NoC topology graph is a directed graph P (U, F ) with each vertex ui ∈ U representing a node in the topology and the directed edge (ui , uj ), denoted as fi,j ∈ F representing a direct communication between the vertices ui and uj . The weight of the edge fi,j , denoted by bwi,j , represents the bandwidth available across the edge fi,j . The mapping of the application cores onto an NoC architecture is de ﬁned by the one-to-one mapping function: map : V → U , s.t. map(vi ) = uj , ∀vi ∈ V , ∃uj ∈ U (2) Each link in the mapped NoC should satisfy the bandwidth constraints corresponding to the constraint graph CG(Q, R). The design objective (area, power or hop delay) of a mapping is obtained from the physical planning of the mapping. This ensures that the heterogeneity in the size of the cores and network components is taken into account for accurate estimation of the design objectives. V I . MA P P ING AND PHY S ICAL PLANN ING ALGOR ITHM In this section we present the algorithm for mapping and physical planning. The general problem of embedding one graph into another is intractable and is a special case of the Quadratic Assignment Problem (QAP) [26]. QAP is well studied in the literature with many heuristic algorithms available [27]. In [27], robust tabu search is shown to be most effective for many classes of QAP and we use this to solve our mapping problem. The general structure of the mapping-physical planning algorithm is shown in Figure 7. In the ﬁrst step an initial greedy mapping of the cores onto the topology is obtained. We also assume a greedy mapping                                                                                                                                                                      Swtiches restricted to regions cores distributed in a 2D plane Swtiches restricted to regions (a) Direct topologies (b) Indirect topologies Fig. 8. Switch Position Restriction for direct and indirect topologies of higher dimensional topologies (such as hypercube) onto the 2D plane. Then for each iteration of the robust tabu search, we perform the following computations: • Compute the routes for the trafﬁc ﬂowing between the cores, based upon the routing function chosen from the library. Details of the algorithms for route computation and their implementation for different topologies are presented in our earlier works [20],[21]. • Physical planning for this mapping. This includes computing the positions of the cores and the switches, sizes of the switches & soft cores and automatic computation of switch buffers needed for the application. These steps are explained in detail in the next section. • Check whether the mapping satisﬁes the delay/jitter and area constraints. For delay constraints, the links in the NoC should support the trafﬁc through them, which is determined by the (σ ,ρ) regulator values as explained in sections IV and V. We also check whether the real-time constraints for the critical streams are met by checking whether the hop delay for the streams are lower than the required value, which is obtained from the application characteristics. In each step of the tabu search we try to optimize the design objective (area, power or hop delay) satisfying the QoS and criticality constraints. The area and power values are obtained from physical planning of that particular mapping. The parameters of the tabu search (such as the size of tabu list, aspiration function computation, etc.) are chosen as explained in [27]. This tabu search is applied to all topologies in the library. Our library currently has mesh, torus, hypercube, Clos and butterﬂy topologies, while other topologies can be easily added to the library. The best topology is selected and the switches and links are optimized to match the application characteristics. In this step, redundant switch ports and links (i.e. the links that don’t carry any trafﬁc and the corresponding switch ports) a re eliminated. The links are sized (by changing the bit-width of the links or frequency of operation) according to the trafﬁc ﬂowing through them. After this network optimization step, the SystemC design of the NoC is generated automatically using the ×pipesCompiler. All these steps of topology mapping, physical planning, topology selection, optimization and instantiation are seamlessly integrated, so that all these processes are completely automated 1 . V I I . PHY S ICAL PLANN ING We use a Mixed Integer Linear Program (MILP) based physical planning algorithm. An MILP based physical planning for minimizing area, power of a design is presented in [24]. We modify this approach for NoCs by considering NoC speciﬁc features such as switch positioning, switch buffer calculation, etc. 1We also provide a feature that allows the user to interact in each of the phases if manual intervention is desired.  30 D E S I GN A R EA F O R V I D EO A P P L I CAT IONS TABLE II Appln VOPD MPEG PIP MWA Avg Area-1 Area-2 Ratio sqr mm sqr mm 20.25 18.01 36 20.25 20.25 10.565 33 25 1.12 1.19 1.92 1.32 1.39 As the cores are pre-designed components, we assume the area and power values of the cores as an input. We also assume the type of the core (hard or soft) and aspect ratio constraints as an input. We use area, power libraries for various con ﬁgurat ion of switches that are developed in [18],[21]. For a given mapping, the relative position of the cores with respect to each other is obtained from the tabu search, but the relative position of the switches is unknown. The switches in a direct topology (such as mesh, torus, hypercube) can be placed anywhere around the core to which it is connected. An important constraint to be considered in the MILP is that the switches and the cores should not overlap each other. If the switch positions are not restricted to a small region around the core, solving this overlap calculation as an MILP will be time consuming for large problem sizes (for > 20 cores). To allow scaling of the algorithm, we restrict each switch to lie in a region of adjacent cores surrounding the core to which it’s connected (refer Figure 8(a)). By restricting the switch positions to a small region, the overlap calculations are several orders of magnitude faster and are scalable for large problem sizes. The solution obtained in this scheme, for all our simulations, are within 1% from the solution obtained without restricting switch sizes as the switch position tends to be close to the core to which it’s connected. For the indirect topologies (such as the Clos and butter ﬂy), we distribute the switches along the cores in a 2D plane, based on their connectivity to the cores and to other switches (refer Figure 8(b)). Here again we restrict switch locations to lie with in certain regions as shown in the ﬁgure. Then during each step of the tabu search, we compute the actual positions of the switches and cores. During the physical planning, we also compute the buffering needed at each switch. We assume that the links are pipelined with the number of pipeline stages depending upon the link length. For wormhole (or virtual channel) based switches with credit based ﬂow control, for maximum throughput, the number of buffers in the switches should be equal to 2N + M , where N is the number of pipeline stages in the link and M is the delay incurred for credit processing at the upstream and downstream switches [25]. As the switch size (power) depends on the number of buffers, we integrate this as a constraint in the MILP by breaking down the switch area (power) as a sum of buffer area (power) and crossbar (including logic) area (power). The buffer area (power) is a function of link length and is automatically calculated during physical planning. V I I I . EX PE R IMENT S AND CA SE STUD IE S A.EffectofPhysicalPlanning In this sub-section we investigate the effect of combined mapping and physical planning applied to a variety of video applications. We consider four different video applications: Video Object Plane Decoder (VOPD-12 cores), MPEG4 decoder (mapped onto 12 cores), Picture-In-Picture application (PIP-8 cores), Multi-Window Application (MWA-14 cores). We assume that our design objective is to minimize design area subject to delay/jitter and criticality constraints. We consider two schemes: in the ﬁrst scheme the mapping and physical planning phases are done separately (as in past works) and in the second scheme we use our integrated approach to mapping and physical planning. The design area for the video applications as obtained for both the schemes are presented in Table II. On an average we have 1.4× area savings in our approach. B.Design forQoSGuarantees In traditional design methodology, QoS can be guaranteed by designing the network to support the worst-case bandwidth needs of the application. Such a worst-case design approach, however, leads to an over-design of the network components. By using (σ , ρ) trafﬁc regulation methodology for NoCs presented in this paper, the network components are designed optimally to support the QoS constraints of the application. As an example, for the DSP Filter application (Figure 3), the minimum bandwidth needed (assuming minimum-path routing) for our design methodology is 5× lower than a worst-case design approach. Moreover, in our design methodology, the network is made to operate at very low contention, thereby reducing contention delay and power. Figure 9 shows the packet latency as obtained from the actual simulations of the DSP Filter application. In the ﬁrst case, the links are designed to handle the average trafﬁc through them. As the trafﬁc is bursty in nature, such a design approach leads to high network contention resulting in large packet latency. In the second case, the links are designed with our design methodology. The average latency is almost equal to the worst-case design approach (case 3) where the network components are over designed. As our design methodology for trafﬁc regulators is based on ini tial simulation, it is static in nature and doesn’t capture dynamic variations in the input data streams. But for many SoC applications, the trafﬁc characteristics don’t vary a lot wit h the input data [14], [15]. Thus our design methodology incurs only slight increase in latency (around 10%) due to dynamic changes in data when compared to the worst-case design approach. C.VOPDDesign In our earlier work [21], butter ﬂy topology was found to be the best topology for VOPD. However, the design approach was based on average case analysis without considering QoS guarantees. In this sub-section, we explore VOPD mapping and physical-planning with QoS guarantees. We assume a conservative link bandwidth of 2GB/S . The bandwidth constraint graph for the VOPD application, based on the trafﬁc characteristics and QoS needs of the appl ication is presented in Figure 10. For minimum-path mapping, the minimum bandwidth needed to support the application is 2.4GB/S and can’t be supported by any of the topologies. So we apply split-trafﬁc routing, spreading the trafﬁc betw een the cores across multiple paths. As a butter ﬂy network has no path diversity (only one path from any source to any destination) [25], it can’t support the trafﬁc requirements of the a pplication. All other topologies produce feasible mappings with split-trafﬁc routing. We assume that the objective is to min imize power consumption of the design, satisfying QoS and area constraints. Figure 11 shows the power consumption of the topologies. Mesh has the least power and is the best topology for VOPD for the chosen design objective. D.BufferSizingandNetworkOptimization During physical planning, the number of buffers needed for the switches is automatically computed based on the link lengths (refer section VII) and this is integrated into the area (power) calculations of the physical planner. When the number of buffers is lower than the required number, throughput of the network is low. On the other hand, when the number of buffers is more than needed, the throughput remains the same, but switch area and power are increased. As an example, let us consider a homogeneous 16-node torus NoC in which each link has 4 pipeline stages. Let us assume that the credit processing delay (the M value) is 2 cycles, which is typical for most credit-based switches. Figure 12 shows the throughput  31 350 vld inv scan run le dec 675 1810 1810 stripe  mem 245 1785 iquan idct acdc pred 1810 up samp 80 1765 Arm 1565 pad 750 1565 vop rec vop mem 2350 1250 Fig. 9. Avg. Latency for DSP Fig. 10. Bandwidth constraint graph for VOPD with bandwidth in MB/S TABLE III N E TWO RK O P T IM I ZAT ION Component Buffers Wire count Ports Savings 2.2× 3.77× 1.6× dependence on the total number of buffers in the switches for the NoC. As seen, the relative throughput increases till the optimal count of 702 buffers, after which it remains constant. With our buffer computation methodology, the physical planner automatically computes this optimum number of buffers needed to support maximum throughput. Note that in a heterogeneous SoC, the number of buffers can be different for different switches and even different for different inputs of the same switch as the link lengths are non-uniform in nature. Even in this case, the physical planner automatically computes the optimum number of buffers needed at each input of the switch based on the corresponding link lengths. For the VOPD application, compared to an average-case design (where all the switches have the same number of buffers) we get 2.2× reduction in buffer count in our scheme. After the topology selection phase, the network components (switches and links) are optimized based on the trafﬁc ﬂowin g through them. The links and switch ports that don’t carry any trafﬁc are removed. Other links and switches are optimized t o match the trafﬁc rate through them by changing the bit-width of the links. The effect of network optimization on VOPD design is reported in Table III. For all our experiments, the mapping and physical planning phases are executed in few minutes on a 1GHz SUN workstation and the algorithms are scalable for hundreds of cores. IX . CONCLU S ION S AND FUTURE WORK Networks on Chip (NoC) based communication architectures are needed to handle the heavy communication demands of future SoCs. SoCs are aggressively designed with mostly heterogeneous processor/memory cores to maximize system performance. The mapping of such heterogeneous cores onto NoCs, physical planning (computing position and size of the cores and network elements), topology selection, topology optimization and instantiation are important phases in designing application-speciﬁc NoCs. In this work we have presented a design methodology that automates all these steps. Mapping and physical planning phases are integrated together, resulting in better NoC design (up to 2× area improvement) than traditional methodology that decouples the phases. Another important design consideration for NoCs is to guarantee Quality-of-Service (QoS) for the application. In this work we have presented a methodology for guaranteeing QoS during mapping/physical planning phase, considering the burstiness of trafﬁc, satisfying delay/jitter constraints and real-t ime constraints for the trafﬁc streams. Our design approach result s in large bandwidth savings (up to 5×) and network component savings compared to traditional design approaches. In future, we plan to enhance the methodology for guaranteeing QoS to (in mW) Topol. Power No mapping 542 930 960 753 Bfly Mesh Torus Hyp. Clos Fig. 11. VOPD Design Fig. 12. Throughput vs. buffer count take dynamic variations in the input streams. X . ACKNOW L EDGEMENT S This research is supported by MARCO Gigascale Systems Research Center (GSRC) and NSF (under contract CCR0305718). "
2005,SAGA - synthesis technique for guaranteed throughput NoC architectures.,"We present SAGA, a novel genetic algorithm (GA) based technique for synthesis of custom NoC architectures that support guaranteed throughput traffic. The technique accepts as input a communication trace graph, amount of data, period, and deadline for each trace, interconnection network architecture elements, and generates a custom NoC topology, and routing and schedule of the communication traces on the architecture. SAGA minimizes both the energy consumption and area of the design by solving a multi-objective optimization problem. We present a detailed analysis of the quality of the results and the solution times of the proposed technique by extensive experimentation with realistic benchmarks and comparisons with optimal MILP solutions. SAGA is able to generate solutions that are as good as the optimal solutions produced by the MILP formulation. Whereas the MILP formulation run time rises exponentially for even moderately sized graphs, SAGA generates solutions for large graphs in reasonable time.","SAGA: Synthesis Technique for Guaranteed Throughput NoC Architectures Krishnan Srinivasan, and Karam S. Chatha Department of CSE, PO BOX 875406, Arizona State University, Tempe, AZ 85287-5406 Email:   ksrinivasan,kchatha  @asu.edu Abstract— We present SAGA, a novel genetic algorithm (GA) based technique for synthesis of custom NoC architectures that support guaranteed throughput trafﬁc. The technique accepts as input a communication trace graph, amount of data, period, and deadline for each trace, interconnection network architecture elements, and generates a custom NoC topology, and routing and schedule of the communication traces on the architecture. SAGA minimizes both the energy consumption and area of the design by solving a multi-objective optimization problem. We present a detailed analysis of the quality of the results and the solution times of the proposed technique by extensive experimentation with realistic benchmarks and comparisons with optimal MILP solutions. SAGA is able to generate solutions that are as good as the optimal solutions produced by the MILP formulation. Whereas the MILP formulation run time rises exponentially for even moderately sized graphs, SAGA generates solutions for large graphs in reasonable time. PACKET TAIL FLIT FLIT HEADER FLIT LINK SEST ROUTER INPUT PORT LELT BUFFER SWITCH OUTPUT PORT INPUT PORT I . INTRODUCT ION Fig. 1. Router model SYSTEM−LEVEL COMPUTATION SPECIFICATION P/M 1 C1(N,P,D) P/M 2 C2(N,P,D) C4(N,P,D) C6(N,P,D) C3(N,P,D) P/M 3 C5(N,P,D) P/M 4 C7(N,P,D) P/M 5 C8(N,P,D) P/M 6 SYSTEM−LEVEL COMMUNICATION ARCHITECTURE SYNTHESIS ROUTER LIBRARY 1) Number of ports 2) T  , T   3) E  , E   L     S L     S P/M 4 P/M 5 NOC C3,C6,C7 C5,C7,C8 ROUTERS LINKS C1,C2,C3,C6 R1 P/M 1 C2,C4,C7 R2 C8 P/M 6 C1,C4 P/M 2 SCHEDULE C2,C4,C5 P/M 3 C2 C4 C7 SYSTEM−LEVEL COMPUTATION AND COMMUNICATION SPECIFICATION Fig. 2. NoC Synthesis Network-on-Chip (NoC) has been proposed as a solution for the communication challenges in the nanoscale regime [1] [2]. Packet switching supports asynchronous transfer of information. It provides extremely high bandwidth by distributing the propagation delay across multiple switches, and thus pipelining the signal transmission. In the lower half of Figure 2, an SoC architecture with an NoC is depicted. In the ﬁgure, the various “P/M” blocks denote processing (DSP, ASIC, FPGA) cores or storage elements (Cache, SRAM, CAM), and “R” denotes the router nodes. The lines between various blocks represent the network links. The “R” blocks along with the physical links form the NoC. Many real-time applications such as multimedia applications require that the jitter in the latency of data communication be minimal. Low delay jitter communication can be ensured by scheduling of the onward transmission of the various traces at the router ports. The paper focuses on the synthesis of application speciﬁc NoC architectures that support guaranteed throughput trafﬁc. The interconnection architecture, and router model that we assume for synthesis is shown in Figure 1. Data is transmitted as packets. Each packet is composed of a series of ﬂits. Flit is the basic unit of data communication and is transfered in parallel between two routers. Every packet has two special ﬂits known as header and tail ﬂit respectively, that denote the start and end of the packet. Each router consists of several ports. For the sake of simplicity, we only show a few ports for the two routers in the ﬁgure. We assume that the router performs is  wormhole switching and it contains a buffer only at the input port. On reception the header ﬂit is decoded and a request is generated for crossbar access to the appropriate output port. We assume that shortest time required for obtaining crossbar access, and related energy consumption is  clock cycles, and joules, respectively. Typically,  	 clock cycles. Once the crossbar access has been granted, each ﬂit is transfered over the physical links from the input port of the current router, through the switch to the input port of the neighboring router. We assume that time and energy consumed to perform this transmission per ﬂit clock cycles, and  joules, respectively. Typically,  clock cycle. Hence, the shortest time (  ) for transferring a packet of  ﬂits over  router hops is given by:    . Since we assume wormhole switching, the ﬁrst term (in square braces) denotes the time required for transmitting the header ﬂit from source to destination. After, the header ﬂit reaches the destination, each successive ﬂit of the packet arrives every  clock cycles. Similarly, the energy consumption (  ) for transferring a packet of  ﬂits over  router hops is given by:   . The ﬁrst term refers to the energy consumed for obtaining crossbar access, and the second term refers to the energy required for actual transmission of ﬂits. Guaranteed throughput NoC synthesis is depicted in Figure 2. The input to the synthesis problem is the computation   ! #""$ '(     % &  489            architecture speciﬁcation, characterized library of interconnection network elements, and performance constraints. The computation architecture consists of processing and memory elements shown by rectangular blocks labeled “P/M” in the top of the ﬁgure. The directed edges between two blocks represent the communication traces. The communication traces are annotated as “C m(N,P,D)” where “ m ” represents the trace identiﬁer , “N” denotes the packet size in ﬂits, “P” and “D” denote the period and deadline constraints in clock cycles, respectively. In the example, the library consists of only one interconnection router architecture speciﬁcation. The router speciﬁcation consists of i) number of ports, ii)  and  , and  and  iii)  . The output of the communication architecture synthesis problem is a topology of the network, a mapping or static routing of the communication traces on the network, and a schedule for packet transmission at every port, such that the performance constraints are satisﬁed, and the energy consumption and area are minimized. The topology of the network speciﬁes the number of routers, and their interconnections. The static routing of a communication trace is shown by the annotation of physical links in the bottom half of Figure 2. A periodic schedule for transmission of traces at the input port of router “R2” is also shown in the ﬁgure. After the NoC architecture has been synthesized, elimination of possible deadlocks between the communication traces can be achieved by introduction of additional virtual channels in the routers as a post-processing step [3]. In the following section we deﬁne the guaranteed throughput NoC synthesis problem.  , , DE!7 ) ,'.0/ rected edge 798+:;132</<1>=3 ?@4 to 1>= . A:>1 /B1 ?C4 F-!7 A. Problem Deﬁnition Given: ) A directed communication trace graph ( *+-, where each 132546. denotes either a processing element or a memory unit (henceforth called a node), and the didenotes a communication trace from 132 ) For every 7  denotes the number of ﬂits,  denotes the period requirement in clock cycles, and GH'7  denotes the deadline constraint in clock cycles. ) A characterized library I of different router architectures. For each router architecture J denotes the number of input/output ports of the router,  and   denote time (in clock cycles) and energy (in joules) for obtaining and  the crossbar access, and  denote time (in clock cycles) and energy (in joules) for switching the ﬂits between routers. In the above formulation, without loss of generality, we assume that the size or width of ﬂits is deﬁned by the computation architecture. Let K denote the set of routers utilized in the synthesized architecture, 5L represent the set of links between two routers, and 5M represent the set of links between routers and nodes. The objective of the NoC synthesis problem is to obtain a network topology NKO/P.0/  where: ) For every 7 , there exists a unique path in  , ) For every input port of T that has (one or more)  mapped to it there exists a periodic schedule, Q!1 /U!T /<T N:S!1 /BT such that, F-' 798  , and GH!798  are satisﬁed, and a) The total energy consumption is minimized, and b) total area is minimized. In this paper, we present SAGA, a genetic algorithm (GA) based technique for synthesizing guaranteed throughput NoC architectures. The energy consumption of the NoC is optimized by minimizing the cumulative trafﬁc ﬂo wing through the ports of all routers. The total area consumption is be minimized by reducing the total number of routers used. The paper is organized as follows: Section II discusses the previous work, Section III presents the genetic algorithm, Section IV presents the experimental results, and ﬁnally Section V concludes the paper. I I . PREV IOU S WORK Pinto et al. [4] presented a technique for constraint driven communication architecture synthesis of point to point links by utilizing deterministic heuristic based k-way merging. Their technique results in network topologies that have only two routers between each source and sink. Hence, their problem formulation does not address routing. Hu et al. [5] presented a branch and bound technique to map IPs onto a regular mesh based NoC architecture. In [6], the authors extended [5] to incorporate a deadlock free deterministic routing function. In both papers the authors assume that an NoC architecture already exists, and has a mesh topology. Our work on the other hand addresses design of an application speciﬁc NoC, and does not assume an existing interconnection network architecture. We synthesize a custom NoC architecture, and map or route the communication traces on the topology such that the performance constraints are satisﬁed and the communication power consumption and area of the NoC are minimized. In [7], the authors presented an algorithm that schedules both computation and communication transactions onto mesh based NoC architectures under real time constraints. The authors address the problem of scheduling tasks on different cores that are interconnected in an NoC with regular mesh topology. We solve the problem of synthesis of a custom NoC topology, mapping of cores on the topology, and scheduling of trafﬁc on the ports to minimize the jitter (guaranteed throughput) in the trafﬁc. I I I . SAGA : SYNTHE S I S O F GUARANT E ED THROUGH PUT NOC ARCH IT ECTURE S In this section, we present SAGA, our GA based technique for the synthesis of guaranteed throughput NoC architectures. The four key issues that must be addressed for application of GA for an optimization problem are representation of the solution, selection of the ﬁtness function, overall technique for evolution, and deﬁnition of genetic operators. In the following section we ﬁrst give an overview of the GA, and then address each of the four issues in detail. A. Overview of GA A GA is based on the biological phenomenon of genetic evolution. The GA maintains a set of solutions known as the population or a generation. GA operates in an iterative manner 5M /B1 /;VWVWV>T /<1 4XK  490     8 2 =  8 8 8    L / 8 2 =  4  R 2 2  2 =  8 =  ? 2 7 8 4 ) ) and evolves a new generation from the current generation by application of genetic operators. The size of the population is maintained constant across generations. A new generation is created by ﬁrst increasing the population by creating new individual solutions, and then selecting a constant number of solutions based on their ﬁtness criteria. The ﬁtness criteria is a cost function that captures the optimization goal. The selection of solutions based on their ﬁtness criteria models the evolutionary behavior known as the survival of the ﬁttest. A GA based technique typically applies three operators, namely reproduction, crossover, and mutation to produce new members. The algorithm continues to operate in an iterative manner till the termination condition is reached. On termination the ﬁttest member of the ﬁnal generation is declared as the solution of the problem. b 2 3 a 0 c 4 1 d 5 e 6 Communication Trace Graph R1 R2 R3 R4 R5 Level I: Router Allocation 3 4 5 5 4 R2 6 7 3 9 2 8 R3 10 11 Level II: Node Mapping  3 9 3 2 8 R3 10 11 5 4 5 4 R2 6 7 1 0 1 0 6 0 R1 2 1 6 0 R1 2 1 B. Representation of the solution The synthesis problem involves the generation of both the structure of the NoC, and speciﬁcation of node and trafﬁc mapping on the router elements. For a particular number of routers, there are several ways in which a node maybe mapped to the router ports. Similarly, for a particular router allocation and node mapping, there are several ways in which trafﬁc may be routed or mapped through the network. Hence, SAGA models the population in a hierarchical manner consisting of three levels as shown in Figure 3. At the ﬁrst level, SAGA maintains Y different architectures with various number of routers in each architecture. At the second level, for each router speciﬁcation at level 1, SAGA saves Z different node to port mappings. Finally, at the third level, SAGA maintains [ different routes of the communication traces on the ports of the routers for every node to port mapping at level 2. Hence, the total number of solutions that SAGA maintains in a given population is given by \]^Y . Note that SAGA does not explicitly model the physical links between the routers. Rather, the physical links are derived from the routing or mapping of the communication traces. For the remainder of the paper we will refer to the 3 levels as router level or level 1, node level or level 2, and trace level or level 3, respectively. At the ﬁrst level, number of routers in a solution is represented by a binary array arstr[ _+`badc ] where _+`b adc   and there are _f`badchg routers of each type. We denote each router by T>2 with ikjmlb n _+` aoc . Every router that can be possibly utilized in the architecture is assigned a random location in the array given by loc( T;2 ). Finally, every port of the _ routers is assigned a unique number. SAGA maintains Y instances of array arstr at the ﬁrst level. arstr is a binary array where a “1” in location “i” denotes that the router assigned to arstr[i] is possibly utilized in the architecture, and a “0” denotes otherwise. As mentioned earlier, for each instance of arstr array, SAGA maintains Z instances of node to port mapping. The node to port mapping at second level is stored in an integer array given by npstr[ e e ]. Location l contains the unique port number (described in the previous paragraph) to which node 1 is assigned. A legal node to port mapping must satisfy the following two conditions: (i) a port is assigned to one and `aoc 46. Fig. 3. Hierarchical representation Level III:Trace Mapping Router architectures G(V,E) Generate initial population Set exit criterion to FALSE Output  topology, energy and area YES Exit  Criterion Satisfied? NO Perform architecture level crossover and mutation with probability Pa Retain best architecture strings. Reject remaining strings Perform node level crossover and mutation with probability Pn Retain best node level strings. Reject remaining strings Perform trace level crossover and mutation with probability Pt Retain best trace strings. Reject remaining strings Fig. 4. Overview of SAGA only one location in npstr, and (ii) all ports that are in npstr belong to a router T>2 that is included in the particular solution. That is, arstr[loc !T;2  ] = 1. SAGA maintains [ instances of communication trace mappings for every instance of npstr array. The communication trace mapping is represented by a set of integer arrays pqT3r;pqT;2 , where the l&s!t array refers to the communication mapping of trace l . Each pqT3rWpqT array is an ordered set of ports indicating the ﬂo w of the trafﬁc. A legal communication trace mapping has the following properties: (i) the array starts at position 0 with the port to which the source of the trace is mapped, followed by different port of the same router. The array ends with an integer that represents a port mapped to the sink node. (ii) Port numbers at even (odd) positions denote input (output) ports. (iii) An input port of a router is always followed by an output port of the same router. (iv) A port number in the  491  Z  [ e I e  	   e . e  e  e e I e . 2 EDF scheduler(topology, trace mapping) begin ^v begin 1 Initialize() 2 while ready list u 3 4 : trace,port ? = get most urgent() 5 schedule trace on port(trace,port) 6 remove : trace,port ? from ready list 7 earliest next schedule time 8 update urgency 8 update ready list 9 end while pwn  "" basis of EDF heuristic. The selected trace is then scheduled at the earliest possible time on the corresponding port by updating the schedule table. Once a trace is scheduled on a port, it advances to the next node in its dependence graph. The global time is advanced to the earliest time that a trace can be scheduled on any port, and the ready list is updated. After scheduling it is possible that the period and deadline constraints of some traces are not satisﬁed. A trace with the largest deadline miss is unmapped, and a new schedule is generated. The procedure is repeated until all mapped traces satisfy the performance constraints. 0 4 1 R1 3 R1 7 5 2 A 6 B So AB,2 AB,5 Si Fig. 6. Graph representation for EDF end Fig. 5. EDF Scheduler C. Fitness calculation array is not repeated. (v) If a pair of ports adjacent to each other represent ports of two different routers, they should be consistent over all pqT3 r;pqT arrays. The pair represents two ports of different routers connected together. This constraint forbids a port from being connected to multiple ports. 1) Generation of initial population: The initial router allocation is generated by randomly assigning “0” and “1” to all locations of every instance of arstr array. The node to port mapping is obtained by randomly selecting a node of the *+- , and mapping it to a randomly selected port subject to the legality criterion described in the previous section. The generation of an initial communication trace mapping is more involved. Initially, a trace is selected at random, and a modiﬁed shortest path algorithm (MSP) is applied to generate the trafﬁc mapping from the source to the sink node, respectively. As mentioned earlier, a trace mapping is denoted by an array of port numbers. Every trace that is mapped through ports of multiple routers establishes a physical link between two consecutive ports of different routers. MSP differs from classic shortest path since it maps the trafﬁc along the shortest route subject to the links established by already mapped traces. The distance between source and sink nodes of a communication trace is given as the minimum number of routers a trafﬁc must go through before it can reach the destination, subject to the previously mapped traces. It is possible that MSP is unable to ﬁnd a path from the source to sink. In such a case, the trafﬁc trace is left un-mapped, and a penalty is accrued as described in the following section. Finally, an earliest deadline ﬁrst (EDF) scheduling algorithm is applied on the mapped traces at every port of the router to generate the periodic schedule. The schedule denotes the priority of transmission to next hop. Our algorithm utilizes a list scheduling based technique with EDF priority function (see Figure 5). Let the pair : tr,p ? denote that the trafﬁc trace “tr” enters port “p”. In the initialization step, a dependence graph is generated for each trafﬁc based upon its mapping. A dependence graph is shown in Figure 6. In the ﬁgure, a trafﬁc “ AB” originates from a source, enters port 2, and then enters port 5, and ﬁnally sinks at its destination. The corresponding graph is generated by denoting each node in the graph by its : trace, port ? pair. The ready list is updated to contain : trace, port ? pairs of all trafﬁcs and their corresponding source port. Selection of a trace from the ready list is done on the  492 lypqz7{r{rS' r ,  -}~<L `6   }~BL `]  We utilize a weighted linear combination of normalized energy, normalized area consumption, and unmapped traces of a solution r to determine its ﬁtness: z `baBW< where | , and  are weights. The normalized energy is calculated as the ratio of the energy consumption of the solution, and the maximum possible energy consumption of the network due to designer speciﬁed deadline constraints. We calculate the maximum possible energy consumption by assuming that all the trafﬁc traces are routed via their maximum possible hops. Similarly, the normalized area is calculated as the ratio of the number of routers utilized in the solution to the maximum possible number of routers, _+`b adc . D. Overview of SAGA a+ ,'.0/ Figure 4 shows the top level ﬂo wchart of SAGA. The input to SAGA is the library of router architectures I , and the *+-,  . An initial population of solutions is generated by utilizing the algorithms described in Section III-B.1, and the ﬁtness of each solution is calculated. Initially, the exit criterion is set to false. SAGA applies genetic operators at the three levels of solution hierarchy with different probabilities. If the probability of application of genetic operator at the router deﬁnition level is R a , at the node mapping level is R , and trafﬁc mapping level is R , then R . At each level the number of solutions produced by crossover ( z ) is much larger than those produced by mutation ( z z ). At each hierarchical level new individual solutions are produced by the application of the genetic operators. A new generation is produced by selection of the  ﬁttest members among the current generation and the new individual solutions, where for level 1,  for level 2, and  for level 3. Selection of members of current generation for the next generation models the reproduction operation. At all three levels of the hierarchy, the ﬁtness is given by the strongest complete solution at the corresponding trace level. Termination condition is reached if the  successive generations do not result in the improvement of the ﬁtness of the strongest solution. We set  to summation of the number of nodes and edges in the *+f , , that is  Z #[ #Y /<z `A e . x    |   }  } s R }  R s `  e . e  e  shown in Table I. The communication trace graphs for the benchmarks were obtained from the work presented by Hu et. al. [5]. Video processing requires more bandwidth than audio processing. Moreover, as the encoding process requires more processing than the decoding process, the H.263 encoder has a much higher bandwidth requirement, compared to H.263 decoder. Similarly, mp3 encoder has much higher bandwidth requirement compared to mp3 decoder. Among the four traces, mp3 decoder has the minimum bandwidth requirement, H.263 decoder and mp3 encoder have comparable bandwidth requirement, and H.263 encoder has the maximum bandwidth requirement. Library of components : We experimented with 4-port routers that were characterized for technology as follows [8]: clock cycle, clock cycles, , and  . Experimentation set-up : The results of SAGA were compared with the respective solutions generated by an optimal MILP formulation. The MILP solver was con ﬁgured to output the optimal solution, or the best solution available after 12 hours, whichever came ﬁrst. The MILP formulation does not perform scheduling of communication transactions. Incorporating scheduling in the MILP causes its run-time to explode even for small task graphs. When deadline is equal to period, scheduling can be avoided and a bandwidth capacity constraint on each port can be utilized for mapping the traces. For the purposes of this experiment we assume that the maximum bandwidth at a port is one ﬂit/ clock cycles. We obtained results by varying the relative weights given to the constants | , and  to generate solutions that were i) highly power efﬁcient  kmiE V >i3z 	 mE V ( | L~B , ZS L~B and [ E. Genetic operators Crossover operation The crossover operation considers two equal sized arrays that denote solutions (for example A and B), cuts them at same location (A = A’A”, B = B’B”), and exchanges and appends the two sub-arrays (A’B ”, A”B’) to generate two new solutions. The router level, node level and trace level crossover operations are applied on Y , Z , and [ LP~P new individumappings to generate YW als respectively. Once the crossover strings are produced, lower level strings are generated for each new string. Attempt is made to maintain the locality of the two parents in the children by duplicating as many lower level strings as possible subject to the legality criteria. Mutation operation The router level mutation operation is applied to every set of Y router allocations to generate Yo` new individuals. Router level mutation is applied by selection of a random location in the array, and inversion of the corresponding bit. If a ‘0’ is inverted to ’1’ a router is added and no change is applied to the lower levels. On the other hand, if a ‘1’ is inverted to ‘0’ a router is removed. Hence, all node level mappings that contain any ports belonging to the removed router and associated solutions are re-generated similar to the initial population creation. The node level mutation operation is applied to every set of Z mappings to generate Z  new individuals. Two ports , 1 ) in a particular router allocation are selected at random. A node mapping for the same router allocation is also selected at random. If the node mapping has two nodes assigned to and 1 respectively, their mappings are exchanged. If the node mapping has only one node mapped to a port, say  , its mapping is changed to 1 . If the node mapping has no nodes assigned to either  or 1 , the process is repeated by selection of two new ports. Once a new node mapping is generated, all communication traces associated with the moved nodes are added to the set of unmapped traces in all trace mappings. The MSP and EDF algorithms are invoked to generate the complete solutions. The trace level mutation operation is applied to every set of [ trace level strings to generate [`  new individuals. The trace level mutation operator selects a trace level string at random from the [ strings, and adds it to the set of unmapped traces. Next, it proceeds to randomly select an unmapped trace and maps it to the architecture by invocation of the MSP algorithm. The process continues till as many unmapped traces are mapped as possible subject to the legal mapping constraints. Finally, the EDF scheduler is invoked to generate the complete solution. ` (  IV. RE SULT S Benchmarks : We present the results obtained by execution of SAGA on four benchmarks namely, i) mp3 audio encoder, ii) mp3 audio decoder, iii) H.263 video encoder, and iv) H.263 video decoder algorithms. We obtained three other benchmarks by combinations of two applications. Finally, we generated a large 40 node benchmark running mp3 audio encoder, mp3 audio decoder, H.263 video encoder, and H.263 video decoder algorithms simultaneously. The benchmarks are s a s  s a s  s a s      R Z R Z   Graph Graph ID Nodes Edges mp3 encoder 263 encoder 263 decoder 263 enc mp3 dec 263 enc mp3 enc mp3 enc mp3 dec 40 node graph G1 G2 G3 G4 G5 G6 G7 8 7 9 12 15 13 40 9 8 8 11 17 12 32 TABLE I G RA P H CHA RAC T ER I S T IC S Graph G1 G2 G3 G4 G5 G6 G7 ) ¬d­ ¬o® Energy ( ªW« 0.619 0.7 15.46 21.8 0.750 1.13 14.86 17.22 15.25 26.45 0.33 0.33 40.74 41.35 ¬d® 3 3 4 5 8 5 19 Routers ¬o® 3 3 4 5 7 5 19 Runtime (sec) ¬d­ 442 376 288 1500 1435 900 10997 ¬d® 420 378 360 890 1440 987 10898 Graph G1 G2 G3 G4 G5 G6 G7 Energy ( ªW« ) MILP SAGA 0.65385 0.660 15.4737 15.47 0.77355 0.773 15.911 14.61 17.5257 18.42 1.07235 1.028 NA 39.17 App. 1.010 1 1 0.91 1.05 0.95 NA Routers MILP SAGA 3 3 4 5 6 5 NA 3 3 4 5 7 5 19 App. 1 1 1 1 1.166 1 NA Runtime (sec) MILP SAGA 117 78 118 43200 43200 43200 43200 428 361 324 901 1441 876 10800 App. 3.65 4.620 2.747 0.021 0.034 0.021 0.25 TABLE III COM PA R I S ON O F M IL P, AND ¯E°²±³°³´ App. Graph F O R D EAD L I N E = P ER I OD App. Runtime (sec) MILP SAGA 1 1 1 1 1 1 NA 117 78 118 43200 43200 43200 43200 431 366 341 1500 1467 860 10870 App. 3.68 4.69 2.89 0.03 0.03 0.02 0.25 F O R D EAD L I N E = P ER I OD Energy ( ªW« ) MILP SAGA 0.65385 0.660 15.473 15.47 0.77355 0.773 15.911 14.61 17.5257 18.42 1.07235 1.028 NA 39.17 Routers MILP SAGA 1.01 1 1 0.92 1.05 0.96 NA 3 3 4 5 6 5 NA 3 3 4 5 6 5 19 G1 G2 G3 G4 G5 G6 G7 TABLE II R E S U LT S F O R D EAD L I N E = P ER I OD / 1 0 TABLE IV COM PA R I S ON O F M IL P, AND ¯E°²±³°²µ 5 presents the area consumption (in number of routers) of the MILP, column 6 presents the area consumption of SAGA, column 7 presents a comparison of the two techniques in terms of area, column 8 presents the run time of the MILP technique, column 9 presents the run time of SAGA, and ﬁnally , column 10 presents the comparison between the two techniques in terms of run time. The comparison of power consumption, area, and run time are done by taking the ratio of the respective values of the SAGA and the MILP solution. An “N A” in a MILP or approximation column in the two tables implies that MILP was unable to generate a solution within 12 hours (43200 seconds). A timeout MILP solution (rows 4 to 7) is indicated by the maximum value (43200 seconds) in the MILP run-time column. The results are summarized in Table V. Both "
2005,Time and energy efficient mapping of embedded applications onto NoCs.,,"Time and Energy Efficient Mapping of Embedded Applications onto NoCs  César Marcon, André Borin, Altamiro Susin, Luigi Carro, Flávio Wagner  Instituto de Informática – UFRGS – Av. Bento Gonçalves, 9500, Porto Alegre, RS – Brazil  {marcon, borin, flavio}@inf.ufrgs.br, {susin, carro}@eletro.ufrgs.br  Abstract - This work analyzes the mapping of applications onto  generic regular Networks-on-Chip (NoCs). Cores must be placed  considering communication requirements, so as to minimize the  overall application execution time and energy consumption. We  expand previous mapping strategies by taking into consideration  the dynamic behavior of the target application and thus potential  contentions  in  the  intercommunication of  the  cores.  Experimental results for a suite of 22 benchmarks and various  NoC sizes show that a 42% average reduction in the execution  time of the mapped application can be obtained, together with a  21% average reduction in the total energy consumption for stateof-the-art technologies.  1. Introduction  New technologies allow many millions of transistors  integrated onto a single chip and thus the implementation of  complex  systems-on-chip  (SoC)  that  need  special  communication resources  to handle very  tight design  requirements. In addition, deep sub-micron effects pose  formidable physical design challenges for long wires and  global on-chip communication. Many designers propose to  change the full synchronous design paradigm to a global  asynchronous and  local synchronous  (GALS) design  paradigm [1]. GALS design subdivides the application into  sub-applications. Each sub-application is a synchronous  design physically placed inside a tile, and the communication  between tiles is provided by an asynchronous communication  resource. A network-on-chip (NoC) is an infrastructure  essentially composed by a set of routers interconnected by  communication channels. A NoC is suitable to deal with the  GALS  paradigm,  since  it  provides  asynchronous  communication, high scalability, reusability, reliability, and  efficient energy consumption [2].  An application composed by a set of existing cores, such  as  processors  and memories  together with  their  communication channels, must be mapped onto a physical  network structure. To fulfill this goal, many mapping  strategies have been proposed, which look for an ideal  placement of the cores. For instance, in [3] and [4] a model  based on a weighted graph reflecting the communication  capacity of each channel is used. However, previously  published approaches tend to overestimate the channel  occupation, thus requiring extra bandwidth to ensure that all  communications are performed within the allocated time. The  overall effect is a major increase in the energy consumption.  In addition, models like the ones presented in [3] and [4],  which are based on weighted graph, are appropriated to model  applications where the communication need is estimated in  advance and not during the application execution. Hence, a  conservative approach must be taken by the designer  regarding bandwidth requirements, increasing the energy  consumption of the NoC.  In this paper we introduce a new model, called CDM,  which captures the dynamic behavior of the messages of an  application. This new model allows a CAD tool to take into  account the varying necessity of bandwidth along the  execution of an application and hence helps reduce the total  energy consumption of the system. Comparing our approach  with previous published work, we achieve an average  reduction of 42% in application execution time, at the same  time reducing the total energy consumption of the system by  21% for state-of-the-art technologies, for a suite of 22  benchmarks and various NoC sizes.  The remaining of this paper is organized as follows.  Section 2 discusses previous work related to the applicationmapping problem. Section 3 describes our target architecture.  Section 4 explains the mapping strategy to reduce the  application execution time and the energy consumption on the  target architecture. Section 5 presents experimental results,  and Section 6 draws final conclusions.  2. Related Work  Hu and Marculescu [3] propose a mapping approach  called communication weighted model (CWM), based on an  application characterization graph (APCG), where the weight  of a channel corresponds to the bit volume of the messages  transmitted over this channel. With this model, they show that  it is possible to reduce the energy consumption by more than  60%, when compared to ad-hoc mapping solutions.  Murali and De Micheli [4] implement a similar solution.  Their CWM is also characterized by an application graph,  which they call core graph. Their algorithm maps cores onto a  mesh NoC architecture under bandwidth constraints, with the  goal of minimizing the average communication delay.  Ye, Benini and De Micheli [5] propose a model to  evaluate  the energy consumption  in a communication  infrastructure considering switches, internal buffers, and  interconnect wires. The same authors, in [6], describe the  contention problem in NoCs and the associated performance  reduction. They recommend a solution employing a routing  algorithm that minimizes the energy consumption, because  the required buffers in the network are reduced.  In all approaches that use the CWM strategy, essential  information regarding the exact time instant at which  messages are exchanged is lost.  We have developed experimental work that shows that for  embedded applications and  random benchmarks  this  information cannot be neglected. By not considering the  varying nature of the communication bandwidth requirements  along the execution of an application, the mapping algorithm  can produce solutions that require in average 40% more   33   execution time than a minimal solution. On the other hand, by  introducing the communication bandwidth variability in the  model, our tool can reduce both the application execution  time and the energy consumption.  Our approach is based on a communication dependence  model (CDM), where the application graph transports the  knowledge of dependences between messages. The placement  of the cores onto the NoC is based on this extra information,  relating the amount of bits to be transmitted with the moment  when the communication must take place. Moreover, the  energy model presented in [5] is extended to consider static  energy consumption, which is very relevant in new submicron technologies. This work assumes that application tasks  are previously partitioned and assigned into a set of cores.  3. Target Architecture Description  Mapping approaches such as CDM and CWM are useful  for all communication infrastructures where mappings may  affect the overall performance, like hierarchical busses and  NoCs. This paper approaches only NoCs as  target  architecture, with a 2D-mesh topology and composed by ϕ ×  ω tiles. Figure 1 depicts that each tile τ contains a router r and  a core c.  ω  r1,1  r1,2  c1,1  τ1  c1,2  τϕ+1  c1,ω  r2,1  r2,2  c2,1  τ2  c2,2  τϕ+2  c2,ω  r1,ω  τ (ω-1).ϕ+1  r2,ω  τ (ω-1).ϕ+2  cϕ,1  τϕ  cϕ,2  τ2.ϕ  cϕ,ω  τω.ϕ  rϕ,1  rϕ,2  ϕ rϕ,ω  Figure 1 – Schematic of the target architecture  Tile-based architectures require the implementation of  routing algorithms to transmit packets across the network.  Routing algorithms can be divided  into  two classes:  deterministic and adaptive. Deterministic routing algorithms  completely specify the path from the position of the source  tile to the position of the target tile. In adaptive routing  algorithms, the possible paths depend on the network traffic.  Adaptive routing algorithms increase the number of possible  paths and require more resources because of their complexity.  Therefore, we decided to choose the deterministic XY routing  algorithm, which is free from deadlocks and livelocks and still  route with minimal path [7]. Other works also use the same  routing policy, like in [3] and [4], so that the comparison with  our work may be based on the same ground rules. The  algorithm behavior can be summarized in two steps: (i) first,  packets are routed along the X-axis until they reach the target  tile column; (ii) packets are then routed along the Y-axis until  they reach the target tile row.  4. Problem Formulation  The problem of mapping application cores onto NoCs is a  complex one. The designer splits the application tasks into  cores. Each core has given computation and communication  requirements, which can be obtained through simulation and  profiling techniques. To better understand the mapping  application problem, we present three definitions.  Definition 1: A communication weighted graph is a directed  graph CWG = <C, M>, where C = {c1, c2, …, cn} represents  the set of application cores, corresponding to the set of CWG  vertices. Let Wij be the weight that corresponds to the bit  volume of all messages exchanged between cores ci and cj,  then the set M = {(ci, cj, Wij) | ci, cj ∈ C} symbolizes the traffic  volume of all messages between all application cores. CWG  has an equivalent definition to APCG [3] and core graph [4].  Definition 2: A communication dependence graph is a  directed graph CDG = <V, D>. Let vq = (ca, cb, wab) be the q-th  message from core ca to core cb with bit volume wab.  V = {v1, v2, …, vk} denotes the set of all messages between all  application cores and correspond to the set of CDG vertices,  and D = {(vi, vj) | vi, vj ∈ V} represents the set of message  dependences, corresponding to the set of CDG edges. Edges  are non-valued, and the edge direction means message  dependence.  Definition 3: A communication resource graph is a directed  graph CRG = <Γ, L>, where Γ = {τ1, τ2, …, τp} denotes the  set of tiles, corresponding to the set of CRG vertices, and  L = {(τi, τj) | τi, τj ∈ Γ} designates the set of routing paths  between tiles, corresponding to the set of CRG edges. p is the  total number of tiles and is equal to ϕ × ω (Figure 1). CRG  has  an  equivalent  definition  to  the  architecture  characterization graph [3] and to the NoC topology graph [4].  The CDM approach implies the extraction of message  dependence from the application cores. This means that all  messages are relatively ordered by their dependences. On the  other hand,  the CWM approach does not  take  the  communication ordering into consideration. Only the volume  of bits exchanged between cores is considered in the CWG  [3][4]. As a consequence, CWM cannot prevent contentions,  thus disabling the precise estimation of the application  execution  time, and require a conservative approach,  increasing bandwidth  requirements and hence energy  consumption.  A → D  35  Start  D → A  30  A → B  40  C → D  25  C → A  45  A → C  50  End  (a)  A  40  B  30  45  35  D  B  D  50  25  C  A  C  (b) (c) Figure 2 – CDG (a), CWG (b) and CRG (c) examples  To a better understanding of these concepts, Figure 2  depicts the CDG of a hypothetical example, where four cores  C = {A, B, C, D} exchange six messages with communication  rates between 25 and 50 units. Figure 2(a) shows the CDG,  which highlights the message interdependences. Figure 2(b)   34         shows the equivalent CWG, and Figure 2(c) portrays a CRG  where C is arbitrarily mapped onto a 2x2 NoC. Since the set  of CDG vertices contains information of all messages and  cores, CWG can be obtained from CDG.  4.1 Energy Model  The energy consumption of the application is originated  from both cores and network operation. This work focuses  only on NoC energy consumption and presents a model to  estimate dynamic and static energy consumption. This energy  model is used as an objective function to evaluate the cost of  each mapping. It is important to notice that the consumption  of the cores is independent of the mapping, as this is the same  assumption also found in [3][4].  Static energy consumption is mainly originated from  subthreshold leakage current and is proportional to application  execution time and the number of gates. Usually, static energy  contributes with the smallest part of total energy consumption.  However, for sub-micron technologies, the leakage current  cannot be neglected, and the static energy becomes a  meaningful part of total energy consumption, reaching up to  20% in state-of-the-art technologies [8]. Dynamic energy  consumption is proportional to switching activity, which  happens when packets move across the NoC dissipating  energy inside each router and on the router interconnection  wires.  Our energy model computes static and dynamic power  dissipation to estimate the total energy consumption of the  NoC. This work uses an approach similar to the one presented  in [3] and [4] and extends the concepts to static energy  consumption. We use the same concept of bit energy Ebit to  estimate the dynamic energy consumption for each bit when  the bit flips its polarity from the previous value. Ebit is split  onto dynamic energy EWbit consumed on the switch wires,  dynamic energy EBbit consumed on the buffers, dynamic  energy ESbit consumed on the logic gates of each switch, and  dynamic energy ELbit consumed on the links between tiles, as  described in equation 1.  (1)  Ebit = EWbit + EBbit + ESbit + ELbit  EBbit, ESbit and EWbit model the total energy consumed by a  bit passing through a router. EBbit depends on the buffer size  and technology to estimate how many bit flips occur to write,  read, and preserve the information. When technology and  routing policy are defined, EBbit and ESbit can be estimated by  electrical simulation. For regular mesh NoCs, with square  dimension tiles, it is reasonable to estimate that ELbit is the  same for all NoC interconnections. While ELbit is directly  proportional to tile dimension, EWbit becomes negligible for  large tiles, since EWbit does not depend on the increase of tile  size. This makes equation 2 a reasonable estimation for bit  dynamic energy consumption.  (2)  Ebit = EBbit + ESbit + ELbit  Equation 3 computes the dynamic energy consumed on the  NoC by a bit traffic from core ci to cj, where η corresponds to  the number of routers that the bit goes through.   35 (3)  , jcic bit E  = η (ESbit + EBbit) + (η - 1) ELbit  Let λq be the bit volume of each message vq ∈ V. Then,  qv , jcic bit k (4)  bitE = λq ×  E  | (ci, cj) ∈ vq.  Equation 4 gives the total amount of NoC dynamic energy  consumption EDyNoC, which considers all bit flips during the  transmission of messages across the NoC.  EDyNoC = ∑ The static power consumption of each router PSRouter is  proportional to the number of powered elements, with a very  small influence of switching activity. With p representing the  number of tiles, equation 5 computes the NoC static power  consumption PStNoC.  (5)  PStNoC = p × PSRouter  qv bitE q = 0 Static energy consumption is proportional to the total  number of gates dissipating static power and to the execution  time texec. Thus, equation 6 computes NoC static energy  consumption EStNoC.  (6)  EStNoC = PStNoC × texec  Finally, equation 7 gives  the  total NoC energy  consumption ENoC, which computes the consumption of static  and dynamic energies.  (7)  ENoC = EStNoC + EDyNoC  With the objective of inserting the energy parameters into  the CDM and CWM approaches, a NoC was described and  synthesized to a 0.35micron TSMC ASIC standard cell  library. The synthesis result is a netlist of cells. The  manufacturer supplies energy values for the standard cell  library, allowing the extraction of ESbit, EBbit, ELbit, and PSRouter  parameters. These parameters are  independent  from  application and NoC dimension. On the other hand, p and texec  are application-dependent parameters. The execution time texec  is measured in clock cycles, considering a 100 MHz operation  frequency, and p is greater or equal to the number of cores in  the application.  4.2 Comparing Communication Algorithms  As both communication weighted algorithms (CWAs) and  communication dependence algorithms (CDAs) implement  solutions for NP-complete problems [3][4], we have used a  simulated annealing search method to reach the best mapping  solutions in both cases. Moreover, we have also implemented  exhaustive search methods, so that we could compare the  quality of the solution.  For both modeling approaches the algorithms start from an  initial mapping, evaluate the mapping cost, and search for a  new mapping that reduces the previous cost until reaching a  stop condition. CRG edges and vertices  represent  communication resources: links and routers, respectively. For  both algorithms, cost variables are associated to each CRG    edge and vertex to store the corresponding part of the  mapping cost. The mapping objective function is defined as  the sum of all cost variables of CRG edges and vertices. The  CWA and CDA objective functions are not the same. While  CWA searches only for mappings that reduce the dynamic  energy consumption, as it is described in equation 4, CDA  also evaluates  static energy consumption, which  is  proportional to execution time, as it is described in equation 7.  As a result, CDA indirectly searches for mappings that reduce  the overall execution time.  The initial mapping of CDA or CWA is selected by  randomly associating application graphs (CWG or CDG) with  CRG; i.e. all cores ∈ C are randomly mapped onto a possible  tile ∈ Γ. To compute the mapping objective function, all cost  variables of CRG edges and vertices are initially reset.  Let tiles τi and τj be mappings of cores ca and cb,  respectively. For CWA, all bits of the communication channel  (ca, cb),  represented by Wab, are associated  to  the  correspondent cost variable of vertices and edges of CRG,  starting from τi, following the XY routing algorithm and  ending in τj. The cost variable of each CRG edge computes  the dynamic energy of a link by multiplying Wab by ELbit, and  the cost variable of each CRG vertex computes the dynamic  energy of a router by multiplying Wab by ESbit + EBbit. The sum  of all cost variables of CRG results in the total dynamic  energy EDyNoC, for a given mapping. The goal of CWA is to  find mappings that reduce EDyNoC. EStNoC is not computed  because this model is inappropriate to capture the time taken  by the whole application.  While CWG considers only the communication volume,  CDM captures the message dependences. Messages that have  producer-consumer precedence can not be concurrent.  However, temporally independent messages can occur at the  same time and may consequently lead to package contention.  To obtain benefits from this time notion, to each edge and  vertex of CRG a cost variable list is associated, where each  list position contains the energy sum of all independent  messages that share the same communication resource. The  algorithm considers the worst case, i.e. all independent  messages that share the same communication resource  produce contention. The message contention implies a larger  application execution time texec and consequently more static  energy dissipation EStNoC. Therefore, CDA minimizes the  probability of contentions by searching core mappings that  spread the messages over parallel links.  The total delay of messages depends on the mapping, on  the bandwidth, and on the number of bits. The algorithm  computes the total delay of messages by adding the message  delay only when messages are dependent from each other or  when independent messages occupy the same communication  resource. With the total delay of messages we apply equation  6 to obtain EStNoC. Similarly to CWA, for CDA all bits of the  message vc = (ca, cb, wab), represented by wab, are associated to  the correspondent vertices and edges of CRG, starting from τi,  following the XY routing algorithm, and ending in τj. The  cost variable list of a CRG edge computes the dynamic energy  of a link, in a given period, by multiplying wab by ELbit. The  cost variable list of a CRG vertex computes the dynamic  energy of a router, in a given time period, by multiplying wab  by ESbit + EBbit. The sum of all cost variables of CRG results in  the total dynamic energy EDyNoC, for a given mapping. CDA  uses equation 7 as an objective function to evaluate the  mapping cost. The goal of the CDM algorithm is to find  mappings that minimize ENoC.  If the mapping cost achieved with a new mapping is  smaller than the one previously stored, the current mapping  and cost are saved for further comparison. Simulated  annealing may accept worse mappings, depending on the  temperature, which is a convergence parameter of the  algorithm. While the stop condition has not yet been reached,  a new mapping is randomly chosen, and the cost is evaluated  again. While simulated annealing considers parameters as  initial  temperature and number of iterations, the stop  condition for an exhaustive search requires the evaluation of  all mappings.  In embedded applications like the graphical ones used in  this work, the number of messages between cores is much  larger than the number of cores. Since each vertex of CDG  represents a message between two cores and each vertex of  CWG represents a core, CDGs are larger than CWGs,  implying more CPU time and more data storage area for the  algorithm execution. A comparison between CDA and CWA  is presented in Section 5.  4.3 Comparing Communication Models  The main advantages of CWM are (i) easy extraction of  the application core graph (CWG), since this can be done by  simulation techniques; (ii) low computational complexity; and  (iii) the accurate estimation of EDyNoC, since dynamic energy  may be well computed by the total bit traffic in the NoC. On  the other hand, the extraction of CDG is hard to be  automatically obtained, since simulation allows the extraction  of the possible message ordering, but not the message  dependences. This implies that CDGs have to be described in  design time by hand, and this is an error prone task. The  greater complexity of CDM directly reflects in the complexity  of the algorithm to deal with it, which increases the  computation time and the memory usage. However, CDM  captures both the bit volume, which allows computing the  value of EDyNoC, and the message ordering, which allows  estimating the instants of time when more than one message  can pass through the same link, and consequently avoiding  such occurrence by a better core mapping. Such approach is  pessimistic, since not all communications that can occur  concurrently will happen concurrently. Even so, the overall  application performance  tends  to  increase,  if potential  contentions were avoided.  The global communication behavior of a certain  application can be expressed as a function of start of  transmission times, bit volume, and transmission rate of each  message. For many applications, the exact determination of  communication needs at design time may not be possible,  since it depends on the specific input data that can only be  available at runtime. These data have an important influence   36 on the bit volume of messages and less or no influence on  other parameters. The order  in which messages are  transmitted is usually not changed, since it depends on the  algorithm executed by the application, which is usually fixed  in embedded systems. Since CDM models the dependency  between messages, a feature not available in CWM, it allows  evaluating the potential for contention among dependent  messages, even without the precise knowledge of the exact bit  volume for each message. This capacity enables to find  mappings  that further reduce energy consumption and  execution time. Simultaneously, it makes CDM less sensitive  to input data variations at runtime, as it will be shown in the  next section.  4.4 Problem Illustration  This section illustrates the application of the CDM and  CWM approaches with the same hypothetical example of  Figure 2, where two mappings imply different execution times  and energy consumptions. It is also shown that CWM is not  suitable to capture such differences, since this model  computes the same energy consumption for both mappings.  Energy consumption = 805 × 10-12J  D  A  90  60  30  225  B  C  40  0  40  160  0  0  90  70  D  B  130  25  40  65  A  C  200 45 50 120 30  75  0  25 (a)  (b)  Figure 3 – Two mappings with energy estimated by CWA  Figure 3 illustrates two mappings of the example shown in  Figure 2. Just for illustration purposes, this example assumes  that ESbit + EBbit = ELbit = 1. 10-12 J. Each vertex and edge of  CRG  is annotated with  its  total amount of energy  consumption. As CWM cannot capture contention problems,  CWA estimates that both mappings consume the same energy  (805 × 10-12 J).  Figure 4 shows the same mappings of Figure 3, now  evaluated with CDM. Each edge and each input link of each  vertex is annotated with its energy at a given slice of time. For  instance, in Figure 4 (a) the tile corresponding to core D is  annotated with 35S1 and 25S1, which means that there are 2  messages with 35 and 25 bits, respectively. These two  messages are concurrent – see Figure 2(a) – and are annotated  in the cost variable list of vertex D. Both use the SOUTH (S)  link of core D (one from A to D and the other from C to D)  increasing the overall execution time of the application and,  consequently, the static energy EStNoC. Just for illustration  purposes, consider t the necessary number of clock cycles to  transfer one bit from one tile to its neighbor tile and  PStNoC = 1 10-12 J/t the power consumed by clock cycle. In this  case, the energy consumption and execution time are 2.7%  and 20% greater, respectively, when comparing mapping (a)  with mapping (b). These differences are only captured with  CDA. We emphasize that when the number of messages and   37 cores of the application increases these differences also  increase, as it will be shown in Section 5. The overhead in  performance requirements of CWA-like approaches is the  cause of extra power dissipation. In the experiments of the  next section we show that the CDM approach can remove this  overhead.  Energy = 955 × 10-12 J  Execution time = 150 t  Energy = 930 × 10-12 J  Execution time = 125 t  D  35S1, 25S1 30L1  0 0 251, 351 301 A  B  C  40S2 D  30L1, 25S1  35E1  40E2  301 351, 402 402 0  251  402  A  C  35L1, 30W1 45S2, 40L2 50L3  452 503 25E1, 25L1 30N1, 45E3 40L2, 50L3 402, 503 251, 452 25L1 45L2, 40W2 50N3 B  25E1  40N2  0  251 25L1 45L2 50N3 (a)  (b)  Figure 4 – Mappings of the Figure 3 estimated by CDM  5. Experimental Results  Table 1 summarizes the characteristics of 22 applications  mapped onto 8 different NoC sizes (NS). There are 4  embedded applications (a distributed Romberg integration [9],  an 8-point Fast Fourier Transform [10], and 2 image  applications for object recognition and image encoding) with  some variations, in a total of 8 embedded applications. The  remaining applications are benchmarks randomly generated  by a proprietary system, which is similar to TGFF [11];  however, our system describes a benchmark by a CDG, which  represents message dependence and bit volume of each  message. The chosen application characteristics are: number  of cores (NC), number of messages between cores (NM), and  total amount of bit traffic during application execution (TBT).  NS is equivalent to the number of CRG vertices, NC  corresponds to the number of CWG vertices, and NM matches  the number of CDG vertices.  Table 1 – NoC dimensions and application characteristics  NS  2 x 2  3 x 2  2 x 4  3 x 3  2 x 5  3 x 4  3 x 2  2 x 4  3 x 3  3 x 4  8 x 8  10 x 10 NC  3; 4; 4  5; 6; 6  7  7; 9; 9  8; 9; 10  11  5  5; 8  8  10; 12  62  93  NM  15; 12; 23  43; 17; 43  33  16; 18; 32  24; 51; 22  62  16  16; 18  31  15; 25  344  415  TBT  213; 450; 23,234  78,817; 174; 49,003  23,235  1,600; 1,860; 43,120  2,215; 23,244; 322,221  123,337  1,600  1,600; 5,930  4,655,025  3,100; 2,578,920  9,799,200  562,565,990  s k r m m a o d n h a c n e b R s i l i t d n e o d d a e c b p p a m E For each application, the best mapping achieved with  CWM is compared to the best mapping achieved with CDM.  Gains obtained with CDM when compared to CWM are  summarized in Table 2. ES represents evaluations obtained by  exhaustive search, while SA symbolizes evaluations obtained  with simulated annealing algorithm. ETR gives the average  execution time reduction, and ECS denotes the average                      energy consumption saving, for a given technology, when the  best mapping obtained with CDM is compared to the best one  obtained with CWM. ECS0.35 column refers to values obtained  from 0.35micron technology, and ECS0.07 column refers to  values obtained by scaling results from 0.35micron to  0.07micron [8].  Table 2 – Average energy consumption saving and execution  time reduction obtained from comparison of CWM and CDM  evaluations  s k r m m a o d n h a c n e b R s i t d n e o d d a e c b p p a m E i l Algorithm  ES / SA  ES / SA  NS  2 x 2  3 x 2  2 x 4  3 x 3  2 x 5  3 x 4  3 x 2  2 x 4  3 x 3  3 x 4  8 x 8  SA  10 x 10  Total average  ETR  32 %  39 %  31 %  43 %  50 %  47 %  39 %  31 %  43 %  47 %  44 %  51 %  42 %  ECS0.07  16 %  17 %  15 %  25 %  27 %  25 %  17 %  15 %  25 %  25 %  22 %  28 %  21 %  ECS0.35  0,51 %  0,54 %  0,48 %  0,8 %  0,86 %  0,8 %  0,54 %  0,48 %  0,8 %  0,8 %  0,7 %  0,89 %  0,67 %  As seen in the ETR column, CDM results, in average, a  reduction of 42% of execution time when compared to CWM.  The ECS0.35 column  illustrates a very small energy  consumption saving, since the static leakage current is not that  important for this technology generation. However, for submicron technologies, where the static dissipation is more  relevant,  there  is a significant  reduction  in energy  consumption (21% in average), as we can see in column  ECS0.07. In addition, Table 2 shows a slight tendency of better  energy consumption savings and execution time reduction  when the NoC size increases. Finally, results obtained with  exhaustive search are very similar to the ones achieved with  simulated annealing. For all small NoCs (up to 3x4 or 2x5),  both algorithms reached the same results. For larger ones (8x8  and 10x10), it is not possible to find optimal mappings with  the exhaustive search within a reasonable computation time.  The mapping cost evaluation of CWA considers mainly  the number of links between cores. At the same time, the  number of messages has higher influence in CDA, because  messages cannot occupy the same link at the same time. This  leads the CWA computational complexity to be proportional  to the number of links (NL) and the CDA computational  complexity to be proportional to the number of messages  (NM). In embedded applications, NM may be much larger  than NL. However, the increase in CPU time with the increase  of the NM/NL ratio is practically linear and has a small slope.  In our experiments, the worst case of CDA took only 15%  more CPU time then CWA.   The main drawback of CDA is associated to the extra  memory to run the algorithm, since for CDA all vertex and  edges of CRG preserve a list of concurrent messages, while  CWA implies the use of only one data element for each vertex  and edge. In our experiments, the worst case of CDA took 26  times more memory than CWA.   38 6. Conclusions  This paper addressed the problem of mapping application  cores onto NoCs. A communication dependence model  (CDM) is introduced and compared to a communication  weighted model (CWM). We conclude that a mapping  algorithm that implements CDM is able to reduce some  application requirements, when compared to a mapping  algorithm that implements CWM. Experimental results show  an average reduction of 42% in the application execution  time. The CDM approach also  reduces  the energy  consumption. For instance, for a 0.07micron technology an  average of 21% in energy savings is achieved. This reduction  is obtained because CDM may avoid or, at least, reduce  message contention, while CWM may not. Moreover, to map  applications where the communication needs of each core are  not known at design time, CDA may also achieve mappings  that reduce the energy consumption and execution time, while  CWA may not. Algorithms that implement CDM present only  a moderate increase in the execution time when compared to  algorithms that implement CWM, with much better mapping  results.  "
2005,Performance driven reliable link design for networks on chips.,"With decreasing feature size of transistors, the interconnect wire delay is becoming a major bottleneck in current systems on chips (SoCs). Another effect of shrinking feature size is that the wires are becoming unreliable as they are increasingly susceptible to various noise sources such as cross-talk, coupling noise, soft errors etc. Increasing importance of wire delay and reliability has lead to a communication centric design approach, networks on chip (NoC), for building complex SoCs. Current NoC communication design methodologies are based on conservative design approaches and consider worst case operating conditions for link design, resulting in large latency penalty for data transmission. In order to substantially decrease the link delay and thereby increase system performance an aggressive design approach is needed. In this work we present Terror, timing error tolerant communication system, for aggressively designing the links of NoCs. In our methodology, instead of avoiding timing errors by a worst-case design, we do aggressive design by tolerating timing errors. Simulation results show large latency savings (up to 35%) for the Terror based system compared to traditional design methodology.","Performance Driven Reliable Link Design for Networks on Chips Rutuparna Ramesh Tamhankar SUN Microsystems Inc Sunnyvale, CA-94085 rutu@sun.com Srinivasan Murali Giovanni De Micheli CSL, Stanford University CSL, Stanford University Stanford, CA-94305, USA Stanford, CA-94305, USA smurali@stanford.edu nanni@stanford.edu Abstract —With decreasing feature size of transistors, the interconnect wire delay is becoming a major bottleneck in current Systems on Chips (SoCs). Another effect of shrinking feature size is that the wires are becoming unreliable as they are increasingly susceptible to various noise sources such as cross-talk, coupling noise, soft errors etc. Increasing importance of wire delay and reliability has lead to a communication centric design approach, Networks on Chip (NoC), for building complex SoCs. Current NoC communication design methodologies are based on conservative design approaches and consider worst case operating conditions for link design, resulting in large latency penalty for data transmission. In order to substantially decrease the link delay and thereby increase system performance an aggressive design approach is needed. In this work we present Terror, timing error tolerant communication system, for aggressively designing the links of NoCs. In our methodology, instead of avoiding timing errors by a worst-case design, we do aggressive design by tolerating timing errors. Simulation results show large latency savings (up to 35%) for the Terror based system compared to traditional design methodology. Keywords: Networks on Chips, Reliability, Performance, Link, Aggressive design I . INT RODUCT ION With continued scaling of transistors the wire delay as a fraction of the total delay is increasing [2]. The delay in crossing a chip diagonally for 50nm technology is around 6 to 10 cycles, with only a small fraction of chip area (0.6% to 1.4 %) being reachable in a single clock cycle [2]. Scaling is accompanied with a decrease in supply voltage and an increase in clock rate. This makes wires unreliable as the effect of various noise sources such as cross-talk, coupling noise, soft errors increase [3], [4], [5]. To effectively design future SoCs, Networks on Chips (NoCs), a communication centric design approach that considers the delay and reliability issues of the wires has been proposed in [1], [6]. As the total system performance increasingly depends upon the communication system, the NoCs need to support high throughput, low latency communication with low power requirements. To effectively tackle the delay of NoC links, the links can be pipelined by adding buffers (i.e. ﬂip- ﬂops) tha t segment the links into stages (Figure 1) [7]. Link pipelining increases the link throughput and reduces the average packet latency for data transmission. The number of cycles needed for a data bit to traverse a pipelined link is equal to the number of pipeline stages in the link. The number of pipeline stages in the link, in turn, depends upon the distance a data bit can travel on the link in one cycle and the length of the link. As the links are becoming increasingly susceptible to various noise sources such as cross-talk, coupling noise, local temperature variations, process variations, etc., the data bit delay on the link in a cycle is becoming unpredictable as the various noise sources introduce signiﬁcant delay variations in the trans mission of a bit. Link design in current NoC design methodoloSender pipeline buffer 1 pipeline buffer 2 . . . pipeline buffer b Receiver Fig. 1. Pipelined link design, with b pipeline stages in the link pipeline buffer changed to pipeline buffer input error control circuit main flip−flop ck delayed flip−flop ckd XOR errq Fig. 2. Proposed Desgin Fig. 3. Basic Idea gies are based on a worst-case design approach that consider all the delay variations that can possibly occur due to the various noise sources and environmental effects and target a safe operation of the system under all conditions. Such a conservative design approach targets timing error-free operation of the system. In such a scheme, the distance traveled by a data bit is signiﬁ cantly lesser than the ideal case when no timing variations occur due to noise sources. Thus, a worst-case design methodology leads to poor system performance as the number of pipeline stages in the link and hence the link latency is much higher compared to the ideal case when no timing errors occur. An aggressive design approach (in which the data bit is assumed to travel the maximum distance possible in one cycle) can result in large reduction in latency for communication. However, in such a system, timing errors can occur when the noise sources introduce delay variations in data communication. As an example, let us consider an on-chip link that connects all the communicating blocks in an SoC designed in 100nm technology. Assuming a conservative design approach, the number of pipeline stages (and buffers) on the link is 6 (detailed explanation is given in section VIII). On the other hand, if the link is designed aggressively, the distance between successive pipeline buffers can be increased up to 50%, resulting in a 4-stage pipeline. Thus, a 33% reduction in latency is obtained by the aggressive design approach. But, in this aggressive design, timing errors can occur due to delay variations introduced by the environment and wire characteristics. Traditionally, such timing errors are detected by adding redundant bits to the data that is transmitted. Once an error is detected in the receiver, the receiver requests retransmission of the data. The latency and power overheads for retransmission can be quite high. As an example, with 5% error rate, the latency incurred in sending 1000 bits on a 4-stage pipeline link is 1553 cycles, while ideally (when there are no errors) it should take 1003 cycles. Here we have assumed that only the data bits that  749 had errors are resent. In most network designs, a Go Back-N retransmission strategy is used, where all the data bits following the data with error will be resent [9]. In this case, the latency penalty is much higher. As the power consumption of the communication system in the NoC increases linearly with the amount of data sent, the network power consumption also increases signiﬁcantly when data is retransmitted. There is a signiﬁcant need to mitigate the effect of parasiti cs on link performance. In this work, we present a timing error tolerant communication system, Terror, that makes the NoC links tolerant to timing errors caused by the unpredictability in environment and wire characteristics. In the Terror based system, the distance between successive pipeline stages is designed such that the latency for data transmission on the link and the number of pipeline stages in the link are much lower than the traditional worst-case design approach. In order to cope with the timing errors that may occur in such an aggressive design approach, the pipeline buffers in the Terror system are augmented with error detection and correction capability, as shown in Figure 2. Any timing error at a pipeline buffer is detected and corrected with a maximum of single cycle penalty. In the Terror system, the latency overhead for error detection and correction is independent of data size and error rate. The system is highly scalable with link (or bus) width and has a latency penalty which is bounded by the number of pipeline stages on the link. For the above example, the Terror based design results in a latency of 1007 cycles, a 35% decrease in latency compared to the traditional design approach in which errors are handled by retransmission of data. The area overhead for the Terror system is less than 0.6% of the total design area of the SoC, which is negligible when compared to the latency savings achieved. I I . T E RROR DE S IGN PR INC I PL E In the Terror system, each pipeline buffer (main ﬂip- ﬂop ) is augmented with a second ﬂip- ﬂop ( delayed ﬂip- ﬂop ) as shown in Figure 3. The delayed ﬂip- ﬂop operates on a delayed clock (ckd) compared to the main ﬂip- ﬂop (clocked at ck). The incoming data is sampled twice, once by the main ﬂip- ﬂop (at clock edge ck) and then by the delayed ﬂip- ﬂop (at clock edge ckd). The distance between two main ﬂip- ﬂops is designed aggressively to improve performance, thereby resulting in an error-prone operation. However, the clock-delay between the main and the delayed ﬂip- ﬂop (i.e. the delay between the clock edges ck and ckd) is designed such that even in the presence of unpredictability in the wire characteristics, there is sufﬁcient time for the data bits to reach the delayed ﬂip- ﬂop by clock edge ckd. Thus the delayed ﬂip- ﬂops are designed to operate in an error-free manner. There are two modes of operation at each pipeline buffer of the Terror: normal mode and delayed mode. Initially all the buffers are set to the normal mode and data transmission begins. In every cycle, at the clock edge ck, the main ﬂip- ﬂop captures and transmits the incoming data. At clock edge ckd, the delayed ﬂip- ﬂop captures the incoming data and the error detection control circuit checks whether there is any difference between the main and the delayed ﬂip- ﬂop values. If there is a difference, there is an error in the main ﬂipﬂop value and the data that was transmitted at ck is incorrect. The correct data from the delayed ﬂip- ﬂop is sent at the next clock edge ck and the Terror buffer enters the delayed mode. Suitable control signals for the downstream buffers/receiver to recover from the error are also generated and sent. The latency overhead incurred for recovering from the error is one cycle. Once the Terror buffer has entered the delayed mode, all subsequent data is captured and transmitted by the delayed ﬂip- ﬂop and the buffer always operates in an error-free manner. Thus after a single cycle penalty, there is no additional penalty for the rest of the data transmitted through this buffer. The same argument applies to all the pipeline buffers of the link. Thus the maximum (worst-case) overhead in sending data input data delayed flip−flop ckd M U X sel main flip−flop output ck XOR errq Fig. 4. Logic level implementation of Terror errq1 errq2 errqw ... OR err prev_corr sel err prev_corr AND AND Fig. 5. Error control circuit SR  sel ck ckd Clk genera tion circuit correction flip−flop corr_out OR ckdd through the Terror based link is just the number of pipeline buffers in that link, independent of data size and error-rate. When the data transmission is completed, the Terror buffers that are in the delayed mode return back to the normal mode. I I I . LOG IC L EVE L IM PL EMENTAT ION In the Terror based communication scheme, each pipeline ﬂip- ﬂop is replaced by the Terror buffer, which is composed of two ﬂip- ﬂops ( main ﬂip- ﬂop and delayed ﬂip- ﬂop ), a 2:1 multiplexer (MUX) and an XOR gate (refer Figure 4). The delayed clock ckd for the delayed ﬂip- ﬂop is derived from the main clock ck locally at each buffer by using a delay chain (a chain of inverters). At each pipeline stage, an error control circuit (Figure 5) is added for generating suitable control signals when an error is detected. Let the number of bit-lines in the link be w lines. The XOR outputs (errq signals) generated at all the w bit lines, at each pipeline stage of the link are ORed and fed as an input to the error control circuit. The error control circuit consists of a SR latch, AND, OR gates and a correction ﬂip- ﬂop . For proper operation, the correction ﬂipﬂop is clocked by ckdd, which is delayed from the clocks ck and ckd and locally generated at each pipeline stage. A Terror based link with w bit-lines (width of the link) and b pipeline Bit 1 . . . Terror    b Terror     1 errq1 Terror    2 .. . Terror     1 errqw sel1 ... OR Control      1 Terror    2 ... OR sel2 Control     2 Bit w . . . Terror    b selb ... OR . . . Control     b Fig. 6. Link Design using Terror  750     CK DATA CKD ERR CKDD SEL OUT CORR OUT Fig. 7. Normal to delayed mode CK CKDD DATA  PREV  CORR SEL OUT Fig. 8. Delayed to normal mode buffers (on each bit line) is shown in Figure 6. Note that only one error-correction circuit is used at each pipeline stage for all the w bit-lines of the link, so that all the bit-lines of the link are in synchronization with each other. Moreover, the overhead of the error correction circuitry is also reduced by this design. When an error is detected by any of the Terror buffer pipeline stages, the err signal, which is an input to the SR latch is set. The SR latch output, sel is then set to 1, so that the MUX starts sampling the delayed ﬂip- ﬂop output and the Terror buffers at this pipeline stage enter the delayed mode. Control signal corr out is set to 1 and sent to the next pipeline stage/receiver to indicate that the previously sent data was an error (this corr out signal is received as the prev corr signal by the next pipeline stage/receiver). Once the Terror buffer enters delayed mode, no more errors occur as the data sampling is through the delayed ﬂip- ﬂop (refer Figure 4). After all the data is transmitted, the MUX control signal is reset to 0 and the Terror buffer returns to normal mode. In Figure 7, we show an example where an error is corrected in cycle 2 and the Terror buffer operation transitions from normal mode to delayed mode. In the above scheme, we note that if the previous pipeline stage had an error at clock cycle t − 1 and the current pipeline stage has an error at clock cycle t, then there is no need to resend the data at the current pipeline stage in the next cycle as anyway it is incorrect (because of the error in the previous stage). Instead of correcting and sending the current pipeline stage’s data, we can switch back to the normal mode and send the new data coming from the previous pipeline stage. This gives a latency savings of one cycle for the case of two contiguous errors. This is shown in Figure 8. To implement this scheme, the SR latch is modiﬁed, such that the output is 0 when prev corr is 1 (meaning that the previously received data is wrong and the current data is correct) and the err signal is set (meaning that there is an error at this pipeline stage). Similarly, the corr out signal is set to 1 when both the prev corr and err are set to 1. IV. A LT E RNAT IVE A P PROACHE S There are many error correcting schemes that correct error without retransmission of data. Teatime [10], tracks the logic delay variation and dynamically changes the clock frequency to eliminate any errors. It tries to avoid timing errors and requires complex analog frequency controller and tracking logic. Also, the correction efﬁciency depends upon the latency between actual error detection and change in clock frequency. Razor [11] based system has same basic principle as Terror and is used to control power (supply voltage) by monitoring error rate. It detects and corrects error but has one cycle penalty per error occurrence. Also, it gates the global clock to delay the whole pipeline. Clock control may not be always possible, especially in heterogeneous systems. In Terror, only the ﬁr st error occurrence incurs a single cycle penalty for correction and previous pipeline stages are oblivious to an error occurring at the current stage. The latency savings in Terror comes at the expense of slightly more complex error control circuit when compared to Razor. Favalli et al. [12] assume an encoded data signal which is checked by a small decoder present at input of each ﬂip- ﬂop. In case of error, clock is delayed for one cycle, till the correct value of data settles. Mousetrap [13] is a high speed asynchronous pipeline which ensures correct data availability to consecutive stages, but has a substantial overhead of communication signals (acknowledge and request signals). Eric Dupont et al. [14] suggest to include a latch with delayed clock to detect transient faults due to soft errors. This technique is similar but gives error penalty per occurrence of soft error and involves clock control circuitry. There are many coding techniques for correcting errors such as the Hamming code. But they require extra wires, decoding and encoding circuitry at sender and receiver ends and do not scale well with bus widths. Moreover correcting multiple errors in the data is difﬁcult in such coding schemes. All of these techniques int roduce large latency overheads depending on the error rate and have substantial overhead for large bus widths. Terror enabled systems give a bounded penalty irrespective of the data error rate. As an example, in most previous error correcting mechanisms such as Razor [11], if an error occurs at each cycle, then for transmitting N cycles of data, we need 2N cycles. This is because, each error occurrence results in a single cycle overhead. Thus the percentage of useful cycles is just N/2N =50% and the whole pipeline is delayed by N cycles. In a Terror system, the maximum penalty is limited by the number of Terror elements in the communication link. This is because the one cycle penalty for error correction is incurred only for the ﬁrst occurrence of error at the input of a Terror element. The operation is such that subsequent data transmissions are errorfree. Thus, if the total number of Terror elements between the sender and receiver is b, then the percentage of useful cycles is N/(N + b) and the pipeline is delayed by only b cycles. For b < N , which is the usual case in NoC designs, the bene ﬁts are substantial. V. T IM ING ANALY S I S In a Terror based system, the reduction in latency when compared to a traditional design approach depends on the delay between the clock edges ck and ckd. Ideally, the clock ckd can be delayed by one cycle from ck, so that the number of pipeline stages in the link (and hence the latency) is reduced by 50%. In practice, the delay between the clock edges ck and ckd is much lower than one cycle as it is bounded by the delay and timing requirements (setup time, hold time) of the logic elements. Note that the effect of the logic delay can be decreased signiﬁcantly by optimizing the transistor level implement ation of the design. As shown in ﬁgure 4, the main ﬂop (clocked by ck) has 2:1 MUX at its input. This introduces additional delay in the data  751 path. Now the data has to arrive earlier at the input pin as compared to a ﬂop without MUX . This increases the setup time of the data input to the Terror element. This increase in setup time is given by tmux (which is the MUX delay). Similarly, we have an AND gate and an OR gate at the input of the correction ﬂipﬂop , which are in the path of prev corr signal. This increases the setup time requirement of the prev corr signal over the normal set-up time (tsetup (nominal)). The new set-up time for the correction ﬂip- ﬂop is given by: tsetup = tand + tor + tsetup (nominal). (1) The minimum spacing required between rising edges (assuming all ﬂip- ﬂops are rising edge triggered) of ckd and ckdd is determined by the total path delay of the err signal. The err signal has to satisfy the setup time of the correction ﬂip- ﬂop . The err signal path delay starts from the clock ckd to q delay of the delayed ﬂip- ﬂop , through the XOR and OR gate (ORs errq signals) and then through the AND and OR gates. The clock ckdd to the correction ﬂip- ﬂop should arrive such that it captures the correct value of the err signal. This correct value of err signal is only available sometime after the rising edge of ckd and this time delay is the summation of all the delays in the err path and the nominal setup time of the correction ﬂip- ﬂop . Thus ckdd should be spaced from ckd to accommodate the err path delay. The minimum spacing tckd between rising edges of ckd and ckdd is given by below equation. tckd = tckq + txor + tdomino−or + tand + tor + tsetup (2) In the case of a bus, the errq signals of all bit lines in a link are ORed. This ensures that all the bit lines in the link are in synchronization with each other, simplifying the receiver design. This wide OR can be implemented as a domino gate, instead of a static gate to reduce delay. Also, there is some delay for Terror to go from delayed to normal mode. Terror goes from delayed to normal mode when prev corr is set to one. This resets the SR latch output (sel signal). This sel signal is an input to the 2:1 MUXs. For correct functionality, the sel signal should change value before the rising edge of ck. The minimum spacing between rising edges of clocks ckdd and ck is determined by the total path delay of the sel signal. The prev corr signal satisﬁes the setup time of the correction ﬂipﬂop and hence comes sometime before the rising edge of ckdd. Path delay of sel signal starts when prev corr arrives, then it goes through the SR latch and then the 2:1 MUX. Minimum spacing (tckdd ) between ck and ckdd is the total path delay minus the setup time of the prev corr signal. tckdd = tSRlatch + tmux − tsetup (correctionf lop) (3) In error correcting circuit, the err signal (which is the output of the OR of the errq signals) asserts the sel signal. The sel and err signals are also fed as inputs to the correction ﬂipﬂop of the error control circuitry. Thus sel should not change before thold of the correction ﬂip- ﬂop. We get below hold-time condition: thold < tSRlatch + tand + tor domino OR instead of a static OR, (4) The timing delays and overheads can be minimized by transistor level optimizations such as including the input MUX into the ﬂip- ﬂop, using a simplifying the latch, combining OR logic into correction ﬂipﬂop . Figure 9 shows the optimized transistor-level circuit diagram where ﬂip- ﬂops with embedded logic have been used. Ideally, the clock (ckd) to the delayed ﬂip- ﬂop can be delayed by one cycle, such that even if the data arrives one cycle late it is captured and sent the next cycle. But due to timing overheads this window is decreased by (tckdd + tckd ). This gives an upper bound on the frequency by which the clock cycle can be decreased beyond speciﬁcations. 2:1 MUX 36 36 sel d0 clk P:16 N:8 d1 clk vdd 40 36 36 36 36 gnd FLOP N:16 P:16 : P 1 6 N : 8 P 8 : N : 4 clk vdd 90 45 45 gnd clk P:12 N:4 N:12 P:8 q P:64 N:32 N:4 P:8 q_b Fig. 9. Semi-Dynamic Flip Flop Design TABLE I T IM I NG OV E RH EAD S Parameter Hold Time 32-bit OR delay tckdd tckd ckd delay % Overhead 10 8.6 9.7 27.0 36.7 V I . TRAN S I STOR L EVE L S IMULAT ION We designed a transistor level Terror element for a 32 bit bus in 100 nm technology targeted for 1GHz operating frequency, operating at 1.2 V. From SPICE simulations, we obtained values for timing overheads, presented in Table I, where the overheads are expressed as a percentage of cycle. This table is an estimate of the timing constraints and the timing overheads can be reduced substantially by using better transistor sizing techniques, process technology and commercially available CAD tools. From Table I we see that practically clock ckd cannot be delayed beyond 63.3% of cycle time. This is because there is a minimum spacing requirement tckd + tckdd = 36.7% which should be satisﬁed. Hence only 100-36.7 = 63.3% of cycle is available for ckd delay. Also, to satisfy hold time requirement (10%) of the main ﬂip- ﬂop , minimum spacing between ck and ckd should be 10% of the cycle. Due to this, range for variation of ckd is limited to 53.3% of the clock cycle. Errors due to meta-stability of data, as discussed in [11] can occur at the input of the delayed ﬂip- ﬂop . These cannot be completely eliminated but can be minimized. Since we use a delayed ﬂip- ﬂop instead of a delayed latch used in [11] we signiﬁcantly minimize short-path constraint problem. Note th at for proper Terror operation, the corr out line for signaling the occurrence of an error should be error-free. The corr out line can be made error free by various means such as shielding the line from other bit lines, routing the line in higher metal layer so that it propagates faster, providing parity checker to detect an error in the line, etc. As only a single corr out line is added to the link that typically has multiple bit-lines, the overhead in shielding or conservatively designing the corr out line is low. V I I . ANALY S I S O F PENALTY The maximum latency penalty in the Terror system is bounded by the number of pipeline stages in the link and is independent of the amount of data sent and the error rate. This is because, a single cycle latency penalty is incurred at a pipeline stage only for the ﬁrst detection and correction of an error.  752 800 0 20 40 60 80 100 120 900 1000 1100 1200 1300 1400 1500 % ckd delay R e v e c e i r l a t y c n e ( s e c y c l ) 1% error rate 3% error rate 5% error rate Fig. 10. Receiver latency variation with delay between clocks ck and ckd for ideal case 800 0 20 40 60 1000 1200 1400 1600 % ckd delay R e v e c e i r l a t y c n e ( s e c y c l ) 1% error rate 3% error rate 5% error rate Fig. 11. Receiver latency for practical case 0 10 20 30 40 50 0 2 4 6 8 10 % agressiveness R e v e c e i r l a t y c n e Fig. 12. Latency variation vs.aggressiveness 5000 10000 15000 0.5 1 1.5 2 x 104 R e v e c e i r l a t y c n e ( s e c y c l ) Number of bits sent 1% (retransmit) 1% (terror) 5% (retransmit) 5% (terror) Fig. 13. Comparison of latency for the retransmission and Terror scheme Once an error occurs at a pipeline stage, the pipeline stage enters the delayed mode, so that subsequent data transmission at this pipeline stage is guaranteed to be error free. The actual latency penalty is a function of temporal and spatial probability of error occurrence. For a Terror link with b pipeline stages, we get 1 ≤ P enalty ≤ b (5) The magnitude of penalty is a function of when and where the timing error occurs in the pipeline. In Figure 1, if a timing error occurs ﬁrst at the pipeline stage b, followed by an error at pipeline stage (b − 1) and so on up to the pipeline stage 1, then the total penalty for error correction is just one cycle. This is because the error in pipeline stage (b− 1) is absorbed by Terror at pipeline stage b (since Terror b goes from delayed to normal mode, incoming error is not propagated by Terror b). On the other hand, if a timing error occurs ﬁrst at the pipeline stage 1, followed by an error at pipeline stage 2 and so on up to Terror b, then penalty for error correction is b cycles. This is because at each pipeline stage, a single cycle penalty is incurred for error detection and correction. Thus, the maximum penalty in the Terror scheme is b cycles, while the actual penalty lies between 1 and b cycles. Also, we note that maximum penalty is independent of the total amount of data sent. This makes Terror design very attractive for high bandwidth data communication of current and future SoCs. V I I I . S IMULAT ION RE SULT S We performed several simulation case-studies to quantify the performance (latency) bene ﬁts of Terror for different e rror rates, data sizes and pipeline stages. For the simulations, we consider an SoC with an on-chip link of length 12mm that connects the various components of the SoC. We assume that the link operates at a frequency of 1 GHz. For a conservative design approach that takes into account all the delay variations that can possibly occur due to the unpredictability in the wire and environment characteristics, we assume that the distance between successive pipeline stages is 2mm for safe operation of the links [2]. Thus, for a conservative design the number of pipeline stages on the link is 6. In an aggressive design approach, the distance between two successive pipeline stages can be increased, so that the total number of pipeline stages in the link and the link latency are reduced. When the data is double sampled (as in Terror), in the ideal case (when we neglect the timing overheads associated with the Terror logic elements), the distance between successive pipeline stages can be doubled when compared to the conservative design. Thus, the number of pipeline stages on the link can be reduced to 3. In this case, the delay between the clocks ck and ckd should be one cycle to detect all timing errors that can occur due to the aggressive design approach. Figure 10 shows the latency for transmitting 1000 bits of data on the link as a function of the delay between the clocks ck and ckd of the main and delayed ﬂip- ﬂops. As the difference between the clocks increases, the number of errors detected and corrected by the Terror scheme starts to increase as the delayed ﬂip- ﬂop gets a larger time window to sample the incoming data. In Figure 10, when the difference between the clocks is less than one cycle, we assume that the data bit errors that are not corrected by Terror buffers are retransmitted using an end-to-end ﬂow control mechanism. We have assume that only the data bits that had errors are resent. In most network designs, a Go Back-N retransmission strategy is used, where all the data bits following the data with error will be resent [9]. In this case, the latency penalty is much higher. As seen from the plot, there is a signiﬁcant reduction in the latency as th e delay between the clocks ck and ckd increases. Though, ideally the distance between successive pipeline stages can be doubled in a Terror scheme, as explained in section VI, practically the distance can only be increased by 50% due to the timing overheads associated with the ﬂip- ﬂops and the logic elements of the Terror system. For the Terror system to detect all the timing errors, the distance between successive pipeline stages for the on chip link needs to be 3mm, so that the above on-chip link is pipelined with 4 stages. Figure 11 shows the latency for transmitting 1000 bits of data for various error rates as a function of the delay between the clocks ck and ckd for the practical case. In Figure 12, we plot the variation of the link latency with the percentage aggressiveness. We de ﬁne the percentage aggressiveness as the percentage by which the distance between successive pipeline stages is increased when compared to the conservative design approach. The plot terminates at 50% aggressiveness, since in our Terror system, the distance between successive pipeline stages can be increased only up to 50% of the conservative design. As expected, the latency of communication decreases signiﬁcantly as the aggressiveness incre ases. We obtain a 33% reduction in latency with the Terror based system that utilizes 50% aggressiveness, compared to the conservative design approach. In Figure 13, the latency for data transmission for two different error rates (1% and 5%) is plotted for various data sizes for the Terror based design and a traditional design where errors are corrected by retransmission. As seen from the ﬁgure , the latency for the Terror system for various error rates is almost equal to the latency for an ideal case when there are no timing errors. For a chosen error rate, as the size of data transferred increases, there is signiﬁcant latency savings in th e Terror system when compared to the traditional scheme of retransmission. Moreover, as the error rate starts to increase, there is much larger savings in latency for the Terror based system. For the data size of 1000 bits and error rate of 5%, there is a 35% reduction in latency in the Terror based system when compared to the retransmission scheme.  753               ) l s e c y c ( y t l a n e P 6 5 4 3 2 1 0 200 data_input prev_corr RECEIVER rec_out LOOK AHEAD ck CK DATA IN CORRECT IN REC_OUT 0.5% error rate 1% error rate 1.5% error rate 2% error rate  3% error rate 4% error rate 400 Number of bits 600 800 TABLE II A R EA OV E RH EAD O F T E R RO R Design 1. Merlot 2. DSP 3. MIT RAW 4. Alpha MP Average Area overhead 0.12% 0.6% 0.86% 0.9% 0.62% Fig. 14. Terror penalty for different data sizes Fig. 15. Receiver interface design Fig. 16. Look-ahead stage operation Figure 14 shows the variation of maximum penalty for error correction in the Terror system, with respect to the total number of bits sent on the link. With increase in error rate and size of data transmitted, the penalty increases until it reaches 4. After the penalty of 4 cycles, which is the number of pipeline stages in the link, the penalty is constant with increasing data size and error rate. Note that a typical error correction mechanism would degrade at higher error rates and large data size. But in Terror enabled system, the latency overhead does not increase with error rate or data size, making it suitable for large bandwidth data transmission of SoCs. Moreover, as explained in [11], the system can be operated at lower voltage levels when errors are permitted to occur in the system. In such a design, signiﬁcant power savings can be achieved as th e error-rate increases and Terror based communication mechanism supports such a design. We estimated the area overhead when Terror based communication system is added to some example Multiprocessor System on Chips (MPSoCs) available in literature [16], [17], [18], [19]. The area overhead estimation is based on the increase in gate count due to the addition of Terror elements. As seen from Table II, on the average, there is 0.6% increase in area, which is negligible when compared to the large latency savings achieved by the Terror system. The power overhead incurred by the Terror elements during error free operation is negligible when compared to the design power of the MPSoCs (less than 10−5% of total power consumption). IX . RE CE IVE R DE S IGN The changes required in the receiver for supporting the Terror based communication system are simple as all the bit-lines of a link are synchronized in the case of an error. As shown in Figure 15, the receiver waits in a look-ahead stage when it gets the ﬁrst data ﬂit. When the prev corr signal that arrives in the next cycle is 0, which means that data received in the previous cycle was correct, the previous data is sent to the subsequent stages on the rec out lines. If prev corr is 1 (refer Figure 16), then it indicates that the data received in the previous cycle had an error and the data is discarded. The receiver now waits on the new data received at this cycle in the look-ahead stage. In this scheme, only when the incoming data is known to be correct it is used for further processing. This receiver design introduces a single cycle latency penalty for the ﬁrst ﬂit of the packet, even if it is error free, due to the look-ahead stage. Note that a scheme where the data is processed by the receiver before receiving the prev corr signal would not incur this one cycle penalty, but would require complex recovery mechanism when the data is found to be have an error in the next cycle. X . CONCLU S ION S AND FUTURE WORK As Systems on Chips become increasingly interconnect limited, efﬁcient design of the interconnects is required for h igh performance of the overall system. Networks on Chips (NoCs), a communication centric design approach has evolved in the recent years for tackling the delay, throughput and reliability issues of the interconnects. Current NoC design methodologies are based on conservative design approaches for link design that results in poor performance. In this work, we have presented Terror, timing error tolerant communication system, where the NoC links are designed aggressively for high performance operation. Instead of avoiding timing errors in the NoC links by a conservative design approach, the links in the Terror system are designed aggressively by tolerating timing errors. Our aggressive design methodology provides large (up to 35%) latency savings compared to traditional design approaches with very little area overhead. In our future, we plan to enhance the Terror design methodology to incorporate Quality-of-Service (QoS) guarantees for the applications. X I . ACKNOW L EDGEMENT S This research is supported by MARCO Gigascale Systems Research Center (GSRC) and NSF (under contract CCR0305718). "
2005,MAIA - a framework for networks on chip generation and verification.,"The increasing complexity of SoCs makes networks on chip (NoC) a promising substitute for busses and dedicated wires interconnection schemes. However, new tools need to be developed to integrate NoC interconnection architectures and IP cores into SoCs. Such tools have to fulfill three main requirements: (i) automated NoC generation; (ii) automated production of NoC-IP core interfaces; (iii) seamless analysis of NoC traffic parameters. The objective of this paper is to present the MAIA framework, which includes functions to address all these requirements. NoCs generated by the MAIA framework have been used to successfully prototype SoCs in FPGAs.","MAIA – A Framework for Networks on Chip Generation and Verification Luciano Ost1, Aline Mello1, José Palma2, Fernando Moraes1, Ney Calazans1 1FACIN-PUCRS Av. Ipiranga, 6681 - Porto Alegre – 90619-900 – BRAZIL {ost, alinev, moraes, calazans}@inf.pucrs.br 2II - UFRGS Caixa Postal 15064- 91501-970 - Porto Alegre – BRAZIL jcspalma@inf.ufrgs.br Abstract - The increasing complexity of SoCs makes networks on chip (NoC) a promising substitute for busses and dedicated wires interconnection schemes. However, new tools need to be developed to integrate NoC interconnection architectures and IP cores  into SoCs. Such tools have to fulfill three main requirements: (i) automated NoC generation; (ii) automated production of NoC-IP core interfaces; (iii) seamless analysis of NoC traffic parameters. The objective of this paper is to present the MAIA framework, which includes functions to address all these requirements. NoCs generated by the MAIA framework have been used to successfully prototype SoCs in FPGAs. 1. Introduction The increasing complexity of Systems-on-Chip drives the research of new intra-chip interconnection architectures. Traditional on-chip interconnection architectures, such as dedicated wires and shared busses, can be considered inefficient for future SoCs. Dedicated wires present poor reusability and flexibility, while shared busses transmit only one word per clock cycle and offer limited scalability. According to ITRS estimation, in 2012, SoCs will have hundreds of hardware blocks (called IP cores), operating at clock frequencies near 10 GHz. In this context, a Network on Chip (NoC) appears as a possible solution for future on-chip interconnections. A NoC is an on-chip network [1] composed by cores connected to routers, and routers interconnected by communication channels. Shared bus architectures are the current dominant on-chip communication structure used in SoCs. Consequently, CAD tools are available to support shared bus architectures. NoC design requires new features, such as support for different switching modes, routing algorithms, and message packets segmentation and reassembly procedures [1][2][3]. It is important to develop tools for NoC generation and verification, due to the expected relevance of NoCs in future SoCs. The objective of this paper is to present a framework which allows exploring and optimizing SoC designs employing the HERMES1 NoC [4]. The objective of this paper is to present the MAIA framework for NoC generation and verification. MAIA can generate different  traffic patterns,  for different  load conditions and source/target pairs. Using the generated traffic and the automatically produced simulation scripts, it is possible to validate and evaluate the NoC and the associated SoC using commercial tools such as the Modelsim simulator. MAIA can also generate OCP (Open Core Protocol) network interfaces (NIs). While generating NIs, the tool is capable of 1 In Greek mythology, HERMES is the messenger of Gods, son of MAIA and ZEUS. This work was partially funded by grant 550009/03-5, from CNPq, Brazil. supporting different communication models. Finally, MAIA contains a module named traffic analyzer. It verifies if all packets were correctly received, and generates basic statistic data concerning time do deliver packets. Besides, this module can be used to validate the NoC internal implementation. This paper is organized as follows. Section 2 presents related works in NoC design space exploration. The main features of the HERMES infrastructure are briefly presented in Section 3. Section 4 details MAIA main features. Section 5 presents how the MAIA framework can be used to evaluate the performance of different NoC configurations. Section 6 presents conclusions and directions for future work. 2. Related Work Several research groups have proposed techniques to specify, simulate and generate NoCs. Some of these try to adapt generic network simulators to the intra-chip environment, while others propose NoC specific tools. This Section reviews several such efforts, including: (i) NoCSim, (ii) OPNET, (iii) Kogel framework, (iv) NoCGEN, (v) NS-2, (vi) OCCN, (vii) Pestana environment, and (viii) NOCIC. NoCSim  is a NoC simulator based on IP cores communicating  through a packet switching network. NoCSim generates limited forms of statistic traffic, using Constant Bit Rate and random Poisson distributions [4]. OPNET [6] is a general purpose network simulator used to simulate NoC-based architectures [7]. OPNET presented some disadvantages to modeling NoCs, including that: (i) it does not allow setting a time unit smaller than 1 second; (ii) distance between nodes in the network is measured in meters only; (iii) OPNET assumes asynchronous communication. NS-2 is another general purpose network simulator that gives support to describing the network topology, the communication protocols, routing algorithms and traffic (e.g. random traffic) [8]. NS-2 provides simulation traces for interpreting results and provides NAM (Network AniMator), a graphic aid to observe network message flow. Kogel et al. [9], proposed a modular framework for system level exploration of the on-chip interconnection architecture. This framework is able to capture performance effects (like latency and throughput) of different on-chip architectures like shared bus and NoC topologies. NoCGEN creates VHDL NoC descriptions used for simulations and synthesis [10]. This tool employs a set of parameterizable templates to build routers, with variable number of ports, routing algorithms, data width and buffer depth. Besides NoC parameterization, it presents a mixed SystemC/VHDL simulation environment.  49                                                            Another framework for NoC modeling and simulation is the OCCN [11]. OCCN enables the creation of NoC at different abstraction levels, protocol refinement, design space exploration, and NoC components development and verification based on a communication API [11]. Pestana presents in [12] a NoC simulator based on usergenerated XML files that describe a NoC topology, the IP to NoC mapping and the interconnection details. The simulator also allows describing traffic generators to evaluate NoCs. Finally, the NOCIC tool allows estimating performance and power for NoC structures using a set of design parameters simulated with HSPICE tools [13]. The MAIA framework has some features in common with NoCGEN,  including automated  traffic and network generation. One significant difference between MAIA and the previously reviewed works is that the latter are centered on the network itself, with little reference on how to map NIs to the rest of the SoC. An exception is [12]. MAIA automatically generates the NIs, using the OCP standard [14]. 3. HERMES Network-on-Chip HERMES is an infrastructure used to implement low area overhead packet-switching NoCs for different topologies, flit sizes, buffer size, routing algorithms, and flow control strategies [4]. It supports the implementation of the three lower OSI-RM layers, namely physical, data link and network. Initially, HERMES is based on NoCs using only wormhole routing. The basic component of this infrastructure is the router. It contains a centralized control logic module, responsible for arbitration and routing, and up to five bi-directional ports (East, West, North, South, and Local). Each port has an input buffer for temporary storage of flits. The Local port connects the router and its local IP core. Two flow control strategies are available: handshake protocol and credit based. When a 4-phase asynchronous handshake protocol is used, the external router interface is composed by six signals: rx, ack_rx and data_in for input and tx, ack_tx and data_out for output. When credit based flow control is used, a transmission clock is sent to the receiver and a credit signal is asserted from the receiver to the transmitter indicating available buffer space. Signal ack_rx does not exist in this case. This flow control algorithm enables implementing GALS networks. Several  instances of NoC-based  systems were successfully prototyped in FPGAs [4]. An example system contains two simple 16-bit processors, an embedded memory and an interface IP core to provide communication with a host processor. It is estimated that a 5x5 HERMES-based system can be implemented in a 4-million gate device, with the same small 16-bit processor connected to the Local port of each router. 4. MAIA Features The design of NoCs is an error prone process, even if routers and IP to NIs are assumed given. This is due to the large number of wires used to connect routers among them and IP cores to routers. Each router has a set of control and data signals that amount to more than a 100 wires. The primary function of MAIA is to build NoCs from parameterizable templates. Fig. 1 shows the MAIA design flow. l r y e d a o b L M r i N oC specification  a nd ge ne ration NoC N oC  configuration ge ne ration NoC  testbench  g eneratio n NoC- IP (black b ox ) N I configuration NoC and N I generat ion NI testbench ge neration r I y a L F b L r i T ra ffic generation  FLI traffic  (normal/OCP/ STL) NoC simulation (Active -HD L or  ModelSim) NoC ver ification Tr aff ic analysis Simu lation  scr ipt  Inpu t files Input files Input files Inpu t files Outp ut  Input files file s Analysis ` result G r e n e a e d t f i l e s Fig. 1 – MAIA framework structure and execution flow. Three steps compose the design flow: (i) NoC specification and generation comprise: (1) selection of the network parameters; (2) selection of the external interface – native or OCP; (3) selection of  the communication model; (4) selection of the IP type, if OCP interface is selected. After parameter selection, all VHDL and C files are created. A UNIX script is created as well, to start simulation. (ii) Traffic generation produces the packet files, according to chosen parameters as described in Section 4.3. (iii) Traffic analysis creates traffic analysis reports. Fig. 2 displays the primary graphical interface of MAIA. The visualization area (1) represents a 4x4 mesh network composed by routers with master-slave OCP NIs. The NI type is individually chosen, on a router by router basis. The second region (2) allows  the user  to select network parameters. These are used  to configure  the VHDL implementation files. The third region (3) contains menus used to start simulation, traffic generation and traffic analysis. The fourth region (4) presents messages resulting from execution of user operations. 3 2 1  4  Fig. 2 – MAIA framework graphical user interface. The main contribution of the MAIA design flow is to relieve the user from worrying about the NoC structure and its internal functionality, i.e. enabling the use of NoCs as a generic communication IP core – a NoC-IP. It also simplifies the interconnection of IP cores to the NoC. The only requirement imposed on the MAIA user is to know in advance the structure of the transactions supported by the NIs at the network boundaries.  50       4.1. Network Generation The NoC components in the Model Library can be described in RTL VHDL, RTL SystemC and Transaction Level (TL) SystemC. The  integration of parameterizable SystemC components is an ongoing work. All results presented in this paper regard VHDL models only. Currently, the following parameters can be chosen by the MAIA user: (i) network topology, (ii) routing algorithm, (iii) flit width, (iv) buffer size, (v) network structure, and (vi) control flow strategy. To parameterize the NoC, a set of tokens are inserted in the VHDL code, such as: (i) $flit_size$, (ii) $buff_depth$, and (iii) $router_no$. Network ports  and  associated buffers,  can be automatically by MAIA, depending on the network topology and specific router position. For example, in a mesh topology some buffers of routers at the edges of the network are always eliminated. Consider the upper left router in a mesh network. The North and East buffers are suppressed, contributing to reduce the overall interconnect area. Today, MAIA supports three NoC topologies: mesh, torus and ring. Two other parameters affect network performance: flowcontrol strategies (handshake or credit based protocols are supported) and routing algorithms. Deadlock free routing algorithms for torus and ring topologies are not yet available. Virtual channel support that can be used to build these is another ongoing work. For mesh topologies, four routing algorithms are available: pure XY, west-first, north-last and negative-first [3]. 4.2. Network Interfaces Generation A NoC is formed by two main components: routers and NIs. The NI  is  responsible  for packet segmentation and reassembly. NIs should be designed taking into account the communication model. Since a NoC-based SoC is similar to a parallel processing system in communication structure, it is possible to employ classical communication models from the latter domain. The Uniform Memory Access (UMA) model is not useful in the intra-chip domain. Accordingly, MAIA only supports two other communication models: non-uniform memory access (NUMA) and no-remote memory access (NORMA). The NUMA communication model assumes all IP cores connected  to  the NoC share a single address space. Communication then takes place using memory-mapped I/O operations. This model is adequate for fast migration of legacy IP cores to NoC-based SoCs, once most bus architecture-based systems assume  this communication model. The NORMA communication model does not assume the existence of a global address map. Communication is achieved directly, through the exchange of messages between IP cores. In a NORMA SoC, IP cores are independent, providing services to other IP cores. However, the NI may have to deal with the overhead of adapting its IP core communication paradigm. The IP core-NI interface can be either proprietary or standard. A proprietary interface reduces the reusability of the network but may improve its performance. A standard interface has opposite characteristics. The user can choose the native HERMES interface or an OCP interface. OCP defines a point-to-point interface between two IP cores. One of these operates as master and the other as slave [14]. Only the master sends commands to initiate transactions. The slave answers to the commands, receiving data from or sending data to the master. Typical master IP cores are processors, while a memory is often a slave IP core. Using a standard interface does not change the way IP cores are developed, since they will still be exchanging the same information with its environment, as predicted by its specification. The form how this exchange occurs is by means of a standard industry-accepted procedure, as occurs with the PCI standard for microcomputer manufacturers. Thereby, IP cores reusability is higher and design time can be reduced, since IP core integration occurs at higher levels of abstraction, becoming a simpler process. NIs generated from MAIA templates (master, slave and master-slave) have been certified using the CoreCreator tool [14], and prototyped in Xilinx FPGAs. 4.3. Traffic Generation and Analysis The MAIA framework Traffic generation is responsible for testbench and traffic files generation. Traffic files contain packets, which are read by the IPs connected to the NoC. Files are generated in one of four formats: (i) VHDL testbenches for the native interface; (ii) C/VHDL testbenches for the native interface; (iii) C/VHDL testbenches for the OCP interface; and (iv) STL (Sonics Transaction Language) format, to be used in the CoreCreator tool [14]. The following parameters define a traffic: (i) network load [2]; (ii) number of packets each IP core sends; (iii) number of flits in each packet; (iv) flit size; (v) the target IP core, which can be random or fixed. According to these parameters, a set of input files is created. During simulation, another set of files is generated, containing the received packets and their respective time stamps (time spent by a packet to be transmitted from source to destination). The files generated during simulation are read by the Traffic analysis module, which produces a report file. This report file presents some traffic analysis results, such as: (i) total number of received packets; (ii) average time to deliver the packets, in clock cycles; (iii) total time to deliver all packets, in clock cycles; (iv) the average, minimal, maximal and standard deviation time to deliver a packet, in clock cycles; and (v) the total simulation time, in seconds. 5. NoC Design Space Exploration Using the MAIA framework, the performance of routing algorithms, the effect of buffer sizes and of external interfaces was evaluated. All generated NoCs case studies have a 4 x 4 mesh topology, with a flit size equal to 16. The following structural parameters were varied: routing algorithm (XY and negative first, NF); buffer size (4 and 8 flits); external interface (native and OCP). Three randomly generated traffics were used, each with a fixed load of 70%. Table 1 presents these traffic characteristics. All  examples  employ  a NORMA communication model. Fig. 3 (a) illustrates the performance evaluation for different buffer sizes and routing algorithms, for different traffic conditions, using the HERMES native interface. On the other hand, Fig. 3 (b) reproduces the results obtained when using OCP interfaces.  51 Table 1 – Case study traffic characteristics (4 x 4 mesh). Traffic 1 Traffic 2 Traffic 3 # of packets per router 100 20 500 # of flits per packet 100 500 10 Total # of flits 160,000 160,000 80,000 l s e c y c s k c o c l f o r e b m u N 100000 90000 80000 70000 60000 50000 40000 30000 20000 10000 0 Hermes NoC with native interface Mesh 4x4 with OCP NI  l s e c y c s k c o c l f o r e b m u N 160000 140000 120000 100000 80000 60000 40000 20000 0 NF-B4 NF-B8 XY-B4 XY-B8 Routing Algorithm and Buffer size (B) traffic1 traffic 2 traffic 3 NF-B4 NF-B8 XY-B4 XY-B8 Routing Algorithm and Buffer size (B) traffic 1 traffic 2 (a) using HERMES NI. (b) using OCP interface. Fig. 3 - Performance evaluation for different buffer sizes and routing algorithms, for different traffic conditions. Some conclusions of these results are: (i) routing algorithms: the performance of the XY routing algorithm was consistently superior to the NF algorithm. (ii) buffer size: a very small advantage is observed when buffer size is equal to 8. Since NoC area is dominated by the buffer size, in this experiment, buffers with size 8 are oversized and unnecessary [4]. (iii) traffic conditions: in adaptive algorithms (NF), the use of smaller packets (traffic 1) improves the performance over larger ones (traffic 2), since packets can explore alternative paths  in  the network when blocking conditions arrive. An opposite behavior is observed in the deterministic algorithm (XY), where the performance of large packets is superior. The expected time to deliver all packets of traffic 3 would be half of that for traffic 1 or 2, since the number of flits to transmit is reduced to half. The overhead due to frequent routing/arbitration operations (small packets) reduces this performance. A more extensive set of experiments showed that, in terms of total clock cycles to deliver all packets, deterministic XY routing is consistently faster than the other three partially adaptive algorithms. The latter can potentially speed up the time to deliver individual packets. However, globally, results point out to performance poorer than that of the XY algorithm. Glass and Ni [3], suggest that reducing the number of turns that a message takes may reduce blocking and hence improve performance. This is justified because adaptive routing tends to concentrate traffic in the center of the network, increasing the number of blocked paths. The Northlast algorithm presents a small advantage over the XY algorithm for 30% traffic load and small packets (10 and 100 flits). This situation leads to a reduced number of blocked paths and the availability of idle time between packets. As the XY algorithm cannot explore different paths, even when they are available, adaptive algorithms have a potential advantage in this case. Fig. 3 (b) illustrates the performance when using the OCP interface. This Figure also showed that buffer size 8 is oversized and that the XY algorithm showed superior performance over NF. The most import result concerning Fig. 3 (b) is the cost of adding a standard NI. As already  52 mentioned, standard  interfaces have  the advantage of improving reuse (plug-and-play feature). On the other hand, performance is penalized. Comparing Fig. 3 (a) to Fig. 3 (b), packets are delivered to their targets almost 50% faster in the NoC without OCP NI. Currently, OCP interfaces are attached and adapted to the native NoC interface using a dedicated finite state machine. The OCP interface cost could be significantly reduced  if  the NoC Local port directly implemented the OCP protocol, suppressing the native NoC interface. Therefore, router adaptation is not necessary. This option simplifies the design process and can reduce the NI area overhead. This can be justified because fewer states are necessary to packet segmentation and reassembly. 6. Conclusions and Future works The MAIA CAD framework is useful to generate and evaluate NoCs with varying architectural parameters. Ongoing work includes: (i) SystemC modeling; (ii) virtual channels implementation to give support to QoS and deadlock free algorithm in torus topologies; (iii) more elaborate statistical traffic models generation. Future versions of the MAIA framework will include: (i) a library of IP blocks that can be configured by the user to enable SoC generation and simulation; (ii) a module for enabling more thorough traffic analyses using real traffic, such as video streaming; (iii) new NoC architectures, such as irregular meshes of multiple IPs per router. 7. "
2006,Mapping and configuration methods for multi-use-case networks on chips.,"To provide a scalable communication infrastructure for systems on chips (SoCs), networks on chips (NoCs), a communication centric design paradigm is needed. To be cost effective, SoCs are often programmable and integrate several different applications or use-cases on to the same chip. For the SoC platform to support the different use-cases, the NoC architecture should satisfy the performance constraints of each individual use-case. In this work we motivate the need to consider multiple use-cases during the NoC design process. We present a method to efficiently map the applications on to the NoC architecture, satisfying the design constraints of each individual use-case. We also present novel ways to dynamically reconfigure the network across the different use-cases and explore the possibility of integrating dynamic voltage and frequency scaling (DVS/DFS) techniques with the use-case centric NoC design methodology. We validate the performance of the design methodology on several SoC applications. The dynamic reconfiguration of the NoC integrated with DVS/DFS schemes results in large power savings for the resulting NoC systems","Mapping and Conﬁguration Methods for Multi-Use-Case Networks on Chips Srinivasan Murali CSL, Stanford University Stanford, USA smurali@stanford.edu Martijn Coenen, Andrei Radulescu, Kees Goossens Philips Research Laboratories The Netherlands {martijn.coenen,andrei.radulescu,kees.goossens}@philips.com Giovanni De Micheli LSI, EPFL Switzerland giovanni.demicheli@epﬂ.ch To provide a scalable communication infrastructure for SysAB STRACT tems on Chips (SoCs), Networks on Chips (NoCs), a communication centric design paradigm is needed. To be cost effective, SoCs are often programmable and integrate several different applications or use-cases on to the same chip. For the SoC platform to support the different use-cases, the NoC architecture should satisfy the performance constraints of each individual use-case. In this work we motivate the need to consider multiple use-cases during the NoC design process. We present a method to efﬁciently map the applications on to the NoC architecture, satisfying the design constraints of each individual use-case. We also present novel ways to dynamically reconﬁgure the network across the different use-cases and explore the possibility of integrating Dynamic Voltage and Frequency Scaling (DVS/DFS) techniques with the use-case centric NoC design methodology. We validate the performance of the design methodology on several SoC applications. The dynamic reconﬁguration of the NoC integrated with DVS/DFS schemes results in large power savings for the resulting NoC systems. Keywords: Systems on Chips, Networks on chips, UseCases, Multiple application platforms, Dynamic, Reconﬁguration, Voltage Scaling, Frequency Scaling, Guaranteed Throughput, Best Effort. I . INTRODUCT ION As the number of transistors on a chip increases with every technological generation, the number of processor, memory and hardware cores available on the chip also increases. Thus, functionalities that were carried out by several different chips are being integrated on to a single chip, forming a Systems on Chip (SoC). This, coupled together with the increase in the operating speed of the transistors has created the availability of large computational power for such systems. The challenges facing the SoC designer are to efﬁciently tap the available computational power under tight power budgets and meet the tight time-to-market constraints. As the computational loads on the SoC increases, so does the load on the communication architecture. To support the high communication needs of multi-application SoC platforms, scalable on-chip interconnection networks are needed. A communication centric design paradigm, Networks on Chips (NoCs), has been presented to address the interconnect issues 100 MB/s mem1 mem2 100 MB/s 50 MB/s 50 MB/s filter 1 input 50 MB/s filter 2 150 MB/s filter 3 200 MB/s output 50 MB/s mem1 mem2 100 MB/s 50 MB/s 50 MB/s 50 MB/s filter 1 input 50 MB/s filter 2 150 MB/s filter 3 200 MB/s output (a) Use-case 1 (b) Use-case 2 Fig. 1. A fragment of communication for two different use-cases of a set-top box SoC of SoCs [2]-[6]. NoCs provide a scalable communication infrastructure with structured and modular wiring between the components. NoCs also help meet the tight time-to-market constraints, as the scalable architecture can be re-used across multiple platforms. To be cost effective, SoCs are often programmable and integrate several different applications or use-cases on to the same chip. As an example, a SoC for a set-top box has multiple resolution video processing capabilities (like high deﬁnition, standard deﬁnition), multiple picture modes (like split-screen, picture-in-picture), video recording features, high speed internet access and ﬁle transfer services, etc. [9]. Such convergence of multiple use-cases on to the same device is being observed in other electronic devices as well, such as the cell-phone or the personal digital assistant. The different use-cases run on the SoC, although share many of the hardware components, could have very different performance requirements and design constraints for the communication architecture. As an example, we consider a simpliﬁed version of a SoC used in television set-top boxes [9], with support for four different use-cases. The communication bandwidth requirements for some of the connections between the components of the SoC for two of the use-cases are shown in Figure 1. Although we want the NoC to support all the usecases, a NoC that is designed to run exactly one use-case does not necessarily meet the design constraints of the other user cases. In many of the existing NoC design methods, the NoC is designed and optimized for a single use-case or for a single application-trace of the design [10]-[15]. Such a trace based approach captures the characteristics and constraints of a single use-case very well, but fails to capture the multiple usecase scenario. Such a method averages out the communication effects across all the use-cases, which may result in a design that is unacceptable for many use-cases. As an example, when such a method is applied to perform NoC mapping for the settop box SoC, the resulting NoC violates the design constraints of all the four use-cases. Today’s high-end SoCs support several hundred use-cases and manually checking whether the design constraints of the individual use-cases are satisﬁed by the NoC is a tedious process. Moreover, if the NoC design for the use-cases is carried out individually, it is difﬁcult to converge to a single NoC design that satisﬁes the design constraints of all the use cases. In this work we motivate the need to consider the design constraints of the individual use-cases during the NoC design process. We present a design method for mapping of cores on to the NoC, considering the NoC conﬁguration (i.e. path selection and resource reservation in the NoC) as sub-problems during the mapping phase, such that the resulting design satisﬁes the constraints of all the use-cases of the SoC. We then present methods to decrease the required network resources by dynamically reconﬁguring the network across different usecases. We also explore the effect of DVS/DFS techniques for reducing the power consumption of the network across the different use-cases. The methods are validated by performing experiments on several SoC designs. I I . PREV IOU S WORK Several researchers have proposed different architectures and design methodologies for the switches, links and Network Interfaces (NI), which are the major components of a NoC [18, 7, 20, 8]. Design ﬂows that automate many of the steps of the design process have been presented in [17, 19]. In [8], the Æthereal architecture that supports Quality-of-Service (QoS) for applications by using Guaranteed Throughput (GT) connections for trafﬁc streams that has bandwidth/latency constraints and by using Best Effort (BE) connections for the remaining trafﬁc streams is presented. The topology selection process and mapping of applications on to NoC architectures have been explored by many researchers. In [10, 11], branch-and-bound algorithms to map cores on to a mesh NoC topology for different routing functions are presented. In [12, 13], design methods and tools for mapping applications on to regular NoC topologies and automating the topology selection process has been presented. In [16], the methods are extended to consider the QoS constraints during the mapping phase. Building application speciﬁc buses and NoC topologies has been presented in [21, 14]. In [15], a tool that automates the combined mapping and NoC conﬁguration steps for the Æthereal is presented. In all these NoC design works, the design methods assume a single set of communication constraints, which is obtained either for a single application or is obtained from a single trace for multiple applications. In the RAW chip-multiprocessor, the interconnection network connectivity is reconﬁgured with the assistance of the compiler [23]. In the FLEXBUS architecture [22], the authors present methods to dynamically remove the overhead of bridges in multi-bus communication and provide methods Multiple Use cases (design constraints,communication patterns) UC1 UC2 UC3 ... UCn Topology Generation Mapping Path Selection Slot Table Allocation NoC Configuration Traffic Generation RTL Synthesis & Back End NoC Performance Verification SystemC & RTL VHDL NoC Simulation Fig. 2. Design Flow for NoCs Fig. 3. Example input ﬁle with design constraints for an MPEG application where a core can be connected to different buses dynamically. I I I . TH E U S E -CA S E CENTR IC D E S IGN FLOW In this section we present the NoC design ﬂow with the support for multiple use-cases integrated in to the ﬂow (Figure 2). The NoC design ﬂow and the mapping algorithms for the NoC for a single use-case were presented in [15, 17]. In this work, we extend the tool chain to support the multiple use-case scenario that is commonly encountered in SoCs. The communication design constraints for the different use-cases of the SoC are input to the design ﬂow in the excel and xml ﬁle formats. The communication design constraints for each use-case includes the required bandwidth for various connections between the cores in the use-case, the maximum latency allowed for the connection, the QoS level required for the connection (like GT or BE), etc. An example fragment of the input ﬁle is presented in Figure 3. With the different use-cases as input, in the ﬁrst two phases of the design ﬂow, the topology exploration and mapping of the use-cases on to the NoC are performed. The NoC conﬁguration phase in which path selection and TDMA slot-table allocation (required for the GT trafﬁc) are performed, is integrated with the mapping phase. The RTL level VHDL and SystemC models for the resulting NoC conﬁguration are then automatically generated, which can then be simulated. The performance of the NoC can also be veriﬁed in parallel by the automatic performance veriﬁer, which analytically checks whether the design constraints are met. The extension of the tool chain to support multiple use-cases is performed in a modular fashion without affecting most of the existing ﬂow. As the multi-usecase NoC design methods are integrated with the tool chain, performance validations of the methods can be easily carried out to analyze the efﬁciency of the design methods. IV. TH E MA P P ING ALGOR ITHM In this section, we ﬁrst present the mapping algorithm for a single use-case and then present methods to extend the algorithm for multiple use-cases. A.MappingAlgorithmforsingleuse-case The mapping algorithm for a single use-case is presented in detail in [15]. In this sub-section, we only present a brief version of the algorithm highlighting the major phases. As in general, graph mapping is a NP-Hard problem [10, 13], a heuristic algorithm is used to perform the mapping. The selection of paths for the different trafﬁc ﬂows and the reservation of TDMA slot-table entries for the GT trafﬁc ﬂows are uniﬁed with the mapping process. The mapping algorithm is presented in Algorithm 1. At the outermost level of the algorithm, a NoC topology is generated. In the outer loop, the size of the topology is increased until a feasible mapping is obtained in the subsequent phases. Initially, all the cores of the SoC are unmapped. In the ﬁrst step of the mapping algorithm, the trafﬁc ﬂows between the communicating cores are sorted in a non-increasing order. Then for each ﬂow in the order, the source and destination cores of the ﬂow, if they are not already mapped, are mapped on to the NoC. When performing the mapping of these cores, the path with the least cost that satisﬁes the bandwidth and latency constraints for the ﬂow is chosen and the cores are mapped to the NIs in the path. A path is assumed to originate from a NI, traverse one or more switches and terminate in a NI. The cost of the path is a combined metric that considers an afﬁne combination of the latency and bandwidth requirements for the ﬂow. The slot-table reservation for the ﬂow is also carried out in this step. The procedure is repeated for all the ﬂows in the SoC. The approach also takes in to account the possibility of multiple cores sharing a single NI for communication. Note that once the initial mapping step is performed, the solution space can be explored by considering swapping of vertices using simulated annealing or tabu search, as performed in [16]. Algorithm 1 Mapping Algorithm for a single use-case OUTERLOOP: Generate a NoC topology. 1. Sort the trafﬁc ﬂows between the cores in a nonincreasing order of the bandwidth requirements. 2. For each ﬂow in order: a. Choose a least-cost path for the ﬂow that satisﬁes the bandwidth, latency constraints. b. If the source or destination cores of the ﬂow are not yet mapped, map them on to the NIs in the path. c. Reserve the required bandwidth across the ports and reserve the slot-table entries for the ﬂow. If the resulting mapping violates design constraints, increase the size/resources of the topology and go to OUTERLOOP. We refer the interested reader to [15] for the time complexity of the algorithm, details of path selection, other optimizations carried out and for the performance evaluation of the algorithm for several SoC designs. B.MappingDesignApproachforMultipleUse-cases When the SoC has multiple use-cases, we assume that all of the use-cases utilize the same mapping of cores on to the NoC components. This is because, if each individual use-case has a different mapping, then each core potentially needs to be connected to several different NIs, which may not be feasible because of physical layout restrictions and wiring complexity. A direct extension of the single use-case mapping algorithm to support multiple use-cases would be to perform the mapping for the most communication intensive use-case and reuse the mapping for the other use-cases. However, as the design constraints of the use-cases can be very different, such a method may result in a mapping that does not satisfy the performance constraints of many of the use-cases. As an example, when such an approach is applied to the SoC considered in section I, the resulting NoC design satisﬁes only 2 of the 4 use-cases. We use the following design method to extend the mapping design procedure for multiple use-cases (Figure 4(a)). In order to obtain a mapping that satisﬁes all the use-cases, we construct a synthetic Worst-Case (WC) use-case from the given set of input use-cases. For the communication ﬂow between every pair of cores, the maximum required bandwidth values and the minimum required latency values for the ﬂow across all the use-cases are selected and used in the WC use-case. A small example is presented in Figure 4(b). Thus the design constraints of all the individual use-cases are subsumed in the WC use-case and any NoC design that satisﬁes the constraints in the WC use-case will satisfy the constraints of each individual use-case. The WC use-case is then used for the mapping process. Due to the manner in which the WC use-case is constructed, the selected paths and slot-table allocations from the mapping process will satisfy the design constraints of each individual use-case. Once the mapping is obtained from the WC use-case, we perform an optimization step, where we ﬁx the mapping that is obtained from the WC use-case, but re-run the path selection and slot-table reservation phases individually for each usecase. We perform this for two reasons. First, the WC use-case had the worst case constraints from each use-case and by rerunning the path selection and slot-table reservation steps and choosing the maximum values from the individual use-cases we can reduce the NoC resources, while still satisfying the constraints of all the use-cases (experimental evidence presented in Section VI A). Second, when the conﬁguration time between the use-cases is large, the frequency and voltage of operation of the NoC can be scaled to match each individual use-case, which can result in signiﬁcant power savings for the system. In general, when the paths and slot-tables used by the different use-cases are different, we need mechanisms to store them in memory and load them on to the network dynamically or compute them on the ﬂy for the use-case. This is explored in the next section. V. DYNAM IC RECON FIGURAT ION O F TH E NOC For most SoC designs, when the system switches between use-cases, some conﬁguration time is needed for loading the Multiple Use cases UC1 UC2 UC3 ... UCn Generate the worst case (WC) use case Topology generation and Mapping Path Slot Table Selection Allocation Opimize path selection,  slot table allocation for  each use case Integrate DVS/DFS techniques NoC generation <160 MB/s, 2900 cycles> Core 1 UC1 core 3 < 1 5 0 M Core 2 B /s, 3 0 0 0 c y cles > <170 MB/s, 3000 cycles> Core 1 B /s, 2 8 0 0 c y cles > UC2 Core 2 core 3 < 1 6 0 M <170 MB/s, 2900 cycles> Core 1 WC12 Core 2 B /s, 2 8 0 0 c y cles > core 3 < 1 6 0 M (a) Multi use-case mapping design ﬂow (b) Example of WC use-case constraints generation Fig. 4. Multi use-case mapping ﬂow and WC use-case generation new use-case. This is mostly attributed for loading the use-case data and code, sending control signals to different parts of the design and for gracefully shutting down the already running use-case. In many designs, this use-case switching time is of the order of few milli-seconds. This conﬁguration time can be utilized by the NoC for switching to a different path and slot-table allocation for the mapping. This time delay can also be utilized to vary the clock frequency/voltage of the NoC to match the use-case performance level. In the Æthereal architecture [8], a static path routing scheme is used, where the paths are selected at the source NI of the trafﬁc ﬂow. Thus, the NIs maintain the path and slot-tables for the various connections. When the paths and slot-tables used by the NIs vary across different use-cases, the tables need to be stored in memory. As the on-chip memory available is mostly limited and as the use-case switching time is large, we use the off-chip memory to store the paths for the different use-cases. We investigated the overhead for the reconﬁguration mechanism for the set-top box SoC. The amount of data required to store the path and slot-table information for each use-case is around 560 Bytes. With 4 use-cases, the memory requirement for the reconﬁguration mechanism is 2.24 KB. The time required to load the data from the memory and spread it around the NoC for an use-case is of the order of micro-seconds and the energy dissipation is of the order of micro-Joules. Using traditional mechanisms to scale the frequency and voltage of the system may require few milliseconds for conﬁguration. Thus we can envision three different ways of NoC operation. First, when the use-cases that run on the SoC switch very frequently or when the initial conﬁguration times are not acceptable (as in real-time use-cases), the different use-cases can use the WC use-case conﬁguration. In this conﬁguration, all the use-cases will use the same set of paths and slot-table allocations, thereby not requiring the NoC to be re-conﬁgured when the use-cases switch, resulting in seamless switching between the use-cases. However, this leads to an over-design of the network when compared to the scenario where the NoC is reconﬁgured to suit the individual use-cases. Second, when the use-case switching is not that frequent, the NoC conﬁguration (path and slot-tables) can be changed dynamically across usecases, leading to a smaller NoC design (in terms of network components or frequency of operation). Third, when the usecases are expected to run for a long time, the voltage or frequency of operation of the NoC can be varied to match the use-cases, resulting in large power savings for the system. The simulation results for these cases are presented in the next section. V I . S IMU LAT ION RE SULT S We present simulation results on applying the multi-usecase design procedure on to 4 different SoC designs: P1 (with 2 use-cases), P2 (2 use-cases), P3 (4 use-cases) and P4 (8 usecases). The designs P1-P3 are simpliﬁed versions of set-top box SoCs [9] and the design P4 is a video processing SoC used in TVs. Each use-case has a large number of (50 to 150) communicating pairs of components. A fragment of two of the use-cases used in the P3 design was presented earlier in Figure 1. The set-top box SoCs and the TV processor have different functionalities and communication patterns. The designs P1P3 use an external memory for storing and retrieving data and the amount of data communicated to the memory is very large when compared to the rest of the design. The P4 design uses a streaming architecture with local memories on the chip, there by distributing the communication load across several components. We apply our design method to these SoCs with different architectures to validate the generality of the methods. A.EffectofMappingontheNoCFrequency To evaluate the mapped designs, we ﬁxed the topology and the maximum slot-table size for each design and we found the WC12 UC1 UC2 0 50 100 150 200 250 300 Use Cases R u q e i r F d e r y c n e u q e ( i n M H z ) (a) Design P1 WC12 UC1 UC2 0 100 200 300 400 Use Cases R u q e i r F d e r y c n e u q e ( i n M H z ) (b) Design P2 WC1234UC1 UC2 UC3 UC4 0 100 200 300 400 500 Use Cases R u q e i r F d e r y c n e u q e ( i n M H z ) (c) Design P3 WC UC1 UC2 UC3 UC4 UC5 UC6 UC7 UC8 0 50 100 150 200 Use Cases R u q e i r F d e r y c n e u q e ( i n M H z ) (d) Design P4 Fig. 5. The NoC operating frequencies required to support the different use-cases for the various designs. The WC use-case values are obtained when the path selection and the slot-table reservation are based on the WC use-case. The other values show the effect of re-applying the path selection and slot-table reservation for each use-case with the mapping obtained from the WC use-case. WC1234UC1 UC2 UC3 UC4 0 20 40 60 80 100 120 Use Cases S l o t e b a T l S i e z (a) Slot Table Size WC1234UC1 UC2 UC3 UC4 0 0.2 0.4 0.6 0.8 1 Use Cases N o r m a i l d e z N o A C r a e (b) NoC Area UC1 UC2 UC3 UC4 0 100 200 300 400 Use Cases R u q e i r F d e r y c n e u q e ( i n M H z ) (c) P3 individual mappings UC1 UC2 UC3 UC4 UC5 UC6 UC7 UC8 0 50 100 150 Use Cases R u q e i r F d e r y c n e u q e ( i n M H z ) (d) P4 individual mappings Fig. 6. (a)-(b) The effect of the mapping design procedure on the slot-table size and NoC area, (c)-(d) NoC frequency requirements for individual mapping of use-cases in designs P3 and P4. minimum frequency of operation required by the NoC to support the different use-cases. The results of the mapping procedure for the 4 SoC designs are presented in Figure 5. The frequency of operation required for the WC use-case is obtained from applying the NoC conﬁguration (i.e. the path selection and the slot-table reservation) procedure on the WC use-case. The frequency of operation of the NoC after reapplying the conﬁguration procedure for each of the use-cases, ﬁxing the mapping from the WC use-case is also presented in the ﬁgures. When DVS/DFS techniques are not used and a single frequency of operation is used for all the use-cases, we need to take the maximum of the frequencies of each of the individual use-cases as the operating frequency of each design. In this case, re-applying the NoC conﬁguration step results in 9% to 38% reduction in the required NoC operating frequency across the different designs. A lower operating frequency implies lower power consumption and smaller impact of noise sources. In the above analysis, we assumed that the slot-table size is ﬁxed and the NoC frequency is varied to support the use-cases. We also explored the effect of ﬁxing the NoC frequency (at 500 MHz) and varying the slot-table size. As similar results were observed for all the designs, we only present the results for the P3 design (Figure 6(a)). We obtain 58% reduction in the slot-table size for the design by re-applying the NoC conﬁguration step. A smaller slot-table size usually corresponds to a lower area for the NoC and lower packet latencies (as the trafﬁc streams wait lesser to get the slots). The NoC area reduction due to the slot-table reduction (Figure 6(b)) is 10% for this design (the NoC area includes the area of the switches and the network interfaces). Trade-offs involving the frequency savings and area savings can also be explored. B.Comparisonswiththeindividualuse-casemappings To evaluate the optimality of the NoC design produced by the above method, we performed individual mappings for each of the use-cases in the P3 and P4 designs. The required NoC frequencies for the use-cases in the resulting designs are presented in Figures 6(c) and 6(d). These frequency values are the minimum possible values for the use-cases, as we have done the mappings individually and they provide a lower bound on the quality of solutions that can be obtained when all the usecases share the same mapping1. When the results from Figures 6(c) and 6(d) are compared with the results from Figures 5(c) 1Note that the heuristic nature of the mapping algorithm can sometimes invalidate this general statement.                                             WC UC1 UC2 0 0.2 0.4 0.6 0.8 1 Use Cases N o r m a i l d e z P o w e r C u s n o m p i t n o (a) Design P1 WC UC1 UC2 0 0.2 0.4 0.6 0.8 1 Use Cases N o r m a i l d e z P o w e r C u s n o m p i t n o (b) Design P2 WC UC1 UC2 UC3 UC4 0 0.2 0.4 0.6 0.8 1 Use Cases N o r m d e z a l i P o w e r C u s n o m p i t n o (c) Design P3 WC UC1 UC2 UC3 UC4 UC5 UC6 UC7 UC8 0 0.2 0.4 0.6 0.8 1 Use Cases N o r m a i l d e z P o w e r C u s n o m p i t n o (d) Design P4 Fig. 7. Effects of DVS/DFS. and 5(d), we ﬁnd that the multi-use-case mapping design procedure results in mappings that require operating frequencies that are with in 10% of the minimum possible operating frequencies. We also performed experiments ﬁxing the frequency of operation for the multi-use-case mappings to be the same as the individually mapped designs and varied the network resources needed to support all the use-cases in the designs. The multi-use-case mappings required slightly more resources (1%-10% increase in the NoC area) to support the same frequency of operation as the individual mappings. C.EffectofDVS/DFS When the frequency of the NoC is scaled to match the frequencies required for the individual use-cases, large power savings can be achieved. As the frequency of the network is scaled, the supply voltage required for operation can also be scaled to match the frequency. We use a conservative model for voltage scaling, where we assume that the square of the voltage scales linearly with the frequency [24]. The power savings achieved by the DVS/DFS techniques for the entire SoC platform depends on the amount of time each use-case is expected to run. Thus, in this experiment, we present the power savings achieved for each use-case of the platform separately. The power consumption of each of the use-cases, normalized with respect to the power consumption of the WC use-case is presented in Figure 7. On average, we obtain 59.21% power savings by using the DVS/DFS techniques across the different use-cases for the designs. V I I . CONCLU S ION S As the number of applications or use-cases integrated on to a single SoC increases, the designer is faced with the challenge of building an interconnect structure that supports the design constraints of all the use-cases. In this paper we motivated the importance of the problem and presented use-case centric design methods to map applications on to NoC architectures. We also presented a way to dynamically conﬁgure the interconnect to support multiple use-cases and integrated Dynamic Voltage and Frequency (DVS/DFS) techniques with the reconﬁguration mechanism. In future, we plan to extend the algorithms for supporting concurrent operation of use-cases and apply the use-case models for addressing other NoC design issues such as the application speciﬁc topology design. "
2006,The design and implementation of a low-latency on-chip network.,"Many of the issues that will be faced by the designers of multi-billion transistor chips may be alleviated by the presence of a flexible global communication infrastructure. In the short term, such a network will provide scalable chip-wide communication and ease the complexity of handling multi-cycle communications. In the long term, the network will become a primary tool for optimising power and data transfers and for scheduling computations. This paper details the design and implementation of a low-latency on-chip network. The network's speculative routers are in the best case able to route flits in a single clock cycle, helping to minimise on-chip communication latencies and maximise the effectiveness of buffering resources. Results from our 180nm test chip demonstrate an inter-router data transfer rate in excess of 16Gbit/s for each link. In the best case each router hop adds just 1 clock cycle to the final communication latency.","The Design and Implementation of a Low-Latency On-Chip Network Robert Mullins, Andrew West and Simon Moore Computer Laboratory, University of Cambridge Robert.Mullins@cl.cam.ac.uk Abstract— Many of the issues that will be faced by the designers of multi-billion transistor chips may be alleviated by the presence of a ﬂexible global communication infrastructure. In the short term, such a network will provide scalable chip-wide communication and ease the complexity of handling multi-cycle communications. In the long term, the network will become a primary tool for optimising power and data transfers and for scheduling computations. This paper details the design and implementation of a low-latency on-chip network. The network’s speculative routers are in the best case able to route ﬂits in a single clock cycle, helping to minimise on-chip communication latencies and maximise the effectiveness of buffering resources. Results from our 180nm test chip demonstrate an inter-router data transfer rate in excess of 16Gbit/s for each link. In the best case each router hop adds just 1 clock cycle to the ﬁnal communication latency. I . IN TRODUC T ION Transistor switching speeds are continually improved through scaling. Unfortunately, the impact of scaling on long wires is a negative one. This forces an increase in communication latencies and the energy required to communicate each bit of information. The growing disparity between communication and switching times will soon make the provision of a chip-wide communication infrastructure a central problem in achieving performance and power dissipation goals. The resulting shift in design trade-offs will lead to an era of “communication-centric” system design. While constant length global wires fail to scale well, the distance reachable in a single clock cycle in multiples of λ does remain essentially constant [7]. This allows the performance of designs of ﬁxed complexity to scale when ported to the next technology node. This observation leads to the concept of a scalable architecture composed of a number of tiles or modules of ﬁxed complexity. The performance of each tile scales as expected and additional performance is possible by adding tiles as scaling permits. Inter-tile communication is handled by an on-chip network which consumes only a few percent of the total chip area. The way in which such a system could scale is illustrated in Table I. In this example the die size is assumed to be ﬁxed at 256mm2 . Each tile contains around 11M transistors and the clock period is set to 16 FO4 delays. The table shows how the frequency, size (width) and number of tiles scale. The interconnect delay along one edge of a tile remains constant at around one clock cycle. delay may be calculated as 500ps ∗ Lgate (where Lgate is the The calculation of cycle time in Table I assumes one FO4 physical gate length as speciﬁed in [13]). The channel delays were estimated using results from [1], [7]. In all cases it should Technology Node 90nm 65nm 45nm 32nm No. of Tiles Width of Tile 32 2.8mm 64 2mm 128 1.4mm 256 1mm Tile Frequency 3.4GHz 5.0GHz 7.0GHz 13.0GHz PR ED IC T ED SCA L ING O F A G EN ER IC T I L E -BA S ED SY S T EM TABLE I be possible to traverse the channel between two routers in less than one clock cycle. Of course, if a longer clock period is employed a smaller number of larger tiles may be used. Such tile-based systems may implement arrays of homogeneous processor/cache tiles [9], [10], ﬁner-grain computing fabrics [14] or networks of heterogeneous IP blocks. Such approaches provide highly reconﬁgurable platforms for a wide range of performance hungry applications. The provision of an efﬁcient chip-wide dynamic on-chip network is fundamental in achieving performance goals, ﬂexibility and mitigating complexity in such systems. Packet-switched networks employing Virtual Channel (VC) ﬂow control have recently been proposed as one approach to implementing a chip-wide interconnection network [3]. Figure 1 illustrates the major components of a generic virtual-channel router. Packets gain access to a physical channel by ﬁrst obtaining a virtual-channel (VC allocation). Each of these virtual-channels has its own private input FIFO at the destination router allowing ﬂits1 from different packets to be sent in an interleaved manner. Access to a physical channel is now allocated on a cycle-by-cycle basis (switch allocation) amongst waiting ﬂits from any of the buffered packets which have been assigned a VC. This scheme improves both throughput and latency when compared to a simple wormhole routed network by allowing blocked packets to be bypassed. Particular classes of trafﬁc may be restricted to a subset of the available virtual-channels in order to provide QoS enhancements or circumvent message-dependent deadlocks. I I . S P ECU LAT IV E ROU T ER ARCH I T EC TUR E S The description of virtual-channel ﬂow control in Section I implies that VC allocation and switch allocation are performed sequentially. Peh and Dally [12] describe how this dependency may be relaxed if we speculate that a waiting packet will be successful in acquiring a VC. In this way both VC and switch allocation may be performed in parallel. In order to avoid a negative impact on performance, the 1A packet is composed of a number of ﬂits (ﬂow-control digits) Input Channel credit out Input Channel credit out VC identifier Routing Logic VC Allocator credits in V Switch Allocator VC Buffer Output Channel Output Channel Crossbar (P x P) Input Port Fig. 1. A Virtual-Channel Router switch allocator must prioritise non-speculative requests over speculative ones. This is achieved by implementing two switch allocators: one handling non-speculative requests from packets which have been allocated a VC and one for requests from packets awaiting VC allocation. We will refer to these as the high and low priority switch allocators respectively from this point onwards. Speculative requests are only granted for a particular output when no regular requests are present. In the case that a speculative request is granted we must ensure that the VC has in fact been allocated and buffer space exists downstream. Fortunately, such checks may be performed in parallel with crossbar traversal. I I I . S ING L E -CYC L E ROU T ER S The introduction of further speculative optimisations to reduce the router pipeline depth to a single pipeline stage was proposed in [11]. These optimisations almost completely remove any control overhead from the critical path. Both VC and switch allocation are now performed concurrently with the transport of ﬂits across the datapath and physical channel. The ability to make such optimisations is based on the following observations: if we assume that the network is heavily loaded it should be possible to make scheduling decisions accurately one clock cycle in advance. This is because all the information necessary to make such a decision is present when many packets are buffered. At the other extreme, when the network is very lightly loaded, we may assume that contention for a VC or physical channel is low. In this case it is also possible to schedule one cycle in advance by speculating that any new request for a VC or physical channel may be granted immediately. Simulation results predicted that for all intermediate throughputs the router sacriﬁces only a few percent of performance over a perfect single-cycle sequential scheme [11]. Figure 2 provides an outline of the single-cycle router architecture. In this scheme VC and switch allocation is effectively performed one cycle in advance and concurrently with the transport of ﬂits. Each allocator’s output is a set of grant-enable signals which are registered and used on the succeeding clock cycle to generate VC and switch allocation grant signals. The presence of buffered ﬂits allow resources to be scheduled one cycle in advance, in this case the asserted grant-enable signals correspond to the subset of requests to be granted on the next clock cycle. If it is not possible to schedule a particular VC or output in advance, a prediction is made that there will be only one subsequent request for the resource. In this case multiple grant-enable signals are set. This allows any request on the next clock cycle, for the resource in question, to be successful. Cases where multiple requests are made on the following clock cycle are detected by the abort logic described in Section III-A. Datapath control signals are produced early in the clock cycle by the “fast” logic blocks, simply by combining the output port requests from each buffered (or newly arrived) ﬂit and the registered grant-enable signals. The output port required by each ﬂit is known without the need to ﬁrst evaluate a routing function by performing this task in the previous router (look-ahead routing [6]). A. Abort Detection One issue which must be considered carefully is the case when our prediction that requests on the next clock cycle will not contend is subsequently proven false. Fortunately, to detect these abort cases we only need consider newly arrived ﬂits. If ﬂits were buffered on the previous clock cycle, speculation would not have been necessary. The abort logic associated with both VC and switch allocation consists mainly of a comparison between each of the output ports required by each new ﬂit2 . If we assume there are P -input ports this requires P (P − 1)/2 comparisons. The abort logic detects cases where we are speculating and two or more ﬂits requiring the same output port resource (physical link or VC) have arrived simultaneously. In these cases the allocation of the resource is blocked incurring a one clock cycle penalty. The correct scheduling of the resource takes place during this time and non-speculative grant-enable signals are generated for use on the following clock cycle. In order to use the simple abort logic described above some additional logic is required to account for one remaining corner case. This is the scenario when a tail ﬂit leaves an input buffer and exposes a new packet (buffered head ﬂit). As this packet can now request any output port it is possible it will contend with another such packet or newly arrived ﬂit. The solution adopted for such cases is to always stall such head ﬂits for one cycle to ensure they are handled properly by the switch and VC allocation logic and need no further special treatment. This has a negligible impact on performance. B. Calculating the next set of requests To enable the VC and switch allocators to produce accurate grant-enable signals for the next clock cycle they may be fed a set of requests that we know will be present on the next clock cycle. These may be calculated by considering available 2At most one new ﬂit may be received at each input port per clock cycle Switch Allocation Output VC status (blocked?) switch grant enables Switch Fast Switch Next Current switch allocation requests (A simple implem. passes the current reqs.) Switch Allocator High Priority Switch Allocator Low Priority Switch Allocator Speculatively Allocate If Necessary Next Buffered Flit’s Output Port, VC Alloc. Abort Signals, Blocked VCs Virtual Channel Allocation VC grant enables VC Fast VC Next Current VC alloc. requests (masked by NextFreeVC) Virtual Channel Allocator Speculatively Allocate If Necessary Switch Control Output VC status (blocked?) Output Port Channel Level Flow Control FSM NextFreeVC Free VC FIFO Tail Flits Return VC ID to Free List Permit? channel flow control signals Output Channel Input Port valid VC ID Reg. VC Flit Buffer Newly Allocated VC IDs input channel channel flow control signals New Flit’s output port requests New Flit’s VC IDs Where is  speculation taking place? Crossbar FIFO Empty? Abort Detection Flit Kill Logic Output VC status (blocked?) Newly Allocated VC IDs Fig. 2. A single-cycle speculative virtual-channel router architecture. When necessary the router is able to speculate that ﬂits arriving on the next clock cycle may be routed without contention. During switch allocation the router is also able to speculate on the successful acquisition of VCs by new packets and on the availability of buffer space at the ﬂit’s destination. information such as the current requests and those granted on the current cycle. Information about the next buffered ﬂit in each VC buffer may also be exploited. To ensure that the abort logic is the only place where we need to handle mispredictions, it is important that the set of requests output by the next request logic contains at least the requests from those ﬂits already buffered. Presenting additional requests, e.g. those granted on the current cycle, may reduce performance but will not cause the router to malfunction. If requests that are to be made by buffered ﬂits are not considered, grant-enable signals may be set speculatively enabling multiple buffered ﬂits to gain access to the same output (or VC). As only newly arrived ﬂits are considered by the abort logic, this problem would go unchecked. In the ﬁnal router implementation we chose to accurately calculate VC next requests using all the information available. In the case of the switch scheduler we simply used the current set of requests to schedule the switch for the next cycle. This provided a signiﬁcant improvement in cycle time with a small architectural performance penalty (see comparison between spec-fast and spec-accurate in Section V). The simpliﬁcation is aided by the fact that those switch requests recently granted have a low arbitration priority. C. Pipelining the use of VC state The use of VC status information provides an example of how internal control paths may be pipelined with only minor changes to the architecture. In order to reduce cycle time it was advantageous to pipeline the VC status data used by the switch allocation logic. By adding a pipelining register, the information provided to the switch allocator about which VC is blocked becomes more out-of-date. In order to ensure the quality of the switch schedule does not suffer signiﬁcantly the availability of both high- and low-priority switch allocators is exploited. If a request is associated with a VC that appears to be blocked it is steered to the low-priority allocator. Actual VC blocked status is checked when the ﬂit is selected for transport (in parallel with its journey to its output port). This sort of modiﬁcation is simpliﬁed by the way in which the architecture decouples scheduling from the datapath. The allocator’s task is simply to provide the best schedule it can for the next clock cycle with the information available. Final checks on the validity of the schedule are delayed until the schedule is applied. If advantageous, further pipelining of the control logic internally could be exploited without compromising the best case single cycle routing latency. This could involve further pipelining of the allocators themselves. IV. IM P L EM EN TAT ION The Lochside test chip consists of 16 trafﬁc generating tiles interconnected by a 4x4 mesh network. The chip is implemented in UMC’s L180 logic process (1.8V core, 0.18µm) with all aluminium interconnect. Tiles and routers are interconnected as shown in Figure 3. Each router is connected to its neighbour using two unidirectional 80-bit channels (64-bits of data and 16-bits of control information). Each of the router’s input ports support 4 virtual-channels and may buffer 4 ﬂits on each virtual-channel. The implementation is fully testable via traditional scan chain techniques. An on-chip PLL may be used to provide a clock source and is distributed to each tile using a simple hand-crafted H-tree. Alternatively, a Distributed Clock Generator (DCG) [5] may be selected as the global clock source. In both cases, tile level clock distribution was achieved by running a standard-cell clock tree synthesis tool. The vast majority of the design is implemented in a standard cell style. Exceptions include the DCG nodes and latch-based virtual-channel buffers which beneﬁted from a full-custom implementation. The ﬁnal router design was generated from a highly parameterised network router model that allows a range of router designs to be synthesized. The performance of the design is limited by our current PGA package (due to both thermal and bond-wire IR drop issues). This limits the performance when running all trafﬁc generators to around 250MHz. If only two random packet sources are enabled the maximum clock rate may be increased to 300MHz. Once a ﬂit is received at a router’s input port it may be allocated a virtual-channel and access to an output port, traverse the crossbar and arrive at the destination router in a single clock cycle (best case latency is simply one cycle per hop). At 250MHz each router is able to transfer data at a maximum rate of 16Gbits/s on each input and output link. V. R E SU LT S Each tile’s trafﬁc generator is able to produce a wide range of trafﬁc patterns. Trafﬁc destinations may be selected randomly or deterministically with control over packet length. Error detecting code and ﬂit ordering checks are also performed at each tile. Each tile maintains statistics on the number of packets sent and received, together with the timing information necessary to calculate average latency and throughput. Each tile is able to inject trafﬁc at a controlled rate into a tile output queue. Packets injected into this queue when it is full may be counted. The conﬁguration system also provides the necessary control logic in order to synchronise the execution of commands at each tile, e.g. in order to start and stop all tiles simultaneously. Figure 5 shows the recorded average packet latency versus measured throughput for our 4x4 mesh network. For each experiment 16K packets were sent to uniformly distributed destinations from each tile. Curves are plotted for a range of ﬁxed packet lengths. Experiments which resulted in the tile output queue becoming full and generated packets being dropped are not plotted. A. Performance The router was synthesized to operate at 200MHz under worst case PVT operating conditions (around 35 FO4 including clocking overhead). If the speculative scheduling optimisations are removed but VC and switch allocation is still performed in parallel, the clock period is extended by a factor of 1.65. It may be noted that the optimised router does 64+16 bit Channel Traffic Generator Router Pull−Up DCG Node Pull−Down DCG Node Distributed Clock Generator (DCG) Network Fig. 3. Block Diagram of the Lochside Chip ’sequential’ ’spec-accurate’ ’spec-fast’  30  25  20  15  10  5 ) l s e c y c k c o c l t s a f c e p s n i ( y c n e t a L e g a r e v A  0  0  0.05  0.1  0.15  0.2  0.25  0.3 Throughput (flits/node/clock cycle)  0.35  0.4  0.45 Fig. 6. Latency/Throughput comparison for three router architectures. Latency is scaled to account for differences in each router’s cycle time. 256-bits (4 ﬂits). In our ﬁnal design the switch allocation critical path is composed of the following delays: input register, steering and buffering switch request to allocator, (53%) switch allocation, (15%) selecting speculative or non-speculative switch allocator result and speculatively setting grant-enables if necessary. The performance achieved by this implementation closely tracks that predicted by earlier router simulation models. (32%) B. Area A 4x4 mesh network is not really practical for the size of our test chip or its technology. The small tile size is dominated by the area of each router (more than two thirds of a tile’s area is taken by the network). However, if we move to the next technology node (130nm) and imagine a larger chip (4x4 array of 3mm x 3mm tiles) synthesis results have shown that the network area overhead is reduced to only 5 − 6%. This overhead would drop even further if the scaling in Table I was adopted. The area overhead of the speculative single-cycle architecture is small at around 8%. This compares the area of two single cycle routers one with our optimisations and one without. If the unoptimised case was pipelined, the difference in area would fall as registers would be required to buffer intermediate results. V I . R E LAT ED WORK Our implementation compares favourably to other on-chip network designs and implementations published to date. Comparable networks which have been implemented include the Philips Æthereal network-on-chip [4] and the RAW processor’s dynamic networks [14]. The Philips team report a similar peak link bandwidth of 16Gbit/s while operating at 500MHz in a 130nm process. Control decisions are actually taken at 166MHz. The router differs from ours in its ability to offer guaranteed services by reserving consecutive Fig. 4. Lochside Die Micrograph. Die size is 5mm x 5mm. The chip contains approximately 5 million transistors. ’8’ ’6’ ’4’ ’2’ ’1’ Packet Length  30  25  20  15  10  5 ) l s e c y c k c o c l ( y c n e t a L e g a r e v A  0  0  0.05  0.1  0.15  0.2  0.25 Throughput (flits/node/clock cycle)  0.3  0.35  0.4 Fig. 5. Latency versus throughput measured from test-chip for a range of packet lengths. not quite achieve a speedup equal to the reduction in clock cycle time. This is due to our router producing a slightly inferior routing and VC allocation schedule as a result of the approximations exploited to reduce cycle time. It is not, as may be expected, directly as a consequence of VC and switch aborts. The number of aborts is in fact consistently very low. Overall our speculative scheduling optimisations reduce average communication latency by a factor of 1.3 to 1.6. Figure 6 plots latency against throughput for three router architectures: (spec-fast) full speculation with no switch next request logic, (spec-accurate) full speculation with accurate switch next request logic and (sequential) concurrent switch and VC allocation followed by switch traversal in the same clock cycle. Each architecture was synthesized to calculate its minimum clock period and the latency ﬁgures scaled to account for these differences. The clock period results were, 30, 41 and 50 FO4 delays respectively (including 2 FO4 of clock uncertainty). The packet length in these experiments was                 routing slots in consecutive routers. Virtual-channels are not supported for improving the performance of best-effort trafﬁc. RAW’s dynamic networks operate at 225 MHz (worst-case PVT). In this case the whole network is duplicated in preference to exploiting virtual-channels. The RAW processor was implemented using IBM’s 180nm 6LM ASIC copper process Other research projects include Netchip [8] which aims to automatically generate application-speciﬁc on-chip networks. The authors emphasise the need to maintain a high switch operating frequency and adopt a deeply pipelined architecture (7-stage router). Unfortunately this signiﬁcantly increases buffering requirements by extending round-trip time. It also incurs a signiﬁcant overhead in terms of the additional pipelining registers required. Even if a very short clock period of less than 10 FO4 is possible, best case communication latencies would still be more than double that of our current single cycle design. A study of virtual-channel router implementations undertaken by Peh and Dally [12] suggests that an on-chip network typically requires 3 pipeline stages operating at a clock frequency of 20 FO4. While our actual switch and VC allocator implementations offer improvements over the published delay models, clocking and test overheads and internal buffering delays extend our clock period to around 35 FO4 in the ﬁnal implementation. Improvements to the input port logic to reduce this delay are ongoing. Even at 35 FO4 our network’s best case latency would be nearly half that of their reported pipelined design. A. Global Synchronisation The speculative techniques at the heart of our router exploit the presence of a global clock. Global synchronisation offers regular snapshots of state and ensures the system proceeds in a deterministic fasion. This simpliﬁes the implementation of the speculative scheduling mechanisms and ensures abort detection and handling mispredictions is relatively simple. The cost of providing a low-skew high-frequency global clock is in both its complexity and the power it consumes. In many designs this cost may be considered to be too high. This has prompted asynchronous on-chip interconnect techniques to be investigated [2], [15]. While such approaches are promising, it is also possible to make similar trade-offs while retaining a synchronous router implementation. The ﬁrst approach is to exploit known relationships between router clock signals while relaxing global synchronisation. Examples include source-synchronous communication and the use of clock predictive synchronisers. Global synchronisation may be relaxed further by generating clock pulses locally on demand or in a data-driven manner. This allows each router to operate at a rate dictated by the data it is transporting. This both reduces synchronisation overheads and provides a simple high-level approach to clock gating. Work in this area is ongoing. Techniques such as the DCG [5] may also be employed as previously discussed. V I I . CONC LU S ION This paper has detailed the design of an on-chip network which can provide an efﬁcient global communications infrastructure for future gigascale ICs. A speculative architecture is able to accurately produce datapath control signals one cycle in advance of their use. This enables both datapath and control logic to operate concurrently providing signiﬁcant latency improvements over previously published work. A number of trade-offs between cycle time and speculation accuracy have also been introduced and evaluated. The optimisations proposed are orthogonal to other well known techniques for boosting performance such as adaptive routing and are independent of the network topology selected. ACKNOW L EDG EM EN T S This work is supported by EPSRC (grant GR/L86326) and the Cambridge-MIT Institute. "
2006,Improving routing efficiency for network-on-chip through contention-aware input selection.,"The performance of network-on-chip (NoC) largely depends on the underlying routing techniques, which have two constituencies: output selection and input selection. Previous research on routing techniques for NoC has focused on the improvement of output selection. This paper investigates the impact of input selection, and presents a novel contention-aware input selection (CAIS) technique for NoC that improves the routing efficiency. When there are contentions of multiple input channels competing for the same output channel, CAIS decides which input channel obtains the access depending on the contention level of the upstream switches, which in turn removes possible network congestion. Simulation results with different synthetic and real-life traffic patterns show that, when combined with either deterministic or adaptive output selection, CAIS achieves significant better performance than the traditional first-come-first-served (FCFS) input selection, with low hardware overhead (<3%)","Improving Routing Efficiency for Network-on-Chip through  Contention-Aware Input Selection  Dong Wu, Bashir M. Al-Hashimi, Marcus T. Schmitz  School of Electronics and Computer Science  University of Southampton  Southampton, SO17 1BJ, UK  e-mail: {dw, bmah, ms4}@ecs.soton.ac.uk channels may request simultaneously the access of the same  output channel, e.g., packets p0 of input_0 and p1 of input_1  can request output_0 at the same time. The input selection  chooses one of the multiple input channels to get the access.  Whilst the impact of the output selection on routing  efficiency has been investigated [5-7], no explicit work has  been reported on the impact of the input selection, which is  the aim of this paper. The main contribution of this paper is  a novel contention-aware input selection (CAIS), as part of  the routing techniques implemented in switches. With CAIS,  each output channel within a switch observes the contention  level (i.e., the number of request from the input channels,  Section III), and transmits this contention level to the input  channel of the downstream switch. During the input  selection within the downstream switch, CAIS chooses an  input channel depending on the contention levels. Input  channels with higher contention levels get higher priority.  CAIS tries to remove possible network congestion by  keeping the traffic flowing even in the paths with heavy  traffic load, which in turn improves routing efficiency.  Experimental results with synthetic and real-life examples  show that CAIS can be combined with either deterministic  or adaptive output selection, and it is capable of decreasing  Abstract - The performance of Network-on-Chip (NoC) largely  depends on the underlying routing techniques, which have two  constituencies: output selection and input selection. Previous  research on routing techniques for NoC has focused on the  improvement of output selection. This paper investigates the  impact of input selection, and presents a novel contention-aware  input selection (CAIS) technique for NoC that improves the  routing efficiency. When there are contentions of multiple input  channels competing for the same output channel, CAIS decides  which input channel obtains the access depending on the  contention level of the upstream switches, which in turn  removes possible network congestion. Simulation results with  different synthetic and real-life traffic patterns show that, when  combined with either deterministic or adaptive output selection,  CAIS achieves significant better performance than the  traditional first-come-first-served (FCFS) input selection, with  low hardware overhead (<3%).  I Introduction As technology scales and chip integrity grows, on-chip  communication is playing an increasingly dominant role in  System-on-Chip (SoC) design. To meet the performance and  design productivity requirements, Network-on-Chip (NoC)  [1-4] has been proposed as a solution to provide better  modularity, scalability, reliability and higher bandwidth  compared to bus-based communication infrastructures. Fig.  1(a) shows a mesh-based NoC, which consists of a grid of  16 cores. Each core is connected to a switch by a network  interface. Cores communicate with each other by sending  packets via a path consisting of a series of switches and  inter-switch links. For each packet, there are several possible  paths, which directly influence the time needed for delivery.  Therefore, the performance of NoC largely depends on the  underlying routing technique, which chooses a path for a  packet and decides the routing behaviour of the switches.  Fig. 1(b) shows a block diagram of a switch with n+1  input channels and output channels interconnected by a  crossbar. In order to route packets through the network, the  switch needs to implement a routing technique. A routing  technique has two constituencies: output selection and input  selection. A packet coming from an input channel may have  a choice of multiple output channels, e.g., a packet p0 of  input_0 can be forwarded via output_0, output_1 and so on.  The output selection chooses one of the multiple output  channels to deliver the packet. Similarly, multiple input  * This work is supported in part by the EPSRC, UK, under  grant EP/C512804, GR/S95770.  Fig. 1. Block diagram of NoC and switch                                                          packet latency significantly compared to the traditional  first-come-first-served (FCFS) input selection. Furthermore,  the synthesis of prototype switches with CAIS shows low  hardware overhead compared to FCFS.  The rest of paper is organised as follows. Related work is  reviewed in Section II. Section III describes the proposed  contention-aware input selection (CAIS) in detail. The  experiment results and switch implementation are presented  in Section IV. Finally Section V gives the conclusion.  II. Related Work  The idea of NoC is derived from large-scale computer  networks and distributed computing [8, 9]. However, the  routing techniques for NoC have some unique design  considerations besides low latency and high throughput. Due  to tight constraints on memory and computing resources, the  routing techniques for NoC should be reasonably simple [7].  Several switch architectures have been developed for NoC  [10-12], employing XY output selection and wormhole  routing (Section III). In [6], a deflective routing technique is  proposed to avoid network congestion by spreading the  traffic over a larger area. It performs output selection based  on the number of packets being handled in the neighbouring  switches. Packets are forwarded to switches with less traffic  load. The routing technique proposed in [7] is similar to [6]  in terms of acquiring information from the neighbouring  switches to avoid network congestion, but uses the buffer  levels of the downstream switches to perform the output  selection. A routing scheme which combines deterministic  and adaptive routing is proposed in [5], where the switch  works in deterministic mode when the network is not  congested, and switches to adaptive mode when the network  becomes congested. All the routing techniques [5-7] focused  on the output selection. The motivation of this paper is to  investigate the impact of input selection and develop a  simple yet effective input selection, aiming to improve the  routing efficiency with low hardware cost.  III. Proposed Technique  Two  input selections have been used  in NoC,  first-come-first-served (FCFS)  input selection [5] and  round-robin input selection [10, 11]. In FCFS, the priority of  accessing the output channel is granted to the input channel  which requested the earliest. Round-robin assigns priority to  each input channel in equal portions on a rotating basis.  FCFS and round-robin are fair to all channels but do not  consider the actual traffic condition. This section presents a  contention-aware input selection (CAIS) as part of the  routing techniques implemented in switches. CAIS performs  more intelligent input selection by considering the actual  traffic condition, leading to higher routing efficiency.  A. Preliminaries  In this paper we consider NoCs with 2D mesh topology  (Fig. 1(a)). Wormhole switching [9, 13] is employed because  of its low latency and low buffer requirement. In wormhole  switching, a packet is divided into flits for transmission. The  header flit contains the routing information, which is used by  the switches to establish the routing path. The remaining flits  simply follow the path in a pipeline fashion. A flit is passed  to the next switch as soon as enough buffer space is  available to store it, even though there is not enough space to  store the whole packet. If the header flit encounters a  channel already in use, the subsequent flits have to wait at  their current locations and are spread over multiple switches,  thus blocking the intermediate links.  The proposed contention-aware input selection (CAIS,  Section III.B) has been combined with an output selection,  either deterministic or adaptive [5], to complete the routing  function. In this paper, the XY routing [9] is used as a  representative of deterministic output selection for its  simplicity and popularity in NoC. In the XY output selection,  packets are sent first along the X dimension then along the Y  dimension. For example, considering the NoC of Fig. 1(a), a  packet from (0, 3) to (2, 2) will take a path as follows: (0, 3)  (cid:968) (1, 3) (cid:968) (2, 3) (cid:968) (2, 2). The wormhole switching is  sensitive to deadlock [9, 13]. To avoid deadlock, the  minimal odd-even  (OE)  routing  [8]  is used as a  representative of adaptive output selection. In the OE output  selection, a packet chooses a path from multiple alternatives,  but paths with certain turns are prohibited to avoid deadlock.  Considering the previous example again, a packet from (0,  3) to (2, 2) has two alternative paths: (0, 3) (cid:968) (0, 2) (cid:968) (1,  2) (cid:968) (2, 2) and (0, 3) (cid:968) (1, 3) (cid:968) (1, 2) (cid:968) (2, 2). Note  the path (0, 3) (cid:968) (1, 3) (cid:968) (2, 3) (cid:968) (2, 2) is invalid  because an east-south turn is not allowed in the switch  positioned at (2, 3) to avoid deadlock.  B. Contention-Aware Input Selection  To show the influence of input selection and output  selection on the routing efficiency, consider the example of  Fig. 2, which shows a network of switches (cores are  ignored for simplicity). Note the grey scale of the switches  indicates the number of packets waiting at the switches. The  white colour switches have low number of waiting packets,  whilst the grey colour switches have higher number of  waiting packets, and the black colour switche at (2, 2) has  the highest number of waiting packets. To demonstrate the  influence of output selection, consider a packet p0 traveling  from (3, 0) to (0, 2), which has a choice of multiple paths. A  good path would be to avoid the congested area (i.e., the  grey and black switches), as indicated by the dashed line.  This shows a suitable output selection can avoid network  Fig. 2. Motivation of CAIS  congestion. Now consider the input selection. Packets p1 at  (3, 2) and p2 at (4, 3) both want to travel through (3, 3). In  this case, a good choice would be let p1 take the priority to  access (3, 3), because the switch at (3, 2) has more waiting  packets than the switch at (4, 3). Such an input selection  helps reduce the number of waiting packets in congested  areas. This removes possible network congestions and leads  to better NoC performance. Based on this observation, a  contention-aware input selection (CAIS) is developed.  The basic idea of CAIS is to give the input channels  different priorities of accessing the output channels. The  priorities are decided dynamically at run-time, based on the  actual traffic condition of the upstream switches. More  precisely, each output channel within a switch observes the  contention level (the number of requests from the input  channels) and sends this contention level to the input  channel of the downstream switch, where the contention  level is then used in the input selection. When multiple input  channels request the same output channel, the access is  granted to the input channel which has the highest  contention level acquired from the upstream switch. This  input selection removes possible network congestion by  keeping the traffic flowing even in the paths with heavy  traffic load, which in turn improves routing performance.  Fig. 3 illustrates the detailed architecture of a switch with  CAIS. As can be seen, besides wires for data transmission,  CAIS requires additional wires to transmit contention levels  (CLs) between neighbouring switches. The switch has n+1  input channels and output channels. Input channels contain a  buffer to store the incoming flits temporarily before  forwarding them to one of the output channels. The output  selection (OS) module examines the header flit and decides  to which output channel the packet should be passed. The  OS sends an access request to the corresponding output  channel. In an output channel, once it becomes available, the  CAIS module examines the access requests, and sends a  selection signal to the MUX module which accordingly  connects one of the input data signals to the output data  signal. If there is only one access request, the request is  granted. If there are multiple access requests, a selection of  which input channel gets the access has to be made. This  selection mechanism is explained next.  CAIS performs input selection based on the contention  level (CL). The contention level of an output channel is the  number of access request received at a certain time. The CL  of an input channel is acquired from the output channel of  the upstream switch through signal wires. Fig. 4 shows the  algorithm of CAIS, which consists of two processes working  in parallel. Process observe_cl is activated when the status of  req_0..n changes. It observes the number of request to this  output channel (i.e., CL) and puts the CL value at out_cl_i. Then the CL is transmitted to the input channel of the  downstream switch, where the CL will be used to perform  the input selection. For example, considering the switch of  Fig. 3, if input channels 0 and 1 are requesting output  channel 0, then the CL of output channel 0 (out_cl_0) is 2,  and the CL of the input channel of the downstream switch is  also 2. Note that, to avoid high complexity and hardware  cost, CL is only sent to the immediate downstream switches  and is not spread any further. For example, considering the  previous example again, if the CLs of the input channels 0  and 1 (in_cl_0 and in_cl_1) are 3 and 4 respectively, the CL  of output channel 0 is NOT 3+4, but 2. Process select_input is activated when the output channel is available and there  are requests. It examines all requests and the CLs of the  corresponding input channels, and grants the request with  the highest CL.  For the input channels connected to the cores, there are no  upstream switches transmitting CL to them. The CL value is  set to 0 for these input channels. Therefore, the packets  already in the network have higher priority than the packets  waiting to be injected into the network.  Fig. 3. Switch architecture with CAIS  Fig. 4. Pseudo VHDL code of the CAIS algorithm  IV. Experimental Results  Experiments are conducted to evaluate the performance of  the contention-aware input selection (CAIS) and give a  comparison between CAIS and traditional input selections.  Two traditional input selections have been used in NoC,  first-come-first-served (FCFS)  input selection [5] and  round-robin  input  selection  [10, 11]. Due  to  the  advancement of FCFS over round-robin, FCFS is selected to  compare with CAIS. Both CAIS and FCFS are combined  with a deterministic output selection (XY routing [9]) and an  adaptive output selection (OE routing [8]). Four switch  models are developed using VHDL to implement the four  routing schemes: XY+FCFS, XY+CAIS, OE+FCFS and  OE+CAIS. Simulations are carried out on a 6×6 mesh NoC  using these four switch models. As in previous work [5, 8,  14], the performance of the routing scheme is evaluated  through latency-throughput curves. For a given packet  injection rate (i.e., the number of packets injected to the  network per cycle), a simulation is conducted to evaluate the  average packet latency. It is assumed that the packet latency  is the duration from the time when the first flit is created at  the source core, to the time when the last flit is delivered to  the destination core. For each simulation, the packet  latencies are averaged over 50,000 packets. Latencies are not  collected for the first 5,000 cycles to allow the network to  stabilise. It is assumed that the packets have a fixed length of  5 flits and the buffer size of input channels is 5 flits. Since  the network performance is greatly influenced by the traffic  pattern, we applied four different traffic patterns, including  three synthetic traffic patterns (uniform, transpose and hot  spot) and a real-life traffic pattern (GSM voice CODEC).  A. Synthetic Traffic  In the first set of experiments we consider three synthetic  traffic patterns: uniform, transpose, and hot spot [8]. In the  uniform traffic pattern, a core sends a packet to any other  cores with equal probability. In the transpose traffic pattern,  a core at (i, j) only send packets to the core at (5-j, 5-i). In  the hot spot traffic pattern, the core at (3, 3) is designated as  the hot spot, which receives 10% more traffic in addition to  the regular uniform traffic.  Fig. 5 shows the performance of the four routing schemes  under uniform traffic. The X-axis represents the packet  injection rate per node (the packet injection rate for the  whole NoC is 36 times higher), and the Y-axis represents the  average packet latency. As can be seen from the figure, the  four schemes have almost the same performance at low  traffic load (<0.038 packets/cycle). As the traffic load  increases, the packet latency rises dramatically due to the  network congestion. Comparing the curves of OE+FCFS  and OE+CAIS, it can be seen that, using the OE output  selection, CAIS performs significantly better than FCFS.  Similarly, the curves of XY+FCFS and XY+CAIS show that  CAIS also outperforms FCFS when using XY output  selection. As reported in [5, 8], the XY output selection has  better performance than the OE output selection. This is  because  the XY output selection  incorporates global,  long-term information about the uniform traffic, leading to  even distribution of traffic. On the other hand, the OE  routing is based on local, short-term information, which only  benefits the immediate future packets while loses the  evenness of uniform traffic in the long run.  Fig. 6 shows the performance of the four routing schemes  under transpose traffic. It can be seen that FCFS and CAIS  have the same performance when using the XY output  Fig. 5. Routing performance under uniform traffic  Fig. 6. Routing performance under transpose traffic  Fig. 7. Routing performance under hot spot traffic  selection; FCFS works slightly better than CAIS when using  the OE output selection. This is because with transpose  traffic, it is rarely the case that more than one input channels  compete for the same output channel. Therefore, the input  selection policy has little impact on the routing performance.  Fig. 7 shows the routing performance under hot-spot  traffic. Once again, it can be seen that CAIS significantly  outperforms FCFS, either using XY or OE output selection.  Furthermore, although the OE output selection performs  worse than the XY output selection when using the FCFS  input selection, it achieves similar performance as XY when  using the CAIS input selection.  B. Voice CODEC Traffic  To evaluate the performance of CAIS under more realistic  traffic loads, we have conducted simulations using a GSM  voice CODEC [15]. The GSM voice CODEC is partitioned  into 9 cores. The communication trace between the cores is  recorded for an input voice stream of 500 frames (10  seconds of voice). The cores are mapped manually to a 6×6  mesh NoC. Fig. 8 shows the partition and communication  trace of the GSM voice CODEC. As can be seen, the  encoder and decoder are partitioned into core_0 – core_4  and core_5 – core_8 respectively. The directed edges  between the cores represent communications. For example,  the edge between core_0 and core_1 means there is a  communication from core_0 to core_1, the communication  has a size of 320 bytes and occurs at cycle 0. Due to the  space limitation, only communications in the first 1000  cycles are shown. Fig. 8 also shows the mapping of the cores  to the NoC using the dashed lines. The CODEC cores  generate packets according to the recorded communication  trace. The other cores in the NoC generate uniform traffic,  with the same average packet injection rate as the CODEC  cores. The packet injection rate is increased incrementally to  get the latency-throughput curve, which is shown in Fig. 9.  As can be seen, in both cases of using XY and OE output  selection, CAIS achieves better performance than FCFS.  Furthermore, although XY has worse performance than OE  when using FAFS, it achieves similar performance as OE  when the CAIS input selection is used. This observation and  the one obtained from Fig. 7 show that the employment of  CAIS can help the otherwise less effective output selection  when using FCFS catch up with the more effective output  selection. This demonstrates further the importance of input  selection in routing efficiency and the effectiveness of CAIS.  One consideration of CAIS is that some packets may  experience  indefinite waiting. Although  theoretically  possible, it does not happen in the experiments. To give an  insight, Fig. 10 shows the maximum packet latency of the  routing schemes under the GSM voice CODEC traffic. Note  the curves in Fig. 10 are NOT average latency, but the  maximum latency experienced by the packets, thus the  curves show some dips and jumps. As can be seen, when the  network load is low, CAIS has similar maximum packet  latency as FCFS; when the network load is high, CAIS has  shorter maximum packet latency than FCFS. CAIS does not  cause indefinite waiting. The reason is that, due to the  latency introduced by the processing of the header flit, there  is a gap between the access requests of two consecutive  packets. During this gap, one of the packets waiting in input  channels with low contention levels can get the access to the  output channel, thus indefinite waiting is avoided.  Overall, the experiments of Sections IV.A and IV.B have  demonstrated the importance of input selection, which is in  line with that obtained in [14] when applied in distributed  computing. The experiments have also shown that CAIS  Fig. 9. Routing performance under voice CODEC traffic  Fig. 8. Partition, communication trace and core mapping  of a GSM voice CODEC  Fig. 10. Maximum packet latency under CODEC traffic  effectively improves the routing efficiency for NoCs.  C. Implementation of Prototype Switch  To evaluate the area overhead of CAIS and show the  performance/area trade-off, switches with four different  routing schemes have been implemented. The first scheme is  XY+FCFS, i.e., XY output selection and FCFS input  selection. The other  three schemes are XY+CAIS,  OE+FCFS and OE+CAIS. The switches were coded in  VHDL and synthesized with Synplify ASIC using an ST  Microelectronics 0.12 µm standard cell library. For all  switches, the data width is set to 32 bits, and each input  channel has a buffer size of 5 flits. Fig. 11 shows the area  cost of  the four switches. As expected, using  the  deterministic XY output selection and the simple FCFS  input selection, XY+FCFS has the lowest area cost of  0.109725mm2. Due to the relative complexity of the  adaptive OE output selection and the CAIS input selection,  XY+CAIS and OE+FCFS have slightly higher area costs of  0.111480mm2  and  0.110355mm2  respectively,  and  OE+CAIS has the highest area cost of 0.113580mm2. Comparing the area costs of XY+FCFS and XY+CAIS,  CAIS introduces 1.6% additional overhead than FCFS.  Similarly, when comparing OE+FCFS and OE+CAIS, CAIS  introduces 2.9% additional overhead than FCFS.  CAIS requires additional wires to transmit the contention  levels (CLs). In the case of 2D mesh NoC, each switch have  at most 5 input channels, receiving packets from the core  and the 4 neighbouring switches. Thus at most 3 ((cid:203)log25(cid:219)) wires are needed to transmit a CL. This is acceptable  because NoC have abundant wiring resources [3, 4].  Note although this paper has considered mesh-based NoC,  CAIS is flexible enough to support other NoC topologies  including irregular topologies. This can be easily done by  configuring the value of n (Fig. 3 and Fig. 4), and the  number of wires for CL transmission accordingly.  V. Conclusion  This paper has shown the importance of input selection in  routing efficiency, and presented a simple yet effective  contention-aware input selection (CAIS) as part of the  routing techniques implemented in switches. CAIS performs  the input selection considering the contention level of the  upstream switches. By granting busier input channel higher  priority to access the output channel, CAIS keeps the traffic  in busy paths flowing, therefore removes possible  Fig. 11. Area cost of the switches  network congestion. Simulation has been carried out with a  number of different traffic patters, including synthetic traffic  and realistic voice CODEC traffic. The results shows that,  no matter which output selection is used (deterministic XY  routing or adaptive odd-even routing), the proposed CAIS  achieved  better  performance  than  the  traditional  first-come-first-served (FCFS) input selection for most  traffic patters except the transpose traffic, where CAIS has  similar performance as FCFS. Furthermore, the employment  of CAIS can make the XY routing (low complexity and  hardware cost) to achieve similar or even better performance  than the higher complexity and hardware cost odd-even  routing for some traffic patters, i.e., area saving. The  prototype switch with CAIS has been implemented and  shows that CAIS has slight hardware overhead compared to  FCFS (< 3%). As explained in Section IV.B, although not  shown in the experiments, there is a starvation possibility in  CAIS, which remains to be addressed in future work.  "
2006,Co-synthesis of a configurable SoC platform based on a network on chip architecture.,"The constant increase of gate capacity and performance of configurable hardware chips made it possible to implement systems-on-chip (SoC) able to tackle the demanding requirements of many embedded systems. In this paper, we propose an approach to the design space exploration of a configurable SoC (CSoC) platform based on a network on chip (NoC) architecture for the execution of dataflow dominated embedded systems. The approach has been validated with the design of a color image compression algorithm in an FPGA","Co-Synthesis of a Configurable SoC Platform   based on a Network on Chip Architecture  Mário P. Véstias  mpv@fidelio.inesc-id.pt INESC-ID, Lisboa  Portugal  Horácio C. Neto  hcn@inesc-id.pt  INESC-ID, Lisboa  Portugal  Abstract - The constant  increase of gate capacity and  performance of configurable hardware chips made it possible  to  implement systems-on-chip (SoC) able to tackle the  demanding requirements of many embedded systems. In this  paper, we propose an approach to the design space exploration  of a configurable SoC (CSoC) platform based on a network on  chip (NoC) architecture for the execution of dataflow  dominated embedded systems. The approach has been  validated with the design of a color image compression  algorithm in an FPGA.  I Introduction  Configurable hardware is constantly being upgraded with  higher working frequencies and gate capacity that allow the  implementation of faster complex systems in a single chip,  making it a competitive solution for embedded systems. This  gate capacity leads to a complexity challenge needing new  architectures and design methodologies to increase design  productivity. An approach to the design of such complex  systems is to reuse hardware and software blocks resulting in  a number of interconnected IP cores. Gajski et.al. [1] have  proposed  an  IP-centric  embedded  system  design  methodology and Vahid et al. [2] have proposed a platform  based methodology which not only allows reuse of  components but also of system architectures and topologies.  Many architectural templates have been proposed for  hardware platforms for future SoCs with a general emphasis  on providing efficient and standardized communication  infrastructures for connecting multiple resources on the chip,  like the Network-on-Chip (NoC) [3], [4]. The NoC has been  introduced as a new interconnection paradigm able to  integrate a many cores while keeping a high communication  bandwidth. The increased computational power and internal  communication bandwidth of NoC can provide better timing  performances to the embedded applications than the shared  medium in current SoC architectures.   Some works have contributed with concepts in the area of  networks on chip, like [5]. Among the few implementations  of NoC, we are mainly interested on the configurable  hardware implementations of [6] and [7]. Marescaux [6] have  implemented a bidirectional torus in a Virtex/VirtexII FPGA.  The network uses 16 bits data packets, the XY routing  algorithm, virtual output buffers and supports up  to  320Mbits/s at 40MHz with two virtual channels time  multiplexed for QoS support. HERMES [7] is a NoC mesh  topology implemented in a VirtexII FPGA. The network  supports up to 500 Mbits/s at 25MHz without QoS.  Many co-synthesis tools will be required to develop NoC  based architectures. Tools to choose the best platform  configuration and to map applications to the target NoC  architecture will be essential. There has been a lot of research  on co-synthesis for bus-based architectures [8], [9], [10].  NoC researchers can adapt many of the techniques and ideas  from these approaches for NoC tool development.  Since NoC is a novel research area only a few mapping and  scheduling approaches have been developed. Lei et al. [11]  use a genetic algorithm (GA) for task mapping and  list-scheduling (LS) for task scheduling. The communication  is neither mapped nor scheduled and delay is estimated as the  average distance between processors. Shin et al. [12]  proposed a methodology with network assignment and link  speed allocation for reducing communication energy. They  use GA for mapping and network assignment and LS for task  scheduling and link assignment. To our knowledge, there is  not a methodology for the development of NoC based SoC  considering all aspects of the co-synthesis process.   In this paper, we propose a co-synthesis methodology with  the integration of allocation, mapping and scheduling steps  for the development of SoC based on a parameterizable NoC  for the execution of dataflow-dominated applications, like  multimedia. The platform supports hardware/software  multitasking and includes hardware support for the operating  system.  Increased productivity  is achieved  through  orthogonalization of communication and computation and  design reuse. A real multimedia example has been simulated  and implemented on a Virtex II XC2V6000 FPGA.  II. CSoC Architecture Our CSoC platform consists of an array of  tiles  interconnected with a NoC. The NoC consists of an array of  routers (R), where a router is connected to at most four  neighbor routers and to a local IP core. Among the many  interconnection topologies, we use a 2D mesh topology  because it fits naturally in a 2-dimensional chip (see example  in figure 1).  R Mem NI    IP Core Mem N I    IP Core R R Mem NI    IP Core R Mem NI    IP Core Mem N I    IP Core R R Mem NI    IP Core R Mem NI    IP Core Mem N I    IP Core R R Mem NI    IP Core Fig. 1.  SoC architecture  A tile consists of an IP core, local memory and a network  interface (NI). An IP core is a piece of configurable hardware  or a processor. Each core has direct access to local memory  and uses the NoC to exchange data with other cores. The link  between a router and a core is established with a NI. The  platform connects to the environment using IP cores that  implement a particular type of interface.   A NoC can be described by its topology and by the  strategies used  for routing,  flow control, switching,  arbitration and buffering. Routing determines how a message  chooses a path in this graph, while flow control deals with the  allocation of channels and buffers to a message as it traverses  this path. Switching is the mechanism that removes data from  an input channel of a router and places it on an output  channel, while arbitration is responsible for scheduling the  access to channels and buffers. Buffering defines the  approach used to temporarily store messages.  The communication behavior follows a layered approach  similar to the OSI communication architecture (see figure 2).  Layered Architecture Appl icat ion Session Transport Network Data-Link Physical Platform Components IP cores Network Interfaces Network Routers Physical links Fig. 2.  Communication architecture of the SoC platform  The application layer includes all tasks implemented by a  core that consume/produce data. The session layer includes  OS services, namely, memory management and  task  scheduling. The transport layer manages the identification of  task ports for the correct end-to-end delivery of data between  tasks on different IP cores and the segmentation of data  output into packets and reassembly of packets into input data.  The network layer includes services for packet routing. The  data-link  layer  includes protocols  for  reliable data  communication between two routers and between a router  and an IP core. Finally, the physical layer models the physical  links for transmission of bits.  A. Parameterization  The NoC  infrastructure has a set of configurable  parameters, including:  1) The size of the 2D mesh topology.   2) The type of IP core of each tile.  3) The data width of point-to-point channels between routers.  Supported values are 8, 16 and 32 bits.  III. Router Design A router forwards packets between IP cores. For each  packet received, the router reads the destination address and  forwards it to the correct output port. Our router consists of a  set of input and output ports with buffering and a set of  control blocks for routing, flow control, switching and  arbitration (see figure 3).  Switch Matrix Flow control Arbitration Routing Buffer Buffer B u f f e r B u f f e r Ack Rcv Rqst Rcv Packet In Packet Out Rqst Trm Ack Trm B u f f e r Packet In Rqst Rcv Ack Rcv Ack Trm Rqst Trm Packet Out P e k c a t O u t R s q t T r m A k c T r m P e k c a t I n R s q t R v c A k c R v c P e k c a t O u t R s q t T r m A k c T r m P e k c a t I n R s q t R v c A k c R v c Ack Trm Rqst Trm Packet Out P e k c a t I n R s q t R v c A k c R v c WEST LOCAL EAST S U O T H R O N T H Fig. 3. Architecture of a NoC router  Router ports are used to exchange packets with neighbor  routers and with the local IP. A port guarantees the  communication reliability through a two-way handshake  point-to-point flow control.   The arbitration mechanism uses a round-robin scheme to  arbitrate requests from different input ports and grants the  output buffer to an input request port. Among the deadlock  free routing algorithms for mesh topologies, we implemented  the XY algorithm. The XY algorithm routes packets first  along the X direction, then along the Y direction until  reaching the target. The switching mechanism is based on the  store and forward process.  IV. Network Interface Design The NI consists of an input and an output controller, shared  memory ports to connect to the core, a port to connect to the  router and an OS memory (see figure 4).                                IP Core Memory y r o m e M S O NI Input Control Memory  Management Task Scheduling Reassemb ly n I t e k c a P v c R t s q R v c R k c A LOCAL Router m r T k c A Output Control Segmentation Encapsulation End-to-end Flow con trol Buffer m r T t s q R t u O t e k c a P Fig. 4. – Architecture of the network interface  The interface between the NI and the router is identical to  the interface between two routers. The interface between the  IP core and the NI is implemented with shared memory. The  memory may be dual-port RAM or FIFO, depending on the  type of the core. For a processor core, the shared memory is  always implemented with dual-port RAM. For a hardware  core, it may be any of the two kinds of memory. The NI also  performs data split and data merge to transmit packets with a  data size different than the link size.  The input controller receives data from the router and  sends it to the shared memory to an address defined by the  port number of the packet. For data transmitted in multiple  packets, the controller stores each packet in sequential  memory positions until the complete data port is available.  When all data of the input ports of a task are available and the  corresponding output port is free, the input controller sends a  token to the core indicating the task can be executed.   The output controller reads data from the shared memory  and sends it in one or more packets. It also implements an  end-to-end dataflow control, that is, when the input buffers of  a task are available, the output controller sends a token to the  producers indicating that they can send more data.  V. Packet Structure  Data to be transmitted is encapsulated in packets at the  transmitter and deencapsulated at the receiver. Besides data  packets, our NoC implementation uses configuration and  token packets. Configuration packets are used to configure  the NI and token packets are used for end-to-end flow control.   The highest level of the packet structure is at the  application layer where data is produced. If data is too large  to fit into one packet it will be divided into several packets.  Each packet is encapsulated with its type at the session layer.  Next, the transport layer adds the destination port and the  network layer adds the destination address.  VI. CSoC Performance Evaluation  We have conducted a simulation of a prototype of the NoC  to characterize the following set of parameters (see table 1):  Link latency (LL) - the delay for a packet to move from  the output of a router to the output of a neighbor router;  Resource generation latency (RGL) – the delay for a NI  to generate a packet;  Resource reception latency (RCL) – the delay for a NI to  consume a packet;  Resource to resource latency (R2RL) – the delay for  moving a packet from one IP core to a neighbor IP core;  Resource  to resource bandwidth  (R2RB)  -  transmission throughput between IP cores.  the  TABLE I  NoC Characterization  RCL  R2RB  f ( frequency 5 cycles  5 RGL  4 cycles  ) LL  1 cycle  Pack ets/s  From these parameters, we calculate the communication  delay of a packet between two tiles. In a NoC, the  communication delay depends on the distance between tiles,  the size of transmission data and the network traffic. In the  execution of an application, it is possible to have many  concurrent transmissions, which conflicts in the use of the  communication resources. If tasks have variable execution  times, the network traffic is non-deterministic, which makes  the analysis more difficult. To simplify, we assume that at any  time a link is dedicated to a single data transfer. Hence, the  transmission delay of a packet between two tiles separated by  NR routers, EdgeDelay, is given by:  EdgeDelay =  RGL + RCL + LL × NR (f working frequency) (1)  Since the packets are buffered at the routers, the  transmission of data requiring more than one packet can be  pipelined. In this case, the transmission delay, EdgeDelaypipe, of NP packets is given by:  EdgeDelaypipe = (2)  + × ( 1 RBR 2 NR )NP Besides performance, we have also determined the  maximum area occupied by a router and a NI after the  synthesis and placement of the components on the target  FPGA with the Xilinx ISE 6.2i software (see table 2).  TABLE II  Slice Areas of the Interconnection Components  Block  Size (slices)  BRAM % XC2V6000  Router (8bits)  189  0  0,56  NI  121  1  0,36  A router can forward five packets per clock cycle at  150MHz. Therefore, a router can forward at most 6Gbps.                                  VII. CSoC Co-Synthesis  Generic  NoC plat form Ins tanc iate Init ial  Set of IP Cores To configure our CSoC architecture for a specific  application, we have developed  a platform-based  co-synthesis methodology. It finds a hardware/software  architecture  that runs  the application with optimized  performance and meets the design constraints (see figure 5).  Generic NoC  Platform Architecture instance Quality Applications Mapping Design Constraints Analysis Design Constraints Quality Allocation Design Constraints Fig. 5. Co-synthesis flow  It starts with an architecture instance, maps the application  onto the architecture and uses the analysis step to determine the  quality of the architecture based on cost and performance  metrics. The analysis yields quality values that together with  design constraints over cost and performance guide the  allocation to improve the architecture.  A. Application Model  The applications are modeled with an iterative dataflow  graph (IDFG) that can represent iterative behaviors. This  model is a directed cyclic graph G = (V, E), where each vertex  v ∈ V represents a task (atomic computation) and each edge e  ∈ E represents intra or inter data dependencies between tasks.   A task vertex has associated its worst and/or average case  execution time, data, and program memory size or hardware  area in each of the available cores. An edge has an associated  vector (v,d), where v is the data size to be transferred and d is  the inter delay between two connected tasks.   B. Allocation  The allocation step determines the most appropriate SoC  architecture from the generic CSoC platform for the execution  of the application. It determines the size of the NoC topology  and the type of core associated with each tile. This is a hard  problem. Therefore, we have used an heuristic (see figure 6).  From the generic CSoC platform, the algorithm instantiates  an initial set of IP cores. From this set, it generates an initial  architecture. To generate an architecture from a set of cores,  the algorithm reads sequentially the set of tiles with its IP core  association (including the NI) and fills the FPGA (including  the routers) until it is full or all tiles are associated.   Init ial Set of map Tile < -> IP Core Generate NoC  Architecture Mapping & Analy s is yes Good? no Final NoC Change Tile < -> IP Core Fig. 6. Allocation algorithm  Next, it maps the application and finds the quality of the  architecture with the analysis tool. If the quality is considered  acceptable, the algorithm stops. Otherwise, it changes the IP  core of a tile and restarts the evaluation flow. The iterative  process of the algorithm is controlled with a simulated  annealing (SA) algorithm [13] as follows:  Move generation function: generates a new architecture  instance from the previous by changing the IP core of a tile.  Cooling schedule: the cooling schedule includes the initial  temperature, t0, the decrement rule for the temperature, the  stop criterion and the length of the Markov chain. t0 is obtained  by incrementing the temperature until the percentage of  accepted transitions is higher then 70%. The decrement rule is  given by tk = t0  × 0.95k. The algorithm stops when three  consecutive Markov chains end with the same value. The  length of the Markov chain is equal to the size of the  neighborhood.  Cost function: obtained with the mapping and analysis steps.  C. Mapping  Mapping consists on assigning each application object  (task, data transfer and variable) onto an architectural element  (IP core, link and memory, respectively) in order to maximize  the quality of the architecture while satisfying design  constraints. Even for small instances, the mapping problem  has exponential complexity so that heuristics must be used.  Our mapping approach uses SA to improve an initial  solution found with LS while exploring the advantages of  both pipelining and unrolling to increase the throughput.  Pipelining allows tasks belonging to different iterations to be  executed at the same time and unrolling increases the number  of tasks within an iteration to explore more parallelism. The  NoC structure can easily implement pipelining since the  output of a task is buffered and the flow of data is easily  controlled by the end-to-end flow control of the NI.   The algorithm starts with the IDFG of the application and a  NoC architecture and iteratively runs a mapping design space  exploration step with different unrolling values (U) (see  figure 7).  IHDFG Unroll <- Unroll + 1 Map Analysis Improved? Unro ll? Unrolled Graph Save  Solution End no no yes yes NoC Architecture Fig. 7. Mapping algorithm  The iterative process is controlled with SA using the same  cooling schedule of the allocation process. Each mapping  solution is evaluated with the analysis tool.  D. Analysis  The analysis finds the quality of an architecture based on  its performance, cost and memory requirements.  For performance evaluation, it uses LS to order the  execution of tasks assigned to a single core to optimize the  throughput of the graph, Cthroughput. To schedule tasks on  software processors, the algorithm assumes the processor  executes one task at a time and the program code of the task is  in local memory before starting its execution.   The memory requirements of a CSoC architecture depends  on the local memory requirements of each IP core. The local  memory of a core is used to store the program instructions of  tasks (for a software core), the OS data of the NI and the tasks  data. We assume the local memory is implemented with  BRAM and the instruction, the OS and data memories use  independent BRAM.  To calculate the memory utilization of a core, the analysis  uses a table with the instruction memory size of each task on  each software core and the size of input and output ports of  each task. The analysis process determines the number of  local BRAMs, BR, necessary to implement core k as follows:  BR(k) =   (3)  where instrMem(i) is the code memory size of task i, portSize(i, j) is the data size of port j of task i. The total  number of BRAM used, Cmemory, is given by:  { } { } { } 1 ) ,( i 2 )( i + × + (cid:166) (cid:166) (cid:166) ∈ ∈ ∈ BRAMSize j portSize BRAMSize instrMem coreTasks i taskPorts j coreTasks i Cmemory = (cid:166)∈ i { setofcores } iBR   )( (4)  The final quality of the architecture is given by Ct + Cm where  ( ) ( ) constraint ,    w/  Cmetric P Cmetric Cmetric(P)  - Cmetric Constra CmetricCon stra int int constraint    w/o , (cid:173) (cid:176)(cid:176) (cid:174) (cid:176) (cid:176) (cid:175) × × = Ka K Cx (5) where K and Ka are weighting factors specified by the user  with typical values of 0.5 and 100, respectively. Cmetric(P) is  the metric value (throughput or memory) of a solution P and  Cmetri cConstraint is a metric (throughput or memory) constraint.  For a non-constrained metric, we use the average value  ( Cmetric ) calculated from the values of the metric in some  (<20) previous solutions.   VIII. Design Evaluation  This section describes the design of a JPEG encoder with  the proposed CSoC environment. A simulation has been  executed and a prototype is under development based on the  Xilinx VirtexII XC2V6000.  A. JPEG Encoder  The standard JPEG compression with a block size of 8×8  pixels for color images was implemented. The compression  method used is based on the DCT (see IDFG of the JPEG  encoder in figure 8).  RGB2YCbC r 2D-DCT 2D-DCT 2D-DCT Quantizer  & Zigzag Quantizer  & Zigzag Quantizer  &  Zigzag Huffman Encoder Image.bmp Image.jpg [8x8] [8x8] [8x8] [8x8] [8x8] [8x8 ] [8x8] [8x8] [8x8] Fig. 8. IDFG of the JPEG encoder  The tasks of the JPEG where characterized considering a  hardware implementation (see table 3).  TABLE III  JPEG Hardware Task Characterization  Size (slices)  BRAM  Latency  204  0  64  1612  1  168  312  1  64  176  1  192  Task  RGB2YCbCr  2D-DCT  Quantizer  Huffman  For this application, the co-synthesis tool took less than 5  minutes to found a hardware only solution that can process  two blocks of [8×8]×24 bits in 3.8 µs (cores frequency =  100MHz, NoC frequency = 150MHz with 16 bit packets),  which is equivalent to a processing capacity of 800 Mbps.  With this throughput, we can process color images at the  processing times of table 4.                    TABLE IV  Execution Time of Hardware Solutions  HW solution (NoC)  Pentium 4 at  HW solution  seconds (fps)  1.7 GHz  (bus) seconds  0.009 (108)  0.046  0,055  0.015 (67)  0.071  0,086  0.024 (42)  0.110  0,14  Image size  640×480  800×600  1024×768  The JPEG solution was placed & routed and simulated on  the FPGA with the Xilinx ISE 6.2i software (see figure 9).  promising since we where capable of easily integrate several  IP cores in a single chip and obtain high quality solutions.  Future research includes developing a more flexible CSoC  with different network topologies and including a generic  parameterizable router as part of the design space exploration  in order to improve the area, performance and energy  dissipation of the final SoC architecture.  Acknowledgments  Router0 Router1 Router2 Router3 The authors thank the support granted by INESC-ID.  RGB2YCbCr DCT DCT DCT "
2007,Application Specific Network-on-Chip Design with Guaranteed Quality Approximation Algorithms.,"Network-on-chip (NoC) architectures with optimized topologies have been shown to be superior to regular architectures (such as mesh) for application specific multiprocessor system-on-chip (MPSoC) devices. The application specific NoC design problem takes as input the system-level floorplan of the computation architecture, characterized library of NoC components, and the communication performance requirements. The objective is to generate an optimized NoC topology, and routes for the communication traces on the architecture such that the performance requirements are satisfied and power consumption is minimized. The paper discusses a two stage automated approach consisting of i) core to router mapping, and ii) topology and route generation for design of custom NoC architectures. In particular it presents an optimal technique for core to router mapping (stage i), and a factor 2 approximation algorithm for custom topology generation (stage ii). The superior quality of the techniques is established by experimentation with benchmark applications, and comparisons with integer linear programming (ILP) formulations, and heuristic techniques.","2B-5 Application Speciﬁc Network-on-Chip Design with Guaranteed Quality Approximation Algorithms Krishnan Srinivasan, Karam S. Chatha, and Goran Konjevod Department of CSE, PO BOX 875406, Arizona State University, Tempe, AZ 85287-5406 Email: {ksrinivasan,kchatha,goran}@asu.edu Abstract— Network-on-Chip (NoC) architectures with optimized topologies have been shown to be superior to regular architectures (such as mesh) for application speciﬁc multiprocessor System-on-Chip (MPSoC) devices. The application speciﬁc NoC design problem takes as input the system-level ﬂoorplan of the computation architecture, characterized library of NoC components, and the communication performance requirements. The objective is to generate an optimized NoC topology, and routes for the communication traces on the architecture such that the performance requirements are satisﬁed and power consumption is minimized. The paper discusses a two stage automated approach consisting of i) core to router mapping, and ii) topology and route generation for design of custom NoC architectures. In particular it presents an optimal technique for core to router mapping (stage i), and a factor 2 approximation algorithm for custom topology generation (stage ii). The superior quality of the techniques is established by experimentation with benchmark applications, and comparisons with integer linear programming (ILP) formulations, and heuristic techniques. I . IN TRODUC T ION Network-on-Chip (NoC) has been proposed by academia and industry as a solution for the on-chip communication challenges for the future MPSoC architectures. A NoC is composed of routers that are inter-connected by physical links. Each computation or storage core interacts with the NoC through a resource to network interface. The NoC supports packet switching based asynchronous communication. NoC is inherently scalable and supports high bandwidth by enabling concurrent communication. Application speciﬁc MPSoC architectures that are aimed at a narrow domain such as multimedia set-top boxes integrate numerous heterogeneous computing and storage cores. Each core implements a limited set of application functionality. Consequently, inter-core communication depicts well deﬁned patterns as determined by the speciﬁc domain. For such designs, the application speciﬁc custom NoC architecture has been demonstrated to be superior to regular topologies in terms of power consumption and NoC resources [1] [2]. The paper focuses on automated design of application speciﬁc custom NoC architectures. The overall MPSoC design ﬂow can be divided into two stages focusing on computation architecture, and NoC architecture, respectively. The output of the computation architecture design stage is a collection of processing and storage cores, and inter-core communication performance requirements for NoC. The objective of NoC design stage is to construct an optimized interconnection architecture such that the communication performance requirements CTG System−level floorplan Characterized NoC components ROUTER ALLOCATION CORE TO ROUTER MAPPING TOPOLOGY GENERATION TRAFFIC ROUTING D MC rni D C rni M rni P M rni FPGA M rni D C rni ASIC rni ASIC ASIC rni rni ASIC ASIC rni M rni rni: resource network interface C: Cache D: DSP P: Processor M: Memory Fig. 1. Custom NoC design ﬂow are satisﬁed and the power consumption is minimized. It has been shown that the percentage of power consumption in NoC physical links increases from about 20% in 160 nm technology [3] to upward of 30% in 65 nm technology [2]. The power consumption in the physical links is directly dependent upon the length of the link and bandwidth of trafﬁc ﬂowing through the link. The length of the physical link is in turn governed by the system-level layout. The designer also speciﬁes a maximum inter-router link length (Dmax ) for single clock cycle data transfer. Therefore, in order to account for link power consumption and to ensure that the link lengths are less than Dmax , the design of the NoC architecture must consider the system-level layout. System-level ﬂoorplanning is a well researched topic, and existing techniques [4] can be utilized to generate the ﬂoorplans. Our automated NoC design techniques operate upon the system-level ﬂoorplan of the computation architecture. A. Custom NoC design ﬂow Figure 1 depicts the application speciﬁc NoC design ﬂow. The inputs to the custom NoC design problem are: i) the system-level ﬂoorplan of the computation architecture, ii) communication bandwidth requirements, and iii) characterized component library of interconnection elements (namely, routers and physical links). The NoC performance constraints are speciﬁed by a communication trace graph (CTG), where the nodes represent the 1-4244-0630-7/07/$20.00 ©2007 IEEE. 184 2B-5 processing cores or memory elements, and the edges represent the communication traces between the cores. Each node in the CTG is speciﬁed by its physical dimensions, and each edge is speciﬁed by its bandwidth requirement. The cores in application speciﬁc MPSoC architectures for multimedia and network processing domains demonstrate well deﬁned periodic inter-core communication characteristics and hence, can be easily modeled in the trace graph. The output of the custom NoC design ﬂow is a speciﬁcation of the interconnection network topology, a mapping of computation architecture cores to the NoC routers, and routing of CTG traces on the topology such that the communication bandwidth requirements are satisﬁed, and power consumption is minimized. In this paper, we address the NoC design problem by dividing it into two stages namely, core to router mapping, and routing and topology generation. We present a polynomial time optimal algorithm for mapping of cores to routers such that the power consumption is minimized. We also present a linear programming based approximation algorithm for routing and topology generation that minimizes the number of router resources in the topology subject to low power constraints. Our algorithm runs in polynomial time and guarantees that the number of routers in the topology is at most twice the optimal solution, and the power consumption is minimized. B. Deﬁnition of the NoC design problem The NoC design problem was formally deﬁned in [2]. We restate the problem for the sake of completeness. “ Given: • A directed communication trace graph G(V , E ), where each vi ∈ V denotes either a processing element or a edge ek = (vi , vj ) ∈ E denotes a communication trace memory unit (henceforth called a core), and the directed from vi to vj . • For every ek = {vi , vj } ∈ E , ω(ek ) denotes the bandwidth requirement in bits per cycle. • A router architecture characterized by: – a parameterizable value η , which denotes the number of ports of a router, – a value ηmax that denotes the maximum number of available ports in a router, – Ω denotes the peak input and output bandwidth that the router can support on any one port, – Ψi that denotes the power consumed per unit bandwidth of trafﬁc ﬂowing in the input direction for any port of the router, and – Ψo which denotes the power consumed per unit bandwidth of trafﬁc ﬂowing in the output direction for any port of the router. • A physical link power model where Ψl denotes the power consumed per unit bandwidth of trafﬁc ﬂowing through the link per unit length of the link. • A system-level ﬂoorplan of the cores. • A maximum inter-router distance Dmax . The objective of the custom NoC design problem is to construct: • a NoC architecture J (R, L, C ), where R denotes the set of routers utilized in the synthesized architecture, L represents the set of links between two routers, and a many to one mapping function C : V → R that denotes • a set B of ordered tuples of routers, where each bi (cid:3)ri , rj , the mapping of a core to a router, . . . , rk (cid:4) ∈ B , ri , . . . , rk ∈ R denotes a route for a trace e(vi , vk ) ∈ E (C (vi ) = ri , C (vk ) = rk ), and such that the NoC power consumption is minimized. ” I I . PR EV IOU S WORK Benini et al. [5] presented a survey of design techniques for application speciﬁc NoC architectures. Pinto et al. [6] proposed a technique for synthesis of point to point links that utilize at most two routers between source and sink. Thus, their problem formulation does not address routing. Jalabert et al. [1] proposed a custom NoC instantiation framework based on designer speciﬁed inputs. It does not synthesize the custom topology. Ogras et al. [7] proposed graph decomposition based heuristic techniques for application speciﬁc NoC architectures. Ogras et al. [8] also proposed heuristic incremental techniques that modiﬁed mesh based topologies via long link insertion. All papers cited above assume a constant predetermined link length, and hence do not incorporate system-level ﬂoorplanning to estimate their actual lengths. Srinivasan et al. [2] observed that the physical links are expected to consume upward of 30% of the NoC power consumption. They proposed integer linear programming (ILP) based techniques that incorporated system-level ﬂoorplanning to account for physical link power consumption. This paper accounts for link power consumption by taking the system-level ﬂoorplanning of the cores as input to the NoC design ﬂow. To the best of our knowledge, this is the ﬁrst attempt at designing approximation algorithms for NoC design The quality of the solutions produced by our technique is demonstrated by experimenting with several multimedia and network processing benchmarks, and comparisons with existing techniques. The paper is organized as follows. In Section III we describe our technique for mapping of cores to the routers of the NoC. In Section IV, we present our technique that generates static routes for the traces, and generates the NoC topology. Section V presents the results, and ﬁnally, Section VI concludes the paper. I I I . COR E TO ROU T ER MA P P ING Given a system-level ﬂoorplan, our technique assigns possible router locations to be the corners of the cores in the ﬂoorplan. Once the possible router locations are determined, our technique connects each core in the computation architecture to a unique router in the layout. Note that while several cores can be mapped to the same router, each core is uniquely mapped to a particular router. We assume that a core can be mapped to one of the four routers located at its corner. Thus, if (x, y) denotes the lower left hand side corner of a node 185 2B-5 Case I v k Case II v k w(i) a w(k) a w(k) b vi v k (a/2) −X (a/2)+w(i) vi C1 (a/2)+w(k) X+ (a/2) C2 −X 0 a vi v k w(k) vi 0 b X+ −X 0 a Fig. 2. Core alignments and ﬂow graphs Case III v k b c c 0 X+ a vi v k b vi v , the core is mapped to one of the routers located at (x, y), (x + Wv , y), (x, y + Hv ) or (x + Wv , y + Hv ), where Wv is the width of the core, and Hv is its height. The objective of the core to router mapping is to minimize the power consumption. As the topology of the NoC is not deﬁned at this stage, we abstract the power consumption as the power consumed by point to point physical links between two routers. For each core i, let Ri denote the set of routers located at its four corners. Core i is mapped to one of the routers in Ri . Let Xi,j denote a (0,1) integer variable that is set to 1 if node i is mapped to router j ∈ Ri , else 0. Let Xi,j,k,l denote a (0,1) integer variable that is set to 1 if node i We deﬁne these variables only when (i, k) ∈ E or (k , i) ∈ E . is mapped to router j , and node k is mapped to router l, else 0. The objective is to minimize the power consumption expressed as: M inimize Z = ω(i, k) ·ψl ·dist(j, l) ·Xi,j,k,l (cid:1) (cid:1) (cid:1) (i,k)∈E j∈Ri l∈Rk (1) where dist(j, l) is the Manhattan distance between the two routers j and l. In this section we prove that the core to router mapping problem is equivalent to max-ﬂow min-cut problem, and therefore can be solved optimally in polynomial time. A. Equivalence to max-ﬂow min-cut problem The minimization goal can be split as the sum of two terms: Z = σ(i, k)·x(j, l)·Xi,j,k,l+ σ(i, k)·y(j, l)·Xi,j,k,l (cid:1) (cid:1) (i,k),j,l (i,k),j,l (2) where σ(i, k) = ω(i, k) · ψl , x(j, l) is the x-offset between the two routers, and y(j, l) is the y-offset between the two routers. We can consider the overall problem as a composition of two sub-problems that determine the x and y co-ordinate, respectively of the router to which the core is mapped. Without loss of generality we ﬁrst consider the sub-problem that determines the x co-ordinates of the routers for all the cores. Thus, we wish to determine if the core should be mapped to a router in the x− (left hand of the core) or x+ (right hand side of the core) location. Based on the relative locations of two communicating cores vi and vk on the ﬂoorplan we have three cases as shown in top row of Figure 2. For each case we construct a ﬂow graph as shown in the lower row of the ﬁgure. In each graph we introduce two nodes x− and x+ in addition to vi and vk . We also add edges between the various nodes and annotate them with weights. The weight of an edge is derived from the various distances as speciﬁed in top row of the ﬁgure. A cut of each of the graphs that assigns x− and x+ to different partitions denotes the mapping of the cores to the routers. The weight of the cut given by the summation of weights of edges that are cut denotes the x-offset between the routers to which the cores have been mapped. For example in Case I, the cut C1 denotes that vk is mapped to x− and vi is mapped to x+ . The weight of C1 given by a denotes the x-offset between the x− router of vk and x+ router of vi . Similarly, the cut C2 denotes that vk and vi are both mapped to their respective x+ router, the x-offset is a + W (k). The cut weight multiplied by the bandwidth of trafﬁc between the two cores (ω(i, k)) and link power model (ψl ) denotes x-offset component of the power consumption due to the mapping (ﬁrst term of equation 2). For a pair of communicating cores (i, k) we can ﬁnd the lowest power consumption router mapping by generating the minimum cost cut in the ﬂow graph. We now generalize the above construction to the entire CTG. We construct a ﬂow graph G(W, F ) with W = V ∪ x+ ∪ x− where x+ and x− are additional nodes that represent the routers. For every trace (i, k) ∈ C T G(V , E ) we classify the trace in to one of the three categories based on the core locations, and introduce edges in ﬂow graphs. The edge weights are given by the product of the communication bandwidth (σ(i, k)), link power model (ψl ) and distance weights as described in the previous paragraph. Theorem 1: A cut in Graph G(W, F ) that assigns x− and x+ to different partitions represents a solution to the x coordinate sub-problem. Proof: Consider a core vi . A cut either intersects the edge from x− to vi , or the edge from x+ to vi , but not both. Moreover, a cut must intersect one of the two edges. If the edge from i to x− is cut, it represents that core vi is mapped to router at x− off-set (and vice versa). The weight of the cut represents the power consumption contribution due to the mapping of the routers to the respective x+ or x− routers. Thus, the cut captures both the mapping of the cores to the routers, as well as the power consumption incurred due to the assignments. Therefore, the cut represents a solution to the x co-ordinate sub-problem. From Theorem 1, it follows that if we could generate a cut of minimum cost, we have an optimal solution to the x co-ordinate sub-problem. This is a polynomial time solvable max-ﬂow-min-cut problem and we solve it by invoking the Push-Relabel algorithm [9] that has a complexity of O(n3 ) in the number of nodes of the graph. We can similarly solve the y co-ordinate sub-problem, the utilize the two solutions to determine the unique router for mapping each core. IV. ROU T ING AND TO PO LOGY G EN ERAT ION The output of the ﬁrst stage of our custom NoC design technique is a system-level layout with cores and routers, 186 B A H C D E F G X C F G H B A D E Core to router mapping Routing graph X’ Fig. 3. Generating routing graph and an assignment of the cores to speciﬁc routers (see left hand side of Figure 3). Thus, the source and sink routers for each trace are known. Our routing and topology generation technique addresses two problems namely, minimization of power consumption, and minimization of number of router resources in the network. We present our technique by ﬁrst discussing a base case that minimizes the number of routers in the NoC, and build upon the base case to incorporate low power requirements. A. Topology generation with least number of routers In this section, we present our technique to solve the problem of NoC topology generation such that the total number routers utilized in the topology is minimized. This problem is a node weighted generalized steiner forest problem, and is known to be NP hard [10]. We formulate the problem as an ILP. As mentioned before, it is known that integer relaxation of ILP formulations can be solved in polynomial time [9]. We prove that by utilizing iterative rounding on the integer relaxation of the ILP we can establish a 2-approximation on the quality of the solution in polynomial time. In other words the number of routers in the topology generated by our solution will be at most twice that of the optimal solution. In the following paragraphs, we ﬁrst present a cut based ILP formulation for our problem. The cut based ILP has exponential constraints, and hence, is not suitable for practical applications. Therefore, we also present a equivalent ﬂow based LP with polynomial number of constraints. We prove our 2-approximation bound by utilizing the LP of the cut based ILP, and iteratively solve the equivalent ﬂow based LP to obtain the solution in polynomial time. 1) Cut based ILP for minimizing routers: Given the set of routers and core to router mappings, our technique initially determines the available paths from the source to the sink of each trace. This is done as follows. We construct a graph Gr (Vr , Er ) called the routing graph, where Vr is the set of available routers, and there exists an edge e = {ri , rj } whenever the Manhattan distance between ri and rj is less than Dmax . Figure 3 depicts a core to router mapping and the corresponding routing graph. In the ﬁgure, the black squares that denote the routers, are the nodes of the routing graph. The edges of the routing graph denote the possible physical links in the ﬁnal NoC. Let C denote a cut that divides the nodes of the routing graph Gr into two subsets S and S (cid:2) . In Figure 3, the curve X − X (cid:2) represents a cut. For the cut, let δ(S ) denote the set of edges that cross the cut. The edges intersected by cut X − X (cid:2) form the set of edges of δ . For each cut C that divides the routers into sets S and S (cid:2) , we deﬁne a function called F (S ) 2B-5 that is set to 1 if there exists a trace such that its source lies in S and sink lies in S (cid:2) , or vice versa. Otherwise it is set to 0. Let Xr denote a (0,1) variable that is set to 1 if router r of the CTG is utilized in the NoC topology. Now, the LP can be formulated as follows: M inimize Z = Xr S.T (cid:1) r (cid:1) ∀C ∀e(r,s)∈δ(S ) Xr ≥ F (S ) The above constraint states that for a cut C that partitions the routers into sets S and S (cid:2) such that F (S ) = 1, at least one router which is adjacent to the cut must be utilized in the ﬁnal topology. Applying this constraint on all the cuts in the graph ensures that a route exists for all traces. Since the number of cuts in a graph is exponential, the cut based formulation deﬁnes a polytope with exponential constraints. We can reduce the number of constraints by utilizing an alternative ﬂow based formulation. It is a well known fact that the cut based formulation and the ﬂow based formulation are equivalent [11]. 2) Flow based ILP for minimizing routers: The objective of the ﬂow based formulation is same as that of the cut based formulation. Let Yr,s,k denote a (0,1) variable that is set to 1 if trace k passes through the physical link (r, s) ∈ L. For each core m in V , let mMr denote the relation that m is mapped to router r . We have the following constraints : • Any router that maps a core must be present in the network. Therefore, we have the following constraint. ∀r, ∃m ∈ V : mMr, Xr = 1 • A trace in the CTG always passes through the source and sink routers. For trace t = (m, n), let q denote the router that maps m, and r denote the router that maps n. Since the trace is routed through the two routers, one physical link connected to the respective routers must be present in the topology. Hence, the following equalities must hold. (cid:1) (cid:1) Yq ,s,t = 1 Ys,q ,t = 0 q :(q ,s)∈L q :(s,q)∈L The ﬁrst constraint ensures that there is exactly one link through which the trace t leaves router q . The second constraint ensures that the trace does not enter router q from any other router. Therefore, the constraint ensures that there are no cycles in the trafﬁc routes. Similar constraints for the router mapping the sink are as follows. (cid:1) (cid:1) Ys,r,t = 1 Yr,s,t = 0 r :(s,r)∈L r:(r,s)∈L • For each trace, the ﬂow due to the trace must be conserved in all routers except the routers that map the source and sink. (cid:1) ∀t ∈ E (cid:1) Yq ,r,t = Yr,s,t q :qr∈Gr r:rs∈Gr 187 ∀r, ∀k , Xr ≥ Yq ,r,k (cid:1) s:qr∈L 2B-5 A B A B A B F(A) = F(B) = F(A−B) = F(B−A) = 1 F(A) = F(B) = F(A−B) = F(B−A) = 1 F(A) = 0, F(B) = 1 F(A−B) = 0, F(B−A) = 1 A B A B A B F(A) = F(B) = 1  F(A       B) = F(A       B) = 1  F(A) = F(B) = F(A−B) = F(B−A) = 1  F(A) = F(B) = 1  F(A       B) = F(A       B) = 1  A B A B A B F(A) = 0, F(B) = 1  F(A      B) = 0, F(A       B) = 1  F(A) = F(B) = F(A−B) = F(B−A) = 1  F(A) = 1, F(B) = 0 F(A       B) = 1, F(A       B) = 0  A B A B F(A) = 0, F(B) = 1 F(A        B) = 1, F(A        B) = 0  F(A) = F(B) = F(A       B) = F(A         B) = 1 Fig. 4. Proof of supermodularity • If there exists a ﬂow through a router, that router is utilized in the topology. The ﬂow-based ILP only has O(|E ||L|) constraints. Therefore, the integer relaxation of the ﬂow based ILP is suitable for direct solution by solvers. 3) Algorithm and proof of 2-approximation: In this section, we present our technique for obtaining integer solution from the LP relaxation, and prove that our technique utilizes at most twice the number of routers compared to the optimal solution. Deﬁnition 1: A function F : 2R → Z is said to be weakly following conditions is true for any two sets A, B ∈ R : supermodular [10] if F (R) = 0, and at least one of the • F (A) + F (B ) ≤ F (A − B ) + F (B − A) • F (A) + F (B ) ≤ F (A ∩ B ) + F (B ∪ A) Lemma 1: The function F deﬁned in the cut based ILP is weakly supermodular. Proof: As all traces are contained in the Gr , F (R) = 0. In order to prove the second part of the theorem, we enumerate all cases, and show that the supermodularity condition holds. The different cases are shown in Figure 4. The ovals A, B denote the two sets, the black circles denote two cores which may either lie inside or outside the sets, and the line joining the cores denotes the communication trace. Each case in the ﬁgure is annotated with one of the inequalities that satisﬁes the supermodularity conditions. Fact 1: For any weakly supermodular function F , any to the extent of at least a half. In other words, Xr ≥ 1 extreme point solution x to the LP must pick some router 2 for at least one router r [10]. Our algorithm exploits Lemma 1 and Fact 1, and utilizes an iterative rounding procedure on the integer relaxation solutions of the ﬂow based ILP to generate the topology. The algorithm is presented below. Initialize H ← φ : F (cid:2) ← F Begin algorithm LP round While F (cid:2) (cid:12)= 0 do Solve LP to obtain solution X. For each variable x ∈ X do If (x ≥ 1 2 ) then round x to 1 H = H ∪ x (cid:2) ∀q ,∀t Yp,q ,t + Xp ≥ (cid:2) end-If Update ∀S ⊆ V , F (cid:2) (S ) = F (S ) − |δH (S )| End for End While Output H End algorithm LP round Theorem 2: Algorithm LP round achieves an approximation ratio of 2 for the routing problem. Proof: Jain [10] gave an LP formulation for the edge weighted steiner tree problem, and utilized an iterative rounding procedure that results in factor 2 approximation for the problem. Our problem is an instance of the node weighted steiner tree. Consider an optimal solution (X, Y ) to our LP. First, note that Y alone is feasible for the LP of Jain, because the constraints that do not involve X variables in our LP are the same as the ones of Jain’s LP: they require that a certain ﬂow be supported (or that certain cuts be crossed) by the selected edges. Thus Jain’s result implies that in an optimal solution to the LP there exists a triple (p, q , t) such that Yp,q ,t ≥ 1/2. Now for this particular p, we have the constraint ∀o,∀t Yo,p,t , and so Xp ≥ 1/2. That is, in any optimal solution to our LP, there exists an p with Xp ≥ 1/2, and so in the for loop that the algorithm exercises in each iterative rounding phase, at least one more variable is ﬁxed to 1. By Lemma 1, and Fact 1, our LP in the next phase satisﬁes all the requirements for Jain’s theorem, and so the algorithm proceeds until a feasible integral solution is found. Now we note that every time any variable Xp was ﬁxed to 1, this was either as part of an optimal LP solution, or because its fractional value was at least 1/2. To prove the approximation bound, we use induction on the number of iterations (phases). In the base case, the algorithm only rounds up edges with fractional value at least 1/2 and so the cost of the integral solution is at most twice the LP cost which is no more than the optimum. Suppose now for the induction step that X is the LP solution found in the ﬁrst iteration. Let W be the set of vertices (routers) picked by rounding in the ﬁrst iteration. By Fact 1, W is nonempty. Let X (cid:2) be the vector we get from X when we set all its components that are less than 1/2 to zero. Since the rounding was applied only to the variables with values at least 1/2, cost(W ) ≤ 2cost(X (cid:2) ). Let W (cid:2) be the set of vertices picked in the later phases to satisfy the remaining requirements. The crucial observation is that cost(W (cid:2) ) ≤ 2cost(X −X (cid:2) ). This follows because X −X (cid:2) satisﬁes all the constraints that are not satisﬁed by W , and so is a feasible solution to the residual LP instance. By the induction hypothesis, the cost of the set of vertices picked solution OP T (cid:2) , that is, cost(W (cid:2) ) ≤ 2OP T (cid:2) ≤ 2cost(X −X (cid:2) ). in subsequent phases is at most twice the optimal residual LP Now since W + W (cid:2) satisﬁes all the original constraints, we have cost(W + W (cid:2) ) ≤ cost(W )+cost(W (cid:2) ) ≤ 2cost(X (cid:2) ) + 188 snk src 1 0 1 snk 3 dummy source 5 3 3 6 10 6 6 4 3 4 1 2 5 8 3 4 snk 2 0 2 5 8 snk 1 src 2 src 7 9 6 10 src 3 (A) Graph Gr(Vr, Er) dummy sink (B) Shortest path graph (SPG) Fig. 5. Minimum power versus minimum routers Fig. 6. Construction of SPG 2cost(X − X (cid:2) ) ≤ 2cost(X ). Comparing the leftmost side to the rightmost, we see that the total cost of vertices selected by the algorithm is at most twice the LP relaxation cost, and so at most twice the optimum. B. NoC topologies with minimum power consumption For a trace tr , we deﬁne a shortest path route to be one that consumes minimum power. The total power consumption for the route is given by the power consumption due to the routers as well as the physical links. The technique presented in the previous section does not ensure shortest path routes for the traces. Therefore, some traces may be routed by longer paths, resulting in higher power consumption. For example, consider the route for trace tr in Figure 5. In the ﬁgure, the darkened circles represent routers that are already present in the topology, and the empty circles are not present in the topology. A trafﬁc route for tr that consumes minimum number of routers is shown by heavy lines in the ﬁgure. Clearly, this is not the shortest path for the trace. The shortest path shown by dashed lines in the ﬁgure, consumes two extra routers. If the design objective is to minimize power consumption, the shortest path should be chosen at the expense of extra routers. 1) Determination of shortest paths: In order to determine the shortest paths, we again consider the routing graph. We associate a weight with each edge, which intuitively gives us the power consumption if the physical link corresponding to the edge is utilized in the ﬁnal topology. For each edge e = {ri , rj }, ρe denotes its edge weight, and is given by ρe = ψi + ψo + le × ψl where le denotes the Manhattan distance between ri and rj . The edge weights are assigned such that they capture the link as well as the router power consumption. Now, for each trace, we invoke Dijkstra’s shortest path algorithm to determine the available shortest paths. Note that there can be multiple shortest paths from a source to a sink. The output of the call to the algorithm is a collection of subgraphs, each subgraph denoting the shortest paths for a particular trace. Consider the routing graph Gr depicted in Figure 6(A). In the example, we are required to route three traces, (src1 , snk1 ), (src2 , snk2 ), and (src3 , snk3 ). The corresponding shortest path graphs are depicted in part (B) of the ﬁgure. Each node has a unique ID expressed as a double (r, tr) where r denotes the router in Vr , and tr denotes the 2B-5 trace to which the node belongs. For example, router (2, 1) denotes router with ID 2 in Vr , and belonging to the graph corresponding to trace 1. We denote this graph by shortest path graph or SPG. We deﬁne I (r, tr) = 1 if node (r, tr) is present in the solution obtained by invoking our routing and topology generation technique on the SPG. Otherwise, I (r, tr) = 0. We now deﬁne a mapping function, I (r × tr) → Xr as follows. (cid:3) Xr = 1, if ∀tr ∈ E , ∃I (r, tr) = 1 0, otherwise Xr is set to 1 if any of the traces utilizes router r . Therefore, Xr denotes the number of routers in the topology. We obtain our topology with minimum routers and shortest paths by invoking our linear programming based technique on the SPG, with an objective of minimizing the the sum of Xr . The cut based formulation is formulated as (cid:1) M inimize (cid:1) ∀C ∀e((r,tr),(s,tr))∈δ(S ) Xr S.T r Xr ≥ F (S ) As before, we formulate an equivalent ﬂow based formulation for the problem, and solve it in polynomial time with a 2approximation guarantee. C. Post processing steps At the end of the mapping and routing stages, the architecture may have routers that are placed very close to each other. In order to eliminate redundant routers, we merge pairs of routers that are less than distance Dmax apart. Our merging technique checks all pairs of available routers. Two routers are candidates for merging if the distance between them is less than Dmax , merging the two routers does not violate the Dmax constraint for any other router in the topology, and the overall power consumption as a result of merging does not increase. The merging step is continued as long as there are pairs of routers that satisfy the above criteria. Finally, our technique invokes post processing steps for bandwidth satisfaction, and deadlock avoidance. The width and number of physical links between two routers are increased such that they can support the required bandwidth. Potential deadlocks can be detected at design time by constructing the (virtual) channel dependency graph (CDG) and checking for cycles [12]. The cycles in the CDG can be removed by introducing additional virtual channels at selective router locations. V. R E SULT S In this section, we present the results obtained by execution of our technique on several multimedia and network processing benchmark applications. We compare the results generated our technique (henceforth called guaranteed quality algorithms (GQA)) with an optimal ILP based technique [2], and a recursive partitioning based heuristic [13] called ANOC that addresses the same problem. The benchmarks included 189 Graph JPEG Encoder MPEG-4 decoder MWD VOPD Set-top Box AH Auth-IPv4 Diffserv-IPv4 NP1 NP2 NP3 Graph ID G1 G2 G3 G4 G5 G6 G7 G8 G9 G10 Nodes 8 12 12 12 25 9 11 15 17 24 Edges 21 13 13 13 27 8 10 22 24 42 Fig. 7. Benchmarks 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 2.2 2.4 2.6 G1 G2 G3 G4 G5 G6 Benchmark G7 G8 G9 G10 V s e u a l N o r m a i l d e z t o M I u o S P L l i t n o GQA(power) ANOC(power) GQA(routers) ANOC(routers) Fig. 8. Power and router comparisons for Case-1 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 2.2 2.4 G1 G2 G3 G4 G5 G6 Benchmark G7 G8 G9 G10 V s e u a l N o r m a i l d e z t o M I u o S P L l i t n o QOA(power) ANOC(power) GQA(routers) ANOC(routers) Fig. 9. Power and router comparisons for Case-2 5 multimedia applications (benchmarks G1 through G5) and 5 network processing applications (benchmarks G6 through G10). The sizes of the benchmarks ranged from 8 nodes and 21 edges to 24 nodes and 42 edges. The benchmarks were obtained from the work presented by Hu et al. [14], Pasricha et al. [15], and Ramamurthi et al. [16]. We present results for two cases: 1) NoC design subject to a maximum link length (Dmax ) constraint of 6mm henceforth called Case-1, and 2) NoC design without link length constraints, henceforth called Case-2. In accordance with the proof in Section III, the core to router mapping stage produced optimal solutions. The solutions generated by the routing and topology generation stage also converged to optimal solutions. Figures 8 and 9 present the comparisons of power and router resource consumption between our technique and the existing techniques. The bars in the ﬁgures are normalized to the optimal solution generated by the ILP based technique. In the ﬁgures, the ﬁrst bar denotes the power consumption of our technique, the second bar denotes the power consumption of ANOC, the third bar denotes the router resource consumption of our technique, and the fourth bar denotes the router resource consumption of ANOC. Since the ILP based technique optimizes power consumption in isolation, our technique consumed less routers than the ILP for some benchmarks. For Case-1 (Case-2), our technique consumed 1.04(1.04) times the power consumption of the optimal ILP based technique, and 0.93(0.9) times the power consumption of the heuristic technique. The corresponding values for router comparison were 1.12(1.21) and 1.21(1.31) respectively. The higher router consumption is due to the imposition of low power requirements that trades of the number of routers for lower power. While our technique generated results in less than a second, the ILP based technique took several hours to converge to the optimal solution. However, due to its lower complexity, the heuristic generated solutions in less than a second as well. V I . CONC LU S ION In this paper, we presented approximation algorithms for the design of custom NoC architectures. We presented polynomial time optimal and factor-2 approximation algorithm for the core to router mapping, and topology generation stages, respectively. We also presented graph transformations and heuristic techniques such that the NoC consumes minimum power, minimum router resources, and satisﬁes the architecture speciﬁc port and bandwidth constraints on the routers. We demonstrated the superior quality of the solutions generated by our technique by experimenting with an optimal ILP formulation, and an existing heuristic called ANOC. ACKNOW L EDG EM EN T This work was partially supported by NSF grant (Career CCF-0546462 and CCF-0509540) and Consortium for Embedded Systems. "
2008,Interconnect modeling for improved system-level design optimization.,"Accurate modeling of delay, power, and area of interconnections early in the design phase is crucial for effective system-level optimization. Models presently used in system-level optimizations, such as network-on-chip (NoC) synthesis, are inaccurate in the presence of deep-submicron effects. In this paper, we propose new, highly accurate models for delay and power in buffered interconnects; these models are usable by system-level designers for existing and future technologies. We present a general and transferable methodology to construct our models from a wide variety of reliable sources (Liberty, LEF/ITF, ITRS, PTM, etc.). The modeling infrastructure, and a number of characterized technologies, are available as open source. Our models comprehend key interconnect circuit and layout design styles, and a power-efficient buffering technique that overcomes unrealities of previous delay-driven buffering techniques. We show that our models are significantly more accurate than previous models for global and intermediate buffered interconnects in 90 nm and 65 nm foundry processes - essentially matching signoff analyses. We also integrate our models in the COSI-OCC synthesis tool and show that the more accurate modeling significantly affects optimal/achievable architectures that are synthesized by the tool. The increased accuracy provided by our models enables system-level designers to obtain better assessments of the achievable performance/power/area tradeoffs for (communication-centric aspects of) system design, with negligible setup and overhead burdens.","3B-1 Interconnect Modeling for Improved System-Level Design Optimization Luca Carloni‡ , Andrew B. Kahng† , Swamy Muddu† Alessandro Pinto+ , Kambiz Samadi† , Puneet Sharma† ‡ CS Department, Columbia University, New York, NY † ECE Department, University of California San Diego, La Jolla, CA + EECS Department, University of California Berkeley, Berkeley, CA Email: luca@cs.columbia.edu, {abk,smuddu,ksamadi,sharma}@ucsd.edu, apinto@eecs.berkeley.edu Abstract Accurate modeling of delay, power, and area of interconnections early in the design phase is crucial for effective system-level optimization. Models presently used in system-level optimizations, such as network-on-chip (NoC) synthesis, are inaccurate in the presence of deep-submicron effects. In this paper, we propose new, highly accurate models for delay and power in buffered interconnects; these models are usable by system-level designers for existing and future technologies. We present a general and transferable methodology to construct our models from a wide variety of reliable sources (Liberty, LEF/ITF, ITRS, PTM, etc.). The modeling infrastructure, and a number of characterized technologies, are available as open source. Our models comprehend key interconnect circuit and layout design styles, and a power-efﬁcient buffering technique that overcomes unrealities of previous delay-driven buffering techniques. We show that our models are signiﬁcantly more accurate than previous models for global and intermediate buffered interconnects in 90nm and 65nm foundry processes - essentially matching signoff analyses. We also integrate our models in the COS I -OCC synthesis tool and show that the more accurate modeling signiﬁcantly affects optimal/achievable architectures that are synthesized by the tool. The increased accuracy provided by our models enables system-level designers to obtain better assessments of the achievable performance/power/area tradeoffs for (communication-centric aspects of) system design, with negligible setup and overhead burdens. 1 Introduction Due to increasing complexity of Systems-on-Chip (SoCs) and poor scaling of interconnects with technology, on-chip communication is becoming a performance bottleneck and a signiﬁcant consumer of power and area budgets [11, 28]. Decisions made in the early stages of the design process have the maximum potential to optimize the system for objectives such as power [21]. Therefore, in order to drive meaningful optimizations and to reduce guardbanding, it is crucial to account for interconnects during system-level design by modeling their performance, power, and area. During system design, organizational and technological choices are performed. At this stage, we are concerned with implementing the hardware architecture sketched in the conceptualization and modeling steps. Design is supported by hardware synthesis tools and software compilers. Energy efﬁciency can be obtained by leveraging the degrees of freedom of the underlying hardware technology. Even within the framework of a standard implementation technology, there are ample possibilities for reducing power consumption. System-level decisions affect primarily the global interconnects by setting their lengths, bit widths, and speed requirements. Local interconnects are typically less affected as they are either already routed in IP blocks or are routed by automatic back-end routing tools. This paper focuses on interconnect delay, power, and area models that are usable by the system-level designer at an early phase of the design process. We ﬁrst study the requirements of a system-level designer to model global interconnects and discuss the shortcomings of the models that are presently used. We then describe our models and present a reproducible methodology to obtain them. Since the accuracy of such models relies on the accuracy of the underlying technology parameters, we also highlight reliable sources that are easily available to the system-level designer for present and future technologies. We compare predictions from our models with existing models and show the impact of improved accuracy on systemlevel design choices by contrasting the NoC topologies generated by COS I -OCC [19] with the existing models and with our models. The remainder of the paper is organized as follows. In Section 2 we describe a CAD tool for the automatic synthesis of networks-onchip. Section 3 investigates the model requirements for such automatic tool. In Section 4 we develop accurate physical models for wires and repeaters, which are building block components of NoCs. In Section 5 we validate the accuracy of our buffered interconnect delay model against industry’s golden tool (i.e., PrimeTime SI) and also show the impact of the new models on the optimal NoC conﬁgurations that can be achieved with the CAD tool. Finally, Section 6 concludes and gives directions for future work. 2 Communication Synthesis Packet-switched networks-on-chip (NoC) have been proposed as the solution to the problem of connecting an increasing number of processing cores on the same die [4, 8, 10]. Key steps in the optimization of the NoC design include topology selection and assignment of routes for packets as they travel from a source core to a destination core. Some network design ideas can be borrowed from the computer science community that addressed the same problems for local area networks and supercomputer networks. However, the challenge is leveraging the intrinsic characteristics of on-chip communication to achieve both energy efﬁciency and high performance [15]. Each target silicon technology offers a variety of possibilities to the NoC designers who, for instance, can decide the number and positions of network access points and routers as well as which metal layer to use for implementing each given channel. Because the design space of the possible topologies is so large, choosing the best one is a difﬁcult problem that cannot be solved only by experience. In fact, the problem is even harder given the heterogeneity of cores, and the trafﬁc patterns among them. Therefore, the development of automatic tools to design NoC is a key enabler for the success of the NoC design paradigm. COS I OCC is an open-source software infrastructure for the automatic synthesis of On-Chip Communication (OCC) [19]. Figure 1 shows the design ﬂow implemented in COS I -OCC . The input is a project ﬁle that contains pointers to the communication constraint ﬁle and to the library ﬁle. The constraint ﬁle contains the description of the IP cores and the communication constraints among them. An IP core can be manually placed on the chip, thereby having ﬁxed position and dimensions, or it can be characterized by its area only. If there are unplaced IP cores, PAR QU E T [1] is used to ﬂoorplan the chip. An end-to-end communication constraint is deﬁned by a source core, a destination core, a minimum bandwidth and a maximum number of hops. The library ﬁle contains the description of the library elements. 978-1-4244-1922-7/08/$25.00 ©2008 IEEE 1 258 3B-1 count and introduce unnecessary routers between communicating blocks; this in turn can increase chip area and power consumption. Existing methods for on-chip communication synthesis [18] and analysis [10] primarily use “classic” delay and power models of Bakoglu [3], or more recently of Pamunuwa et al. [16]. Popularity of these models with the NoC research community is likely due to the following reasons: • Simplicity and ease of use. Bakoglu’s delay model for buffered interconnect lines [3] is based on lumped approximation of the distributed parasitics of the interconnect. Driver and buffers are modeled as simple voltage sources with series resistances connected to the interconnect load. These approximations make the buffered line delay model amenable to analytical, closedform representation, and hence adoptable in NoC synthesis ﬂows. • First-order accuracy. Bakoglu and Pamunuwa et al. use a simple step voltage source with series resistance to represent a driver/buffer. Interconnect load is lumped at the output of the cell to compute cell delay. Interconnect delay is computed as Elmore delay, i.e., the ﬁrst moment of the impulse response of the distributed RC line. These models of buffers and wires are accurate to ﬁrst order and capture signiﬁcant contributors to delay. • Inertia. There have not been any compelling reasons to use alternative, more accurate models. To this point, our present work shows that accurate models can still be simple, and that different optimization results and trends follow from use of improved models. The remainder of this subsection lists key factors that are not addressed by existing delay models. In 180nm and below process nodes, these factors lead to inaccuracy in delay (latency) computation. Transition time (slew) dependence. The simple cell delay model (step voltage source with constant series resistance) no longer captures delay impact. A ﬁnite input slew rate changes the drive resistance and, consequently, cell delay as well as the output voltage waveform that drives other cells. To the best of our knowledge, none of the delay models used in NoC literature consider the impact of slew on delay. Interconnect resistivity. Resistance affects interconnect delay directly and it shields the load capacitance experienced by driving buffers. As the interconnect dimensions continue to scale, electron scattering has started to affect the resistivity. Copper interconnect manufacturing requires use of a barrier layer that reduces the effective width of the metal. Existing delay models ignore these two effects and sacriﬁce considerable accuracy. Coupling capacitance. Crosstalk from capacitive coupling affects signal transition times and delay along interconnects. Classic models such as Bakoglu’s do not consider coupling between neighboring interconnects, and hence are oblivious to resulting delay and transition time changes. Pamunuwa et al. consider the impact of switching activity on the ‘Miller’ coupling between neighboring lines and hence delay, but fails to model the impact on transition time. This leads to inaccurate delay computation for cells driven by the affected signal. The aforementioned deﬁciencies in gate and wire delay models are addressed to some extent in the large body of work on gate delay [2, 9] and interconnect delay [17, 22] modeling. However, such models (e.g., AWE-based approaches [22]) need detailed interconnect parasitic information which is unavailable at the system-level design phase. For gate delay, works such as that of Arunachalam et al. [2] model input voltage as a piecewise-linear function and choose the value of series resistance more elaborately. The main drawbacks of such approaches is that they model drive resistance independent of input transition time (slew). In reality, drive resistance (Rd ) varies with input slew. This also affects output slew. Shao et al. [25] recently proposed a gate delay model that relies on a second-order RC model of the gate. They propose analytical formulas for computing 	&$$+% * &% ' . &%  %%'# 	&(  +% ' .* &% (&!* '* ($*() #&( *$ #&( *$ #&( *$ #*&($  ((&#) $'#$%** &% 	&%(* &% ,  &* '&(* -)*$	 "".#  -)	   $+#* &% Figure 1: COS I -OCC design ﬂow. Each element is characterized by a set of architectural parameters (such as ﬂit width, maximum number of input and output ports of a router, etc.) and a model that deﬁnes its performance and cost (in terms of area and power) The user can select the appropriate synthesis algorithm to derive an implementation depending on the optimization goal (minimum power, minimum area or minimum delay). The development of new synthesis algorithms is simpliﬁed by the simple standard interface with the library. This interface deﬁnes an API to retrieve the performance and cost of a component (e.g. a point-to-point link) given its conﬁguration (e.g. clock speed, total bandwidth). Such simple API, available for each component, is of extreme importance to system level designers that are not concerned with low-level technology details. COS I -OCC provides a set of code generators (S Y S T EMC , S VG, DOT) including a textual report of the properties of the communication implementation like power consumption, area, number of hops, total wire-length, number of routers etc. 3 Model Requirements System-level designers require accurate, yet simple models of implementation fabrics (i.e., communicating entities and interconnections between them) in order to bridge planning and implementation, and enable meaningful system design optimization choices. Today, performance and power modeling for system-level optimization suffers from: • poor deﬁnition of inputs required to make design choices; • ad hoc selection of models as well as sources of model inputs; • lack of model extensibility across multiple/future technologies; and, • inability to explore different implementation choices and design styles. In this section, we discuss the accuracy and extensibility of previous models as well as key modeling deﬁciencies that our work addresses. 3.1 Accuracy Communication mechanisms between subsystems (such as IP blocks and routers) are realized using high-speed bus structures or point-topoint interconnects. The delay and bandwidth envelope of such interconnects is deﬁned by optimally buffered structures, and must be accurately modeled to enable synthesis of optimal (i.e., minimumlatency or minimum-power) communication topologies. Just as with technology mapping in logic synthesis, on-chip communication synthesis is driven by models of latency and power consumption. The accuracy of such models should be comparable to that available during physical synthesis due to the high sensitivity of design outcomes. For example, poor models of interconnect latency can increase hop 259 3B-1 the output voltage waveform for a given ramp input waveform. However, they do not address gate loading during model construction. For a gate delay model to be accurate, drive resistance dependence on input slew, and output slew dependence on load capacitance and input slew, must both be considered. 3.2 Design Styles and Buffering Schemes System-level designers usually ignore design-level degrees of freedom such as wire shielding, wire width and spacing perturbation, etc. when modeling interconnect latency and power. Yet, optimizations of design styles or buffering schemes can have huge impact on the envelope of achievable system performance. For example, shielding an interconnect with quiescent lines on both sides reduces worstcase capacitive coupling and improves delay. Wire width sizing and spacing also improve delay. In addition to design style choices, the buffering objective can also be signiﬁcant. Interconnect delay models of Bakoglu and Pamunuwa et al. incorporate buffering schemes that minimize end-to-end delay (min-delay buffering), and are used extensively in the NoC literature. However, min-delay buffering can result in unrealistically large buffer sizes, and high dynamic and leakage power. It is necessary for system-level design optimization to comprehend power-aware buffering schemes, and more generally the key circuit-level choices that maximize achievable performance. 3.3 Model Inputs and Technology Capture Perhaps the most critical gap in existing system-level and NoC optimizations has been the lack of well-deﬁned pathways to capture necessary technology and device parameters from the wide range of available sources. Since exploration of the system-level performance and power envelope is typically done for current and future technologies, the models driving system-level design must be derivable from standard technology ﬁles (e.g., Liberty format [14], LEF [13]), as well as extrapolatable models of process (e.g., PTM [20], ITRS [12]). Earlier works on NoC design space exploration and synthesis [18, 27] collect inputs from ad hoc sources to drive internal models of performance, power and area. However, the use of non-standard interfaces and data sources can often lead to misleading conclusions that can have signiﬁcant impact on the ﬁnal outcome precisely because exploration is being performed at a very-high level. Instead, inputs that accompany system-level models must come from standard sources and be conveyed through standardized interfaces and formats. 4 Buffered Interconnect Model In this section we describe our models and present a methodology to construct them from reliable and easily accessible sources for existing and future technologies. We account for previously-ignored crucial effects such as slew-dependent delay and scattering-dependent wire resistivity change. Our models are by construction calibrated against SPICE and contain well-deﬁned parameters. 4.1 Repeater Delay Model We now present our repeater delay model and describe its derivation.1 For brevity, the following study is presented only for rise transitions in inverters implemented in 65nm technology. The derived functional forms are identical for fall transitions, for buffers, and for 90nm technology; only the function coefﬁcients change. Repeater delay can be decomposed into load independent and load dependent components as follows: dr = i + rd · cl (1) where dr is the repeater delay and i is the load-independent or intrinsic delay of the gate. rd · cl is the load dependent delay term, where rd is the drive resistance and cl is the load capacitance. 1We use the term ‘repeater’ to denote both an inverter and a buffer. 260 0.12 0.1 0.08 0.06 0.04 0.02 ) s n ( y a e l D c i s n i r t n I y = -0.1436x2 + 0.234x + 0.0083 INVD4 INVD6 INVD8 INVD12 INVD16 INVD20 INVD24 Quad (INVD12) 0.6 0.8 0 0 0.2 0.4 Input Slew (ns) Figure 2: Dependence of repeater intrinsic delay on input slew and inverter size. Intrinsic delay is essentially independent of repeater size, and depends quadratically on input slew. The intrinsic delay, i, can potentially depend on the input slew of the gate and the gate size. However, as seen in Figure 2, i is practically independent of the gate size but depends nearly quadratically on the input slew.2 The quadratic dependence of intrinsic delay on input slew is captured by the equation i(si ) = α0 + α1 · si + α2 · s2 i (2) where si denotes the input slew and α0 , α1 , and α2 are the coefﬁcients determined by quadratic regression. The dependence of drive resistance on input slew has often been ignored [3, 16, 7], but this can contribute to substantial error in delay prediction. Figure 3 shows the dependence of rd on input slew and repeater size. We observe that rd is nearly linear with input slew especially for larger input slew values. We also note that both the intercept and slope vary with repeater size; hence, rd can be written as rd = rd 0 + rd 1 · si (3) where rd 0 and rd 1 are the coefﬁcients, both of which can depend on the repeater size. Both rd 0 and rd 1 can be readily calculated using linear regression for a few repeater sizes and their dependence on repeater size studied. Previous works (e.g., [3]) have assumed rd to be inversely proportional to the repeater size. We have conﬁrmed this relationship to be sufﬁciently accurate for sub-90nm technology modeling. To be precise, we use the PMOS (NMOS) device width as the repeater size for rise (fall) transitions. As shown in Figure 4, both rd 0 and rd 1 are linearly proportional to the inverse of the repeater size (i.e., are inversely proportional to the repeater size), and the exact coefﬁcients can be calculated using linear regression with zero intercept 3 . I.e., (4) (5) rd 0 (wr ) = β0 /wr rd 1 (wr ) = β1 /wr 2 The independence of intrinsic delay from gate size can be understood as follows. For inverters, larger sizes are attained by connecting in parallel multiple identical devices (ﬁngers), which switch simultaneously and have negligible impact on each other. As the inverter size increases, the number of parallel-connected devices increases but the intrinsic delay remains unaffected due to the independent switching of the devices. For buffers, the intrinsic delay additionally comprises of the delay of the inverter in the ﬁrst stage which drives the inverter in the second stage. As the buffer size increases, the size of the second stage inverter increases but the size of the ﬁrst stage inverter is also increased to maintain small intrinsic delay. Consequently, the total intrinsic delay of buffers is nearly independent of the buffer size. 3 All graphs are generated using simple SPICE simulations for a set of input slew values, output capacitance values, and repeater sizes. 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 0 0.2 0.4 Input Slew (ns) 0.6 0.8 D r i e v R s e i s t e c n a ( O K h m s ) INVD4 INVD6 INVD8 INVD12 INVD16 INVD20 INVD24 Linear (INVD4) Linear (INVD6) Linear (INVD8) Linear (INVD12) Linear (INVD16) Linear (INVD20) Linear (INVD24) Figure 3: Dependence of drive resistance on input slew and repeater size. Drive resistance depends linearly on the input slew. Both the intercept and the slew are affected by the repeater size. where wr is the repeater size and is equal to the PMOS (NMOS) width for rise (fall) transitions, and β0 and β1 are the ﬁtted coefﬁcients. y = 2.219x y = 1.2521x 0 0.2 0.4 0.6 0.8 1 1.2 0 0.1 0.2 0.3 0.4 0.5 Inverse of Repeater Size (1/micron) 0.6 r 0 d d n a r 1 d rd0 rd1 Linear (rd0) Linear (rd1) Figure 4: Coefﬁcients rd 0 and rd 1 vary linearly with the inverse of the repeater size with zero intercept. Repeater Output Slew Model Since our gate delay model depends on input slew, we must also model output slew of the previous stage of the buffered interconnect. Slew is not a crucial metric at the system level, and its only use arises in delay calculation. Furthermore, while repeater delay depends on slew, inaccuracies arising in slew estimation tend to be masked in delay calculation. As a result, accuracy requirements for the slew model are less stringent than those for the delay model. As with gate delay, slew depends on repeater size, input slew, and load capacitance. Figure 5 shows the dependence of output slew on load capacitance and input slew. Slew depends strongly on the load capacitance, and we have found a linear relationship to be a good tradeoff between simplicity and accuracy. We note that the slope is nearly independent from the input slew, while the intercept is dependent on it. Assuming that the intercept depends linearly on the input slew, the output slew for a given repeater can be written as so (cl , si ) = so0 + so1 .si + so2 .cl (6) where so is the output slew, and so0 , so1 , and so2 are the ﬁtting coefﬁcients readily derived from multiple linear regressions. 0 0.1 0.2 0.3 0.4 0.5 0.6 0 0.5 1 1.5 Load Capacitance (pF) 2 2.5 O u t u p t S l e w ( s n ) 0.0056 0.0168 0.0392 0.084 0.1728 0.352 0.7088 Linear (0.352) Linear (0.7088) Figure 5: Dependence of output slew on load capacitance and input slew. Output slew depends nearly linearly on load capacitance. The slope of the linear ﬁt is nearly independent of the input slew, but the intercept depends on it. y = 1.9381x 0 0.2 0.4 0.6 0.8 1 1.2 1.4 0 0.2 0.4 0.6 Inverse of Repeater Size (1/micron) 0.8 0 o s , 1 0 s , 2 o s d n a so0 s01 s02 Linear (s01) Figure 6: Dependence of coefﬁcients so0 , so1 and so2 on inverse of repeater size. so0 and so2 are independent of repeater size, while so1 varies inversely with repeater size. The impact of repeater size on the coefﬁcients so0 , so1 , and so2 is shown in Figure 6. We consistently observe that so0 and so2 are independent of the repeater size, but so1 varies inversely with repeater size. Hence, output slew can be calculated as so (cl , si , wr ) = γ0 + γ1 .cl wr + γ2 .si (7) where γ0 , γ1 , and γ2 are constants. Repeater Input Capacitance Model The input capacitance of a repeater is required to calculate the load capacitance of the previous stage. As expected, the input capacitance is proportional to the repeater size. Typically, the P/N ratio is kept constant for repeaters of all sizes and the previous models (e.g., [3]) are sufﬁcient. However, even if the P/N ratio changes with repeater size, input capacitance can be modeled as ci = η × (w p + wn ) (8) where ci is the input capacitance, w p and wn are PMOS and NMOS widths respectively, and η is a coefﬁcient derived using linear regression with zero intercept. 4.2 Wire Delay Model For wire delay we use the model proposed by Pamunuwa et al. [16] which accounts for crosstalk-induced delay: dw = rw (0.4cg + λi 2 cc + 0.7ci ) (9) 3B-1 261 3B-1 where dw , rw , cg , cc and ci respectively denote wire delay, wire resistance, ground capacitance, coupling capacitance, and input capacitance of the next-stage repeater. λi is a coefﬁcient due to switching patterns of the neighboring wires and is equal to 1.51 for worstcase switching. We enhance the quality of the wire delay model by considering two important factors that affect wire resistance: • Scattering-aware resistivity. The rapid rise of wire resistivity due to electron scattering effects (grain boundaries and interfaces) at small cross-sections poses a critical challenge for onchip interconnect delay. For 65nm and beyond, scattering can degrade delay by up to 70% [26] and must be accounted in delay modeling. We adopt the following closed-form widthdependent resistivity equation [26]: ρ(w) = ρB + Kρ ww (10) where ww is the wire width, ρB =2.202 μΩ.cm, and Kρ=1.030×10−15 Ωm2 . The above model has been veriﬁed against measurement data from [23] and is used in the 2004 ITRS [12]. • Interconnect Barrier. To prevent copper from diffusing into surrounding oxide, a thin barrier layer is added to three sides of a wire. This barrier affects the wire resistance calculation as [11]: rw = ρ.lw (tm − tb )(ww − 2tb ) (11) where tm and tb respectively denote the metal and barrier thicknesses, lw is length of the wire, and ρ is computed using Equation (10). 4.3 Power Models Power is a ﬁrst-class design objective and must be modeled early in the design ﬂow [21]. In current technologies, leakage and dynamic power are the primary forms of power dissipation, and we develop models for them. In repeaters, leakage occurs in both output states. NMOS devices leak when the output is high, while PMOS devices leak when the output is low. This is applicable for buffers also because the second stage devices are the primary contributors due to their large sizes. Leakage power has two main components: (1) subthreshold leakage, and (2) gate-tunneling current. Both components depend linearly on device size. Thus, leakage can be calculated using: 2 0 , κn 1 , κ p ps = pn s + p p s pn s = κn 0 + κn 1 .wn P p s = κ p 0 + κ p 1 .w p (12) (13) (14) where pn s and p p s are the leakage for NMOS and PMOS devices, respectively, and κn 0 and κ p 1 are coefﬁcients determined using linear regression. State-dependent leakage modeling can also be performed using Equations (13) and (14) separately. In present and future technologies, the dynamic power of devices is primarily due to charging and discharging of capacitive loads (wire and input capacitance of next-stage repeater). Internal power dissipation, arising from charging and discharging of internal capacitances and short-circuit power, is noticeable for repeaters only when the input slews are extremely large. Dynamic power is given by the wellknown equations: pd = a · cl · v2 d d · f cl = ci + cg + cc (15) (16) 262 where pd , a, cl , vd d and f respectively denote the dynamic power, activity factor, load capacitance, supply voltage, and frequency. The load capacitance is the sum of the input capacitance of the next repeater, ci , and the ground (cg ) and coupling (cc ) capacitances of the wire driven. 4.4 Area Models Since repeaters are composed of several ﬁngered devices connected in parallel, repeater area grows linearly with the repeater size. For existing technologies, the area can be calculated as ar = τ0 + τ1 .wn (17) where ar denotes repeater area, and τ0 and τ1 are coefﬁcients found using linear regression. For future technologies, area values may not be available for performing linear regression. Hence, we propose the use of feature size, contacted pitch, and row height - all of which become available early in process and library development and are also predictable - to estimate area as: N F = (w p + wn + 2 · F )/RH RW = N F × (F + CP) + CP ar = RH × RW (18) (19) (20) where N F is the calculated number of ﬁngers, F is the feature size, RH is the row height, RW is the calculated row width, and CP is the contacted pitch. The area of global wiring can be calculated as aw = n × (ww + sw ) + sw (21) where aw denotes the wire area, n is the bit width of the bus, and ww and ss are the the wire width and spacing computed from the width and spacing of the layer (global or intermediate) on which the wire is routed, and from the design style. 4.5 Overall Modeling Methodology Our delay, power, and area models can be mathematically derived from the following inputs. • For repeater delay calculation, delay and slew values for a set of input slew and load capacitance values, along with input capacitance values, are required for a few repeaters. Since the coefﬁcients are derived using regression, a larger data set improves accuracy. The required data set is available from Liberty/TLF library ﬁles or can be generated using SPICE simulations for existing technologies. Since libraries are not available for future technologies, SPICE simulations must be used along with SPICE netlists for repeaters and predictive device models such as PTM [20]. To construct the repeater netlists, a PMOS/NMOS ratio is assumed (from previous technology experience or from expected PMOS/NMOS drive strengths, and is kept constant for all repeaters), and a variety of repeaters are constructed for different device sizes. • For wire delay calculation, we require the wire dimensions and inter-wire spacings for global and intermediate layers. These values are available in LEF (lateral dimensions) and ITF (vertical dimensions) ﬁles for existing technologies, and in the ITRS for future and existing technologies. • For power calculations, input capacitance (computed in repeater delay calculation) and wire parasitics (computed in wire delay calculation) are used. Additionally, device leakage is required and can be computed from the Liberty/TLF library ﬁles or SPICE simulations. 3B-1 4.7 Publicly-Available Framework Finally, we have developed a framework [29] capable of modeling and optimizing buffered interconnects for various technologies and under different design styles. The framework is accessible through XML ﬁles or through a C++ API. We have packaged the framework with XML ﬁles corresponding to a number of future and existing technologies corresponding to commercial foundry processes (90nm and 65nm), ITRS, and PTM. 5 Validation and Signiﬁcance Assessment We now assess the accuracy of our model and compare it with that of previously-proposed models ([3] and [16]). We perform the accuracy comparison on a 5mm long buffered interconnect for two technology choices (90nm and 65nm), two routing layer regimes (global and intermediate), two design styles (single-width-singlespacing and single-width-double-spacing). Since delay is linear with length for buffered interconnects, a length of 5mm is representative of other lengths that require buffering. To create the layout of a 5mm long buffered interconnect, we ﬁrst create the layout to deﬁne the chip area using Cadence SoC Encounter (version 6.1). Repeaters are then placed at equal distances along the length to buffer the interconnect uniformly. Connections between inputs, outputs and the buffers are created by Cadence NanoRoute. The values of minimum wire spacing and wire width are chosen from the input LEF ﬁle. Parasitic extraction on the buffered lines is performed using SoC Encounter’s built-in extractor. To perform timing analysis, we read in the parasitics output from SoC Encounter in SPEF format and the timing library (Liberty format) into PrimeTime (version 2006.12) for signoff delay calculation. We use TSMC 90nm and 65nm Liberty and technology ﬁles in our experimental setup. Results of our accuracy studies are presented in Table 1. Column 4 shows the delay of the buffered line evaluated in PrimeTime (input transition time = 300 ps). Columns 5, 6, and 7 show the error in delay prediction from Bakoglu, Pamunuwa and our model respectively. From the table we can observe that the delay from our proposed method matches that from PrimeTime within 15%. In comparison, previous models have error in the range of −65% to 160%. To assess the impact of improved accuracy on system-level design-space exploration, we integrate our models in COS I -OCC . We use two representative SoC designs as test cases. The ﬁrst design (VPROC) is a video processor with 42 cores and 128-bit datawidths. The second design is based on a dual video object plane decoder (dVOPD), where two video streams are decoded in parallel by utilizing 26 cores and 128-bit datawidths. Table 2 compares the interconnect power, delay, and area when the original [19] and proposed models are used. The clock frequencies used are 1.5 GHz and 2.25 GHz for 90nm, 65nm technology nodes, respectively. Hop count, which captures the communication latency, is also shown. The main differences between the NoC obtained using the original and the proposed models are in the area and hop-count. The critical sequential length, i.e. the maximum distance that a signal can travel in an optimally-sized and optimally-buffered interconnect within a single clock period [24], that was computed with the original model turns out to be very optimistic, allowing the use of excessively long wires. This is the case of a non-conservative abstraction that leads to design solutions that are actually not implementable. We note that the difference in area estimates between the original and proposed models is very large because of the simplistic area modeling in the original models. 6 Conclusions Accurate estimation of delay, power, and area of interconnections early in the design phase can drive effective system-level exploration. Existing models of buffered interconnections are inaccurate for current and future technologies (due to deep-submicron effects) and can ) s t t a w o r c i m ( r e w o P 60 50 40 30 20 10 0 90nm 65nm 0.6 0.7 0.8 0.9 Delay (ns) 1 1.1 Figure 7: Pareto-optimal frontier of the delay-power tradeoff for a 5mm buffered interconnect in 90nm and 65nm technologies. • For area calculations, wire dimensions used in wire delay calculation are used for wire area. Repeater area is readily available for existing technologies in Liberty or LEF ﬁles or from layouts. For future technologies, ITRS A-factors can be used or Equations 18-20 can be used along with the feature size, row height, and contacted pitch, all of which values are available early in process and library development. Finally, the total delay of a buffered interconnect is the sum of the delays of all repeaters and wire segments in it. We assume that there is negligible slew degradation and resistive shielding (of capacitive load) due to the wires. Table 3 lists the coefﬁcients derived for TSMC 90nm and 65nm high-speed technologies. 4.6 Interconnect Optimization Delay-optimal buffering optimizes the size and number of repeaters, and has been addressed under simple delay models in, e.g., [3, 16, 7]. However, delay-optimal buffering results in extremely large repeaters having sizes that are never used in practice due to area and power consumption considerations. Cao et al. [5] showed that use of smaller buffers improves the energy-delay product signiﬁcantly while only marginally worsening delay. While previously-proposed closed-form optimal buffering solutions are efﬁcient to compute, they are difﬁcult to adapt to more complex and accurate delay models such as ours. Furthermore, hybrid objective functions that optimize delay, power, and area are even more difﬁcult to handle. With this in mind, we have developed an iterative optimization technique that evaluates a given objective function for a given number and size of repeaters, while searching for the optimal (number, size) values. We have found that realistic objective functions are convex, making binary search for the optimal repeater size feasible. Our iterative optimization is easily extensible to other interconnect optimizations such as wire sizing and wire spacing, but the runtime grows exponentially with the number of optimization knobs. In general, wire sizing and wire spacing are weaker optimization knobs and their effect at the system-level can be ignored. We optimize only the number and size of repeaters during interconnect optimization. However, we support the use of double-width and double-spacing design styles which the system designer can invoke to optimize interconnect area, delay, noise, and power. Figure 7 shows the Pareto-optimal delay-power tradeoff for a 5mm global buffered wire in 90nm and 65nm technologies. We note that for both technologies, power can be reduced by 20% at the cost of under 2% degradation in delay. 263 3B-1 Table 1: Comparison of accuracy of Bakoglu [3], Pamunuwa [16], and the proposed models with respect to PrimeTime. Tech. Node Layer Design Style 90nm Global Intermediate 65nm Global Intermediate SW-SS SW-DS SW-SS SW-DS SW-SS SW-DS SW-SS SW-DS PrimeTime (ns) 0.670 0.515 0.620 0.650 0.505 0.395 0.735 0.505 Bakoglu (%) 97.0 120.4 9.7 -6.2 -6.9 6.3 -65.3 -52.5 Pamunuwa (%) 66.4 49.5 160.5 60.0 33.7 31.6 42.2 42.6 Proposed (%) -8.2 -9.7 0.8 6.9 -5.0 1.3 10.9 14.9 Table 2: Comparison of dynamic power (Pd yn ), static power (Pl eak ), device area (Ad ) and total area (At ot ) metrics relative to wires, and number of hops, between the original and proposed models. SoC VPROC Pd yn (mW) Pl eak (mW) Orig. Prop. Orig. Prop. 90nm 117.3 364.8 38.1 99.6 65nm 51.1 179.9 69.9 86.7 dVOPD 90nm 63.4 88.0 14.2 32.5 65nm 27.3 73.2 25.7 33.2 Ad (mm2 ) Orig. Prop. 0.070 0.009 0.036 0.007 0.026 0.003 0.013 0.003 At ot (mm2 ) Ave. # of hops Max. # of hops Orig. Prop. Orig. Prop. Orig. Prop. 0.370 0.346 3.09 3.01 4 5 0.217 0.223 3.10 3.42 4 6 0.141 0.162 1.76 1.76 3 3 0.082 0.085 1.76 1.91 3 4 Table 3: Coefﬁcients for our model derived from TSMC 90nm and 65nm technologies. α, β, and γ are for the rise transition. 90nm 65nm 90nm 65nm α0 0.013 0.008 γ0 0.015 0.012 κn 1 90nm 29.313 65nm 26.561 α1 0.217 0.234 γ1 5.553 4.162 κ p 0 1.261 1.238 α2 -0.088 -0.144 γ2 0.128 0.142 κ p 1 13.274 27.082 β0 3.008 2.219 η 0.0015 0.0011 τ0 1.312 0.657 β1 1.494 1.252 κn 0 -6.128 -6.034 τ1 1.099 0.866 lead to misleading design targets. We have proposed accurate models for buffered interconnects that are easily usable by system-level designers. We have presented a reproducible methodology for extracting inputs to our models from reliable sources. Existing delaydriven buffering techniques minimize interconnect delay without any consideration for power and area impact. This can often result in buffered interconnects that are infeasible during implementation. We have proposed a power-efﬁcient buffering technique that minimizes total power with minimal delay impact. To demonstrate the accuracy of our model, we evaluated its delay prediction for buffered interconnects in global and intermediate wiring layers across 90nm and 65nm technologies. Our results showed that delay from our proposed model matches that from a commercial sign-off tool within 15%. We integrated our model in an NoC topology synthesis tool (COS I -OCC) and found that accurate models substantially affect the explored topology solution. Our future work is in two directions: (1) development of models for higher-level communication architectures such as NoC’s and AMBA, and (2) extension of modeling to other upcoming metrics such as variability and reliability. 7 Acknowledgment This research is partially supported by the GSRC Focus Center, one of the ﬁve research centers funded under the Focus Center Research Program, a Semiconductor Research Corporation program. "
2008,NoCOUT - NoC topology generation with mixed packet-switched and point-to-point networks.,"Networks-on-Chip (NoC) have been widely proposed as the future communication paradigm for use in next-generation System-on-Chip. In this paper, we present NoCOUT, a methodology for generating an energy optimized application specific NoC topology which supports both point-to-point and packet-switched networks. The algorithm uses a prohibitive greedy iterative improvement strategy to explore the design space efficiently. A system-level floorplanner is used to evaluate the iterative design improvements and provide feedback on the effects of the topology on wire length. The algorithm is integrated within a NoC synthesis framework with characterized NoC power and area models to allow accurate exploration for a NoC router library. We apply the topology generation algorithm to several test cases including real-world and synthetic communication graphs with both regular and irregular traffic patterns, and varying core sizes. Since the method is iterative, it is possible to start with a known design to search for improvements. Experimental results show that many different applications benefit from a mix of ""on chip networks"" and ""point-to-point networks"". With such a hybrid network, we achieve approximately 25% lower energy consumption (with a maximum of 37%) than a state of the art min-cut partition based topology generator for a variety of benchmarks. In addition, the average hop count is reduced by 0.75 hops, which would significantly reduce the network latency.","3B-2 NoCOUT : NoC Topology Generation with Mixed Packet-switched and Point-to-Point Networks Jeremy Chan School of Computer Science and Engineering The University of New South Wales Sydney, Australia jeremyc@cse.unsw.edu.au Sri Parameswaran School of Computer Science and Engineering The University of New South Wales Sydney, Australia sridevan@cse.unsw.edu.au ABSTRACT Networks-on-Chip (NoC) have been widely proposed as the future communication paradigm for use in next-generation System-on-Chip. In this paper, we present NoCOUT, a methodology for generating an energy optimized application speciﬁc NoC topology which supports both point-to-point and packet-switched networks. The algorithm uses a prohibitive greedy iterative improvement strategy to explore the design space efﬁciently. A system-level ﬂoorplanner is used to evaluate the iterative design improvements and provide feedback on the effects of the topology on wire length. The algorithm is integrated within a NoC synthesis framework with characterized NoC power and area models to allow accurate exploration for a NoC router library. We apply the topology generation algorithm to several test cases including real-world and synthetic communication graphs with both regular and irregular trafﬁc patterns, and varying core sizes. Since the method is iterative, it is possible to start with a known design to search for improvements. Experimental results show that many different applications beneﬁt from a mix of “on chip networks” and “point-to-point networks”. With such a hybrid network, we achieve approximately 25% lower energy consumption (with a maximum of 37%) than a state of the art min-cut partition based topology generator for a variety of benchmarks. In addition, the average hop count is reduced by 0.75 hops, which would signiﬁcantly reduce the network latency. 1. INTRODUCTION In the last decade, the available number of transistors per chip has increased by several orders of magnitude. To maximize the use of these transistors, meet crucial time to market deadlines, simplify veriﬁcation, overcome clock skew problems and reduce reliance upon extremely large design teams, there has been an increased need for microprocessors in the design of emerging embedded systems. Such systems often contain many microprocessors, with a few designs even containing up to 300 microprocessors. This trend of having multiple processors is only expected to increase. Multi-processing in an SoC promises increased performance with reduced power consumption. Reduced power consumption is enabled in Multi-Processor SoC (MPSoC), by reducing the context switching, having task speciﬁc processors (thus small) and switching off idling processors. In embedded systems, which typically execute a single application or a class of applications, it is possible to optimize the system to reduce power. Reduced power consumption beneﬁts the system by performing longer on limited battery supplies, reducing weight by eliminating the need for chip cooling technologies, and increasing reliability by dissipating less energy and running cooler. Limitations of traditional bus-based architectures start to become apparent when numerous processing elements compete for communication resources. A typical bus-based architecture will require the processing elements to wait, while other data transfers are being carried out. Waiting, in turn, increases energy consumption due to leakage currents and decreases performance. Thus, a new communication paradigm is a prerequisite in MPSoCs to enable processing systems to reduce both latency and energy of data transfers between processors. Networks-on-chip (NoCs) has been proposed as a future replacement of on-chip buses in SoC applications where large bandwidth requirements and plentiful wiring resources are available. However, these networks add considerable logic to the design and require optimization to make them energy efﬁcient. Motivation for this work Customizing NoC topologies leads to faster, smaller and more energy efﬁcient networks. They can be made faster, by adding links between congested routers and direct routes between frequently communicating processing cores; smaller, by eliminating links, ports and routers which are under-utilized; and more energy efﬁcient, simply by the virtue of a faster, smaller network. In this paper, we present an iterative reﬁnement strategy to generate an optimized NoC topology that supports both packet-switched networks and point to point connections. We use cost functions to rank various local improvements to guide the optimization process. The ﬁrst phase creates a good initial starting point while the second phase optimizes the topology. For each topology change, a systemlevel ﬂoorplanning tool estimates the wire lengths. We use characterized energy macro-models to account for routers with differing number of ports. Experiments are performed to illustrate the potential for the algorithm to generate good custom NoC topologies for speciﬁc applications for both asymmetric communication patterns as well as symmetric. The remainder of the paper is organized as follows: Section 2 surveys the related work and states our contribution. Section 3 overviews the framework and describes the topology generation problem and application and architecture model. Section 4 describes the topology generation algorithm and its implementation. Section 5 presents the experimental results. Section 6 concludes the paper. 2. RELATED WORK Various NoC router circuits supporting services such as fault tolerance and quality of service have been previously proposed using several different regular topologies. Regular networks beneﬁt by allowing for a simpliﬁed placement and early exploration of topologies. Several researchers have developed algorithms to map processing cores onto tiles of regular topologies such as the mesh and torus [8, 12]. In [13], Ogras et al. enhanced the mesh topology by adding customized longrange links to minimize hop distance. Other works have addressed the selection of the best topology from a large library of alternatives [9]. Several methods have been proposed for synthesizing application speciﬁc topologies for point-to-point networks and ring architectures [2, 15]. Krishnan et al. [18] presented several mixed integer linear programming (MILP) formulations that minimize total network bandwidth. However, these works lack ﬂoorplanning information. More recently, Krishnan et al. [17] presented a MILP formulation that addresses both wire and router energy by splitting the topology generation problem into two distinct sub-problems: system-level ﬂoorplanning and topology and route generation. In [14], Pinto et al. considered the synthesis of topologies including both networks and pointto-point buses. They mapped the topology generation into two subproblems: k-median and multi-commodity min-cost ﬂow and solved these problems using approximation algorithms. The main drawback of these two approaches is that it assumes that the routing resources have negligible impact on the ﬂoorplan. In [11], Murali et al. proposed a two step topology generation procedure using a general purpose min-cut partitioner to cluster highly communicating cores on the same router and a path allocation algorithm to connect the clusters together. Our work is similar to that presented in [11], in that we produce an energy optimized topology from an application graph using calibrated energy models with the aid of a system-level ﬂoorplanner. How978-1-4244-1922-7/08/$25.00 ©2008 IEEE 265 3B-2 7HFKQRORJ\/LEUDULHV &RUH6L]HV 3DUDOOHO$SSOLFDWLRQ7UDFH (QHUJ\(VWLPDWLRQ 7RSRORJ\*HQHUDWLRQ (QHUJ\0DFURPRGHO ([WUDFWLRQ 5RXWHU3RZHU0RGHO 5RXWHU$UHD 0RGHO /$07UDFH7*))(6 $SSOLFDWLRQ $QDO\VLV &RPP9ROXPH&RUH6L]HV 1R&(QHUJ\ (VWLPDWRU )ORRUSODQQHU &DSR 1R&2877RSRORJ\ 2SWLPL]DWLRQ 1R&*HQHUDWLRQ 5RXWHUOLEUDU\ 7RSRORJ\*UDSK)ORRUSODQ 1R&*HQHUDWRU 12&6<17+(6,6)/2: 9+'/ Figure 1: Complete NoC synthesis ﬂow The novel contributions of our work are: ever, our approach differs from [11] in the following ways: we support both point-to-point connections and packet-switched networks allowing further energy reductions in certain applications. Instead of exhaustively exploring all balanced cuts of the application graph, we start from a point in the design space and iteratively evolve the solution through a series of guided local moves. The beneﬁt of this approach is that it enables us to control the parts of the design space to be explored, thus allowing additional architecture constraints to be added. • an iterative optimization technique and selection heuristic for guided design space exploration of NoC topologies using ﬂoorplan feedback • a design ﬂow that allows architectural constraints to be added without major algorithmic changes • a methodology that supports hybrid networks with both packet• an augmented system-level energy model to explore active, idle switched routers and point-to-point connections and leakage energy in custom NoC topology generation 3. SYNTHESIS OF A CUSTOM TOPOLOGY Figure 1 presents the proposed NoC synthesis ﬂow. There are three major parts to this framework: (i) energy estimation, (ii) NoC optimization and (iii) NoC generation. The focus of this paper is the NoC topology optimization component. The other parts of the ﬂow allow energy characterization and generation of a realizable NoC. We refer interested readers to [4] and [5]. We have implemented all three parts as an extensible web-application. This NoC generation framework allows the creation of new router models through the deﬁnition of router templates, automated characterization of their energy characteristics and the creation of an optimized NoC using the algorithms described in this paper. The input to the NoC optimization step is the application model, NoC router libraries for implementation and an estimate of the silicon area of the processing cores. The NoC optimization step produces an annotated topology graph that describes the interconnection network between the processing cores as well as their placements. Our HDL generator takes this topology graph and produces a synthesizable hardware description of the routers and their interconnections. A ﬂoorplanner is used to estimate wire lengths and estimate the energy contribution of the topology. The NoC energy is estimated using characterized macro-models. 3.1 Application Model In this work, an application volume graph is used to capture the trafﬁc ﬂow characteristics to enable energy estimation. The application graph is a directed graph A(V , E ), where each vi represents a core and the directed edge (vi , vj ) represents communication between the cores. The communication volume between cores volij is speciﬁed for each edge. An application run-time t is speciﬁed and it is used to calculate the total leakage energy and idle energy dissipated. An example application volume graph for a H.263 decoder combined with an MP3 decoder (263decmp3dec) is shown in Figure 2 (a) from [19]. We use this example throughout the paper to illustrate the operation of our algorithms. 266                                             (a) Application graph (b) Topology Graph Figure 2: Input graphs In addition to volume, each core in the application graph is annotated with a ﬁxed core size with width W (vi ) and height H (vi ). An application volume capacity constraint volM AX is speciﬁed for each edge to ensure that the generated topology can support the bandwidth requirements. The communication volume volij must be less than volM AX for each edge. 3.2 Architecture Model A topology graph is used to describe the interconnections between the routers and cores. The topology graph is a directed graph T (N , L), where each vertex ni ∈ N represents a node in the topology. Each node can be either a core pei or router ri . Each core in the application graph vi must be mapped to one node pei in the topology graph. Each directed edge in the graph (ni , nj ) represents a physical bus connecting nodes ni and nj with trafﬁc ﬂowing in a single direction. For every edge (vi , vj ) ∈ A, there must exist a path (ni , ri ), (ri , rk ), ...(rk , nj ) in the topology graph T that connects communicating cores ni and nj . Figure 2(b) shows the topology output for the 263decmp3dec benchmark. We use the custom packet-switched wormhole routers from the NoCGEN framework [4]. We assume a ﬁxed buffering amount on each router and support a maximum of sixteen ports. We limit the number of ports because we lack calibration data for a greater number of ports. 3.3 Problem Description The topology synthesis problem can be deﬁned as follows: Given an application abstraction, a NoC router area model and a characterized router power/energy model, ﬁnd an NoC topology T that minimizes the communication energy Enoc . The NoC energy can be modeled by the combination of the router logic E (r), network interfaces E (n) and interconnect energy E (e). The NoC energy Enoc can be modeled as follows: Enoc = (cid:2) r∈R E (r) + (cid:2) e∈L E (e) + (cid:2) n∈N E (n) where R is the set of routers and L is the set of links in the interconnection network and N is the set of nodes. The energy dissipation of each NoC router can be separated into four main categories: active energy Edyn ; idle Estatic ; static leakage Eleak and wire energy Ewire . Active energy is consumed by packet related activities and increases linearly with increasing volume. The router energy of a single router can be deﬁned as: Erouter (ri ) = Edyn (ri ) + Ewire (ri ) + Estatic (ri ) + Eleak (ri ) To enable efﬁcient computation of the total NoC energy, we separate the router’s active energy into input and output components. The active energy component of the edge weights are computed as the output energy of the source node Edyn s summed with the input energy of the target node Edyn t . We assume the wire capacitance scales linearly per unit length Cw for a particular technology library. The wire energy also becomes part of the edge weight to the topology graph. The idle energy and leakage is modeled as a ﬁxed cycle energy that is consumed at a given frequency f . Combining these components together,   $FWLYLW\:HLJKWHG )ORRUSODQ &OXVWHUDQG)RUP,QLWLDO 3DUWLWLRQV 0HUJH6SOLW5RXWHUV DQG*HQHUDWH373 $GGDQG5HPRYH /LQNV 5RXWHDVVLJQPHQW , Q L W L D  O \ J R R S R 7 O 2SWLPL]HG7RSRORJ\*UDSK )ORRUSODQ 0RYH3URFHVVLQJ (OHPHQWV &RUHVL]HV $SSOLFDWLRQ       ,7(5$7,9(,03529(0(17)/2: , Q L W L D  O \ J R R S R 7 O  * H Q H U D L W Q R \ J R R S R 7 O  5 H L I H Q P Q H W Figure 3: Iterative Improvement Flows the total NoC energy can be expressed as: Enoc = (cid:2) (i,j )∈L (Edyn s (ni ) + Edyn t (nj ) + Cw dij ) · volij + (cid:2) n∈N (Estatic (n) + Eleak (n, f )) · t The linear regression based technique from [5] is used to develop analytical energy models for a library of routers [4] with varying input and output ports. We extracted energy models for a large number of routers, validated them on numerous traces with varying switching activity and timing characteristics with low error rates (< 5%). The energy contributions of the abstract NoC events for each component were collated into lookup tables to allow quick evaluation of energy. Non-characterized points in the energy model are estimated using cubic interpolation. Analytical area models are developed from logic synthesis estimates with input ports, output ports and buffering depth as model parameters. These analytical expressions were found to be very accurate at predicting the gate count (< 2%) as the router is highly modular. 3.4 Floorplanning A ﬂoorplanner is used to evaluate the wire lengths between each processing element and router. Each topology graph is ﬂoorplanned and mapped to a two-dimensional layout. Each node in the topology graph ni is represented by a rectangular region with width, W (ni ) and height, H (ni ) and network interfaces at coordinates X (ni ) and Y (ni ). The distance dij between two nodes is measured by the manhattan distance between their network interfaces |X (ni ) − X (nj )| + |Y (ni ) − Y (nj )|. For point-to-point links, the network interfaces are located on the perimeter of cores. As there may be multiple network interfaces on each core, the network interface is assumed to be connected from the center of the cores. The network interfaces for the processing cores are assumed to be located on the corner while routers nodes are in the center. The edges in the ﬂoorplanning problem are weighted proportional to the activity (the relative number of packets) that traverses across the links between the nodes. We create dummy edges based on the application graph to allow high communicating pairs of nodes to organize themselves close together to reduce the wire length for the point-to-point networks. We evaluated three academic standard-cell ﬂoorplanners (Parquet [1], Capo 10.2 [16] and mPL6 [6]) for our ﬂoorplanning input. Both Parquet and Capo 10.2 produced adequate ﬂoorplans but exhibited large run-to-run wire length variability (about 20% in the several tested cases) due to the randomized algorithms used. The analytical placer mPL6 produced stable results but its ﬂoorplans had signiﬁcantly longer wire lengths. The high variability in wire lengths makes the comparison of two topologies difﬁcult as one may have lower energy because of a better ﬂoorplan run. We overcame the inherent instability of the ﬂoorplanning algorithm, by picking the best ﬂoorplan out of multiple runs (about 100). To reduce the runtime, we parallelize the multiple runs over multiple processors with a near linear speed up in processing time. 4. TOPOLOGY GENERATION ALGORITHM Our topology generation algorithm consists of two distinct phases: an initial topology creation phase and a reﬁnement phase shown in Figure 3. In the ﬁrst phase, we create a ﬂoorplan based on the application volume graph (Step 1). The pin locations from the ﬂoorplan are used to group together frequently communicating cores into partitions (Step 2). We use the term partition or cluster interchangeably to describe processing elements connected to the same router. To connect the routers in the initial partition, we use a modiﬁed route shortest path algorithm similar to that presented in [11] (This will be explained in Section 4.2). The ﬁrst three steps form an initial topology that can be reﬁned quickly towards the optimized solution. It is possible to substitute this stage with a known initial topology such as a mesh or other known custom topology. In the second phase, the topology is reﬁned through three steps: (i) coarse partition reﬁnement, (ii) ﬁne partition reﬁnement and (iii) route reﬁnement. The coarse partition reﬁnement step evolves the topology by changing the number of routers or aggressively swapping groups of processing elements across routers or adding point-to-point links (Step 4). It transforms the partitions by increasing/decreasing the number of routers by merging smaller partitions, splitting large partitions and moving groups of processing elements between partitions. Figure 4 shows the scaling trends of the input and output ports for dynamic and static energy. Static energy increases linearly with input ports at about 0.1 mW per router port for a 4-ﬂit buffer due to an increase in buffering. The merge operation is typically used to combine two routers to reduce their buffer resources. A split operation breaks large partitions into two smaller independent pieces which results in lower dynamic energy costs as there are fewer router ports. In Step 5, the ﬁne partition reﬁnement step moves single routers to improve the partitioning. The route reﬁnement procedure (Step 6) modiﬁes the links between the routers to either decrease dynamic energy by reducing the number of links or reduces hop energy by adding proﬁtable links. 2 4 6 8 10 12 14 16 3.5 4 4.5 5 x 10 −12 Dynamic Energy Input ports F t i l e n e r y g ) J ( 2 4 8 12 16 (a) 2 4 6 8 10 12 14 16 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 x 10 −3 Static Energy o P w e r ( W ) Input Ports (b) Figure 4: Port scaling of dynamic energy Each design is evaluated using the cost function in Equation 1. The topology is ﬂoorplanned and trafﬁc ﬂows are mapped to the topology. The paths taken by each trafﬁc stream can be determined using a modiﬁed shortest path algorithm on every processing element in the topology graph with edges weighted based on their dynamic energy and wire energy. Once shortest paths are determined, the total NoC energy is computed from the edge volumes. To support the volume constraint, each trafﬁc ﬂow is mapped to a commodity in the multi-commodity min-cost ﬂow (MCMCF) problem with capacity constraints. We have used the linear programming based MCMCF solver PPRN [3] to determine the shortest paths. If the volume capacity constraint on an edge is not met, the candidate topology is rejected. Deadlock prevention is implementable using turn prohibition methods[20]. The prohibited turns can be implement using the MCMCF solver. Due to space constraints, we are unable to discuss this in detail. We use tabu lists to prohibit the same sequence of moves from repeating. Separate tabu lists are maintained for each of the possible moves (split, merge, split-move, move, add and remove). Some tabu lists are cleared when a split and merge occurs as the nodes no longer correspond to the nodes in the tabu list. In the route reﬁnement and ﬁne partition reﬁnement steps, we maintain an unchanged threshold count Uthreshold to decrease the algorithm run-time when the current set of moves is making no improvement to the topology. 4.1 Forming the initial topology The initial topology is formed by performing an activity weighted ﬂoorplan based on the application graph [7]. The network interfaces of frequently communicating cores will be clustered together by the ﬂoorplanner as it attempts to minimize wire length. The pin locations 3B-2 267           3B-2                        Figure 5: Activity weighted ﬂoorplan without routers are assigned to be the corner of the rectangular cores as shown in Figure 5. For each unique set of pin locations, we group together pins reachable within the distant constraint D . The diamond in the ﬁgure shows the pins reachable within 0.5mm (manhattan distance) of the clustered network interfaces for cores 0, 6 and 8. We form partitions greedily by grouping the pins with the most intra-cluster volume. In the diagram, the cluster with the highest intra-cluster trafﬁc was centered around cores 0, 6 and 8 which includes cores 0, 1, 2, 3, 4, 5, 6, 8, 7 and 10. All these cores are initially connected on a single router. The next iteration groups 11, 12, 13 and ﬁnal iteration groups core 9 onto a separate router. From experiments, we found that setting the distance constraint to the height of the largest core produces a good initial partitioning for most of the benchmarks tested. A distance constraint that is set too high under-partitions the topology, resulting in large routers. On the other hand, a small distance constraint will create small partitions and many routers. Neither pose a problem because the reﬁnement procedure quickly merges the small partitions or splits the large partitions. 4.2 Route assignment The purpose of the route assignment procedure is to add links to the topology graph to connect routers and provide more direct paths, thus reducing hop count. A weighted fully connected graph with |R| nodes is used to determine the shortest paths for each ﬂow, where |R| is the number of routers in the topology. Each edge that already has a direct link between two routers is weighted by the cost of transferring one packet of data (dynamic energy cost). Edges that do not currently exist in the topology graph include an additional installation cost, i.e., the sum of the dynamic energy cost of transferring the packet on the larger router; the increase in cost for other trafﬁc on both source and destination router; and the static energy for lifetime of the application. Each communication stream is mapped to communication paths using a shortest path algorithm to determine whether it is better to use existing routes or add additional routes to connect that trafﬁc ﬂow. installation costs are minimized by assigning highest communication trafﬁc streams ﬁrst. Algorithm 1 Coarse Partition Reﬁnement 1: while merge, split and split-move candidates exist do Attempt best merge, split, split-move or multi-merge Perform ﬁne partition reﬁnement (return reﬁned T ) if last merge/split or split-move improves on topology then Accept reﬁned topology T , adjust tabu list 2: 3: 4: 5: 6: 7: 8: return topology else Add that merge/split/split-move to the tabu list 4.3 Coarse partition reﬁnement The pseudo code for the partition reﬁnement step is shown in Algorithm 1. We use suitability functions to evaluate the beneﬁt of the each potential merge, split and split-move operation. The method used to evaluate merge, split and split-move will be described in the next three subsections. After each potential move is evaluated, it is ranked and the best move is chosen. 4.3.1 Selecting a merge candidate The merge gain is evaluated by estimating the change in router dynamic, static and leakage energies for each pair of merge candidates. There are three contributing factors that affect the suitability of a merge: (i) static energy reduction due to the removal of common ports (ii) decreased volume due to reduction in hop distance and (iii) increase in 268                                     (a) Application subgraph (b) 1st split partition Figure 6: Split partition example packet energy due to more router ports. The decrease in volume can be estimated by summing the trafﬁc between the two partitions. The increase in dynamic energy is computed by estimating the cost before and after the merge. Any pair of router nodes that has a positive merge gain is added to a list to be considered for merging by the coarse reﬁnement algorithm. The coarse partition reﬁnement algorithm will attempt to merge multiple routers at once, if they are beneﬁcial. New links may need to be added to ensure the connection of all streams. In these cases, the route assignment procedure is used to route the unconnected trafﬁc streams. It is also possible to perform two merges simultaneously where they are likely to be beneﬁcial. Algorithm 2 Get Split Candidates 1: for each router partition p ∈ P do Construct application subgraph A(cid:3) from p Add edges in subgraph A(cid:3) to edge list EL Sort edge list EL by communication volume (ascending order) while partition p can be split further do Split N Partition(A(cid:3) , EL) if Gain(p) > split threshold, add to split candidates 8: return split candidates 2: 3: 4: 5: 6: 7: 4.3.2 Selecting a split candidate The split is the opposite operation to the merge operation. Similar to the merge case, the split gain is evaluated by estimating the change in router dynamic, static and leakage energies. However, for each partition there may be multiple ways to split the nodes. For example breaking a large router such as a sixteen port router into two eight port routers halves the per packet dynamic energy (See Figure 4(a)). Conversely, splits should be avoided in cases that would result in a large amount of trafﬁc between the newly formed partitions. Given a router and an assignment of nodes into two partitions, the potential gain can be computed in a similar fashion as the merge candidates. The potential gain/loss due to a split operation can be analytically calculated by examining: (i) the increase in total network volume between the newly formed routers; (ii) the decrease in dynamic energy due to smaller routers and (iii) the change in static energy. A split is added to the list of candidates when the potential gain is above the split threshold. The split threshold is set to a slightly negative value so as to include split candidates that increase router energy but decreases wire energy through greater placement freedom. The router can be split in two partitions in multiple ways. We use a greedy edge removal strategy, which is presented in lines (2 - 8) of Algorithm 2 to generate several splits of the router partition. For each partition p, construct a subgraph A(cid:3) of the original application volume graph containing only the nodes and edges in the partition. Next, create edge list EL containing all the trafﬁc ﬂows/edges of subgraph A(cid:3) , sorted in ascending order by volume (Algorithm 2 Line 4) and iteratively remove the minimum volume edge from the application subgraph until two unconnected partitions are formed. A depth ﬁrst search is used to determine whether two partitions are formed. If two or more partitions are formed, we evaluate the split gain as described above. To ﬁnd the next split, the last removed edge is added back to the application volume graph and edges are removed until another split partition is found. Each additional partition found by splitting will ﬁnd partitions with increasing inter-partition communication costs. An example application subgraph from the 263decmp3dec application in shown in Figure 6(a). In this example, the processing elements    {0, 1, 2, 4, 5, 6, 7, 9 and 10} are connected to the same router. If we remove the smallest edges {20, 25} as shown in Figure 6(b), two partitions {9, 10} and {0, 1, 2, 4, 5, 6, 7} are formed. The next partition is formed by adding back the 0 to 9 edge and removing the next two is {0, 9, 10} and {1, 2, 4, 5, 6, 7}. This procedure continues until no smallest edges with volumes 25 and 250. The next partition formed more partitions can be formed. The split-move is a variant of the split with the newly created split router merged with another partition. The procedure for a split-move is similar to both the split and merge methods, except the cost function considers both split and merge simultaneously. Due to space constraints, we do not describe this suitability function. 4.3.3 Point-to-point networks When there are no more split and merge candidates left, each edge in the application graph is analyzed to determine which trafﬁc streams would beneﬁt from point-to-point links. Point-to-point links are added late in the design space exploration phase, to avoid prematurely adding direct links which would artiﬁcially improve a poorly designed initial topology. Two optimization passes are performed to discover potential point-to-point links. The ﬁrst pass evaluates the effects of adding point-to-point links to replace streams in the original application graph. The second pass evaluates the gain or losses from disconnecting an entire node from the network by converting all of its communication into point-to-point links. Evaluation is performed by comparing the energy cost of routing the stream through the network and by a point-to-point link. Adding a point-to-point link, will add a static installation cost which includes the static and leakage energy of network interfaces. By adding one or more point-to-point links, the network interfaces or router links may become redundant and hence can be removed. These are evaluated as part of the cost function. The wire length in point-to-point networks is evaluated by considering the distance from the two closest sides of communicating cores. Algorithm 3 Fine Partition Reﬁnement(T ) 1: while move candidates != ∅ and u < Uthreshold do Attempt best move candidate Perform route reﬁnement (return reﬁned T ) if Last move improves topology then Accept reﬁned topology T Add Last move to the tabu list, increment u 2: 3: 4: 5: 6: 7: 8: return topology else 4.4 Fine partition reﬁnement The ﬁne partition reﬁnement step (Algorithm 3) moves processing elements between partitions to reduce the size of large routers and/or reduces the hop count by migrating processing elements nearer to their communicating partners. Similar to the coarse partitioning reﬁnement step, processing element - router pairs are evaluated based on a suitability function. The suitability function for moving a processing element determines the gain/loss due to router sizing and hop gain. A function is used to determine the best existing router to move a processing element to. The move is attempted and the routes reﬁnement step is executed. The route-reﬁned topology is evaluated for the energy cost. If the move improves on the best topology so far, it is accepted and new move processing element candidates are computed. If the move does not improve the design, it is rejected and removed from the candidate list and prohibited from future move candidate lists. When there are no more move processing element candidates, the best topology found will be returned. 4.5 Reﬁning routes Routes are improved by adding new links to provide more direct routes or removing existing links that are infrequently used. Addition of new links is not always proﬁtable as there is a signiﬁcant cost for opening up a channel between two routers. Additional router-to-router links are evaluated to determine an estimate of the energy effects. For a link addition to be beneﬁcial, the network volume reduction must be great enough to justify the addition of ports between the two routers. A volume threshold criteria is used 269 3B-2 Component NI src dynamic NI target dynamic NI src static NI target static Energy/Power 1.6e-12 J 1.2e-12 J 0.16 mW 0.16 mW Table 1: Network interface power to ﬁlter out candidates whose network volume reduction is below the installation cost. The addition of some links may cause others to become redundant. Where a redundant link is removed, the installation cost of the new link is not considered. The candidate edges for addition are ranked according to their potential beneﬁt to the topology, i.e., how much bandwidth is reduced. We use a lottery system to decide which of the candidate edges will be added. The number of tickets allotted to an edge is equal to network volume that is decreased. Edges that reduce greater bandwidth are given more, while the worst edge receives no tickets (edges that do not reduce total volume in the network). A random ticket is selected and that edge addition is evaluated for energy improvements. Removal of edges is done in a similar fashion to addition. We prohibit the removal of routes that would cause the trafﬁc requirements of the application to be violated. 4.6 Supporting additional constraints Additional architectural constraints such as a maximum router port constraints can be added to ensure that the topology is implementable. A maximum router port constraint restricts the number of ports in any router in the topology. This requires the selection criteria for merge, move processing element, and add to be modiﬁed to ﬁlter out candidates that would violate this constraint. A swap processing element operation is also added to allow local partition reﬁnement when router partitions approach the maximum port constraint. In a swap, pairs of moves are considered such that they do not increase the number of router ports past the port constraint. 5. EXPERIMENTAL RESULTS We conducted experiments to evaluate our topology generation algorithm against the min-cut based algorithm presented by Murali et al. [11]. To allow fair comparison, we re-implement their algorithm using our energy models and the hMetis partitioner [10]. M processor. The ﬂoorplans were distributed on a cluster of 3 × fourThe topology generation algorithm was run on a 2.0 GHz Pentiumcore Opteron 2.0 GHz servers running the Capo 10.2 placer with the block ﬂipping and rotation option enabled. The chip dimensions were selected for each benchmark relative to the total sum of the core sizes. A ﬁxed capacitance of 500 fF per mm for each bus line is set. Point-topoint network interfaces are located on the perimeter of the core and have a minimum net length of 0.1mm. We set the optimizer to use energy models for 250 MHz operation. In most benchmarks, the core sizes varied between 62500μm2 to 4mm2 . We select the best ﬂoorplan of one hundred runs of the ﬁnal topology comparison purposes. The network interface power consumption is shown in Table 1. 5.1 Experiments Conducted Graph 263decmp3dec 263encmp3dec mp3encmp3dec vopd imp large long broadcast mesh4x4 random-a G1 G2 G3 G4 G5 G6 G7 G8 G9 G10 PE 14 12 13 13 27 14 14 16 16 100 Flows 16 12 12 12 96 16 16 120 40 200 k ﬂits 23.56 230.2 16.52 3.116 11040 23.56 23.56 120.0 48.00 283.0 t (cycles) 20000 200000 20000 3000 1600000 20000 100000 50000 30000 50000 Table 2: Benchmark Description Table 2 summarizes the characteristics of the ten benchmarks graphs. The ﬁrst seven benchmarks (G1-G7) are application volume graphs obtained from [8, 19, 11] which contain a mixture of MP3, H263 decoders and encoders, video object plane decoders and image processors with varying cores sizes. The image processing benchmark imp, features twelve processors with private memories and three shared resources with are used equally by each processor. The large and long 3B-2 Graph G1 G2 G3 G4 G5 G6 G7 G8 G9 G10 |R| 2 2 2 2 8 3 1 1 2 20 |L| 23 18 21 23 68 24 21 30 34 215 Mincut Hops 1.003 1.000 1.010 1.008 1.236 1.031 1.000 1.000 1.356 1.389 P(mW) 7.45 6.50 6.43 7.46 42.5 7.79 5.05 24.0 15.5 75.0 CPU |R| 50 4 40 2 37 2 37 3 200 13 51 3 50 1 50 1 50 2 250 14 |L| 24 18 22 24 78 23 21 30 34 218 NoCOUT w/o ptp Hops P(mW) 1.068 7.38 1.001 6.50 1.001 6.45 1.110 7.39 1.290 39.5 1.026 7.69 1.000 5.05 1.000 24.0 1.356 15.5 1.222 73.5 CPU |R| 130 1 16 0 160 1 200 0 1050 1 165 1 40 2 350 1 200 2 1500 15 |L| 17 12 14 12 58 17 19 30 38 224 NoCOUT Hops P(mW) 0.012 6.01 0.000 4.93 0.020 4.90 0.000 5.14 0.121 26.6 0.012 6.24 0.786 4.70 1.000 24.0 1.002 15.5 0.627 66.5 CPU 150 20 310 280 1200 375 55 375 220 1800 Impr % 19 24 24 31 37 20 7 0 3 11 Table 3: Energy consumption of Min-cut and NoCOUT                              (a) Mincut (b) NoCOUT with point-to-point Figure 7: 263decmp3dec ﬂoorplans benchmarks are identical to 263decmp3dec except that the core area is increased by a factor of nine in G6 and the total application execution time is increased in the long benchmark. These two benchmarks illustrate the effect of increasing the relative wire energy, and operating at lower activity rates. Graphs G8 and G9 show mesh and broadcast type communication patterns used in scientiﬁc applications. In the broadcast benchmark, all cores communicate with each other. We generated a synthetic asymmetric volume graph (Graphs G10) with one hundred cores to test the scalability of the algorithms. We control the number of in-edges, outedges and the distribution of volumes of edges and core sizes. 5.2 Results Table 3 reports the number of routers |R|, links |L|, average number of hops, power consumption and algorithm runtime (in seconds) of both the min-cut and NoCOUT algorithms. For the SoC benchmarks, our algorithm generated designs with a mixture of routers and pointto-point networks. Figure 7 shows two ﬂoorplan generated by the Mincut algorithm and NoCOUT for the 263decmp3enc benchmark. Most of the trafﬁc streams use point-to-point links (shown with the dashed lines connecting the centers of cores) to communicate with their neighboring cores resulting in shorter wire lengths. Point-to-point networks were beneﬁcial in these benchmarks (G1-G7) because the cores communicated in a mostly pipelined fashion with an in-degree and outdegree close to one. The 263encmp3dec and vopd applications created topologies containing only point-to-point connections. If we disable the point-to-point generation in our algorithm, it produces designs which are similar to the min-cut solution with small improvements in energy up to 6%. In some cases, we are able to produce a better partition of the router nodes that takes into account energy effects of power/energy model. For the broadcast benchmark, a single 16-port router was created by both algorithms. As there were many communication partners, no individual stream beneﬁted greatly from point-to-point connections, hence the algorithm correctly chose not to add any. For the mesh4x4 benchmark, both algorithms created a two router network with half of the mesh nodes connected to each router. Point-to-point links were added between nodes 1 and 2, and 14 and 15, as these reduced the wire energy signiﬁcantly to justify the extra network interface. 5.2.1 Runtime and Scalability of algorithms In the benchmarks presented above, the number of generations needed to produce the ﬁnal solution is typically limited to about 100. Each generation can take about ﬁve to ten seconds to evaluate due to the need to ﬂoorplan every design many times. The complete NoC topology solution is generated in minutes. This runtime can be further reduced with the use of a larger cluster running ﬂoorplanning problems. In future, we plan to investigate analytical ﬂoorplanning techniques that will remove the need to run the ﬂoorplanner multiple times. Although the min-cut partitioner is more efﬁcient at producing solutions, as more parameters such as link sizes are added in, it requires the mincut procedure to be run in the inner loop of any optimization algorithm. We believe our method of exploring the design space iteratively will be competitive in runtime when other parameters are added and constrained further. In very large cases, for example greater than 1000 cores, multi-level partitioning approaches similar to that used in ﬂoorplanning should be considered. We found that in the ﬁne and route reﬁnement stages, the average processor-to-router and router-to-router wire lengths remain relatively consistent. It may be possible to estimate the potential gain in these stages without running the ﬂoorplanning for every design iteration, reducing the number of runs. 6. CONCLUSIONS The topology generation problem contains multiple NP-hard problems. These problems are often treated separately and solved using approximation algorithms. In this paper, we presented a technique for the generation of an energy efﬁcient application speciﬁc NoC topology with awareness of ﬂoorplan and characterized energy and area models. We show that using a guided local search, it is possible to ﬁnd solutions of similar quality to a min-cut partitioner on various benchmarks. Our algorithm allows a mixture of both point-to-point networks and packetswitched networks to allow further reduction in energy consumption through shorter point-to-point wires and fewer routers. "
2008,A Multi-Processor NoC platform applied on the 802.11i TKIP cryptosystem.,"Since 2001, there have been a myriad of papers on systematic analysis of multi-processor system on chip (MPSoC) and network on chip (NoC). Nevertheless, we only have a few of their practical application. Till now, main interest of researchers has been to adapt NoC to the communication intensive multimedia system like H.263. However, this paper attempts to expand the domain of NoC platform to one of the wireless security algorithms (TKIP), because its inter-component transaction pattern shows considerable characteristic for NoC. This paper consists of the explanation on operational sequence of the algorithm in chosen architecture and the brief illustration of important composing NoC blocks (Network Interface, Router).","7B-5 A Multi-Processor NoC Platform Applied on the 802.11i TKIP Cryptosystem Jung-Ho Lee, Sung-Rok Yoon, Kwang-Eui Pyun, and Sin-Chong Park Information and Communications University 119, Munjiro, Yuseong-gu, Daejeon, 305-732, Korea Abstract— Since 2001, there have been a myriad of papers on systematic analysis of Multi-Processor System on Chip (MPSoC) and Network on Chip (NoC). Nevertheless, we only have a few of their practical application. Till now, main interest of researchers has been to adapt NoC to the communication intensive multimedia system like H.263. However, this paper attempts to expand the domain of NoC platform to one of the wireless security algorithms (TKIP), because its inter-component transaction pattern shows considerable characteristic for NoC. This paper consists of the explanation on operational sequence of the algorithm in chosen architecture and the brief illustration of important composing NoC blocks (Network Interface, Router). I . INT RODUCT ION TKIP is the one of three algorithms, deﬁned in 802.11i enhanced security standard [1]. We have chosen TKIP cryptosystem, because it is the most viable solution for MPSoC system, in that it can satisfy both 802.11n minimum throughput requirement (76 Mbps) and reasonable robustness against known attacks. WEP is the one we should choose, if we consider performance only. However, WEP has already exposed too wide crevice against malicious intrusion, such as fragmentation attack and weak IV (Initialization Vector) attack [2]. In 0.13μ CMOS process, typical ARM7 core occupies 0.24mm2 area [3]. Considering this small area, integrating 10 ARM cores on a chip seems to be reasonable. In the future at hand, that sort of on-chip core grid processor will come into the market as a facility for general applications. Motivated by this expectation, in this paper, we analyze TKIP security algorithm on a general purpose MPSoC platform. The paper is organized as follows: Section II discusses about the way how to divide a workload to many processors to meet the throughput requirement. Section III explains the applied NoC architecture. Experimental results are shown in Section IV. Finally, Section V concludes this paper. I I . CONCURRENT O PE RAT ION O F THE TK IP COM PONENT S As shown in Fig. 1, TKIP cryptosystem is composed of three major components: RC4, Michael, and CRC-32. RC4 is the crux of the cryptographic algorithm, which grants conﬁdentiality to the payload. RC4 accompanies a key mixing algorithm that protects the system from replay attack. And the second one - Michael - is to prevent the system from forgery attack by hijacker. CRC-32 is for generating ICV (Integrity Check Value), which checks sanity of the transmitted payload. TABLE I E X E CU T I ON CY C L E S F O R P RO C E S S I NG 2 3 0 4 BY T E S O F M SDU ON A S I NG L E 1 0 0M H Z ARM P RO C E S S O R RC4 Michael CRC-32 # of Execution Cycles 75963(RC4)+5203(KeyGen) 63040 32969 Fig. 1. TKIP cryptosystem overview To determine the degree of concurrency, we ﬁrst calculate maximum allowable number of cycles for processing 2304 Bytes of MSDU (MAC Service Data Unit). M ax. cy . = 2304By tes × 8bits/B y te 10 × 10−9sec × 76 × 106bps = 24253 cy . (1) Table I is showing the single CPU execution result of each component. Based on the observation, the equation (1) tells that the system ought to complete all operations on a MSDU packet (2304 Byte) every 24253 cycles (243μs in 100Mhz CPU). RC4, Michael, and CRC-32 should be divided into 4, 3, and 2, respectively, to meet the throughput goal. A. Chronological Relationship Among The Components We can expect strict parallelism if and only if a problem is divided into several pieces of independent part. Unfortunately, vast amount of cryptographic algorithm are fully sequential, and cannot be broken into independent parts. To make it worse, Michael and RC4 have a loop, in which result of previous loop is fed as an input to the consecutive loop, forming dependent chain. Therefore, we have no other choice than to juxtapose 978-1-4244-1922-7/08/$25.00 ©2008 IEEE 607 7B-5 RC4−1 RC4−2 Tcomp,RC4 Tcomm,RC4 TXOR,RC4 Tcomp,RC4 Tcomm,RC4 TXOR,RC4 Tcomp,RC4 Tcomm,RC4 TXOR,RC4 Tcomp,RC4 RC4−3 Load TK,TA,TSC Tcomp,RC4 Tcomm,RC4 TXOR,RC4 RC4−4 Tcomp,RC4 Tcomm,RC4 TXOR,RC4 MIC−1 Tcomm,MIC Tcomp,MIC Transfer 8Bytes MIC MIC−2 MIC−3 Tcomm,MIC Tcomp,MIC Tcomm,MIC Tcomp,MIC Tcomm,MIC Tcomp,MIC Tcomm,MIC Tcomp,MIC CRC−1 Tcomm,CRC Tcomp,CRC Transfer 4Bytes ICV Tcomm,CRC Tcomp,CRC Tcomm,CRC Tcomp,CRC CRC−2 Tcomm,CRC Tcomp,CRC Tcomm,CRC Tcomp,CRC Tcomm,CRC Tcomp,CRC 243us 486us 729us First Output Generated 972us 1215us Fig. 2. Synchronization timing of TKIP components each component in a pipelining style. Fig. 2 illustrates timing sequences how 9 CPUs are pipelined to achieve 76Mbps throughput. Fig. 2 shows that new MSDU is fed into an idle block every 243μs, which is the maximum allowable time to meet the throughput requirement (76Mbps). Here, number of CPUs, allocated for each operations (RC4, MIC, and CRC), is determined so that at least one resource of each component can be released every 243μs. Due to pipelining delay, we should wait until 972μs for this system to yield the ﬁrst output. Starting from 972μs, en/decapsulated ouput packet, including MIC and ICV, is generated every 243μs. In the pipelining, we can observe that the sequence of the set of cooperating components for processing a MSDU, are repeated every 12 intervals. For instance, RC4-1, MIC-1, and CRC-1 blocks work together to process the ﬁrst MSDU, which arrives at the ﬁrst interval (0 ∼ 243μs). Then, RC4-2, MIC-2, and CRC2 blocks collaborate to encapsulate the MSDU, arriving at the second time interval (244 ∼ 486μs). And after 12th time interval (2916 ∼ 3159μs), RC4-1, MIC-1, and CRC-1 blocks are selected for processing an incoming MSDU. This means that the number of the cooperating set is ﬁnite, and each block has predetermined working partner. B. Concurrent Calculation of RC4 PRNG We have 4 RC4 blocks. The role of RC4 block is to calculate RC4 PRNs, based on the key generated from TK (Temporal Key), TA (Transmitter Address), TSC (TKIP Sequence Counter). Note that the block separates two steps: generating PRNs and XORing with data (MSDU, MIC, and ICV). There are two reasons behind this scheme. Firstly, we can save CPU cycle by fully utilizing burst memory access. It is easy to think of reading 16 × 4By tes (bus bandwidth = 32bits = 4Bytes) at a time gives us beneﬁt, than reading1By te in every loop. Secondly, ﬁnal XOR operation can be performed in SIMD (Single Instruction Multiple Data) manner (1By te X OR × 4), because payload byte stream has no dependence, and the RPNs are readily computed. When we use 32bit XOR operation we can make the number of loops 1 4 of original one. Fig. 3. NI kernel block diagram C. Concurrent Calculation of Michael MIC As mentioned above, sequential partitioning of Michael is algorithmically infeasible. Therefore, we have to juxtaposes 3 MIC block units and feed them the workloads every 243μs. In realization, Michael block reads 16bits (2Bytes) MIC and 2304 Bytes MSDU key from shared memory, and produces write back into the ﬁnal result into the shared memory, so that RC4 can read them and use them. D. Concurrent Calculation of CRC-32 Block CRC (Cyclic Redundancy Code) itself is parallelizable, because it is based on division in GF (232 ). Nevertheless, we don’t need to take the beneﬁt of this art, because input packet stream of other cooperating components doesnŠt split up into several parts. For this reason, we just allocate two processor cores for independent CRC operation, and feed each of them with a MSDU payload. Fig. 4. Packetization delay of NI, when CPU clock is 100MHz and NoC clock is 75MHz. I I I . INT E RCONNE CT ION ARCH IT E CTURE O F THE PRO PO SED SY ST EM NoC is proposed as a design space solution for increasing cost of synchronization between cores, originated multi-GHz 608 core clock against worsening RC delay and capacitive coupling between wires [4]. Our system is interwoven with the NoC fabrics, forming 3x3 torus topology shown in Fig. 7. In this topology, any pair of two components is separated by more than two hops from each other. A. Router and Routing Our system adopts deterministic routing model, compensating for the two limitations: stringent latency requirement, and trafﬁc predictability [5]. TKIP system doesn’t have possibility of waiting for reordering or retransmission of packet stream, and the trafﬁc pattern is regular and fully predictable. Beside performance and simplicity beneﬁt, deterministic routing has livelock-free property, while it still suffers from deadlock problem. In our design, deadlock problem is resolved by using the virtual channel, which shares a physical channel by several logically separated channel [6]. As shown in Fig. 7, our router has two destination differentiated buffers sharing a pair of up/down transmission line. As for forwarding strategy, we mobilize wormhole method. As its name implies, if header ﬁlt opens the circuit, the subsequent body ﬂit (of the worm) follows the path without any intervention of complex control algorithm. Thus, although the nature of wormhole routing is packet switching, it is almost as efﬁcient as the circuit switching. With wormhole routing, the packet reaches to its destination node in T = Tc ,where Tc is the channel cycle time, L is the packet length, n is the number of links and W is the channel width. For instance, in Fig. 7, It takes 10ns = 11.52μs for 2304 Bytes MSDU leaving from NI (Network Interface) shared memory node and arrives at the NI of CPU2, when the cycle time is 10ns channel width is 32bits. (3 − 1) + 2304×8 32 (cid:3) (cid:3) (cid:2) (n − 1) + L W (cid:2) × 7B-5 ﬂit. Then the header is forwarded into the NoC domain. Fig. 4 shows a waveform of Verilog simulation of our NI. Simulation parameters are as follows-ARM core clock: 100MHz, NoC clock: 75MHz, Burst length: 4, ﬂit width: 32bits, BUS width: 32bits, Burst mode: INCREMENT. This waveform says that the system delays of packet header and body ﬂit generation processes are 40ns and 70ns, respectively. Once the header is fed into the NoC router, the routing path is open, thus subsequent ﬂits follow the path, without any delay. Our NI also supports multicast operation, in which data packet is ﬂooded into multiple paths simultaneously. This multicast functionality presents the greatest advantage over conventional BUS architecture to the architecture; BUS cannot avoid bottleneck because only one master (CPU) can access the data source. Fig. 5. Mealy FSM of NI Shell B. NI (Network Interface) Network Interface decouples computation and communication workload by translating the language CPU can understand (AMBA/AXI signals), into the language of router (header ﬂit followed by packet body ﬂits), and vice versa. Among several researchs on NI, Radulescu et al.’s work [7] is robust and practical enough to connect commercial cores with AMBA/AXI interface. Basically, our NI design depicted in Fig. 3 followed their design criteria. The NI provides extensive set of services including multicast, narrowcast, with QoS (Quality of Service) guaranteed. But their NI is too heavy at current position, because they have complex QoS scheduler and corresponding queues. But usual multiprocessor application, including TKIP, doesn’t need service differentiation, because all trafﬁcs have the same signiﬁcance. Thus we decide to exclude QoS scheduling logic and reduce the packetization delay of NI, instead of providing delicate QoS control. Our NI kernel is depicted in Fig. 3. It operates as follows: Message is forwarded from FIFO to request generator, on its arrival. Then the header builder generates routing path, by referring to the deterministic routing table, and forms header Fig. 6. Flit format deﬁned in our system 1) Interfacing NI with AXI Ports: AXI protocol has 5 distinct channels, as listed here: read/write address channel, read/write data channel, and response channel. Address channels are composed of 32bit address data and associated control signals, and data channels are composed of 32bit payload and associated control signals. On the one hand, read operation is performed by sending address with associated control signals. 609 7B-5 On the other hand, write operation is done by sending initial address with associated control signals, followed by data burst, whose length speciﬁed from awlen of control signals. The awlen control signal declared at the beginning of the transaction, makes the NI design more efﬁcient, because the system can predict the number of ﬂits to transmit before all data is stacked into the NI. And every slave component attach to each NI also requires burst size (awsize), burst length (awlen), and operation ID (identiﬁer for data interleaving, awid). Hence the NI packetizes all of these control signals, as shown in Fig. 6. For a response message (Fig. 6), we’ve deﬁned a bit(r/b) to indicate if the message corresponds read data or to a write response. Next two bits contain R/BRESP signal which indicates the result status of read or write transaction. According to response packet from slave, master determines whether retransmission is required or not. Fig. 5 shows you a mealy machine for operating NI shell. On receipt of reset signal the state is initialized to IDLE. In IDLE state, if control and address signals are asserted on the channel, NI shell snatches them and generates messages described in Fig. 6. After shedding all data messages followed by control message, the state machine returns to IDLE state stimulated by wlast signal. As for response phase, it is a completely reverse process of message generation; based on incoming message it makes corresponding response signal or forwards data ﬂits. CPU1 CRC−1 CPU2 RC4−2 CPU3 MIC−1 NI AMBA/AXI BUS AMBA/AXI BUS NI AMBA/AXI BUS NI Router1 Router2 Router3 CPU4 RC4−3 CPU5 RC4−1 AMBA/AXI BUS NI AMBA/AXI BUS NI CPU6 MIC−2 AMBA/AXI BUS NI Router4 [Center] Router5 Router6 Shared Memory Slave NI CPU7 CRC−2 CPU8 RC4−4 CPU9 MIC−3 AMBA/AXI BUS NI AMBA/AXI BUS NI AMBA/AXI BUS NI Router7 Router Router8 Router9 Fig. 7. TKIP cryptosystem mapped on the 3x3 torus MPSoC platform interconnected by on-chip network IV. THE EVALUAT ION O F COMMUN ICAT ION ARCH IT E CTURE The key of the communication performance in TKIP system is determined by the capability of the concurrent MSDU transmission. This concurrency is given by distributed channels of NoC connecting various components, as opposed to the shared channel in BUS. Before designing a RTL circuit, we made an exploration on these communication architectures with SystemC, to know them inside out. In this experiment, all the components - CPUs, NIs, and Routers - operate at the same frequency (100Mhz). The overall architecture is illustrated in Fig. 7. Three cooperating components (RC4, MIC, and CRC32) can read MSDU from shared memory, on the ﬂy, because the NI supports for multicast operation. For instance, at the ﬁrst interval (0 ∼ 243μs) in Fig. 2, MSDU ﬂits are forwarded toward RC4-1, MIC-1, and CRC-1, simultaneously. This is possible, because the RC4-1 block imposes multicast read request on the shared memory, instead of point-to-point read command. In the realization, it takes 100ns for the read request to go from RC4-1 node to NI of shared memory, passing through router1, router4, and router5. And it takes 190ns for RC4-1 node to receive ﬁrst response from the time at which ﬁrst read instruction is fetched in RC4-1 CPU. And the total collapsed time was 350ns, when the transmission of ﬁrst 16Bursts × 4By tes = 64B y tes is completed. For the complete transmission of the whole 2304B y tes MSDU, 36 read instructions should be executed, because the maximum burst length of AXI protocol is 16. Therefore, every 243μs, 36 reads operation is done in 10.15μs. V. CONCLU S ION S This paper explores an example of cryptographic application on a Multi-Processor NoC system. The platform seeks to meet the throughput requirement of 802.11n wireless LAN. To achieve the goal, ﬁrstly, the load of each component is balanced evenly, according to the performance measurement on a single ARM core. Then, the 9 components are interconnected with NoC fabric to optimize the communication cost and resolve the synchronization problem. As a result of efﬁcient communication architecture, which is equipped with multicast operation, three components can read a MSDU in 10.15μs, as opposed to 30.29μs in the case without multicast operation. "
2008,A debug probe for concurrently debugging multiple embedded cores and inter-core transactions in NoC-based systems.,"Existing SoC debug techniques mainly target bus-based systems. They are not readily applicable to the emerging system that use network-on-chip (NoC) as on-chip communication scheme. In this paper, we present the detailed design of a novel debug probe (DP) inserted between the core under debug (CUD) and the NoC. With embedded configurable triggers, delay control and timestamping mechanism, the proposed DP is very effective for inter-core transaction analysis as well as controlling embedded cores' debug processes. Experimental results show the functionalities of the proposed DP and its area overhead.","5A-4 A Debug Probe for Concurrently Debugging Multiple Embedded Cores and Inter-Core Transactions in NoC-Based Systems Shan Tang and Qiang Xu Depar tment of Computer Science & Engineering The Chinese University of Hong Kong, Shatin, N.T., Hong Kong Email: {tangs,qxu}@cse.cuhk.edu.hk ABSTRACT Existing SoC debug techniques mainly target bus-based systems. They are not readily applicable to the emerging system that use Networkon-Chip (NoC) as on-chip communication scheme. In this paper, we present the detailed design of a novel debug probe (DP) inserted between the core under debug (CUD) and the NoC. With embedded conﬁgurable triggers, delay control and timestamping mechanism, the proposed DP is very effective for inter-core transaction analysis as well as controlling embedded cores’ debug processes. Experimental results show the functionalities of the proposed DP and its area overhead1 . 1. INTRODUCTION Existing system-on-a-chips (SoCs) usually utilize on-chip buses to connect embedded cores. However, it is well known that functional bus does not scale well with the shrinking technology feature size, because of its speed limitation and high power consumption when a large number of cores are attached to the bus. Network-on-Chip (NoC), as a promising solution for integrating IPs in the future giga-scale SoCs, is gaining wide acceptance in both academia and industry (e.g., [5, 6, 9]). A typical NoC contains three parts: network interfaces (NIs) that connect the IP cores to the NoC and convert transaction messages into packets, routers that transport packets between NIs according to a predeﬁned protocol, and links that connect routers and provide the raw bandwidth. While the NoC facilitates designers to integrate more IP cores onchip, the veriﬁcation problems become more challenging and it is likely that the NoC-based systems need to go through one or several re-spins to become bug-free. An efﬁcient and effective silicon debug strategy for such complex systems is therefore of crucial importance. Existing silicon debug techniques (e.g., [4, 12, 18]), however, mainly target busbased systems and are not readily applicable to NoC-based systems with a totally different and more complex communication scheme. In this paper, we show the detailed design of a novel debug probe (DP), one of the key elements of the debug platform for NoC-based systems presented in [19]. The proposed DP, inserted between every core under debug (CUD) and the NI associated to it, not only supports effective inter-core transaction tracing and analysis, but also provides full debug access to the CUD through its debug interface (e.g., JTAG interface). With embedded programmable triggers, delay control and timestamping mechanism, the DP facilitates multi-core concurrent debug with cross triggering and real-time tracing capabilities. Moreover, the proposed DP uses the NoC connections to transport debug and trace data instead of using dedicated wires, which ﬁts better in the NoC environment. When compared to the monitoring probes attached to the NoC routers presented in [7, 8] that monitor the NoC only, the proposed DP is able to debug and trace the activities on both the NoC and the CUD concurrently at the system-level. 1This work was supported in part by the Hong Kong SAR RGC Earmarked Research Grants 2150503 and 2150558. The remainder of this paper is organized as follows. Section 2 and Section 3 overview related work and the NoC debug platform presented in [19]. The proposed DP design is detailed in Section 4. Section 5 gives the simulation results and analyzes the area cost of the debug probe. Finally, Section 6 concludes this paper. 2. PRIOR WORK Debugging an SoC (as a blend of hardware and software) is an extremely complex problem and cannot be tackled without effectively observing the operations of the design’s internal nodes [11]. While capturing snapshots through JTAG interface provides basic postmortem debuggability and are widely utilized in practice [21], the trend is to embed more design-for-debug (DfD) logics for hardware tracing difﬁcult-to-ﬁnd bugs (e.g., [1, 3, 12, 13, 22]). With these techniques, debugging a single core is a relatively well studied problem (still challenging though). However, since embedded cores (processors, DSPs, etc.) have to work together for certain functions, debugging one core at a time can be ineffective and sometimes misleading [18]. For a complex SoC with a number of embedded cores, since they communicate with each other during normal operation, concurrently debugging several cores at a time greatly increase the debug effectiveness and efﬁciency. In addition, as the on-chip communication scheme is an important part of the design, designers often want to see the transactions between cores to identify the root cause of bugs and/or the system performance bottleneck. As a result, an effective multi-core debug solutions should fulﬁll the following requirements: • concurrent debug access to multiple embedded cores; • inter-core transaction tracing and analysis; • real-time tracing of debug components; • cross-triggering among debug components; • low DfD overhead in terms of area, routing and device pins; Several multi-core debug solutions were proposed by introducing various on-chip instrumentation (OCI) blocks customized for diverse processors, logic cores and embedded buses (e.g., First Silicon’s debug solution [12, 18] and ARM CoreSight [4]). The above multi-core debug solutions for bus-based systems, however, are not readily applicable for NoC-based SoCs. First of all, the drastic change in the communication infrastructure affects the debug access mechanism.In addition, the new inter-core communication scheme (i.e., the NoC) requires new tracing and analysis methods. Moreover, embedded cores often work asynchronously and this makes the distribution of trigger events very difﬁcult. There are also limited works related to the debug of NoC-based systems. Ciorda¸s et al. [7] proposed to attach dedicated monitoring probes to the NoC routers, which provides basic observability of the NoC in the form of bits. Later, to increase debug efﬁciency, the same authors [8] improved their monitoring probe to be able to monitor transactions by reconstructing them from the bit-level monitored data. Although [7, 978-1-4244-1922-7/08/$25.00 ©2008 IEEE 416 5A-4 P C O D U C G A T J e c a r T C Por t D Por t T Por t D P NI NoC R R R R C Por t D Por t D P C Por t NI NI NI Ctr MSA T Por t NoC based SoC NoC Configuration P C O G A T J D U C Debug Software Debug g er For Cor es Cross-Tr igger Transaction Debug g er Mul ti-core Debug D r iver PC Inter face JTAG DA Off -chip Debug Control ler Trace Hardware Debug Contr ol ler C por t : Communication Por t D por t : Debug Por t T por t : Trace Por t MSA : Monitor Service Access Figure 1: Debug Platform for NoC-based Systems 8] is powerful for ﬁnding bugs inside the NoC, it is not suitable for debugging the entire system because an effective debug process should get all the active parties involved, i.e., the activities on both the NoC and the cores under debug (CUDs) need to be traced in a uniﬁed debug architecture. In fact, the monitoring service presented in [7, 8] can be treated as built-in DfD logics for the NoC itself, similar to the debug functionalities built into an embedded processor (e.g., ARM in-circuit emulator (ICE) [3]) and logic cores. They can be used to facilitate to precisely pin-point the exact location of the bugs after determining whether the bug exists inside the CUD or inside the NoC. The above observations has motivated us to develop a debug platform suitable for NoC-based systems [19], as brieﬂy introduced in Section 3. 3. NOC DEBUG PLATFORM OVERVIEW The debug platform in [19], as shown in Fig. 1, is composed of three main parts, on-chip debug architecture, including debug probes (focus of this paper) and a debug agent, off-chip debug controller and supporting debug software. Debug commands and data are transferred through NoC connections with guaranteed quality of service (QoS) in terms of throughput and latency, which is ﬂexible for different debug applications as we can simply adjust the NoC bandwidth used for debug2 . Using NoC connections for debug access also reduces the routing overhead as no dedicated wires are introduced. For the ease of discussion, we assume that embedded cores communicate with each other using the OCP protocol [15]. Note that, however, the debug platform works with other transaction based communication protocols (e.g., AXI [2] or DTL [16]) as well. Supporting Debug Software: The supporting debug software provides the graphical user interface (GUI) and/or command line interface for controlling the debug process and displaying the debug results and other information. The supporting debug software is made up of three layers of different tools. In the middle, various core speciﬁc debuggers from core providers (e.g., ARM [4]) and a transaction debugger control and observe corresponding debug entities. Upon these debuggers, a cross debugger communicates to them at the same time and controls multi-core cross-debug. The bottom layer is a multi-core debug driver, which forwards the debug requests and collects debug information to/from the off-chip debug controller, through a PC peripheral interface, such as parallel interface or ethernet. Since multiple debug requests/information may arrive the driver asynchronously, the driver needs to sort them and add/remove addressing labels to/from them. Off-chip Debug Controller: The off-chip debug controller serves as a translation layer between the software debuggers and the on-chip debug architecture, which builds transparent connections between them. 2We assume that the NoC can be reconﬁgured through its control interface as in [9]. It receives the debug commands or sends debug data from/to software debuggers, schedules them and controls the on-chip debug agent (discussed later) through the system-level JTAG interface. In addition, the off-chip debug controller uses a TDM (Time Division Multiplexing) scheme for sharing single chip-level trace port among multiple CUDs that send out trace data in real-time (detailed in Section 4.3). On-chip debug architecture: The on-chip debug architecture is the core of the debug platform. It mainly contains the following components: • a debug probe (DP) between every CUD and its network interface; • a system-level debug agent (DA) that provides multi-core debug access through a single chip-level JTAG interface and an optional trace port; OCP NI OCP  Master INF OCP NI OCP  Master INF Debug Control Reg Trace Control TAP Chip JTAG Chip Trace Por t Figure 2: Block Diagram of Debug Agent As the DP is the focus of this paper, it will be discussed in detail in Section 4. Fig. 2 gives an overview of the debug agent. The DA translates the command sequences on the chip-level JTAG port into read/write operations of on-chip debug resources. With QoS guaranteed NoC channels between the DPs and the DA, we can map all debug resources (e.g., debug control and status registers inside DPs or CUDs) to memory-mapped address space as the slave of the DA and access them in a uniﬁed manner. We introduce an extra JTAG instruction “DEBUG_REG” and the corresponding JTAG data register “DEBUG_REG_DATA” for accessing the debug resources with the following format: Field Description WR/RD ‘1’ for write operation; ‘0’ for read operation ADDR Register address DATA Register write/read data READ_VALID ‘1’ means read data available The off-chip debug controller can then read or write debug registers inside the chip through standard JTAG interface, as shown in the following table. 417 5A-4 Off-chip Debug Controller Write ’DEBUG_REG’ command into IR; Write operation Shift in ’write’ command to DR: :WR:ADDR:DATA:-: Read operation Shift in ’read’ command to DR: :RD:ADDR:-:-: Do {Shift out the contents of DR;} While (READ_VALID) != ’1’; The DATA ﬁeld in RD is the valid data; Debug Agent Use DEBUG_REG_DATA register as DR; Write speciﬁed register; Read speciﬁed register; Set READ_VALID when data is ready; The DA also supports the optional chip-level trace port controlled by the off-chip debug controller, where the software debuggers are able to access the trace data inside the DPs and CUDs in real-time, if any. 4. DEBUG PROBE the delay control unit. Concurrent debug of multiple cores usually requires synchronizing the debug operations of multiple DPs and CUDs by ‘delaying’ these operations properly [19]. This is done by ﬁrst setting appropriate values to the delay counter in the delay control unit before really issuing debug commands. The delay counter will then count down on its local clock to zero before the debug operation is performed. For example, to write a DP’s register after ﬁve clock cycles, debugger should set the initial delay value as ﬁve before the write operation (so-called “delayed write”). 4.2 Core Debug Module The core debug module controls and observes the CUD through its debug interface (usually JTAG), by translating the debug register read/write commands from the debug access module into the CUD’s debug access protocol. For example, our experimental design implements the JTAG debug protocol used for ARM7TDMI core (i.e., embedded ICE [3]). In this module, the access to CUD’s debug registers is achieved by certain JTAG sequences. From the external core debugger point of view, these translations are transparent except extra processing delay. Transaction Trace Buffer Tr igger & Trace 4.3 Core Trace Module OCP (Communication  por t ) CUD JTAG (Debug por t ) JTAG Control Transact ion Trace Module Delay  Control OCP  Slave IF OCP N I Debug Access Module Core Debug Module Debug Connection NoC The core trace module controls the CUDs’ trace port (if any), buffers the traced data and provides a OCP slave interface for the DA to read out the buffered data through NoC connections. However, since there is only one chip-level trace port in most cases, when several CUDs have data to send out at the same time, the “many-to-one” transfer may result in contentions. To avoid that problem, a TDM scheme is used at the chip-level trace port, which is controlled by the off-chip debug controller. By assigning the time slot to each CUD, it grants certain amount of tracing bandwidth(as shown in Fig. 4). System- level Trace Port Read  overhead Read  overhead Read  overhead Time Slot 1 Time Slot 2 CUD 1 CUD 2 CUD i Time Slot i Figure 4: Assign the Time Slot for Each CUD Apparently, the sum of the bandwidth assigned to the CUDs can not exceed the total bandwidth supported by chip-level trace port. As a result, we may need to limit the trace data generated by the CUD. Given the bandwidth of the chip-level trace port (Bchi p b ps), the number of traced CUDs (N ), the time slot for each core CU D(i) (T S(i) second s), and the necessary interval (read overhead, such as set the read address etc.) between two read operations (Tint second s), the following equation gives the available bandwidth for CU D(i): BCU D(i) = Bchi p ( T S(i) (T S(i)+Tint ) ∑N i=1 ) (b ps) (1) Even if the CUD is assigned with enough trace bandwidth, the DP still needs to buffer trace data when waiting for its time slot. For CU D(i), it has to wait for N Tint + ∑N j (cid:2)=i T S( j) second s. During this time, the volume of trace data T D(i) from CU D(i) is as follow: T D(i) = BCU D(i) (N Tint + ∑N j (cid:2)=i T S( j)) )(N Tint + ∑N j (cid:2)=i T S( j)) (bit s) (2) T S(i) (T S(i)+Tint ) = Bchi p ( ∑N i=1 Trace Por t Core Trace Core Tr ace Buffer OCP  Slave IF Core Trace Module OCP N I Trace Connection Figure 3: Debug Probe Block Diagram From the debug point of view, embedded CUDs have three kinds of interfaces: the functional communication port (OCP interface here), debug interface (usually JTAG) and the optional trace port (e.g., for processors [4, 13]). Therefore, at the CUD side, the role of the DP is also three-fold: (i) it generates the necessary JTAG signals to control and observe the CUD; (ii) it traces the OCP transactions with reconﬁgurable trigger conditions; and (iii) it transfers the trace data that come from the CUD out of the chip for further analysis. On the NoC side, the DP need two NoC connections: one for debug access and the other for the trace (if the trace function is needed). As shown in Fig. 3, the debug probe is composed of four main components: transaction trace module, debug access module, core debug module, and core trace module. Among them, the core trace module can be removed for those CUDs that do not have trace interfaces. In addition, the functionalities of these components can be enhanced or degraded according to the debug requirements(e.g., adding or removing triggers). With this scalable architecture, designers are able to trade-off the cost and the functionalities of the DP. We detail the design of these modules in the following. 4.1 Debug Access Module The debug access module provides an OCP slave interface as the service access point of the DP. All debug resource are mapped into a uniﬁed address space and accessed through memory-mapped OCP read/write operations. The OCP slave interface decodes the OCP address to distinguish between the registers and the buffer, and then passes the register access to 418 5A-4 Trace Unit: The trace unit is responsible for recording the transactions with predeﬁned format on a trigger event (see Fig. 5). However, as the trigger event may be activated several clock cycles after the transaction message starts, if we only store the transactions at the trigger time, some essential information may be already lost. To tackle this problem, we design a shadow buffer inside the trace unit to store the “start cycle” of the transaction and its timstamp temporarily. Usually, this cycle can be identiﬁed from the “master command”. In addition, the “trigger cycle” and its timestamp are also stored for generating the transaction records. When the trigger event is activated, part of the data in the shadow buffer are fetched out to generate a transaction record and then write to the transaction trace buffer by the transaction records generation unit. To simplify the record generation process and buffer control mechanism, we use a ﬁxed-length record format as follows: 127 Record  Ident if ier 125 77 0 Times tamp (s tart cyc le) Pay load The record identiﬁer is a predeﬁned codeword that identiﬁes the trigger event associated to this record. The width of the record identiﬁer depends on the number of the detectors. Each bit indicates the trigger status of one detector. The timestamp represents the time when the transaction starts. It is the number of clock cycles passed from the start of the debugging. The payload ﬁeld contains selected transaction data. In most cases, designers only need to see part of the transaction data(e.g., commands and address) in one debug iteration. Therefore, the record generation module supports ﬂexibly choosing part of the OCP signals stored in the shadow buffer by setting a payload mask. Only those signals whose corresponding mask bits are set will be stored in the payload of the record. By doing so, the length of the record payload can be effectively reduced without losing debug accuracy. The size of payload is a design-time trade-off between better observability and smaller area and trafﬁc cost. In our experimental design, we chose 2-bit record identiﬁer (for two detectors), 48-bit timestamp and 78-bit payload to build a 128-bit width record. For example, designer wants to know the round-trip delay when the CUD reads from address 0x80000010. Then the “read started start”, which is the timestamp of the “start cycle”, and the “response time” which is the timestamp of the “trigger cycle” have to be recorded. The “start cycle” is the cycle where the OCP master command (OCP signal MCmd) is 3 b010 (’read’ operation) and the master address (OCP signal MAddr) is 32 h80000010. The “trigger cycle” is detected when the OCP slave response (OCP signal SRep) is 2’b01 (valid data). So we set the trigger and payload mask as follows: (cid:4) (cid:4) (cid:4) (cid:2) 1. set trigger condition: MCmd = 3 b010 MAd d r = 32 h80000010 SRe p = 2 b01 2. set payload mask to enable “OCP MCmd” of start cycle; 3. set payload mask to enable “Timestamp” of trigger cycle; 4. start debug; (cid:2) (cid:4) (cid:4) With the above settings, when the trigger event is activated, the “timestamp” and “MCmd” of “start cycle” and the “timestamp” of “trigger cycle” are fetched from the shadow buffer and then ﬁlled in the “timestamp” and “payload” ﬁelds of the record respectively as: 127 125 ‘11 ’ Times tamp (s tart cyc le) 77 74 MCmd Times tamp  (response cyc le ) 26 0 NA The transaction records are ﬁrstly written to the transaction trace buffer and then wait for the off-chip debug controller to read it out. A straightforward method is to fetch the entire trace data out at once after the debug process terminates. However, it requires that the transaction trace buffer is large enough to store all the records, whose number OCP h c t i w S t u p n I Trigger Unit Detector 1 Detector 2 Detector n Tr igger Control M U X Tr igger signals Trace Un it r e m i T Shadow Buffer l o r t n o C e c a r T Records Generation Buffer  W r ite Buffer ful l Register W /R inter face Figure 5: Transaction Trigger and Trace Unit T S = Bchi p (T S − T S2 In case the time slot for every CUD is the same (T S), we get: T D(i) = Bchi p ( N (T S+Tint ) )((N − 1)T S + N Tint ) N (T S+Tint ) ) (bit s) (3) As the Tint is generally much smaller than T S, the trace data for CU D(i) can be further simpliﬁed as follows: T D(i) ≈ Bchi p (T S( N−1 )) (bit s) (4) N As above equations, larger time slot provides more efﬁcient bandwidth utilization (larger BCU D(i) ), but requires larger trace buffer because every core has to wait longer for the next time slot. 4.4 Transaction Trace Module The transaction trace module monitors inter-core transactions and records them based on conﬁgurable trigger conditions. The key building block in this module is a transaction trace and trigger unit, as shown in Fig. 5. Trigger Unit: The modular trigger unit contains a number of detectors (see Fig. 5) which realize various trigger conditions. The inputs of these detectors are chosen from the OCP signals with an input switch; while their outputs are supplied to a MUX to generate the ﬁnal trigger signals. A detector can be a simple two-input comparator, which just compares a set of OCP signals (e.g., command, address or data) to a runtime conﬁgurable value. A more complex detector can be a transaction analyzer that is able to identify transaction errors or certain patterns. The basic function of the transaction analyzer in our experimental design is to check whether the transactions conform with the OCP protocol. It detects errors such as ‘wrong command coding’, ‘illegal address’, ‘no command acceptation’, ‘no response’, and ‘wrong response’. It can be extended for different debug requirements and ﬁtted in the system with well deﬁned interface. For example, by embedding some counters, it is able to calculate the delay of a transaction (e.g., the round-trip read delay), for evaluating whether the NoC design achieved the QoS target. In addition, multiple detectors can work together to form more complex trigger conditions by supplying one detector’s output to other detectors as part of its inputs or even as the its enable/disable control signal. As an example, we need a DP to “trigger when the transaction initiator reads address 0x8000xxxx but do not get response data in 20 clock cycles”. We can combine a comparator that checks the OCP command and address signals and a transaction analyzer which counts the response time. Instantiating how many detectors is a design-time decision, but what the trigger conditions can be run-time controlled by programming them. 419     5A-4 Test Bench OCP Communication IP CUD JTAG NEXUS D P SoC RTL Model Debug Trace NoC JTAG DA ODC Figure 6: Simulation Environment is not even predictable. Another strategy is to fetch the debug data constantly during debug process before the buffer is full. But, due to the bandwidth limitation of the debug connections, reading out the trace data in real-time may be quite difﬁcult. Therefore, as a trade-off, we use reasonable buffer size with ‘best effort’ read policy, i.e., the trace data is read out as soon as when no other high-priority operations (e.g., debug control) uses the same debug connection. Clearly, the transaction buffer may overﬂow under this strategy. In this case one bit in the debug status register is set to indicate the buffer overﬂow. We expect designers to set a tighter trigger condition to avoid buffer overﬂow in the next debug run in case it happens. Since the debug process itself is a “try-and-error” process and usually requires multiple iterations to pinpoint the bug locations, we believe this debug strategy will not increase the debug time signiﬁcantly. 4.5 DP Programming Model By default, the DP is disabled and only debug access module is listening to the debug commands, so that the power consumption is reduced in normal functional mode. Under such circumstance, the DP needs to be conﬁgured and enabled ﬁrst before the debug process starts. Most modules can work with default settings and do not need to be reconﬁgured frequently except for the Transaction Trace Module, which should be programmed in the following sequence in one debug iteration: 1. Select the inputs of the detector and the transaction analyzer from the OCP signals; 2. Set the trigger output of MUX. 3. Set the operation parameters of the detector and the transaction analyzer; 4. Set the start and stop counters; 5. Set the payload mask for record generation; 6. Set the timestamp counter initial value; 7. Set the ‘enable’ bit in the control register to start the trigger and trace process; 5. EXPERIMENTAL RESULTS 5.1 Functional Simulation To verify the functionalities of the debug probe described in Section 4, we build a simple yet effective simulation environment, as shown in Fig. 6. The behavioral model of the CUD implements the debugrelated functions of a processor, which serves as the transaction initiator that communicates to another IP module (a memory core here) through a NoC channel. During the simulation, the CUD reads and writes the IP core randomly, where these transactions are monitored and recorded by the DP. The debug port is functionally equivalent to the ARM Embedded ICE JTAG port and the trace port is a NEXUS auxiliary port [14] that transports messages containing traced data with predeﬁned throughputs. The off-chip debug controller generates JTAG debug commands for setting up the DP’s debug registers as described in Section 4.5. These commands are translated by the DA and then access the debug resources in the DP or the CUD through NoC debug connections. It should be noted that we use connections with pre-deﬁned delays to model the QoS-guaranteed NoC communication channels. We conduct two simulations to illustrate the functionalities of the proposed DP, as discussed in the following paragraphs. Fig. 7 shows a ‘delayed write’ operation to the CUD’s debug control registers. In the beginning, we set the delay counter register in the “Debug Access Module” with a pre-deﬁned value (‘5’ in this example). Next, we send the address of the target debug control register inside the CUD and the data to be written into it to the CUD’s JTAG interface. As can be observed from the ﬁgure, since the value of the delay counter is ‘5’, this write operation is delayed for 5 clock cycles. When the CUD’s JTAG interface unit receives the address and data, they are transferred to the CUD (with protocol speciﬁed in [3]). Finally, the Debug Control Register is set with expected value after the JTAG sequence is complete. Since the JTAG sequence for a speciﬁc register is ﬁxed, we can control the time when the debug commands (by setting debug control register) take into effect with the above method. Similarly, the debug registers inside the DP can also be written with a pre-determined delay. Fig. 8 presents an example on the transaction trace process. In this experiment, we ﬁrst program the trigger and trace registers as described in Section 4.5.The DP then monitors the CUD’s OCP port continuously according to the pre-assigned conﬁgurations. When one of the transactions try to read from the speciﬁed address, the detector starts the transaction analyzer (as detailed in Section 4.4). If the transaction analyzer ﬁnd that the delay of the response is larger than the pre-deﬁned value, it triggers the “Record Generation Unit"". Finally, this transaction is recorded after one clock cycle into the transaction trace buffer for further analysis. 5.2 Debug Probe Cost Analysis As introduced in Section 2, the on-chip debug architecture contains a number of debug probes. Therefore, the area of the proposed DP should be well controlled to reduce the total DfD area cost. We implement an experimental DP design in a commercial 90nm CMOS library. A detector for comparing OCP commands and addresses and a transaction analyzer that detects the read delay are instantiated in our design. The transaction trace buffer used in the design is an 32 × 128bits asynchronous FIFO, in which we can store 32 transaction records, bits of the payload. We also implement a 32 × 32-bits core trace buffer. including 2 bits of the record identiﬁer, 48 bits of the timestamp, 78 These two buffers are implemented by general-purpose ﬂip-ﬂops in our current implementation. Debug Probe Top Transaction Trace Module Trigger and Trace Top Input Switch Detector Transaction Analyzer MUX Record Generation Shadow Buffer Trigger and Trace Control Transaction Trace Buffer Debug Access Module Delay Control OCP Slave INF Core Debug Module JTAG INF Core Trace Module Core Trace INF Core Trace Buffer OCP Slave INF Area (μm2 ) 188059.0 Percentage 100.0% 19692.1 481.8 543.3 364.4 14.3 4629.7 9197.9 4456.3 120689.9 5384.8 2020.7 3912.9 3099.6 31225.6 2024.0 10.5% 0.3% 0.3% 0.2% 0.0% 2.5% 4.9% 2.4% 64.2% 2.9% 1.1% 2.1% 1.6% 16.6% 1.1% Table 1: DfD Area Cost of the Debug Probe 420 write to delay counter with  expected clock cycle number Write operation is delayed  5A-4 Start JTAG command secquence CUD ’s debug control register is set Figure 7: Simulation Waveform for the Delayed Write Operation configure the trigger and trace module OCP transactions detector hit here transaction analyzer hit here  transaction record generated here Figure 8: Simulation Waveform for the Transaction Trace Operation Table 1 presents the detailed area data of all the blocks shown in Fig. 3. The total area of this design is 188059 μm2 (about 57k gates). Further analysis shows that the two trace buffers occupy most of the area in the DP (as expected), more than 80 percent. If using SRAMs instead of registers to form these buffers, their areas can be reduced signiﬁcantly. The control logic inside the DP is about 10k gates, which is considered to be acceptable for giga-scale NoC-based systems. Adding more triggers and core trace modules will not increase the area of control logic signiﬁcantly. The area of the transaction trace buffer depends on the number and width of the trace records, which differs from one application to another. The core trace buffer size varies with different cores’ debug requirements and also the time slot sizes as analyzed in Section 4.3. Generally speaking, our DP design provides the ﬂexibilities for designers to make the best trade-off according to their debug needs. 6. CONCLUSION In this paper, we present the detailed design and analysis of a novel debug probe for the NoC-based systems, which is composed of four main components: transaction trace module, debug access module, core debug module, and core trace module. The DP is well ﬁtted in the debug platform presented in [19] to facilitate concurrent debug of embedded cores and inter-core transactions, at a moderate DfD area overhead. 7. "
2008,Application-specific Network-on-Chip architecture synthesis based on set partitions and Steiner Trees.,"This paper considers the problem of synthesizing application-specific network-on-chip (NoC) architectures. We propose two heuristic algorithms called CLUSTER and DECOMPOSE that can systematically examine different set partitions of communication flows, and we propose Rectilinear-Steiner-tree (RST) based algorithms for generating an efficient network topology for each group in the partition. Different evaluation functions in fitting with the implementation backend and the corresponding implementation technology can be incorporated into our solution framework to evaluate the implementation cost of the set partitions and RST topologies generated. In particular, we experimented with an implementation cost model based on the power consumption parameters of a 70 nm process technology where leakage power is a major source of energy consumption. Experimental results on a variety of NoC benchmarks showed that our synthesis results can on average achieve a 6.92 x reduction in power consumption over the best standard mesh implementation. To further gauge the effectiveness of our heuristic algorithms, we also implemented an exact algorithm that enumerates all distinct set partitions. For the benchmarks where exact results could be obtained, our CLUSTER and DECOMPOSE algorithms on average can achieve results within 1% and 2% of exact results, with execution times all under 1 second whereas the exact algorithms took as much as 4.5 hours.","3B-4 Application-Speciﬁc Network-on-Chip Architecture Synthesis based on Set Partitions and Steiner Trees Shan Yan and Bill Lin Department of ECE, University of California, San Diego La Jolla, CA, 92093 Email: shyan@ucsd.edu, billlin@ece.ucsd.edu Abstract— This paper considers the problem of synthesizing application-speciﬁc Network-on-Chip (NoC) architectures. We propose two heuristic algorithms called CLUSTER and DECOMPOSE that can systematically examine different set partitions of communication ﬂows, and we propose Rectilinear-Steiner-Tree (RST) based algorithms for generating an efﬁcient network topology for each group in the partition. Different evaluation functions in ﬁtting with the implementation backend and the corresponding implementation technology can be incorporated into our solution framework to evaluate the implementation cost of the set partitions and RST topologies generated. In particular, we experimented with an implementation cost model based on the power consumption parameters of a 70nm process technology where leakage power is a major source of energy consumption. Experimental results on a variety of NoC benchmarks showed that our synthesis results can on average achieve a 6.92× reduction in power consumption over the best standard mesh implementation. To further gauge the effectiveness of our heuristic algorithms, we also implemented an exact algorithm that enumerates all distinct set partitions. For the benchmarks where exact results could be obtained, our CLUSTER and DECOMPOSE algorithms on average can achieve results within 1% and 2% of exact results, with execution times all under 1 second whereas the exact algorithms took as much as 4.5 hours. I . INT RODUCT ION Network-on-Chip (NoC) architectures have been proposed as a scalable solution to the global communication challenges in nanoscale SoC designs [1, 2]. They can be designed as regular or application-speciﬁc network topologies. Regular topologies have been successfully employed in a number of tile-based chip-multiprocessor projects, e.g. [15, 16], which are appropriate because of processor homogeneity and application trafﬁc variability. On the other hand, for custom SoC applications, the design challenges are different in terms of varied core sizes, irregularly spread core locations, and different communication bandwidth requirements. Therefore, an application-speciﬁc network architecture customized to the needs of the application is more appropriate. This synthesis problem is the focus of this paper. The NoC synthesis problem is challenging for a number of reasons. First, for a large complex SoC design, an optimal solution will likely involve multiple networks since each core will likely communicate only with a small subset of cores. Therefore, a single network that spans all nodes is often unnecessary. Part of the synthesis problem is to partition the set of speciﬁed communication ﬂows into subsets and derive a separate optimal physical network topology for each subset. In general, ﬂows may be grouped together even though they don’t share common sources or destinations because they may be able to beneﬁcially share common intermediate network resources. Second, besides deciding on the set partition, our synthesis problem must also decide on the physical network topology of each group in the set partition. Finally, depending on the optimization goals and the implementation backend, the appropriate cost function may be quite complex. In particular, in this paper, we consider the power minimization problem that considers both leakage power and dynamic switching power. It is well-known that leakage power is becoming increasingly dominating [10, 12]. Therefore, it is important to properly account for leakage power when adding routers and channels to the synthesized architecture. However, when considering leakage power, the cost function may need to account for possibly discrete as well as nonlinear cost increments of links and routers whereas dynamic switching power may be best modeled as a function of cumulative data rates. Other optimization goals may include minimizing hop-counts along with power minimization. In this paper, we describe two heuristic algorithms called CLUSTER and DECOMPOSE that systematically examine different set partitions of communication ﬂows. For each set partition considered, we use well-developed RectilinearSteiner-Tree (RST) algorithms [18–20] to generate a physical network topology for each group in the set partition. Though the RST problem is in itself NP-hard, well-developed fast RST algorithms are available that can be effectively used, as indicated by the run-times presented in Section VII. For each RST derived, the routes for the corresponding ﬂows and the bandwidth requirements for the corresponding network channels are determined. We then use the speciﬁed evaluation function to evaluate the implementation cost of the corresponding set partition and RST-generated physical topologies. At the end of each algorithm, the best solution found is returned. Although we use Steiner trees to generate a physical network topology for each group in the set partition, the ﬁnal NoC architecture synthesized is not necessarily limited to just trees as RST implementations of different groups may be connected to each other to form non-tree structures. For the rest of the paper, Section II outlines related work. Section III presents the problem description and our formulation. Sections IV and V describe the CLUSTER and DECOMPOSE algorithms respectively. The router merging algorithm for further performance improvement is discussed in 978-1-4244-1922-7/08/$25.00 ©2008 IEEE 277 3B-4 Section VI. The experimental results and the conclusion are presented in Section VII, and VIII respectively. I I . RE LAT ED WORK The work presented in [9] addresses the complementary problem of providing custom network architecture instantiations. Other existing NoC solutions assume a regular meshbased NoC architecture [3, 4], and their focus is on the mapping problem. On the problem of designing application-speciﬁc NoC architectures without assuming an existing interconnection network architectures, several techniques have been proposed [5–8]. The optimization method presented in [5] is primarily concerned with providing connectivity under node degree constraints, but it does not consider the dimensioning of links or routers and their implementation and power costs. In [6], techniques were presented for the constraintdriven communication architecture synthesis of point-to-point links by using heuristic-based k-way merging. However, their technique is limited to topologies with speciﬁc structures that have only two routers between each source and sink pair. In [7], novel NoC synthesis algorithms were presented, but their solutions only consider topologies based on a slicing structure where router locations are restricted to corners of cores and links run around cores. In addition, as stated in [7], their power minimization problem is one of minimizing the total trafﬁc ﬂowing through the routers of the derived NoC architecture, which corresponds well to process technologies where dynamic switching power dominates, but not necessarily when leakage power is substantial. In [8], an innovative NoC synthesis ﬂow was presented with detailed backend integration. Their method is based on the min-cut partitioning of cores to routers. I I I . PROBL EM D E SCR I PT ION AND FORMULAT ION A.Description Figure 1 illustrates an example of our application-speciﬁc NoC architecture synthesis problem. Consider the small example speciﬁcation shown in Figure 1(a) where the nodes represent cores, edges represent communication ﬂows, and edge labels represent the bandwidth requirements for the corresponding ﬂows. In our design ﬂow, an initial ﬂoorplanning step is performed in advance of NoC synthesis to obtain a placement of the cores. An example ﬂoorplan is shown in Figure 1(b). In general, ﬂoorplanning solutions do not have to follow a slicing structure, and the state-of-the-art ﬂoorplanning methods allow for non-slicing ﬂoorplans to achieve more efﬁcient solutions. The only requirement is that the cores are nonoverlapping. Also, in our problem deﬁnition, modules in a design do not necessarily have to be attached to the on-chip network. Modules can also be connected by means of conventional routing of interconnects, as shown in the un-labeled rectangles in Figure 1(b). The ﬂoorplanning problem has been extensively studied with many well-developed solutions (e.g. [21–24]). Recent work has considered modiﬁcations to ﬂoorplanning metrics in the context of NoC synthesis [7]. After ﬂoorplanning, the (x, y ) coordinates of the communication ports of the cores are known, and they are provided as input to our synthesis problem. In addition, the communication bandwidth requirement for each trafﬁc ﬂow is also provided, as 200 v0 v1 v2 100 v5 v3 v4 400 v6 200 (a) Example. v0 v2 v4 v1 v3 v5 v6 v0 λ(e1) = 200 λ(e3) = 100 v2 λ(e4) = 200 v4 v1 λ(e2) = 200 v3 (b) Floorplan. (c) CDG. v5 200 v6 v2 300 v4 v0 200 300 200 200 v3 200 200 v1 v0 200 300 200 v2 300 v4 v1 v3 200 v5 v6 v5 200 v6 (d) One architecture. (e) Alternative architecture. Fig. 1. Illustration of the NoC synthesis problem. shown in Figure 1(c). Speciﬁcally, the input to our synthesis problem is a directed graph G(V , E , π , λ), called a communication demand graph (CDG), where each node vi ∈ V is associated with a communication port of a core, and each directed edge ek = vi → vj ∈ E represents a communication ﬂow from vi to vj . The position of each node vi is given by π(vi ) = (xi , yi ). The bandwidth requirement for each communication ﬂow ek is given by λ(ek ). Based on the optimization goals and cost functions speciﬁed by the users, the output of our NoC architecture synthesis problem is a optimized custom network topology with minimum cost with pre-determined routes for the speciﬁed communication ﬂows on the network such that the bandwidth requirements are satisﬁed. For example, Figures 1(d) and 1(e) show two different topologies for the CDG shown in Figure 1(c). Figure 1(d) shows a network topology that interconnects all the nodes in the system. In this topology, the pre-determined route for ﬂow e1 traverses through node v0 , v2 , v4 , v3 , and v1 . Figure 1(e) shows an alternative topology comprising of two separate networks. Here, ﬂow e1 is simply transferred over the network channel from v0 to v1 with the corresponding dimensioning. After an optimized NoC architecture has been synthesized with the corresponding routers and links, the ﬂoorplanning step can be invoked again to update the placement of cores and communication ports, and the NoC can be resynthesized with the updated ﬂoorplanning information. As shown experimentally in Section VII, our NoC synthesis algorithms are fast, making it possible to iterate NoC synthesis with ﬂoorplanning. B.Formulation In general, the solution space of possible application-speciﬁc network architectures is quite large. Depending on the commu278 nication demand requirements of the speciﬁc application under consideration, the best network architecture may indeed be comprised of multiple networks. However, the decision on the number of networks and the partitioning of communication of ﬂows among them is not a simple question of partitioning communication ﬂows into groups where the corresponding sets of communication ports are disjoint. Consider again the example depicted in Figure 1(c). The communication ﬂows e3 and e4 have disjoint communication ports. However, it may still be best to group them together on the same network, as depicted in Figure 1(e), because both ﬂows must travel a common distance in the horizontal direction, and they are able to share the cost of the network channel that spans the horizontal distance. In general, the solution space of distinct set partitions of n ﬂows, commonly known as the nth Bell number, is known to grow Θ(n log n)n [17]. Bn grows rapidly, with e.g. B10 , B11 , and B12 equal to 115975, 678570, and 4213597, respectively, and so on. The goal of the heuristic algorithms CLUSTER and DECOMPOSE presented in Sections IV and V, is to signiﬁcantly reduce the number of set partitions needing to be examined in a systematic manner. Besides deciding on the set partitioning of ﬂows, physical network topologies must be decided for carrying the communication ﬂows. In current process technologies, layout rules for implementing wires dictate physical topologies where the network channels run horizontally or vertically. Thus, the problem is similar to Rectilinear Steiner Tree (RST) problem. Given a set of nodes, the RST problem is to ﬁnd a network with the shortest length using horizontal and vertical edges such that all nodes are interconnected. The RST problem is well-studied [18–20] with very fast implementations available [19, 20]. Figures 1(d) and 1(e) show possible RSTs for the set partitions {{1, 2, 3, 4}} and {{1, 2}, {3, 4}}, respectively. We use an RST solver in the inner loop of our heuristic algorithms to generate topologies for the set partitions considered. RSTs are only re-evaluated for the the portion of the set partition that changed at each step, and previously computed RSTs can be cached. After a physical topology is generated for each group in a set partition, the routes for the corresponding ﬂows and the bandwidth requirements for the corresponding network channels can be readily derived. The routes for the ﬂows follow directly from the tree structure of the RST solution. Correspondingly, the cumulative bandwidth requirements along the edges also follow. Routers are allocated at junctions where either ﬂows from multiple channels must be multiplexed to the same outgoing channel or ﬂows from the same channel must be de-multiplexed to multiple outgoing channels. We can also consider merging of ”nearby” routers if merging them will lead to a reduction in cost (this is further discussed in Section VI). Our RST formulation provides us a way to generate physical topologies. However, depending on the optimization goals and the implementation backend, the appropriate evaluation function may be quite a bit more sophisticated than the total length metric used in RST algorithms. To facilitate different objective functions and implementation backends, the evaluation function can be provided as an input to our algorithms. For example, in this paper, we investigate the NoC synthesis problem with the objective to minimize the power consumption, includ3B-4 ing both the leakage power and dynamic switching power, as well as satisfy the performance constraints. Other optimization goals may include minimizing hop-counts along with power minimization. IV. CLUSTER In this section, we present an algorithm called CLUSTER that reduces the number of set partitions considered from Θ(n log n)n to Θ(n3 ), which is a signiﬁcantly smaller subset of set partitions. The details of the algorithm is shown in Algorithm 1. The CLUSTER algorithm takes a communication demand graph and an evaluation function as input and generates an optimized network architecture implementation details as output. The algorithm starts by implementing each edge in the communication demand graph separately. The solution for each single edge is a simple RST connecting two terminals. It P 0 = {{e1}, {e2}, . . . , {en}}, as shown in lines 2-5. sets these single edges as the initial set partition, denoted as Then, in lines 7-18, at each iteration, the algorithm systematically generates new candidate set partitions starting from the set partition chosen from the previous iteration. In particular, tition P 0 = {{e1}, {e2}, . . . , {en}} with n groups, each group in the ﬁrst iteration, the algorithm starts with the initial set parcontaining exactly one edge. Then, in lines 8-13, the algorithm generates new candidate set partitions from P 0 by considering all pairwise mergings of groups in P 0 . The groups are denoted as gu and gv in the pseudo-code. For each pairwise merging considered, an RST solver is called to generate a physical network topology for the merged set of ﬂows, and the cost of this network is calculated using the speciﬁed evaluation function C . We do not need to solve an RST problem for the entire set of ﬂows, just the subset of ﬂows in the merged groups is considered. We then, in line 12, compute the total cost of the resulting set partition by summarizing the cost of implementing all the other sets using their own networks. In lines 15-16, we select the merging that achieves the best cost in this iteration and choose it as P 1 . In general, we start from the chosen set partition, P t , from the iteration to generate pairwise mergings of groups from P t , and the best merging is selected as the new chosen set partition P t+1 . At each iteration, the number of groups that need to be considered is reduced by 1, but the size of groups will become increasingly larger. Finally, in the last iteration, we only need to consider the mergings of two groups. At each iteration in lines 7-18, we maintain the chosen set partition and the associated cost calculations for that iteration. Then, in the end of the algorithm, lines 23-24, we choose the there can be at most (n − t)(n − t − 1)/2 possible pairwise set partition with the minimum cost. Since at each iteration t, group mergings, and there are (n − 1) iterations, the number of set partitions considered in the CLUSTER algorithm is Θ(n3 ). V. DECOMPOSE The DECOMPOSE algorithm described in this section reduces the number of set partitions considered from Θ(n log n)n to Θ(n2 ). The details of the algorithm is shown in Algorithm 2. This algorithm works in the opposite direction as CLUSTER when generating candidate set partitions and the corresponding RST topologies. It starts by considering all communication demands as a single cluster. In each iteration, the algorithm 279 3B-4 Algorithm 1 CLUSTER (G(V , E , π , λ), C, T ) v1’=e1 w(e1’)= 0.008 v2’=e2 v1’=e1 w(e1’)= 0.008 v2’=e2 6: t = 0 guv = gu ∪ gv P 0 = P 0 ∪ {ek } Input: G(V , E , π , λ): communication demand graph C : speciﬁed evaluation function for implementation cost Output: T : synthesized network architecture 1: initialize P 0 = ∅ 2: for all ek ∈ E do 3: cost({ek }) = EvaluateCost(T ({ek }), C ) 4: 5: end for 7: while |P t | > 1 do for all gu , gv ∈ P t do 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 19: for all t ∈ [0, n − 1] do 18: end while 20: 21: 22: end for (u, v) = arg mingu ,gv ∈P t β (gu , gv ) P t+1 = P t\{gu , gv } P t+1 = P t+1 ∪ {gu ∪ gv } t = t + 1 cost(guv ) = EvaluateCost(T (guv ), C ) (cid:2) β (gu , gv ) = cost(guv ) + gu∈P t cost(gu ) (cid:3) gu ∈P t T (gu ) c(P t ) = soln[P t ] = T (guv ) = SolveRST(guv ) (cid:2) end for gi∈P t ,gi (cid:3)=gu ,gv cost(gi ) 23: t = arg mint∈[0,n−1] c(P t ) 24: T = soln[P t ] 25: return T (cid:2) (cid:2) (cid:2) (cid:2) i , v (cid:2) = (v (cid:2) ) = cost({ei , ej }) + ek ∈E ,ek (cid:4)=ei ,ej cost({ek }), considers different ways of breaking up an existing group in the set partition chosen from the previous iteration into two smaller ones. Then, the differential cost of splitting a group is evaluated by generating an RST for each sub-group and evaluating their costs using the speciﬁed evaluation function. To facilitate this decomposition process, two important graphs are used in DECOMPOSE: Afﬁnity Graph (AG) and its Minimum Spanning Tree (MST). The afﬁnity graph A is built by associating each ﬂow in the communication demand graph to a vertex in the afﬁnity graph. An edge is added between each pair of the vertices in the afﬁnity graph to form a complete graph. A weight is attached to each edge e j ) and is calculated as w(e where ei is the ﬂow in the communication demand graph asi in the afﬁnity graph. cost({ek }) is calculated sociated with v by calling on the evaluation function to evaluate the cost of implementing {ek } separately and cost({ei , ej }) is calculated by evaluating the cost of implementing a generated RST topology for {ei , ej } together. The weights of the edges in the afﬁnity graph reﬂect the beneﬁts of implementing ﬂows represented by vertices in the afﬁnity graph together using shared resources. by only negative weighted edges so that the total implementation cost is minimized. The smaller the weight, the less the resulting total cost of clustering the two ﬂows connected by that edge. The motivation is to only cluster ﬂows that are connected by small weighted edges so that the total implementation cost is minimized. Then the minimum spanning tree M of A that contains the minimum number of minimal weighted edges connecting all the vertices in A is derived. The afﬁnity graph and its MST of the example in Figure 1 are shown in Figure 2. The cost considered is the total power consumption based on the 70nm technology power estimations shown in Table I. Recall that the vertices in the spanning tree M corresponds w(e2’) = 0.008 w(e3’) = 0.008 w(e5’) = 0.008 w(e4’) = 0.008 w(e2’) = 0.008 w(e3’) = 0.008 v4’=e4 w(e6’)= 0.013 v3’=e3 v4’=e4 v3’=e3 (a) Afﬁnity graph. (b) MST of afﬁnity graph. Fig. 2. Afﬁnity graph and MST of AG for the NOC synthesis example to ﬂows in the communication demand graph G. Since M is initially a spanning tree, it interconnects all vertices, which is interpreted as having all ﬂows in a single cluster. During the course of the DECOMPOSE algorithm, we will selectively remove edges from M to create disjoint set of vertices, which will correspond to disjoint sets of ﬂows into groups, thus forming a particular set partition. In each iteration shown in lines 5-9, the algorithm systematically generates new candidate set partitions starting from the set partition chosen from the previous iteration. Inside the while loop, new set partitions are generated by temporarily removing one edge from M . This is achieved by calling the routine SelectEdgetoDelete(M ). With an edge removed, the corresponding group is split into two sub-groups. We evaluate the cost of this splitting by solving an RST problem for each subgroup and calling on the evaluation function to compute the new costs. In the ﬁrst iteration of the algorithm, the spanning tree M has (n − 1) edges. Thus, (n − 1) new candidate set partitions will be generated. The set partition with the best cost will be chosen as the set partition for the current iteration. This set partition, and the corresponding modiﬁed M , will be used will have (n − t − 1) remaining edges. Therefore, (n − t − 1) as the starting point for the next iteration. At iteration t, M candidate set partitions will be generated and considered. The algorithm ends when all ﬂows in the problem have been split into their own individual groups. Then, at the end of the algorithm, at line 10, we choose the set partition with the minimum at each iteration t, there can be at most (n − t − 1) candidate set cost among the set partitions chosen from all iterations. Since partitions, the number of set partitions considered in the DECOMPOSE algorithm is Θ(n2 ), which is again considerably smaller than Θ(n log n)n . V I . ROUT ER M ERG ING After the physical network topology has been generated for each set partition of ﬂows, a router merging step is used to further optimize the topology and reduce the cost. Since for each set partition, routers are allocated at Steiner points or terminal points of the RST generated, routers that connect with each other can be merged to eliminate router ports and thus possibly the corresponding costs. Routers that connect to the same ports can also be merged to reduce ports and costs. We propose a greedy router merging algorithm, which works iteratively by considering all possible mergings of two routers connected with each other in each iteration. For each candidate merging, the cost difference of the resulting topology after merging and the one before merging is calculated. Then they are sorted in the increasing order of the cost difference. In the merging step, for each candidate merging from the sorted list, routers are merged if they have not merged yet and the cost is 280 Algorithm 2 DECOMPOSE (G(V , E , π , λ), C, T ) 3: t = 0 Input: G(V , E , π , λ): communication demand graph C : speciﬁed evaluation function for implementation cost Output: T : synthesized network architecture 1: A = GenerateAfﬁnityGraph(G) 2: M = GenerateMinSpanningTree(A) 4: n = |E | 5: while |M | < n do 6: (e, soln[t], cost[t]) = SelectEdgeToDelete(M ) 7: remove e from M 8: 9: end while t = t + 1 10: T = soln[arg mint cost[t]] 11: return T SelectEdgeToDelete(M ) 1: for all ei ∈ M do 2: temporarily remove ei from M 3: components(M ) = CalculateConnectedComponents(M ) 4: 5: 6: 7: 8: end for 9: add ei back to M 10: end for for all gi ∈ components(M ) do soln[ei ] = soln[ei ] ∪ T (gi ) cost[ei ] = cost[ei ] + EvaluateCost(T (gi ), G, C ) T (gi ) = SolveRST(gi ) 11: e = arg minei∈M cost[ei ] 12: return (e, soln[e], cost[e]) improving. After all routers are considered in the current iteration, they are updated by replacing the routers merged with the new one generated. Those routers are reconsidered in the next iteration. The algorithm keeps merging routers until no improvement can be made further. V I I . RE SULT S A.ExperimentalSetup We have implemented our two proposed algorithms CLUSTER and DECOMPOSE in C++, using a fast public domain Rectilinear Steiner Tree solver called GeoSteiner4.0 [19, 20] to generate the physical network topologies in the inner loop. The proposed router merging algorithm has been integrated into the two algorithms as well. Three sets of benchmarks were used to evaluate these algorithms. The ﬁrst set of benchmarks are four different video processing applications obtained from [9], including a Video Object Plane Decoder (VOPD), an MPEG4 decoder, a PictureIn-Picture (PIP) application, and a Multi-Window Display (MWD) application. The next set of benchmarks were obtained from [3] and [7]. They correspond to different encoder/decoder combinations of a H.263 video codec, a MP3 audio codec, and a generic MultiMedia System (MMS). Finally, to generate larger benchmark instances, we generated synthetic benchmarks from the above video applications. All experimental results were obtained on a 1.5 GHz Intel P4 processor machine with 512 MB of memory running Linux. B.MethodofEvaluation In our experiments, we aim to evaluate the performance of the two proposed algorithms CLUSTER and DECOMPOSE on all benchmarks with the objective of minimizing the total power consumption of the synthesized NoC architectures. The total power consumption includes the dynamic switching 3B-4 P OW E R CON S UM P T I ON O F N OC COM P ON EN T S [ 1 1 , 1 4 ] TABLE I (a) Power consumption of routers Ports (in x out) Leakage power (W) Switching bit energy (pJ/bit) 2x2 0.0069 0.3225 3x2 0.0099 0.0676 3x3 0.0133 0.5663 4x3 0.0172 0.1080 4x4 0.0216 0.8651 5x4 0.0260 0.9180 5x5 0.0319 1.2189 (b) Power consumption of links Wire length (mm) Leakage power (W) Switching bit energy (pJ/bit) 1 0.000496 0.6 4 0.001984 2.4 8 0.003968 4.8 12 0.005952 7.2 16 0.007936 9.6 power which is a function of data rate passing through each component and the leakage power which is related to all the components in the NoC architecture. To estimate power consumption different router conﬁgurations considered during the synthesis process, we used a state-of-the-art power simulator called Orion [11, 12] that considers both leakage and dynamic power. We set the operational frequency to be 1 GHz, the buffer size to be 4 ﬂits, and the size of each ﬂit to be 128 bits. The leakage power and switching bit energy of routers with different example port conﬁgurations in 70nm technology are showed in Table I. For the link power parameters, we used RC wires with repeated buffers and a minimum global wire pitch. The static power and switching bit energy parameters in 70nm technology were obtained using the models from [14] and are listed in Table I. For evaluation, fair direct comparison with previously published NoC synthesis results is difﬁcult in part because of vast differences in the power parameters assumed1 . Therefore, to evaluate the performance of our proposed algorithms, we have designed two sets of experiments. In the ﬁrst set of experiments, we generated mesh topologies for the benchmarks by modifying the design procedure to synthesize NoCs based on mesh structure. To obtain mesh topologies, we generated a design with each core connecting to a single router and restricted the router sizes to have 5 input/output ports. We also generated a variant of the basic mesh topology, optimized mesh (optmesh), by removing those ports and links that are not used by the trafﬁc ﬂows. These experiments are designed to show the beneﬁts of application-speciﬁc NoC architectures. In the second set of experiments, we implemented an exact algorithm, referred to as EXACT, that exhaustively enumerates all distinct set partitions. These experiments are designed to show how close our heuristic algorithms are to exact results. C.ComparisonofResults The synthesis results of CLUSTER and DECOMPOSE on all benchmarks at 70nm with comparison to mesh and optmesh topologies are shown in Table II. The power results show that both algorithms can achieve substantial reduction in power consumption over the standard mesh and opt-mesh topologies in all cases. In particular, a 6.92× and a 6.77× reduction on average in power consumption over standard mesh topologies can be achieved by CLUSTER and DECOMPOSE respectively, with a 2.68× and a 2.60× reduction over the optimized mesh topologies for each of them. The execution times reported 1We use the Orion simulator to evaluate the NoC power consumption [11, 12]. The power estimates from Orion are consistent with another published power-optimized NoC implementation described in [13]. The power estimates are on the same order of magnitude for the same router conﬁguration in the same technology. 281 3B-4 N OC S YN TH E S I S R E S U LT S COM PA R I S ON W I TH M E S H TO P O LOG I E S TABLE II CLUSTER DECOMPOSE mesh opt-mesh Appli. VOPD MPEG4 PIP MWD H263 G5 mp3enc mp3dec h263dec MMEnc MMDec MMS V+M M+P V+M+M 4in1 |V| 12 12 8 12 7 12 7 6 7 14 11 25 24 20 36 44 |E| 14 13 8 13 10 12 8 6 8 19 14 33 27 21 40 48 Power (W) 0.042 0.039 0.021 0.028 0.037 0.028 0.023 0.016 0.026 0.068 0.054 0.123 0.086 0.053 0.120 0.139 Time (sec) 1.35 1.09 0.24 1.57 0.37 0.82 0.09 0.11 0.19 3.89 1.88 32.07 15.47 7.45 73.02 130.50 Impro.  CL/ mesh Impro.  CL/ opt 6.42 7.07 8.65 9.24 4.31 9.11 4.28 6.02 6.77 5.50 4.74 5.21 7.39 9.29 8.37 10.25 3.60 2.35 2.93 4.31 2.24 3.76 2.16 2.60 2.47 1.91 2.24 2.11 2.35 2.89 2.49 2.47 Power (W) 0.042 0.040 0.021 0.031 0.036 0.031 0.023 0.016 0.026 0.073 0.054 0.132 0.090 0.053 0.124 0.142 Time (sec) 0.34 0.30 0.09 0.32 0.14 0.23 0.05 0.04 0.07 0.77 0.32 3.96 1.30 0.96 7.54 12.68 Impro. DE/ mesh Impro. DE/ opt 6.42 6.84 8.65 8.44 4.18 8.29 4.28 6.02 6.77 5.12 4.74 4.85 7.04 9.29 8.13 10.02 3.60 2.27 2.93 3.94 2.17 3.42 2.16 2.60 2.47 1.78 2.24 1.96 2.24 2.89 2.42 2.42 Power (W) 0.272 0.276 0.178 0.260 0.155 0.258 0.100 0.099 0.178 0.376 0.257 0.642 0.633 0.495 1.008 1.425 Power (W) 0.152 0.092 0.060 0.121 0.080 0.106 0.050 0.043 0.065 0.131 0.122 0.260 0.202 0.154 0.300 0.344 N OC S YN TH E S I S R E S U LT S COM PA R I S ON W I TH EXAC T S O LU T I ON S TABLE III CLUSTER DECOMPOSE EXACT Appli. VOPD MPEG4 MWD MMEnc MMDec MMS V+M M+P V+M+M 4in1 mp3enc mp3dec h263dec PIP H263 G5 Power (W) 0.042 0.039 0.028 0.068 0.054 0.123 0.086 0.053 0.120 0.139 0.023 0.016 0.026 0.021 0.037 0.028 Time (sec) 1.35 1.09 1.57 3.89 1.88 32.07 15.47 7.45 73.02 130.50 0.09 0.11 0.19 0.24 0.37 0.82 Improv. CL/EX Power (W) Time (sec) Improv. DE/EX Power (W) n.a. n.a. n.a. n.a. n.a. n.a. n.a. n.a. n.a. n.a. 1.00 1.00 1.00 1.00 1.03 1.00 0.042 0.040 0.031 0.073 0.054 0.132 0.090 0.053 0.124 0.142 0.023 0.016 0.026 0.021 0.036 0.031 0.34 0.30 0.32 0.77 0.32 3.96 1.30 0.96 7.54 12.68 0.05 0.04 0.07 0.09 0.14 0.23 n.a. n.a. n.a. n.a. n.a. n.a. n.a. n.a. n.a. n.a. 1.00 1.00 1.00 1.00 1.00 1.10 t.o. t.o. t.o. t.o. t.o. t.o. t.o. t.o. t.o. t.o. 0.023 0.016 0.026 0.021 0.036 0.028 Time (sec) t.o. t.o. t.o. t.o. t.o. t.o. t.o. t.o. t.o. t.o. 1.00 1.00 7.00 9.00 293.00 14440.00 show that both algorithms work very fast, in just seconds. As can be seen, CLUSTER can achieve better results than DECOMPOSE because it examines more set partition candidates in its solution space, but it requires longer run times. In the next set of experiments, we compared our heuristic algorithms with an exact algorithm that enumerates all distinct set partitions. As the number of distinct set partitions grows Θ(n log n)n , the CPU times for generating the exact solutions increase very quickly. We set a CPU timeout period of 5 hours. The results are compared in Table III. Out of the 16 benchmarks tested, we were able to determine the exact solutions for 6 of the benchmarks. Of these 6 benchmarks, both algorithms were able to achieve the exact solution in 5 out of the 6 cases, and on average, the results are within just 1% and 2% of the exact results for CLUSTER and DECOMPOSE respectively. Moreover, the CPU times for the 6 benchmarks are all under 1 second whereas the EXACT algorithm took as much as 4.5 hours to achieve similar results. V I I I . CONCLU S ION S In this paper, we proposed a formulation of the applicationspeciﬁc NoC synthesis problem based on the decomposition of the problem into the inter-related steps of ﬁnding a good set partition of communication ﬂows, deriving a good physical network topology for each group in the partition, and calculating the cost of the set partitions and topologies by means of an evaluation function. We proposed two heuristic algorithms called CLUSTER and DECOMPOSE for systematically examining different possible set partitions, and we proposed the use of Rectilinear Steiner Tree algorithms for deriving good physical network topologies. Although we use Steiner trees to generate a physical network topology for each group in the set partition, the ﬁnal NoC architecture synthesized is not necessarily limited to just trees as Steiner tree implementations of different groups may be connected to each other to form non-tree structures. Experimental results on a variety of benchmarks using a power consumption cost model show that our algorithms can produce effective solutions with fast execution times comparing to both mesh implementation and EXACT solutions. "
2009,"Adaptive inter-router links for low-power, area-efficient and reliable Network-on-Chip (NoC) architectures.","The increasing wire delay constraints in deep sub-micron VLSI designs have led to the emergence of scalable and modular network-on-chip (NoC) architectures. As the power consumption, area overhead and performance of the entire NoC is influenced by the router buffers, research efforts have targeted optimized router buffer design. In this paper, we propose iDEAL - inter-router, dual-function energy and area-efficient links capable of data transmission as well as data storage when required. iDEAL enables a reduction in the router buffer size by controlling the repeaters along the links to adaptively function as link buffers during congestion, thereby achieving nearly 30% savings in overall network power and 35% reduction in area with only a marginal 1 - 3% drop in performance. In addition, aggressive speculative flow control further improves the performance of iDEAL. Moreover, the significant reduction in power consumption and area provides sufficient headroom for monitoring negative bias temperature instability (NBTI) effects in order to improve circuit reliability at reduced feature sizes.","Adaptive Inter-router Links for Low-Power, Area-Efﬁcient and Reliable Network-on-Chip (NoC) Architectures 1A-1 †, Ashwini Sarathy ‡ , Ahmed Louri ‡ , Janet Wang ‡ Avinash Karanth Kodi †Dept. of EEC S, Ohio U niversity , Athens, OH 45701, U SA ‡Dept. of EC E , U niversity of Arizona, T ucson, AZ 85719, U SA E-mail: kodi@ohio.edu, {sarathya,louri,wml}@ece.arizona.edu Abstract— The increasing wire delay constraints in deep submicron VLSI designs have led to the emergence of scalable and modular Network-on-Chip (NoC) architectures. As the power consumption, area overhead and performance of the entire NoC is inﬂuenced by the router buffers, research efforts have targeted optimized router buffer design. In this paper, we propose iDEAL - inter-router, dual-function energy and area-efﬁcient links capable of data transmission as well as data storage when required. iDEAL enables a reduction in the router buffer size by controlling the repeaters along the links to adaptively function as link buffers during congestion, thereby achieving nearly 30% savings in overall network power and 35% reduction in area with only a marginal 1 − 3% drop in performance. In addition, aggressive speculative ﬂow control further improves the performance of iDEAL. Moreover, the signiﬁcant reduction in power consumption and area provides sufﬁcient headroom for monitoring Negative Bias Temperature Instability (NBTI) effects in order to improve circuit reliability at reduced feature sizes. I . INT RODUCT ION Technology scaling into the deep sub-micron regime has led to the development of multi-core architectures with increased transistor density on a single chip. However, the increasing global wire delays [1] limit the potential improvement in performance, leading to the modular and scalable packet-switched NoC paradigm [2, 3, 4]. As concluded in the recent NSF sponsored workshop on on-chip interconnects [5], one of the major research challenges currently faced by NoC designers is that of power dissipation with the router buffers consuming about 46% of the router power. The increased power density and temperature accelerate device degradation, and hence reduce the lifespan of the circuit. On the other hand, simply reducing the number of router buffers to reduce the power and area overhead degrades the network performance as the performance and ﬂow control of the network are primarily characterized by the router buffers [6]. Current high speed VLSI designs require repeater insertion along the wires to overcome the quadratic increase in delay with the wire length [1]. It has been shown in [7] that the repeaters can be designed to sample and hold a data bit, in addition to their conventional functionality. Therefore, with repeaters as potential buffer elements, we can employ them to adaptively function as buffers along the links under high network loads, when there are no more buffers in the router. Data storage along the links has been explored by [8, 9] to achieve a latency-insensitive design and by [10] in order to resolve timing errors in overclocked NoCs. In this paper, we propose iDEAL - inter-router dualfunction, energy and area-efﬁcient links for NoCs by employing circuit and architectural techniques at the inter-router links and the router buffers respectively. At the links, we deploy circuit level enhancements to the existing repeaters so that they adaptively function as buffers during congestion. As the correct operation of the adaptive link buffers depends on the errorfree detection of the congestion signal, we employ a doublesampling technique within the control block to overcome arbitrary timing errors along the congestion signal. The doublesampling technique [11] has been employed in correcting timing errors due to voltage scaling [12] and increased operating frequency [10]. In our proposed architecture we doublesample the congestion signal to overcome potential timing errors due to increased coupling noise and process variations at the deep sub-micron technologies. Compared to a conventional repeater-inserted control line, this control technique operates reliably and consumes signiﬁcantly less power as it can be disabled in the absence of congestion. At the router buffer, we deploy architectural techniques such as static and dynamic buffer allocation [13, 14] to prevent performance degradation, while sustaining or improving the performance of a generic router. While static buffer management leads to insufﬁcient or unused buffers [13], dynamic buffer management allocates the incoming ﬂit (the basic ﬂow control unit, a packet consists of several ﬂits) to any free buffer, leading to higher throughput, although at the cost of higher control management. In [13] the number of virtual channels (VCs) and the depth of buffers/VC are dynamically adjusted based on the trafﬁc load, complicating the control logic and increasing the packet latency due to higher packet interleaving [6]. Therefore, in our proposed work, we adopt a dynamic VC table based approach with ﬁxed number of VCs, thereby achieving the ﬂexibility of storing ﬂits dynamically without excess control overhead. Moreover, the congestion control from the downstream to the upstream router enables aggressive ﬂit transmission without waiting for credits to return, thereby further improving the throughput. 978-1-4244-2749-9/09/$25.00 ©2009 IEEE 1 1A-1 The combination of circuit and architectural techniques in iDEAL using the dual-function links and the dynamic router buffer allocation, allows us the ﬂexibility of reducing the router buffer size to decrease the power and area overhead without signiﬁcant performance degradation. In addition, the reduction in power consumption and area provides sufﬁcient design headroom for improving the reliability of the circuits. Reliability, especially the Negative Bias Temperature Instability (NBTI) effects has emerged as a major threat to the current CMOS technology. Prior research has focused on modeling NBTI at the device level and gate level [15, 16, 17]. [18] has reviewed current NBTI research and provides guidelines towards reliability in general purpose logic and memory circuits, by proposing guard-banding and adaptive body biasing techniques. [19] is one of ﬁrst papers to propose an NBTI-aware processor design. Our proposed architecture with the iDEAL methodology offers a possible NBTI-aware solution for NoCs. The additional circuits required to monitor NBTI effects increase the overall power consumption and area. However, our proposed low-power, area-efﬁcient architecture provides design headroom for the Thermal Design Power (TDP) of the auxiliary circuits and thereby improves the circuit reliability without additional overhead. Unlike other NoC designs where performance is improved at the cost of major changes to the router design, the changes in the proposed architecture pertain only to the input buffer and the allocation of the input buffer space to the incoming ﬂits. Synthesized designs in the 90 nm technology at 500 M hz and 1.0 V , show a reduction of 30% in the overall network power and a reduction of 35% in area, when half the router buffers are removed. Moreover, the proposed architecture enables NBTI monitoring without additional overhead and improves device Cycle accurate network simulation on 8 × 8 mesh and folded lifetime by providing both power and area design headroom. torus network topologies shows only a marginal 1-3% loss in throughput. In addition, the throughput can be further improved by 10% with aggressive transmission of ﬂits without credit turn-around. I I . DE S IGN O F INT ER -ROUT ER DUAL - FUNCT ION L INK S Repeater Stage-2 Repeater Stage-1 Output Port of Router A Bit 1 Bit n Input Port of Router B Control block Control block Congestion Main Flip-Flop MUX Congestion Clock XOR Shadow Flip-Flop Three-state repeater Error Delay Buffer Fig. 1. Proposed inter-router dual-function links with the control blocks. B.ControlBlock Implementation The control block shown in Figure 1 enables the three-state repeaters to adaptively function as link buffers during congestion. A single control block is sufﬁcient to control the functionality of all the repeaters in one stage. The incoming congestion signal is delayed by one clock cycle at each control block. In the next clock cycle, the repeaters in that stage are tri-stated and the congestion signal travels to the next control block. Hence each repeater stage is successively tri-stated to hold the data in position, until the congestion-release signal arrives. The control block in Figure 1 is more efﬁcient than a conventional repeater-inserted control line [7], as it provides the following advantages: (1) Unlike conventional repeaters, the control circuit operates accurately at variable clock speeds. The congestion signal is double-sampled and the shadow ﬂipﬂop makes the circuit tolerant to timing errors. (2) The control circuit can be disabled when there is no congestion, thus reducing the power consumption along the congestion control line. A.ProposedLink Implementation I I I . DE S IGN O F ROUT ER BU FF ERS Figure 1 shows the proposed repeater-inserted interconnect, with the conventional repeaters replaced by three-state repeaters. When the control input to a repeater stage is low, the three-state repeaters in that stage function like the conventional repeaters transmitting data. When the control input to the repeater stage is high, the repeaters in that stage are tri-stated and hold the data bit in position. Once congestion is alleviated, the control logic is disabled and the three-state repeaters return to the conventional mode of operation. The adaptive dual-function links hence enable a decrease in the number of buffers within the router and save appreciable power and area. In packet-switched NoCs, every processing element (PE) is connected to an NoC router as shown in Figure 2(a), with most NoCs commonly adopting network topologies such as mesh, or folded torus for regularity and modularity [6, 20, 21, 22]. In wormhole switching, each packet that arrives on the input port progresses through router pipeline stages (routing computation(RC), virtual channel allocation (VA), switch allocation (SA), switch traversal (ST)) before it is delivered to the appropriate output port [6]. At each intermediate router, only the header ﬂit of every packet is responsible for the ﬁrst two pipeline stages of RC and VA. 2 Crossbar Switch Processing Element (PE) + x - x + y - y  Route Computation (RC)  Virtual Channel  (VC)  Switch Allocator  (SA) (a) Flit 1 Flit r VC State  Table Flit 1 Flit r D E M U X M X U vc 1 vc v VCID VC CR OVC OP WP RP Status v Congestion Control C* Credit Return (b) VC State Table Input Port P Input Buffers Fig. 2. (a) A generic 5 × 5 NoC router architecture (b) The proposed static buffer allocation with congestion control. A.StaticallyAllocatedRouterBuffers The proposed statically allocated router buffer design with congestion control is shown in Figure 2(b). For a router architecture with P ports, v VCs/port and r ﬂit buffers/VC the total number of buffers/port is z = vr. Each input VC is associated with a VC state table [6, 13]. It maintains the state for each incoming packet and ensures that the body ﬂits are routed to the correct output port. The VC identiﬁer (VCID) of the incoming ﬂit allows the DEMUX to switch to the correct input VC. The Read Pointer (RP) and the Write Pointer (WP) are used to read the ﬂit into the buffer and write the ﬂit out to the crossbar. The Output Port (OP) is provided by the RC stage, Output VC (OVC) is provided by the VA stage. Credits (CR) indicate the total storage available at the downstream router. Given that each VC has r credits, a credit is consumed when a ﬂit is transmitted to the downstream router. A credit is returned to the upstream router when a ﬂit is read out of the buffer. The Status ﬁeld indicates the current status of the VC idle, waiting, RC, VA, SA, ST, and others. In the generic NoC design, the total number of input buffers is vr per input port. With the additional c buffers in the link, the total storage now becomes vr + c. The number of credits available at each VC is (vr + c)/z . This allows routers to send additional ﬂits into the network, even if the storage is in the link, instead of the router buffer. Every VC state table maintains another ﬁeld C ∗ which indicates congestion. When C ∗ is set, the congestion control is activated which in turn holds the data in the network link itself. When a ﬂit is read from the buffer, the C ∗ ﬁeld is cleared, which in turn allows data ﬂits to enter into the router. This nominal change does not impact the design of the network router and leads to signiﬁcant power savings and area gain. However, at high network load, this design leads to headof-line (HoL) blocking in the link buffers. When the congestion ﬁeld C ∗ is set for a particular VC, the corresponding ﬂits Buffer Slot Free 1 2 z Y N N Input Port P Flit 1 Flit r Flit (v-1)r + 1 Flit z D E M U X M X U Flit r+1 Flit 2r Write Read Pointer Pointer Unified VC State Table Credit Return Input Flit Tracking Output Flit Tracking Buffer Slot  Availability Congestion Control VC 0 1 … v CR OVC OP WP F0 F1 F(z+c)/v RP N N … 3 N 5 … 6 … … … N N N 5 N … N N … 6 3 Status … … … Fig. 3. Proposed dynamic buffer allocation with congestion control. are held in the network link. These ﬂits block the ﬂits headed towards other VCs, although the other VCs may have their C ∗ ﬁeld cleared. A more attractive alternative is dynamically allocated router buffers as explained in the next section. B.DynamicallyAllocatedRouterBuffers As ViChaR’s [13] table based approach had solved issues pertaining to latency and scalability, we have adopted a similar idea but limited the number of VCs to prevent excess control overhead. We adopt the uniﬁed buffer architecture and augment the architecture with a ‘Uniﬁed VC State Table’ (UVST). The maximum size of the UVST is O(v) as compared to the ViChaR which is O(vr). For an incoming ﬂit, the ‘Buffer Slot Availability’ (BSA) tracking system keeps track of all buffer slots and allocates the ﬁrst buffer slot found to be free. Similarly, for a departing ﬂit, BSA deallocates the buffer slot and adds it to the list of free slots maintained in the table, as shown in the inset of Figure 2(b). The table contains buffer slots F0 , F1 , ... F(z+c)/v in addition to the regular ﬁelds of RP, WP, OP, OVC, CR and Status ﬁelds. For fairness purposes, the number of credits is equally divided between all the VCs as (z + c)/v per VC. When BSA ﬁnds only a single non-null pointer in its base table, it triggers the congestion signal and when a free buffer slot is created by a departing ﬂit, the BSA releases the congestion signal. In iDEAL, the link buffers can be viewed as serial FIFO buffers as opposed to the parallel FIFO buffers used within the routers. Therefore, eliminating the HoL blocking is critical in iDEAL. Dynamic allocation of buffer slots using the table based approach signiﬁcantly reduces the HoL blocking. 1A-1 3 1A-1 IV. PER FORMANCE EVALUAT ION O F THE PRO PO S ED ARCH IT ECTURE In this section, we evaluate the proposed dual-function links and router buffers in terms of power dissipation and area overvnV − rnR − cnC , where nV is the number of VCs per input head. The notation followed for the different cases is of port, nR is the number of router buffers per VC and nC is the number of link buffers. For example, the baseline is denoted as v4 − r4 − c0, implying 4 VCs per input port, 4 router buffers per VC and 0 link buffers. In each case, the design is implemented in Verilog and synthesized using the Synopsys Design Compiler tool and the TSMC 90 nm technology library, at an operating frequency of 500 M H z and a supply voltage of 1 V . A.PowerandAreaEstimation for theLinks The links are assumed to be 2 mm long and the baseline design has 8 optimally spaced conventional repeaters along each wire of the 128-bit wide links. For the baseline design,the total power consumed by the link per ﬂit traversal is 2.45 mW . When all the conventional repeaters are replaced by the threestate repeaters, the total power consumed by the link for each ﬂit traversal is found to be 3.94 mW . In the presence of congestion, the control block consumes a power of only about 6 μW . The area consumed by the repeater stages is found to be 8,960 μm2 in case of the baseline and 10,240 μm2 when all the conventional repeaters along the link are replaced by the three-state repeaters. B.PowerandAreaEstimationfor theRouter The router buffers are implemented as FIFO registers with the associated control logic. Considering both the write and read operations in the buffer, the total power consumed for a 128-bit ﬂit in the buffer is estimated to be 19.54 mW , for the baseline design with 16 buffers. Decreasing the buffer size by 50% reduces the power per ﬂit traversal to 11.57 mW , resulting in a 40.77% savings in buffer power alone. For the conﬁgurations with 4 VCs per input port, the arbiter in the router consumes a power of 0.15 mW , for a single arbitration task and the switch in the router consumes 0.31 mW per ﬂit traversal. When the number of VCs is decreased to 3 per input port, the power consumed by the arbiter reduces to 0.09 mW per arbitration and the power consumed by the switch decreases to 0.27 mW per ﬂit traversal. The buffer area in case of the baseline is 81,407 μm2 . A 50% decrease in the buffer size reduces the area to 48,066 μm2 , leading to a 35% savings in the overall area including the links and the router buffers. C.DesignHeadroomforNBTI-relatedReliability Issues NBTI affects PMOS transistors whose gate input is at logic ‘0’, introducing the need for a higher threshold voltage and constraining the transistor speed. Among all the circuit parameters affecting the inﬂuence of NBTI, temperature plays a decisive role. Higher temperatures accelerate the NBTI-related degradation [15, 19]. In order to effectively manage the inﬂuence of NBTI, designers monitor the temperature in cores and functional blocks. Additional circuitry is included to change the input of the PMOS gates to logic ‘1’ while the PMOS gates are in the ‘idle’ state [19]. This technique utilizes the selfhealing ability of the PMOS [19], as the PMOS is not stressed when it is turned ‘OFF’ by the logic ‘1’ input. However, the additional circuits lead to an increase in power consumption and area overhead. iDEAL achieves a signiﬁcant reduction in power density and temperature of the router by ‘moving some of the buffers from the router to the link’. As is well known, each router in the NoC is closely placed to the core or the PE in the multicore architecture. Therefore reducing the router power and area has a signiﬁcant impact on the temperature of the PEs. Moreover, iDEAL provides additional design headroom for NBTI-related reliability issues. In order to quantify the additional design headroom achieved by the iDEAL methodology we consider the Thermal Design Power (TDP), which is the maximum power that is required to be dissipated by the cooling system [19]. In the current context, it is the additional power that is consumed by the auxiliary circuits enabling the self-healing of the PMOS gates. The TDP for an NoC with N cores, T DP N oC is given by T DP N oC = N(cid:2) i=1 T DP i (1) The complexity of the auxiliary circuits in turn depends on the functional block under consideration [19]. For a powerefﬁcient architecture, it is essential to minimize the overhead due to the auxiliary circuits. The signiﬁcant reduction in power consumption achieved by iDEAL provides the additional headroom to overcome this constraint, as shown by Equation (2). T DP N oC ≤ Psaved (2) where Psaved is the reduction in power consumption achieved by iDEAL. As the area linearly impacts TDP [19], it is required to limit the area in order to decrease TDP. iDEAL achieves an areaefﬁcient architecture, thereby reducing the impact on the TDP. The additional headroom provided by the area-efﬁcient iDEAL methodology is given by T DP N oC = K × Sr (3) where Sr is the area reduction achieved by iDEAL and K is the factor determining the impact of the area on the TDP. Therefore, the iDEAL methodology offers a good opportunity to add auxiliary circuits such as inverting input circuits for memory-like structures, mitigating mechanisms in latches and self-healing circuits for the combinational logic in the design [19]. 4 1A-1 V. S IMULAT ION RE SULT S AND D I SCU S S ION V I . CONCLU S ION A cycle-accurate on-chip network simulator was used to conduct a detailed evaluation of the proposed architecture in 8 × 8 mesh and folded torus networks under several synthetic trafﬁc patterns (Uniform Random(UN), Bit-Reversal(BR), Butterﬂy(BU), Matrix Transpose(MT), Complement(CO), Perfect Shufﬂe(PS), Tornado(TO), Neighbor(NE)) as well as the SPLASH-2 suite benchmarks (FFT with input data set 64K points; LU with 256 × 256, 16 × 16 block; MP3D with 48000 molecules; RADIX with 1M integers, 1024 radix and WATER with 512 molecules). For simplicity, the test conﬁgurations are referred to as nV − nR − nC in the following discussion. Total Network Power and Throughput: Figures 4(a) and 4(b) show the total power consumed for a network load of 0.5, in the mesh and folded torus networks respectively. The power dissipated in the control blocks is negligible and is not visible at the scale considered. By reducing the router buffer size, all the conﬁgurations achieve a reduction in power compared to the baseline, with the 4-2-8 case showing about 30% savings in the total network power. From Figures 4(c) and 4(d), the saturation throughput shows almost similar performance for 44-0, 3-4-4 and 4-3-4. The 4-2-8 shows only about 3% drop in performance. This result is signiﬁcant as we can save about 35% of the area and yet achieve similar performance as the baseline by dynamically allocating the router buffers and using the additional link buffers at high network loads. Router Buffer Power and Throughput for All Trafﬁc Patterns: Figure 4(e) shows the power consumed at the router buffers and Figure 4(f) shows the throughput achieved at a network load of 0.5 for the 8 × 8 mesh, under all the synthetic trafﬁc patterns considered, for the 4-4-0, 4-3-4 and 4-2-8 conﬁgurations. Power savings is obtained for both the 4-3-4 and the 4-2-8 cases under all the trafﬁc patterns. From Figure 4(f), there is no signiﬁcant decrease in throughput under any of the trafﬁc patterns considered. Throughput and Power for SPLASH-2 suite benchmarks: Figures 4(g) and 4(h) show the normalized execution time and normalized total power consumed for the selected SPLASH2 suite benchmarks for 4-4-0, 4-3-4 and 4-2-8 conﬁgurations. From Figure 4(g), the 4-3-4 and 4-2-8 conﬁgurations do not show signiﬁcant drop in performance, in fact the drop is less than 1%. From Figure 4(h), the power savings from the 4-3-4 and 4-2-8 conﬁgurations are 20% and 30% respectively. Throughput using Aggressive Speculation: Figure 4(i) tion technique, for the 8 × 8 folded torus network under unishows the saturation throughput using an aggressive speculaform trafﬁc. The number of credits available to the upstream router is speculatively increased to 8 as the congestion control circuit enables an aggressive ﬂit transmission without waiting for the credits from the downstream router. This technique improves the performance of iDEAL by about 10% (as seen in the 4-2-8 case), without additional power or area overhead. As recent research has shown, the major issue in NoC design is the increasing power consumption. iDEAL proposes to reduce the number of router buffers, thereby achieving a significant savings in power and area. As this impacts performance, we provide adaptive dual-function links for data storage when required. Simulation results show that by reducing the router buffer size in half, iDEAL achieves nearly 40% reduction in buffer power alone, more than 30% savings in the overall network power and 35% savings in the total area. In addition, the dynamically assigned buffers with aggressive speculative ﬂow control show up to 10% improvement in performance. The savings in power consumption and area provides signiﬁcant design headroom to overcome circuit reliability issues due to NBTI effects. This paper shows that eliminating some of the buffers in the router and using adaptive link buffers saves an appreciable amount of power and area, without signiﬁcant degradation in the throughput or latency. ACKNOW L EDGM EN T This research was partially supported by NSF grants CCR0538945 and ECCS-0725765. "
2009,Synthesis of networks on chips for 3D systems on chips.,"Three-dimensional stacking of silicon layers is emerging as a promising solution to handle the design complexity and heterogeneity of Systems on Chips (SoCs). Networks on Chips (NoCs) are necessary to efficiently handle the 3D interconnect complexity. Designing power efficient NoCs for 3D SoCs that satisfy the application performance requirements, while satisfying the 3D technology constraints is a big challenge. In this work, we address this problem and present a synthesis approach for designing power-performance efficient 3D NoCs. We present methods to determine the best topology, compute paths and perform placement of the NoC components in each 3D layer. We perform experiments on varied, realistic SoC benchmarks to validate the methods and also perform a comparative study of the resulting 3D NoC designs with 3D optimized mesh topologies. The NoCs designed by our synthesis method results in large interconnect power reduction (average of 38%) and latency reduction (average of 25%) when compared to traditional NoC designs.","3A-2 Synthesis of Networks on Chips for 3D Systems on Chips Srinivasan Murali(cid:2) , Ciprian Seiculescu(cid:2) , Luca Benini‡ , Giovanni De Micheli(cid:2) (cid:2) LSI, EPFL, Lausanne, Switzerland, {srinivasan.murali, ciprian.seiculescu, giovanni.demicheli}@epﬂ.ch ‡ DEIS, Univerity of Bologna, Bologna, Italy, lbenini@deis.unibo.it ABSTRACT Three-dimensional stacking of silicon layers is emerging as a promising solution to handle the design complexity and heterogeneity of Systems on Chips (SoCs). Networks on Chips (NoCs) are necessary to efﬁciently handle the 3D interconnect complexity. Designing power efﬁcient NoCs for 3D SoCs that satisfy the application performance requirements, while satisfying the 3D technology constraints is a big challenge. In this work, we address this problem and present a synthesis approach for designing power-performance efﬁcient 3D NoCs. We present methods to determine the best topology, compute paths and perform placement of the NoC components in each 3D layer. We perform experiments on varied, realistic SoC benchmarks to validate the methods and also perform a comparative study of the resulting 3D NoC designs with 3D optimized mesh topologies. The NoCs designed by our synthesis method results in large interconnect power reduction (average of 38%) and latency reduction (average of 25%) when compared to traditional NoC designs. Keywords 3D, networks on chip, topology synthesis, application-speciﬁc 1. INTRODUCTION The 2D chip fabrication technology is facing lot of challenges in utilizing the exponentially growing number of transistors on a chip. The wire delay and power consumption is increasing dramatically and achieving interconnect design closure is becoming a challenge. Designing the clock-tree network for a large chip is becoming very challenging and its power consumption is a signiﬁcant fraction of total chip power consumption. Moreover, diverse components that are digital, analog, MEMS and RF are being integrated on the same chip, resulting in large complexity for the 2D manufacturing process [19]. Vertical stacking of multiple silicon layers, referred to as 3D stacking, is emerging as an attractive solution to continue the pace of growth of Systems on Chips (SoCs) [19]-[24]. The 3D technology results in smaller footprint in each layer and shorter vertical wires that are implemented using Through Silicon Vias (TSVs) across the layers. Heterogeneous systems can be built easily, with each layer supporting a diverse technology [19]. The 3D technology has been maturing over the years in addressing thermal issues and achieving high yield [20]. To tackle the on-chip communication problem, a scalable communication paradigm, Networks on Chips (NoCs) has recently evolved [1]-[3]. NoCs are composed of switches and links and use circuit or packet switching technology to transfer data inside a chip. They provide better structure, modularity and scalability when compared to traditional interconnect solutions. NoCs are a necessity for 3D chips: they provide arbitrary scalability of the interconnects across additional layers, efﬁciently parallelize communication in each layer and help controlling the number of vertical wires (and hence TSVs) needed for inter-layer communication. The combined use of 3D integration technologies and NoCs introduces new opportunities and challenges for designers. Building power-efﬁcient NoCs for 3D systems that satisfy the performance requirements of applications, while satisfying the technology constraints is an important problem. To address this issue, new architectures and design methods are needed. While the issue of designing NoC architectures for 3D has received some attention [30]-[33], there has been little work on design methods for 3D NoCs. The design methods for 2D NoCs do not consider important 3D information, such as the technology constraints on the number of TSVs that can be supported, constraints on communication between adjacent layers, determining layer assignment for switches and placement of switches in 3D. In this work, we address this important problem and present a synthesis approach for designing the most power efﬁcient 3D NoC that meets application performance and technology constraints. We present a synthesis approach to determine the most power efﬁcient topology for the application and for ﬁnding paths for the trafﬁc ﬂows that meet the TSV constraints. Our methods account for power and delay of both switches and links. The assignment of cores to different 3D layers and the ﬂoorplan of the cores in each layer are taken as inputs to the synthesis process. To accurately model the link delay and power consumption, for the given core positions, we present a method to determine the optimal positions of switches in the ﬂoorplan in each layer. We then place the switches on each layer, removing any overlap with the cores. Please note that the assignment of cores to the different layers and the ﬂoorplan of each layer needs to consider several performance and technological constraints, such as thermal issues. There are several works that address these issues [21]-[24] and our work is complementary to them. Here, we only address the issue of designing the NoC topology and determining the placement of the NoC switches. As in our output ﬂoorplan (after placing the switches), the core positions are almost the same as the input ﬂoorplan, we minimally affect these (such as the thermal) issues. We perform experiments on varied, realistic SoC benchmarks to validate the methods. Our results show that the topologies synthesized by our method results in large interconnect power reduction (an average of 38%) and latency reduction (25% on average), when compared to optimized standard NoC topologies. 978-1-4244-2749-9/09/$25.00 ©2009 IEEE 242 3A-2 cores to the different layers in 3D is also obtained as an input. In the communication speciﬁcation ﬁle, the communication characteristics of the application are speciﬁed. This includes the bandwidth of communication across different cores, latency constraints and message type (request/response) of the different trafﬁc ﬂows. To achieve high yield, the number of TSVs that can be established across two layers may need to be restricted below a threshold [25]. In the rest of the paper, we model the maximum TSV constraint by using a constraint on the number of NoC links that can cross two adjacent layers, denoted max ill (for maximum number of inter-layer links). For a particular link width, the maximum number of links can be directly determined from the TSV constraints. For the synthesis procedure, the power, area and timing models of the NoC switches and links are also taken as inputs. We also take the power consumption and latency values of the vertical interconnects as inputs. The output of the topology synthesis procedure is a set of Pareto design points of topologies that meet the constraints, with different values of power, latency and design area. From the resulting points, the designer can choose the optimal point for the application. The synthesis procedure also produces a placement of the switches in the 3D layers and the positions of the switches. The TSV macros needed for establishing vertical links are directly integrated in the switch input/output ports, as done in [33]. As the topology synthesis and mapping problem is NP-Hard [10], we present efﬁcient heuristics to synthesize the best topology for the design. For achieving high yield, it is important to restrict the number of vertical links used and to allow vertical connections only across adjacent layers on the 3D chip [25]. Thus, in our procedure, we connect cores in a layer only to switches in the same layer, and ensure that switches of a layer are directly connected only to switches in adjacent layers. A NoC having fewer switches leads to longer core to switch links and hence, higher link power consumption. On the other hand, when many smaller switches are used, the ﬂows have to traverse more switches, leading to larger switch power consumption. Thus, we need to explore designs with several different switches to obtain the best solution, starting from one where all the cores are connected to a single switch in a layer to a design point where each core is connected to a separate switch. For each switch count, we determine the core to switch connectivity, as explained in Section 4. Then, we determine connectivity across the different switches (Section 5). Then, we determine the optimal positions of the switches on the ﬂoorplan (Section 6) and determine the wire lengths and link power consumption. 4. ESTABLISHING NUMBER OF SWITCHES In this section, we present methods for establishing connectivity between the cores and switches. From the core speciﬁcation ﬁle, we obtain the core speciﬁcations: D E FIN I T ION 1. Let n be the number of cores in the design. The yci respectively, ∀i ∈ 1 · · · n. The 3D layer to which the core i is x and y co-ordinate positions of a core i are represented by xci and assigned is represented by layeri . From the communication speciﬁcation ﬁle, the communication characteristics of the application are obtained and represented by a graph [6], [9], deﬁned as follows: G(V , E ) with each vertex vi ∈ V representing a core and the diD E FIN I T ION 2. The communication graph is a directed graph, rected edge (vi , vj ) representing the communication between the cores vi and vj . The bandwidth of trafﬁc ﬂow from cores vi to vj Figure 1: Proposed 3D NoC design approach 2. RELATED WORK The use of NoCs to replace bus-based designs has been presented in [1]-[2]. Several different NoC architectures and design methods [4]-[5] have been developed over the past few years. A detailed description of the important design issues and the current state-ofthe-art in NoC architectures and design methods is presented in [3]. Synthesis of bus and NoC architectures has been addressed by several researchers for 2D systems. Mapping and placement of cores onto standard NoC topologies has been explored in [6]-[9]. Synthesis of application speciﬁc NoC topologies has been addressed in [10]-[17]. In [17], we presented a NoC synthesis method for 2D SoCs and performed detailed comparisons with standard topologies and other mapping tools. In this paper, we use the basic principles from the 2D method to address the important issues in 3D NoC design. Several works have been presented on the 3D manufacturing processes and interconnects [19], [20], [33]. A performance and cost trade-off analysis of 3D integration is presented in [26]. Several works have explored 3D ﬂoorplanning, placement and temperature issues of cores [21]-[24]. These works do not consider the interconnect synthesis problem. Multi-dimensional topologies (such as k-ary n-cubes, hypercubes) have been extensively explored in the chip-to-chip interconnection ﬁeld [18]. However, such works only consider standard topologies suitable for homogeneous designs. Most SoCs, especially in 3D, are heterogeneous in nature and require application-speciﬁc interconnect architecture to optimize power and performance. Moreover, such works do not address the optimization of topologies based on trafﬁc patterns. Analysis and synthesis of NoCs for 3D technology is a relatively new topic. Novel NoC switch architectures for 3D are presented in [30] and [32]. In [31], the authors present the use of NoCs in 3D multi-processors. In [33], the authors analyze the electrical characteristics of vertical interconnects and show a back-end design ﬂow to implement 3D NoCs. In [27], the authors present an analytical model for cost metrics of 3D NoCs and compare them with 2D NoCs. In [28], design of standard NoC topologies (such as mesh) for 3D is analyzed. Mapping and placement of cores with thermal constraints on to NoC topologies is presented in [29]. However, none of these works address the issue of synthesizing applicationspeciﬁc 3D NoC topologies. 3. DESIGN APPROACH The approach used for topology synthesis is presented in Figure 1. In the core speciﬁcation ﬁle, the name of the different cores, the sizes and positions are obtained as inputs. The assignment of the 243 3A-2 Figure 2: Communication graph example Figure 3: LPGs for the two layers Figure 4: Two min-cut partitions of LPGs is represented by bwi,j and the latency constraint for the ﬂow is represented by lati,j . We deﬁne the Local Partitioning Graph for each layer: D E FIN I T ION 3. A local partitioning graph, LPG(Z , M , ly ), is a directed graph, with the set of vertices represented by Z and edges by M . Each vertex represents a core in the layer ly . An edge connecting two vertices is similar to the edge connecting the corresponding cores in the communication graph. The weight of the edge (mi , mj ), deﬁned by hi,j , is set to a combination of the bandwidth and the latency constraints of the trafﬁc ﬂow from core mi to mj : hi,j = α × bwi,j /max bw + (1 − α) × min lat/lati,j , where max bw is the maximum bandwidth value over all ﬂows, min lat is the tightest latency constraint over all ﬂows and α is a weight parameter. For cores that do not communicate with any other core in the same layer, edges with low weight (close to 0) are added between the corresponding vertices to all other vertices in the layer. This will allow the partitioning process to still consider such isolated vertices. The LPGs for the two layers of the communication graph from Figure 2 are shown in Figure 3. Since the LPGs are built layer by layer, the graphs for the two layers are independent of one another. Extra edges with low weights are added (dotted edges in the ﬁgure) from the vertices that have no connections to the other vertices of the LPG. The algorithm for establishing core to switch connectivity is presented in Algorithm 1. As the number of input/output ports of a switch increases, the maximum frequency of operation that can be supported by it reduces, as the combinational path inside the crossbar and arbiter increases with size. In the ﬁrst step of the algorithm, for the required operating frequency of the NoC, the maximum size of the switch (denoted by max sw size) that can support that frequency is obtained as an input. Based on this and the number of cores in each layer, in the next steps (2-4), we determine the minimum number of switches needed in each layer. Then the local partitioning graph for each layer is built. Then, the number of switches in each layer is incremented (starting from the initial count calculated in steps 2-4) every iteration, until it equals the number of cores in the layer. The term |LP G(Z, M , j )| represents the number of cores in layer j . For each switch count, that many min-cut partitions of the LPG of the layer are obtained (step 13). The cores in the same partition are connected to the same switch. Two min-cut partitions of the LPGs of Figure 3 are shown in Figure 4. Once the partitions for all the layers are obtained, the cores in a partition are attached to the same switch and hence the core to switch connectivity is obtained. The next step is to determine switch to switch connectivity, by ﬁnding paths for the inter switch trafﬁc ﬂows. This is explained further in the next section. 244 Algorithm 1 Core-to-switch connectivity 1: Obtain maximum switch size max sw size for current fre2: for each layer j ∈ 1 · · · lr do quency 3: 4: end for 5: Build LP G(Z, M , j ) for each layer j . for each layer j ∈ 1 · · · lr do nij = (cid:4) number of cores in layerj /max sw size(cid:5) 6: for i = 0 to max∀j∈1···lr {|LP G(Z, M , j )| − nij } do if nij + i ≤ |LP G(Z, M , j )| then np = nij + i np = |LP G(Z, M , j )| 7: 8: 9: 10: 11: 12: end if 13: Obtain np min-cut partitions of LPG(Z,M,j) end for 14: 15: Compute paths for inter-switch ﬂows (Section 5). 16: If valid paths found, save the current design point 17: end for else 5. PATH COMPUTATION The procedure to establish physical links and paths for trafﬁc ﬂows is based on the power consumption increase and latency in using the link. This cost computation in the 3D case is similar to the 2D case, such as those presented in [14], [17], but it needs to account for the max ill and max switch size constraints. Here, we do not show the entire path computation algorithm, but only present the steps needed to meet these constraints. In [14], [17], the authors present methods to remove both routing and messagedependent deadlocks when computing the paths. We also use the methods to obtain paths that are free of deadlocks. D E FIN I T ION 4. Let nsw be the total number of switches used across all the layers and let layeri be the layer in which switch i is present. Let ill(i, j ) be the number of vertical links established between layers i and j . Let the switch size inpi and switch size outi be the number of input and output ports of switch i. Let costi,j be the cost of establishing a physical link between switches i and j . In Algorithm 2, we show the use of hard and soft thresholds when evaluating the cost of establishing a physical link between switches i and j. In steps 3, 4, we assign a cost of INF for establishing a link across switches in non adjacent layers and for switches in layers that have reached the maximum vertical link (max ill) threshold. To ensure meeting the maximum link constraint, we assign a very high cost (denoted by SOF T IN F ) for establishing links between switches that are in layers having vertical links close to the max ill value, denoted by sof t max ill (steps 5, 6). From experiments, we found that a reasonable value for SOF T IN F to be 10 times the maximum cost of any ﬂow and sof t max switch ill to be few (2 to 3) links less than max ill value. We use a similar technique to meet the maximum switch size constraints (steps 10-12). By using these softer constraints ﬁrst, we facilitate the path computation procedure to determine valid paths when compared only using the hard constraints. weighted by their bandwidth values, so that higher bandwidth links are shorter than lower bandwidth ones. Formulating the objective function mathematically, we get: P P P P obj = + ∀i ∀i ∀k coredisti,k ∗ bw sw2corei,k ∀j swdisti,j ∗ bw sw2swi,j 3A-2 (3) (4) Algorithm 2 CHECK CONSTRAINTS(i,j) 1: for i = 1 to nsw do 2: for j = 1 to nsw do 3: if |layeri − layerj | ≥ 2 or ill(layeri , layerj ) ≥ max ill then else if |layeri − layerj | = 1 and ill(layeri , layerj ) ≥ costi,j = IN F sof t max ill then costij = SOF T IN F else if switch size inpi + 1 ≥ max switch size or switch size outj + 1 ≥ max switch size then costi,j = IN F else if switch size inpi + 1 ≥ sof t max switch size or switch size outj + 1 ≥ sof t max switch size 4: 5: 6: 7: 8: 9: costi,j = SOF T IN F then 10: 11: end if 12: end for 13: end for When paths are computed, if it is not feasible to meet the max switch size constraints, we introduce new switches in the topology that are used to connect the other switches together. These indirect switches help in reducing the number of ports needed in the direct switches. Due to space limitations, in this paper, we do not explain the details of how the indirect switches are established. 6. SWITCH POSITION COMPUTATION Once a topology for a particular switch count is obtained, the next step is to ﬁnd the latency and power consumption on the wires. In order to do this, based on the input positions of the cores, the optimal position of the switches needs to be determined. For this, we model the problem as a Linear Program (LP) [34]. Let us consider a topology with nsw switches. We denote the co-ordinates of a switch i by (xsi , ysi ), ∀i ∈ 1 · · · nsw . The goal of the LP is to determine the values of xsi and ysi , for all switches in the particular topology. The sum of the Manhattan distances between a switch i and a core k is given by: coredisti,k = |xsi − xck | + |ysi − yck | 0 , if switchi connected to corek , otherwise (1) The sum of the Manhattan distances between a switch i and switch j to which it is connected to is given by: |xsi − xsj | + |ysi − ysj | swdisti,j = , if switchi connected to switchj , otherwise 8< : 8< : 0 (2) The above equations can be easily represented as a set of linear equations [34]. Let bw sw2corei,k and bw sw2swi,j be the total bandwidth of trafﬁc ﬂows between switch i and core k and switches i and j , respectively. To minimize the total power consumption of the links, we need to minimize the length of the links 245 The LP for optimization is written as follows: minimize obj subject to E quations 1 − 3 xsi , ysi ≥ 0, ∀i ∈ 1 · · · nsw We use the lp solve package [35] to obtain the optimum solution for the switch co-ordinates. Even for big applications (65 cores, tens of switches), the optimal solution is obtained in few seconds. However, the optimal positions can result in overlap of switches among themselves or with the cores. To remove the overlaps, we use the ﬂoorplanner, Parquet [36], layer by layer. We feed the core and switch positions as an input solution to the ﬂoorplanner. We allow it to move the switches around the cores, maintaining the relative positions of the cores and minimizing the movement of the switches from the optimal positions computed by the LP. We also pipeline long links to support full throughput on the NoC and add Network Interfaces (NIs) to connect the cores to the network. The resulting design is a valid ﬂoorplan of the NoC. 7. EXPERIMENTS AND CASE STUDIES For our experiments, we use the switch and link libraries from [5]. The power consumption and latency numbers of the components of the library are obtained after post-layout analysis. We use 65nm low power technology libraries for the layout studies. For the electrical characteristics of vertical interconnects, we use the models from [33]. To obtain the electrical characteristics, a waferto-wafer bonding technique is used as the underlying 3D integration technology. The vertical links are shown to have an order of magnitude lower resistance and capacitance than a horizontal link of the same dimension. This translates to a traversal delay of less than 10% of clock cycle for 1 GHz operation and negligible power consumption on the vertical links. 7.1 Multimedia SoC case study For experimental case study, we consider a multi-media SoC, Triple Video Object Plane Decoder, that has 38 cores (D 38 tvopd). The communication graph of the benchmark is presented in Figure 5, where each vertex represents a core and the weight on the edge represents the bandwidth between the cores expressed in MB/s. The application is highly heterogeneous in nature, having three independent decoders working in parallel to improve performance. Each decoder has 12 cores organized in a pipeline fashion. There are two extra memories that are shared between the pipelines that serve as input and output buffers. We consider the design implemented on to 3 layers in 3D. The assignment of cores to the different layers and the ﬂoorplan of each layer were done manually, such that the performance and manufacturing constraints (such as thermal issues) are met. The processing cores are placed on the top and bottom layers, so that they are close to the heat sink. The large memory cores are all placed on the middle layer because they produce less heat and because this allows the manufacturer to use an efﬁcient integration process for implementing the memories. The ﬂoorplan of the design (along with the network components synthesized by our procedure) is presented in Figure 7. The data width of the NoC links is ﬁxed to 32 bits, to match the data width of the cores in the design. We allowed the synthesis method to sweep the NoC frequency and obtain NoC design points 3A-2 Figure 5: Communication graph for the D 38 tvopd benchmark Figure 7: Resulting 3D ﬂoorplan with switches for different frequencies. From the resulting design points, we found that the lowest operating frequency (of 500 MHz) resulted in least power consumption for this design. The power consumption of NoC designs synthesized by our procedure for different switch counts, at 500 MHz operation, is presented in Figure 8. In the ﬁgure, we show the core-to-switch link power, the switch-to-switch link power, the switch power and the total power consumption. The plot starts with 5 switches (on x-axis), as the maximum size of a switch to support 500 MHz operation was 11x11 and the top and bottom layers needed 2 switches each (topology shown in Figure 6), as they have more than 10 cores each. Because the number of cores and the communication demand on each layer is different, we obtain different number of switches on each layer. Since the area of each 3D layer is small (approximately 20 mm2 ), the links are short and switch power has higher impact on the total power consumption. With increasing switch count, the switch power increases signiﬁcantly, leading to higher power consumption. For this design, the NoC with 5 switches is most power optimal and the resulting ﬂoorplan is shown in Figure 7. Figure 6: Most power-efﬁcient topology ) W m ( n o i t p m u s n o c r e w o P 120 100 80 60 40 20 0 5 Switch power Core−to−switch link power Switch−to−switch link power Total power 10 15 20 Switch count 25 30 35 Figure 8: Power consumption 7.1 and ﬁve other benchmarks that model different trafﬁc scenarios. We consider 3 benchmarks: D 36 4, D 36 6 and D 36 8 with 36 cores, each core communicating to 4, 6 and 8 other cores, respectively, modeling designs with multiple local memories. We also consider a benchmark with shared memory bottleneck communication (D 35 bot). For a larger design, we performed tests on the D 65 pipe which has 65 processing elements distributed on three layers and organized in a pipeline fashion. All of the benchmarks are mapped on to 3 layers in 3D. We compared the custom topologies generated for the bench marks against an optimized mesh topology. For the optimized mesh each core is connected to a switch and only the necessary links among switches are opened. The results of the comparison between the best custom topology and the optimized mesh are presented in Figure 9. As can be seen from results, the topology synthesized by our method results in large power savings (38% on average) when compared to the optimized mesh topologies. The synthesized topologies also resulted in 24.5% reduction in average zero-load latency, when compared the optimized mesh based NoC. 7.2 Comparisons with mesh Custom topologies that match the application characteristics can result in large power-performance improvement when compared to the standard topologies, such as mesh and torus [17]. For this comparison we used the D 38 tvopd benchmark presented in Section 7.3 Impact on inter-layer link constraint Limiting the number of inter-layer links has a great impact on power consumption and average latency. Reducing the number of TSVs is desirable for improving the yield of a 3D design. However, a very tight constraint on the number of inter-layer links can 246     ) W m ( n o i t p m u s n o c r e w o P 400 350 300 250 200 150 100 50 0 3D Application specific 3D Opt−mesh D_36_4 D_36_6 D_36_8 D_35_bot D_65_pipeD_38_tvopd 240 230 220 210 200 190 180 ) W m ( n o i t p m u s n o c r e w o p m u m i n i M 170 10 3A-2 3.7 3.65 3.6 3.55 3.5 ) s e l c y c ( y c n e t a l m u m i n i M 11 12 13 14 15 16 Maximum number of inter−layer links (max_ill) 17 18 3.45 10 11 12 13 14 15 16 Maximum number of inter−layer links (max_ill) 17 18 Figure 9: Comparisons with mesh Figure 10: Impact of max ill on power Figure 11: Impact of max ill on latency lead to a signiﬁcant increase in power consumption. To see the impact of the constraint, we varied the value of max ill constraint and performed topology synthesis for each value, for one of the benchmarks (D 36 4). The power and latency values for the different max ill design points are shown in Figures 10 and 11. The dotted line in the ﬁgures represent points where the max ill constraint was too tight to produce any feasible topologies. When there is a tight constraint on the inter-layer links, more ﬂows are routed through existing inter-layer links instead of opening new ones. This leads to traversing more intermediate switches and higher switch activities, leading to higher latency and power consumption. Please note that our synthesis algorithm also allows the designers to perform such power, latency trade-offs for yield, early in the design cycle. The synthesis algorithm explores a large solution space. However, thanks to the efﬁcient heuristic methods presented, the entire topology design process completed in few hours for all the experiments, when run on a 2 GHz Linux workstation. Please note that the synthesis process is performed once at design time and this computational time incurred is negligible. 8. ACKNOWLEDGEMENTS This work is supported by the Swiss National Science Foundation (FNS, Grant 20021-109450/1). 9. CONCLUSIONS The use of Networks on Chips (NoCs) for communication in 3D chips has posed new opportunities and challenges for designers. One of the most important problems is to design the most powerperformance efﬁcient NoC topology that satisﬁes the application characteristics and 3D technology requirements. In this work, we presented a synthesis approach to solve this problem. We also presented methods to place switches optimally on the 3D ﬂoorplan, so that accurate power and delay numbers are obtained for the wires. Our detailed comparisons with regular 3D optimized mesh show that the custom 3D topologies lead to a large reduction in interconnect power consumption. In future, we plan to explore tuning the link data widths to meet the TSV constraints and to improve the yield of the 3D NoCs. 10. "
2009,Analysis of communication delay bounds for network on chips.,"In network-on-chip, computing worst-case delay bound for packet delivery is crucial for designing predictable systems but yet an intractable problem due to complicated resource contention scenarios. In this paper, we present an analysis technique to derive the communication delay bound for individual flows. Based on a network contention model, this technique, which is topology independent, employs the network calculus theory to first compute the equivalent service curve for individual flows and then calculate their packet delay bound. To exemplify our method, we also present the derivation of a closed-form formula to calculate the delay bound for all-to-one gather communication. Our experimental results demonstrate the theoretical bounds are correct and tight.","1A-2 Analysis of Communication Delay Bounds for Network on Chips Yue Qian† , Zhonghai Lu‡ , and Wenhua Dou† †School of Computer Science, National University of Defense Technology, China ‡Dept. of Electronic, Computer and Software Systems, Royal Institute of Technology, Sweden E-mail: †{yueqian, douwh}@nudt.edu.cn, ‡ zhonghai@kth.se Abstract—In network-on-chip, computing worst-case delay bound for packet delivery is crucial for designing predictable systems but yet an intractable problem due to complicated resource contention scenarios. In this paper, we present an analysis technique to derive the communication delay bound for individual ﬂows. Based on a network contention model, this technique, which is topology independent, employs the network calculus theory to ﬁrst compute the equivalent service curve for individual ﬂows and then calculate their packet delay bound. To exemplify our method, we also present the derivation of a closed-form formula to calculate the delay bound for all-to-one gather communication. Our experimental results demonstrate the theoretical bounds are correct and tight. I . IN TRODUC T ION Quality-of-Service (QoS) has been a major concern for Network-on-Chip (NoC) since its birth around the year 2000 [2]. The reason is due to the fact that routing packets in shared resource networks inherently brings about unpredictable performance. This non-determinism does not meet the requirement of building predictable communication systems in which delay bounds must be guaranteed in any case. Apparently many applications such as multimedia, HDTV, set-top and gaming boxes have stringent requirements on communication delay bounds [11]. For example, processing 25 high deﬁnition video frames (1680 x 1024, 1920 x 1280, etc.) must be completed within 20 ms. There are a number of approaches to address QoS for onchip communication [2]. From a service-oriented point of view, a network may provide best-effort (BE) and guaranteed services to satisfy the requirements of different QoS provisions. To offer strict promises, a guaranteed service typically reserves resources for exclusive use. This essentially isolates interferences. For example, Time Division Multiplexing (TDM) virtual circuits (VCs) are proposed for the Æthereal [5] and Nostrum [10] NoCs. However, there are two main drawbacks. First, while such VCs provide guarantees once they are established, the setup procedure itself is non-predictable if it is done dynamically. Second, resources may often be over-reserved, leading to lower resource utilization. To make a good utilization of the shared network resources, BE networks are preferred. However, BE networks are known in achieving good average performance, but the worst case performance is extremely hard to predict. The reasons are that (1) network contention for shared resources (buffers and links) includes not only direct contention but also indirection contention. They are difﬁcult to capture in entirety; (2) identifying the worst case is 978-1-4244-2749-9/09/$25.00 ©2009 IEEE 7 nontrivial. The worst case is in general unknown or uncertain. This hard situation leaves designers with simulation as almost the only tool to ﬁnd the maximal delay by simulating various trafﬁc scenarios. While the simulation based approach can offer the highest accuracy but can be very time-consuming. Each simulation run may take considerable time and evaluates only a single network conﬁguration, trafﬁc pattern, and load point. It is difﬁcult, if not impossible, to cover all the system states [9]. In contrast, a formal-analysis-based (mathematical) method is much more efﬁcient that it provides approximate performance numbers with a minimum amount of effort and gives insight into how different factors affect performance [9]. The accuracy of results can be rough but gives an initial and quick estimation. A calculated performance bound may be also tight enough. In this paper, we take the formal analysis approach, aiming for deriving the worst case delay bound for individual ﬂows, called tagged ﬂows, for on-chip networks. Our assumption is that the application-speciﬁc nature of on-chip communication enables to characterize trafﬁc with sufﬁcient accuracy. This trafﬁc characterization may follow the abstraction of the arrival curve in network calculus [1, 4]. The router model follows the abstraction of the service curve. We also assume a deterministic routing, which is cheap to implement and give more determinism. For a tagged ﬂow, we ﬁrst construct its contention model. This model is a contention tree, which captures not only its contention with other interfering ﬂows along its routing path but also the indirect contention experienced by the interfering ﬂows. Based on this tree, we scan the tree to compute the output arrival curves of each branch in the contention tree. Then we derive the equivalent service curve for the tagged ﬂow traversing the trunk of tree and calculate its delay bound. To show the usage and potential of our method, we take all-to-one gather communication, which is an important collective operation for parallel processing, as an example to derive a closedform formula to calculate the delay bound. The remainder of the paper is organized as follows. Section II reviews related work and summarizes our contributions. In Section III, we present the general analytical approach for calculating the delay bound for a tagged trafﬁc ﬂow. In Section IV, we exemplify the potential of the analysis method with the all-to-one gather collective communication. Experiments are reported in Section V. Finally we draw conclusions in Section VI. I I . R ELATED WORK In the context of testing the feasibility of packet delivery within time constraints in wormhole networks, researchers 1A-2 have proposed various ways to capture network contention and then utilize scheduling theory to ﬁnd and estimate the worstcase delay bounds for packets [7, 8]. In [7], the lumped link model was proposed by which the links that a packet traverses are lumped into a single link. This model does not distinguish direct and indirection contention, thus the estimated bounds are pessimistic. In [8], Lu et al. proposed a contention tree model, which differentiates direct and indirect contention. More importantly, the tree structure allows further differentiating trunk contention and branch contention. Both works assume deterministic routing. In adaptively routed networks, it is a must to avoid live-lock. The best assurance of live-lock freedom is to make sure that there exists an upper bound for any packets. Studies have been carried to analytically compute packet delivery bounds. However, they consider only special cases that assume very simple trafﬁc scenarios. For example, [3] considers only batch admissions, which inject trafﬁc into the network once, in a deﬂectionrouted network. In general queuing networks, network calculus provides the means to deterministically reason about timing properties of trafﬁc ﬂows. Based on the powerful abstraction of arrival curve for trafﬁc ﬂows and service curve for network elements (routers, servers), it allows computing the worst-case delay and backlog bounds. Network calculus has been extremely successful for ensuring performance bounds in ATM, Internet with differentiated and integrated services, and other types of networks. Systematic accounts of network calculus can be found in books [1, 4]. Our work applies the network calculus theory for on-chip networks. One main reference we use is [6], where a method for computing least upper delay bound (LUDB) with leakybucket constrained ﬂows and rate-latency service curves for the aggregates is presented. It derives per-ﬂow service guarantees from per-aggregate service guarantees at a single node and consists in applying that theorem iteratively to obtain a set of endto-end service guarantees for a ﬂow. Then, the LUDB among all the bounds is derived from each single end-to-end service guarantee. In fact, each node is characterized by means of a lower bound on the service it offers. This method yields better bounds than those previously proposed methods [12, 13, 14]. In our approach, we also start with modeling network contention. Our model is similar to the contention tree in [8]. However, we extend this model by identifying basic contention patterns for ﬂows. Based on these patterns, any complex contention scenario can be decomposed into the basic patterns. This divide-and-conquer approach enables to systematically study the comprehensive network contention scenarios. Furthermore, we derive the equivalent service curve for the tagged ﬂow under these basic patterns, as we shall see in Section III. Moreover, we combine the network contention tree model with network calculus analysis. Consequently, this combination makes it possible to calculate delay bounds for any tagged ﬂow in networks with an arbitrary topology. We summarize our contributions as follows: • We identify three basic ﬂow contention patterns. Any complex contention scenario for a tagged ﬂow can be described by a composition of these scenarios. Furthermore, we give formal representations for calculating the equivalent service curve for the three basic patterns. • We propose an analysis procedure to compute the delay bound for a tagged ﬂow interfered by other ﬂows. It contains two main steps: (1) construct a contention tree; (2) scan the tree and compute the equivalent service curve for the tagged ﬂow. • We present a case study on all-to-one gather communication and derive the closed-form formula to compute the delay bound. I I I . THE D ELAY BOUND ANALY S I S We ﬁrst illustrate the problem, and then detail our delay bound analysis technique. A.TheProblemIllustration P P 1 5 9 2 6 3 7 10 11 4 8 12 P P P 13 14 15 16 P Fig. 1. A problem example. We use an example to illustrate the problem that we address and to explain terminology used in the paper. Fig. 1 shows a network with 16 nodes, numbered from 1, 2, , 16. A node contains a core and a router. We assume deterministic routing for the network, and routers serve ﬂows in the FIFO order. There are 5 packet ﬂows sent from ﬁve nodes 1, 4, 8, 12 and 16 to node 15. We denote f(i,j ) as the ﬂow injected at router i and ∗ ejected at router j , αi,j as arrival curve at ingress router, α as output arrival curve at egress router, and βi as service curve of router i. We call the ﬂow for which we shall derive its delay bound tagged ﬂow, other ﬂows that share resources with it interfering or contention ﬂows. In this example, f(1,15) is the tagged ﬂow, and f(4,15) , f(5,15) , f(8,15) and f(12,15) are inter(i,j ) fering ﬂows. B.TheRouterServiceModel R Fig. 2. Two ﬂows multiplexing in a router. Fig. 2 shows a lossless router serving two ﬂows, tagged and contention, in the FIFO order. Assume the router guarantees a service curve β to both ﬂows. The tagged and contention ﬂow have α1 , α2 as arrival curve, respectively. We deﬁne (β , α2 ) as the equivalent service curve [4, 6] to the tagged ﬂow, where (., .) is a function to compute the equivalent service curve. Thus, according to [4], the output arrival curve of 1 = α1 (cid:2)(β , α2 ), and its ∗ tagged ﬂow can be derived easily as α delay bound is h(α1 , (β , α2 )), where h(., .) is the function to compute the maximum horizontal distance between the arrival curve and the service curve. 8 C.InterferencePatternsand theirAnalyticalModels Apparently, network ﬂow contention scenarios are diverse and complicated. A tagged ﬂow directly contends with interfering ﬂows. Also, interfering ﬂows may contend with each other and then contend with the tagged ﬂow again. This indirect contention may in turn inﬂuence the performance of the tagged ﬂow. To decompose a complex contention scenario, we identify three primitive contention patterns. 1 1 1 j j j g g h (a) Nested h (b) Parallel g h (c) Crossed k N k N k N Fig. 3. The three basic contention patterns for a tagged ﬂow. Scenario I: Nested contention ﬂows. As illustrated in Suppose that a tagged ﬂow f(1,N ) traverses a tandem of N routers from source to destination, and is multiplexed with contention ﬂows. The contention scenarios the tagged ﬂow may experience can be classiﬁed into three patterns, Nested, Parallel and Crossed, as shown in Fig. 3. In the following, we analyze the three scenarios and derive their basic analytical models with focus on the derivation of service curve the tandem provides. Once obtaining the tandem service curve, the output arrival curve and delay bound can be derived. Fig. 3(a), contention ﬂow f(h,j ) is nested in f(g ,k) , 1 ≤ g ≤ h ≤ j ≤ k ≤ N . The sub-tandem (h, j ) serves the aggregation of f(1,N ) , f(g ,k) and f(h,j ) with service curve βi , if excluding f(h,j ) , we can get the equivalent service curve for βi , α(h,j ) ). Then the service curve of sub-tandem (g , k) for the aggregation of f(1,N ) and f(g ,k) is computed as (h−1(cid:2) βi ), thus (h−1(cid:2) we can derive the equivalent service curve (g ,k) for f(1,N ) as . Therefore the tang−1(cid:2) dem (1, N ) serves f(1,N ) with the service curve β(1,N ) = ( βi ). βi ) ⊗ (h,j ) ⊗ ( βi ) ⊗ (g ,k) ⊗ ( N(cid:2) f(1,N ) and f(g ,k) as (h,j ) = ( i=h βi ) ⊗ (h,j ) ⊗ ( βi ), α(g ,k) j(cid:2) k(cid:2) j(cid:2) k(cid:2) i=j+1 i=j+1 (cid:4) i=h (cid:3)  i=g i=g i=1 i=k+1 Scenario II: Parallel contention ﬂows. As shown in (g ,h) = ( h(cid:2) i=g Fig. 3(b), contention ﬂows f(h,j ) and f(g ,k) are independent. Similarly to Scenario I, we can get the equivalent service curve of sub-tandem (g , h) and (j, k) for f(1,N ) respectively, as βi , α(g ,h) ) and (j,k) = ( k(cid:2) βi , α(j,k) ). Hence βi ) ⊗ (j,k) ⊗ ( N(cid:2) βi ) ⊗ (g ,h) ⊗ ( the service curve of tandem (1, N ) for f(1,N ) can be calculated g−1(cid:2) j−1(cid:2) βi ). as β(1,N ) = ( i=j i=1 i=h+1 i=k+1 Scenario III: Crossed contention ﬂows. As shown in Fig. 3(c), contention ﬂow f(g ,j ) is crossed with f(h,k) . We can 1A-2 see there are two cross points produced by these two ﬂows, one at the ingress of router h and the other at the egress of router j . If we cut f(g ,j ) arbitrarily at the ﬁrst cross point, i.e. at ingress of router h, f(g ,j ) will be split into two ﬂows f(g ,h−1) and f(h,j ) , as shown in Fig. 4. Then the problem is strictly transformed to the combination of scenario I and II that f(g ,h−1) is separate and f(h,j ) is nested in f(h,k) . Apparently the arrival curve α(g ,h−1) of f(g ,h−1) equals to α(g ,j ) and the arrival ∗ ∗ curve α(h,j ) of f(h,j ) equals to α (g ,h−1) . To compute α g−1(cid:2) we need to get the arrival curve of f(1,N ) at the ingress of subtandem (g , h− 1) which equals to α ∗ βi ). for f(g ,h−1) is derived as (h−1(cid:2) Then the equivalent service curve of sub-tandem (g , h − 1) (1,g−1) ). Thus we obtain ∗ (h−1(cid:2) ∗ the output arrival curve of f(g ,h−1) as α ∗ g−1(cid:2) we can calculate the tandem service curve for f(1,N ) directly as βi ). (g ,h−1) = α(g ,h−1) (cid:2) βi ) ⊗ (g ,h−1) ⊗ (h,k) ⊗ ( N(cid:2) i=k+1 (g ,h−1) , (1,g−1) as α(1,N ) (cid:2) ( (1,g−1) ). With α(g ,h−1) , α(h,j ) and α(h,k) obtained, βi , α β(1,N ) = ( βi , α i=1 i=1 i=g i=g 1 g h j k N Fig. 4. Transform the crossed ﬂows to non-crossed ﬂows. D.TheAnalysisProcedure In Section III-C, we have derived three basic analytical models for analyzing the delay bound for the tagged ﬂow multiplexed with two contention ﬂows in three different scenarios. For complex scenarios with more than two contention ﬂows in the tandem, the problem can be split into the basic models and expressed with a contention tree. The analysis procedure is detailed as follows: Step 1: Construct a contention tree to model the network contentions produced by interfering ﬂows; Step 1.1: Let the tandem traversed by the tagged ﬂow be the trunk; Step 1.2: Let the tandems traversed by the interfering ﬂows before reaching the trunk node be branches, which may also have its own sub-branches; Step 2: Scan the contention tree and compute all the output arrival curves of ﬂows traversing the branches using the basic analytical models iteratively; Step 3: Compute the equivalent service curve for the trunk ﬂow (tagged ﬂow) and derive the delay bound. We use the example in Fig. 1 to explain this procedure. In Fig. 1, the tagged ﬂow f(1,15) is multiplexed with f(4,15) , f(5,15) , f(8,15) and f(12,15) . We construct a contention tree shown in Fig. 5 to model the ﬂow contentions, where the tandem (1, 15) traversed by f(1,15) composes the trunk with the ingress router 1 up to egress router 15 and the tandems traversed by contention ﬂows before reaching the trunk nodes compose the branches. At router 15, f(5,15) is ﬁrst injected into the trunk and the sub-tandem traversed by f(5,15) builds 9 1A-2 14 10 7 6 2 3 4 5 1 15 11 7 3 2 1 16 16 12 12 8 10 5 4 6 1 8 3 7 2 4 Fig. 5. The contention tree for the tagged ﬂow f(1,15) . the ﬁrst branch. Branch (5, 14) has sub-branch (4, 7) produced by its own contention ﬂow f(4,15) as indicated by the dashed blue line in Fig. 5. Also f(1,15) contends with f(4,15) at the sub-tandem (3, 7) that builds the sub sub-branch (1, 2) as the dashed red line indicates for sub-branch (4, 7). If a tandem containing a sequence of nodes, at the node where one ﬂow is multiplexed with the trafﬁc coming form the upstream node, we treat the ﬂows in the tandem as an aggregate ﬂow. For instance, f(8,15) and f(12,15) injected into router 15 produce two branches (8, 16) and (12, 16), where branch (12, 16) has sub-branch (8, 8). These two branches are composed of the same nodes. Thus, we treat f(8,15) and f(12,15) as an aggregate ﬂow (f(8,15) , f(12,15) ) and reserve only one branch (8, 16) for optimization. If ﬂows coming from different branches injected into and ejected out a tandem at the same ingress and egress nodes, we treat the ﬂows at the ingress node as an aggregate ﬂow. For instance, since both f(5,15) and the aggregate ﬂow (f(8,15) , f(12,15) ) are injected into router 15 and ejected out router 15 to the core, they can further compose into a larger aggregate ﬂow (f(5,15) , f(8,15) , f(12,15) ) for the trunk. f(4,15) contends with f(1,15) twice. It is ﬁrst injected into the trunk at router 3 and ejected out at router 7, and then injected at router 11 again. Hence two branches extend at router 3 and 11 in the trunk. To derive the equivalent service curve for trunk ﬂow f(1,15) , we scan the contention tree in Fig. 5 using the Depth-FirstSearch to ﬁrst compute the output arrival curves of f(1,15) traversing the sub sub-branches (1, 2) for sub-branch (4, 7), then the output arrival curve of f(4,15) traversing the subbranches (4, 7) for branch (5, 14), thus the output arrival curve of f(5,15) traversing branch (5, 14) for trunk (1, 15) is derived using the basic analytical models. Analogously, the output arrival curves of f(4,15) traversing branch (4, 10) and (4, 4) for trunk (1, 15) are derived. We compute the output arrival curve of the aggregate ﬂow (f(8,15) , f(12,15) ). After all arrival curves of injected ﬂows are obtained, we then compute the trunk service curve for f(1,15) and thus delay bound for f(1,15) can be derived. We have designed two algorithms, one for constructing the contention tree and the other for computing the equivalent service curves of branches and trunk. Due to space limitation, they are not presented here. IV. A LL - TO -ONE D ELAY BOUND To demonstrate the potential of our analysis method, we study an all-to-one communication case for which we derive 10 closed-form formula to calculate the delay bound. A.All-to-oneGatherCommunicationonaMeshNoC We consider all-to-one gather communication, which is one a m × n mesh NoC with the XY routing. We assume a hoof common collective operations for parallel applications, on mogeneous NoC where all cores and routers have the same capacity. To simplify our discussions, we assume that all ﬂows from the cores are constrained by an afﬁne arrival curve αr,b (t) = rt + b and all routers provide a rate-latency service curve βR,T (t) = R(t − T )+ . As there coexist m × n − 1 ﬂows simultaneously, (mn − 1)r ≤ R, meaning that the sum of the rates for the ﬂows must be less than or equal to the router’s service rate. Fig. 6(a) shows an all-to-one gather communication in a 4×4 NoC with the gathering node (3, 3). Since the NoC is a mesh, we naturally use 2D coordinates (x, y) to represent a node. We also deﬁne a labeling function l(x, y) = n(x − 1) + y to mark the nodes with a sequence of numbers. Let node (x, y) be the gathering node, we use f(n(i−1)+j,n(x−1)+y) to represent the ﬂow from node (i, j ) to node (x, y). Note this is consistent with the labeling convention for ﬂows in Section III. By the labeling function, nodes (1, 1) and (3, 3) are labeled as 1 and 11, respectively. Fig. 6(b) depicts the contention tree for ﬂow f(1,11) considering aggregate ﬂows in the branches. (3,3) (3,2) (3,4) (2,3) (4,3) (3,1) (2,1) (2,2) (2,4) (1,3) (4,4) (4,2) (4,1) (1,2) (1,4) (1,1) (a)                  (b)  Fig. 6. All-to-one gather communication in 4 × 4 NoC and the contention tree for f(1,11) . B.Closed-formFormulaforDelayBound Without loss of generality, we set f(1,n(x−1)+y) as the tagged ﬂow and (x, y) as the gathering node. The contention tree for f(1,n(x−1)+y) can be built similarly to Fig. 6(b). Next we scan the contention tree to compute the arrival curves of all injected aggregate ﬂows coming from branches to the trunk. After that we can calculate the equivalent service curve of the trunk for the tagged ﬂow. Finally, the delay bound for the tagged ﬂow can be derived. While scanning the tree, we classify the branches into three types and derive the formulas for computing the output arrival curves of the aggregate ﬂows injected to the trunk. For branches (i, 1) → (i, y − 1), i = 2, . . . , m and (i, n) → (i, y − 1), i = 1, . . . , m, shown in Fig. 7, we use Theorem 4.15 at egress node (i, y − 1) and (i, y + 1) as follows. in [6] to compute the output arrival curve of the aggregate ﬂow At node (i, y − 1), α ∗ 1 with (i,y−1) = αr 1 ,b ∗ ∗ (cid:5) r∗ b∗ 1 = (y − 1)r 1 = (y − 1)(b + y 2 rT ), R(i,1) R(i,2) R(i,y-2) R(i,y-1) y-1 routers R(i,n) R(i,n-1) R(i,y+2) R(i,y+1) n-y routers (a) (b) Fig. 7. Output arrival curves for the aggregate ﬂow at egress node (i, y − 1) and (i, y + 1). and at node (i, y + 1), α ∗ (i,y+1) = αr ∗ 2 ,b ∗ 2 with ⎧⎨ ⎩r∗ b∗ 2 = (n − y)r 2 = (n − y)(b + n − y + 1 2 rT ). (a) (b) Fig. 8. Input arrival curves for the aggregate ﬂow at ingress node (i, y). At trunk node (1, j ), j = 2, . . . , y − 1, only one ﬂow with arrival curve αr,b is newly arriving. At trunk node (1, y), the aggregate ﬂow is the sum of trafﬁc coming from the local core and a branch as depicted in Fig. 8(a), for that α(1,y) = αr1 ,b1 = ∗ (1,y+1) and the parameters r1 and b1 is derived below. At node (i, y), i = 2, . . . , x − 1, x + 1, . . . , m, the newly injected trafﬁc is composed of three incoming ﬂows such as ∗ ∗ (i,y+1) as illustrated in Fig. 8(b), thus the ag∗ gregate ﬂow’s arrival curve is α(i,y) = αr2 ,b2 = α ∗ (i,y+1) with parameters as follows. ∗ αr,b + α α (i,y−1) , αr,b , α (i,y−1) + αr,b + α At trunk node (1, y), α(1,y) = αr1 ,b1 = αr,b + α (1,y+1) with ⎧⎨ ⎩r1 = (n − y + 1)r b1 = (n − y + 1)(b + n − y 2 rT ), (1) and the input arrival curve at node (i, y) is α(i,y) = αr2 ,b2 = ∗ ∗ α (i,y−1) + αr,b + α (i,y+1) with ⎧⎨ ⎩r2 = nr b2 = nb + n2 + 2y2 − 2ny + n − 2y 2 rT . (2) R (m,y) R (m-1,y) R (x+2,y) R (x+1,y) m-x routers (a) (b) Fig. 9. Output arrival curves for the aggregate ﬂow at egress node (x + 1, y) and input arrival curve at trunk node (x, y). Finally, at trunk node (x, y), the newly injected ﬂow is coming from three branches with the arrival curve ∗ ∗ ∗ of α respectively. The branch (m, y) → (x + 1, y) has sub-branches at each branch node. Fig. 9(a) indicates the ﬂows aggregated at the branch and the output at (x + 1, y). The corresponding output arrival curve is ∗ 3 , where (x,y−1) , α (x+1,y) , α (x,y+1) α (x+1,y) = αr ∗ ∗ 3 = (m − x)r2 3 = (m − x)(b2 + 3 ,b ⎧⎨ ⎩r∗ b∗ m − x + 1 2 r2T ). Hence we get the aggregated branch arrival curve α(x,y) , ∗ ∗ ∗ α(x,y) = αr3 ,b3 = α (x,y−1) + α (x+1,y) + α (x,y+1) with (cid:5) r3 = r∗ 1 + r∗ 2 + r∗ b3 = b∗ 1 + b∗ 2 + b∗ 3 . 3 (3) R(1,1) R(1,y-1) y-1 routers R(2,y) R(x-1,y) R(x,y) x-2 routers P(x,y) P(1,1) R(1,y) Fig. 10. All aggregate ﬂows multiplexed with the tagged ﬂow in the trunk. Now we derive all branch arrival curves for the trunk. As shown in Fig. 10, the contention ﬂows are nested. We can apply the basic analytical model of Scenario I iteratively to derive the equivalent service curve for the tagged ﬂow. Then we get Formula (4) for computing the delay bound ¯D , which has been proved to be the least upper delay bound [6]. ¯D = x+y−1(cid:9) i=1 Rx+y−j · y−1−j(cid:10) k=1 T + y−1(cid:9) j=1 b 1 R + r1 1 R + r · · x−2(cid:10) l=1 1 R + r2 · 1 R + r3 + Rx · x−2(cid:10) b1 1 R + r2 Rx−j · x−2−j(cid:10) l=1 · 1 R + r3 + x−2(cid:9) j=1 b2 k=1 1 R + r2 · 1 R + r3 + b3 R . (4) V. EX P ER IM EN T We design experiments to verify our analytical approach. We use the all-to-one gather communication. Since we have derived the delay bound formula for all-to-one communication in Section IV, we conduct simulations to compare observed maximum delays in simulations with calculated bounds. We consider a 4 × 4 mesh NoC with different gathering node (3, 3) and (4, 4) and a 5 × 5 mesh NoC with gathering node (3, 3), (4, 4) and (5, 5). The delay bound for the ﬂow generated by node (1, 1) to the gathering node is calculated and simulated. Each ﬂow generated by the core is constrained by the arrival curve of αr,b (t) = 1t + 4 where the generating rate r = 1 packet/cycle and the burst b = 4 packets, and each router guarantees the service curve of βR,T (t) = 25(t − 3)+ , where the serving rate R = 25 packets/cycle and the latency T = 3 cycles. We build a NoC simulator that each core generates a Poisson stream with rate λ = 1.1 packets/cycle, which then passes through a trafﬁc shaper with leaky rate r = 1 packet/cycle and bucket size b = 4 packets. We construct the router by ratelatency server as β25,3 with big enough buffer size. The delay experienced by a packet of the tagged ﬂow is recorded when it reaches the gathering node. We assume zero link delay in experiments. We run every simulation for 1E+9 cycles and get the calculated delay bounds using Eq. (4) for different settings. The calculated bounds and experimental maximum delays of all packets are listed in Table I. We denote calculated delay bound and 1A-2 11 1A-2 simulated maximum delay as ¯D and Dmax , respectively. For all results, the unit for delay is cycle. indicates that increasing the service rate can reduce the delay bound until the delay approaches the propagation delay. NoC Tagged Dmax (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) Flow → (3, 3) → (4, 4) → (3, 3) → (4, 4) → (5, 5) ¯D 19.97 19 25.85 23 25.43 23 4 × 4 Mesh 5 × 5 Mesh 30.36 29 36.37 34 CA LCU LAT ED BOUND S AND OB S ERV ED MAX IMUM D E LAY S . TABLE I From Table I we can see that the calculated delays bound the experimental maximum delays tightly. It also shows that the delay of the same tagged ﬂow becomes larger when the NoC mesh size increases. When the gathering node is nearer to the network edge from (3, 3) to (4, 4) and later to (5, 5), the maximum delay and the delay bound increase since the overall distance from sources to the gathering node increases. s e c y l C = t i n U , y a e l D 20 15 10 5 0 0 Calculated Delay Bound 19.97 Cycles 200 400 600 Packet No. (1~1000) 800 1000 Fig. 11. Calculated bound and observed maximum delay for ﬂow (1, 1) → (3, 3) in 4 × 4 mesh NoC. We also plot a sequence of 1000 observed delays including the maximum delay for tagged ﬂow (1, 1) → (3, 3) in 4 × 4 mesh NoC in Fig. 11. The blue circle indicates the delay experienced by a packet and the red line represents the calculated delay bound. We can see the simulated delays are totally constrained by the calculated delay bound and the bound is tight. The calculated bound is 19.97 cycles, and the observed maximum delay is 19 cycles. s e c y l C = t i n U , y a e l D 28 26 24 22 20 18 16 14 Calculated Delay Bound Simulated Maximum Delay 0 0.2 0.4 0.6 Relative Supply Factor 0.8 1 Fig. 12. Delays under different relative supply factors. We also consider the relationship between the delay and the vice rate R = (mn−1) ·r ·(1+η), i.e., η is a relative supply facservice capacity of the routers. Assume the routers have a sertor to express the extra service capability of the routers. Fig. 12 reports the calculated delay bounds and simulated maximum delays for the 4 × 4 mesh NoC with r = 1 packet/cycle, b = 4 packets, T = 3 cycles and η varying form 0 to 1 with step 0.1. As can be seen, the simulated maximum delays are bounded by the calculated delay bound. As η increases, the maximum delay decreases but the decrement is becoming smaller. This 12 V I . CONC LU S ION Application exerts stringent requirements on on-chip networks to ensure performance bounds even under worst cases. In this work, we present an analysis method to compute the delay bound for tagged ﬂows. This method is the consequence of synergistically combining the network contention tree model with network calculus. It assumes deterministic routing but is topology independent. Thus it can be applied to a variety of networks with a regular or irregular topology. To exemplify the potential of our technique, derivation of a closed-form formula for computing the delay bound for all-to-one communication is detailed. Our experiments demonstrate the correctness and tightness of the calculated bounds compared with simulated results. We conclude that derivation of the delay bound for trafﬁc ﬂows in best-effort routed NoCs can be well conducted. Our work has followed the network calculus theory. This theory allows computing buffer bounds in routers as well. We shall dig out this in the future. As we have considered feedforward networks in this work, investigation on feedback networks is another dimension to carry out. This would lead to capture back pressures for given buffer sizes. "
2009,Frequent value compression in packet-based NoC architectures.,"The proliferation of chip multiprocessors (CMPs) has led to the integration of large on-chip caches. For scalability reasons, a large on-chip cache is often divided into smaller banks that are interconnected through packet-based network-on-chip (NoC). With increasing number of cores and cache banks integrated on a single die, the on-chip network introduces significant communication latency and power consumption. In this paper, we propose a novel scheme that exploits  <i xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">frequent</i>   <i xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">value</i>  compression to optimize the power and performance of NoC. Our experimental results show that the proposed scheme reduces the router power by up to 16.7%, with CPI reduction as much as 23.5% in our setting. Comparing to the recent zero pattern compression scheme, the  <i xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">frequent</i>   <i xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">value</i>  scheme saves up to 11.0% more router power and has up to 14.5% more CPI reduction. Hardware design of the FV table and its overhead are also presented.","1A-3 Frequent Value Compression in Packet-based NoC Architectures Ping Zhou (cid:2), Bo Zhao (cid:2), Yu Du †, Yi Xu (cid:2), Youtao Zhang †, Jun Yang (cid:2), Li Zhao ‡ † CS Department (cid:2) ECE Department University of Pittsburgh University of Pittsburgh email:{piz7,boz6,yix13,juy9}@pitt.edu Pittsburgh, PA 15261 email: {ﬁsherdu,zhangyt}@cs.pitt.edu Pittsburgh, PA 15260 ‡ System Technology Lab Intel Corporation Hillsboro, OR 97124 email:li.zhao@intel.com Abstract— The proliferation of Chip Multiprocessors (CMPs) has led to the integration of large on-chip caches. For scalability reasons, a large on-chip cache is often divided into smaller banks that are interconnected through packet-based Network-on-Chip (NoC). With increasing number of cores and cache banks integrated on a single die, the on-chip network introduces signiﬁcant communication latency and power consumption. In this paper, we propose a novel scheme that exploits Frequent Value compression to optimize the power and performance of NoC. Our experimental results show that the proposed scheme reduces the router power by up to 16.7%, with CPI reduction as much as 23.5% in our setting. Comparing to the recent zero pattern compression scheme, the frequent value scheme saves up to 11.0% more router power and has up to 14.5% more CPI reduction. Hardware design of the FV table and its overhead are also presented. I . IN TRODUC T ION Chip Multiprocessors (CMPs) have recently gained popularity in both embedded and high-performance systems. By integrating multiple cores on a single die, CMP can provide better Thread-Level Parallelism (TLP) than single-core solution. To fully exploit the computing power of multiple cores, a large shared on-chip cache is preferred in addition to small private caches. The large cache is often divided into multiple banks that are interconnected through an on-chip network. This type of caches is referred as Non-Uniform Cache Architecture (NUCA) as the cache access time is not the same across different banks. Recent studies showed that it is increasingly important to optimize on-chip interconnection network under chip area and power constraints [7]. Due to its advantages in predictability, reusability and scalability [2] [3], packet-based Network-onChip (NoC) is the most widely used interconnection fabric for CMPs. Unfortunately as the size of on-chip network scales, packet-based NoC introduces signiﬁcant power consumption and communication latency overhead. While many approaches have been proposed to optimize packet-based NoC designs, compressing on-chip data communication has been proven to be beneﬁcial because it reduces the 978-1-4244-2749-9/09/$25.00 ©2009 IEEE 13 on-chip trafﬁc and thus, optimizes both performance and power of the network. Recently R. Das et al. discovered that a few value sequences i.e. patterns occur with very high frequencies in on-chip trafﬁc [11]. They then proposed a scheme to exploit frequent patterns for packet-based NoC architectures [11]. Data values are processed through pattern matching hardware, and are encoded in forms of pattern preﬁx bits to accompany data packets. Among all patterns, the zero value pattern is most effective in achieving desired beneﬁts, and requires the simplest hardware implementation. They also exploited the possibility of combining storage compression (cache compression) and communication compression (network compression) to store data in cache and increase the effective cache capacity at some addition hardware cost. We propose to compress the NoC trafﬁc through exploiting frequent values instead of value patterns. Frequent values refer to a small set of distinct values that present both spatially and temporally in the memory space. When utilized for data encoding on off-chip buses between the chip and memory, frequent values can reduce signiﬁcant bus energy through lowering their switching activities. [4]. Previous effort has also used frequent value compression to reduce the energy consumption for the on-chip shared bus in CMPs [12]. This scheme incorporates a communicating value cache (CVC) to store frequent values, and leverages the snooping bus to mirror the CVC across all cores for consistent compression and decompression. However, this is not applicable to scalable NoCs with packet-based communication (instead of broadcasting) because CVC cannot be easily maintained consistently across all cores. In this paper, we develop a frequent value (FV) based compression scheme for on-chip packet-based NoCs. We use a very small code book for end-to-end communications, and code books among different cores need not be the same. This can help to capture local FVs in addition to global FVs obtained through a uniform code book such as CVC. Overall, the contributions of this paper are: • To the best of our knowledge, this is the ﬁrst work that exploits the usage of Frequent Value compression in packetbased NoC architectures. We develop and compare the effectiveness of four alternative FV replacement policies. 1A-3 Fig. 1. Appearance ratio of top 8 FVs. • Our FV compression scheme reduces the average length of data messages by 24.0% on average — a 15.2% more reduction over the zero pattern scheme [11]. This results in a CPI reduction of up to 23.5%, a 14.5% improvement over the zero pattern scheme on average. FV compression also delivers 10.9% router power savings on average — a 7% more reduction over zero pattern scheme. • Our FV compression scheme is general–purpose, and transparent to both the cache controller and network interface. This means it can be easily adopted in generic CMP/NoC platforms without major architectural changes. In the following section, we discuss the motivation and design of our FV compression scheme. Section III describes our experimental platform, and Section IV presents the results of our simulation. Section V concludes the paper. I I . FR EQU EN T VA LU E COM PR E S S ION ON NOC S A.DynamicFrequentValues Studies have shown that a small number of frequently repeated values (FV) account for a large percentage of on-chip data trafﬁc [4], [12]. For example, Fig.1 presents the ratio of top 8 frequent values when running different applications on a 6x4 packet-based NoC architecture (with the setting described in Section III). From the ﬁgure, up to 77% appeared values are frequent values. An important characteristic of frequent values is that they are highly dynamic. Frequent values change with different workloads, different inputs, and at different runtime intervals. In practice, this helps to achieve better compression rate and better accommodation to different workloads than ﬁxed patterns (e.g. zero pattern [11]). As an example, the data trafﬁc in swim is almost uncompressible using zero pattern while it exhibits 10% compression potential using FV scheme (Fig.1). B.FrequentValueTable To ensure the correct compression and decompression for an end-to-end communication channel, an FV table is maintained in a synchronized way on both sides. When sending a data message, the sender matches the values in the data message with its FV table. Each value with a hit will be substituted with its index into FV table. Otherwise the original value is used. To distinguish between compressed (hit) and uncompressed (missed) values in the encoded message, an extra ﬂag bit is attached to each encoded value. This bit is used to indicate whether the F V 0 m a t c h 0 C 0 F V 1 m a t c h 1 C 1 F V 2 m a t c h 2 C 2 h i t F V 3 . . . . . . m a t c h 3 C 3 e n c o d e . . . . . . . . . . . . 3 i n d e x M U X 3 2 i n p u t Fig. 2. FV control logic. Fig. 3. Pipelined operation of CAM and controller. corresponding value is compressed. Therefore, when using a FV table with 8 entries, the encoded length of a 32-bit value is either 4 bits (hit) or 33 bits (miss). On reception, the receiver decodes the message using its FV table. Since the FV tables on both sides contain the same values with the same indices, the message can be decoded properly. We use Content-Addressable Memory (CAM) structure to realize the FV table. We implemented the FV table circuit and compressor/decompressor logic using Verilog HDL and synthesized the hardware with 45nm standard cell library [15, 16]. Fig.2 illustrates the logic of FV table controller. The CAM and control circuits operate in a pipelined way, so that the total cycles needed to process N values are N +2. Fig.3 shows the pipelined operation. For an 8-entry FV table whose compressor and decompressor circuits work under 1GHz clock frequency, our results showed that the area overhead and dynamic power consumption of the compressor circuit are 4438.8μm2 and 4.26mW , respectively; the area overhead and dynamic power consumption of decompressor circuit are 4320.2μm2 and 3.95mW , respectively. Data messages are packaged into network ﬂits before being injected into the network. Since values are sequentially processed, we integrate the FV compressor into the packaging stage in each node’s network interface device, and overlap the latency of compression and packaging operations. This helps to reduce the visible latency of FV compression to two cycles. Similarly, decompression of a message can start as soon as the ﬁrst data ﬂit arrives. Decompressed values are pushed out while following ﬂits are being received and processed at same time. Fig.4 shows an example when decompressing a four-ﬂit message. 14 1A-3 0 1 2 3 4 5 E n c o d e d V a l u e s D e c o m p r e s s e d V a l u e s Fig. 4. Overlapping decompression with unpackaging. V i s i b l e L a t e n c y C.FVReplacementPolicies To capture dynamically appeared frequent values, the FV table needs to be updated periodically at runtime. The update should be synchronized to maintain the same table content on both the sender and the receiver sides. When replacing new values into the FV table, two factors must be considered. On one hand, replacing new values more aggressively leads to faster adaptivity to workloads’ dynamic behavior. On the other hand, however, it is desirable to give old values enough time to take effect before they are evicted. We found that different FV replacement policies exhibit different trade-offs. We implemented and evaluated four different FV replacement policies as follows: • Counter-based replacement With this replacement policy, each entry in the FV table contains a 32-bit value and an 8-bit counter. To compress a data message (e.g. a cache line) that contains multiple 32-bit values, the controller tries to match each value in the FV table. To facilitate the discussion, we introduce following terms. A hit-value is a value in the data message that is also in the FV table; a miss-value is a value in the message but not in the table; a hit-entry is an entry in the FV table whose value appears in the current message; a miss-entry is an entry in the table whose value does not appear in the current message. To update the counter, the controller processes the values in the data message one by one. A hit increases the corresponding counter by two while multiple appearances of the same value update the same hit-entry multiple times. At the end of processing, the counters of all miss-entries are decreased by one. The counter value ranges from 0 to 255; it does not overﬂow or underﬂow i.e. increasing a counter with value 255 gets 255 while decreasing a counter with value 0 still gets 0. After processing all values, the controller tries to update the table based on counter values. If it can ﬁnd a missvalue, and an entry whose counter is zero (it must be a miss-entry), then the missed value replaces the current value of the miss-entry. The above processing repeats until no distinct miss-value or no zero-counter entry can be found. The replacement process is not on the critical path. • Approximate LRU/one replacement per message Using this policy, each entry in the FV table is comprised of a 32-bit value and an 8-bit timestamp. The timestamp is updated as follows. When processing values in the message, each hit left-shifts a bit-1 to the corresponding timestamp. At the end of the processing, each miss-entry leftshifts a bit-0 to its timestamp. This update policy is similar Fig. 5. Normalized message length of different FV replacement policies. to the counter-based policy except that the timestamps of miss-entries decrease much faster. After processing all values in the data message, the FV controller scans the FV table to ﬁnd one entry with zero timestamp and replaces it with a miss-value, if both the entry and the miss-entry can be found. • Approximate LRU/one replacement per value Similar to the above policy, each entry in the table has a 32-bit value and an 8-bit timestamp. However, timestamp update and table replacement decisions are performed per each value instead of per message: as each value in the data message is being matched to the FV table, timestamps of FV table entries are updated based on matching results. If current value is a miss, the FV controller ﬁnds an entry with zero timestamp and replaces it with current value. This is an aggressive policy that evicts old values very quickly. • Approximate LRU/one replacement per group This policy is a compromise between the above two policies. Values in a data message are divided into several groups e.g. each group contains 4 values. The update and replacement are performed per group instead of per message or per value. We evaluated the effectiveness of the four FV replacement policies with multiple workloads (the settings are described in section III). Fig.5 compares the normalized data message lengths of these four policies. As we can see, the counterbased policy has the best average compression rate, therefore, we adopt it in our following study. The results are also used in distributed random trafﬁc to study the effects of different compression schemes. D.FrequentValueCompression forPacket-basedNoCs As we discussed, to ensure the correct compression and decompression for an end-to-end communication channel, the updates to FV tables on both ends must be synchronized. It is easy to achieve for bus interconnect as all nodes attached to a snooping bus can see all sent and received messages in the same order and update their local FV tables accordingly. However the synchronization becomes more complicated in a mesh NoC. Given an end-to-end communication node pair, one node can send out messages without knowing the status of the other one. Therefore the nodes on two ends may see different orders of their exchanged messages. In Fig.6 the order of processed messages on A is A1 → A2 → B1 while on node B the order is A1 → B1 → A2. To remove this problem, we 15 1A-3 N o d e A S e n d M s g A 1 S e n d M s g A 2 N o d e B Re ce i ve M sg A1 TABLE I S IMU LAT ION MOD E L S e n d M s g B 1 Component Parameters Re ce i ve M sg B1 Re ce i ve M sg A2 Fig. 6. Interleaved messages in bidirectional communication. Fig. 7. On-chip network layout. maintain two FV tables for each node pair — one for A-send/Breceive and the other one for A-receive/B-send i.e. both node A and B get A1 → A2 and B1 for the two tables respectively. Since a node on a CMP processor may simultaneously communicate with multiple other nodes, it needs to maintain two distinct FV tables for each node it talks to. Although it might be possible that several nodes share a same FV table, the complexity of synchronization and the need for a centralized controller offset the beneﬁts it might bring. Although each node has multiple FV tables, only those that correspond to current communication channels are active. I I I . EX P ER IM EN T A.SimulationModel We simulated an 8-core CMP using Simics [13] full system simulator and GEMS [8] toolset. Each core has a 32KB private L1 cache; all cores share an 8MB L2 cache that is divided into 16 512KB banks each with a local directory. The 16 L2 banks together with 8 cores are interconnected using a 6x4 mesh NoC (Fig.7). We used a state-of-art ﬁxed pipeline router with deterministic X-Y routing algorithm in the NoC. The details are summarized in Table I. Since zero pattern takes majority of frequent patterns in data trafﬁc [11], and is simple to implement in hardware, we set zero pattern scheme as the baseline and compared our FV compression scheme with it. B.Workloads We collected a diverse set of parallel computation workloads from SPLASH2 [9] and OpenMP2001, as well as server workload. The details of workloads are speciﬁed in Table II. In all simulation runs, we skipped the initialization phase to avoid biased data values. Processor Cores Eight cores, each at 1GHz clock. L1 Cache L2 Cache Memory Network Router FV Table Each core has a 32KB private L1 cache, split I and D cache, 4-way set associate, 64 byte per line, 3 cycles access time. Shared L2 Cache divided into 16 512KB banks, 8way set associative, 64 bytes per line, 6 cycles access time. 4GB RAM, 200 cycles access time. 6x4 mesh network, with 8 cores on two sides and 16 L2 banks in middle (Fig.7). Fixed ﬁve-stage router with X-Y routing, 2 VCs per PC, 64-bit ﬂits. Routers run at same frequency as processor (1GHz). An unencoded data message is comprised of 1 header ﬂit and 8 data ﬂits which stores a cache line. Each table has 8 entries, and use counter-based replacement policy. TABLE II WORK LOAD S Benchmarks Description Barnes Ocean FMM Raytrace Standard 8 processor input set. 514x514 ocean with 8 processors. 8 processor input set with 16384 particles. 8 processor, 256MB shared memory and 2-pixel antialiasing. Volrend 8 processors, standard input set (“head”). Water-Spatial Standard input set for 8 processors. Swim Mgrid Apache 8 processors, reference input set. 8 processors, reference input set. We use Apache 2.2.6 for Solaris 9 with default conﬁguration, and use SURGE [10] to generate requests (16 clients, 100 threads per client). C.DistributedRandomTrafﬁc In order to fully evaluate our FV compression scheme under various NoC trafﬁc conditions, we developed a tunable trafﬁc generator which is capable of generating uniform distributed random trafﬁc at speciﬁed rate. Such random trafﬁc exists at the background of the benchmarks under test and does not affect their execution. Their existence emulates the situation when there are certain levels of network trafﬁc contention due to multi-threading, multiple workloads etc. that are overly time-consuming in full system simulation. When conﬁgured with a random trafﬁc rate between 0 and 1, a node can generate distributed random trafﬁc so that: G ≈ T ∗ R where T is the time (cycles) elapsed since simulation is started, R is the random trafﬁc rate and G is the number of random trafﬁc ﬂits generated. These random trafﬁc ﬂits are sent with a 16 1A-3 Fig. 8. Router energy saving % under random trafﬁc rate. Fig. 9. CPI reduction % under random trafﬁc rate. special ﬂag so that they can be specially handled at receiver side without disturbing the timing model of the Simics simulator. The data messages injected by the random trafﬁc generator conforms with the value characteristics that we observed in different benchmarks i.e. the compression rate is kept at about the same as (Fig.5) when simulating different compression schemes. IV. R E SU LT S Fig. 10. Normalized data message length. A.RouterEnergySaving To compare the router energy saving of FV compression and zero pattern, we ran both schemes with same workloads under random trafﬁc rate 0.39 (0.39 ﬂit/cycle per node on average). We compared their router energy savings against non-compression scheme and summarized the results in Fig.8. From the ﬁgure, FV compression yields as much as 16.7% router energy reduction, and up to 11.0% more reduction than zero pattern. To measure the energy overhead of FV compression, we calculated the average energy consumption for transmitting one data message using the Orion [14] power model (at 45nm), and compared it with the energy consumed by FV compression and decompression. Our results show that the compression and decompression energy per message is 0.148nJ while the average energy for a message to traverse the network is 3.56nJ . Therefore the energy overhead from FV compression operations is reasonable comparing to its router energy savings. B.Performance Improvement We then measured the performance changes using FV compression and zero pattern respectively. We ran both schemes with same workloads under random trafﬁc rate 0.39, and compared their reduction of Cycles-Per-Instruction (CPI) number against non-compression scheme in Fig.9. Our simulation results show that FV compression yields up to 23.5% CPI reduction, and up to 14.5% more reduction than zero pattern compression. C.CompressionRate Next we studied the average length of data messages using FV compression and zero pattern compression (Fig.10). From the ﬁgure, FV compression yields up to 42% more reduction, and 15% more reduction on average. In particular, FV compression adapts to a larger variety of workloads, e.g. for Barnes, zero pattern compression increases the length by 8% while our FV compression achieves 34% length reduction. D.LatencyReduction We also ran simulation to collect the average latency per ﬂit (including network latency and queuing latency) under different random trafﬁc rates. We compared the latency trend of different schemes in Fig. 11. Our results show that FV compression reduces average latency and signiﬁcantly defers the saturation point. V. CONC LU S ION In this paper, we examine how FV compression can be applied in NoC architectures. We evaluated four FV replacement policies from which we picked up the counter-based policy as it provides the best average compression rate. We also explored the hardware implementation of FV table as well as its area and energy overhead. Our simulation results indicate that the FV compression scheme reduces message length by as much as 49%, 15% more comparing to zero pattern on average. As a result, FV compression delivers up to 11.0% more router energy savings and up to 14.5% more CPI reduction over zero pattern (under random trafﬁc rate 0.39). Fig. 11. Average latency under different random trafﬁc rates. 17 1A-3 In addition, our FV compression scheme is transparent to both cache controller and network interface. Thus it can be easily adopted into existing CMP/NoC architecture without major architectural change. "
2010,Efficient throughput-guarantees for latency-sensitive networks-on-chip.,"Networks-on-chip (NoC) for future multi- and many-core processor platforms face an increasing diversity of traffic requirements, ranging from streaming traffic with real-time requirements to bursty best-effort. The best-effort traffic usually results from applications running on general-purpose processors with caches and is very sensitive to latency. Hence, the NoC must provide guaranteed services to some traffic streams, while maintaining low latency and high throughput of best-effort traffic. In this paper, we propose a run-time configurable NoC that enables bandwidth guarantees with minimum impact on latency for best-effort traffic. This is achieved by prioritization and distributed traffic shaping of best-effort traffic. The analysis and evaluation of our quality-of-service scheme show that it can provide tight bandwidth guarantees for streaming traffic. At the same time, the average latencies of best-effort traffic improved by up to 47% compared to a standard prioritization scheme.","6C-3 Efﬁcient Throughput-Guarantees for Latency-Sensitive Networks-On-Chip Jonas Diemer∗, Rolf Ernst Institute of Computer and Network Engineering Technische Universit ¨at Braunschweig, Germany e-mail: {diemer, ernst}@ida.ing.tu-bs.de Michael Kauschke Intel Germany e-mail: michael.kauschke@intel.com Abstract— Networks-on-chip (NoC) for future multi- and many-core processor platforms face an increasing diversity of trafﬁc requirements, ranging from streaming trafﬁc with realtime requirements to bursty best-effort. The best-effort trafﬁc usually results from applications running on general-purpose processors with caches and is very sensitive to latency. Hence, the NoC must provide guaranteed services to some trafﬁc streams, while maintaining low latency and high throughput of best-effort trafﬁc. In this paper, we propose a run-time conﬁgurable NoC that enables bandwidth guarantees with minimum impact on latency for best-effort trafﬁc. This is achieved by prioritization and distributed trafﬁc shaping of best-effort trafﬁc. The analysis and evaluation of our quality-of-service scheme show that it can provide tight bandwidth guarantees for streaming trafﬁc. At the same time, the average latencies of best-effort trafﬁc improved by up to 47% compared to a standard prioritization scheme. I . IN TRODUC T ION As more and more processors and IP cores are being integrated in current and future multiprocessor systems on chip (MPSoC), networks-on-chip become increasingly important as a scalable and modular interconnect, e.g. [10, 17]. These MPSoCs will be used in multi-functional consumer and mobile devices, where they will execute different application classes in parallel, such as streaming multimedia and general-purpose productivity applications. This imposes new challenges to the NoC, as it must accommodate applications with very different characteristics and requirements. Trafﬁc originating from streaming applications imposes a high load and often requires a guaranteed throughput (GT), but at the same time can tolerate high latency due to its predictable access patterns. On the other hand, trafﬁc from general-purpose (GP) applications running on processors with caches does not require timing guarantees and is hence provided best-effort (BE) service. However, an increase in latency often reduces the GP application performance dramatically (as has been shown e.g. in [16]), so the GP trafﬁc can be considered latency-sensitive. Furthermore, the characteristics of trafﬁc from GP applications are usually unknown, and accesses often occur in unpredictable bursts. An efﬁcient co-execution of such diverse application types is crucial to the success of these architectures, as it enables new usage scenarios and adds value. As an example, consider Mobile Internet Devices, which are used for communi∗ This work was supported by Intel Corporation. 978-1-4244-5767-0/10/$26.00 2010 IEEE 529 cation (hard real-time), multimedia playback, content creation and augmented reality (soft real-time), as well as ofﬁce applications (best-effort), potentially all at the same time. Existing NoCs with quality-of-service prioritize real-time trafﬁc over best-effort trafﬁc and hence do not take into account the latency sensitivity of many best-effort applications. A major difﬁculty is that the characteristics of best-effort trafﬁc are highly volatile and difﬁcult to determine. Also, guarantees given to other trafﬁc must not be jeopardized when improving best-effort latency. The goal is to give best possible latency to the best-effort trafﬁc as long as the throughput guarantees are not violated. Furthermore, the design should allow conﬁguration of the QoSmechanisms at run-time to allow ﬂexible use of the system. The NoC proposed in this paper addresses these problems by allowing throughput guarantees for speciﬁc trafﬁc streams while retaining low latency of best-effort trafﬁc. In contrast to existing approaches, we achieve this by prioritizing best-effort trafﬁc over guaranteed throughput trafﬁc while limiting the bandwidth allocated to the best-effort trafﬁc such that enough resources remain to meet the guarantees. The bandwidth limitation is implemented by distributed trafﬁc shapers at the output ports of each NoC router, which allows a ﬁne-grained bandwidth limitation that is independent of the characteristics of best-effort trafﬁc. The paper is organized as follows. In section II, we give an overview over related work. Our proposed approach is described in detail in section III, which includes a formal analysis of the throughput guarantees. We demonstrate our approach experimentally in section IV before we conclude in section V. I I . BACKGROUND AND R E LAT ED WORK As a baseline, we assume a packet-switched canonical router architecture [5] shown in Fig.1, which is commonly used in related work. The baseline router has no support for quality-ofservice. Separate virtual channels (VC) in the input modules store incoming packets from different trafﬁc streams and prevent head-of-line blocking. There is no buffering at the outputs. Flow control follows the wormhole scheme, which means that buffer space and crossbar switch are allocated on a ﬂit1 basis, reducing buffer requirements compared to a cut-through ﬂow control. A credit-based scheme is used for ﬂow control between routers. Hence, the routers have four ports to attach to 1A ﬂit (ﬂow-control digit) is the basic unit of ﬂow control. Packets are made up of a header ﬂit, multiple body ﬂits and a tail ﬂit. 6C-3                          	 	                     	                                    Fig. 1.: Baseline router architecture with proposed trafﬁc shapers (shaded). the neighboring routers and typically one or two up-links for clients such as processor and cache-tile. In this paper, we use a mesh topology, which is often used in other state-of-the-art designs because it is relatively simple and easily mapped to a 2D technology. However, our mechanisms are not speciﬁc to this topology. The baseline router has four pipeline stages. We do not consider speculation and lookahead mechanisms like [11], which optimize best-effort latency and do not incorporate quality-of-service. They can be considered orthogonal to our approach in the sense that the latency optimizations can be integrated with our quality-of-service mechanisms. In fact, as we reduce the contention latency of BE trafﬁc, the relative effect of our approach will be even larger when combined with lowlatency routers. In this paper, we focus on two trafﬁc classes: BE and GT. Finer-grained classiﬁcations are often employed in the embedded domain [7], where application requirements are known in detail at design time. This is usually not the case for generalpurpose multiprocessors used in consumer electronics. However, it may be beneﬁcial to subdivide the best-effort class into latency-sensitive (e.g. cache-miss request) and latencyinsensitive (e.g. write-back), as proposed in [13]. Existing on-chip networks enable support for quality-ofservice either by static pre-allocation of time-slots (e.g. [8, 15]) or by reservation of VC and dynamic scheduling (e.g. [3, 4, 7, 12]). The static scheduling results in high service latencies, especially for best-effort trafﬁc, because it has to wait for the next free time slot. Also, adjusting the QoS parameters to changing trafﬁc requirements can become complex for static schedules. Hence, static QoS does not seem optimal under the presence of latency sensitive best-effort trafﬁc. In dynamic QoS, on the other hand, priorities are used along with scheduling analysis to provide service guarantees. Most of the existing NoCs with QoS support have in common that they favor trafﬁc with service requirements over best-effort trafﬁc. In other words, besteffort trafﬁc may only be sent when there is no other trafﬁc to be serviced. This leads to a noticeable increase of the average BE latency when there is lots of trafﬁc with service requirements. In return, the reduced latency implied by prioritizing GT trafﬁc is of no value, as it usually has large latency margins. Our approach is comparable to Bjerregaard [3], which uses an admission control stage combined with a static-priority queue to give throughput and latency guarantees. However, best-effort trafﬁc is treated independently and may only use otherwise-idle bandwidth. Furthermore, throughput guarantees cannot be arbitrarily chosen and depend on the number of available VCs and the VC allocation. A scheduling strategy which is similar to our approach can also be found in some memory controllers with QoS support (e.g. [9, 1]). These also use a static priority schedule to control latencies and rate regulation (i.e. trafﬁc shaping) at the input queues to separate independent streams and hence provide bandwidth guarantees. However, while memory controllers regulate trafﬁc at a single point, our scheme uses distributed trafﬁc shapers at the network routers. A very preliminary stage of our work was published as interactive paper in [6], but that approach was restricted to highoverhead virtual cut-through switching and did not provide an experimental evaluation. I I I . QOS -AWARE L INK ARB I TRAT ION SCH EM E In this section, we describe how we provide a quality-ofservice class for guaranteed-throughput trafﬁc. The key idea is to give priority to BE trafﬁc for optimal latency and at the same time limit its rate to leave enough free bandwidth for the GT trafﬁc. This mechanism is implemented in the switch allocator of each router, as shown in Fig.1. We arbitrate for the switch and output port together, so every output port requires a separate arbiter. Each of these consists of two sub-modules: a selective priority arbiter and a token bucket trafﬁc shaper. The shaper itself consists of a bucket of tokens, each of which is worth one cycle on the switch/link. Tokens are added to the shaper at an average token rate rt oken . The selective priority arbiter grants requests from BE VCs ﬁrst as long there are tokens in the shaper’s bucket. Consequently, the token rate determines the maximum average rate of prioritized BE trafﬁc. For every BE ﬂit sent, one token is removed from the bucket. When the shaper is depleted, the GT trafﬁc is prioritized, so it is guaranteed an average rate rGT = 1 − rt oken (with a link capacity of 1). A GT connection between two routers can be set up by adjusting the trafﬁc shaper settings on the corresponding output ports along the route of packets between those routers. To efﬁciently shape BE trafﬁc only on the affected routes, routing must be deterministic for GT trafﬁc. We use distributed dimension-ordered XY-routing in this paper. If adaptive source-routing is used instead, alternative routes can be set up to improve load balance, but the same route must be used for every packet of a GT connection. Best-effort trafﬁc can even use a fully adaptive routing scheme. The workings of the two sub-modules are described in detail in the following sections. A. Selective Priority Arbiter Fig.2 shows our selective priority arbiter for one speciﬁc output port. Requests ri from VCs of all other input modules are asserted along with their corresponding priority pi . The NOR530 	 	 	 	  ( ) *   ""     ! "" # $% &  &             	      	   ! '   '             	            6C-3   !""  #$ 	   	 	  %	& $	 $	     !"" %	& # 	 #	   &           	 	  		  		 Fig. 2.: Selective Priority Arbiter Fig. 3.: Token Bucket Shaper. MAL priority corresponds to BE trafﬁc, while the LOW priority is used for GT trafﬁc. These requests are demultiplexed based on their priority and fed to the corresponding arbiter, which selects one of the requesting signals in each cycle and generates a corresponding grant signal. For the LOW priority, a round-robin arbitration is used. For the NORMAL priority, we use a “winner-takes-all” arbiter, which is similar to a roundrobin arbiter, but maintains a grant until a request is no longer active (i.e. until the end of a packet). This improves average latency, as packets are sent in one piece if possible [5]. The grant signals generated by the respective arbiters are qualiﬁed by a priority selection logic, which either asserts gNORM or gLOW based on whether there are any incoming requests on the respective priority. The state of the “allow prioritization” signal aN determines whether NORMAL or LOW requests are favored. OR-gates ﬁnally combine the qualiﬁed grant signals back to a single signal to be used to control the switch. In summary, the proposed arbiter selects a NORMAL priority request if aN is asserted, and a LOW priority request otherwise. If there are no requests of a speciﬁc priority, requests from the other priority are selected regardless of aN . This means that GT is allowed to send if the link is otherwise idle, but also enables BE trafﬁc to use unused reserved GT bandwidth, avoiding waste of over-allocated bandwidth. B. Trafﬁc Shaper As explained above, the “select priority” signal S of the selective priority arbiter is controlled by a trafﬁc shaper. The shaper itself consists of a bucket which can hold b tokens. Each token is worth one cycle on the switch/link. Every T cycles, c tokens are added to the bucket with tokens exceeding the bucket’s capacity being discarded. This results in an average token rate rt oken = c/T . The bucket size b controls how many unused tokens can accumulate and hence inﬂuences the maximum length of a prioritized BE burst. Since at most one token can be consumed per cycle, c ≥ T is equivalent to an unlimited BE rate or no throughput guarantees. Also, no more than b tokens can be added each period, i.e. T ≤ b. Fig.3 shows the implementation of the trafﬁc shaper. Its main components are registers for the current bucket level and the tokens per period c, a timer with a period T , and an adder with saturation logic. The adder is used to update the bucket, i.e. remove 1 token on a grant and add c tokens when the timer overﬂows. The case when both happen at the same time is handled by restricting c to odd values. Only the C − 1 most signiﬁcant bits are stored in the register. An overﬂow during addition of tokens is handled by a multiplexer which selects a constant b (the bucket size) if the adder indicates an overﬂow. Because the bucket is only decremented if grant token is asserted, it can never underﬂow and hence, no underﬂow logic is required. C. Analysis of Shaper Parameters In this section, we show how bandwidth guarantees can be given by the mechanisms described above. We have already seen that the average rate guaranteed for a GT stream is rGT = 1 − c/T . However, up to b tokens can accumulate in the bucket, allowing a BE burst during which GT trafﬁc does not receive its average rate. We can derive the maximum time tsha per d e pl et e it takes for the shaper to deplete under the condition of a continuous BE burst. This is equivalent to the maximum time that BE trafﬁc can block a single GT stream. In the worst case, the bucket is full and there is a continuous stream of BE trafﬁc (i.e. one token is removed every cycle). The ﬁrst addition of new tokens happens after c cycles, when just enough tokens have been taken from the bucket so that no tokens have to be discarded. From these considerations, tsha per d e pl et e can be expressed as shown in Eq.1, which can be solved by iteration until a ﬁxed point is reached, starting from tsha per d e pl et e = b. (cid:2) tsha per d e pl et e − c (cid:3) · c (1) tsha per d e pl et e = b + T During the time GT trafﬁc is blocked, it does not receive its requested average throughput and incoming GT ﬂits must be buffered. These can be sent later when BE trafﬁc is blocked by the shaper or when there is no BE trafﬁc to be sent. In that case, the GT trafﬁc will temporarily receive more than its requested average throughput and can catch up again. Note 531           6C-3 that it is absolutely required that GT trafﬁc which was blocked earlier is sent now, because new tokens accumulate for the next BE burst. This can only be guaranteed if blocked GT trafﬁc is buffered locally (at the router’s input queue) and not in some upstream buffers, because trafﬁc shapers are not synchronized between routers, and hence trafﬁc may be blocked upstream. The required minimum buffer size sGT for GT trafﬁc is: (cid:4) sGT = rGT · tsha per d e pl et e (cid:5) = (cid:6) (cid:7) (1 − c T ) · tsha per d e pl et e (2) In Fig.4, the maximum shaper depletion time and minimum buffer size are shown for different bucket sizes b and different token rates c/T (= maximum BE rate). The data for the plots were obtained by solving equations 1 and 2 for b = [2, 32], T = [2, b], and c = [0, T ). The shaper depletion time in Fig.4a grows moderately with bucket size, but rapidly with increasing token rate. The minimum buffer size, however, has no such dependency on the token rate and grows about liner only with the bucket size, as can be seen in Fig.4b. The reason is that at the same time as the shaper depletion time grows with an increasing token rate, the average GT rate decreases and hence less trafﬁc needs to be buffered. Fig.4c shows different buffer sizes for varying token rates c/T (x-axis) and a ﬁxed bucket size of 16, which is a slice through Fig.4b at the red line. Different colors represent different settings for T . As can be seen, the minimum required buffer size peaks for a token rate in the range of (0, 0.5), i.e. if GT requests more than half of the link bandwidth. However, there are alternative conﬁgurations for c and T that yield very similar token rates but reduce the buffer requirements. Hence, by a clever choice of c and T , we can minimize the buffer requirements. Setting T to b is Paretooptimal in terms of buffer size. This, on the other hand, has an effect on the BE latency, as tokens are generated less frequently, which we will explore in Section IV. The worst case per-hop delay for a single GT stream td el ay GT can be calculated similarly to the maximum bucket depletion time. As arbitration between GT streams is done round-robin, a speciﬁc GT stream can be blocked at most once from every other GT stream sharing the same output. For nGT overlapping GT streams, the resulting blocking time is: td el ay GT = b + nGT − 1 + (cid:2) td el ay GT − c T (cid:3) · c (3) D. Packet Concatenation As explained above, a router must be able to send GT ﬂits that have been blocked when the corresponding output is available, so a minimum local VC buffer space must be reserved for GT trafﬁc. A standard router implementation, however, only allows one packet at a time in a VC because there is only one set of status registers per VC. Hence, a VC can only be arbitrated for a new head ﬂit after the tail ﬂit of the previous packet has been sent. This creates “bubbles” between packets during which no data is available for reading from a given VC, reducing the effectively usable space of a VC. If there is only a single VC reserved for a GT stream, the assumption that blocked GT ﬂits can always be buffered and resent later during idle periods is no longer satisﬁed. Consequently, throughput guarantees can only be given if enough VCs are reserved for each GT connection, the exact number depends on the minimum packet length. We overcome this limitation by “packet concatenation”. With this technique, we do not generate head or tail ﬂits for intermediate GT packets in the network interface. Instead, a new “delimiter” ﬂit type is introduced that marks the end of a packet so individual packets can be reconstructed at the receiving end. Delimiter ﬂits are treated as regular body ﬂits in the routers, i.e. they do not induce routing or VC allocation (like head ﬂits) nor do they free a VC (like tail ﬂits). In other words, packets of a GT connection are concatenated into a single huge worm. This has the effect that a VC remains allocated to the GT stream throughout the complete lifetime of the connection avoiding unnecessary routing and VC allocation stages. Furthermore, packet concatenation allows a completely dynamic allocation of VC to GT connections without any additional logic in the VC allocator of the baseline router. A VC is reserved by sending the ﬁrst packet starting with a regular header ﬂit and released by sending the last packet with a regular tail ﬂit. E. Establishing GT Connections In order to establish a GT connection, two things must be done: First, set of VCs must be reserved along the path of the connection. Second, the output trafﬁc shapers must be adjusted on every output port along the GT connection including the uplink at the destination. This can achieved either by a central conﬁguration instance or using a distributed setup protocol as described in [6]. When multiple GT connections overlap, i.e. share the same output port of at least one router, the corresponding shapers must be set up to reserve the combined bandwidth requirements. However, overlapping GT streams can still interfere with each other, because they share the same priority level within which a simple round-robin arbitration is performed. A connection that reserved little bandwidth can “steal” bandwidth from others if it requests more bandwidth than it reserved. This can be avoided by limiting the injection bandwidths of GT streams, so they do not use more bandwidth than they have reserved. A simple implementation is to add trafﬁc shapers in the network interfaces that limit the average bandwidth of every GT stream if it overlaps with other streams. Note that GT connections that don’t overlap with others do not require injection shaping and hence can even be allowed to use more than their reserved bandwidth, resulting in a “SuperGT” service class introduced in [15]. This optimization can be implemented in the centralized reservation scheme, where the central control instance can check whether connections overlap or not. F. Area and Power Overhead Considerations The area overhead of the mechanism presented in this paper is relatively low. In a synthesis on a Xilinx Virtex4 FPGA, a single trafﬁc shaper with 8 bit for c, T and b amounts to 891 equivalent gates. For a 20-input arbiter (5 ports with 4 VC each) including the trafﬁc shaper, 2712 gates are required. In comparison, a plain round-robin arbiter with no QoS requires 583 gates, while a simple prioritized arbiter requires 1821 gates. The most signiﬁcant cost in our approach are the VC 532 6C-3 (a) Shaper depletion time (b) Buffer size (c) Buffer size for b=16 Fig. 4.: Shaper parameters buffers required for the GT trafﬁc. However, as we have outlined above, these are dynamically assigned from the pool of BE VCs, so that buffers not required for GT connections are used by BE trafﬁc. With respect to power, our mechanism has also little overhead. In fact, all added logic only needs to be active on output ports through which a GT connection travels. Also, buffering is only done in the event of contention, where it is absolutely necessary even in a standard approach. We just optimize the selection of which stream should be buffered when. IV. EX P ER IM EN T S We use a cycle-accurate SystemC behavioral model of our proposed router architecture for the evaluation. In all experiments, we model an 8x8 mesh network with four VCs per port, each of which stores up to 16 ﬂits. Flits are 4 bytes wide and traverse the switch/link in one cycle. The total router delay is four cycles. We use synthetic trafﬁc generators to model BE and GT trafﬁc in most experiments, as these allow fast exploration of the design space and cover important corner-cases. If not stated otherwise, packets are generated with a ﬁxed size of 32 bytes, with randomly distributed delay between packets. Trace-driven workloads are used to evaluate the effects on real applications. All experiments were conducted multiple times with varying random seeds to obtain standard deviations shown in the plots. We will look at a scenario with a single guaranteedthroughput trafﬁc stream, which allows the examination of individual effects. These observations can be generalized, as there is nothing speciﬁc to the selected stream. The GT trafﬁc is generated from node (0,3) to node (6,2), at an average of 2 bytes/cycle (50% of the link capacity). Hence, the corresponding BE shapers are conﬁgured to b = T = 8 and c = 4. All other nodes send best-effort trafﬁc with destinations either randomly distributed over the whole network (uniform random) or following the tornado pattern. In the latter case, packets are sent to the node half way across the network (3 hops) to the east direction (with a wrap-around at the network’s edge). Hence, tornado overlaps maximally with the GT trafﬁc. With respect to the arbitration scheme, we run three different scenarios: (1) BE and GT trafﬁc use the same priority. This means that no quality-of-service is provided. (2) GT trafﬁc is prioritized over BE. This is the “na¨ıve” approach which was outlined in the in(a) Medium packets (b) Large packets (c) Medium packets (d) Large packets Fig. 6.: GT-throughput and BE-Latency for different shaper settings troduction. (3) BE trafﬁc is prioritized over GT but is trafﬁc shaped, following the arbitration scheme described above. Fig.5a shows the throughput of the GT stream. It can be seen that throughput drops signiﬁcantly below the requested level with increasing BE load for scenario 1 (no QoS), while it remains at the requested level for scenarios 2 and 3. Fig.5b and 5c show the average latency of BE trafﬁc originating from node (1,3), which lies in the path of GT trafﬁc. Our approach shows a signiﬁcantly reduced latency under low and medium loads compared to prioritizing the BE trafﬁc and even compared to scenario 1 without QoS. For a medium BE load of 0.5 bytes/cycle/node, the average latency improvement is 5.7 cycles (18%) for uniform random and 13 cycles (47%) for tornado trafﬁc. The latter improvement is more signiﬁcant because the tornado pattern exhibits maximum overlap with the GT stream when using static XY-routing. We also observe a moderate increase of network capacity for uniform random trafﬁc because trafﬁc can leave the hot spots more quickly. For a speciﬁc GT rate, there are multiple combinations of b, c, T that yield the same or a very similar token rates. Selecting 533 6C-3 (a) Throughput of GT stream (b) BE latency for Uniform random trafﬁc (c) BE latency for Tornado trafﬁc Fig. 5.: GT throughput and BE latencies for a single GT stream from node (0,3) to node (6,2) a favorable setup can minimize the required buffer size, but also affects the BE burst and latency. To explore these effects, we use the same setup as in scenario 3 above, but vary the bucket size and period, while setting c to T /2 to keep the bandwidth guarantee the same. We do this for different BE packet sizes: medium (8 ﬂits), and large (64 ﬂits). Fig.6a and 6b show that the throughput drops below the requested level if the bucket size exceeds the size of the VCs (16 ﬂits). The reason is that the VC buffers are unable to capture all of the blocked GT trafﬁc during a BE burst, as we have discussed in Sec.C. In Fig.6c and 6d we observe that trafﬁc is quite sensitive to bucket size under higher loads for large packet sizes. A larger bucket size increases the maximum BE burst and hence reduces the average latency. Lowering the period T also has a positive effect on latency as tokens come more frequently and thus a blocked BE packet is served sooner. However, this effect is only marginal for the trafﬁc patterns used, as they distribute their trafﬁc evenly over time. Consequently, setting the period to the bucket size is a good option here to reduce buffer requirements. To evaluate the effect of this latency improvement on a real application, we replace the synthetic trafﬁc generator at node (1,3) by a memory trace of a real application. For this the streamcluster and canneal benchmarks from the PARSECSuite are used, both of which are latency-sensitive benchmarks [2]. The traces are obtained with the pin tool [14] in a singlethreaded conﬁguration using the simsmall working set for canneal and simdev for streamcluster. The node running the traces models a dual-issue processor with 8KiB 2-way-associative L1 data cache with 1-cycle latency and a perfect instruction cache. The processor accesses a perfect L2 cache over the NoC at node (5,3), which is four hops away (plus one hop for the up-link). All caches have a block size of 32 bytes. The L2 cache has 1-cycle access time, hence most of the L2 hit time is attributed to NoC delay. All other nodes inject a medium background load of 0.5 bytes/cycle each. With uniform random background load, the runtime of the memory trace is reduced by 7% for the canneal and 5% for streamcluster when our QoS mechanism is used instead of the simple prioritization of GT trafﬁc. For tornado background load, the run-times improve by 14% and 11% respectively. Furthermore, the standard deviation of trace run-times over multiple randomized runs improves by a factor of 3 to 6 with our approach, which means that run-times are much more predictable even for best-effort trafﬁc. V. CONC LU S ION In this paper, we presented a novel network-on-chip architecture that allows bandwidth guarantees for speciﬁc trafﬁc streams with low impact on contending latency-sensitive besteffort trafﬁc. Our analysis and experiments have shown that the proposed mechanism is capable of tight guarantees. At the same time, the latency of best-effort trafﬁc is improved by up to 47%, which results in a speed-up of up to 14% for the latency-sensitive best-effort applications evaluated in this paper. Providing low latency to best-effort applications is crucial in systems that run workloads with real-time requirements and general-purpose applications concurrently. "
2010,Improved on-chip router analytical power and area modeling.,"Over the course of this decade, uniprocessor chips have given way to multi-core chips which have become the primary building blocks of today's computer systems. The presence of multiple cores on a chip shifts the focus from computation to communication as a key bottleneck to achieving performance improvements. As industry moves towards many-core chips, networks-on-chip (NoCs) are emerging as the scalable fabric for interconnecting the cores. With power now the first-order design constraint, early-stage estimation of NoC power has become crucially important. Existing power models (e.g., ORION 2.0 [12], Xpipes [7], etc.) are based on certain router microarchitecture and circuit implementation. Therefore, when validated against different NoC prototypes - different router implementations - we saw significant deviation (up to 40% on average) that can lead to erroneous NoC design choices. This has prompted our development of a new, accurate architecture- and circuit implementation-independent router power and area modeling methodology with complete portability across existing NoC component libraries. Also, validation against a range of implemented router designs confirms substantial improvement in accuracy over existing models.","3C-4 Improved On-Chip Router Analytical Power and Area Modeling Andrew B. Kahng†‡ , Bill Lin‡ and Kambiz Samadi‡ †CSE and ‡ECE Departments, UC San Diego, La Jolla, CA Abstract—Over the course of this decade, uniprocessor chips have given way to multi-core chips which have become the primary building blocks of today’s computer systems. The presence of multiple cores on a chip shifts the focus from computation to communication as a key bottleneck to achieving performance improvements. As industry moves towards many-core chips, networks-on-chip (NoCs) are emerging as the scalable fabric for interconnecting the cores. With power now the ﬁrst-order design constraint, early-stage estimation of NoC power has become crucially important. Existing power models (e.g., ORION 2.0 [12], Xpipes [7], etc.) are based on certain router microarchitecture and circuit implementation. Therefore, when validated against different NoC prototypes - different router implementations – we saw signiﬁcant deviation (up to 40% on average) that can lead to erroneous NoC design choices. This has prompted our development of a new, accurate architecture- and circuit implementation-independent router power and area modeling methodology with complete portability across existing NoC component libraries. Also, validation against a range of implemented router designs conﬁrms substantial improvement in accuracy over existing models. I . IN TRODUC T ION As diminishing returns in performance of uniprocessor architectures have led to multi-core chips, networks-on-chip (NoCs) are emerging as the scalable fabric for interconnecting the cores. When the number of on-chip cores increases, the need for scalable and high-bandwidth communication fabric becomes more evident [8], [16]. However, with increasing demand for network bandwidth, interconnection network power has become increasingly substantial, e.g., 28% and 36% of the total chip power in the Intel 80-core [10] and Raw [21] processors, respectively. This requires designers to work within a tight power budget. To aid designers in early stages of the design, architectural power models have been proposed for rough power estimations (see Section II). Architectural power estimation is used to (1) verify that power budgets are approximately met by the different parts of the design and the entire design, and (2) evaluate the effect of various highlevel optimizations, which have been shown to have much more signiﬁcant impact on power than low-level optimizations. Therefore, a number of recent research efforts (e.g., [12], [17], [22]) have focused on developing architectural router power models. However, these models are derived from a mix of template circuit implementations and technology trends, and hence can not guarantee accuracy within an architecture-speciﬁc ﬂow. The main advantages of these models are ﬂexibility and applicability in early design space explorations when the NoC component library is not available. In addition, existing router power models [5], [18] do not consider all relevant microarchitectural parameters, which limits accuracy and applicability. The above shortcomings of the existing models has led us to explore new directions to improve the efﬁciency of these models for early-stage design space exploration. To accomplish our goal, we start from an existing RTL description of a router with any given architecture. We then create a library of fully-synthesizable router RTLs with different microarchitectural parameters. Using an industrial implementation ﬂow we take the RTL descriptions through physical design steps and compute corresponding power and area values. We apply a machine learning technique, i.e., nonparametric regression, using the generated power and area data sets to develop accurate architectural-level router power and area models. The highlight of our modeling methodology is the decoupling of the router microarchitecture and underlying circuit implementation from the modeling effort. One might question the runtime complexity of the approach; however, as technology advances every year, designers ﬁnd themselves with increasing access to multiple, parallel computing resources in the form of multi-processor workstations, multi-core microprocessors, load sharing facilities (LSF), etc. [20]. These ample computing resources can be efﬁciently utilized to enhance modeling accuracy and applicability. The contributions of our work are as follows. 1) We are the ﬁrst to consider decoupling of router microarchitecture and underlying circuit implementations from the modeling effort. This enables a modeling methodology in which both router microarchitecture and underlying circuit implementations are transparent to the system-level designer. 2) We use nonparametric regression to develop accurate router microarchitectural power and area models using power and area data sets gathered from layout information. 3) We propose a modeling methodology for on-chip router power and area that can be used within any existing NoC component library. The remainder of this paper is organized as follows. In Section II we contrast against prior related work. Section III describes our ﬂow implementation for our experiments and the details about our design of experiments. Section IV describes our modeling methodology, while Subsections IV-B and IV-C derive the corresponding power and area models, respectively. In Section V we validate our proposed models against layout data, and show the impact of the new models on achievable NoC conﬁgurations. Finally, Section VI concludes the paper. I I . R E LAT ED WORK Power modeling can be carried out at different levels of ﬁdelity, trading off modeling time with accuracy, ranging from real-chip power measurements [11], to pre- and post-layout transistor-level simulations [29], to RTL power estimation tools [30] to earlystage architectural power models [4], [12], [23]. Low-level power estimation tools, even RTL power estimation, require complete RTL code to be available, and simulate slowly, on the order of hours, while an architectural power model takes on the order of seconds [12]. Circuit-level power estimation tools, though providing excellent accuracy, has even longer simulation times, and require even more substantive development effort. These shortcomings have led to a plethora of architectural power models, such as the widelyused Wattch [4] and SimplePower [23] for uniprocessors. Power models have also been proposed for a variety of on-chip networks. These models can be divided into two categories: (1) models derived from a mix of circuit templates, and (2) models derived from pre- and post-layout simulation results. In the ﬁrst category, Patel et al. [19] ﬁrst proposed an analytical power model for interconnection networks, derived from transistor count. Since the model does not consider the architectural parameters, it cannot be used to explore tradeoffs in router microarchitecture design. ORION 2.0 [12], a set of architectural NoC power models, is an enhanced version of its predecessor, ORION [22], which provides parameterized power and area models derived from a mix of circuit templates and technology trends. ORION models can be more efﬁciently deployed for early-stage design space exploration as they take into account the router microarchitectural parameters. In the second category, power models are based on either prelayout [3], [5], [15], [18] or post-layout [1], [2], [17] simulation results. Bona et al. [3] presented a methodology for automatically 978-1-4244-5767-0/10/$26.00 2010 IEEE 241 3C-4 generating energy models for bus-based and crossbar-based communication fabrics. Lee et al. [15] proposed a high-level power model based on the number of ﬂits passing through a router, and used parametric regression to derive the model. In [5] and [18], cycleaccurate architectural power models are proposed, however, limited dependence of these models on the microarchitectural parameters degrades their applicability for efﬁcient design space exploration framework. Chan et al. [6] used the parametric models proposed in [5] to drive their NoC synthesis cost functions. On the other hand, Banerjee et al. [1] presented an accurate power characterization of a range of NoC routers using standard ASIC tool ﬂow, but failed to present any analytical models. In [2], a RTL-level NoC power model was developed by ﬁrst extracting the SPICE level netlist from the layout and then integrating the characterized values into the RTL description. More recently, Meloni et al. [17] proposed an architectural regression analysis for router power based on energy numbers obtained from post-layout simulations. The existing models assume a speciﬁc architecture and underlying circuit implementation in their modeling efforts, in one way or the other. For example, [12] assumes certain architecture (i.e., VC router) and a set of circuit implementations or [17], even though claims that the proposed models are architecture-speciﬁc, it still requires the modeler to have understanding about the underlying architecture. In addition, most of the current work fail to consider the contributing microarchitectural as well as implementation parameters in their models. In contrast, we develop an architecture- / circuit implementation-independent router power and area modeling methodology. We efﬁciently utilize the accuracy of post-layout simulation results and use nonparametric regression to develop closed-form analytical power and area models. Finally, our models include all the router microarchitectural parameters to enhance its usability at the system-level and efﬁciency for design space explorations. I I I . IM P L EM EN TAT ION F LOW AND D E S IGN O F EX P ER IM EN T S A. Implementation Flow and Tools Figure 1 shows our implementation ﬂow which includes traditional synthesis, placement and routing (SP&R) ﬂow plus power estimation and model generation steps that we have scripted for “push-button” use in our experiments. The steps in Figure 1 represent the major physical design steps. At each step we require that the design must meet the timing requirements before it can pass on to the next step. Router RTL Synthesis Floorplan Placement CTS Routing area reports Power Simulation Model Generation user-defined switching activity Fig. 1. Implementation ﬂow. In the ﬂow, we ﬁrst generate a library of fully-synthesizable router RTLs using Netmaker [27]. We generate the library by choosing a router microarchitecture and sweeping the corresponding microarchitectural parameters space. Then we synthesize RTL codes with worst-case timing libraries. During synthesis process we can set certain optimization objectives among which are: (1) target frequency, (2) target area, and (3) target power. Choosing different combinations of these objectives yields widely different quality of results. Hence, to mimic a typical industrial timing-driven ﬂow, where certain performance constraints must be satisﬁed, we impose the target frequency as the primary objective while area and power minimization are considered as secondary objectives [17]. In addition to optimization parameters, there are some ‘inherent noise’ in IC implementation tools that may be aggravated with respect to tightness of the timing constraint (i.e., target frequency) [13]. Therefore, we implement the designs at a slightly slower frequency than the maximum achievable. To obtain near-maximum clock frequency we run several synthesis simulations with different target frequencies, and pick the clock frequency beyond which area is increased signiﬁcantly without timing improvement. Using synthesized netlist we implement the designs through place & route (P&R) steps. We set the row utilization and aspect ratio of all the designs to 65% and 1, respectively, at the ﬂoorplan stage. Since our goal is to develop microarchitectural router power and area models we do not perturb the row utilization and aspect ratio values after they are ﬁxed. At the end of routing stage we extract the design power/area and associate them with the corresponding microarchitectural values. We use Netmaker v0.82 [27] to generate a library of synthesizable router RTLs. We perform our experiments using libraries in TSMC 130nm, 90nm, and 65nm technologies. Target clock frequencies for the above technologies are 200 MHz, 300 MHz, and 400 MHz, respectively. We use Synopsys Design Compiler v2009.06-SP2 to synthesize the RTL designs and Cadence SOC Encounter v7.1 [31] to execute the P&R ﬂow. Finally, MARS3.0 tool suite is used for model generation [28]. B. Design of Experiments As with any other model characterization task, it is not trivial to obtain a meaningful power data set, as large number of variables determine the ultimate results. Hence, we only choose the microarchitectural parameters that are of interest at the systemlevel. In addition, as explained in Section III-A, we ﬁx certain implementation-related objectives / parameters across the entire design of experiments. In our experiments, we use Netmaker to generate a library of fully-synthesizable parameterized router RTLs. We pick a baseline virtual channel (VC) router in which VC allocation and switch allocation are performed sequentially in one clock cycle. In a VC router the microarchitectural parameters are: • f w: ﬂit-width is the width of link or channel between routers and is speciﬁed in bits • nvc : number of virtual channels • nport : number of input and output ports (e.g., 5 for mesh topology: N, S, E, W, and tile connection) • lbuf : buffer length which is the number of ﬂit buffers per virtual channel (i.e., per input port) L I S T O F ROU T ER M ICROARCH I T EC TURA L PARAM E T ER S U S ED IN OUR D E S IGN O F EX P ER IM EN T. TABLE I Parameter f w nvc nport lbuf Variables {16, 24, 32, 64}-bits {2, 3, 5, 7} {3, 5, 7, 9} {2, 3, 5, 7}-ﬂit buffers Using automation scripts we vary the above parameters and generate corresponding RTL for each combination of parameters. Table I shows the router microarchitectural parameters and the values they take on in our design of experiments. We then implement the RTL codes in three technology nodes using TSMC 130nm G, 90nm G, and 65nm GP libraries. As explained in Section III-A for synthesis step we use a target frequency slightly slower than 242 the near-maximum clock frequency. We also ﬁx row utilization and aspect ratio values to 65% and 1, respectively. According to Table I there are 256 conﬁgurations (i.e., RTL descriptions) per technology node. This implies a total of 768 synthesis and 768 place and route (P&R) runs across all three technology nodes. Due to relatively simple router logic, each synthesis simulation takes up to 3 minutes, and each place and route simulation takes up to 15 minutes on a 2.0GHz Intel Xeon processor. Hence, using 10 CPUs we can perform the entire design of experiments in less than a day. Note that having access to LSF and/or other multiprocessor computing environments (as is the case in large companies) will signiﬁcantly reduce the setup time to generate the necessary power and area data sets for our models. We also note that the router conﬁguration space is quite large. For example, if we assume that buffer length can vary from 1 to 40 ﬂits, number of ports can vary from 2 to 16 (for high radix routers), ﬂit-width can vary from 8 to 128 bits in 8-bit increments, and number of VCs can vary from 1 to 10, then there are approximately 100000 possible router conﬁgurations, which are impractical to explore exhaustively. Our models, which are developed using only a small subset (of conﬁgurations), have very small overhead in terms of number of experiments. IV. MOD E L ING M E THODO LOGY Approximating a function of several to many variables using only the dependent variable space is a well-known problem with applications in many disciplines. The goal is to model the dependence of a target variable y on several predictor variables x1 , · · · , xn given 1 . The system that generates the data is presumed to be described by [9] R realizations {yi , x1i , · · · , xni }R y = f (x1 , · · · , xn ) +  (1) over some domain (x1 , · · · , xn ) ∈ D ⊂ Rn containing the data. Function f captures the joint predictive relationship of y on x1 , · · · , xn , and the additive stochastic noise component , usually reﬂects the dependence of y on quantities other than x1 , · · · , xn that neither controlled nor observed. Hence, the aim of the regression analysis is to construct a function ˆf (x1 , · · · , xn ) that can accurately approximate f (x1 , · · · , xn ) over the domain D of interest. There are two main regression analysis methods: (1) global parametric, and (2) nonparametric. The former approach has limited ﬂexibility, and can produce accurate approximations only if the assumed underlying function ˆf is close to f . In the latter approach, ˆf does not take a predetermined form, but is constructed according to information derived from the data. Multivariate adaptive regression splines (MARS) is a nonparametric regression technique which is an extension of linear models that automatically models nonlinearities and interactions, and is used in our methodology. A. Problem Formulation Given a router microarchitecture X μarch , circuit implementation C circ , we apply MARS to construct router power and area models, p = ˆf (xμarch n ), and n ), respectively. Varidenote router microarchitectural and implementation-related parameters, respectively. The general MARS model can be represented as [24] , · · · , xμarch a = ˆg(xμarch , · · · , xμarch , · · · , ccirc ables xμarch , · · · , xμarch , and ccirc , · · · , ccirc , ccirc 1 1 , · · · , ccirc , ccirc 1 n n n n 1 1 1 I(cid:2) J(cid:3) ˆy = c0 + ci bij (xij ) (2) i=1 j=1 where ˆy is the target variable (i.e., router power and area in our problem), c0 is a constant, ci are ﬁtting coefﬁcients, and bij (xij ) is the truncated power basis function with xij being the microarchitectural parameter used in the ith term of the j th product. I is the number of basis functions and J limits the order of interactions. The basis functions bij (xij ) are deﬁned as 3C-4 (3) (4) (cid:4) (cid:4) = = b− ij (xμarch − tij ) = [−(xμarch − tij )]q + (tij − xμarch )q xμarch < tij 0 otherwise b− ij (xμarch − tij ) = [+(xμarch − tij )]q + (xμarch − tij )q x > tij 0 otherwise where q (≥ 0) is the power to which the splines are raised to adjust the degree of ˆy smoothness, and tij is called a knot. When q = 0 simple linear splines are applied. The optimal MARS model is built in two passes. (1) Forward pass: MARS starts with just an intercept, and then repeatedly adds basis function in pairs to the model. Total number of basis functions is an input to the modeling. Backward pass: during the forward pass MARS usually builds an overﬁt model; to build a model with better generalization ability, the backward pass prunes the model using a generalized cross-validation (GCV) scheme (cid:5) GC V (K ) = 1 n n k=1 (yk − ˆy)2 [1 − C (M ) n ]2 (5) where n is the number of observations in the data set, K is the number of non-constant terms, and C (M ) is a complexity penalty function to avoid overﬁtting. To generate the required data set for power and area modeling we use a library of fully-synthesizable router RTLs and implement them through a complete physical design ﬂow to get placed and routed designs. W then perform switching activity-aware power estimation to accurately estimate power values. Area values can be extracted once we have the placed and routed design. Router microarchitecture impacts its power as it determines the underlying circuit components and their activity. The area of a router is also determined by the chosen microarchitecture and the circuit implementations. We assume that across all possible microarchitectural parameter values, circuit implementation (i.e., ﬂow, optimization objectives, etc.) are ﬁxed. Hence, our proposed models accurately capture the impact of microarchitectural parameters on router power and area within a CAD-speciﬁc ﬂow. B. Power Modeling In this subsection, we describe an architectural-level parameterized power model for network routers given router microarchitecture χμarch and underlying circuit implementations C circ . We derive models for both dynamic and leakage power. Dynamic power is due to charging and discharging of switching capacitances, and leakage power is due to subthreshold and gate leakage currents. Dynamic power consumption in CMOS circuits is formulated as × fclk , with fclk the clock frequency, α the switching activity, csw the switching capacitance, and vdd the supply voltage. Dynamic power can be further divided into two categories: (1) switching power, and (2) internal power. The former (Equation (6)) is the dynamic power consumed by the charging and discharging of the output load external to the cells, while the latter (Equation (7)) is the dynamic power dissipated within cells. 2 α × csw × v pdyn = 1 2 dd pswitching = pinternal = 1 2 1 2 × cload × v dd × fclk 2 × cint × v dd × fclk + vdd × isc 2 (6) (7) where, cload and cint are capacitive and internal loads, respectively. Short-circuit current, isc , is the current between NMOS and PMOS when they are both on. Short-circuit current is noticeable only when the input slews are extremely large. Also, we note that accurate power estimation requires accurate switching activity estimation, 243 3C-4 1 n n dd dd α×v2 α×v2 ×fclk ˆf1 (xμarch 1 , · · · , xμarch , · · · , xμarch pinternal ×fclk ), and ˆcint = ˆf2 (xμarch hence we report power with different switching activity values. Finally, we extract switching and internal power data sets and then normalize the power values with respect to switching activity, supply voltage, and clock frequency. Thus, the goal is to model dependence of pswitching and on microarchitectural parameters. The modeling task becomes one of ﬁnding ˆcload and ˆcint as functions of the microarchitectural parameters: ˆcload = ). As technology scales to deep sub-micron processes, leakage power becomes increasingly important as compared to dynamic power. There is thus a growing need to characterize and optimize network leakage power as well. The largest percentage of static power results from source-to-drain subthreshold leakage current. However, from 65nm and beyond gate leakage gains importance and becomes a signiﬁcant portion of the leakage power. This is even more visible for high-performance applications where gate oxides are much thinner (i.e., ∼1.5nm in 65nm HP library). We consider both subthreshold and gate leakage currents by using the average leakage current from the Liberty. Leakage power is dependent on the supply voltage and temperature, which are deﬁned by the library used (i.e., through the characterization process). Leakage power can be shown as, pleak = vdd × ileak , where ileak is the leakage current which includes both subthreshold and gate leakage currents. To model leakage power, we ﬁrst normalize the leakage power values with respect to the supply voltage, and then estimate the leakage current as a function of microarchitectural parameters, ). Figure 2 shows our proposed router power model for the target router described above in 65nm. Table I shows all the microarchitectural parameters in our models (in Section III-B). ileak = ˆf3 (xμarch 1 , · · · , xμarch n Basis Functions b1 = max(0, nport – 3); b2 = max(0, nvc – 2)×b1 ; b3 = max(0, lbuf – 2)×b2 ; b4 = max(0, f w – 16)×b3 ; b5 = max(0, lbuf – 2); b6 = max(0, f w –16); b7 = max(0, nvc – 2); b8 = max(0, nport – 5)×b6 ; b9 = max(0, 5 – nport )×b6 ; b10 = max(0, nport – 5)×b5 ; b11 = max(0, 5 – nport )×b5 ; b12 = max(0, nvc – 2)×b11 ; b13 = max(0, f w – 16)×b12 ; b14 = max(0, f w – 16)×b7 ; b15 = max(0, f w – 16)×b5 ; b16 = max(0, nport – 7); b17 = max(0, 7 – nport ); b18 = max(0, nvc – 2)×b10 ; b19 = max(0, nport – 5)×b7 ; b21 = max(0, nport – 3)×b15 ; b22 = max(, nport – 5)×b14 ; b23 = max(0, 5 – nport )×b14 ; b24 = max(0, nvc – 2)×b15 ; b25 = max(0, nvc – 2)×b17 ; Power Model p = α × (1.714 + 0.861 × b1 + 0.199 × b2 + 0.180 × b3 + 0.002 × b4 + 0.741 × b5 + 0.055 × b6 + 0.690 × b7 + 0.017 × b8 − 0.007 × b9 + 0.233 × b10 − 0.106 × b11 + 0.120 × b12 + 0.002 × b13 + 0.019 × b14 + 0.012 × b15 + 0.382 × b16 − 0.078 × b18 + 0.224 × b19 + 0.004 × b21 + 0.004 × b22 − 0.003 × b23 + 0.004 × b24 + 0.050 × b25 ) ×v2 dd × fclk Fig. 2. Power model for our target router in 65nm. Another useful contribution of our method is to identify the importance of each of the microarchitectural parameters in the overall router power. To calculate variable importance, we use MARS to reﬁt the model after dropping all terms involving the variable in question and calculating the reduction in goodness of ﬁt (i.e., microarchitectural parameters are independent). The least important variable is the one with the smallest impact on the model quality; similarly, the most important variable is the one that, when omitted, degrades the model ﬁt the most. C. Area Modeling With the increase in number of cores on a single design, the area occupied by the communication components such as links and network routers increases (e.g., 19% of total area in the Intel 80core chip [10]). As area is an important economic incentive in IC (integrated circuit) design, it needs to be estimated early in the design ﬂow to enable efﬁcient design space exploration. In this section we present an accurate model for router area. For each router conﬁguration, we extract the logic area from the corresponding placed and routed netlist. We then use nonparametric regression to develop router area model. The general form of the area model can be shown as: a = f (xμarch 1 , · · · , xμarch n , ccirc 1 , · · · , ccirc n ) (8) 1 1 n n and ccirc , · · · , ccirc , · · · , xμarch where xμarch are router microarchitectural and circuit implementation parameters, respectively. The circuit implementation parameters are deﬁned by (1) design library, and (2) the speciﬁc choice of implementation parameters (e.g., target frequency, aspect ratio, row utilization, etc.). To factor out the impact of ﬂoorplanning on area we use sum of standard cells areas as the router area. The sum of standard cell areas depends on logical depth of different router components. Figure 3 shows our proposed router area model for our target router in 65nm. Basis Functions b1 = max( 0, nport – 3); b2 = max(0, nvc – 2)×b1 ; b3 = max(0, lbuf – 2)×b2 ; b4 = max(0, f w – 16)×b2 ; b5 = max(0, lbuf – 2); b6 = max(0, f w – 16)×b5 ; b7 = max(0, nvc – 2); b8 = max(0, nport – 5)×b7 ; b9 = max(0, 5 – nport )×b7 ; b10 = max(0, nport – 7)×b6 ; b11 = max(0, 7 – nport )×b6 ; b12 = max(0, nvc – 2)×b6 ; b13 = max(0, f w – 16); b14 = max(0, nport – 7)×b5 ; b15 = max(0, 7 – nport )×b5 ; b16 = max(0, nport – 5)×b13 ; b17 = max(0, 5 – nport )×b13 ; b18 = max(0, lbuf – 2)×b9 ; b19 = max(0, nvc – 5)×b10 ; b20 = max(0, 5 – nvc )×b10 ; b21 = max(0, nport – 7); b23 = max(0, nvc – 5)×b1 ; b25 = max(0, nvc – 2)×b13 ; Area Model a = 0.008 + 0.006 × b1 + 0.001 × b2 + 0.0004 × b3 + 2.351e − 5 × b4 + 0.006 × b5 + 0.0001 × b6 + 0.006 × b7 + 0.002 × b8 + 2.736e − 5 × b10 – 1.819e − 5 × b11 + 4.480e − 5 × b12 + 0.0004 × b13 + 0.001 × b14 − 0.0002 × b15 + 0.001 × b16 – 0.001 × b17 + 0.001 × b18 + 9.684e − 5 × b20 – 3.100e − 5 × b21 + 1.073e − 5 × b22 − 3.299e − 6 × b23 + 7.777e − 5 × b24 − 5.265e − 5 × b25 ; Fig. 3. Area model for our target router in 65nm. V. VA L IDAT ION AND S IGN I FICANC E A S S E S SM EN T We now validate the router power and area models described in earlier sections. We compare four different models: (1) our proposed new model (New), (2) a model derived using synthesis data sets, i.e., with the same methodology as in (1) (Synth.), (3) a model derived using parametric regression (Reg.), and (4) ORION 2.0. In (1) and (2) we use nonparametric regression method to develop the models. In (2) switching power is unrealistic due to the use of inaccurate wireload models during synthesis. Also, during P&R stages certain cells are up or down sized accordingly which results in further differences in internal and leakage power between (1) and (2). The relatively large error of “Synth.” models is because we use a different target variable space for model generation (i.e., post-synthesis power values), but evaluate the model at a new target variable space (i.e., post-layout power values). If we evaluate the “Synth.” models with post-synthesis power and area values their accuracy matches that of the new models. To develop “Reg.” model we extract corresponding power and area data sets from the P&R tool for a baseline VC router. The drawback of parametric regression methods (i.e., linear, quadratic regression, etc.) is their limited accuracy since there are no procedures in 244 3C-4 S EN S I T IV I TY O F TH E PRO PO S ED MOD E L S W I TH R E S P EC T TO D I FF ER EN T TRA IN ING / T E S T ING S E T S I Z E S . TABLE II Metric minimum % error maximum % error average % error str = 1/2 0.006 12.415 1.662 str = 1/3 0.006 49.226 4.012 Power Model str = 1/5 0.007 81.112 7.997 str = 1/10 0.01 109.224 27.177 str = 64 0.006 77.321 21.230 str = 1/2 0.0004 14.105 1.814 str = 1/3 0.038 43.099 3.700 Area Model str = 1/5 0.042 78.236 8.568 str = 1/10 0.054 111.384 25.163 str = 64 0.044 61.236 16.417 TABLE III COM PAR I SON O F D I FF ER EN T POW ER AND AR EA MOD E L ACCURACY W I TH R E S P EC T TO IM P L EM EN TAT ION DATA . Metric minimum % error maximum % error average % error New 130nm 0.0112 90nm 0.0081 65nm 0.0067 130nm 62.053 90nm 60.073 65nm 59.413 130nm 6.012 90nm 5.654 65nm 5.817 Power Model Synth. Reg. 30.453 7.659 28.361 7.236 28.854 6.921 47.735 96.512 47.232 92.312 48.343 108.369 42.610 23.463 40.197 25.110 37.494 24.431 ORION2.0 9.526 6.865 7.730 103.214 85.345 81.810 41.326 30.224 32.780 New 0.0011 0.0015 0.0010 60.723 60.151 61.844 5.961 5.019 5.411 Area Model Synth. Reg. 0.062 29.884 0.064 27.821 0.056 29.115 30.231 107.772 29.787 109.236 30.914 111.278 13.143 26.332 12.225 27.113 11.410 26.226 ORION2.0 10.121 8.229 9.111 104.118 88.331 86.228 38.117 32.566 33.298 the modeling methodology to help the modeler understand the interactions between the predictors (i.e., independent variables). To model router power, using parametric regression, we need to understand the speciﬁc circuit implementation used for each of router building blocks (i.e., FIFO buffers, crossbar switch, and VC / switch allocation). In the baseline VC router, FIFO buffers are implemented as ﬂipﬂop registers, and a multiplexer tree crossbar implementation along with combined VC and switch allocation is used. For FIFO buffers, power is linearly dependent on the buffer length (lbuf ), ﬂit-width (f w), number of ports (nport ), and number of VCs (nvc ). As either buffer length or ﬂit-width increases, we expect FIFO dynamic and leakage power to increase linearly. This is because buffer length and ﬂit-width linearly increase the number of ﬂip-ﬂops required. If we increase the number of VCs, buffer dynamic power will not change since the number of ﬂits arriving at each input port is the same. However, we expect leakage power to increase linearly because in VC routers there are nvc queues in each input port. Finally, the number of router ports linearly changes FIFO dynamic and leakage power because addition of a new port will add a new buffer set, i.e., with the same buffer length and ﬂit-width. In a nport × nport multiplexer tree crossbar implementation, power is linearly (quadratically) dependent on ﬂit-width (number of ports), respectively. Buffer length and the number of VCs have no impact on crossbar power. Flit-width linearly increases power since it linearly increases the number of parallel multiplexer needed to implement the f w-bit nport :1 multiplexers. As we increase the number of ports, we expect crossbar dynamic and leakage power to increase quadratically since a nport ×nport crossbar allows arbitrary one-to-one connections between nport input ports and nport output ports.1 In our baseline VC router VC allocation is modeled as VC “selection” instead, as was ﬁrst proposed in [14]. In this approach, the pipeline ﬁrst goes through switch allocation, before selecting an available VC, effectively simplifying VC allocation to reading from a queue of available VCs. Power consumed thus becomes largely invariant to actual number of VCs, however, power is quadratically dependent on the number of ports. This is because the request width for each arbiter, in allocation stage, increases linearly with respect to the number of ports, and also the number of such arbiters is 1Note that we have extracted the power data set from after the P&R stage which includes accurate interconnect switching power. Hence, crossbar power quadratically increases with respect to the number of ports due to port increase in interconnect length. n2 proportional to the number of ports. We use MATLAB [26] to develop the parametric regression-based models using 64 (out of 256) data points for training and the rest of the sample set for testing. For dynamic power we normalize the dynamic power values with respect to switching activity α, supply voltage vdd , and clock frequency fclk and deﬁne the switching capacitance as in Equation (9). For leakage power, we normalize the leakage power values with respect to supply voltage, and deﬁne the leakage current as in Equation (10). Dynamic and leakage power models are then constructed by plugging back the normalizing parameters into the corresponding equations. cswitching = a1 × (lbuf × f w × n 2 port ) + 2 2 port ) b1 × (n port × f w) +c 1 × (n ileak = a2 × (lbuf × f w × nvc × n b2 × (n port × f w) +c 2 × (n 2 2 port ) + 2 port × nvc ) (9) (10) where a1 , a2 , b1 , b2 , c1 , and c2 are ﬁtting coefﬁcients. In both Equations (9) and (10) the ﬁrst, second, and third terms are due to FIFO buffers, crossbar, and VC / switch allocation, respectively. We do not account for the contribution of the control logic to the overall power, and this could degrade the quality of the ﬁt compared against the implementation data. In parametric regression-based techniques the ﬁtting accuracy cannot be enhanced unless the underlying functional form is close approximation of the original function. We also run ORION 2.0 with similar microarchitectural parameters as those of the corresponding implemented router designs. Table III shows a comparison of the aforementioned models against the implemented router designs. We notice the signiﬁcant accuracy improvement of our new models. Table IV shows relative variable importance using post-synthesis and post-layout power data sets. As described in Section IV-B, we use MARS3.0 to ﬁnd the relative importance of each of the microarchitectural parameters in the overall power estimation. We observe that nvc and nport are the dominant parameters with respect to the post-synthesis and post-layout data sets, respectively. This shows the impact of missing layout information at the postsynthesis stage. After synthesis, the RTL design is only mapped to a standard-cell library and does not include detailed wiring information. This results in underestimation of the crossbar power. In a multiplexer tree crossbar, power is due to (1) multiplexers and (2) the interconnection grid between nport input and nport output 245 ports. When the interconnection power is ignored, crossbar power is modeled as O(nport log2 nport ). This changes to a quadratic dependence after layout information becomes available. In our current router microarchitecture, this is the major difference between post-synthesis and post-layout data; however, absolute power values of all microarchitectural blocks differ between post-synthesis and post-layout power data sets. To assess the robustness of our approach we used ﬁve different scenarios to train and test our models. Table II shows minimum, maximum, and average error of our new models based on these scenarios. In the ﬁrst four scenarios we train our models using a fraction str of the available data points, and validate them on the rest of the points. This is to assess the sensitivity of the model to the sampling size. However, in the ﬁfth case, to verify whether the model is general enough we use only 64 (out of 256) data points to train the model, but validate it across all 256 available data points. We observe that as the sampling size decreases, the accuracy of the model also degrades. However, we can obviously trade off model accuracy for additional setup time. In addition, Figures 4, 5, 6, and 7 show the signiﬁcant accuracy improvement of our new model relative to ORION 2.0 when compared against router implementation data. TABLE IV R E LAT IV E VAR IAB L E IM PORTANC E U S ING POW ER DATA S E T S . Parameter Variable Importance (%) Post-Synthesis Post-Layout 92.98 100.00 100.00 95.44 88.41 73.99 67.03 64.81 nport nvc lbuf f w 0.00 5.00 10.00 15.00 20.00 25.00 30.00 35.00 40.00 0 2 4 Buffer length 6 8 R e u o t r t a o t l o p w e r ( W m ) Implementation Model ORION 2.0 Fig. 4. Router total power with respect to buffer length. 0.00 5.00 10.00 15.00 20.00 25.00 30.00 35.00 0 20 40 Flit-width 60 80 R e u o t r t a o t l o p w e r ( W m ) Implementation ORION 2.0 Model Fig. 5. Router total power with respect to ﬂit-width. 0.00 10.00 20.00 30.00 40.00 50.00 60.00 0 2 4 6 Number of ports 8 10 R e u o t r t a o t l o p w e r ( W m ) Implementation Model ORION 2.0 Fig. 6. Router total power with respect to number of ports. 0.00 5.00 10.00 15.00 20.00 25.00 30.00 35.00 40.00 0 2 4 Number of VCs 6 8 R e u o t r t a o t l o p w e r ( W m ) Implementation Model ORION 2.0 Fig. 7. Router total power with respect to number of virtual channels. V I . CONC LU S ION S Accurate estimation of power and area of interconnection network routers in early phases of the design process can drive effective NoC design space exploration. Existing router power and area models (e.g., ORION 2.0 [12], Xpipes [7], etc.) are based on certain architectures and circuit implementations. Therefore, they cannot guarantee maximum accuracy within an architecture-speciﬁc CAD ﬂow. In this paper, we have proposed an efﬁcient router power and area modeling methodology in which the underlying architecture and circuit implementation are decoupled from the modeling effort. In addition, our models show a signiﬁcant accuracy improvement over the existing models, i.e., within 6% (on average) of the implementation data. This enables system-level designers to efﬁciently perform design space exploration without having to know the underlying router microarchitecture and circuit implementation details. We have also presented a reproducible methodology for developing our models given a NoC component library. V I I . ACKNOW L EDGM EN T S Partial support for this research was provided by the MARCO Gigascale Systems Research Center, the National Science Foundation, the Semiconductor Research Corporation, and the UC MICRO program / Qualcomm. "
2010,A3MAP - architecture-aware analytic mapping for networks-on-chip.,"In this article, we propose novel and global Architecture-Aware Analytic MAPping (A3MAP) algorithms applied to Networks-on-Chip (NoCs) not only with homogeneous Processing Elements (PEs) on a regular mesh network as done by most previous application mapping algorithms but also with heterogeneous PEs on an irregular mesh or custom network. As the main contributions, we develop a simple yet efficient interconnection matrix that can easily model any core graph and network. Then, an application mapping problem is exactly formulated to Mixed Integer Quadratic Programming (MIQP). Since MIQP is NP-hard, we propose two effective heuristics, a successive relaxation algorithm achieving short runtime, called A3MAP-SR and a genetic algorithm achieving high mapping quality, called A3MAP-GA. We also propose a partition-based application mapping approach for large-scale NoCs, which provides better trade-off between performance and runtime. Experimental results show that A3MAP algorithms reduce total hop count, compared to the previous application mapping algorithms optimized for a regular mesh network, called NMAP [Murali and Micheli 2004] and for an irregular mesh and custom network, called CMAP [Tornero et al. 2008]. Furthermore, A3MAP algorithms make packets travel shorter distance than CMAP, which is related to energy consumption.","A3MAP: Architecture-Aware Analytic Mapping for Networks-on-Chip  6C-2 Wooyoung Jang and David Z. Pan  Department of Electrical and Computer Engineering  University of Texas at Austin  wyjang@cerc.utexas.edu, dpan@ece.utexas.edu Abstract - In this paper, we propose a novel and global A3MAP  (Architecture-Aware Analytic Mapping) algorithm applied to  NoC (Networks-on-Chip) based MPSoC (Multi-Processor  System-on-Chip) not only w ith homogeneous cores on regular  mesh architecture as done by most previous mapping  algorithms but also with heterogeneous cores on irregular mesh  or custom architecture. As a main contribution, we develop a  simple yet efficient interconnection matrix that models any task  graph and network. Then, task mapping problem is exactly  formulated  to an MIQP  (Mixed  Integer Quadratic  Programming). Since MIQP is NP-hard [15], we propose two  effective heuristics, a successive relaxation algorithm and a  genetic algorithm. Experimental results show that A3MAP by  the successive relaxation algorithm reduces an amount of  traffic up to 5.7%, 16.1% and 7.3% on average in regular mesh,  irregular mesh and custom network, respectively, compared to  the previous state-of-the-art work [1]. A3MAP by the genetic  algorithm reduces more traffic up to 8.8%, 29.4% and 16.1 %  on average than [1] in regular mesh, irregular mesh and  custom network, respectively even if its runtime is longer. I. Introduction  So far, most of the NoC (Networks-on-Chip) based  MPSoCs  (Multi-Processor System-on-Chip)  targeting  general-purposed computing favor a regular mesh network  [1-9]. The regular mesh network lets task-to-tile mapping  easier,  increases routing efficiency, provides desirable  electrical and physical properties and reduces the complexity  of resource management. However, real MPSoCs consist of  various processors together with a DSP (Digital Signal  Processor) and a memory as shown in Philips Nexperia  platform [10], STMicroelectronics Nomadik [11] and Texas  Instruments OMAP [12]. Since the physically different-sized  processor, DSP and memory cannot be floorplanned to a  regular mesh network, a resulting NoC-based MPSoC gets  an irregular mesh network or even a custom network. The  irregular mesh network can be also found in a regular mesh  network when some links become faulty and degraded by  process variation and temperature variation. After the fault  and degradation of link are detected, task mapping and  routing path allocation [13] should deal with the abnormal  links and compensate for the loss of yield and performance.  The previous task mapping algorithms make it inefficient to  perform tasks in the irregular/custom network since they are  not adaptive to the variation of network.   In this paper, we propose an A3MAP (Architecture-Aware  Analytic Mapping) algorithm that is analogous to analytical  traffic minimization in a given NoC-based MPSoC. We use a  metric space that exactly captures the interconnection of  network and that is simple yet efficient for a task mapping  problem  in various networks. In our  task mapping  978-1-4244-5767-0/10/$26.00 2010 IEEE 523 formulation, we seek to embed a task graph into the metric  space of network. Then, the quality of task mapping is  measured by the total distortion of metric embedding.  Through this formulation, our A3MAP can map a task  adaptively  to any different sized  tile both on a  regular/irregular mesh and on a custom network. Fig. 1  shows the methodology of our A3MAP. Given a task graph  and a network as inputs, an interconnection matrix that can  model any task graph and network along interconnection is  generated. Then,  task mapping problem  is exactly  formulated  to an MIQP  (Mixed  Integer Quadratic  Programming) and is solved by two effective heuristics since  the MIQP is NP-hard [15]. One is successive relaxation of  MIQP to a sequence of QP (Quadratic Programming) as a  fast algorithm and the other is a genetic algorithm that is  efficient random search algorithm to find better mapping  solution. Importantly, our framework not only enables global,  rather than local, optimization but also maps a task to both a  regular mesh and an irregular mesh/custom network.  To the best of our knowledge, this is the first work that  addresses task mapping in an irregular mesh network and a  custom network. The rest of this paper is organized as  follows: Section 2 reviews related works. Section 3 presents  our novel A3MAP formulation. In Section 4, our successive  relaxation algorithm and genetic algorithm are proposed to  solve A3MAP formulated to a MIQP. Section 5 shows  experiment results  in comparison with  the previous  state-of-art work [1]. Section 6 is used for conclusion.  Fig. 1. Overview of A3MAP  II. Related Works and Our Contributions  In the last decade, a task mapping problem has been  solved on a regular mesh network [1-5]. Murali et al. [1]  present NMAP that is a fast algorithm, where the tasks are  mapped onto a regular mesh network under bandwidth  6C-2 constrains, aiming at minimizing average communication  delay. In [2], a branch and bound algorithm is adopted for  task mapping in a regular mesh-based NoC architecture,  which minimizes the total amount of power consumed in  communications. Shin et al. [3] explores the design space of  NoC based systems, including task assignment, tile mapping,  routing path allocation, task scheduling and link speed  assignment using three nested genetic algorithms. The work  presented in [4] proposes an efficient technique for run-time  application mapping onto a homogeneous NoC platform  with multiple voltage levels. Chen et al. in [5] proposes a  complier-based application mapping algorithm that consists  of task scheduling, processor mapping, data mapping and  packet routing to reduce energy consumption.   Recently, heterogeneous cores have been considered in an  NoC-based MPSoC for low energy consumption [6-9]. Smit  et al. in [6] solve the problem of run-time task assignment on  heterogeneous processors. Carvalho et al. in [7] investigate  the performance of several mapping heuristics promising for  run-time use  in NoC based MPSOCs with dynamic  workloads, targeting NoC congestion minimization. Chang  et al. in [8] propose ETAHM to allocate tasks on a target  multiprocessor system. It mixes task scheduling, mapping  and DVS (Dynamic Voltage Scaling) utilization and couples  ant colony optimization algorithm. ADAM presented in [9]  is run-time application mapping in a distributed manner  using  agents  targeting  for  adaptive NoC based  heterogeneous MPSoCs.   However, those task mapping solutions do not consider  the irregularity of tiles and interconnections which generates  more traffic on a network so that high energy may be  consumed or QoS (Quality of Service) requirement may not  be guaranteed. Our main novelty and contribution for  solving these problems include followings:  (cid:121) A simple yet efficient metric space is proposed to capture  the interconnection of task graph or network. Then, a  task mapping problem is exactly formulated to a MIQP  based on a metric embedding technique.  (cid:121) Successive  relaxation  approximation  and genetic  algorithm are employed to solve the MIQP. They fit  well our formulation and provide a reasonable trade-off  between performance and runtime.  (cid:121) We show that the A3MAP can achieve good mapping  performance not only in a regular network but also in  an irregular network or custom network.  III. Problem Formulation  In this section, we formulate a task mapping problem to  an MIQP using metric embedding. As inputs, we take a task  graph and a network. We assume that each router is  interconnected with one processor in the network. First, we  present the metric space of graph using a proposed  interconnection matrix to capture the dependencies of tasks  in a task graph or tiles in a network. The graph G(V,E) with n vertices is a directed graph, where each vertex vi (cid:143) V represents a task or a tile and where each directed edge   524 ei,j(cid:143) E represents communication between vi to vj. vol(ei,j) represents communication volume between vi to vj in a task  graph and bw(ei,j)  represents bandwidth  requirement  between vi to vj in a network. We construct an n×n interconnection matrix, CN corresponding to a network,  where cNi,j(cid:143) CN is equal to bw(ei,j) as shown in Fig. 2(a). Each row of CN represents interconnection relation with  respect to a single tile on an NoC. Thus, CN contains  interconnection relations for an entire network, representing  the metric space of network. Similarly, we construct a n×n interconnection matrix CC, corresponding to a task graph,  where cCi,j(cid:143) CC is equal to vol(ei,j) as shown in Fig. 2(b).  For example, Fig. 2(c), (d) and (e) show three network  graphs and  their metric spaces using  the proposed  interconnection matrix. In Fig. 2(c) that is a regular mesh, all  routers are interconnected bidirectionally. Its interconnection  matrix is composed symmetrically as shown in the below of  Fig. 2(c). In case of an irregular mesh network in Fig. 2(d),  interconnections between tile A and tile B or between tile C  and tile F are unidirectional and tile D is not interconnected  to tile E. Since the bandwidth of links is also different, its  interconnection matrix is composed asymmetrically. This  type of network is shown in VFI (Voltage Frequency Island)  based NoC where each processor operates with own voltage  and frequency and in NoC with faulty [14] and degraded  links by process and temperature variation [13]. In case of a  custom network,  there  is slightly difference  in  the  composition of interconnection matrix. In Fig. 2(e), the  wirelength between tile E and tile F is different from others  due  to different sized  tile E. The composition of  interconnection matrix for the custom network is similar  with the previous two cases, yet weight (cid:302) is added in the  matrix in order to consider low energy consumption. Let the  energy consumption of single link, Elink, computed as:  E E E          (1) where Edrivers and Erepeaters are the energy consumed by  drivers and repeaters on a link respectively. If Elink1 and Elink2 are the energy consumption of dashed link and dotted link  respectively, (cid:302) is equal to the ratio of Elink1 and Elink2 (=Elink1/ Elink2). The weigh (cid:302) (0<(cid:302)<1) reduces the bandwidth of link  with a long wire in the network. For example, we assume  that a packet generated in tile A is transmitted to tile D, the  dotted links are three times longer than the solid links and it  takes the packet one cycle to pass each router. The packet  can choose either A-B-C-D or A-E-F-D as a routing path.  Since they pass the same number of router, all two paths  take the same clock cycle to reach tile D while the total  wirelengh of path A-E-F-D is longer than that of path  A-B-C-D. Consequently, the path A-E-F-D consumes high  energy since more repeaters on link EF are inserted or a  bigger output driver is required for fast transmission time.  Thus, it is good to assign a task with small traffic to a long  link and a task with big traffic to a short link for low energy  consumption. The weight (cid:302) lets a task with heavier traffic  mapped into a tile with short wires such that the energy  consumption can be more minimized.   Graph embedding [16] maps the vertices of graph G(V,E) into a chosen metric space by minimizing distortion. Thus,  repeaters drivers (cid:32) link (cid:14) task mapping has a natural correspondence with graph  embedding into a 2D metric space representing the network.  Our task mapping formulation is in the form of assignment  problem. We seek to embed a task graph into the metric  space of network. The goal is that a task is mapped to each  tile, satisfying performance in a task-mapped network while  the number of traffic generated between NoC tiles is  minimal. If a network has the same architecture that a task  graph has, such a task mapping does not have any distortion  of edges in the task graph. Consequently, it produces the best  possible mapping performance on the network. Generally,  since most task graphs are different from a network, some  distortion of task graph is not evitable in the network. Then,  the quality of task mapping is measured by the total  distortion of the embedding. By minimizing the extent by  which edges in the task graph are stretched or distorted with  intermediate tiles when embedded into the network, we seek  to reduce the total amount of communications and to obtain  a better global task mapping solution in terms of energy  consumption and performance.   Our  formulation  is similar  to an FPGA  (Field  Programmable Gate Array) [17]. However, the crucial  difference in our work is the use of metric space that  accurately captures the interconnections of network. A task  mapping problem  is equivalent  to determining  the  assignment of task to each tile with various objectives such  as low energy consumption and high performance. This task  assignment action is mathematically presented by an n×n permutation matrix P. Column indices and row indices in P represent  task  identifiers and processor  identifiers,  respectively. For example, if P(i,j)=1, then task j is mapped  to tile i. Thus only one element in each row and each column  of P can be 1; all others must be 0. The action of P on the  task graph is represented by PTCCP. P is found at once to  minimize  the  difference  between  the  permuted  interconnection matrix of task graph PTCCP and the  interconnection matrix of network CN for generating the less  number of traffic and to minimize the distortion of CC for a  short routing path. For P that is orthogonal, we formulate  this problem mathematically by our objective as:  2 2 N F min T obj C N C F f P C P C C P PC (cid:32) (cid:16) (cid:16) (cid:32)       (2)  where  (cid:32) (cid:166) (cid:166) , xi,j (cid:143) X, i.e., the Frobenius  norm of the matrix X, subject to integrity and linearity  constrains as follows:  2 ,i j F i j X x 1 P i ( , ) j 1, 1, 2, ..., n i n j n (cid:32) (cid:32) (cid:5) (cid:32) (cid:166)              (3)  (cid:32) 1 1, 1, 2, ..., P i ( , ) j j i n (cid:32) (cid:5) (cid:32) (cid:166)              (4)  (cid:11) (cid:12) (cid:94) (cid:96) P i , j (cid:143) 0,1                  (5)  While our formulation has a convex quadratic object  function, the binary constraints on the elements of P restrict  the solution space to a non-convex set. Thus, convex  optimization techniques like gradient descent cannot be  directly applied to solve this problem. Actually, this type of  formulation is well known as an MIQP (Mixed Integer  Quadratic Programming) problem that is NP-hard [15]. To  solve our formulation, we propose two effective heuristics  which can tradeoff solution quality and runtime, which will  be presented in the next section.  IV. A3MAP Algorithms  A. A3MAP-SR  In this section, we solve A3MAP formulated to an MIQP  by using successive relaxation [18], called A3MAP-SR. The  optimal MIQP formulation becomes a QP if we relax the  discrete constraint Eq. (5) to a continuous constraint as:  0 P i , j (cid:100)                 (6)  1 Then, the key idea behind this algorithm is using this QP as  a subroutine. The QP is faster to solve and scales much  better. Then, continuous values obtained by a QP solver are  (cid:11) (cid:12) (cid:100) 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 (e) custom network 0 0 (d) irregular mesh network (c) regular mesh network 0 5 0 5 0 0 0 0 0 0 0 5 5 5 0 0 5 0 0 5 0 5 0 0 5 0 5 5 0 5 0 0 0 5 0 5 vA vB Metric space conversion vC vD vE vF vA vB vC vD vE vF vA Metric space conversion vB vC vD vE vF vA vB vC vD vE vF vol(eAB) vol(eAC) vol(eAD) vol(eAE) 0 vol(eBC) vol(eBD) vol(eBE) vol(eCB) 0 vol(eCD) vol(eCE) vol(eDB) vol(eDC) 0 vol(eDE) vol(eEB) vol(eEC) vol(eED) 0 vol(eFB) vol(eFC) vol(eFD) vol(eFE) 0 vol(eBA) vol(eCA) vol(eDA) vol(eEA) vol(eFA) vol(eAF) vol(eBF) vol(eCF) vol(eDF) vol(eEF) 0 (a) Interconnection matrix, CN vA vB vC vD vE vF vA vB vC vD vE vF (b) Interconnection matrix, CC bw(eAB) bw(eAC) bw(eAD) bw(eAE) 0 bw(eBC) bw(eBD) bw(eBE) bw(eCB) 0 bw(eCD) bw(eCE) bw(eDB) bw(eDC) 0 bw(eDE) bw(eEB) bw(eEC) bw(eED) 0 bw(eFB) bw(eFC) bw(eFD) bw(eFE) 0 bw(eBA) bw(eCA) bw(eDA) bw(eEA) bw(eFA) bw(eAF) bw(eBF) bw(eCF) bw(eDF) bw(eEF) 0 vA vB vC vD vE vF vA vB vC vD vE vF A B C F E D 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 3 0 5 0 0 0 0 0 0 0 0 5 5 0 0 0 0 0 1 0 0 0 0 2 0 0 4 0 5 0 0 0 10 0 4 vA vB Metric space conversion vC vD vE vF vA vB vC vD vE vF A B C F E D 3 1 5 4 4 5 2 5 5 10 A B C F E 1 3 5 5 8 5 D 3 9 5 5 0 5 1 5 0 9 0 3 5 (cid:302)8 0 (cid:302)5 5 tile router Fig. 2. Various graphs and their interconnection matrices  6C-2 525 6C-2 guided to 0 or 1 depending upon a predefined threshold,  respectively. The concrete successive relaxation algorithm is  shown in Algorithm 1.  After relaxing the constraint Eq. (5) to Eq. (6) (line 1), we  set all P(i,j) to a variable since any P(i,j) is not guided to a  permanent value, 0 or 1 (line 2). In line 3, initial  inverse_threshold is set to the number of tile in a given  network. As a threshold, we use 1/inverse_threshold to guide  continuous P(i,j) solved by a QP solver to 1, where the  threshold means the expected average that a variable P(i,j) can get. On executing the successive relaxation, the  inverse_threshold decreases by 1 (line 12) whenever one  P(i,j) is set to 1, which means the threshold gets increased.  The rest of algorithm 1 attempts to constraint the continuous  values solved by the QP solver to binary values inversely. In  line 7, we look for the maximum P(i,j) and compare it to the  threshold in line 8. If it is above the threshold, it is set to 1  and a non-variable in line 9. In addition, all elements with  the same row and column address as the maximum P(i,j) are  also set to 0 and a non-variable (line 10 and 11) since the  summation of elements of one row and one column in the  permutation matrix P should be 1 by the constraints, Eq. (3)  and (4). This procedure repeats if the next maximum P(i,j) is  also above the threshold (line 14). Otherwise, we again solve  QP for the rest of variable P(i,j) and continue to guide the  continuous values to binary values. If all P(i,j) are guided to  0 or 1, we get the final permutation matrix P. Finally, a  pair-wise swapping of task mapped by A3MAP-SR is  executed until the amount of traffic is not decreased. Algorithm 1 A3MAP-SR  Input: MIQP problem 1: relax P(i,j) (cid:143) {0,1} to 0(cid:148) P(i,j) (cid:148)1; 2:  set all P(i,j) to a variable; 3: inverse_threshold = n(VN); 4: repeat  5:  solve relaxed MIQP by QP solver only for a variable P(i,j); 6: repeat  7:  find max{P} and store its location to (imax,jmax) for (cid:5) P(i,j) that is a variable; 8: if max{P} (cid:149) 1/inverse_threshold do 9: P(imax,jmax) = 1 and set to a non-variable; 10:  P(i,jmax)  = 0 and set to a non-variable, (cid:5) i = 1,2,..,n; 11:  P(imax,j)  = 0 and set to a non-variable, (cid:5) j = 1,2,..,n; inverse_threshold decreases by 1; 12:  13:  end if 14:  until (max{P} < 1/inverse_threshold) 15: until (all P(i,j) are a non-variable) Output: Permutation Matrix P B. A3MAP-GA  For a static task mapping where runtime is not more  important than the reduction of traffic, we develop another  heuristic using a genetic algorithm, called A3MAP-GA.  Algorithm 2 is the pseudo-code of our genetic algorithm to  solve the MIQP. First, we generate two arbitrary permutation  matrices as parent individuals (line 1 and 3). A crossover  scheme is widely acknowledged as critical to the success of  the genetic algorithm. The crossover scheme should be  capable of producing a new feasible solution (i.e., new child)  526 Algorithm 2 A3MAP-GA  Input: MIQP problem 1: generate arbitrary parent 1; 2: Repeat  3: generate arbitrary parent 2; 4: (child 1, child 2) = cycle crossover (parent 1, parent 2); 5: mutation of child 1and child 2 by pair-wise swapping;  6: parent 1 = one of two children with minimum fobj computed       by Eq. (2) for the next evolution; 7: until (no improvement during i-iterations) Output: Permutation Matrix P by combining the good characteristics of parent individuals  while child individuals should be considerably different  from their parent individuals. In A3MAP-GA algorithm, we  use a cycle crossover as shown in Fig. 3. This crossover  prevents more than two tasks being allocated into the same  tile together. Fig. 3 shows how to generate two children  using the cycle crossover. In the first step, Fig. 3(a), child 1 inherits a column from parent 1 and child 2 inherits a  column from parent 2. We start to choose the inherited  column from one arbitrary column in parent 1. In Fig. 3(a),  the first column is chosen in parent 1 and then the same  column as parent 1 is also chosen in parent 2. Next, we look  for the same content in parent 1 as the first column of parent  2 gets. In Fig. 3(a), the fifth column of parent 1 contains the  same content as the first column of parent 2 contains. Then,  the fifth column in parent 2 is chosen together. Similarly,  this procedure repeats until the column that is already  chosen is again chosen. In the second step, Fig. 3(b), child 1 inherits a column from parent 2 and child 2 inherits a  column from parent 1 inversely. The procedure is similar to  the first step except the choice of column starts from any  unselected column of parent 2. If all columns of children are  not filled with the column of parent after the second step, we  repeat the first and second step with the unselected columns  of parents by turns. In Fig. 3, all columns of child are filled  only after the second step. Then, a mutation operation is  performed for each child (line 5 and 6). In this operation,  two randomly selected columns are swapped to generate a  new individual. Then, the swapping is kept if it reduces the  number of traffic. The pair-wise swapping operation for each  child continues until the pair of swapped columns cannot  minimize our object function, Eq. (2). After the mutation  operation, we choose the best child with the minimum cost  as a parent for next evolution (line 7). If there is no  improvement during  i-iterations, we get  the  final  permutation matrix with the optimal mapping solution.  Fig. 3. Cycle crossover  6C-2 B. Irregular Mesh Network  Our task mapping proves more merits on an irregular  mesh or a custom network. We implement NMAP [1] for an  irregular mesh network to compare our mapping algorithm  even if it was proposed for a regular mesh network. Fig. 5  shows six irregular mesh networks on which we experiment  A3MAP and NMAP. The results for MPEG-4 VOPD  benchmark are present in Table 2. Our A3MAP shows better  mapping improvement in an irregular mesh network than in  a regular mesh. A3MAP-SR and A3MAP-GA reduce a  significant amount of traffic between tiles up to 16.1% and  29.4% on average, respectively, when compared to NMAP.  Especially, A3MAP shows better mapping improvement in  more complex network as shown in Table 2. A3MAP avoids  mapping a task containing heavy traffic to a tile containing  little bandwidth and considers not only disconnection  between tiles but also the direction of communication by  architecture-aware manner.   Fig. 5. Irregular mesh architectures  TABLE 2. Traffic Comparison of A3MAP and NMAP [1] in  Irregular Mesh Network  Network Fig. 5(a) Fig. 5(b) Fig. 5(c) Fig. 5(d) Fig. 5(e) Fig. 5(f) average NMAP  [1]  4869  5699  7810  4923  5706  8103  6185  A3MAP -SR  4839  4619  7317  4301  4199  4844  5187  Imp. (%) A3MAP -GA  0.6  4237  19  4457  6.3  4619  12.6  4295  26.4  4183  40.2  4410  16.1  4367  Imp. (%) 13  21.8  40.9  12.8  26.7  45.8  29.4  C. Custom Network  In this section, we perform our A3MAP algorithm on  custom networks with MPEG-4 VOPD benchmark. Fig. 6 is  custom networks on which we experiment A3MAP and  NMAP. Table 3 shows task mapping results performed on  those custom networks. A3MAP-SR and A3MAP-GA  reduce an amount of communication between NoC tiles up  to 7.3% and 12.9% on average, respectively, when compared  to NMAP. We also compute total wirelength passed by all  traffics, which is more correlated to energy consumption  than the number of traffic between NoC tiles. We count the  Otherwise, the chosen child becomes parent 1 and parent 2 is also generated randomly. Then, those procedures repeat  until there is no improvement for several iterations.  V. Experimental Results  We implement the A3MAP-SR algorithm by AIMMS/  CPLEX11.2 [19] and the A3MAP-GA algorithm by C++. All  of the experiments were performed on a Linux machine with  Intel 2.4GHz CoreDuo and 8GB RAM.   A. Regular Mesh Network  We carry out experiments by applying our task mapping  algorithm on MPEG-4 VOPD (Video Object Plane Decoder)  [20] and E3S benchmark suites [21]. The first application  including 16 tasks is mapped onto a 4x4 regular mesh  network. The second benchmark consists of consumer, AI  (Auto-Industry) and telecomm application containing 12, 24  and 30 tasks respectively, which are mapped to a given 3x3,  4x4 and 5x5 regular mesh network, respectively. Since the  number of task is different from the number of tile, the  pre-processing is required. If the number of task is less than  the number of  tile,  redundant vertices with no  communication should be added in a task graph to make the  same number of vertex as a network has. If the number of  task is more than the number of tile, we perform n(VN) min-cut partitioning of task graph, where n(VN) is the  number of NoC tile. Then, we perform A3MAP-SR and  A3MAP-GA algorithms. Finally, we allocate the routing  path of traffic by Dijkstra’s shortest path algorithm to  compute the total number of traffic between routers on the  network. Table 1 shows  that our A3MAP-SR and  A3MAP-GA generates less traffic in a regular mesh network  than NMAP [1] that is one of the famous NoC mapping  algorithms. However, the runtime of our task mapping is a  bit longer than NMAP as shown in Fig. 4.  TABLE 1. Traffic Comparison of A3MAP and NMAP [1] on  Industrial Benchmarks in Regular Mesh Network  application NMAP  [1]  consumer  50  VOPD  4309  AI  187  telecomm  127  A3MAP  -SR  50  4265  151  115  Imp.  (%)  0  1.0  19.3  9.4  A3MAP -GA  49  4141  147  102  Imp.  (%) 2  3.9  21.4  19.7  Fig. 4. Runtime comparison of A3MAP and NMAP[1] 527 6C-2 wirelength under the assumption that a short wire is 1 and a  long wire is 2. In Table 3, total wirelength passed by  communication traffic reduces up to 16.6% and 29.6% on  average, respectively, when compared to NMAP. From these  results, our weighted interconnection matrix is efficient to  reduce energy consumption since  the  reduction of  wirelength passed by communication traffic is better than  the reduction of the amount of traffic. Thus, our weighted  interconnection matrix is more desirable for a custom  network with different-sized tiles. Similarly, our task  mapping algorithm can be easily manageable for complex  NoC by controlling the weighted interconnection matrix.   A B E A D F K O J M J N I L C F G (a) B E G K N (c) H L I M P D C H O P A F E J N E A D H K N B D G (b) B K E L M O (d) H L O F I C C P I M P G J Fig. 6. Custom NoC topologies VI. Conclusion  In this paper, we propose a novel and global A3MAP  algorithm for an NoC-based MPSoC targeting general-  purposing computing. Based on a metric embedding  technique, we analytically formulate an NoC task mapping  problem to an MIQP. Then, the MIQP problem is solved by  two effective heuristics, successive relaxation as a fast  algorithm and genetic algorithm to find better mapping  solution. Experimental results show that A3MAP algorithm  reduces a significant amount of traffic on all types of  network when compared to the previous state-of-the-art  work [1]. Especially, A3MAP algorithm shows more merits  on an irregular mesh or a custom network. A3MAP has no  limitation to map tasks to tiles on any arbitrary, faulty and  degraded network. Furthermore, A3MAP  is easily  manageable  for  low energy consumption and high  performance by an architecture-aware analytical manner. "
2010,Design of networks on chips for 3D ICs.,"Three-dimensional integrated circuits, where multiple silicon layers are stacked vertically have emerged recently. The 3DICs have smaller form factor, shorter and efficient use of wires and allow integration of diverse technologies in the same device. The use of Networks on Chips (NoCs) to connect components in a 3D chip is a necessity. In this short paper, we present an outline on designing application-specific NoCs for 3D ICs.","2D-1 Design of Networks on Chips for 3D ICs Srinivasan Murali§ (cid:2) , Luca Benini‡ , Giovanni De Micheli(cid:2) § iNoCs, Lausanne, Switzerland, murali@inocs.com (cid:2) LSI, EPFL, Lausanne, Switzerland, giovanni.demicheli@epﬂ.ch ‡ DEIS, University of Bologna, Bologna, Italy, lbenini@deis.unibo.it Abstract Three-dimensional integrated circuits, where multiple silicon layers are stacked vertically have emerged recently. The 3D ICs have smaller form factor, shorter and eﬃcient use of wires and allow integration of diverse technologies in the same device. The use of Networks on Chips (NoCs) to connect components in a 3D chip is a necessity. In this short paper, we present an outline on designing application-speciﬁc NoCs for 3D ICs. 1 Introduction Three-dimensional integrated circuits have recently emerged as an attractive solution for designing Systems On Chips (SoCs) [3]. The 3D technology is based on the integration of multiple silicon layers vertically. The use of 3D packaging using oﬀ-chip interconnects to connect the diﬀerent layers, such as in System In Package, has been around for a long time. Compared to such 3D packaging techniques, the recent advances in the ﬁeld allow direct vertical interconnections between the diﬀerent 3D layers. Thus, we can have a processor in one silicon layer directly connected to a memory in another silicon layer. The 3D integration provides several major advantages: it addresses the critical interconnect bottleneck in today’s chip designs. In 3D, the silicon footprint in each layer is much smaller than a 2D implementation of the system. This is intuitive, as a 3D design can be thought of as a folding of a 2D design in to multiple layers. The vertical interconnects connecting two silicon layers are short and typically have lower delay and power consumption than a corresponding 2D interconnect. Thus, the wire delay and wire power consumption will be lower in a 3D design. The other advantages of 3D integration include smaller form factor and ease of integration of diverse technologies in a single chip. For example, MEMS, sensors, RF and diﬀerent memory technologies can be easily integrated by stacking one layer on top of another. Researchers have been addressing successfully some of the major issues in 3D design, including achieving high yield and improving the thermal heat dissipation across the layers. Several diﬀerent manufacturing technologies have evolved over the last several years. The use of Through Silicon Via (TSV) to achieve vertical interconnects is commonly used in many 3D technologies. With shrinking geometry sizes, the interconnect delay and power consumption has been rapidly increasing. A communication-centric design approach, Networks on Chips has evolved to address the interconnect issues [1], [2]. NoCs use networking principles on a chip scale. The use of NoCs are a necessity for 3D chips. NoCs are scalable, as network bandwidth can be increaed by adding more switches and links as needed. Thus, when integrating multiple silicon layers, the interconnect can be scaled easily. An important advantage in using NoCs is the large degree of freedom that can be exploited during the design process. For example, NoCs allow the multiplexing of 978-1-4244-5767-0/10/$26.00 2010 IEEE 167 2D-1 an assignment is done prior to the NoC design phase, either manually or using existing tools. We also optionally obtain the ﬂoorplan of the design, i.e. the position of the cores as an input. Using the input ﬂoorplan, our design process automatically places the NoC components, minimally perturbing the positions of the cores. The ﬂoorplan information is used to estimate the wire power and delay accurately. The 3D technology constraints are taken as the third set of inputs. In order to achieve high yield, there may be a restriction on the number of TSV that can be used. Also, any other 3D technology constraints, such as a requirement for establishing vertical links only across adjacent layers are taken as inputs. We build area, power and delay models for the network components and vertical links, which are taken as inputs to the design ﬂow. The topology synthesis procedure produces a set of valid design points that meet the application performance and 3D technology constraints, with diﬀerent power, delay and area values. From the Pareto curves, the designer can then choose the best design point. The placement and ﬂoorplan of the switches and TSV on each layer is also produced as an output. Once the best design point is chosen, the RTL design can be automatically generated for the NoC. "
2010,Floorplanning and topology generation for application-specific network-on-chip.,"Network-on-chip (NoC) architectures have been proposed as a promising alternative to classical bus-based communication architectures. In this paper, we propose a two phases framework to solve application-specific NoCs topology generation problem. At floorplanning phase, we carry out partition driven floorplanning. At post-floorplanning phase, a heuristic method and a min-cost max-flow algorithm is used to insert switches and network interfaces. Finally, we allocate paths to minimize power consumption. The experimental results show our algorithm is effective for power saving.","6C-4 Floorplanning and Topology Generation for Application-Speciﬁc Network-on-Chip Bei Yu, Sheqin Dong Department of Computer Science & Technology TNList Tsinghua University, Beijing, China ∗ Song Chen, Satoshi GOTO Graduate School of IPS Waseda University, Kitakyushu, Japan Abstract— Network-on-Chip(NoC) architectures have been proposed as a promising alternative to classical bus-based communication architectures. In this paper, we propose a two phases framework to solve application-speciﬁc NoCs topology generation problem. At ﬂoorplanning phase, we carry out partition driven ﬂoorplanning. At post-ﬂoorplanning phase, a heuristic method and a min-cost max-ﬂow algorithm is used to insert switches and network interfaces. Finally, we allocate paths to minimize power consumption. The experimental results show our algorithm is eﬀective for power saving. I. Introduction Network-on-Chip(NoC) architectures have been proposed as a promising alternative to classical bus-based and point-to-point communication architectures when the CMOS technology entered the nanometer era [1, 2, 3]. In NoCs, the communication among various cores is achieved by on-chip micro-networks components(such as switch and network interface) instead of the traditional nonscalable buses. Comparing with bus-based architectures, NoCs have better modularity and design predictability. Besides, the NoC approach oﬀers lower power consumption and greater scalability. NoCs can be designed as regular or application-speciﬁc network topologies. For regular Noc topology design, some existing NoC solutions assume a mesh-based NoC architecture [4, 5], and their focus is on the mapping problem. For application-speciﬁc topology design, the design challenges are diﬀerent in terms of irregular core sizes, various core locations, and diﬀerent communication ﬂow requirements [6, 7, 8, 9, 10]. Most SoCs are typically composed of heterogeneous cores and the core sizes are highly non-uniform. An application-speciﬁc NoCs architecture with structured wiring, which satisﬁes the design ob jectives and constraints is more appropriate. In this paper, we focus on synthesis problem of application-speciﬁc NoCs architecture. ∗ Tsinghua National Laboratory for Information Science and Technology Network components, such as switches and network interfaces(NI), consume area and power. The area consumption of these network components should be considered during topology generation. Besides, power eﬃciency is one of the most important concerns in NoCs architecture design. Many characteristics inﬂuence NoCs power consumption: total wirelength; communication ﬂow distributions and path choosing. In this paper, we propose a methodology to design the best topology that is minimize power consumption of interconnects and network components. There are a number of works addressing NoCs topology generation. In [6], a novel NoC topology generation algorithms were presented, however their solutions only consider topologies based on a slicing structure where switch locations are restricted to corners of cores. In [7], Murali et al. proposed a two step topology generation procedure using a min-cut partitioner to cluster highly communicating cores on the same switch and a path allocation algorithm to connect the clusters together. In [9], Chan et.al. presented an iterative reﬁnement strategy to generate an optimized NoC topology that supports both packet-switched networks and point to point connections. In most of the previous works, system-level ﬂoorplanning tool is used only estimates the area and the wire lengths. Partition is carried out at pre-ﬂoorplanning, so physical information such as the distances among cores are not able to be taken into account. Besides, area of switches and network interfaces are not consider during topology generation. In this paper, we integrate partition into ﬂoorplanning to make use of physical information such as the length of interconnects among cores. At post-ﬂoorplanning optimization, a heuristic method is used to insert switches and a min-cost max-ﬂow algorithm is used to insert network interfaces. Finally, we allocate paths to minimize power consumption. The remainder of this paper is organized as follows. Section 2 deﬁnes the partition driven ﬂoorplanning problem. Section 3 presents our algorithm ﬂow. Section 4 reports our experimental results. At last, Section 5 concludes this paper. 978-1-4244-5767-0/10/$26.00 2010 IEEE 535 6C-4                          	       	  	    ! "" . 	   	   .   # Fig. 1. CCG and SCG examples. (a)A simple CCG. (b)CCG is partitioned based on communication requirements and related positions. (c)Corresponding SCG. II. Problem Formulation Deﬁnition 1 (Core Communication Graph(CCG)) ¯G = ( ¯V , ¯E ) with each vertex vi ∈ ¯V representing a The core communication graph is a directed graph, core and the edge eij representing the communication requirement between the core vi and vj . The weight of edge eij is denoted as wij . Deﬁnition 2 (Switch Communication Graph(SCG)) The switch communication graph is a directed graph, G = (V , E ) with each vertex vi ∈ V representing a switch, and the directed edge eij = {vi , vj } ∈ E denotes a communication trace from vi to vj . A simple CCG with six cores is shown in Fig.1(a). After partition, corresponding SCG with three switches are generated as shown in Fig.1(c). Deﬁnition 3 (Cluster Bounding Resource) The cluster bounding resource of a cluster is evaluated by the half perimeter wirelength of the minimal bounding box enclosing the cluster. Problem 1 (NoCs Topology Generation) The topology generation problem can be deﬁned as fol lows: given a set of n cores C = {c1 , c2 , . . . , cn }, a switches number constraint m, a core communication graph(CCG) and network components power model, ﬁnd an NoC topology that satisﬁes several objectives: minimize area consumption of cores and network components(m switches and n network interfaces); minimize the communication energy. Cores with more communication requirements are incline to be assigned into same cluster to minimize communication energy. Relative positions of cores should be considered during partition to minimize area consumption. Besides, positions of network components, such as switches and network components, should be taken into account to minimize interconnect length. Finally, the actual physical connections between switches are established to ﬁnd paths minimizing traﬃc ﬂows energy across the switches. Fig. 2. Topology Synthesis Algorithm Overall III. Topology Synthesis Algorithm As shown in Fig.2, the algorithm ﬂow consists of two phases: (I)partition driven ﬂoorplanning, (II)postﬂoorplanning optimization. In Phase I, we integrate partition into ﬂoorplanning. When generate a new packing, we carry out partition to assign each core into one cluster. Partition should consider not only communication requirements among cores but also physical information of cores. In Phase II, in switches insertion, a heuristic method is adopted to calculate every switch’s position in white space. In network interfaces insertion, we present a MinCost Max-Flow based method to insert each NI in white space. Finally, an eﬀective incremental path allocation method is proposed to minimize power consumption. A. Partition Driven Floorplanning Traditionally, ﬂoorplanning tool is only used to evaluate the wire lengths between each cores and switches. And partition is carried out before ﬂoorplanning, so physical information such as the distances among modules are not able to be taken into account during partition. In this paper, we integrate partition into ﬂoorplanning phase. During ﬂoorplanning, after generating a new chip ﬂoorplan, we can estimate the interconnect length between module i and module j, denoted as lenij . Given core communication graph(CCG) and switches number constraint m, partition assign cores into m min-cut clusters. Those cores with larger communication requirements and less distances are assigned to the same cluster and hence use the same switch for communication. On the one hand, cores with larger communication requirements are more incline to cluster together to minimize interconnect power consumption. On the other hand, cores with less distances should be cluster to minimize cluster bounding resource. The partitioning is done in such a way that the edges of the graph that are cut between the partitions have lower weights than the edges that are within a partition and the number of vertices assigned to each partition is almost the (cid:2) same. In partition, we deﬁne new edge weight w ij in CCG: w (cid:2) ij = αw × wij max w + αd × mean dis disij (1) 536                                    	 	             .            	 6C-4  Fig. 4. A simple example of network interfaces insertion. (a)When l sets as the width of grid, l-box of core c3 includes ﬁve free grids(label 1, 2, 3, 6, 7). (b) Corresponding network ﬂow model. For switch swk , its communication requirement is deﬁne as follow: f lowk = wij , ∀eij ∈ ¯E & i ∈ pk & j /∈ pk (3) (cid:2) i,j where i ∈ pk means core i is assigned to cluster pk . We sort switches by their communication requirements, and assign each switch into one of its candidate grids one by one. If one free grid(label g) is candidate grid of switch swk , then the insertion cost C ostgk is deﬁned as follow: (cid:2) wij ×(disgi+disgj ), ∀eij ∈ ¯E & i ∈ pk & j /∈ pk Fig. 3. A ﬂoorplan with four cores, in which white spaces are divided into grids(label from 1 to 10). (a)Core c1 and c3 are partitioned into one cluster and c2 and c4 are partitioned into another cluster. Two dots are initial positions of two switches. (b)Switches are assigned to grids one by one and ﬁnally two switches sw1 and sw2 have decided their positions. where wij denotes communication requirement between core i and core j , disij denotes distance between core i and j , max w is the maximum communication requirement over all ﬂows and mean dis is average distance among cores. During ﬂoorplanning, we use CBL[12] to represent every ﬂoorplan generated. CBL is a topological representation dissecting the chip into rectangular rooms. The cost function in simulated annealing is: Φ = λAA + λF F + λRR (2) C ostgk = where A represent the ﬂoorplan area; F represents the total communication amount between clusters; and R represents the sum of all cluster bounding resources. The parameters λA , λF and λR can be used to adjust the relative weighting between the contributing factors. i,j (4) where disgi is the distance from grid g to core i. Each switch chooses one of the candidate grids with lest insertion cost to insert. As shown in Fig. 3(b), sw1 inserts into grid 4 and sw2 inserts into grid 5. B. Switches Insertion Once a ﬂoorplan with m clusters P = {p1 , p2 , . . . pm} is obtained, the next step is to ﬁnd the latency and power consumption on the wires. In order to do this, the position of the switches needs to be determined. Each cluster has one switch and communication among clusters are through switches. We denote the set of switches as SW = {sw1 , sw2, . . . swm}, and switch swk belongs to cluster pk . Due to the restriction that switches cannot be placed on a core, the location must be within a white space. We partition the dead space into grids and each grid provides sites for switches insertion. Then a heuristic method is proposed to insert each switch into one grid(as shown in Fig. 3). The minimal bounding box enclosing cluster pk is deﬁned as Bk . For switch swk , its candidate grids are the free grids inside Bk . For example, in Fig. 3(a), cluster p1 includes core c1 and core c2, and switch sw1’s candidate grids are label from 1 to 4. Switch sw2’s candidate grids are those label 5, 6, 8, 9. Initially, each switch swk is located in the center of cluster’s bounding box. C. Network Interfaces Insertion After switches insertion, every switch is assigned a grid in white space. Then we carry out minimum cost ﬂow based network interfaces insertion to assign each NI into one grid. We deﬁne set of Network Interfaces as N I = {ni1 , ni2 , . . . nin }, where n is number of cores. Each core ck needs one network interface nik to connect to switch. ck , Deﬁnition 4 (l-bounding box) Given a core whose width is widk and height is heik . The l-bounding box of ck is B lk , which has the same centric position. Besides, width of B lk is (widk + 2× l) and height is (heik + 2 × l) (as shown in Fig.4(a)). For each core ck , we construct its l-bounding box. The free grids in the l-bounding box are ck ’s candidate grids, denoted as CGk . ∗ = (V ∗ ∗ ), and then We construct a network graph G use a min-cost max-ﬂow algorithm to determine which grid each network interface belong to. A simple example is shown in Fig.4. , E • V ∗ = {s, t} ∪N I ∪ Grids. 537 	    	    	         6C-4 tij P re(i) P ost(i) TABLE I Notation used in Path Allocation power consumption to connect eij . {vk |∀vk ∈ V & eki ∈ E } {vk |∀vk ∈ V & eik ∈ E } dise (i, j, d) minimum distance from node vi to vd while edge eij is used. minimum distance from node vi to vd . denote which node vi connect to go to vd . disn (i, d) path(i, d) • E ∗ = {(s, nik )|nik ∈ N I } ∪ {(nik , gj )|∀gj ∈ CGk } ∪ {(gj , t)|gj ∈ Grids}. • Capacities: C (s, nik ) = 1, C (nik , gj ) = 1, C (rj , t) = 1. • Cost: F (s, nik ) = 0, F (gj , t) = 0; F (nik , gj ) = Fkj . where Fkj equals to distance from grid j to switch swk . Network Interfaces insertion can be solved eﬀectively by minimum cost ﬂow algorithm(run in polynomial time[14]). D. Energy Aware Path Allocation After switches insertion, we use dynamic programming based method for path allocation to minimize power assumption. Given switch communication graph(SCG) G = (V , E ) representing communication requirement among switches. The communication requirement of eij ∈ E denoted as wsij : wsij = (wab + wba ) (5) (cid:2) (cid:2) ∀a∈pi ∀b∈pj where pi is cluster i and wab is communication requirement from core ca to core cb . We denote nodes in SCG as v1 , v2 , . . . , vm , where m is the number of switches. We assume SCG only exists directed edge eij that i < j because eij represents both communication from switch swi to swj and swj to swi . As shown in Table I, we deﬁne set P re(i) as vi ’s frontend nodes and P ost(i) as vi ’s back-end nodes. We also deﬁne two kind of distance dise (i, j, d) and disn (i, d). Besides, path(i, d) denotes which node vi should connect to go to vd . We use the following ways to solve dise , disn and path: (cid:3) (cid:3) dise (i, j, d) = tid , tij + disn (j, d), j = d & i ∈ P re(d) otherwise (6) 0, i = d disn (i, d) = mink dise (i, k , d), ∀k ∈ P ost(i) path(i, d) = j, ∀j s.t. dise (i, j, d) = disn (i, d) (7) (8)                                          Fig. 5. A simple example of paths allocation with seven switches. (a)Initial network, the value on each edge eij is tij . (b)After I nitS olve(7), the value on each edge eij is dise (i, j, 7) and each bold edge eij means path(i, 7) = j . (c)Compare with (b), t67 decreases from 4 to 2, update some edges(labeled as dotted arrows). (d)Compare with (b), t57 increases from 2 to 10, update some edges(labeled as dotted arrows). Algorithm 1 I nitS olve(d) 1: //Given d, solve all dise (i, j, d) and disn (i, d); 2: Initialize all D(i, j, d) ← M ; 3: for all k ∈ P re(d) do dise (k , d, d) ← tkd ; disn (k , d) ← tkd ; 6: end for 7: for i = d − 1 to 1 do for all j ∈ P ost(i) do 8: 4: 5: 9: 10: 11: 12: end for dise (i, j, d) ← tij + disn (j, d); disn (i, d) ← minj dise (i, j, d), ∀j ∈ P ost(i); path(i, d) ← j ; 13: end for We use a dynamic programming based method to solve distance dise (i, j, d), disn (i, d) and path(i, d), as shown in Algorithm 1. Theorem 1 The required time for Algorithm I nitS olve() is at most O(|E |). The run time to solve al l the nodes is bounded by O(|V | · |E |). If t(i, j ) changes, instead resolving all the dise (i, j, d) and disn (i, d), we can eﬀectively update them. If t(i, j ) decreases, we use Algorithm 2, otherwise we use Algorithm 3. We consider a simple paths allocations as shown in Fig.5. A SCG with seven switches is shown in (a), the value on each edge eij is initial tij . Using Algorithm 1 setting d = 7, we can solve each dise (i, j, 7)(labeled on each edge in (b)). If t67 decreases from 4 to 2, we use Algorithm 2 to update some dise (i, j, 7) and disn (i, 7). As shown 538 	      	      	    	            	          	               1: //Update when tij change to (tij − Δt); Algorithm 2 DecreaseU pdate(i, j, Δt, d) 2: tij ← (tij − Δt); 3: queue q.push(eij ); 4: while q is not empty do eab ← q.pop(); dise (a, b, d) ← tab + disn (b, d); if tab + disn (b, d) < disn (a, d) then disn (a, d) ← tab + disn (b, d); path(a, d) ← b; 10: 11: q.push(epa ), ∀p ∈ P re(a); end if 12: end while 5: 6: 7: 8: 9: 6: 7: 8: 9: Algorithm 3 I ncreaseU pdate(i, j, Δt, d) 2: tij ← (tij + Δt); 1: //Update when tij change to (tij + Δt); 3: queue q.push(eij ); 4: while q is not empty do eab ← q.pop(); 5: dise (a, b, d) ← tab + disn (b, d); if P AT H [a][d] = b then Find k ∈ P ost(a) to minimize disn (k , d) + tak ; disn (a, d) ← disn (k , d) + tak ; path(a, d) ← k ; 10: 11: q.push(epa ), ∀p ∈ P re(a); end if 13: end while 12: in (c), queue q pushes edges e67 , e36 , e46 , e13 , e23 one by one(labeled as dotted arrows). And path3,7 changes from 5 to 6 and path1,7 changes from 2 to 3. If t57 increases from 2 to 10, we use Algorithm 3 to update dise (i, j, 7) and disn (i, 7). As shown in (d), queue q pushes edges e37 , e35 , e45 , e13 , e23 one by one(labeled as dotted arrows). And path3,7 changes from 5 to 6. IV. Experimental Results We implemented our algorithm in the C++ programming language and executed on a Linux machine with a 3.0GHz CPU and 1GB Memory. During ﬂoorplanning we use hMetis[13], an eﬃcient hierarchical graph partitioning tool. 6C-4 TABLE II Power Model of Switch ports (pJ/bit) 2 0.22 3 0.33 4 0.44 5 0.55 6 0.66 7 0.78 8 0.90 TABLE III Power Model of Interconnects Wire length(mm) (pJ/bit) 1 0.6 4 2.4 8 4.8 12 7.2 16 9.6 We use Orion[11] as power simulator. Table II gives the switch bit energy in 0.18um technology and Table III gives the power model of links. B. Results and discussion We have applied our topology generation procedure to three sets of benchmarks. The ﬁrst set of benchmarks are several video processing applications obtained from [2]: MPEG4, MWD and VOPD. The next set of benchmarks are obtained from [6]: 263decmp3dec, 263encmp3dec and mp3encmp3dec. The last benchmark is obtained from [8]: D 38 tvopd. Fig.6 shows two ﬂoorplan generated for the 263decmp3dec and D 38 tvopd benchmark. We performed experiments to evaluate our topology generation algorithm. For comparison, we have also generated another approach PBF, which is similar to the mincut based algorithm presented in [7]. In PBF, partition is solved only before ﬂoorplanning. Table IV shows comparisons between our experimental results and PBF. The column Power means the actual power consumption and column Hops means average number of hops. Our method can save 41.8% of power and 2.6% of hops number. For test cases that have more communication requirements, such as 263encmp3dec, our algorithm can save much more power(reduce power consumption from 58.6 mW to 19.2 mW). The column W.S means the white spaces and column Time is run time. The white space of our method increases from 12.31% to 13.92% and run time is reasonable. Since power saving is the most important concern, the deteriorating is acceptable. We further demonstrated the eﬀectiveness of Algorithm 2 and Algorithm 3. To update routing when link cost changes, we performed another contrastive approach DSP. DSP re-solves all the distances of ﬂows by Dijkstra’s shortA. Power Model NoC power consumption consists of two parts: power consumed by interconnects and power consumed by switches For each network link e, we assume Pe represents bit energy on link e and the corresponding switches. Pe = Pl + Ps , where Pl and Ps are bit energy on interconnects and switches, respectively. Power consumption is P = Pe × f , where f represents communication requirements passing the link and the corresponding switch. t 01 t 02 t 03 539 TABLE V Comparison for Fault Tolerant V# Flow# Update# Run Time(s) DSP ours 0.024 0.008 0.604 0.016 20.35 0.08 20 100 300 34 130 457 20 30 50 Diﬀ -66.7% -97.4% -99.6% TABLE IV The Consumption Between the PDF and the PBF Benchmark V# E# Part# Power(mW) PBF ours 25.9 16.0 24.3 14.1 3.05 3.08 3.19 3.02 7.43 6.12 7.62 6.59 4.96 3.92 7.86 4.35 24.7 19.2 58.6 19.2 8.4 4.4 11.2 8.6 12.7 8.2 12.3 6.8 15.16 8.83 -41.8% Hops PBF ours 1.17 1.0 1.25 1.041 1.33 1.33 1.25 1.25 1.0 1.0 1.0 1.15 1.0 1.0 1.25 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.33 1.33 1.44 1.4 1.14 1.11 -2.6% W.S(%) PBF ours 12.25 16.43 7.63 16.43 12.22 11.82 12.22 12.22 12.16 13.54 12.17 13.85 14.24 13.44 13.59 14.50 6.06 8.82 9.58 9.58 15.23 17.60 15.23 15.24 15.1 24.5 14.7 22.60 12.31 13.92 Time(s) ours 13.86 15.07 13.37 15.46 14.54 17.32 23.78 24.96 13.19 15.42 20.29 21.0 92.7 104.0 28.93 MPEG4 12 13 3 4 3 4 3 4 3 4 3 4 3 4 3 4 MWD 12 12 VOPD 12 14 263decmp3dec 14 15 263encmp3dec 12 12 mp3encmp3dec 13 13 D 38 tvopd 38 47 Avg Diﬀ (cid:358)20 0 20 40 60 80 100 120 140 160 0 20 40 60 80 100 120 140 160 0 1 2 3 4 5 6 7 8 9 10 11 0 50 100 150 200 0 20 40 60 80 100 120 140 160 180 200 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 Fig. 6. Experimental results of 263decmp3dec and D 38 tvopd with four clusters. est path algorithm[14]. We have applied another set of test cases: t 01, t 02 and t 03. For each case, table V reports the number of nodes V#, ﬂow number and update times Update#. We can see that our updating algorithm can save lots of run time: t 01 saves 66.7%, t 02 saves 97.4% and t 03 can save 99.6%. V. CONCLUSIONS We have proposed a two phases framework to solve topology synthesis for NoCs: phase one is partition driven ﬂoorplanning; phase two is switches insertion, network interfaces insertion and paths allocations to minimize power consumption. Experimental results have shown that our framework is eﬀective and can save power consumption by 41.8%. "
2010,Application-specific 3D Network-on-Chip design using simulated allocation.,"Three-dimensional (3D) silicon integration technologies have provided new opportunities for Network-on-Chip (NoC) architecture design in Systems-on-Chip (SoCs). In this paper, we consider the application-specific NoC architecture design problem in a 3D environment. We present an efficient floorplan-aware 3D NoC synthesis algorithm, based on simulated allocation, a stochastic method for traffic flow routing, and accurate power and delay models for NoC components. We demonstrate that this method finds greatly improved topologies for various design objectives such as NoC power (average savings of 34%), network latency (average reduction of 35%) and chip temperature (average reduction of 20%).","6C-1 Application-Speciﬁc 3D Network-on-Chip Design Using Simulated Allocation ∗ Pingqiang Zhou Ping-Hung Yuh Sachin S. Sapatnekar Department of Electrical and Computer Engineering University of Minnesota Minneapolis, MN 55455 pingqiang@umn.edu Department of Computer Science and Information Engineering National Taiwan University Taipei 106, Taiwan r91089@csie.ntu.edu.tw Department of Electrical and Computer Engineering University of Minnesota Minneapolis, MN 55455 sachin@umn.edu Abstract— Three-dimensional (3D) silicon integration technologies have provided new opportunities for Network-on-Chip (NoC) architecture design in Systems-on-Chip (SoCs). In this paper, we consider the application-speciﬁc NoC architecture design problem in a 3D environment. We present an efﬁcient ﬂoorplanaware 3D NoC synthesis algorithm, based on simulated allocation, a stochastic method for trafﬁc ﬂow routing, and accurate power and delay models for NoC components. We demonstrate that this method ﬁnds greatly improved topologies for various design objectives such as NoC power (average savings of 34%), network latency (average reduction of 35%) and chip temperature (average reduction of 20%). I . IN TRODUC T ION Three dimensional (3D) integrated circuits, in which multiple tiers are stacked above each other and vertically interconnected using through-silicon vias (TSVs), are emerging as a promising technology for SoCs [1–4]. As compared to 2D designs, 3D circuits permit reduced latencies for critical interconnect structures, resulting in higher system throughput, performance, and power, and allow other beneﬁts such as heterogeneous integration. All of these ﬂexibilities enable the design of new high-performance System-on-Chip (SoC) structures that were previously thought to have prohibitive overheads. In spite of well-known challenges such as thermal bottlenecks (to which several solutions have been proposed), the beneﬁts of 3D integration are considerable. In the context of intrachip communication, 3D technologies have created significant opportunities and challenges in the design of low latency, low power and high bandwidth interconnection networks. In 2D SoCs choked by interconnect limitations, networkson-chip (NoCs), composed of switches and links, have been proposed as a scalable solution to the global communication challenges: compared to previous architectures for on-chip communication such as bus-based and point-to-point networks, NoCs have been shown to provide better predictability, lower power consumption and greater scalability [5, 6]. 3D circuits enable the design of more complex and more highly interconnected systems: in this context, NoCs promise major beneﬁts, but impose new constraints and limitations. 3D NoC design introduces new issues, such as the technology constraints on the number of TSVs that can be supported, problems related to optimally determining tier assignments and the placement of switches in 3D circuits, and accurate power and delay modeling issues for 3D interconnects. This work addresses the problem of designing applicationspeciﬁc 3D NoC architectures for custom SoC designs, in conjunction with ﬂoorplanning. Speciﬁcally, our work determines both the NoC topology and the ﬂoorplan of the NoC switches ∗ This work was supported in part by the SRC under contract 2008-TJ-1819. 978-1-4244-5767-0/10/$26.00 2010 IEEE 517 and cores. We propose a synthesis method to ﬁnd the best topology for the application, under different optimization objectives such as power and network latency, and determine the paths for trafﬁc ﬂows. We use a 3D thermally-aware ﬂoorplanner to assign the cores to different 3D tiers, while optimizing chip temperature, and ﬁnd an initial ﬂoorplan for the cores on each tier. Given the positions of cores, we use a stochastic ﬂow allocation method, Simulated Allocation (SAL), to route the trafﬁc ﬂows and build the topology for the application, initially using a simple strategy for determining the approximate locations of the switches. When the best topology is found, a fast ﬂoorplanner is applied to further optimize the positions of the added switches. Accurate power and delay models for switches and links are integrated into our algorithm. Our approach has three signiﬁcant features that together make it uniquely different from competing approaches: ﬁrst, we use improved trafﬁc ﬂow routing using SAL that accommodates a realistic objective function that has components that are nonlinear and/or unavailable in closed form; second, we interleave ﬂoorplanning with NoC synthesis, using speciﬁc measures that encourage convergence by discouraging blocks from moving from their locations in each iteration; and third, we use an accurate NoC delay model that incorporates the effects of queueing delays and network contention. Our algorithm is extremely ﬂexible and is applicable both to 2D and 3D layouts, but we demonstrate that the use of 3D designs results in signiﬁcantly reduced NoC power and latency, when compared to optimal 2D implementations. I I . CON TR IBU T ION S O F OUR WORK There has been a great deal of prior work on NoCs alone and on 2D and 3D layout alone, but less on integrating the two. In the area of designing NoC architectures for 3D ICs, most of the literature has focussed on regular 3D NoC topologies such as meshes [7–11], which are appropriate for regular 3D designs [12, 13]. However, most modern SoC architectures consist of heterogenous cores such as CPU or DSP modules, video processors, and embedded memory blocks, and the trafﬁc requirements among the cores can vary widely. Therefore, regular topologies such as meshes may have signiﬁcant area and power overhead [14, 15], and tuning the topology for application-speciﬁc solutions can provide immense beneﬁts. The synthesis of an application-speciﬁc NoC topology includes ﬁnding the optimal number and size of switches, establishing the connectivity between the switches and with the cores, and ﬁnding deadlock-free routing paths for all the trafﬁc ﬂows. For 2D systems, the problem of designing applicationspeciﬁc NoC topologies has been explored by several researchers [16–20]. Srinivasan et al. [17] present a three-phase NoC synthesis technique consisting of sequential steps that 6C-1 ﬂoorplan the cores, next perform core-to-router mapping, and then generate the network topology. In [19], Murali et al. present an NoC synthesis method that incorporates the ﬂoorplanning process to estimate link power consumption and detect timing violations. Several topologies, each with a different number of switches, are explored, starting from one where all the cores are connected to one switch, to one where each core is connected to a separate switch. The trafﬁc ﬂows are ordered so that larger ﬂows are routed ﬁrst. In the 3D domain, Yan et al. [14] present an applicationspeciﬁc 3D NoC synthesis algorithm that is based on a ripup-and-reroute procedure for routing ﬂows, where the trafﬁc ﬂows are ordered in the order of increasing rate requirements so that smaller ﬂows are routed ﬁrst, followed by a router merging procedure. Murali et al. [15] propose a 3D NoC topology synthesis algorithm, which is an extension to their previous 2D work [19], described above. The 3D NoC synthesis problem has been shown to be NP-hard in [21]. Our work is motivated by the following observations: • The ﬁnal results of application-speciﬁc NoC topology synthesis depend on the order in which the trafﬁc ﬂows are routed. In some cases, routing larger ﬂows ﬁrst provides better results [18, 19], while in others, routing the smaller ﬂows ﬁrst may yield better results [14]. A strategy is required to reduce the dependency of the results on ﬂow ordering. • In all of the works mentioned previously, the average hop count is used to approximate the average packet latency in NoCs. This ignores the queueing delays in switch ports and the contention among different packets for network resources such as switch ports and physical links, and cannot reﬂect the impact of physical core-to-switch or switchto-switch distances on network latency. More accurate delay models that include the effects of queueing delay and network contention, and better delay metrics, should be applied for NoC performance analysis. • The delays and power dissipation for physical links in NoCs are closely linked to the physical ﬂoorplan and topology of cores and switches. We show in Section VI that interleaving ﬂoorplanning and NoC topology synthesis process leads to superior results. We address these important problems in application-speciﬁc NoC topology synthesis. Our solution to overcoming the ordering problem is based on the use of a multicommodity ﬂow network formulation for the NoC synthesis problem: the advantage of such an approach is that it takes a global view of the problem and eliminates the problem, described above, of ﬁnding the best order in which to route the trafﬁc ﬂows. The multicommodity ﬂow problem is a well-known approach for solving such problems, but has seen little use in NoC design, with a few exceptions. In [22, 23], Hu et al. propose a scheme to optimize NoC power consumption through topology exploration and wire style optimization, subject to the average communication latency constraints, but do not handle layout synthesis issues, and assume simple linear objective functions. Our work utilizes a stochastic SAL approach to efﬁciently solve the multicommodity ﬂow problem under a nonlinear objective function that can be evaluated by an oracle, but is hard to express in closed form. The SAL framework has previously been used to solve multicommodity ﬂow problems in computer network design. We also use an accurate delay model for switches in NoCs which consider the queueing delay and network contention. Finally, our algorithm performs the ﬂoorplanning of cores/switches and NoC topology synthesis in an integrated iterative loop, attempting to ﬁnd the optimal solution for the problem of application-speciﬁc NoC design. I I I . PROB L EM IN PU T S , OB J EC T IV E S , AND CON S TRA IN T S The input to our 3D NoC synthesis problem is a directed graph, called the core graph, G(V , E , λ). Each node vi ∈ V represents a core (either a processing element or a memory unit) and each directed edge evi ,vj ∈ E denotes a trafﬁc ﬂow from source vi to destination vj . The bandwidth of trafﬁc ﬂow from core vi to vj is given by λ(evi ,vj ) in M B /s. In addition, NoC architectural parameters such as the NoC operating frequency, f , and the data link width, W , are also assumed to be provided as inputs. The operating frequency is usually speciﬁed by the design and data link width is dictated by the IP interface standards. Our 3D NoC synthesis framework permits a variety of objectives and constraints, including considerations that are particularly important in 3D, such as power dissipation, temperature, and the number of TSVs, and NoC-speciﬁc issues such as minimizing the average/maximum network latency, limitations on the maximum bandwidth, as well as general factors such as the design area. In addition, the solution must be free of deadlocks, which can occur during routing ﬂows due to cyclic dependencies of resources such as buffers. We use the turn prohibition algorithm presented in [24] to ensure that our topology is deadlock-free. The speciﬁc optimization objectives in each step of our approach are described in Section IV. The output of our 3D NoC synthesis solution is an optimized custom deadlock-free network topology with pre-determined paths on the network to route the trafﬁc ﬂows in the core graph and the ﬂoorplan of the cores and switches in the NoC such that the constraints are satisﬁed. IV. THE OV ERA L L D E S IGN F LOW The design ﬂow of our NoC synthesis algorithm is presented in Fig. 1. Given a given a core graph, we ﬁrst obtain an initial ﬂoorplan of the cores using a thermally-aware ﬂoorplanner. This precedes the 3D NoC synthesis step, and is important because the core locations signiﬁcantly inﬂuence the NoC architecture. Associating concrete core positions with the NoC synthesis step better enables it to account for link delays and power dissipation. bandwidth v2 v1 v3 v4 v5 v6 core Core graph Design goals  & cosntraints  Design  parameters  Area, power,  delay models of  NoC components 3D NoC synthesis algorithm 3D  floorplanning Application-specific  3D NoC synthesis 1.  3D custom NoC architecture 2.  Floorplan of cores & switches Fig. 1. Application-speciﬁc 3D NoC synthesis ﬂow. (cid:2) (V Our 3D NoC synthesis algorithm is performed on a directed (cid:2) (cid:2) ): V (cid:2) is the vertex set, which is the routing graph G union of core set V in the input core graph G(V , E , λ) and the set of added switches, Vs . We assume that the maximum number of switches that can be used in each 3D tier l equals , E 518 to the number of cores in that tier, although it is easy to relax (cid:2) is constructed as follows: we this restriction. The edge set E connect cores in a tier l only to the switches in the same tier l and adjacent tiers l − 1, l + 1 and the switches from all the 3D tiers form a complete graph. A custom NoC topology is a (cid:2) . subgraph of the routing graph, G The 3D NoC synthesis problem can be viewed as a multicommodity ﬂow (MCF) problem. For a core graph G(V , E , λ) (cid:2) (cid:2) ) (correspondand a corresponding routing graph G ing to a ﬂow network), let c(u, v) be the capacity of edge (cid:2) . The capacity c(u, v) equals to the product of the operating frequency f and data link width W . Each commodity Ki = (si , ti , di ), i = 1, · · · , k corresponds to the weight (trafﬁc ﬂow) along edge esi ,ti in the core graph from source modity i. Therefore, there are k = |E | commodities in the si to destination ti , and di = λ(esi ,ti ) is the demand for comcore graph. Let the ﬂow of commodity i along edge (u, v) be fi (u, v). Then the MCF problem is to ﬁnd the optimal assignment of ﬂow which satisﬁes the constraints: (u, v) ∈ E (cid:2) (V , E (cid:2)k Capacity constraints: Flow conservation: Demand satisfaction: (cid:2) (cid:2) i=1 fi (u, v) ≤ c(u, v) ∀v , u fi (u, v) = −fi (v , u) ω∈V (cid:2) fi (si , ω) = ω∈V (cid:2) fi (ω , ti ) =d i ω∈V (cid:2) ,u (cid:4)=si ,ti fi (u, ω) = 0 where (cid:2) Superﬁcially, this idea seems similar to [23], where an MCF formulation is proposed. However, that work is directed to 2D NoC synthesis with a single objective of minimizing NoC power, modeled as a linear function of the ﬂow variables fi (u, v). The corresponding Linear Programming (LP) problem is solved using an approximation algorithm. Our more general formulation integrates more objectives and more accurate modeling for NoC components. In fact, most components of our objective function are nonlinear or, as in case of network latency, unavailable in closed form, rendering an LP-based approach impossible. We choose to apply an SAL-based ﬂow allocation approach that is particularly suitable for (see Section V-A for details) solving the MCF problems where the objective function is in such a form. The SAL procedure yields the NoC topology and the paths for all the trafﬁc ﬂows in the core graph. In this optimization, we assume single-path routing for each trafﬁc ﬂow in the core graph, but conceptually, the SAL method can easily be extended to deal with the multipath routing problem. After the 3D NoC synthesis step, the actual switches and links in the synthesized 3D NoC architecture are fed back to the ﬂoorplanner to update the ﬂoorplan of the cores and used switches, and the reﬁned ﬂoorplan information is used to obtain more accurate power and delay estimates. The process continues iteratively: with the reﬁned ﬂoorplan, a new SAL based 3D NoC synthesis procedure is invoked to ﬁnd a better synthesis solution, and so on. The speciﬁc optimization objectives used in various steps of our approach are as follows: • For NoC topology construction, we optimize a linear combination of the network power, average network latency and TSV count, with constraints on bandwidth. • For the initial ﬂoorplanning step (Section V-C), we optimize a linear combination of chip temperature and weighted inter-core distance. • For subsequent steps that ﬂoorplan the cores and switches, we optimize a linear combination of design area, link power, link delay and chip temperature. 6C-1 V. T ECHN ICA L D E TA I L S In this section, we present the major elements in our 3D NoC synthesis algorithm. A.SimulatedAllocationAlgorithm Simulation Allocation (SAL) [25, 26] is a stochastic framework for ﬁnding near-optimal solutions for the multicommodity trafﬁc ﬂow problems. It has been shown to be simpler, but often faster and more efﬁcient, than other stochastic algorithms such as simulated annealing and evolutionary algorithms. The details of the SAL framework used in our work are described in Algorithm 1. In the core graph G(V , E , λ), let • Pi be the number of available paths for trafﬁc demand • xip be the amount of trafﬁc ﬂow realizing the trafﬁc Ki = (cid:2) , (si , ti , di ) allocated to path p in routing graph G • x = {xip : i = 1, 2, · · · , k , p = 1, 2, · · · , Pi } be the Ki = (si , ti , di ), • H = • |x| = allocation state, p xip be the total allocated trafﬁc ﬂow, and i di be the total amount of trafﬁc ﬂow. Note that since we consider single-path routing for each commodity, at most k paths, one per commodity, will have nonzero ﬂows. Therefore, even though the number of paths can be exponentially large, it is never necessary to enumerate Pi ; storing the allocation state x does not impose a signiﬁcant memory overhead. (cid:2) (cid:2) (cid:2) i Algorithm 1 Simulated Allocation (SAL) (adapted from [26]) 1: n = 0; counter = 0; x = 0; F best = +∞; if random(0, 1) < q(|x|) then 2: repeat else allocate(x); disconnect(x); end if if |x| = H then 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: until n = N or counter = M n = n + 1; counter = counter + 1; if F (x) < F best then F best = F (x); xbest = x; counter = 0; end if end if state x0 or the with the zero state (xip ≡ 0). The SAL algorithm may start with a given partial allocation it chooses, with state-dependent probability q(|x|), between In each step, allocation(x), i.e., adding the trafﬁc ﬂow for one non-allocated commodity to the current state x, and disconnect(x), i.e., removing the trafﬁc ﬂow for one allocated commodity from current state x. After a sequence of such moves, from time to time, the algorithm will reach a full allocation state, yielding a feasible solution for the considered problem. The procedure terminates when the number of visited full allocation states reaches a user-speciﬁed limit N or no better solution is found within M visited full allocation states. Procedure allocation(x) selects one currently non-allocated commodity, Ki = (si , ti , di ), at random and allocates it to one 519 6C-1 of the allowable paths that have enough residual capacity to support Ki . The path for allocating Ki is chosen to be the minimum cost path p with respect to the cost function for the NoC topology construction step. Then we add ﬂow xip = di to the current state x and reduce the capacities of the links on the selected path p in the routing graph by di . When routing commodity Ki , several new links and switches from the routing graph may be added to the NoC topology and the sizes of the switches on the path p may need to be adjusted accordingly. Procedure disconnect(x) selects an allocated commodity Ki = (si , ti , di ) at random and removes the corresponding ﬂow xip from current state x. We then increase the capacities of the links on the path p by di . If some links/switches become unused in the resulting solution, such links/switches are also removed from the NoC topology. The sizes of the switches on the path p may need to be adjusted accordingly. Function q(γ ), deﬁned for 0 ≤ γ ≤ H , has the properties: ⎧⎨ ⎩ q(0) = 1 q(H ) = 0 2 < q(γ ) ≤ 1, 1 0 < γ < H According to [26], if q(|x|) = q0 > 1 2 for 0 < γ < H then the expected average number of steps (allocations and disconnections) required to reach a full allocation state starting from state x is no greater than (H − |x|)/(2q0 − 1) For instance, if q0 = 2 3 then a full allocation state will be reached from the zero allocation state in only 3H steps. B.AnalyticalSwitchModelingforNoCs Accurate delay models for switches are required as an input to our 3D NoC synthesis problem. We utilize the analytical delay model presented in [27], which includes the effects of queueing delay and network contention. The model considers ﬁrst-come-ﬁrst-serve input buffered switches and targets wormhole ﬂow control under deterministic routing algorithms. Let S be the packet size and H the service time for a header ﬂit passing through switch i. The service time of a packet passing through switch i, excluding the queueing delay, is T = H + S − W f · W (1) put port j , and N = [N1 , N2 , . . . , Np ]T . where W is the data link width and f is the operating frequency. For switch i, let • p be the total number of ports. • λj be the trafﬁc arrival rate at port j . • Nj be the average number of packets in the buffers of in• cjk be the probability that packets of input ports j and k compete for the same output port, and Cj be the row • R be the residual service time seen by the incoming packets, deﬁned as follows: if another packet n is being served when packet m arrives, then R is the remaining time before packet n leaves the switch. vector Cj = [cj 1 , cj 2 , . . . , cjp ]. Then we can write the equilibrium condition for the switch as: (2) (I − T ΛC )N = Λ ¯R where Λ = diag{λ1 , λ2 , . . . , λp }, C = [C1 , C2 , . . . , Cp ]T , ¯R = ([R, R, . . . , R]1×p )T . The switch model described by Equation (2) provides a closed form expression for the average number of packets at each input port of the switch i, given the trafﬁc arrival rate (Λ), the packet contention probabilities (C ), switch design speciﬁcations (H , W ) and packet size S . For further details, the reader is referred to [27]. C.3DFloorplanning An initial step of thermally-aware ﬂoorplanning is applied to assign the cores into 3D tiers under thermal considerations, and to optimize the positions of the cores so that highly communicating cores are placed close to each other. In our implementation, we use the 3D thermally-aware ﬂoorplanner tool in [28] based on B(cid:3) -tree ﬂoorplan model. The ﬂoorplanner uses a built-in thermal analysis technique based on the HS3D [28] tool. Of course, any other similar tools can also be integrated into our program. For each edge evi ,vj which connects two cores, vi and vj , the edge weight of evi ,vj is set to be the product of edge bandwidth λ(evi ,vj ) and the distance dij between vi and vj . Our cost function is a weighted sum of the chip temperature and the sum of these edge weights. Therefore, we use the ﬂoorplanner to ﬁnd a good initial ﬂoorplan of cores that favors our next step of 3D NoC synthesis. During initial ﬂoorplanning, we only consider the communicating cores, since no switches have been introduced at this time. Once a full allocation of trafﬁc ﬂows is found, the topology of the NoC is determined, including the switches that are used to route trafﬁc. We then invoke the ﬂoorplanner to ﬁnd a reﬁned ﬂoorplan of cores and NoC switches, under an objective function that is a linear combination of design area, link power, link delay and chip temperature. D.SwitchLocationEstimationandPathCostEstimation When routing a ﬂow from source s to destination d, our objective is to ﬁnd a minimum cost path in the routing graph. While the initial solution considers the physical locations of only the cores, as ﬂow allocation proceeds, new switches will be included in the NoC topology and their physical positions must be estimated to compute the link power and delay. We estimate the switch locations in the following way: for a newly added switch i, the switch is initially placed at the centroid of the source and destination nodes of switch i in the routing graph. Given these initial estimates of the positions of the newly added switches, we apply Dijkstra’s shortest path algorithm on the routing graph to ﬁnd the minimum cost path for the trafﬁc ﬂow, which is required by allocation(x). When the 3D NoC synthesis step is complete, we feed the actual switches and links in the synthesized architecture to the ﬂoorplanner to update the switch locations, for more accurate power and delay estimation. Since the ﬂoorplanner is stochastic, it is possible for the new ﬂoorplan to be vastly different from the one that was used to generate the NoC topology, negating the assumptions used to build the topology. To avoid this, we add a penalty to the objective function of the ﬂoorplanner to ensure that the blocks do not move far away from their initial locations, and optimize the precise locations of the switches, which were initially placed in (possibly illegal) centroid locations. 520 V I . EX P ER IM EN TA L R E SULT S A.ExperimentalSetup We have implemented 3D-SAL-FP, our SAL-based 3D NoC synthesis algorithm with ﬂoorplan feedback, in C++. All experiments were conducted on an Intel Pentium 4 CPU 3.20GHz machine with 2G memory running Linux. The design parameters are set as: 900MHz clock frequency, 512-bit packets, 4-ﬂit buffers and 32-bit ﬂits. We use Orion [29] to estimate the power dissipation of the switches. The link power and delay are modeled based on the equations from Pavlidis et al. [8], and the delay of switches are estimated using the model described in Section V-B. All switches and links are evaluated under a 45nm technology. Several parameters affect the efﬁciency and performance of the SAL algorithm (Section V-A). we found that a constant function q(γ ) = q0, where q0 ∈ In our implementation, [0.75, 0.90], can produce good solutions. The user-speciﬁed iteration limit N is empirically set to be three times of k , the number of commodities in the core graph, and M is set to be 50. We ﬁnd that the best solutions are often obtained within k full visited allocation states for all the benchmarks. B.Impactofeachstrategyapplied inouralgorithm Our algorithm 3D-SAL-FP improves upon the previous algorithms in [14, 15] by: 1) using a more sophisticated trafﬁc ﬂow routing algorithm (SAL), 2) adding a feedback loop of ﬂoorplanning and NoC synthesis to reﬁne the NoC architecture, 3) using a more accurate switch delay model including the effects of queueing delay and network contention. To show the separate impact of these techniques on the NoC design, we have implemented three other 3D NoC synthesis algorithms. The ﬁrst algorithm, based on the work by Murali et al. [15], has two stages: 3D NoC synthesis and ﬂoorplanning of the synthesized NoC architecture. At the 3D NoC synthesis stage, simple delay model (average hop count) is used to approximate the average network latency and the trafﬁc ﬂows are routed in ﬁxed order (in the order of decreasing ﬂow rate). In the next stage, we move on to ﬁnd the ﬂoorplan of cores and used switches in the NoC architecture. We refer to this algorithm as the Baseline1 algorithm. The second algorithm differs from Baseline1 in that it applies an improved trafﬁc ﬂow routing strategy (SAL) in the 3D NoC synthesis stage. We refer to this algorithm as the Baseline2 algorithm. The third algorithm improves upon Baseline2 by feeding back the results of ﬂoorplanning stage to reﬁne the NoC synthesis. The process continues iteratively: after the 3D NoC synthesis step, the actual switches and links in the synthesized solution is fed back to the ﬂoorplanner to reﬁne the ﬂoorplan of the cores and used switches; with the reﬁned ﬂoorplan, a new NoC synthesis procedure is invoked to ﬁnd a better synthesis solution, and so on. We refer to this algorithm as the Baseline3 algorithm. Our 3D-SAL-FP differs from Baseline3 in that it use the accurate switch delay model (described in Section V-B ) to incorporate the queueing delay and network contention issues. We then applied these four algorithms to design 3D application speciﬁc NoC topologies. We compared these algorithms on both a set of existing published benchmarks and several large synthetic 3D benchmarks. Since large standard benchmarks are not available, we use the method proposed in [14] to generate the large synthetic 3D benchmarks. This method is based on the NoC-centric bandwidth version of Rent’s rule proposed by Greenﬁeld et al. [30]. For the small published benchmarks, two 3D tiers are used, where each tier contains one layer of devices and multiple layers of interconnect. For 6C-1 all of the large synthetic benchmarks, four 3D tiers are used. The corresponding results are shown in Tables I and II. For each algorithm, we report the following: the network power (in mW , including switch power and link power), the average network latency (in ns, evaluated by the accurate delay model), the number of TSVs and the maximum chip temperature (in ◦ C ). We can observe that using the improved trafﬁc ﬂow routing algorithm, the Baseline2 algorithm outperforms Baseline1, achieving 23% power saving for the published benchmarks, 10% reduction in chip temperature and better network performance. The corresponding numbers for synthetic benchmarks is 21% in power saving and 9% in chip temperature reduction. Furthermore, Baseline3 uses the feedback from the ﬂoorplanning step to improve upon Baseline2, and shows 34% reduction in the power dissipation for both published and synthetic benchmarks, about 20% reduction in chip temperature and 10% reduction in average network latency. Finally, with more accurate delay model, 3D-SAL-FP improves upon Baseline3, with 26% reduction in average network latency for published benchmarks and 44% for the synthetic benchmarks. Since the objective function for these algorithms is a linear combination of several metrics, the use of different sets of weighting factors can result in different Pareto-optimal solutions. For a fair comparison, we have used identical weighting factors for all four algorithms discussed here. In the solutions shown here, 3DSAL-FP performs signiﬁcantly better than Baseline3 in reducing the delay, and is slightly better on average (and sometimes worse on speciﬁc examples) in terms of power and temperature. By altering the weights, other tradeoff points may be identiﬁed. C.DelayandPowerReductionPotential in3DNoCs In this section, we further investigate the beneﬁts that 3D circuits can bring to the NoC architecture design. The benchmark B3, with 69 cores and 136 ﬂows, was selected and our 3DSAL-FP algorithm was applied to synthesize this benchmark with different numbers of 3D tiers, from 1 to 4. The 1-tier case is the design that uses conventional 2D technology. The results are shown in Table III. For each case, we list the following results: the design footprint, the network power, the maximum path length, the maximum total link delay, the maximum network latency, the average network latency, the total number of TSVs, the maximum chip temperature and the CPU time. As we can see from Table III, as the number of 3D tiers increases, the footprint size continues to decrease, together with the maximum length of the path to route the packets. The reduced path length further brings down the maximum link delay and the total link power at the cost of increased number of TSVs and higher chip temperature. The switch power depends on the speciﬁc network trafﬁc and the sizes of the switches determined by the customized NoC architectures and does not change much as the number of 3D tiers increases. V I I . CONC LU S ION We have proposed an efﬁcient algorithm, 3D-SAL-FP, to synthesize application-speciﬁc 3D NoC architectures. Our algorithm utilizes a stochastic approach called simulated allocation to reduce the dependency of NoC design results on ﬂow ordering. We also use accurate delay model for switches in NoCs which consider the queueing delay and network contention. Finally, our algorithm performs the ﬂoorplanning of cores/switches and NoC topology synthesis in an integrated iterative loop, attempting to ﬁnd the optimal solution for the problem of application-speciﬁc NoC design. Experimental re521 6C-1 Ben Cores Flows PIP 8 MWD 12 VOPD 12 MEPG4 12 IMP 27 8 13 15 26 96 TABLE I COM PAR I SON O F THR E E A LGOR I THM S ON S EV ERA L SMA L L PUB L I SH ED B ENCHMARK S Baseline1 Power Delay # of Tmax Switch Link Total TSVs 54 5 59 3.8 8 66.4 94 8 102 4.1 10 72.8 99 11 110 7.3 14 67.8 165 15 180 10.3 14 70.8 612 90 702 9.4 42 78.8 1 1 1 Baseline2 Power Delay # of Tmax Switch Link Total TSVs 44 4 48 3.7 6 60.6 74 7 81 4.0 9 66.5 82 10 92 7.2 7 64.5 108 15 123 10.1 13 64.7 413 99 512 8.0 44 65.2 0.77 0.95 0.90 Baseline3 Power Delay # of Tmax Switch Link Total TSVs 39 4 43 3.6 7 58.2 65 6 71 3.8 12 62.5 73 10 83 6.9 7 59.4 88 12 100 9.0 14 58.2 335 87 422 7.8 42 55.7 0.66 0.91 0.82 3D-SAL-FP Power Delay # of Tmax Switch Link Total TSVs 38 4 42 3.2 6 55.1 65 6 71 3.5 9 62.3 72 9 81 5.1 9 50.9 90 13 103 6.3 14 59.6 346 79 425 6.4 40 56.9 0.66 0.74 0.80 COM PAR I SON O F THR E E A LGOR I THM S ON LARG E SYN TH E T IC B ENCHMARK S TABLE II Ben Cores Flows B1 B2 B3 B4 B5 56 80 69 114 124 196 96 136 396 266 Baseline1 Power Delay # of Tmax Switch Link Total TSVs 1033 291 1324 16.3 119 157.8 783 128 911 7.9 117 133.5 866 210 1076 13.1 122 150.6 3128 827 3955 15.9 196 166.4 1827 848 2675 13.9 254 135.9 1 1 1 Baseline2 Power Delay # of Tmax Switch Link Total TSVs 956 302 1258 16.0 132 145.4 561 118 689 7.9 116 119.6 494 243 737 12.0 95 134.4 2230 888 3118 15.5 214 151.6 1517 686 2203 11.8 264 125.2 0.79 0.94 0.91 Baseline3 Power Delay # of Tmax Switch Link Total TSVs 808 209 1017 15.0 139 128.3 490 99 589 7.6 124 107.1 509 165 674 11.5 105 118.2 1826 643 2469 13.9 192 128.6 1352 432 1784 11.4 256 104.4 0.66 0.89 0.79 3D-SAL-FP Power Delay # of Tmax Switch Link Total TSVs 785 214 999 6.7 132 133.2 494 96 590 4.6 126 107.5 504 141 645 9.4 116 118.0 1721 632 2353 7.3 208 137.0 1338 468 1806 9.1 241 102.7 0.65 0.56 0.80 TABLE III COM PAR I SON O F TH E IM PAC T O F D I FF ER EN T NUMB ER S O F 3D T I ER S ON NOC ARCH I T EC TUR E D E S IGN FOR B ENCHMARK B3 Layers Footprint (mm2 ) 216.8 110.3 72.0 56.1 1 2 3 4 Network Power Maximum Path Maximum Link Maximum Network Average Network # of Tmax Time Switch Link Total Length Delay Latency Latency TSVs ◦C ) (mW ) (mW ) (mW ) (mm) (ns) (ns) (ns) ( (s) 510.5 288.4 798.9 22.1 6.45 14.40 12.42 0 43.8 85.8 505.8 189.2 695.0 17.0 4.95 12.28 9.56 86 63.7 83.9 510.7 164.8 675.5 11.9 3.50 11.51 9.49 94 96.2 87.3 504.8 141.0 645.8 9.2 2.68 11.32 9.44 116 118.0 87.4 sults on a set of benchmarks show that our algorithm can produce greatly improved solutions. ACKNOW L EDGM EN T S The authors would like to thank professor Yuan Xie in Pennsylvania State University for providing the 3D ﬂoorplanner. "
2011,On the design and analysis of fault tolerant NoC architecture using spare routers.,"The aggressive advent in VLSI manufacturing technology has made dramatic impacts on the dependability of devices and interconnects. In the modern manycore system, mesh based Networks-on-Chip (NoC) is widely adopted as on chip communication infrastructure. It is critical to provide an effective fault tolerance scheme on mesh based NoC. A faulty router or broken link isolates a well functional processing element (PE). Also, a set of faulty routers form faulty regions which may break down the whole design. To address these issues, we propose an innovative router-level fault tolerance scheme with spare routers which is different from the traditional microarchitecture-level approach. The spare routers not only provide redundancies but also diversify connection paths between adjacent routers. To exploit these valuable resources on fault tolerant capabilities, two configuration algorithms are demonstrated. One is shift-and-replace-allocation (SARA) and the other is defect-awareness-path-allocation (DAPA) that takes advantage of path diversity in our architecture. The proposed design is transparent to any routing algorithm since the output topology is consistent to the original mesh. Experimental results show that our scheme has remarkable improvements on fault tolerant metrics including reliability, mean time to failure (MTTF), and yield. In addition, the performance of spare router increases with the growth of NoC size but the relative connection cost decreases at the same time. This rare and valuable characteristic makes our solution suitable for large scale NoC design.","5B-1 On the Design and Analysis of Fault Tolerant NoC Architecture Using  Spare Routers Yung-Chang Chang  Ching-Te Chiu  Shih-Yin Lin, Chung-Kai Liu  Information & Communications Research Lab.  Industrial Technology Research Institute  Hsinchu, Taiwan, R.O.C.  ycchangs@itri.org.tw Dept. of Computer Science  National Tsing Hua University  Hsinchu, Taiwan, R.O.C.  ctchiu@cs.nthu.edu.tw  Information & Communications Research Lab.  Industrial Technology Research Institute  Hsinchu, Taiwan, R.O.C.  {bryan.lin, ckliu}@itri.org.tw Abstract - The aggressive advent in VLSI manufacturing  technology has made dramatic impacts on the dependability of  devices and interconnects. In the modern manycore system, mesh  based Networks-on-Chip (NoC) is widely adopted as on chip  communication infrastructure. It is critical to provide an  effective fault tolerance scheme on mesh based NoC. A faulty  router or broken link isolates a well functional processing  element (PE). Also, a set of faulty routers form faulty regions  which may break down the whole design. To address these issues,  we propose an innovative router-level fault tolerance scheme with  spare routers which  is different  from  the  traditional  microarchitecture-level approach. The spare routers not only  provide redundancies but also diversify connection paths  between adjacent routers. To exploit these valuable resources on  fault tolerant capabilities, two configuration algorithms are  demonstrated. One is shift-and-replace-allocation (SARA) and  the other is defect-awareness-path-allocation (DAPA) that takes  advantage of path diversity in our architecture. The proposed  design is transparent to any routing algorithm since the output  topology is consistent to the original mesh. Experimental results  show that our scheme has remarkable improvements on fault  tolerant metrics including reliability, mean time to failure  (MTTF), and yield. In addition, the performance of spare router  increases with the growth of NoC size but the relative connection  cost decreases at the same time. This rare and valuable  characteristic makes our solution suitable for large scale NoC  design.  I. Introduction  As semiconductor technology advances into deep submicron  (DSM) regime, the increasing transistor budget and chip density have  enabled the integration of ten to hundreds of pre-designed IPs. This  shrinking of feature size and intensive combinations of homogeneous  and heterogeneous modules have revealed the significance of on-chip  networks (OCNs) design [1]. Traditional bus-based OCN architecture  suffers from the issues of scalability, performance, and power.  Therefore, packet based Network-on-Chip (NoC) has evolved as a  new paradigm to overcome these design challenges [2], [3]. Recently,  many NoC architectures have been proposed [4]. Among these  candidates, 2D-mesh is considered as the most attractive one since it  provides regularity  in both physical structures and electrical  properties [5]. As depicted in fig. 1(a), a homogeneous manycore  system with 4x4 2D-mesh NoC consists of 4 routers in each rows and  columns, and each router is a switching element with 5 ports  connecting its four neighboring routers and a local processing  element (PE).  However,  this aggressive  integration of computation and  communication elements has worsened the dependability issues in  DSM. Manufacturing defects introduce permanent faults on nodes  and links. It relies on the built-in self-test (BIST) circuit to capture  and mask these faulty elements in the testing procedure [6]. To  sustain the reliability, the system should provide a mechanism to  bypass and replace  these faulty elements. Recently, several  researches have been made on the fault tolerance routing algorithm,  in which faulty routers/paths are detoured [7], [8]. However, they are  not capable of solving a series of problems caused by faulty routers.  For example, fig. 1(b) shows four well functional PEs which are  isolated by faulty routers and broken links. This leads to a great  waste of hardware resource since the cost of a PE is much higher  than that of a router or links. A more elaborate example is illustrated  in fig. 1(c). A set of faulty routers may shape faulty regions which  break down the whole design. Existing fault tolerance routing  algorithms fail because there are no detouring paths between these  faulty regions. Although faulty regions can operate individually, this  deteriorates the performance in the manycore architecture in which  applications are highly parallelized and intensively cooperated. To  recover from a permanent fault, hardware redundancy is mandatory.  Microarchitecture level redundancy has been proposed in NoC design.  However, as the size of mesh increases and the cost of a single router  becomes relatively inexpensive when comparing to the entire NoC  (e.g., a 128 routers), microarchitecture  level redundancy  is  considered inefficiency [9]. As the result, it is reasonable to provide  redundancies in the router level.  In this work, we proposed a novel router level redundancy scheme  that  ingeniously  solves  the  problems mentioned  above  simultaneously. In addition, our design does not need any support  from the routing algorithm. This transparency to the upper layer  routing algorithms convinces our architecture a viable solution in the modern complex hierarchical hardware/software system designs. In  summary, our contributions are threefold:  1. Resolve the problem of isolated PE  2. Resolve the problem of faulty regions  3. The proposed architecture is routing algorithm transparency  We evaluate our design with fault tolerance metrics including  system reliability, mean time to failure (MTTF), and yield. Also,  while the underlying physical configuration is changed, applications  can be remapped to optimize the performance. This mapping problem  belongs to the class of NP-hard. We apply a heuristic algorithm to  reveal the potential of performance improvement.     The rest of this paper is organized as following: Section II  provides reviews of previous researches on fault tolerance NoC  designs. Section III describes the proposed fault tolerance NoC  architecture. In section IV, evaluation methodology and experimental  results are provided. Section V shows a case study to illustrate the  potential of performance improvement using application remapping.  Section VI concludes this paper. II. Related Works  978-1-4244-7516-2/11/$26.00 ©2011 IEEE 431 5B-1 (a) (b)  (c) Fig. 1. Problems of isolated PE and faulty regions. (a) A conventional faulty free 4x4 2D mesh. (b) Isolated PE caused by faulty routers and  broken links. (c) Faulty regions caused by faulty routers  Fault tolerance design in VLSI has been an active research topic.  As the raise of NoC for complex nanometer SoC design solutions,  several error control mechanisms have been proposed to address chip  level network failures on channels and routers. VLSI circuit faults  can be classified into three categories: transient, intermittent, and  permanent failures [12].   Transient faults are introduced by power supply noise, ground bounce, energetic particles, and interconnection noise such as  crosstalk and electromagnetic interference, etc. These faults occur  randomly and do not cause physical damages on circuits. Encoding  techniques are pervasively used to tackle these errors. For example,  Srinivas et al. presented an encoding framework to combat coupling  noise and transitions on bus wires [18]. On the other hand, devices  experience intermittent failures by marginal designs which are too  sensitive to the disturbances of outside environment conditions.  Intermittent faults may appear with locality in time and/or space.  Permanent  faults  are  induced by manufacturing defects,  electromigration, and wearout failures. In order to maintain the  system operations, spare hardware is usually used to replace  intermittently or permanently faulty units. Error control mechanisms  can be applied at different hierarchies spanning from the link layer to  the highest thread layer [14], [15], [19]. In our work, we address  these faults using spare units on both links and routers.  More specifically, we focus on faults that can be caught by add-on  mechanisms such as BIST or periodical run-time testing circuits.  Teijo et al. use spare wires as vehicle for in-line testing and erroneous  path detouring [16]. Opens and shorts occurred on adjacent switch  channels are tested periodically with error control code (ECC).  However, only the channel failures are addressed. Michihiro et al.  propose a mechanism in which faulty router is bypassed using default  backup paths (DBPs) [17]. Unfortunately, both the underlying  physical topology and the logical topology exposed to upper layer  applications are changed. New routing algorithm must be added to  support the proposed design. In our work, we adopt the idea of spare  links and combine them with spare routers. This innovative approach  does not impose any interference on routing algorithms.  Several fault tolerant routing algorithms have been proposed to  tackle defect on NoC. In [7], [13], randomized and stochastic  algorithms are presented in which packets are sent to a set of  destinations. Traditional fault tolerant routing algorithms introduce  congestion around faulty nodes. In [10], a traffic-balanced fault  tolerant routing algorithm is proposed to distribute this heavy traffic  load. However, routing level fault tolerance does not address the  problems of isolated PE and faulty regions mentioned above.  Traditionally, microarchitectural redundancy is widely adopted for  improving dependability in processor designs [20]. Recently, the state  of the art researches have shown that microarchitecture level  redundancy has the drawback of scalability [9]. As the number of  cores increases, microarchitectural redundancy within cores has  higher cost than that of core level redundancy. Motivated by this idea,  we propose a new scalable fault tolerance scheme for NoC at the  router level.  In contrast to previous researches, our design is innovative since  router-level redundancy on NoC is not seen before. We tackle defects  in both NoC routers and interconnection channels. We also consider  the mapping problem to minimize the cost parasitizing from  reconfiguring the NoC interconnections.  III. Designing Fault Tolerant Mesh Based On Chip  Networks Using Spare Routers In this section, we first formalize the fault tolerant mesh based  interconnection into graph embedding problem. After that, the  proposed spare router architecture is described. We also present two  path allocation algorithms. The first one utilizes spare routers is  shift-and-replace-allocation  (SARA). The  second one  takes  advantage  of  path  diversity  in  our  architecture  is  defect-awareness-path-allocation (DAPA).   A. Problem Definition  A fault tolerant interconnection system can be modeled using  graph theory. A graph G* = (V, E) where V is the vertex set  representing switching nodes, and E is the edge set representing  interconnection channels. G* is said to be fault tolerant with respect  to G if after removing the faulty nodes/channels in G*, G* contains  G. There are a lot of variants of G*. A G* is said to be k-optimal node  free fault tolerant if any k nodes can be removed and G* has the  minimum edges. In practice, it is unfeasible to design an optimal G* for any k. In our design, for an N x N mesh with N spare router, each  column can have one faulty router. As the result, there are at most N faulty routers simultaneously.  B. 2D Mesh with Spare Router Redundancy  To address the problems mentioned above, we propose a novel  router level redundancy scheme as shown in fig. 2(a). The original  2D mesh is partitioned into groups of column set. Every router within  a column set collectively shares a common spare router (SR) setting  on the top row. We duplicate the connection path from the PE to a  pair of adjacent routers. As the result, every PE has a chance to  detour the attached faulty router or broken links. This dual link/router  scheme relieves the symptoms of isolated PE. The connection path is  configured by the ROM through add-on circuits like BIST. Fig. 2(b)  shows an example of three faulty routers and the connection patterns  between PEs and routers. To support the idea of spare router  architecture, the underlying interconnection structure is depicted in  fig. 3(a). For clarity, PEs are not shown in this figure. In the vertical  direction, every input port is attached with a 2-to-1 mux (except the  boundary routers). In the horizontal direction, every input port is  432            (a)                        (b)   Fig. 2. Spare router architecture. (a) 2D mesh NoC with spare  router. (b) Isolated PE path recovery attached with a 3-to-1 mux except the boundary routers (which are  attached with 2-to-1 mux). If a router is failed, for example, marked  R5 in fig. 3(b), the vertical interconnections are reconfigured to  detour the faulty router using 2-to-1 muxs. The original faulty router  is replaced by its upper adjacent router. For this reason, every router  above the faulty one is shifted up cascaded by one position.  Comparing fig. 3(a) with fig. 3(b), it is noted that the original R6 is  replaced by R5 and SR7 is replaced by R6. On the other hand, the  horizontal paths between adjacent columns are configured to  maintain the mesh topology.   We address the path allocation problem in the next two  subsections. Fig. 3(b) also demonstrates the faulty region problem in  which faulty routers (marked R2, R5, and R10) cut the design into  upper (R2, R5, R6, and R10) and lower (R0, R1, R4, R8, and R9)  half separated regions. As you can see, faulty routers/regions are  hardware bypassed. The resulting topology is still connected in the  2D mesh manner as shown  in fig. 3(c). In addition,  this  interconnection structure also inherently provides path diversities  5B-1 between adjacent router columns. With our defect awareness  configuration approach, as the experimental results show, this scheme  dramatically improves the system yield.  C. Shift-And-Replace-Allocation (SARA)  We first provide a simple and fast path configuration algorithm  called shift-and-replace-allocation (SARA). Routers with various  complex hardware failure modes are modeled by a survival matrix.  For example, fig. 3(b) has three faulty routers R2, R5, and R10. Fig.  3(d) is the corresponding survival matrix. The vertical path  configuration is straightforward since there exists only one way to  detour faulty router vertically. The horizontal paths are configured  through two iterations steps. The first step is to pick up the first  non-marked survival router. This step is performed column by  column. For example, R0, R4, R8 are selected and marked in the first  step. In the second step, muxs are configured to set up paths  connecting those candidate routers. In the second iteration, R1, R5,  R9 are picked up, and so on. The detail of SARA is listed in  algorithm 1. We assume that all routers are initially connected to their  neighboring routers. SARA is a linear time algorithm.  Algorithm 1. SARA (Ms, Mc) Input: Ms: survival matrix  Output: Mc: configured spared mesh.  1: initialize Rs = (cid:1486) 2: for r = 1 to RowSize(Ms)-1             //horizontal connection  3: for c = 0 to ColumnSize(Ms)-1  4:     p = first non-marked “1” in column c of Ms 5:     Rs = Rs(cid:1046)p 6:     Mark(p) 7: end for  8:   Connect(Rs , Mc) 9: Rs = (cid:1486) 10: end for 11: for c = 0 to ColumnSize(Ms)-1          // vertical connection  12: z = find “0” in column c of Ms 13: if z (cid:1035) (cid:1486) and is not in the boundary  14:     Connect(z-1,z+1)  15: end if 16: end for Return Mc 17: D. Defect-Awareness-Path-Allocation (DAPA)  Although SARA provides a convenient way to configure paths, it  only takes advantages of spare routers and does not exploit the full  potential of our architecture. As the complexity of SoC increases,  link reliability plays a critical role in the chip yield. The proposed  architecture inherently provides path diversities between adjacent  router columns. According to the faulty patterns, there are three cases  between two adjacent router columns. In the first case, as shown in  fig. 4(a), two adjacent routers are all faulty free (column 0 and 1). In  the second case, only one column has a faulty router (column 1 and  2). In the final case, both columns have a faulty router (column 2 and  3). For a column with N routers, in case one, there exists C(N,N-1)* C(N,N-1) connection patterns. In case two, there exists C(N,N-1)  connection patterns. In the final case, there exists only one possible  connection pattern. Fig. 4(b) is a 4-router column example.  Connection patterns between column 1 and 2 are demonstrated in fig.  4(c). Since there are a variety of possible connections in case 1 and 2,  to  take  advantage  of  this  facility, we  propose  a  defect-awareness-path-allocation  (DAPA)  algorithm which  dramatically improves the chip yield. The horizontal path allocation  of DAPA is listed in algorithm 2. In the iteration, DAPA examines  every possible match between two adjacent columns until a defect  free one is found.  Fig. 3. (a) Underlying interconnections of spared mesh. (b) Path  configurations to detour faulty regions/routers. (c) Resulting  topology is consistent with 2D mesh. (d) Survival matrix. 433 Fig. 4. (a) Faulty patterns between router columns. The left column  is column 0. (b) Connection pattern numbers of a 4-router column.  (c) All connection patterns between column 1 and 2. Algorithm 2. DAPA (Mo, Mc) Input: Mo: original spared mesh  Output: Mc: configured spared mesh.   1: initialize Cs = ColumnSize(Mo) 2: for c = 0 to Cs 3: tc = column c of Mo without faulty router  4: tc+1 = column c+1 of Mo without faulty router  5: Tc = Combinations(tc , Cs) 6: Tc+1 = Combinations(tc+1 , Cs) 7: for all vector vc (cid:1488) Tc , vc+1 (cid:1488) Tc+1 8:    Sort vc , vc+1 with router index order 9:    z = FindMatch(vc , vc+1) 10:    if PathTest(z) is pass 11:     Connect(z, Mc) 12:     Break 13:    end if  14: end for 15: end for Return Mc 16: IV. Evaluation Methodology and Experimental Results  To evaluate our design, we perform three most important fault  tolerance metrics: reliability, mean time to failure, and yield.  A. Reliability Analysis  The reliability of an NoC router Rr(t) is the probability that a  router performs its functionalities correctly from time 0 to time t. It is  decided by the failure rate, (cid:1267)(t) , which is measured by the number  of failures per time unit. After the router has passed the infant  mortality period, we can express Rr(t) using exponential failure law  [11]:  The analytical model for the traditional NxN mesh and (N+1)xN spared mesh are given in equation (2) and (3). In this paper, we  assume n = N in all equations. It is obvious in equation (2) that all  routers in an NxN mesh have to be operational to ensure the system’s  operations. As the result, the system reliability is the product of all  routers’ reliability. On the other hand, we split a spared mesh into N columns. To ensure the system’s operation, every column must be  operational. As shown in equation (3), the reliability of a system is  the product of columns’ reliability Rc. A column is operational if all  the routers within the column are operational or if only N of the N+1 routers are operational.  R We can now perform the quantitative reliability analysis with  equations (1) ~ (3). We assume the failure rate(cid:691) of a router is (cid:1267)= 0.00315(times/year) [14]. The reliability of different mesh size over 1  to 10 years is shown in fig. 5. As you can see, the spared mesh (SM)  architecture successfully outperforms the traditional one. As the mesh  size increases, the reliability curve of traditional 10*10 meshes drops  sharply from 0.74 to less than 0.1 over 1 to 10 years. However, the  spared one still maintains a considerable reliability in 10th year. To  highlight the effectiveness of our design, the reliability gain is shown  in fig.6. As the mesh size increases, the relative cost of spare router  decreases. However, as you can see, the reliability gain increases  proportionally to the mesh size and time. This result is valuable in the  architectural design.  B. Mean Time to Failure Analysis  The mean time to failure (MTTF) is the average time before a  system fails which can be expressed as the area under the reliability  curve:  We substitute equation (2) into (4), the MTTF for an NxN mesh  can be derived as the following:  (4) Fig. 6. Reliability gain of various size of (spared) mesh from  1~10 years 14 12 10 8 6 4 2 0 1 2 3 4 5 6 7 8 9 10 R e i l b a t i l i y G n a i Years Mesh vs. SM 10x10 vs.  11x10 8x8 vs. 9x8 6x6 vs. 7x6 4x4 vs. 5x4 Fig. 5. Reliability of various size of (spared) mesh from 1~10  years  1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 1 2 3 4 5 6 7 Years 8 9 10 R e i l b a t i l i y 5x4 SM 7x6 9x8 11x10 4x4  Mesh 6x6 8x8 10x10 (2)  (3)  (cid:540)t r −= e (t)R  (1)  nn × r mesh sys R = ) ( _ n nn r r n n n r c spared sys RR )( C R R R ]) 1( [ ) ( + 1 + 1 _ − + = = (cid:179) ∞ = 0 tR )( dt MTTF 5B-1 434                    1 nn ** λ By the same way, equation (3) is substituted into (4), the MTTF of  an (N+1)xN spared mesh is given by:   (6)  The MTTF for mesh (spared mesh) size from 3*3 (4*3) to 10*10  (11*10) is shown in fig. 7. The gain of MTTF is also depicted to  highlight that the effect of spared router grows with the NoC size.  C. Yield Analysis  The yield of our NoC router is modeled by the yield of router  switch fabric, control logic, and input ports (including multiplexers in  our design). Recent research has shown that correlation factor ((cid:1274)) between components should also be introduced in the yield  calculation [21]. According to [21], our router ’s yield is modeled by  equation (7). We assume a channel has 64-bit data wires, 4-bit control  and 8-bit parity wires. Table I lists the parameters used in our yield  evaluation work [21]. We perform the Monte Carlo simulation for the  NoC yield. Fig. 8 shows the flow chart of our simulation. The  experimental results are shown in fig. 9. The spared mesh with  SARA has the same yield curve in contrast with the traditional mesh.  However, the spared mesh with DAPA has different curve since it  drops slowly as the mesh size increases. The margin between SARA  and traditional mesh is purely earned by adding spare routers.  However, our architecture also provides path diversities between  adjacent router columns. DAPA takes advantage of this characteristic  and, as you can see, dramatically improve the system yield.   V. Performance Improvement with Application  Remapping The user applications of NoC-based manycore system are usually  optimized to meet the topology at the design time. Programmers need  to keep in mind the logical mesh topology. However, the underlying  physical interconnections are reconfigured to mask hardware defects.  A detouring wire introduces overhead of double wire length in  Manhattan grid as shown in fig. 10(a). This reveals the opportunity of  further performance tuning with application remapping. To map  applications onto a mesh topology is an instance of well known  NP-hard problem. We exemplify this remapping idea through a video  object plane (VOP) decoder [22] using parameterized simulated  annealing [23]. The task graph of the VOP decoder in [22] is redrawn  in fig. 10(b). A 4x4 mesh and a 5x4 mesh with spare routers are used  Fig. 7. MTTF and MTTF gain 7.7581 2.6988 1.2342 3.0864 0.7716 0.2778 2 2.5 3 3.5 4 4.5 8 7 6 5 4 3 2 1 0 3x3 4x4 5x5 6x6 Mesh Size 7x7 8x8 9x9 10x10 M F T T G n a i M F T T ( 0 1 5 H u o ) s r Spared Mesh Mesh MTTF Gain Fig. 9. Yield analysis  1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 Y i e l d Mesh Size Spared  Mesh  (DAPA) Spared  Mesh  (SARA) Mesh Fig. 8. Monte Carlo simulation  TABLE I  PARAMETERS FOR YIELD CALULATIONS Parameter Description Clustering parameter  Correlation factor  Wire yield Yield of data, control, and parity wires Logic yield Yield of switch fabric, control, and port Value 2 0.5 99.99% 99.5% (cid:1257) (cid:1274) (7)  λ ) ∞ − 0 nn (* dt e MTTF t mesh (cid:179) = =         (5) [ [ ] RR ) ] dt R n nR dt C R MTTF n n r n r n n r r n n n r mesh spared (cid:179) (cid:179) 0 ∞ + ∞ + + + + − = − + = 0 1 1 1 _ )1 ( 1( − α 1 − α port 1 − α port − 1 α port 1 − α ctrl 1 − α port − 1 α sw 1 − α ctrl 1 − α sw 1 − α ctrl 1 − α sw 1 − α ctrl 1 − α sw σ ( Yield σ ( Yield σ ( Yield σ ( Yield − − − +− )1 − +− )1 − +− )1 − +− )1 +− )1 +− )1 += (1( Yield ))1 )(1 Yield )(1 Yield )(1 Yield )(1 Yield )(1 Yield ( Yield ( Yield router Yield 5B-1 435     5B-1 as the underlying topology. We consider the wo rst case of a defect  mesh with our spare router architecture usin ng remapping and  detouring schemes. That is, every column has a a faulty router. The  cost is calculated by multiplying the VOP task gra aph edge weight and  inter-router channel length (measured by Manha attan distance). Fig.  11 is an example of mapping result. Simulation  results are listed in  table II. Under the worst case mentioned above,  a defect mesh with  spare routers using remapping scheme introduces s only 6% overhead  compared to the non-defect version and the overh head increase is 22%  using detouring scheme.               (a)                      (b b) Fig. 10. (a) Detouring/Unchanged path cost under r Manhattan  distance (b) Task graph of VOP decoder   Fig. 11. Example of mapping VOP decoder onto  and a defect spared mesh with 4 faulty routers.   a non-defect mesh  APPLICATION REMAPPING AVERAGE C COST TABLE II  Non-defect  mesh  4153  Average  Cost  Defect mesh with  spare router using  remapping Defect mesh with  spare router using  detouring scheme 4424  5093  VI. Summary and Conclusio ons In this paper, we propose a router-level redu undancy scheme on  fault tolerance NoC design. The architecture is sim mple and structural.  The combination of spare routers and diversifie ed inter-router links  successfully improve the system reliability, MT TTF, and yield. In  addition, two notorious problems on NoC includ ding isolated PE and  faulty regions can be relieved through our  design. Two path  configuration algorithms are demonstrated. Faul lty routers/links are  hardware detoured and the resulting topology is s still consistent to the  original mesh. This implies that our design can a achieve transparency  to the upper layers, i.e. routing algorithms, oper rating systems, and  user applications. In addition, the reliability gai in and yield of our  design increases with the growth of NoC size b but the relative cost  decreases in the same time. This valuable chara acteristic convinces  that our fault tolerance scheme is especially sui itable for the future  large scale NoC design. "
2011,Power-efficient tree-based multicast support for Networks-on-Chip.,"In this paper, a novel hardware support for multicast on mesh Networks-on-Chip (NoC) is proposed. It supports multicast routing on any shape of tree-based paths. Two power-efficient tree-based multicast routing algorithms, Optimized tree (OPT) and Left-XY-Right-Optimized tree (LXYROPT) are also proposed. XY tree-based (XYT) algorithm and multiple unicast copies (MUC) are also implemented on the router as baselines. Along with the increase of the destination size, compared with MUC, OPT and LXYROPT achieve a remarkable improvement in both latency and throughput while the average power consumption is reduced by 50% and 45%, respectively. Compared with XYT, OPT is 10% higher in latency but gains 17% saving in power consumption. LXYROPT is 3% lower in latency and 8% lower in power consumption. In some cases, OPT and LXYROPT give power saving up to 70% less than the XYT.","4B-4 Power-efﬁcient Tree-based Multicast Support for Networks-on-Chip Wenmin Hu1,2 , Zhonghai Lu2 , Axel Jantsch2 , Hengzhu Liu1 1School of Computer, National University of Defense Technology, Changsha, P.R. China 2Royal Institute of Technology, Stockholm, Sweden 1{huwenmin,hengzhuliu}@nudt.edu.cn 2{whu,zhonghai,axel}@kth.se Abstract— In this paper, a novel hardware support for multicast on mesh Networks-on-Chip (NoC) is proposed. It supports multicast routing on any shape of tree-based paths. Two powerefﬁcient tree-based multicast routing algorithms, Optimized tree (OPT) and Left-XY-Right-Optimized tree (LXYROPT) are also proposed. XY tree-based (XYT) algorithm and multiple unicast copies (MUC) are also implemented on the router as baselines. Along with the increase of the destination size, compared with MUC, OPT and LXYROPT achieve a remarkable improvement in both latency and throughput while the average power consumption is reduced by 50% and 45%, respectively. Compared with XYT, OPT is 10% higher in latency but gains 17% saving in power consumption. LXYROPT is 3% lower in latency and 8% lower in power consumption. In some cases, OPT and LXYROPT give power saving up to 70% less than the XYT. I . IN TRODUC T ION Many-core architectures have become the mainstream for designing System-on-Chip. The traditional bus structure is unable to meet the requirement of performance and is not scalable. The Network-on-Chip (NoC) is proposed to solve the global interconnection problems of these systems. Some NoCs have been developed, such as NOSTRUM [1], RAW [2], TRIPS [3], SPIN [4] etc. NOC architectures supporting for unicast can be used to implement multicast trafﬁc by replicating multiple unicast messages to different destinations, however this way is inefﬁcient. If the destination set is large, the injection port of the source node will have difﬁculty in injecting all the packets into NoCs in a limited period, which leads to late startup time for some destinations. Therefore the architectures that support multicast are promising solutions to the scenario. This paper proposes two power-efﬁcient tree-based multicast routing algorithms for Mesh NoC. OPT tries to use the minimum number of links to construct the multicast tree. LXYROPT, on the premise of keeping the low latency, tries to minimize the number of links in the multicast tree. Compared with XYT algorithm, OPT is 10% higher in latency but gains 17% saving in power consumption. LXYROPT is 3% lower in latency and 8% lower in power consumption. The paper is organized as follows. In section II, a brief review of related work is presented. In section III, the multicast mechanism and algorithm realization are discussed. In section IV, the multicast protocol and implementation are described. The results of experiment are shown in Section V. The conclusion and future work are given at the last section. I I . R E LAT ED WORK Multicast on off-chip network were well researched in [5, 6, 7, 8]. The results show that multicast on off-chip network has outstanding effect on improving the performance. Beneﬁt from multicast on off-chip network should also be suitable for the communications on-chip. Æthereal [9] and Nostrum [1] both declared multicast is supported on their NoC architecture. Connected-oriented multicasting in wormhole-switched NoC has been presented in [10]. In their scheme, multicast includes establishment, communication and release phases. For the reason that they used just one setup packet to build the whole path, the latency of the setup is long [10]. An ID-tag-based multicast routing is proposed in [11]. In this paper, a ﬂexible mechanism to manage broadcast-ﬂow to share the communication link in NoCs is presented. They used an ID-tag to manage each multicast ﬂow [11]. A tree-based routing algorithm which has hardware support for multicast (VCTM) was presented in [12]. Similar to [11], VCTM used ID-tag-based Multicast routing named virtual circuit table (VCT). It constructs the multicast tree incrementally by sending a unicast packet to each destination node. Each setup packet is routed using the DimensionOrdered Routing (DOR) algorithm. The method has the advantage of low latency for the packets. But in some cases, it is not power efﬁcient if the destinations are distributed along the X axis. Since the tree-based approaches are easily blocked at branches, they perform badly in the case of high trafﬁc load. Path-based approaches have also been researched to overcome the shortcoming of the tree-based ones. One path-based routing algorithm is Hamilton path algorithm where a unidirectional Hamilton path of the network is constructed [13]. It can be organized as dual path (DP) and multi path (MP) [13]. Another way is to organize based on column path (CP), destinations are partitioned into the 2k subsets, where k is the number of columns in the mesh [14]. A LD path-based algorithm is presented in [15], which partitioned the destination set into four subsets, just like MP [13], but did some optimization on the path distance of the subsets [15]. Path-based ones also have a shortcoming in long latency for the packet transfer. This can be optimized by partitioned the destinations into many sets [13, 14], but it increases the possibility of the injection contention at the source node and reduces the sharing of the links. 978-1-4244-7516-2/11/$26.00 ©2011 IEEE 363 4B-4 I I I . MU LT ICA S T M ECHAN I SM AND A LGOR I THM R EA L I ZAT ION In this section we propose two multicast algorithms, which are both tree based. A tree can be decomposed into several node pairs. The ﬁrst node of the pair is considered as starting point of a branch while the second node is the end point. The multicast tree is built incrementally by adding branch one by one to the existing tree. The initial tree is just the source node. The ﬁrst step of constructing multicast tree is to ﬁnd all the node pairs that form the tree. The same destinations may be covered by different shape of trees. Different shape of trees may cause different performance and power consumption. Both proposed algorithms are power-efﬁcient. A.Mechanism We give an example to illustrate the mechanism of constructing a tree. Fig. 1 shows a multicast tree on a 3× 3 mesh NoC. A is the source node, while C and B are the destination nodes and D is the branch node. Assume we use the XY routing algorithm to construct the branch of the tree. The tree is decomposed into two branches, which are represented by two node pairs: (A,B) and (D,C). node pair with shortest distance and less branches. Secondly, add (u, v) to Dpair . Add all the nodes in the path from u to v to the set Dnode . Remove v from D . If D is not empty, the sequence will be repeated. By the way, it is possible to ﬁnd a node pair that u and v are the same node, for the reason that the ﬁrst path may contain the destination nodes. However, it doesn’t matter and we can just put the pair to Dpair . When D is empty, the procedure is ﬁnished. Deﬁne: k(a, b) = |a.y − b.y | + |a.x − b.x|; Algorithm: Generate the optimized multicast tree based on the west-ﬁrst turn model Input: Destination set D , Source node s : (x0 , y0 ); Output: Pair set Dpair ; Initial: Dnode ← s, Dpair ← ∅; 1: Find the node v ∈ D , ∀a ∈ D , v .y ≤ a.y . Add (s, v) into Dpair , remove v from D , add the nodes on the path from s to v into Dnode 2: while D is not empty do 3: Dpair tmp ← {(u, v)|u ∈ Dnode , v ∈ D , that ∀a ∈ Dnode , ∀b ∈ D , k(u, v) ≤ k(a, b)} Select (u, v) ∈ Dpair tmp , that ∀(a, b) ∈ Dpair tmp , v .y ≤ b.y 4: A 5: Add (u, v) into Dpair , remove v from D , add the nodes on the path from u to v into Dnode 6: end while C D B Fig. 1. Example for tree construction Fig. 2. Algorithm for generating OPT. B.OptimizedTree(OPT)Algorithm OPT is an optimized tree based on the west-ﬁrst turn model [16], which avoids deadlock on mesh networks. In order to minimize the number of links in the tree, an algorithm similar to the minimal spanning tree algorithm is proposed, which makes the tree power-efﬁcient. The algorithm is shown in Fig. 2. Dpair is deﬁned as the pair set that forms the tree. Dnode is the set of the nodes covered by the existing tree. It contains the forwarding nodes which are not the destination nodes. It is used as candidates of branch nodes for the proposed algorithm. D is the set of destinations. The ﬁrst step is to add the most western node to the multicast tree. Add all the nodes in the path that from source node to the most western node into Dnode . This makes it possible that later ﬁnding a node in Dnode to connect other destination nodes to conform to the west-ﬁrst turn model. Add the source node and the most western node as an element of Dpair . The most western node is removed from the D . Then if the D is not empty, the algorithm enters a stage similar to the minimal spanning tree construction. Firstly, ﬁnd a pair of nodes (u, v) from Dnode and D which has the shortest distance and conforms to the west-ﬁrst turn model. If there are some pairs have the same shortest distance, select the pair that v is more western. This makes it more possible later to ﬁnd a C.Left-XY-Right-OptimizedTree(LXYROPT)Algorithm OPT is a power-efﬁcient multicast algorithm which optimizes the multicast tree generation globally by using less links. But this may increase multicast latency. To obtain both low latency and low power consumption, we propose another algorithm named Left-XY-Right-Optimized tree (LXYROPT). In this algorithm, the destination set is partitioned into two subsets. One contains the nodes that lie to the left of the source node, the other contains the rest. For the destinations that are left of the source node, the XY algorithm is used to generate the multicast path. For the rest node, the algorithm takes both the minimum hops for each node and the link sharing into consideration. Fig. 3 shows the detail of LXYROPT. For the optimization, ﬁrstly, it should be make sure that the routing distance from the source node to a destination node on the multicast tree is the same as the Manhattan distance from the source node to the destination node. Base on this, we select the node u from Dnode , v from Dmid−right that the Manhattan distance between u and v is the minimum. This means that a new destination node is added to the existing tree with minimum links. Similar to OPT, the pair (u, v) is added into the Dmrpair , and v is removed from the Dmid−right . The nodes from u to v are also added into Dnode . If Dmid−right is not empty, the sequence will be repeated. Otherwise, the procedure is ﬁnished. 364 Algorithm: Generate the LXYROPT multicast tree based on the west-ﬁrst turn model Input: Destination set D , Source node s : (x0 , y0 ); Output: Pair set Dlpair , Dmrpair ; Deﬁne: k(a, b) = |a.y − b.y | + |a.x − b.x|; Initial: Dnode ← s, Dlpair ← ∅, Dmrpair ← ∅; 1: Dlef t ← {(x, y)|(x, y) ∈ D , y < y0 } 2: Dmid−right ← {(x, y)|(x, y) ∈ D , y ≥ y0 } 3: while Dlef t is not empty do Find a node v ∈ Dlef t , add (s, v) into Dlpair , remove v 4: from Dlef t 5: end while 6: while Dmid−right is not empty do 7: Dpair tmp ← {(u, v)|u ∈ Dnode , v ∈ Dmid−right , that k(s, v) = k(s, u) + k(u, v) Select (u, v) ∈ Dpair tmp , that ∀(a, b) ∈ Dpair tmp , k(u, v) ≤ k(a, b) Add (u, v) into Dmrpair , remove v from Dmid−right , 8: 9: add the nodes on the path from u to v into Dnode 10: end while 4B-4 copies of the packet to each branch and then continues. Once a multicast tree is setup, the packet will be routed based on the multicast table (MCT) number at each router. At the source node, a table recording the destination set is composed of n bits (n represents the number of nodes on the NoC). If a bit is set, it means that the corresponding node is the destination node. A bit indicating whether the entry is valid is also included. At each router, MCT is partitioned to n sub-tables corresponding to each source node. Each sub-table has 16 entries or more. Paket(cid:3)Type Set/clear SRC MCT # DST VCID Payload 3(cid:3)bits 000 UC 001 MC _SET_1 010 MC _SET_2 100(cid:3) MC_CLEAR 1(cid:3)bit 6(cid:3)bits 4bits 6(cid:3)bits 2(cid:3)bits Payload x 0:(cid:3) (cid:3)set 1:(cid:3)clear 0:(cid:3) (cid:3)set 1:(cid:3)clear x x xxx xxx xxx 6(cid:3)bits(cid:255)2nd(cid:3) DST xxx 00 :(cid:3)Head 011(cid:3)MC(cid:3)NORMAL Head/Body(cid:3) /Tail/HBT 2(cid:3)bits Unicast 00 :(cid:3)Head Multicast(cid:3) setup_f irst Multicast(cid:3) setup_second Multicast(cid:3) normal Multicast(cid:3) clear Multicast(cid:3) Setup_Reply Multicast(cid:3) Clear_Reply 11:(cid:3)HBT 11:(cid:3)HBT 11:(cid:3)HBT 11:(cid:3)HBT 11:(cid:3)HBT 101 MC _SETUP(cid:3)(cid:3)RPLY 0:(cid:3) (cid:3)set 1:(cid:3)clear 110 MC _CLEAR_RPLY Valid(cid:3)fie ld(cid:3)(cid:3)for (cid:3)corresponding(cid:3)packe t(cid:3)type Fig. 3. Algorithm for generating LXYROPT. Fig. 5. Packet format. D.Anexampleforproposedalgorithms Fig. 4 shows a multicast tree built with different algorithms. The black node is the source node, the green nodes are the destination nodes. Fig. 4.a is for XYT, which contains 18 hops to do a multicast. Fig. 4.b is for the OPT, which contains 13 hops. Fig. 4.c is for LXYOPT, which contains 16 hops. The multiple unicast contains 20 hops. Our proposed algorithms can reduce the total hops for multicast effectively. Fig. 4. An example for XYT, OPT, LXYROPT IV. MU LT ICA S T PROTOCO L AND IM P L EM EN TAT ION A special router architecture is proposed to support the multicast. We organize this section as follows: ﬁrst, we will introduce the packet format which is suitable for the multicast and unicast. Then, the router micro-architecture is presented. Finally, we will discuss the deadlock freedom of the protocol. A.Protocol Each multicast forms a tree connecting the source with the destination set, which is identiﬁed by a Multicast ID number to each source node and its destination nodes combination. For a tree-based approach, a multicast packet travels along a common path until it arrives at the branch node, where it sends The proposed router supports any shape of tree-based multicast, because incrementally building up is adopted by decomposing the tree into many branches (pair set). Multicast setup is divided into two periods. During the ﬁrst period, the setup packet is named MCT SET 1, whose destination ﬁeld (DST) is ﬁlled with the ID of the ﬁrst node of the pair from a pair set that generated by the proposed algorithms while the second node ID is ﬁlled into the ﬁrst 6 bits of the payload ﬁeld. The packet is routed to the destination just like unicast. While the MCT SET 1 arrives at the input port of the destination, The Packet Type ﬁeld of the packet is changed to MCT SET 2, DST is covered by the ﬁrst 6 bits of Payload ﬁeld. Then the setup process enters the second period while traveling to the new destination and the corresponding multicast table entry is being updated based on the result of routing. Once the MCT SET 2 reaches destination, a multicast reply packet (MC SETUP REPLY) is sent to source. The setup packet can be sent out without waiting for getting the reply of the former setup packet. Each branch of the tree can be built simultaneously. When the source node receives replies of all the destinations, the setup is completed. In case when MCT is full, we have to generate a new multicast tree. Therefore we should clear an existing multicast tree. Only the source node has the right to evict the multicasts tree. MC CLEAR packet will be routed by looking up MCT. After getting the routing result, the corresponding table entry will be cleared. When the MC CLEAR packet sinks at the destination node, the destination node will generate a reply packet (MC CLEAR RPLY). Once the source node receives all the reply packets, the multicast tree is evicted. Fig. 5 shows the formats of all the packets that our router supports. B.Micro-architecture The router micro-architecture is shown in Fig. 6. The unicast packets are routed via existing hardware (using, e.g. XY rout365 4B-4 ing). The multicast of normal packets are routed by looking up the multicast table. SRC MCT# E S W N C DIR_NUM (cid:452) (cid:452) (cid:452) Head flit BW RC SA VA ST LT Body/Tail flit BW SA ST LT Virtual Channel Allocator 0 1 0 1 1 3 Switch Allocator . . . Input  Ports . . . E PTC N E PTC S PTC W C PTC PTC Fig. 6. Router architecture. . . . CrossBar E N S W C Fig. 6 also shows the pipeline stages of the proposed router. Both unicast and multicast follow the pipeline stages: buffer write/routing computation (BW/RC), switch allocation/virtual channel allocation (SA/VA), switch traversal/link traversal (ST/LT). When a head ﬂit arrives at an input port, the routing computation begins on the ﬁeld of DST (unicast) or MCT#(multicast). The only exception is for the MCT SET 1 packet. If the current router is the destination, the ﬁrst 6 bits of payload instead of the DST ﬁeld will be sent to the routing computation unit. If the ﬂit is the head ﬂit of MCT SET 1, during the BW, the packet type convert logic (PTC) block will check if the destination is the current router. If so, the PTC will change the packet type ﬁled of the ﬂit from MC SET 1 to MC SET 2, and copy the ﬁrst 6 bits of payload to the DST ﬁeld. This means that the multicast setup enters the second period. For unicast routing, the SA and VA are executed at the next cycle. Both SA and VA must succeed at the same time, otherwise will retry next time. If the head ﬂit is granted in both SA and VA, the ﬂit traverses the crossbar and ﬁnally is transferred to the downstream router. For the multicast packet routing, the RA may return multiports. The ﬂit is replicated to one port at one SA/VA stage when successfully gets the grant. Only when the ﬂit is successfully transferred to all the destination ports, can the ﬂit be deleted from the buffer. To keep the state of each multicast ﬂit, the input virtual channel (VC) reserves a separate VC state register and buffer pointers. It is necessary to forward and control the pipeline stage by using the state register and buffer pointers. If the port belongs to the RC results, then its state register will be set to advance to SA/VA, otherwise the state will be idle. The buffer pointers contain a head pointer and tail pointers for an input VC, 5 read pointers for all the destination ports. They will be discussed in the subsection of deadlock freedom. C.Deadlockfreedom To avoid deadlock, multicast tree should be generated based on the turn model that can never cause deadlock, for example XY, West-ﬁrst, North-last[16]. Another possibility is that the deadlock may happen when multiple multicast existing on the network. As shown in Fig. 7.a, two ﬂits request both north and south output ports, but none of them get two output ports at the same time. The packet a holds the southern input channel of the router 2 and tries to forward the head ﬂit to the northern input channel of the router 3. The packet b holds the northern input channel of router 3 and tries to forward the head ﬂit to the southern input channel of router 2, so the deadlock occurs. Fig. 7. Deadlock and input virtual channel architecture. There are some researches to solve this kind of deadlock. Partitioning and systematically allocating virtual channels is presented to avoid deadlock in [14]. Kumar proposed a hardware tree-based multicast routing algorithm with deadlock detection and a recovery mechanism [17]. Young proposed a dynamic packet fragmentation to solve the deadlock [18]. This scheme allows to release the hold of an output virtual channel (VC) and enable other packet to use the freed VC when deadlock happens. The proposed router avoids deadlock by using enough buffers to hold entire packet and reserving buffer pointers for each port in input VC, as illustrated in Fig. 7.b. As can be seen in Fig. 7.a, the packet a gets the southern input channel of router 2 but fails to get the northern input channel of router 3. The proposed router can continue transferring the body ﬂit and tail ﬂit to the north output port by advancing the north port pointer N. So the whole packet can successfully be transferred to the router 2. After the southern input channel of router 2 is released, the packet b can get grant of the input channel and be transferred to router 2. In a similar way, the packet b is transferred to router 3 and the input VC is released, then packet a can use it. In this way, the deadlock is avoided. V. EX P ER IM EN T To evaluate the performance and efﬁciency of the OPT and LXYROPT multicast routing algorithms, two other multicast routing algorithms were also implemented. These algorithms included XY tree-based multicast routing [12] and multiple unicast (MUC). We have developed a cycle accurate virtual cutthrough NoC simulator implemented in SystemC. The simulator calculates the average packet latency and power consumption for the packet transmission. The network parameters are shown in Table I. For the performance metric, we deﬁne the packet latency as the number of cycles from when the packet entering into the waiting queue till the time the packet ejected from the network [19]. As a baseline, transferring multiple unicast copies (MUC) is used to realize the multicast function. To compare these cases easily, we deﬁne the injection rate of MUC as an equivalent injection rate of the tree-based multicast. For exam366 4B-4 Fig. 8. Performance for the destination group of 5 nodes (a), 10 nodes (b), and 20 nodes (c) under only multicast trafﬁc. Fig. 9. Performance for mixed trafﬁc: (a)Multicast trafﬁc (b)Unicast trafﬁc. TABLE I N E TWORK PARAM E T ER S Topology Routing Packet Size Virtual Channels Buffers per Channel Router ports Number of MCT entries 8 × 8 mesh Multicast: Tag ID based Unicast: XY 1 ﬂit: Multicast setup packet 3 ﬂits: others 4 3 3 for corner nodes 4 for border nodes 5 for others 16 ple, if a tree-based multicast group has 5 destinations and its injection rate is 0.1 ﬂit/cycle, then the MUC needs to generate 5 packets to the 5 destinations and its actual injection rate is 0.5 ﬂit/cycles, but the results of MUC at this injection rate will be compared with the tree-based multicast injection rate of 0.1 ﬂit/cycle. A.Multicasttrafﬁcproﬁle The ﬁrst case of simulations contains only multicast trafﬁc without any other trafﬁc. In these simulations, the PEs generate packets of 3 ﬂits each and injects them into the waiting queue using the constant time intervals based on a given injection rate. A uniform distribution was used to construct the destination set of each multicast packet. The source node is also selected randomly. We conﬁgure three scenarios: 16 source nodes, each node to 5 destination nodes; 8 source nodes, each node to 10 destination nodes; 4 source nodes, each node to 20 destination nodes. Fig. 8 shows the average communication delay as a function of the average injection rate of the sending nodes. For groups of 5 nodes (Fig. 8.a), the XYT and LXYROPT lead to the lowest latency among all the multicasting algorithms. OPT is about 10% more than the LXYROPT and XYT. The MUC is 30% more than the LXYROPT and XYT at low injection rates. When the injection rate is increased to 0.25 ﬂits/cycle/sending node, the latency of MUC is about 60% more than the XYT and LXYROPT. For groups of 10 nodes (Fig. 8.b), the LXYROPT leads to the best performance, while the XYT takes 2%, OPT takes 13% and MUC takes 67% more than it (MUC is in case of low injection rate). For groups of 20 nodes (Fig. 8.c), the LXYROPT is still best in performance, while the XYT takes 5%, OPT takes 20% and MUC takes 144% more than it. Fig. 8 reveals that in the case of MUC with larger size destination groups, the network is more susceptible to being saturated. As a result, in such a system packets experience longer latency. 367 4B-4 B.Unicastandmulticast(mixed)trafﬁcproﬁle The second case of simulations contains mixed trafﬁc where we used a mixture of unicast and multicast trafﬁc where multicast trafﬁc accounted for 20%, which is similar to the scenarios of some cache coherence protocols [15]. The unicast trafﬁc is also uniformly distributed. Fig. 9 shows the average communication latencies against the packet injection rate. Fig. 9.a indicates that the LXYROPT is best in performance while the XYT takes 4%, OPT takes 15% and MUC takes 110%-140% more than the LXYROPT. Fig. 9.b shows the effect that the multicast exerted on the unicast trafﬁc. Since OPT uses the minimum number of links, it outperforms the others. C.PowerConsumption We calculate the power consumption according to the library of noxim [20]. The results are calculated and compared under the multicast trafﬁc with different destination group sizes. We normalize the consumption of MUC as 1. The results are shown in Fig. 10. For groups of 5 nodes, XYT takes 70%, LXYROPT takes 67% and OPT takes 63% of MUC. For groups of 10 nodes, XYT takes 60%, LXYROPT takes 55% and OPT takes 50% of MUC. For groups of 20 nodes, XYT takes 49%, LXYROPT takes 45% and OPT takes 41% of MUC. For the best case, the OPT and LXYROPT gain 96% power consumption reduction compared to MUC and 69% compared to XYT. More power reduction is due to more efﬁciently using of links and reduction of buffer reading/writing operations. Fig. 10. Power consumption under only multicast trafﬁc V I . CONC LU S ION Two power efﬁcient tree-based algorithms have been presented in this paper. A router which supports any tree-shaped construction is proposed to implement the algorithms. OPT tries to use the minimum number of links to construct the multicast tree. LXYROPT, on the premise of keeping the low latency, tries to minimize the links number of the multicast tree. These algorithms are deadlock-free because they use the westﬁrst turn model to construct the path. The deadlock resulted from the multi-port requirement at a branch node is avoided by using enough buffers to hold the whole packet and reserving the private buffer pointer for each output port in the input virtual channel. As for the future work, we will implement path-based mutlicast algorithms. Evaluating the performance and power consumption for the tree-based and path-based algorithms will be an interesting topic to investigate further. ACKNOW L EDG EM EN T S This work was supported by National Science Foundation of China under grant No.60970037 and No.60873212. We thank Abbas Eslami Kiasari from the Royal Institute of Technology in Sweden and Chaochao Feng from the National University of Defense Technology in China for their valuable comments to improve this paper. "
2011,A resilient on-chip router design through data path salvaging.,"Very large scale integrated circuits typically employ Network-on-Chip (NoC) as the backbone for on-chip communication. As technology advances into the nanometer regime, NoCs become more and more susceptible to permanent faults such as manufacturing defects, device wear-out, which hinder the correct operations of the entire system. Therefore, effective fault-tolerant techniques are essential to improve the reliability of NoCs. Prior work mainly focuses on introducing redundancies, which can't achieve satisfactory reliability and also involve large hardware overhead, especially for data path components. In this paper, we propose fine-grained data path salvaging techniques by splitting data path components, i.e., links, input buffers and crossbar into slices, instead of introducing redundancies. As long as there is one fault-free slice for each component, the router can be functional. Experimental results show that the proposed solution achieves quite high reliability with graceful performance degradation even under high fault rate.","5B-2 A Resilient On-chip Router Design Through Data Path Salvaging ∗ Cheng Liu, Lei Zhang, Yinhe Han, Xiaowei Li Key Laboratory of Computer System and Architecture Institute of Computing Technology, Chinese Academy of Sciences Beijing, China e-mail: {liucheng, zlei, yinhes, lxw}@ict.ac.cn Abstract— Very large scale integrated circuits typically employ Network-on-Chip (NoC) as the backbone for on-chip communication. As technology advances into the nanometer regime, NoCs become more and more susceptible to permanent faults such as manufacturing defects, device wear-out, which hinder the correct operations of the entire system. Therefore, effective fault-tolerant techniques are essential to improve the reliability of NoCs. Prior work mainly focuses on introducing redundancies, which can’t achieve satisfactory reliability and also involve large hardware overhead, especially for data path components. In this paper, we propose ﬁne-grained data path salvaging techniques by splitting data path components, i.e., links, input buffers and crossbar into slices, instead of introducing redundancies. As long as there is one faultfree slice for each component, the router can be functional. Experimental results show that the proposed solution achieves quite high reliability with graceful performance degradation even under high fault rate. 1 Introduction As technology advances, more and more processing elements (PEs), storage elements, DSP cores, I/O modules will be integrated on a single die. On-chip interconnection facilitating the PE communication soon becomes a critical part. Network-onChip (NoC) due to its scalability and high throughput is generally considered to be one of the most promising solutions for the interconnection challenge [1] [2] [3]. Meanwhile, with continuously shrinking feature sizes, devices and interconnects turn to be especially vulnerable to both permanent and transient faults, which will result in severe manufacturing yield loss or tremendous reliability problems [4] [5]. Fault-tolerant techniques are now indispensable for large scale integrated circuits (ICs) designs. Relaxing the requirement of 100% correctness can greatly reduce the manufacturing and testing cost [4]. As more and more processing cores can be integrated on a single chip, it is possible that a faulty core is replaced by a spare one or simply disabled. In this case, the system can still be functional with certain performance degradation. However, for onchip network, which is a distributed router-based architecture that manages the communication between embedded cores, it is ∗ The work was supported in part by National Basic Research Program of China (973) under grant No. 2011CB302503, in part by National Natural Science Foundation of China (NSFC) under grant No.( 60806014, 60831160526, 60633060, 60906018, 60921002, 61076037), and in part by Hi-Tech Research and Development Program of China (863) under grant No. 2009AA01Z126. essential for the reliability of the entire on-chip system [6] [7] [8]. One single router or channel failure might destroy the connectivity among cores and corrupt the system. Fault-tolerant routing is typically employed in NoC around the fault region. However, to prevent communication deadlock, even some fully functional cores may still be isolated from the system in faulttolerant routing [9]. Therefore, this urges us to make NoC itself robust enough to avoid activating fault-tolerant routing as much as possible. In this paper, we mainly investigate router architecture design for fault tolerance purpose. Prior work in fault-tolerant architecture design mainly focus on redundancy scheme, i.e., introducing spare components to replace faulty ones [10] [11] [12]. However, redundancy will incur expensive hardware overhead usually more than 100%. In fact, the reliability of a component greatly depends on its area occupation, and the probability that a larger component is affected by a defect or a fault is generally higher. Therefore, for larger on-chip component, coarse-grained redundancy such as dual/triple modular redundancy is not quite effective. We can divide a larger component into smaller sub-components and apply ﬁner-grained redundancy as in [13], however, it requires much more conﬁguration and control logic that also shrinks reliability beneﬁts. For the on-chip router, it will be shown in the next section that the chip area occupation of various components differ signiﬁcantly. Data path units, i.e., links, input buffers and crossbars consumes most of the chip area, while control logic such as routing computing, arbiters are pretty small. For data path components within a router, they can be viewed as many small parallel and independent path slices. For example, a 64-bit link can be regarded as four separate 16-bit link slices. Whenever a permanent fault is detected in the 64-bit link, the fault may only destroy a single link slice rather than the whole 64-bit bundle. Therefore, we can salvage the other 3 slices rather than abandon them as a whole, which is a waste of critical on-chip communication resources. Based on the above, in this paper, instead of introducing extra redundancies to data path components, we propose a ﬁne-grained data path salvaging strategy, which elaborately develops inherent redundancy within routers and achieves high reliability with moderate overhead. Fault may destroy partial data path and reduce its bandwidth. We introduce time division multiplexer (TDM) to salvage the whole data path using surviving slices. We show in experiments that the proposed fault tolerance design could achieve quite high reliability with moderate area overhead as well as graceful performance degradation even when the fault rate is high. 978-1-4244-7516-2/11/$26.00 ©2011 IEEE 437 5B-2 The rest of the paper is organized as follows. Section II analyzes the area occupation for router components and motivates this work. Section III reviews prior related work. Detailed router design and implementation is presented in Section IV. Section V describes experimental results. Section VI concludes this paper. 2 Motivation Network-on-Chip can be viewed as a set of structured routers to which IP cores are attached via network interfaces [14]. Onchip routers in NoC are responsible for routing and ﬂow control of trafﬁc streams across the chip. In this paper, we adopt a conventional two-stage pipelined wormhole router design as our baseline model, as depicted in Fig.1. It consists of pipeline register (PR), routing computing (RC), buffers, virtual channel allocator (VA), switch allocator (SA) and crossbar (CR). A packet is divided into head ﬂit (ﬂow-control-unit), body ﬂit and tail ﬂit in a wormhole router. To explain the basic function of each router component, we give a brief introduction on how a ﬂit ﬂows from input port to output port. When a ﬂit arrives at an input register after link transmission, it is then stored in a buffer slot. When a head ﬂit is detected in the buffer port, RC begins to select the output port for the packet. At the same time, VA and SA present the allocation results with speculation. When a ﬂit receives both VA and SA grant, it is evicted from buffer and transmitted to output register through CR. Among all these components, PR, buffer and CR that scale with the data width are classiﬁed as data path, while the rest logic belongs to control path. PR Data Path Control Path PR RC Arbiter (VA/SA) CR Buffer Buffer Buffer Figure 1: The General On-chip Router Architecture It can be safely assumed that defects generally distributed randomly across the chip footprint, and thus a larger component is more error-prone than a smaller one. As shown in [15], circuit reliability deteriorates quadratically as the circuit area increases. Therefore, it motivates us to verify the area distribution of different router components. A router as shown in Fig.1 with 64-bit data width and 8-ﬂit buffer depth in each input port is implemented using Verilog and then synthesized with Design Compiler under SMIC 90nm technique. We assume that the point to point link between routers needs to cross the width of a core and the width of a typical embedded core is approximately 0.8mm, e.g., ARM9. Then we obtain all the router component area as shown in Fig.2. Note that, we consider the output link as part of router components as well in this paper. From the ﬁgure, it is obvious that link, buffer, PR and CR take up the majority of a router area. Thus, straightforward replication of data path component results in unaffordable area overhead. In fact, when data path such as a 64-bit channel fail, it rarely means all the 64 wires are broken. Since data path can be viewed as several parallel slices, it is probable that there are still functional slices left even the link suffers quite a few faults. Data path salvaging that explores partial faulty resources to transmit data in a time division multiplexing (TDM) mode can achieve quite high reliability level, since as long as there are still available data path slices left, it can be functional. At the same time, there are no additional redundancy except small TDM control logic, thus the area overhead is negligible. For other control path logic such as arbiter, RC, we simply employ redundancy as they are pretty small in this paper. ECC is also adopted to protect PR. ) a e r A C o N ( % 50 40 30 20 10 0 Buffer Link PR CR Arbi ter RC Figure 2: Router Area Breakdown 3 Related Work For fault tolerant router microarchitecture design, Constantinides et al. propose a component-level diagnose and sparepart reconﬁguration strategy [13]. With automatic cluster decomposition algorithm, the proposed method is able to minimize area cost and maximize error resilience through replicating and reconﬁguring design components in proper granularity. Nevertheless, the area overhead is even larger than that of conventional dual or triple modular redundancy. Koibuchi et al. present a lightweight fault-tolerance mechanism based on default backup paths (DBP) [16]. The method delicately adds DBP instead of complete hardware duplication and consumes much less area overhead. However it takes faulttolerant design in such a large granularity that network will soon degrades to a unidirectional ring suffering higher fault rate and thus network bandwidth degrades dramatically. Furthermore, virtual channels that are imperative for the speciﬁc routing algorithm might be prohibitively expensive for light load applications. Fick et al. combine ECC, port-swapping, and a crossbar bypass to mitigate wear-out induced hard faults [17]. They exploit inherent redundancy of routers as well as previous bypass method, maintaining correct operation and low area overhead. However, when only buffer faults or link faults appear, the proposed hardware structure becomes useless. Several prior work proposed to serialize transmission channels for defect tolerance [18], while in this paper, we take the entire router’s data path into considerations. In addition, our proposed router microarchitecture design also considers 438   Control Path RC Arbiter (VA/SA) PR PR Data Path PR Redundancy Buffer Slices Slice Reuse L i S k n i l s e c MUX Slices ECC CR PR Data Path Figure 3: Different Fault-tolerant Approaches Applied to Different Router Components the bandwidth unmatching problem between adjacent routers. Techniques have been proposed recently for on-line fault detection, fault-tolerant routing and network dynamic reconﬁguration [19] [20] [21] [22]. These approaches are all orthogonal to the proposed fault-tolerant mechanism. In the following, we only focus on microarchitecture design to tolerance fault, and assume that any fault within a component can be detected. 4 The Proposed Data Path Slicing and Salvaging Scheme Data path takes up a large number of a router’s area, but it can be regarded as independent parallel slices. For example, a 64bit link can be seen as four 16-bit link slices; a 64-bit FIFO can be viewed as four 16-bit FIFO slices ; a 64-bit MUX can also be divided into four 16-bit MUX slices. Whenever partial slices fail, the remaining slices can be reused to continue transmitting data in TDM mode. Data path salvaging organizes the data path components as a group of workable subcomponents, which backup each others. To construct a resilient router, other parts also need to be protected. In this paper, control logic including RC, VA and SA is pretty small and critical to a router’s function. Thus, redundancy is applied. ECC proves to be efﬁcient for PR protection which consumes moderate overhead and enhance reliability as well. Fig. 3 presents various fault tolerant approaches applied in a our router design. In the following, we give detailed description on how to modify the conventional router for data path salvaging. 4.1 Principle Slicing allows data path to be salvaged using TDM. Fig. 4 shows the basic principle of the slicing technique. When some of the input data path slices fail and are detected, faulty status of the slices are then recorded in input fault vector (IFV). Input data slice scheduler (IDSS) initialized with IFV conﬁgures crossbar cycle by cycle, and transmits input data slices to PR in order through the crossbar. When data in PR is unavailable, ready ﬂag (RF) is set invalid and all the logic driven by PR is stalled. When PR receive all the data slice, RF is set valid. If the data in PR is granted by the arbiter at the same time, output data slice scheduler (ODSS) initialized with output fault vector ODSS Crossbar 0 IDSS PR Crossbar Input Valid IFV Input Fault Configure Input Data Path Slice Ready Flag In-slice0 In-slice1 In-slice2 In-slice3 Output Data Path Slice Out-slice0 Out-slice1 Out-slice2 Out-slice3 1 2 3 Output Valid OFV Available Slice Faulty Slice Output Fault Configure Figure 4: Slicing and Salvaging Principle /64 /64 /32 / 3 2 /32 /32 /32 /32 /16 /16 /16 /64 /64 /64 /64 /48 /48 /48 /48 /64 /64 /32 /32 /16 /16 /16 /16 /32 /32 /48 /48 /64 /64 /48 /48 /64 /64 /32 /32 /32 /32 /32 /32 /64 IFV D O S S I D S S Data In [63:0] Data Out [63:0] 64'b0 Ready Flag PR M X U 1 M X U 3 M X U 4 M X U 5 M X U 6 M X U 7 M X U 8 M X U 9 M X U 0 1 Valid In Valid Out OFV Input Fault Configure Output Fault Configure /16 M X U 2 /64 /64 Figure 5: Slicing and Salvaging Structure (OFV) is then triggered and data slices in PR are redirected to output data path slices, and then next hop router. 4.2 Structure Slicing structure is shown in Fig. 5, which is composed of IFV, MUX, IDSS, OFV, IFV and IDSS. The upstream logic before PR including MUX1 - MUX5, IFV and IDSS mainly transfer available input slices to PR and the procedure can be divided into two steps. At ﬁrst, choose correct data slices according to IFV. Since data path is split into four slices in this example, there are three possible data width including single data slice, two and four data slices. MUX5 is used to choose from one of them. To simplify logic design, when there are three data slices available, we treat them as two. MUX1 (1 out of 4) and MUX2 (2 out of 4) in the ﬁgure select a single data slice and two data slices respectively, while if there are four slices remaining functional, i.e., no faults in the input data path, no control is needed. Secondly, MUX3 and MUX4 are used to combine 4 single slices (16 bits) and 2 two slices (32 bits) respectively to construct a 64-bit ﬂit, which is then stored in PR. The logic ﬂow behind PR including MUX6 - MUX10, OFV and ODSS are responsible for the transmission from PR to output data path. It works almost the same as upstream logic except when slices in output data path fail. In that case, data sent to the slices are simply set 0, i.e, meaningless data. In addition, we can see from Fig. 5 that slicing logics are inherent independent. MUX1, MUX4 and MUX5 constitute a logic path when there is a single input data path slice left available; while MUX2, MUX3 and MUX5 construct the other when there are two available input data path slices. Therefore, when we have two available slices while MUX1 or MUX4 fail, the 5B-2 439                       5B-2 Link L1 ECC (cid:81)(cid:19) Router 1 L2 (cid:81)(cid:20) RC/VA/SA L3 (cid:81)(cid:21) L4 L5 (cid:81)(cid:22) Router 2 PR Buffer MUX Slice Reuse Figure 6: Typical NoC Data Path with Fault Tolerance structure still works. 4.3 Working Styles Slicing is applied in pipelined router design, and it has two kinds of work styles, i.e. intermittent transmission style and slice pipeline transmission style. When intermittent transmission style is adopted, logic in downstream of PR has to wait for an integrated ﬂit in PR when RF is set valid. At the same time, logic in upstream of PR must also pend for fully free PR when RF is set invalid. Wen slice pipeline transmission style is employed, the data transmission is more aggressive. As long as there are available data slices in PR, they can be accessed and sent to the next pipeline stage. Meanwhile, whenever there are free slices in PR, input data slices can be accepted. Thus slice pipeline transmission mode is more efﬁcient than intermittent transmission mode. However, when an integrated data is necessary for logic in downstream of PR such as routing computing, slice pipeline transmission style can not be used. Fig. 6 shows a typical data transmission path between adjacent routers and L1 - L5 in this ﬁgure indicate ﬁve sliced logic blocks across the data path. It is easy for them to employ both transmission styles except L2. For one thing slices within L2 is connected to different MUXs of CR and there might be different fault status in different MUXs. It will introduce many complex control if slice pipeline style is employed. Furthermore, routing computing generally needs multi-bits of a ﬂit. Consequently, intermittent transmission style is employed in L2. Other slicing stages along the path can employ slice pipeline transmission style to minimize performance penalty. 4.4 Correlation between adjacent routers When data path slicing technique is applied, faulty status of data path within neighboring routers impact each other. As shown in Fig. 6, to explain slice fault status correlations between neighboring pipelines, we assume that data path for each pipeline is divided into 𝑛 parts. Fault-free slice number in CR of router1, link between router1 and router2, and buffer in router2 are 𝑛1 ,𝑛2 and 𝑛3 respectively. When 𝑛3 < 𝑛2 , i.e. data from link transmits faster than buffer can receive, thus PR data in router2 will be overwritten. To tackle this problem, link transmission has to wait for the release signal of PR in router2. However, additional latency in the link makes the ﬂow control more complex and less efﬁcient. When 𝑛2 < 𝑛1 , the problem is the same. To guarantee an efﬁcient pipeline, available slice number of the data path components is then modiﬁed as 𝑚1 , 𝑚2 and 𝑚3 where 𝑚1 ⩽ 𝑚2 ⩽ 𝑚3 ⩽ 𝑛3 . In this paper, 𝑛 is initialized to be 4. Note that if 𝑚𝑖 = 3(𝑖 = 1, 2, 3), 𝑚𝑖 will be set as 2 to simplify hardware design. At the same time, the maximum available buffer capacity is then modiﬁed as 𝑐 × 𝑚3/4 where 𝑐 represents original buffer capacity. Besides, it can be seen that both the data path between MUX and link and that between link and input buffer are exclusive. Pipeline bandwidth in upstream is always smaller than or equal to that in downstream. As a result, data from upstream don’t have to wait for pipeline in downstream to release PR. This will greatly simply ﬂow control design. Moreover, as stated above, L2 employs intermittent transmission style, which makes slice fault status in upstream of L2 independent from that in downstream of L2. Similarly, slice fault status in upstream of L4 and that in downstream of L4 are also independent. Therefore, data path slice status correlation is limited between buffers of neighboring routers, which will not affect the scalability of the proposed method. 4.5 Extension The proposed data path slicing method can also be applied in virtual channel (VC) routers. As there are multi virtual channels in each input port, these virtual channels can also be viewed as redundancy. However, slicing technique makes the buffer fault tolerance more ﬂexible and area-efﬁcient. Since each VC can be viewed as multiple parallel slices, we can simply disable one or more virtual channels and specify partial slices of the virtual channels to serve as redundancy and salvage the other virtual channels in the same input port. When there are no faults at all, all the virtual channels can still be used as functional ones to enhance performance. Further more, slicing inner VC can be removed from buffer protection, which makes virtual channel protection more easier. 5 Experiments 5.1 Area Overhead Using Synopsys Design Compiler with SMIC90 standard cell library, a baseline router and the proposed router as described in previous sections are both synthesized with an operating frequency of 200MHz Flit width is 64-bit, and buffer capacity for each input port is 8 ﬂits. Different fault-tolerant mechanisms including redundancy, ECC and slicing are employed in this router. To get a comprehensive perspective of slicing technique, four different slicing conﬁgurations–DPSR4 (data path salvaging router with 4 slices), DPSR2 (data path salvaging router with 2 slices), DPSRM (data path salvaging router with mixed slice number), i.e. buffer and link adopt 4 slices, while CR chooses 2 slices, and DPSRD (data path salvaging router discarding CR protection), i.e. buffer and link are still divided into 4 slices while CR simply goes unprotected are synthesized under the same situation. Experimental results show that area overhead of the four routers are 65.4%, 26.5%, 52.46% and 45.9% respectively. Without link slicing logic, area overhead of the routers turn to be 45.9%, 20.0%, 33.1% and 26.5%. Fig. 7 presents area overhead of TMR (three modular redundancy), MRR (most reliable router), LOR (least overhead router), Vicis and the proposed design. Note that Vicis has also considered the overhead of fault detection which costs additional 10% area. To make a fair comparison, we have removed 440 5B-2 area overhead of this part. Additionally, since all these methods mentioned above concentrate on router fault tolerance, there are no link fault tolerance overhead available. Therefore we also remove link fault tolerance overhead from our design in Fig.7. In this ﬁgure, it can be seen that TMR and MRR which employ direct replication consume more than 100+% area overhead. LOR, Vicis and the proposed designs in this paper develop router’s inherent redundancy with various methods, and the area overhead are mostly around 20% to 50%. They are more area efﬁcient. F P S 14 12 10 8 6 4 2 0 TMR MRR LOR Vic is DPSR4 DPSR2 DPSRM DPSRD (cid:3) 3.5 3 2.5 2 1.5 1 0.5 0 TMR MRR LOR V ic is DPSR4 DPSR2 DPSRM DPSRD (cid:3) Figure 7: Router Area Overhead Comparison Normalized to Original Router Area 5.2 Reliability In Constantinides’ work, the number of faults that a router can tolerate before becoming unfunctional is normalized by the area overhead of the technique. This metric is called SPF (Silicon Protection Factor) to evaluate router reliability. In this paper, a random fault injection scheme is employed based on area of the components. When a fault falls on certain router components and the fault can still be tolerated, the counter that records the total fault injection number is increases by one. Finally, when one of the components totaly corrupts and thereby induces router failure, fault injection completes. With repeated fault injection of 100000 times, the average counter value is regarded as average number of faults that a router can tolerate. Experiments show that, fault number that DPSR4, DPSR2, DPSRM and DPSRD can tolerate are 16.80, 6.18, 17.51 and 11.84 respectively. Normalized to the area overhead, the SPF metrics of DPSR4, DPSR2, DPSRM and DPSRD are 11.52, 5.15, 13.17 and 9.36 as shown in Fig. 8. Although SPF of MRR is good, its area overhead is more than 200%, which is too large to be acceptable. Vicis has a moderate area overhead and SPF. However,with similar area overhead, DPSR4, DPSRM and DPSRD can achieve much higher SPF, and especially SPF of DPSRM even doubles that of Vicis. Although DPSR2 has a relatively lower SPF, it has the lowest area overhead. The reason that SPF of DPSR4 is lower than DPSRM lies in the fact that crossbar in planar 2D Mesh NoC is small. As a result, four-slice reuse method will be a little overprotected which brings down the average SPF. When the proposed method is applied in a high dimension network with much larger crossbar area, DPSR4 will outperform the other conﬁgurations. Additionally, we can also observe that DPSRM has higher SPF than DPSR2 does, and it suggests that buffer and link with four slices is superior to that with two slices. Figure 8: Router SPF Comparison of Various Reliable Design Schemes To get an deep insight into the proposed method, we also evaluate the reliability of the entire NoC and no fault tolerant routing algorithms are used. As torus topology is both node and edge symmetric and faults have equivalent inﬂuences on all routers, which makes the analysis scalable, we evaluate the reliability of 8 × 8 torus network considering link faults as well. Any router component failure or corresponding output link failure is considered to be node failure. When extra logic that is used as control for fault tolerant purpose fails, the being protected component is assumed to be off. In this case, Fig. 9 shows the number of available nodes when the network suffers faults that vary from 0 to 1300. Note that the above failure rate equals 0 to 1/2000 fault per gate. When the fault number is no more than 300, 95% nodes remain fully functional. Even when the faults are as many as 1000, 70% nodes survives. 1 0.95 0.85 0.75 0.65 t e a R l a n o i t c n u F 0.55 0 200 400 600 Fault Number 800 1000 1200 (cid:3) Figure 9: Proportion of Available Nodes in 8x8 Torus 5.3 Performance To evaluate the inﬂuence of the proposed method on NoC performance, we implement a cycle-accurate simulator in SystemC with 4 × 4 2D mesh topology constructed by wormhole routers. XY routing is employed and each input port had a 8-ﬂit depth buffer. In this experiment, each node of the network generates packet according to poisson process. Destinations of the packets are selected randomly. 100k packets with 8 ﬂits long per node are injected with 20k cycles warm-up before collecting performance information. We inject 6 to 48 faults on links and buffers. The simulation results shown in Fig. 10 illustrate that the proposed design keeps graceful performance degradation when the number of faults increase. Moreover, in the proposed design, both a single-slice fault and double-slice fault are assumed to be equivalent, link faults result in associated CR 441   in IEEE Computer society Annual Symposium on VLSI, 2004. Proceedings, pp. 46–51, 2004. [8] F. Angiolini, D. Atienza, S. Murali, L. Benini, and G. De Micheli, “Reliability support for on-chip memories using networks-on-chip,” in Computer Design, 2006. ICCD 2006. International Conference on, pp. 389– 396, 2007. [9] R. Boppana and S. Chalasani, “Fault-tolerant routing with non-adaptive wormhole algorithms inmesh networks,” in Supercomputing’94. Proceedings, pp. 693–702, 1994. [10] D. Siewiorek and R. Swarz, Reliable computer systems: design and evaluation. AK Peters, Ltd., 1998. [11] J. Smolens, B. Gold, J. Kim, B. Falsaﬁ, J. Hoe, and A. Nowatzyk, “Fingerprinting: bounding soft-error detection latency and bandwidth,” in Proceedings of the 11 th international conference on Architectural support for programming languages and operating systems: Boston, MA, USA, 2004. [12] C. Weaver and T. Austin, “A fault tolerant approach to microprocessor design,” in Proceedings of the International Conference on Dependable Systems and Networks, pp. 411–420, Citeseer, 2001. [13] K. Constantinides, S. Plaza, J. Blome, B. Zhang, V. Bertacco, S. Mahlke, T. Austin, and M. Orshansky, “Bulletproof: A defect-tolerant CMP switch architecture,” in High-Performance Computer Architecture, 2006. The Twelfth International Symposium on, pp. 5–16, 2006. [14] L. Benini and G. De Micheli, “Networks on chips: A new SoC paradigm,” Computer, vol. 35, no. 1, pp. 70–78, 2002. [15] T. Lehtonen, P. Liljeberg, and J. Plosila, “Fault tolerance analysis of NoC architectures,” in IEEE International Symposium on Circuits and Systems, 2007. ISCAS 2007, pp. 361–364, 2007. [16] M. Koibuchi, H. Matsutani, H. Amano, and T. Pinkston, “A lightweight fault-tolerant mechanism for Network-on-Chip,” in Proceedings of the Second ACM/IEEE International Symposium on Networks-on-Chip, pp. 13–22, IEEE Computer Society, 2008. [17] D. Fick, A. DeOrio, J. Hu, V. Bertacco, D. Blaauw, and D. Sylvester, “Vicis: a reliable network for unreliable silicon,” in Proceedings of the 46th Annual Design Automation Conference, pp. 812–817, ACM, 2009. [18] M. Palesi, S. Kumar, and V. Catania, “Leveraging partially faulty links usage for enhancing yield and performance in networks-on-chip,” Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on, vol. 29, pp. 426 –440, march 2010. [19] A. Alaghi, N. Karimi, M. Sedghi, and Z. Navabi, “Online NoC switch fault detection and diagnosis using a high level fault model,” in 22nd IEEE International Symposium on Defect and Fault-Tolerance in VLSI Systems, 2007. DFT’07, pp. 21–29, 2007. [20] D. Fick, A. DeOrio, G. Chen, V. Bertacco, D. Sylvester, and D. Blaauw, “A highly resilient routing algorithm for fault-tolerant NoCs,” in Proc. DATE, 2009. [21] M. Gomez, J. Duato, J. Flich, P. Lopez, A. Robles, N. Nordbotten, O. Lysne, and T. Skeie, “An efﬁcient fault-tolerant routing methodology for meshes and tori,” Computer Architecture Letters, vol. 3, no. 1, pp. 3–3, 2004. [22] C. Ho and L. Stockmeyer, “A new approach to fault-tolerant wormhole routing for mesh-connected parallel computers,” IEEE Transactions on Computers, vol. 53, no. 4, pp. 427–438, 2004. 5B-2 partially working, and buffer faults lead to disabling of corresponding link and CR. It means that performance of 6 buffer faults equals that of 6-36 faults including 6-12 buffer faults, 012 links faults and 0-12 CR faults. Similarly, performance of 6 link faults is equivalent to that of 6-24 faults including 6-12 link faults and 0-12 CR faults. The other fault situations also implicit a much larger number of fault situations. Thus NoC performance will keep stable in a huge range of faults. ) l s e c y c ( t y c n e a L e g a r e v A 200 180 160 140 120 100 80 60 40 20 Fault 0 Buffer Fault 48 Buffer Fault 24 Buffer Fault 12 Buffer Fault 6 Link Fault 48 Link Fault 24 Link Fault 12 Link Fault 6 0.05 0.1 0.15 0.2 0.25 Injection Rate(fl its/node/cycle) 0.3 0.35 0.4 (cid:3) Figure 10: Average Network Latency Under Various Fault Scenarios 6 Conclusion In this paper, we exploited NoC router’s inherent redundancy by splitting large data path components into slices, each of which can maintain function of the whole component in presence of partial failures using TDM. For small logic with comparatively low fault probability, conventional redundancy or ECC is employed. Evaluation results show that the proposed design provides several conﬁgurations with high reliability and low overhead. The area overhead varies from 26.5% to 65.4% and SPF scales from 5.15 to 13.17. When the network suffers medium fault rate, 90% nodes in 8x8 torus keep fully functional. Even the network is exposed to highest fault rate, about 55% nodes survive. Simulation also indicates that NoC performance degrades gracefully when fault rate rises dramatically. "
2011,Realization and performance comparison of sequential and weak memory consistency models in network-on-chip based multi-core systems.,"This paper studies realization and performance comparison of the sequential and weak consistency models in the network-on-chip (NoC) based distributed shared memory (DSM) multi-core systems. Memory consistency constrains the order of shared memory operations for the expected behavior of the multi-core systems. Both the consistency models are realized in the NoC based multi-core systems. The performance of the two consistency models are compared for various sizes of networks using regular mesh topologies and deflection routing algorithm. The results show that the weak consistency improves the performance by 46.17% and 33.76% on average in the code and consistency latencies over the sequential consistency model, due to relaxation in the program order, as the system grows from single core to 64 cores.","2B-2 Realization and Performance Comparison of Sequential and Weak Memory  Consistency Models in Network-on-Chip based Multi-core Systems  Abdul Naeem, Xiaowen Chen, Zhonghai Lu and Axel Jantsch Department of Electronic Systems, Royal Institute of Technology, Sweden  E-mail: {abduln, xiaowenc, zhonghai, axel}@kth.se Abstract-This paper studies realization and performance  comparison of the sequential and weak consistency models in  the network-on-chip (NoC) based distributed shared memory  (DSM) multi-core systems. Memory consistency constrains the  order of shared memory operations for the expected behavior  of the multi-core systems. Both the consistency models are  realized in the NoC based multi-core systems. The performance  of the two consistency models are compared for various sizes of  networks using regular mesh topologies and deflection routing  algorithm. The results show that the weak consistency improves  the performance by 46.17% and 33.76% on average in the code  and consistency latencies over the sequential consistency model,  due to relaxation in the program order, as the system grows  from single core to 64 cores.   I. Introduction  There is a trend in the processor development from single  core  to multi-core  architectures  [1, 2, 3]. The  Network-on-chip (NoC) can be used as a reliable and  scalable communication medium among these cores in the  system [4, 5, 6, 7]. As the number of processors grows on  the chip  their memory requirements also grow. The  distributed shared memory (DSM) architecture is preferred,  because a single centralized shared memory has the  performance bottleneck in the multi-core systems. The  processor development still confronts many challenges like  memory consistency, coherence and parallel programming  issues. The memory consistency decides the order of  execution of shared memory operations up  to  the  expectation of programmer in the multi-core systems.  Various memory consistency models already proposed [8, 9,  10] are based on the ordering constraints on the shared  memory operations in the multi-core systems. The strict  memory consistency models disable  the performance  optimizations due to more restrictions on the ordering of  shared memory operations. For instance, the sequential  consistency model [11] allows minimum performance  optimizations due to the strictness in the program order. The  strict ordering on the shared memory operations are  prohibitively expensive in the DSM based multi-core  systems. Consequently, several relaxed ordering (like weak  ordering, release consistency) [12, 13] emerged to allow  these performance optimizations. Relaxed consistency  models enhance the system performance significantly at the  reasonable cost.  We investigate the performance of the sequential and  weak consistency models that are realized in the NoC based  multi-core  (McNoC)  systems. The  strict  sequential  consistency model (also called as strong ordering) is the  extension of uni-processor memory model applied to the  multi-processor systems. It does not allow reordering in the  shared memory operations. The weak consistency model  (often called as weak ordering) is a relaxed consistency  model which permits overlapping in the shared memory  operations  in  the McNoC systems. The sequential  consistency model is implemented by stalling the processor  till the completion of previously issued operation. The weak  consistency model is implemented by using a transaction  counter in the platform hardware to avoid the interference  between the data and synchronization operations. In the  results section, the average, maximum code and consistency  latencies are explored and compared for the various sizes of  the 2D mesh networks. The average code latency is the  average of execution times of codes running on the  concurrent nodes in the McNoC system. The consistency  latency is the code latency without the network latency and  synchronization wait time. The differences in the code and  consistency latencies for both the consistency models are  significant in the large networks. These latencies are reduced  by the weak consistency model in comparison to the  sequential consistency model due to the allowed reordering  in the memory operations.  The rest of the paper is organized as follows. In the next  section, we review the related work. In section III, memory  consistency background is discussed. In section IV, DSM  based McNoC platform is discussed. In section V, realization  of the sequential and weak consistency models are focused.  In section VI, simulation results and performance analysis of  the two consistency models in the McNoC system are  described. Section VII summarizes our contribution.  II. Related Work  Very  little work has been done on  the memory  consistency issue in the McNoC systems. Sarita V et al. [8]  discussed memory consistency issues with an emphasis on  the system optimizations they allow. They proposed the  counter based technique to realize the weak memory  consistency. The proposed counter keeps track of the  outstanding data operations between the two consecutive  synchronization operations. The data operations may still be  reordered and overlapped with respect to each other. We  realized the weak consistency model using the same concept  in the McNoC systems. Petrot et al. [14] explored the  reordering of synchronization and data operations due to the  routing scheme, diverse paths, and physical location of the  target in the NoC based shared memory multi-processor SoC  architectures. They proposed that the initiator should wait  for the response of the first target before sending the request  to the second target to avoid the interference between the  synchronization and data operations. But in our work such  interference is avoided without suffering the relaxation in  the program order. In [15],  the scalability of weak  consistency model in the NoC based multi-core architectures  978-1-4244-7516-2/11/$26.00 ©2011 IEEE 154 is discussed. However, the performance of the weak  consistency model  is not compared with any other  consistency model in McNoC system. In this paper the  performance of the weak consistency is compared with the  sequential consistency in the more flexible McNoC platform.  Monchiero et al. [16] proposed a synchronization buffer, a  hardware unit to support the memory controller and the lock  is locally polled in the shared memory NoC based MPSoC  systems. However, since all the synchronization requests  issued by all nodes flow through the network into the stand  alone synchronization module, the network congestion due  to the heavy traffic may affect the system performance. Also,  the target system architecture assumes the weak consistency  model but does not mention how to constrain the shared  memory operations for the weak consistency model.   III. Memory Consistency Background  The memory consistency model is a contract between the  programmer and parallel system. The programmer follows  the rules that are guaranteed by the parallel system to get the  predictable results of the shared memory operations. Shared  memory access latency can be reduced by the hardware and  software optimizations in the system architecture. These  performance optimizations can reorder the shared memory  operations and the system may give unexpected results. For  the expected results, these reordering should be controlled  carefully. Different memory consistency models enforce  different ordering constraints on  the shared memory  operations [8]. The sequential consistency enforces strict  ordering constraints on the shared memory operations. The  IBM 370,  total store ordering (TSO) and processor  consistency (PC) models eliminate the ordering constraints  between write followed by a read to a different location. The  partial store ordering (PSO) model also in addition removes  the ordering constraints among writes to the different  locations. The weak consistency, release consistency, Alpha,  relaxed memory order (RMO), and PowerPC models relax  the program order requirement among all the shared  memory operations to the different locations. In this paper,  we focus on the two memory consistency models (sequential  and weak consistency) that are realized in the McNoC  systems.  A. Sequential Consistency Model  The sequential consistency defined by Lamport [11] has  to maintain the program order among operations of each  individual processor and sequential order among multiple  processors in the system. In fact, according to the definition,  reorder can be allowed since the result is the same as the  strict program order expects. However, it will be difficult to  implement due to that the consequence of each re-order has  to be globally analyzed to ensure correctness. In the  following, we discuss the sequential consistency model that  literally follows the program order.   The sequential consistency (strong ordering) is a strict  model expected by the programmer. A memory operation  (read, write) cannot be reordered or overlapped with the  following memory operation to the different locations in the  shared memory. The shared memory operations are  155 2B-2 completed according to the program order as given in Fig.  1(a). The sequential consistency enforces the global orders  on shared memory operations as given in Fig. 1(b).                (a)                 (b)   Fig. 1.  a) Strong Ordering    b) Global orders to enforce  The sequential memory consistency model does not allow  the performance optimizations [8] in the hardware (write  buffer, cache, interconnection network) and in the software  (compiler reordering, register allocation) due to the strict  order enforcement on the shared memory operations.  Relaxed memory  consistency models permit  such  optimizations. These models relax the program order  requirement to allow the possible reordering in the shared  memory operations by the system optimizations. The shared  memory operations may not complete according to the  program order. The overall program correctness is ensured  to enforce ordering constraints on a subset of shared  memory operations. We consider the weak consistency  model which relaxes the strict program order requirement  and allows reordering in the shared memory operations. The  shared memory operations (read, write) can be reordered  with the following shared memory operations in the specific  program segments.  B. Weak Consistency Model  The weak consistency model proposed by Dubois et al.  [12] classifies shared memory operations as synchronization and data operations. Synchronization operations are related  to the special synchronization variables (locks, semaphores)  in the shared address space. The lock must be gained  exclusively in the shared memory multi-processor systems.  Data operations are the load-store operations related to the  ordinary shared variables. To make the strong ordering  comparable with the weak ordering Fig. 2(a) also considers  the synchronization operation.                 (a)                      (b)   Fig. 2.  a) Strong Ordering           b) Weak Ordering      2B-2 According  to  the weak memory consistency data  operations (R1, W1) can be reordered with respect to one  another as shown in Fig. 2(b). Similarly, the data operations  (W2, R2) and (R3, R4) in their respective sections can also  be reordered with respect to each. The data operations (R1,  W1) are not allowed to be reordered and overlapped with the  data operations (W2, R2) and (R3, R4) in another section.  Also, the data operations (R1, W1, R2, W2, R3, R4) are not  allowed to be reordered with respect to the synchronization  operations.  The weak consistency model enforces some global orders  on the shared memory operations as shown in Fig. 3(b). The  enforcement of these global orders on the shared memory  operations ensures the program correctness in the weak  consistency model with the permitted relaxation in data  operations. It also ensures to get the final consistent result of  the program execution in the multi-processor systems. All  previously issued outstanding data operations must be  completed before the issuance of synchronization operation.  Similarly, previously issued outstanding synchronization  operation must also be completed before the issuance of any  data operation. The transaction counter based technique for  the enforcement of the global orders, required for the weak  consistency model is illustrated in the later section.               (a)                        (b)   Fig. 3.  a) Weak Ordering    b) Global orders to enforce  IV. DSM based McNoC Platform  Fig. 4(a) shows a homogenous NoC based multi-core system  having one type of nodes. The system is comprised of 16  nodes interconnected via a packet-switching network. All  the nodes are connected in a 2D mesh topology. Each node  represents a typical processor-memory (PM) node in the  platform. The structure of a PM node is given in Fig. 4(b).  Each PM node consists of a processor, data management  engine (DME) and a local memory. The DME in the PM  node is connected to the NoC, processor and local memory.  Routers in the NoC use deflection routing algorithm to route  the packets to the proper destinations. The platform uses  distributed shared memories (DSM) which are integrated  with processors in the nodes. All shared parts in the local  memories form virtual memory, which is organized as a  DSM and use a single global memory address space. Two  addressing schemes are adapted and for the shared memory  access, a virtual-to-physical (V2P) address translation is  required. 156             (a)                       (b)   Fig. 4.   a) Homogeneous McNoC      b) PM node  The DME is in each node of the McNoC platform. Its  architecture is given in Fig. 5. The DME is connected to the  CPU core, the local memory, and the network. The DME  contains core  interface control unit (CICU), network  interface  control  unit  (NICU),  control  store,  mini-processor-A, mini-processor-B,  synchronization  supporter and a transaction counter. The CICU and NICU  provide the hardware interface to the local core and network  respectively. The  two mini-processors are  the central  processing engine. Micro-program is initially stored in the  local memory, and is dynamically uploaded into the control  store during the program execution. The synchronization  supporter coordinates the two mini-processors to avoid  simultaneous accesses to the same synchronization variable  (lock) and guarantees atomic read-modify-write operations  over the lock. Both the local memory and the control store is  dual ported, port A and B, which are connected to the  mini-processor A and B, respectively. The transaction  counter (TC) is used in the DME hardware to realize the  weak consistency model in the McNoC platform. The  sequential consistency model platform does not use the TC.  The  shared memory operations  (read, write) and  synchronization operations are implemented in the DME  micro-code. The DME offers flexibility to implement a  verity of read-write commands and different synchronization  primitives.   Fig. 5.   Structure of DME  Local memory  Local shared memory operations are accomplished by  mini-processor-A within the node. Similarly, for the remote  shared memory and lock access, messages are sent to the  remote node by mini-processor-A via network. Remote  shared memory operations are completed either by the  remote data  return or write  acknowledgment by  mini-processor-B in the remote node. The DME provides  both the hardware and software support for the memory  synchronization. The  synchronization  supporter  (SS)  provides the underlying hardware support for the memory  synchronization. The SS can simultaneously receive and  respond to two synchronization requests from the local core  via mini-processor-A  and  the  remote  cores  via  mini-processor-B. Two special micro-operations (ll and sc) together with the SS ensure atomic read-modify-write  operations over locks. Various synchronization primitives  (spin lock, queue locks) like test-and-set() are implemented  in the DME micro-code using these special ll and sl micro-operations. The ll micro-operation checks the lock  address in the SS, if it is not there, then the lock address is  recorded in the SS over the entire period of lock acquire or  release operations. The lock is gained exclusively in the  shared memory. If the address is present i.e., other node is  gaining access  to  the same  lock,  then  the relevant  mini-processor is stalled until the completion of preceding  acquire or release operation (removal of lock address from  SS) by the other node. The sl micro-operation not only  acquires or releases a lock but also remove the lock address  from the SS. For more detail we refer you to [17].  V. Realization of Memory Consistency Models  A. Sequential Consistency Model  The sequential memory consistency model in the McNoC  platform is realized by stalling the processor on the issuance  of shared memory operation till the completion of preceding  operation. On  the completion of previous operation,  processor issues the next operation. The completion of  preceding operation is indicated by the return data or  acknowledgment. The program order is maintained due to  the strict order between the shared memory operations and  sequential order is maintained by the read-modify-write  memory operation in the system. Fig. 6 demonstrates the  realization scheme of the sequential consistency model in  the McNoC system.   2B-2 The transaction processing FSM for the sequential  consistency model (FSM-SEQ) in the DME is given in Fig.  7. The FSM-SEQ initializes and configures the DME in the  working  state.  The  FSM-SEQ  asserts  the  Keep-Transaction-Going (KTG) output of the DME low  (refer to Fig. 4) as it receives a memory operation from the  processor and issue it to the memory system. The active low  KTG stall the processor and the FSM-SEQ switches to the  wait state. The FSM-SEQ returns to the working state on the  completion of the previously issued operation, and asserts  the KTG output high. Active high KTG signals the processor  to issue the next memory operation in the program.(cid:2) Fig. 7. Transaction processing FSM for sequential consistency  B. Weak Consistency Model  The transaction counter (TC) based approach [8] is  adapted for the realization of the weak memory consistency  model in the McNoC systems. The counter is implemented  in the DME hardware of each node to keep track of the  outstanding data operations  issued between  the  two  synchronization points. It is incremented and decremented  by  the  issuance and completion of data operations  correspondingly. It is not affected by the synchronization  operations. Fig. 8 illustrates the realization scheme of the  weak consistency model in the McNoC platform.   Fig. 6.  Sequential consistency implementation scheme  The local shared memory operations are issued (1) and  completed (2) in the local node. For the remote shared  memory accesses, message passing (3, 4) is carried out to  the remote node via network. Remote operations complete  via response messages (5, 6). Overall,  the memory  operations are issued and completed in the order specified in  the program.  Fig. 8. Weak consistency implementation scheme  (cid:2) The issuance (1) and completion (2) of the data operations  to the local shared memory increment and decrement the TC.  Local or remote synchronization operations are not issued (3,  9) until the TC becomes zero, i.e., the completion of all the  previously issued outstanding data operations. The TC is  neither incremented nor decremented by the synchronization  157 2B-2 operation. The atomicity over the synchronization operation  is achieved by using two special purpose micro-operations sl and ll and the synchronization supporter. The subsequent  data operations wait and cannot be issued until the  completion of  the previously  issued synchronization  operation. A  local synchronization operation  (3)  is  completed by synchronization acknowledgment (4). For the  remote data operations message passing (5, 6) is carried out  to the remote node via network. Remote data operations are  completed by response messages (7, 8) from the remote  node. The completions of remote data operations also  decrement the same TC in the local node. The message  passing (9, 10, 11, 12) is involved to the remote node for the  remote synchronization operations. In summary, the TC in  each node is incremented with the issuance of the local and  remote data operations (1, 5). It is decremented by the  completion of previously issued local and remote data  operations (2, 8). It is not affected by the local (3, 4) and  remote (9, 10, 11, 12) synchronization operations.  The transaction processing FSM for the weak memory  consistency model (FSM-WK) in the DME is depicted in  Fig. 9. The FSM-WK is in the working state initially. The  FSM-WK issues outstanding data operations in the working  state as it asserts the KTG output of the DME high to the  processor on the issuance of each data operation. When the  FSM-WK receives  the synchronization operation,  its  issuance to the memory system is postponed until the TC  becomes zero (completion of previously issued outstanding  data operations). The FSM-WK asserts the KTG output low  on the issuance of the synchronization operation to the  memory system to stop the subsequent data operations from  the processor. The FSM-WK switches to the synchronization  wait state and waits for the completion of the previously  issued synchronization operation. The FSM-WK returns to  the working state on the reception of the synchronization  acknowledgment and asserts the KTG output high. The  processor can send now the outstanding data operations  again until the next synchronization point.  Fig. 9. Transaction processing FSM for weak consistency  VI. Experiments and Results  A. Purpose and Setup  We analyze the performance of the sequential and weak  consistency models that are realized in the McNoC systems.  The affects of network size on the code and consistency  latencies are investigated. Average and maximum latencies  of the sequential and weak consistency models are compared  with the increasing size of the NoC in the system. The  experimental setup uses the DME based flexible and  158 configurable McNoC platform. Tests are performed for  various sizes of the networks. Regular 2D mesh topology  network using deflection routing algorithm is considered in  the tests. Two hotspot nodes are used one for the critical  section (CS-node) and the other for the lock (SYNC-node).  As shown in Fig. 10, the critical section in the CS-node is  protected by the lock maintained in the SYNC-node.   Fig. 10. Synchronization and data requests   Each node sends the synchronization request to the  SYNC-node. The shared memory locations in the CS-node  are accessed exclusively after acquiring  the  lock  successfully in the SYNC-node. After the critical section  execution, the lock is released for the other waiting acquires  requests. The hotspot traffic is generated by the code  running on each node in the platform. The pseudo-test-code  is given in Fig. 11.  Fig. 11.  Pseudo-test-code   (cid:2) B. Code Latency   The average code latency is the average of codes  execution time running on the concurrent nodes in the  McNoC system. The average and maximum code latencies  for the different sizes of networks are shown in Fig. 12. The  average code latency increases exponentially for both the  sequential and weak consistency models as the network  grows from single core to 64 cores. It is due to the  increasing  synchronization wait  time  and network  congestion in the larger networks. The network congestion  and synchronization wait time may suffer the system  performance in the larger networks. Average code latency  for the weak consistency model in the 8x8 network is about  241.96 times of the single core, whereas for the sequential  consistency model it is 353.67 times. The weak consistency  model reduces these latencies due to the relaxation in the  program order as compared to the sequential consistency  model. The performance gain of the weak consistency over  the sequential consistency model is 46.17% in the average  code latency, as the system grows from single core to 64  cores.      Average Code Latency (Weak consistency ) Max Code Latency (Weak consistency ) Average Code Latency (Sequential consistency ) Max Code Latency (Sequential consistency ) x 104 10 s e l c y C 9 8 7 6 5 4 3 2 1 0 s e l c y C 500 450 400 350 300 250 200 150 100 50 0 1x1 1x2 2x2 2x4 Network Size 4x4 4x8 8x8 Fig. 12. Code latency     C. Consistency Latency  The consistency latency is the code latency without the  network latency and synchronization wait time. The average  and maximum consistency  latencies observed  in  the  experiments for various sizes of network are shown in Fig.  13. The increasing trend in the consistency latency is linear  for both the consistency models as the network size scales.  The average consistency latency for the weak consistency  model in the 8x8 network is about 1.54 times of the single  core, while for the sequential consistency model it is 1.96  times. The performance gain of the weak consistency over  the sequential consistency model in the average consistency  latency is 33.76%.   Average Consistency Latency (Weak consistency )  Max Consistency Latency (Weak consistency )  Average Consistency Latency (Sequential consistency )  Max Consistency Latency (Sequential consistency ) Fig. 13. Consistency latency  1x1 1x2 2x2 2x4 Network Size 4x4 4x8 8x8 VII. Conclusion  We explore the memory consistency models in the DSM  based McNoC systems. The sequential consistency model is  realized by stalling the processor to serialize the issuance  and completion of the shared memory operations. The  hardware transaction counter can enforces the required  global orders needed for the weak consistency model. The  transaction counter based  realization of  the weak  consistency model avoids the possible interference problem  between the data and synchronization operations. We  analyze the performance of both the consistency models in  the McNoC systems. The mesh topology networks are  considered in the McNoC platform for both the consistency  models. All the nodes synchronized over the same block  (queue) lock in a particular node. The average and maximum  code and consistency latencies increase significantly for  both the consistency models as the network scales. The  experimental results show that the weak consistency model  performs 46.17% and 33.76% better on average in the code  and consistency latencies in comparison to the sequential  consistency model due to the relaxation in the program  order.   159 2B-2 Acknowledgements  This work has been supported partially by the FP7 EU  project Mosart under contract number IST-215244, and the  HEC/SI joint scholarship program of Pakistan and Sweden.  "
2011,Vertical interconnects squeezing in symmetric 3D mesh Network-on-Chip.,"Three-dimensional (3D) integration and Network-on-Chip (NoC) are both proposed to tackle the on-chip interconnect scaling problems, and extensive research efforts have been devoted to the design challenges of combining both. Through-silicon via (TSV) is considered to be the most promising technology for 3D integration, however, TSV pads distributed across planar layers occupy significant chip area and result in routing congestions. In addition, the yield of 3D integrated circuits decreased dramatically as the number of TSVs increases. For symmetric 3D mesh NoC, we observe that the TSVs' utilization is pretty low and adjacent routers rarely transmit packets via their vertical channels (i.e. TSVs) at the same time. Based on this observation, we propose a novel TSV squeezing scheme to share TSVs among neighboring router in a time division multiplex mode, which greatly improves the utilization of TSVs. Experimental results show that the proposed method can save significant TSV footprint with negligible performance overhead.","4B-3 Vertical Interconnects Squeezing in Symmetric 3D Mesh Network-on-Chip ∗ Cheng Liu, Lei Zhang, Yinhe Han, Xiaowei Li Key Laboratory of Computer System and Architecture Institute of Computing Technology, Chinese Academy of Sciences Beijing, China e-mail: {liucheng, zlei, yinhes, lxw}@ict.ac.cn Abstract— Three-dimensional (3D) integration and Networkon-Chip (NoC) are both proposed to tackle the on-chip interconnect scaling problems, and extensive research efforts have been devoted to the design challenges of combining both. Through-silicon via (TSV) is considered to be the most promising technology for 3D integration, however, TSV pads distributed across planar layers occupy signiﬁcant chip area and result in routing congestions. In addition, the yield of 3D integrated circuits decreased dramatically as the number of TSVs increases. For symmetric 3D mesh NoC, we observe that the TSVs’ utilization is pretty low and adjacent routers rarely transmit packets via their vertical channels (i.e. TSVs) at the same time. Based on this observation, we propose a novel TSV squeezing scheme to share TSVs among neighboring router in a time division multiplex mode, which greatly improves the utilization of TSVs. Experimental results show that the proposed method can save signiﬁcant TSV footprint with negligible performance overhead. cost and operation frequency as well [10]. Although 3D NoC designs vary, they fall in three categories, i.e. symmetric, hybrid and true 3D fabric NoC [10]. Symmetric 3D NoC is a simple extension of 2D NoC by adding two additional ports in 2D router design, i.e., upwards and downwards i/o ports. A two-layer symmetric 3D Mesh NoC is shown in Fig.1. There are two active silicon layers in this example and each layer can be potentially placed with multiple processing elements (PEs) such as processing cores, memories and some other user deﬁned logic, which are attached to routers. Inter-layer communication channels are composed of TSVs which cut across thinned silicon substrates to build connectivity after bounding [11]. To make the best use of existing IPs and take least design efforts to migrate from 2D to 3D, symmetric 3D NoC is preferred for the time being. This paper focuses on the symmetric 3D mesh structure. 1 Introduction With continuously shrinking feature size, more and more processing cores and memories are integrated on a single chip. Two major trends, i.e. communication-centric interconnect architecture based on network-on-chip (NoC) that addresses scalability challenges as well as bandwidth bottleneck [1][2] and three dimensional integrated circuits (3D ICs) that alleviate interconnect latency pressure as well as heterogeneous integration problems [3] [4] [5] [6], are emerging for such complex integrated systems. 3D NoCs combining both the beneﬁts soon become one of the most promising on-chip communication techniques in complex System-on-Chip (SoC) [7][8]. Several prior work has investigated the architecture design of 3D NoCs. Feero et al. compared both 2D Mesh and 2D Torus to 3D Mesh and 3D stacked Mesh [7]. Results showed that 3D NoCs offer more competitive performance and energy consumption for communication. Li et al. introduced a 3D NoC by hybriding a common NoC router with a bus link in vertical dimension. The hybrid system allows single-hop communication between nodes connected by the vertical bus, and it provides both performance and area beneﬁts [9]. Kim et al. proposed a dimension decomposition method to optimize 3D NoC area ∗ The work was supported in part by National Basic Research Program of China (973) under grant No. 2011CB302503, in part by National Natural Science Foundation of China (NSFC) under grant No.( 60806014, 60831160526, 60633060, 60906018, 60921002, 61076037), and in part by Hi-Tech Research and Development Program of China (863) under grant No. 2009AA01Z126. PE Router TSV Pad TSV Horizontal Link Figure 1: A Two-layer Symmetric 3D Mesh NoC The vertical interconnects, i.e. TSVs in 3D ICs alleviate the interconnect problems that 2D ICs design encounters [9] [12] [13] [14]. TSVs also allow good compatibility with standard CMOS process and have been proved to be efﬁcient in 3D integration [15][16][17]. However, TSVs also bring challenging problems that can not be ignored. First, as shown in Fig.1, TSV pads on the planar layer are needed for bounding purpose, which will consume chip area in each layer [18]. For example, assume the vertical link data width is 64-bit and there are 5 × 5 nodes in each layer. It is easy to see that we need 3200 simplex TSVs between two layers. For the time being, low density TSV pitch is more than 50um [19], thus total TSV pads will occupy at least 8.00 mm2 on each layer. Even high density TSVs with 16um pitch is employed [19], this area overhead will 0.82 mm2 , which is almost equivalent to the size of an embedded core under 65nm technique. TSV technology is quite different from planar metal wires, and it is not expected to scale with 978-1-4244-7516-2/11/$26.00 ©2011 IEEE 357 4B-3 feature size [20]. Therefore, the above problem could become even severe when transistors and wires shrink. Secondly, a large number of distributed TSV pads across the whole network aggravate routing congestion [21] which is already a challenging problem for high performance IC designs. Finally, since TSV fabrication using current techniques still suffers a relatively low yield [15] [22] [17], more TSVs will result in lower product yield [11]. Vertical interconnect serialization technique is proposed as one way to address above challenges in [18]. For instance, a 4to-1 serialization of TSV interconnects can save more than 70% of TSV area footprint. However, serialization will cause vertical link bandwidth decreases, for example at least 75% for 4-to-1 serialization. 3D NoC throughput decreases especially when the trafﬁc is nonuniform. Even worse, serialization inevitably increases the zero-load latency, and thus even network latency under light workload deteriorates, particularly when aggressive serialization policy is applied. Although serialization reducing the number of TSV enable higher frequency TSV transmission, additional higher frequency clock domain may not be accepted by most synchronous design. In this paper, we propose a TSV squeezing scheme among neighboring NoC routers. The idea is mainly based on a key observation that the TSVs’ utilization is pretty low and adjacent routers rarely transmit packets via their TSVs at the same time in symmetric 3D mesh NoC as will be shown in the next section. Therefore, we propose to squeeze adjacent TSVs together, i.e. neighboring routers share the same vertical TSVs in a time division multiplex mode. TSV squeezing is not only able to greatly reduce planar chip area overhead but also improve TSVs’ utilization thus NoC’s performance in terms of latency and throughput signiﬁcantly. Experimental results prove the efﬁciency of the proposed scheme. The rest of the paper is organized as follows. Section 2 motivates this work. Section 3 presents the detailed design and implementation of TSV squeezing scheme. Section 4 shows experimental results. Finally, Sections 5 concludes this paper. 2 Motivation As stated in above, the beneﬁts of three dimensional integration are mainly brought by vertical interconnects, i.e., TSVs. However, TSVs also bring overhead. Therefore, we can not afford unlimited TSVs, while more importantly, we need to improve TSVs’ utilizations. In this section, we ﬁrstly give in-depth analysis on TSVs’ utilization in 3D mesh NoC. 4 × 4 × 2 3D mesh NoC is implemented in a cycle-accurate NoC simulator written in SystemC. Dimension order routing algorithm and wormhole ﬂow control are employed in the network. Each router in the network has two stage pipelines and two virtual channels with 8-ﬂit depth in each input port. Each packet contains 8-ﬂit and is injected according to a poison process. Then we analyze both the average router TSVs’ utilization and neighboring routers’ TSV conﬂiction probability every 5000 cycles. Note that TSV conﬂiction means the occasion when both TSVs in neighboring routers transmit a ﬂit at the same time. It is not a real conﬂiction physically. Fig.2(a) and Fig.2(b) show average TSV utilization and conﬂiction probabilities under uniform trafﬁc and shufﬂe trafﬁc respectively. p1 and p2 in the ﬁgure stand for average TSV utilization rate and TSV conﬂiction rate correspondingly when average injection rate is 0.3. While p3 and p4 represent the same information when average injection rate is 0.1. It can be seen that average TSV utilization is quite low especially under light network load. Even when network load goes up to 0.3, TSV still keeps idle more than 80% of the time. Moreover, TSVs in neighboring nodes are seldom used at the same time. Particularly, when the trafﬁc is unbalanced across the network, TSV conﬂiction probability is almost negligible as shown in shufﬂe trafﬁc pattern. Based on above analysis, we can conclude that TSV in the symmetric 3D mesh NoC is under utilized signiﬁcantly. Meanwhile, data transmission in 3D NoC also exhibits a temporal characteristic that TSV in different nodes are rarely busy at the same time. In order to make full use of the vertical TSVs in 3D NoC, we develop a delicate scheme allowing neighboring nodes sharing a single TSV bundle. The idle time of TSVs can be seen as bubbles, and TSV squeezing get rid of these bubbles and maximize the utilization of TSVs, which not only save planar chip area but also maintain high performance. p1 p2 p3 p4 0.2 0.18 0.16 0.14 0.12 0.1 0.08 0.06 0.04 0.02 0 0 20 40 60 80 100 Time Intervals (a) Uniform Trafﬁc 0.18 0.16 0.14 0.12 0.1 0.08 0.06 0.04 0.02 0 0 20 40 60 80 100 Time Intervals 120 140 160 p1 p2 p3 p4 120 140 160 (b) Shufﬂe Trafﬁc Figure 2: TSV Utilization and Conﬂiction Probability 3 TSV Squeezing Scheme 3.1 General Idea The general idea of TSV squeezing scheme in 3D NoC is shown in Fig.3 in which four nodes share a single TSV bundle. In each 358 silicon layer, router groups in upper layer and router groups in bottom layer are both coupled correspondingly. Therefore, physical vertical channels are seen as shared resources and should be granted for a transmission. Whenever there is data transmitting from one layer to the other, it will have to acquire grant of TSV sharing logic at ﬁrst. Then data with grant will go straight to TSV sharing logic in current layer and ﬂow through TSV later. Finally the data will be forwarded to corresponding router from TSV sharing logic in target layer. TSV Sharing Region Router TSV Pad TSV Sharing Logic TSV Figure 3: 3D Mesh NoC with TSV Squeezing Despite of above transmission process, there are still another two issues needed to be explained in detail. Firstly, although routers in the group share TSV, we do not change original 3D mesh logic topology and we comply with strict dimension order algorithm instead. Therefore, there is still only one path available from a source router to a destination router. However, the new physical topology actually enables more ﬂexible routing choices and shorter average network hops. For example, data in upper layer transmitting to nether layer doesn’t have to reach the right node above destination before ﬂowing through TSV. Namely, as long as the data arrives at the right group above destination, it can travel down to the destination. Nevertheless, new algorithm may be necessary in case of routing deadlock, and we leave it for future work. Secondly, as any data transmitting from a router within one layer to a router within the other layer now need to traverse longer link i.e. upper horizontal link, and then vertical link, and ﬁnally bottom horizontal link. One single clock cycle will not be sufﬁcient. To maintain the working frequency of the router, additional pipelines are inserted during vertical transmission. We will describe thoroughly how additional pipeline and the TSV squeezing logic are integrated in router architecture design in next subsection. 4B-3 Router TSV Arbiter Router Router Data TSV Router Router Router Ctrl TSV Router Router Figure 4: TSV Squeezing Architecture detects data in buffer queue and calculates output port for each packet according to the routing algorithm. VA in the router simply serves for the head ﬂit and it mainly aims to reserve buffer in next router. SA in the router performs the arbitration to decide the winner of all the candidates for each output port. A ﬂit has to forward across the above components before it leaves a conventional router. For TSV squeezing router, however, the vertical interconnects are now considered as a shared resource instead of dedicated for each router. When data from router A is going to be sent to router a, it has to request grant of TSV Arbiter (TA) in advance. Then when the data ﬁnally acquires TA grant as well as arbitration grants inner router, it can ﬂow along the data TSV to the bottom layer. Meanwhile, TA grant is also sent to the bottom layer through Ctrl TSVs to guide crossbar forwarding the data to the destination node. Fig.5 exhibits a conventional router architecture with TSV squeezing and logic in dashed line represents original router architecture. Additionally, TA is simply a normal arbiter, all the conventional arbitration strategies including round robin, cyclic priority and oldest ﬁrst can be used. RC VA SA TA Buffer Buffer In Buffer Out Crossbar Figure 5: Modiﬁed Router Architecture 3.2 Router Architecture and Implementation Fig.4 illustrates the proposed basic TSV squeezing architecture. Since TSV squeezing logic for up-to-down TSV and down-toup TSV are almost the same, we only show squeezing logic for up-to-down TSV as an example in this ﬁgure. Corresponding down-to-up logic in planar layer is also ignored. In order to support TSV squeezing scheme, we need to make minor modiﬁcation based on conventional router. Conventional router usually consists of buffer, routing computing (RC), virtual channel allocator (VA), switch allocator (SA) and crossbar. When a ﬂit from link arrives, it will then be stored in buffer. RC However, adding TA will increase the critical path of a conventional router. Fortunately, pre-allocation and speculation, which are widely applied to parallelize RC, VA, SA, crossbar operation and reduce critical path [23] [24], can also be adapted to TA design. Fig.6 shows a single data path of modiﬁed router microarchitecture with TSV squeezing. Shaded blocks in the ﬁgure is the additional logic for TSV squeezing. It can be seen that data now needs the grants from VA, SA and TA at the same time, while VA grant and SA grant are sufﬁcient for data transmission in conventional design. Meanwhile, as pre-allocation results are stored in registers for next cycle, they are removed from potential critical path and ensure parallel operations of 359 4B-3 VA SA VA_req SA_req Reg Reg Buffer Request & crossbar Reg Reg Reg TA_req TA Reg x u M Reg x u m e D Input Link Current Router Horizontal Link in  upper layer TSV Horizontal Link in  nether layer Next Router Figure 6: Data Path of Modiﬁed Router Architecture with TSV Squeezing VA, SA, TA and crossbar. In order to make sure TA is not far from any of the sharing nodes, TA is ideally located in the center of sharing nodes. In this case, wire latency from request to grant can’t be ignored. Data path is lengthened and additional pipeline is added in case of this additional delay as shown in Fig.6. Additional data path pipelines have little control association, we simply store TA grant signal accordingly. However, it is not easy to decide how many additional pipelines should be added, because layout has quite a signiﬁcant inﬂuence on horizontal link latency and TSV latency with different sizes also varies. Assume that critical path latency inner router is 𝑡0 , horizontal link latency from router to TA is 𝑡1 (𝑡1 < 𝑡0 ), TSV latency is 𝑡2 (𝑡2 < 𝑡0 ), Mux latency is 𝑡3 (𝑡3 < 𝑡0 ), Demux latency is 𝑡4 (𝑡4 < 𝑡0 ), buffer write latency is 𝑡5 (𝑡5 < 𝑡0 ). Then original number of pipeline stages between two neighboring routers of different layers is approximately 𝑜𝑝 = ⌈(𝑡2 + 𝑡5 )/𝑡0 ⌉, and current number of pipeline stages between neighboring routers of different layers is approximately 𝑐𝑝 = ⌈(𝑡1 + 𝑡2 + 𝑡3 + 𝑡4 + 𝑡5 )/𝑡0 ⌉. 𝑜𝑝 can be safely considered to be 1 that means it takes one clock cycle for a ﬂit to traverse the TSVs, while 𝑐𝑝 depends on the relative location between TSV and routers. In order not to introduce too much overhead, we restrict the sharing domain to be small, e.g. no more than 4 adjacent routers, so that we need at most 2 cycles to transmit a ﬂit across the TSV, i.e., 𝑐𝑝 = 2. 3.3 Global Organization for TSV Squeezing Although TSV squeezing scheme could reduce considerable TSVs, it is not proper for two distant routers to share their TSVs as it will introduce quite long global wires and may degrade the performance of NoCs. In this work, we constrain routers that can share their TSVs only if they can construct a 2× 2 mesh or a subpart within it. This can restrict routers sharing TSVs within 2 hops. To save more TSVs, we ﬁrst need to group routers into 2 × 2 meshes as many as possible, then however we should handle several corner cases as examples shown in Fig.7. For case A, we may have two design alternatives, the ﬁrst one is ”4+2” conﬁguration and the other is ”3+3”. Both conﬁgurations need two vertical channels for the 6 routers, and save 66.7% TSV footprint when compared to conventional 3D NoC design. For case B, we have much more choices and two of them are shown in the ﬁgure. The differences among these conﬁgurations are the placement of TSV pads in the planar chip, as shown in Fig.7. When to decide how to organize the routers to share TSVs, it greatly depends on the placement of other on-chip components to avoid congestions. In addition, we need to group as many as routers without letting the shared TSVs being far away from them. This routing and placement problem is out of the scope of this paper. TSV pads Corner case A  Corner case B  Figure 7: Three Detailed Network Partitions 4 Experimental Results In this section, the proposed TSV squeezing scheme is evaluated through simulations in terms of performance penalty and area overhead respectively. 4.1 Performance We developed a cycle-accurate NoC simulator using SystemC. And 4 × 4 × 2 3D mesh NoC is implemented based on the simulator. The proposed router design in the network adopts two-staged pipeline design employing wormhole ﬂow control, dimensional order routing algorithm and round robin arbitration strategy. Furthermore, there are two 8-ﬂit buffers in each input port. To get a comprehensive view of the proposed design, performance of the original symmetric 3D NoC, 3D NoC with 360 0 0.1 0.2 0.3 0.4 0.5 20 30 40 50 60 70 80 90 100 Injection Rate (flits/node/cycle) A e v r y c n e a L e g a t ( s e c y c l ) Original Se2-1 Sq2 Se4-1 Sq4 (a) Uniform Trafﬁc 0 0.1 0.2 0.3 0.4 0.5 20 30 40 50 60 70 80 90 100 Injection Rate (flits/node/cycle) A e v r y c n e a L e g a t ( s e c y c l ) Original Se2-1 Sq2 Se4-1 Sq4 (b) Shufﬂe Trafﬁc 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 20 30 40 50 60 70 80 90 100 Injection Rate (flits/node/cycle) A e v r y c n e a L e g a t ( s e c y c l ) Original Se2-1 Sq2 Se4-1 Sq4 (c) Hotspot Trafﬁc Figure 8: Latency and Throughput Comparison under Synthetic Trafﬁc Patterns serialized vertical channels and the proposed TSV squeezing NoC are all compared under various trafﬁc patterns including uniform, shufﬂe, and hotspot. The comparison is presented in Fig.8. Se2-1 and Se4-1 in the ﬁgure represent NoC using 2:1 TSV serialization and NoC using 4:1 TSV serialization accordingly. While Sq2 and Sq4 stand for NoC with at most two neighboring routers sharing a single bundle of TSV and NoC with at most four neighboring routers sharing a single bundle of TSV respectively. It is obviously that zero-load latency of Sq4, which is close to that of original design, decreases by about 20% compared with that of Se4-1 under all these trafﬁc patterns. The reason mainly lies in that Se4-1 brings in additional three-cycle latency induced by vertical link transmission while Sq4 adds only a single cycle penalty and pipeline transmission further alleviates the penalty. Moreover, throughput of Sq4 is approximate 30% higher than that of Se4-1 under uniform and shufﬂe (Assume injection rate to be throughput when network latency reaches 100 cycles). For Se2-1 and Sq2, since vertical link transmission penalty are almost the same except that the latter can be pipelined, thus Sq2 only performs slightly better than Se2-1. We can also observe that for shufﬂe trafﬁc which results in signiﬁcant unbalance in TSV utilization, it is quite suitable to apply the proposed squeezing scheme. While serialization scheme suffers more network throughput loss, as some busy while serialized TSVs become the bottleneck. In this case, as can be seen from Fig.8(b), Se4-1 and Se2-1 show signiﬁcant lower throughput when compared to Sq2 and Sq4. Finally, we consider the impact of TSV arbitration on NoC performance. Fig.9 shows performance of Sq4 with various arbitration schemes including oldest ﬁrst, round robin and cyclic priority under uniform trafﬁc. For other trafﬁc patterns, we can observe much similar results and are omitted due to page limits. The results indicate that arbitration strategy has little inﬂuence on NoC performance, which again proves that TSV contention rarely happens. Therefore, we can employ simple arbitration method that can reduce chip area overhead. 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 20 30 40 50 60 70 80 90 100 Injection Rate (flits/node/cycle) A e v r y c n e a L e g a t ( s e c y c l ) Oldest First Round Robin Cycl ic Priority Figure 9: TSV Arbitration Impact under Uniform Trafﬁc 4.2 Planar Chip Area To quantify the impact using the proposed design on TSV area footprint, TSV squeezing and serialization schemes are implemented at RTL level and synthesized with Design Compiler [25]. The synthesis was performed using TSMC standard cell libraries. Fig.10 shows detailed comparison of planar chip area savings. We assume 16nm TSV pitch and 64-bits width vertical TSVs in this experiment, then we calculate the maximum planar chip area saving due to TSV reductions. For example, if 4 routers share one vertical channels, i.e., Sq4 as above, maximum 75% amount of TSVs can be eliminated. However, the additional logic, such as TSV arbiters, serializer, deserializer etc shrink the beneﬁts of TSV reductions, indicated as overhead in this ﬁgure. It can be seen from the ﬁgure, Sq2 achieves a little less area savings than Se2-1, while Sq4 is much superior than Se4-1. Sq4 can save more than 60% TSV planar footprint when compared to a conventional 3D mesh NoC. The reason is that additional pipeline registers are the major source for additional chip area cost. As four routers share a pipeline register in Sq4, each 4B-3 361                 [9] F. Li, C. Nicopoulos, T. Richardson, Y. Xie, V. Narayanan, and M. Kandemir, “Design and management of 3D chip multiprocessors using network-in-memory,” ACM SIGARCH Computer Architecture News, vol. 34, no. 2, p. 141, 2006. [10] J. Kim, C. Nicopoulos, D. Park, R. Das, Y. Xie, V. Narayanan, M. Yousif, and C. Das, “A novel dimensionally-decomposed router for on-chip communication in 3D architectures,” ACM SIGARCH Computer Architecture News, vol. 35, no. 2, p. 149, 2007. [11] I. Loi, S. Mitra, T. Lee, S. Fujita, and L. Benini, “A low-overhead fault tolerance scheme for TSV-based 3D network on chip links,” in Proceedings of the 2008 IEEE/ACM International Conference on Computer-Aided Design, pp. 598–602, IEEE Press, 2008. [12] D. Park, S. Eachempati, R. Das, A. Mishra, Y. Xie, N. Vijaykrishnan, and C. Das, “MIRA: A multi-layered on-chip interconnect router architecture,” in Proceedings of the 35th International Symposium on Computer Architecture, pp. 251–261, IEEE Computer Society, 2008. [13] V. Pavlidis and E. Friedman, “3-D topologies for networks-on-chip,” IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 15, no. 10, pp. 1081–1090, 2007. [14] Y. Xu, Y. Du, B. Zhao, X. Zhou, Y. Zhang, and J. Yang, “A low-radix and low-diameter 3D interconnection network design,” [15] A. Topol et al., “Three-dimensional integrated circuits,” IBM Journal of Research and Development, vol. 50, no. 4/5, p. 506, 2006. [16] K. Bernstein, P. Andry, J. Cann, P. Emma, D. Greenberg, W. Haensch, M. Ignatowski, S. Koester, J. Magerlein, R. Puri, et al., “Interconnects in the third dimension: Design challenges for 3D ICs,” in Proceedings of the 44th annual Design Automation Conference, p. 567, ACM, 2007. [17] R. Patti, “Three-dimensional integrated circuits and the future of systemon-chip designs,” Proceedings of the IEEE, vol. 94, no. 6, 2006. [18] S. Pasricha, “Exploring serial vertical interconnects for 3D ICs,” in Proceedings of the 46th Annual Design Automation Conference, pp. 581–586, ACM, 2009. [19] H. Sangki, “3D Super-Via for Memory Applications,” [20] S. Das, A. Fan, K. Chen, C. Tan, N. Checka, and R. Reif, “Technology, performance, and computer-aided design of three-dimensional integrated circuits,” in Proceedings of the 2004 international symposium on Physical design, p. 115, ACM, 2004. [21] C. Ababei, Y. Feng, B. Goplen, H. Mogal, T. Zhang, K. Bazargan, and S. Sapatnekar, “Placement and routing in 3D integrated circuits,” [22] B. Swinnen, W. Ruythooren, P. De Moor, L. Bogaerts, L. Carbonell, K. De Munck, B. Eyckens, S. Stoukatch, D. Tezcan, Z. Tokei, et al., “3D integration by Cu-Cu thermo-compression bonding of extremely thinned bulk-Si die containing 10¿ m pitch through-Si vias,” in Electron Devices Meeting, 2006. IEDM’06. International, pp. 1–4, 2006. [23] R. Mullins, A. West, and S. Moore, “Low-latency virtual-channel routers for on-chip networks,” in Proceedings of the 31st annual international symposium on Computer architecture, IEEE Computer Society Washington, DC, USA, 2004. [24] R. Mullins, A. West, and S. Moore, “The design and implementation of a low-latency on-chip network,” in Proceedings of the 2006 Asia and South Paciﬁc Design Automation Conference, p. 169, IEEE Press, 2006. [25] S. Inc, “Design compiler,” Online, http://www. synopsys. com, 2009. 4B-3 router will have less area overhead than Sq2 in which only two routers share a pipeline register. Furthermore, when the technique scales down, logic area decreases dramatically, total area savings of both TSV schemes get close to maximum. ) g n v a i S a e r A ( % 80 75 70 65 60 55 50 45 40 35 30 Se2-1 Sq2 Se4-1 Sq4 Overhead 45nm 65nm 90nm 130nm Figure 10: Planar Chip Area Saving 5 Conclusion In this paper, we propose a TSV squeezing scheme that shares vertical interconnects among adjacent NoC routers. The idea is mainly based on the observation that TSVs’ utilization in 3D symmetric mesh NoC is pretty low and adjacent routers rarely transmit data via their vertical channels at the same time. When compared to prior TSV serialization method, the proposed solution can achieve much better performance in terms of network latency and throughput. The squeezing router has quite small chip area overhead while the reduced TSV can save much planar chip area. Experimental results show that the proposed scheme is able to save more than 60% TSV planar footprint, while network latency penalty is negligible. The throughput of TSV squeezing scheme is approximate 30% higher than that of serialization scheme under certain unbalanced trafﬁc patterns. "
2011,NS-FTR - A fault tolerant routing scheme for networks on chip with permanent and runtime intermittent faults.,"In sub-65nm CMOS technologies, interconnection networks-on-chip (NoC) will increasingly be susceptible to design time permanent faults and runtime intermittent faults, which can cause system failure. To overcome these faults, NoC routing schemes can be enhanced by adding fault tolerance capabilities, so that they can adapt communication flows to follow fault-free paths. A majority of existing fault tolerant routing algorithms are based on the turn model approach due to its simplicity and inherent freedom from deadlock. However, these turn model based algorithms are either too restrictive in the choice of paths that flits can traverse, or are tailored to work efficiently only on very specific fault distribution patterns. In this paper, we propose a novel fault tolerant routing scheme (NS-FTR) for NoC architectures that combines the North-last and South-last turn models to create a robust hybrid NoC routing scheme. The proposed scheme is shown to have a low implementation overhead and adapt to design time and runtime faults better than existing turn model, stochastic random walk, and dual virtual channel based routing schemes.","1RWLFH7KLVSDSHUZDVQRWSUHVHQWHGE\RQHRIWKHDXWKRUVLQ$63'$& 5B-3 NS-FTR: A Fault Tolerant Routing Scheme for Networks on Chip with  Permanent and Runtime Intermittent Faults  Sudeep Pasricha, Yong Zou  Colorado State University, Fort Collins, CO  {sudeep, yong.zou}@colostate.edu  Abstract (cid:177) In sub-65nm CMOS technologies, interconnection  networks-on-chip (NoC) will increasingly be susceptible to  design time permanent faults and runtime intermittent faults,  which can cause system failure. To overcome these faults, NoC  routing schemes can be enhanced by adding fault tolerance  capabilities, so that they can adapt communication flows to  follow fault-free paths. A majority of existing fault tolerant  routing algorithms are based on the turn model approach due  to its simplicity and inherent freedom from deadlock. However,  these turn model based algorithms are either too restrictive in  the choice of paths that flits can traverse, or are tailored to  work efficiently only on very specific fault distribution patterns.  In this paper, we propose a novel fault tolerant routing scheme  (NS-FTR) for NoC architectures that combines the North-last  and South-last turn models to create a robust hybrid NoC  routing scheme. The proposed scheme is shown to have a low  implementation overhead and adapt to design time and  runtime faults better than existing turn model, stochastic  random walk, and dual virtual channel based routing schemes.  I. Introduction  Chip multiprocessors (CMPs) with multiple processing and  memory cores integrated on a single chip are today at the heart of  many computing devices. As digital convergence continues to  increase application complexity, aggressive CMOS technology  scaling has allowed CMP designs to keep pace by allowing more  and more cores to be integrated in the same area budget with each  successive technology generation. However, with scaling heading  towards nanometer geometries, a major challenge that has appeared  on the horizon is the increased likelihood of failure due to  permanent, transient, and intermittent faults caused by a variety of  factors that are becoming more and more prevalent. Permanent  faults occur due to manufacturing defects, or after irreversible  wearout damage due to electromigration in conductors, negative  bias temperature instability (NBTI), dielectric breakdown, etc  [1][2]. Transient faults (also called soft errors) occur when an event  such as high-energy cosmic neutron particle strike, alpha particle  strike due to trace uranium/thorium impurities in packages,  capacitive and inductive crosstalk, electromagnetic noise, etc.  causes the deposit or removal of enough charge to invert the state  of a transistor, wire, or storage cell [3][4]. These errors occur for a  very short duration of time and can be hard to predict. A third class  of faults, called Intermittent faults, occurs frequently and  irregularly for several cycles, and then disappears for a period of  time [5][6]. These faults commonly arise due to process variations  combined with variation in the operating conditions, such as  voltage and temperature fluctuations.   On-chip interconnect architectures are particularly susceptible to  faults that can corrupt transmitted data or altogether prevent it from  reaching its destination. Reliability concerns in sub-65nm nodes  have in part contributed to the shift from traditional bus based  communication fabrics to network-on-chip (NoC) architectures that  provide better scalability, predictability, and performance than  buses [7][8]. The inherent redundancy in NoCs due to multiple  paths between packet sources and sinks can greatly improve  communication fault resiliency. Several CMP designs are emerging  that make use of NoCs as interconnection fabrics [9]-[12]. To  ensure reliable data transfers in these communication fabrics,  utilizing fault tolerant design strategies is essential. Traditionally,  error detection coding and retransmission has been a popular means  of achieving resiliency towards transient and intermittent faults  [13][14]. Alternatively, forward error correction (FEC) schemes  can provide better resiliency against these faults, but usually at a  higher performance and energy overhead [15]. Circuit and layout  optimizations such as shield insertion and wire sizing to reduce  crosstalk induced transient faults have also been proposed [16][17].  To overcome permanent faults in NoCs, fault tolerant routing  schemes are a critical requirement and have been the focus of  several research efforts over the last few years [20]-[38]. In the  presence of intermittent or permanent faults on NoC links and  routers, routing schemes can ensure error free data flit delivery by  using an alternate route that is free of faults.  In this paper, we propose a new fault tolerant routing scheme  (NS-FTR) for 2D mesh NoCs that combines the North-last and  South-last turn models together with opportunistic replication to  increase successful packet arrival rate while optimizing energy  consumption. For the first time, we explore the performance of  fault tolerant routing schemes in the presence of both runtime  intermittent and design time permanent faults. Compared to our  recently proposed fault tolerant routing algorithm (OE+IOE [39]),  the proposed NS-FTR algorithm has much lower implementation  overhead. Our extensive experimental results indicate that NS-FTR  outperforms OE+IOE as well as other state-of-the-art NoC fault  tolerant routing schemes based on single and hybrid turn models,  and stochastic random walk heuristics.  II. Related Work  NoC routing schemes can be broadly classified as either static  (also called deterministic or oblivious) or dynamic (also called  adaptive). While static routing schemes [18][19] use fixed paths  and offer no fault resiliency, dynamic (or adaptive) routing schemes  [20]-[38] can alter the path between a source and its destination  over time as traffic conditions and the fault distribution changes.  The design of adaptive routing schemes is mainly concerned with  increasing flit arrival rate, avoiding deadlock, and trying to use a  minimal hop path from the source to the destination to decrease  transmission energy. Unfortunately,  these goals are often  conflicting, requiring a complex trade-off analysis that is rarely  addressed in existing literature. In general, fault tolerant routing  schemes can be broadly classified into three categories: (i)  stochastic, (ii) fully adaptive, and (iii) partially adaptive.   Stochastic routing algorithms provide fault tolerance through  data redundancy by replicating packets multiple times and sending  them over different routes [20]. For instance, the probabilistic  gossip flooding scheme [21] allows a router to forward a packet to  any of its neighbors with some pre-determined probability.  Directed flooding refines this idea by preferring hops that bring the  packet closer to its addressed destination [22]. N-random walk [22]  limits the number of packet copies by allowing replications at the  source only. In the rest of the network, these copies stochastically  take different routes without further replication. The major  challenges with  these approaches are  their high energy  consumption, strong likelihood of deadlock and livelock, overhead  of calculating, storing, and updating probability values, and poor  performance even at low traffic congestion levels.   Fully adaptive routing schemes make use of routing tables in  978-1-4244-7516-2/11/$26.00 ©2011 IEEE 443               5B-3 every router or network interface (NI) to reflect the runtime state of  the NoC and periodically update the tables when link or router  failures occur to aid in adapting the flit path [23]-[25]. However,  these schemes have several drawbacks, including (i) the need for  frequent global fault and route information updates which can take  thousands of cycles at runtime during which time the NoC is in an  unstable state, (ii) lack of scalability (cid:177) table sizes increase rapidly  with NoC size, increasing router (or NI) area, energy, and latency,  and (iii) strong possibility of deadlock, unless high overhead  deadlock recovery mechanisms such as escape channels are used.  Partially adaptive routing schemes enable a limited degree of  adaptivity, placing various restrictions on routing around faulty  nodes, primarily to avoid deadlocks. Turn model based routing  algorithms such as negative-first, odd-even, north-last, and  south-last [26]-[29] are examples of partially adaptive routing,  where certain flit turns are forbidden to avoid deadlock. [30]  combines the XY and YX schemes to achieve fault resilient  transfers. [31] combines routing tables with a customized turn  model to avoid deadlock. Unfortunately, the degree of adaptivity  provided by these routing algorithms is highly unbalanced across  the network, which in some cases results in poor performance.  A few works have proposed using convex or rectangular fault  regions with turn models [32]-[38]. Routing detours around these  regions can be selected so that deadlock-prone cyclic dependencies  are impossible, such as by using turn models or cycle free contours.  In general, fault tolerant routing schemes that make use of fault  regions are either too conservative (e.g., disabling fully functional  routers to meet region shape requirements), have restrictions on the  locations of the faults that can be bypassed, or cannot adapt to  runtime intermittent/permanent faults.   Our recent work [39] combined the OE and IOE turn models to  create a hybrid fault tolerant routing scheme (OE+IOE) that  achieved high flit arrival rates. In contrast, this work proposes a  new fault tolerant routing algorithm (NS-FTR) that combines the  North-last (NL) and South-last (SL) turn models to create a hybrid  routing scheme with better fault tolerance and lower energy, latency,  and area overhead. NS-FTR balances data replication and resource  and path redundancy to optimize power dissipation while ensuring  increased  tolerance  to design  time permanent and runtime  intermittent faults. Data replication is limited to implementations  with high fault rates, which ensures a low cost operation in the  absence of faults (or for scenarios with very few faults). Path  diversity as a result of utilizing dual turn models ensures that even  if one packet hits a dead end, the other has a possibility of arriving  at the destination. Our experimental results (Section IV) that  compare NS-FTR with several state-of-the-art fault tolerant routing  schemes make a strong case for considering NS-FTR as a viable  fault tolerant routing scheme for emerging CMPs.  III. NS-FTR Routing Scheme  In this section we present an overview of our proposed fault  tolerant routing scheme (NS-FTR) for 2D mesh NoCs. Section A  introduces north last (NL) and south last (SL) turn model routing.  Section B presents details of the implementation and operation of  the proposed routing scheme that combines the NL and SL turn  models. Finally, Section C describes the router architecture and  control network that supports our NS-FTR scheme in a NoC fabric.  A. NL and SL Turn Models  The north last (NL) turn model for deadlock-free routing was  first introduced by Glass et al. [26] for large scale off-chip 2D mesh  networks. A turn in this context refers to a 90-degree change of  traveling direction for a flit. There are eight types of turns that are  possible in a 2D mesh based on the traveling directions of the  associated flits. A deadlock in wormhole routing can occur because  of flits waiting on each other in a cycle. In the NL turn model  deadlock-free routing is achieved by prohibiting two out of the  eight possible turns, as shown in Fig. 1(a) where dashed arrows  indicate prohibited turns. A flit in the NL turn model is initially  444 routed in the E, W, or S directions before finally turning in the N  direction, after which it cannot make further turns. In a similar  manner, the south last (SL) turn model ensures deadlock-free  routing by prohibiting two out of the eight possible turns as shown  in Fig. 1(b). A flit in the SL turn model is initially routed in the E,  W, or N directions before finally turning in the S direction, after  which it cannot make further turns.  (a)                             (b)  Fig. 1: Turns allowed in the (a) North-last (b) South-last algorithms  B. NS-FTR Routing Scheme: Overview  To ensure robustness against faults, redundancy is a necessary  requirement, especially for environments with high fault rates and  unpredictable fault distributions. As the level of redundancy is  increased, system reliability improves as a general rule. However,  redundancy also detrimentally impacts other design objectives such  as power and performance. Therefore in practice it is important to  limit redundancy to achieve a reasonable trade-off between  reliability, power, and performance. Unlike directed and random  probabilistic flooding algorithms that propagate multiple copies of  a packet to achieve fault tolerant routing in NoCs, the proposed  NS-FTR scheme sends only one redundant packet for each  transmitted packet, and only if the fault rate is above a replication  (cid:87)(cid:75)(cid:85)(cid:72)(cid:86)(cid:75)(cid:82)(cid:79)(cid:71)(cid:3) (cid:303)(cid:17)(cid:3) (cid:55)(cid:75)(cid:72)(cid:3) (cid:82)(cid:85)(cid:76)(cid:74)(cid:76)(cid:81)(cid:68)(cid:79)(cid:3) (cid:83)(cid:68)(cid:70)(cid:78)(cid:72)(cid:87)(cid:3) (cid:76)(cid:86)(cid:3) (cid:86)(cid:72)(cid:81)(cid:87)(cid:3) (cid:88)(cid:86)(cid:76)(cid:81)(cid:74)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3) (cid:81)(cid:82)(cid:85)(cid:87)(cid:75)(cid:3) (cid:79)(cid:68)(cid:86)(cid:87)(cid:3) (cid:11)(cid:49)(cid:47)(cid:12)(cid:3) turn model while the redundant packet is propagated using south  last (SL) turn model based routing scheme. The packet replication  happens only at the source. Two separate virtual channels (VCs),  one dedicated to the NL packets and the other for the SL packets  (cid:72)(cid:81)(cid:86)(cid:88)(cid:85)(cid:72)(cid:3) (cid:71)(cid:72)(cid:68)(cid:71)(cid:79)(cid:82)(cid:70)(cid:78)(cid:3) (cid:73)(cid:85)(cid:72)(cid:72)(cid:71)(cid:82)(cid:80)(cid:17)(cid:3) (cid:44)(cid:73)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3) (cid:73)(cid:68)(cid:88)(cid:79)(cid:87)(cid:3) (cid:85)(cid:68)(cid:87)(cid:72)(cid:3) (cid:76)(cid:86)(cid:3) (cid:79)(cid:82)(cid:90)(cid:3) (cid:11)(cid:76)(cid:17)(cid:72)(cid:17)(cid:15)(cid:3) (cid:69)(cid:72)(cid:79)(cid:82)(cid:90)(cid:3) (cid:303)(cid:12),  replication is not utilized and only the original packet is sent using  the NL scheme while power/clock gating the SL virtual channel to  (cid:86)(cid:68)(cid:89)(cid:72)(cid:3) (cid:83)(cid:82)(cid:90)(cid:72)(cid:85)(cid:17)(cid:3) (cid:55)(cid:75)(cid:72)(cid:3) (cid:89)(cid:68)(cid:79)(cid:88)(cid:72)(cid:3) (cid:82)(cid:73)(cid:3) (cid:303)(cid:3) (cid:76)(cid:86)(cid:3) (cid:68)(cid:3) (cid:71)(cid:72)(cid:86)(cid:76)(cid:74)(cid:81)(cid:72)(cid:85)-specified parameter that  depends on several factors such as the application characteristics,  routing complexity, and power-reliability trade-off requirements.   The NS-FTR routing algorithm prioritizes minimal paths that  have higher probabilities of reaching the destination even if faults  are encountered downstream. Minimal paths and the replication  threshold ensure low power dissipation under a diverse set of fault  rates and distributions. No restriction on the number or location of  faults is assumed, but the routers must know which of their  adjacent (neighbor) links/nodes are faulty, which is accomplished  using control signaling. The proposed routing approach can be  combined with error control coding (ECC) techniques for transient  fault resiliency and optimizations such as router buffer reordering  [40] and router/NI backup paths [41] to create a comprehensive  fault tolerant NoC fabric. In the following subsections, we describe  the implementation of the proposed NS-FTR scheme.  B.1 Turn Restriction Checks  Whenever a packet arrives at a router in the NS-FTR scheme,  three scenarios are possible: (i) there is no fault in the adjacent  links and the packet is routed to the output port based on the NL (or  SL) scheme that selects a minimal path to the destination, (ii) there  are one or more faults on adjacent links that prevent the packet  from propagating in a valid direction (an output port with a fault-  free link that does not violate turn model routing rules) in which  case the packet must be dropped as a back turn is not allowed, and  (iii) one or more adjacent links have faults but a valid direction  exists to route the packet towards, based on the NL (or SL) rules. A  packet traversing an intermediate router must choose among four  output directions (N, S, W, E). However, choosing some directions  may lead to a violation of a basic NL (or SL) turn rule immediately  or downstream based on the relative location of the destination.                 Fig.2 shows the SystemC [44] pseudocode of the turn restriction  check phase for the NL turn model in our router model to aid in  detecting an invalid routing direction. The input to the turn  restriction check phase is a potential output port direction for a  header flit currently in the router input buffer, and the output of the  phase indicates whether the output port direction is valid or not.   First, we check whether the output port direction has a fault in  its attached adjacent link (steps 3-4) or is a back turn (steps 5-6), in  which case this is an invalid direction. Next, we check for scenarios  where choosing the given output port direction may lead to  violations of the NL turn model rules immediately or eventually at  some point downstream. In steps 7-8, we check for the scenario  where the packet arrives in the same column as its destination  which is located in the N direction. If the packet attempts to make a  turn to E, W, or S, it will violate the basic NL turn model rules, and  this is not allowed. Steps 9-12 test for two cases when the packet  cannot be routed in the N direction without violating the NL turn  model rules: if the current router node is already to the north of the  destination code, or if the current node is not on the same column  as its destination node. Finally, in steps 13-18, we check for a  particular scenario in which the current node is on the southern  border of the mesh, and with the packet attempting to go in the  opposite direction to where the destination is located (e.g., going W  when the destination node is in the E direction; steps 14-15). As the  packets can go north only once they arrive at the column of their  destination nodes, going in the opposite direction will eventually  violate the NL turn model rules and is thus not allowed.  Pseudocode:  Turn Restrictions for North-last Implementation   1:   RESULT   check_neighbor(UI ip_dir, UI direct, ULL cur_id, ULL dest_id)         //ip_dir: direction where the packet comes from         //direct: direction we want to check         //cur_id: current node position          //dest_id: destination node position.         //dest_yco,dest_xco: y,x coordinates of destination node         //cur_yco,cur_xco: y,x coordinates of current node         //dif_yco=dest_yco-cur_yco           //RESULT: LINK_FAULT, TURN_FAULT, BACK_TURN, OK  2:  {  3:     if (check_linkfault(direction)==LINK_FAULT)    4:       return LINK_FAULT;  5:     if (ip_dir==direct)   6:        return BACK_TURN;  7:     if(((cur_yco==dest_yco)&&(cur_xco>dest_xco))&&(direct!=N))  8:       return TURN_FAULT;  9:     if((cur_xco<dest_xco)&&(direct==N))  10:      return TURN_FAULT;  11:    if((cur_yco!=dest_yco)&&(direct==N))  12:      return TURN_FAULT;  13:    if(borderS(cur_id)) {  14:      if((dest_yco>cur_yco)&&direct==W)  15:        return TURN_FAULT;  16:      else if((dest_yco<cur_yco)&&direct==E)  17:        return TURN_FAULT;  18:     }  19:         return OK;  20:  }  Fig.2. SystemC pseudocode for NL turn restriction checking  The turn restriction check phase described above is invoked in  parallel for all four possible directions in an intermediate node for a  header flit. Fig. 3 shows the circuit level diagram for the NL turn  restriction check phase, implemented in the route compute unit of a  NoC router. The circuit is replicated at each input port of the router.  At the TSMC 65nm technology node, the circuit dissipates 1.34  μW of power on average, and has a critical path latency of 0.48 ns.  The pseudocode and circuit level diagrams for the SL turn model  are similar and not shown here for brevity. In contrast, the circuit  level implementation overhead for the turn restriction check phase  in our previous work that combined the OE and IOE turn models  [39] was much greater, with 2.03 μW of power dissipation on  average, and a critical path latency of 0.54 ns at the 65 nm node.  Section IV presents results to compare the fault tolerance  capabilities of these two schemes, as well as several other schemes  proposed in literature.  445 5B-3 Fig.3. NL turn restriction check phase circuit diagram  B.2 Prioritized Valid Path Selection  After the turn restriction checks, it is possible for a packet to  have multiple valid directions to choose from. In the NL scheme,  we identify two scenarios where such a choice between valid paths  exists and show how NS-FTR selects among them. Again, we omit  describing analogous scenarios for the SL scheme for brevity.      (a)                          (b)  Fig.4. Path selection scenarios for NL turn scheme  Fig. 4(a) demonstrates an example of the first scenario. (cid:179)(cid:54)(cid:180)(cid:3) (cid:68)(cid:81)(cid:71)(cid:3) (cid:179)(cid:39)(cid:180)(cid:3) (cid:68)(cid:85)(cid:72)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3) (cid:86)(cid:82)(cid:88)(cid:85)(cid:70)(cid:72)(cid:3) (cid:68)(cid:81)(cid:71)(cid:3) (cid:71)(cid:72)(cid:86)(cid:87)(cid:76)(cid:81)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3) (cid:81)(cid:82)(cid:71)(cid:72)(cid:86)(cid:17)(cid:3) (cid:58)(cid:76)(cid:87)(cid:75)(cid:3) (cid:81)(cid:82)(cid:3) (cid:73)(cid:68)(cid:88)(cid:79)(cid:87)(cid:86)(cid:15)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3) (cid:49)(cid:47)(cid:3) minimal route would take the path S-11-10-5-D. Suppose that the  link between S and node 11 has a fault. Then two choices exist:  sending the packet to node 13 or 17. In this situation, we should  choose node 17 as the resulting path is shorter by two hops than if  node 13 is selected. A shorter path is preferred as it has lower  probability of encountering a fault and consumes lower energy.  This example can be generalized as follows: if a packet is to the SE  or SW of its destination node, and the minimal path cannot be  chosen due to a fault, then we give the S direction higher priority.   Fig. 4(b) shows the second scenario. At source S, a packet  intended for destination D can be sent along two minimal paths. We  observe that if the packet is sent along the path with node 18, the  only paths to D that are available are S-18-17-16-15-D, S-18-17-  16-21-D, S-18-17-22-21-D, S-18-23-22-21-D, and S-18-19-24-23-  22-21-D. In contrast, if the packet is sent along the path with node  12, many more paths exist. This greater path diversity is critical in  the presence of faults, as it can ensure a better probability of packet  arrival at D. Again we can generalize the example as follows: if a  packet is at the NE (or NW) of the destination and if multiple valid  minimal paths exist, then we prioritize the E (or W) directions.  C. Router Architecture and Control Network  We make use of a dual virtual channel (VC) router architecture.  Packets traversing the network have a header flit with a destination  ID field that is used to determine the output port at each router. The  router datapath consists of the buffers and the switch. The input  FIFO buffers store up to 16 flits waiting to be forwarded to the next  hop. There are two input FIFO buffers each dedicated to a VC, with  one VC for the NL routed packets and the other for the SL routed  packets. When a flit is ready to move, the switch connects an input  buffer to an appropriate output channel. To control the datapath, the  router contains three major control modules: a route compute  control unit (RC_CTRL), a virtual-channel (VCA) allocator, and a  switch allocator (SA). These control modules determine the next  hop direction, the next virtual channel, and when a switch is                                5B-3 available for each packet/flit. The routing operation takes four  phases: route computation (RC), virtual-channel allocation (VCA),  switch allocation (SA), and switch traversal (ST). When a header  flit (the first flit of a packet) arrives at an input channel, the router  stores the flit in the buffer for the allocated VC and determines the  next hop for the packet using the checks and priority rules  described in the previous sections (RC phase). Given the next hop,  the router then allocates a virtual channel (VCA phase). Finally, the  flit competes for a switch (SA phase) if the next hop can accept the  flit, and moves to the output port (ST phase). In our scheme, a  packet carries its virtual channel (NL or SL) information in a bit in  the header flit, therefore the output port and the destination VC can  be determined early, at the end of the RC stage.  To reduce energy consumption, if the fault rate is below the  (cid:85)(cid:72)(cid:83)(cid:79)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3) (cid:87)(cid:75)(cid:85)(cid:72)(cid:86)(cid:75)(cid:82)(cid:79)(cid:71)(cid:3) (cid:303)(cid:15)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3) RC control unit shuts down the SL  virtual channels in the router. We assume that dedicated control  signals connected to a lightweight fault detection unit (FDU) can  allow estimation of the fault rate at runtime in the NoC, and the  decision to change the replication status (initiate or cut-off) can be  communicated to the nodes and implemented with a delay of at  most a few hundred cycles. Typically, such a change in status  would be a rare occurrence as intermittent or permanent faults do  not appear at runtime as frequently as transient faults.   An important requirement for any fault tolerant NoC fabric is a  control network that can be used to send acknowledgement signals  to inform the source whether the packet successfully arrived at the  destination. We assume a lightweight and fault-free control network  that is topologically similar to the data network for the purpose of  routing ACK signals from the destination to the source node on  successful packet arrival, or NACK signals from an intermediate  node to the source node if the packet must be dropped due to faults  that make it impossible to make progress towards the destination.  In our implementation, a source can re-send a packet at most twice  after receiving NACK signals, before assuming that the packet can  never reach its destination. While this can signal unavoidable  failure for certain types of applications, other applications may still  be able to operate and maintain graceful degradation. For instance,  a multiple use-case CMP can shut down certain application  use-cases when their associated inter-core communication paths  encounter fatal faults, but there might be other use cases that can  still continue to operate on fault-free paths. We assume that a  higher level protocol (e.g. from the middleware or OS levels) can  step in to initiate thread migration and other strategies to ensure  useful operation in the presence of unavoidable system faults.  IV. Experimental Studies  A. Evaluation Setup  We implemented and evaluated our proposed algorithm using a  SystemC based cycle-accurate 2D mesh NoC simulator that was  created by extending the open source Nirgam simulator [42] to  support faults and different fault tolerant routing schemes. In  addition to communication performance estimates, we were  interested in NoC energy estimation as well. For this purpose we  incorporated dynamic and leakage power models for standard NoC  components from Orion 2.0 [43] into the simulator. We also  incorporated the power overhead of the circuits in the routers  (obtained after synthesis) required to realize the fault tolerant  routing algorithms. The NoC fabric was clocked at 1 GHz. The  target implementation technology was 65nm, and this node was  used to determine parameters for delay and energy consumption. A  9×9 (i.e., 81 cores) CMP with a mesh NoC was considered as the  base system in our experiments. The stimulus for the simulations  was generated for specific injection rates using a uniform random  traffic model. A fixed number of flits (3000/core) were transmitted  according to the specified traffic pattern in the simulations, to  enable comparisons for not only successful arrival rates, but also  for overall communication energy and average packet latency.   We modeled two types of faults in the NoC: (i) design time  446 permanent faults that are randomly distributed link-level hard  failures, and (ii) runtime intermittent faults, which are also  randomly distributed and appear due to reasons such as periods of  thermal hotspots and high crosstalk noise during system operation.  For intermittent faults, we assumed a fault duration of 5000 cycles,  with the fault location being randomly generated, as for permanent  faults. Ten different random fault distributions were generated and  analyzed for each fault rate to obtain a more accurate estimate of  fault resiliency for the routing schemes. To ensure fair comparison,  the randomized fault locations and durations were replicated across  simulation runs for different fault tolerant routing schemes, to study  the performance of the schemes under the same fault conditions.  B. Comparison with Existing FT Routing Schemes  Our first set of experiments compared the packet arrival rates for  the proposed NS-FTR fault tolerant routing scheme with several  fault tolerant routing schemes from literature. The following  schemes were considered in the comparison study: (i) XY  dimension order routing [18] (although not inherently fault tolerant,  it is considered here because of its widespread use in 2D mesh  NoCs), (ii) N-random walk [22] for N = 1,2,4,8 (iii) adaptive OE  turn model [29], (iv) adaptive North-last (NL) turn model [26], (v)  adaptive OE+IOE [39] which combines replication with the OE  and IOE turn models on two VCs, and (vi) XYX [30] which  combines replication with the XY and YX routing schemes on two  VCs. Other fault tolerant routing schemes such as probabilistic  flooding [21] and fully adaptive table based routing [24] were  considered but their results are not presented because these  schemes have major practical implementation concerns - their  energy consumptions is an order of magnitude higher than other  schemes and frequent deadlocks hamper overall performance. For  our NS-FTR scheme, we used a prioritized valid path selection  introduced in Section III.B.2 and a replication th(cid:85)(cid:72)(cid:86)(cid:75)(cid:82)(cid:79)(cid:71)(cid:3) (cid:89)(cid:68)(cid:79)(cid:88)(cid:72)(cid:3) (cid:82)(cid:73)(cid:3) (cid:303)(cid:3) = 6%, based on our extensive simulation studies that showed a  good trade-(cid:82)(cid:73)(cid:73)(cid:3)(cid:69)(cid:72)(cid:87)(cid:90)(cid:72)(cid:72)(cid:81)(cid:3)(cid:68)(cid:85)(cid:85)(cid:76)(cid:89)(cid:68)(cid:79)(cid:3)(cid:85)(cid:68)(cid:87)(cid:72)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:72)(cid:81)(cid:72)(cid:85)(cid:74)(cid:92)(cid:3)(cid:73)(cid:82)(cid:85)(cid:3)(cid:27)(cid:8)(cid:3)(cid:149)(cid:3)(cid:303)(cid:3)(cid:149)(cid:3)(cid:21)(cid:8)(cid:17)  Fig. 5 (a)-(c) compare the successful packet arrival rate for the  fault tolerant routing schemes described above under different fault  rates and types. Results are shown for a 20% injection rate (0.2  flits/node/cycle). The arrival rates are shown for permanent faults  (Fig. 5(a)), intermittent faults (Fig. 5(b)), and an equal distribution  of permanent and intermittent faults (Fig. 5(c)). Not surprisingly,  arrival rates for all routing schemes in general are higher under  intermittent faults than under permanent faults, as intermittent  faults only last for a short period of time. On analyzing individual  routing scheme performance, it can be immediately seen that the  N-random walk schemes have a low arrival rate even in the  absence of faults. This is due to frequent deadlocks during  simulation which the scheme is susceptible to, causing packets to  be dropped. The best arrival rate for the N-random walk scheme is  achieved for a value of N=8. Out of the turn/dimension-order based  routing schemes, XY not surprisingly performs worse than the  other schemes due to the lack of any built in fault tolerance  capability. The arrival rates for the NL and OE routing schemes are  comparable. Among the dual VC schemes, OE+IOE outperforms  XYX due to greater path diversity with the less restrictive OE and  IOE turn model schemes than with XY or YX schemes. However,  our NS-FTR scheme has a higher successful packet arrival rate than  OE+IOE and every other scheme we compared. For fault rates  under 6%, replication in NS-FTR is disabled to save energy  (discussed later), which explains the slight dip in successful arrival  rate for low fault rates. However, with replication enabled, NS-FTR  outperforms every other scheme at low fault rates as well. NS-FTR  scales particularly well under higher fault rates that are expected to  be the norm in future CMP designs.  In addition to arrival rate, an increasingly important metric of  concern for designers is the energy consumption in the on-chip  communication network. Fig. 6 shows the energy consumption for  the different routing schemes under the same fault rates and types,  and injection rates as in the previous experiment. The energy        5B-3 resulting lower energy consumption may lead to a much longer  battery life and better overall user experience. Conversely, if fault  (cid:85)(cid:72)(cid:86)(cid:76)(cid:79)(cid:76)(cid:72)(cid:81)(cid:70)(cid:92)(cid:3)(cid:76)(cid:86)(cid:3)(cid:83)(cid:68)(cid:85)(cid:68)(cid:80)(cid:82)(cid:88)(cid:81)(cid:87)(cid:15)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:89)(cid:68)(cid:79)(cid:88)(cid:72)(cid:3)(cid:82)(cid:73)(cid:3)(cid:303)(cid:3)(cid:70)(cid:68)(cid:81)(cid:3)(cid:69)(cid:72)(cid:3)(cid:85)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:71)(cid:3)(cid:87)(cid:82)(cid:3)(cid:70)(cid:79)(cid:82)(cid:86)(cid:72)(cid:3)(cid:87)(cid:82)(cid:3)(cid:19)(cid:15)(cid:3) to enable the highest possible successful packet arrival rate.    consumption accounts for circuit level implementation overheads  as well as acknowledgements and retransmission in the schemes.  Energy consumption under intermittent faults is lower than under  permanent faults as intermittent faults last for only a short amount  of time, incurring lower retransmission and non-minimal path  selection overheads in the schemes. It can be seen that the  N-random walk fault tolerant routing scheme has significantly  higher energy consumption compared to the other schemes. This is  due to the high level of replication with increasing values of N, as  well as because of the non-minimal paths chosen by default to  route the packets on. For single VC schemes (OE, NL, XY), the  XY scheme has the lowest energy dissipation which explains its  popularity in NoCs today. But as it is lacking in any fault tolerance,  the scheme may not be a viable option for future CMPs.   (a)  (b)  (a)  (b)  (c)  Fig. 6. Communication energy for 9×9 NoC with (a) permanent faults  (b) intermittent faults, (c) both permanent and intermittent faults  Fig.7 (a)-(b) summarizes the average packet latency for the  routing schemes for various injection rates, with a low fault rate  (Fig. 7(a); 1% permanent faults and 1% intermittent faults) and a  high fault rate (Fig. 7(b); 10% permanent faults and 10%  intermittent faults). The N-random walk scheme has significantly  higher packet latencies than other schemes due to its stochastic  non-minimal path selection process. For low fault rates (Fig. 7(a)),  low injection rates show comparable packet latencies for the rest of  the schemes. However, as injection rates increase, the minimal path  schemes (XY, XYX) have lower average latencies. But the latency  figure becomes irrelevant for the XY and XYX schemes as their  packet arrival rates suffer under high fault rates (Fig. 5). Among  schemes that support adaptivity in path selection, the NS-FTR has  the lowest packet latencies even for high injection rates.   For high fault rates (Fig. 7(b)), at low injection rates, both XY  and XYX schemes surprisingly have higher packet latencies than  turn model schemes such as NS-FTR, OE+IOE, OE, and NL. This  is because while turn model based schemes such as NS_FTR or OE  may not always use minimal paths (as XY and XYX schemes do),  they have greater adaptivity to avoid faults and thus avoid  retransmission, which can increase packet latencies directly and  also indirectly (e.g., by increasing congestion). At higher injection  rates, XY and XYX again have lower packet latencies, but suffer  from lower packet arrival rates than the NS-FTR and OE+IOE  schemes. Out of the two, NS-FTR again shows lower packet  latencies than OE+IOE, even as injection rates increase.   (c)  Fig. 5 Packet arrival rate for 9×9 NoC with (a) permanent faults, (b)  intermittent faults, (c) both permanent and intermittent faults  The energy consumption for the NL and OE schemes is  comparable and higher than the XY scheme because they use non-  minimal paths at times. But both NL and OE schemes offer higher  successful packet arrival rate due to greater path diversity  compared to the XY scheme, with the gap increasing especially  under higher fault rates. For high fault rates, the XYX scheme has  lower energy consumption than NS-FTR, OE+IOE, and even the  single VC schemes (NL and OE) because it only uses minimal  paths, much like the XY scheme. However, the XYX arrival rates  are lower than that for NS-FTR and OE+IOE. Under low fault rate  conditions (< 6%), both NS-FTR and OE+IOE disable replication,  and therefore have lower energy consumption than XYX. In some  cases, such a replication threshold based mechanism can be  extremely useful to trade-off energy with packet arrival rate. For  instance, for multimedia applications, occasionally dropped pixel  data may not be perceivable by viewers, and in such scenarios the  447                                         5B-3 (a)  (b)  Fig. 7. Average packet latency for 9×9 NoC with both permanent and  intermittent faults (a) 2% fault rate, (b) 20% fault rate  To summarize, from the results it is quite apparent that the  proposed NS-FTR scheme possesses high fault tolerance that scales  well with rising (permanent and intermittent) fault rates and  injection rates. Compared to the XYX scheme, NS-FTR has much  higher successful packet arrival rates (>10%) especially when fault  rates are high. Compared to the OE+IOE scheme, NS-FTR in  general has lower implementation overhead, higher packet arrival  rates, lower energy consumption, and lower packet latencies. These  results strongly motivate the use of NS-FTR in future CMP designs  to ensure fault tolerant on-chip communication.  V. Summary and Conclusions  In this paper, we propose a novel fault tolerant routing scheme  (NS-FTR) for NoCs that combines the North-last and South-last  turn models to create a robust hybrid NoC routing scheme. The  proposed scheme is shown to have a low implementation overhead  and adapt to design time and runtime faults better than existing turn  model, stochastic random walk, and dual virtual channel based  routing schemes such as XYX and OE+IOE. Our ongoing work is  developing a theoretical formulation to characterize the coverage  and fault tolerance capabilities of various NoC routing schemes.   "
2011,Enabling quality-of-service in nanophotonic network-on-chip.,"With the recent development in silicon photonics, researchers have developed optical network-on-chip (NoC) architectures that achieve both low latency and low power, which are beneficial for future large scale chip-multiprocessors (CMPs). However, none of the existing optical NoC architectures has quality-of-service (QoS) support, which is a desired feature of an efficient interconnection network. QoS support provides contending flows with differentiated bandwidths according to their priorities (or weights), which is crucial to account for application-specific communication patterns and provides bandwidth guarantees for real-time applications. In this paper, we propose a quality-of-service framework for optical network-on-chip based on frame-based arbitration. We show that the proposed approach achieves excellent differentiated bandwidth allocation with only simple hardware additions and low performance overheads. To the best of our knowledge, this is the first work that provides QoS support for optical network-on-chip.","4B-2 Enabling Quality-of-Service in Nanophotonic Network-on-Chip Jin Ouyang∗ and Yuan Xie Computer Science and Engineering, the Pennsylvania State University jouyang@cse.psu.edu ∗ Abstract—With the recent development in silicon photonics, researchers have developed optical network-on-chip (NoC) architectures that achieve both low latency and low power, which are beneﬁcial for future large scale chip-multiprocessors (CMPs). However, none of the existing optical NoC architectures has quality-of-service (QoS) support, which is a desired feature of an efﬁcient interconnection network. QoS support provides contending ﬂows with differentiated bandwidths according to their priorities (or weights), which is crucial to account for application-speciﬁc communication patterns and provides bandwidth guarantees for real-time applications. In this paper, we propose a quality-of-service framework for optical network-on-chip based on frame-based arbitration. We show that the proposed approach achieves excellent differentiated bandwidth allocation with only simple hardware additions and low performance overheads. To the best of our knowledge, this is the ﬁrst work that provides QoS support for optical network-on-chip. I . IN TRODUC T ION The diminishing return from instruction-level parallelism (ILP) drives the shift to many-core processors that exploit task-level parallelism (TLP). However, with the increasing number of cores, the burgeoning on-chip bandwidth requirement is becoming difﬁcult to satisfy. To address this problem, researchers have developed various on-chip interconnection networks (or network-on-chip, NoC). Nevertheless, existing analyses show that interconnects account for 30%–50% of total chip power [1], [2], which set researchers on the quest for power-efﬁcient on-chip interconnects. A. Optical Network-on-Chip The emerging nanophotonic technology enables on-chip optical interconnects that are faster and less power-consuming than electrical wires [3]. Therefore it has been leveraged to build various on-chip networks. Kirman et al. [4] propose to use optical components to build on-chip buses, which however has limited scalability when the network size increases. A major branch of optical network researches are focused on direct network topologies, such as meshes and tori [5]–[7]. These researches migrate the topologies widely used in electrical networks to optical networks. A common feature of these networks is that the optical network is overlaid over an electrical network with the same topology. The optical network uses circuit-switching to avoid intermediate buffering. Circuit setup is done by sending set-up packets in the packet-switching electrical network. Another major branch of researches are focused on token-ring based networks such as Corona [8] and its extension [9], Fireﬂy [10] and MPNoC [11]. All these networks implement alloptical arbitration and ﬂow control mechanisms to exploit the full strength of nanophotonic technology. While direct network topologies win popularity in electrical networks, optical direct networks have several severe problems. First, the circuit set-up latency is long since set-up packets are delivered in the electrical network. To amortize this overhead and achieve good utilization, the length of data packets needs to be over 2KB [5]. However, in CMPs, the majority of trafﬁcs are coherency data which are typically 10s of bytes long but require very short delivery latency. Second, the hop-by-hop electrical network consumes signiﬁcant power, which offsets the power efﬁciency of the optical network. Third, direct networks inevitably introduce large number of waveguide crossings which severely affects the integrity of optical signal [12]. In contrast, token-ring based optical networks do not have overheads of a second electrical network, and there are few or no waveguide crossings even for a large scale network. Therefore token-ring based optical networks are likely to outperform direct optical networks in future many-core CMPs. However, token-ring based networks suffer from severe fairness issues since aggressive sources can easily starve other sources on the same ring. B. Quality-of-Service Another important aspect of on-chip networks is allocating bandwidths to contending ﬂows with different bandwidth requirements. Quality-of-service (QoS) encapsulates mechanisms that service contending ﬂows according to their respective importance and requirements. This is important to account for application-speciﬁc communication patterns and improve system throughput. It is also critical to provide bandwidth guarantees to real-time applications. A number of researchers have proposed QoS-enabled NoC architectures for electrical network [13]–[16]. Frame-based arbitration is used in Lee et al.’s [15] and Grot et al.’s [16] work to achieve differentiated bandwidth allocation with a simple mechanism that incurrs low overheads. However, according to our knowledge, there is no existing work to provide QoS support in nanophotonic NoCs. In this paper, we propose a QoS-enabled optical network-on-chip that uses frame-based arbitration to provide differentiated bandwidth allocation. Due to the simplicity of proposed architectural innovations, the QoS-enabled optical NoC architecture incurs low hardware and performance overheads compared to a baseline optical NoC, while achieving excellent fairness in bandwidth allocation. Due to its low overheads, we believe the proposed QoS-enabled architecture is suitable to be implemented in future nanophotonic network-on-chips. I I . NANO PHOTON IC IN T ERCONN EC T COM PON EN T S A nanophotonic interconnect consists of a laser source (typically located off-chip), waveguides carrying light injected by the laser source, and micro-rings to modulate and detect optical signals. A conceptual view of a nanophotonic link is shown in Figure 1. With densewavelength-division-multiplexing (DWDM), up to 128 wavelengths can be generated and carried by the waveguides [3], [11], which increases the bandwidth density to over 320Gb/s/um. Micro-rings can be electrically tuned into resonance (the “on” state) and remove light from waveguides; or out of resonance (the “off ” state) and let light pass by unaffected. This mechanism is leveraged to modulate light into on-off signals. Doping Ge in a micro-ring turns it into a optical detector. When the doped micro-ring is turned on, it removes light from the waveguide and converts optical signals to electrical ones. Detecting is destructive which means if a detector is turned on then 978-1-4244-7516-2/11/$26.00 ©2011 IEEE 351 4B-2 Detectors Laser light Laser source Waveguides Splitter Detectors On (lit) On (not lit) Off Modulators Modulator Detector Fig. 1: A conceptual nanophotonic link, which consists of a laser source, waveguides, and micro-rings as modulators or detectors. downstream detectors will not be able to detect light. A splitter is used to direct a fraction of light power to another waveguide without affecting modulated signals. It is needed to implement broadcast in nanophotonic links. The pitches of nanophotonic components are small, on the order of 5–10um. I I I . O P T ICA L NOC ARCH I T EC TUR E In this section, we ﬁrst describe the baseline optical NoC architecture without QoS support, followed by our proposed QoS enhancements to the baseline architecture. A. Baseline Architecture Our baseline architecture is derived from Corona [8], [9], which we consider to be more promising than other alternatives for CMPs, as discussed in Section I. MWSR Token Rings. The on-chip network in Corona consists of multiple token rings, each of which is a Multiple Write, Single Read (MWSR) ring . On a MWSR ring, there is a single destination, and multiple sources that send data to the destination. Light ﬂows unidirectionally in the ring, passing each source and ﬁnally terminated by the destination. The sources and the destination modulate and senses the light with micro-rings in the same way as described in Section II. Figure 2a shows a single MWSR ring with three sources (P1, P2, P3) and one destination (P0). For a connected n-node network, n MWSR rings are needed. Figure 2b shows an example of a connected 4-node network. In Corona’s terminology, the destination node terminating a MWSR ring is called the home node of that ring. For example, P0 is the home node of the MWSR ring in Figure 2a. Arbitration and Flow Control. Arbitration is needed to avoid data collision on the MWSR rings. A token-based arbitration mechanism called token slot is proposed in [9] that achieves all-optical arbitration and up to 100% bandwidth utilization. For a single MWSR ring, the home node emits a one-bit token at every clock cycle. The requesters with data to send try to seize the tokens. Capturing a token grants the requester with the right to send one phit of data (1 phit=1 ﬂit in Corona). As long as the delay between capturing a token and sending the data is a constant, the data sent from different requesters will not collide. To do ﬂow control, the home node can simply stop emitting tokens when there are no sufﬁcient buffers considering the round-trip latency on the ring. Overall Architecture. Corona assumes a CMP with 256 cores aggregated into 64 clusters. Each cluster contains 4 cores and one optical router. 64 MWSR rings form a wide optical waveguide bundle that visits every cluster. The approximate ﬂoorplan is shown in Figure 3. Corona uses DWDM with 64 wavelengths per waveguide. With 64 wavelengths, a single waveguide is used to carry arbitration tokens of all MWSR rings. The length of the rings is estimated to P2 O/E 1 P O/E O/E P 3 1 P E/O P0 (a) P 3 P2 P0 (b) Fig. 2: (a) A single MWSR ring. Black boxes refer to O/E and E/O converters. P0-P4 are processors that send and receive signals. (b) A connected 4-node network with 4 MWSR rings. For clarity, O/E and E/O converters are omitted. Laser source Cluster (0, 0) Core Core s t & c e s n e n h o c c a e r C t n I Core Core Router Waveguides 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 Fig. 3: Floorplan of the 256-core CMP interconnected by optical tokenrings (Corona [8]). be 160mm [8], [9], [11], leading to an 8-cycle round-trip latency with a 5GHz clock. In addition, Corona uses virtual output queues (VOQs) [9], which means each source queue is decomposed into multiple virtual queues. Each virtual queue is dedicated to buffering ﬂits for a different destination. VOQs prevents ﬂits destined for different nodes from blocking each other and improves performance. It also allows us to provide QoS support with frame-based arbitration as discussed in the next subsection. B. Optical NoC with QoS Support With token slot arbitration, Corona suffers from a severe fairness issue: since the tokens ﬂow unidirectionally, upstream requesters have absolutely higher priority than downstream requesters in seizing tokens; in the worst case, one requester can starve all other requesters on the same MWSR ring (e.g., P1 may starve P2 and P3 in Figure 2a). In the original work, the authors proposed fair token slot to address this problem. While this approach tries to provide equal bandwidths to contending requesters, it does not provide bandwidth differentiation and is ignorant of weights of different requesters. Hence, fair token ... ... ... P1 P2 P3 P0 (home) X U M ... P1 P2 P3 ... ... ... P0 (home) X U M ... Data flit Frame 0 Frame 1 Frame 2 (a) Abstract model for a 4-node MWSR ring (b) Frame-based arbitration for a 4node MWSR ring Fig. 4: Bandwidth allocation models. 352     4B-2 F-S Ring Comp. Ring F-S Ring Comp. Ring F-S Ring Comp. Ring Home node Home  node Off On (not lit) On (lit) Home node Modulator Detector (a) One of the sources still has ﬂits of the head frame and diverts light from the completion ring (b) The source ﬁnishes sending head frame ﬂits and turns off its microring. The home node detects light from the completion ring (c) The home node then broadcast one-bit frame-switching signal to trigger frame-switching. Fig. 5: All-optical frame-switching mechanism. The inner ring is the completion ring while the outer is the frame-switching ring. slot can hardly be considered as providing QoS support. In order to develop a QoS mechanism for optical NoC, we ﬁrst derive an abstract model that is focused on bandwidth allocation in token-based MWSR rings. Figure 4a shows this model for the single 4-node MWSR ring in Figure 2a. Each source is represented solely by a source queue, and the destination is represented by a destination queue. To study bandwidth allocation, we ignore all processing and propagation latencies for the moment and view the role of the shared MWSR ring as a multiplexer which picks a ﬂit at each cycle from one source queue and pushes it to the destination queue. For example, when all sources are backlogged, with the default token slot mechanism the multiplexer always picks ﬂits from the requester with the highest priority (P1 in this example). On the other hand, with fair token slot it picks ﬂits from source queues in a roundrobin order, achieving equal bandwidth allocation. Note that since VOQs are used, each MWSR ring works independently. This allows us to study the model of a single ring and the conclusion about bandwidth allocation can be readily applied to multiple rings. To enable QoS in optical NoC, we exploit frame-based arbitration [15], [16] to provide differentiated bandwidth allocation according to the weights of requesters. We ﬁrst describe the principles of frame-based arbitration, followed by its application to optical tokenrings. 1) Principles of Frame-Based Arbitration: In frame-based arbitration, a frame is a batch of ﬂits that is delivered in entirety. The frame size (F ) is deﬁned as the maximum number of ﬂits a frame can contain. A share RP i from a frame is allocated to each source P i RP i ≤ F ). Multiple frames are allowed to exist simultaneously ( in a network. For clarity, we associate a frame number (FN ) with each frame, starting from frame 0. When the network is initially powered up, all queues are empty and no frame contains any ﬂits. When a source, say P i, pushes its ﬁrst ﬂit into the source queue, it marks it as belonging to frame 0. Using the same terminology with [15], we call this action injecting a ﬂit into frame 0. Further incoming ﬂits of P i are also injected into frame 0 until the total number of ﬂits injected into frame 0 reaches RP i . After that, P i updates its injection frame to be frame 1 and ﬁlls it with further incoming ﬂits, until the total number of injected ﬂits reaches RP i again. This process is repeated forever when the network is operating. That is, each source node injects its share into frames with increasing frame number. Figure 4b shows an example for the 4node MWSR ring, where ﬂits ﬁlled with different patterns belong to different frames. In this example, RP 1=1, RP 2=1, RP 3=2, and F =4. Frame-based arbitration requires ﬂits belonging to a same frame to be delivered together. In addition, frames are delivered according (cid:2) to their numbers. For example, in Figure 4b, ﬂits belonging to the ﬁrst frame (black boxes) are delivered before any other ﬂits. In general, ﬂits belonging to frame (i + 1) cannot be delivered until frame i is drained. This method essentially introduces a strict ordering between frames, but not among ﬂits within a same frame. We call the frame currently being drained (the current oldest frame) the head frame, and ﬂits of the head frame “ready” ﬂits, while all other ﬂits “unready” ﬂits. 2) Implement Frame-Based Arbitration in Optical NoC: In practice, implementing frame-based arbitration only requires each node to track the status of one frame—the head frame. A source node needs to be throttled if its ﬂits belonging to the head frame have all been delivered, but there are ﬂits belonging to the head frame left in some other source queues (that is, the head frame is not drained). Only when the current head frame is drained, source nodes can generate a new head frame by admitting ﬂits to the new head frame in compliance with RP i s. We call this action frame-switching, which is the key to implementing frame-based arbitration. We leverage optical interconnect to provide an efﬁcient and all-optical frame-switching mechanism, which is suitable to be used with our baseline architecture. We propose to introduce two additional rings: • The Completion Ring is used to gather local status of each source node on a MWSR ring. On the completion ring, each source node has a micro-ring which is tuned into resonance when it still has ﬂits of the head frame. The home node injects a continuous light into the ring that passes each source node. Therefore the completion ring essentially implements a “NOR” function. When there is at least one ﬂit of the head frame remaining in the network, the source node owning that ﬂit will remove light from the completion ring (Figure 5a). The home node has a detector at the end of the ring, and only detects light when the current head frame is drained (Figure 5b). • The Frame-Switching Ring is used to broadcast the global status to each source node on the MWSR ring. When the home node detects light on the completion ring, it will send one-bit frame-switching signal on the frame-switching ring, which reaches the detectors of all source nodes on this ring (Figure 5c). Receiving a frame-switching signal triggers the source nodes to perform frame-switching the operation and tune their micro-rings on the completion ring into resonance again. Note that it is possible to use a single broadcast ring to realize the functions of both the completion ring and the frame-switching ring. However, this means this single broadcast ring needs to pass each node twice and its length is doubled. According to our analysis, to account for exponential signal attenuation, this long broadcast 353 5: 6: 7: 8: 9: 10: 13: 14: 15: 16: 17: 18: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 4B-2 TABLE I: Summary of notations Var. Name F RP i CP i TCHN TP ROC Range 1 .. ∞ 1 .. F 1 .. RP i 1 .. ∞ 1 .. ∞ W L QP i 0 .. (TCHN 1 .. ∞ +TP ROC ) 0 .. L Description frame size in number of ﬂits node P i’s share in a frame node P i’s remaining share in the current head frame round-trip latency (8 cycles for Corona) node’s processing delay of frame -switching wait cycle counter for the home node idle cycle threshold to trigger early frame-switching idle cycle counter for node P i Receive frame-switching signal/ Refresh CPi, admit existing flits as  “ready” flits, turn on the micro-ring 1. Incoming flits and CPi>0/ Mark flits as “ready” flits,  decrement CPi 2. Have “ready” flits/ Request to send “ready” flits 1. Incoming flits and CPi>0/ Mark flits as “ready” flits,  decrement CPi 2. Have “ready” flits/ Request to send “ready” flits Spin Busy (CPi is 0 and no “ready” flits) || (no  “ready” flits for L cycles) / Turn off the micro-ring (a) Source node behavior Detect light on the completion ring/ Broadcast one-bit frame-switching  signal Wait for (TCHN + TPROC) Eject flits, send tokens if  cycles, eject flits, send  buffer available tokens if buffer available Spin Wait After (TCHN + TPROC) cycles (b) Home node behavior Fig. 6: State machines for source and home nodes. ring requires a laser power of 32 Watts, while the total laser power needed by both completion and frame-switching rings is only 0.85 Watts. Therefore we choose the double-ring architecture with a small hardware overhead. Node Behavior. The source node and the home node are both controlled by state machines, which are described by Figure 6 and Procedure 1 and 2. The notations used in the ﬁgure and pseudo-codes are summarized in Table I. Note that the 3 loops in Procedure 1 runs independently and concurrently. The source node has two states: spin and busy (Figure 6a). Initially each source node is put into the busy state, during which the microring on the completion ring is turned on. Upon the generation of a new ﬂit, it is marked as a “ready” ﬂit if there are still available share in the current head frame (CP i > 0). The source node goes to the spin state when its share is used up (CP i = 0) and all “ready” ﬂits have been delivered. In the spin state, the source node is throttled and turns off its micro-ring on the completion ring. The transition from the spin state to the busy state is triggered by the frame-switching signal. During this transition, the source node refreshes its share (CP i ← RP i ), admits existing ﬂits to the head frame (line 31–34 of Procedure 1), and turns on its micro-ring on the completion ring. The home node also has two states: spin and wait. It is initialized to be in the spin state. Upon detecting light on the completion ring, it emits the frame-switching signal and goes to the wait state. In the wait Procedure 1 Behavior of source node P i. 1: Initialize: CP i ← RP i , QP i ← 0 2: Initialize: Initial state ← busy 3: Initialize: Turn on the micro-ring on the completion ring // Flit generation loop 4: loop AT EACH FL I T G EN ERAT ION EV EN T: Push the ﬂit into the source queue if CP i > 0 then Mark the ﬂit as “ready” CP i ← CP i − 1 // Flit delivery loop if there exists a “ready” ﬂit then Request to send the “ready” ﬂit QP i ← 0 QP i ← QP i + 1 else end if 11: end loop 12: loop end if 19: end loop 20: loop // Node state manipulation loop WH I L E IN TH E busy S TATE: if (CP i = 0 and no “ready” ﬂit exists) or QP i = L then Go to the spin state Turn off the micro-ring on the completion ring end if WH I L E IN TH E spin S TAT E: if detect the frame-switching signal then Go to the busy state Turn on the micro-ring on the completion ring CP i ← RP i repeat Mark the ﬁrst existing “unready” ﬂit as “ready” CP i ← CP i − 1 until CP i = 0 or all existing ﬂits are “ready” end if 36: end loop Procedure 2 Behavior of the home node. 1: Initialize: Initial state ← spin // Node state manipulation loop 2: loop WH I L E IN TH E spin STATE: if detect light on the completion ring then Go to the wait state Emit frame-switching signal W ← TCHN + TP ROC end if 3: 4: 5: 6: 7: 8: 10: W ← W − 1 9: 11: 12: 13: end if 14: end loop if W = 0 then WH I L E IN TH E wait STATE: Go to the spin state state, the home node waits for (TCHN + TP ROC ) cycles—the time for all source nodes to receive frame-switching signal and complete frame-switching. This is necessary to prevent the home node from sending multiple frame-switching signals during the on-going frameswitching process. Early Frame-Switching. While providing bandwidth differentiation, frame-based arbitration has the risk to underutilize the network capacity. Consider the extreme case that one source node does not generate ﬂits at all. This “silent” node blocks frame-switching and other nodes are throttled even if they have data to send. In this extreme case network utilization is reduced to 0. Similarly, network utilization is low when low-injection-rate nodes prevent high-injection-rate nodes from exploiting unused bandwidth. To address this problem we propose to do early frame-switching: 354 frame-switching is triggered even when some nodes have not used up their shares, based on the prediction that those nodes are unlikely to generate ﬂits for a prolonged period. In this work we use the length of current idle period (QP i ) as a predictor. When the current idle period is longer than some threshold (L), a source node also goes from the busy state to the spin state. The threshold L can be determined statically or adaptively according to the network status. Currently we statically set L = 2, an empirical value found in experiment to provide good utilization and bandwidth allocation. Adaptive methods to determine L is the subject of our future work. Note that with this modiﬁcation, a source node may go to the spin state even if its share is not used up; therefore it may still admit and send “ready” ﬂits even in the spin state. Multiple MWSR Rings. The discussion up to this point assumes a single MWSR ring. Since with VOQs ﬂits destined for different nodes will not affect each other, the above mechanisms can be straightforwardly extended to multiple MWSR rings. Each MWSR ring simply implements the aforementioned frame-based arbitration independently. With 64 wavelengths per waveguide, two waveguides can implement all completion and frame-switching rings in a 64-node network. A source node P i can have different RP i s on different rings. If we denote node P i’s share on ring H i as RH i P i ≤ F , ∀P i on ring H i. P i , it is still required that Bandwidth Allocation. With frame-based arbitration, the bandwidth allocated to a source node P i on ring H i is RH i P i /F of the maximum bandwidth of ring H i. RH i (cid:2) IV. EX P ER IM EN T We evaluate and compare the original Corona and our enhanced QoS-enabled network using an cycle-accurate NoC simulator. Each simulation is run until results are stabilized. We model a 64-node token-ring based network as shown in Figure 3. The baseline conﬁguration is exactly the same as Corona, while for the QoS-enabled network enhancements discussed in Section III-B are added. The default frame size (F ) is set to 128 ﬂits. Synthetic trafﬁc patterns are used to exercise both networks. In addition, considering the features of the baseline architecture, the synthetic trafﬁcs are divided into two classes: • For uniform and hotspot trafﬁcs, multiple sources may send data to one destination. In the token-ring based network, these sources will • The other class of trafﬁcs are those based-on permutation patterns: contend for bandwidth of a single or multiple MWSR rings. transpose, bit-reversal, perfect-shufﬂe, complement, etc. In all these trafﬁcs, data are sent only between pairs of nodes; and a given destination only receives data from one source. For the baseline architecture, this means on each MWSR ring, there is only one active source, and no bandwidth contention exists. A. Fairness and Performance We use the hotspot trafﬁc to evaluate the fairnesses of the baseline and the QoS-enabled architectures. In this experiment, we pick node (0, 0) as the hotspot, and each other node sends data to this node at a rate of 0.05 ﬂits/cycle. The resulted aggregate offered load exceeds the network capacity. The result for the baseline architecture is shown in Figure 7a. As can be seen, in this case the upstream nodes exhaust all available bandwidth, while downstream nodes only drips trafﬁcs. Figure 7b shows the result for the QoS-enabled network with equal allocation, and Figure 7c and 7d show the results with differentiated allocation. In Figure 7c the 64-node network is divided into 4 quadrants and differentiated services are provided to different quadrants. In Figure 7d, the network is partitioned in to 2×2 node groups and bandwidth is allocated in a checkerboard pattern. We see that in all cases, the accepted throughput of each source is compliant with allocation. 4B-2 Accepted throughput (flits/node/cycle) 0.06 0.05 0.04 0.03 0.03 0.02 0.01 0 Accepted throughput (flits/node/cycle) 0.025 0 02 0.02 0.015 0.01 0.005 0 0 1 2 3 4 5 6 7 01234567 0 1 2 3 2 3 4 5 6 7 0123 34567 (a) Baseline (b) QoS with Equal Allocation Accepted throughput (flits/node/cycle) 0.025 0.02 0.02 0.015 0.01 0.005 0 0 1 2 3 4 4 5 6 7 012345 4567 Accepted throughput (flits/node/cycle) 0.025 0.02 0 015 0.015 0.01 0.005 0 0 1 2 3 4 5 6 7 6 7 01234567 67 (c) Diff. allocation case 1 (d) Diff. allocation case 2 Fig. 7: Accepted throughput for (a) the baseline architecture and for (b– d) the QoS-enabled architecture with equal and differentiated allocations. The red arrow indicates the hotspot node. We next examine the performance with synthetic trafﬁcs. For uniform and hotspot, we assign (cid:5)F /64(cid:6) of a frame to each source. For the second class of trafﬁcs, we allocate the whole frame to each source since there is no contention. The results are shown in Figure 8, where offered load and throughput are normalized to network capacity. Due to space limit, we only show results for transpose from trafﬁcs in the second class, since their performances share identical traits. As can be seen, the ﬂit latency of QoS-enabled network is almost identical to the baseline architecture. However, beyond the saturation point the ﬂit latency of QoS-enabled network rises more rapidly, especially in hotspot and transpose. The maximum accepted throughputs of QoS-enabled network are 17% and 7% lower than the baseline for uniform and hotspot respectively. This is due to the overheads of frame-switching latency and idle cycles of source nodes. On the other hand, the throughput overhead of transpose is negligible. This is because the share of each source node is a whole frame, and those extra latencies are amortized by the large share (128 ﬂits). A large frame size can potentially improve throughputs by amortizing overheads of frame-switching. This is reﬂected by Figure 9a, where the throughputs of the QoS-enabled network improve with increasing frame size. The improvement saturates beyond the frame size of 512 ﬂits. With a frame size of 512 ﬂits, the throughput reductions of uniform and hotspot are only 10% and 2% respectively. B. Energy and Hardware Overheads Energy consumption of an optical network consists of both static and dynamic components. The static component includes external laser power and ring heating power. The dynamic power is expended by ring modulation and electrical back-end components including pre-driver, analog receiver, sampling circuits, and ampliﬁer. We use data from [11] and [3] for 22nm node to calculate the overall energy consumption. The results for uniform trafﬁc with different offered loads are plotted in Figure 9b. First, we observe that for both architectures the static energy dominates overall energy when the offered load is low. With offered load increasing, the contribution of static energy is amortized by the increased data rate. On the other hand, the 355 60 80 100 NoͲQoS QoS t t n e y c ( e c y c l ) s 0.6 0.8 1 NoͲQoS QoS u p h g g u o t 0 20 40 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 11 . 1 1 . 2 A e v r e g a l  a Offeredload 0 0.2 0.4 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 11 . 1 1 . 2 h T r Offeredload (a) Uniform 40 60 80 100 NoͲQoS QoS a t n e y c ( e c y c l ) s 0.6 0.8 1 1.2 NoͲQoS QoS u p h g u o o t 0 20 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 11 . 1 1 . 2 A e v r e g a l  Offeredload 0 0.2 0.4 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 11 . 1 1 . 2 h T r Offeredload (b) Hotspot 40 60 80 100 NoͲQoS QoS e g g a l  a t n e y c ( e c y c l ) s 0.4 0.6 0.8 1 1.2 NoͲQoS QoS h T r u p h g u o t 0 20 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 11 . 1 1 . 2 A e v r Offeredload 0 0.2 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 11 . 1 1 . 2 Offeredload (c) Transpose Fig. 8: Average ﬂit latency (left), achieved throughput (right) for (a) uniform , (b) hotspot, and (c) transpose. 0 6 0.6 0.8 1 QoS/128 QoS/256 QoS/384 Normalized Throughput 0 0.2 0.4 Uniform Hotspot Transpose QoS/512 QoS/640 NoͲQoS (a) 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2 0 e n E r y g (  p J / b ) ElectricalBackend RingModulation RingHeating LaserSource N o Ͳ Q S o Q S o N o Ͳ Q S o Q S o N o Ͳ Q S o Q S o N o Ͳ Q S o Q S o (cid:1)(cid:1081)(cid:872)(cid:1083)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1081)(cid:872)(cid:1085)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1081)(cid:872)(cid:1087)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1081)(cid:872)(cid:1089) (cid:920)(cid:972)(cid:972)(cid:971)(cid:984)(cid:971)(cid:970)(cid:1)(cid:1) (cid:978)(cid:981)(cid:967)(cid:970) (b) Fig. 9: (a) Maximum throughputs of the QoS-enabled network normalized to the baseline, with different frame sizes (128–640 ﬂits). (b) Energy decomposition with different offered loads. dynamic energy is almost constant for the baseline architecture. Second, we observe that QoS-enabled network incurs little static energy overhead since only few additional optical components are used. On the other hand, the dynamic energy overhead is signiﬁcant, due to the activities associated with frame-switching. As expected, the dynamic energy overhead is more prominent with low offered loads when source nodes become idle more frequently; and it is much lower with high offered loads. Overall, the total energy overhead ranges from 32% at 0.2 load rate to 8% at 0.8 load rate. It is possible to adaptively adjust L to control the frame-switching rate, which can reduce the overhead at low loads. This is the subject of our future work. The optical component budget is shown in Table II, where the TABLE II: Summary of notations Photonic Subsystem Waveguides Micro-rings Data MWSR rings Arbitration ring 256 1 1024K 4K Comp. ring Frame-switch. ring 1 1 4K 4K Total 259 0.8% 1036K 0.8% QoS overhead overheads introduced by the QoS enhancements are in bold. Framebased arbitration only introduces 0.8% overheads for both waveguides and micro-rings. Due to the small sizes of optical components, the resulted area overhead is also likely to be small. V. CONC LU S ION Emerging nanophotonic technology has the potential to boost performance and reduce power of future many-core CMPs. In this work we propose a nanophotonic network-on-chip architecture with quality-of-service support, which to our best knowledge is the ﬁrst work on this topic. Our frame-based QoS enhancements achieve excellent bandwidth allocation, while only introducing simple extra hardware and small performance overheads. Based on this initial work, we are currently working on adaptively adjusting the idle cycle threshold (L) to further reduce energy overheads. ACKNOW L EDG EM EN T We are thankful to NSF 0905365, 0903432, 0702617, 0643902 and SRC grants for supporting this work, and also to Guangyu Sun for his valuable comments on the paper. "
2011,OPAL - A multi-layer hybrid photonic NoC for 3D ICs.,"Three-dimensional integrated circuits (3D ICs) offer a significant opportunity to enhance the performance of emerging chip multiprocessors (CMPs) using high density stacked device integration and shorter through silicon via (TSV) interconnects that can alleviate some of the problems associated with interconnect scaling. In this paper we propose and explore a novel multi-layer hybrid photonic NoC fabric (OPAL) for 3D ICs. Our proposed hybrid photonic 3D NoC combines low cost photonic rings on multiple photonic layers with a 3D mesh NoC in active layers to significantly reduce on-chip communication power dissipation and packet latency. OPAL also supports dynamic reconfiguration to adapt to changing runtime traffic requirements, and uncover further opportunities for reduction in power dissipation. Our experimental results and comparisons with traditional 2D NoCs, 3D NoCs, and previously proposed hybrid photonic NoCs (photonic Torus, Corona, Firefly) indicate a strong motivation for considering OPAL for future 3D ICs as it can provide orders of magnitude reduction in power dissipation and packet latencies.","1RWLFH7KLVSDSHUZDVQRWSUHVHQWHGE\RQHRIWKHDXWKRUVLQ$63'$& 4B-1 OPAL: A Multi-Layer Hybrid Photonic NoC for 3D ICs  Sudeep Pasricha, Shirish Bahirat  Colorado State University, Fort Collins, CO  {sudeep, shirish.bahirat}@colostate.edu  Abstract - Three-dimensional integrated circuits (3D ICs) offer a  significant opportunity to enhance the performance of emerging  chip multiprocessors (CMPs) using high density stacked device  integration and shorter through silicon via (TSV) interconnects that  can alleviate some of the problems associated with interconnect  scaling. In this paper we propose and explore a novel multi-layer  hybrid photonic NoC fabric (OPAL) for 3D ICs. Our proposed  hybrid photonic 3D NoC combines low cost photonic rings on  multiple photonic layers with a 3D mesh NoC in active layers to  significantly reduce on-chip communication power dissipation and  packet latency. OPAL also supports dynamic reconfiguration to  adapt to changing runtime traffic requirements, and uncover  further opportunities for reduction in power dissipation. Our  experimental results and comparisons with traditional 2D NoCs, 3D  NoCs, and previously proposed hybrid photonic NoCs (photonic  Torus, Corona, Firefly) indicate a strong motivation for considering  OPAL for future 3D ICs as it can provide orders of magnitude  reduction in power dissipation and packet latencies.  I Introduction   Rapid improvements in CMOS fabrication technology and a steady  rise in application complexity in recent years have led to the emergence  of chip multiprocessors (CMPs) as compact and powerful computing  paradigms. Emerging CMP designs are integrating in the order of a  hundred or more cores on a chip [1]-[3]. To cope with the growing  communication demands of these massively multi-core systems, shared  bus-based communication fabrics are being replaced by networks-on-  chip (NoCs) that offer higher reliability, scalability, and bandwidths  [4][42]. In practice, the large number of network interfaces (NIs),  routers, links, and buffers in NoCs lead to significant power dissipation,  e.g., (cid:1533)30% in the Intel 80-core teraflop chip [2] and (cid:1533)40% in the MIT  RAW chip [3]. Recent studies have suggested that NoC power  dissipation is much higher (by a factor of 10×) than what is needed to  meet peta- and exa-flop performance levels of future CMPs [5]. Thus,  radical new approaches are required to overcome the power and  performance brick walls facing NoCs in the near future [6].  Of the several different disruptive technologies that are being  investigated today, 3D integrated circuits (3D-ICs) with wafer-to-wafer  bonding technology is one of the most promising candidates [7][8][45].  In wafer-to-wafer bonded 3D-ICs, active devices (processors, memories,  peripherals) are placed on multiple active layers and vertical Through  Silicon Vias (TSVs) are used to connect cores across the stacked layers.  Multiple active layers in 3D ICs can enable increased integration of  cores within the same area footprint as traditional single layer 2D ICs.  In addition, long global interconnects between cores can be replaced by  shorter inter-layer TSVs, improving performance and reducing on-chip  power dissipation. Recent 3D IC test chips from Intel [2], IBM [7], and  Tezzaron [8] have confirmed the benefits of 3D IC technology.    While 3D ICs are promising, the fundamental power, delay, and  noise susceptibility limitations of traditional copper (Cu) interconnects  will still limit their achievable improvements. To overcome these  limitations, novel interconnect materials need to be explored. Photonic  interconnects [9] are an extremely promising emerging solution that can  replace Cu interconnects and help overcome their latency and power  bottlenecks. Photonic interconnect technology can transfer data with  much more energy efficiency than Cu interconnects especially over  long distances on chip. In addition, the ability of photonic waveguides  to carry many information channels simultaneously using wavelength  division multiplexing (WDM) increases interconnect bandwidth density  significantly, eliminating the need for a large number of wires to  achieve bandwidth goals. Photonic interconnects are becoming standard  in data centers, and chip-to-chip photonic links have been demonstrated  [10]. This trend will naturally bring photonic interconnects into the  on-chip stack, particularly as a means to enable high bandwidth and low  power data transfers between hundreds of cores in future CMPs. Recent  advances in the field of silicon photonics have enabled highly integrated  photonic interconnect-based components in CMOS-based ICs [11]-[14].  While several research efforts have individually explored the benefits  of photonic interconnects and 3D IC technology, using 3D ICs as a  platform for the realization of hybrid electro-photonic NoCs has not  been addressed so far. The question arises: can hybrid photonic NoCs  be viable interconnect fabrics for future 3D ICs? In this paper, we  attempt to answer this question, and propose OPAL, a novel hybrid 3D  NoC architecture that combines low cost photonic rings on multiple  photonic layers with 3D mesh NoC fabrics in active layers. The  photonic paths offload global communication from the electrical  network, improving packet latency and reducing communication power  dissipation. In addition, OPAL supports dynamic reconfiguration of the  electrical and photonic networks. This enables runtime adaptation to  changing traffic volumes, which allows network resources to be  optimized for even lower power dissipation. Our experimental results  and comparisons with traditional 2D NoCs, 3D NoCs, and previously  proposed hybrid photonic NoCs (photonic torus, Corona, Firefly)  indicate a strong motivation for considering OPAL for future 3D ICs as  it can provide several orders of magnitude reduction in power  dissipation and average packet latencies.  II. Related Work   Over the last several years, there has been a growing interest in 3D  ICs as a means to alleviate the interconnect bottleneck currently facing  2D ICs. A key challenge with 3D ICs is their high thermal density due  to multiple cores being stacked together, that can adversely impact chip  performance and reliability. Therefore several researchers have  proposed thermal-aware floorplanning techniques for 3D ICs [15]-[17].  A few researchers have explored interconnect architectures for 3D ICs  such as 3D mesh and stacked mesh NoC topologies [18] and a hybrid  bus-NoC topology [19]. Some recent work has looked at decomposing  cores (processors [20], NoC routers [21], and on-chip cache [22]) into  the third dimension which allows reducing wire latency at the intra-core  level, as opposed to the inter-core level. Circuit level models for TSVs  were presented in [23].  Recent advances in silicon photonics have led to the development of  fabrication technologies to stack optical devices in multiple layers [24]  in 3D ICs. Literature abounds in comparisons of the physical properties  of on-chip electrical and photonic interconnects [25]-[27] at the circuit  level, highlighting the signal speed and power benefits of photonic  interconnects. Other work from industry and academia has been  focusing on photonic device fabrication, e.g., gigascale modulators [11],  photodetectors [12], switches [13], couplers, buffers, waveguides and  on-chip wave division multiplexing (WDM) devices [14]. A few recent  works have explored the system- level impact of using hybrid  electro-photonic  interconnect architectures and proposed hybrid  Cu-photonic crossbars (Corona [28], Firefly [29]), Clos networks [30],  fat-trees [31] and torii [32][33]. However these architectures possess  high area and fabrication complexity (e.g. more than a million  resonators in Corona [28]). Our recent work [34] explored a simpler  WDM-enabled hybrid Cu-photonic architecture with a parallel photonic  ring waveguide interfaced to an electrical 2D mesh NoC. While this  architecture is more area and cost effective than other hybrid  electro-photonic topologies (e.g., ~15× lower photonic layer overhead  vs. hybrid photonic torus [32]), it is not scalable for large CMP designs.   In this paper, we propose a hybrid 3D multi-ring/mesh topology to  improve performance scalability for emerging CMPs with hundreds of  cores. Runtime reconfiguration of the electrical and photonic networks  is explored with the goal of significantly reducing communication  power dissipation. Several works [35]-[37] have explored runtime  electrical NoC adaptation schemes including DVS/DFS, dynamic  978-1-4244-7516-2/11/$26.00 ©2011 IEEE 345             4B-1 routing schemes, and dynamic arbitration to adapt to changing runtime  traffic needs, and improve performance and power dissipation. However,  previous work has not explored runtime reconfiguration in 3D hybrid  electro-photonic NoCs.  III. OPAL Overview   A. Photonic Building Blocks  Fig. 1 shows a high level overview of the primary on-chip photonic  transmission components: a multi-wavelength laser light source,  resonant modulators/filters, photonic waveguide, and photodetector  receivers. Multiple wavelengths of  light from a mode-locked,  multi-wavelength laser [38] enable WDM that allows several data  streams  to coexist  in  the same waveguide,  improving  transfer  bandwidth. In this work, we assume that wavelengths are allocated to  (cid:87)(cid:85)(cid:68)(cid:73)(cid:73)(cid:76)(cid:70)(cid:3) (cid:86)(cid:87)(cid:85)(cid:72)(cid:68)(cid:80)(cid:86)(cid:3) (cid:88)(cid:86)(cid:76)(cid:81)(cid:74)(cid:3) (cid:181)(cid:80)(cid:88)(cid:79)(cid:87)(cid:76)(cid:83)(cid:79)(cid:72)(cid:91)(cid:76)(cid:81)(cid:74)(cid:3) (cid:69)(cid:92)(cid:3) (cid:70)(cid:82)(cid:85)(cid:72)(cid:182)(cid:15)(cid:3) (cid:90)(cid:76)(cid:87)(cid:75)(cid:3) (cid:72)(cid:68)(cid:70)(cid:75)(cid:3) (cid:82)(cid:73)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3) n  interfacing cores having exclusive access to (cid:540)(cid:18)(cid:81) wavelengths, where (cid:540)(cid:3)is  the total number of wavelengths supported. This limits the number of  transmitters, but provides substantial power savings.   Microring resonant modulators [11] convert electrical data signals  into light, which is propagated through a CMOS-compatible silicon  oxide photonic waveguide. The light in the waveguide is eventually  coupled into microring filters at the destination that drop the light on  photodetectors [39], and thereafter the light signal is converted back  into an electrical data signal. Trans-impedance amplifier (TIA) circuits  finally amplify analog electrical signals from the photodetector to  digital voltage levels. It is vital for all microring resonators to be  thermally tuned (using thermal heater elements) to maintain their  resonance under on-die temperature variations [30].   Fig 1: Building blocks of photonic interconnects  To accurately estimate performance and power overhead of on-chip  photonic communication, we characterize  the delay and power  overhead of each of the described photonic components, including the  thermal heaters and the laser. As laser power is determined by the  magnitude of losses in photonic components, we account for losses due  to couplers, resonators, photodetectors, waveguide length and bends,  and non-linearity. Section IV.A summarizes the per-component delay,  power, and loss characterization considered in this work in detail.  B. Motivation for Multiple Photonic Layers in 3D ICs  In general, 2D hybrid electro-photonic NoCs have an active layer  with processor and memory cores interconnected using an electrical  NoC interfaced to a separate silicon photonics layer consisting of  photonic waveguide-based interconnect paths. In 3D ICs, multiple  active layers exist and a hybrid electro-photonic NoC for 3D ICs can  utilize a single photonic layer, or multiple photonic layers.   A single photonic layer has the lowest design complexity, but may  lack scalability. For instance, if a hybrid electro-photonic torus topology  [32] is extended to 3D ICs with many more cores, a single photonic  torus layer will need to be modified by increasing number of  waveguides (and thus resonators, photodetectors etc) to satisfy higher  bandwidth requirements from cores in multiple active layers. Not only  may this not be feasible due to waveguide spacing and layout  constraints, but the ensuing wider waveguide crossing losses will be  prohibitively high, leading to very high laser and photonic component  power dissipation [9]. Using simpler topologies such as a photonic ring  [34] can be beneficial as they do not possess any crossing losses.  However, a single photonic ring does not scale well when the number of  cores is increased. Fig. 2(a) shows the percentage improvement in  energy-delay product for a hybrid ring-mesh NoC (with a photonic ring  interfaced to an electrical mesh NoC) compared to a conventional 2D  346 electrical mesh NoC, with increasing CMP core counts. For each CMP  configuration, results were averaged for various SPLASH-2 benchmark  [40] implementations. It can be clearly seen that with rising core counts,  the benefits of using a single photonic ring become insignificant. This is  primarily because the photonic ring is under-utilized due to a limited  number of uplinks/downlinks and coverage, even though the global  communication requirements are higher.              (a)                          (b)  Fig 2: percentage improvement in (a) energy-delay product for hybrid  photonic ring NoC vs. 2D electrical mesh NoC, with scaling core count, (b)  average latency and power for E2P1, E2P3, E4P1, E4P7 vs. E1P1  One way to improve scalability for the hybrid ring-mesh NoC is to  utilize 3D ICs with multiple photonic layers. For the same number of  cores, a 3D IC has a smaller die area, which can enable improved  coverage for the photonic ring for intra-layer transfers. In addition,  dedicated photonic rings can be used to also enable inter-layer global  transfers. To validate our conjecture, we performed a feasibility study to  determine whether having multiple photonic layers is beneficial in 3D  ICs. Fig 2(b) shows the results of a comparison study for a hybrid  ring-mesh NoC for a 100 core CMP with the following configurations:  (i) two active layers, with 50 cores/layer and one photonic ring layer  (E2P1), (ii) two active layers, with 50 cores/layer and three photonic  ring layers (E2P3; Fig. 3), (iii) four active layers, with 25 cores/layer  and one photonic ring layer (E4P1), and (iv) four active layers, with 25  cores/layer and seven photonic ring layer (E4P7). For configurations  with multiple photonic layers, each layer has a dedicated photonic ring  layer for intra-layer transfers, and another photonic ring layer for  inter-layer transfers. Fig. 2(b) shows the percentage improvement in  average power and average packet latency compared to a 100 core CMP  with a single photonic ring layer and a single active layer with a mesh  NoC (E1P1). A WDM degree of 32 is assumed for all configurations. It  can be seen that 3D IC configurations with single photonic layers (E2P1,  E4P1) provide some improvements over the E1P1 configuration,  primarily due to smaller inter-layer links between cores in separate  layers that replace longer global links in E1P1. However, the photonic  ring was found to be the bottleneck due to high levels of traffic that  caused inter-core data flows to stall. The multiple photonic layer  configurations (E2P3, E4P7) perform significantly better due to a  greater number of photonic paths. We present more comprehensive  experimental results in Section IV. In the following sections, we  describe our multi-layer hybrid photonic NoC architecture in detail.      Fig 3: E2P3 OPAL configuration  C. OPAL System Level Architecture  In this work, we propose OPAL, which is a hybrid electro-photonic 3D  NoC architecture that employs multiple active layers and multiple  photonic layers with photonic ring paths in a stack. The active layers  consist of cores interconnected to each other using a 3D electrical mesh  NoC. The photonic layers consist of ring shaped waveguides. Gateway  interface routers provided the connectivity between the electrical layer                                    and the modulators and photodetectors in the photonic layer. The choice  of a photonic ring topology is motivated by the goal of reducing  fabrication cost and photonic component area overhead, compared to  other topologies such as mesh, torii, crossbars, and fat trees.   Fig 3 shows an example of a two active layer 3D IC modified to create  a hybrid 3D photonic-electric network. This E2P3 OPAL configuration  has two active electrical (E) layers and three photonic (P) layers. Each E  layer has a dedicated P layer with photonic rings for intra layer global  transfers between cores in the same layer. For every two E layers, a  dedicated P layer exists that facilitates inter-layer (e.g. E1 to E2) global  transfers. Vertical TSVs are used for transfers between E1 and E2 in the  electrical 3D mesh NoC, as well as to transfer data between photonic  layers and active layers. Higher complexity OPAL configurations can be  created by reusing this basic E2P3 configuration. For instance, for a four  active layer 3D IC, an E4P7 OPAL configuration is created by stacking  two E2P3 stacks and adding a dedicated P layer for inter E2P3 photonic  communication. Throughout this paper we focus on two and four active  layer 3D ICs when exploring OPAL, although the architecture is  applicable to 3D ICs with a greater number of layers as well.  D. 3D Photonic Region of Influence (3D-PRI)  To balance traffic between the photonic rings and the electrical NoC,  OPAL has a 3D parameterizable photonic region of influence (3D-PRI)  which refers to the number of cores around the gateway interface that can  utilize the photonic path for communication. Changing the PRI sizes can  have a notable impact on communication power, latency, and bandwidth.  For smaller sized systems (e.g., 2 layer, 3×3 cores/layer 3D CMPs),  limiting the number of cores interfacing with each gateway interface to  one may be sufficient to offload a majority of the global communication  from the electrical network. However for more complex systems (e.g., 4  layer, 10×10 cores/layer 3D CMPs) a larger region size may be more  appropriate. Fig 4(a) shows examples of 3D PRIs for two OPAL  configurations (E2P3 and E4P7). The 3D PRI for the E2P3 configuration  has a size 4, which specifies 3D blocks containing 8 cores (2×2×2 (cid:177) i.e., 4  cores/layer in 2 layers) around gateway interfaces that are allowed to use  the photonic waveguide for transfers. For the E4P7 configuration, the 3D  PRI shown has a size 9 and consists of 36 cores (3×3×4).                   (a)                                     (b)  Fig 4: (a) 3D Photonic Region of Influence (3D-PRI) (b) Photonic channels  E. Router Architecture  Data flits in the OPAL network are transferred using wormhole  switching, with flit width = 256 bits. There are broadly two types of  electrical layer routers used in our proposed architecture: (i) electrical  mesh routers that can have up to seven I/O ports (N, S, E, W, up, down,  local core) and facilitate intra- and inter-layer transfers on the 3D  electrical mesh NoC, and (ii) gateway interface routers that have one or  more additional photonic interface ports and are responsible for  sending/receiving flits to/from photonic interconnects in the photonic  layers. As each photonic interface port has access to (cid:540)(cid:18)(cid:81) wavelengths for  transmission (where (cid:540) is WDM degree), we have (cid:540)(cid:18)(cid:81) buffers for sending  data. Although it is theoretically possible to have (n-(cid:20)(cid:12)(cid:17)(cid:540)(cid:18)(cid:81) data flows  received at a gateway interface, we restrict the number of received flows  (and hence receive buffers) to (cid:540)(cid:18)(cid:81) to maintain symmetry and reduce cost.  (cid:36)(cid:79)(cid:79)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:83)(cid:75)(cid:82)(cid:87)(cid:82)(cid:81)(cid:76)(cid:70)(cid:3)(cid:83)(cid:82)(cid:85)(cid:87)(cid:86)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:70)(cid:82)(cid:81)(cid:81)(cid:72)(cid:70)(cid:87)(cid:72)(cid:71)(cid:3)(cid:87)(cid:82)(cid:3)(cid:68)(cid:3)(cid:181)(cid:58)(cid:39)(cid:48)(cid:3)(cid:70)(cid:82)(cid:81)(cid:87)(cid:85)(cid:82)(cid:79)(cid:182)(cid:3)(cid:80)(cid:82)(cid:71)(cid:88)(cid:79)(cid:72)(cid:3)(cid:87)(cid:75)(cid:68)(cid:87)(cid:3) controls wavelength assignment to different traffic flows, to enable  WDM for high bandwidth photonic communication. To reduce the  overhead on router complexity, only a few routers (four in our initial  baseline configuration) are chosen as gateway interface routers in each  active layer. To support flexible 3D-PRI sizes at runtime, each router has  a region validation unit with tables that contain region boundary  347 coordinates. Details of this, along with an overview of routing and flow  control mechanisms in OPAL are presented next.   4B-1 Fig 5: SWMR reservation channels and MWMR data channels  F. Routing and Flow Control  To route flits in OPAL, a deadlock-free XYZ dimension order routing  scheme is used in the electrical 3D NoC, and a modified PRI-aware XYZ  routing scheme is employed for selective data transmission through the  photonic links. Communicating cores lying within the same 3D-PRI  communicate using the electrical NoC (i.e., intra-PRI transfers using  TSVs and horizontal links). Cores that need to communicate and reside  in separate PRIs communicate using the photonic paths (inter-PRI  transfers), provided they satisfy two criteria: (i) the size of the data to be  transferred is above a user-defined threshold Mth, and (ii) the number of  hops from the source core to its closest PRI gateway interface is less than  the number of hops to its destination core. In this way, large data  messages can be offloaded from the electrical NoC and sent over a faster,  more energy efficient photonic path. In addition, local communication  can be done quickly via the electrical NoC without going through  expensive electrical-to-photonic and photonic-to-electrical conversions.   Transfers between cores lying outside photonic regions of influence  occur normally via the electrical network using XYZ routing. Network  interfaces (NIs) ensure that header flits contain coordinates of the source  and destination of the packet being injected into the NoC, as well as a  flag indicating that the message size is large enough to traverse a  photonic path (for inter-PRI transfers). All routers in the OPAL  architecture have region validation units that select XYZ routing for  intra-PRI transfers, for transfers to cores not residing in any PRIs, or of  the two photonic path criteria listed above are not satisfied. Otherwise if  an inter-PRI transfer is detected by the region validation unit at the router  connected to the source NI, the flits are re-routed to the gateway interface  of the closest PRI using XYZ routing, traverse the photonic ring to the  destination gateway interface, and then are routed to the destination core,  again using XYZ routing. If multiple requests contend for access to the  photonic waveguide at a gateway interface, then the request with the  farthest distance to the destination is given priority.  The photonic waveguides in OPAL are logically partitioned into four  channels: reservation, reservation acknowledge, data, and data  acknowledge, as shown in Fig 4 (b) and Fig. 5. In order to reserve a  photonic path for a data transfer, OPAL utilizes a Single Writer Multiple  Reader (SWMR) configuration on dedicated reservation channel  waveguides. Each gateway interface has a subset of (cid:540)(cid:18)(cid:81) wavelengths  available for transmission, where (cid:540) is the total number of wavelengths  available from the multi-wavelength laser and n is the number of  gateway interfaces. Every gateway interface must be able to receive  (n-(cid:20)(cid:12)(cid:17)(cid:540)(cid:18)(cid:81) wavelengths (from the rest of the gateway interfaces), each  with a separate microring resonator receiver. A source gateway interface  uses one of its available wavelengths ((cid:540)t) to multicast the destination ID  via the reservation channel to other gateway interfaces. Each gateway  interface has (cid:1727)log(n-1)(cid:1728) dedicated SWMR reservation photonic  waveguides that it writes the destination ID to, after which the other  gateway interfaces read the request. Only the intended destination  gateway interface accepts the request, while others ignore it. As each  gateway interface has a dedicated set of (cid:540)(cid:18)(cid:81) wavelengths allocated to it,  the destination can determine the source of the request, without the  sender needing to send its ID with the multicast.  If the request can be serviced by the available wavelength and buffer  resources at the destination, a reservation acknowledgement is sent back  via the reservation ACK channel on an available wavelength. The  reservation ACK channel also has a SWMR configuration, but a single  waveguide per gateway interface is sufficient to indicate the success or  failure of the request. Once the photonic path has been reserved in this                    4B-1 manner, data transfer proceeds on the data channel, which has a low cost  Multiple Writer Multiple Reader (MWMR) configuration, unlike the  high overhead of several Multiple Writer Single Reader (MWSR) data  channels used in Corona [28] and Firefly [29]. In OPAL, the number of  data channel waveguides is equal to the chosen flit width (i.e., 256). The  same wavelength ((cid:540)t) used for the reservation phase is used by the source  to send data on. The destination gateway interface tunes one of its  available microring resonators to receive data from the sender on that  wavelength after the reservation phase. Once data transmission has  completed, an acknowledgement is sent back from the destination to the  source gateway interface via a data ACK channel that also has a SWMR  configuration with a single waveguide per gateway interface to indicate  if the data transfer completed with success. Thus the overall reservation  process takes a single cycle each for the path request and ACK phases at  the beginning of the transfer, and one cycle for the data ACK at the end.   The advantage of having a fully photonic path setup and ACK/NACK  flow control in OPAL is that it avoids using the electrical network for  path setup, as is proposed with some other approaches [32]-[34], which  our analysis shows can be a major latency and power bottleneck to the  point of mitigating the advantage of having fast and low power photonic  paths. Allowing gateway interfaces to request for access to the photonic  paths whenever data is available is also more efficient than using a token  ring scheme, which can suffer from low throughput and high latencies,  especially under low traffic conditions [28].  G. Deadlock Avoidance  While XYZ routing has been proven to be deadlock-free for mesh-like  regular 3D NoCs (as no channel dependency cycles can be formed  between dimensions), the modifications made to this routing scheme to  accommodate photonic transfers in OPAL may end up creating deadlock  conditions. We extensively studied deadlocks  in  the proposed  architecture when packets traverse the photonic ring paths. To overcome  a potential deadlock, we arrived at using low overhead timeout flits  sporadically interleaved with the flits for the long data messages  traversing the photonic paths. This is a form of regressive deadlock  recovery [47]. If a timeout flit reaches a router where flits are blocked, a  (cid:181)(cid:87)(cid:76)(cid:80)(cid:72)(cid:82)(cid:88)(cid:87)(cid:3) (cid:80)(cid:82)(cid:81)(cid:76)(cid:87)(cid:82)(cid:85) (cid:182)(cid:3) (cid:80)(cid:82)(cid:71)(cid:88)(cid:79)(cid:72)(cid:3) (cid:76)(cid:81)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3) (cid:85)(cid:82)(cid:88)(cid:87)(cid:72)(cid:85)(cid:3) (cid:70)(cid:68)(cid:81)(cid:3) (cid:71)(cid:72)(cid:87)(cid:72)(cid:70)t a timeout event and  recognize potential cases where flits are blocked due to deadlock, and  drop the blocked flits, while sending a NACK signal in the reverse  direction to indicate the flits being dropped. This allows the system to  unblock and recover from potential deadlock. While the method has the  overhead of the additional flits in long messages intended for photonic  links and a monitoring module in the routers, this is still simpler than  other potential deadlock resolution alternatives such as keeping reserved  deadlock free escape channels in every router and draining deadlocked  packets through the escape channels until the deadlock condition clears.  H. Runtime Optimizations  OPAL supports runtime dynamic reconfiguration as a way to  optimize power dissipation while meeting application throughput and  latency constraints. There are three primary ways in which OPAL  enables runtime reconfiguration: (i) DVS/DFS: Dynamic supply  voltage and clock frequency scaling is used during periods when  performance demand is low to scale down operating voltage for the  communication network to save power. OPAL uses a conservative  model for voltage scaling, where it is assumed that the square of the  voltage scales linearly with the frequency [41]; (ii) Dynamic WDM:  Wavelength division multiplexing allows several photonic signals to be  transmitted simultaneously in a single photonic waveguide using  different wavelengths which do not interfere with each other. OPAL  supports varying the number of WDM channels in waveguides at  runtime, by shutting off channels (modulators/receivers) when data  bandwidth requirements are low to save power, and enabling the  channels when bandwidth requirements become high, to maintain  performance goals; (iii) 3D-PRI reconfiguration: Small PRI region  sizes promote more transfers via the electrical 3D NoC, while large  region sizes increase the traffic flows eligible for transfer via the  photonic rings. OPAL supports varying the PRI size at runtime to adapt  to changing application traffic requirements and achieve low power  operation. The reconfiguration step involves updating region boundary  coordinates in tables in the region validation units of the NoC routers.  The update phase generally lasts a few hundred cycles, during which flit  injection is not allowed to maintain consistency.    348 IV. Experiments  A. Experimental Setup  Photonic waveguides enable faster signal propagation compared to  electrical  interconnects because  they do not suffer from RLC  impedances. But in order to exploit the propagation speed advantage of  photonic interconnects, electrical signals must be converted into light  and then back into an electrical signal. This process requires a  performance and power overhead that must be taken into account for an  accurate analysis. To explore the impact of using OPAL in CMPs, we  modeled OPAL by extensively modifying our in-house cycle accurate  SystemC-based NoC simulator. Six benchmarks from the SPLASH-2  suite [40] were selected (Cholesky, FFT, Fmm, Lu, Radix, Ocean),  parallelized, and implemented on multiple cores in the simulator model.  Fig. 6 shows an example of the SPLASH-2 benchmarks implemented  on an 8x8 CMP. Each cell represents a core, with lighter colored cores  sending/receiving fewer packets than darker colored cores.  Fig 6: SPLASH-2 implementation traffic maps for 8x8 CMP  We targeted a 32 nm process technology, and assumed a fixed 400  mm2 CMP active die area budget. Thus a single active layer 2D NoC  configuration has a 400 mm2 active E layer die area, an E2P3  configuration has a 200 mm2 die area per active E layer, and an E4P7  configuration has a 100 mm2 die area per active E layer. The operating  frequency of the photonic rings was estimated by calculating the time  needed for the light to travel from any node to the farthest node, so that  data can be transmitted to all nodes in one cycle. Through geometric  calculations for the rings, using delay values from Table 1, and  incorporating latching delays (using ITRS data [6]) we obtained a  maximum operating frequency of greater than 3 GHz for the different  sizes of CMPs we considered. Ultimately, the photonic rings and the  communication network were clocked conservatively at 2.3 GHz. The  data message threshold size for inter 3D-PRI photonic transfers was  fixed at 2048 bits, and the packet size was kept at 10 flits. Delay  estimates for the various photonic interconnect-centric components  used in OPAL were obtained from [43] and from device fabrication  results [44]. Table 1 shows these delays for the 32 nm node. The delay  of an optimally repeated and sized electrical (Cu) wire at 32 nm was  assumed to be 42ps/mm [9].  TABLE I  Delay and energy consumption for OPAL elements (32nm) DDE = Data  traffic dependent energy, SE = Static energy (clock, leakage), TTE =  Thermal tuning energy (20K temperature range)  Component  Delay   DDE  SE  TTE  Modulator driver  9.5 ps   20 fJ/bit 5 fJ/bit 16 fJ/bit/heater  Modulator  3.1 ps  Waveguide  15.4 ps/mm   -  -  -  Photo Detector  0.22 ps   20 fJ/bit 5 fJ/bit 16 fJ/bit/heater  Receiver  4.0 ps  The power dissipated in OPAL can be categorized into two  components: electrical network power and photonic ring network power.  The static and dynamic power dissipation of electrical routers and links  in this work is based on results from Orion 2.0 [46] incorporated into  our simulator. For calculating power dissipation of the modulator driver  and TIA power we used ITRS device projections [6] and standard  circuit procedures. Energy dissipation values for the modulators and  receivers are summarized in Table 1 [30]. In addition, an off-chip  electrical laser power of 3.3 W per photonic layer (with 30% laser  efficiency) is also considered in the power calculations. The laser power  value accounts for per component optical losses for the coupler/splitter  (1.2dB), non-linearity  (1dB at 30mW), waveguide  (3dB/cm),  waveguide crossings (0.05dB), ring modulator (1dB), receiver filter  (1.5dB) and photodetector (0.1 dB).                            B. Results  B.1 Comparisons with 2D and 3D Electrical Mesh NoC  In the first set of experiments, we compared the performance and  power characteristics of the E2P3 and E4P7 OPAL configurations, but  without enabling any dynamic reconfiguration, with traditional 2D and  3D electrical mesh NoCs. The 2D configurations considered included a  64 core (8×8) and 100 core (10×10) NoC, while the 3D configurations  included a 2 layer 64 core (8×4×2), a 2 layer 100 core (10×5×2), a 4  layer 64 core (4×4×4), and a 4 layer 100 core (5×5×4) NoC. The OPAL  configurations have four uplinks between an active layer and its  associated photonic layer, a PRI size of two, and WDM with 32  wavelengths in the photonic waveguides.  4B-1 number of uplinks for a 100 core CMP with a fixed PRI region size of  four for E2P3 (2×2×2 cores/region), and E4P7 (2×2×4 cores/region),  and WDM with 32 wavelengths in the photonic waveguides. For a  configuration with (cid:536) uplinks, there are 2(cid:536) gateway interfaces per active  (E) layer for E2P3 ((cid:536) interfaces to the private P layer, and (cid:536) interfaces to  the shared P layer), and 3(cid:536) gateway interfaces per active (E) layer for  E4P7 ((cid:536) interfaces to the private P layer, and 2(cid:536) interfaces to the two  shared P layers). Improvements in power dissipation and latency were  significant when the number of uplinks were increased from 4 to 8. The  improvements drop when uplinks are increased from 8 to 16 due to  overlapped PRI regions  leading  to  less opportunity for global  communication. This trend continues with further increase in the  number of uplinks, with the 32 uplink case providing negligible  improvements over the 16 uplink case, while significantly increasing  complexity in the photonic and electrical NoC layers.                    (a)                           (b)  Fig 7: percentage improvement for OPAL configurations compared to 2D  and 3D electrical mesh NoCs (a) power, (b) average packet latency  Fig 7 (a)-(b) show the improvements in power and average packet  latency for the E2P3 and E4P7 OPAL configurations compared to the  2D and 3D electrical mesh NoCs. It can be seen that 3D electrical mesh  NoCs have a much lower power dissipation and average packet latency  compared to 2D electrical mesh NoCs which explains the recent interest  in 3D ICs and the potential gains that can be achieved by shifting from  2D to 3D ICs. OPAL goes a step farther and outperforms the  all-electrical 3D ICs because of its use of low power and high speed  photonic interconnects. In general, the E4P7 OPAL configuration  outperforms the E2P3 configuration and obtains an up to a 15× power  reduction and 32× average latency reduction over 2D ICs, and up to a  8× power reduction and 20× average latency reduction over 3D ICs.  These results indicate that OPAL has the potential to improve the  benefits that can be achieved by using 3D ICs in future CMP designs.                (a)                               (b)                 (c)                               (d)   Fig 8: Impact of changing number of uplinks on (a) latency of E2P3, (b)  latency of E4P7, (c) average power of E2P3, (d) average power of E4P7  B.2 Impact of Varying Number of Uplinks  To overcome the bottleneck of a limited number of uplinks (i.e.,  gateway interfaces), we next explored the impact of varying the number  of uplinks in the OPAL architecture at design time and measured the  performance and power for the various configurations. As the number  of gateway interface routers with photonic interfaces increases, it also  results in an increase in power due to electro-photonic conversion.  Increasing the number of uplinks also increases real estate usage in the  silicon layer, as well as the complexity of the photonic layer. However  the additional complexity of more uplinks can translate into better  photonic path utilization for communication flits in some applications.  In addition, increasing the number of uplinks can also provide fault  tolerance in case of uplink failures. Fig 8 shows results of varying the  349 Fig 9: percentage improvement in average power dissipation for E2P3 and  E4P7 OPAL configurations, with all runtime adaptations enabled (DVS/DFS,  WDM, PRI) relative to baseline case with no runtime adaptation enabled  B.3 Impact of Enabling Runtime Adaptations  In the next set of experiments, we explored the impact of enabling  runtime adaptation in OPAL on the overall power dissipation. As  discussed in Section III.H, dynamically adapting resources based on  runtime traffic requirements can expose opportunities for power savings.  Fig 9 presents results of power savings for the six SPLASH-2  benchmark  implementations when  the dynamic  reconfiguration  schemes described in Section III.H (PRI resizing, WDM scaling,  DVS/DFS) are applied simultaneously, compared to the baseline case  without any dynamic reconfiguration enabled. The implementation of  these schemes was guided by offline profiling of the selected  benchmark implementations. Results are shown for the E2P3 and E4P7  OPAL configurations, for 64 and 100 core CMPs with a WDM degree  of 32 and four uplinks. It can be seen from Fig 9 that the cumulative  improvement in power savings for the optimizations is significant. It  was found that the improvements due to DVS/DFS diminish with  increasing core count due to the increased overhead of the DVS/DFS  circuitry, and smaller sized links which provide lower power savings. A  similar  trend  is noticed with WDM scaling, with diminishing  improvements as core count increases. This is due to greater demand for  photonic communication by the increased number of traffic flows which  limits the opportunities for reducing wavelength channels for WDM. As  the number of active (E) and photonic (P) layers increase, the number  of gateway interfaces and consequently area covered by 3D-PRI regions  also increase. The E4P7 configuration therefore has more opportunities  for fine tuning traffic distribution among electrical and photonic paths  by utilizing PRI reconfiguration compared to the E2P3 configuration,  leading to an increase in power savings. The improvements due to PRI  resizing overshadow the diminishing returns from DVS/DFS and WDM  scaling for the E4P7 configuration as core counts increase, which is  why its power dissipation improves (reduces) with increasing core  counts. The E2P3 configuration does not benefit as much by utilizing  PRI resizing with increasing numbers of cores, and consequently has  lower power savings for higher core counts.    B.4 Comparison with Existing Hybrid Photonic NoCs  Our final set of experiments compares the E2P3 and E4P7 OPAL 3D  hybrid photonic NoC configurations with three previously proposed 2D  hybrid photonic communication architectures: (i) a hybrid photonic  torus interfaced with an electrical 2D torus NoC [32], (ii) the hybrid  Corona architecture [28], and (iii) the hybrid Firefly architecture [29].  Both OPAL configurations utilized dynamic reconfiguration and 8  uplinks. For fairness of comparison, all the compared architectures were                                        4B-1 modeled with a WDM degree of 128, and were simulated using the  same set of technology parameters, component delay and power models,  and traffic. Results were obtained for a 100 core CMP. Fig 10 (a)-(b)  shows the percentage improvement for the E2P3 and E4P7 OPAL  configurations in terms of power dissipation and average packet latency  over the hybrid photonic torus, Corona, and Firefly architectures. From  the results it can be seen that the OPAL configurations improve upon  existing 2D hybrid photonic NoC architectures, with the E4P7  configuration showing somewhat higher improvements than the E2P3  configuration. For instance, compared to the Firefly hybrid NoC, the  E4P7 OPAL configuration shows up to approx. 10× reduction in power  dissipation and a 3× reduction in average packet latency. The ability to  better balance traffic between the electrical and photonic paths, a more  effective photonic path setup, support for runtime adaptations of the  electrical and photonic networks, and the use of shorter TSVs to replace  longer global wires are the primary reasons for (cid:50)(cid:51)(cid:36)(cid:47)(cid:182)(cid:86) superior  performance. In terms of photonic component area overhead, our  calculations indicate that the E4P7 OPAL configuration has lower  photonic component area by a factors of 1.5×, 1.6×, and 2.1× compared  to Firefly, photonic torus, and Corona architectures respectively. We  conjecture that compared to having a single complex photonic layer,  having multiple simpler photonic layers as in OPAL can not only ease  fabrication challenges, but also provide lower average power and  latency as the experimental results indicate. These results also make a  strong case for considering the use of photonic interconnects in  emerging 3D ICs.                (a)                             (b)  Fig 10: percentage improvement for E2P3 and E4P7 OPAL configurations  compared with hybrid photonic torus [32], Corona [28] and Firefly [29]  NoCs: (a) power dissipation (b) average packet latency  V. Summary and Conclusions  In this paper, we proposed and explored a multi-layer hybrid  electro-photonic NoC fabric (OPAL) for 3D ICs. Our proposed 3D  hybrid ring-mesh NoC combines low cost photonic rings on multiple  photonic layers with 3D mesh NoCs in active layers to reduce on-chip  communication power dissipation and latency. OPAL also supports  mechanisms for adaptation to changing traffic at runtime to optimize  power dissipation. Experimental comparisons with traditional 2D NoCs,  3D NoCs, and previously proposed hybrid photonic NoCs indicate a  strong motivation for considering OPAL for future 3D ICs as it can  provide several orders of magnitude reduction in power dissipation and  average latency. Our future work will explore the thermal design  considerations in 3D hybrid electro-photonic NoCs in more detail.  "
2011,A thermal-aware application specific routing algorithm for Network-on-Chip design.,"In this work, we propose an application specific routing algorithm to reduce the hot-spot temperature for Network-on-chip (NoC). Using the traffic information of applications, we develop a routing scheme which can achieve a higher adaptivity than the generic ones and at the same time distribute the traffic more uniformly. A set of deadlock-free admissible paths for all the communications is first obtained. To reduce the hot-spot temperature, we find the optimal distribution ratio of the communication traffic among the set of candidate paths. The problem of finding this optimal distribution ratio is formulated as a linear programming (LP) problem and is solved offline. A router microarchitecture which supports our ratio-based selection policy is also proposed. From the simulation results, the peak energy reduction considering the energy consumption of both the processors and routers can be as high as 16.6% for synthetic traffic and real benchmarks.",(cid:32)(cid:34)(cid:20)(cid:31)(cid:137) (cid:40)(cid:137) (cid:63)(cid:97)(cid:89)(cid:122)(cid:111)(cid:75)(cid:109)(cid:22)(cid:75)(cid:135)(cid:75)(cid:122)(cid:89)(cid:137) (cid:40)(cid:119)(cid:119)(cid:109)(cid:101)(cid:78)(cid:75)(cid:129)(cid:101)(cid:116)(cid:114)(cid:137) (cid:60)(cid:119)(cid:89)(cid:78)(cid:101)(cid:92)(cid:78)(cid:137) (cid:57)(cid:116)(cid:132)(cid:129)(cid:101)(cid:114)(cid:95)(cid:137) (cid:40)(cid:109)(cid:95)(cid:116)(cid:122)(cid:101)(cid:129)(cid:97)(cid:111)(cid:137) (cid:91)(cid:116)(cid:122)(cid:137) (cid:53)(cid:137)(cid:89)(cid:129)(cid:135)(cid:116)(cid:122)(cid:105)(cid:22)(cid:116)(cid:114)(cid:22)(cid:42)(cid:97)(cid:101)(cid:119)(cid:137) (cid:43)(cid:89)(cid:127)(cid:101)(cid:95)(cid:114)(cid:137) (cid:70)(cid:96)(cid:100)(cid:108)(cid:100)(cid:74)(cid:113)(cid:94)(cid:137) (cid:56)(cid:100)(cid:74)(cid:113)(cid:137) (cid:74)(cid:113)(cid:82)(cid:137) (cid:41)(cid:96)(cid:100)(cid:21)(cid:69)(cid:100)(cid:113)(cid:94)(cid:137) (cid:62)(cid:126)(cid:131)(cid:100)(cid:137) (cid:34)(cid:71)(cid:92)(cid:67)(cid:95)(cid:98)(cid:87)(cid:71)(cid:89)(cid:98)(cid:110) (cid:90)(cid:72)(cid:110) (cid:35)(cid:84)(cid:71)(cid:69)(cid:98)(cid:95)(cid:90)(cid:89)(cid:80)(cid:69)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:33)(cid:90)(cid:87)(cid:92)(cid:100)(cid:98)(cid:71)(cid:95)(cid:110) (cid:35)(cid:89)(cid:78)(cid:80)(cid:89)(cid:71)(cid:71)(cid:95)(cid:80)(cid:89)(cid:78)(cid:110) (cid:53)(cid:79)(cid:71)(cid:110) (cid:39)(cid:90)(cid:89)(cid:78)(cid:110) (cid:43)(cid:90)(cid:89)(cid:78)(cid:110) (cid:58)(cid:89)(cid:80)(cid:101)(cid:71)(cid:95)(cid:97)(cid:80)(cid:98)(cid:104)(cid:110) (cid:90)(cid:72)(cid:110) (cid:52)(cid:69)(cid:80)(cid:71)(cid:89)(cid:69)(cid:71)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:53)(cid:71)(cid:69)(cid:79)(cid:89)(cid:90)(cid:84)(cid:90)(cid:78)(cid:104)(cid:8) (cid:39)(cid:90)(cid:89)(cid:78)(cid:110) (cid:43)(cid:90)(cid:89)(cid:78)(cid:110) (cid:35)(cid:87)(cid:67)(cid:80)(cid:84)(cid:27)(cid:106)(cid:94)(cid:80)(cid:67)(cid:89)(cid:105)(cid:84)(cid:13)(cid:71)(cid:71)(cid:98)(cid:97)(cid:100)(cid:80)(cid:108)(cid:29)(cid:100)(cid:97)(cid:98)(cid:11) (cid:79)(cid:83)(cid:110) (cid:67)(cid:110) (cid:97)(cid:80)(cid:78)(cid:89)(cid:80)(cid:76)(cid:69)(cid:67)(cid:89)(cid:98)(cid:110) (cid:92)(cid:67)(cid:95)(cid:98)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:69)(cid:79)(cid:80)(cid:92)(cid:3)(cid:97)(cid:110) (cid:98)(cid:90)(cid:98)(cid:67)(cid:84)(cid:110) (cid:92)(cid:90)(cid:102)(cid:71)(cid:95)(cid:110) (cid:68)(cid:100)(cid:70)(cid:78)(cid:71)(cid:98)(cid:110) (cid:4)(cid:19)(cid:26)(cid:2)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:90)(cid:109) (cid:98)(cid:67)(cid:84)(cid:110) (cid:98)(cid:80)(cid:84)(cid:71)(cid:110) (cid:92)(cid:90)(cid:102)(cid:71)(cid:95)(cid:110) (cid:80)(cid:89)(cid:110) (cid:62)(cid:20)(cid:64)(cid:6)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:70)(cid:71)(cid:69)(cid:80)(cid:97)(cid:80)(cid:90)(cid:89)(cid:110) (cid:90)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:67)(cid:69)(cid:83)(cid:71)(cid:98)(cid:97)(cid:110)(cid:102)(cid:80)(cid:84)(cid:84)(cid:110)(cid:78)(cid:95)(cid:71)(cid:67)(cid:98)(cid:84)(cid:104)(cid:110)(cid:67)(cid:73)(cid:71)(cid:69)(cid:98)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:92)(cid:90)(cid:102)(cid:71)(cid:95)(cid:110)(cid:70)(cid:80)(cid:97)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:67)(cid:89)(cid:70)(cid:110)(cid:79)(cid:71)(cid:89)(cid:69)(cid:71)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110) (cid:90)(cid:101)(cid:71)(cid:95)(cid:67)(cid:84)(cid:84)(cid:110) (cid:69)(cid:79)(cid:80)(cid:92)(cid:110) (cid:79)(cid:90)(cid:98)(cid:10)(cid:97)(cid:92)(cid:90)(cid:98)(cid:110) (cid:98)(cid:71)(cid:87)(cid:92)(cid:71)(cid:95)(cid:67)(cid:98)(cid:100)(cid:95)(cid:71)(cid:110) (cid:62)(cid:17)(cid:64)(cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:95)(cid:71)(cid:72)(cid:90)(cid:95)(cid:71)(cid:110) (cid:80)(cid:98)(cid:110) (cid:80)(cid:97)(cid:110) (cid:80)(cid:87)(cid:92)(cid:90)(cid:95)(cid:109) (cid:98)(cid:67)(cid:89)(cid:98)(cid:110) (cid:98)(cid:90)(cid:110) (cid:69)(cid:90)(cid:89)(cid:97)(cid:80)(cid:70)(cid:71)(cid:95)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:95)(cid:87)(cid:67)(cid:84)(cid:110)(cid:69)(cid:90)(cid:89)(cid:97)(cid:98)(cid:95)(cid:67)(cid:80)(cid:89)(cid:98)(cid:110) (cid:80)(cid:89)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110)(cid:92)(cid:79)(cid:67)(cid:97)(cid:71)(cid:110)(cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:47)(cid:90)(cid:33)(cid:110) (cid:70)(cid:71)(cid:97)(cid:80)(cid:78)(cid:89)(cid:11)(cid:110) (cid:23)(cid:70)(cid:120)(cid:124)(cid:117)(cid:68)(cid:74)(cid:124)(cid:9)(cid:34)(cid:107)(cid:136) (cid:98)(cid:79)(cid:80)(cid:97)(cid:110) (cid:102)(cid:90)(cid:95)(cid:83)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:92)(cid:95)(cid:90)(cid:92)(cid:90)(cid:97)(cid:71)(cid:110) (cid:67)(cid:89)(cid:110) (cid:67)(cid:92)(cid:92)(cid:84)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:97)(cid:92)(cid:71)(cid:109) (cid:69)(cid:80)(cid:76)(cid:69)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110) (cid:98)(cid:90)(cid:110) (cid:95)(cid:71)(cid:70)(cid:100)(cid:69)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:79)(cid:90)(cid:98)(cid:10)(cid:97)(cid:92)(cid:90)(cid:98)(cid:110) (cid:98)(cid:71)(cid:87)(cid:92)(cid:71)(cid:95)(cid:67)(cid:98)(cid:100)(cid:95)(cid:71)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:47)(cid:71)(cid:98)(cid:102)(cid:90)(cid:95)(cid:83)(cid:10)(cid:90)(cid:89)(cid:10)(cid:69)(cid:79)(cid:80)(cid:92)(cid:110) (cid:4)(cid:47)(cid:90)(cid:33)(cid:6)(cid:110) (cid:11)(cid:110) (cid:58)(cid:97)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:80)(cid:89)(cid:72)(cid:90)(cid:95)(cid:87)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:90)(cid:72)(cid:110) (cid:67)(cid:92)(cid:109) (cid:92)(cid:84)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:97)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:70)(cid:71)(cid:101)(cid:71)(cid:84)(cid:90)(cid:92)(cid:110) (cid:67)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:97)(cid:69)(cid:79)(cid:71)(cid:87)(cid:71)(cid:110) (cid:102)(cid:79)(cid:80)(cid:69)(cid:79)(cid:110) (cid:69)(cid:67)(cid:89)(cid:110) (cid:67)(cid:69)(cid:79)(cid:80)(cid:71)(cid:101)(cid:71)(cid:110) (cid:67)(cid:110) (cid:79)(cid:80)(cid:78)(cid:79)(cid:71)(cid:95)(cid:110) (cid:67)(cid:70)(cid:67)(cid:92)(cid:98)(cid:80)(cid:101)(cid:80)(cid:98)(cid:104)(cid:110) (cid:98)(cid:79)(cid:67)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:78)(cid:71)(cid:89)(cid:71)(cid:95)(cid:80)(cid:69)(cid:110) (cid:90)(cid:89)(cid:71)(cid:97)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:67)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:67)(cid:87)(cid:71)(cid:110) (cid:98)(cid:80)(cid:87)(cid:71)(cid:110) (cid:70)(cid:80)(cid:97)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:98)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:87)(cid:90)(cid:95)(cid:71)(cid:110) (cid:100)(cid:89)(cid:80)(cid:72)(cid:90)(cid:95)(cid:87)(cid:84)(cid:104)(cid:11)(cid:110) (cid:30)(cid:110) (cid:97)(cid:71)(cid:98)(cid:110) (cid:90)(cid:72)(cid:110) (cid:70)(cid:71)(cid:67)(cid:70)(cid:84)(cid:90)(cid:69)(cid:83)(cid:10)(cid:72)(cid:95)(cid:71)(cid:71)(cid:110) (cid:67)(cid:70)(cid:87)(cid:80)(cid:97)(cid:97)(cid:80)(cid:68)(cid:84)(cid:71)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:67)(cid:84)(cid:84)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:97)(cid:110) (cid:80)(cid:97)(cid:110) (cid:76)(cid:95)(cid:97)(cid:98)(cid:110) (cid:90)(cid:68)(cid:98)(cid:67)(cid:80)(cid:89)(cid:71)(cid:70)(cid:11)(cid:110) (cid:53)(cid:90)(cid:110) (cid:95)(cid:71)(cid:70)(cid:100)(cid:69)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:79)(cid:90)(cid:98)(cid:10)(cid:97)(cid:92)(cid:90)(cid:98)(cid:110) (cid:98)(cid:71)(cid:87)(cid:92)(cid:71)(cid:95)(cid:67)(cid:98)(cid:100)(cid:95)(cid:71)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:76)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:90)(cid:92)(cid:98)(cid:80)(cid:87)(cid:67)(cid:84)(cid:110) (cid:70)(cid:80)(cid:97)(cid:109) (cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:95)(cid:67)(cid:98)(cid:80)(cid:90)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:67)(cid:87)(cid:90)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:71)(cid:98)(cid:110) (cid:90)(cid:72)(cid:110) (cid:69)(cid:67)(cid:89)(cid:70)(cid:80)(cid:70)(cid:67)(cid:98)(cid:71)(cid:110)(cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:110)(cid:92)(cid:95)(cid:90)(cid:68)(cid:84)(cid:71)(cid:87)(cid:110)(cid:90)(cid:72)(cid:110)(cid:76)(cid:89)(cid:70)(cid:80)(cid:89)(cid:78)(cid:110)(cid:98)(cid:79)(cid:80)(cid:97)(cid:110)(cid:90)(cid:92)(cid:98)(cid:80)(cid:87)(cid:67)(cid:84)(cid:110)(cid:70)(cid:80)(cid:97)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:109) (cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:95)(cid:67)(cid:98)(cid:80)(cid:90)(cid:110)(cid:80)(cid:97)(cid:110)(cid:72)(cid:90)(cid:95)(cid:87)(cid:100)(cid:84)(cid:67)(cid:98)(cid:71)(cid:70)(cid:110)(cid:67)(cid:97)(cid:110)(cid:67)(cid:110)(cid:84)(cid:80)(cid:89)(cid:71)(cid:67)(cid:95)(cid:110)(cid:92)(cid:95)(cid:90)(cid:78)(cid:95)(cid:67)(cid:87)(cid:87)(cid:80)(cid:89)(cid:78)(cid:110) (cid:4)(cid:44)(cid:50)(cid:6)(cid:110)(cid:92)(cid:95)(cid:90)(cid:68)(cid:84)(cid:71)(cid:87)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:80)(cid:97)(cid:110) (cid:97)(cid:90)(cid:84)(cid:101)(cid:71)(cid:70)(cid:110) (cid:90)(cid:88)(cid:80)(cid:89)(cid:71)(cid:11)(cid:110) (cid:30)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:110) (cid:87)(cid:80)(cid:69)(cid:95)(cid:90)(cid:67)(cid:95)(cid:69)(cid:79)(cid:80)(cid:98)(cid:71)(cid:69)(cid:98)(cid:100)(cid:95)(cid:71)(cid:110) (cid:102)(cid:79)(cid:80)(cid:69)(cid:79)(cid:110) (cid:97)(cid:100)(cid:92)(cid:109) (cid:92)(cid:90)(cid:95)(cid:98)(cid:97)(cid:110) (cid:90)(cid:100)(cid:95)(cid:110) (cid:95)(cid:67)(cid:98)(cid:80)(cid:90)(cid:10)(cid:68)(cid:67)(cid:97)(cid:71)(cid:70)(cid:110) (cid:97)(cid:71)(cid:84)(cid:71)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:92)(cid:90)(cid:84)(cid:80)(cid:69)(cid:104)(cid:110) (cid:80)(cid:97)(cid:110) (cid:67)(cid:84)(cid:97)(cid:90)(cid:110) (cid:92)(cid:95)(cid:90)(cid:92)(cid:90)(cid:97)(cid:71)(cid:70)(cid:11)(cid:110) (cid:37)(cid:90)(cid:87)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:80)(cid:87)(cid:100)(cid:84)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:95)(cid:71)(cid:97)(cid:100)(cid:84)(cid:98)(cid:97)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:71)(cid:67)(cid:83)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:95)(cid:71)(cid:70)(cid:100)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:69)(cid:90)(cid:89)(cid:97)(cid:80)(cid:70)(cid:71)(cid:95)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110)(cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110)(cid:69)(cid:90)(cid:89)(cid:97)(cid:100)(cid:87)(cid:92)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:90)(cid:72)(cid:110)(cid:68)(cid:90)(cid:98)(cid:79)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:92)(cid:95)(cid:90)(cid:69)(cid:71)(cid:97)(cid:97)(cid:90)(cid:95)(cid:97)(cid:110)(cid:67)(cid:89)(cid:70)(cid:110)(cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:97)(cid:110)(cid:69)(cid:67)(cid:89)(cid:110) (cid:68)(cid:71)(cid:110) (cid:67)(cid:97)(cid:110) (cid:79)(cid:80)(cid:78)(cid:79)(cid:110) (cid:67)(cid:97)(cid:110) (cid:16)(cid:23)(cid:11) (cid:23)(cid:2)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:97)(cid:104)(cid:89)(cid:98)(cid:79)(cid:71)(cid:98)(cid:80)(cid:69)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:95)(cid:71)(cid:67)(cid:84)(cid:110) (cid:68)(cid:71)(cid:89)(cid:69)(cid:79)(cid:87)(cid:67)(cid:95)(cid:83)(cid:97)(cid:11)(cid:110) (cid:21)(cid:15)(cid:116) (cid:35)(cid:42)(cid:55)(cid:50)(cid:45)(cid:27)(cid:57)(cid:25)(cid:55)(cid:36)(cid:45)(cid:42)(cid:136) (cid:35)(cid:87)(cid:68)(cid:71)(cid:70)(cid:70)(cid:71)(cid:70)(cid:110)(cid:97)(cid:104)(cid:97)(cid:98)(cid:71)(cid:87)(cid:97)(cid:110)(cid:79)(cid:67)(cid:101)(cid:71)(cid:110) (cid:84)(cid:71)(cid:70)(cid:110)(cid:98)(cid:90)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:67)(cid:70)(cid:101)(cid:71)(cid:89)(cid:98)(cid:110)(cid:90)(cid:72)(cid:110)(cid:87)(cid:100)(cid:84)(cid:98)(cid:80)(cid:92)(cid:95)(cid:90)(cid:69)(cid:71)(cid:97)(cid:97)(cid:90)(cid:95)(cid:110) (cid:97)(cid:104)(cid:97)(cid:98)(cid:71)(cid:87)(cid:10)(cid:90)(cid:89)(cid:10)(cid:69)(cid:79)(cid:80)(cid:92)(cid:110) (cid:4)(cid:45)(cid:50)(cid:52)(cid:90)(cid:33)(cid:6)(cid:110) (cid:70)(cid:71)(cid:97)(cid:80)(cid:78)(cid:89)(cid:11)(cid:110) (cid:60)(cid:80)(cid:98)(cid:79)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:89)(cid:100)(cid:87)(cid:68)(cid:71)(cid:95)(cid:110) (cid:90)(cid:72)(cid:110)(cid:90)(cid:89)(cid:10)(cid:69)(cid:79)(cid:80)(cid:92)(cid:110) (cid:69)(cid:90)(cid:95)(cid:71)(cid:97)(cid:110) (cid:80)(cid:89)(cid:69)(cid:95)(cid:71)(cid:67)(cid:97)(cid:71)(cid:97)(cid:8)(cid:110) (cid:67)(cid:110) (cid:97)(cid:69)(cid:67)(cid:84)(cid:67)(cid:68)(cid:84)(cid:71)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:87)(cid:90)(cid:70)(cid:100)(cid:84)(cid:67)(cid:95)(cid:110) (cid:97)(cid:90)(cid:84)(cid:100)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:98)(cid:90)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:80)(cid:89)(cid:98)(cid:71)(cid:95)(cid:109) (cid:69)(cid:90)(cid:89)(cid:89)(cid:71)(cid:69)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:92)(cid:95)(cid:90)(cid:68)(cid:84)(cid:71)(cid:87)(cid:110) (cid:68)(cid:71)(cid:69)(cid:90)(cid:87)(cid:71)(cid:97)(cid:110) (cid:69)(cid:95)(cid:80)(cid:98)(cid:80)(cid:69)(cid:67)(cid:84)(cid:84)(cid:104)(cid:110) (cid:80)(cid:87)(cid:92)(cid:90)(cid:95)(cid:98)(cid:67)(cid:89)(cid:98)(cid:11)(cid:110) (cid:47)(cid:71)(cid:98)(cid:102)(cid:90)(cid:95)(cid:83)(cid:109) (cid:90)(cid:89)(cid:10)(cid:69)(cid:79)(cid:80)(cid:92)(cid:110) (cid:4)(cid:47)(cid:90)(cid:33)(cid:6)(cid:110) (cid:79)(cid:67)(cid:97)(cid:110) (cid:68)(cid:71)(cid:71)(cid:89)(cid:110) (cid:92)(cid:95)(cid:90)(cid:92)(cid:90)(cid:97)(cid:71)(cid:70)(cid:110) (cid:98)(cid:90)(cid:110) (cid:87)(cid:71)(cid:71)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:97)(cid:71)(cid:110) (cid:70)(cid:80)(cid:97)(cid:98)(cid:80)(cid:89)(cid:69)(cid:98)(cid:80)(cid:101)(cid:71)(cid:110) (cid:69)(cid:79)(cid:67)(cid:84)(cid:84)(cid:71)(cid:89)(cid:78)(cid:71)(cid:97)(cid:110) (cid:62)(cid:40)(cid:64)(cid:11)(cid:110) (cid:36)(cid:90)(cid:95)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:70)(cid:71)(cid:97)(cid:80)(cid:78)(cid:89)(cid:110)(cid:90)(cid:72)(cid:110)(cid:97)(cid:90)(cid:92)(cid:79)(cid:80)(cid:97)(cid:98)(cid:80)(cid:69)(cid:67)(cid:98)(cid:71)(cid:70)(cid:110)(cid:45)(cid:50)(cid:52)(cid:90)(cid:33)(cid:110)(cid:90)(cid:89)(cid:110)(cid:79)(cid:80)(cid:78)(cid:79)(cid:110)(cid:92)(cid:71)(cid:95)(cid:72)(cid:90)(cid:95)(cid:87)(cid:67)(cid:89)(cid:69)(cid:71)(cid:110) (cid:47)(cid:90)(cid:33)(cid:8)(cid:110) (cid:92)(cid:90)(cid:102)(cid:71)(cid:95)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:71)(cid:87)(cid:92)(cid:71)(cid:95)(cid:67)(cid:98)(cid:100)(cid:95)(cid:71)(cid:110) (cid:79)(cid:67)(cid:101)(cid:71)(cid:110) (cid:68)(cid:71)(cid:69)(cid:90)(cid:87)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:70)(cid:90)(cid:87)(cid:80)(cid:89)(cid:67)(cid:89)(cid:98)(cid:110) (cid:69)(cid:90)(cid:89)(cid:109) (cid:97)(cid:98)(cid:95)(cid:67)(cid:80)(cid:89)(cid:98)(cid:97)(cid:110) (cid:62)(cid:17)(cid:64)(cid:11)(cid:110) (cid:39)(cid:80)(cid:78)(cid:79)(cid:71)(cid:95)(cid:110) (cid:92)(cid:90)(cid:102)(cid:71)(cid:95)(cid:110) (cid:69)(cid:90)(cid:89)(cid:97)(cid:100)(cid:87)(cid:92)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:84)(cid:71)(cid:67)(cid:70)(cid:97)(cid:110) (cid:98)(cid:90)(cid:110) (cid:79)(cid:80)(cid:78)(cid:79)(cid:71)(cid:95)(cid:110) (cid:98)(cid:71)(cid:87)(cid:109) (cid:92)(cid:71)(cid:95)(cid:67)(cid:98)(cid:100)(cid:95)(cid:71)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:67)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:67)(cid:87)(cid:71)(cid:110) (cid:98)(cid:80)(cid:87)(cid:71)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:100)(cid:89)(cid:71)(cid:101)(cid:71)(cid:89)(cid:110) (cid:92)(cid:90)(cid:102)(cid:71)(cid:95)(cid:110) (cid:70)(cid:80)(cid:97)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:67)(cid:69)(cid:95)(cid:90)(cid:97)(cid:97)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:69)(cid:79)(cid:80)(cid:92)(cid:110) (cid:102)(cid:80)(cid:84)(cid:84)(cid:110) (cid:69)(cid:95)(cid:71)(cid:67)(cid:98)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:95)(cid:87)(cid:67)(cid:84)(cid:110) (cid:79)(cid:90)(cid:98)(cid:10)(cid:97)(cid:92)(cid:90)(cid:98)(cid:97)(cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:97)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:95)(cid:87)(cid:67)(cid:84)(cid:110) (cid:79)(cid:90)(cid:98)(cid:10)(cid:97)(cid:92)(cid:90)(cid:98)(cid:97)(cid:110)(cid:102)(cid:80)(cid:84)(cid:84)(cid:110)(cid:70)(cid:71)(cid:78)(cid:95)(cid:67)(cid:70)(cid:71)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:92)(cid:71)(cid:95)(cid:72)(cid:90)(cid:95)(cid:87)(cid:67)(cid:89)(cid:69)(cid:71)(cid:110)(cid:67)(cid:89)(cid:70)(cid:110)(cid:95)(cid:71)(cid:84)(cid:80)(cid:67)(cid:68)(cid:80)(cid:84)(cid:80)(cid:98)(cid:104)(cid:110)(cid:70)(cid:95)(cid:67)(cid:87)(cid:67)(cid:98)(cid:109) (cid:80)(cid:69)(cid:67)(cid:84)(cid:84)(cid:104)(cid:11)(cid:110) (cid:53)(cid:79)(cid:80)(cid:97)(cid:110) (cid:80)(cid:87)(cid:92)(cid:90)(cid:97)(cid:71)(cid:97)(cid:110) (cid:67)(cid:110) (cid:69)(cid:90)(cid:89)(cid:97)(cid:98)(cid:95)(cid:67)(cid:80)(cid:89)(cid:98)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:47)(cid:90)(cid:33)(cid:110) (cid:70)(cid:71)(cid:97)(cid:80)(cid:78)(cid:89)(cid:110) (cid:98)(cid:90)(cid:110) (cid:67)(cid:101)(cid:90)(cid:80)(cid:70)(cid:110) (cid:100)(cid:89)(cid:109) (cid:71)(cid:101)(cid:71)(cid:89)(cid:110) (cid:92)(cid:90)(cid:102)(cid:71)(cid:95)(cid:110) (cid:92)(cid:95)(cid:90)(cid:76)(cid:84)(cid:71)(cid:110) (cid:97)(cid:90)(cid:110) (cid:67)(cid:97)(cid:110) (cid:98)(cid:90)(cid:110) (cid:95)(cid:71)(cid:70)(cid:100)(cid:69)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:79)(cid:90)(cid:98)(cid:10)(cid:97)(cid:92)(cid:90)(cid:98)(cid:110) (cid:98)(cid:71)(cid:87)(cid:92)(cid:71)(cid:95)(cid:67)(cid:98)(cid:100)(cid:95)(cid:71)(cid:11)(cid:110) (cid:41)(cid:89)(cid:110) (cid:67)(cid:110) (cid:98)(cid:104)(cid:92)(cid:80)(cid:69)(cid:67)(cid:84)(cid:110) (cid:47)(cid:90)(cid:33)(cid:10)(cid:68)(cid:67)(cid:97)(cid:71)(cid:70)(cid:110)(cid:45)(cid:50)(cid:52)(cid:90)(cid:33)(cid:110) (cid:70)(cid:71)(cid:97)(cid:80)(cid:78)(cid:89)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:89)(cid:71)(cid:71)(cid:70)(cid:110) (cid:98)(cid:90)(cid:110) (cid:67)(cid:84)(cid:84)(cid:90)(cid:69)(cid:67)(cid:98)(cid:71)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:97)(cid:69)(cid:79)(cid:71)(cid:70)(cid:100)(cid:84)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:67)(cid:97)(cid:83)(cid:97)(cid:110) (cid:90)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:67)(cid:101)(cid:67)(cid:80)(cid:84)(cid:67)(cid:68)(cid:84)(cid:71)(cid:110) (cid:92)(cid:95)(cid:90)(cid:69)(cid:71)(cid:97)(cid:97)(cid:90)(cid:95)(cid:97)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:87)(cid:67)(cid:92)(cid:110) (cid:98)(cid:79)(cid:71)(cid:97)(cid:71)(cid:110) (cid:92)(cid:95)(cid:90)(cid:69)(cid:71)(cid:97)(cid:97)(cid:90)(cid:95)(cid:97)(cid:110) (cid:90)(cid:89)(cid:98)(cid:90)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:47)(cid:90)(cid:33)(cid:110) (cid:67)(cid:95)(cid:69)(cid:79)(cid:80)(cid:98)(cid:71)(cid:69)(cid:98)(cid:100)(cid:95)(cid:71)(cid:110) (cid:76)(cid:95)(cid:97)(cid:98)(cid:11)(cid:110) (cid:30)(cid:72)(cid:98)(cid:71)(cid:95)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:67)(cid:97)(cid:83)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:92)(cid:95)(cid:90)(cid:69)(cid:71)(cid:97)(cid:97)(cid:90)(cid:95)(cid:110) (cid:87)(cid:67)(cid:92)(cid:92)(cid:80)(cid:89)(cid:78)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110) (cid:80)(cid:97)(cid:110) (cid:70)(cid:71)(cid:101)(cid:71)(cid:84)(cid:109) (cid:90)(cid:92)(cid:71)(cid:70)(cid:110)(cid:98)(cid:90)(cid:110)(cid:70)(cid:71)(cid:69)(cid:80)(cid:70)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110)(cid:92)(cid:79)(cid:104)(cid:97)(cid:80)(cid:69)(cid:67)(cid:84)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:97)(cid:71)(cid:89)(cid:70)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110)(cid:92)(cid:67)(cid:69)(cid:83)(cid:71)(cid:98)(cid:97)(cid:110)(cid:72)(cid:95)(cid:90)(cid:87)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:90)(cid:100)(cid:95)(cid:69)(cid:71)(cid:97)(cid:110) (cid:98)(cid:90)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:70)(cid:71)(cid:97)(cid:98)(cid:80)(cid:89)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:97)(cid:11)(cid:110) (cid:35)(cid:67)(cid:69)(cid:79)(cid:110) (cid:92)(cid:79)(cid:67)(cid:97)(cid:71)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110)(cid:47)(cid:90)(cid:33)(cid:110)(cid:70)(cid:71)(cid:97)(cid:80)(cid:78)(cid:89)(cid:110) (cid:67)(cid:73)(cid:71)(cid:69)(cid:98)(cid:97)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:90)(cid:98)(cid:67)(cid:84)(cid:110)(cid:92)(cid:90)(cid:102)(cid:71)(cid:95)(cid:110)(cid:69)(cid:90)(cid:89)(cid:97)(cid:100)(cid:87)(cid:92)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:67)(cid:89)(cid:70)(cid:110)(cid:67)(cid:84)(cid:97)(cid:90)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:90)(cid:102)(cid:71)(cid:95)(cid:110)(cid:92)(cid:95)(cid:90)(cid:76)(cid:84)(cid:71)(cid:110) (cid:67)(cid:69)(cid:95)(cid:90)(cid:97)(cid:97)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:69)(cid:79)(cid:80)(cid:92)(cid:11)(cid:110) (cid:50)(cid:95)(cid:71)(cid:101)(cid:80)(cid:90)(cid:100)(cid:97)(cid:84)(cid:104)(cid:8)(cid:110) (cid:98)(cid:67)(cid:97)(cid:83)(cid:110) (cid:87)(cid:67)(cid:92)(cid:92)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:92)(cid:95)(cid:90)(cid:69)(cid:71)(cid:97)(cid:97)(cid:90)(cid:95)(cid:110) (cid:69)(cid:90)(cid:95)(cid:71)(cid:110) (cid:77)(cid:90)(cid:90)(cid:95)(cid:92)(cid:84)(cid:67)(cid:89)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:97)(cid:110) (cid:62)(cid:19)(cid:64)(cid:110) (cid:79)(cid:67)(cid:101)(cid:71)(cid:110) (cid:68)(cid:71)(cid:71)(cid:89)(cid:110)(cid:92)(cid:95)(cid:90)(cid:92)(cid:90)(cid:97)(cid:71)(cid:70)(cid:110) (cid:98)(cid:90)(cid:110) (cid:67)(cid:69)(cid:79)(cid:80)(cid:71)(cid:101)(cid:71)(cid:110) (cid:67)(cid:110) (cid:98)(cid:79)(cid:71)(cid:95)(cid:109) (cid:87)(cid:67)(cid:84)(cid:110) (cid:68)(cid:67)(cid:84)(cid:67)(cid:89)(cid:69)(cid:71)(cid:70)(cid:110) (cid:47)(cid:90)(cid:33)(cid:110) (cid:70)(cid:71)(cid:97)(cid:80)(cid:78)(cid:89)(cid:11)(cid:110) (cid:39)(cid:90)(cid:102)(cid:71)(cid:101)(cid:71)(cid:95)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110) (cid:80)(cid:97)(cid:110) (cid:95)(cid:67)(cid:95)(cid:71)(cid:84)(cid:104)(cid:110) (cid:71)(cid:103)(cid:92)(cid:84)(cid:90)(cid:80)(cid:98)(cid:71)(cid:70)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:98)(cid:79)(cid:80)(cid:97)(cid:110) (cid:92)(cid:100)(cid:95)(cid:92)(cid:90)(cid:97)(cid:71)(cid:11)(cid:110) (cid:52)(cid:80)(cid:89)(cid:69)(cid:71)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:89)(cid:71)(cid:98)(cid:109) (cid:102)(cid:90)(cid:95)(cid:83)(cid:110) (cid:4)(cid:80)(cid:89)(cid:69)(cid:84)(cid:100)(cid:70)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:97)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:79)(cid:104)(cid:97)(cid:80)(cid:69)(cid:67)(cid:84)(cid:110) (cid:84)(cid:80)(cid:89)(cid:83)(cid:97)(cid:6)(cid:110) (cid:69)(cid:90)(cid:89)(cid:97)(cid:100)(cid:87)(cid:71)(cid:97)(cid:110) (cid:43)(cid:88)(cid:82)(cid:15)(cid:122) (cid:22)(cid:16)(cid:116) (cid:61)(cid:70)(cid:107)(cid:91)(cid:122) (cid:82)(cid:105)(cid:70)(cid:102)(cid:86)(cid:122) (cid:70)(cid:100)(cid:73)(cid:122) (cid:108)(cid:88)(cid:93)(cid:75)(cid:122) (cid:97)(cid:70)(cid:102)(cid:102)(cid:88)(cid:100)(cid:82)(cid:122) (cid:76)(cid:101)(cid:105)(cid:122) (cid:64)(cid:55)(cid:57)(cid:41) (cid:67)(cid:26)(cid:69)(cid:122) (cid:71)(cid:75)(cid:100)(cid:72)(cid:86)(cid:97)(cid:70)(cid:105)(cid:91)(cid:122) (cid:41)(cid:89)(cid:110) (cid:98)(cid:79)(cid:80)(cid:97)(cid:110) (cid:92)(cid:67)(cid:92)(cid:71)(cid:95)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:98)(cid:67)(cid:69)(cid:83)(cid:84)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:92)(cid:95)(cid:90)(cid:68)(cid:84)(cid:71)(cid:87)(cid:110) (cid:98)(cid:90)(cid:110) (cid:67)(cid:69)(cid:79)(cid:80)(cid:71)(cid:101)(cid:71)(cid:110) (cid:67)(cid:89)(cid:110) (cid:71)(cid:101)(cid:71)(cid:89)(cid:110) (cid:98)(cid:71)(cid:87)(cid:92)(cid:71)(cid:95)(cid:67)(cid:98)(cid:100)(cid:95)(cid:71)(cid:110) (cid:70)(cid:80)(cid:97)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:67)(cid:89)(cid:110) (cid:67)(cid:92)(cid:92)(cid:84)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:97)(cid:92)(cid:71)(cid:69)(cid:80)(cid:76)(cid:69)(cid:110) (cid:45)(cid:50)(cid:109) (cid:52)(cid:90)(cid:33)(cid:11)(cid:110) (cid:38)(cid:80)(cid:101)(cid:71)(cid:89)(cid:110) (cid:67)(cid:89)(cid:110) (cid:67)(cid:92)(cid:92)(cid:84)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:67)(cid:97)(cid:97)(cid:100)(cid:87)(cid:71)(cid:110) (cid:98)(cid:79)(cid:67)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:67)(cid:97)(cid:83)(cid:97)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:67)(cid:84)(cid:109) (cid:95)(cid:71)(cid:67)(cid:70)(cid:104)(cid:110) (cid:97)(cid:69)(cid:79)(cid:71)(cid:70)(cid:100)(cid:84)(cid:71)(cid:70)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:67)(cid:84)(cid:84)(cid:90)(cid:69)(cid:67)(cid:98)(cid:71)(cid:70)(cid:110) (cid:98)(cid:90)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:95)(cid:90)(cid:69)(cid:71)(cid:97)(cid:97)(cid:90)(cid:95)(cid:97)(cid:11)(cid:110) (cid:60)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:89)(cid:110) (cid:100)(cid:98)(cid:80)(cid:109) (cid:84)(cid:80)(cid:105)(cid:71)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110)(cid:80)(cid:89)(cid:72)(cid:90)(cid:95)(cid:87)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:90)(cid:72)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:67)(cid:92)(cid:92)(cid:84)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:97)(cid:8)(cid:110) (cid:102)(cid:79)(cid:80)(cid:69)(cid:79)(cid:110)(cid:69)(cid:67)(cid:89)(cid:110)(cid:68)(cid:71)(cid:110)(cid:90)(cid:68)(cid:109) (cid:98)(cid:67)(cid:80)(cid:89)(cid:71)(cid:70)(cid:110) (cid:98)(cid:79)(cid:95)(cid:90)(cid:100)(cid:78)(cid:79)(cid:110)(cid:92)(cid:95)(cid:90)(cid:76)(cid:84)(cid:80)(cid:89)(cid:78)(cid:110) (cid:62)(cid:22)(cid:8)(cid:19)(cid:64)(cid:8)(cid:110) (cid:98)(cid:90)(cid:110)(cid:70)(cid:71)(cid:69)(cid:80)(cid:70)(cid:71)(cid:110)(cid:79)(cid:90)(cid:102)(cid:110) (cid:98)(cid:90)(cid:110) (cid:97)(cid:92)(cid:84)(cid:80)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:67)(cid:87)(cid:90)(cid:89)(cid:78)(cid:110)(cid:70)(cid:80)(cid:73)(cid:71)(cid:95)(cid:71)(cid:89)(cid:98)(cid:110)(cid:92)(cid:79)(cid:104)(cid:97)(cid:80)(cid:69)(cid:67)(cid:84)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:97)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:67)(cid:110) (cid:68)(cid:67)(cid:84)(cid:67)(cid:89)(cid:69)(cid:71)(cid:70)(cid:110)(cid:92)(cid:90)(cid:102)(cid:71)(cid:95)(cid:110)(cid:70)(cid:80)(cid:97)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:109) (cid:98)(cid:80)(cid:90)(cid:89)(cid:11)(cid:110) (cid:36)(cid:80)(cid:78)(cid:100)(cid:95)(cid:71)(cid:110) (cid:16)(cid:110) (cid:97)(cid:79)(cid:90)(cid:102)(cid:97)(cid:110) (cid:67)(cid:89)(cid:110) (cid:71)(cid:103)(cid:67)(cid:87)(cid:92)(cid:84)(cid:71)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:67)(cid:97)(cid:83)(cid:110) (cid:77)(cid:90)(cid:102)(cid:110) (cid:78)(cid:95)(cid:67)(cid:92)(cid:79)(cid:110) (cid:102)(cid:80)(cid:98)(cid:79)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:68)(cid:67)(cid:89)(cid:70)(cid:102)(cid:80)(cid:70)(cid:98)(cid:79)(cid:110) (cid:95)(cid:71)(cid:94)(cid:100)(cid:80)(cid:95)(cid:71)(cid:87)(cid:71)(cid:89)(cid:98)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:69)(cid:90)(cid:95)(cid:95)(cid:71)(cid:97)(cid:92)(cid:90)(cid:89)(cid:70)(cid:109) (cid:80)(cid:89)(cid:78)(cid:110) (cid:87)(cid:71)(cid:97)(cid:79)(cid:110) (cid:98)(cid:90)(cid:92)(cid:90)(cid:84)(cid:90)(cid:78)(cid:104)(cid:110) (cid:87)(cid:67)(cid:92)(cid:92)(cid:80)(cid:89)(cid:78)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:67)(cid:110) (cid:101)(cid:80)(cid:70)(cid:71)(cid:90)(cid:110) (cid:90)(cid:68)(cid:82)(cid:71)(cid:69)(cid:98)(cid:110) (cid:92)(cid:84)(cid:67)(cid:89)(cid:71)(cid:110) (cid:70)(cid:71)(cid:69)(cid:90)(cid:70)(cid:71)(cid:95)(cid:110) (cid:4)(cid:59)(cid:48)(cid:50)(cid:34)(cid:6)(cid:110)(cid:67)(cid:92)(cid:92)(cid:84)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:62)(cid:19)(cid:64)(cid:11)(cid:110) (cid:60)(cid:79)(cid:71)(cid:89)(cid:110)(cid:102)(cid:71)(cid:110) (cid:70)(cid:71)(cid:69)(cid:80)(cid:70)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110)(cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:84)(cid:80)(cid:89)(cid:83)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:79)(cid:67)(cid:101)(cid:71)(cid:110) (cid:98)(cid:90)(cid:110) (cid:87)(cid:67)(cid:83)(cid:71)(cid:110) (cid:97)(cid:100)(cid:95)(cid:71)(cid:110) (cid:67)(cid:84)(cid:84)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:70)(cid:71)(cid:67)(cid:70)(cid:84)(cid:90)(cid:69)(cid:83)(cid:10)(cid:72)(cid:95)(cid:71)(cid:71)(cid:11)(cid:110) (cid:41)(cid:89)(cid:110) (cid:98)(cid:79)(cid:80)(cid:97)(cid:110) (cid:102)(cid:90)(cid:95)(cid:83)(cid:8)(cid:110) (cid:97)(cid:80)(cid:87)(cid:80)(cid:84)(cid:67)(cid:95)(cid:110) (cid:98)(cid:90)(cid:110) (cid:62)(cid:23)(cid:64)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:100)(cid:97)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:71)(cid:67)(cid:83)(cid:110) (cid:92)(cid:90)(cid:102)(cid:71)(cid:95)(cid:110) (cid:4)(cid:90)(cid:95)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:71)(cid:67)(cid:83)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:80)(cid:89)(cid:110) (cid:67)(cid:110) (cid:78)(cid:80)(cid:101)(cid:71)(cid:89)(cid:110) (cid:98)(cid:80)(cid:87)(cid:71)(cid:110) (cid:102)(cid:80)(cid:89)(cid:70)(cid:90)(cid:102)(cid:6)(cid:110) (cid:87)(cid:71)(cid:98)(cid:95)(cid:80)(cid:69)(cid:97)(cid:110) (cid:98)(cid:90)(cid:110) (cid:71)(cid:101)(cid:67)(cid:84)(cid:100)(cid:67)(cid:98)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:71)(cid:73)(cid:71)(cid:69)(cid:98)(cid:80)(cid:101)(cid:71)(cid:89)(cid:71)(cid:97)(cid:97)(cid:110) (cid:80)(cid:89)(cid:110) (cid:95)(cid:71)(cid:70)(cid:100)(cid:69)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:79)(cid:90)(cid:98)(cid:10)(cid:97)(cid:92)(cid:90)(cid:98)(cid:110) (cid:98)(cid:71)(cid:87)(cid:92)(cid:71)(cid:95)(cid:67)(cid:98)(cid:100)(cid:95)(cid:71)(cid:11)(cid:110) (cid:53)(cid:79)(cid:95)(cid:90)(cid:100)(cid:78)(cid:79)(cid:110) (cid:97)(cid:80)(cid:87)(cid:100)(cid:84)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:70)(cid:71)(cid:87)(cid:90)(cid:89)(cid:97)(cid:98)(cid:95)(cid:67)(cid:98)(cid:71)(cid:110) (cid:98)(cid:79)(cid:67)(cid:98)(cid:110) (cid:90)(cid:100)(cid:95)(cid:110) (cid:92)(cid:95)(cid:90)(cid:92)(cid:90)(cid:97)(cid:71)(cid:70)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110) (cid:69)(cid:67)(cid:89)(cid:110) (cid:95)(cid:71)(cid:70)(cid:100)(cid:69)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:71)(cid:67)(cid:83)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:80)(cid:84)(cid:71)(cid:97)(cid:110) (cid:67)(cid:97)(cid:110) (cid:79)(cid:80)(cid:78)(cid:79)(cid:110) (cid:67)(cid:97)(cid:110) (cid:16)(cid:23)(cid:11) (cid:23)(cid:2)(cid:110) (cid:102)(cid:79)(cid:80)(cid:84)(cid:71)(cid:110) (cid:80)(cid:87)(cid:92)(cid:95)(cid:90)(cid:101)(cid:80)(cid:89)(cid:78)(cid:110) (cid:90)(cid:95)(cid:110) (cid:87)(cid:67)(cid:80)(cid:89)(cid:98)(cid:67)(cid:80)(cid:89)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:79)(cid:95)(cid:90)(cid:100)(cid:78)(cid:79)(cid:92)(cid:100)(cid:98)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:84)(cid:67)(cid:98)(cid:71)(cid:89)(cid:69)(cid:104)(cid:110) (cid:92)(cid:71)(cid:95)(cid:72)(cid:90)(cid:95)(cid:109) (cid:87)(cid:67)(cid:89)(cid:69)(cid:71)(cid:11)(cid:110) (cid:35)(cid:35)(cid:13)(cid:136) (cid:49)(cid:50)(cid:29)(cid:60)(cid:36)(cid:45)(cid:57)(cid:53)(cid:136) (cid:62)(cid:45)(cid:50)(cid:37)(cid:136) (cid:53)(cid:79)(cid:71)(cid:95)(cid:71)(cid:110) (cid:79)(cid:67)(cid:101)(cid:71)(cid:110) (cid:68)(cid:71)(cid:71)(cid:89)(cid:110) (cid:67)(cid:110) (cid:84)(cid:90)(cid:98)(cid:110) (cid:90)(cid:72)(cid:110) (cid:102)(cid:90)(cid:95)(cid:83)(cid:97)(cid:110) (cid:90)(cid:89)(cid:110) (cid:47)(cid:90)(cid:33)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:97)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:101)(cid:67)(cid:95)(cid:80)(cid:90)(cid:100)(cid:97)(cid:110) (cid:92)(cid:100)(cid:95)(cid:92)(cid:90)(cid:97)(cid:71)(cid:97)(cid:110) (cid:80)(cid:89)(cid:69)(cid:84)(cid:100)(cid:70)(cid:80)(cid:89)(cid:78)(cid:110) (cid:84)(cid:90)(cid:102)(cid:110) (cid:92)(cid:90)(cid:102)(cid:71)(cid:95)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:62)(cid:24)(cid:64)(cid:8)(cid:110) (cid:72)(cid:67)(cid:100)(cid:84)(cid:98)(cid:109) (cid:98)(cid:90)(cid:84)(cid:71)(cid:95)(cid:67)(cid:89)(cid:98)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:62)(cid:25)(cid:64)(cid:110) (cid:71)(cid:98)(cid:69)(cid:11)(cid:110) (cid:39)(cid:90)(cid:102)(cid:71)(cid:101)(cid:71)(cid:95)(cid:8)(cid:110) (cid:90)(cid:89)(cid:84)(cid:104)(cid:110) (cid:67)(cid:110) (cid:72)(cid:71)(cid:102)(cid:110) (cid:102)(cid:90)(cid:95)(cid:83)(cid:97)(cid:110) (cid:62)(cid:23)(cid:64)(cid:62)(cid:17)(cid:64)(cid:110) (cid:98)(cid:67)(cid:83)(cid:71)(cid:110) (cid:35)(cid:33)(cid:34)(cid:8)(cid:23)(cid:8)(cid:27)(cid:25)(cid:27)(cid:27)(cid:8)(cid:33)(cid:30)(cid:23)(cid:31)(cid:8)(cid:25)(cid:18)(cid:23)(cid:23)(cid:18)(cid:1)(cid:25)(cid:31)(cid:17)(cid:19)(cid:19)(cid:116)(cid:115)(cid:25)(cid:19)(cid:23)(cid:23)(cid:116)(cid:55)(cid:51)(cid:51)(cid:51)(cid:116) (cid:18)(cid:18)(cid:20)(cid:136) (cid:32)(cid:34)(cid:20)(cid:31)(cid:137) (cid:98)(cid:71)(cid:87)(cid:92)(cid:71)(cid:95)(cid:67)(cid:98)(cid:100)(cid:95)(cid:71)(cid:110) (cid:80)(cid:97)(cid:97)(cid:100)(cid:71)(cid:110) (cid:80)(cid:89)(cid:98)(cid:90)(cid:110) (cid:67)(cid:69)(cid:69)(cid:90)(cid:100)(cid:89)(cid:98)(cid:11)(cid:110) (cid:41)(cid:89)(cid:110) (cid:62)(cid:23)(cid:64)(cid:8)(cid:110) (cid:67)(cid:89)(cid:110) (cid:67)(cid:89)(cid:98)(cid:10)(cid:69)(cid:90)(cid:84)(cid:90)(cid:89)(cid:104)(cid:10)(cid:68)(cid:67)(cid:97)(cid:71)(cid:70)(cid:110) (cid:70)(cid:104)(cid:89)(cid:67)(cid:87)(cid:80)(cid:69)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110) (cid:80)(cid:97)(cid:110) (cid:92)(cid:95)(cid:90)(cid:92)(cid:90)(cid:97)(cid:71)(cid:70)(cid:110)(cid:98)(cid:90)(cid:110) (cid:95)(cid:71)(cid:70)(cid:100)(cid:69)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:71)(cid:67)(cid:83)(cid:110) (cid:92)(cid:90)(cid:102)(cid:71)(cid:95)(cid:11)(cid:110) (cid:39)(cid:71)(cid:67)(cid:101)(cid:104)(cid:110) (cid:92)(cid:67)(cid:69)(cid:83)(cid:71)(cid:98)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:97)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:70)(cid:80)(cid:97)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:98)(cid:71)(cid:70)(cid:110) (cid:90)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:69)(cid:79)(cid:80)(cid:92)(cid:110) (cid:68)(cid:67)(cid:97)(cid:71)(cid:70)(cid:110) (cid:90)(cid:89)(cid:110) (cid:98)(cid:79)(cid:80)(cid:97)(cid:110) (cid:70)(cid:104)(cid:89)(cid:67)(cid:87)(cid:80)(cid:69)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:109) (cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110)(cid:98)(cid:90)(cid:110)(cid:87)(cid:80)(cid:89)(cid:80)(cid:87)(cid:80)(cid:105)(cid:71)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:90)(cid:69)(cid:69)(cid:100)(cid:95)(cid:95)(cid:71)(cid:89)(cid:69)(cid:71)(cid:110)(cid:90)(cid:72)(cid:110)(cid:79)(cid:90)(cid:98)(cid:10)(cid:97)(cid:92)(cid:90)(cid:98)(cid:97)(cid:11)(cid:110) (cid:53)(cid:79)(cid:80)(cid:97)(cid:110)(cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110) (cid:80)(cid:97)(cid:110) (cid:78)(cid:71)(cid:89)(cid:71)(cid:95)(cid:80)(cid:69)(cid:110) (cid:80)(cid:89)(cid:110) (cid:89)(cid:67)(cid:98)(cid:100)(cid:95)(cid:71)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:70)(cid:90)(cid:71)(cid:97)(cid:110) (cid:89)(cid:90)(cid:98)(cid:110) (cid:98)(cid:67)(cid:83)(cid:71)(cid:110) (cid:80)(cid:89)(cid:98)(cid:90)(cid:110) (cid:67)(cid:69)(cid:69)(cid:90)(cid:100)(cid:89)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:92)(cid:71)(cid:109) (cid:69)(cid:80)(cid:76)(cid:69)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:80)(cid:89)(cid:72)(cid:90)(cid:95)(cid:87)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:67)(cid:92)(cid:92)(cid:84)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:97)(cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:95)(cid:71)(cid:72)(cid:90)(cid:95)(cid:71)(cid:110) (cid:80)(cid:98)(cid:110) (cid:87)(cid:67)(cid:104)(cid:110) (cid:89)(cid:90)(cid:98)(cid:110) (cid:68)(cid:71)(cid:110) (cid:67)(cid:68)(cid:84)(cid:71)(cid:110) (cid:98)(cid:90)(cid:110) (cid:67)(cid:69)(cid:79)(cid:80)(cid:71)(cid:101)(cid:71)(cid:110) (cid:67)(cid:89)(cid:110) (cid:90)(cid:92)(cid:98)(cid:80)(cid:87)(cid:67)(cid:84)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:110) (cid:70)(cid:80)(cid:97)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:98)(cid:80)(cid:90)(cid:89)(cid:11)(cid:110) (cid:52)(cid:92)(cid:71)(cid:69)(cid:80)(cid:67)(cid:84)(cid:110) (cid:69)(cid:90)(cid:89)(cid:98)(cid:95)(cid:90)(cid:84)(cid:110) (cid:92)(cid:67)(cid:69)(cid:83)(cid:71)(cid:98)(cid:97)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:97)(cid:71)(cid:89)(cid:98)(cid:110) (cid:67)(cid:87)(cid:90)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:97)(cid:110) (cid:98)(cid:90)(cid:110) (cid:80)(cid:87)(cid:92)(cid:84)(cid:71)(cid:87)(cid:71)(cid:89)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110) (cid:102)(cid:79)(cid:80)(cid:69)(cid:79)(cid:110) (cid:80)(cid:89)(cid:69)(cid:95)(cid:71)(cid:67)(cid:97)(cid:71)(cid:97)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:90)(cid:102)(cid:71)(cid:95)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:67)(cid:95)(cid:71)(cid:67)(cid:110) (cid:90)(cid:101)(cid:71)(cid:95)(cid:79)(cid:71)(cid:67)(cid:70)(cid:11)(cid:110) (cid:41)(cid:89)(cid:110) (cid:62)(cid:17)(cid:64)(cid:8)(cid:110) (cid:67)(cid:110) (cid:95)(cid:100)(cid:89)(cid:10)(cid:98)(cid:80)(cid:87)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:95)(cid:87)(cid:67)(cid:84)(cid:10)(cid:69)(cid:90)(cid:95)(cid:95)(cid:71)(cid:84)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:10)(cid:68)(cid:67)(cid:97)(cid:71)(cid:70)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110) (cid:80)(cid:97)(cid:110) (cid:92)(cid:95)(cid:90)(cid:92)(cid:90)(cid:97)(cid:71)(cid:70)(cid:11)(cid:110) (cid:60)(cid:79)(cid:71)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:71)(cid:67)(cid:83)(cid:110) (cid:98)(cid:71)(cid:87)(cid:92)(cid:71)(cid:95)(cid:67)(cid:98)(cid:100)(cid:95)(cid:71)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:69)(cid:79)(cid:80)(cid:92)(cid:110)(cid:71)(cid:103)(cid:69)(cid:71)(cid:71)(cid:70)(cid:97)(cid:110) (cid:67)(cid:110) (cid:98)(cid:79)(cid:95)(cid:71)(cid:97)(cid:79)(cid:90)(cid:84)(cid:70)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:47)(cid:90)(cid:33)(cid:110) (cid:80)(cid:97)(cid:110) (cid:100)(cid:89)(cid:70)(cid:71)(cid:95)(cid:110) (cid:98)(cid:79)(cid:71)(cid:95)(cid:87)(cid:67)(cid:84)(cid:110) (cid:71)(cid:87)(cid:71)(cid:95)(cid:78)(cid:71)(cid:89)(cid:69)(cid:104)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:70)(cid:104)(cid:109) (cid:89)(cid:67)(cid:87)(cid:80)(cid:69)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110) (cid:102)(cid:80)(cid:84)(cid:84)(cid:110) (cid:98)(cid:79)(cid:95)(cid:90)(cid:98)(cid:98)(cid:84)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:84)(cid:90)(cid:67)(cid:70)(cid:110) (cid:90)(cid:95)(cid:110) (cid:95)(cid:71)(cid:10)(cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:67)(cid:69)(cid:83)(cid:71)(cid:98)(cid:97)(cid:110) (cid:100)(cid:97)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110) (cid:98)(cid:79)(cid:67)(cid:98)(cid:110) (cid:79)(cid:67)(cid:101)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:84)(cid:71)(cid:67)(cid:97)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:95)(cid:87)(cid:67)(cid:84)(cid:110) (cid:69)(cid:90)(cid:95)(cid:95)(cid:71)(cid:84)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:102)(cid:80)(cid:98)(cid:79)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110)(cid:95)(cid:100)(cid:89)(cid:98)(cid:80)(cid:87)(cid:71)(cid:110)(cid:79)(cid:90)(cid:98)(cid:98)(cid:71)(cid:97)(cid:98)(cid:110)(cid:95)(cid:71)(cid:78)(cid:80)(cid:90)(cid:89)(cid:97)(cid:11)(cid:110) (cid:39)(cid:90)(cid:102)(cid:71)(cid:101)(cid:71)(cid:95)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110)(cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110)(cid:67)(cid:84)(cid:97)(cid:90)(cid:110)(cid:70)(cid:90)(cid:71)(cid:97)(cid:110) (cid:89)(cid:90)(cid:98)(cid:110)(cid:69)(cid:90)(cid:89)(cid:97)(cid:80)(cid:70)(cid:71)(cid:95)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:97)(cid:92)(cid:71)(cid:69)(cid:80)(cid:76)(cid:69)(cid:110)(cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110)(cid:80)(cid:89)(cid:72)(cid:90)(cid:95)(cid:87)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:90)(cid:72)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:67)(cid:92)(cid:92)(cid:84)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:97)(cid:11)(cid:110) (cid:41)(cid:98)(cid:110)(cid:70)(cid:90)(cid:71)(cid:97)(cid:110)(cid:89)(cid:90)(cid:98)(cid:110) (cid:69)(cid:84)(cid:71)(cid:67)(cid:95)(cid:84)(cid:104)(cid:110)(cid:70)(cid:71)(cid:97)(cid:69)(cid:95)(cid:80)(cid:68)(cid:71)(cid:110)(cid:79)(cid:90)(cid:102)(cid:110) (cid:98)(cid:90)(cid:110)(cid:70)(cid:90)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:71)(cid:10)(cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:102)(cid:79)(cid:80)(cid:84)(cid:71)(cid:110)(cid:97)(cid:98)(cid:80)(cid:84)(cid:84)(cid:110) (cid:78)(cid:100)(cid:67)(cid:95)(cid:67)(cid:89)(cid:98)(cid:71)(cid:71)(cid:80)(cid:89)(cid:78)(cid:110) (cid:70)(cid:71)(cid:67)(cid:70)(cid:84)(cid:90)(cid:69)(cid:83)(cid:110) (cid:72)(cid:95)(cid:71)(cid:71)(cid:11)(cid:110) (cid:41)(cid:89)(cid:110) (cid:98)(cid:79)(cid:80)(cid:97)(cid:110) (cid:102)(cid:90)(cid:95)(cid:83)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:72)(cid:90)(cid:69)(cid:100)(cid:97)(cid:110) (cid:90)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:95)(cid:87)(cid:67)(cid:84)(cid:10)(cid:67)(cid:102)(cid:67)(cid:95)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:67)(cid:92)(cid:109) (cid:92)(cid:84)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:97)(cid:92)(cid:71)(cid:69)(cid:80)(cid:76)(cid:69)(cid:110) (cid:47)(cid:90)(cid:33)(cid:11)(cid:110) (cid:53)(cid:90)(cid:110) (cid:78)(cid:100)(cid:67)(cid:95)(cid:67)(cid:89)(cid:98)(cid:71)(cid:71)(cid:110) (cid:70)(cid:71)(cid:67)(cid:70)(cid:84)(cid:90)(cid:69)(cid:83)(cid:10)(cid:72)(cid:95)(cid:71)(cid:71)(cid:110) (cid:92)(cid:95)(cid:90)(cid:92)(cid:71)(cid:95)(cid:98)(cid:104)(cid:8)(cid:110) (cid:78)(cid:71)(cid:89)(cid:71)(cid:95)(cid:80)(cid:69)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:97)(cid:69)(cid:79)(cid:71)(cid:87)(cid:71)(cid:97)(cid:110) (cid:100)(cid:97)(cid:71)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:97)(cid:110) (cid:97)(cid:100)(cid:69)(cid:79)(cid:110) (cid:67)(cid:97)(cid:110) (cid:98)(cid:100)(cid:95)(cid:89)(cid:110) (cid:87)(cid:90)(cid:70)(cid:71)(cid:84)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:62)(cid:16)(cid:64)(cid:11)(cid:110) (cid:39)(cid:90)(cid:102)(cid:71)(cid:101)(cid:71)(cid:95)(cid:8)(cid:110) (cid:98)(cid:79)(cid:80)(cid:97)(cid:110) (cid:102)(cid:80)(cid:84)(cid:84)(cid:110) (cid:84)(cid:80)(cid:87)(cid:80)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:77)(cid:71)(cid:103)(cid:80)(cid:68)(cid:80)(cid:84)(cid:80)(cid:98)(cid:104)(cid:110) (cid:90)(cid:72)(cid:110) (cid:95)(cid:71)(cid:109) (cid:70)(cid:80)(cid:97)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:98)(cid:90)(cid:110) (cid:67)(cid:69)(cid:79)(cid:80)(cid:71)(cid:101)(cid:71)(cid:110) (cid:67)(cid:89)(cid:110) (cid:71)(cid:101)(cid:71)(cid:89)(cid:110)(cid:92)(cid:90)(cid:102)(cid:71)(cid:95)(cid:110) (cid:92)(cid:95)(cid:90)(cid:76)(cid:84)(cid:71)(cid:11)(cid:110) (cid:39)(cid:71)(cid:95)(cid:71)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:100)(cid:98)(cid:80)(cid:84)(cid:80)(cid:105)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:69)(cid:79)(cid:67)(cid:95)(cid:67)(cid:69)(cid:98)(cid:71)(cid:95)(cid:80)(cid:105)(cid:71)(cid:70)(cid:110) (cid:67)(cid:92)(cid:92)(cid:84)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:80)(cid:89)(cid:72)(cid:90)(cid:95)(cid:87)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:98)(cid:90)(cid:110) (cid:67)(cid:69)(cid:79)(cid:80)(cid:71)(cid:101)(cid:71)(cid:110) (cid:67)(cid:110) (cid:84)(cid:67)(cid:95)(cid:78)(cid:71)(cid:95)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:110) (cid:97)(cid:71)(cid:98)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:67)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:67)(cid:87)(cid:71)(cid:110) (cid:98)(cid:80)(cid:87)(cid:71)(cid:110) (cid:92)(cid:95)(cid:90)(cid:101)(cid:80)(cid:70)(cid:71)(cid:110) (cid:70)(cid:71)(cid:67)(cid:70)(cid:84)(cid:90)(cid:69)(cid:83)(cid:110) (cid:67)(cid:101)(cid:90)(cid:80)(cid:70)(cid:67)(cid:89)(cid:69)(cid:71)(cid:11)(cid:110) (cid:38)(cid:80)(cid:101)(cid:71)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:71)(cid:98)(cid:110) (cid:90)(cid:72)(cid:110) (cid:92)(cid:90)(cid:97)(cid:97)(cid:80)(cid:68)(cid:84)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110)(cid:72)(cid:90)(cid:95)(cid:87)(cid:100)(cid:84)(cid:67)(cid:98)(cid:71)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:92)(cid:95)(cid:90)(cid:68)(cid:84)(cid:71)(cid:87)(cid:110)(cid:90)(cid:72)(cid:110)(cid:67)(cid:84)(cid:84)(cid:90)(cid:69)(cid:67)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:90)(cid:92)(cid:98)(cid:80)(cid:87)(cid:67)(cid:84)(cid:110)(cid:98)(cid:95)(cid:67)(cid:72)(cid:109) (cid:76)(cid:69)(cid:110) (cid:67)(cid:87)(cid:90)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:84)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110) (cid:67)(cid:97)(cid:110) (cid:67)(cid:110)(cid:87)(cid:67)(cid:98)(cid:79)(cid:71)(cid:87)(cid:67)(cid:98)(cid:80)(cid:69)(cid:67)(cid:84)(cid:110) (cid:92)(cid:95)(cid:90)(cid:78)(cid:95)(cid:67)(cid:87)(cid:87)(cid:80)(cid:89)(cid:78)(cid:110) (cid:92)(cid:95)(cid:90)(cid:68)(cid:84)(cid:71)(cid:87)(cid:11)(cid:110) (cid:33)(cid:33)(cid:33)(cid:11)(cid:136) (cid:39)(cid:29)(cid:55)(cid:32)(cid:45)(cid:27)(cid:45)(cid:38)(cid:45)(cid:31)(cid:64)(cid:136) (cid:46)(cid:60)(cid:29)(cid:50)(cid:60)(cid:36)(cid:29)(cid:63)(cid:136) (cid:60)(cid:71)(cid:110) (cid:67)(cid:97)(cid:97)(cid:100)(cid:87)(cid:71)(cid:110) (cid:98)(cid:79)(cid:67)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:78)(cid:80)(cid:101)(cid:71)(cid:89)(cid:110) (cid:67)(cid:92)(cid:92)(cid:84)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:80)(cid:97)(cid:110) (cid:97)(cid:92)(cid:71)(cid:69)(cid:80)(cid:76)(cid:71)(cid:70)(cid:110) (cid:68)(cid:104)(cid:110) (cid:67)(cid:110) (cid:98)(cid:67)(cid:97)(cid:83)(cid:110) (cid:77)(cid:90)(cid:102)(cid:110) (cid:78)(cid:95)(cid:67)(cid:92)(cid:79)(cid:110) (cid:102)(cid:79)(cid:80)(cid:69)(cid:79)(cid:110) (cid:69)(cid:79)(cid:67)(cid:95)(cid:67)(cid:69)(cid:98)(cid:71)(cid:95)(cid:80)(cid:105)(cid:71)(cid:97)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:70)(cid:71)(cid:92)(cid:71)(cid:89)(cid:70)(cid:71)(cid:89)(cid:109) (cid:69)(cid:80)(cid:71)(cid:97)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:68)(cid:67)(cid:89)(cid:70)(cid:102)(cid:80)(cid:70)(cid:98)(cid:79)(cid:110) (cid:95)(cid:71)(cid:94)(cid:100)(cid:80)(cid:95)(cid:71)(cid:87)(cid:71)(cid:89)(cid:98)(cid:97)(cid:110) (cid:67)(cid:87)(cid:90)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:67)(cid:97)(cid:83)(cid:97)(cid:11) (cid:30)(cid:110) (cid:98)(cid:80)(cid:84)(cid:71)(cid:109) (cid:68)(cid:67)(cid:97)(cid:71)(cid:70)(cid:110)(cid:87)(cid:71)(cid:97)(cid:79)(cid:110)(cid:98)(cid:90)(cid:92)(cid:90)(cid:84)(cid:90)(cid:78)(cid:104)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110)(cid:47)(cid:90)(cid:33)(cid:110) (cid:80)(cid:97)(cid:110)(cid:100)(cid:97)(cid:71)(cid:70)(cid:110)(cid:70)(cid:100)(cid:71)(cid:110) (cid:98)(cid:90)(cid:110) (cid:80)(cid:98)(cid:97)(cid:110)(cid:92)(cid:90)(cid:92)(cid:100)(cid:84)(cid:67)(cid:95)(cid:80)(cid:98)(cid:104)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:97)(cid:80)(cid:87)(cid:92)(cid:84)(cid:80)(cid:69)(cid:80)(cid:98)(cid:104)(cid:11)(cid:110) (cid:39)(cid:90)(cid:102)(cid:71)(cid:101)(cid:71)(cid:95)(cid:110) (cid:90)(cid:100)(cid:95)(cid:110) (cid:87)(cid:71)(cid:98)(cid:79)(cid:90)(cid:70)(cid:90)(cid:84)(cid:90)(cid:78)(cid:104)(cid:110) (cid:69)(cid:67)(cid:89)(cid:110) (cid:68)(cid:71)(cid:110) (cid:71)(cid:67)(cid:97)(cid:80)(cid:84)(cid:104)(cid:110) (cid:71)(cid:103)(cid:109) (cid:98)(cid:71)(cid:89)(cid:70)(cid:71)(cid:70)(cid:110) (cid:98)(cid:90)(cid:110) (cid:90)(cid:98)(cid:79)(cid:71)(cid:95)(cid:110) (cid:80)(cid:95)(cid:95)(cid:71)(cid:78)(cid:100)(cid:84)(cid:67)(cid:95)(cid:110) (cid:98)(cid:90)(cid:92)(cid:90)(cid:84)(cid:90)(cid:78)(cid:80)(cid:71)(cid:97)(cid:11)(cid:110) (cid:32)(cid:67)(cid:97)(cid:71)(cid:70)(cid:110) (cid:90)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:67)(cid:97)(cid:83)(cid:110) (cid:77)(cid:90)(cid:102)(cid:110) (cid:78)(cid:95)(cid:67)(cid:92)(cid:79)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:67)(cid:95)(cid:78)(cid:71)(cid:98)(cid:110) (cid:98)(cid:90)(cid:92)(cid:90)(cid:84)(cid:90)(cid:78)(cid:104)(cid:8)(cid:110) (cid:98)(cid:67)(cid:97)(cid:83)(cid:110) (cid:67)(cid:84)(cid:84)(cid:90)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:92)(cid:95)(cid:90)(cid:69)(cid:71)(cid:97)(cid:97)(cid:90)(cid:95)(cid:110) (cid:87)(cid:67)(cid:92)(cid:92)(cid:80)(cid:89)(cid:78)(cid:110) (cid:69)(cid:67)(cid:89)(cid:110) (cid:68)(cid:71)(cid:110) (cid:70)(cid:71)(cid:98)(cid:71)(cid:95)(cid:87)(cid:80)(cid:89)(cid:71)(cid:70)(cid:110) (cid:68)(cid:71)(cid:72)(cid:90)(cid:95)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:92)(cid:79)(cid:67)(cid:97)(cid:71)(cid:11)(cid:110) (cid:53)(cid:67)(cid:83)(cid:109) (cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:84)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:67)(cid:68)(cid:90)(cid:101)(cid:71)(cid:110) (cid:80)(cid:89)(cid:72)(cid:90)(cid:95)(cid:87)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:67)(cid:97)(cid:110) (cid:80)(cid:89)(cid:92)(cid:100)(cid:98)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:67)(cid:70)(cid:70)(cid:95)(cid:71)(cid:97)(cid:97)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:80)(cid:97)(cid:97)(cid:100)(cid:71)(cid:110) (cid:90)(cid:72)(cid:110) (cid:95)(cid:71)(cid:70)(cid:100)(cid:69)(cid:80)(cid:89)(cid:78)(cid:110) (cid:92)(cid:71)(cid:67)(cid:83)(cid:110) (cid:98)(cid:71)(cid:87)(cid:92)(cid:71)(cid:95)(cid:67)(cid:98)(cid:100)(cid:95)(cid:71)(cid:110) (cid:101)(cid:80)(cid:67)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110) (cid:70)(cid:71)(cid:97)(cid:80)(cid:78)(cid:89)(cid:11)(cid:110) (cid:60)(cid:71)(cid:110) (cid:67)(cid:70)(cid:90)(cid:92)(cid:98)(cid:110) (cid:67)(cid:89)(cid:110) (cid:67)(cid:70)(cid:67)(cid:92)(cid:98)(cid:80)(cid:101)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:97)(cid:98)(cid:95)(cid:67)(cid:98)(cid:71)(cid:78)(cid:104)(cid:110) (cid:80)(cid:89)(cid:97)(cid:98)(cid:71)(cid:67)(cid:70)(cid:110) (cid:90)(cid:72)(cid:110) (cid:97)(cid:98)(cid:67)(cid:98)(cid:80)(cid:69)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:109) (cid:80)(cid:89)(cid:78)(cid:110) (cid:97)(cid:69)(cid:79)(cid:71)(cid:87)(cid:71)(cid:110)(cid:68)(cid:71)(cid:69)(cid:67)(cid:100)(cid:97)(cid:71)(cid:110) (cid:80)(cid:98)(cid:110) (cid:92)(cid:95)(cid:90)(cid:101)(cid:80)(cid:70)(cid:71)(cid:97)(cid:110)(cid:92)(cid:67)(cid:98)(cid:79)(cid:110)(cid:70)(cid:80)(cid:101)(cid:71)(cid:95)(cid:97)(cid:80)(cid:98)(cid:104)(cid:110) (cid:98)(cid:90)(cid:110)(cid:70)(cid:80)(cid:97)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:98)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:62)(cid:16)(cid:64)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:67)(cid:69)(cid:79)(cid:80)(cid:71)(cid:101)(cid:71)(cid:97)(cid:110) (cid:68)(cid:71)(cid:98)(cid:98)(cid:71)(cid:95)(cid:110) (cid:92)(cid:71)(cid:95)(cid:72)(cid:90)(cid:95)(cid:87)(cid:67)(cid:89)(cid:69)(cid:71)(cid:110) (cid:80)(cid:89)(cid:110) (cid:98)(cid:71)(cid:95)(cid:87)(cid:97)(cid:110) (cid:90)(cid:72)(cid:110) (cid:84)(cid:67)(cid:98)(cid:71)(cid:89)(cid:69)(cid:104)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:95)(cid:90)(cid:100)(cid:78)(cid:79)(cid:92)(cid:100)(cid:98)(cid:11)(cid:110) (cid:36)(cid:90)(cid:95)(cid:110) (cid:67)(cid:70)(cid:67)(cid:92)(cid:98)(cid:80)(cid:101)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:8)(cid:110) (cid:89)(cid:90)(cid:95)(cid:87)(cid:67)(cid:84)(cid:84)(cid:104)(cid:110) (cid:98)(cid:79)(cid:71)(cid:95)(cid:71)(cid:110) (cid:102)(cid:80)(cid:84)(cid:84)(cid:110)(cid:68)(cid:71)(cid:110)(cid:87)(cid:90)(cid:95)(cid:71)(cid:110) (cid:98)(cid:79)(cid:67)(cid:89)(cid:110) (cid:90)(cid:89)(cid:71)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:110) (cid:92)(cid:95)(cid:90)(cid:101)(cid:80)(cid:70)(cid:71)(cid:70)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:71)(cid:101)(cid:71)(cid:95)(cid:104)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:92)(cid:67)(cid:80)(cid:95)(cid:11)(cid:110) (cid:41)(cid:72)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:80)(cid:97)(cid:110) (cid:70)(cid:80)(cid:97)(cid:109) (cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:98)(cid:71)(cid:70)(cid:110)(cid:71)(cid:94)(cid:100)(cid:67)(cid:84)(cid:84)(cid:104)(cid:110)(cid:90)(cid:89)(cid:110)(cid:67)(cid:84)(cid:84)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:8)(cid:110) (cid:97)(cid:90)(cid:87)(cid:71)(cid:110)(cid:90)(cid:72)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:97)(cid:110)(cid:87)(cid:67)(cid:104)(cid:110)(cid:79)(cid:67)(cid:101)(cid:71)(cid:110) (cid:87)(cid:90)(cid:95)(cid:71)(cid:110) (cid:92)(cid:67)(cid:69)(cid:83)(cid:71)(cid:98)(cid:97)(cid:110) (cid:98)(cid:90)(cid:110) (cid:92)(cid:95)(cid:90)(cid:69)(cid:71)(cid:97)(cid:97)(cid:11)(cid:110) (cid:60)(cid:71)(cid:110) (cid:97)(cid:79)(cid:90)(cid:102)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:89)(cid:71)(cid:71)(cid:70)(cid:110) (cid:98)(cid:90)(cid:110) (cid:67)(cid:84)(cid:84)(cid:90)(cid:69)(cid:67)(cid:98)(cid:71)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:92)(cid:95)(cid:90)(cid:92)(cid:71)(cid:95)(cid:84)(cid:104)(cid:110) (cid:67)(cid:87)(cid:90)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110) (cid:68)(cid:104)(cid:110) (cid:67)(cid:89)(cid:110) (cid:71)(cid:103)(cid:67)(cid:87)(cid:92)(cid:84)(cid:71)(cid:110) (cid:80)(cid:84)(cid:84)(cid:100)(cid:97)(cid:98)(cid:95)(cid:67)(cid:98)(cid:71)(cid:70)(cid:110) (cid:80)(cid:89)(cid:110) (cid:76)(cid:78)(cid:100)(cid:95)(cid:71)(cid:110) (cid:17)(cid:11)(cid:110) (cid:41)(cid:89)(cid:110) (cid:76)(cid:78)(cid:100)(cid:95)(cid:71)(cid:110) (cid:17)(cid:8)(cid:110) (cid:98)(cid:79)(cid:95)(cid:71)(cid:71)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:92)(cid:67)(cid:80)(cid:95)(cid:97)(cid:110) (cid:90)(cid:69)(cid:69)(cid:100)(cid:95)(cid:110) (cid:69)(cid:90)(cid:89)(cid:69)(cid:100)(cid:95)(cid:95)(cid:71)(cid:89)(cid:98)(cid:84)(cid:104)(cid:110) (cid:72)(cid:95)(cid:90)(cid:87)(cid:110) (cid:98)(cid:80)(cid:84)(cid:71)(cid:110) (cid:50)(cid:19)(cid:110) (cid:98)(cid:90)(cid:110) (cid:50)(cid:16)(cid:8)(cid:110) (cid:98)(cid:80)(cid:84)(cid:71)(cid:110) (cid:50)(cid:16)(cid:110) (cid:98)(cid:90)(cid:110) (cid:50)(cid:22)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:80)(cid:84)(cid:71)(cid:110) (cid:50)(cid:20)(cid:110) (cid:98)(cid:90)(cid:110) (cid:50)(cid:49)(cid:11)(cid:110) (cid:60)(cid:71)(cid:110) (cid:67)(cid:97)(cid:109) (cid:97)(cid:100)(cid:87)(cid:71)(cid:110) (cid:16)(cid:15)(cid:15)(cid:15)(cid:110) (cid:92)(cid:67)(cid:69)(cid:83)(cid:71)(cid:98)(cid:97)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:78)(cid:71)(cid:89)(cid:71)(cid:95)(cid:67)(cid:98)(cid:71)(cid:70)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:92)(cid:67)(cid:80)(cid:95)(cid:110) (cid:102)(cid:80)(cid:98)(cid:79)(cid:80)(cid:89)(cid:110) (cid:67)(cid:110) (cid:98)(cid:80)(cid:87)(cid:71)(cid:110) (cid:102)(cid:80)(cid:89)(cid:70)(cid:90)(cid:102)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:97)(cid:71)(cid:89)(cid:98)(cid:110) (cid:90)(cid:101)(cid:71)(cid:95)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:89)(cid:71)(cid:98)(cid:102)(cid:90)(cid:95)(cid:83)(cid:110) (cid:80)(cid:89)(cid:110) (cid:98)(cid:79)(cid:80)(cid:97)(cid:110) (cid:71)(cid:103)(cid:67)(cid:87)(cid:92)(cid:84)(cid:71)(cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:110) (cid:71)(cid:89)(cid:109) (cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:69)(cid:90)(cid:89)(cid:97)(cid:100)(cid:87)(cid:92)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:90)(cid:72)(cid:110) (cid:92)(cid:95)(cid:90)(cid:69)(cid:71)(cid:97)(cid:97)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:110) (cid:97)(cid:80)(cid:89)(cid:78)(cid:84)(cid:71)(cid:110) (cid:92)(cid:67)(cid:69)(cid:83)(cid:71)(cid:98)(cid:110) (cid:80)(cid:89)(cid:110) (cid:67)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:110) (cid:80)(cid:97)(cid:110) (cid:70)(cid:71)(cid:89)(cid:90)(cid:98)(cid:71)(cid:70)(cid:110)(cid:67)(cid:97)(cid:110)(cid:18)(cid:10)(cid:63)(cid:64) (cid:56)(cid:90)(cid:110)(cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110)(cid:97)(cid:98)(cid:95)(cid:67)(cid:98)(cid:71)(cid:78)(cid:80)(cid:71)(cid:97)(cid:110)(cid:67)(cid:95)(cid:71)(cid:110)(cid:69)(cid:90)(cid:87)(cid:92)(cid:67)(cid:95)(cid:71)(cid:70)(cid:11)(cid:110) (cid:52)(cid:98)(cid:95)(cid:67)(cid:98)(cid:71)(cid:78)(cid:104)(cid:110) (cid:16)(cid:110) (cid:100)(cid:97)(cid:71)(cid:97)(cid:110) (cid:100)(cid:89)(cid:80)(cid:72)(cid:90)(cid:95)(cid:87)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:70)(cid:80)(cid:97)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:67)(cid:87)(cid:90)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:84)(cid:110) (cid:92)(cid:90)(cid:97)(cid:97)(cid:80)(cid:68)(cid:84)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:11)(cid:110) (cid:52)(cid:98)(cid:95)(cid:67)(cid:98)(cid:71)(cid:78)(cid:104)(cid:110) (cid:17)(cid:110) (cid:67)(cid:84)(cid:84)(cid:90)(cid:69)(cid:67)(cid:98)(cid:71)(cid:97)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:89)(cid:90)(cid:89)(cid:10)(cid:100)(cid:89)(cid:80)(cid:72)(cid:90)(cid:95)(cid:87)(cid:84)(cid:104)(cid:110) (cid:67)(cid:87)(cid:90)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:69)(cid:67)(cid:89)(cid:70)(cid:80)(cid:70)(cid:67)(cid:98)(cid:71)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:92)(cid:67)(cid:80)(cid:95)(cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:110) (cid:98)(cid:90)(cid:98)(cid:67)(cid:84)(cid:110) (cid:89)(cid:100)(cid:87)(cid:68)(cid:71)(cid:95)(cid:110) (cid:90)(cid:72)(cid:110) (cid:92)(cid:67)(cid:69)(cid:83)(cid:71)(cid:98)(cid:97)(cid:110) (cid:79)(cid:67)(cid:89)(cid:70)(cid:84)(cid:71)(cid:70)(cid:110) (cid:68)(cid:104)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:110) (cid:80)(cid:97)(cid:110) (cid:70)(cid:80)(cid:73)(cid:71)(cid:95)(cid:71)(cid:89)(cid:98)(cid:110) (cid:80)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:97)(cid:71)(cid:110) (cid:98)(cid:102)(cid:90)(cid:110) (cid:97)(cid:98)(cid:95)(cid:67)(cid:98)(cid:71)(cid:78)(cid:80)(cid:71)(cid:97)(cid:11)(cid:110) (cid:32)(cid:104)(cid:110) (cid:92)(cid:95)(cid:90)(cid:92)(cid:71)(cid:95)(cid:84)(cid:104)(cid:110)(cid:67)(cid:84)(cid:84)(cid:90)(cid:69)(cid:67)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:69)(cid:67)(cid:89)(cid:110)(cid:95)(cid:71)(cid:70)(cid:100)(cid:69)(cid:71)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:71)(cid:67)(cid:83)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:43)(cid:88)(cid:82)(cid:15)(cid:122) (cid:24)(cid:19)(cid:122) (cid:36)(cid:122) (cid:97)(cid:101)(cid:108)(cid:88)(cid:113)(cid:70)(cid:108)(cid:88)(cid:101)(cid:100)(cid:122) (cid:75)(cid:115)(cid:70)(cid:97)(cid:102)(cid:93)(cid:75)(cid:122) (cid:101)(cid:76)(cid:122) (cid:70)(cid:93)(cid:93)(cid:101)(cid:72)(cid:70)(cid:108)(cid:88)(cid:100)(cid:82)(cid:122) (cid:108)(cid:105)(cid:70)(cid:78)(cid:72)(cid:122) (cid:70)(cid:97)(cid:101)(cid:100)(cid:82)(cid:122) (cid:102)(cid:70)(cid:108)(cid:86)(cid:107)(cid:122) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:80)(cid:84)(cid:71)(cid:110) (cid:68)(cid:104)(cid:110) (cid:16)(cid:23)(cid:2)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:70)(cid:80)(cid:73)(cid:71)(cid:95)(cid:71)(cid:89)(cid:69)(cid:71)(cid:110) (cid:67)(cid:87)(cid:90)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:80)(cid:84)(cid:71)(cid:97)(cid:110) (cid:68)(cid:104)(cid:110) (cid:19)(cid:24)(cid:11) (cid:22)(cid:2) (cid:11)(cid:110) (cid:41)(cid:89)(cid:110) (cid:98)(cid:79)(cid:80)(cid:97)(cid:110) (cid:97)(cid:80)(cid:87)(cid:92)(cid:84)(cid:71)(cid:110) (cid:71)(cid:103)(cid:67)(cid:87)(cid:92)(cid:84)(cid:71)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:67)(cid:97)(cid:97)(cid:100)(cid:87)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:67)(cid:101)(cid:71)(cid:95)(cid:67)(cid:78)(cid:71)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:69)(cid:90)(cid:89)(cid:97)(cid:100)(cid:87)(cid:92)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:95)(cid:90)(cid:69)(cid:71)(cid:97)(cid:97)(cid:90)(cid:95)(cid:110) (cid:69)(cid:90)(cid:95)(cid:71)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:67)(cid:68)(cid:90)(cid:100)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:67)(cid:87)(cid:71)(cid:11)(cid:110) (cid:60)(cid:71)(cid:110) (cid:100)(cid:97)(cid:71)(cid:110) (cid:39)(cid:90)(cid:98)(cid:97)(cid:92)(cid:90)(cid:98)(cid:110) (cid:62)(cid:16)(cid:15)(cid:64)(cid:110) (cid:98)(cid:90)(cid:110) (cid:71)(cid:101)(cid:67)(cid:84)(cid:100)(cid:67)(cid:98)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:95)(cid:87)(cid:67)(cid:84)(cid:110) (cid:92)(cid:95)(cid:90)(cid:76)(cid:84)(cid:71)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:95)(cid:87)(cid:67)(cid:84)(cid:110) (cid:92)(cid:67)(cid:95)(cid:67)(cid:87)(cid:71)(cid:98)(cid:71)(cid:95)(cid:97)(cid:110) (cid:80)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:80)(cid:87)(cid:100)(cid:84)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:67)(cid:70)(cid:90)(cid:92)(cid:98)(cid:71)(cid:70)(cid:110) (cid:67)(cid:69)(cid:69)(cid:90)(cid:95)(cid:70)(cid:80)(cid:89)(cid:78)(cid:110)(cid:98)(cid:90)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:79)(cid:90)(cid:98)(cid:97)(cid:92)(cid:90)(cid:98)(cid:110)(cid:87)(cid:90)(cid:70)(cid:71)(cid:84)(cid:11)(cid:110) (cid:30)(cid:97)(cid:110)(cid:97)(cid:79)(cid:90)(cid:102)(cid:89)(cid:110)(cid:80)(cid:89)(cid:110)(cid:76)(cid:78)(cid:100)(cid:95)(cid:71)(cid:110)(cid:17)(cid:8)(cid:110) (cid:97)(cid:98)(cid:95)(cid:67)(cid:98)(cid:71)(cid:78)(cid:104)(cid:110) (cid:17)(cid:110) (cid:80)(cid:89)(cid:70)(cid:71)(cid:71)(cid:70)(cid:110)(cid:87)(cid:67)(cid:83)(cid:71)(cid:97)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:95)(cid:87)(cid:67)(cid:84)(cid:110) (cid:92)(cid:95)(cid:90)(cid:76)(cid:84)(cid:71)(cid:110) (cid:87)(cid:90)(cid:95)(cid:71)(cid:110) (cid:100)(cid:89)(cid:80)(cid:72)(cid:90)(cid:95)(cid:87)(cid:11)(cid:110) (cid:48)(cid:89)(cid:71)(cid:110) (cid:69)(cid:95)(cid:80)(cid:98)(cid:80)(cid:69)(cid:67)(cid:84)(cid:110) (cid:80)(cid:97)(cid:97)(cid:100)(cid:71)(cid:110) (cid:90)(cid:72)(cid:110)(cid:70)(cid:71)(cid:98)(cid:71)(cid:95)(cid:87)(cid:80)(cid:89)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:110) (cid:97)(cid:71)(cid:98)(cid:110) (cid:100)(cid:97)(cid:71)(cid:70)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:109) (cid:80)(cid:89)(cid:78)(cid:110) (cid:80)(cid:97)(cid:110) (cid:98)(cid:90)(cid:110) (cid:92)(cid:95)(cid:90)(cid:101)(cid:80)(cid:70)(cid:71)(cid:110) (cid:70)(cid:71)(cid:67)(cid:70)(cid:84)(cid:90)(cid:69)(cid:83)(cid:110) (cid:67)(cid:101)(cid:90)(cid:80)(cid:70)(cid:67)(cid:89)(cid:69)(cid:71)(cid:11)(cid:110) (cid:41)(cid:89)(cid:110) (cid:78)(cid:71)(cid:89)(cid:71)(cid:95)(cid:80)(cid:69)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:109) (cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:97)(cid:8)(cid:110) (cid:70)(cid:71)(cid:67)(cid:70)(cid:84)(cid:90)(cid:69)(cid:83)(cid:110) (cid:80)(cid:97)(cid:110) (cid:92)(cid:95)(cid:71)(cid:101)(cid:71)(cid:89)(cid:98)(cid:71)(cid:70)(cid:110) (cid:68)(cid:104)(cid:110) (cid:70)(cid:80)(cid:97)(cid:67)(cid:84)(cid:84)(cid:90)(cid:102)(cid:80)(cid:89)(cid:78)(cid:110) (cid:101)(cid:67)(cid:95)(cid:80)(cid:90)(cid:100)(cid:97)(cid:110) (cid:98)(cid:100)(cid:95)(cid:89)(cid:97)(cid:110) (cid:62)(cid:16)(cid:64)(cid:11)(cid:110) (cid:36)(cid:90)(cid:95)(cid:110)(cid:67)(cid:92)(cid:92)(cid:84)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:97)(cid:92)(cid:71)(cid:69)(cid:80)(cid:76)(cid:69)(cid:110)(cid:47)(cid:90)(cid:33)(cid:97)(cid:8)(cid:110) (cid:80)(cid:98)(cid:110)(cid:102)(cid:80)(cid:84)(cid:84)(cid:110)(cid:69)(cid:90)(cid:89)(cid:97)(cid:71)(cid:95)(cid:101)(cid:67)(cid:98)(cid:80)(cid:101)(cid:71)(cid:84)(cid:104)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110)(cid:100)(cid:89)(cid:89)(cid:71)(cid:69)(cid:109) (cid:71)(cid:97)(cid:97)(cid:67)(cid:95)(cid:80)(cid:84)(cid:104)(cid:110)(cid:92)(cid:95)(cid:90)(cid:79)(cid:80)(cid:68)(cid:80)(cid:98)(cid:110)(cid:97)(cid:90)(cid:87)(cid:71)(cid:110) (cid:84)(cid:71)(cid:78)(cid:80)(cid:98)(cid:80)(cid:87)(cid:67)(cid:98)(cid:71)(cid:110)(cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110)(cid:98)(cid:90)(cid:110)(cid:68)(cid:71)(cid:110)(cid:100)(cid:97)(cid:71)(cid:70)(cid:110)(cid:62)(cid:26)(cid:64)(cid:11)(cid:110) (cid:32)(cid:104)(cid:110)(cid:100)(cid:97)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:80)(cid:89)(cid:72)(cid:90)(cid:95)(cid:87)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:97)(cid:92)(cid:71)(cid:69)(cid:80)(cid:76)(cid:71)(cid:70)(cid:110) (cid:68)(cid:104)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:67)(cid:97)(cid:83)(cid:110) (cid:77)(cid:90)(cid:102)(cid:110) (cid:78)(cid:95)(cid:67)(cid:92)(cid:79)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:70)(cid:71)(cid:109) (cid:95)(cid:80)(cid:101)(cid:71)(cid:70)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:78)(cid:95)(cid:67)(cid:92)(cid:79)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:69)(cid:67)(cid:89)(cid:110) (cid:76)(cid:89)(cid:70)(cid:110)(cid:87)(cid:90)(cid:95)(cid:71)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110) (cid:67)(cid:101)(cid:67)(cid:80)(cid:84)(cid:67)(cid:68)(cid:84)(cid:71)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:80)(cid:89)(cid:69)(cid:95)(cid:71)(cid:67)(cid:97)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:77)(cid:71)(cid:103)(cid:80)(cid:68)(cid:80)(cid:84)(cid:80)(cid:98)(cid:104)(cid:110) (cid:90)(cid:72)(cid:110) (cid:70)(cid:80)(cid:97)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:67)(cid:87)(cid:90)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:90)(cid:100)(cid:95)(cid:69)(cid:71)(cid:97)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:70)(cid:71)(cid:97)(cid:98)(cid:80)(cid:89)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:97)(cid:11)(cid:110) (cid:30)(cid:72)(cid:98)(cid:71)(cid:95)(cid:110)(cid:90)(cid:68)(cid:98)(cid:67)(cid:80)(cid:89)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110)(cid:67)(cid:70)(cid:87)(cid:80)(cid:97)(cid:97)(cid:80)(cid:68)(cid:84)(cid:71)(cid:110)(cid:92)(cid:67)(cid:98)(cid:79)(cid:110)(cid:97)(cid:71)(cid:98)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110)(cid:76)(cid:89)(cid:70)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:90)(cid:92)(cid:98)(cid:80)(cid:87)(cid:67)(cid:84)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110)(cid:67)(cid:84)(cid:84)(cid:90)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:98)(cid:90)(cid:110)(cid:71)(cid:67)(cid:69)(cid:79)(cid:110)(cid:92)(cid:67)(cid:98)(cid:79)(cid:110)(cid:68)(cid:67)(cid:97)(cid:71)(cid:70)(cid:110)(cid:90)(cid:89)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:68)(cid:67)(cid:89)(cid:70)(cid:102)(cid:80)(cid:70)(cid:98)(cid:79)(cid:110) (cid:95)(cid:71)(cid:94)(cid:100)(cid:80)(cid:95)(cid:71)(cid:109) (cid:87)(cid:71)(cid:89)(cid:98)(cid:110) (cid:98)(cid:90)(cid:110) (cid:67)(cid:69)(cid:79)(cid:80)(cid:71)(cid:101)(cid:71)(cid:110) (cid:67)(cid:89)(cid:110) (cid:71)(cid:101)(cid:71)(cid:89)(cid:110)(cid:92)(cid:90)(cid:102)(cid:71)(cid:95)(cid:110) (cid:70)(cid:80)(cid:97)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:92)(cid:95)(cid:90)(cid:76)(cid:84)(cid:71)(cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:110) (cid:92)(cid:95)(cid:90)(cid:68)(cid:109) (cid:84)(cid:71)(cid:87)(cid:110) (cid:80)(cid:97)(cid:110) (cid:72)(cid:90)(cid:95)(cid:87)(cid:100)(cid:84)(cid:67)(cid:98)(cid:71)(cid:70)(cid:110) (cid:67)(cid:97)(cid:110) (cid:67)(cid:110) (cid:87)(cid:67)(cid:98)(cid:79)(cid:71)(cid:87)(cid:67)(cid:98)(cid:80)(cid:69)(cid:67)(cid:84)(cid:110) (cid:92)(cid:95)(cid:90)(cid:78)(cid:95)(cid:67)(cid:87)(cid:87)(cid:80)(cid:89)(cid:78)(cid:110) (cid:92)(cid:95)(cid:90)(cid:68)(cid:84)(cid:71)(cid:87)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:97)(cid:90)(cid:84)(cid:101)(cid:71)(cid:70)(cid:110) (cid:68)(cid:104)(cid:110) (cid:67)(cid:110) (cid:44)(cid:50)(cid:110) (cid:97)(cid:90)(cid:84)(cid:101)(cid:71)(cid:95)(cid:11)(cid:110) (cid:47)(cid:90)(cid:98)(cid:71)(cid:110) (cid:98)(cid:79)(cid:67)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:97)(cid:71)(cid:110) (cid:92)(cid:79)(cid:67)(cid:97)(cid:71)(cid:97)(cid:110) (cid:90)(cid:72)(cid:110) (cid:70)(cid:71)(cid:97)(cid:80)(cid:78)(cid:89)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:70)(cid:90)(cid:89)(cid:71)(cid:110) (cid:90)(cid:75)(cid:80)(cid:89)(cid:71)(cid:110) (cid:67)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:70)(cid:71)(cid:97)(cid:80)(cid:78)(cid:89)(cid:110) (cid:98)(cid:80)(cid:87)(cid:71)(cid:11)(cid:110) (cid:30)(cid:72)(cid:98)(cid:71)(cid:95)(cid:110) (cid:98)(cid:79)(cid:67)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:90)(cid:92)(cid:98)(cid:80)(cid:87)(cid:67)(cid:84)(cid:110) (cid:70)(cid:80)(cid:97)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:95)(cid:67)(cid:98)(cid:80)(cid:90)(cid:110) (cid:90)(cid:72)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:110) (cid:80)(cid:97)(cid:110) (cid:90)(cid:68)(cid:98)(cid:67)(cid:80)(cid:89)(cid:71)(cid:70)(cid:11)(cid:110) (cid:36)(cid:90)(cid:95)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:67)(cid:110) (cid:92)(cid:67)(cid:95)(cid:98)(cid:80)(cid:69)(cid:100)(cid:84)(cid:67)(cid:95)(cid:110) (cid:97)(cid:90)(cid:100)(cid:95)(cid:69)(cid:71)(cid:10)(cid:70)(cid:71)(cid:97)(cid:98)(cid:80)(cid:89)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:92)(cid:67)(cid:80)(cid:95)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:67)(cid:98)(cid:80)(cid:90)(cid:97)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110) (cid:92)(cid:67)(cid:97)(cid:97)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:95)(cid:90)(cid:100)(cid:78)(cid:79)(cid:110) (cid:80)(cid:98)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:69)(cid:90)(cid:87)(cid:68)(cid:80)(cid:89)(cid:71)(cid:70)(cid:110) (cid:80)(cid:89)(cid:98)(cid:90)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:95)(cid:90)(cid:68)(cid:67)(cid:68)(cid:80)(cid:84)(cid:80)(cid:98)(cid:80)(cid:71)(cid:97)(cid:110) (cid:90)(cid:72)(cid:110) (cid:100)(cid:97)(cid:80)(cid:89)(cid:78)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:92)(cid:90)(cid:95)(cid:98)(cid:110) (cid:98)(cid:90)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:110) (cid:98)(cid:90)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:70)(cid:71)(cid:97)(cid:98)(cid:80)(cid:89)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:11)(cid:110) (cid:41)(cid:89)(cid:110) (cid:95)(cid:100)(cid:89)(cid:98)(cid:80)(cid:87)(cid:71)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:97)(cid:71)(cid:110) (cid:92)(cid:95)(cid:90)(cid:68)(cid:67)(cid:68)(cid:80)(cid:84)(cid:80)(cid:98)(cid:104)(cid:110) (cid:101)(cid:67)(cid:84)(cid:100)(cid:71)(cid:97)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:97)(cid:98)(cid:90)(cid:95)(cid:71)(cid:70)(cid:110) (cid:80)(cid:89)(cid:98)(cid:90)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:67)(cid:68)(cid:84)(cid:71)(cid:97)(cid:110) (cid:80)(cid:89)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:11)(cid:110) (cid:33)(cid:58)(cid:11)(cid:136) (cid:39)(cid:24)(cid:36)(cid:42)(cid:136) (cid:24)(cid:38)(cid:31)(cid:45)(cid:50)(cid:36)(cid:55)(cid:32)(cid:40)(cid:136) (cid:41)(cid:89)(cid:110)(cid:98)(cid:79)(cid:80)(cid:97)(cid:110) (cid:97)(cid:71)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:8) (cid:102)(cid:71)(cid:110)(cid:76)(cid:95)(cid:97)(cid:98)(cid:110)(cid:68)(cid:95)(cid:80)(cid:71)(cid:77)(cid:104)(cid:110)(cid:70)(cid:71)(cid:97)(cid:69)(cid:95)(cid:80)(cid:68)(cid:71)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110)(cid:90)(cid:72)(cid:110)(cid:76)(cid:89)(cid:70)(cid:109) (cid:80)(cid:89)(cid:78)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:97)(cid:71)(cid:98)(cid:110)(cid:90)(cid:72)(cid:110)(cid:67)(cid:70)(cid:87)(cid:80)(cid:97)(cid:97)(cid:80)(cid:68)(cid:84)(cid:71)(cid:110)(cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110)(cid:72)(cid:90)(cid:95)(cid:110)(cid:71)(cid:67)(cid:69)(cid:79)(cid:110)(cid:97)(cid:90)(cid:100)(cid:95)(cid:69)(cid:71)(cid:10)(cid:70)(cid:71)(cid:97)(cid:98)(cid:80)(cid:89)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:69)(cid:90)(cid:87)(cid:109) (cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:92)(cid:67)(cid:80)(cid:95)(cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:89)(cid:110) (cid:102)(cid:71)(cid:110) (cid:92)(cid:95)(cid:71)(cid:97)(cid:71)(cid:89)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:90)(cid:92)(cid:98)(cid:80)(cid:87)(cid:67)(cid:84)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:67)(cid:84)(cid:84)(cid:90)(cid:69)(cid:67)(cid:109) (cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:92)(cid:95)(cid:90)(cid:68)(cid:84)(cid:71)(cid:87)(cid:110) (cid:72)(cid:90)(cid:95)(cid:87)(cid:100)(cid:84)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:11)(cid:110) (cid:22)(cid:12)(cid:136) (cid:22)(cid:114)(cid:114)(cid:99)(cid:92)(cid:73)(cid:67)(cid:123)(cid:92)(cid:110)(cid:106)(cid:136) (cid:119)(cid:114)(cid:78)(cid:73)(cid:92)(cid:84)(cid:73)(cid:136) (cid:114)(cid:67)(cid:123)(cid:89)(cid:136) (cid:119)(cid:78)(cid:123)(cid:136) (cid:84)(cid:106)(cid:76)(cid:92)(cid:106)(cid:86)(cid:136) (cid:67)(cid:99)(cid:86)(cid:110)(cid:116)(cid:92)(cid:123)(cid:89)(cid:102)(cid:136) (cid:33)(cid:105)(cid:136) (cid:62)(cid:26)(cid:64)(cid:8)(cid:110) (cid:67)(cid:110) (cid:70)(cid:104)(cid:89)(cid:67)(cid:87)(cid:80)(cid:69)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110) (cid:98)(cid:79)(cid:67)(cid:98)(cid:110) (cid:80)(cid:89)(cid:69)(cid:95)(cid:71)(cid:67)(cid:97)(cid:71)(cid:97)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:67)(cid:101)(cid:109) (cid:71)(cid:95)(cid:67)(cid:78)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:70)(cid:67)(cid:92)(cid:98)(cid:80)(cid:101)(cid:80)(cid:98)(cid:104)(cid:110) (cid:102)(cid:79)(cid:80)(cid:84)(cid:71)(cid:110) (cid:87)(cid:67)(cid:80)(cid:89)(cid:98)(cid:67)(cid:80)(cid:89)(cid:80)(cid:89)(cid:78)(cid:110) (cid:70)(cid:71)(cid:67)(cid:70)(cid:84)(cid:90)(cid:69)(cid:83)(cid:110) (cid:72)(cid:95)(cid:71)(cid:71)(cid:110) (cid:80)(cid:97)(cid:110) (cid:92)(cid:95)(cid:90)(cid:92)(cid:90)(cid:97)(cid:71)(cid:70)(cid:11)(cid:110) (cid:51)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:70)(cid:67)(cid:92)(cid:98)(cid:80)(cid:101)(cid:80)(cid:98)(cid:104)(cid:110) (cid:90)(cid:72)(cid:110) (cid:67)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:80)(cid:97)(cid:110) (cid:70)(cid:71)(cid:76)(cid:89)(cid:71)(cid:70)(cid:110) (cid:67)(cid:97)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:67)(cid:98)(cid:80)(cid:90)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:89)(cid:100)(cid:87)(cid:68)(cid:71)(cid:95)(cid:110) (cid:90)(cid:72)(cid:110) (cid:72)(cid:71)(cid:67)(cid:97)(cid:80)(cid:68)(cid:84)(cid:71)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110) (cid:98)(cid:90)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:90)(cid:98)(cid:67)(cid:84)(cid:110) (cid:89)(cid:100)(cid:87)(cid:109) (cid:68)(cid:71)(cid:95)(cid:110)(cid:90)(cid:72)(cid:110)(cid:87)(cid:80)(cid:89)(cid:80)(cid:87)(cid:67)(cid:84)(cid:110)(cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110)(cid:71)(cid:103)(cid:80)(cid:97)(cid:98)(cid:71)(cid:70)(cid:12)(cid:110) (cid:39)(cid:71)(cid:95)(cid:71)(cid:110) (cid:102)(cid:71)(cid:110)(cid:100)(cid:97)(cid:71)(cid:110)(cid:67)(cid:110)(cid:97)(cid:80)(cid:87)(cid:80)(cid:84)(cid:67)(cid:95)(cid:110)(cid:67)(cid:92)(cid:92)(cid:95)(cid:90)(cid:67)(cid:69)(cid:79)(cid:11)(cid:110) (cid:32)(cid:100)(cid:98)(cid:110) (cid:80)(cid:89)(cid:97)(cid:98)(cid:71)(cid:67)(cid:70)(cid:110) (cid:90)(cid:72)(cid:110) (cid:87)(cid:67)(cid:103)(cid:80)(cid:87)(cid:80)(cid:105)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:67)(cid:101)(cid:71)(cid:95)(cid:67)(cid:78)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:70)(cid:67)(cid:92)(cid:98)(cid:80)(cid:101)(cid:80)(cid:98)(cid:104)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:67)(cid:80)(cid:87)(cid:110) (cid:98)(cid:90)(cid:110) (cid:87)(cid:67)(cid:103)(cid:80)(cid:87)(cid:80)(cid:105)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:77)(cid:71)(cid:103)(cid:80)(cid:68)(cid:80)(cid:84)(cid:80)(cid:98)(cid:104)(cid:110) (cid:98)(cid:90)(cid:110) (cid:95)(cid:71)(cid:10)(cid:70)(cid:80)(cid:101)(cid:71)(cid:95)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:98)(cid:90)(cid:110) (cid:71)(cid:101)(cid:71)(cid:89)(cid:110) (cid:18)(cid:19)(cid:14)(cid:136) (cid:32)(cid:34)(cid:20)(cid:31)(cid:137) (cid:90)(cid:100)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:90)(cid:102)(cid:71)(cid:95)(cid:110) (cid:70)(cid:80)(cid:97)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:98)(cid:80)(cid:90)(cid:89)(cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:95)(cid:71)(cid:72)(cid:90)(cid:95)(cid:71)(cid:110) (cid:102)(cid:71)(cid:110) (cid:79)(cid:67)(cid:101)(cid:71)(cid:110) (cid:98)(cid:90)(cid:110) (cid:69)(cid:90)(cid:89)(cid:97)(cid:80)(cid:70)(cid:71)(cid:95)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:68)(cid:67)(cid:89)(cid:70)(cid:102)(cid:80)(cid:70)(cid:98)(cid:79)(cid:110) (cid:95)(cid:71)(cid:94)(cid:100)(cid:80)(cid:95)(cid:71)(cid:87)(cid:71)(cid:89)(cid:98)(cid:110) (cid:90)(cid:72)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:67)(cid:84)(cid:97)(cid:90)(cid:110) (cid:97)(cid:80)(cid:89)(cid:69)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:67)(cid:87)(cid:90)(cid:100)(cid:89)(cid:98)(cid:110)(cid:90)(cid:72)(cid:110)(cid:92)(cid:67)(cid:69)(cid:83)(cid:71)(cid:98)(cid:110)(cid:92)(cid:95)(cid:90)(cid:69)(cid:71)(cid:97)(cid:97)(cid:71)(cid:70)(cid:110)(cid:102)(cid:80)(cid:84)(cid:84)(cid:110)(cid:70)(cid:80)(cid:95)(cid:71)(cid:69)(cid:98)(cid:84)(cid:104)(cid:110)(cid:67)(cid:73)(cid:71)(cid:69)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110)(cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:69)(cid:90)(cid:89)(cid:109) (cid:97)(cid:100)(cid:87)(cid:92)(cid:98)(cid:80)(cid:90)(cid:89)(cid:11)(cid:110) (cid:41)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:72)(cid:90)(cid:84)(cid:84)(cid:90)(cid:102)(cid:80)(cid:89)(cid:78)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:102)(cid:80)(cid:84)(cid:84)(cid:110) (cid:90)(cid:89)(cid:84)(cid:104)(cid:110) (cid:68)(cid:95)(cid:80)(cid:71)(cid:77)(cid:104)(cid:110) (cid:70)(cid:80)(cid:97)(cid:69)(cid:100)(cid:97)(cid:97)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:110) (cid:95)(cid:71)(cid:67)(cid:70)(cid:71)(cid:95)(cid:97)(cid:110) (cid:69)(cid:67)(cid:89)(cid:110) (cid:95)(cid:71)(cid:72)(cid:71)(cid:95)(cid:110) (cid:98)(cid:90)(cid:110) (cid:62)(cid:26)(cid:64)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:70)(cid:71)(cid:98)(cid:67)(cid:80)(cid:84)(cid:97)(cid:11)(cid:110) (cid:41)(cid:89)(cid:110) (cid:62)(cid:26)(cid:64)(cid:9)(cid:110) (cid:80)(cid:98)(cid:110) (cid:102)(cid:67)(cid:97)(cid:110) (cid:97)(cid:79)(cid:90)(cid:102)(cid:89)(cid:110) (cid:98)(cid:79)(cid:67)(cid:98)(cid:110) (cid:100)(cid:97)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:89)(cid:90)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:90)(cid:72)(cid:110) (cid:30)(cid:92)(cid:92)(cid:84)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:33)(cid:79)(cid:67)(cid:89)(cid:89)(cid:71)(cid:84)(cid:110) (cid:34)(cid:71)(cid:92)(cid:71)(cid:89)(cid:70)(cid:71)(cid:89)(cid:69)(cid:104)(cid:110) (cid:38)(cid:95)(cid:67)(cid:92)(cid:79)(cid:110) (cid:4)(cid:30)(cid:33)(cid:34)(cid:38)(cid:6)(cid:8)(cid:110) (cid:70)(cid:71)(cid:67)(cid:70)(cid:84)(cid:90)(cid:69)(cid:83)(cid:110) (cid:80)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:69)(cid:67)(cid:89)(cid:110) (cid:68)(cid:71)(cid:110) (cid:70)(cid:71)(cid:98)(cid:71)(cid:69)(cid:98)(cid:71)(cid:70)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:95)(cid:71)(cid:87)(cid:90)(cid:101)(cid:71)(cid:70)(cid:11)(cid:110) (cid:30)(cid:89)(cid:110)(cid:30)(cid:33)(cid:34)(cid:38)(cid:110) (cid:69)(cid:67)(cid:89)(cid:110) (cid:68)(cid:71)(cid:110) (cid:72)(cid:90)(cid:95)(cid:87)(cid:71)(cid:70)(cid:110) (cid:72)(cid:95)(cid:90)(cid:87)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:67)(cid:92)(cid:92)(cid:84)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:3)(cid:97)(cid:110) (cid:53)(cid:36) (cid:38)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:70)(cid:71)(cid:95)(cid:80)(cid:101)(cid:71)(cid:70)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:78)(cid:95)(cid:67)(cid:92)(cid:79)(cid:11)(cid:110) (cid:34)(cid:71)(cid:67)(cid:70)(cid:84)(cid:90)(cid:69)(cid:83)(cid:110) (cid:72)(cid:95)(cid:71)(cid:71)(cid:110) (cid:80)(cid:97)(cid:110) (cid:71)(cid:89)(cid:97)(cid:100)(cid:95)(cid:71)(cid:70)(cid:110) (cid:80)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:95)(cid:71)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:89)(cid:90)(cid:110) (cid:69)(cid:104)(cid:69)(cid:84)(cid:71)(cid:97)(cid:110) (cid:80)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110)(cid:33)(cid:34)(cid:38)(cid:110) (cid:62)(cid:12) (cid:64)(cid:11)(cid:110) (cid:52)(cid:90)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:110) (cid:97)(cid:71)(cid:98)(cid:110) (cid:76)(cid:89)(cid:70)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110) (cid:97)(cid:98)(cid:67)(cid:95)(cid:98)(cid:97)(cid:110) (cid:102)(cid:80)(cid:98)(cid:79)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:30)(cid:33)(cid:34)(cid:38)(cid:110) (cid:98)(cid:90)(cid:110) (cid:76)(cid:89)(cid:70)(cid:110) (cid:67)(cid:84)(cid:84)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:69)(cid:104)(cid:69)(cid:84)(cid:71)(cid:97)(cid:110) (cid:80)(cid:89)(cid:110) (cid:80)(cid:98)(cid:11)(cid:110) (cid:36)(cid:90)(cid:95)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:69)(cid:104)(cid:69)(cid:84)(cid:71)(cid:8)(cid:110) (cid:67)(cid:110) (cid:97)(cid:100)(cid:68)(cid:97)(cid:71)(cid:98)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:71)(cid:70)(cid:78)(cid:71)(cid:97)(cid:110) (cid:1)(cid:27)(cid:38)(cid:37)(cid:41)(cid:38)(cid:2)(cid:64) (cid:67)(cid:95)(cid:71)(cid:110) (cid:95)(cid:71)(cid:87)(cid:90)(cid:101)(cid:71)(cid:70)(cid:110) (cid:98)(cid:90)(cid:110) (cid:68)(cid:95)(cid:71)(cid:67)(cid:83)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:69)(cid:104)(cid:69)(cid:84)(cid:71)(cid:11)(cid:110) (cid:60)(cid:79)(cid:71)(cid:89)(cid:110) (cid:97)(cid:90)(cid:87)(cid:71)(cid:110) (cid:71)(cid:70)(cid:78)(cid:71)(cid:97)(cid:110) (cid:80)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:30)(cid:33)(cid:34)(cid:38)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:95)(cid:71)(cid:87)(cid:90)(cid:101)(cid:71)(cid:70)(cid:8)(cid:110) (cid:80)(cid:98)(cid:110) (cid:67)(cid:84)(cid:97)(cid:90)(cid:110) (cid:87)(cid:71)(cid:67)(cid:89)(cid:97)(cid:110) (cid:98)(cid:79)(cid:67)(cid:98)(cid:110) (cid:97)(cid:90)(cid:87)(cid:71)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110) (cid:68)(cid:71)(cid:98)(cid:102)(cid:71)(cid:71)(cid:89)(cid:110) (cid:97)(cid:90)(cid:87)(cid:71)(cid:110) (cid:97)(cid:90)(cid:100)(cid:95)(cid:69)(cid:71)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:70)(cid:71)(cid:97)(cid:98)(cid:80)(cid:89)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:92)(cid:67)(cid:80)(cid:95)(cid:97)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:71)(cid:84)(cid:80)(cid:87)(cid:80)(cid:89)(cid:67)(cid:98)(cid:71)(cid:70)(cid:110) (cid:72)(cid:95)(cid:90)(cid:87)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:72)(cid:71)(cid:67)(cid:97)(cid:80)(cid:68)(cid:84)(cid:71)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:110) (cid:97)(cid:71)(cid:98)(cid:11)(cid:110) (cid:41)(cid:89)(cid:110) (cid:62)(cid:26)(cid:65)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:67)(cid:101)(cid:71)(cid:95)(cid:67)(cid:78)(cid:71)(cid:110) (cid:67)(cid:70)(cid:67)(cid:92)(cid:98)(cid:80)(cid:101)(cid:80)(cid:98)(cid:104)(cid:110) (cid:80)(cid:97)(cid:110)(cid:87)(cid:67)(cid:103)(cid:80)(cid:87)(cid:80)(cid:105)(cid:71)(cid:70)(cid:110) (cid:102)(cid:79)(cid:71)(cid:89)(cid:110) (cid:102)(cid:71)(cid:110) (cid:70)(cid:71)(cid:69)(cid:80)(cid:70)(cid:71)(cid:110) (cid:102)(cid:79)(cid:80)(cid:69)(cid:79)(cid:110) (cid:97)(cid:71)(cid:98)(cid:110) (cid:90)(cid:72)(cid:110) (cid:71)(cid:70)(cid:78)(cid:71)(cid:97)(cid:110) (cid:98)(cid:90)(cid:110) (cid:95)(cid:71)(cid:87)(cid:90)(cid:101)(cid:71)(cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:110) (cid:67)(cid:101)(cid:71)(cid:95)(cid:67)(cid:78)(cid:71)(cid:110) (cid:67)(cid:70)(cid:67)(cid:92)(cid:98)(cid:80)(cid:101)(cid:80)(cid:98)(cid:104)(cid:110)(cid:55)(cid:137)(cid:80)(cid:97)(cid:110) (cid:70)(cid:71)(cid:76)(cid:89)(cid:71)(cid:70)(cid:110) (cid:67)(cid:97)(cid:110) (cid:110)(cid:72)(cid:136)(cid:55)(cid:39)(cid:110)(cid:72)(cid:136)(cid:65)(cid:137) (cid:16)(cid:110) (cid:83)(cid:49)(cid:63)(cid:116) (cid:55)(cid:85)(cid:39)(cid:137) (cid:107)(cid:37)(cid:124)(cid:84)(cid:80)(cid:93)(cid:84)(cid:137)(cid:7)(cid:83)(cid:14)(cid:27)(cid:137) (cid:20)(cid:64)(cid:37)(cid:137)(cid:7)(cid:83)(cid:14)(cid:137)(cid:20)(cid:64) (cid:111)(cid:88)(cid:85)(cid:105)(cid:85)(cid:116) (cid:37)(cid:8)(cid:137)(cid:83)(cid:14)(cid:137) (cid:67)(cid:89)(cid:70)(cid:110)(cid:37)(cid:124)(cid:84)(cid:80)(cid:93)(cid:84)(cid:137)(cid:7)(cid:83)(cid:14)(cid:137) (cid:67)(cid:95)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:71)(cid:98)(cid:110) (cid:90)(cid:72)(cid:110) (cid:67)(cid:84)(cid:84)(cid:110)(cid:87)(cid:80)(cid:89)(cid:80)(cid:87)(cid:67)(cid:84)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:83)(cid:137)(cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:71)(cid:98)(cid:110) (cid:90)(cid:72)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110) (cid:67)(cid:72)(cid:98)(cid:71)(cid:95)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:71)(cid:70)(cid:78)(cid:71)(cid:97)(cid:110) (cid:80)(cid:89)(cid:110) (cid:27)(cid:38)(cid:37)(cid:41)(cid:38)(cid:64) (cid:67)(cid:95)(cid:71)(cid:110) (cid:95)(cid:71)(cid:87)(cid:90)(cid:101)(cid:71)(cid:70)(cid:8)(cid:110) (cid:95)(cid:71)(cid:97)(cid:92)(cid:71)(cid:69)(cid:98)(cid:80)(cid:101)(cid:71)(cid:84)(cid:104)(cid:11)(cid:110) (cid:42)(cid:89)(cid:110) (cid:98)(cid:79)(cid:80)(cid:97)(cid:110) (cid:102)(cid:90)(cid:95)(cid:83)(cid:8)(cid:110) (cid:97)(cid:71)(cid:101)(cid:71)(cid:95)(cid:67)(cid:84)(cid:110) (cid:87)(cid:90)(cid:70)(cid:80)(cid:76)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:97)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:87)(cid:67)(cid:70)(cid:71)(cid:110) (cid:72)(cid:95)(cid:90)(cid:87)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:67)(cid:84)(cid:109) (cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110) (cid:80)(cid:89)(cid:110) (cid:62)(cid:26)(cid:64)(cid:11)(cid:110) (cid:41)(cid:89)(cid:97)(cid:98)(cid:71)(cid:67)(cid:70)(cid:110) (cid:90)(cid:72)(cid:110) (cid:76)(cid:89)(cid:70)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:84)(cid:110) (cid:69)(cid:104)(cid:69)(cid:84)(cid:71)(cid:97)(cid:110) (cid:80)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:30)(cid:33)(cid:34)(cid:38)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:68)(cid:95)(cid:71)(cid:67)(cid:83)(cid:80)(cid:89)(cid:78)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:69)(cid:104)(cid:69)(cid:84)(cid:71)(cid:110) (cid:95)(cid:71)(cid:97)(cid:92)(cid:71)(cid:69)(cid:98)(cid:80)(cid:101)(cid:71)(cid:84)(cid:104)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:67)(cid:92)(cid:92)(cid:84)(cid:104)(cid:110) (cid:53)(cid:67)(cid:95)(cid:82)(cid:67)(cid:89)(cid:3)(cid:97)(cid:110) (cid:67)(cid:84)(cid:109) (cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110) (cid:62)(cid:16)(cid:17)(cid:64)(cid:110) (cid:98)(cid:90)(cid:110) (cid:76)(cid:89)(cid:70)(cid:110) (cid:67)(cid:84)(cid:84)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:98)(cid:95)(cid:90)(cid:89)(cid:78)(cid:84)(cid:104)(cid:110) (cid:69)(cid:90)(cid:89)(cid:89)(cid:71)(cid:69)(cid:98)(cid:71)(cid:70)(cid:110) (cid:69)(cid:90)(cid:87)(cid:92)(cid:90)(cid:89)(cid:71)(cid:89)(cid:98)(cid:97)(cid:110) (cid:4)(cid:52)(cid:33)(cid:33)(cid:6)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:95)(cid:104)(cid:110) (cid:98)(cid:90)(cid:110) (cid:71)(cid:84)(cid:80)(cid:87)(cid:80)(cid:89)(cid:67)(cid:98)(cid:71)(cid:110) (cid:69)(cid:104)(cid:69)(cid:84)(cid:71)(cid:97)(cid:110) (cid:102)(cid:80)(cid:98)(cid:79)(cid:80)(cid:89)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:89)(cid:90)(cid:89)(cid:98)(cid:95)(cid:80)(cid:101)(cid:80)(cid:67)(cid:84)(cid:110) (cid:69)(cid:90)(cid:87)(cid:109) (cid:92)(cid:90)(cid:89)(cid:71)(cid:89)(cid:98)(cid:97)(cid:110) (cid:4)(cid:69)(cid:90)(cid:87)(cid:92)(cid:90)(cid:89)(cid:71)(cid:89)(cid:98)(cid:97)(cid:110) (cid:69)(cid:90)(cid:89)(cid:98)(cid:67)(cid:80)(cid:89)(cid:80)(cid:89)(cid:78)(cid:110) (cid:87)(cid:90)(cid:95)(cid:71)(cid:110) (cid:98)(cid:79)(cid:67)(cid:89)(cid:110) (cid:90)(cid:89)(cid:71)(cid:110) (cid:101)(cid:71)(cid:95)(cid:98)(cid:71)(cid:103)(cid:6)(cid:11)(cid:110) (cid:48)(cid:89)(cid:71)(cid:110) (cid:80)(cid:87)(cid:92)(cid:90)(cid:95)(cid:98)(cid:67)(cid:89)(cid:98)(cid:110)(cid:72)(cid:71)(cid:67)(cid:98)(cid:100)(cid:95)(cid:71)(cid:110)(cid:90)(cid:72)(cid:110) (cid:52)(cid:33)(cid:33)(cid:110) (cid:80)(cid:97)(cid:110) (cid:98)(cid:79)(cid:67)(cid:98)(cid:110)(cid:69)(cid:104)(cid:69)(cid:84)(cid:71)(cid:97)(cid:110)(cid:90)(cid:72)(cid:110)(cid:67)(cid:110)(cid:70)(cid:80)(cid:95)(cid:71)(cid:69)(cid:98)(cid:71)(cid:70)(cid:110)(cid:78)(cid:95)(cid:67)(cid:92)(cid:79)(cid:110)(cid:67)(cid:95)(cid:71)(cid:110) (cid:69)(cid:90)(cid:89)(cid:98)(cid:67)(cid:80)(cid:89)(cid:71)(cid:70)(cid:110) (cid:80)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:67)(cid:87)(cid:71)(cid:110) (cid:69)(cid:90)(cid:87)(cid:92)(cid:90)(cid:89)(cid:71)(cid:89)(cid:98)(cid:97)(cid:11)(cid:110) (cid:60)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:89)(cid:110) (cid:71)(cid:84)(cid:80)(cid:87)(cid:80)(cid:89)(cid:67)(cid:98)(cid:71)(cid:110) (cid:69)(cid:104)(cid:69)(cid:84)(cid:71)(cid:97)(cid:110) (cid:102)(cid:80)(cid:98)(cid:79)(cid:80)(cid:89)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110)(cid:69)(cid:90)(cid:87)(cid:92)(cid:90)(cid:89)(cid:71)(cid:89)(cid:98)(cid:97)(cid:110)(cid:98)(cid:90)(cid:110)(cid:67)(cid:69)(cid:79)(cid:80)(cid:71)(cid:101)(cid:71)(cid:110)(cid:70)(cid:71)(cid:67)(cid:70)(cid:84)(cid:90)(cid:69)(cid:83)(cid:110) (cid:72)(cid:95)(cid:71)(cid:71)(cid:11)(cid:110) (cid:53)(cid:67)(cid:95)(cid:82)(cid:67)(cid:89)(cid:3)(cid:97)(cid:110) (cid:67)(cid:84)(cid:109) (cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110)(cid:80)(cid:97)(cid:110)(cid:100)(cid:97)(cid:71)(cid:70)(cid:110)(cid:68)(cid:71)(cid:69)(cid:67)(cid:100)(cid:97)(cid:71)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:69)(cid:90)(cid:87)(cid:92)(cid:84)(cid:71)(cid:103)(cid:80)(cid:98)(cid:104)(cid:110) (cid:80)(cid:97)(cid:110) (cid:84)(cid:90)(cid:102)(cid:71)(cid:95)(cid:110)(cid:1)(cid:17)(cid:1)(cid:32)(cid:20)(cid:64)(cid:3)(cid:64)(cid:48)(cid:18)(cid:20)(cid:2)(cid:2)(cid:7)(cid:64) (cid:30)(cid:84)(cid:97)(cid:90)(cid:110) (cid:80)(cid:89)(cid:110) (cid:87)(cid:67)(cid:89)(cid:104)(cid:110) (cid:69)(cid:67)(cid:97)(cid:71)(cid:97)(cid:8)(cid:110) (cid:97)(cid:71)(cid:101)(cid:71)(cid:95)(cid:67)(cid:84)(cid:110) (cid:71)(cid:70)(cid:78)(cid:71)(cid:97)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:97)(cid:79)(cid:67)(cid:95)(cid:71)(cid:70)(cid:110) (cid:67)(cid:87)(cid:90)(cid:89)(cid:78)(cid:110) (cid:70)(cid:80)(cid:73)(cid:71)(cid:95)(cid:71)(cid:89)(cid:98)(cid:110) (cid:69)(cid:104)(cid:69)(cid:84)(cid:71)(cid:97)(cid:11)(cid:110) (cid:41)(cid:72)(cid:110) (cid:102)(cid:71)(cid:110) (cid:80)(cid:89)(cid:97)(cid:92)(cid:71)(cid:69)(cid:98)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:69)(cid:104)(cid:69)(cid:84)(cid:71)(cid:110) (cid:97)(cid:71)(cid:92)(cid:67)(cid:95)(cid:67)(cid:98)(cid:71)(cid:84)(cid:104)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:87)(cid:67)(cid:104)(cid:110) (cid:69)(cid:90)(cid:89)(cid:97)(cid:80)(cid:70)(cid:71)(cid:95)(cid:110) (cid:98)(cid:79)(cid:71)(cid:97)(cid:71)(cid:110) (cid:71)(cid:70)(cid:78)(cid:71)(cid:97)(cid:110)(cid:87)(cid:90)(cid:95)(cid:71)(cid:110) (cid:98)(cid:79)(cid:67)(cid:89)(cid:110)(cid:90)(cid:89)(cid:69)(cid:71)(cid:11)(cid:110) (cid:60)(cid:79)(cid:71)(cid:89)(cid:110) (cid:102)(cid:71)(cid:110) (cid:100)(cid:97)(cid:71)(cid:110)(cid:53)(cid:67)(cid:95)(cid:82)(cid:67)(cid:89)(cid:3)(cid:97)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:8)(cid:110) (cid:69)(cid:104)(cid:69)(cid:84)(cid:71)(cid:97)(cid:110) (cid:102)(cid:80)(cid:98)(cid:79)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:90)(cid:89)(cid:110) (cid:71)(cid:70)(cid:78)(cid:71)(cid:97)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:80)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:67)(cid:87)(cid:71)(cid:110) (cid:69)(cid:90)(cid:87)(cid:92)(cid:90)(cid:89)(cid:71)(cid:89)(cid:98)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:79)(cid:71)(cid:89)(cid:69)(cid:71)(cid:110)(cid:70)(cid:71)(cid:69)(cid:80)(cid:97)(cid:80)(cid:90)(cid:89)(cid:110)(cid:69)(cid:67)(cid:89)(cid:110)(cid:68)(cid:71)(cid:110)(cid:87)(cid:67)(cid:70)(cid:71)(cid:110)(cid:87)(cid:90)(cid:95)(cid:71)(cid:110)(cid:71)(cid:74)(cid:69)(cid:80)(cid:71)(cid:89)(cid:98)(cid:84)(cid:104)(cid:110) (cid:80)(cid:72)(cid:110)(cid:102)(cid:71)(cid:110)(cid:95)(cid:71)(cid:87)(cid:90)(cid:101)(cid:71)(cid:110) (cid:97)(cid:90)(cid:87)(cid:71)(cid:110) (cid:97)(cid:79)(cid:67)(cid:95)(cid:71)(cid:70)(cid:110) (cid:71)(cid:70)(cid:78)(cid:71)(cid:97)(cid:110) (cid:98)(cid:90)(cid:110) (cid:68)(cid:95)(cid:71)(cid:67)(cid:83)(cid:110) (cid:98)(cid:79)(cid:71)(cid:97)(cid:71)(cid:110) (cid:69)(cid:104)(cid:69)(cid:84)(cid:71)(cid:97)(cid:110) (cid:97)(cid:80)(cid:87)(cid:100)(cid:84)(cid:98)(cid:67)(cid:89)(cid:71)(cid:90)(cid:100)(cid:97)(cid:84)(cid:104)(cid:11)(cid:110) (cid:60) (cid:79)(cid:71)(cid:89)(cid:110) (cid:102)(cid:71)(cid:110) (cid:97)(cid:71)(cid:84)(cid:71)(cid:69)(cid:98)(cid:110) (cid:71)(cid:70)(cid:78)(cid:71)(cid:97)(cid:110) (cid:98)(cid:90)(cid:110) (cid:68)(cid:95)(cid:71)(cid:67)(cid:83)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:69)(cid:104)(cid:69)(cid:84)(cid:71)(cid:8)(cid:110) (cid:80)(cid:89)(cid:97)(cid:98)(cid:71)(cid:67)(cid:70)(cid:110) (cid:90)(cid:72)(cid:110) (cid:90)(cid:92)(cid:98)(cid:80)(cid:87)(cid:80)(cid:105)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:67)(cid:101)(cid:109) (cid:71)(cid:95)(cid:67)(cid:78)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110)(cid:67)(cid:70)(cid:67)(cid:92)(cid:98)(cid:80)(cid:101)(cid:80)(cid:98)(cid:104)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110)(cid:87)(cid:67)(cid:103)(cid:80)(cid:87)(cid:80)(cid:105)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:72)(cid:90)(cid:84)(cid:84)(cid:90)(cid:102)(cid:80)(cid:89)(cid:78)(cid:110) (cid:90)(cid:68)(cid:82)(cid:71)(cid:69)(cid:98)(cid:80)(cid:101)(cid:71)(cid:110) (cid:72)(cid:100)(cid:89)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:27)(cid:110) (cid:110)(cid:72)(cid:136)(cid:137)(cid:64)(cid:137) (cid:55)(cid:85)(cid:68)(cid:85)(cid:137)(cid:39)(cid:137)(cid:110)(cid:72)(cid:136)(cid:137)(cid:64)(cid:137) (cid:16)(cid:110) (cid:16)(cid:110) (cid:83)(cid:49)(cid:63)(cid:116) (cid:18)(cid:67)(cid:7)(cid:83)(cid:14)(cid:137) (cid:8)(cid:9) (cid:83)(cid:49)(cid:63)(cid:116) (cid:20)(cid:64)(cid:37)(cid:124)(cid:84)(cid:80)(cid:93)(cid:84)(cid:137)(cid:7)(cid:83)(cid:14)(cid:27)(cid:137) (cid:27)(cid:36)(cid:7)(cid:83)(cid:14)(cid:137)(cid:20)(cid:64) (cid:66)(cid:41)(cid:67)(cid:41)(cid:60)(cid:50)(cid:71)(cid:50)(cid:67)(cid:68) (cid:116) (cid:41) (cid:62)(cid:47)(cid:116) (cid:62)(cid:64)(cid:71)(cid:41)(cid:71)(cid:54)(cid:64)(cid:62)(cid:68)(cid:116) (cid:54)(cid:62)(cid:116) (cid:59)(cid:66)(cid:116) (cid:52)(cid:64)(cid:67)(cid:60)(cid:74)(cid:58)(cid:41)(cid:71)(cid:54)(cid:64)(cid:62)(cid:116) (cid:103)(cid:102)(cid:116) (cid:104)(cid:80)(cid:106)(cid:80)(cid:100)(cid:86)(cid:108)(cid:86)(cid:106)(cid:107)(cid:116) (cid:53)(cid:101)(cid:108)(cid:70)(cid:108)(cid:47)(cid:56)(cid:100)(cid:107)(cid:122) (cid:61)(cid:36)(cid:38)(cid:51)(cid:42)(cid:122) (cid:46)(cid:122) (cid:19)(cid:42)(cid:43)(cid:64)(cid:50)(cid:99)(cid:103)(cid:137) (cid:97)(cid:4)(cid:90)(cid:8)(cid:94)(cid:8)(cid:136)(cid:34)(cid:3)(cid:52) (cid:31)(cid:42)(cid:64) (cid:112)(cid:4)(cid:90)(cid:8)(cid:94)(cid:7)(cid:136) (cid:29)(cid:53)(cid:33)(cid:39)(cid:45)(cid:38)(cid:57)(cid:64) (cid:29)(cid:34)(cid:42)(cid:57)(cid:64) (cid:108)(cid:101)(cid:108)(cid:70)(cid:93)(cid:122) (cid:100)(cid:112)(cid:97)(cid:71)(cid:75)(cid:105)(cid:122) (cid:101)(cid:76)(cid:122) (cid:108)(cid:88)(cid:93)(cid:75)(cid:107)(cid:122) (cid:88)(cid:100)(cid:122) (cid:53)(cid:101)(cid:40)(cid:122) (cid:93)(cid:88)(cid:100)(cid:91)(cid:122) (cid:72)(cid:70)(cid:102)(cid:70)(cid:72)(cid:88)(cid:108)(cid:116)(cid:122) (cid:71)(cid:75)(cid:108)(cid:114)(cid:75)(cid:75)(cid:100)(cid:122) (cid:108)(cid:88)(cid:93)(cid:75)(cid:122) (cid:90)(cid:136) (cid:70)(cid:100)(cid:73)(cid:122)(cid:94)(cid:136) (cid:100)(cid:112)(cid:97)(cid:71)(cid:75)(cid:105)(cid:122) (cid:101)(cid:76)(cid:122) (cid:97)(cid:88)(cid:100)(cid:88)(cid:97)(cid:70)(cid:93)(cid:122) (cid:102)(cid:70)(cid:108)(cid:86)(cid:107)(cid:122) (cid:71)(cid:75)(cid:108)(cid:114)(cid:75)(cid:75)(cid:100)(cid:122) (cid:108)(cid:88)(cid:93)(cid:75)(cid:122) (cid:90)(cid:136)(cid:70)(cid:100)(cid:73)(cid:122)(cid:94)(cid:136) (cid:108)(cid:86)(cid:75)(cid:122) (cid:34)(cid:46)(cid:28)(cid:52) (cid:102)(cid:70)(cid:108)(cid:86)(cid:122) (cid:90)(cid:101)(cid:88)(cid:100)(cid:88)(cid:100)(cid:82)(cid:122) (cid:108)(cid:88)(cid:93)(cid:75)(cid:122) (cid:90)(cid:136)(cid:70)(cid:100)(cid:73)(cid:122)(cid:94)(cid:136) (cid:39)(cid:116) (cid:22)(cid:116) (cid:38)(cid:116) (cid:34)(cid:52) (cid:38)(cid:116) (cid:22)(cid:42)(cid:43)(cid:64) (cid:107)(cid:75)(cid:108)(cid:122) (cid:101)(cid:76)(cid:122) (cid:102)(cid:70)(cid:108)(cid:86)(cid:107)(cid:122) (cid:102)(cid:70)(cid:107)(cid:107)(cid:122) (cid:108)(cid:88)(cid:93)(cid:75)(cid:122) (cid:90)(cid:136) (cid:102)(cid:70)(cid:72)(cid:91)(cid:75)(cid:108)(cid:122) (cid:88)(cid:100)(cid:90)(cid:75)(cid:72)(cid:108)(cid:88)(cid:101)(cid:100)(cid:122) (cid:105)(cid:70)(cid:108)(cid:75)(cid:122) (cid:71)(cid:75)(cid:108)(cid:114)(cid:75)(cid:75)(cid:100)(cid:122) (cid:108)(cid:88)(cid:93)(cid:75)(cid:122) (cid:90)(cid:136) (cid:70)(cid:100)(cid:73)(cid:122)(cid:94)(cid:136) (cid:100)(cid:112)(cid:97)(cid:71)(cid:75)(cid:105)(cid:122) (cid:101)(cid:76)(cid:122) (cid:81)(cid:88)(cid:108)(cid:107)(cid:122) (cid:88)(cid:100)(cid:122) (cid:70)(cid:122) (cid:102)(cid:70)(cid:72)(cid:91)(cid:75)(cid:108)(cid:122) (cid:100)(cid:112)(cid:97)(cid:71)(cid:75)(cid:105)(cid:122) (cid:101)(cid:76)(cid:122) (cid:71)(cid:88)(cid:108)(cid:107)(cid:122) (cid:88)(cid:100)(cid:122) (cid:70)(cid:122) (cid:102)(cid:70)(cid:72)(cid:91)(cid:75)(cid:108)(cid:122) (cid:86)(cid:102)(cid:86)(cid:106)(cid:87)(cid:114)(cid:116) (cid:104)(cid:80)(cid:106)(cid:80)(cid:100)(cid:86)(cid:108)(cid:86)(cid:106)(cid:107)(cid:116) (cid:46)(cid:90)(cid:115)(cid:121)(cid:134)(cid:73)(cid:121)(cid:81)(cid:137) (cid:46)(cid:77)(cid:130)(cid:90)(cid:90)(cid:87)(cid:121)(cid:19)(cid:121)(cid:134)(cid:137) (cid:46)(cid:125)(cid:134)(cid:137) (cid:46)(cid:121)(cid:88)(cid:24)(cid:46)(cid:59)(cid:87)(cid:1)(cid:137) (cid:46)(cid:133)(cid:88)(cid:137) (cid:70)(cid:113)(cid:75)(cid:105)(cid:70)(cid:82)(cid:75)(cid:122) (cid:102)(cid:105)(cid:101)(cid:72)(cid:75)(cid:107)(cid:107)(cid:101)(cid:105)(cid:122) (cid:75)(cid:100)(cid:75)(cid:105)(cid:82)(cid:116)(cid:122) (cid:101)(cid:76)(cid:122) (cid:90)(cid:121)(cid:87)(cid:136) (cid:108)(cid:88)(cid:93)(cid:75)(cid:122) (cid:75)(cid:100)(cid:75)(cid:105)(cid:82)(cid:116)(cid:122) (cid:101)(cid:76)(cid:122) (cid:76)(cid:101)(cid:105)(cid:114)(cid:70)(cid:105)(cid:73)(cid:88)(cid:100)(cid:82)(cid:122) (cid:70)(cid:122) (cid:81)(cid:88)(cid:108)(cid:122) (cid:88)(cid:100)(cid:122) (cid:105)(cid:101)(cid:112)(cid:108)(cid:75)(cid:105)(cid:122) (cid:71)(cid:112)(cid:77)(cid:75)(cid:105)(cid:122) (cid:105)(cid:75)(cid:70)(cid:73)(cid:122) (cid:70)(cid:100)(cid:73)(cid:122) (cid:114)(cid:105)(cid:88)(cid:108)(cid:75)(cid:122) (cid:75)(cid:100)(cid:75)(cid:105)(cid:82)(cid:116)(cid:122) (cid:76)(cid:101)(cid:105)(cid:122) (cid:70)(cid:122) (cid:81)(cid:88)(cid:108)(cid:122) (cid:88)(cid:100)(cid:122) (cid:105)(cid:101)(cid:112)(cid:108)(cid:75)(cid:105)(cid:122) (cid:107)(cid:114)(cid:88)(cid:108)(cid:72)(cid:86)(cid:122) (cid:70)(cid:93)(cid:93)(cid:101)(cid:72)(cid:70)(cid:108)(cid:88)(cid:101)(cid:100)(cid:122) (cid:75)(cid:100)(cid:75)(cid:105)(cid:82)(cid:116)(cid:122) (cid:76)(cid:101)(cid:105)(cid:122) (cid:70)(cid:122) (cid:81)(cid:88)(cid:108)(cid:122) (cid:88)(cid:100)(cid:122) (cid:105)(cid:101)(cid:112)(cid:108)(cid:75)(cid:105)(cid:122) (cid:105)(cid:101)(cid:112)(cid:108)(cid:88)(cid:100)(cid:82)(cid:122) (cid:72)(cid:101)(cid:97)(cid:102)(cid:112)(cid:108)(cid:70)(cid:108)(cid:88)(cid:101)(cid:100)(cid:20)(cid:101)(cid:112)(cid:108)(cid:102)(cid:112)(cid:108)(cid:122) (cid:102)(cid:101)(cid:105)(cid:108)(cid:122) (cid:107)(cid:75)(cid:93)(cid:75)(cid:72)(cid:108)(cid:88)(cid:101)(cid:100)(cid:122) (cid:75)(cid:100)(cid:75)(cid:106)(cid:82)(cid:116)(cid:122) (cid:113)(cid:88)(cid:105)(cid:108)(cid:112)(cid:70)(cid:93)(cid:122) (cid:72)(cid:86)(cid:70)(cid:100)(cid:100)(cid:75)(cid:93)(cid:122) (cid:70)(cid:93)(cid:93)(cid:101)(cid:72)(cid:70)(cid:108)(cid:88)(cid:101)(cid:100)(cid:122) (cid:75)(cid:100)(cid:75)(cid:105)(cid:82)(cid:116)(cid:122) (cid:43)(cid:14)(cid:20)(cid:116) (cid:28)(cid:104)(cid:77)(cid:115)(cid:85)(cid:131)(cid:136) (cid:72)(cid:109)(cid:104)(cid:118)(cid:125)(cid:101)(cid:113)(cid:122)(cid:91)(cid:109)(cid:104)(cid:136) (cid:101)(cid:109)(cid:75)(cid:77)(cid:98)(cid:136) (cid:60)(cid:71)(cid:110) (cid:67)(cid:97)(cid:97)(cid:100)(cid:87)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:69)(cid:90)(cid:89)(cid:97)(cid:100)(cid:87)(cid:92)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:90)(cid:72)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:92)(cid:95)(cid:90)(cid:69)(cid:71)(cid:97)(cid:97)(cid:90)(cid:95)(cid:110) (cid:90)(cid:116) (cid:1)(cid:18)(cid:53)(cid:6)(cid:42)(cid:2)(cid:64) (cid:92)(cid:71)(cid:95)(cid:110) (cid:100)(cid:89)(cid:80)(cid:98)(cid:110) (cid:98)(cid:80)(cid:87)(cid:71)(cid:110) (cid:80)(cid:97)(cid:110) (cid:67)(cid:101)(cid:67)(cid:80)(cid:84)(cid:67)(cid:68)(cid:84)(cid:71)(cid:110) (cid:67)(cid:72)(cid:98)(cid:71)(cid:95)(cid:110) (cid:98)(cid:67)(cid:97)(cid:83)(cid:110) (cid:87)(cid:67)(cid:92)(cid:92)(cid:80)(cid:89)(cid:78)(cid:11)(cid:110) (cid:60)(cid:90)(cid:95)(cid:87)(cid:109) (cid:79)(cid:90)(cid:84)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:80)(cid:97)(cid:110) (cid:100)(cid:97)(cid:71)(cid:70)(cid:110) (cid:80)(cid:89)(cid:110) (cid:90)(cid:100)(cid:95)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:97)(cid:69)(cid:79)(cid:71)(cid:87)(cid:71)(cid:11)(cid:110) (cid:36)(cid:90)(cid:95)(cid:110) (cid:71)(cid:101)(cid:71)(cid:95)(cid:104)(cid:110) (cid:70)(cid:67)(cid:98)(cid:67)(cid:110) (cid:92)(cid:67)(cid:69)(cid:83)(cid:71)(cid:98)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:79)(cid:71)(cid:67)(cid:70)(cid:110) (cid:77)(cid:80)(cid:98)(cid:110) (cid:97)(cid:71)(cid:98)(cid:97)(cid:110) (cid:100)(cid:92)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:110) (cid:70)(cid:80)(cid:95)(cid:71)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:97)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:68)(cid:90)(cid:70)(cid:104)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:98)(cid:67)(cid:80)(cid:84)(cid:110)(cid:77)(cid:80)(cid:98)(cid:97)(cid:11)(cid:110) (cid:53)(cid:79)(cid:100)(cid:97)(cid:8)(cid:110)(cid:18)(cid:54) (cid:39)(cid:4)(cid:64)(cid:18)(cid:56)(cid:38)(cid:47)(cid:64) (cid:67)(cid:89)(cid:70)(cid:110)(cid:18)(cid:60)(cid:39)(cid:64) (cid:90)(cid:89)(cid:84)(cid:104)(cid:110)(cid:80)(cid:89)(cid:69)(cid:100)(cid:95)(cid:110)(cid:102)(cid:79)(cid:71)(cid:89)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110) (cid:79)(cid:71)(cid:67)(cid:70)(cid:110)(cid:77)(cid:80)(cid:98)(cid:110) (cid:80)(cid:97)(cid:110)(cid:92)(cid:95)(cid:90)(cid:69)(cid:71)(cid:97)(cid:97)(cid:71)(cid:70)(cid:110)(cid:68)(cid:104)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110)(cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:11)(cid:110) (cid:53)(cid:90)(cid:98)(cid:67)(cid:84)(cid:110)(cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110)(cid:69)(cid:90)(cid:89)(cid:97)(cid:100)(cid:87)(cid:92)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:92)(cid:95)(cid:90)(cid:69)(cid:71)(cid:97)(cid:97)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:110) (cid:97)(cid:80)(cid:89)(cid:78)(cid:84)(cid:71)(cid:110) (cid:92)(cid:67)(cid:69)(cid:83)(cid:71)(cid:98)(cid:110) (cid:80)(cid:89)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:110)(cid:29)(cid:52) (cid:80)(cid:97)(cid:110) (cid:78)(cid:80)(cid:101)(cid:71)(cid:89)(cid:110) (cid:68)(cid:104)(cid:110) (cid:40)(cid:18)(cid:54)(cid:6)(cid:42)(cid:64)(cid:39)(cid:137) (cid:1)(cid:18)(cid:34)(cid:59)(cid:44)(cid:44)(cid:38)(cid:54)(cid:6)(cid:54)(cid:61)(cid:64)(cid:3)(cid:64)(cid:18)(cid:44)(cid:51)(cid:54) (cid:61)(cid:33)(cid:54)(cid:37)(cid:64)(cid:3)(cid:64)(cid:18)(cid:56)(cid:61)(cid:2)(cid:64) (cid:8)(cid:9) (cid:27)(cid:53)(cid:33)(cid:85)(cid:93)(cid:38)(cid:46)(cid:52)(cid:3)(cid:64)(cid:18)(cid:54) (cid:39) (cid:64)(cid:3)(cid:64)(cid:18)(cid:56)(cid:38)(cid:47)(cid:64) (cid:3)(cid:64)(cid:18)(cid:60)(cid:39)(cid:7)(cid:64) (cid:43)(cid:14)(cid:24)(cid:116) (cid:57)(cid:65)(cid:116) (cid:113)(cid:115)(cid:109)(cid:69)(cid:98)(cid:77)(cid:101)(cid:136) (cid:80)(cid:109)(cid:115)(cid:101)(cid:125)(cid:98)(cid:66)(cid:122)(cid:91)(cid:109)(cid:104)(cid:136) (cid:38)(cid:80)(cid:101)(cid:71)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:71)(cid:98)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:70)(cid:71)(cid:67)(cid:70)(cid:84)(cid:90)(cid:69)(cid:83)(cid:10)(cid:72)(cid:95)(cid:71)(cid:71)(cid:110) (cid:67)(cid:70)(cid:87)(cid:80)(cid:97)(cid:97)(cid:80)(cid:68)(cid:84)(cid:71)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:71)(cid:101)(cid:71)(cid:95)(cid:104)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:92)(cid:67)(cid:80)(cid:95)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:102)(cid:67)(cid:89)(cid:98)(cid:110) (cid:98)(cid:90)(cid:110) (cid:90)(cid:68)(cid:98)(cid:67)(cid:80)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:67)(cid:98)(cid:80)(cid:90)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:67)(cid:84)(cid:84)(cid:90)(cid:69)(cid:67)(cid:98)(cid:71)(cid:70)(cid:110) (cid:98)(cid:90)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:110) (cid:98)(cid:90)(cid:110) (cid:87)(cid:80)(cid:89)(cid:80)(cid:87)(cid:80)(cid:105)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:87)(cid:67)(cid:103)(cid:80)(cid:87)(cid:100)(cid:87)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:69)(cid:90)(cid:89)(cid:97)(cid:100)(cid:87)(cid:92)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:67)(cid:87)(cid:90)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:84)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:80)(cid:84)(cid:71)(cid:97)(cid:11)(cid:110) (cid:60)(cid:71)(cid:110) (cid:72)(cid:90)(cid:95)(cid:87)(cid:100)(cid:84)(cid:67)(cid:98)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:95)(cid:90)(cid:68)(cid:84)(cid:71)(cid:87)(cid:110) (cid:67)(cid:97)(cid:110) (cid:72)(cid:90)(cid:84)(cid:84)(cid:90)(cid:102)(cid:97)(cid:27)(cid:110) (cid:26)(cid:13)(cid:137) (cid:59)(cid:68)(cid:117)(cid:93)(cid:68)(cid:70)(cid:100)(cid:79)(cid:120)(cid:136) (cid:43)(cid:1)(cid:29)(cid:4)(cid:31)(cid:4)(cid:33)(cid:2)(cid:7)(cid:52) (cid:95)(cid:67)(cid:98)(cid:80)(cid:90)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:67)(cid:84)(cid:84)(cid:90)(cid:69)(cid:67)(cid:98)(cid:71)(cid:70)(cid:110) (cid:98)(cid:90)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:33)(cid:46)(cid:28)(cid:52) (cid:92)(cid:67)(cid:98)(cid:79)(cid:110) (cid:36)(cid:1)(cid:29)(cid:4)(cid:31)(cid:4)(cid:33)(cid:2)(cid:52) (cid:68)(cid:71)(cid:98)(cid:102)(cid:71)(cid:71)(cid:89)(cid:110) (cid:98)(cid:80)(cid:84)(cid:71)(cid:97)(cid:110) (cid:29)(cid:52) (cid:67)(cid:89)(cid:70)(cid:110) (cid:31)(cid:52) (cid:67)(cid:87)(cid:90)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:84)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:21)(cid:42)(cid:43)(cid:64) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:11)(cid:110)(cid:102)(cid:79)(cid:71)(cid:95)(cid:71)(cid:110) (cid:16)(cid:110)(cid:9)(cid:52) (cid:29)(cid:52)(cid:9)(cid:52) (cid:13)(cid:4)(cid:12)(cid:52)(cid:9)(cid:52) (cid:31)(cid:52) (cid:28)(cid:110) (cid:46)(cid:8)(cid:110) (cid:16)(cid:110)(cid:9)(cid:52) (cid:33)(cid:52)(cid:9)(cid:52) (cid:21)(cid:42)(cid:43)(cid:64) (cid:29)(cid:12)(cid:137) (cid:44)(cid:70)(cid:95)(cid:79)(cid:74)(cid:124)(cid:93)(cid:129)(cid:79)(cid:136) (cid:81)(cid:127)(cid:108)(cid:74)(cid:124)(cid:93)(cid:111)(cid:108)(cid:120)(cid:21)(cid:136) (cid:53)(cid:79)(cid:71)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:69)(cid:90)(cid:89)(cid:97)(cid:100)(cid:87)(cid:92)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:29)(cid:46)(cid:28)(cid:52) (cid:98)(cid:80)(cid:84)(cid:71)(cid:8)(cid:110) (cid:18)(cid:42)(cid:4)(cid:64) (cid:80)(cid:97)(cid:110) (cid:78)(cid:80)(cid:101)(cid:71)(cid:89)(cid:110) (cid:68)(cid:104)(cid:110) (cid:18)(cid:42)(cid:64)(cid:39)(cid:137)(cid:18)(cid:53)(cid:6)(cid:42)(cid:64)(cid:3)(cid:64)(cid:40)(cid:18)(cid:54)(cid:6)(cid:42)(cid:64) (cid:8)(cid:9) (cid:50)(cid:42)(cid:64) (cid:111)(cid:88)(cid:85)(cid:105)(cid:85)(cid:116) (cid:50)(cid:42)(cid:64) (cid:80)(cid:97)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:90)(cid:98)(cid:67)(cid:84)(cid:110) (cid:89)(cid:100)(cid:87)(cid:68)(cid:71)(cid:95)(cid:110) (cid:90)(cid:72)(cid:110) (cid:92)(cid:67)(cid:69)(cid:83)(cid:71)(cid:98)(cid:97)(cid:110) (cid:95)(cid:71)(cid:69)(cid:71)(cid:80)(cid:101)(cid:71)(cid:70)(cid:110) (cid:68)(cid:104)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:110) (cid:29)(cid:52) (cid:92)(cid:71)(cid:95)(cid:110) (cid:100)(cid:89)(cid:80)(cid:98)(cid:110) (cid:98)(cid:80)(cid:87)(cid:71)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:80)(cid:97)(cid:110) (cid:71)(cid:94)(cid:100)(cid:67)(cid:84)(cid:110) (cid:98)(cid:90)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:100)(cid:87)(cid:87)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:67)(cid:69)(cid:83)(cid:71)(cid:98)(cid:97)(cid:110) (cid:72)(cid:95)(cid:90)(cid:87)(cid:110)(cid:67)(cid:84)(cid:84)(cid:110)(cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110) (cid:98)(cid:79)(cid:67)(cid:98)(cid:110)(cid:92)(cid:67)(cid:97)(cid:97)(cid:110) (cid:98)(cid:79)(cid:95)(cid:90)(cid:100)(cid:78)(cid:79)(cid:110) (cid:98)(cid:80)(cid:84)(cid:71)(cid:110)(cid:29)(cid:52) (cid:1)(cid:4) (cid:80)(cid:11) (cid:71)(cid:11)(cid:110) (cid:30)(cid:42)(cid:64) (cid:11)(cid:110) (cid:50)(cid:42)(cid:64) (cid:80)(cid:97)(cid:110) (cid:78)(cid:80)(cid:101)(cid:71)(cid:89)(cid:110) (cid:68)(cid:104)(cid:27)(cid:110) (cid:39)(cid:71)(cid:95)(cid:71)(cid:110) (cid:102)(cid:71)(cid:110) (cid:102)(cid:71)(cid:80)(cid:78)(cid:79)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:70)(cid:67)(cid:92)(cid:98)(cid:80)(cid:101)(cid:80)(cid:98)(cid:104)(cid:110) (cid:90)(cid:72)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:102)(cid:80)(cid:98)(cid:79)(cid:110) (cid:80)(cid:98)(cid:97)(cid:110) (cid:69)(cid:90)(cid:95)(cid:95)(cid:71)(cid:97)(cid:92)(cid:90)(cid:89)(cid:70)(cid:80)(cid:89)(cid:78)(cid:110) (cid:68)(cid:67)(cid:89)(cid:70)(cid:102)(cid:80)(cid:70)(cid:98)(cid:79)(cid:110) (cid:95)(cid:71)(cid:94)(cid:100)(cid:80)(cid:95)(cid:71)(cid:87)(cid:71)(cid:89)(cid:98)(cid:110) (cid:7)(cid:67)(cid:7)(cid:83)(cid:14)(cid:137) (cid:80)(cid:97)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:68)(cid:67)(cid:89)(cid:70)(cid:102)(cid:80)(cid:70)(cid:98)(cid:79)(cid:110) (cid:95)(cid:71)(cid:94)(cid:100)(cid:80)(cid:95)(cid:71)(cid:87)(cid:71)(cid:89)(cid:98)(cid:110) (cid:90)(cid:72)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:83)(cid:137) (cid:67)(cid:89)(cid:70)(cid:110) (cid:18)(cid:137) (cid:80)(cid:97)(cid:110) (cid:67)(cid:110) (cid:98)(cid:100)(cid:89)(cid:109) (cid:80)(cid:89)(cid:78)(cid:110) (cid:69)(cid:90)(cid:71)(cid:74)(cid:69)(cid:80)(cid:71)(cid:89)(cid:98)(cid:6)(cid:110) (cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:110) (cid:95)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:67)(cid:84)(cid:71)(cid:110) (cid:80)(cid:97)(cid:110) (cid:98)(cid:79)(cid:67)(cid:98)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:97)(cid:110) (cid:98)(cid:79)(cid:67)(cid:98)(cid:110) (cid:79)(cid:67)(cid:101)(cid:71)(cid:110) (cid:84)(cid:67)(cid:95)(cid:78)(cid:71)(cid:110) (cid:68)(cid:67)(cid:89)(cid:70)(cid:102)(cid:80)(cid:70)(cid:98)(cid:79)(cid:110) (cid:95)(cid:71)(cid:94)(cid:100)(cid:80)(cid:95)(cid:71)(cid:87)(cid:71)(cid:89)(cid:98)(cid:8)(cid:110) (cid:87)(cid:90)(cid:95)(cid:71)(cid:110) (cid:92)(cid:67)(cid:69)(cid:83)(cid:71)(cid:98)(cid:97)(cid:110) (cid:89)(cid:71)(cid:71)(cid:70)(cid:110) (cid:98)(cid:90)(cid:110) (cid:68)(cid:71)(cid:110) (cid:92)(cid:95)(cid:90)(cid:69)(cid:71)(cid:97)(cid:97)(cid:71)(cid:70)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:70)(cid:8)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:80)(cid:87)(cid:92)(cid:67)(cid:69)(cid:98)(cid:110) (cid:90)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:90)(cid:102)(cid:71)(cid:95)(cid:110) (cid:70)(cid:80)(cid:97)(cid:98)(cid:95)(cid:80)(cid:109) (cid:68)(cid:100)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:80)(cid:97)(cid:110) (cid:79)(cid:80)(cid:78)(cid:79)(cid:71)(cid:95)(cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:95)(cid:71)(cid:72)(cid:90)(cid:95)(cid:71)(cid:110) (cid:102)(cid:71)(cid:110) (cid:97)(cid:79)(cid:90)(cid:100)(cid:84)(cid:70)(cid:110) (cid:79)(cid:67)(cid:101)(cid:71)(cid:110) (cid:79)(cid:80)(cid:78)(cid:79)(cid:71)(cid:95)(cid:110) (cid:77)(cid:71)(cid:103)(cid:80)(cid:68)(cid:80)(cid:84)(cid:80)(cid:98)(cid:104)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:98)(cid:79)(cid:71)(cid:97)(cid:71)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:97)(cid:110) (cid:98)(cid:90)(cid:110) (cid:95)(cid:71)(cid:10)(cid:70)(cid:80)(cid:101)(cid:71)(cid:95)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:79)(cid:71)(cid:89)(cid:69)(cid:71)(cid:110) (cid:79)(cid:80)(cid:78)(cid:79)(cid:71)(cid:95)(cid:110) (cid:67)(cid:70)(cid:67)(cid:92)(cid:98)(cid:80)(cid:101)(cid:80)(cid:98)(cid:104)(cid:11)(cid:110) (cid:42)(cid:13)(cid:116) (cid:43)(cid:114)(cid:123)(cid:92)(cid:102)(cid:67)(cid:99)(cid:136) (cid:123)(cid:116)(cid:67)(cid:83)(cid:73)(cid:136) (cid:67)(cid:99)(cid:99)(cid:110)(cid:73)(cid:67)(cid:123)(cid:92)(cid:110)(cid:106)(cid:136) (cid:41)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:72)(cid:90)(cid:84)(cid:84)(cid:90)(cid:102)(cid:80)(cid:89)(cid:78)(cid:110) (cid:72)(cid:90)(cid:95)(cid:87)(cid:100)(cid:84)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:100)(cid:97)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:67)(cid:95)(cid:67)(cid:87)(cid:71)(cid:98)(cid:71)(cid:95)(cid:97)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:89)(cid:90)(cid:98)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:97)(cid:110) (cid:97)(cid:100)(cid:87)(cid:87)(cid:67)(cid:95)(cid:80)(cid:105)(cid:71)(cid:70)(cid:110) (cid:80)(cid:89)(cid:110)(cid:53)(cid:67)(cid:68)(cid:84)(cid:71)(cid:110) (cid:41)(cid:11)(cid:110) (cid:120)(cid:86)(cid:72)(cid:17)(cid:137)(cid:22)(cid:4)(cid:52)(cid:33)(cid:2)(cid:52) (cid:8)(cid:9) (cid:117)(cid:86)(cid:72)(cid:17)(cid:137)(cid:22)(cid:2)(cid:52) (cid:76)(cid:97)(cid:3)(cid:79)(cid:5)(cid:81)(cid:5)(cid:93)(cid:4)(cid:49)(cid:69)(cid:91)(cid:116) (cid:41)(cid:89)(cid:110) (cid:90)(cid:95)(cid:70)(cid:71)(cid:95)(cid:110) (cid:98)(cid:90)(cid:110) (cid:68)(cid:67)(cid:84)(cid:67)(cid:89)(cid:69)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:80)(cid:84)(cid:71)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:18)(cid:42)(cid:4)(cid:64) (cid:90)(cid:100)(cid:95)(cid:110) (cid:90)(cid:68)(cid:82)(cid:71)(cid:69)(cid:98)(cid:80)(cid:101)(cid:71)(cid:110) (cid:72)(cid:100)(cid:89)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:80)(cid:97)(cid:110) (cid:102)(cid:95)(cid:80)(cid:98)(cid:98)(cid:71)(cid:89)(cid:110) (cid:80)(cid:89)(cid:110) (cid:67)(cid:110) (cid:87)(cid:80)(cid:89)(cid:10)(cid:87)(cid:67)(cid:103)(cid:110) (cid:72)(cid:90)(cid:95)(cid:87)(cid:110) (cid:67)(cid:97)(cid:110) (cid:72)(cid:90)(cid:84)(cid:84)(cid:90)(cid:102)(cid:97)(cid:27)(cid:110) (cid:40)(cid:22)(cid:31)(cid:8)(cid:52) (cid:38)(cid:29)(cid:39)(cid:1)(cid:38)(cid:21)(cid:49)(cid:1)(cid:11)(cid:30) (cid:2) (cid:2) (cid:52) (cid:26)(cid:40)(cid:43)(cid:52)(cid:16)(cid:110)(cid:9)(cid:52) (cid:29)(cid:52) (cid:28)(cid:110) (cid:46)(cid:110) (cid:11)(cid:110) (cid:41)(cid:98)(cid:110) (cid:80)(cid:97)(cid:110) (cid:71)(cid:94)(cid:100)(cid:80)(cid:101)(cid:67)(cid:84)(cid:71)(cid:89)(cid:98)(cid:110) (cid:98)(cid:90) (cid:27)(cid:110) (cid:110)(cid:29)(cid:50) (cid:10) (cid:18)(cid:6)(cid:110) (cid:44)(cid:5)(cid:45)(cid:5)(cid:52)(cid:18)(cid:13)(cid:14)(cid:64) (cid:18)(cid:42)(cid:64) (cid:26)(cid:40)(cid:43) (cid:12)(cid:9)(cid:52) (cid:29)(cid:9)(cid:52) (cid:46)(cid:110) (cid:18)(cid:5)(cid:110) (cid:48)(cid:117)(cid:111)(cid:70)(cid:100)(cid:79)(cid:103)(cid:136) (cid:74)(cid:111)(cid:108)(cid:120)(cid:124)(cid:117)(cid:68)(cid:93)(cid:108)(cid:124)(cid:120)(cid:21)(cid:136) (cid:53)(cid:79)(cid:71)(cid:110) (cid:90)(cid:68)(cid:82)(cid:71)(cid:69)(cid:98)(cid:80)(cid:101)(cid:71)(cid:110) (cid:72)(cid:100)(cid:89)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:80)(cid:97)(cid:110) (cid:90)(cid:92)(cid:98)(cid:80)(cid:109) (cid:87)(cid:80)(cid:105)(cid:71)(cid:70)(cid:110) (cid:97)(cid:100)(cid:68)(cid:82)(cid:71)(cid:69)(cid:98)(cid:110) (cid:98)(cid:90)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:72)(cid:90)(cid:84)(cid:84)(cid:90)(cid:102)(cid:80)(cid:89)(cid:78)(cid:110) (cid:69)(cid:90)(cid:89)(cid:97)(cid:98)(cid:95)(cid:67)(cid:80)(cid:89)(cid:98)(cid:97)(cid:27)(cid:110) (cid:19)(cid:10)(cid:16)(cid:6)(cid:110)(cid:55)(cid:67)(cid:74)(cid:69)(cid:110) (cid:97)(cid:92)(cid:84)(cid:80)(cid:98)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:69)(cid:90)(cid:89)(cid:97)(cid:98)(cid:95)(cid:67)(cid:80)(cid:89)(cid:98)(cid:97) (cid:27)(cid:110) (cid:52)(cid:100)(cid:87)(cid:87)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:90)(cid:72)(cid:110) (cid:67)(cid:84)(cid:84)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:67)(cid:84)(cid:84)(cid:90)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:95)(cid:67)(cid:98)(cid:80)(cid:90)(cid:97)(cid:110) (cid:68)(cid:71)(cid:98)(cid:102)(cid:71)(cid:71)(cid:89)(cid:110) (cid:67)(cid:110) (cid:78)(cid:80)(cid:101)(cid:71)(cid:89)(cid:110) (cid:92)(cid:67)(cid:80)(cid:95)(cid:110) (cid:1)(cid:29)(cid:4)(cid:52)(cid:31) (cid:2) (cid:52) (cid:102)(cid:79)(cid:80)(cid:69)(cid:79)(cid:110) (cid:68)(cid:71)(cid:84)(cid:90)(cid:89)(cid:78)(cid:97)(cid:110) (cid:98)(cid:90)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:97)(cid:71)(cid:98)(cid:110) (cid:44)(cid:116) (cid:97)(cid:79)(cid:90)(cid:100)(cid:84)(cid:70)(cid:110) (cid:71)(cid:94)(cid:100)(cid:67)(cid:84)(cid:110) (cid:98)(cid:90)(cid:110) (cid:16)(cid:11)(cid:110) (cid:18)(cid:19)(cid:15)(cid:136) (cid:32)(cid:34)(cid:20)(cid:31)(cid:137) (cid:49)(cid:98)(cid:15)(cid:102)(cid:137) (cid:104)(cid:38)(cid:35)(cid:52) (cid:1) (cid:43)(cid:1)(cid:29)(cid:4)(cid:31)(cid:4)(cid:33)(cid:2)(cid:52) (cid:2)(cid:4) (cid:16)(cid:110)(cid:18)(cid:1)(cid:29)(cid:4)(cid:31)(cid:2)(cid:52) (cid:48)(cid:116) (cid:44)(cid:116) (cid:43)(cid:1)(cid:29)(cid:4)(cid:31)(cid:4)(cid:33)(cid:2)(cid:52) (cid:135)(cid:136) (cid:15)(cid:18)(cid:29)(cid:4)(cid:31)(cid:52) (cid:48)(cid:116) (cid:61)(cid:40)(cid:8) (cid:46)(cid:63)(cid:8)(cid:110) (cid:33)(cid:9)(cid:52) (cid:21)(cid:42)(cid:43)(cid:64) (cid:19)(cid:10)(cid:17)(cid:6)(cid:110) (cid:32)(cid:67)(cid:89)(cid:70)(cid:102)(cid:80)(cid:70)(cid:98)(cid:79)(cid:110) (cid:69)(cid:90)(cid:89)(cid:97)(cid:98)(cid:95)(cid:67)(cid:80)(cid:89)(cid:98)(cid:97) (cid:27)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:67)(cid:78)(cid:78)(cid:95)(cid:71)(cid:78)(cid:67)(cid:98)(cid:71)(cid:110) (cid:68)(cid:67)(cid:89)(cid:70)(cid:102)(cid:80)(cid:70)(cid:98)(cid:79)(cid:110) (cid:100)(cid:97)(cid:71)(cid:70)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:67)(cid:110) (cid:97)(cid:92)(cid:71)(cid:69)(cid:80)(cid:76)(cid:69)(cid:110) (cid:84)(cid:80)(cid:89)(cid:83)(cid:110) (cid:97)(cid:79)(cid:90)(cid:100)(cid:84)(cid:70)(cid:110) (cid:89)(cid:90)(cid:98)(cid:110) (cid:71)(cid:103)(cid:69)(cid:71)(cid:71)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:84)(cid:80)(cid:89)(cid:83)(cid:110) (cid:69)(cid:67)(cid:92)(cid:67)(cid:69)(cid:80)(cid:98)(cid:104)(cid:11)(cid:110) (cid:52)(cid:80)(cid:89)(cid:69)(cid:71)(cid:110) (cid:23)(cid:40)(cid:38)(cid:38)(cid:47)(cid:39)(cid:29)(cid:23)(cid:21)(cid:45)(cid:29)(cid:40)(cid:39)(cid:52) (cid:22)(cid:21)(cid:39)(cid:24)(cid:48)(cid:29)(cid:24)(cid:45)(cid:27)(cid:52) (cid:3)(cid:4) (cid:41)(cid:21)(cid:23)(cid:33)(cid:25)(cid:45)(cid:52) (cid:29)(cid:39)(cid:31)(cid:25)(cid:23)(cid:45)(cid:29)(cid:40)(cid:39)(cid:52) (cid:43)(cid:21)(cid:45)(cid:25)(cid:52) (cid:8)(cid:9) (cid:41)(cid:21)(cid:23)(cid:33)(cid:25)(cid:45)(cid:52)(cid:44)(cid:29)(cid:50)(cid:25)(cid:52) (cid:8)(cid:9) (cid:23)(cid:36)(cid:40)(cid:23)(cid:33)(cid:52)(cid:26)(cid:43)(cid:25)(cid:42)(cid:52) (cid:11)(cid:110) (cid:44)(cid:71)(cid:98)(cid:110)(cid:30)(cid:64) (cid:68)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:69)(cid:104)(cid:69)(cid:84)(cid:71)(cid:110) (cid:98)(cid:80)(cid:87)(cid:71)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110)(cid:1)(cid:29)(cid:4)(cid:52)(cid:31)(cid:2)(cid:52) (cid:68)(cid:71)(cid:110) (cid:67)(cid:110) (cid:92)(cid:79)(cid:104)(cid:97)(cid:80)(cid:69)(cid:67)(cid:84)(cid:110) (cid:84)(cid:80)(cid:89)(cid:83)(cid:110) (cid:80)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:87)(cid:71)(cid:97)(cid:79)(cid:8)(cid:110) (cid:67)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:110) (cid:36)(cid:1)(cid:21)(cid:4)(cid:52)(cid:22)(cid:4)(cid:52)(cid:33)(cid:2)(cid:52) (cid:102)(cid:80)(cid:84)(cid:84)(cid:110) (cid:98)(cid:95)(cid:67)(cid:101)(cid:71)(cid:95)(cid:97)(cid:71)(cid:110) (cid:98)(cid:79)(cid:80)(cid:97)(cid:110) (cid:84)(cid:80)(cid:89)(cid:83)(cid:110) (cid:80)(cid:72)(cid:110) (cid:36)(cid:1)(cid:21)(cid:4)(cid:52)(cid:22)(cid:4)(cid:52)(cid:33)(cid:2)(cid:52) (cid:48)(cid:116) (cid:30)(cid:42)(cid:64)(cid:101)(cid:116) (cid:30)(cid:43)(cid:7)(cid:64) (cid:52)(cid:90)(cid:110) (cid:102)(cid:71)(cid:110) (cid:79)(cid:67)(cid:101)(cid:71)(cid:110) (cid:66)(cid:48)(cid:6)(cid:71)(cid:16)(cid:76)(cid:16)(cid:104)(cid:11)(cid:44)(cid:61)(cid:98)(cid:112)(cid:61)(cid:102)(cid:137) (cid:43)(cid:1)(cid:21)(cid:4)(cid:52)(cid:22)(cid:4)(cid:52)(cid:32)(cid:2)(cid:52) (cid:8)(cid:9) (cid:41)(cid:1)(cid:21)(cid:4)(cid:52)(cid:22)(cid:2)(cid:52) (cid:8)(cid:9) (cid:27)(cid:34)(cid:42)(cid:57)(cid:64) (cid:40)(cid:116) (cid:44) (cid:7)(cid:12)(cid:116) (cid:2) (cid:1)(cid:3)(cid:9) (cid:30)(cid:64) (cid:38)(cid:80)(cid:101)(cid:71)(cid:89)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:67)(cid:92)(cid:92)(cid:84)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:3)(cid:97)(cid:110) (cid:98)(cid:67)(cid:97)(cid:83)(cid:110)(cid:77)(cid:90)(cid:102)(cid:110) (cid:78)(cid:95)(cid:67)(cid:92)(cid:79)(cid:110)(cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:67)(cid:97)(cid:83)(cid:110)(cid:87)(cid:67)(cid:92)(cid:92)(cid:80)(cid:89)(cid:78)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110)(cid:92)(cid:67)(cid:69)(cid:83)(cid:71)(cid:98)(cid:110)(cid:80)(cid:89)(cid:82)(cid:71)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:95)(cid:67)(cid:98)(cid:71)(cid:110)(cid:41)(cid:1) (cid:52)(cid:21)(cid:4)(cid:52)(cid:22)(cid:2)(cid:52) (cid:69)(cid:67)(cid:89)(cid:110)(cid:68)(cid:71)(cid:110)(cid:69)(cid:67)(cid:84)(cid:69)(cid:100)(cid:84)(cid:67)(cid:98)(cid:71)(cid:70)(cid:110)(cid:68)(cid:104)(cid:110) (cid:97)(cid:100)(cid:87)(cid:87)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:68)(cid:67)(cid:89)(cid:70)(cid:102)(cid:80)(cid:70)(cid:98)(cid:79)(cid:110) (cid:95)(cid:71)(cid:94)(cid:100)(cid:80)(cid:95)(cid:71)(cid:87)(cid:71)(cid:89)(cid:98)(cid:110) (cid:72)(cid:95)(cid:90)(cid:87)(cid:110) (cid:67)(cid:84)(cid:84)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:92)(cid:67)(cid:80)(cid:95)(cid:97)(cid:110) (cid:102)(cid:79)(cid:71)(cid:95)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:90)(cid:100)(cid:95)(cid:69)(cid:71)(cid:110) (cid:98)(cid:67)(cid:97)(cid:83)(cid:97)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:87)(cid:67)(cid:92)(cid:92)(cid:71)(cid:70)(cid:110) (cid:90)(cid:89)(cid:98)(cid:90)(cid:110) (cid:98)(cid:80)(cid:84)(cid:71)(cid:110)(cid:21)(cid:52) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:70)(cid:71)(cid:97)(cid:98)(cid:80)(cid:109) (cid:89)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:98)(cid:67)(cid:97)(cid:83)(cid:97)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:87)(cid:67)(cid:92)(cid:92)(cid:71)(cid:70)(cid:110) (cid:90)(cid:89)(cid:98)(cid:90)(cid:110) (cid:98)(cid:80)(cid:84)(cid:71)(cid:110)(cid:22)(cid:5)(cid:52) (cid:53)(cid:79)(cid:71)(cid:110)(cid:67)(cid:68)(cid:90)(cid:101)(cid:71)(cid:110)(cid:72)(cid:90)(cid:95)(cid:87)(cid:100)(cid:84)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:80)(cid:97)(cid:110)(cid:67)(cid:110)(cid:98)(cid:104)(cid:92)(cid:80)(cid:69)(cid:67)(cid:84)(cid:110)(cid:84)(cid:80)(cid:89)(cid:71)(cid:67)(cid:95)(cid:110)(cid:92)(cid:95)(cid:90)(cid:78)(cid:95)(cid:67)(cid:87)(cid:87)(cid:80)(cid:89)(cid:78)(cid:110)(cid:92)(cid:95)(cid:90)(cid:68)(cid:109) (cid:84)(cid:71)(cid:87)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:69)(cid:67)(cid:89)(cid:110) (cid:68)(cid:71)(cid:110) (cid:97)(cid:90)(cid:84)(cid:101)(cid:71)(cid:70)(cid:110) (cid:71)(cid:67)(cid:97)(cid:80)(cid:84)(cid:104)(cid:11)(cid:110) (cid:43)(cid:14)(cid:26)(cid:116) (cid:56)(cid:118)(cid:91)(cid:104)(cid:85)(cid:136) (cid:122)(cid:88)(cid:77)(cid:136) (cid:113)(cid:66)(cid:122)(cid:88)(cid:136) (cid:115)(cid:66)(cid:122)(cid:91)(cid:109)(cid:118)(cid:136) (cid:91)(cid:104)(cid:136) (cid:122)(cid:88)(cid:77)(cid:136) (cid:115)(cid:109)(cid:125)(cid:122)(cid:77)(cid:115)(cid:136) (cid:53)(cid:90)(cid:110) (cid:100)(cid:97)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:71)(cid:98)(cid:110) (cid:90)(cid:72)(cid:110) (cid:67)(cid:70)(cid:87)(cid:80)(cid:97)(cid:97)(cid:80)(cid:68)(cid:84)(cid:71)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:80)(cid:95)(cid:110) (cid:69)(cid:90)(cid:95)(cid:95)(cid:71)(cid:97)(cid:92)(cid:90)(cid:89)(cid:70)(cid:109) (cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:67)(cid:84)(cid:84)(cid:90)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:95)(cid:67)(cid:98)(cid:80)(cid:90)(cid:97)(cid:110) (cid:80)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:80)(cid:87)(cid:92)(cid:84)(cid:71)(cid:87)(cid:71)(cid:89)(cid:98)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:47)(cid:90)(cid:33)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:100)(cid:97)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:67)(cid:68)(cid:84)(cid:71)(cid:97)(cid:110) (cid:97)(cid:98)(cid:90)(cid:95)(cid:71)(cid:70)(cid:110) (cid:80)(cid:89)(cid:110)(cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:11)(cid:110) (cid:52)(cid:80)(cid:89)(cid:69)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:70)(cid:71)(cid:69)(cid:80)(cid:97)(cid:80)(cid:90)(cid:89)(cid:110) (cid:80)(cid:97)(cid:110) (cid:87)(cid:67)(cid:70)(cid:71)(cid:110) (cid:102)(cid:80)(cid:98)(cid:79)(cid:80)(cid:89)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:110) (cid:102)(cid:80)(cid:98)(cid:79)(cid:90)(cid:100)(cid:98)(cid:110) (cid:83)(cid:89)(cid:90)(cid:102)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:71)(cid:89)(cid:98)(cid:80)(cid:95)(cid:71)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:110) (cid:80)(cid:89)(cid:72)(cid:90)(cid:95)(cid:87)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:10)(cid:67)(cid:84)(cid:84)(cid:90)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:95)(cid:67)(cid:98)(cid:80)(cid:90)(cid:97)(cid:110) (cid:69)(cid:67)(cid:89)(cid:110) (cid:89)(cid:90)(cid:98)(cid:110) (cid:68)(cid:71)(cid:110) (cid:70)(cid:80)(cid:95)(cid:71)(cid:69)(cid:98)(cid:84)(cid:104)(cid:110) (cid:100)(cid:97)(cid:71)(cid:70)(cid:11)(cid:110) (cid:60)(cid:71)(cid:110) (cid:102)(cid:80)(cid:84)(cid:84)(cid:110) (cid:97)(cid:79)(cid:90)(cid:102)(cid:110) (cid:79)(cid:90)(cid:102)(cid:110) (cid:98)(cid:90)(cid:110) (cid:69)(cid:90)(cid:89)(cid:101)(cid:71)(cid:95)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:97)(cid:71)(cid:110) (cid:95)(cid:67)(cid:98)(cid:80)(cid:90)(cid:97)(cid:110) (cid:80)(cid:89)(cid:98)(cid:90)(cid:110) (cid:84)(cid:90)(cid:69)(cid:67)(cid:84)(cid:110) (cid:92)(cid:95)(cid:90)(cid:68)(cid:67)(cid:68)(cid:80)(cid:84)(cid:80)(cid:98)(cid:104)(cid:110) (cid:101)(cid:67)(cid:84)(cid:100)(cid:71)(cid:97)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:110) (cid:98)(cid:90)(cid:110) (cid:97)(cid:71)(cid:84)(cid:71)(cid:69)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:90)(cid:100)(cid:98)(cid:92)(cid:100)(cid:98)(cid:110) (cid:92)(cid:90)(cid:95)(cid:98)(cid:11)(cid:110) (cid:53)(cid:90)(cid:110) (cid:97)(cid:100)(cid:92)(cid:92)(cid:90)(cid:95)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:95)(cid:87)(cid:67)(cid:84)(cid:10)(cid:67)(cid:102)(cid:67)(cid:95)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:67)(cid:68)(cid:84)(cid:71)(cid:110) (cid:80)(cid:97)(cid:110) (cid:90)(cid:95)(cid:78)(cid:67)(cid:89)(cid:80)(cid:105)(cid:71)(cid:70)(cid:110) (cid:67)(cid:97)(cid:110) (cid:72)(cid:90)(cid:84)(cid:84)(cid:90)(cid:102)(cid:97)(cid:11)(cid:110) (cid:36)(cid:90)(cid:95)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:110) (cid:45)(cid:52) (cid:67)(cid:89)(cid:70)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:90)(cid:72)(cid:110) (cid:80)(cid:98)(cid:97)(cid:110) (cid:80)(cid:89)(cid:92)(cid:100)(cid:98)(cid:110) (cid:70)(cid:80)(cid:95)(cid:71)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:24)(cid:52) (cid:48)(cid:116) (cid:16)(cid:42)(cid:49)(cid:1)(cid:58)(cid:2)(cid:4)(cid:64) (cid:98)(cid:79)(cid:71)(cid:95)(cid:71)(cid:110) (cid:80)(cid:97)(cid:110) (cid:67)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:67)(cid:68)(cid:84)(cid:71)(cid:110) (cid:16)(cid:17)(cid:1)(cid:45)(cid:4)(cid:52)(cid:24)(cid:2)(cid:52) (cid:19)(cid:6)(cid:20)(cid:5)(cid:52) (cid:36)(cid:90)(cid:95)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:90)(cid:100)(cid:98)(cid:92)(cid:100)(cid:98)(cid:110) (cid:92)(cid:90)(cid:95)(cid:98)(cid:110) (cid:8)(cid:4)(cid:64) (cid:98)(cid:79)(cid:71)(cid:95)(cid:71)(cid:110) (cid:80)(cid:97)(cid:110) (cid:67)(cid:110) (cid:97)(cid:71)(cid:98)(cid:110) (cid:90)(cid:72)(cid:110) (cid:69)(cid:90)(cid:95)(cid:95)(cid:71)(cid:97)(cid:92)(cid:90)(cid:89)(cid:70)(cid:80)(cid:89)(cid:78)(cid:110) (cid:71)(cid:89)(cid:98)(cid:95)(cid:80)(cid:71)(cid:97)(cid:110) (cid:80)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:67)(cid:68)(cid:84)(cid:71)(cid:11)(cid:110) (cid:35)(cid:67)(cid:69)(cid:79)(cid:110) (cid:71)(cid:89)(cid:98)(cid:95)(cid:104)(cid:110) (cid:69)(cid:90)(cid:89)(cid:97)(cid:80)(cid:97)(cid:98)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:80)(cid:84)(cid:71)(cid:110) (cid:80)(cid:70)(cid:110) (cid:89)(cid:100)(cid:87)(cid:68)(cid:71)(cid:95)(cid:97)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:90)(cid:100)(cid:95)(cid:69)(cid:71)(cid:110) (cid:1)(cid:44)(cid:2) (cid:4) (cid:52)(cid:67)(cid:89)(cid:70)(cid:110) (cid:70)(cid:71)(cid:97)(cid:98)(cid:80)(cid:89)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:1)(cid:22)(cid:2)(cid:52) (cid:92)(cid:67)(cid:80)(cid:95)(cid:8)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110)(cid:92)(cid:95)(cid:90)(cid:68)(cid:67)(cid:68)(cid:80)(cid:84)(cid:80)(cid:98)(cid:104)(cid:110) (cid:101)(cid:67)(cid:84)(cid:100)(cid:71)(cid:97)(cid:110) (cid:41)(cid:1)(cid:40)(cid:2)(cid:52) (cid:90)(cid:72)(cid:110) (cid:100)(cid:97)(cid:80)(cid:89)(cid:78)(cid:110) (cid:90)(cid:100)(cid:98)(cid:92)(cid:100)(cid:98)(cid:110) (cid:92)(cid:90)(cid:95)(cid:98)(cid:110) (cid:8)(cid:64) (cid:98)(cid:90)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:110) (cid:98)(cid:90)(cid:110) (cid:22) (cid:5) (cid:52)(cid:36)(cid:90)(cid:95)(cid:87)(cid:67)(cid:84)(cid:84)(cid:104)(cid:8)(cid:110) (cid:16)(cid:17)(cid:1)(cid:45)(cid:4)(cid:52)(cid:24)(cid:2)(cid:52) (cid:2)(cid:4) (cid:51)(cid:1)(cid:44)(cid:4)(cid:22)(cid:4)(cid:40)(cid:4)(cid:41)(cid:1)(cid:40)(cid:2)(cid:37)(cid:40)(cid:52) (cid:48)(cid:116) (cid:16)(cid:51)(cid:59)(cid:57)(cid:1)(cid:58)(cid:2)(cid:4)(cid:23)(cid:12)(cid:64) (cid:41)(cid:1)(cid:40)(cid:2)(cid:9)(cid:52) (cid:40)(cid:107) (cid:11)(cid:110) (cid:47)(cid:90)(cid:102)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:80)(cid:97)(cid:97)(cid:100)(cid:71)(cid:110)(cid:80)(cid:97)(cid:110)(cid:79)(cid:90)(cid:102)(cid:110)(cid:98)(cid:90)(cid:110)(cid:90)(cid:68)(cid:98)(cid:67)(cid:80)(cid:89)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:92)(cid:95)(cid:90)(cid:68)(cid:67)(cid:68)(cid:80)(cid:84)(cid:80)(cid:98)(cid:104)(cid:110)(cid:101)(cid:67)(cid:84)(cid:100)(cid:71)(cid:97)(cid:93)(cid:4)(cid:91)(cid:6)(cid:110)(cid:72)(cid:90)(cid:95)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110)(cid:90)(cid:100)(cid:98)(cid:92)(cid:100)(cid:98)(cid:110)(cid:92)(cid:90)(cid:95)(cid:98)(cid:110)(cid:40)(cid:5)(cid:52) (cid:36)(cid:90)(cid:95)(cid:110)(cid:71)(cid:67)(cid:69)(cid:79)(cid:110)(cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:110)(cid:45)(cid:52)(cid:67)(cid:89)(cid:70)(cid:110)(cid:71)(cid:67)(cid:69)(cid:79)(cid:110)(cid:90)(cid:72)(cid:110)(cid:80)(cid:98)(cid:97)(cid:110)(cid:80)(cid:89)(cid:92)(cid:100)(cid:98)(cid:110)(cid:92)(cid:90)(cid:95)(cid:98)(cid:110) (cid:98)(cid:79)(cid:95)(cid:90)(cid:100)(cid:78)(cid:79)(cid:110)(cid:45)(cid:52)(cid:100)(cid:97)(cid:80)(cid:89)(cid:78)(cid:110)(cid:80)(cid:89)(cid:92)(cid:100)(cid:98)(cid:110)(cid:92)(cid:90)(cid:95)(cid:98)(cid:110)(cid:24)(cid:5)(cid:52) (cid:30)(cid:97)(cid:97)(cid:100)(cid:87)(cid:71)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:97)(cid:71)(cid:98)(cid:110)(cid:30)(cid:57)(cid:5)(cid:37) (cid:5) (cid:25)(cid:4)(cid:64)(cid:44)(cid:4)(cid:52)(cid:22)(cid:2)(cid:52) (cid:134)(cid:136) (cid:30)(cid:57)(cid:5)(cid:37)(cid:64) (cid:24)(cid:4)(cid:52) (cid:102)(cid:71)(cid:110) (cid:90)(cid:68)(cid:98)(cid:67)(cid:80)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:100)(cid:68)(cid:97)(cid:71)(cid:98)(cid:110) (cid:90)(cid:72)(cid:110) (cid:67)(cid:70)(cid:87)(cid:80)(cid:97)(cid:97)(cid:80)(cid:68)(cid:84)(cid:71)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110) (cid:3)(cid:30)(cid:57)(cid:5)(cid:37) (cid:6)(cid:136) (cid:98)(cid:79)(cid:67)(cid:98)(cid:110) (cid:92)(cid:67)(cid:97)(cid:97)(cid:110) (cid:69)(cid:90)(cid:89)(cid:98)(cid:67)(cid:80)(cid:89)(cid:97)(cid:110)(cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110)(cid:98)(cid:79)(cid:67)(cid:98)(cid:110)(cid:70)(cid:71)(cid:92)(cid:67)(cid:95)(cid:98)(cid:110)(cid:72)(cid:95)(cid:90)(cid:87)(cid:110)(cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:110)(cid:45)(cid:52)(cid:98)(cid:79)(cid:95)(cid:90)(cid:100)(cid:78)(cid:79)(cid:110)(cid:90)(cid:100)(cid:98)(cid:92)(cid:100)(cid:98)(cid:110)(cid:92)(cid:90)(cid:95)(cid:98)(cid:110)(cid:8)(cid:64) (cid:102)(cid:80)(cid:98)(cid:79)(cid:110) (cid:97)(cid:90)(cid:100)(cid:95)(cid:69)(cid:71)(cid:110) (cid:44)(cid:52) (cid:67)(cid:89)(cid:70)(cid:110) (cid:70)(cid:71)(cid:97)(cid:98)(cid:80)(cid:89)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:22)(cid:5)(cid:52) (cid:36)(cid:90)(cid:95)(cid:110) (cid:67)(cid:110) (cid:78)(cid:80)(cid:101)(cid:71)(cid:89)(cid:110)(cid:70)(cid:71)(cid:97)(cid:98)(cid:80)(cid:89)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:98)(cid:80)(cid:84)(cid:71)(cid:110)(cid:22)(cid:4)(cid:52) (cid:100)(cid:97)(cid:80)(cid:89)(cid:78)(cid:110) (cid:87)(cid:80)(cid:89)(cid:80)(cid:87)(cid:67)(cid:84)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:8)(cid:110) (cid:90)(cid:89)(cid:84)(cid:104)(cid:110) (cid:98)(cid:102)(cid:90)(cid:110) (cid:69)(cid:67)(cid:89)(cid:70)(cid:80)(cid:70)(cid:67)(cid:98)(cid:71)(cid:110) (cid:90)(cid:100)(cid:98)(cid:92)(cid:100)(cid:98)(cid:110) (cid:92)(cid:90)(cid:95)(cid:98)(cid:97)(cid:110) (cid:1)(cid:8)(cid:10)(cid:64) (cid:67)(cid:89)(cid:70)(cid:110) (cid:8)(cid:11)(cid:2)(cid:64) (cid:67)(cid:95)(cid:71)(cid:110) (cid:72)(cid:71)(cid:67)(cid:97)(cid:80)(cid:68)(cid:84)(cid:71)(cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:110) (cid:92)(cid:95)(cid:90)(cid:68)(cid:67)(cid:68)(cid:80)(cid:84)(cid:80)(cid:98)(cid:80)(cid:71)(cid:97)(cid:110) (cid:90)(cid:72)(cid:110) (cid:100)(cid:97)(cid:80)(cid:89)(cid:78)(cid:110) (cid:92)(cid:90)(cid:95)(cid:98)(cid:97)(cid:110) (cid:8)(cid:10)(cid:64) (cid:67)(cid:89)(cid:70)(cid:110) (cid:8)(cid:11)(cid:64) (cid:67)(cid:95)(cid:71)(cid:110) (cid:95)(cid:71)(cid:92)(cid:95)(cid:71)(cid:97)(cid:71)(cid:89)(cid:98)(cid:71)(cid:70)(cid:110)(cid:67)(cid:97)(cid:110)(cid:26)(cid:57)(cid:5)(cid:37)(cid:64)(cid:1)(cid:8)(cid:10)(cid:4)(cid:64)(cid:44)(cid:4)(cid:52)(cid:22)(cid:2)(cid:52) (cid:67)(cid:89)(cid:70)(cid:110)(cid:26)(cid:57)(cid:5)(cid:37)(cid:64)(cid:1)(cid:8)(cid:11)(cid:4)(cid:64)(cid:44)(cid:4)(cid:52)(cid:22)(cid:2) (cid:5)(cid:52) (cid:53)(cid:79)(cid:71)(cid:104)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:69)(cid:67)(cid:84)(cid:69)(cid:100)(cid:84)(cid:67)(cid:98)(cid:71)(cid:70)(cid:110) (cid:68)(cid:104)(cid:110) (cid:69)(cid:90)(cid:87)(cid:92)(cid:67)(cid:95)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:67)(cid:78)(cid:78)(cid:95)(cid:71)(cid:78)(cid:67)(cid:98)(cid:71)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110) (cid:80)(cid:89)(cid:97)(cid:80)(cid:70)(cid:71)(cid:110) (cid:30)(cid:57)(cid:5)(cid:37)(cid:64) (cid:98)(cid:79)(cid:67)(cid:98)(cid:110) (cid:100)(cid:97)(cid:71)(cid:110) (cid:92)(cid:90)(cid:95)(cid:98)(cid:97)(cid:110) (cid:8)(cid:10)(cid:64) (cid:67)(cid:89)(cid:70)(cid:110) (cid:8)(cid:11)(cid:4)(cid:64) (cid:95)(cid:71)(cid:97)(cid:92)(cid:71)(cid:69)(cid:98)(cid:80)(cid:101)(cid:71)(cid:84)(cid:104)(cid:8)(cid:110) (cid:98)(cid:90)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:110) (cid:98)(cid:90)(cid:110) (cid:98)(cid:80)(cid:84)(cid:71)(cid:110) (cid:22)(cid:5)(cid:52) (cid:36)(cid:90)(cid:95)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110) (cid:36)(cid:1)(cid:44)(cid:4)(cid:22)(cid:4)(cid:33)(cid:2)(cid:52) (cid:48)(cid:116) (cid:30)(cid:57)(cid:16)(cid:37) (cid:1)(cid:14)(cid:35)(cid:4)(cid:28)(cid:4)(cid:22)(cid:2)(cid:52) (cid:67)(cid:89)(cid:70)(cid:110) (cid:36)(cid:1)(cid:44)(cid:4)(cid:22)(cid:4)(cid:36)(cid:2)(cid:52) (cid:48)(cid:116) (cid:30)(cid:57)(cid:5)(cid:37)(cid:1)(cid:8)(cid:11)(cid:4)(cid:64)(cid:44)(cid:4)(cid:52)(cid:22)(cid:2)(cid:4)(cid:52)(cid:102)(cid:71)(cid:110) (cid:79)(cid:67)(cid:101)(cid:71)(cid:110) (cid:26)(cid:57)(cid:5)(cid:37) (cid:3) (cid:25)(cid:46)(cid:4)(cid:28)(cid:4)(cid:35) (cid:9)(cid:137)(cid:3) (cid:26)(cid:57)(cid:5)(cid:37)(cid:1)(cid:8)(cid:11)(cid:4)(cid:28)(cid:4)(cid:35)(cid:2)(cid:64) (cid:2)(cid:4) (cid:16)(cid:110) (cid:26)(cid:57)(cid:5)(cid:37) (cid:4) (cid:25)(cid:46)(cid:4)(cid:64)(cid:44)(cid:4)(cid:52)(cid:22)(cid:2)(cid:52) (cid:26)(cid:57)(cid:5)(cid:37)(cid:1)(cid:8)(cid:11)(cid:4)(cid:64)(cid:44)(cid:4)(cid:52)(cid:22)(cid:2)(cid:52) (cid:30)(cid:66)(cid:48)(cid:6)(cid:123)(cid:16)(cid:76)(cid:16)(cid:104)(cid:11)(cid:44)(cid:61)(cid:128)(cid:15)(cid:79)(cid:6)(cid:54)(cid:106)(cid:16)(cid:58)(cid:16)(cid:76)(cid:11)(cid:137)(cid:41)(cid:1)(cid:44)(cid:4)(cid:52)(cid:22)(cid:2)(cid:52) (cid:8)(cid:9) (cid:43)(cid:1)(cid:44)(cid:4)(cid:52)(cid:22)(cid:4)(cid:52)(cid:33)(cid:2)(cid:52) (cid:30)(cid:66)(cid:48)(cid:6)(cid:33)(cid:16)(cid:76)(cid:16)(cid:48)(cid:11)(cid:44)(cid:61)(cid:128)(cid:15)(cid:79)(cid:6)(cid:25)(cid:28)(cid:16)(cid:33)(cid:16)(cid:76)(cid:11)(cid:137)(cid:41)(cid:1)(cid:44)(cid:4)(cid:52)(cid:22)(cid:2)(cid:52) (cid:8)(cid:9) (cid:43)(cid:1)(cid:44)(cid:4)(cid:52)(cid:22)(cid:4)(cid:52)(cid:47)(cid:10)(cid:137) (cid:43)(cid:88)(cid:82)(cid:15)(cid:122) (cid:26)(cid:19)(cid:122) (cid:71)(cid:93)(cid:101)(cid:72)(cid:91)(cid:122) (cid:73)(cid:88)(cid:70)(cid:82)(cid:105)(cid:70)(cid:97)(cid:122) (cid:101)(cid:76)(cid:122) (cid:108)(cid:86)(cid:75)(cid:122) (cid:105)(cid:101)(cid:112)(cid:108)(cid:75)(cid:105)(cid:122) (cid:41)(cid:89)(cid:110)(cid:90)(cid:95)(cid:70)(cid:71)(cid:95)(cid:110)(cid:98)(cid:90)(cid:110)(cid:87)(cid:67)(cid:80)(cid:89)(cid:98)(cid:67)(cid:80)(cid:89)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:78)(cid:84)(cid:90)(cid:68)(cid:67)(cid:84)(cid:110)(cid:92)(cid:67)(cid:98)(cid:79)(cid:110)(cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110)(cid:67)(cid:84)(cid:84)(cid:90)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:95)(cid:67)(cid:98)(cid:80)(cid:90)(cid:97)(cid:8)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:71)(cid:89)(cid:98)(cid:95)(cid:104)(cid:110) (cid:80)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:67)(cid:68)(cid:84)(cid:71)(cid:110) (cid:16)(cid:17)(cid:1)(cid:45)(cid:4)(cid:52)(cid:24)(cid:2)(cid:52) (cid:89)(cid:71)(cid:71)(cid:70)(cid:97)(cid:110) (cid:98)(cid:90)(cid:110) (cid:70)(cid:80)(cid:97)(cid:98)(cid:80)(cid:89)(cid:78)(cid:100)(cid:80)(cid:97)(cid:79)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:90)(cid:100)(cid:95)(cid:69)(cid:71)(cid:110) (cid:98)(cid:80)(cid:84)(cid:71)(cid:110) (cid:84)(cid:90)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:44)(cid:52) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:67)(cid:69)(cid:83)(cid:71)(cid:98)(cid:11)(cid:110) (cid:53)(cid:79)(cid:100)(cid:97)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:89)(cid:100)(cid:87)(cid:68)(cid:71)(cid:95)(cid:110) (cid:90)(cid:72)(cid:110) (cid:71)(cid:89)(cid:98)(cid:95)(cid:80)(cid:71)(cid:97)(cid:110) (cid:80)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:67)(cid:68)(cid:84)(cid:71)(cid:110) (cid:80)(cid:97)(cid:110) (cid:80)(cid:89)(cid:69)(cid:95)(cid:71)(cid:67)(cid:97)(cid:71)(cid:70)(cid:11)(cid:110) (cid:60)(cid:71)(cid:110) (cid:69)(cid:67)(cid:89)(cid:110) (cid:95)(cid:71)(cid:70)(cid:100)(cid:69)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:67)(cid:68)(cid:84)(cid:71)(cid:110) (cid:97)(cid:80)(cid:105)(cid:71)(cid:110) (cid:68)(cid:104)(cid:110) (cid:78)(cid:95)(cid:90)(cid:100)(cid:92)(cid:80)(cid:89)(cid:78)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:71)(cid:89)(cid:98)(cid:95)(cid:80)(cid:71)(cid:97)(cid:110)(cid:90)(cid:72)(cid:110)(cid:70)(cid:80)(cid:73)(cid:71)(cid:95)(cid:71)(cid:89)(cid:98)(cid:110)(cid:97)(cid:90)(cid:100)(cid:95)(cid:69)(cid:71)(cid:97)(cid:110) (cid:102)(cid:80)(cid:98)(cid:79)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:67)(cid:87)(cid:71)(cid:110) (cid:70)(cid:71)(cid:97)(cid:98)(cid:80)(cid:89)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:98)(cid:90)(cid:78)(cid:71)(cid:98)(cid:79)(cid:71)(cid:95)(cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:110) (cid:89)(cid:71)(cid:102)(cid:110) (cid:72)(cid:90)(cid:95)(cid:87)(cid:67)(cid:98)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:67)(cid:68)(cid:84)(cid:71)(cid:110) (cid:68)(cid:71)(cid:69)(cid:90)(cid:87)(cid:71)(cid:97)(cid:110) (cid:16)(cid:17)(cid:1)(cid:45)(cid:4)(cid:24)(cid:2)(cid:52) (cid:2)(cid:4) (cid:51)(cid:1)(cid:22)(cid:4)(cid:40)(cid:4)(cid:41)(cid:1)(cid:40)(cid:2)(cid:37)(cid:40)(cid:52) (cid:48)(cid:116) (cid:16)(cid:51)(cid:59)(cid:57)(cid:1)(cid:58)(cid:2)(cid:4)(cid:23)(cid:64)(cid:12)(cid:64) (cid:41)(cid:1)(cid:40)(cid:2)(cid:52) (cid:28)(cid:110) (cid:40)(cid:107)(cid:11)(cid:110) (cid:41)(cid:89)(cid:110) (cid:98)(cid:79)(cid:80)(cid:97)(cid:110) (cid:69)(cid:67)(cid:97)(cid:71)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:90)(cid:95)(cid:98)(cid:110) (cid:92)(cid:95)(cid:90)(cid:68)(cid:67)(cid:68)(cid:80)(cid:84)(cid:80)(cid:98)(cid:104)(cid:110) (cid:101)(cid:67)(cid:84)(cid:100)(cid:71)(cid:110)(cid:41)(cid:1)(cid:40)(cid:4)(cid:52)(cid:22)(cid:2)(cid:52) (cid:80)(cid:89)(cid:110) (cid:16)(cid:17)(cid:1)(cid:45)(cid:4)(cid:52)(cid:24)(cid:2) (cid:2)(cid:52) (cid:80)(cid:97)(cid:110) (cid:69)(cid:67)(cid:84)(cid:69)(cid:100)(cid:84)(cid:67)(cid:98)(cid:71)(cid:70)(cid:110) (cid:67)(cid:97)(cid:27)(cid:110) (cid:26)(cid:57)(cid:5)(cid:37) (cid:2) (cid:25)(cid:46)(cid:4)(cid:64)(cid:22)(cid:2)(cid:52) (cid:26)(cid:57)(cid:5)(cid:37)(cid:1)(cid:8)(cid:11)(cid:4)(cid:64)(cid:22)(cid:2)(cid:52) (cid:30)(cid:137)(cid:28)(cid:64) (cid:30)(cid:66)(cid:48)(cid:6)(cid:123)(cid:16)(cid:76)(cid:16)(cid:104)(cid:11)(cid:44)(cid:61)(cid:128)(cid:15)(cid:79)(cid:6)(cid:54)(cid:106)(cid:16)(cid:58)(cid:16)(cid:76)(cid:11)(cid:137)(cid:41)(cid:1)(cid:44)(cid:4)(cid:52)(cid:22)(cid:2)(cid:52) (cid:8)(cid:9) (cid:43)(cid:1)(cid:44)(cid:4)(cid:52)(cid:22)(cid:4)(cid:52)(cid:33)(cid:2)(cid:52) (cid:30)(cid:137)(cid:28)(cid:64) (cid:30)(cid:66)(cid:48)(cid:6)(cid:33)(cid:16)(cid:76)(cid:16)(cid:48)(cid:11)(cid:44)(cid:61)(cid:128)(cid:15)(cid:79)(cid:6)(cid:25)(cid:28)(cid:16)(cid:58)(cid:16)(cid:76)(cid:11)(cid:137)(cid:41)(cid:1)(cid:44)(cid:4)(cid:52)(cid:22)(cid:2)(cid:52) (cid:8)(cid:9) (cid:43)(cid:1)(cid:44)(cid:4)(cid:52)(cid:22)(cid:4)(cid:52)(cid:47)(cid:10)(cid:137) (cid:30)(cid:84)(cid:98)(cid:79)(cid:90)(cid:100)(cid:78)(cid:79)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:71)(cid:103)(cid:67)(cid:69)(cid:98)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:67)(cid:84)(cid:84)(cid:90)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:95)(cid:67)(cid:98)(cid:80)(cid:90)(cid:97)(cid:110) (cid:69)(cid:67)(cid:89)(cid:89)(cid:90)(cid:98)(cid:110) (cid:68)(cid:71)(cid:110) (cid:87)(cid:67)(cid:80)(cid:89)(cid:98)(cid:67)(cid:80)(cid:89)(cid:71)(cid:70)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:70)(cid:80)(cid:97)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:90)(cid:89)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:84)(cid:80)(cid:89)(cid:83)(cid:110) (cid:69)(cid:67)(cid:89)(cid:110) (cid:97)(cid:98)(cid:80)(cid:84)(cid:84)(cid:110) (cid:68)(cid:71)(cid:110) (cid:87)(cid:67)(cid:80)(cid:89)(cid:98)(cid:67)(cid:80)(cid:89)(cid:71)(cid:70)(cid:110) (cid:102)(cid:79)(cid:80)(cid:69)(cid:79)(cid:110) (cid:80)(cid:97)(cid:110) (cid:87)(cid:90)(cid:95)(cid:71)(cid:110) (cid:80)(cid:87)(cid:92)(cid:90)(cid:95)(cid:98)(cid:67)(cid:89)(cid:98)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:71)(cid:101)(cid:71)(cid:89)(cid:110)(cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:70)(cid:80)(cid:97)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:92)(cid:100)(cid:95)(cid:92)(cid:90)(cid:97)(cid:71)(cid:11)(cid:110) (cid:37)(cid:90)(cid:87)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:80)(cid:87)(cid:100)(cid:84)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:95)(cid:71)(cid:97)(cid:100)(cid:84)(cid:98)(cid:97)(cid:110) (cid:102)(cid:79)(cid:80)(cid:69)(cid:79)(cid:110)(cid:102)(cid:80)(cid:84)(cid:84)(cid:110)(cid:68)(cid:71)(cid:110)(cid:92)(cid:95)(cid:71)(cid:97)(cid:71)(cid:89)(cid:98)(cid:71)(cid:70)(cid:110)(cid:80)(cid:89)(cid:110)(cid:52)(cid:71)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:59)(cid:41)(cid:8)(cid:110) (cid:98)(cid:79)(cid:80)(cid:97)(cid:110)(cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110)(cid:98)(cid:67)(cid:68)(cid:84)(cid:71)(cid:110)(cid:80)(cid:87)(cid:92)(cid:84)(cid:71)(cid:109) (cid:87)(cid:71)(cid:89)(cid:98)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:102)(cid:80)(cid:98)(cid:79)(cid:110)(cid:70)(cid:71)(cid:97)(cid:98)(cid:80)(cid:89)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:90)(cid:89)(cid:84)(cid:104)(cid:110) (cid:67)(cid:69)(cid:79)(cid:80)(cid:71)(cid:101)(cid:71)(cid:97)(cid:110) (cid:97)(cid:80)(cid:87)(cid:80)(cid:84)(cid:67)(cid:95)(cid:110)(cid:80)(cid:87)(cid:92)(cid:95)(cid:90)(cid:101)(cid:71)(cid:87)(cid:71)(cid:89)(cid:98)(cid:110) (cid:80)(cid:89)(cid:110) (cid:92)(cid:71)(cid:67)(cid:83)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:95)(cid:71)(cid:70)(cid:100)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:84)(cid:67)(cid:98)(cid:71)(cid:89)(cid:69)(cid:104)(cid:110) (cid:92)(cid:71)(cid:95)(cid:72)(cid:90)(cid:95)(cid:87)(cid:67)(cid:89)(cid:69)(cid:71)(cid:110) (cid:69)(cid:90)(cid:87)(cid:92)(cid:67)(cid:95)(cid:71)(cid:70)(cid:110) (cid:102)(cid:80)(cid:98)(cid:79)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:67)(cid:68)(cid:84)(cid:71)(cid:110) (cid:100)(cid:97)(cid:80)(cid:89)(cid:78)(cid:110) (cid:97)(cid:90)(cid:100)(cid:95)(cid:69)(cid:71)(cid:10)(cid:70)(cid:71)(cid:97)(cid:98)(cid:80)(cid:89)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:92)(cid:67)(cid:80)(cid:95)(cid:97)(cid:11)(cid:110) (cid:61)(cid:13)(cid:136) (cid:51)(cid:45)(cid:57)(cid:55)(cid:29)(cid:50)(cid:136) (cid:39)(cid:36)(cid:25)(cid:50)(cid:45)(cid:24)(cid:50)(cid:25)(cid:32)(cid:36)(cid:55)(cid:29)(cid:25)(cid:55)(cid:57)(cid:50)(cid:29)(cid:136) (cid:53)(cid:79)(cid:71)(cid:110) (cid:68)(cid:84)(cid:90)(cid:69)(cid:83)(cid:110) (cid:70)(cid:80)(cid:67)(cid:78)(cid:95)(cid:67)(cid:87)(cid:110) (cid:90)(cid:72)(cid:110) (cid:90)(cid:100)(cid:95)(cid:110) (cid:92)(cid:95)(cid:90)(cid:92)(cid:90)(cid:97)(cid:71)(cid:70)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:110) (cid:98)(cid:90)(cid:110) (cid:97)(cid:100)(cid:92)(cid:92)(cid:90)(cid:95)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:95)(cid:109) (cid:87)(cid:67)(cid:84)(cid:110) (cid:67)(cid:102)(cid:67)(cid:95)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:80)(cid:97)(cid:110) (cid:80)(cid:84)(cid:84)(cid:100)(cid:97)(cid:98)(cid:95)(cid:67)(cid:98)(cid:71)(cid:70)(cid:110) (cid:80)(cid:89)(cid:110)(cid:76)(cid:78)(cid:100)(cid:95)(cid:71)(cid:110) (cid:19)(cid:10)(cid:67)(cid:11)(cid:110) (cid:41)(cid:72)(cid:110) (cid:98)(cid:102)(cid:90)(cid:110) (cid:69)(cid:67)(cid:89)(cid:70)(cid:80)(cid:70)(cid:67)(cid:98)(cid:71)(cid:110) (cid:90)(cid:100)(cid:98)(cid:92)(cid:100)(cid:98)(cid:110) (cid:92)(cid:90)(cid:95)(cid:98)(cid:97)(cid:110) (cid:9)(cid:10)(cid:64) (cid:67)(cid:89)(cid:70)(cid:110) (cid:24)(cid:11)(cid:64) (cid:71)(cid:103)(cid:80)(cid:97)(cid:98)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:97)(cid:71)(cid:84)(cid:71)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:100)(cid:89)(cid:80)(cid:98)(cid:110) (cid:80)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:110) (cid:102)(cid:80)(cid:84)(cid:84)(cid:110) (cid:69)(cid:79)(cid:90)(cid:90)(cid:97)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110)(cid:90)(cid:100)(cid:98)(cid:92)(cid:100)(cid:98)(cid:110)(cid:92)(cid:90)(cid:95)(cid:98)(cid:110)(cid:24)(cid:37)(cid:42)(cid:54)(cid:64) (cid:68)(cid:104)(cid:110) (cid:69)(cid:90)(cid:87)(cid:92)(cid:67)(cid:95)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:95)(cid:90)(cid:68)(cid:67)(cid:68)(cid:80)(cid:84)(cid:80)(cid:98)(cid:104)(cid:110) (cid:101)(cid:67)(cid:84)(cid:100)(cid:71)(cid:110) (cid:41)(cid:1)(cid:14)(cid:35)(cid:2)(cid:52) (cid:102)(cid:80)(cid:98)(cid:79)(cid:110) (cid:67)(cid:110) (cid:95)(cid:67)(cid:89)(cid:70)(cid:90)(cid:87)(cid:110)(cid:89)(cid:100)(cid:87)(cid:68)(cid:71)(cid:95)(cid:110) (cid:6)(cid:9) (cid:80)(cid:89)(cid:110) (cid:61)(cid:15)(cid:8)(cid:110)(cid:40)(cid:63)(cid:11)(cid:110) (cid:41)(cid:98)(cid:110) (cid:102)(cid:80)(cid:84)(cid:84)(cid:110) (cid:97)(cid:71)(cid:84)(cid:71)(cid:69)(cid:98)(cid:110) (cid:92)(cid:90)(cid:95)(cid:98)(cid:110) (cid:9)(cid:10)(cid:64) (cid:29)(cid:26)(cid:52) (cid:6)(cid:9) (cid:9)(cid:52) (cid:41)(cid:1)(cid:14)(cid:35)(cid:2)(cid:4)(cid:52) (cid:90)(cid:98)(cid:79)(cid:71)(cid:95)(cid:102)(cid:80)(cid:97)(cid:71)(cid:110) (cid:92)(cid:90)(cid:95)(cid:98)(cid:110) (cid:24)(cid:11)(cid:64) (cid:102)(cid:80)(cid:84)(cid:84)(cid:110) (cid:68)(cid:71)(cid:110) (cid:69)(cid:79)(cid:90)(cid:97)(cid:71)(cid:89)(cid:11)(cid:110) (cid:30)(cid:110) (cid:92)(cid:97)(cid:71)(cid:100)(cid:70)(cid:90)(cid:110) (cid:95)(cid:67)(cid:89)(cid:70)(cid:90)(cid:87)(cid:110) (cid:89)(cid:100)(cid:87)(cid:68)(cid:71)(cid:95)(cid:110) (cid:78)(cid:71)(cid:89)(cid:71)(cid:95)(cid:67)(cid:98)(cid:90)(cid:95)(cid:110) (cid:100)(cid:97)(cid:80)(cid:89)(cid:78)(cid:110) (cid:84)(cid:80)(cid:89)(cid:71)(cid:67)(cid:95)(cid:110) (cid:72)(cid:71)(cid:71)(cid:70)(cid:68)(cid:67)(cid:69)(cid:83)(cid:110) (cid:97)(cid:79)(cid:80)(cid:72)(cid:98)(cid:110)(cid:95)(cid:71)(cid:78)(cid:80)(cid:97)(cid:98)(cid:71)(cid:95)(cid:110) (cid:4)(cid:44)(cid:36) (cid:52)(cid:51)(cid:6)(cid:110)(cid:80)(cid:97)(cid:110)(cid:71)(cid:87)(cid:92)(cid:84)(cid:90)(cid:104)(cid:71)(cid:70)(cid:110)(cid:98)(cid:90)(cid:110)(cid:78)(cid:71)(cid:89)(cid:71)(cid:95)(cid:67)(cid:98)(cid:71)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:95)(cid:67)(cid:89)(cid:70)(cid:90)(cid:87)(cid:110)(cid:89)(cid:100)(cid:87)(cid:109) (cid:68)(cid:71)(cid:95)(cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:110) (cid:79)(cid:71)(cid:67)(cid:70)(cid:71)(cid:95)(cid:110) (cid:77)(cid:80)(cid:98)(cid:97)(cid:110) (cid:72)(cid:95)(cid:90)(cid:87)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:80)(cid:89)(cid:92)(cid:100)(cid:98)(cid:110) (cid:92)(cid:90)(cid:95)(cid:98)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:76)(cid:95)(cid:97)(cid:98)(cid:110) (cid:70)(cid:71)(cid:69)(cid:90)(cid:70)(cid:71)(cid:70)(cid:110) (cid:68)(cid:104)(cid:110) (cid:67)(cid:110) (cid:92)(cid:67)(cid:95)(cid:97)(cid:71)(cid:95)(cid:110) (cid:98)(cid:90)(cid:110) (cid:71)(cid:103)(cid:98)(cid:95)(cid:67)(cid:69)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:70)(cid:71)(cid:97)(cid:98)(cid:80)(cid:89)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:97)(cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:89)(cid:110) (cid:68)(cid:104)(cid:110) (cid:69)(cid:79)(cid:71)(cid:69)(cid:83)(cid:80)(cid:89)(cid:78)(cid:110) (cid:16)(cid:17)(cid:1)(cid:45)(cid:4)(cid:52)(cid:24)(cid:2) (cid:4)(cid:52) (cid:98)(cid:102)(cid:90)(cid:110) (cid:69)(cid:67)(cid:89)(cid:70)(cid:80)(cid:70)(cid:67)(cid:98)(cid:71)(cid:110) (cid:90)(cid:100)(cid:98)(cid:92)(cid:100)(cid:98)(cid:110) (cid:92)(cid:90)(cid:95)(cid:98)(cid:97)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:95)(cid:71)(cid:98)(cid:100)(cid:95)(cid:89)(cid:71)(cid:70)(cid:110) (cid:102)(cid:80)(cid:98)(cid:79)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:69)(cid:90)(cid:95)(cid:95)(cid:71)(cid:97)(cid:92)(cid:90)(cid:89)(cid:70)(cid:80)(cid:89)(cid:78)(cid:110) (cid:92)(cid:95)(cid:90)(cid:68)(cid:67)(cid:68)(cid:80)(cid:84)(cid:80)(cid:98)(cid:104)(cid:110) (cid:101)(cid:67)(cid:84)(cid:100)(cid:71)(cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:110) (cid:90)(cid:100)(cid:98)(cid:92)(cid:100)(cid:98)(cid:110) (cid:92)(cid:90)(cid:95)(cid:98)(cid:110) (cid:97)(cid:71)(cid:84)(cid:71)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:100)(cid:89)(cid:80)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:89)(cid:110)(cid:87)(cid:67)(cid:83)(cid:71)(cid:110) (cid:67)(cid:110) (cid:70)(cid:71)(cid:69)(cid:80)(cid:97)(cid:80)(cid:90)(cid:89)(cid:110) (cid:90)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:90)(cid:100)(cid:98)(cid:92)(cid:100)(cid:98)(cid:110) (cid:92)(cid:90)(cid:95)(cid:98)(cid:11)(cid:110) (cid:41)(cid:89)(cid:110) (cid:67)(cid:70)(cid:70)(cid:80)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:98)(cid:90)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:95)(cid:90)(cid:68)(cid:67)(cid:68)(cid:80)(cid:84)(cid:80)(cid:98)(cid:104)(cid:110) (cid:97)(cid:71)(cid:84)(cid:71)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:8)(cid:110) (cid:68)(cid:67)(cid:69)(cid:83)(cid:109) (cid:92)(cid:95)(cid:71)(cid:97)(cid:97)(cid:100)(cid:95)(cid:71)(cid:110) (cid:80)(cid:89)(cid:72)(cid:90)(cid:95)(cid:87)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:72)(cid:95)(cid:90)(cid:87)(cid:110) (cid:70)(cid:90)(cid:102)(cid:89)(cid:97)(cid:98)(cid:95)(cid:71)(cid:67)(cid:87)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:97)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:67)(cid:84)(cid:97)(cid:90)(cid:110) (cid:100)(cid:97)(cid:71)(cid:70)(cid:11)(cid:110) (cid:41)(cid:72)(cid:110) (cid:90)(cid:89)(cid:71)(cid:110) (cid:69)(cid:67)(cid:89)(cid:70)(cid:80)(cid:70)(cid:67)(cid:98)(cid:71)(cid:110) (cid:90)(cid:100)(cid:98)(cid:92)(cid:100)(cid:98)(cid:110) (cid:92)(cid:90)(cid:95)(cid:98)(cid:110) (cid:80)(cid:97)(cid:110) (cid:89)(cid:90)(cid:98)(cid:110) (cid:67)(cid:101)(cid:67)(cid:80)(cid:84)(cid:67)(cid:68)(cid:84)(cid:71)(cid:110) (cid:70)(cid:100)(cid:71)(cid:110) (cid:98)(cid:90)(cid:110) (cid:84)(cid:80)(cid:87)(cid:80)(cid:98)(cid:71)(cid:70)(cid:110) (cid:68)(cid:100)(cid:73)(cid:71)(cid:95)(cid:110) (cid:97)(cid:92)(cid:67)(cid:69)(cid:71)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:68)(cid:67)(cid:69)(cid:83)(cid:92)(cid:95)(cid:71)(cid:97)(cid:97)(cid:100)(cid:95)(cid:71)(cid:110) (cid:97)(cid:80)(cid:78)(cid:89)(cid:67)(cid:84)(cid:110) (cid:102)(cid:80)(cid:84)(cid:84)(cid:110) (cid:70)(cid:80)(cid:97)(cid:67)(cid:68)(cid:84)(cid:71)(cid:110) (cid:98)(cid:79)(cid:80)(cid:97)(cid:110) (cid:90)(cid:100)(cid:98)(cid:92)(cid:100)(cid:98)(cid:110) (cid:92)(cid:90)(cid:95)(cid:98)(cid:110) (cid:72)(cid:95)(cid:90)(cid:87)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:71)(cid:84)(cid:71)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:11)(cid:110) (cid:30)(cid:72)(cid:98)(cid:71)(cid:95)(cid:110)(cid:69)(cid:90)(cid:89)(cid:97)(cid:80)(cid:70)(cid:71)(cid:95)(cid:80)(cid:89)(cid:78)(cid:110)(cid:67)(cid:84)(cid:84)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:97)(cid:110)(cid:67)(cid:89)(cid:70)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:92)(cid:67)(cid:80)(cid:95)(cid:97)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:90)(cid:95)(cid:98)(cid:110) (cid:92)(cid:95)(cid:90)(cid:68)(cid:67)(cid:68)(cid:80)(cid:84)(cid:80)(cid:98)(cid:104)(cid:110) (cid:101)(cid:67)(cid:84)(cid:100)(cid:71)(cid:97)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:90)(cid:68)(cid:98)(cid:67)(cid:80)(cid:89)(cid:71)(cid:70)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:97)(cid:98)(cid:90)(cid:95)(cid:71)(cid:70)(cid:110) (cid:80)(cid:89)(cid:98)(cid:90)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:67)(cid:68)(cid:84)(cid:71)(cid:97)(cid:110) (cid:90)(cid:88)(cid:80)(cid:89)(cid:71)(cid:11)(cid:110) (cid:61)(cid:35)(cid:13)(cid:136) (cid:54)(cid:36)(cid:40)(cid:57)(cid:38)(cid:24)(cid:55)(cid:36)(cid:45)(cid:42)(cid:136) (cid:51)(cid:29)(cid:53)(cid:57)(cid:38)(cid:55)(cid:53)(cid:136) (cid:18)(cid:19)(cid:16)(cid:136) (cid:32)(cid:34)(cid:20)(cid:31)(cid:137) (cid:43)(cid:88)(cid:82)(cid:15)(cid:122) (cid:28)(cid:16)(cid:122) (cid:70)(cid:73)(cid:70)(cid:102)(cid:108)(cid:88)(cid:113)(cid:88)(cid:108)(cid:116)(cid:122) (cid:72)(cid:101)(cid:97)(cid:102)(cid:70)(cid:105)(cid:88)(cid:107)(cid:101)(cid:100)(cid:107)(cid:122) (cid:101)(cid:76)(cid:122) (cid:73)(cid:88)(cid:77)(cid:75)(cid:105)(cid:75)(cid:100)(cid:108)(cid:122) (cid:71)(cid:75)(cid:100)(cid:72)(cid:86)(cid:97)(cid:70)(cid:105)(cid:91)(cid:107)(cid:122) (cid:43)(cid:88)(cid:85)(cid:15)(cid:122) (cid:29)(cid:9)(cid:116) (cid:57)(cid:75)(cid:70)(cid:91)(cid:122) (cid:75)(cid:100)(cid:75)(cid:105)(cid:85)(cid:117)(cid:122) (cid:107)(cid:88)(cid:97)(cid:112)(cid:93)(cid:70)(cid:108)(cid:88)(cid:101)(cid:100)(cid:122) (cid:76)(cid:101)(cid:105)(cid:122) (cid:105)(cid:75)(cid:70)(cid:93)(cid:122) (cid:71)(cid:75)(cid:100)(cid:72)(cid:86)(cid:97)(cid:70)(cid:105)(cid:91)(cid:107)(cid:122) (cid:22)(cid:12)(cid:136) (cid:52)(cid:92)(cid:102)(cid:126)(cid:99)(cid:67)(cid:123)(cid:92)(cid:110)(cid:106)(cid:136) (cid:78)(cid:106)(cid:128)(cid:92)(cid:116)(cid:110)(cid:106)(cid:102)(cid:78)(cid:106)(cid:123)(cid:136) (cid:119)(cid:78)(cid:123)(cid:126)(cid:114)(cid:136) (cid:60)(cid:71)(cid:110) (cid:80)(cid:87)(cid:92)(cid:84)(cid:71)(cid:87)(cid:71)(cid:89)(cid:98)(cid:71)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110)(cid:92)(cid:95)(cid:90)(cid:92)(cid:90)(cid:97)(cid:71)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:95)(cid:87)(cid:67)(cid:84)(cid:10)(cid:67)(cid:102)(cid:67)(cid:95)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:97)(cid:98)(cid:95)(cid:67)(cid:98)(cid:109) (cid:71)(cid:78)(cid:104)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:87)(cid:71)(cid:97)(cid:79)(cid:10)(cid:68)(cid:67)(cid:97)(cid:71)(cid:70)(cid:110) (cid:47)(cid:90)(cid:33)(cid:110) (cid:67)(cid:95)(cid:69)(cid:79)(cid:80)(cid:98)(cid:71)(cid:69)(cid:98)(cid:100)(cid:95)(cid:71)(cid:11)(cid:110) (cid:30)(cid:110) (cid:33)(cid:7)(cid:7)(cid:110) (cid:92)(cid:95)(cid:90)(cid:78)(cid:95)(cid:67)(cid:87)(cid:110) (cid:102)(cid:67)(cid:97)(cid:110) (cid:70)(cid:71)(cid:101)(cid:71)(cid:84)(cid:90)(cid:92)(cid:71)(cid:70)(cid:110) (cid:98)(cid:90)(cid:110) (cid:78)(cid:71)(cid:89)(cid:71)(cid:95)(cid:67)(cid:98)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:69)(cid:90)(cid:95)(cid:95)(cid:71)(cid:97)(cid:92)(cid:90)(cid:89)(cid:70)(cid:80)(cid:89)(cid:78)(cid:110) (cid:44)(cid:50)(cid:110) (cid:72)(cid:90)(cid:95)(cid:87)(cid:100)(cid:84)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:67)(cid:110) (cid:78)(cid:80)(cid:101)(cid:71)(cid:89)(cid:110) (cid:67)(cid:92)(cid:92)(cid:84)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:110) (cid:44)(cid:50)(cid:110) (cid:92)(cid:95)(cid:90)(cid:68)(cid:84)(cid:71)(cid:87)(cid:110) (cid:80)(cid:97)(cid:110) (cid:98)(cid:79)(cid:71)(cid:89)(cid:110) (cid:97)(cid:90)(cid:84)(cid:101)(cid:71)(cid:70)(cid:110) (cid:68)(cid:104)(cid:110) (cid:69)(cid:101)(cid:103)(cid:110) (cid:62)(cid:16)(cid:20)(cid:64) (cid:11)(cid:110) (cid:41)(cid:89)(cid:110) (cid:90)(cid:95)(cid:70)(cid:71)(cid:95)(cid:110) (cid:98)(cid:90)(cid:110) (cid:71)(cid:101)(cid:67)(cid:84)(cid:100)(cid:67)(cid:98)(cid:71)(cid:110) (cid:47)(cid:90)(cid:33)(cid:110) (cid:92)(cid:71)(cid:95)(cid:72)(cid:90)(cid:95)(cid:87)(cid:67)(cid:89)(cid:69)(cid:71)(cid:8)(cid:110) (cid:67)(cid:110) (cid:97)(cid:104)(cid:97)(cid:98)(cid:71)(cid:87)(cid:33)(cid:110) (cid:68)(cid:67)(cid:97)(cid:71)(cid:70)(cid:110) (cid:47)(cid:90)(cid:33)(cid:110) (cid:97)(cid:80)(cid:87)(cid:100)(cid:84)(cid:67)(cid:98)(cid:90)(cid:95)(cid:110) (cid:102)(cid:67)(cid:97)(cid:110) (cid:70)(cid:71)(cid:101)(cid:71)(cid:84)(cid:90)(cid:92)(cid:71)(cid:70)(cid:110) (cid:69)(cid:90)(cid:87)(cid:68)(cid:80)(cid:89)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:72)(cid:71)(cid:67)(cid:98)(cid:100)(cid:95)(cid:71)(cid:97)(cid:110) (cid:90)(cid:72)(cid:110)(cid:47)(cid:90)(cid:103)(cid:80)(cid:87)(cid:110) (cid:62)(cid:16)(cid:19)(cid:64)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:32)(cid:90)(cid:90)(cid:83)(cid:97)(cid:80)(cid:87)(cid:110) (cid:62)(cid:16)(cid:22)(cid:64)(cid:110) (cid:97)(cid:80)(cid:87)(cid:100)(cid:84)(cid:67)(cid:98)(cid:90)(cid:95)(cid:11)(cid:110) (cid:60)(cid:71)(cid:110) (cid:100)(cid:97)(cid:71)(cid:70)(cid:110)(cid:68)(cid:90)(cid:98)(cid:79)(cid:110) (cid:97)(cid:104)(cid:89)(cid:109) (cid:98)(cid:79)(cid:71)(cid:98)(cid:80)(cid:69)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110)(cid:67)(cid:89)(cid:70)(cid:110)(cid:95)(cid:71)(cid:67)(cid:84)(cid:110)(cid:68)(cid:71)(cid:89)(cid:69)(cid:79)(cid:87)(cid:67)(cid:95)(cid:83)(cid:97)(cid:110)(cid:98)(cid:90)(cid:110)(cid:71)(cid:101)(cid:67)(cid:84)(cid:100)(cid:67)(cid:98)(cid:71)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:92)(cid:71)(cid:95)(cid:72)(cid:90)(cid:95)(cid:87)(cid:67)(cid:89)(cid:69)(cid:71)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:69)(cid:90)(cid:87)(cid:92)(cid:67)(cid:95)(cid:71)(cid:110) (cid:102)(cid:80)(cid:98)(cid:79)(cid:110) (cid:90)(cid:98)(cid:79)(cid:71)(cid:95)(cid:110) (cid:70)(cid:71)(cid:67)(cid:70)(cid:84)(cid:90)(cid:69)(cid:83)(cid:110) (cid:72)(cid:95)(cid:71)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:97)(cid:110) (cid:4)(cid:102)(cid:71)(cid:97)(cid:98)(cid:76)(cid:95)(cid:97)(cid:98)(cid:8)(cid:110) (cid:89)(cid:90)(cid:95)(cid:98)(cid:79)(cid:84)(cid:67)(cid:97)(cid:98)(cid:8)(cid:110) (cid:89)(cid:71)(cid:78)(cid:67)(cid:98)(cid:80)(cid:101)(cid:71)(cid:76)(cid:95)(cid:97)(cid:98)(cid:8)(cid:110) (cid:90)(cid:70)(cid:70)(cid:71)(cid:101)(cid:71)(cid:89)(cid:110) (cid:71)(cid:98)(cid:69)(cid:110) (cid:62)(cid:16)(cid:64)(cid:6)(cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:110) (cid:95)(cid:71)(cid:67)(cid:84)(cid:110) (cid:68)(cid:71)(cid:89)(cid:69)(cid:79)(cid:87)(cid:67)(cid:95)(cid:83)(cid:97)(cid:110) (cid:80)(cid:89)(cid:69)(cid:84)(cid:100)(cid:70)(cid:71)(cid:110) (cid:50)(cid:41)(cid:50)(cid:110) (cid:4)(cid:50)(cid:80)(cid:69)(cid:98)(cid:100)(cid:95)(cid:71)(cid:10)(cid:41)(cid:89)(cid:10)(cid:92)(cid:80)(cid:69)(cid:98)(cid:100)(cid:95)(cid:71)(cid:6)(cid:62)(cid:19)(cid:64)(cid:3)(cid:110) (cid:45)(cid:60) (cid:34)(cid:110) (cid:4)(cid:45)(cid:100)(cid:84)(cid:98)(cid:80)(cid:10)(cid:60)(cid:80)(cid:89)(cid:70)(cid:90)(cid:102)(cid:110) (cid:34)(cid:80)(cid:97)(cid:92)(cid:84)(cid:67)(cid:104)(cid:6)(cid:62)(cid:19)(cid:64)(cid:8)(cid:110) (cid:45)(cid:50)(cid:35)(cid:38)(cid:20)(cid:62)(cid:19)(cid:64)(cid:8)(cid:110) (cid:59)(cid:48)(cid:50)(cid:34)(cid:110) (cid:4)(cid:59)(cid:80)(cid:70)(cid:71)(cid:90)(cid:110) (cid:48)(cid:68)(cid:109) (cid:82)(cid:71)(cid:69)(cid:98)(cid:110) (cid:50)(cid:84)(cid:67)(cid:89)(cid:71)(cid:110) (cid:34)(cid:71)(cid:69)(cid:90)(cid:70)(cid:71)(cid:95)(cid:110)(cid:6)(cid:62)(cid:19)(cid:64)(cid:8)(cid:110) (cid:45)(cid:45)(cid:52)(cid:110)(cid:66)(cid:16)(cid:110)(cid:4)(cid:45)(cid:110)(cid:100)(cid:84)(cid:98)(cid:80)(cid:87)(cid:71)(cid:70)(cid:80)(cid:67)(cid:110) (cid:97)(cid:104)(cid:97)(cid:98)(cid:71)(cid:87)(cid:110) (cid:87)(cid:67)(cid:92)(cid:92)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:90)(cid:110) (cid:16)(cid:23)(cid:110) (cid:69)(cid:90)(cid:95)(cid:71)(cid:97)(cid:6)(cid:110) (cid:62)(cid:22)(cid:64)(cid:8)(cid:110) (cid:45)(cid:45)(cid:52)(cid:66)(cid:17)(cid:4)(cid:45)(cid:100)(cid:84)(cid:98)(cid:80)(cid:87)(cid:71)(cid:70)(cid:80)(cid:67)(cid:110) (cid:97)(cid:104)(cid:97)(cid:98)(cid:71)(cid:87)(cid:110) (cid:87)(cid:67)(cid:92)(cid:92)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:90)(cid:110) (cid:17)(cid:22)(cid:110) (cid:69)(cid:90)(cid:95)(cid:71)(cid:97)(cid:6)(cid:110)(cid:62)(cid:16)(cid:16)(cid:64)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:34)(cid:59)(cid:48)(cid:50)(cid:34)(cid:110) (cid:4)(cid:34)(cid:100)(cid:67)(cid:84)(cid:110) (cid:59)(cid:80)(cid:70)(cid:71)(cid:90)(cid:110) (cid:48)(cid:68)(cid:82)(cid:71)(cid:69)(cid:98)(cid:110) (cid:50)(cid:84)(cid:67)(cid:89)(cid:71)(cid:110) (cid:34)(cid:71)(cid:69)(cid:90)(cid:70)(cid:71)(cid:95)(cid:6)(cid:110) (cid:62)(cid:16)(cid:23)(cid:64)(cid:11)(cid:110) (cid:36)(cid:90)(cid:95)(cid:110)(cid:97)(cid:104)(cid:89)(cid:98)(cid:79)(cid:71)(cid:98)(cid:80)(cid:69)(cid:110)(cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110)(cid:100)(cid:97)(cid:71)(cid:110) (cid:72)(cid:90)(cid:100)(cid:95)(cid:110)(cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:97)(cid:69)(cid:71)(cid:89)(cid:67)(cid:95)(cid:80)(cid:90)(cid:97)(cid:8)(cid:110) (cid:89)(cid:67)(cid:87)(cid:71)(cid:84)(cid:104)(cid:110) (cid:100)(cid:89)(cid:80)(cid:72)(cid:90)(cid:95)(cid:87)(cid:110) (cid:95)(cid:67)(cid:89)(cid:70)(cid:90)(cid:87)(cid:8)(cid:110) (cid:98)(cid:95)(cid:67)(cid:89)(cid:97)(cid:92)(cid:90)(cid:97)(cid:71)(cid:10)(cid:40)(cid:8)(cid:110) (cid:98)(cid:95)(cid:67)(cid:89)(cid:97)(cid:92)(cid:90)(cid:97)(cid:71)(cid:10)(cid:17)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:79)(cid:90)(cid:98)(cid:97)(cid:92)(cid:90)(cid:98)(cid:110) (cid:80)(cid:89)(cid:110)(cid:69)(cid:71)(cid:89)(cid:109) (cid:98)(cid:71)(cid:95)(cid:110) (cid:62)(cid:26)(cid:64)(cid:11)(cid:110) (cid:19)(cid:110)(cid:8)(cid:9) (cid:19)(cid:8) (cid:21)(cid:110)(cid:8)(cid:9)(cid:21)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:28)(cid:116) (cid:8)(cid:9) (cid:28)(cid:116) (cid:47)(cid:90)(cid:33)(cid:110)(cid:87)(cid:71)(cid:97)(cid:79)(cid:71)(cid:97)(cid:110)(cid:67)(cid:95)(cid:71)(cid:110)(cid:100)(cid:97)(cid:71)(cid:70)(cid:110)(cid:72)(cid:90)(cid:95)(cid:110)(cid:70)(cid:80)(cid:73)(cid:71)(cid:95)(cid:71)(cid:89)(cid:98)(cid:110) (cid:68)(cid:71)(cid:89)(cid:69)(cid:79)(cid:87)(cid:67)(cid:95)(cid:83)(cid:97)(cid:11)(cid:110) (cid:42)(cid:13)(cid:116) (cid:22)(cid:76)(cid:67)(cid:114)(cid:123)(cid:92)(cid:128)(cid:92)(cid:123)(cid:132)(cid:136) (cid:92)(cid:102)(cid:114)(cid:116)(cid:110)(cid:128)(cid:78)(cid:102)(cid:78)(cid:106)(cid:123)(cid:136) (cid:36)(cid:80)(cid:95)(cid:97)(cid:98)(cid:110) (cid:102)(cid:71)(cid:110) (cid:69)(cid:90)(cid:87)(cid:92)(cid:67)(cid:95)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:70)(cid:67)(cid:92)(cid:98)(cid:80)(cid:101)(cid:80)(cid:98)(cid:104)(cid:110) (cid:102)(cid:80)(cid:98)(cid:79)(cid:110) (cid:90)(cid:98)(cid:79)(cid:71)(cid:95)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:109) (cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110) (cid:100)(cid:89)(cid:70)(cid:71)(cid:95)(cid:110) (cid:97)(cid:71)(cid:101)(cid:71)(cid:95)(cid:67)(cid:84)(cid:110) (cid:95)(cid:71)(cid:67)(cid:84)(cid:110) (cid:68)(cid:71)(cid:89)(cid:69)(cid:79)(cid:87)(cid:67)(cid:95)(cid:83)(cid:97)(cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:110) (cid:95)(cid:71)(cid:97)(cid:100)(cid:84)(cid:98)(cid:97)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:97)(cid:79)(cid:90)(cid:102)(cid:71)(cid:70)(cid:110) (cid:80)(cid:89)(cid:110) (cid:76)(cid:78)(cid:100)(cid:95)(cid:71)(cid:110) (cid:20)(cid:11)(cid:110) (cid:41)(cid:98)(cid:110) (cid:69)(cid:67)(cid:89)(cid:110) (cid:68)(cid:71)(cid:110) (cid:97)(cid:71)(cid:71)(cid:89)(cid:110) (cid:98)(cid:79)(cid:67)(cid:98)(cid:110) (cid:17)(cid:15)(cid:2)(cid:10)(cid:19)(cid:15)(cid:2)(cid:110) (cid:87)(cid:90)(cid:95)(cid:71)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110)(cid:67)(cid:101)(cid:67)(cid:80)(cid:84)(cid:67)(cid:68)(cid:84)(cid:71)(cid:110) (cid:80)(cid:72)(cid:110) (cid:102)(cid:71)(cid:110) (cid:98)(cid:67)(cid:83)(cid:71)(cid:110) (cid:67)(cid:92)(cid:92)(cid:84)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:80)(cid:89)(cid:98)(cid:90)(cid:110) (cid:69)(cid:90)(cid:89)(cid:97)(cid:80)(cid:70)(cid:71)(cid:95)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:11)(cid:110) (cid:71)(cid:10)(cid:136) (cid:47)(cid:78)(cid:67)(cid:96)(cid:136) (cid:78)(cid:106)(cid:78)(cid:116)(cid:86)(cid:132)(cid:136) (cid:119)(cid:92)(cid:102)(cid:126)(cid:99)(cid:67)(cid:123)(cid:92)(cid:110)(cid:106)(cid:136) (cid:116)(cid:78)(cid:119)(cid:126)(cid:99)(cid:123)(cid:119)(cid:136) (cid:47)(cid:71)(cid:103)(cid:98)(cid:110)(cid:102)(cid:71)(cid:110)(cid:71)(cid:101)(cid:67)(cid:84)(cid:100)(cid:67)(cid:98)(cid:71)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:92)(cid:71)(cid:95)(cid:72)(cid:90)(cid:95)(cid:87)(cid:67)(cid:89)(cid:69)(cid:71)(cid:110)(cid:90)(cid:72)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:92)(cid:95)(cid:90)(cid:92)(cid:90)(cid:97)(cid:71)(cid:70)(cid:110)(cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110) (cid:80)(cid:89)(cid:110) (cid:92)(cid:71)(cid:67)(cid:83)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:95)(cid:71)(cid:70)(cid:100)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:11)(cid:110) (cid:41)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:71)(cid:103)(cid:92)(cid:71)(cid:95)(cid:80)(cid:87)(cid:71)(cid:89)(cid:98)(cid:8)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:92)(cid:67)(cid:95)(cid:67)(cid:87)(cid:109) (cid:71)(cid:98)(cid:71)(cid:95)(cid:97)(cid:110) (cid:2) (cid:18)(cid:34)(cid:59)(cid:44)(cid:44)(cid:38)(cid:54)(cid:6)(cid:54) (cid:61)(cid:4)(cid:64) (cid:18)(cid:54) (cid:36)(cid:64) (cid:71)(cid:98)(cid:69)(cid:11)(cid:6)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:67)(cid:70)(cid:90)(cid:92)(cid:98)(cid:71)(cid:70)(cid:110) (cid:72)(cid:95)(cid:90)(cid:87)(cid:110) (cid:47)(cid:90)(cid:103)(cid:80)(cid:87)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:32)(cid:90)(cid:90)(cid:83)(cid:97)(cid:80)(cid:87)(cid:110) (cid:97)(cid:80)(cid:87)(cid:100)(cid:84)(cid:67)(cid:98)(cid:90)(cid:95)(cid:11)(cid:110) (cid:30)(cid:72)(cid:98)(cid:71)(cid:95)(cid:110) (cid:16)(cid:15)(cid:15)(cid:15)(cid:110) (cid:102)(cid:67)(cid:95)(cid:87)(cid:10)(cid:100)(cid:92)(cid:110) (cid:69)(cid:104)(cid:69)(cid:84)(cid:71)(cid:97)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:69)(cid:67)(cid:95)(cid:95)(cid:104)(cid:110) (cid:90)(cid:100)(cid:98)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:97)(cid:80)(cid:87)(cid:100)(cid:84)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:100)(cid:97)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:110)(cid:76)(cid:103)(cid:71)(cid:70)(cid:110) (cid:98)(cid:80)(cid:87)(cid:71)(cid:110) (cid:102)(cid:80)(cid:89)(cid:70)(cid:90)(cid:102)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:67)(cid:84)(cid:84)(cid:110) (cid:69)(cid:67)(cid:97)(cid:71)(cid:97)(cid:11)(cid:110) (cid:53)(cid:71)(cid:89)(cid:110) (cid:70)(cid:80)(cid:73)(cid:71)(cid:95)(cid:71)(cid:89)(cid:98)(cid:110)(cid:97)(cid:80)(cid:87)(cid:100)(cid:84)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:97)(cid:110)(cid:102)(cid:71)(cid:95)(cid:71)(cid:110)(cid:70)(cid:90)(cid:89)(cid:71)(cid:110)(cid:72)(cid:90)(cid:95)(cid:110) (cid:71)(cid:67)(cid:69)(cid:79)(cid:110) (cid:92)(cid:67)(cid:69)(cid:83)(cid:71)(cid:98)(cid:110)(cid:80)(cid:89)(cid:82)(cid:71)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:95)(cid:67)(cid:98)(cid:71)(cid:110)(cid:98)(cid:90)(cid:110) (cid:90)(cid:68)(cid:98)(cid:67)(cid:80)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:67)(cid:101)(cid:71)(cid:95)(cid:67)(cid:78)(cid:71)(cid:110) (cid:92)(cid:71)(cid:67)(cid:83)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:69)(cid:90)(cid:89)(cid:97)(cid:100)(cid:87)(cid:92)(cid:98)(cid:80)(cid:90)(cid:89)(cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:110) (cid:97)(cid:80)(cid:87)(cid:100)(cid:84)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:95)(cid:71)(cid:97)(cid:100)(cid:84)(cid:98)(cid:97)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:71)(cid:67)(cid:84)(cid:110) (cid:68)(cid:71)(cid:89)(cid:69)(cid:79)(cid:87)(cid:67)(cid:95)(cid:83)(cid:97)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:104)(cid:89)(cid:98)(cid:79)(cid:71)(cid:98)(cid:80)(cid:69)(cid:110) (cid:68)(cid:71)(cid:89)(cid:69)(cid:79)(cid:87)(cid:67)(cid:95)(cid:83)(cid:97)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:97)(cid:79)(cid:90)(cid:102)(cid:89)(cid:110) (cid:80)(cid:89)(cid:110) (cid:76)(cid:78)(cid:100)(cid:95)(cid:71)(cid:97)(cid:110) (cid:22)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:23)(cid:8)(cid:110) (cid:95)(cid:71)(cid:97)(cid:92)(cid:71)(cid:69)(cid:98)(cid:80)(cid:101)(cid:71)(cid:84)(cid:104)(cid:11)(cid:110) (cid:41)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:76)(cid:78)(cid:100)(cid:95)(cid:71)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:95)(cid:90)(cid:92)(cid:90)(cid:97)(cid:71)(cid:70)(cid:110) (cid:97)(cid:90)(cid:100)(cid:95)(cid:69)(cid:71)(cid:10)(cid:70)(cid:71)(cid:97)(cid:98)(cid:80)(cid:89)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:67)(cid:109) (cid:68)(cid:84)(cid:71)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:70)(cid:71)(cid:97)(cid:98)(cid:80)(cid:89)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:10)(cid:90)(cid:89)(cid:84)(cid:104)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:67)(cid:68)(cid:84)(cid:71)(cid:110) (cid:97)(cid:69)(cid:79)(cid:71)(cid:87)(cid:71)(cid:97)(cid:110) (cid:67)(cid:95)(cid:71)(cid:110) (cid:89)(cid:67)(cid:87)(cid:71)(cid:70)(cid:110) (cid:67)(cid:97)(cid:110) (cid:90)(cid:92)(cid:98)(cid:110)(cid:66)(cid:110)(cid:97)(cid:90)(cid:100)(cid:95)(cid:69)(cid:71)(cid:110)(cid:66)(cid:110)(cid:70)(cid:71)(cid:97)(cid:98)(cid:110)(cid:67)(cid:89)(cid:70)(cid:110)(cid:90)(cid:92)(cid:98)(cid:110)(cid:66)(cid:110)(cid:70)(cid:71)(cid:97)(cid:98)(cid:8)(cid:110)(cid:95)(cid:71)(cid:97)(cid:92)(cid:71)(cid:69)(cid:98)(cid:80)(cid:101)(cid:71)(cid:84)(cid:104)(cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:110)(cid:92)(cid:71)(cid:67)(cid:83)(cid:110)(cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:90)(cid:72)(cid:110) (cid:67)(cid:84)(cid:84)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:97)(cid:69)(cid:79)(cid:71)(cid:87)(cid:71)(cid:97)(cid:110) (cid:80)(cid:97)(cid:110) (cid:89)(cid:90)(cid:95)(cid:87)(cid:67)(cid:84)(cid:80)(cid:105)(cid:71)(cid:70)(cid:110) (cid:102)(cid:80)(cid:98)(cid:79)(cid:110) (cid:98)(cid:79)(cid:67)(cid:98)(cid:110) (cid:90)(cid:72)(cid:110) (cid:90)(cid:92)(cid:98)(cid:110)(cid:66)(cid:110)(cid:70)(cid:71)(cid:97)(cid:98)(cid:11)(cid:110) (cid:37)(cid:90)(cid:87)(cid:110) (cid:76)(cid:78)(cid:100)(cid:95)(cid:71)(cid:97)(cid:110) (cid:22)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:23)(cid:110) (cid:102)(cid:71)(cid:110) (cid:69)(cid:67)(cid:89)(cid:110) (cid:97)(cid:71)(cid:71)(cid:110) (cid:98)(cid:79)(cid:67)(cid:98)(cid:110) (cid:69)(cid:90)(cid:87)(cid:92)(cid:67)(cid:95)(cid:80)(cid:89)(cid:78)(cid:110) (cid:102)(cid:80)(cid:98)(cid:79)(cid:110) (cid:90)(cid:98)(cid:79)(cid:71)(cid:95)(cid:110) (cid:67)(cid:70)(cid:67)(cid:92)(cid:98)(cid:80)(cid:101)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:97)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:69)(cid:67)(cid:89)(cid:110) (cid:67)(cid:69)(cid:79)(cid:80)(cid:71)(cid:101)(cid:71)(cid:110) (cid:87)(cid:90)(cid:95)(cid:71)(cid:110) (cid:98)(cid:79)(cid:67)(cid:89)(cid:110) (cid:17)(cid:15)(cid:2)(cid:110) (cid:43)(cid:88)(cid:82)(cid:15)(cid:122) (cid:32)(cid:10)(cid:116) (cid:36)(cid:100)(cid:122) (cid:75)(cid:115)(cid:70)(cid:97)(cid:102)(cid:93)(cid:75)(cid:122) (cid:101)(cid:76)(cid:122) (cid:53)(cid:101)(cid:40)(cid:122) (cid:75)(cid:100)(cid:75)(cid:105)(cid:82)(cid:116)(cid:122) (cid:102)(cid:105)(cid:101)(cid:79)(cid:93)(cid:75)(cid:122) (cid:112)(cid:100)(cid:73)(cid:75)(cid:105)(cid:122) (cid:86)(cid:101)(cid:108)(cid:107)(cid:102)(cid:101)(cid:108)(cid:14)(cid:28)(cid:72)(cid:122) (cid:108)(cid:105)(cid:70)(cid:78)(cid:72)(cid:122) (cid:92)(cid:71)(cid:67)(cid:83)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:95)(cid:71)(cid:70)(cid:100)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:80)(cid:89)(cid:110) (cid:87)(cid:90)(cid:97)(cid:98)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:68)(cid:71)(cid:89)(cid:69)(cid:79)(cid:87)(cid:67)(cid:95)(cid:83)(cid:97)(cid:11)(cid:110) (cid:41)(cid:98)(cid:110) (cid:67)(cid:84)(cid:97)(cid:90)(cid:110) (cid:97)(cid:79)(cid:90)(cid:102)(cid:97)(cid:110) (cid:98)(cid:79)(cid:67)(cid:98)(cid:110) (cid:100)(cid:97)(cid:80)(cid:89)(cid:78)(cid:110) (cid:90)(cid:100)(cid:95)(cid:110) (cid:92)(cid:95)(cid:90)(cid:92)(cid:90)(cid:97)(cid:71)(cid:70)(cid:110) (cid:97)(cid:69)(cid:79)(cid:71)(cid:87)(cid:71)(cid:97)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:67)(cid:101)(cid:71)(cid:95)(cid:67)(cid:78)(cid:71)(cid:110) (cid:70)(cid:71)(cid:84)(cid:67)(cid:104)(cid:110) (cid:80)(cid:97)(cid:110) (cid:97)(cid:87)(cid:67)(cid:84)(cid:84)(cid:71)(cid:95)(cid:11)(cid:110) (cid:41)(cid:89)(cid:110) (cid:76)(cid:78)(cid:100)(cid:95)(cid:71)(cid:110) (cid:24)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:80)(cid:84)(cid:84)(cid:100)(cid:97)(cid:98)(cid:95)(cid:67)(cid:98)(cid:71)(cid:110) (cid:67)(cid:110) (cid:97)(cid:69)(cid:71)(cid:89)(cid:67)(cid:95)(cid:80)(cid:90)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:80)(cid:84)(cid:71)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:70)(cid:80)(cid:97)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:109) (cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:100)(cid:89)(cid:70)(cid:71)(cid:95)(cid:110) (cid:79)(cid:90)(cid:98)(cid:97)(cid:92)(cid:90)(cid:98)(cid:10)(cid:20)(cid:69)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:11)(cid:110) (cid:36) (cid:80)(cid:78)(cid:100)(cid:95)(cid:71)(cid:24)(cid:10)(cid:67)(cid:6)(cid:110) (cid:80)(cid:97)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:70)(cid:80)(cid:97)(cid:98)(cid:95)(cid:80)(cid:109) (cid:68)(cid:100)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:100)(cid:97)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:95)(cid:90)(cid:92)(cid:90)(cid:97)(cid:71)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:95)(cid:87)(cid:67)(cid:84)(cid:10)(cid:67)(cid:102)(cid:67)(cid:95)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:8)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:36) (cid:80)(cid:78)(cid:100)(cid:95)(cid:71)(cid:24)(cid:10)(cid:68)(cid:6)(cid:110) (cid:97)(cid:79)(cid:90)(cid:102)(cid:97)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:70)(cid:80)(cid:97)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:100)(cid:97)(cid:80)(cid:89)(cid:78)(cid:110) (cid:90)(cid:70)(cid:70)(cid:10)(cid:71)(cid:101)(cid:71)(cid:89)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:11)(cid:110) (cid:48)(cid:100)(cid:95)(cid:110) (cid:92)(cid:95)(cid:90)(cid:92)(cid:90)(cid:97)(cid:71)(cid:70)(cid:110) (cid:97)(cid:69)(cid:79)(cid:71)(cid:87)(cid:71)(cid:110) (cid:84)(cid:71)(cid:67)(cid:70)(cid:97)(cid:110) (cid:98)(cid:90)(cid:110) (cid:67)(cid:110) (cid:87)(cid:90)(cid:95)(cid:71)(cid:110) (cid:100)(cid:89)(cid:80)(cid:72)(cid:95)(cid:90)(cid:87)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:92)(cid:95)(cid:90)(cid:76)(cid:84)(cid:71)(cid:11)(cid:110) (cid:36)(cid:90)(cid:95)(cid:110) (cid:28)(cid:116) (cid:8)(cid:9) (cid:28)(cid:116) (cid:87)(cid:71)(cid:97)(cid:79)(cid:110) (cid:97)(cid:80)(cid:105)(cid:71)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:25)(cid:15)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:92)(cid:67)(cid:80)(cid:95)(cid:97)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:71)(cid:103)(cid:71)(cid:109) (cid:69)(cid:100)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:98)(cid:80)(cid:87)(cid:71)(cid:110) (cid:90)(cid:72)(cid:110) (cid:90)(cid:100)(cid:95)(cid:110) (cid:90)(cid:73)(cid:10)(cid:84)(cid:80)(cid:89)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110) (cid:4)(cid:80)(cid:89)(cid:69)(cid:84)(cid:100)(cid:70)(cid:80)(cid:89)(cid:78)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:110) (cid:78)(cid:71)(cid:89)(cid:71)(cid:95)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:44)(cid:50)(cid:110) (cid:97)(cid:90)(cid:84)(cid:101)(cid:80)(cid:89)(cid:78)(cid:6)(cid:110) (cid:80)(cid:97)(cid:110) (cid:67)(cid:68)(cid:90)(cid:100)(cid:98)(cid:110) (cid:17)(cid:20)(cid:19)(cid:110) (cid:97)(cid:71)(cid:69)(cid:90)(cid:89)(cid:70)(cid:97)(cid:11)(cid:110) (cid:36)(cid:90)(cid:95)(cid:110) (cid:71)(cid:101)(cid:71)(cid:89)(cid:110) (cid:84)(cid:67)(cid:95)(cid:78)(cid:71)(cid:95)(cid:110) (cid:87)(cid:71)(cid:97)(cid:79)(cid:110) (cid:97)(cid:80)(cid:105)(cid:71)(cid:110) (cid:4)(cid:110) (cid:24)(cid:110) (cid:8)(cid:9) (cid:24)(cid:110) (cid:90)(cid:95)(cid:110) (cid:87)(cid:90)(cid:95)(cid:71)(cid:110) (cid:6)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:87)(cid:90)(cid:95)(cid:71)(cid:110) (cid:69)(cid:90)(cid:87)(cid:87)(cid:100)(cid:89)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:92)(cid:67)(cid:80)(cid:95)(cid:97)(cid:110) (cid:4)(cid:16)(cid:15)(cid:15)(cid:110) (cid:90)(cid:95)(cid:110) (cid:87)(cid:90)(cid:95)(cid:71)(cid:6)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:89)(cid:100)(cid:87)(cid:68)(cid:71)(cid:95)(cid:110) (cid:90)(cid:72)(cid:110) (cid:87)(cid:80)(cid:89)(cid:80)(cid:87)(cid:67)(cid:84)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110) (cid:80)(cid:89)(cid:69)(cid:95)(cid:71)(cid:67)(cid:97)(cid:71)(cid:110) (cid:70)(cid:95)(cid:67)(cid:87)(cid:67)(cid:98)(cid:80)(cid:69)(cid:67)(cid:84)(cid:84)(cid:104)(cid:11)(cid:110) (cid:41)(cid:98)(cid:110) (cid:98)(cid:67)(cid:83)(cid:71)(cid:97)(cid:110) (cid:84)(cid:90)(cid:89)(cid:78)(cid:71)(cid:95)(cid:110) (cid:98)(cid:80)(cid:87)(cid:71)(cid:110) (cid:4)(cid:17)(cid:10)(cid:19)(cid:110) (cid:79)(cid:90)(cid:100)(cid:95)(cid:97)(cid:110) (cid:90)(cid:89)(cid:110) (cid:67)(cid:101)(cid:71)(cid:95)(cid:67)(cid:78)(cid:71)(cid:97)(cid:6)(cid:110) (cid:98)(cid:90)(cid:110) (cid:90)(cid:68)(cid:98)(cid:67)(cid:80)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:67)(cid:84)(cid:84)(cid:90)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:95)(cid:67)(cid:98)(cid:80)(cid:90)(cid:11)(cid:110) (cid:52)(cid:80)(cid:89)(cid:69)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:67)(cid:84)(cid:84)(cid:90)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:80)(cid:97)(cid:110) (cid:70)(cid:71)(cid:98)(cid:71)(cid:95)(cid:87)(cid:80)(cid:89)(cid:71)(cid:70)(cid:110) (cid:90)(cid:88)(cid:80)(cid:89)(cid:71)(cid:8)(cid:110) (cid:80)(cid:89)(cid:110) (cid:87)(cid:90)(cid:97)(cid:98)(cid:110) (cid:69)(cid:67)(cid:97)(cid:71)(cid:97)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:80)(cid:87)(cid:71)(cid:110) (cid:69)(cid:90)(cid:97)(cid:98)(cid:110) (cid:80)(cid:97)(cid:110) (cid:97)(cid:98)(cid:80)(cid:84)(cid:84)(cid:110) (cid:67)(cid:72)(cid:109) (cid:72)(cid:90)(cid:95)(cid:70)(cid:67)(cid:68)(cid:84)(cid:71)(cid:11)(cid:110) (cid:41)(cid:89)(cid:110) (cid:69)(cid:67)(cid:97)(cid:71)(cid:110) (cid:102)(cid:71)(cid:110) (cid:102)(cid:67)(cid:89)(cid:98)(cid:110) (cid:98)(cid:90)(cid:110) (cid:95)(cid:71)(cid:70)(cid:100)(cid:69)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:71)(cid:103)(cid:71)(cid:69)(cid:100)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:98)(cid:80)(cid:87)(cid:71)(cid:8)(cid:110) (cid:102)(cid:71)(cid:110) (cid:69)(cid:67)(cid:89)(cid:110) (cid:95)(cid:71)(cid:97)(cid:98)(cid:95)(cid:80)(cid:69)(cid:98)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:97)(cid:80)(cid:105)(cid:71)(cid:110) (cid:90)(cid:72)(cid:110) (cid:67)(cid:70)(cid:87)(cid:80)(cid:97)(cid:97)(cid:80)(cid:68)(cid:84)(cid:71)(cid:110) (cid:87)(cid:80)(cid:89)(cid:80)(cid:87)(cid:67)(cid:84)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:110) (cid:98)(cid:90)(cid:110) (cid:67)(cid:110) (cid:97)(cid:87)(cid:67)(cid:84)(cid:84)(cid:71)(cid:95)(cid:110) (cid:97)(cid:100)(cid:68)(cid:97)(cid:71)(cid:98)(cid:11)(cid:110) (cid:46)(cid:11)(cid:116) (cid:52)(cid:92)(cid:102)(cid:126)(cid:99)(cid:67)(cid:123)(cid:92)(cid:110)(cid:106)(cid:136) (cid:116)(cid:78)(cid:119)(cid:126)(cid:99)(cid:123)(cid:119)(cid:136) (cid:130)(cid:92)(cid:123)(cid:89)(cid:136) (cid:76)(cid:92)(cid:82)(cid:78)(cid:116)(cid:78)(cid:106)(cid:123)(cid:136) (cid:114)(cid:116)(cid:110)(cid:73)(cid:78)(cid:119)(cid:119)(cid:110)(cid:116)(cid:23)(cid:116)(cid:110)(cid:126)(cid:123)(cid:78)(cid:116)(cid:136) (cid:78)(cid:106)(cid:133) (cid:78)(cid:116)(cid:86)(cid:132)(cid:136) (cid:116)(cid:67)(cid:123)(cid:92)(cid:110)(cid:136) (cid:30)(cid:97)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:98)(cid:80)(cid:84)(cid:71)(cid:3)(cid:97)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:80)(cid:97)(cid:110) (cid:69)(cid:90)(cid:89)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:98)(cid:71)(cid:70)(cid:110) (cid:68)(cid:104)(cid:110) (cid:68)(cid:90)(cid:98)(cid:79)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:95)(cid:90)(cid:69)(cid:71)(cid:97)(cid:97)(cid:90)(cid:95)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:11)(cid:110) (cid:36)(cid:90)(cid:95)(cid:110) (cid:70)(cid:80)(cid:73)(cid:71)(cid:95)(cid:71)(cid:89)(cid:98)(cid:110) (cid:67)(cid:92)(cid:92)(cid:84)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:97)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:90)(cid:102)(cid:71)(cid:95)(cid:110) (cid:69)(cid:90)(cid:89)(cid:98)(cid:95)(cid:80)(cid:109) (cid:68)(cid:100)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:95)(cid:90)(cid:69)(cid:71)(cid:97)(cid:97)(cid:90)(cid:95)(cid:110) (cid:101)(cid:67)(cid:95)(cid:80)(cid:71)(cid:97)(cid:110) (cid:78)(cid:95)(cid:71)(cid:67)(cid:98)(cid:84)(cid:104)(cid:11)(cid:110) (cid:48)(cid:100)(cid:95)(cid:110) (cid:98)(cid:79)(cid:71)(cid:95)(cid:87)(cid:67)(cid:84)(cid:10)(cid:67)(cid:102)(cid:67)(cid:95)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110) (cid:69)(cid:67)(cid:89)(cid:110) (cid:90)(cid:89)(cid:84)(cid:104)(cid:110) (cid:95)(cid:71)(cid:10)(cid:70)(cid:80)(cid:97)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:98)(cid:71)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:110) (cid:92)(cid:90)(cid:102)(cid:71)(cid:95)(cid:11)(cid:110) (cid:60)(cid:71)(cid:110) (cid:102)(cid:67)(cid:89)(cid:98)(cid:110) (cid:98)(cid:90)(cid:110) (cid:71)(cid:101)(cid:67)(cid:84)(cid:100)(cid:67)(cid:98)(cid:71)(cid:110) (cid:71)(cid:73)(cid:71)(cid:69)(cid:98)(cid:80)(cid:101)(cid:71)(cid:89)(cid:71)(cid:97)(cid:97)(cid:110) (cid:90)(cid:72)(cid:110) (cid:90)(cid:100)(cid:95)(cid:110) (cid:67)(cid:84)(cid:109) (cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110) (cid:90)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:71)(cid:67)(cid:83)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:95)(cid:71)(cid:70)(cid:100)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:102)(cid:79)(cid:71)(cid:89)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:67)(cid:98)(cid:80)(cid:90)(cid:110) (cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:69)(cid:90)(cid:89)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:72)(cid:95)(cid:90)(cid:87)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:95)(cid:90)(cid:69)(cid:71)(cid:97)(cid:97)(cid:90)(cid:95)(cid:110) (cid:101)(cid:67)(cid:95)(cid:80)(cid:71)(cid:97)(cid:11)(cid:110) (cid:44)(cid:71)(cid:98)(cid:110) (cid:55)(cid:64) (cid:2)(cid:4) (cid:15)(cid:60)(cid:38)(cid:54) (cid:33)(cid:41)(cid:38)(cid:64) (cid:38)(cid:49)(cid:38)(cid:54) (cid:41)(cid:62)(cid:64) (cid:11)(cid:110) (cid:53)(cid:79)(cid:71)(cid:110) (cid:71)(cid:103)(cid:92)(cid:71)(cid:95)(cid:80)(cid:87)(cid:71)(cid:89)(cid:98)(cid:67)(cid:84)(cid:110) (cid:95)(cid:71)(cid:97)(cid:100)(cid:84)(cid:98)(cid:97)(cid:110) (cid:38)(cid:64) (cid:15)(cid:60)(cid:38)(cid:54) (cid:33)(cid:41)(cid:38)(cid:64)(cid:54) (cid:51)(cid:59)(cid:57)(cid:38)(cid:54)(cid:64)(cid:38)(cid:49)(cid:38)(cid:54) (cid:41)(cid:62)(cid:64) (cid:97)(cid:79)(cid:90)(cid:102)(cid:89)(cid:110) (cid:80)(cid:89)(cid:110) (cid:97)(cid:71)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:59)(cid:41)(cid:11) (cid:33)(cid:110) (cid:67)(cid:97)(cid:97)(cid:100)(cid:87)(cid:71)(cid:110) (cid:55)(cid:38)(cid:64) (cid:2)(cid:4) (cid:16)(cid:11)(cid:110) (cid:60)(cid:71)(cid:110) (cid:97)(cid:80)(cid:87)(cid:100)(cid:84)(cid:67)(cid:98)(cid:71)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:71)(cid:67)(cid:83)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:95)(cid:71)(cid:70)(cid:100)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:100)(cid:97)(cid:80)(cid:89)(cid:78)(cid:110) (cid:90)(cid:100)(cid:95)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:97)(cid:69)(cid:79)(cid:71)(cid:87)(cid:71)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110)(cid:70)(cid:80)(cid:73)(cid:71)(cid:95)(cid:71)(cid:89)(cid:98)(cid:110)(cid:55)(cid:38)(cid:64) (cid:101)(cid:67)(cid:84)(cid:109) (cid:100)(cid:71)(cid:97)(cid:11)(cid:110) (cid:53)(cid:67)(cid:68)(cid:84)(cid:71)(cid:97)(cid:110) (cid:41)(cid:41) (cid:110) (cid:97)(cid:100)(cid:87)(cid:87)(cid:67)(cid:95)(cid:80)(cid:105)(cid:71)(cid:97)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:71)(cid:97)(cid:100)(cid:84)(cid:98)(cid:97)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:97)(cid:104)(cid:89)(cid:98)(cid:79)(cid:71)(cid:98)(cid:80)(cid:69)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:18)(cid:19)(cid:17)(cid:136) (cid:32)(cid:34)(cid:20)(cid:31)(cid:137) (cid:43)(cid:88)(cid:82)(cid:15)(cid:122) (cid:30)(cid:16)(cid:122) (cid:51)(cid:70)(cid:108)(cid:75)(cid:100)(cid:72)(cid:116)(cid:122) (cid:70)(cid:100)(cid:73)(cid:122) (cid:102)(cid:75)(cid:70)(cid:91)(cid:122) (cid:75)(cid:100)(cid:75)(cid:105)(cid:82)(cid:116)(cid:122) (cid:107)(cid:88)(cid:97)(cid:112)(cid:93)(cid:70)(cid:108)(cid:88)(cid:101)(cid:100)(cid:122) (cid:112)(cid:100)(cid:73)(cid:75)(cid:105)(cid:122) (cid:113)(cid:70)(cid:105)(cid:88)(cid:101)(cid:112)(cid:107)(cid:122) (cid:108)(cid:105)(cid:70)(cid:78)(cid:72)(cid:122) (cid:61)(cid:36)(cid:38)(cid:51)(cid:42)(cid:122) (cid:46)(cid:46)(cid:122) (cid:66)(cid:50)(cid:41)(cid:56)(cid:116) (cid:50)(cid:62)(cid:50)(cid:67)(cid:53)(cid:78)(cid:116) (cid:67)(cid:50)(cid:47)(cid:74)(cid:45)(cid:71)(cid:54)(cid:64)(cid:62)(cid:116) (cid:74)(cid:62)(cid:47)(cid:50)(cid:67)(cid:116) (cid:77)(cid:41)(cid:67)(cid:54)(cid:64)(cid:74)(cid:68)(cid:116) (cid:4)(cid:7)(cid:9) (cid:75)(cid:100)(cid:75)(cid:105)(cid:82)(cid:116)(cid:122) (cid:60)(cid:116)(cid:100)(cid:108)(cid:86)(cid:75)(cid:108)(cid:88)(cid:72)(cid:122) (cid:42)(cid:100)(cid:75)(cid:105)(cid:82)(cid:116)(cid:122) (cid:59)(cid:70)(cid:108)(cid:88)(cid:101)(cid:122) (cid:1) (cid:4)(cid:7) (cid:5)(cid:136) (cid:21)(cid:15)(cid:30)(cid:31)(cid:122) (cid:23)(cid:15)(cid:21)(cid:21)(cid:122) (cid:23)(cid:15)(cid:30)(cid:31)(cid:122) (cid:24)(cid:15)(cid:21)(cid:21)(cid:122) (cid:24)(cid:15)(cid:30)(cid:31)(cid:122) (cid:26)(cid:15)(cid:21)(cid:21)(cid:122) (cid:26)(cid:15)(cid:30)(cid:31)(cid:122) (cid:28)(cid:15)(cid:21)(cid:21)(cid:122) (cid:51)(cid:67)(cid:89)(cid:70)(cid:90)(cid:87)(cid:110) (cid:69)(cid:71)(cid:89)(cid:98)(cid:71)(cid:95)(cid:110) (cid:16)(cid:110) (cid:113)(cid:107)(cid:15)(cid:65)(cid:66)(cid:122) (cid:113)(cid:107)(cid:15)(cid:55)(cid:42)(cid:122) (cid:113)(cid:107)(cid:15)(cid:65)(cid:66)(cid:122) (cid:113)(cid:107)(cid:15)(cid:55)(cid:42)(cid:122) (cid:113)(cid:107)(cid:15)(cid:65)(cid:66)(cid:122) (cid:113)(cid:107)(cid:15)(cid:55)(cid:42)(cid:122) (cid:113)(cid:107)(cid:15)(cid:122) (cid:65)(cid:66)(cid:122) (cid:113)(cid:107)(cid:15)(cid:122) (cid:55)(cid:42)(cid:122) (cid:113)(cid:107)(cid:15)(cid:122) (cid:65)(cid:66)(cid:122) (cid:113)(cid:107)(cid:15)(cid:122) (cid:55)(cid:42)(cid:122) (cid:113)(cid:107)(cid:15)(cid:122) (cid:65)(cid:66)(cid:122) (cid:113)(cid:107)(cid:15)(cid:122) (cid:55)(cid:42)(cid:122) (cid:33)(cid:15)(cid:28)(cid:3)(cid:122) (cid:31)(cid:15)(cid:31)(cid:3)(cid:122) (cid:23)(cid:21)(cid:15)(cid:24)(cid:3)(cid:122) (cid:23)(cid:26)(cid:15)(cid:21)(cid:3)(cid:122) (cid:32)(cid:15)(cid:33)(cid:3)(cid:122) (cid:23)(cid:23)(cid:15)(cid:23)(cid:3)(cid:122) (cid:23)(cid:21)(cid:15)(cid:28)(cid:3)(cid:122) (cid:31)(cid:15)(cid:21)(cid:3)(cid:122) (cid:31)(cid:15)(cid:30)(cid:3)(cid:122) (cid:23)(cid:28)(cid:15)(cid:32)(cid:3)(cid:122) (cid:33)(cid:15)(cid:26)(cid:3)(cid:122) (cid:23)(cid:21)(cid:15)(cid:29)(cid:3)(cid:122) (cid:31)(cid:15)(cid:23)(cid:122) (cid:32)(cid:15)(cid:23)(cid:122) (cid:95)(cid:71)(cid:67)(cid:84)(cid:110) (cid:68)(cid:71)(cid:89)(cid:69)(cid:79)(cid:87)(cid:67)(cid:95)(cid:83)(cid:97)(cid:11)(cid:110) (cid:37)(cid:90)(cid:87)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:71)(cid:97)(cid:100)(cid:84)(cid:98)(cid:97)(cid:110) (cid:102)(cid:71)(cid:110) (cid:69)(cid:67)(cid:89)(cid:110) (cid:97)(cid:71)(cid:71)(cid:110) (cid:98)(cid:79)(cid:67)(cid:98)(cid:110) (cid:102)(cid:79)(cid:71)(cid:89)(cid:110) (cid:5)(cid:7)(cid:9) (cid:80)(cid:89)(cid:69)(cid:95)(cid:71)(cid:67)(cid:97)(cid:71)(cid:97)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110)(cid:92)(cid:71)(cid:67)(cid:83)(cid:110)(cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110)(cid:95)(cid:71)(cid:70)(cid:100)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:80)(cid:97)(cid:110)(cid:97)(cid:87)(cid:67)(cid:84)(cid:84)(cid:71)(cid:95)(cid:110)(cid:67)(cid:97)(cid:110)(cid:98)(cid:79)(cid:71)(cid:110)(cid:69)(cid:90)(cid:89)(cid:98)(cid:95)(cid:80)(cid:68)(cid:100)(cid:109) (cid:98)(cid:80)(cid:90)(cid:89)(cid:110)(cid:90)(cid:72)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:68)(cid:71)(cid:69)(cid:90)(cid:87)(cid:71)(cid:97)(cid:110) (cid:84)(cid:71)(cid:97)(cid:97)(cid:11)(cid:110) (cid:48)(cid:101)(cid:71)(cid:95)(cid:67)(cid:84)(cid:84)(cid:110) (cid:102)(cid:71)(cid:110) (cid:69)(cid:67)(cid:89)(cid:110) (cid:67)(cid:69)(cid:79)(cid:80)(cid:71)(cid:101)(cid:71)(cid:110) (cid:67)(cid:89)(cid:110) (cid:67)(cid:101)(cid:71)(cid:95)(cid:67)(cid:78)(cid:71)(cid:110) (cid:24) (cid:11)(cid:20)(cid:2)(cid:10)(cid:16)(cid:23) (cid:11)(cid:23)(cid:2)(cid:110) (cid:92)(cid:71)(cid:67)(cid:83)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:95)(cid:71)(cid:70)(cid:100)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:90)(cid:101)(cid:71)(cid:95)(cid:110) (cid:90)(cid:98)(cid:79)(cid:71)(cid:95)(cid:110) (cid:98)(cid:102)(cid:90)(cid:110) (cid:71)(cid:103)(cid:80)(cid:97)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:80)(cid:89)(cid:78)(cid:110) (cid:97)(cid:69)(cid:79)(cid:71)(cid:87)(cid:71)(cid:97)(cid:110) (cid:102)(cid:79)(cid:71)(cid:89)(cid:110) (cid:5)(cid:7)(cid:9) (cid:95)(cid:67)(cid:89)(cid:78)(cid:71)(cid:97)(cid:110) (cid:72)(cid:95)(cid:90)(cid:87)(cid:110) (cid:15)(cid:11) (cid:23)(cid:24)(cid:110) (cid:98)(cid:90)(cid:110) (cid:20)(cid:110) (cid:67)(cid:69)(cid:95)(cid:90)(cid:97)(cid:97)(cid:110) (cid:67)(cid:84)(cid:84)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:68)(cid:71)(cid:89)(cid:69)(cid:79)(cid:87)(cid:67)(cid:95)(cid:83)(cid:97)(cid:11)(cid:110) (cid:58)(cid:33)(cid:33)(cid:11)(cid:136) (cid:26)(cid:45)(cid:42)(cid:25)(cid:38)(cid:57)(cid:53)(cid:36)(cid:45)(cid:42)(cid:136) (cid:60)(cid:71)(cid:110)(cid:92)(cid:95)(cid:90)(cid:92)(cid:90)(cid:97)(cid:71)(cid:110)(cid:67)(cid:89)(cid:110)(cid:67)(cid:92)(cid:92)(cid:84)(cid:80)(cid:69)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:10)(cid:97)(cid:92)(cid:71)(cid:69)(cid:80)(cid:76)(cid:69)(cid:110) (cid:67)(cid:89)(cid:70)(cid:110) (cid:98)(cid:79)(cid:71)(cid:95)(cid:87)(cid:67)(cid:84)(cid:110)(cid:67)(cid:102)(cid:67)(cid:95)(cid:71)(cid:110)(cid:95)(cid:90)(cid:100)(cid:98)(cid:109) (cid:80)(cid:89)(cid:78)(cid:110) (cid:67)(cid:84)(cid:78)(cid:90)(cid:95)(cid:80)(cid:98)(cid:79)(cid:87)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110) (cid:89)(cid:71)(cid:98)(cid:102)(cid:90)(cid:95)(cid:83)(cid:97)(cid:110) (cid:90)(cid:89)(cid:110) (cid:69)(cid:79)(cid:80)(cid:92)(cid:11)(cid:110) (cid:30)(cid:110) (cid:44)(cid:50)(cid:110) (cid:92)(cid:95)(cid:90)(cid:68)(cid:84)(cid:71)(cid:87)(cid:110) (cid:80)(cid:97)(cid:110) (cid:72)(cid:90)(cid:95)(cid:87)(cid:100)(cid:109) (cid:84)(cid:67)(cid:98)(cid:71)(cid:70)(cid:110) (cid:98)(cid:90)(cid:110) (cid:67)(cid:84)(cid:84)(cid:90)(cid:69)(cid:67)(cid:98)(cid:71)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110) (cid:92)(cid:95)(cid:90)(cid:92)(cid:71)(cid:95)(cid:84)(cid:104)(cid:110) (cid:67)(cid:87)(cid:90)(cid:89)(cid:78)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:67)(cid:98)(cid:79)(cid:97)(cid:11)(cid:110) (cid:30)(cid:110) (cid:98)(cid:67)(cid:68)(cid:84)(cid:71)(cid:109) (cid:68)(cid:67)(cid:97)(cid:71)(cid:70)(cid:110) (cid:95)(cid:90)(cid:100)(cid:98)(cid:71)(cid:95)(cid:110) (cid:80)(cid:97)(cid:110) (cid:67)(cid:84)(cid:97)(cid:90)(cid:110) (cid:92)(cid:95)(cid:90)(cid:92)(cid:90)(cid:97)(cid:71)(cid:70)(cid:110) (cid:98)(cid:90)(cid:110) (cid:97)(cid:71)(cid:84)(cid:71)(cid:69)(cid:98)(cid:110) (cid:90)(cid:100)(cid:98)(cid:92)(cid:100)(cid:98)(cid:110) (cid:92)(cid:90)(cid:95)(cid:98)(cid:97)(cid:110) (cid:67)(cid:69)(cid:69)(cid:90)(cid:95)(cid:70)(cid:80)(cid:89)(cid:78)(cid:110) (cid:98)(cid:90)(cid:110) (cid:95)(cid:67)(cid:98)(cid:80)(cid:90)(cid:97)(cid:11)(cid:110) (cid:37)(cid:90)(cid:87)(cid:110) (cid:97)(cid:80)(cid:87)(cid:100)(cid:84)(cid:67)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:95)(cid:71)(cid:97)(cid:100)(cid:84)(cid:98)(cid:97)(cid:8)(cid:110) (cid:98)(cid:79)(cid:71)(cid:110) (cid:92)(cid:71)(cid:67)(cid:83)(cid:110) (cid:71)(cid:89)(cid:71)(cid:95)(cid:78)(cid:104)(cid:110) (cid:95)(cid:71)(cid:70)(cid:100)(cid:69)(cid:98)(cid:80)(cid:90)(cid:89)(cid:110) (cid:69)(cid:67)(cid:89)(cid:110)(cid:68)(cid:71)(cid:110)(cid:67)(cid:97)(cid:110)(cid:79)(cid:80)(cid:78)(cid:79)(cid:110)(cid:67)(cid:97)(cid:110)(cid:16)(cid:23)(cid:11) (cid:23)(cid:2)(cid:110) (cid:72)(cid:90)(cid:95)(cid:110)(cid:68)(cid:90)(cid:98)(cid:79)(cid:110)(cid:97)(cid:104)(cid:89)(cid:98)(cid:79)(cid:71)(cid:98)(cid:80)(cid:69)(cid:110) (cid:98)(cid:95)(cid:67)(cid:74)(cid:69)(cid:110)(cid:67)(cid:89)(cid:70)(cid:110) (cid:80)(cid:89)(cid:70)(cid:100)(cid:97)(cid:98)(cid:95)(cid:104)(cid:110) (cid:68)(cid:71)(cid:89)(cid:69)(cid:79)(cid:87)(cid:67)(cid:95)(cid:83)(cid:97)(cid:11)(cid:110) (cid:51)(cid:29)(cid:30)(cid:29)(cid:50)(cid:29)(cid:42)(cid:25)(cid:29)(cid:53)(cid:136) (cid:67)(cid:23)(cid:69)(cid:122) (cid:53)(cid:70)(cid:108)(cid:70)(cid:93)(cid:88)(cid:75)(cid:122) (cid:42)(cid:100)(cid:105)(cid:88)(cid:82)(cid:86)(cid:108)(cid:122) (cid:49)(cid:75)(cid:105)(cid:82)(cid:75)(cid:105)(cid:122) (cid:70)(cid:100)(cid:73)(cid:122) (cid:51)(cid:88)(cid:14)(cid:60)(cid:86)(cid:88)(cid:112)(cid:70)(cid:100)(cid:122) (cid:57)(cid:75)(cid:86)(cid:15)(cid:122) (cid:55)(cid:100)(cid:14)(cid:72)(cid:86)(cid:88)(cid:102)(cid:122) (cid:53)(cid:75)(cid:108)(cid:119) (cid:114)(cid:101)(cid:105)(cid:91)(cid:107)(cid:15)(cid:52)(cid:101)(cid:105)(cid:82)(cid:70)(cid:100)(cid:122) (cid:2)(cid:116) (cid:40)(cid:93)(cid:70)(cid:116)(cid:102)(cid:101)(cid:101)(cid:93)(cid:122) (cid:102)(cid:112)(cid:71)(cid:93)(cid:88)(cid:107)(cid:86)(cid:75)(cid:105)(cid:107)(cid:12)(cid:122) (cid:24)(cid:21)(cid:21)(cid:33)(cid:15)(cid:122) (cid:67)(cid:24)(cid:69)(cid:122) (cid:51)(cid:15)(cid:122) (cid:60)(cid:86)(cid:70)(cid:100)(cid:82)(cid:122) (cid:75)(cid:108)(cid:15)(cid:70)(cid:93)(cid:122) (cid:1)(cid:122) (cid:61)(cid:75)(cid:97)(cid:102)(cid:75)(cid:105)(cid:70)(cid:108)(cid:112)(cid:105)(cid:75)(cid:14)(cid:36)(cid:114)(cid:70)(cid:105)(cid:75)(cid:122)(cid:55)(cid:100)(cid:14)(cid:72)(cid:86)(cid:88)(cid:102)(cid:122)(cid:53)(cid:75)(cid:108)(cid:114)(cid:101)(cid:105)(cid:91)(cid:107)(cid:2)(cid:122) (cid:46)(cid:42)(cid:42)(cid:42)(cid:122) (cid:52)(cid:88)(cid:72)(cid:105)(cid:101)(cid:15)(cid:122) (cid:24)(cid:30)(cid:8)(cid:23)(cid:10)(cid:122) (cid:12)(cid:122) (cid:102)(cid:102)(cid:15)(cid:23)(cid:26)(cid:21)(cid:14)(cid:23)(cid:26)(cid:33)(cid:12)(cid:122) (cid:24)(cid:21)(cid:21)(cid:30)(cid:122) (cid:67)(cid:26)(cid:69)(cid:122) (cid:41)(cid:15)(cid:122) (cid:38)(cid:75)(cid:105)(cid:108)(cid:101)(cid:118)(cid:118)(cid:88)(cid:122) (cid:75)(cid:108)(cid:15)(cid:70)(cid:93)(cid:122) (cid:1)(cid:53)(cid:101)(cid:40)(cid:122) (cid:107)(cid:116)(cid:100)(cid:108)(cid:86)(cid:75)(cid:107)(cid:88)(cid:107)(cid:122) (cid:81)(cid:101)(cid:114)(cid:122) (cid:76)(cid:101)(cid:105)(cid:122) (cid:72)(cid:112)(cid:107)(cid:108)(cid:101)(cid:97)(cid:88)(cid:118)(cid:75)(cid:73)(cid:122) (cid:73)(cid:101)(cid:97)(cid:70)(cid:88)(cid:100)(cid:122) (cid:107)(cid:102)(cid:75)(cid:72)(cid:88)(cid:79)(cid:72)(cid:122) (cid:97)(cid:112)(cid:93)(cid:108)(cid:88)(cid:102)(cid:105)(cid:101)(cid:72)(cid:75)(cid:107)(cid:107)(cid:101)(cid:105)(cid:122) (cid:46)(cid:42)(cid:42)(cid:42)(cid:122) (cid:62)(cid:70)(cid:100)(cid:107)(cid:70)(cid:72)(cid:108)(cid:88)(cid:101)(cid:100)(cid:107)(cid:122) (cid:101)(cid:100)(cid:122) (cid:57)(cid:70)(cid:105)(cid:70)(cid:93)(cid:93)(cid:75)(cid:93)(cid:122) (cid:70)(cid:100)(cid:73)(cid:122) (cid:41)(cid:88)(cid:107)(cid:108)(cid:105)(cid:88)(cid:71)(cid:112)(cid:108)(cid:75)(cid:73)(cid:122) (cid:102)(cid:102)(cid:15)(cid:23)(cid:23)(cid:26)(cid:14)(cid:23)(cid:24)(cid:33)(cid:12)(cid:122) (cid:24)(cid:21)(cid:21)(cid:29)(cid:122) (cid:67)(cid:28)(cid:69)(cid:122) (cid:60)(cid:15)(cid:122) (cid:64)(cid:70)(cid:100)(cid:82)(cid:70)(cid:93)(cid:122) (cid:75)(cid:108)(cid:15)(cid:70)(cid:93)(cid:122) (cid:1)(cid:122) (cid:36)(cid:122) (cid:29)(cid:15)(cid:23)(cid:44)(cid:45)(cid:118)(cid:122) (cid:21)(cid:15)(cid:26)(cid:28)(cid:98)(cid:98)(cid:25)(cid:122) (cid:59)(cid:101)(cid:112)(cid:108)(cid:75)(cid:105)(cid:122) (cid:76)(cid:101)(cid:105)(cid:122) (cid:53)(cid:75)(cid:108)(cid:114)(cid:101)(cid:105)(cid:91)(cid:119) (cid:101)(cid:100)(cid:14)(cid:40)(cid:86)(cid:88)(cid:102)(cid:122) (cid:36)(cid:102)(cid:102)(cid:93)(cid:88)(cid:72)(cid:70)(cid:108)(cid:88)(cid:101)(cid:100)(cid:107)(cid:2)(cid:122) (cid:46)(cid:100)(cid:122) (cid:57)(cid:106)(cid:101)(cid:72)(cid:15)(cid:122) (cid:46)(cid:42)(cid:42)(cid:42)(cid:122) (cid:60)(cid:116)(cid:97)(cid:102)(cid:101)(cid:107)(cid:88)(cid:112)(cid:97)(cid:122) (cid:101)(cid:100)(cid:122) (cid:64)(cid:51)(cid:60)(cid:46) (cid:122) (cid:40)(cid:88)(cid:105)(cid:72)(cid:112)(cid:88)(cid:108) (cid:12)(cid:122) (cid:102)(cid:102)(cid:18)(cid:28)(cid:24)(cid:14)(cid:28)(cid:26)(cid:12)(cid:24)(cid:21)(cid:21)(cid:31)(cid:122) (cid:67)(cid:29)(cid:69)(cid:122) (cid:45)(cid:15)(cid:49)(cid:88)(cid:100)(cid:82)(cid:72)(cid:70)(cid:101)(cid:122) (cid:70)(cid:100)(cid:73)(cid:122) (cid:59)(cid:15)(cid:122) (cid:52)(cid:70)(cid:105)(cid:72)(cid:112)(cid:93)(cid:75)(cid:107)(cid:72)(cid:112)(cid:122) (cid:70)(cid:100)(cid:73)(cid:122) (cid:50)(cid:75)(cid:105)(cid:76)(cid:101)(cid:105)(cid:97)(cid:70)(cid:100)(cid:72)(cid:75)(cid:119) (cid:70)(cid:114)(cid:70)(cid:105)(cid:75)(cid:122) (cid:97)(cid:70)(cid:102)(cid:102)(cid:88)(cid:100)(cid:82)(cid:122) (cid:76)(cid:101)(cid:105)(cid:122) (cid:105)(cid:75)(cid:82)(cid:112)(cid:93)(cid:70)(cid:105)(cid:122) (cid:53)(cid:101)(cid:40)(cid:122) (cid:46)(cid:42)(cid:42)(cid:42)(cid:122) (cid:108)(cid:105)(cid:70)(cid:100)(cid:107)(cid:119) (cid:70)(cid:72)(cid:108)(cid:88)(cid:101)(cid:100)(cid:107)(cid:122) (cid:101)(cid:100)(cid:122) (cid:40)(cid:101)(cid:97)(cid:102)(cid:112)(cid:108)(cid:75)(cid:105)(cid:14)(cid:36)(cid:88)(cid:73)(cid:75)(cid:73)(cid:122) (cid:41)(cid:75)(cid:107)(cid:88)(cid:82)(cid:100)(cid:122) (cid:101)(cid:76)(cid:122) (cid:46)(cid:100)(cid:108)(cid:75)(cid:82)(cid:105)(cid:70)(cid:108)(cid:75)(cid:73)(cid:122) (cid:40)(cid:88)(cid:105)(cid:72)(cid:112)(cid:88)(cid:108)(cid:107)(cid:122) (cid:70)(cid:100)(cid:73)(cid:122) (cid:60)(cid:116)(cid:107)(cid:108)(cid:75)(cid:97)(cid:107)(cid:15)(cid:24)(cid:28)(cid:8)(cid:28)(cid:10)(cid:12)(cid:122) (cid:102)(cid:102)(cid:15)(cid:29)(cid:29)(cid:23)(cid:14)(cid:29)(cid:30)(cid:24)(cid:12)(cid:122) (cid:24)(cid:21)(cid:21)(cid:29)(cid:122) (cid:67)(cid:30)(cid:69)(cid:122) (cid:52)(cid:15)(cid:122) (cid:41)(cid:70)(cid:100)(cid:75)(cid:107)(cid:86)(cid:108)(cid:70)(cid:93)(cid:70)(cid:71)(cid:122) (cid:75)(cid:108)(cid:15)(cid:70)(cid:93)(cid:122) (cid:1)(cid:53)(cid:101)(cid:40)(cid:122) (cid:45)(cid:101)(cid:108)(cid:122) (cid:60)(cid:102)(cid:101)(cid:108)(cid:122) (cid:97)(cid:88)(cid:100)(cid:88)(cid:97)(cid:88)(cid:118)(cid:70)(cid:108)(cid:88)(cid:101)(cid:100)(cid:122) (cid:63)(cid:107)(cid:88)(cid:100)(cid:82)(cid:122) (cid:36)(cid:100)(cid:108)(cid:122)(cid:53)(cid:75)(cid:108)(cid:122) (cid:41)(cid:116)(cid:100)(cid:70)(cid:97)(cid:88)(cid:72)(cid:122) (cid:59)(cid:101)(cid:112)(cid:108)(cid:88)(cid:100)(cid:82)(cid:122) (cid:36)(cid:93)(cid:82)(cid:101)(cid:105)(cid:88)(cid:108)(cid:86)(cid:97)(cid:2)(cid:122) (cid:46)(cid:100) (cid:122) (cid:57)(cid:105)(cid:101)(cid:72)(cid:15)(cid:122) (cid:36)(cid:102)(cid:102)(cid:93)(cid:88)(cid:72)(cid:70)(cid:108)(cid:88)(cid:101)(cid:100)(cid:119) (cid:107)(cid:102)(cid:75)(cid:72)(cid:88)(cid:79)(cid:72)(cid:122) (cid:60)(cid:116)(cid:107)(cid:108)(cid:75)(cid:97)(cid:107)(cid:12)(cid:36)(cid:105)(cid:72)(cid:86)(cid:88)(cid:108)(cid:75)(cid:72)(cid:108)(cid:112)(cid:105)(cid:75)(cid:107)(cid:122) (cid:70)(cid:100)(cid:73)(cid:122) (cid:57)(cid:105)(cid:101)(cid:72)(cid:75)(cid:107)(cid:107)(cid:101)(cid:105)(cid:107)(cid:12)(cid:122) (cid:58)(cid:48)(cid:26)(cid:26)(cid:14)(cid:26)(cid:32)(cid:12)(cid:24)(cid:21)(cid:21)(cid:30)(cid:122) (cid:67)(cid:31)(cid:69)(cid:122) (cid:52)(cid:15)(cid:122) (cid:52)(cid:70)(cid:105)(cid:113)(cid:70)(cid:107)(cid:108)(cid:88)(cid:122) (cid:75)(cid:108)(cid:15)(cid:70)(cid:93)(cid:122) (cid:1)(cid:57)(cid:36)(cid:52)(cid:57)(cid:59) (cid:34)(cid:122) (cid:57)(cid:101)(cid:114)(cid:75)(cid:105)(cid:14)(cid:70)(cid:114)(cid:70)(cid:105)(cid:75)(cid:122) (cid:70)(cid:100)(cid:73)(cid:122) (cid:97)(cid:99)(cid:88)(cid:97)(cid:112)(cid:97)(cid:122) (cid:102)(cid:70)(cid:108)(cid:86)(cid:122) (cid:70)(cid:93)(cid:82)(cid:101)(cid:105)(cid:88)(cid:108)(cid:86)(cid:97)(cid:122) (cid:76)(cid:101)(cid:105)(cid:122) (cid:53)(cid:101)(cid:40)(cid:107)(cid:2)(cid:122) (cid:46)(cid:42)(cid:42)(cid:42)(cid:122) (cid:46)(cid:100)(cid:108)(cid:75)(cid:105)(cid:100)(cid:70)(cid:108)(cid:88)(cid:101)(cid:100)(cid:70)(cid:93)(cid:122) (cid:40)(cid:101)(cid:100)(cid:76)(cid:75)(cid:105)(cid:75)(cid:100)(cid:72)(cid:75)(cid:122) (cid:101)(cid:100)(cid:122) (cid:102)(cid:102)(cid:15)(cid:28)(cid:23)(cid:32)(cid:14)(cid:28)(cid:24)(cid:23)(cid:122) (cid:24)(cid:21)(cid:21)(cid:32)(cid:122) (cid:67)(cid:32)(cid:69)(cid:122) (cid:41)(cid:15)(cid:122) (cid:43)(cid:88)(cid:72)(cid:91)(cid:122) (cid:75)(cid:108)(cid:15)(cid:70)(cid:93)(cid:122) (cid:1)(cid:122) (cid:36)(cid:122) (cid:86)(cid:88)(cid:82)(cid:86)(cid:95)(cid:116)(cid:122) (cid:105)(cid:75)(cid:107)(cid:88)(cid:93)(cid:88)(cid:75)(cid:100)(cid:108)(cid:122) (cid:105)(cid:101)(cid:112)(cid:108)(cid:88)(cid:100)(cid:82)(cid:122) (cid:70)(cid:93)(cid:82)(cid:101)(cid:105)(cid:88)(cid:108)(cid:86)(cid:97)(cid:122) (cid:76)(cid:101)(cid:105)(cid:122) (cid:76)(cid:70)(cid:112)(cid:93)(cid:108)(cid:119) (cid:108)(cid:101)(cid:93)(cid:75)(cid:105)(cid:70)(cid:100)(cid:108)(cid:122)(cid:53)(cid:101)(cid:40)(cid:107)(cid:2)(cid:122) (cid:46)(cid:100) (cid:122)(cid:57)(cid:105)(cid:101)(cid:72)(cid:15)(cid:122) (cid:41)(cid:75)(cid:107)(cid:88)(cid:82)(cid:100)(cid:12)(cid:122)(cid:36)(cid:112)(cid:108)(cid:101)(cid:97)(cid:70)(cid:108)(cid:88)(cid:101)(cid:100)(cid:122) (cid:2)(cid:116)(cid:61)(cid:75)(cid:107)(cid:108)(cid:122) (cid:88)(cid:100)(cid:122) (cid:42)(cid:112)(cid:105)(cid:101)(cid:102)(cid:75)(cid:122) (cid:40)(cid:101)(cid:100)(cid:76)(cid:75)(cid:105)(cid:75)(cid:100)(cid:72)(cid:75)(cid:122) (cid:2)(cid:116) (cid:42)(cid:115)(cid:86)(cid:88)(cid:71)(cid:88)(cid:108)(cid:88)(cid:101)(cid:100)(cid:11)(cid:122) (cid:102)(cid:102)(cid:15)(cid:24)(cid:23)(cid:14)(cid:24)(cid:30)(cid:122) (cid:24)(cid:21)(cid:21)(cid:33)(cid:122) (cid:67)(cid:33)(cid:69)(cid:122) (cid:52)(cid:15)(cid:122) (cid:57)(cid:70)(cid:93)(cid:75)(cid:107)(cid:88)(cid:122) (cid:75)(cid:108)(cid:15)(cid:70)(cid:93)(cid:122) (cid:1)(cid:36)(cid:102)(cid:102)(cid:93)(cid:88)(cid:72)(cid:70)(cid:108)(cid:47)(cid:56)(cid:100)(cid:122) (cid:60)(cid:102)(cid:75)(cid:72)(cid:88)(cid:80)(cid:72)(cid:122) (cid:59)(cid:101)(cid:112)(cid:108)(cid:88)(cid:100)(cid:82)(cid:122) (cid:70)(cid:93)(cid:82)(cid:101)(cid:105)(cid:88)(cid:108)(cid:86)(cid:97)(cid:107)(cid:122) (cid:76)(cid:101)(cid:105)(cid:122) (cid:53)(cid:75)(cid:108)(cid:114)(cid:101)(cid:105)(cid:91)(cid:107)(cid:122) (cid:101)(cid:100)(cid:122) (cid:72)(cid:86)(cid:88)(cid:102)(cid:4)(cid:4)(cid:46)(cid:42)(cid:42)(cid:42)(cid:122) (cid:62)(cid:70)(cid:100)(cid:107)(cid:70)(cid:72)(cid:108)(cid:88)(cid:101)(cid:100)(cid:107)(cid:122) (cid:101)(cid:100)(cid:122) (cid:57)(cid:70)(cid:105)(cid:70)(cid:93)(cid:93)(cid:75)(cid:93)(cid:122) (cid:70)(cid:100)(cid:73)(cid:122) (cid:41)(cid:88)(cid:107)(cid:119) (cid:108)(cid:105)(cid:88)(cid:71)(cid:112)(cid:108)(cid:75)(cid:73)(cid:122) (cid:60)(cid:116)(cid:107)(cid:108)(cid:75)(cid:97)(cid:107)(cid:17)(cid:122) (cid:24)(cid:21)(cid:13)(cid:8)(cid:26)(cid:10)(cid:13)(cid:122) (cid:102)(cid:102)(cid:17)(cid:26)(cid:23)(cid:30)(cid:14)(cid:26)(cid:26)(cid:21)(cid:13)(cid:122) (cid:24)(cid:21)(cid:21)(cid:33)(cid:122) (cid:45)(cid:101)(cid:108)(cid:107)(cid:102)(cid:101)(cid:108)(cid:122) (cid:29)(cid:15)(cid:21)(cid:122) (cid:86)(cid:108)(cid:108)(cid:102)(cid:34)(cid:20)(cid:20)(cid:93)(cid:70)(cid:113)(cid:70)(cid:15)(cid:72)(cid:107)(cid:15)(cid:113)(cid:88)(cid:105)(cid:82)(cid:88)(cid:100)(cid:88)(cid:70)(cid:15)(cid:75)(cid:73)(cid:112)(cid:20)(cid:45)(cid:101)(cid:108)(cid:60)(cid:102)(cid:101)(cid:108)(cid:122) (cid:52)(cid:15)(cid:122) (cid:57)(cid:70)(cid:93)(cid:75)(cid:107)(cid:88)(cid:122) (cid:75)(cid:108)(cid:15)(cid:70)(cid:93)(cid:122) (cid:1)(cid:38)(cid:70)(cid:100)(cid:73)(cid:114)(cid:88)(cid:73)(cid:108)(cid:86)(cid:14)(cid:70)(cid:114)(cid:70)(cid:105)(cid:75)(cid:122) (cid:105)(cid:101)(cid:112)(cid:108)(cid:88)(cid:100)(cid:82)(cid:122) (cid:70)(cid:93)(cid:82)(cid:101)(cid:105)(cid:88)(cid:108)(cid:86)(cid:97)(cid:107)(cid:122) (cid:76)(cid:101)(cid:105)(cid:122) (cid:100)(cid:75)(cid:108)(cid:114)(cid:101)(cid:105)(cid:91)(cid:107)(cid:14)(cid:101)(cid:100)(cid:14)(cid:72)(cid:86)(cid:88)(cid:102)(cid:122) (cid:102)(cid:93)(cid:70)(cid:108)(cid:76)(cid:101)(cid:105)(cid:97)(cid:107)(cid:2)(cid:122) (cid:96)(cid:42)(cid:61) (cid:12)(cid:122) (cid:40)(cid:101)(cid:97)(cid:102)(cid:112)(cid:108)(cid:75)(cid:105)(cid:122) (cid:2)(cid:116) (cid:41)(cid:88)(cid:82)(cid:88)(cid:108)(cid:70)(cid:93)(cid:122) (cid:61)(cid:75)(cid:72)(cid:86)(cid:119) (cid:100)(cid:88)(cid:104)(cid:112)(cid:75)(cid:107)(cid:15)(cid:26)(cid:8)(cid:29)(cid:10)(cid:15)(cid:122) (cid:102)(cid:102)(cid:15)(cid:28)(cid:23)(cid:26)(cid:14)(cid:28)(cid:24)(cid:33)(cid:12)(cid:24)(cid:21)(cid:21)(cid:33)(cid:122) (cid:67)(cid:23)(cid:24)(cid:69)(cid:122) (cid:59)(cid:15)(cid:122) (cid:61)(cid:70)(cid:105)(cid:90)(cid:70)(cid:100)(cid:122) (cid:1)(cid:122) (cid:41)(cid:75)(cid:102)(cid:108)(cid:86)(cid:14)(cid:79)(cid:105)(cid:107)(cid:108)(cid:122) (cid:107)(cid:75)(cid:70)(cid:105)(cid:72)(cid:86)(cid:122) (cid:70)(cid:100)(cid:73)(cid:122) (cid:93)(cid:88)(cid:100)(cid:75)(cid:70)(cid:105)(cid:122) (cid:82)(cid:105)(cid:70)(cid:102)(cid:86)(cid:122) (cid:70)(cid:93)(cid:82)(cid:101)(cid:105)(cid:88)(cid:108)(cid:86)(cid:97)(cid:107)(cid:2)(cid:122) (cid:60)(cid:46)(cid:36)(cid:52)(cid:122) (cid:49)(cid:101)(cid:112)(cid:105)(cid:100)(cid:70)(cid:93)(cid:122) (cid:101)(cid:100)(cid:122) (cid:40)(cid:101)(cid:97)(cid:102)(cid:112)(cid:108)(cid:88)(cid:100)(cid:82)(cid:15)(cid:122) (cid:23)(cid:8)(cid:24)(cid:10)(cid:15)(cid:102)(cid:102)(cid:15)(cid:23)(cid:28)(cid:30)(cid:14)(cid:23)(cid:30)(cid:21)(cid:12)(cid:122) (cid:23)(cid:33)(cid:31)(cid:24)(cid:122) (cid:67)(cid:23)(cid:26)(cid:69)(cid:122) (cid:53)(cid:101)(cid:115)(cid:88)(cid:97)(cid:122) (cid:107)(cid:88)(cid:97)(cid:112)(cid:93)(cid:70)(cid:108)(cid:101)(cid:105)(cid:34)(cid:122) (cid:86)(cid:108)(cid:108)(cid:102)(cid:34)(cid:20)(cid:20)(cid:114)(cid:114)(cid:114)(cid:15)(cid:100)(cid:101)(cid:115)(cid:88)(cid:97)(cid:15)(cid:101)(cid:105)(cid:82)(cid:122) (cid:82)(cid:110)(cid:113)(cid:36)(cid:116) (cid:86)(cid:108)(cid:108)(cid:102)(cid:34)(cid:20)(cid:20)(cid:72)(cid:113)(cid:115)(cid:105)(cid:15)(cid:72)(cid:101)(cid:97)(cid:20)(cid:72)(cid:113)(cid:115)(cid:122) (cid:38)(cid:101)(cid:101)(cid:91)(cid:107)(cid:88)(cid:97)(cid:122) (cid:107)(cid:88)(cid:97)(cid:112)(cid:93)(cid:70)(cid:108)(cid:101)(cid:105)(cid:12)(cid:122) (cid:86)(cid:108)(cid:108)(cid:102)(cid:34)(cid:20)(cid:20)(cid:100)(cid:101)(cid:72)(cid:107)(cid:15)(cid:107)(cid:108)(cid:70)(cid:100)(cid:76)(cid:101)(cid:105)(cid:73)(cid:18)(cid:75)(cid:73)(cid:112)(cid:20)(cid:122) (cid:67)(cid:23)(cid:30)(cid:69)(cid:122) (cid:36)(cid:15)(cid:122) (cid:57)(cid:112)(cid:93)(cid:93)(cid:88)(cid:100)(cid:88)(cid:122) (cid:75)(cid:108)(cid:15)(cid:70)(cid:93)(cid:122) (cid:1)(cid:53)(cid:101)(cid:40)(cid:122) (cid:41)(cid:75)(cid:107)(cid:88)(cid:82)(cid:100)(cid:122) (cid:70)(cid:100)(cid:73)(cid:122) (cid:46)(cid:97)(cid:102)(cid:93)(cid:75)(cid:97)(cid:75)(cid:100)(cid:108)(cid:70)(cid:108)(cid:88)(cid:101)(cid:100)(cid:122) (cid:88)(cid:100)(cid:122) (cid:30)(cid:29)(cid:100)(cid:97)(cid:122) (cid:61)(cid:75)(cid:72)(cid:86)(cid:100)(cid:101)(cid:93)(cid:101)(cid:82)(cid:116)(cid:4)(cid:4)(cid:43)(cid:88)(cid:105)(cid:107)(cid:108)(cid:122) (cid:46)(cid:100)(cid:108)(cid:75)(cid:105)(cid:100)(cid:70)(cid:108)(cid:88)(cid:101)(cid:100)(cid:70)(cid:93)(cid:122) (cid:60)(cid:116)(cid:97)(cid:102)(cid:101)(cid:107)(cid:88)(cid:112)(cid:97)(cid:122) (cid:101)(cid:100)(cid:122) (cid:53)(cid:75)(cid:108)(cid:114)(cid:101)(cid:105)(cid:91)(cid:107)(cid:14)(cid:101)(cid:100)(cid:120) (cid:72)(cid:86)(cid:88)(cid:102)(cid:12)(cid:122) (cid:102)(cid:102)(cid:15)(cid:24)(cid:31)(cid:26)(cid:14)(cid:24)(cid:32)(cid:24)(cid:12)(cid:24)(cid:21)(cid:21)(cid:31)(cid:122) (cid:18)(cid:19)(cid:18)(cid:136) 
2011,Network-on-Chip router design with Buffer-Stealing.,"Communication in a Network-on-Chip (NoC) can be made more efficient by designing faster routers, using larger buffers, larger number of ports and channels, and adaptive routing, all of which incur significant overheads in hardware costs. As a more economic solution, we try to improve communication efficiency without increasing the buffer size. A Buffer-Stealing (BS) mechanism is proposed, which enables the input channels that have insufficient buffer space to utilize at runtime the unused input buffers from other input channels. Implementation results of the proposed BS design for a 64-bit 5-input-buffer router show a reduction of the average packet transmission latency by up to 10.17% and an increase of the average throughput by up to 23.47%, at an overhead of 22% more hardware resources.","2B-3 Network-on-Chip Router Design with Buffer-Stealing Wan-Ting Su, Jih-Sheng Shen, Pao-Ann Hsiung Computer Science and Information Engineering National Chung Cheng University Chiayi 62102, Taiwan, ROC Tel: +886-5-272-0411 ext 33119 e-mail: {swt97m, sjs92, pahsiung}@cs.ccu.edu.tw Fax: +886-5-272-0859 Abstract— Communication in a Network-on-Chip (NoC) can be made more efﬁcient by designing faster routers, using larger buffers, larger number of ports and channels, and adaptive routing, all of which incur signiﬁcant overheads in hardware costs. As a more economic solution, we try to improve communication efﬁciency without increasing the buffer size. A Buffer-Stealing (BS) mechanism is proposed, which enables the input channels that have insufﬁcient buffer space to utilize at runtime the unused input buffers from other input channels. Implementation results of the proposed BS design for a 64-bit 5-input-buffer router show a reduction of the average packet transmission latency by up to 10.17% and an increase of the average throughput by up to 23.47%, at an overhead of 22% more hardware resources. Buffer(cid:882)shar ing methods Simulation env ironment Routing algorithm Traffic(cid:3)pattern No.(cid:3)of virtual(cid:3) channel Performance(cid:3) enhancement Compare to Lai(cid:3)[7](cid:3)(DAC ’08) Cycle(cid:882)accurate(cid:3) simulator Dimension routing 1) Uniform 2) Hotspot 10 VCs(cid:3) (32(cid:3)bits(cid:3)buffers) 2) 1) Throughput(cid:3) :(cid:3) 8.3% increase Latency :(cid:3) 19.6%(cid:3) decrease 1) 2(cid:3)VCs(cid:3)router 2) 4 VCs(cid:3)router Liu(cid:3)[4] (CSS’07) Liu(cid:3)[9](cid:3)(MWSCAS’06) Flexsim 1.2 Neishaburi [11](cid:3) (GLSVLSI’09) RTL,(cid:3)VHDL Adaptive(cid:3)routing XY(cid:882)YX(cid:3)routing Our (cid:3)method (BS) RTL,(cid:3)VHDL Modelsim SE(cid:3)5.8d(cid:3) XY(cid:3)routing Uniform 4(cid:3)VCs(cid:3)for(cid:3) one(cid:3) physical(cid:3) channel (one(cid:3)VC(cid:3) =(cid:3)4(cid:3)flits) Throughput : • 2% than(cid:3)SAMQ • 1%(cid:3)than(cid:3)DAMQall Uniform Transpose 4(cid:882)16 VCs (64(cid:3)bits(cid:3)buffers) Latency(cid:3): • 7.1% decrease(cid:3) (cid:3) in(cid:3)uniform • 3.5%(cid:3)decrease(cid:3) in(cid:3)transpose 1) SAMQ 2) DAMQall 4(cid:3)VCs router (one(cid:3)VC(cid:3) =(cid:3)16(cid:3)flits) Burst No VC 2) 1) Throughput(cid:3) :(cid:3) 40% increase(cid:3) (23.47%(cid:3)in(cid:3)average) Latency(cid:3):(cid:3) 22.46%(cid:3)decrease(cid:3) (10.17%(cid:3)in(cid:3)average) Extended original(cid:3) buffer(cid:3) (with(cid:3)the(cid:3) same(cid:3) additional(cid:3)hardware(cid:3) overhead(cid:3) ) I . IN TRODUC T ION Fig. 1. Comparisons among state-of-the-art buffer sharing designs. A basic NoC architecture is composed of routers, communication links between routers, and a Network-Interface Component (NIC) between each pair of router and processing element (PE) [2]. NoC allows much higher bandwidth through parallel communication. Each router can accept simultaneously the ﬂits arriving from all of the input channels by storing them in input buffers. The input buffers in a router are used to temporarily store arriving ﬂits that cannot be forwarded immediately to required output channels. The ﬂits in the buffers are then transmitted through the output channels. There are three situations which demand the use of buffering as explained in the following. ∙ Wait for routing decisions: When ﬂits arrive at the input channels of a router, the ﬂits need to be stored in the input buffers, while waiting for the routing decisions. ∙ Contention for the same output channel: If more than one ﬂit contend for the same output channel, then only one ﬂit can be transmitted from the output channel at a time, while the other contending ﬂits are stored in their respective input buffers for later transmission. ∙ Congested downstream router: Due to the lack of input buffer space in a congested downstream router, ﬂits are also required to be stored in their current input buffers. All the above three cases motivate the need for the effective utilization of router buffers such that the communication efﬁciency of interconnection networks can be elevated. To efﬁciently utilize the ﬁxed buffer space provided by a router, a Buffer-Stealing (BS) mechanism is proposed. Instead of adding extra † This work was partially supported by the National Science Council, Taiwan, under project grant NSC98-2221-E-194-049-MY3. buffers into a router at design time, buffer stealing enables the input channels that have insufﬁcient free buffer space to utilize at runtime the free input buffers from other input channels. This paper is organized as follows. Section II summarizes the related works and gives detailed comparisons on buffer schemes. The detailed operations of the buffer stealing method is explained in Section III. Section IV shows the experimental results and ﬁnally we summarize our ﬁndings and future work in Section V. I I . R E LAT ED WORK The design methodology and key research problems of NoCs have been discussed in [5, 10]. In particular, a large buffer size reduces the average packet latency in NoC; however, the large buffer size also increases the overall NoC area. To increase buffer utilization, some buffer sharing methods [4, 7, 9, 11] were proposed for virtual channels (VC) in a router design. As summarized in Figure 1, the performance enhancement achieved by the buffer sharing methods in VC routers is limited, because the control complexity of VC design incurs huge overheads in terms of additional hardware resources and power encumbrances. For example, virtual channel buffers require up to nearly 50% of area and account for 64% of leakage power in a router implemented under the 70 nm CMOS technology [1]. A central buffer design was proposed by Huang et al. [3] and Leung et al. [8], which used multiple-port memory and register ﬁle for buffer sharing. Signiﬁcant performance enhancement could be obtained at 978-1-4244-7516-2/11/$26.00 ©2011 IEEE 160 8 clock Buffer Empty/Full Checker Pointer_Location  Information 3 3 Rx Ack_Rx full empty W_Remaining_Buffer E_Remaining_Buffer W_Steal_Enable E_Steal_Enable W_Steal_Ack E_Steal_Ack 4 4 Write_Pointer Read_Pointer 0 1 2 3 4 5 6 7 1 bit 8 bits Write_FSM_Controller FSM_1 FSM_2 FSM_0 W_Steal_Location (3 bits) E_Steal_Location (3 bits) 4 4 0  & 1  & 8 Data to be stored  in the buffer (8 bits : flits 4 bits :  information of steal location) Steal_Flag for  Current Read  Position Checker Read_FSM_Controller FSM_1 FSM_0 Steal Flag (0 ~ 7) W_Read_Enable E_Read_Enable Steal_Read_Location W_Read_Available E_Read_Available Read_Request Flit_Available Steal Location 4 W_Read_Flit E_Read_Flit 2 8 8 8 Read_Flit 2 3 b 1 t i b 3 t i s W or E 000 111 Stealing  Inf o . Stealing  Inf o . Flit Buffer_ID (3 bits) Store_ MUX Read_MUX Reset Flit Fig. 2. Thief buffer design. the cost of signiﬁcant overhead in hardware resources. It is thus not very cost-efﬁcient to implement in a NoC design. Different to the existing works which focus on buffer sharing in VC-routers, we propose a Buffer Stealing (BS) method for the input buffers of normal routers which do not have VCs. The goal of the buffer stealing method is to enhance the performance of normal routers while avoiding unacceptable hardware overhead. The concept of BS can be easily generalized to routers with VCs; however, to consider the hardware overhead and power drawback incurred by VC-routers, designers should be more careful when they try to use the VC-routers to implement their NoC. Besides, we compare the central buffer design with the proposed BS design in terms of hardware resource, throughput, and latency. The result shows that the BS design is more cost-efﬁcient than the central buffer design. I I I . BU FF ER S T EA L ING D E S IGN Before going into the details of how the proposed Buffer Stealing (BS) mechanism is designed, some terminologies are deﬁned as follows. ∙ Thief Buffer: A thief buffer is a buffer that steals the buffer space of other channels when its free buffer space is not enough to store incoming ﬂits. For proof of concept, in this work the buffers in the north and south input channels are designed as thief buffers. ∙ Victim Buffer: A victim buffer is a buffer whose free space can be stolen by a thief buffer. It needs to record that a stored ﬂit is from its own input channel or from that of a thief buffer. For proof of concept, in this work the buffers in the west and east input channels are designed as victim buffers. ∙ Buffer Storage Unit: A buffer storage unit is the basic amount of memory space to store a ﬂit. In this work, we assume the size of a buffer storage unit is 8 bits and the size of the whole input buffer is 64 bits (8 bits × 8 buffer storage units). ∙ Buffer Stealing Unit (BSU): A buffer stealing unit (BSU) is the basic amount of memory space used to store one stealing record Read_Controller Write_Pointer Read_Pointer 0 1 2 3 4 5 6 7 1 bit 1 bit 8 bits Foreign Flag (0 ~ 7) Valid Bits (0 ~ 7) Buffer Free Space Checker Buffer Empty/Full Checker Remaining_Buffer 8 4 Pointer_Location  Information 3 3 Write_Controller Rx full Write/Read Pointer Location  Information 3 3 N_Steal_Enable S_Steal_Enable N_Steal_Ack S_Steal_Ack N_Steal_Location  S_Steal_Location 8 Flit from North 8 Flit from South 2 Read_Request Flit_Available Write/Read Pointer Location  Information Read Pointer  Location Modifier 3 3 3 clock N_Read_Enable S_Read_Enable N_Read_Location S_Read_Location N_Read_Available S_Read_Available Buffer data Local Read Flit North Read Flit South Read Flit 8 8 8 8 Reset Store_ MUX Read_MUX Flit 3 3 3 3 Ack_Rx empty Fig. 3. Victim buffer design. in the thief buffer. One buffer storage unit (8 bits) can be partitioned at runtime into two BSUs. A stealing record includes 2 parts: (1) the index of the buffer storage unit that was stolen from the victim buffer (3 bits) and (2) the identity of the victim buffer, here west or east buffer, (1 bit). ∙ Stealing Flag (SF): The stealing ﬂag (SF) is an 8-bit record in a thief buffer, where each bit is used to distinguish whether the content in a corresponding buffer storage unit is a ﬂit (SF[𝑖] = 0) or a stealing record (SF[𝑖] = 1), where 𝑖 is the buffer index from 0 to 7 for a buffer with 8 storage units. ∙ Foreign Flag (FF): The foreign ﬂag (FF) is an 8-bit record in a victim buffer, where each bit is used to distinguish whether the ﬂit in a corresponding buffer storage unit is from its own input channel (FF[𝑖] = 0) or from the thief buffer (FF[𝑖] = 1), where 0 ≤ 𝑖 ≤ 7. ∙ Valid Bit (VB): The valid bit (VB) is an 8-bit record in a victim buffer, where each bit is used to distinguish whether the ﬂit in a corresponding buffer storage unit is currently valid or not. More precisely, the ﬂit is valid (VB[𝑖] = 1) if it is not yet read and thus cannot be overwritten; otherwise, it is invalid (VB[𝑖] = 0) if it can be modiﬁed again to receive a new ﬂit, where 0 ≤ 𝑖 ≤ 7. When the buffer is considered full, that is, there is only one free buffer space left, in a thief buffer and if it needs to receive more ﬂits, the buffer stealing mechanism is activated to steal the unused free buffer space in victim buffers. Note that the buffer in the local channel connected to the local PE does not support buffer-stealing so that we do not degrade the PE throughput. In the following subsections, we describe in details the issues in the design and implementation of buffer stealing. A.CircuitDesigns forBufferStealing For a buffer to be able to steal space or to provide free space for stealing, speciﬁc support must be provided. Figure 2 and Figure 3 161 2B-3     2B-3 show the circuit designs for the thief buffers and the victim buffers, respectively. The 8-bit Flit signal is used to transmit a ﬂit from a sender (neighboring router) to an input buffer, which could be a thief buffer or a victim buffer. The request for such a transmission is made by the sender via the RX signal which is asserted (high). When a ﬂit is written into an input buffer, the buffer will assert the Ack RX signal to notify the sender the end of the writing operation. Note that the above descriptions are for the original writing operations for receiving the incoming ﬂits that arrive via the buffer’s own input channel. If a thief buffer has no more free buffer space to receive an incoming ﬂit (the position of Read Pointer is equal to the position of Write Pointer + 1), it will assert the W steal enable signal or E steal enable signal to request for free spaces from a victim buffer. A candidate victim for buffer stealing is chosen by comparing the amount of free buffer spaces in the west buffer and the east buffer. The amount of free buffer spaces are obtained via the 4-bit signals W remaining buffer and E remaining buffer from the west and east victim buffers, respectively. Its range is from 0 (no free space) to 8 (there are 8 buffer storage units). The candidate victim selected for buffer stealing is the one with the largest amount of free space. If the victim buffer has the same remaining free buffer space, then the candidate victim selected for buffer stealing is East buffer. When the thief buffer has selected a victim buffer, the Flit that cannot be stored in the thief buffer will be sent to the selected victim buffer. After the end of a buffer stealing operation, the W steal ack signal or E steal ack signal will be asserted by the current victim buffer to notify the thief buffer about the completion of the process of buffer stealing. The 3-bit signals of W steal location and E steal location that represent the stolen BSU location in the victim buffer are transmitted to a thief buffer to be recorded as the stealing information in a corresponding BSU. To read ﬂits from the buffers, the Read Request signal is asserted by the reader (neighboring router). After receiving the reading request, the Flit Available signal is asserted when the ﬂit is ready to be read by the reader. The ﬂit can be transmitted through the Read Flit signal. Note that the above descriptions are for the original reading operations for reading the ﬂits that arrived via the buffer’s own input channel. If the read pointer in the thief buffer is currently at a location n with SF[n] = 1 (stealing information), the thief buffer needs to read ﬂits from the victim buffers. To read ﬂits from the victim buffers, the W read enable signal or E read enable signal is asserted. The 3-bit signal of Steal Read Location sends the stealing position to the victim buffer. The 8-bit signals of W Read Flit and E Read Flit are used to transmit a ﬂit read from the current victim buffer. By adjusting the ratio of the input period and the output period for each input buffer in routers, it can be used to model the different distributions of trafﬁc loads in an NoC. A.Comparison of BS Design and Conventional Design with theSameAdditionalHardwareOverhead The experiment is to compare with the original buffer design; however, we extend it to a larger buffer which has the same hardware overhead (22.18%) as that of the proposed BS design. Based on the NoC extended to 80 bits (8 bits × 10 buffer storage units), while that in the area model in [6], each input buffer in the conventional NoC design is BS design is maintained at 64 bits. A.1 Increase in data throughput For a ﬁxed time duration, we observe the total number of ﬂits that have been received by the thief buffer in BS design and by the extended buffer (EB). We adjust the ratio I:O of the input period (I) and the output period (O) for each input buffer from I:O = 3 cycles:4 cycles to I:O = 3 cycles:10 cycles. Here, the input period is ﬁxed at 3 cycles, i.e. in each period of 3 cycles, a ﬂit is written into the buffer, and the output period is varied from 4 cycles to 10 cycles. As shown in Figure 4(a), we found that the average increase in the number of ﬂits output from the North input buffer is almost the same, irrespective of whether the trafﬁc loads on the East and West buffers are as heavy as or lighter than that on the North and South buffers or even negligible. Due to buffer stealing, the maximum throughput increase is 40% and average throughput increase is 23.47% compared to that of the extended buffer. This maximum average throughput is achieved when I:O=3:7, which shows that buffer stealing is more effective when there is an appreciable difference in input and output ratio (medium congestion). It becomes less effective when there is little difference in the I/O ratio or when there is very heavy congestion. A.2 Reduction in latency As shown in Figure 4(b), we observed the cycles required to receive a ﬁxed number of 30 ﬂits for different output periods from the buffer. We found that buffer stealing can effectively reduce average latency by 10.17% cycles for 30 ﬂits compared to the extended buffer design. The maximum reduction in latency is 22.46% which is achieved when I:O is 3:6. The reason for maximum reduction in latency at I/O ratio of 3:6 is the same as that for the maximum increase in throughput. Thus, through both experiments, we have shown that buffer stealing can effectively alleviate congestion as long as the congestion is not too heavy. Note that buffer stealing is equally effective irrespective of how much trafﬁc loads are imposed on the east and west buffers. IV. EX P ER IM EN T S A.3 Congestion tolerance rate The proposed buffer stealing design was implemented in VHDL and simulated using Modelsim SE 5.8d at the cycle-accurate level. We analyzed several situations to illustrate the advantages and overheads of the proposed buffer-stealing design. We used burst trafﬁc patterns which represent different trafﬁc loads to compare the proposed design with the original buffer design. Here, burst trafﬁc refers to a periodic data transmission that exhibits a very high data signaling rate for very short transmission durations, that are interrupted by ﬁxed idle time intervals. In a period of 190 cycles, the burst trafﬁc pattern sends either 30 ﬂits (North and South ports) or 15 ﬂits (East and West ports). Note that each ﬂit transmission requires 3 cycles. Thus, in each period, the idle interval of burst trafﬁc on the North and South ports is 100 cycles and on the East and West ports is 145 cycles. Figure 4(c) illustrates the growth in latency for transmitting 300 ﬂits via a router for increasing output period. We observe that with buffer stealing a router exhibits an average growth of 20% in latency, while the extended buffer design shows an average growth of 29%. Also note that the trend of latency growth for BS design is more tardy than that for the extended buffer design. A slower latency growth is achieved by buffer stealing because of the reduced average waiting time for each ﬂit. Also note how the reduced growth in latency is achieved by buffer stealing irrespective of the trafﬁc load on the East and West buffers. Figure 4(d) illustrates the reduction in throughput for increasing I/O output period in a ﬁxed period of 1350 cycles between a router and PE. We observe that at each I/O ratio the BS design can transmit more ﬂits than the extended buffer design, which means that with buffer stealing 162 2B-3 250 200 150 (cid:3) ) # ( (cid:3) s t i l f (cid:3) (cid:3) 0 3 e v e i c e r r (cid:3) 100 100 (a) Average increment of ﬂits in the same cycle period (BS: Buffer stealing, EB: Extended buffer). ) % ( (cid:3) h t w o r (cid:3) g y c c n e t a L 70% 60% 50% 40% 30% 20% 10% 0% 8(cid:3)bits(cid:3)x(cid:3)10(cid:3)buffer BS o t (cid:3) d e r i u q e r (cid:3) s e l c y C ) % ( n o i t c u d d e r t u p h g u o (cid:3) Input(cid:3)period:(cid:3)Output(cid:3)period r h T 50 0 0% (cid:882)5% (cid:882)10% (cid:882)15% (cid:882)20% (cid:882)25% (cid:882)30% (cid:882)35% (cid:882)40% 8(cid:3)bits(cid:3)x(cid:3)10(cid:3)buffer BS Input(cid:3)period:(cid:3)Output(cid:3)period (b) Cycles reduction for receiving 30 ﬂits. 8(cid:3)bits(cid:3)x(cid:3)10(cid:3)buffer BS Input(cid:3)period:(cid:3)Output(cid:3)period (c) Congestion tolerance rate in latency growth. (d) Congestion tolerance rate in throughput reduction. Fig. 4. The comparison of BS design and conventional design in the same additional overhead. SYN TH E S I S R E SU LT S FOR D I FF ER EN T BU FF ER D E S IGN S Buffer Design Original buffer Buffer Stealing Central Buffer Hardware overhead (%) baseline 22.18 215.48 TABLE I Frequency (MHz) 232.056 203.442 142.511 the reduction in throughput is less pronounced when the output period is gradually increased. With the BS design, average reduction in the throughput is 12%, while with the extended buffer design it is 18%. Also note how with the BS design, the throughput reduction is almost negligible for the output period of 4 cycles to 7 cycles. However, with a larger output period of the buffer, buffer congestion occurs and thus the BS mechanism does not have free space to record the stealing information even though the other input channels have freely accessible buffer space. For this reason, the reduction in throughput of the BS design at slower output periods tends to be similar to that of the extended buffer design. In summary, from the perspective of slower latency growth and slower throughput reduction, the buffer stealing design exhibits higher tolerance for congestion. B.ComparisonwithCentralBufferDesign Figure 5(a) shows the number of ﬂits that can be received by a router in a ﬁxed duration of 1350 cycles. For fairness of comparison, the central buffer (CB) allows the sharing among three entire buffers (192 bits) since the thief buffer in BS design has its local buffer and two victim buffers to be used. From Figure 5(a), it shows that the central buffer is able to receive more ﬂits than our BS design. Because the thief buffer in BS can share the free space in victim buffers; however, the victim buffers cannot share the free spaces in the other victim buffer and thief buffer. It results in lower buffer utilization than that of the central buffer. Figure 5(b) shows the number of cycles required to receive a ﬁxed number of 300 ﬂits by the BS design and the CB design. We can see that the CB design outperforms the BS design. From the above two experiments, one might conclude that the CB design is better in reducing buffer congestion and thus enhancing the router throughput and reducing the ﬂit waiting time. However, the CB design suffers from serious hardware resource overhead and performance overhead. Table I shows the synthesis results for different buffer designs, where we can see that the hardware overhead of the CB design is very large (almost an additional amount of 215% resources required) than the conventional buffer design. Thus the CB design is not a cost-efﬁcient implementation. However, the proposed buffer stealing method only incurs a hardware resource overhead of 22% compared to the conventional buffer design. To compare the beneﬁt to overhead ratio for different buffer designs, we compute the throughput to hardware overhead ratio (ﬂit (#)/ hardware overhead (%)), as shown in Figure 5(c), and the ratio of latency to hardware overhead((1/latency (# of cycles))/ hardware overhead (%)), as shown in Figure 5(d). In Figure 5(c), we observe that with buffer stealing a router exhibits an enhancement of maximum 26.7% in throughput to hardware overhead ratio than the original buffer design. However, with the faster output period of the buffer, the buffer congestion does not occur frequently and thus the BS mechanism do not need to steal the free buffer space of other input channel. Based on the reason, the throughput to hardware overhead ratio of 163 500 450 400 350 300 300 250 200 150 100 50 50 0 0 5 3 3 1 n (cid:3) d e v v e i (cid:3) (cid:3) s t t (cid:3) r e c i e e ( # ) e e b r (cid:3) o f (cid:3) i l f c y c l BS Central(cid:3)buffer u n e e h T (cid:3) m Input(cid:3)period(cid:3):(cid:3)Output(cid:3)period (a) Average increment of ﬂits in the 1350 cycles period. 1600 1400 1400 1200 1000 1000 800 600 400 200 0 ( # ) (cid:3) 0 0 3 3 e v e i l f s t (cid:3) e e c i (cid:3) r d e (cid:3) t o (cid:3) r BS Central(cid:3)buffer e e s (cid:3) r u q e i c y C l Input(cid:3)period(cid:3):(cid:3)Output(cid:3)period (b) Cycles reduction for receiving 300 ﬂits. 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 r d w a r e v o e (cid:3) r d a e h (cid:3) t t i o (cid:3) Original(cid:3)buffer Original buffer BS h T r u p h g u o t (cid:3) t a h o (cid:3) r a Input(cid:3)period(cid:3):(cid:3)Output(cid:3)period Central(cid:3)buffer (c) Throughput to hardware overhead ratio. 0.000040(cid:3) 0.000035(cid:3) 0.000030(cid:3) 0.000025(cid:3) 0.000020(cid:3) 0 000015 0.000015(cid:3) 0.000010(cid:3) 0.000005(cid:3) 0.000000(cid:3) r r e v o e (cid:3) r d a e h (cid:3) r a t i o (cid:3) Original buffer Original(cid:3)buffer BS a L t n e y c (cid:3) t a h o (cid:3) r d w a Input(cid:3)period(cid:3):(cid:3)Output(cid:3)period Central(cid:3)buffer (d) Latency to hardware overhead ratio. Fig. 5. The comparison between central buffer design and the proposed buffer stealing design. the BS design at small output period tends to be lower to that of the original design. In contrast to original buffer, the additional hardware overhead of central buffer design is a heavy burden. Similar situations can be found in the ratio of latency to hardware overhead shown in Figure 5(d). Based on the above experiment results, we can conclude that the proposed buffer stealing method can enhance router performance at smaller hardware overhead. V. CONC LU S ION S AND FU TUR E WORK To enhance the performance of NoC for higher throughput and lower communication latency, we proposed a buffer stealing method that can steal the free buffers in the low-load ports to support the ports which require more buffers than others for storing large amount of packets. Experiment results show that the proposed BS design can reduce the cycles required for transmitting a ﬁxed number of packets by up to 10.17% and can transmit up to 23.47% more ﬂits in a given time period, when compared to that without buffer stealing. An overhead of 22.18% more hardware resources is incurred by BS for a 64-bit 5-input-buffer router. The experiments show that the BS design supersedes the original design in terms of both throughput and latency. Thus, BS is more robust in handling congestion. Comparing with CB design, the BS design exhibits a great enhancement in throughput to hardware overhead ratio and a efﬁcient reduction in latency to hardware overhead ratio. Future work will consist of the support for dynamically reconﬁgurable system and more real-world example implementations. "
2012,Using link-level latency analysis for path selection for real-time communication on NoCs.,"We present a path selection algorithm that is used when deploying hard real-time traffic flows onto a chip-multiprocessor system. This chip-multiprocessor system uses a priority-based real-time network-on-chip interconnect between the multiple processors. The problem we address is the following: given a mapping of the tasks onto a chip-multiprocessor system, we need to determine the paths that the traffic flows take such that the flows meet there deadlines. Furthermore, we must ensure that the deadline is met even in the presence of direct and indirect interference from other flows sharing network links on the path. To achieve this, our algorithm utilizes a link-level analysis to determine the impact of a link being used by a flow, and its affect on other flows sharing the link. Our experimental results show that we can improve schedulability by about 8% and 15% over Minimum Interference Routing and Widest Shortest Path algorithms, respectively.","6A-4 Using Link-level Latency Analysis for Path Selection for Real-time Communication on NoCs Hany Kashif, Hiren D. Patel and Sebastian Fischmeister Electrical and Computer Engineering e-mail: {hkashif, hdpatel, sﬁschme}@uwaterloo.ca University of Waterloo, Waterloo, Canada Abstract— We present a path selection algorithm that is used when deploying hard real-time trafﬁc ﬂows onto a chipmultiprocessor system. This chip-multiprocessor system uses a priority-based real-time network-on-chip interconnect between the multiple processors. The problem we address is the following: given a mapping of the tasks onto a chip-multiprocessor system, we need to determine the paths that the trafﬁc ﬂows take such that the ﬂows meet there deadlines. Furthermore, we must ensure that the deadline is met even in the presence of direct and indirect interference from other ﬂows sharing network links on the path. To achieve this, our algorithm utilizes a link-level analysis to determine the impact of a link being used by a ﬂow, and its affect on other ﬂows sharing the link. Our experimental results show that we can improve schedulability by about 8% and 15% over Minimum Interference Routing and Widest Shortest Path algorithms, respectively. I . IN TRODUC T ION To deploy hard real-time embedded applications, it is important to accurately predict its worst-case execution times (WCETs). The tasks of the application are schedulable if the WCETs are less than or equal to the temporal deadlines of the application tasks. A WCET analysis typically uses static program analysis techniques, and combines it with a platform model with details of its pipeline micro-architecture, cache organization, and exception mechanisms to produce an upperbound on the execution time of the task. However, with chip-multiprocessing architectures becoming common nowadays, we must include the execution delays experienced by the communication between multiple processing elements in the WCET analysis. This is known as on-chip real-time communication. The common on-chip interconnects for real-time either use resource reservation or run-time arbitration. An example of resource reservation is time-division multiplexing (TDM) as proposed by AEthereal NoCs [1]. TDM NoCs allocate resources statically prior to the execution of the application, and mandates a static schedule identifying when packets are transferred on speciﬁc channels. It ensures that there is no contention for a resource between any two packets. Runtime arbitration NoCs, on the other hand, use priority-aware routers (PAR) to arbitrate channels at runtime. This introduces contention, but expects the analysis to accommodate it. An example is proposed by Shi et al. [2, 3]. The advantage of PAR NoCs over TDM NoCs is that resources are better shared and reused. In addition, low latency ﬂows in TDM are tightly coupled with the bandwidth resulting in the over-allocation of bandwidth. Therefore, we focus on CMPs with a PAR on-chip interconnection network. There are two approaches to the WCET analysis of PAR NoCs: Shi et al. [2, 3] provide a ﬂow-based analysis (FLA), and the other approach is a link-level analysis (LLA) for WCET estimates. At the expense of a detailed analysis, LLA results in signiﬁcantly tighter bounds than FLA. These analyses model communication as periodic ﬂows on the PAR NoC, and they assume that the mapping of ﬂows, and the paths the ﬂows take are given. The set of ﬂows of an application are schedulable if the WCET of every ﬂow is less than or equal to its deadline. However, we notice that for an application with the same set of ﬂows and deadlines, the choice of paths can greatly inﬂuence the schedulability result of the entire application. Assuming a given mapping of the ﬂows onto the NoC, we contend that by judiciously selecting the paths the ﬂows take, we can increase the number of schedulable ﬂows; in turn, allowing more tasks to be schedulable. This brings us to the main contribution of our work: a path selection algorithm (PSA) assisted by the LLA that aims to improve the number of schedulable ﬂows by selecting appropriate paths in the NoC. We use LLA because it considers the pipelining effect of worm-hole switched NoCs, and it provides tight WCET bounds. This is unlike FLA, which treats the ﬂow as an indivisible unit across multiple network links. In particular, we propose a PSA that utilizes observations from LLA to efﬁciently select paths in the PAR NoC. To avoid the high complexity of an optimal algorithm, PSA uses heuristics to ﬁnd least interference paths and to consider lower priority ﬂows while selecting paths for the higher priority ones. Based on our experimentation results, we can improve schedulability by about 8% and 15% over Minimum Interference Routing and Widest Shortest Path algorithms, respectively. I I . R E LAT ED WORK Worst-case latency computation on inter-process communication in real-time systems is well-established for connectionbased packet networks [4, 5]. However, these methods are inefﬁcient due to the overhead of the establishment and tear down of channels between source and destination pairs which contributes to the communication latency, as well as under utilization of the system’s resources. Storing of packets at intermediate nodes requires expensive buffer capacity for storing early arriving packets and queueing packets in order of arrival [4]. 978-1-4673-0772-7/12/$31.00 ©2012 IEEE 499 6A-4 An alternative option is worm-hole switching, which increases throughput, and decreases the required buffer capacity in the network. The authors in [2, 3] present an efﬁcient method for WCET analysis of ﬂows (recall FLA) with wormhole switching in a PAR NoC. They view a ﬂow and its path as one indivisible entity. This ﬂow-level view of the communication in the PAR NoC leads to two issues: (1) Overestimation of the worst-case latencies when compared to what could be achieved using LLA, and (2) High cost of using FLA for path selection because the smallest unit at which we view a ﬂow is its whole path. Thus, requiring a path selection algorithm using FLA to enumerate all paths. Several algorithms exist for path selection in a network. Among these are Shortest Path, Widest Shortest Path, and Shortest Widest Path, which are greedy approaches. Another class of algorithms consider other ﬂows while selecting a path, but are more computationally expensive. Examples are Minimum Interference Routing [6], Light Minimum Interference Routing [7], and Proﬁle-based Routing [8]. Distributed routing algorithms also exist [9], which are online algorithms that either require a global state which leads to high communication overhead and performance degradation or do not share a global state and compensate with large number of control messages and subsequently do not scale. There is also research on path selection for worm-hole switched networks [10, 11, 12, 13]. Some of these methods attempt to ﬁnd contention free paths or minimize total cost, sometimes leading to higher ratios of unschedulability of ﬂows [13]. Others attempt to minimize the maximum contention value, which is similar to the techniques used in MIRA [6, 7]. However, all of these approaches consider the path of a ﬂow as an indivisible unit, thus cannot use actual latency costs that would require a computationally extensive enumeration of paths. Contrary to that, we construct a path by using link latencies as costs to ﬁnd the most suitable path for a ﬂow. I I I . BACKGROUND : N E TWORK MOD E L For an application with parallel tasks, we assume a given mapping of these tasks on the PAR NoC. We only consider PAR NoCs with mesh topologies. These tasks are marked as source and destination pairs based on the communication ﬂow between them. All nodes of the PAR NoC contain both a processing element that executes tasks, and a router. Recall that in the PAR NoC, the routers are priority-aware arbiters that implement worm-hole switching with ﬂit-level preemption, and ﬂow control. The router architecture we employ was originally proposed in [2, 3], but for clarity we brieﬂy describe its architecture. The router has a VC for every distinct communication ﬂow with a unique priority that passes through the router. Consequently, there exists a VC for each priority level. The VCs are designed as FIFO buffers at the input ports of the router. These FIFOs store the ﬂits to be routed. The router selects the output port for a ﬂit in the VCs based on its desired destination. When there are multiple ﬂits waiting to be routed, the router selects and forwards the ﬂit to the output port with the highest priority amongst all the waiting ﬂits. Flow control guarantees that the router only sends data to the neighbouring router if the neighbour has enough buffer space to store the data. If the highest priority ﬂit is blocked in the network, the next highest priority ﬂit can access the output link. Nodes are connected using bidirectional links with uniform bandwidth. Since there is a VC for every distinct communication ﬂow, this guarantees that deadlocks due to cyclic dependencies never occur. The reason is that each ﬂow has its own buffers and thus never blocks another ﬂow for buffer space. If we, however, extend our model to allow sharing of VCs by multiple ﬂows, we must guarantee that our deterministic path selection algorithm is still deadlock-free. We can achieve this by ensuring that as the algorithm proceeds, we have an acyclic channel dependency graph [14]. We model the network as a graph G = (cid:2)V , E (cid:3), and a set of periodic real-time communication ﬂows Γ = {τ1 , . . . , τk }. i (cid:3) where vs and vd are the source and destination nodes, Li is the basic link latency, Pi is the priority, Ti is the period, Di is the deadline, and J R is the release jitter. The basic link latency Li equals , which is the total packet size (number of ﬂits multiplied by the ﬂit size) divided by the link bandwidth. A ﬂow τi is a tuple (cid:2)vs , vd , Li , Pi , Ti , Di , J R f lit size∗num f lits i bandwidth IV. L INK -L EV E L ANA LY S I S + J R j + J I LLA computes the worst-case latencies by including the direct, and indirect interferences caused by ﬂows sharing links on a path. We present an overview of this link-level analysis in this section. Assume that the latency of a ﬂow τi suffering interference from a higher priority ﬂow τj on a particular link e in the NoC is given by Mie . The interval during which τj preempts τi is Mie j where J I j is the interference jitter of τj which represents any interference τj suffers from higher priority ﬂows. Hence, the number of times that τj preempts τi equals (cid:4) Mie +J R (cid:5). The total contribution of ﬂow τj in the latency of τi is then given by (cid:4) Mie +J R (cid:5) ∗ Lj . Therefore, the worst-case latency of τi is equal to Mie = (cid:5) ∗ Lj + Li . This form can be generalized to (cid:5) ∗ Lj + Li where SD ie is the set of higher priority ﬂows directly interfering with τi on e. (cid:4) Mie +J R j +J I Tj (cid:4) Mie +J R j +J I Tj j +J I Tj j j +J I Tj j ∀τj ∈SD ie j j (cid:2) Mie = τ2 v1 v2 v3 v4 τ0 τ1 Fig. 1: Illustrative example LLA provides tighter bounds than FLA as presented by Shi and Burns [2, 3]. We use the example from Figure 1 to describe the difference between both types of analyses, and further explain the details of LLA. Consider the data given in Table I for ﬂows τ0 , τ1 and τ2 that are ordered in decreasing priorities. The FLA in [2, 3] views the path of τ2 as one entity suffering simultaneous interference from both τ0 and τ1 . Basically, FLA computes the number of times higher priority ﬂows interrupt a packet of a particular ﬂow. It then multiplies the number of interrupts by the latency of the higher priority pack500 by M2 = (cid:4) M2 4 (cid:5) ∗ 2 + (cid:4) M2 ets to get the total interference latency, and adds the interference latency to the basic ﬂow latency to get the worst-case latency. The total worst-case latency for τ2 in this case is given 4 (cid:5) ∗ 2 + 5 (for illustration purposes of the FLA, we neglect the routing delay in this particular computation). Notice, that this equation has no solution which follows directly from the fact that each of the ﬂows τ0 and τ1 utilizes 2 time units with a period of 4 units thus needing 50% of the capacity and deeming τ2 unschedulable. TABLE I: Illustrative example data Flow L τ0 τ1 τ2 2 2 5 T 4 4 30 D J 4 4 30 0 0 0 4 ∗ 2(cid:5) + 5 = 11. τ2 LLA, on the other hand, analyzes each link separately. The latency on link (v1 , v2 ) equals M2 = (cid:4) M2 suffers no interference on link (v2 , v3 ) and, hence, in the worstcase, the packet from the previous link (v1 , v2 ) continues with the same latency of 11 time units. The interference that occurs on (v1 , v2 ) is the intermission of τ2 ﬂits by ﬂits of τ0 resulting in a sequence of ﬂits from both ﬂows. This interference discontinues on the next link but still leaves gaps between the ﬂits of τ2 and, hence, leading to the conclusion that the latency stays the same in the worst-case. On link (v3 , v4 ), τ1 interferes with τ2 , but in this case the time interval between the ﬁrst and last ﬂits of the τ2 packet is different from the ﬁrst case on link (v1 , v2 ) (due to the gaps caused by τ0 ). Therefore, to calculate the latency on link (v3 , v4 ), instead of using L2 in the latency equation, we use the latency from the previous link. The latency then becomes M2 = (cid:4) M2 4 ∗ 2(cid:5) + 11 = 23. The total latency using LLA is M2 = 23 + 3 = 26 (assuming a one time unit routing delay per hop). A conclusion that can directly be drawn from the comparison of LLA and FLA is that LLA provides tighter worst-case latency upper bounds. It can also easily be shown that in the worst-case of interferences (all interfering ﬂows share all links with the ﬂow of interest) the LLA provides an equivalent result to that of the FLA. Tighter latency upper bounds improve the schedulability of time-constrained ﬂows in a NoC as it has been shown in the previous example. V. PATH S E L EC T ION For a hard real-time system, the path selection problem is the following: ﬁnd the possible paths that ﬂows can take given their source and destination (S, D) pairs, and ﬂow requirements such that the ﬂows meet their respective deadlines. That is, given a graph G = (cid:2)V , E (cid:3), and a set of ﬂows Γ = {τ1 , . . . , τk }, is it possible to select a path δi for each ﬂow τi such that its worst-case latency is less than or equal to its deadline Di ? A. Optimal Path Selection Algorithm Objective. Satisfy the deadline requirements of the ﬂows by searching all possible paths from source to the destination of each ﬂow. 6A-4 Assuming that a path visits a node only once, each ﬂow will have 4 ∗ 3v−1 possible paths (assuming a NoC with v nodes) in a mesh topology because from each node descends three possible nodes to traverse (four for the source node). An optimal algorithm selects a path for the ﬁrst ﬂow, then selects one for the second and so on. If at any point the deadline constraints are not met, the algorithm backtracks one step and selects an alternative path. Hence, the decision tree has k levels corresponding to the number of ﬂows, and from each node descends 4 ∗ 3v−1 choices that correspond to all possible paths. The total complexity of the algorithm, therefore, becomes O((3v )k ). B. Heuristic-based Path Selection Algorithm The optimal path selection algorithm has a high exponential complexity which makes it impractical. Therefore, we present a heuristic-based path selection algorithm (PSA) that uses the link-level analysis (LLA) to guide the path selection. We pointed out in Section IV that the worst-case latency of a ﬂow on a link depends on its latency and the interfering ﬂows on the preceding link. Since we are concerned with path selection, we require a heuristic so that the algorithm does not have to backtrack, and in the worst-case enumerate all paths. We also need a heuristic that while the algorithm routes higher priority ﬂows, considers lower priority ones. Objectives Our overall goal with path selection is to select paths for ﬂows that improve the schedulability of the ﬂows while incorporating the following: Objective 1. Account for lower priority ﬂows. While selecting paths for higher priority ﬂows, expected paths for lower priority ones are taken into account to maximize schedulability. Otherwise, the PSA might overload some links making lower priority ﬂows unschedulable. By accounting for lower priority ﬂows, we prevent the lower priority ﬂows from being starved. Objective 2. Consider the availability of shortest paths. The criticality of links for lower priority ﬂows depends on their utilization as part of all available shortest paths. The intuition behind the criticality is that the more the number of available shortest paths, the more likely it is for a lower priority ﬂow to be schedulable. Similarly, the lower the number of available shortest paths, the less likely it is for the lower priority ﬂow to be schedulable. This concept is similar to that of critical links in [6, 7]. Objective 3. Minimize the heterogeneity of interfering ﬂows. We promote sharing of links between ﬂows that share prior links. Consider a ﬂow τ5 that has a path with 3 links. Using the data in Table I, if τ5 has interference with τ0 on the 1st link, with τ1 on the 2nd , and none on the 3rd , the latency on the 1st link equals R5 = (cid:4) R5 8 (cid:5) ∗ 2 + 9 = 13, on the 2nd equals 8 (cid:5) ∗ 2 + 13 = 19, and on the 3rd equals 19. The total worst-case latency is 22. However, if τ5 has interference with τ0 on all 3 links, then the latency on each link equals 13, and the total worst-case latency is 16. Hence, we identify from LLA R5 = (cid:4) R5 501 6A-4 that a ﬂow τi sharing multiple links with a ﬂow τj results in a lower worst-case latency than sharing fewer links with different ﬂows. Therefore, PSA favours sharing links with the same ﬂow over a variety of ﬂows. Algorithm We use the interference that a ﬂow suffers on a link as the cost of that link. We construct the network graph G to capture the topology, and as the algorithm proceeds, it adds edges that represent sharing more than one successive link with the same ﬂow, but do not actually exist as links in the NoC. These edges hold the cost of interference over multiple links, and the intermediate nodes that represent actual NoC nodes. So for example, if the algorithm selects the path [v1 , v2 , v3 ] for ﬂow τ1 , then when selecting the path for τ2 , the algorithm will set the interference for links (v1 , v2 ) and (v2 , v3 ) and creates a new edge (v1 , v3 ) that has a cost of interference with τ1 on both links and saves v2 as an intermediate node. Although, this is not an optimal solution for representing all possible cases of multiple link interferences, this heuristic allows us to achieve objective 3 efﬁciently and maximize schedulability. Since we do not enumerate all possible paths, the order of selecting paths to ﬂows affects the latencies and the overall system schedulability. Accommodating multiple ﬂows in the network is known as the multi-commodity ﬂows problem, which is an NP-Complete problem [15, 8]. Once again, this makes the path selection algorithm intractable. We perform the path selection process according to the priority of the ﬂows: higher priorities ﬁrst. However, we still accommodate lower priority ﬂows while selecting a path for a higher priority ﬂow using three different heuristics. H 1. Identify critical links as ones with least residual capacity. Residual capacity is the difference between the capacity of a link and the ﬂow being transmitted over it. H 2. Identify critical links as the links constituting the paths of lower priority ﬂows with only a single shortest path. H 3. Assign costs to all links in all shortest paths of each lower priority ﬂow. This cost depends on the number of available shortest paths to a ﬂow, and how critical the links are depending on how many shortest paths use the same link. The input to the Algorithm 1 is a PAR NoC with a mesh topology of size n × n represented as a graph G = (cid:2)V , E (cid:3), and Γ with k ﬂows ordered according to their priorities with 1 being the highest. The output is a set of paths for each of the ﬂows in Γ. When selecting a path for a ﬂow, the algorithm updates the cost of that path in the graph G. The cost on each edge accounts for both higher and lower priority ﬂows (using one of the three heuristics). To account for lower priority ﬂows using H3, the algorithm ﬁnds the number of shortest paths available, and the number of times each edge is used amongst all the shortest paths for every lower priority ﬂow. The simplest method to obtain this information ﬁnds all the shortest paths for a ﬂow, which for a mesh topology has a complexity of 22∗(n−1) , i.e. 2n . However, notice that that the information we require depends only on the relative x and y positions of the source and destination nodes: Input: G(cid:2)V , E (cid:3), Γ = {τi : ∀i ∈ [1, k ]} Algorithm 1 PATH-SELECTION Output: {δi : ∀ i ∈ [1, k ]} Let SP C [1, n − 1][1, n − 1] be an empty array Let SP E [1, n − 1][1, n − 1] be an empty array SP C [i, j ] ← M AX ∀ (i, j ) ∈ M LOWER-PRIORITY(SP C, SP E , n − 1, n − 1) (cid:2) (cid:2)V (cid:2) (cid:2) (cid:3) s.t. V (cid:2) = V and E for all τi ∈ Γ do Let G (cid:2) = E for τj ∈ [τi+1 , . . . , τk ] do , E Δx ← |(vsk mod n) − (vdk mod n)| Δy ← |vsk /n − vdk /n| for all e ∈ SP E [Δx, Δy ] do w ← Lk , e) ← w(G w(G , e) + w updateI ntermediate(G , e) × count(SP E [Δx,Δy ],e) SP C [Δx,Δy ] Dk −Ck (cid:2) (cid:2) (cid:2) end for end for δi ← Dij kstra(G δi ← expandI ntermediate(G , vsi , vdi ) (cid:2) (cid:2) , δi ) INTERFERENCE-COSTS(G, δi ) end for return {δi : ∀ i ∈ [1, k ]} Δx and Δy . Hence, we use memoization, which is a form of dynamic programming that reduces the complexity to n. The algorithm makes a single call to Function 3 that calculates the number of shortest paths, and the count of each edge on these paths for all possible combinations of Δx and Δy . Array SP C stores the number of shortest paths available for a given Δy and Δx, and array SP E stores, for every Δy and Δx, the number of times each edge appears on these shortest paths. SP C has n2 entries while SP E has n2 ∗2n∗(n−1) entries. The function recursively uses the information from nodes with lower values of Δy and Δx. Each selected path will add interferences to the graph according to Function 2. The function call calculates the interference on edgeI nterf erence(G, e) edge e, and function nodeI nterf erence(G, [vi , ..., vj ]) computes interference on a sequence of adjacent nodes [vi , . . . , vj ] forming a path. When the algorithm selects a path for a ﬂow, it adds edges between each node on the path and all of its successive nodes. intermediate holds the intermediate nodes in newly created edges. If the edge is an actual link, then the algorithm will add the interference on that link to the weight of the edge. However, if it is not a link, then the algorithm will set the weight of the edge to the interference on the path formed by the intermediate nodes of that edge in one of three cases: (1) the edge does not exist, or (2) the edge exists and has the same intermediate nodes as the one the algorithm is adding, or (3) the interference on the edge being added is less than the existing one. Algorithm 1 adds costs to the edges based on a speculation of the paths that will be selected for lower priority ﬂows as described above. The function count(SP E [i, j ], e) retrieves the count of an edge e for a speciﬁc Δy and Δx. The function expandI ntermediate(G, δ) replaces edges in a path that do not belong to the actual topology with the equivalent interme502 diate nodes. The function updateI ntermediate(G, e) updates the costs of all edges that do not belong to the topology if they have the edge e as an intermediate edge. The algorithm calculates Δy and Δx for each lower priority ﬂow and adds a cost to the links involved. A weight is used to represent the criticality of the edge which is equal to the edge count in SP E divided by the number of shortest paths. This weight is multiplied by Lk the basic latency of the lower priority ﬂow over the slack that it has to its deadline on the speculated path where Dk is the deadline and Ck is the best case latency. Dijkstra’s algorithm is used to ﬁnd the least cost path for the ﬂow being routed. Function 2 INTERFERENCE-COSTS for all vi ∈ δ do Input: G, δ = [vs , . . . , vd ] for vj ∈ [vi , . . . , vd ] do e ← (vi , vj ) w(G, e) ← edgeI nterf erence(G, e) else if w(G, e) = 0 ∨ nodeI nterf erence(G, [vi , . . . , vj ]) < w(G, e) ∨ intermediate(G, e) = [vi , . . . , vj ] then w(G, e) ← nodeI nterf erence(G, [vi , . . . , vj ]) intermediate(G, e) = [vi , . . . , vj ] if j − i = 1 then end if end for end for Function 3 LOWER-PRIORITY Input: SP C , SP E , i, j if SP C [i, j ] < M AX then return SP C [i, j ] end if else if j = 0 then else if i = 0 then if i = 0 & j = 0 then SP C [i, j ] ← 1 SP E [i, j ] ← [ ] SP C [i, j ] ← LOWER-PRIORITY(SP C, SP E , i, j − 1) SP E [i, j ] ← SP E [i, j − 1] + edge(i, j, i, j − 1) SP C [i, j ] ←LOWER-PRIORITY(SP C, SP E , i − 1, j ) SP E [i, j ] ← SP E [i − 1, j ] + edge(i, j, i − 1, j ) SP C [i, j ] ←LOWER-PRIORITY(SP C, SP E , i, j − 1)+ SP E [i, j ] ← SP E [i, j − 1] + SP E [i − 1, j ] + edge(i, j, i, j − 1) + edge(i, j, i − 1, j ) LOWER-PRIORITY(SP C, SP E , i − 1, j ) else end if . . . + 1) = k ∗ (cid:2)v−1 Complexity Analysis Recall that we have k ﬂows in a graph with v vertices. Function 3 has a complexity v and is called only once. At worst, that the algorithm will create is given by: k ∗ ((v − 2)+ (v − 3)+ each ﬂow will have a path with v nodes. The number of edges i=2 (v − i). Thus, in the worst case, the algorithm will create k ∗ v2 edges. The functions Dij kstra and expandI ntermediate have linear complexity v . The function updateI ntermediate uses a structure that saves, for each edge, newly created edges which it is a part of as an intermediate edge. The maximum number of edges that can be involved 6A-4 in all shortest paths between 2 nodes is 2(v − √ complexity is therefore given by: k2 ∗ 2(v − √ v) ∗ k ∗ v2 which v). The overall is O(k3 ∗ v3 ). Although, this is a high complexity compared to minimum interference algorithms for example, PSA never elicits that upper bound computation time which assumes that each edge is part of all newly created edges. It would be beneﬁcial to compute the expected case complexity, but due to space constraints we only present the experimental computation times of the algorithm. V I . EX P ER IM EN TAT ION We quantitatively compare PSA with the three different heuristics against the widest shortest path (WSP), and the minimum interference routing algorithm (MIRA) [6, 7]. We vary several parameters during experimentation to assess its effect different algorithms. Our experiments uses 4 × 4, and 8 × 8 on the schedulability of ﬂows, and the execution time of the mesh topologies for the NoC. The basic link latency of a ﬂow is randomly chosen from a uniform distribution in the range [16, 1024]. The link utilization of a ﬂow τi is its basic link latency divided by its period: Ui = Li /Ti . We vary the link utilization between [0.4, 0.85] in step increments of 0.05. The deadline Di takes values between [0.7, 1.0] in increments of 0.1 as a ratio of period Ti . The number of ﬂows in the network range between [10, 100] in steps of 10. With these parameters, we have 800 different conﬁgurations. For each conﬁguration, we generate 1000 test cases with a random mapping of the source and destination nodes of the ﬂows for each test. All tests were run on an AMD Opteron 6174 2.2 GHz processor with 8.0 GB of memory. Figure 2a shows the ratio of the number of unschedulable ﬂows for MIRA and the three proposed heuristics to WSP against the number of ﬂows for an 8×8 mesh with U = 0.4. For a given number of ﬂows, each box in the ﬁgure represents 1000 random test cases. The boxes represent the lower quartile, median and upper quartile of the data, and the whiskers show the minimum and maximum observations. It is clear that H2 and H3 always perform better than WSP and MIRA except for the maximum observations. H1 has a comparable performance to MIRA for 10 ﬂows and performs better as the number of ﬂows increase. H1 starts performing better than H2 and H3 as the number of ﬂows increases beyond 40 ﬂows. For lower number of ﬂows, using shortest paths heuristics (H2, H3) can help leave these paths open for lower priority ﬂows thus increasing schedulaiblity over H1. However, as number of ﬂows increase, these paths are occupied and avoiding links with low residual capacity (H1) becomes more important and improves schedulability. The graph also shows that for a small number of cases, WSP has less unschedulable ﬂows compared to the other algorithms. The reason is that, in these cases, increasing the cost of critical links leads to selecting non-shortest paths. This leads in some cases to unschedulable ﬂows due to high interference on the chosen paths. Figure 2b shows the average execution times of the algorithms against the number of ﬂows. Each point represents an average of 80, 000 test cases with 1000 for each of the 80 different conﬁgurations. WSP takes the least amount of execution time, and H2 closely follows MIRA. H1 and H3, however, have higher execution times. The reason is that H2 only adds costs 503 6A-4 s w l o F f o r e b m u N Ratio of MIRA to WSP Ratio of H1 to WSP Ratio of H2 to WSP Ratio of H3 to WSP 0 7 0 6 0 5 0 4 0 3 0 2 0 1 WSP MIRA H1 H2 H3 ] c e S m [ e m i T n o i t a t u p m o C 0 0 8 0 0 6 0 0 4 0 0 2 0 0.0 0.2 0.4 0.6 0.8 1.0 1.2 10 20 30 40 50 60 70 80 90 100 Ratio of Unschedulable Flows (a) Ratio of unschedulable ﬂows compared to WSP against the number of ﬂows for an 8 × 8 mesh with U = 0.4 and D = 1.0 Number of Flows (b) Average computation times for the different algorithms against the number of ﬂows Fig. 2: Experimental results to lower priority ﬂows with one shortest path, thus doing less computations than H1 and H3. H3 has the highest computation time because it updates the cost of all edges in all shortest paths of lower priority ﬂows. In summary, for all 800, 000 tests, we observe an average improvement in schedulability of 12.3%, 14.5% and 15.1% over WSP for H1, H2 and H3, respectively. The average improvement over MIRA is 3.5%, 7.2% and 8.0% for H1, H2 and H3, respectively. The computation times of the proposed algorithms are acceptable and close to that of MIRA. V I I . CONC LU S ION This paper presents a new path selection algorithm for routing real-time ﬂows in a priority-based interconnect based on a link-level analysis view of the NoC. Our algorithm introduces an edge creation heuristic that accommodates the dependency of latencies on traversed links, enables PSA to minimize the variation of interfering ﬂows, and consequently reduces the worst-case latency. We use three heuristics to account for expected paths of low priority ﬂows. H1 performs worse than MIRA at lower number of ﬂows and has an overall improvement in schedulability of 3.5%. H2 improves over MIRA in schedulability by 7.2%. H3 improves in schedulability over MIRA by 8.0%. The three heuristics have a schedulability improvement over WSP by 12.3%, 14.5% and 15.1%, respectively. The computation times of the three heuristics are comparable to MIRA’s and much less than that of the optimal algorithm. Our plan is to investigate additional gains in path selection by introducing a joint mapping and path selection technique. ACKNOW L EDG EM EN T This research was supported in part by NSERC DG 3571212008, NSERC DG 386714-2010, ORF RE03-045, ORE RE04036, ORF-RE04-039, ISOP IS09-06-037, APCPJ 386797-09, and CFI 20314 with CMC. "
2012,Proximity-Aware cache Replication.,"We propose Proximity-Aware cache Replication (PAR), an LLC replication technique that elegantly integrates an intelligent cache replication placement mechanism and a hierarchical directory-based coherence protocol into one cost-effective and scalable design. PAR dynamically allocates replicas of either shared or private data to a few predefined and fixed locations that are calculated at chip design time. Therefore, PAR fits well to future many-core CMPs thanks to its scalable on-chip storage and coherence design. Simulation results on a 64-core CMP show that PAR can achieve 12% speedup over the baseline shared cache design with SPLASH2 and PARSEC workloads. It also provides around 5% speedup over a couple contemporary approaches with much simpler and scalable support. Translating this speedup to cache performance, PAR achieves 40% and 70% reduction over the baseline in average L1 miss latency and on-chip network traffic, respectively. Furthermore, PAR shows good speedup with multiprogrammed workloads.","6A-1 Proximity-Aware Cache Replication Chongmin Li Dongsheng Wang Haixia Wang Yibo Xue Jian Li Tsinghua National Laboratory for Information Science and Technology, Beijing, China Department of Computer Science & Technology, Tsinghua University, Beijing, China e-mail: lcm03@mails.thu.edu.cn, wds@thu.edu.cn, hx-wang, yiboxue@mail.thu.edu.cn IBM Research - Austin Austin, TX, USA e-mail: jianli@us.ibm.com Abstract— We propose Proximity-Aware cache Replication (PAR), an LLC replication technique that elegantly integrates an intelligent cache replication placement mechanism and a hierarchical directory-based coherence protocol into one cost-effective and scalable design. PAR dynamically allocates replicas of either shared or private data to a few predeﬁned and ﬁxed locations that are calculated at chip design time. Therefore, PAR ﬁts well to future many-core CMPs thanks to its scalable on-chip storage and coherence design. Simulation results on a 64-core CMP show that PAR can achieve 12% speedup over the baseline shared cache design with SPLASH2 and PARSEC workloads. It also provides around 5% speedup over a couple contemporary approaches with much simpler and scalable support. Translating this speedup to cache performance, PAR achieves 40% and 70% reduction over the baseline in average L1 miss latency and on-chip network trafﬁc, respectively. Furthermore, PAR shows good speedup with multiprogrammed workloads. I . IN TRODUC T ION Cache replication and migration are effective to reduce onchip communication latency [1, 2, 3, 4] in Non-Uniform Cache Architecture (NUCA) [5], because signiﬁcant data locality exists in many workloads. In this work, we focus on cache replication and its associated data migration. By keeping a data copy in a nearby LLC bank, cache replication can achieve low communication latency. Two types of replication approaches have been proposed: static [4, 6] and dynamic [2, 7]. In static replication, a data block is placed through predeﬁned address interleaving; therefore, the LLC banks that may contain that data block is ﬁxed. The data placement of instruction pages in R-NUCA [4] and in S-NUCA [6] are static. In dynamic replication, a data block can be placed in any LLC banks. Victim Replication [7] and Adaptive Selective Replication [2] fall into this category. We propose a Proximity-Aware cache Replication (PAR) mechanism. PAR speciﬁes the potential cache line replica locations at chip design time, while it dynamically assigns the replication of a cache line to a replica location at run time. PAR is an LLC replication technique that elegantly integrates an intelligent cache replication placement mechanism and a hierarchical directory-based coherence protocol [8, 9, 10] into one cost-effective and scalable design. The paper makes the following contributions: • We have developed a method to calculate the ideal average onchip communication latency, the results of which demonstrate signiﬁcant potential to reduce on-chip communication latency through proximity-aware replication. • Based on the insights from our analysis, we propose a scalable proximity-aware replication design for shared last-level cache (LLC) to improve on-chip communication latency via replication. • Through execution-driven full-system simulation, we compare proximity-aware replication with a few state-of-the-art static and dynamic replication approaches and show that PAR achieves better performance with signiﬁcant lower hardware and software overhead. I I . MOT IVAT ION 0 4 8 H 5 R 12 13 2 6 10 14 R 7 R 15 (a) Processor L1 I&D L2 cache 0 4 R H 5 9 12 13 2 6 10 R 3 R 11 15 (b) Fig. 1. A tiled CMP with four replication nodes (Replicas, including Home nodes) in (a) node 1, 3, 9 and 11, and (b)node 1, 7, 8 and 14. A tiled CMP design has the advantage of modularity for low complexity and high scalability for future many-core CMPs. As shown in Fig. 1, we consider a tiled CMP where all the tiles are connected by a 2D mesh on-chip network. Each tile has a processor core, private L1 instruction and data caches, a uniﬁed and shared L2 cache slice, and a router connecting to the onchip network. In such a tiled CMP, a remote L2 access travels through on-chip network and may experience varied latencies depending on the distance between the source and destination. As the on-chip communication latency relates to the distance between the source and destination pairs, we use it as the metric of on-chip communication latency in the following analytical analysis. Cache replication techniques [11, 6, 4] arrange replicas1 of a data block by certain predeﬁned rules. Replicas are regularly positioned with conventional address interleaving [11, 6], as illustrated in Fig. 1(a). The average distance for an arbitrary processor core to get a data copy from a closest replica is then (1+0+2+1)/4 = 1 hop. However, the resulting placement is not optimal. For example, for the same data block whose home node is at node 1, one can place replicas as shown in Fig. 1(b). 1We include the home node in the counting of a group of replicas or replicated nodes for simplicity. 978-1-4673-0772-7/12/$31.00 ©2012 IEEE 481 6A-1 AV ERAG E D I S TANC E O F ADDR E S S IN T ER L EAV ING W I TH D I FF ER EN T R E P L ICA PLAC EM EN T S 32 64 128 256 512 1024 TABLE I Number of replicas 16-core 64-core 256-core 1024-core Address interleaving PAR Ideally Address interleaving PAR Ideally Address interleaving PAR Ideally Address interleaving PAR Ideally 1 2.50 2.50 2.50 5.25 5.25 5.25 10.63 10.63 10.63 21.31 21.31 21.31 2 2.00 1.40 1.25 3.88 3.07 2.63 7.94 6.27 5.31 15.97 12.59 10.66 4 1.00 0.81 0.75 2.50 1.95 1.81 5.25 4.08 3.72 10.63 8.24 7.52 8 0.50 0.50 0.50 2.00 1.29 1.25 3.88 2.80 2.63 7.94 6.81 5.31 16 0.00 0.00 0.00 1.00 0.77 0.75 2.50 2.02 1.81 5.25 4.14 3.72 0.50 0.50 0.50 2.00 1.32 1.25 3.88 2.79 2.63 0.00 0.00 0.00 1.00 0.76 0.75 2.50 1.95 1.81 0.50 0.50 0.50 2.00 1.30 1.25 0.00 0.00 0.00 1.00 0.75 0.75 0.50 0.50 0.50 0.00 0.00 0.00 The average distance for an arbitrary processor core to get a data copy from its closest replica is reduced to (1+0+1+1)/4 = 0.75 hop. This indicates a potential of up to 25% gain in cache access latency, this observation motivates us to ﬁnd an intelligent replica placement for improved cache access latency. (0,0) (0,0) 3 3 2 3 3 2 1 2 3 3 2 1 0 1 2 3 3 2 1 2 3 3 2 3 3 distance 0 1 2 3 i count 1 4 8 12 4*i Fig. 2. Distance to nodes surrounding a replica. Fig. 2 illustrates the distance from a replica node to its nearby nodes. These nodes form an oblique square. Ideally, all the nodes can always access a data block with the shortest available distance from its neighbors, if one ignores the boundary conditions of a placement scheme. Consider a 64-core CMP with four replicas per data block. Four nodes can access a data block with 0 hop, 16 and 32 nodes, at most, can access the same data block with 1 and 2 hops, respectively. In the best case, (64-4-16-32) = 12 nodes can access the same data block with 3 hops. Therefore, the lower bound of the total distance is, (4x0 + 16x1 + 32x2 + 12x3) = 116. The lowest, hence ideal, average distance is then 116/64 = 1.8125. Tab. I shows the average access distance of three placement schemes for a set of tiled CMPs from 16-core to 1024core. The three placement schemes are static address interleaving with conventional replication placement, our proposed Proximity-Aware cache Replication (PAR) which we will discuss in detail in Section III and the ideal average distance serving as the lower bound respectively. We observe a large swing in average distances when the number of replicas changes for a certain number of processors. In all cases, our proposed PAR placement falls in between and, in many cases, has the same or similar average distance compared to the ideal case. I I I . PROX IM I TY-AWAR E R E P L ICAT ION Recall in Fig. 2 we observe an oblique diamond shape of four nearby replicas. Due to varied numbers of cores in CMPs and their ﬂoorplans, such a oblique diamond shape can be (15,15) (15,15) (a) Traditional address interleaving for (0,0) (b) PAR address interleaving for (0,0) (0,0) (0,0) (15,15) (15,15) (c) Traditional address interleaving for (1,2) (d) PAR address interleaving for (1,2) Fig. 3. Address interleaving for a 256-core CMP with 16-replica-on-chip (home node is (0,0)) shifted and disjointed, as shown in Fig. 1(b). Therefore, if the four nearby replicated nodes are positioned in a diamond shape, whether shifted or disjointed on the sides or not, it can result in short average distance. As a result, we apply this insight to allocate the replicas. We call it PAR Address Interleaving, or PAR for short. As a case study of PAR, Fig. 3(a) and (b) show a 16× 16 tiled CMP with 16 replicas in conventional address interleaving and PAR address interleaving for home node at (0,0) respectively. Similarly, Fig. 3(c) and (d) illustrate the conventional address interleaving and it PAR counterpart, respectively, for home node at location (1,2). In all of these cases, the replicas (both home nodes and their copies) and their consumer nodes are color mapped in clusters, to show the difference between conventional mapping and our proposed PAR mapping. To describe the mapping function of such a placement method, we ﬁrst deﬁne the ID of a node as a bit vector. For example, the ID of a node is an 8-bit vector in a 256-core CMP, indexed from (0,0) to (15,15) on a 2D mesh. We further deﬁne 482 a bit vector, Bselect , which can also be used in a conventional cache replication mechanism, to determine the position of a cluster of nodes that share the same replica node. For example, in a 256-core CMP with up-to 16 replica locations of a data block, the Bselect is composed of four bits, (xx xx ), out of an 8-bit vector. The x positions belong to Bselect . The ﬁrst two x’s (on the left) are the two most signiﬁcant bits of the upper four bits of the 8-bit vector that represent the row index, while the other two x’s (on the right) are the two most signiﬁcant bits of the lower four bits that represent the column index. When Bselect = n, it means xxxx = n. The other four bits, denoted as “ ”, are zeros by default. We deﬁne Bhome as the Bselect of the shared cluster of nodes that contains a home node. The mapping function for conventional address interleaving of Fig. 3(a) is: IDreplicas = (IDhome & Mcluster )(cid:2)Bselect (1) where Mcluster is the bit mask to get the offset inside a cluster (0x33 in this case). Bselect is determined by the number of replicas and ranges from 0 to 15 in this case. In contrast, The mapping function for the PAR address interleaving of Fig. 3(b) is IDreplicas P AR = ⎧⎪⎪⎪⎨ ⎪⎪⎪⎩ IDreplicas if (Bselect & 1) == (Bhome & 1) IDreplicas ⊕ Mof f set if (Bselect & 1) != (Bhome & 1) (2) where the xor operation moves the speciﬁed replica up or down a ﬁxed number of rows inside a cluster of nodes. The shifted value is determined by Mof f set (0x20 in this case to move a node by two rows). The invariant 1 is used distinguish if a column index is an even or odd number. IV. D E S IGN A.LogicalCacheOrganization We design proximity-aware replication based on directory protocol, nonetheless, this mechanism can be applied to other protocols as well. The logical cache organization is given in Fig. 4, the home node (node 1) needs to record the sharing information of both its L1 subnodes (node 0, 1 (itself), 2 and 5) and the replicated locations in other L2 slices (node 7, 8 and 14). On the other hand, a replicated node just needs to record the sharing information of its subnodes. Therefore, the directory of each cache line contains two parts: one part for L1 subnodes and the other for replicated nodes. We specify that: (1) for a CMP with n cores and m replicated nodes (including the home node), each node must be chosen as a replicated node exactly m − 1 times; (2) If home node A has a replicated node B, A must also be the replicated node when B is the home node. This way, the L1 subnodes correlated to an L2 cache is always deterministic hence easier to manage. Therefore, a home node needs m − 1 bits to record its replicated node, and the bits for L1 subnodes depends on how many L1 subnodes it has. As a result, the directory bit map of a cache line is on average (cid:4) m − 1 + n/m (cid:5) bits long. 6A-1 1 Home  7 8 14 L2 cache 0 21 5 3 76 11 4 98 12 10 13 14 15 L1 cache Fig. 4. Logical cache organization of PAR. In this example, the correlation between caches relates to the one in Fig. 1(b) Proximity config table (node-1) Address ……..0100 3 R 11 offset Home selection Directory 0 1 2 3 4 5 15 p0 p1 p2 p5 p2 p5.... p0 15 0    1   0 L2 subnode 1    0    0    1 L1  subnode 0 4 R H 5 9 12 13 2 6 10 R Nearest node p2 Subnode config table (node-5) 0 1 2 0 1 2 3 p7 p8 p14 p0 p1 P2 P5 L2 subnode L1 subnode Fig. 5. Two hardware tables in a tile, namely, (1) proximity conﬁguration table to locate an L1 cache’s nearest supplier L2 slice and (2) subnode conﬁguration table to locate the subnodes of an L2 replica, which may itself be a home node. B.HardwareStructure We need to address two design challenges. The ﬁrst is how to locate the nearest L2 cache supplier node, either a home or a replica. The second is how to determine the position of the L1 subnodes, including L2 replicas nodes and the L1 cache nodes, for an L2 data block. We add two tables, a proximity conﬁguration table and a subnode conﬁguration table, into each tile to solve these two problems. The number of entries of the proximity conﬁguration table is the same as the number of L2 banks. The table is hardwired to the cache address decoding logic to handle the mapping to the nearest node. No extra lookup delay incurs. The proximity conﬁguration table is indexed by the corresponding bits which determine the home node, as shown in Fig. 5. A value of the proximity conﬁguration table is the nearest L2 cache node ID among the home and the replica nodes. On the other hand, the subnode conﬁguration table records the positions of all subnodes. It is hardwired to the cache directory to handle the mapping to the L1 and L2 subnodes. No extra lookup delay incurs. It can be divided into two concatenated bit maps: (1) the ﬁrst bit map is for the replica nodes that is only meaningful for a home node rather than its replica, e.g. the 3-bit bit map of L2 subnode in Fig. 5; (2) the other bit map is used to record the position of L1 subnodes, e.g., the 4-bit bit map in Fig. 5 to cover the most skewed placement in Fig. 1(b). In practice, one can reserve a handful more bit, three is sufﬁcient in large-scale CMPs up to 1024 cores, in each of the two bit maps in the directory for skewed placements as shown in Fig. 3 while sustaining the same performance. Note that, the values of each entry in both the proximity conﬁguration table and the subnode conﬁguration table for each tile can be speciﬁed during the chip design time. They can be modiﬁed by the ﬁrmware at chip boot time. 483 6A-1 TABLE II PAR D IR EC TORY STORAG E U SAG E O F EACH 1MB L2 S L IC E IN CMP S W I TH VAR I ED NUMB ER O F COR E S AND RE P L ICA S , NORMA L I Z ED TO SHAR ED L2 ADDR E S S IN T ER L EAV ING . Nreplica 1 2 4 8 16 32KB ≤ 24B 16-core 100% 64% 44% 64% 100% 128KB ≤ 144B 64-core 100% 52% 30% 23% 30% 52% 100% 512KB ≤768B 256-core 100% 50% 26% 15% 12% 15% 26% 50% 100% 2MB ≤3840B 1024-core 100% 50% 25% 13% 8% 6% 8% 13% 25% 50% 100% * Nreplica : Number of replicas; Sdir : Storage usage of directory; Stable : Storage usage of two new-added tables in Fig 5 1024 Sdir Stable 256 512 32 64 128 In PAR, the directory of each cache entry records only the sharing information of replica nodes (for home node) and the L1 caches associated to the L2 node (for both home node and its replica nodes). Therefore, the size of a directory entry is smaller in PAR than the shared conﬁguration. Apparently, the savings in directories increase for CMPs of more cores. Tab. II summarizes the estimated directory usage for different conﬁgurations. It shows that PAR has good scalability particularly for large-scale CMPs, where the directory storage in conventional schemes can be more than the cache size itself. C.StorageUsage We now comment on the storage usage of PAR and compare it with two representative prior proposals, ASR [2] and R-NUCA [4]. ASR is originally designed with token protocol [12]. In our context of a directory-based coherence protocol, the directory of an L2 cache entry must have the ability to record the shared information of all possible sharers. This means the directory usage of ASR is the same as the shared organization (100% in Tab. II). ASR also needs to keep the Next Level Hit Buffers and Victim Tag Buffers of each L2 slice to dynamically determine the replication level, which introduces additional storage usage. In general, it is difﬁcult for dynamic cache replication/migration techniques to reduce the directory usage. Dynamic mechanisms also need hardware support for complicated cache lookup (i.e., tag comparison). The directory usage of R-NUCA is also the same as the shared conﬁguration (100% in Tab. II). This is because R-NUCA has to keep the information of the L1 caches for the data blocks located in a shared page. D.CoherenceProtocol The coherence protocol of PAR is a straightforward variation from prior art [13, 14]. The main difference lies in the parent node selection for each L1 cache line. A parent node can be either a home node or one of its replica, e.g., node 1, 7, 8 or 14 in Fig. 4. In PAR, an L1 cache chooses different parent nodes for different addresses according to the aforementioned Proximity-Aware Address Interleaving scheme. The coherence protocol makes sure the replicated data are coherence and PAR do not handle data migration. With this coherence support to replication, when requests hit at a nearby replica, performance is improved over the shared LLC design thanks to the shorter message communication distance. V. EX P ER IM EN TAL S E TU P TABLE III LLC R E P L ICAT ION SCH EM E S Description Shared organization without replicated nodes Shared organization with up to n regularly replicated nodes Adaptive selective replication [2] Reactive NUCA [4] PAR cache with up to n replicated nodes Scheme BASE Cluster-n ASR R-NUCA PAR-n PAR and a shared LLC address interleaving scheme with conventional replication placement, namely Cluster. We denote our implementation of ASR and R-NUCA with a preﬁx “m” to indicate that mASR and mR-NUCA are our interpretation of the original ASR and R-NUCA. TABLE IV S IMU LAT ED SY S T EM Component CMP size CMP core CMP line size L1 I/D-Cache Sizes, Associativity L1 Load-to-Use Latency L1 Replacement Policy L2 cache Size, Associativity (per tile) L2 Load-to-Use Latency L2 Replacement Policy Network Conﬁguration One-hop Latency Memory Latency, bandwidth Parameter 64-core (SPARC ISA) in-order / 2.0 GHz 64B 8KB, 2-way 2 cycle Pseudo-LRU 128KB, 16-way each tile 15 cycles Pseudo-LRU 8 × 8 mesh, 256b links 3 cycles 300 cycles, 25.6GB/s TABLE V WORK LOAD S Multiprogrammed workload Workload fft cholesky(cho.) lu radix(rad.) raytrace(ray.) fmm barnes(bar.) ocean(oce.) ferret(fer.) ﬂuidanimate(ﬂu.) vips Problem Size 256K points tk15.o 1024*1024 matrix 1024 indexes teapot.env 16K particles 16K particles 514 x 514 ocean Simsmall Simsmall Simsmall fft & lu fft & radix fft & barnes fft & ocean radix & barnes lu & ocean The parameters of conﬁgurations are given in Tab. IV. All designs use write-back, write-allocate caches. L2 and L1 are inclusive. Note that we have to simulate smaller input sizes to make simulation turn-around time manageable (less than two weeks for most workloads) for a targeted future 64-core CMP design. As a result, we conﬁgure relatively smaller but still balanced cache sizes (16KB L1I+D and 128KB L2 slice2 ) to accommodate these smaller input sets that we simulate. That We modify the GEMS [15] tool sets to build our full-system simulator. We evaluate the performance of ASR, R-NUCA, 2 The cache size ratio between L1 and L2 is along the lines of many academic research conﬁgurations [2, 4] as well as some commercial processors, such as Intel Core 2 Duo E8500 R(cid:3) and Quad Q8400 R(cid:3) . 484 way, we can reasonably investigate the capacity overhead of replication and its impact on performance. We evaluate our design using eight workloads from SPLASH-2 [16] and three workloads from PARSEC [17], as shown in Tab. V. V I . EVALUAT ION A.Single-programMultithreadedWorkloads Fig. 6 shows the speedup of ASR, R-NUCA, PAR-16 and Cluster-16 normalized to the conventional shared L2 design with address interleaving but without replication. Cluster-16 is effectively the baseline with 16 replicas scattered regularly as in a conventional static replication scheme. PAR-16 is our proximity-aware replication with 16 replicas3 . Overall, PAR 16 achieves an average speedup (harmonic mean) of 12% (1.12). The average speedup of ASR and R-NUCA are both around 8%. Cluster-16 has the speedup of 7% (1.07). Note that, their performance difference in a particular benchmark can be larger, e.g. ferret. (cid:20)(cid:17)(cid:24) (cid:20)(cid:17)(cid:23) (cid:20)(cid:17)(cid:22) (cid:20)(cid:17)(cid:21) (cid:20)(cid:17)(cid:20) (cid:20) (cid:19)(cid:17)(cid:28) (cid:19)(cid:17)(cid:27) (cid:41)(cid:41)(cid:55) (cid:38)(cid:43)(cid:50)(cid:17) (cid:47)(cid:56) (cid:53)(cid:36)(cid:39)(cid:17) (cid:53)(cid:36)(cid:60)(cid:17) (cid:41)(cid:48)(cid:48) (cid:37)(cid:36)(cid:53)(cid:17) (cid:50)(cid:38)(cid:40)(cid:17) (cid:41)(cid:40)(cid:53)(cid:17) (cid:41)(cid:47)(cid:56)(cid:17) (cid:57)(cid:44)(cid:51)(cid:54) (cid:36)(cid:57)(cid:42) S p u d e e p (cid:80)(cid:36)(cid:54)(cid:53) (cid:80)(cid:53)(cid:16)(cid:49)(cid:56)(cid:38)(cid:36) (cid:51)(cid:36)(cid:53)(cid:16)(cid:20)(cid:25) (cid:38)(cid:47)(cid:56)(cid:54)(cid:55)(cid:40)(cid:53)(cid:16)(cid:20)(cid:25) Fig. 6. Performance speedup of ASR, R-NUCA, PAR 16 and Cluster 16 normalized to the conventional shared L2 design with address interleaving but without replication. 16 means 16 replicas including the home node. (cid:19)(cid:17)(cid:19)(cid:19)(cid:8) (cid:21)(cid:19)(cid:17)(cid:19)(cid:19)(cid:8) (cid:23)(cid:19)(cid:17)(cid:19)(cid:19)(cid:8) (cid:25)(cid:19)(cid:17)(cid:19)(cid:19)(cid:8) (cid:27)(cid:19)(cid:17)(cid:19)(cid:19)(cid:8) (cid:20)(cid:19)(cid:19)(cid:17)(cid:19)(cid:19)(cid:8) (cid:53) (cid:54) (cid:36) (cid:80) (cid:36) (cid:38) (cid:56) (cid:49) (cid:16) (cid:53) (cid:80) (cid:25) (cid:20) (cid:16) (cid:53) (cid:36) (cid:51) (cid:25) (cid:20) (cid:16) (cid:53) (cid:40) (cid:55) (cid:54) (cid:56) (cid:47) (cid:38) (cid:53) (cid:54) (cid:36) (cid:80) (cid:36) (cid:38) (cid:56) (cid:49) (cid:16) (cid:53) (cid:80) (cid:25) (cid:20) (cid:16) (cid:53) (cid:36) (cid:51) (cid:25) (cid:20) (cid:16) (cid:53) (cid:40) (cid:55) (cid:54) (cid:56) (cid:47) (cid:38) (cid:41)(cid:41)(cid:55) (cid:47)(cid:56) N o r m a i l z d e L 1 m i s s l a t n e y c (cid:47)(cid:82)(cid:68)(cid:71) (cid:54)(cid:87)(cid:82)(cid:85)(cid:72) (cid:44)(cid:73)(cid:72)(cid:87)(cid:70)(cid:75) Fig. 7. Normalized L1 miss latency. (cid:19) (cid:19)(cid:17)(cid:21) (cid:19)(cid:17)(cid:23) (cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:27) (cid:20) (cid:20)(cid:17)(cid:21) (cid:41)(cid:41)(cid:55) (cid:47)(cid:56) N o r m a i l z e n d e t w o r k r t a f f i c (cid:80)(cid:36)(cid:54)(cid:53) (cid:80)(cid:53)(cid:16)(cid:49)(cid:56)(cid:38)(cid:36) (cid:51)(cid:36)(cid:53)(cid:16)(cid:20)(cid:25) (cid:38)(cid:47)(cid:56)(cid:54)(cid:55)(cid:40)(cid:53)(cid:16)(cid:20)(cid:25) Fig. 8. Normalized network trafﬁc. mR-NUCA gets same access latency for shared data. For private data that are accessed only once, except the instruction 3We choose 16 replicas for a 64-core CMP design based on two facts: (1) Allocation a quarter nodes as potential replicas gives good average latency as shown in Tab. I; (2) Prior art shows that clustering cache slices with a sharing degree of four works well in general [11, 6]. This rule of thumb also works with 16 and 32-core CMPs in our experiments. We do not shown such sensitivity study due to limited space. request, mR-NUCA only get shorter latency for private data which will be accessed more than once. For mASR, all L1 requests have to check the local L2 slice ﬁrst. When there is a higher possibility to keep a data replica in the local L2, the average access latency will be shorter than the baseline scheme. Otherwise mASR will get higher average access latency. We ﬁnd that, in SPLASH-2 and PARSEC benchmarks, the majority of the L1 requests are on shared data, and the majority of private data are likely to be accessed only once. These workload characteristics affect the performance of mR-NUCA and mASR. Fig. 7 depicts the normalized average access latency breakdown for fft and lu on L1 miss. mR-NUCA’s L1 miss latency is higher than the other three schemes, due to the tradeoffs between latency and capacity. As load accounts for almost 90% of the total L1 requests, the trends of L1 load miss latency across the four schemes are similar to those in the overall miss latency. On the other hand, mASR has the longest store (not overall) miss latency. The reason is that, when an L1 store miss occurs, the L1 cache doesn’t know whether the requested cache block is read-only or not before this request; therefore, it has to lookup the local L2 cache ﬁrst to maintain cache coherence. mR-NUCA gets shortest store latency, because the global page table helps L1 to send its request to the right place for the updated data. Fig. 8 shows the on-chip network trafﬁc of the four schemes normalized to the baseline for fft and lu, which correlate nicely to the L1 miss latency results. The aforementioned SPALSH2 and PARSEC workload characteristics explains such correlation, since local access is not signiﬁcant. Compared to the baseline, all mASR, PAR-16 and Cluster-16 have more than 60% reduction in on-chip network trafﬁc, i.e., 40% of the baseline. The network trafﬁc of mR-NUCA is close to the baseline, which correlates the normalized value of L1 miss latency as well. (cid:19) (cid:19)(cid:17)(cid:21) (cid:19)(cid:17)(cid:23) (cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:27) (cid:20) (cid:20)(cid:17)(cid:21) (cid:41)(cid:41)(cid:55) (cid:38)(cid:43)(cid:50)(cid:17) (cid:47)(cid:56) (cid:53)(cid:36)(cid:39)(cid:17) (cid:53)(cid:36)(cid:60)(cid:17) (cid:41)(cid:48)(cid:48) (cid:37)(cid:36)(cid:53)(cid:17) (cid:50)(cid:38)(cid:40)(cid:17) (cid:41)(cid:40)(cid:53)(cid:17) (cid:41)(cid:47)(cid:56)(cid:17) (cid:57)(cid:44)(cid:51)(cid:54) (cid:36)(cid:57)(cid:42) L h 2 i t r a t e (cid:80)(cid:36)(cid:54)(cid:53) (cid:80)(cid:53)(cid:16)(cid:49)(cid:56)(cid:38)(cid:36) (cid:51)(cid:36)(cid:53)(cid:16)(cid:20)(cid:25) (cid:38)(cid:47)(cid:56)(cid:54)(cid:55)(cid:40)(cid:53)(cid:16)(cid:20)(cid:25) Fig. 9. L2 hit rate on L1 miss. Fig. 9 gives the L2 cache hit rate on L1 miss of the four schemes. PAR-16 gets average hit rate close to 94%, which means most of L1 requests can hit in the nearest replica node. The average hit rate of mR-NUCA is 98.8% which is close to the baseline, while mASR gets 78% average hit rate as it selectively keeps part of read-only data in local L2 cache. B.MultiprogrammedMultithreadedWorkloads We select a few pairs of multithreaded programs for this multiprogrammed workload study, as shown in Tab. V. The input 6A-1 485               6A-1 p u d e e p S (cid:20)(cid:17)(cid:21) (cid:20)(cid:17)(cid:20) (cid:20) (cid:19)(cid:17)(cid:28) (cid:19)(cid:17)(cid:27) (cid:80)(cid:36)(cid:54)(cid:53) (cid:80)(cid:53)(cid:16)(cid:49)(cid:56)(cid:38)(cid:36) (cid:51)(cid:36)(cid:53)(cid:16)(cid:20)(cid:25) (cid:38)(cid:47)(cid:56)(cid:54)(cid:55)(cid:40)(cid:53)(cid:16)(cid:20)(cid:25) ACKNOW L EDGM EN T The authors acknowledge the support of the Nature Science Foundation of China under Grant No. 60773146, 60833004, 60970002, and the National 863 High Technology Research Program of China(No.2008AA01A201). (cid:41)(cid:41)(cid:55)(cid:16)(cid:47)(cid:56) (cid:41)(cid:41)(cid:55)(cid:16)(cid:53)(cid:36)(cid:39) (cid:41)(cid:41)(cid:55)(cid:16)(cid:37)(cid:36)(cid:53) (cid:41)(cid:41)(cid:55)(cid:16)(cid:50)(cid:38)(cid:40) (cid:53)(cid:36)(cid:39)(cid:16)(cid:37)(cid:36)(cid:53) (cid:47)(cid:56)(cid:16)(cid:50)(cid:38)(cid:40)(cid:36)(cid:49) (cid:36)(cid:57)(cid:42) "
2012,A multi-Vdd dynamic variable-pipeline on-chip router for CMPs.,"We propose a multi-voltage (multi-Vdd) variable pipeline router to reduce the power consumption of Network-on-Chips (NoCs) designed for chip multi-processors (CMPs). Our multi-Vdd variable pipeline router adjusts its pipeline depth (i.e., communication latency) and supply voltage level in response to the applied workload. Unlike dynamic voltage and frequency scaling (DVFS) routers, the operating frequency is the same for all routers throughout the CMP; thus, there is no need to synchronize neighboring routers working at different frequencies. In this paper, we implemented the multi-Vdd variable pipeline router, which selects two supply voltage levels and pipeline modes, using a 65nm CMOS process and evaluated it using a full-system CMP simulator. Evaluation results show that although the application performance degraded by 1.0% to 2.1%, the standby power of NoCs reduced by 10.4% to 44.4%.","5A-1 A Multi-Vdd Dynamic Variable-Pipeline On-Chip Router for CMPs Hiroki Matsutani1 , Yuto Hirata1 , Michihiro Koibuchi2 , Kimiyoshi Usami3 , Hiroshi Nakamura4 , and Hideharu Amano1 1Keio University 3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Japan {matutani,yuto,hunga}@am.ics.keio.ac.jp 3Shibaura Institute of Technology 3-7-5 Toyosu, Kohtoh-ku, Tokyo, Japan usami@shibaura-it.ac.jp 2National Institute of Informatics 2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan koibuchi@nii.ac.jp 4The University of Tokyo 7-3-1 Hongo, Bunkyo-ku, Tokyo, Japan nakamura@hal.ipc.i.u-tokyo.ac.jp Abstract—We propose a multi-voltage (multi-Vdd) variable pipeline router to reduce the power consumption of Networkon-Chips (NoCs) designed for chip multi-processors (CMPs). Our multi-Vdd variable pipeline router adjusts its pipeline depth (i.e., communication latency) and supply voltage level in response to the applied workload. Unlike dynamic voltage and frequency scaling (DVFS) routers, the operating frequency is the same for all routers throughout the CMP; thus, there is no need to synchronize neighboring routers working at different frequencies. In this paper, we implemented the multi-Vdd variable pipeline router, which selects two supply voltage levels and pipeline modes, using a 65nm CMOS process and evaluated it using a full-system CMP simulator. Evaluation results show that although the application performance degraded by 1.0% to 2.1%, the standby power of NoCs reduced by 10.4% to 44.4%. I . IN TRODUC T ION Recently, Network-on-Chips (NoCs) have been used in chip multi-processors (CMPs) [1][2][3][4][5] to connect a number of processors and cache memories on a single chip, instead of traditional bus-based on-chip interconnects that exhibit poor scalability. Fig. 1 illustrates an example of a 16-tile CMP. The chip is divided into sixteen tiles, each of which has a processor (CPU), private L1 data and instruction caches, and a uniﬁed L2 cache bank. The L2 cache banks are either private or shared by all tiles. These tiles are interconnected via on-chip routers, and a coherence protocol runs on the NoC. NoCs are evaluated from various aspects, such as performance and cost. Power consumption, in particular, is becoming more and more important in almost all CMPs. Dynamic voltage and frequency scaling (DVFS) is a primary power saving technique that regulates the operating frequency and supply voltage in response to the applied load. It has been applied to various microprocessors and on-chip routers to reduce their power consumption [3][6]. However, there are certain difﬁculties with DVFS when there are multiple entities or power domains, in which the supply voltage and clock frequency are controlled individually, in a chip. That is, the operating frequencies of two neighboring power domains must be 1:k (k is a positive integer). Otherwise, an asynchronous communication protocol, which introduces a signiﬁcant overhead, is required between them. Since an NoC typically has a strong communication locality in a chip, routerlevel ﬁne-grained power management in response to the applied workload is preferred. In this paper, we propose a low-power router architecture that dynamically adjusts its pipeline depth and supply voltage, CPU 0 CPU 1 CPU 2 CPU 3 CPU 4 CPU 5 CPU 6 CPU 7 CPU 8 CPU 9 CPU 10 CPU 11 CPU 12 CPU 13 CPU 14 CPU 15 L1 D/I cache L2 cache bank On-chip router Fig. 1. Example of 16-tile CMP instead of relying on traditional DVFS techniques that change the frequency of each router. The proposed router offers two pipeline structures: 2-cycle and 3-cycle modes. The 2-cycle mode provides low-latency and high-performance communications, while introducing a large critical path delay, which requires a high supply voltage to satisfy the timing constraints. On the other hand, the 3-cycle mode provides moderate performance, while dividing its critical path into multiple cycles; thus, the supply voltage can be reduced without violating the timing constraints. The 2-cycle mode is used for high workload, while the 3-cycle mode is for low workload. The rest of this paper is organized as follows. Section II illustrates the multi-Vdd variable pipeline router architecture and its implementation using a 65nm CMOS process technology. Section III proposes the voltage and pipeline reconﬁguration policies. Section IV evaluates the proposed router in terms of area, pipeline reconﬁguration latency, and overhead energy. It also shows the system-level evaluation results from using a fullsystem CMP simulator. Section V surveys related work and Section VI concludes this paper. I I . MULT I -VDD VAR IAB L E P I P E L IN E ROU T ER Fig. 2 illustrates the multi-Vdd variable-pipeline router architecture. This section introduces its design and implementation using a 65nm CMOS process. 978-1-4673-0772-7/12/$31.00 ©2012 IEEE 407 5A-1 Vdd-high       Vdd-low Clock (392.2MHz) Voltage switch Input ports Arbiter Output ports 64 Level shifters 3VC 3VC 3VC 64 Crossbar switch Fig. 2. Multi-Vdd variable pipeline router TABLE I SU P PORT ED P I P E L IN E MOD E S Mode 2-cycle 3-cycle Pipeline [RC/VA/SA] [ST,LT] [RC/VA/SA] [ST] [LT] Vdd [V] 1.20 0.83 Freq [MHz] 392.2 392.2 A. Baseline Router As a baseline router architecture, we assume a 64-bit wormhole router that has ﬁve physical channels. Each input physical channel has three virtual channels (VCs) each of which has a 4ﬂit input buffer, while each output physical channel has a single 1-ﬂit output buffer. The ﬂit width is 64-bit. The packet processing in the router can be divided into the following four tasks: routing computation (RC), virtual-channel allocation (VA), switch allocation (SA), and switch traversal (ST). In addition, a single cycle is required to the link traversal (LT) between routers. Including the LT stage, we call this router a 5-cycle router in this paper. B. Variable Pipeline Mechanism The multi-Vdd variable pipeline router supports the 2-cycle and 3-cycle modes, as shown in Table I. The notation [X] denotes that task X is performed in a single cycle, [X,Y] denotes that tasks X and Y are performed sequentially within a clock cycle, and [X/Y] denotes a parallel execution. In the 3-cycle mode, RC, VA, and SA operations are performed in the ﬁrst cycle, since VA and SA can be performed in parallel ([VA/SA]) using a speculation technique [7] and RC and VA/SA are also performed in parallel ([RC/VA/SA]) using look-ahead routing [8]. ST and LT are performed in the second and third cycles, respectively. In the 2-cycle mode, on the other hand, ST is lumped into LT ([ST,LT]); thus the router pipeline depth is 1, while it exhibits a long critical path delay. These two modes can be switched in a single cycle if there are no ﬂits in the router pipeline. C. Pipeline Modes The above-mentioned variable pipeline router was designed with Verilog HDL. Using a Fujitsu 65nm CMOS process, it was synthesized with Synopsys Design Compiler and placed and routed with Synopsys IC Compiler. From the timing analysis, the critical path delay of the 3-cycle mode is 1830.8 psec, while that of the 2-cycle mode is 2549.7 psec when the supply voltage is 1.20V. The variable pipeline router does not change the operating frequency but changes the pipeline depth and supply voltage in response to the offered workload. Assuming that the variable pipeline router operates at 392.2MHz, the 2-cycle mode requires 1.20V while the 3-cycle mode requires only 0.83V to satisfy the timing constraints, as shown in Table I. Here, 1.20V and 0.83V are denoted as Vdd-high and Vdd-low, respectively. The voltage and pipeline reconﬁguration is performed as follows. • 3-cycle (cid:2)→ 2-cycle: Supply voltage is increased from Vddlow to Vdd-high. Then pipeline mode is changed from 3cycle to 2-cycle. • 2-cycle (cid:2)→ 3-cycle: Pipeline mode is changed from 2-cycle to 3-cycle. Then supply voltage is reduced from Vdd-high to Vdd-low. If the pipeline reconﬁguration does not follow these rules, a timing violation may be introduced during the reconﬁguration, resulting in bit errors. Overhead latency and energy for the reconﬁguration is measured based on the circuit-level simulations described in Section IV-A2. I I I . P I P E L IN E R ECON FIGURAT ION In this section, we ﬁrst discuss how to reduce the power consumption of NoCs by using the multi-Vdd variable pipeline router. Then we propose a look-ahead power management method that can minimize both the performance penalty and standby power of NoCs. A. Pipeline Reconﬁguration Policy Typically, DVFS provides lower performance with Vdd-low when the workload is low, while it provides higher performance when the workload is high. However, we believe this concept cannot be directly applied to on-chip routers for CMPs because of the following reasons. • Low-power mode (i.e., low-performance mode) of on-chip routers increases the communication latency between processors and cache memories, which signiﬁcantly increases the application execution time. • Power consumption of processors is typically larger than that of on-chip routers; thus, increased application execution time by the low-power mode may adversely increase the total power consumption of the CMP. Therefore, pipeline reconﬁguration should be performed to reduce the power consumption of on-chip routers as long as the communication latency (i.e., application execution time) is not affected. Consequently, the pipeline reconﬁguration of the multi-Vdd variable pipeline router is performed as follows. • The 2-cycle mode with Vdd-high is used as often as possible for packet transfers in order to reduce the performance penalty. • Otherwise, the 3-cycle mode with Vdd-low is used to minimize the standby power consumption of NoCs. B. Look-Ahead Power Management The voltage transition latency from Vdd-low to Vdd-high requires two cycles for 392.2MHz operation, as discussed in the circuit level evaluations in Section IV-A. To process packets with the low-latency 2-cycle mode, the voltage transition from Vdd-low to Vdd-high should be started in 2-cycle ahead before packets actually reach the router. To minimize the standby power of the router, the supply voltage should be reduced to Vdd-low after the packets leave. For deterministic routing (e.g., XY routing), look-ahead routing computation [8] can be used to detect the packet arrivals two hops away [9]. That is, assuming a packet moves from Router A to Router C via Router B, Router C can detect the packet arrival when the routing computation at Router A is completed. 408 Packet arrival Volt trans starts Volt trans completes 1 2 4 6 4 6 8 2 3 4 5 hop 1 hop 2 hop 3 hop 4 6 7 8 9 Fig. 3. Look-ahead based voltage transition Only the ﬁrst hop router cannot detect the packet arrivals preliminarily, because no prior routers that notify the packet arrival exist for the ﬁrst hop. This is a serious problem in the power-gating router [9], since no packets cannot be forwarded before the wakeup procedure of the router components is completed, which signiﬁcantly degrades application performance. In our multi-Vdd variable pipeline router, on the other hand, the performance overhead of the ﬁrst hop problem is small. In fact, if the voltage reconﬁguration from Vdd-low to Vdd-high is not completed before packet arrivals, the packet is processed with the 3-cycle mode; thus, the performance overhead is only a single cycle in this case. Consequently, packets are processed with the 3-cycle Vdd-low mode only in the ﬁrst hop, while they are processed with the 2-cycle Vdd-high mode after the ﬁrst hop. Fig. 3 illustrates the look-ahead based voltage transition from Vdd-low to Vdd-high. The numbers in the boxes denote the clock cycle of an event, such as the packet arrival and voltage transition start and end times. For example, a packet reaches the ﬁrst hop (hop-1) router at the ﬁrst cycle (cycle-1). Similarly, it reaches hop-2, hop-3, and hop-4 routers in cycle-4, cycle6, and cycle-8, respectively. As shown in this ﬁgure, only the hop-1 router forwards a packet with the 3-cycle mode and the others forward it with the 2-cycle mode. Since the look-ahead routing computation detects the next hop router, the low-to-high voltage transition of the hop-2 router is triggered at cycle-2 and is completed at cycle-4; thus, the hop-2 router can forward it with the 2-cycle mode. Similarly, the low-to-high voltage transition of the hop-3 router is triggered at cycle-4 and is completed at cycle-6, so it can forward the packet with the 2-cycle mode. IV. EVALUAT ION S First, the proposed router is evaluated in terms of area, reconﬁguration latency, and energy. Then, these circuit parameters are fed to a full-system CMP simulator, and the proposed and baseline routers are evaluated in terms of application performance and power consumption. A. Circuit-Level Evaluations The multi-Vdd variable pipeline router was implemented with a Fujitsu 65nm CMOS process, as described in Section II. From the GDS ﬁle of the router, its SPICE netlist is extracted using Cadence QRC Extraction. Then, the pipeline reconﬁguration latency and energy of the router are measured using Synopsys HSIM. Fig. 4 shows the measured waveforms during the voltage and pipeline reconﬁguration. Fig. 4(a) shows the transition from Vdd-high (1.20V) to Vdd-low (0.8V), while Fig. 4(b) shows that from Vdd-low to Vdd-high. In each graph, the ﬁrst (top) waveform shows the supply voltage (Vdd) of the router. The second and third waveforms show the clock signal and the select signal that selects the supply voltage level (0 for Vdd-high and 1 for Vdd-low). The fourth (bottom) waveform shows the current. To accurately measure the overhead current induced by the voltage switching, the clock is stopped during the simulation. A certain latency is required so that Vdd reaches the target voltage level after the power source is switched (i.e., select signal (a) Vdd-high to Vdd-low transition (b) Vdd-low to Vdd-high transition Fig. 4. Measured waveforms of Vdd, clock, select, and current of the proposed router  60  62  64  66  68  70 X20 X40 X60 X80 Number of voltage switches X100  0  5  10  15  20  25  30 R e u o t r a r a e [ k l i a g o t s e ] D y a e l t e b o t e h t a r e g t o v . t l [ c e s n ] Area (left) Delay (right) (a) Vdd-high to Vdd-low transition  60  62  64  66  68  70 X20 X40 X60 X80 Number of voltage switches X100  0  5  10  15  20  25  30 R e u o t r a r a e [ k l i a g o t s e ] D y a e l t e b o t e h t a r e g t o v . t l [ c e s n ] Area (left) Delay (right) (b) Vdd-low to Vdd-high transition Fig. 5. Voltage transition latency vs. number of voltage switch cells (router hardware amount) is changed), as shown in the graphs. This voltage-switching latency will affect the router performance. In particular, the transition latency from the 3-cycle mode with Vdd-low to the 2-cycle mode with Vdd-high should be minimized to reduce communication latency, which signiﬁcantly affects application performance in the cases of shared memory CMPs. Fortunately, the latency for the low-to-high voltage transition is shorter than that for high-to-low, as shown in Fig. 4. 1) Hardware Amount: In the multi-Vdd variable pipeline router, voltage switch cells are inserted between the router and the power sources (i.e., Vdd-high and Vdd-low), as illustrated in Fig. 2. In addition, level shifter cells are inserted to all input (or output) ports of the router to convert the Vdd-low control signals to Vdd-high. As the number (size) of voltage switch cells increases, the voltage-switching latency is shortened, while the area overhead of the voltage switch cells increases. Fig. 5 shows the voltage 5A-1 409                                     TABLE II BR EAKDOWN O F ROU T ER HARDWAR E AMOUN T [K I LO GATE S ] Router 59.41 63.13 Level shifter 0.00 4.11 Volt. switch 0.00 0.54 Total 59.41 67.78 Baseline Proposed  300  200  100  0 -100 -200 -300 1.1V 1.0V 0.9V 0.8V 0.7V From Vdd-high (1.2V) to Vdd-low (X-axis) 0.6V  0  5  10  15 E e n r y g f o r t e h r t s n a i t i n o [ J p ] D y a e l t o e b t e h t a r e g t o v . t l [ c e s n ] Energy is charged. Energy (left) Delay (right) (a) Vdd-high to Vdd-low transition  300  200  100  0 -100 -200 -300 1.1V 1.0V 0.9V 0.8V 0.7V From Vdd-low (X-axis) to Vdd-high (1.2V) 0.6V  0  5  10  15 E e n r y g f o r t e h r t s n a i t i n o [ J p ] D y a e l t e b o t e h t a r e g t o v . t l [ c e s n ] Energy is consumed. Energy (left) Delay (right) (b) Vdd-low to Vdd-high transition Fig. 6. Voltage transition delay and energy vs. Vdd-low voltage transition latency vs. number of voltage switch cells (total router required time for Vdd to reach ±0.05V of the target voltage area). Here, the voltage transition latency is deﬁned as the level after the voltage-select signal is changed. In Fig. 5, the x-axis shows the number of voltage switch cells ranging from 20 to 100 cells. A bar graph shows the gate count of the router (including the voltage switch and level shifter cells) [kilo gates], while a line graph shows the voltage transition latency [nsec]. As shown in this ﬁgure, more voltage switch cells reduce transition latency. However, the area overhead of the voltage switch cells is relatively small compared to the total router area. Thus, in this design, 80 voltage switch cells (X80) are inserted into each router by taking into account the latency and area overheads. Table II lists the area breakdown of the baseline 3-cycle router and the proposed multi-Vdd variable pipeline router. 80 voltage switch cells (X80) are inserted to the proposed router. In addition, a level shifter cell is inserted into every input port of the proposed router. As a result, the proposed router is larger than the baseline router by 14.1%. Most of the area overhead comes from the level shifter cells and the control logic for dynamic reconﬁguration of the router pipeline depth. 2) Reconﬁguration Latency: Fig. 6 shows the voltage transition delay and energy vs. Vdd-low level. The line graph shows the high-to-low and low-to-high voltage transition latencies when the Vdd-low level ranges from 0.60V to 1.10V, while Vddhigh is ﬁxed at 1.20V. When Vdd-low is 0.80V, the high-tolow latency is 5.3nsec, while low-to-high latency is 3.1nsec. That is, for the transition from the 3-cycle mode to 2-cycle mode, the router ﬁrst increases the supply voltage and waits for 3.1nsec (2-cycle for 392.2MHz operation). After that, the pipeline reconﬁguration from 3-cycle to 2-cycle is performed. 3) Reconﬁguration Overhead Energy: A certain amount of dynamic energy is consumed when the supply voltage is changed TABLE III S IMU LAT ION PARAM E T ER S (CMP ) Processor L1 I-cache size L1 D-cache size # of processors L1 cache response L2 cache size # of L2 cache banks L2 cache response Memory size Memory response # of memory ports TABLE IV UltraSPARC-III 16 KB (line:64B) 16 KB (line:64B) 16 1 cycle 256 KB (assoc:4) 16 6 cycles 160 (± 2) cycles 4 GB 16 S IMU LAT ION PARAM E T ER S (NOC ) Topology Routing # of VCs Buffer size Flit size Control packet Data packet 4×4 mesh dimension-order 3 4 ﬂits 64 bits 1 ﬂits 9 ﬂits from Vdd-low to Vdd-high. In Fig. 6, the bar graphs show the overhead energy for the high-to-low and low-to-high voltage transitions. The energy consumption is completely different between the high-to-low and low-to-high transitions. In the lowto-high transition, current ﬂows from the Vdd-high power source to the router; thus, overhead energy is consumed. In the high-tolow transition, on the other hand, current ﬂows from the router to the Vdd-low power source. This current is charged at the capacitance of the Vdd-low power source, and it is consumed gradually by the router; thus, the overhead energy becomes negative. When Vdd-high and Vdd-low are 1.20V and 0.80V, respectively, the energy overhead for a low-to-high transition is 231.4pJ, while that for a high-to-low transition is -147.6pJ. That is, a pair of voltage transitions (i.e., low-to-high + high-to-low) consumes 83.8pJ on average. 4) Break-Even Time Analysis: Energy reduction by remaining at the Vdd-low level should be larger than the energy overhead. Otherwise, total power consumption adversely increases due to the energy overhead. Here, break-even time (BET) is deﬁned as the minimum time period that can compensate for the energy overhead (e.g., 83.8pJ) by remaining at the Vdd-low level. If the router remains at Vdd-low for a shorter time than BET, the energy overhead is larger than the beneﬁt and total power consumption adversely increases. The placed-and-routed design of the proposed router was simulated at 392.2MHz. From power analysis based on the switching activity information, the standby power consumption of the 2-cycle mode with Vdd-high is 2.78mW, while that of the 3-cycle mode with Vdd-low is 1.33mW since power consumption is proportional to the square of Vdd. That is, remaining in the 3-cycle mode with Vdd-low for 1sec can save 1.45mJ from the standby energy consumption. To compensate for the energy overhead of 83.8pJ, the router must remain in the 3-cycle mode at least 23 cycles. Any shorter stay at Vdd-low increases the total standby power consumption. B. System-Level Evaluations The obtained circuit parameters discussed in the previous subsections were fed to a full-system CMP simulator to evaluate the application execution time and the standby power reduction by taking into account the energy overhead. 1) Simulation Environments: An NoC used in the 16-tile CMP illustrated in Fig. 1 was simulated. Regarding the L2 cache organization, the multi-Vdd variable pipeline router architecture was applied to the following two CMP conﬁgurations. 5A-1 410                                          0  0.2  0.4  0.6  0.8  1  1.2  1.4  1.6 IS DC MG EP LU SP Benchmark programs BT FT Ave. E u c e x i t n o i t m e ( o n r m a i l d e z ) Penalty +2.1% All 2-cycle transfer Proposed All 3-cycle transfer (a) 16-tile CMP (shared L2 cache)  0  0.2  0.4  0.6  0.8  1  1.2  1.4  1.6 IS DC MG EP LU SP Benchmark programs BT FT Ave. E u c e x i t n o i t m e ( o n r m a i l d e z ) Penalty +1.0% All 2-cycle transfer Proposed All 3-cycle transfer (b) 16-tile CMP (private L2 cache) Fig. 7. Execution time of NPB applications (1.0 denotes execution time with ideal 2-cycle routers) • Shared L2 cache banks: L2 cache banks in all tiles are shared by all tiles. They form a single shared L2 cache in a chip. • Private L2 cache banks: Each L2 cache bank is used as a private L2 cache inside the same tile. It cannot be accessed by the other tiles. A directory-based coherence protocol is used to maintain the cache coherence. To avoid the end-to-end protocol (i.e., request and reply) deadlocks, three virtual channels (VCs) are used for different message classes. The trafﬁc amount on the shared L2 CMP is larger than that on the private L2 CMP, since L2 cache banks in all tiles are shared by all tiles via NoC in the case of the shared L2 CMP. To simulate the above-mentioned CMP, we use a full-system multi-processor simulator: GEMS [10] and Wind River Simics [11]. We modiﬁed a detailed network model of GEMS, called Garnet [12], to accurately simulate the proposed multi-Vdd variable pipeline router. Table III lists the processor and memory system parameters, and Table IV lists the on-chip router and network parameters. To clearly show the worst-case performance degradation induced by the voltage and pipeline reconﬁguration, we assume relatively rich main memory bandwidth, namely one memory port for each tile. To evaluate the application performance on CMPs with the proposed router, we use eight parallel programs (IS, DC, MG, EP, LU, SP, BT, FT) from the OpenMP implementation of NAS Parallel Benchmarks (NPB) [13]. Sun Solaris 9 operating system is running on the CMPs. These benchmark programs were compiled by Sun Studio 12 and are executed on Solaris 9. The number of threads was set to sixteen for the 16-tile CMP. 2) Application Execution Time: The multi-Vdd variable pipeline router is applied to the above-mentioned NoC of the CMPs, and the eight NPB programs are performed on the NoC. The following three router modes are compared in terms of the execution time of these applications. • All 2-cycle transfer: The router mode is ﬁxed in the 2cycle Vdd-high mode (high-performance but high-power).  0  10  20  30  40  50  60 IS DC MG EP LU SP Benchmark programs BT FT Ave. S t o p y b d n a w e r o f N o C [ m W ] -10.4% Tied to high Proposed Tied to low (a) 16-tile CMP (shared L2 cache)  0  10  20  30  40  50  60 IS DC MG EP LU SP Benchmark programs BT FT Ave. S t y b d n a o p w e r o f N o C [ m W ] -44.4% Tied to high Proposed Tied to low (b) 16-tile CMP (private L2 cache) Fig. 8. Standby power of NoC (reconﬁguration power is included) • Proposed: The router mode is changed to the 2-cycle Vdd-high mode whenever a packet approaches the router. Otherwise, it remains at the 3-cycle Vdd-low mode (highperformance and low-power). • All 3-cycle transfer: The router mode is ﬁxed at the 3cycle Vdd-low mode (low-power but low-performance). Fig. 7 shows the application execution times of these router modes. Fig. 7(a) shows the results on the shared L2 CMP, and Fig. 7(b) shows those on the private one. The results are normalized so that the execution time of the All 2-cycle transfer mode is 1.0. Although the proposed power management method degrades the application performance by 1.0%-2.1% compared to the All 2-cycle transfer mode that cannot reduce the power, performance degradation is very small. This slight performance degradation comes from the 3-cycle transfers at the ﬁrst hop of the packet transfers, because the look-ahead mode control cannot detect packet arrival and forces the 3-cycle transfer only at the ﬁrst hop, as mentioned in Section III-B. The next section explains how much standby power can be reduced with the proposed power management compared to the All 2-cycle transfer mode at Vdd-high. 3) Standby Power Reduction: The proposed variable pipeline router is operated in the 2-cycle Vdd-high mode for packet transfers to prevent performance degradation, while it remains at the 3-cycle Vdd-low mode to minimize power consumption when no packets are processed. Thus, the packet processing power is constant since packet processing is performed with the 2-cycle Vdd-high mode except the ﬁrst hop. In this section, therefore, we focus on standby power reduction including the overhead energy. As mentioned in Section IV-A3, a pair of voltage transitions (i.e., low-to-high + high-to-low) consumes 83.8pJ on average. The standby power consumption of the 2-cycle Vdd-high mode is 2.78mW, while that of the 3-cycle Vdd-low mode is 1.33mV. To evaluate the standby-power including overhead energy, all reconﬁguration events are recorded during the full-system simulation of each application. By multiplying the frequency of the mode reconﬁguration by its energy overhead, we estimated the reconﬁguration power in addition to standby power consumption. 5A-1 411                         5A-1 Fig. 8 shows the standby power of the NoCs. In the graphs, “Tied to Vdd-high” corresponds to “All 2-cycle transfer”, and “Tied to Vdd-low” corresponds to “All 3-cycle transfer”. The standby power with the proposed power management includes the reconﬁguration power for each application. Fig. 8(a) shows the results on the shared L2 CMP, and Fig. 8(b) shows those on the private one. The trafﬁc amount in the private L2 case is smaller than that in the shared one; thus, the reconﬁgurations in the private one are infrequent compared to the shared one. As a result, in the private L2 case, the standby power is reduced by 44.4% on average even when the energy overhead is included. In the shared L2 case, the standby power is reduced by 10.4%. Applications with high trafﬁc volume (e.g., FT) introduce frequent reconﬁgurations compared to BET, and thus it sometimes overwhelms the beneﬁt of dynamic voltage reduction. A more sophisticated power management policy that takes into account BET may be able to prevent unnecessary reconﬁgurations, while router design complexity is increased. V. R E LATED WORK DVFS has been applied to various microprocessors and onchip routers [3][6]. Regarding the pipeline stage optimization, [14] proposes the time-stealing technique for on-chip networks. It improves the router operating frequency by exploiting the timing imbalance between router pipeline stages. That is, a router can operate at a higher frequency which is determined by average delay of all pipeline stages, not the slowest pipeline stage. In addition, some techniques that optimize the pipeline structure in response to the workload have been developed for microprocessors [15] and on-chip routers [16]. For example, router pipeline structure, supply voltage, and operating frequency are changed in response to the workload in [16]. Since an NoC typically has a strong communication locality in a chip, router-level ﬁne-grained power management in response to the applied workload is more efﬁcient. However, these previous approaches are not suited to ﬁne-grained power management of NoCs, because these approaches adjust the operating frequency and supply voltage for each power domain. In this case, the operating frequencies of two neighboring power domains must be the same or divisible by another. Otherwise, an asynchronous communication protocol is required. Therefore, unlike these previous approaches, our multi-Vdd variable pipeline router dynamically adjusts its pipeline depth and supply voltage, instead of relying on traditional DVFS techniques that change the frequency of each router. Runtime power gating [9] is another approach to reduce the power consumption of routers. However, it suffers a wakeup latency to activate the sleeping components; thus, a sophisticated wakeup mechanism is required to mitigate the wakeup latency. V I . CONC LU S ION S In this paper, the multi-Vdd variable pipeline router was designed and implemented with a 65nm CMOS process. We evaluated it in terms of area overhead, latency, and energy overhead for pipeline reconﬁguration. We also estimated BET of a mode reconﬁguration required to compensate for the reconﬁguration energy overhead. As a result, area overhead for the pipeline reconﬁguration controller, voltage switch cells, and level shifter cells is 14.1%. Voltage transition latency is 3.1nsec and 5.3nsec for low-to-high and high-to-low transitions, respectively. A pair of voltage transitions consumes 83.8pJ. BET to compensate for this energy overhead is 23 cycles for 392.2MHz operation. We also proposed a look-ahead based power management that detects packet arrivals and pre-conﬁgures itself to the 2cycle Vdd-high mode for minimizing both performance penalty and power consumption. The proposed power management was applied to the multi-Vdd variable pipeline router and evaluated using real 16 threads parallel applications on a full-system CMP simulator. The simulation results showed that, although the application performance is slightly affected (1.0% to 2.1%) compared to the All 2-cycle transfer mode at Vdd-high, the standby power is decreased signiﬁcantly (10.4% to 44.4%) compared to the Tied to Vdd-high mode. For applications with high trafﬁc volume, however, the reconﬁguration power overwhelms the power reduction by the low-power mode. Exploring more sophisticated power management policies that take into account BET while keeping router design simple is our future work. ACKNOW L EDG EM EN T S This research was performed by the authors for STARC as part of the Japanese Ministry of Economy, Trade and Industry sponsored “NextGeneration Circuit Architecture Technical Development” program. The authors thank to VLSI Design and Education Center and Japan Science and Technology Agency CREST for their support. This work was also supported by Grant-in-Aid for Research Activity Start-up #23800053. "
2012,A reconfigurable platform for the design and verification of domain-specific accelerators.,"In this paper we present Vortex: a reconfigurable Network-on-Chip platform suitable for implementing domain-specific hardware accelerators in a design efficient manner. Our Vortex platform provides a flexible means to compose domain-specific accelerators for streaming applications such as performance critical machine vision systems. By substituting a traditional shared-bus architecture with low latency packet-switched routers and high utility network adaptors, maximum performance is exploited with minimal regard to communication infrastructure design and validation. To highlight the utility of the Vortex platform we present a case study in which a video analytics pipeline is mapped onto a multi-FPGA system. The system meets real-time throughput requirements on 3 Megapixel 48-bit image sequences with minimal resource overhead attributed to the Vortex communication infrastructure.","S2-5 A Reconﬁgurable Platform for the Design and Veriﬁcation of Domain-Speciﬁc Accelerators Sungho Park, Yong Cheol Peter Cho, Kevin M. Irick and Vijaykrishnan Narayanan Microsystems Design Laboratory (MDL), Department of Computer Science and Engineering, The Pennsylvania State University, University Park, PA 16802 {szp142, ycho, irick, vijay}@cse.psu.edu In this paper we present Vortex: a reconﬁgurable Network-onChip platform suitable for implementing domain-speciﬁc hardware accelerators in a design efﬁcient manner. Our Vortex platform provides a ﬂexible means to compose domain-speciﬁc accelerators for streaming applications such as performance critical machine vision systems. By substituting a traditional shared-bus architecture with low latency packet-switched routers and high utility network adaptors, maximum performance is exploited with minimal regard to communication infrastructure design and validation. To highlight the utility of the Vortex platform we present a case study in which a video analytics pipeline is mapped onto a multi-FPGA system. The system meets real-time throughput requirements on 3 Megapixel 48-bit image sequences with minimal resource overhead attributed to the Vortex communication infrastructure. I . IN TRODUC T ION Power efﬁciency is a key driver in the push towards the design of domain-speciﬁc accelerators. The design and veriﬁcation of such accelerators is a challenging task due to the variety of end applications. In this work, we present a platform with a ﬂexible communication fabric and a programmable interface for connecting multiple custom hardware to compose domain speciﬁc accelerators. Case studies in the domain of video recognition are presented to highlight the platform and associated tools. The rapid expansion of high performance embedded systems over the last decade can be largely credited to advances in semiconductor fabrication technology [1][2]. Rapidly shrinking transistor geometries were translated into rapidly increasing system operating frequencies. However, as challenges related to semiconductor fabrication reliability and leakage power consumption begin to dominate performance gains, the paradigm of increasing system performance by increasing system frequency has seemingly run it course. While multi-core CPUs and GPUs offer a promising future for some application domains, custom application accelerators remain as leaders in the category of best performance per Watt [3]. The difﬁculty of creating custom accelerators has been the main barrier of wide adoption of custom accelerators beyond specialized military, aerospace, and medical application domains. While abundant research has been conducted to improve programmability of custom hardware including Expression-Grained Reconﬁgurable Arrays and Catapult C Synthesis, the lack of platform-level communication abstrac  ,$ 	   ,""  '$   &%')'%((' (+ ) %*)'   %+&((  "")'  (*"" "" $ ('( $& %'),"")%'#$)+%'! ('( $& Fig. 1. An example system on Vortex tion can make high-level system synthesis an intractable task for a wide range of application scenarios [4][5]. The Vortex platform offers this abstraction by presenting a standard system interface to an underlying robust communication infrastructure. I I . D E SCR I P T ION O F VORT EX P LAT FORM A. Overview All processing modules in a target system can be integrated as a node in the Vortex Platform in the form of either a Switch Attached Processor(SAP) or a Streaming Operator(SOP). The functionality of the component dictates whether it should be mapped as a SAP or as a SOP. The distinction between these nodes is described in the following sections. Modules that are required to initiate and terminate data streams are considered as a SAP. The SAP utilizes 1)the master interface to actively read or write data from/to a memory space attached elsewhere on the network. 2)The slave interface is used to make local memory accessible by any other SAP in the network. Additionally, 3)the message interface provides a medium for light-weight message communication across SAPs for synchronization or to communicate control information. The SOP is used for modules that operate in a streaming fashion where incoming data packets are consumed, processed, and resulting outputs are fed back into the network. The SOP accesses the router via three interfaces: 1) the ingress interface, 2) the egress interface, and 3) the conﬁguration interface. The ingress and egress interfaces are based on simple handshaking protocol, in which both sender and receiver asserts its readiness to send and receive data respectively. The conﬁguration interface follows a similar handshaking protocol providing external access to local conﬁguration SRAM with address, data, request and acknowledge signals. There is no limitation of number of 978-1-4673-0772-7/12/$31.00 ©2012 IEEE 108 	    	        	 	  (a) SAP  	   	  	    	   	 (b) SOP Fig. 2. Interface concept of SAP and SOP ports on each SOP which allows for various functionality such as data multicasting. For example, a SOP connected to the network by two ports may take as input a data stream from one input port and duplicate it to both output ports. The characteristic that distinguishes the SOP from the SAP is that the SOP is purely a streaming node where output data cannot be generated without input data. In contrast, the SAP autonomously initiates transactions with the network in any arbitrary access fashion. One or multiple ports from a Vortex router can be connected to any SOP or SAP as their interface signal sets are completely identical. Ports of a SAP or SOP communicate with router ports via the Network Interface (NIF-). As shown in Fig. 1, Network Interfaces, i.e. NIF-SOP, NIF-SAP, and NIF-MEM, bridge the router with SOPs, SAPs, or Memory respectively. B. Router: 8-port crossbar switch The Vortex Router is an eight-port packet-switched crossbar that supports non-blocking access between any input and any available output: resolving contention using a round-robin arbitration scheme. The router has a 256-bit internal datapath and independently conﬁgurable port widths of either 128-bits or 256-bits. Each port can be conﬁgured as Input, Output, or Bidirectional with each input/output pair being completely decoupled. Input and Output Port buffering supports queueing of at least a single maximum sized packet. Support for up to eight Virtual Channels is useful in avoiding deadlock in many applications. The router supports an asynchronous clocking relationship between its internal clock and the clocks of each Input/Output port. The Vortex packet format closely resembles that of the PCIExpress-4DW TLP Memory Write Transaction format. A 64-bit address ﬁeld is subdivided into a 16-bit Device ID, 10bit Flow ID, and 36-bit address. The 16-bit Device ID is further subdivided into an 8-bit Bus ID, 5-bit Switch ID, and 3-bit Port ID. Each router has an internal Bus Route Table and Switch Route Table, that can be conﬁgured to refer packets to arbitrary outputs based on the Device ID of the incoming packet header. Every router is assigned a unique pair of Bus ID and Switch ID at design time. As a new packet enters an input port, the router matches the Bus ID on the packet header with its own Bus ID. If it matches, it subsequently tries to match the Switch ID with its own. Otherwise, the Bus Route Table is referenced to ﬁnd the port number assigned to the Bus ID of the packet attempting to route. If the Switch ID also matches, the packet is forwarded to the port identiﬁed by the 3-bit Port ID. Otherwise, the Switch Route Table is referenced to ﬁnd the port number assigned to the Switch ID of the packet attempting to route. This routing scheme allows the Vortex platform to support up to 213 , 8192 routers on the network without imposing S2-5 network topology constraints. C. Flow A major contribution of the Vortex platform is the integrated network awareness and support for application data ﬂows or Vortex Flows. A Vortex Flow describes any sequence of operations required to complete a designated computation. Flows offer three major beneﬁts: (1) A large sequential computational process can be decomposed into multiple small operators, where each operator can be a general purpose and reusable component. (2) By overlapping the computation of data with the transport of that data between endpoints (i.e. Memoryto-Memory move) the potential to hide computational latency with communication latency increases. (3) The dataﬂow representation of a computation can be easily mapped to the network architecture, making Design Automation tractable. In addition, the support of non-trivial ﬂow patterns including converging and diverging ﬂows signiﬁcantly extend the applicability of dataﬂow processing in SoCs. We deﬁne a ﬂow as a label that identiﬁes a path that a certain data stream takes from initiation to termination. A ﬂow can start from on-chip or off-chip memory mapped SAPs; travel through one or a sequence of SOPs according to its ﬂow; and ﬁnally retire in the destination memory. The initiator or terminator of a ﬂow is not required to be a memory node. The 10-bit Flow ID allows users to allocate 960 unique application ﬂows on the network: 64 Flow IDs are reserved as system ﬂows. The Flow ID is run-time conﬁgurable and is associated with an initiator Device ID, a terminator Device ID, and one or several next-hop Device IDs. The network interface of each intermediate node decodes the Flow ID to obtain the next hop to which the current packet should be routed. Therefore, individual SAP and SOP nodes do not retain any information about any other nodes in the ﬂow. In fact, the nodes are oblivious to almost all network control information including its own Device ID and is only responsible for properly completing its assigned task or computation. D. NIF-SAP: Network Interface for SAP A NIF-SAP allows a Switch Attached Processor to initiate 6 different types of transaction as shown in Table I. In the table, Initiator denotes the SAP that initiates a transaction through its master interface, while Target denotes the SAP that is the corresponding endpoint of the transaction. The Flow ID is issued by the Initiator during the request such that the Network Interface can establish the virtual connection between the two endpoints and all intermediate nodes. Speciﬁcally, the connection is established between a producer handler of the initiator NIF-SAP and a consumer handler of the target NIF-SAP. The participating NIF-SAPs dynamically allocate producer and consumer handlers from their local handler pool per transaction. If the handler pool is exhausted in the NIF-SAP of either the Initiator or Target, the transaction request is terminated with an appropriate error code. The behavior of error handling is within the scope of the SAP and the degree of thoroughness and complexity dictated by the designer: the SAP may retry the failed transaction indeﬁnitely, or 109 S2-5 TABLE I VAR IOU S TRAN SAC T ION TY P E S Master Request Type Stream Source Stream Destination Device Interface Device Interface !""  ""   $ ""   ""  !! ""    1. Master Write Initiator Master Target Slave 2. Master Read Target Slave Initiator Master 3. Slave Write Initiator Slave Target 4. Slave Read Target Slave Initiator 5. Master Write-Lite Initiator Master Target Slave Slave Slave 6. Master Read-Lite Target Slave Initiator Master it may reschedule other pending transactions that do not share dependency with the failing transaction. The dynamically allocated pool-of-handlers architecture has several beneﬁts: (1) The SAP can be involved in multiple outstanding transactions, thus maximizing task level concurrency;(2) the number of handlers can be conﬁgured at design time to trade-off resource utilization and application performance; and (3) on platforms with support for partial reconﬁguration, customized application speciﬁc handlers can be instantiated dynamically, thus adaptively leveraging application runtime behavior to gain performance [6]. For example, to maximally utilize the bandwidth offered by current DDR3 components while operating at conservative yet achievable clock frequencies, the memory controller can utilize multiple handlers to issue and respond to multiple concurrent read/write transactions. In addition to single Initiator-Target transactions, the NIFSAP allows data streams to converge or diverge such that multiple Initiators or Targets can be involved in a single transaction. This feature directly and efﬁciently supports SIMD and MISD application mapping. In these transactions, the NIFSAP safely detects and avoids deadlock conditions that may arise during the connection setup phase: cancellation packets are issued when one or more targets deny a request while some targets have allocated handlers and accepted the request. Master transactions, i.e. request type 1, 2, 5, and 6 in Table I, can be utilized when the Initiator SAP wants to write or read directly into/from the master interface in a streaming fashion. In these scenarios the SAP directly produces or consumes data utilizing simple FIFO-like handshaking semantics across the SAP/NIF-SAP interface. For low latency Lite transactions, request types 5 and 6 are especially suited. In these cases the Initiator-Target setup phase is circumvented without regard to Target(s) handler availability. This is useful for post boot conﬁguration since most devices are free to accept packets. Conﬁguration accesses can beneﬁt from back-to-back transfers that avoid setup overhead for each small conﬁguration packet. Slave transactions, i.e. request type 3 and 4, are similar to traditional DMA transfer: access to the Initiator and Target SAPs are at memory mapped locations utilizing SRAM-like handshaking semantics.         ! ! ! ! 	 """"#!   !! ""  %! %! ""  #!#"" %""#""    !   ! !  ""   !! ""  Fig. 3. Block diagram of NIF-SAP E. Window Transfer It is often necessary in image processing applications to access a subset of a 2D array of data called a Region of Interest or ROI. One approach to accessing an ROI is to fetch/store each row of the ROI as independent transactions in the largest contiguous amount containing a row. The disadvantage of this approach is that for each row that is accessed, network arbitration and packet overheads are incurred. As the row size decreases this overhead becomes increasingly prominent as the packet payload portion becomes comparable in size with that of the control portion. Consequently, ROIs are not regarded as frames since the transfer of each row is treated as independent transactions. Moreover, if the network can deliver packets of differing transactions out-of-order, then this scheme may result in functionally incorrect behavior. For all of the transaction types listed in Table I the NIF-SAP supports a Window Transfer mode to address these issues. The NIF-SAP includes a runtime conﬁgurable Window Descriptor Table to associate ROI Window Descriptors to a given Flow ID. A Window Descriptor includes row size, row stride, and row count information to describe an access pattern for fetching a rectangular subregion. This region can begin at any offset within a memory mapped space. When a transaction referencing a Window ﬂow is issued, the Initiator and Target NIF-SAP transparently fetch and store data according to the access pattern while fully utilizing the payload capacity of each packet. Since Window transfers are handled exclusively within the NIF-SAP, intermediary nodes maintain a simpliﬁed 1D streaming view of data. The NIF-SAP clocking topology is designed to ease the task of reaching timing-closure on complex SoCs. The NIF-SAP provides four source clocks of coarse frequency granularity to the attached SAP. The SAP synchronizes with the NIF-SAP by returning one of the four clocks, or derivative thereof, to the NIF-SAP. Therefore, SAPs may operate in multiple clock domains as necessary to account for disparate critical paths. 110 F. NIF-SOP: Network Interface for SOP The NIF-SOP propagates data streams to the attached SOP removing all ancillary information not necessary for the processing of that data by the SOP. The NIF-SOP presents frame delimiting information that supports the runtime re-tasking of a single SOP. Upon receipt of the ﬁrst packet of a frame, the NIFSOP presents the SOP with a 16-bit Opcode, conﬁgured to be associated with the Flow ID of the incoming transaction. This mechanism allows the SOP to support many processing modes distinguished by the speciﬁc Opcode accompanying the data. For example, a generic convolution SOP may support the use of several kernel coefﬁcient sets depending on some application determined mode of operation. By associating these kernel sets with distinct opcodes, the particular convolution operation can be dictated by the Flow ID used for associated transactions. In addition, the NIF-SOP manages multiple outstanding frame transfers allowing the SOP processor to be virtualized across many data streams concurrently. The maximum number of concurrent streams is design-time conﬁgurable to accommodate application requirements and resource availability. G. NIF-MEM: Network Interface for Memory Memory is treated as a type of SAP, utilizing the NIFSAP slave interface to expose a globally accessible memory mapped device. The NIF-MEM augments the functionality of the NIF-SAP by expanding the base messaging interface with a message-triggered request manager. The request manager parses memory-request commands and subsequently initiates slave transactions between its local memory-connected slave interface and the slave interface of any other remote SAP(s) (including the NIF-MEM itself in the case of Memory-to-Memory copy). If a memory transfer requires barrier synchronization, the request manager sends completion notiﬁcations per transaction. Because the DMA functionality is incorporated within the NIF-MEM, network utilization can be reduced as compared to traditional shared DMA architectures that implement a ReadStore-Write style of DMA. H. SOP Composition Since SOPs are the key components to accelerating streaming applications, we have developed a library of common operators and connectivity modules to easily compose complex accelerators from general purpose highly conﬁgurable primitives. As illustrated in Fig. 4, a custom SOP can be easily built by combining these primitives manually in an HDL ﬂow. In addition, an automation tool to compose a SOP from a dataﬂow graph as speciﬁed by a system designer has been developed. I I I . D EV E LO PM EN T ENV IRONM EN T Our experimental system has been developed and validated on a DiniGroup multi-FPGA board conﬁgured with 6 Virtex6 SX475T FPGAs and a dual core Marvell processor [7]. 4 corner FPGAs are connected to 4GB of DDR3 memory each totalling 16 GB of memory accessible by any SAP on the network. A host PC is connected to the board via PCIExpress to S2-5 &(,+ %  .     !&&$.+)+')- &(,+')+ )'*+ , %($) ""&# ()+""+""'& ""&# ()+""+""'& ()$ '& ()$ '&   )'*+ ,+')+  ,+')+  ,+')+ ,+')+ ,+')+ ,+')+ ,+')+ ,$$ ,$$ ,$$      Fig. 4. An example of SOP composition provide input data, collect output data, and conﬁgure all necessary components in the system. The PCIExpress interface on the FPGA side is managed by a host adaptor that is wrapped as a SAP. The role of the host PC can be replaced by the on-board Marvell processor to realize a stand-alone system. The memory controller supports up to 8 separate ports yielding 800 MB/s of maximum transfer rate. Inter-FPGA links between adjacent FPGAs are enabled with transfers of 256 bits per cycle at 100MHz yielding 3.2GB/s. IV. CA S E S TUDY AND R E SU LT S : N EUROMOR PH IC V I S ION SY S T EM As a case study we describe the acceleration of an advanced object classiﬁcation system implemented on a multi-FPGA platform. The application consists of three algorithms that collectively perform the feature extraction process for a shape based object classiﬁer. Retinal Preprocessing performs advanced image equalization, dynamic range expansion, and color opponent contrast enhancement. The result of this stage is four images that represent contrast enhancement of various combinations of the original image channels. Through a process termed Center-Surround differencing, the Retinal Preprocessing produces output images that amplify the information hidden in the differences between pairs of image channels. The Saliency process identiﬁes regions of interest for which object classiﬁcation should be selectively performed. It performs this operation by building a per pixel feature map found by accumulating the likelihood of response of several basis functions projected onto an input image. For a given pixel, the feature map represents the likelihood of occurrence of that pixel in the given image. Those pixels with lower likelihood exhibit high information content and therefore are good candidates for belonging to a region of interest. The feature extraction process extracts edges from the preprocessed input images by projecting, onto the images, a set of basis functions that model the response of mammalian cortical cells. These edge responses are subsequently used as object contour components in a shape-based object classiﬁer. 111 	      S2-5    $                 ""    "" !              %   ! #!  ! #! %    %   ! #!               Fig. 5. Layout of Neuromorphic Vision System on 6 FPGAs A. Retinal Preprocessing The Retinal Preprocessor is a Streaming Operator, SOP, implemented as a dataﬂow arrangement of components including convolution, hyperbolic tangent, and various arithmetic operators. For each input channel, the retinal processing is performed on both a full resolution and scaled resolution of the input image channels. All preprocessed channels are stored in on-board memory for access by subsequent processing stages. The Retinal Processor utilizes a single input port and two output ports: collectively occupying two ports on the router. The host adaptor provides an interface between a processor and the Vortex system. Currently the host adaptor supports either an on-board ARM processor, an external PCIExpress attached host computer, or an embedded MicroBlaze or PowerPC processor. In this study, the host adaptor attaches the Vortex system to a PC via a PCIExpress interface. Input images are streamed from the host through the host adaptor into the Retinal Processor and ﬁnally to two memory targets: storing the full and scaled retinal images. B. Saliency Processing Saliency processing consists of projecting up to six orientations of Gabor wavelets onto any number of the four channels of the scaled preprocessed image. Because the choice of channels used in the saliency computation can be altered per frame, and to reduce the hardware resource requirements, the pipeline is designed to accelerate processing of two orientations of a single channel concurrently. Software controls iterative use of the pipeline to compute the entire saliency map across all orientations and channels. In this work the software controlling the pipeline executes on the host but could easily be implemented as a state machine within a dedicated Switch Attached Processor, SAP. The process of building the saliency map for a given Gabor orientation and image channel consists of two phases of processing: Density Estimation and Likelihood Computation. For each orientation and channel, Density Estimation builds a histogram of the Gabor responses, requiring at least a single pass of the input image. For all orientations and channels, Likelihood Computation subsequently performs pixel-wise accumulation of the contents of the associated histograms indexed by the Gabor response. Note that the amount P ER FORMANC E O F N EUROMOR PH IC V I S ION SY S T EM TABLE II Retinal Preprocessing Saliency Feature Extraction Latency 58 ms 62 ms 4ms per ROI of data generated by the Gabor projection would quickly overwhelm the bandwidth offered by a standard DDR3 component. As such the saliency processor minimizes external bandwidth by re-streaming the input image and re-computing the Gabor projection. The particular orientation/channel computation is associated with a speciﬁc Opcode. During operation Memoryto-Memory transfers conﬁgured for those Opcodes are initiated from the memory controller through the Saliency processor. C. Feature Extraction Three FPGAs are dedicated to the Feature Extraction process. Each FPGA performs Gabor projection of two orientations and a single channel. Software controls iterative use of the feature extractors to compute a total of six orientations and any number of channels. Feature extraction is performed on 256 × 256 Regions of Interest, ROI, within the full scale Retina preprocessed image. To process a single ROI a Multi-target Window transfer is issued to the memory controller to efﬁciently stream ROI pixels through the feature extractor. Frame duplicators are utilized to multicast the image frame to all of the three feature extractors. D. Layout on 6-FPGA Board and Results Fig. 5 describes how a neuromorphic video analytics application is mapped onto a 6-FPGA board. Each FPGA contains a single router and supporting infrastructure components such inter-FPGA links and external memory controllers. Feature extraction is distributed across three FPGAs: B, D, and E. Since DDR3 Memory is only available on the corner FPGAs, FPGA D, unlike FPGAs B and E, hosts both a feature extraction processor and a DDR3 memory controller. Fig. 6 shows the utilization percentages for the Retinal Preprocessing, Saliency, Feature Extraction with Memory Controller on FPGA D, and Feature Extraction without Memory Controller on FPGA B and E. The results are obtained after Place and Route when targeting a Xilinx Virtex-6 SX475T FPGA device. Resources are categorized as belonging either to the core processing module (Accelerator), Vortex platform (router, NIF-SAP, NIF-SOP, or NIF-MEM), or Infrastructure (Memory controller and interFPGA links). As shown in Table II, the application on our platform showed 62 ms of total latency for 2048 × 1536, 48 bpp input image frames, achieving 16.13 frame per second. Since those three stages of pipeline run concurrently, latencies of each stage are not accumulated, and Feature Extraction can process 15 ROIs per frame. V. CONC LU S ION Intuitively, computational performance is maximized when the maximum number of processing elements are working in 112 	   	   	   	   	   	   S2-5   	      	 ""$""   	 $%#$) $)##* #')'() &"" #'%&% $*'%* #%%%&$ ("")""#"""" '* #%) &%&   # "" %*"" (a) Retinal Processor  	     	 !#!   	 '(""(# $***% *#$*& %! '"")"""" $#*#' ''%!% '!)!""!! ""*# ""#& $')   #% ! "")# (b) Saliency Processor   	     	 !#!   	 $*$!( #'%(& &!'*) %! $$#%# #'""""! $##"") '!)!""!! ""## '"" ""%!   * ! ""(% (c) Feature Extraction with Memory Controller   	     	 !#!   	 %''*# $*)#' &!&(# %! #$*$) %##!) $$!(! '!)!""!! ""!) ""') ""%!   "" ! ""(% (d) Feature Extraction without Memory Controller Fig. 6. Comparison of resource consumption between Application and Platform parallel with minimal idle time. The Vortex platform provides the means for multiple parallel data ﬂows and computations to occur concurrently. Mapping of computationally demanding applications beneﬁts from a concrete design methodology and framework that leverages the ﬂexibility offered by a reconﬁgurable network-on-chip. Most application-speciﬁc parameters and ﬂow-related information are dynamically reconﬁgurable at runtime, while some are carefully selected to be speciﬁed at synthesis time to accommodate available resources, system conﬁguration, and user requirements. The main objectives driving the design of the Vortex platform was to simplify the process of mapping computationally intensive streaming designs onto FPGA and ASIC targets. Streaming operators are ready to be instantiated or recustomized from an existing library of common operators and pre-built connectivity modules. This simpliﬁcation of a streaming NoC infrastructure enables engineers and designers to reduce time spent on system development, integration, and validation. With the emergence of partial reconﬁguration in current FPGAs, further improvement in performance and/or resource efﬁciency is attainable with the Vortex Platform. By swapping operators at runtime, performance can be improved dramatically when the cost to transfer data to a remote operator exceeds the cost to partially conﬁgure an operator in the vicinity of the data. As such, exploring potential beneﬁts that partial reconﬁguration may provide for the Vortex platform is the focus of ongoing research. ACKNOW L EDG EM EN T This work was supported in part by a DARPA Neovision2 grant, NSF Award 0916887 and 0829607. "
2012,ARB-NET - A novel adaptive monitoring platform for stacked mesh 3D NoC architectures.,"The emerging three-dimensional integrated circuits (3D ICs) offer a promising solution to mitigate the barriers of interconnect scaling in modern systems. In order to exploit the intrinsic capability of reducing the wire length in 3D ICs, 3D NoC-Bus Hybrid mesh architecture was proposed. Besides its various advantages in terms of area, power consumption, and performance, this architecture has a unique and hitherto previously unexplored way to implement an efficient system-wide monitoring network. In this paper, an integrated low-cost monitoring platform for 3D stacked mesh architectures is proposed which can be efficiently used for various system management purposes. The proposed generic monitoring platform called ARB-NET utilizes bus arbiters to exchange the monitoring information directly with each other without using the data network. As a test case, based on the proposed monitoring platform, a fully congestion-aware adaptive routing algorithm named AdaptiveXYZ is presented taking advantage from viable information generated within bus arbiters. Our extensive simulations with synthetic and real benchmarks reveal that our architecture using the AdaptiveXYZ routing can help achieving significant power and performance improvements compared to recently proposed stacked mesh 3D NoCs.","5A-2 ARB-NET: A Novel Adaptive Monitoring Platform for Stacked Mesh 3D NoC Architectures Amir-Mohammad Rahmani1,2 , Khalid Latif1,2 , Kameswar Rao Vaddina1,2 , Pasi Liljeberg1 , Juha Plosila1 , and Hannu Tenhunen1 1Embedded Computer Systems Lab., Department of Information Technology, University of Turku, Finland 2Turku Centre for Computer Science (TUCS), Finland Email: {amir.rahmani, khalid.latif, vaddina.rao, pasi.liljeberg, juha.plosila, hannu.tenhunen}@utu.ﬁ Abstract— The emerging three-dimensional integrated circuits (3D ICs) offer a promising solution to mitigate the barriers of interconnect scaling in modern systems. In order to exploit the intrinsic capability of reducing the wire length in 3D ICs, 3D NoC-Bus Hybrid mesh architecture was proposed. Besides its various advantages in terms of area, power consumption, and performance, this architecture has a unique and hitherto previously unexplored way to implement an efﬁcient system-wide monitoring network. In this paper, an integrated low-cost monitoring platform for 3D stacked mesh architectures is proposed which can be efﬁciently used for various system management purposes. The proposed generic monitoring platform called ARBNET utilizes bus arbiters to exchange the monitoring information directly with each other without using the data network. As a test case, based on the proposed monitoring platform, a fully congestion-aware adaptive routing algorithm named AdaptiveXYZ is presented taking advantage from viable information generated within bus arbiters. Our extensive simulations with synthetic and real benchmarks reveal that our architecture using the AdaptiveXYZ routing can help achieving signiﬁcant power and performance improvements compared to recently proposed stacked mesh 3D NoCs. I . IN TRODUC T ION Multiprocessing is a promising solution to meet the requirements of near future applications. To get full beneﬁts of parallel processing, a multiprocessor system needs an efﬁcient onchip communication architecture. Network-on-Chip (NoC) is a general concept, proposed for complex on-chip communications because of scalability, better throughput and reduced power consumption [1]. However, increasing the number of cores over a 2D plane is not efﬁcient enough due to long interconnects. The advent of 3D silicon integration technology has opened a new horizon for new on-chip interconnect design innovations. In 3D integration technologies, multiple layers of active devices are stacked above each other and vertically interconnected using Through-Silicon Vias (TSVs) [2][3]. It permits a large degree of freedom in choosing an on-chip network topology. Thus a wide range of on-chip network structures that were not explored earlier are being considered [4]. The comparison of 2D and 3D NoC architectures show that, 3D NoCs deliver better system performance with signiﬁcantly lower energy per packet, as compared to the 2D implementations due to increased package density and shorter wires [5]. The straightforward extension of popular planar 2D NoC structure is 3D Symmetric NoC created by simply adding two additional physical ports to each router; one for Up and one for Down [6]. Despite simplicity, this architecture does not exploit the beneﬁcial feature of a negligible inter-wafer distance in 3D chips, because in this architecture, inter-layer and intralayer hops are indistinguishable. Also, due to two extra ports, a considerably larger crossbar is required [7]. The Stacked (Hybrid NoC-Bus) mesh architecture presented in [8] is a hybrid architecture between the packet switched network and the bus architecture to overcome the above mentioned 3D Symmetric NoC challenges. It takes advantage of the short inter-layer distances that are characteristic of 3D ICs. To obtain high performance on large on-chip systems, a careful design of on-chip communication platform and efﬁcient utilization of available resources is required. The effective resource utilization needs an efﬁcient system monitoring platform, which can monitor the system at run-time and manage the resource utilization dynamically. Due to large number of cores in 3D NoC based system, heavy trafﬁc loads can create congestion. These issues direct designers to follow a sophisticated monitoring platform. The existing monitoring schemes for 2D NoCs can be extended for typical 3D NoC architectures. However, 3D NoC-Bus Hybrid mesh architecture necessitates its own customized monitoring platform considering the hybridization structure. In this paper, we address the monitoring issues of 3D NoCBus Hybrid mesh architectures to enhance the overall system performance and reliability and reduce the power consumption. The proposed platform can be efﬁciently used for any kind of system management and monitoring such as trafﬁc monitoring, thermal management and fault tolerance. As a test case, the trafﬁc monitoring is presented. The proposed platform gracefully monitors and extracts trafﬁc information from bus arbiters and provides valuable information for connected routers to implement a fully adaptive 3D routing algorithm and to globally balance the load across all dimensions. The remainder of the paper is organized as follows. Section II elaborates the demands for enhancement of the existing architectures, while Section III motivates this work. Section IV presents the proposed monitoring platform and the adaptive routing algorithm based on the proposed architecture. The simulation methodology and results are shown in Section V and ﬁnally, Section VI concludes this paper. 978-1-4673-0772-7/12/$31.00 ©2012 IEEE 413 5A-2 I I . BACKGROUND AND R ELATED WORK The stacked mesh architecture [8] discussed in Section I suffers from an important drawback. The issue with this architecture is that, each packet is traversed through two intermediate buffers: the source output buffer and destination input buffer. The output buffer hinders implementing congestionaware inter-layer communications. Rahmani et al. [9] have resolved this problem by removing the source output buffers and rearranging the buffers placement at destination input. Based on the new hybridization scheme, the authors introduced a congestion-aware inter-layer routing algorithm called AdaptiveZ for 3D stacked mesh NoC architectures. Chao et al. [10] proposed a thermal-aware adaptive routing using a proactive downward routing to ensure thermal safety for throttled 3D NoCs. The routing technique is limited to symmetric 3D NoCs, uses non-minimal path and increases the zero load latency. The work presented in [9] addressed these issues and proposed a hybrid adaptive routing algorithm to mitigate the thermal issues. A reconﬁgurable inter-layer routing mechanism (RILM) for irregular 3D NoCs was presented in [11]. The proposed architecture exchanges a set of messages during initialization to generate an information tree containing location of 3D routers. This information tree is stored at each node. The mechanism is repeated, whenever a fault on any vertical link occurs. Storing the tree information on each node and message exchanging impose considerable overheads in terms of area, power and latency. In [12] a Hieratical Agent Monitoring (HAM) for complex, parallel and distributed systems was proposed. The authors proposed to have a separate design layer for monitoring operations, separated from computation and communication because adaptive monitors are required for adjusting the system performance under unpredictable situations. The approach provides high level of abstraction for system designers. However, there is no available physical platform to implement HAM approach for 3D NoC-Bus Hybrid mesh architectures. For all the discussed architectures, authors address a speciﬁc issue such as fault tolerance, thermal management or trafﬁc monitoring. From implementation point of view, if a designer considers all the system monitoring and management techniques, the design complexity and overheads will grow dramatically. Apart from design complexity, most of the aforementioned techniques were designed for symmetrical 3D NoCs without consideration of trafﬁc congestion. These issues necessitate an efﬁcient system monitoring platform with minimum overheads capable of considering all the system constraints in parallel without adding extra workloads to the main interconnection network. I I I . MOT IVAT ION AND CON TR IBU T ION RXYZ_VC0_StressValue RXYZ_VC1_StressValue Bus Arbiter LAYER Partitioning RXY(Z+1 ) LAYER Partitioning dest_LayerID req wait grant EOP RXYZ LAYER Partitioning RXY (Z-1) LAYER Partitioning 0 1 0 1 0 1 Fig. 1. Side view of the AdaptiveZ -based stacked mesh architecture [9] buffers, transactions going on and the transactions in the queue waiting for the bus grant. If the router receives the grant signal, it will proceed with the transaction. Instead of grant, if wait signal is received, the router will wait for the grant signal and the bus arbiter will put the request in the queue. The problem arises when no grant or wait signal is received. In this case, the router will withdraw the request and send the packet to one of the neighboring routers without any information of the status of their connected buses. The packet may have to be rerouted from next hop due to the similar situation. In this kind of situations, if the bus arbiter informs the router to choose proper direction for the next hop, it will globally balance the load across all vertical buses. This issue necessitates bus arbiters to have some monitoring information about their neighboring buses. The same kind of problems may arise for fault tolerant routings or during dynamic power and thermal management. Independent implementation of the mentioned monitoring services imposes a considerable area, power, latency and design complexity overheads on the system. Among the available options, an optimal approach is to exploit a dedicated generic monitoring platform, where different monitoring and management services can be integrated. The available monitoring frameworks for 3D NoC architectures have signiﬁcant overheads because monitoring data needs to be exchanged across multiple layers. In addition, the available frameworks cannot efﬁciently utilize the beneﬁts of hybrid architectures such as 3D NoC-Bus Hybrid mesh architecture. To deal effectively with the mentioned challenges, we propose a system monitoring platform for 3D NoC-Bus Hybrid mesh architectures. To the best of our knowledge, this is the ﬁrst effort in implementing generic monitoring platform for 3D NoC-Bus Hybrid mesh architectures. As discussed before, in [9], a stacked mesh 3D NoC architecture enabling congestion-aware interlayer communication has been presented. Their detailed stacked mesh architecture is shown in Fig. 1. In this architecture, after receiving a bus request (req) along with destination layer ID (dest layerID) from the router, the bus arbiter generates the grant and wait signals on the basis of different parameters such as stress value of VC IV. TH E PRO PO S ED ARCH I T EC TUR E The proposed architecture of our 3D NoC including the ARB-NET monitoring platform is shown in Fig. 2. It is basically a 3D stacked mesh architecture which is a hybrid between packet-switched network and a bus. The arbiters resolve the contention between different IP blocks for bus access and 414 5A-2 IP Block Switch ARB-NET Node Bus Node k Monitoring Interconnect (SMS) n Interconnect m Bus SMS(in/out) SMS(in/out) ARBNET SMS(in/out) SMS(in/out) ^^ Fig. 2. ARB-NET-based 3D Hybrid NoC-Bus mesh architecture Fig. 3. Packet format supporting ARB-NET monitoring platform are a better source to keep track of monitoring information. Hence, the arbiters can be prudently used by bringing them together to form a network and thereby creating an efﬁcient monitoring and controlling mechanism by using them. The arbiters exchange very short messages (SMS) among themselves regarding various monitoring services that are on offer. The calculation of SMS is cycle based and are updated every cycle. In this paper we will concentrate on one particular short message type, namely the stress values (its a measure of how much the bus is busy) of the bus pillars in order to effectively monitor and control the network of a 3D NoC system. The arbiters and their network are in the middle layer of a 3D stacked mesh NoC in order to reduce the number of vertically connected TSVs by keeping wire distances as uniform as possible [8][9]. Since the routers are mandated to send extra packet information (like Xdir and Ydir which deﬁne possible directions a packet can take during the minimal path routing), the number of TSVs that are being used are reduced drastically. Our monitoring mechanism being proposed and implemented in this paper is a low-cost and an optimal model, as the control network is required to spawn across in only one layer. The proposed monitoring network which uses very short messages, can also be used for fault tolerance as well as making systems thermally efﬁcient. Since, the proposed monitoring infrastructure differentiates between data network and monitoring network, the system performance is not degraded. A.PacketFormat The packet format that is being used in our 3D NoC is shown in Fig. 3. It can be seen that the packet has a header ﬂit and a number of payload ﬂits. Each ﬂit is n bits wide. The ﬁrst bit in the ﬂits is reserved for the bop (begin-of-packet) ﬂag and the second bit for the eop (end-of-packet) ﬂag respectively. In the header ﬂit, m bits are reserved for the routing information (RI) and k bits are reserved for the monitoring information (MI). The remaining bits are allocated for DATA and to implement some higher level protocols (HLP). Although such a protocol is not deﬁned in this paper, it is reserved for our derivative and future work. As shown in the Fig. 3, the RI ﬁeld consists of ﬁve ﬁelds: Xdest , Ydest , Zdest , Xdir and Ydir . The Xdir and Ydir bits deﬁne the possible directions a packet can take during the minimal path routing within the layer of the network, and is TABLE I Xdir Ydir LOOKU P TAB L E Xdir Ydir 00 01 Possible Directions →↑ →↓ ←↑ ←↓ 10 11 described in Table I. The MI ﬁeld consists of several bits which can be used to effectively monitor the network. One such possible MI ﬁeld information can be the packet length bits (Ptype ). In this paper, in order to classify different types of packets in the network, we use the packet length ﬁeld (Ptype ) as a differentiating unit between packets instead of packet type. The packet length bits can be ignored in our network for packets which are of constant length. B.ARB-NETNodeArchitecture The ARB-NET node that has been used in our 3D network on chip is a central building block for our arbiter network. In its general form, it consists of a measuring unit, control unit and the unit which does the actual arbitration. The measuring unit senses different monitoring parameters (like thermal, faults, trafﬁc, etc), whereas a control unit acts on the parameters that were measured by the measuring unit, by passing actionable control signals to the bus arbiter. In this paper, we use bus stress values to select least stressed bus pillars, while adaptively routing data packets in a more efﬁcient way. Hence, in our case the measuring unit and the control unit will be analogous to bus stress value calculator unit and pillar selector unit respectively as elaborated in the following subsections and illustrated in Fig. 4. The signals to/from the routers to the ARBNET node are also depicted in the Fig. 4. B.1 Bus Stress Value Calculator The measuring unit which senses different monitoring parameters (in our case bus trafﬁc) calculates stress values of the bus and then propagates them to the neighboring arbiters as short messages over the network. Such a measuring unit is depicted as part of the ARB-NET node in Fig. 4. The bus stress values are calculated according to Algorithm 1 and is based on the request signal (req) and packet type (Ptype ) from the router, and queue length of all output buffers connected to the bus within that layer. The total stress value of the bus pillar is calculated by 415 5A-2 Signals to/from RouterXY0 Signals to/from RouterXY(N-1) Algorithm 2 Pillar Selection Algorithm Input: Xdir , Ydir , S tress valueZ{E ,W,N,S} Output: Arb rsp X/Y , Arb rsp Balance Queue length of all connected buffers Bus Arbiter N N req Ptype Queue length of all connected buffers Xdir Ydir SMS_out Global for all 4 directions Bus stress value calculator Pillar selector Monitor SMSE_in SMSW_in SMSN_in SMSS_in Fig. 4. ARB-NET node architecture Algorithm 1 Bus Stress Value Calculation Algorithm Input: req1−N , Ptype1−N , queue length of all connected output buffers Output: S tress valueZ N: Number of layers α: Constant weight factor, 0 ≤ α ≤ 1 1: S tress valueZ = 0; 2: for i = 1 → N do if (reqi = ‘1’) then 3: 4: 5: end if 6: end for 7: SM S [ST VZ ] = S tress valueZ ; S tress valueZ = S tress valueZ + (Ptypei + α × min{queue lengthreqi dest }); if (S tress valueZE > S tress valueZN Arb rsp X/Y = ‘1’; Arb rsp Balance = ‘0’; else if (S tress valueZE < S tress valueZN Arb rsp X/Y = ‘0’; Arb rsp Balance = ‘0’; ) then ) then Arb rsp X/Y = ‘X’; Arb rsp Balance = ‘1’; if (S tress valueZE > S tress valueZS Arb rsp X/Y = ‘1’; Arb rsp Balance = ‘0’; else if (S tress valueZE < S tress valueZS Arb rsp X/Y = ‘0’; Arb rsp Balance = ‘0’; ) then ) then Arb rsp X/Y = ‘X’; Arb rsp Balance = ‘1’; else end if 1: if (Xdir Ydir = “00”) then 2: 3: 4: 5: 6: 7: 8: 9: else if (Xdir Ydir = “01”) then 10: 11: 12: 13: 14: 15: 16: 17: else if (Xdir Ydir = “10”) then 18: 19: 20: 21: 22: 23: 24: end if else else end if 25: else if (S tress valueZW > S tress valueZN Arb rsp X/Y = ‘1’; Arb rsp Balance = ‘0’; else if (S tress valueZW < S tress valueZN Arb rsp X/Y = ‘0’; Arb rsp Balance = ‘0’; ) then ) then Arb rsp X/Y = ‘X’; Arb rsp Balance = ‘1’; if (S tress valueZW > S tress valueZS Arb rsp X/Y = ‘1’; Arb rsp Balance = ‘0’; else if (S tress valueZW < S tress valueZS Arb rsp X/Y = ‘0’; Arb rsp Balance = ‘0’; ) then ) then Arb rsp X/Y = ‘X’; Arb rsp Balance = ‘1’; 26: 27: 28: 29: 30: 31: 32: else end if 33: end if summing up packet type (e.g. length) and the status of the destination buffer associated with the routers request signal. The weight factor (α) is used to regulate the impact of the destination queue length in our algorithm, the optimal value of which is determined based on a series of experiments conducted as described in the experimental results section of this paper. It can also be seen from the experimental results section, that the area and power consumption of the measuring unit is very minimal and does not add to much overhead. B.2 Pillar Selector The controlling unit (pillar selector unit) which selects the next bus pillar the packet has to take en route to its destination is being governed by Algorithm 2 and illustrated in Fig. 4. It takes into account the stress values of the neighboring buses, as well as the Xdir and Ydir bits which give two directions the packet can possibly take depending on the response of the Arb rsp X/Y signal. For example if both the Xdir and Ydir bits are low, then the pillar selector unit checks the stress values of the buses which are north and east bound to make a decision on the direction the packet has to take. Depending on the Arb rsp X/Y signal, the arbiter guides the router to send the packet towards the direction of the bus which is least stressed. When the stress values of buses in both the possible directions is same, then Arb rsp Balance signal is asserted and the choice of the direction which the packet has to take is left to the router. Similarly, for other values of Xdir and Ydir bits, the bus pillar is selected in accordance with Algorithm 2. C.AdaptiveXYZRoutingAlgorithm The routing of packets that takes place in the 3D NoC architecture that has been described above is based on our AdaptiveXYZ routing algorithm which has been elaborated in Algorithm 3. In the AdaptiveXYZ routing, the direction the data packet has to traverse, depends on the location of the current node, location of the destination, queue length of the buffers in the x-direction and in the y-direction, as well as the response of the arbiter which will be guiding the packet through the network. If the destination node is the same as the current node, then the packet is delivered to the local node and the algorithm exits. If the packet has to be delivered with-in the same layer, then ﬁrst stress values which are nothing but the queue lengths of the buffers in the neighboring nodes are calculated, and the node with least stress value is chosen to transmit the packet while on its journey to its destination. If the destination node is connected to the same bus as the current node, then the current node sends a request signal to the bus arbiter along with the destination layer ID. The packets waits at the current node until it receives a grant signal from the bus arbiter. If the destination node is not on the same layer and is not connected to the same bus as the current node, then the packet is routed in the network after the current node sends a request signal to the bus arbiter along with the destination layer ID, 416 Algorithm 3 AdaptiveXYZ Routing Algorithm Input: (Xcurrent , Ycurrent , Zcurrent ), (Xdestination , Ydestination , Zdestination ), queue lengthX , queue lengthY , Arb rsp X/Y , wait Output: Next Hop (E, W, N, S, L, U/D) 1: Xdif f = Xcurrent − Xdestination ; 2: Ydif f = Ycurrent − Ydestination ; 3: Zdif f = Zcurrent − Zdestination ; 4: if (Xdif f = Ydif f = Zdif f = 0) then 5: Deliver the packet to the local node and exit; 6: end if 7: if (Zdif f = 0) then S tress valueX = queue lengthX ; S tress valueY = queue lengthY ; Use S tress valueX and S tress valueY to choose the destination port; 11: else if (Xdif f = Ydif f = 0) then 12: Send a request to the bus arbiter along with the destination layer ID; 14: else if (Xdif f (cid:7)= 0 or Ydif f (cid:7)= 0) then 13: Wait until receiving the grant; 15: Send a request to the bus arbiter along with the destination layer ID, Xdir , Ydir , Ptype ; if (wait = ‘1’) then Wait until receiving the grant; else Withdraw the request; if (Xdif f = 0) then Send the packet to Y Axis towards the destination; else if (Ydif f = 0) then Send the packet to X Axis towards the destination; else if (Xdif f (cid:7)= 0 and Ydif f (cid:7)= 0) then if (Arb rsp Balance = ‘0’) then S tress valueX = {Arb rsp X/Y , queue lengthX }; S tress valueY = {Arb rsp X/Y , queue lengthY }; Use S tress valueX and S tress valueY to choose the output end if port; 30: 31: end if end if 32: end if 8: 9: 10: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 5A-2 in VHDL. Symmetric 3D-mesh NoC [6], Typical 3D NoC-Bus Hybrid mesh [8], AdaptiveZ -based 3D NoC-Bus Hybrid mesh [9] and the proposed architecture were analyzed for synthetic and realistic trafﬁc patterns. For Symmetric and Typical 3D NoC-Bus Hybrid mesh Z-DyXY wormhole routing was used. In Z-DyXY routing, a ﬂit will ﬁrst travel statically along the Z direction, then DyXY algorithm [13] will be used for the intra-layer wormhole routing. For the architecture proposed in [9], AdaptiveZ-DyXY routing was used. We used the proposed AdaptiveXYZ routing for the presented ARB-NET 3D NoC. For the all four architectures, routers used the minimally fully adaptive reserved VC deadlock avoidance technique discussed in [14]. We take advantage of two VCs per input port. node (four layers of 3×3 mesh) network. We assumed ﬁrst In synthetic trafﬁc analysis of our experiments, we use a 36layer is processors and other layers are shared cache memories. We followed SNUCA cache hierarchy and MOESI cache coherency protocol. The ﬂit size was set to 128 bits. Our MOESI CMP directory is a 3-phase protocol, means a request(R)/response(S) control message is issued to the directory(memory)/cache. The directory/cache responds, and then the directory/cache controller sends the actual data. This avoids races. In conclusion, each of the R/S message is just for control, short enough to ﬁt a ﬂit (e.g. 1 message = 1 ﬂit). We used 5-ﬂit packets to transfer each 64-Byte cache line (read response or write request data). Because there are only two packet types (1-ﬂit and 5-ﬂit packets), we used the actual packet length instead of packet type (Ptype ) in packet headers. It was assumed the buffer size of each FIFO was four ﬂits. In Algorithm 1 line 4, α is used as a criterion to regulate the impact of destination queue length on the bus stress value calculation. The more the α is, the more stress value is assigned to the corresponding bus. Based on our throughput comparison between different α values under the uniform random trafﬁc pattern, the case where α = 0.4, sustains the most throughput for the proposed architecture. To perform the simulations, we used uniform, hotspot 10% (2 nodes) and Negative Exponential Distribution (NED) [15] trafﬁc patterns. The average packet latency (APL) curves for uniform, hotspot 10% and NED trafﬁc patterns with varying average packet arrival rates (APAR) are shown in Fig. 5. It can be observed for all the trafﬁc patterns, that the network with proposed architecture saturates at higher injection rates. The reason being that by utilizing the provided information via the ARB-NET, the AdaptiveXYZ routing increases the bus utilization and makes the load, balanced. In the case of Z-DyXY routing and AdaptiveZ-DyXY routing, the 3D NoC-Bus Hybrid mesh architecture cannot deliver the desired performance because of bandwidth limitations. For the proposed architecture, bandwidth limitations are managed by the separated monitoring network without increasing the main 3D NoC communication workload. We conclude the simulation results with a qualitative area comparison with the other work, most closely related to ours. The area of the different routers was computed once synthesized on CMOS 65nm LPLVT STMicroelectronics standard cells using Synopsys Design Compiler. The typical, the AdaptiveZ -based, and the proposed bus arbiters were also synthesized to illustrate the area overhead of the controllers. The laypacket type and Xdir and Ydir information. The packet either waits upon the receipt of the grant signal, or withdraws the request and then proceeds to send the packet towards the destination along the x or y axis. It may also withdraw the request and use the Arb rsp X/Y and Arb rsp Balance signals provided by the ARB-NET node, to choose one output port among them through which the packet is propagated in the network. This is done by calculating the stress values of the neighboring nodes as a function of the queue length and the Arb rsp X/Y value. In order to simplify the hardware infrastructure, we simply ignore the queue length of the buffers in the neighboring buses and just use the Arb rsp X/Y value provided by the ARBNET node. In this way, the problem which has been described in the motivation section of this paper about the routing issue when both the wait signal and the grant signal are not received by the router has been solved in an innovative way with the help of the pillar selector unit. V. S IMU LAT ION R E SU LT S As mentioned before, the ARB-NET 3D NoC provides a low-cost monitoring platform to enhance the overall system performance, power consumption, reliability, etc. In order to demonstrate the efﬁciency of the proposed monitoring platform in network average packet latecy and power reduction, a cycle-accurate NoC simulation environment was implemented 417 5A-2 ) s e l c y c ( y c n e t a L t e k c a P e g a r e v A 800 700 600 500 400 300 200 100 0 0.05 Symmetric NoC 3D Mesh Typical Hybrid NoC-Bus 3D Mesh AdaptiveZ Hybrid NoC-Bus 3D Mesh ARB-NET Hybrid NoC-Bus 3D Mesh ) s e l c y c ( y c n e t a L t e k c a P 0.1 0.15 0.2 Average Packet Arrival Rate (packets/cycle) 0.25 e g a r e v A 800 700 600 500 400 300 200 100 0 0.05 Symmetric NoC 3D Mesh Typical Hybrid NoC-Bus 3D Mesh AdaptiveZ Hybrid NoC-Bus 3D Mesh ARB-NET Hybrid NoC-Bus 3D Mesh ) s e l c y c ( y c n e t a L t e k c a P 0.1 0.15 0.2 Average Packet Arrival Rate (packets/cycle) 0.25 e g a r e v A 800 700 600 500 400 300 200 100 0 0.05 Symmetric NoC 3D Mesh Typical Hybrid NoC-Bus 3D Mesh AdaptiveZ Hybrid NoC-Bus 3D Mesh ARB-NET Hybrid NoC-Bus 3D Mesh 0.1 0.15 0.2 0.25 Average Packet Arrival Rate (packets/cycle) 0.3 (a) Under uniform trafﬁc proﬁle (b) Under hotspot 10% trafﬁc proﬁle (c) Under NED trafﬁc proﬁle Fig. 5. Latency versus average packet arrival rate on a 3×3×4 mesh for different 3D NoC-Bus Hybrid mesh architectures TABLE II HARDWAR E IM P L EM EN TAT ION D E TA I L S Component Area (μm2 ) [1] A. Jantsch and H. Tenhunen. Networks on Chip. Kluwer Academic Publishers, 2003. "
2012,Hungarian algorithm based virtualization to maintain application timing similarity for defect-tolerant NoC.,"Homogeneous manycore processors are emerging in broad application areas, including those with timing requirements, such as real-time and embedded applications. Typically, these processors employ Network-on-Chip (NoC) as the communication infrastructure and core-level redundancy is often used as an effective approach to improve the yield of manycore chips. For a given application's task graph and a task to core mapping strategy, the traffic pattern on the NoC is known a priori. However, when defective cores are replaced by redundant ones, the NoC topology changes. As a result, a fine-tuned program based on timing parameters given by one topology may not meet the expected timing behavior under the new one. To address this issue, a timing similarity metric is introduced to evaluate timing resemblances between different NoC topologies. Based on this metric, a Hungarian method based algorithm is developed to reconfigure a defect-tolerant manycore platform and form a unified application specific virtual core topology of which the timing variations caused by such reconfiguration are minimized. Our case studies indicate that the proposed metric is able to accurately measure the timing differences between different NoC topologies. The standard deviation between the calculated difference using the metric and the difference obtained through simulation is less than 6.58%. Our case studies also indicate that the developed Hungarian method based algorithm using the metric performs close to the optimal solution in comparison to random defect-redundant core assignments.","6A-3 Hungarian Algorithm Based Virtualization to Maintain Application Timing Similarity for Defect-Tolerant NoC Ke Yue, Frank Lockom, Zheng Li, Soumia Ghalim, and Shangping Ren Department of CS Illinois Institute of Technology Email: {kyue, ﬂockom, zli80, sghalim, ren}@iit.edu Chicago, Illinois 60616 Lei Zhang and Xiaowei Li Key Laboratory of CSA Institute of Computing Technology Chinese Academy of Sciences Email: {zlei, lxw}@ict.ac.cn Abstract—Homogeneous manycore processors are emerging in broad application areas, including those with timing requirements, such as real-time and embedded applications. Typically, these processors employ Network-on-Chip (NoC) as the communication infrastructure and core-level redundancy is often used as an effective approach to improve the yield of manycore chips. For a given application’s task graph and a task to core mapping strategy, the trafﬁc pattern on the NoC is known a priori. However, when defective cores are replaced by redundant ones, the NoC topology changes. As a result, a ﬁne-tuned program based on timing parameters given by one topology may not meet the expected timing behavior under the new one. To address this issue, a timing similarity metric is introduced to evaluate timing resemblances between different NoC topologies. Based on this metric, a Hungarian method based algorithm is developed to reconﬁgure a defect-tolerant manycore platform and form a uniﬁed application speciﬁc virtual core topology of which the timing variations caused by such reconﬁguration are minimized. Our case studies indicate that the proposed metric is able to accurately measure the timing differences between different NoC topologies. The standard deviation between the calculated difference using the metric and the difference obtained through simulation is less than 6.58%. Our case studies also indicate that the developed Hungarian method based algorithm using the metric performs close to the optimal solution in comparison to random defect-redundant core assignments. I . IN TRODUC T ION As technology advances, manycore architectures are becoming mainstreams for a large spectrum of applications, including real-time and embedded applications. As there are many cores on-chip, such architectures typically employ Networkon-Chip (NoC) as a scalable communication backbone among processing cores. However, many challenges are yet to be tackled for the design of NoC-based manycore processors. Manufacturing defects and transistor wear-outs are among the top list. According to Sperling’s report [1], for a Cell processor, without considering defect tolerance during the architecture design phase, even under the best case, the yield can be as low as only 10% to 20%. The research is supported by NSFC CNS 1018731, NSF Career 0746643, NSFC 60906018 and NSFC 61173006. As there are many light weighted cores on-chip, and each core occupies only a small area of the chip footprint, core-level redundancy is often used as an efﬁcient technique to overcome the NoC chip yield issue [2]. In particular, if C cores are expected to be provided to customers, R redundant cores will be added on the chip. Cores which fail due to defect or aging can be replaced with a redundant core, thus guaranteeing the demanded computing capability. However, when a defective core is replaced with a redundant core, it is possible that the on chip topology, i.e., the interconnect relationship among cores are changed, for example from a regular 2D mesh topology to a irregular topology. The underlying NoC topology with possible defective cores is called a physical topology. Different chips may have different physical topologies with different failure bitmaps. It will be a great burden for application developers as they have to face various topologies to design, deploy and optimize their programs. Topology virtualization is proposed to isolate various underlying physical structures, and provide programmers with a uniﬁed interface [3]. Prior research on manycore topology virtualization mainly focused on general purpose computing domain and the methods proposed intend to achieve better performance in terms of communication latency and network throughput [2], [4], [3]. However, the goals differ from the above for applications with timing requirements. For a real time application, rather than performance, the most important property is its predictability. That is, its functional and timing behavior should be as deterministic as system speciﬁcations require. Therefore, timing similarity instead of high performance is preferred to avoid introducing extra cost in redesign, re-implementing, retesting, and re-certifying the rest of the system when defect cores are replaced by redundant ones. For a given application that has already been mapped to a manycore platform, if some of the cores on which application tasks are deployed become defective, with core-level redundancy, redundant cores are used to replace the defect cores. However, this reconﬁguration will change the physical distance between two communicating tasks, which may further impact the application’s timing behavior and cause timing variations. 978-1-4673-0772-7/12/$31.00 ©2012 IEEE 493 6A-3 Therefore, the question is how to select redundant candidates for the defect cores that minimize the timing variation. When on a small scale NoC, i.e., both the number of redundant cores R and the number of defect cores D are small, we can traverse all the choices and ﬁnd the optimal one ofﬂine with time cost of O(RD ). However, in large scale manycore systems, for example, when there are thousands of cores per chip, the time cost is unaffordable even if it is computed ofﬂine. In this paper, we focus on homogeneous manycore platforms and have made two major contributions: ﬁrst, we have developed a metric that measures timing similarities between two different topologies upon which a real-time application is deployed. Second, we have developed a polynomial time algorithm to form a defect-free virtual topology that is most similar, with respect to the metric developed, to the application’s initial reference topology. The rest of the paper is structured as follows. Prior research is discussed in Section II. Section III discusses how to estimate timing behavior changes caused by defective core replacement. Section IV gives a brief background on generic Assignment Algorithm (also known as Hungarian method). Our proposed algorithm based on the Hungarian method to solve the replacement problem is presented in Section V. The evaluations of our work are shown through experiments in VI. Finally, we conclude and point out future work in Section VII. I I . R E LAT ED WORK The NoC topology virtualization problem for general purpose computing has recently drawn great attention from the research community. Zhang et. al in [2], [3] studied the performance degradation of virtual topologies when compared to the topology initially designed. A heuristic approach called Row Rippling Column Stealing (RRCS) is proposed in [2] for homegenerous manycore processors. The essence of the heuristic is to maintain the physical regularity of reconﬁgured virtual topologies in both row and column units, and hence to maximize performance. They further extended the work to handle heterogeneous manycore processors [4]. Different optimization goals when reconﬁguring the NoC topology have also been considered. For instance, Srinivasan [5] gives the N M AP algorithm to minimize the average communication delay with the bandwidth constraints for NoC platforms; Hung [6] proposes IP virtualization and placement algorithm to achieve a thermally balanced design that minimizes temperature and energy consumption; and Hu [7] presents an efﬁcient branch-and-bound algorithm to minimize the total communication energy through bandwidth reservation. As pointed out by Flich in [8] the three key metrics: performance, fault-tolerance (including yield), and power consumption are becoming the major concerns on the design of NoC systems. Research in this area have shown that topology virtualization is a promising technique to provide a uniﬁed solution to address all the three key issues. Different from the prior work listed above, we focus on topology reconﬁguration for applications with timing constraints. For these types of applications, rather than performance, the timing predictability and determinism are the paramount requirements. Hence, the reconﬁguration objective is to use redundant cores provided on the chip to replace defective cores with the constraint of maximizing the timing resemblance of the newly conﬁgured topology to the initial one. It is not difﬁcult to see that the problem of ﬁnding appropriate redundant cores to replace defect cores belongs to the class of the assignment problems [9]. As a canonical solution to the assignment problem, the Hungarian method, which is ﬁrst proposed by Harold [9] has been widely used. For instance, Jung-Hoon [10] uses it for solving resource allocation problem in mobile communication system and Gungor [11] applies it for multiple criteria assignment problems. However, the topology reconﬁguration problem with timing similarity constraint we are to address is not identical to the general assignment problem, in which, assignments are independent of each other and the cost of each assignment is ﬁxed. In our problem, if more than one defective core exists, the cost of one’s replacement may be dependent with others. We will address this issue in section V. I I I . T IM ING S IM I LAR I TY B E TW E EN TWO V IRTUA L NOC TO PO LOG I E S FOR A G IV EN R EA L - T IM E A P P L ICAT ION In this section, we give a motivating example, deﬁne the timing similarity metric and give the problem formulation. Before giving an example, we ﬁrst deﬁne our application and NoC models. Deﬁnition 1 (NoC): We represent the NoC as an array of physical cores under mesh topology and deterministic XY routing where • C = {C0 , C1 , ..., CM } are the cores provided by the chip. We use C(i) and Ci to denote virtual core and physical core i respectively. • R = {R0 , R1 , ..., RN } are the redundant cores provided by the chip. • Virtual Topology T : C → C ∪ R is an injective mapping from virtual cores to physical cores. (i),(j ) is the number of hops from C(i) to C(j ) along the physical path of the trafﬁc using XY routing algorithm for a given virtual topology T k . • H k (cid:2) Deﬁnition 2 (Mapped Application Task Graph): A directed acyclic graph A = (J, E ) mapped to a virtual topology where each vertex ji ∈ J represents the task mapped to virtual core C(i) and each edge eij ∈ E is a data dependency between ji and jj . The volume of data sent along the directed edge eij is given by v(eij ). We make two assumptions regarding the NoC. First, only the cores C become defective. Second, there is no contention in the network. In other words, the NoC bandwidth is sufﬁciently large. When there is no network contention, the hop count becomes the most important metric for communication times. (cid:2) 494 A. A Motivating Example For a given 3x3 NoC with 3 redundant cores as shown in Fig.1(c). If, for instance, core C4 is defective, the remaining defect-free cores form a new physical topology. We can virtual 3 × 3 mesh topology. There are several ways to do virtualize the defect-free cores and provide applications a so, for instance, in Fig.1(c), the three redundant cores, R0 , R1 , or R2 , can be used to replace the defective core C4 , and hence generate three different virtual topologies. Even though applications are given a uniﬁed 3 × 3 mesh, different virtual topologies have different properties, such as latency and throughput. For instance, given the application and its initial mapping shown in Fig.1(a), consider if the physical core C4 is defective. It can be replaced by one of the redundant cores R0 , R1 , or R2 . Depending on which redundant physical core is used, the resulting topologies’ hop count between the virtual cores are different under XY routing(Table I). From the table it is clear that using R0 as the virtual core C4 is the best choice if the similarity metric is hop count. Furthermore, if an application is deployed on a topology, the impact on the application of replacing a defective core by different redundant cores also varies. 6A-3 Trafﬁc Between Virtual Cores C(1) → C(4) C(1) → C(4) C(1) → C(4) Corresponding Physical Cores C1 → R0 C1 → R1 C1 → R2 Hop Count 2 3 4 TABLE I: Virtualizing different redundant cores for C(4) B. Similarity Metric In order to analyze the timing resemblance between a reference topology where there is no defect cores and a virtual topologies, the timing behavior of a NoC-based manycore system upon which a speciﬁc application is deployed has to be ﬁrst deﬁned. As for homogeneous manycore systems, processor speeds are the same. Hence, on-chip communication becomes a dominant factor that differentiates various virtual topologies’ timing behavior. We use trafﬁc time delay F(i),(j ) to quantify the communication time cost from virtual cores C(i) to C(j ) . The formal deﬁnition is given below: Deﬁnition 3 (Trafﬁc Time Delay): For a given application and virtual topology T k , the trafﬁc time delay from virtual core C(i) to core C(j ) is deﬁned as F k (i),(j ) = v(eij ) + H k (i),(j ) (1) (cid:2) For two different virtual topologies, T k and T r , their trafﬁc time delay differences from virtual core C(i) to C(j ) can be calculated by (2) (i),(j ) = |F k (i),(j ) − F r (i),(j ) | Δk,r (2) Clearly, the smaller the trafﬁc time delay differences among all virtual core pairs, the higher the timing similarity among the two topologies. We use normalized average value (Ave) and normalized variation (V ar) to model the trafﬁc time delay difference of the entire topology. Their deﬁnitions are given below: Deﬁnition 4: (Normalized Average Trafﬁc Time Delay Difference) Given an application, a new topology T k and a reference topology T r , the normalized average trafﬁc time delay difference is given by (3) (cid:4) Δk,r (i),(j ) eij ∈E Ψr |E | Avek r = where Ψr is the average trafﬁc time delay of the reference topology T r given by (4). (cid:4) eij ∈E F r (i),(j ) Ψr = |E | (3) (4) (cid:2) Fig. 1: Mapping application to cores Generally, for a given application that is mapped to |J | cores supported by a physical topology with |R| redundant cores, if among the |J | cores, p (p ≤ min{|J |, |R|}) cores become (cid:2)|R| defective, to replace only those defective cores with redundant cores, we have p! number of different choices to form a new virtual topology. Then the question is: which one should we choose? (cid:3) p Deﬁnition 5: (Normalized Average Variation of Trafﬁc Time Delay Difference) 495 6A-3 Given an application, a new topology T k and a reference topology T r , the normalized variation of trafﬁc time delay difference is given by (5) (cid:5)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:7) (cid:4) eij ∈E ( Δk,r (i),(j ) Ψr |E | − Avek r )2 V ark r = where Ψr is given by (4). (5) (cid:2) (7) (cid:2) Based on the trafﬁc time delay difference between different topologies, we deﬁne virtual topology timing similarity as weighted sum of normalized average difference and variation. The formal deﬁnition is given below: Deﬁnition 6: (Virtual Topology Timing Similarity) Given an application, a new topology T k and a reference topology T r , their timing similarity χ(T k r ) is deﬁned by (6) r ) = wa × Avek r + wv × V ark χ(T k r (6) where wa and wv (wa + wv = 1) are the weights applied to the average difference and average variation, respectively. (cid:2) C. Problem Formulation With the similarity metric, we formulate the problem the paper is to address as follows: Problem 1: For a given reference topology T r on which a given application A is deployed, if there are cores used by the application A that become defective, construct a virtual topology T opt with defective cores being replaced by redundant cores and satisfying the requirement (7) χ(T opt r ) = min{χ(T k r )} IV. TH E HUNGAR IAN M E THOD For self-containment, in this section, we brieﬂy introduce the assignment problem and the Hungarian method that solves the assignment problem. The assignment problem can be stated as [9]: there are n workers and k jobs, where k ≤ n. Each job can only be assigned to a single worker. The time taken by each worker to ﬁnish a job is independent and can be different. The assignment problem is to ﬁnd a worker to job assignment so that the total time taken to ﬁnish all the jobs is minimized. The Hungarian method [9], [10], [11] solves the assignment problem in two steps: Step 1 : constructs a cost matrix Mn×n where mij is the time cost for worker i (1 ≤ i ≤ n) to work on job j (1 ≤ j ≤ k). If n (cid:6)= k , i.e., the numbers of workers and the number of jobs are not the same, we augment the matrix M to a square one by adding (n − k) number of columns ﬁlled with zeros. Step 2 : uses equivalent matrix reduction to obtain the optimal assignment with respect to the cost matrix [9] ⎤ ⎦ ⎤ ⎦ As an example, assume we have three workers W1 , W2 , and W3 , and three jobs J1 , J2 , and J3 . It takes W1 25, 40, and 35 time units to ﬁnish job J1 , J2 , and J3 , respectively; for W2 , 40, 60, and 35 and W3 , 20, 40, and 25 to ﬁnish the three jobs, respectively. We have the cost matrix M3×3 as: ⎡ ⎣ 25 40 20 40 60 40 35 35 25 M3×3 = By iteratively reducing the value in rows and columns, the Hungarian method is able to get the optimal assignment in O(n3 ) time [12]. For the given cost matrix, the reduced matrix of running the Hungarian Algorithm is ⎡ ⎣ 0 5 0 0 10 5 10 0 5 M3×3 = Hence, the optimal assignment is W1 , W2 , and W3 for J2 , J3 , and J1 , respectively. V. HUNGAR IAN M E THOD BA S ED V IRTU L I ZAT ION A LGOR I THM As formulated in Section III-C, we need to assign appropriate redundant cores to replace defect cores for a given application so that the communication time change among tasks is minimized. It is not difﬁcult to see that, it is also a type of assignment problem. Recall that the ﬁrst step in using the Hungarian method to solve an assignment problem is to form a cost matrix. It is worth pointing out that in the worker/job assignment problem, the time cost for a worker i to perform a job j is ﬁxed and independent from how the job is assigned to other workers. Hence, in order to use the Hungarian method, we have to ﬁrst construct a cost matrix. For defect-redundant core assignment problem, the related cost of replacing defective core d with redundant core r , i.e., Mdr is the communication time delay difference among all the virtual communicating cores. Unfortunately, if there is more than one defective core to be replaced, we will not be able to construct the matrix due to lack of information in computing the trafﬁc delay difference χ given by (6). For example, if physical cores C4 , C5 and C7 in Fig. 1 are defective then redundant cores R0 , R1 , R2 can be used replace them. We need to build up a 3 × 3 matrix, and the entry in the ﬁrst row and ﬁrst column indicating the timing difference when using physical core R0 to replace physical defective core C4 , which needs the trafﬁc time delay between virtual core C(7) and C(5) after reconﬁguration. Unfortunately, these values are not known until we know which redundant cores are assigned to the defective cores. One way to overcome the problem is to assume that there is only one defective core at a time. In other words, when deciding the cost values for replacing defective core f , we treat the other defective cores (if any) as normal defect-free cores. Using the above example, when ﬁlling the entry indicating 496 physical core R0 to replace defective core C4 , we assume physical cores C7 and C5 are not defective. Once the cost matrix with respect to communication time delay differences is obtained, we can use the Hungarian method to ﬁnd the optimal replacement for the defective cores. The Hungarian method based virtualization (HMBV) algorithm is given in Algorithm 1. Algorithm 1 HMBV Algorithm(T r ) 1: for each defective core Ci do for each redundant core Rj do calculate χ(T i where T i maps core C(i) to Rj 2: 3: r ) Mij ← χ(T j i ) 4: 5: end for 6: end for 7: apply Hungarian Algorithm to obtain the solution for cost matrix M The steps 1 − 6 are to construct the cost matrix which has time complexity of O(n2 ), and step 7 uses the general Hungarian algorithm to ﬁnd the defect-redundant mapping solution which is of time complexity O(n3 ). Therefore, the time complexity for the algorithm is O(n3 ). Let us use the example shown in Fig. 1 to illustrate the algorithm. Again, assume the core C4 , C5 and C7 are defective, steps 1 − 6 in algorithm 1 generate the cost matrix ⎡ ⎢⎢⎣ M3×3 = Def ect C ore C4 C5 C7 R0 R1 R2 0.2 0.2727 0.3333 0.0929 0.1245 0.1899 0.1111 0.1724 0.2429 ⎤ ⎥⎥⎦ Apply the Hungarian method to the cost matrix, we have that the new conﬁguration, where R0 is used to replace C4 , R2 for C5 , and R1 for C7 , respectively, most resembles to the initial conﬁguration with respect to the communication time delays among communicating tasks. Discussion Admittedly, the cost matrix generated under the assumption, i.e., when computing the cost values for a speciﬁc defective core, all other defective cores are treated as defect-free, may not reﬂect the whole property of the problem we are to solve. It is not difﬁcult to see that the defect-redundant core assignment problem in its original form is of time complexity O(|R|D ), where D and |R| are the number of defect cores and redundant cores, respectively. The HMBV algorithm of complexity O(|R|3 ), hence, does not generate the optimal solution for our defect-redundant core assignment problem. In the next section, we will empirically evaluate both the similarity metric and the HMBV algorithm. V I . EVA LUAT ION In this section, we perform two sets of experiments. First we evaluate the metric given in (1) by comparing it with the simulated communication time. Second we evaluate the 6A-3 Fig. 2: Model Accuracy remapping obtained by the Hungarian algorithm by comparing it to both the optimal solution and a randomly generated solution. A. Experiment Setup NIRGAM (NoC Interconnect Routing and Application Modeling) [13] is a modular and cycle accurate simulator developed in SystemC. In NIRGAM, a 2D NoC can be simulated by different design options, e.g., virtual channels, clock frequency, buffer parameters, routing mechanisms and applications patterns, etc. Each NIRGAM tile consists of various components, such as input channel, controller, virtual channel allocator, output channel controller, and IPcores. Each IPcore is attached to a router/switch by means of a bidirectional core channel. Wormhole switching and deterministic XY routing are used on the mesh. In our experiments we use TGFF[14] to generate a task graph and create a random mapping of the task to a 5x5 mesh. The mapping is random because we do not assume anything about the original mapping of the application and only aim to achieve similarity. We assume that there is an extra column redundant cores for an n × n virtual topology. The location of cores located on the right side of the chip to provide n and number of the redundant cores is not important however as the reconﬁguration algorithm is aware of their locations. To satisfy the assumption that no contention exists in the network, the ﬂit injection rate of each task is sufﬁciently low so that each router can handle all of the trafﬁc in the application without contention. B. Metric Evaluation In this experiment we show that the metric given in (1) is a good model for the communication time in the application. For an edge in a application eij ∈ E with data volume v(eij ) the communication time is the amount of time from when ji begins transmission of data to when jj receives the last ﬂit in v(eij ). Since the NoC is homogeneous the execution time is the same on any core. Therefore with similar communication times along every edge the start and ﬁnish time of tasks will be similar. 497 deadline satisfactions. Real-time and embedded applications often have other resource constraints, such as peak temperature and energy consumption constraints, the virtualization problem becomes more challenging when these concerns must be taken into consideration. This is another area of our future study. Further extension to the work is to address these concerns for heterogeneous manycore systems. "
2013,MD - Minimal path-based fault-tolerant routing in on-Chip Networks.,"The communication requirements of many-core embedded systems are convened by the emerging Network-on-Chip (NoC) paradigm. As on-chip communication reliability is a crucial factor in many-core systems, the NoC paradigm should address the reliability issues. Using fault-tolerant routing algorithms to reroute packets around faulty regions will increase the packet latency and create congestion around the faulty region. On the other hand, the performance of NoC is highly affected by the network congestion. Congestion in the network can increase the delay of packets to route from a source to a destination, so it should be avoided. In this paper, a minimal and defect-resilient (MD) routing algorithm is proposed in order to route packets adaptively through the shortest paths in the presence of a faulty link, as long as a path exists. To avoid congestion, output channels can be adaptively chosen whenever the distance from the current to destination node is greater than one hop along both directions. In addition, an analytical model is presented to evaluate MD for two-faulty cases.","1B-3 MD: Minimal path-based Fault-Tolerant Routing in  On-Chip Networks  Masoumeh Ebrahimi1, Masoud Daneshtalab1, Juha Plosila1, Farhad Mehdipour2 1Department of Information Technology, University of Turku, Finland  2E-JUST Center, Kyushu University, Japan  {masebr, masdan, juplos}@utu.fi, farhad@ejust.kyushu-u.ac.jp  Abstract— The communication requirements of many-core  embedded systems are convened by the emerging Network-onChip (NoC) paradigm. As on-chip communication reliability is a  crucial factor in many-core systems, the NoC paradigm should  address the reliability  issues. Using fault-tolerant routing  algorithms to reroute packets around faulty regions will increase  the packet latency and create congestion around the faulty  region. On the other hand, the performance of NoC is highly  affected by the network congestion. Congestion in the network  can increase the delay of packets to route from a source to a  destination, so it should be avoided. In this paper, a minimal and  defect-resilient (MD) routing algorithm is proposed in order to  route packets adaptively through the shortest paths in the  presence of a faulty link, as long as a path exists. To avoid  congestion, output channels can be adaptively chosen whenever  the distance from the current to destination node is greater than  one hop along both directions. In addition, an analytical model is  presented to evaluate MD for two-faulty cases.  I. INTRODUCTION As is predicted by Moore’s law, over a billion transistors could  be integrated on a single chip in the near future [1]. In these chips,  hundreds of functional intellectual property (IP) blocks and  embedded memory modules could be placed together to form a  Multi-Processor Systems-on-Chip (MPSoCs) [1]. By increasing  the number of processing elements in a single chip, traditional  bus-based architectures in MPSoCs are not useful any longer and  a new communication infrastructure is needed. Network-on-Chip  (NoC) has become a promising  solution  for on-chip  interconnection in many-core Systems-on-Chip (SoC) due to its  reusability and scalability [2][3][4].   On-chip interconnects implemented with deep submicron  semiconductor technology, running at GHz clock frequencies are  prone to failures [2][5][6]. Due to this extreme device scaling, the  likelihood of failures increases [17]. Two different types of faults  that can occur in NoCs are transient and permanent. Transient  faults have unpredictable causes (e.g. power grid fluctuations,  particle hits) and are often difficult to be detected and corrected.  Permanent faults are caused by physical damages such as  manufacturing defects, device wear-out. In this paper, our focus is  on permanent faults. Routing techniques provide some degrees of  fault  tolerance  in NoCs. Routing algorithms are mainly  categorized into deterministic and adaptive approaches [7][8][9].  A deterministic routing algorithm uses a fixed path for each pair  of nodes, resulting in increased packet latency especially in  congested networks. Implementations of deterministic routing  algorithms are simple but they are unable to balance the load  across the links in non-uniform traffic. The simplest deterministic  routing method is dimension-order routing which is known as XY  or YX algorithm. The dimension-order routing algorithms route  packets by crossing dimensions in strictly increasing order,  reducing to zero the offset in one direction before routing in the  next one. In contrast, in adaptive routing algorithms, a packet is  not restricted to a single path when traveling toward a destination  node. So they can decrease the probability of routing packets  through congested or faulty regions. In sum, unlike deterministic  routing algorithms, adaptive routing algorithms could avoid  congestion in the network and provide better fault-tolerant  characteristics by utilizing alternative routing paths.   In wormhole routings, messages are divided into small flits  traveling through the network in a pipelined fashion. This  approach eliminates the need to allocate large buffers in  intermediate switches along the path [10]. Moreover, in wormhole  routing, message latency is less sensible to distance. However, it  should be used with special care to avoid deadlock in the network.  Deadlock is a situation when the network resources continuously  wait for each other to be released. Routing algorithms are  required to be deadlock-free and break all cyclic dependencies  among channels. Virtual channels are mainly used in the network  for avoiding deadlock, increasing performance and tolerating  faults, but it is an expensive solution.   Conventional fault-tolerant routing algorithms reroute packets  around faulty regions, either convex or concave, so that the  selected paths are not always the shortest ones. However,  rerouting is an expensive solution and considerably increases  packet’s  latency and router’s complexity. In addition  the  information about faulty components is insufficient or the way of  utilizing them is inefficient. On the other hand, most of the  presented fault tolerant algorithms are limited to deterministic  routing algorithms, resulting in considerable performance loss. In  this paper, we present a minimal and defect-resilient (MD)  routing algorithm where the key ideas are threefold. First, it can  tolerate all one-faulty links using a minimal path between each  pair of source and destination nodes, if a minimal path exists.  Second, to avoid congestion, output channels can be adaptively  chosen whenever the distance from the current to destination node  is greater than one hop along both directions. Third, to evaluate  the presented routing for two faults condition, an analytical model  is introduced and analyzed.  The rest of this paper is organized as follows: Section II  reviews the related work, while the underlying fully adaptive  routing algorithm is also discussed in this section. Fault  distribution mechanism, bypassing faulty links and the proposed  fault-tolerant algorithm are explained in Section III. The results  are given in Section IV while we summarize and conclude in the  last section.  978-1-4673-3030-5/13/$31.00 ©2013 IEEE 35 II. RELATED WORK Fault-tolerant routing algorithms can be classified into two  main groups: one can handle convex or concave regions  [11][12][13][14] and the other utilizes the contour strategy for  addressing faults [15][16]. The basic assumptions in all of these  methods are the permanent faulty cases. The methods in the first  group are based on defining fault ring (f-ring) or fault chain (fchain) around faulty regions where healthy nodes are disabled in  order to form a specific shape. A reconfigurable routing algorithm  using the contour strategy provides the possibility of routing  packets through a cycle free surrounding a faulty component. The  presented algorithm in [15] is able to tolerate all one-faulty  routers in 2D mesh network without using virtual channels and  disabling healthy nodes. However, to support more number of  faulty routers, the contours must not be overlapped and thus  faulty routers should be located far away from each other. This  algorithm is deterministic and does not make any effort toward  alleviating congestion in the network.   Fault-tolerant routing algorithms could be also divided into  two classes: the methods using virtual channels [16][17][18] and  those without using virtual channels [19][20]. In general, different  methods define a new tradeoff between the number of virtual  channels, the ability to handle different fault models, and the  degree of adaptiveness. The virtual channel-based fault-tolerant  routing algorithms provide better fault-tolerant characteristics  than those without virtual channels. The methods that do not use  any virtual channel are mainly based on the turn models [21][22].  In turn models, some turns are eliminated in order to guarantee  the deadlock freeness in the network and then the remaining turns  are used both for routing packets and tolerating faults.   In this paper, the fault information is distributed and utilized in  such a way that packets can be routed through the shortest paths  in the presence of faults. This method can be used with any  number of virtual channels in the network. However, in order to  keep the area overhead low, we use two virtual channels in each  direction. The proposed method not only is able to tolerate all  one-faulty links but also all packets can be routed through the  shortest paths as long as any path exists. Moreover, the algorithm  is adaptive and in most cases, packets have alternative choices to  reach the destination node.  A. Dynamic XY Routing Algorithm  Our proposed method is based on a fully adaptive routing  algorithm. To provide this requirement, we take advantage of the  Dynamic XY (DyXY) method. DyXY is an adaptive and  deadlock free routing algorithm proposed in [23]. In this  algorithm, which is based on the static XY algorithm, a packet is  sent either to the X or Y direction depending on the congestion  condition. It uses local information (i.e. the current queue length  of the corresponding input buffer in the neighboring routers) to  decide on the next hop. It is shown that the usage of this local  information leads to a lower latency path from the source to the  destination node. Fig. 1 shows an example of the DyXY method  where the nodes S and D are the source and destination of the  packet. In the DyXY method, at the source node S, the number of  free buffer slots at the west input buffer of node 1 is compared  with that of the south input buffer of node 4. The packet is sent in  a less congested direction that is the east direction in this  example. When the packet arrives at node 1, it has to be delivered  toward the destination node through either node 2 or node 5.  1B-3 According to the congestion condition shown in Fig. 1, the node 5  is less congested and thus the packet is delivered to it. In this way,  the packet chooses a less congested direction at each routing step  until it reaches the destination node.  Due to the fact that packets can be routed in both X and Y  directions, there is possibility of deadlock. DyXY uses two virtual  channels along the Y direction and one virtual channel along the  X direction. This algorithm avoids deadlock as follows: The  network is partitioned into two subnetworks called +X and –X,  each having half of the channels in the Y dimension. If the  destination node is on the right of the source, the packet will be  routed through the +X subnetwork. If the destination node is on  the left of the source, the packet will be routed through the -X  subnetwork. When the destination is in the north or south of the  source node, either subnetwork can be used. Without the loss of  generality, in this work we take advantage of two virtual channels  in each direction. The first virtual channel is used for the eastward  packets and the second virtual channel is utilized for the westward  packets. 12 13 14 8 4 S 9 5 1 D 6 2 15 11 7 3 Fig. 1. Dynamic XY (DyXY) routing algorithm  III. MD: THE PROPOSED APPROACH A. Fault Distribution Mechanism  We present a new fault distribution mechanism and the method  of utilization which avoids taking unnecessary non-minimal  paths. As shown in Fig. 2(d), the fault information is distributed  in a way that each router is informed about the fault condition in  two-hop links. For this purpose, each router transfers the fault  information on its direct links to the neighboring nodes. Note that  in this figure, E, W, N, and S stand for the East, West, North, and  South directions. In Fig. 2(a), the neighboring node in the north of  the current node (C) transfers the fault information on its links in  N, E, and W directions to the current node. Accordingly, the  current node would be informed about the fault information in its  N, NN, NE, and NW paths. In Fig. 2(b), by receiving the fault  information from the neighboring node in the east direction, the  current node knows about the fault information of E, EE, EN, and  ES paths as well. Similarly, in Fig. 2(c), this knowledge is  extended to know about the fault information on the links in S,  SS, SE, and SW paths. Finally, as shown in Fig. 2(d) by receiving  the information from the neighboring node in the west direction,  the current node has the information about the links in E, EE, EN,  ES, W, WW, WN, WS, N, NN, NE, NW, S, SS, SE, and SW  paths in total. For routing a packet in the northeast direction, for  instance, a router uses the fault information on the corresponding  minimal paths (e.g.  EE, EN, NN, and NE). Similarly, for a  southwest packet, the fault’s information on some paths (e.g.  WW, WS, SS, and SW) is beneficial for making a reliable routing  decision. Using this information, packets are routed through  minimal and non-faulty paths which avoids making unnecessary  routing around faulty components.   36 1B-3 Fig. 2. The fault distribution mechanism B. Bypassing Faulty Links   Regarding the relative position of the source and destination  nodes, a packet can be sent in eight directions: north, south, east,  west, northeast, northwest, southeast, and southwest. By using the  DyXY method and the explained distribution mechanism (Fig. 2),  we show that the packets destined for northeast, northwest,  southeast, and southwest directions, take only the shortest paths in  the presence of a faulty link in the network. Therefore, no  rerouting takes place in these cases and the algorithm remains  deadlock free. However, for eastward, westward, northward, and  southward packets, non-minimal paths must be taken if a faulty  link exists in the path.  1. Northeast, Southeast, Northwest, and Southwest Directions Using the DyXY method, all shortest paths in the east direction  are valid for eastward packets. Similarly, westward packets can  utilize all shortest paths in the west direction. When the destination  is in the northeast position of the current node, the packet can be  delivered in either the north or east direction. As illustrated in Fig.  3(a), the distances along both east and north directions are one. On  the other hand, according to the distribution mechanism, the  current node is informed about the faulty condition of the links in  EN and NE paths. Using this information, if a link is faulty in  either the NE or EN path, the other shortest path is selected by the  routing unit. As a result, the packet is always routed through a  minimal path to the destination.   The example in Fig. 3(b) shows the case where the distance  along the X dimension reaches one. The current node knows the  information about EN, NE, and NN paths which are located in the  minimal path. The packet can be delivered in the east direction if  the EN path is non-faulty. However, by this choice, the distance  along the X dimension reaches zero and the packet has to take the  Y direction in the remaining path toward the destination node.  Thereby, if there is a faulty link in the Y dimension, the packet  must take a non-minimal path to bypass the fault. This is not an  optimal solution which is addressed by the presented algorithm.  MD avoids reducing the distance into zero in one direction when  the distance along the other direction is greater than one. In other  words, when the distance between the current and destination  nodes reaches one in at least one dimension, at first all the possible  shortest paths on the greater-distance dimension are checked. The  packet is sent along the greater-distance dimension if any minimal  and non-faulty paths exist; otherwise the links on the smallerdistance dimension are examined. Thereby, in Fig. 3(b) the  availability of NN and NE paths is checked before that of EN. If  either the NN or NE path is healthy, the packet is sent through it.  In the next hop, the packet faces the similar situation as in Fig.  3(a), and thus only the shortest paths are selected by MD so far. In  another case of Fig. 3(b), when both NN and NE paths are faulty  (i.e. the north link is faulty), the packet is routed through the east  direction and is sent to the destination using the shortest path (i.e.  we assume there is one faulty link in the network). Similarly, in  Fig. 3(c) the conditions of EE and EN paths are examined earlier  than the NE path. Finally, in Fig. 3(d), the packet can be delivered  through the east direction if the EE or EN path is non-faulty or it  can be sent through the north direction when either the NN or NE  path is healthy. By these choices, the packet faces the similar  situation as in Fig. 3(b) or Fig. 3(c) and thereby only the shortest  paths are taken by MD in all cases.   Fig. 3. Bypassing faulty links when the destination is located in the  northeast position of the source node (Note that numbers determine  the priority of different paths)  2. East, West, North, and South Directions  As is already mentioned, when a packet is eastward, westward,  northward, or southward and there is a faulty link in the path, the  packet must be routed through a non-minimal path and turned  around the faulty link. As illustrated in Fig. 4(a) for the eastward  packet, at first the east link is checked and if it is healthy, the  packet is sent through this direction. However, if the link is faulty,  the packet is delivered to the north or south direction with the  same priority. The situation is similar for the westward packet 37 1B-3 An example is shown in Fig. 5(a), when the source and  destination are located at nodes S and D while the link (11,D) is  faulty. As can be seen by the figure, there are several paths that  can be taken by MD. The simple rule is that the distance along  each direction should remain greater than or equal to one. The  number of free buffer slots is used to select among the output  channels at each router. As illustrated in Fig. 5(b), the adaptivity  options of the non-faulty case are nearly similar to the faulty case  (Fig. 5(a)). Fig. 5. Alternative paths from source node S to destination D  Fig. 6 shows an example of comparing MD with traditional  methods which are based on contour strategy. Both methods select  the output channels based on the congestion condition and fault  information. Let us assume that the detour-based method knows  only about the fault information in its direct links. In this figure, a  packet is sent from the source node S to the destination node D  when the links (S,1), (7,11), (8,9), (8,12), and (14,D) are faulty.  This example shows that MD can support some multiple faulty  links still using the shortest paths. At the source node, the north  direction is selected by both methods since the east link is faulty.  At the node 4, the detour-based method (Fig. 6(a)) may deliver the  packet in the north direction (unaware of the faults in two-hop  links) where the packet faces the faulty links and has to be  returned to the node 4. If the packet arrives at the node 6, it might  select the intermediate node 7 as the next hop. However, the node  7 has a faulty link in the north direction and returns the packet to  the node 6. The packet might reach the node 14 where the east link  is faulty and thus it has to be rerouted. In sum, the packet takes  several unnecessary non-minimal paths which increase the latency.  In contrast, at the node 4, MD (Fig. 6(b)) is aware of the faulty  links at the NN and NE paths, so it delivers the packet in the east  direction instead of the north direction. By arriving the packet to  the node 6, MD avoids reducing the distance to zero along the X  dimension (also it notices that the EN path is faulty), so it delivers  the packet to the node 10. At this node, the packet is sent through  the east direction since it knows that the NE path is faulty.  Fig. 6. Comparison of (a) a detour-based method with (b) MD  38 Fig. 4. Bypassing faulty links when the destination is located in the  (a) east (b) west (c) north, and (d) south positions of the source node  (Fig. 4(b)). In this case, the fault information in the west direction  is checked before those of north and south directions. When the  packet is northward (Fig. 4(c)) and the north link is faulty, the east  direction is checked earlier than the west direction. It means that  the west direction is used only when the faulty link is located in  the rightmost border. A similar perspective is applied to southward  packets (Fig. 4(d)). It is worth mentioning that for the northward  and southward packets, only the first virtual channel is used unless  the packet is generated at one of the nodes in the rightmost border  (i.e. the second virtual channel is utilized).   C. Adaptive Fault-Tolerant Algorithm   A deterministic routing algorithm is a common method used in  traditional fault tolerant methods since the path of a packet is  predictable. However, in our algorithm we use the deterministic  routing only when a packet gets close to the area of faulty link.  Based on MD, if the distance from the current to destination node  is greater than one hop along both directions, packets can  adaptively choose among the non-faulty links without any  restriction. According to DyXY, a packet is sent in a direction  which has a more number of free slots in the corresponding input  buffer. When both directions have the same number of free buffer  slots, a direction is chosen by random. On the other hand, MD tries  not to reduce the distance in one dimension to zero while the  distance along the other dimension is greater than one. To step  toward this goal, we try to keep the distances along both directions  as equally as possible. However, forcing a packet to choose a  specific direction is against the adaptive characteristic. Our  solution to this issue relies on sending a packet in a desired  direction (greatest-distance dimension) whenever the number of  free buffer slots are nearly equal in both directions. The equality  situation might happen in a few cases, but as it is obvious the  number of free buffer slots does not need to be exactly the same.  For instance, four or five free buffer slots out of eight available  slots can be seen similar in terms of affecting the performance.  Therefore, when the difference between the number of free slots in  two input buffers is less than or equal to two flits, the packet is  sent in the desirable direction.     1B-3 injections into the network over the total number of injection  attempts. The simulator is warmed up for 12,000 cycles and then  the average performance is measured over another 200,000 cycles.  To have a fair comparison, we defined our baseline as a detour  strategy similar to [16]. Like MD, the baseline method has two  virtual channels along X and Y directions, respectively. Unlike  MD, the baseline method may take unnecessary longer paths as  discussed but it is able to support all one-faulty links.  A. Performance Analysis under Uniform Traffic Profile  In the uniform traffic profile, each processing element (PE)  generates data packets and sends them to another PE using a  uniform distribution [21]. The mesh size is considered 4×4. In Fig.  8(a), the average communication latencies of MD, the baseline  method and DyXY are measured for a fault-free case. In addition,  the latencies of the MD and baseline methods are compared in a  one-faulty link case. As observed from the results, in a fault-free  case, DyXY performs better than MD. The reason is that DyXY  method is a fully adaptive routing algorithm while MD limits  packet adaptivity when packets get close to the destination node.  The baseline method shows  larger  latency since  it  is a  deterministic method. In a one-faulty case, MD performs better  than the baseline method. This is due to the fact that MD can route  packets through the shortest paths while in the baseline method,  packets may take longer paths when facing a faulty link.   B. Performance Analysis under Hotspot Traffic Profile   Under the hotspot traffic pattern, one or more nodes are chosen  as hotspots receiving an extra portion of the traffic in addition to  the regular uniform traffic. In simulations, given a hotspot  percentage of H, a newly generated message is directed to each  hotspot node with an additional H percent probability. We  simulate the hotspot traffic with a single hotspot node at (2, 2) in  4×4 2D mesh. The performance of the MD, the baseline method,  and DyXY is also measured for fault-free and one-faulty (except  DyXY) link cases. The performance of each network with H =  10% is illustrated in Fig. 8(b). As observed from the figure, in the  hotspot traffic and in both faulty and non-faulty cases, the  performance improvement of MD is better than the detour-based  scheme.  C. Hardware Analysis  To assess the area overhead and power consumption, the  whole platform of each method is synthesized by Synopsys Design  Compiler. We compared  the area overhead and power  consumption of MD with the baseline and DyXY methods [23].  The power consumption of DyXY is measured only in the faultfree case while the other two methods tolerate one faulty link.  Each  scheme  includes network  interfaces,  routers, and  communication channels. For synthesizing, we use the UMC  90nm technology at the operating frequency of 1GHz and supply  voltage of 1V. We perform place-and-route, using Cadence  Encounter, to have precise power and area estimations. The power  dissipation is calculated using Synopsys PrimePower in a 6×6 2D  mesh. The layout area and power consumption of each platform  are shown in Table II. As indicated on the table, MD has a larger  area overhead than DyXY and a lower one than the baseline  method. It is because of using a simpler routing unit at the DyXY  method and a more complex one in the baseline method. As  indicated on the table even if the MD has to support a one-faulty  link (while DyXY is fault-free), the power consumption of MD  remains relatively small. This is due to the fact that MD could  route packets through the shortest paths and thus consuming less  power.   Fig. 7. Robustness analysis, specified links must be healthy D. Analytical Analysis of Two-Faulty Situations  Since MD could not support all two-faulty cases, we take  advantage of analytical models to evaluate the reliability of MD  when two faults occur in the network. Let us assume that the  location of faults is chosen randomly. We divide the problem into  two cases: the first fault occurs on the border links or it occurs in  central links. If the first fault occurs on one of the border links, the  second fault must not happen on some specific locations. For  example, in Fig.7 (a), if the link 2 is faulty, the specified locations  in the figure must be healthy. A similar situation exists for all  borderline links (i.e. 4(n-1) in n×n network). The probability that  the first faulty link occurs on the border links and the second one  does not locate in specific locations is calculated as follow:  (cid:886)(cid:4666)(cid:1866) (cid:3398) (cid:883)(cid:4667) (cid:884)(cid:1866)(cid:4666)(cid:1866) (cid:3398) (cid:883)(cid:4667) (cid:1499) (cid:4678)(cid:883) (cid:3398) (cid:885) (cid:1499) (cid:4666)(cid:1866) (cid:3398) (cid:883)(cid:4667) (cid:3397) (cid:4666)(cid:1866) (cid:3398) (cid:884)(cid:4667) (cid:4679) (cid:3404) (cid:884)(cid:1866)(cid:4666)(cid:1866) (cid:3398) (cid:883)(cid:4667) (cid:884) (cid:1866) (cid:1499) (cid:4666)(cid:883) (cid:3398) (cid:886)(cid:1866) (cid:3398) (cid:887) (cid:4667) (cid:884)(cid:1866)(cid:4666)(cid:1866) (cid:3398) (cid:883)(cid:4667) Similarly, for the case of Fig.7(b), the probability is obtained by  the following formula:  (cid:884)(cid:4666)(cid:1866) (cid:3398) (cid:883)(cid:4667)(cid:4666)(cid:1866) (cid:3398) (cid:884)(cid:4667) (cid:884)(cid:1866)(cid:4666)(cid:1866) (cid:3398) (cid:883)(cid:4667) (cid:1499) (cid:4678)(cid:883) (cid:3398) (cid:886) (cid:1499) (cid:4666)(cid:1866) (cid:3398) (cid:883)(cid:4667) (cid:3397) (cid:4666)(cid:1866) (cid:3398) (cid:884)(cid:4667) (cid:4679) (cid:884)(cid:1866)(cid:4666)(cid:1866) (cid:3398) (cid:883)(cid:4667) (cid:887)(cid:1866) (cid:3398) (cid:888) (cid:4667) (cid:884)(cid:1866)(cid:4666)(cid:1866) (cid:3398) (cid:883)(cid:4667) (cid:1499) (cid:4666)(cid:883) (cid:3398) (cid:3404) (cid:4666)(cid:1866) (cid:3398) (cid:884)(cid:4667) (cid:1866) According to these formulas, Table I shows the robustness  when two faults occur in different network sizes. It is worth  mentioning that, all packets are still routed adaptively in the  network.  Table I. Robustness for 2-faulty links under different network sizes  Network size  Robustness for 2-faulty links 4×4  48%  6×6  63%  8×8  71%  16×16  85%  IV. EXPERIMENTAL RESULTS To evaluate the efficiency of the proposed routing scheme, a  NoC simulator is developed with VHDL to model all major  components of the on-chip network. For all the routers, the data  width is set to 32 bits. Each input buffer can accommodate 8 flits  on each virtual channel. Moreover, the packet length is uniformly  distributed between 5 and 10 flits. As a performance metric, we  use latency defined as the number of cycles between the initiation  of a message issued by a Processing Element (PE) and the time  when the message is completely delivered to the destination PE.  The request rate is defined as the ratio of the successful message  39 350 300 250 200 150 100 50 0 ) e l c y c ( y c n e t a L e g a r e v A 0 MD: 0-fault DyXY: 0-fault MD: 1-fault Baseline: 1-fault Baseline: 0-fault 0.1 0.2 0.4 Injection Rate (flits/node/cycles) 0.3 (a) ) e l c y c ( y c n e t a L e g a r e v A 0.5 350 300 250 200 150 100 50 0 0 MD: 0-fault DyXY: 0-fault MD: 1-fault Baseline: 0-fault Baseline: 1-fault 0.1 0.3 Injection Rate (flits/node/cycles) 0.2 (b) 1B-3 0.4 Fig. 8. Performance analysis of MD and the baseline method in 4×4 mesh network (a) under uniform traffic profile (b) hotspot traffic profile in  fault-free, 1-faulty link cases.  Table II. Details of hardware implementation   Network platforms  Area (mm2) DyXY  MD  baseline  6.710  6.753  6.913  Power (W)  dynamic & static  2.32  2.39  2.78  V. CONCLUSION  In this paper, a fault-tolerant routing algorithm named MD was  presented using two virtual channels along both directions. The  presented algorithm avoids taking unnecessary non-minimal paths  when any minimal paths are available, resulting in significant  performance hits. This improvement achieves by a new fault’s  information propagation mechanism and utilizing the information  to deliver packets through the shortest paths. Moreover, MD is  able to deliver packets through alternative paths to the destination,  thereby alleviating congestion in the network both in fault-free and  faulty conditions. Finally, we perform robustness analysis based  on the presented analytical model for two faults in the network.  "
2013,A dynamic stream link for efficient data flow control in NoC based heterogeneous MPSoC.,"As Systems-on-Chip size increase, the communication costs become critical and Networks-on-Chip (NoC) bring innovative solutions. Efficient stream-based protocols over NoC have been widely studied to address dataflow communications. They are usually controlled by a set of static parameters. However, new applications, such as high-resolution video decoders, present more data-dependent behaviors forcing communication protocols to support higher dynamicity. For this purpose, we present in this paper dynamic stream links for stream-based end-to-end NoC communications by introducing two link protocols, both independent of the transfer size, allowing to improve the hardware/software control flexibility. The proposed protocols have been modeled in a MPSoC virtual platform and the hardware cost evaluated. Based on simulations, we provide guidelines to exploit these protocols according to application needs.","1B-4 A Dynamic Stream Link for Eﬃcient Data Flow Control in NoC Based Heterogeneous MPSoC Claude Helmstetter1 , Sylvain Basset1 , Romain Lemaire1 , Fabien Clermidy1 , Pascal Vivet1 1CEA-Leti, Minatec Campus, Grenoble, France 1{firstname.lastname}@cea.fr Michel Langevin2 , Chuck Pilkington2 , Pierre Paulin2 , Didier Fuin3 2STMicroelectronics, Ottawa, Canada 3STMicroelectronics, Grenoble, France 2,3{firstname.lastname}@st.com Abstract— As Systems-on-Chip size increase, the communication costs become critical and Networkson-Chip (NoC) bring innovative solutions. Eﬃcient stream-based protocols over NoC have been widely studied to address dataﬂow communications. They are usually controlled by a set of static parameters. However, new applications, such as high-resolution video decoders, present more data-dependent behaviors forcing communication protocols to support higher dynamicity. For this purpose, we present in this paper dynamic stream links for stream-based endto-end NoC communications by introducing two link protocols, both independent of the transfer size, allowing to improve the hardware/software control ﬂexibility. The proposed protocols have been modeled in a MPSoC virtual platform and the hardware cost evaluated. Based on simulations, we provide guidelines to exploit these protocols according to application needs. I. Introduction Eﬃcient System-on-Chip (SoC) are required for sustaining new application demands, such as high-resolution video, 3D imaging or wireless baseband processing in embedded systems. These applications present common characteristics: limited control to data processing ratio, high amount of data with complex operations and realtime requirements. For achieving the performance versus power consumption requirement, heterogeneous architecture composed of Digital Signal Processors (DSP) and hardware reconﬁgurable accelerators are typically used [11, 12]. With such architectures dataﬂow programming is a commonly adopted to take advantage of parallel processing [14]. With fast growing complexity of applications (10x for HEVC compared to H264 [9]), the number of computing cores is increasing [8]. For connecting these cores, the limited bus-based architecture has been replaced by Networks-on-Chip (NoC). When heterogeneous cores are used, well-deﬁned communication interfaces are required in order to ease integration, IP reuse and programming [13]. A direct approach consisting in converting existing bus protocols in NoC protocol [2] shows limitations as buses are not adapted to dataﬂow programming model: address management creates unnecessary complexity. For reducing this problem, high-level protocols, such as Task Transaction Level [7], are proposed but not suitable to ﬁne-grain dataﬂow control. To overcome this issue, authors of [4] propose to use native stream-based protocols, controlled locally inside the Network Interface (NI). This solution combines performance with reduced latency between processing units. However, it considers only static dataﬂow acceptable for a reduced set of applications showing no dynamic behavior [6]. However, many applications, such as video codec deﬁning ﬂexible picture partitioning mechanisms [9], are characterized by variable and data-dependent data transfers. Moreover, fast reaction to dataﬂow modiﬁcations are required introducing strong constraints on control latencies. In [13], separated control and data networks are used: the control NoC ﬁrst setup a connection and then data can be transmitted. The overhead of having two separate networks and the high latency between transfers limit the technique to large size messages. Similarly, [5] proposes a Connection-Then-Credit (CTC) protocol including a ﬁrst phase of request exchanges to establish connection. The advantage is the usage of a single NoC, but latency overhead on short-to-medium size messages is again observed. Finally, [10] proposes a scheme based on token exchange with limited latency overhead. However, message sizes are bounded and the emitter/receiver couples are ﬁxed. In this paper, we propose a novel NI architecture supporting stream-based communication with the following innovation compared to state-of-the-art: non predictable data-rate by implicit or explicit ﬂow stopping criteria, with dynamic emitter/receiver couples modiﬁcations. Section II presents the static stream link concept and the two ﬂavors of our dynamic stream link: the mode-based and the iteration-based; Section III describes the hardware implementation costs; Section IV shows an application of the proposed scheme to the P2012 platform [3] and discusses the performance; ﬁnally, Section V concludes the paper. 978-1-4673-3030-5/13/$31.00 ©2013 IEEE 41 1B-4 II. Streaming InterFace with Stream Links Considering an application with dataﬂow communications among processing units (PU), a link can be deﬁned as a connection between an emitter and a receiver respectively pushing and popping data to and from a FIFO. In order to map this link mechanism on a NoC-based communication infrastructure, NI supporting stream-based communication, called Streaming InterFaces (SIF), are introduced to support on one side FIFO accesses and on the other side NoC protocol as presented on Fig.1. output stream into NoC packets, whose size is ﬁxed by a conﬁguration parameter. On the receiving side, the ICC unpacks data. The data ﬂows are controlled by credits sent in backward direction compared to data packets to indicate available space in the receiver, as in [5]. No explicit synchronization protocol between SIFs is required since it is implicitly performed by the data/credit mechanism. Once all data/credit have been exchanged, the iteration is done, the external controller is notiﬁed to reconﬁgure SIFs for the next iteration. ),)2 (PLWWHU38 VWUHDPRXW ([WHUQDO &RQWUROOHU FRQILJV\QF FRQILJV\QF &RPPXQLFDWLRQOLQN 6,) 2&& 2&& ,&& ,&& 1R& 6,) ,&& ,&& 2&& 2&& 5HFHLYHU38 VWUHDPLQ Fig. 1. NoC-based communication link infrastructure. A SIF is composed of several communication channels, depending on the number of streaming ports of the PU. Each channel is composed of a FIFO and an Input or Output Communication Control ler (ICC/OCC). Moreover, a SIF contains shared hardware to arbitrate outgoing/incoming packets and to manage conﬁgurations in interaction with an external controller, which is usually software, either centralized or distributed (coupled to SIFs). At application level, successive computation phases can be decomposed into iterations, corresponding to the largest step between global synchronizations and control decisions. Before starting an iteration, the external controller sends conﬁguration parameters to all involved PUs and SIFs, which notify events back to the controller when the iteration is done. In this approach, the main advantage compared to point-to-point connections is the ﬂexibility oﬀered by the reconﬁgurable SIF in order to dynamically rearrange the data ﬂow graph between successive iterations. However mechanisms need to be deﬁned at SIF level to manage the global synchronization. In the following sections, three link protocols are introduced. It has to be noticed that the proposed link protocols are considered at transport layer level which is complementary to existing mechanisms for link performance optimization at network level such as circuit pining in [1]. The hypotheses concerning the network is a packet-switched protocol with source routing. A. Static Stream Link The static stream link protocol, presented on Fig.2, is directly inspired from [4]. It is considered as a reference solution to develop the dynamic link protocols. The link is referred as static since, on both side, SIFs are conﬁgured with a predeﬁned total number of data to transfer. Links are activated by starting SIFs involved into the current iteration. The OCC is in charge of decomposing the 42 ([WHUQDO &RQWUROOHU (PLWWHU38 VWUHDPRXW 6,)2&& 6,) ,&& 5HFHLYHU38 VWUHDPLQ UXQQLQJ LGOH UXQQLQJ LGOH F R Q I L J   L O Q F X G Q J L L V ] H V W D U W V W L G H U F H Q R G G D W D G D W D G D W D V W D U W V W L G H U F H Q R G G D W D G D W D G D W D LWHUDWLRQ LWHUDWLRQ Fig. 2. Static stream link protocol. In order to hide the reconﬁguration time and accelerate the sequencing of iterations, the SIF parameters are stored in shadow registers, which enables the controller to preload the next iteration parameters while the current ones are being used (not shown on diagrams). In consequence, assuming the load is done before the end of current iteration, the reconﬁguration time is limited to the register shifting phase. The usage of static stream link is only possible when link conﬁguration is predictable, which is not the case with applications where control is data-dependent. The following dynamic link protocols overcome this limitation. B. Iteration-based Dynamic Stream Link When the number of data tokens to be exchanged during an iteration is not known when this iteration is conﬁgured, total number of data/credit parameters can not be provided to OCCs and ICCs. In the iteration-based dynamic stream link (Fig.3), the total data/credit parameters are replaced by a dynamic stopping criteria. The goal is to maintain a control at iteration level while allowing data-dependent decisions. In this approach, the ICC and the OCC are running freely until the emitter notiﬁes the end of the transfer. Concretely, the emitter triggers a ﬂush signal to the OCC when all data have been produced. Next, the OCC sends the remaining content of the FIFO and set a ﬂag in the last data packet, telling the ICC that no more data are coming. When the ICC receives this last data packet, it sends for acknowledgment a last credit packet on the backward path, informing the OCC that it must discard remaining credits and stop. Compared to the static link protocol, the PUs need a modiﬁcation on the emitter side PU to manage the ﬂush   6,) ,&& 5HFHLYHU38 VWUHDPLQ ([WHUQDO &RQWUROOHU (PLWWHU38 VWUHDPRXW 6,)2&& F R Q I L J   Z L W K R X W V ] H L V W D U W G D W D H Q R G G D W D G D W D F U G H W L V O V D W O D V W V W D U W G D W D H Q R G G D W D G D W D F U G H W L V O V D W O D V W LWHUDWLRQ LWHUDWLRQ UXQQLQJ LGOH UXQQLQJ LGOH I O X V K I O X V K Fig. 3. Iteration-based dynamic stream link protocol. signal. In general, PUs already have some kind of “end of task” signal that can be directly reused as “ﬂush” signal. Concerning the SIF, it still manages ICCs and OCCs when the iteration starts, and waits for their stops before signaling the end-of-iteration to the external controller. C. Mode-based Dynamic Stream Link The communication scheme based on iterations implies some constraints that reduce the ﬂexibility. For example, it is not possible to open a new communication link while another is running, even if distinct ICCs and OCCs are used; at best, the new link will start when the current iteration ends. Thus, we present a second dynamic link protocol that is independent of PU iterations. The idea is to externalize the opening and closing of communication links from the SIF to the external controller, and to keep links open until the datapath changes. In this context, an “application mode” corresponds to the time between two reconﬁgurations of the datapaths. For a given mode, the connectivity stays ﬁxed, while the total number of data to be exchanged is not predetermined, thus the name mode-based dynamic stream link. In concrete terms, when the “mode” starts, the external controller set up all communication links. ICCs immediately start to send credits and OCCs to send data according to received credits. Next, PUs are programmed as usual. At the end of the mode, the external controller stops all links (Fig.4), and OCCs discard all credits. If a reconﬁguration is needed, the external controller must stop, conﬁgure and start each OCC and ICC again. Using this protocol, no last ﬂag is set in data and credit packets. Clearly, the mode-based dynamic link protocol may present advantages compared to the iteration-based dynamic protocol only if application modes contains many PU iterations. In the other cases, the control overhead increases since link closing is no longer a distributed protocol and new links cannot be reprogrammed in advance. III. Hardware Implementation The hardware implementation of the dynamic protocols is derived from the Network Interface architecture presented in [4]. Three design evolutions are proposed: static V W D U W F R Q I L J   Z L W K R X W V ] H L G D W D G D W D H Q R G G D W D G D W D G D W D G D W D H Q R G F U G H W L V F U G H W L V V W D U W V W R S PRGHLQFOLWHUDWLRQV 6,) ,&& 5HFHLYHU38 VWUHDPLQ ([WHUQDO &RQWUROOHU (PLWWHU38 VWUHDPRXW 6,)2&& UXQQLQJ LGOH UXQQLQJ LGOH I O X V K I O X V K Fig. 4. Mode-based dynamic stream link protocol. link replacement by (a) iteration-based dynamic link, (b) mode-based dynamic link, and (c) support for all static and dynamic link proposed protocols. Fig.5 presents the SIF architecture. The interface is divided into three stages. First stage includes Input (IP) and Output Ports (OP), which are in charge of abitrating incoming/outgoing NoC packets. The second stage contains ICC and OCC components, responsible for the management of data/credit mechanism. Last stage is composed by a set of FIFOs connected to the PU stream ports. A local controller is used to monitor the progress of the iteration (start and synchronization commands). The whole design is customizable to ﬁt with system requirements: numbers of incoming and outgoing stream ports, data width, and FIFO depth can be adjusted; e.g., Fig.5 represents a SIF with two streams in both directions. Hardware modiﬁcations in order to implement the iteration-based dynamic stream links are essentially related to ICC and OCC components: registers related to iteration size are removed, while some logic for ﬂush management is added to OCC components. Implementation of the mode-based dynamic link is similar to the iterationbased design regarding ICCs and OCCs. Main changes are localized in the SIF local controller where iteration management logic is removed, because this task is now in charge of the external controller. A last design includes all the three protocols: the SIF components merge the previous hardware modiﬁcations, plus additional conﬁguration registers in order to select the protocol to be activated. &RQWUROSRUW 6,) FUHGLWVRXW FUHGLWVLQ 3URFHVVLQJ 8QLW ,1B),)2 ,1B),)2 /RFDOFRQWUROOHU 5HJLVWHUDFFHVV 5HJLVWHUDFFHVV UHDGRUZULWH ,7 IOXVK IOXVK &RQWUROLQWHUIDFH FRQILJXUDWLRQH[HFXWLRQ ,&& ,&& VWUHDPLQ VWUHDPLQ ,3 2&& 2&& 287B),)2 287B),)2 VWUHDPRXW VWUHDPRXW 23 1R& Fig. 5. Overview of the SIF architecture 1B-4 43       	  	     	 	 	  	     	 	  	 ! $  % ! $  %  $!""  %   	    # $  %   ! "" #   & '   %  Fig. 6. SIF hardware cost related to diﬀerent link protocols. « 3(Q 38 , ) 6 FRQWUROOHU FRUH PHPRU\PDSSHG LQWHUFRQQHFW VKDUHG PHPRU\ '0$ 6,) /,& 3( 3( 3( , ) 6 38 , ) 6 38 38 , ) 6 38 38 , ) 6 3( 38 « PDLQVWUHDP DOWHUQDWLYHVWUHDP Fig. 8. The Pipeline Benchmark. 1B-4 3( 38 , ) 6 For hardware cost analysis, SIF designs have been synthesized using a low-power 32nm CMOS technology, allowing a 600 MHz operating frequency (1 ICC, 1 0CC, 64-bit data width, and FIFO depth is 32). Compared to the initial design implementing only the static stream link (cf Fig. 6), a SIF using iteration-based dynamic stream link (a) enables to save about 5% of the logic gates, while the gain for the mode-based link (b) reaches almost 10%. When implementing all the protocols in the same SIF design (c), many parts of the design are shared between the protocols and thus the cost is only 25% higher. Diﬀerences are limited, since most of the SIF logic gates are dedicated to FIFOs and I/O ports which are not impacted by the type of link. IV. Evaluation and Comparison A. The P2012 Platform The P2012 research pro ject [3] aims at developing an IP for high performance computing, with its related runtime and programming tools. The P2012 IP is a generic many-core SoC that can be extended with speciﬁc hardware accelerators. Adding hardware accelerators allows to reduce the area and power consumption for the same computing capabilities, while losing in ﬂexibility. 1R& URXWHU QHWZRUN LQWHUIDFH V\QFKUR SHULSK FRUH FRUH FRUH PHPRU\PDSSHG LQWHUFRQQHFW '0$ 6,) 3 FOXVWHU 3( 6,) 2&& 38 2&& 38 ,&& 38 /,& 1R& URXWHU 1R& URXWHU VKDUHG PHPRU\ 3( , ) 6 3( , ) 6 processing elements (PE), connected together via a synchronous NoC (local interconnect, LIC). The LIC is used both for end-to-end streamed data transfer and for control accesses (read/write to conﬁguration registers). Each PE is composed of control registers and several processing units (PU), that communicate using a SIF, as presented in Section II. The DMA is attached to a SIF, allowing transfers between memory components and PUs. All SIFs are controlled by software running on the cluster cores. We have integrated the two proposed dynamic link protocols into the P2012 IP. Moreover, P2012 comes with a SystemC/TLM platform, implemented in a loosely-timed style, with processors modeled by ISS, so we have also developed and integrated a SystemC/TLM model of the new SIF, which is approximately timed. B. The Pipeline Benchmark For the validation and performance evaluation of the dynamic link protocols, a dedicated Pipeline Benchmark has been developed. As shown by Fig.8, this benchmark implements a linear stream starting from the memory, then going through successive PE, and ﬁnally reaching the memory again. Inside each PE, the stream goes through one single PU. For testing the cost of a stream reconﬁguration, an alternative path using the same PE but diﬀerent PU is available. In the sequel, experiments use the following conﬁguration: 6 iterations, involving 8 PEs, with 400 data tokens exchanged per iteration; each PU write a data token every 4 cycles and read 100 tokens before writing; FIFO size is 16 and packet size is 8. The PUs are implemented in SystemC/TLM, and record traces in a diagram form (see for example Fig.9). Using the P2012 SystemC/TLM platform, we simulated this benchmark with many conﬁgurations, and we present the most relevant ones in the next subsections. Fig. 7. Architecture of a P2012 heterogeneous cluster. Internally, the P2012 IP is composed of several clusters connected by an asynchronous NoC. Each cluster is composed of many cores, shared memory, a DMA, and synchronizations devices, such as interrupt controllers. Additionally, a heterogeneous cluster (cf Fig.7) contains some C. Mode-Based Dynamic Link Evaluation Fig.9 presents the simulation trace obtained with the mode-based dynamic link. The control is done by software running on one of the cluster cores. This software, called global control ler, is based on coarse grain synchronizations (i.e., two synchronizations per iteration, one for PE and the other for the DMA). The ISS used in the 44       / 7 D E & 7V F I 7V J 1B-4 7V 7 SDWKUHFRQILJXUDWLRQ 7V 7V 7V         D E F Fig. 11. Path reconﬁguration in the context of mode-based dynamic link and asynchronous controller. 7 7V 7V 7V         Fig. 12. Simulation trace for iteration-based dynamic link. the stream after 3 iterations. It appears that: • Using the global controller, reconﬁguring one path is roughly equivalent of ﬂushing the pipeline; the time lost is about the total pipeline latency L. (No Fig.) • Using the asynchronous controller, it is possible to reduce this cost, but not to suppress it; as illustrated on Fig.11, PU 0 and 1 are restarted normally but lose some time due to full FIFO (a), following 4 PU are restarted by the controller as soon as their FIFO are empty (b), and last PU are restarted normally but spend additional time waiting data (c). D. Iteration-Based Dynamic Link Evaluation Fig.12 shows a simulation using the iteration-based dynamic link. The execution is slow, because of the DMA SIF, whose local controller runs iterations one after the other, thus creating a spurious synchronization between the OCC used at the beginning of the pipeline (data read from the memory) and the ICC used at the end of pipeline (data written to the memory); the OCC starts the iteration n only when the ICC completes the iteration n − 1. This is a problem of system architecture, which appears with the static link protocol too. A solution would be to replace the DMA SIF by two smaller SIFs, one managing the ICC (memory read) and the other managing the OCC (memory write). Because the total number of OCC and ICC is unchanged, the total area would be similar. Thus, we ran a second simulation where the DMA was replaced by generator and sink PU. The corresponding simulation trace is quite similar to Fig.9, which uses the same controller but mode-based dynamic link. However, simulations with short iterations show that the iterationU H E P X Q  ( 3         7 VZ KZSX H G UXQ UHDG	UXQ EORFNHGE\HPSW\),)2 ZULWH	UXQ UHDG	ZULWH	UXQ EORFNHGE\IXOO),)2 Fig. 9. Simulation trace for mode-based dynamic link. 7V 7V 7V         Fig. 10. Simulation trace for mode-based dynamic link and asynchronous controller. (Same key than Fig. 9.) P2012 TLM platform is not accurate enough to provide a relevant estimation of the initial conﬁguration timing, so we just note T the date when the ﬁrst PU is started (a). First, the simulation trace shows that the second iteration starts without delay (b), thanks to the shadow register mechanism that allowed the controller to program the second iteration immediately after starting the ﬁrst one. However, we observe a pipeline hole when the 3rd iteration should start (c). The rationale is that the controller waits the second iteration (d) (or iteration n) before programming the third iteration (or iteration n + 1). This pipeline hole appears on the PU 0 line (late PU activation), but propagates to the last PU, under the form of a blocked FIFO read (e). The forth iteration starts on time (f ), but there is again a signiﬁcant pipeline hole when the ﬁfth iteration starts (g). Such pipeline hole appears when the total pipeline latency is large: noting L = total latency time, C = duration of PU iteration (without waiting time), and sw = time for the software to program one iteration; The time lost every second iteration is approximately L + sw − C , with possible variations due to external side eﬀects such as network contention. Because these pipeline holes are a consequence of coarse grain synchronizations, we have developed a second controller software (called asynchronous control ler) that reprograms each PU independently. The simulation trace Fig.10 obtained with this new controller shows that pipeline holes eﬀectively disappears and that the last iteration terminates signiﬁcantly earlier. At last, we have run the benchmark again, reconﬁguring 45 1B-4 7V 7V 7V 7V DLWHUDWLRQíEDVHGG\QDPLFOLQN 7V 7V 7V 7V 7 7         EPRGHíEDVHGG\QDPLFOLQN Fig. 13. Comparison of dynamic links with asynchronous controller (60 tokens long iterations). based dynamic link is slightly slower than the mode-based dynamic link. For example, with the conﬁguration used in Fig.13 (4 PUs, 60 tokens per iteration, “latency” of 20 tokens, asynchronous controller), the penalty is close to 10%. The rationale is that the iteration-based dynamic link closes and reopens each link every iteration, thus making it unavailable during a short period of time. Signiﬁcant diﬀerences between iteration-based dynamic link and mode-based dynamic link appear when a path reconﬁguration is needed. Indeed, when using the iterationbased dynamic link, simulations show that a path reconﬁguration does not slow down the test execution. On the contrary of Fig.11 where mode-based dynamic link is used, all ICCs, PUs, and OCCs are restarted immediately by the SIF local controllers instead of the external software controller. Finally, as veriﬁed by other simulations not detailed in this paper, the performances of the static link protocol are very similar to the iteration-based dynamic link protocol. Thus, using the dynamic or the static link protocol is not decided by performance concerns, but by the possibility to predict eﬃciently the size of the next iteration. V. Conclusion We have presented dynamic stream link protocols that eﬀectively support non predictable iteration size. Their hardware cost is equal or smaller than the hardware cost of the existing static link protocol. As shown by the experiments, the iteration-based dynamic link allows very fast NoC path reconﬁgurations, whereas the mode-based dynamic link provides more ﬂexibility to the external controller in the case where several communication channels of a same SIF must be used independently. In addition, we show that the software control of the streams can have a negative impact on the global performance if the controller is too centralized and coded at coarse grain. On the contrary, the presented asynchronous controller has provided the best performances during our experiments, and its coding style allows easy parallelization and distribution, thus reducing latency between control processors and processing units. Integrated into a heterogeneous MPSoC such as P2012, we believe that these protocols will ease the implementation of complex applications with very dynamic data ﬂow graphs, such as H264 and HEVC. "
2013,Deflection routing in 3D Network-on-Chip with TSV serialization.,"This paper proposes a deflection routing for 3D NoC with serialized TSVs. Bufferless deflection routing provides area- and power-efficient communication under low to medium traffic load. Under 3D circumstances, the bufferless deflection routing can yield even better performance than buffered routing when key aspects are properly taken into account. Evaluation of the proposed scheme shows its effectiveness in throughput, latency, and energy consumption.","1B-2 Deflection Routing in 3D Network-on-Chip with TSV Serialization Jinho Lee, Dongwoo Lee, Sunwook Kim and Kiyoung Choi  School of Electrical Engineering and Computer Science  Seoul National University  icarosj@dal.snu.ac.kr, dongwoolee@dal.snu.ac.kr, sunwookkim@dal.snu.ac.kr, kchoi@snu.ac.kr  Abstract – This paper proposes a deflection routing for 3D NoC  with serialized TSVs. Bufferless deflection routing provides  area- and power-efficient communication under low to medium  traffic load. Under 3D circumstances, the bufferless deflection  routing can yield even better performance than buffered  routing when key aspects are properly taken into account.  Evaluation of the proposed scheme shows its effectiveness in  throughput, latency, and energy consumption.  I. Introduction  NoC (Network-on-Chip) has become a research trend as  manycore CMPs tend to put more and more components into  a single chip, and a number of researches have been carried  out in various perspectives to achieve design goals such as  scalability, power efficiency, and performance.   Among many research areas on NoC, combining 3D  stacking technology to build a true 3D NoC with TSVs (not  3D topology mapped to 2D plane) is a natural stream to obtain more integration and higher performance. DimDe [1]  has proposed using decomposed crossbar architecture to  cope with the complexity due to increased degree of the  routers. Wang et al. [2] studied efficient multicasting  schemes on 3D NoCs. Chao et el. [3] suggested a run-time  thermal management scheme for 3D NoCs, and Rahmani et  al. published their work on hybrid NoC-bus 3D architectures  [4][5]. There is also a 2D NoC that exploits multiple layers  to reduce area and energy consumption [6].  Although TSV seems to be a promising solution for inter-layer interconnect in 3D stacking, it suffers from large  consumption of silicon area as well as metal area (due to  large landing pads). Considering that a low density TSV has  a pitch of 50μm and high density TSV has a pitch of 16um  [7], if a bundle of TSV links has 128-bit width, its area costs  320,000μm2 for low-density TSVs and 32,768μm2 for high  density TSVs, just for a single link. It is a large overhead  considering that a traditional 7x7 router at 45nm process  consumes about 100,000μm2 1. Researchers are trying to  reduce this pitch, but manufacturers would still want small  number of TSVs due to many reasons including misalignment or physical stress problems. One way to tackle this  problem is TSV serialization, which can greatly reduce the  number of TSVs and their footprint at the cost of sacrificing  some performance [9]. However, if used in NoCs, it has potential to degrade latency and bandwidth of TSV links significantly and to become the performance bottleneck.   Our work targets such an architecture where serialized  TSVs are used in 3D NoCs. We devise a way to mitigate  1 It is estimated by using DSENT [8], a NoC area and power estimator. such a TSV bottleneck problem by using appropriate routing  scheme that allows the flits to fully utilize the TSV links to  reduce latency. To be more specific, we use a modified version of deflection routing to provide performance improvements while maintaining the original benefits (low cost and  low power) of deflection routing. Our contributions are as  followings:   (cid:3253)(cid:2) We propose using deflection routing to solve the TSV  bottleneck problem in 3D NoC with TSV serialization.  (cid:3253)(cid:2) We solve a set of new problems including TSV link  deflection, deadlock, and livelock to apply deflection  routing to the abovementioned architecture:   (cid:3253)(cid:2) We provide a thorough evaluation of the proposed architecture in terms of latency, throughput, and power efficiency. For this, we compare the proposed architecture  against four existing ones.   The rest of this paper is organized as follows. Section II  gives a simple motivation of the work and Section III introduces background and related work. Section IV explains our  routing scheme as well as the proposed architecture in detail.  Section V shows experimental results and Section VI concludes the paper.  II. Motivation  It is expected that applying TSV serialization scheme to  3D network routers will drop the bisection bandwidth of the  network significantly. In other words, it is very likely that  those TSV links will be highly congested and be the performance bottleneck. The problem is serious when static  routing is used and the load to TSV links is unevenly distributed since it will worsen the congestion in TSV links. As  flits fill up the buffer in a router due to such a congested  TSV link, backpressure starts to propagate from the TSV  link and slows down the entire network. In this case, it may  help mitigate the problem to take a detour and use other idle  TSV links. Even though it may not render a minimal path, it  can send a packet much faster than sending it through a  congested minimal path.  The situation described above is shown in Fig. 1(a), where  packets A and B are to be sent to Dest. When DOR (Dimension-Order Routing) is used, we cannot avoid the packets  passing through the congested TSV link in the middle, while  the other two TSV links are idle. If one of the packets can be  transferred through another TSV link as in Fig. 1(b), both  latency and throughput can be improved. In this situation,  deflection routing seems to be an excellent solution. When  two or more flits compete for a single TSV link, all flits other than the winning one will be automatically deflected to  978-1-4673-3030-5/13/$31.00 ©2013 IEEE 29 1B-2 bility. However, since deflection network does not always  need full permutability, it incurs little performance overhead.  Also, the injection/ejection port is handled specially. Instead  of leaving through the permutation network, a locally destined flit is ejected in an early stage. When a flit is to be injected, the injection logic looks for an input port with free  slot which is made either by no input or ejection. Only when  a free slot is found, the injection logic injects the flit into the  slot.  MinBD [12] proposes the use of a small side buffer inside  a router. When some flits are to be deflected, the router randomly selects one of them and puts it into the side buffer.  This scheme helps reducing deflection rate and increases  network performance.  There are a number of previous publications that dealt  with similar problems in 3D network. In [4] and [5], routing  algorithms to avoid 3D bus congestion were proposed. The  architecture they assumed is different from ours in that they  used a 3D bus hybrid architecture. However, the restrictions  are similar in the sense of limited bandwidth. Recently, a  kind of adaptive routing was proposed to split the traffic  load to TSVs [13].   All these approaches are based on adaptive routing approach. As mentioned in the previous section, although they  can acquire the primary goal, they increase router complexity and consume more power because the routers need to  gather additional information. Additionally, adaptive routing  often requires large number of virtual channels to avoid  deadlock, adding even more complexity.  IV. 3D Deflection Routing  A. Serialized TSV link model  In its simplest model, a serialized TSV link is composed  of a transmit buffer, a bulk of TSVs and a receive buffer.  When a flit is to be transmitted to the other side, it is first put  into the transmit buffer. After the transmit buffer is filled, the  TSV starts sending the flit part by part. When all the flit has  been sent after a few cycles, the flit reconstructed in the receive buffer becomes ready to leave the TSV link. To avoid  deadlock, additional flits cannot be put into the transmit  buffer (closed status) until the received flit leaves the link  and the receive buffer is emptied (the reason will be explained in sub-section IV.B). This control can be done with  the overhead of adding a single TSV for sending information  on open/closed status of TSV link, which is required regardless of the routing algorithm.  In this work, we assume synchronous serialization. In  other words, when serialization ratio is n:1, it takes n network cycles to transmit one flit. Asynchronous serialization  may allow higher bandwidth or more area saving, but it  needs additional circuits such as separate oscillator. Nevertheless, our work can be easily extended to asynchronous  serialization without any other consideration as long as  open/closed status of the link is known to the router.  B. TSV link injection/ejection scheme  Our routing algorithm and architecture is based on  (cid:1703)(cid:1724)(cid:1748)(cid:1728) (cid:1741)(cid:1659)(cid:1676) (cid:1692) (cid:1711)(cid:1710)(cid:1713) (cid:1667)(cid:1742)(cid:1735)(cid:1738) (cid:1746) (cid:1668) (cid:1703)(cid:1724)(cid:1748)(cid:1728) (cid:1741)(cid:1659)(cid:1675) (cid:1693) (cid:1703)(cid:1724)(cid:1748)(cid:1728) (cid:1741)(cid:1659)(cid:1676) (cid:1692) (cid:1693) (cid:1703)(cid:1724)(cid:1748)(cid:1728) (cid:1741)(cid:1659)(cid:1675) (cid:1695) (cid:1728) (cid:1742)(cid:1743) (cid:1695) (cid:1728)(cid:1742)(cid:1743) (a)                        (b)  Fig. 1. Motivational example. In (a), static routing makes two flits  conflict at a single TSV link, which result in long latency and  waste of bandwidth. If flit A can take a detour through another idle  link as in (b), both bandwidth and latency can be improved.  other directions, possibly looking for other available links.  Being deflected to another direction may incur longer latency in an ordinary network. However, in the case of congested TSV links, the long serialization latency of TSV links  can outweigh the additional delay coming from detour. Thus,  in addition to area and energy efficiency of bufferless deflection routing, we can also obtain higher performance.   Alternatively, buffered adaptive routing can be used to  realize similar effect. However, its cost is in general very  high. Complex architecture is needed because it needs to  gather information on congestion in the neighborhood and it  often demands a large number of virtual channels to avoid  deadlock. If all aspects are optimally considered, adaptive  routing can be made better than deflection routing in terms  of hop latency. However, we will show that our deflection  routing is much more efficient in terms of power consumption and the performance is comparable or even better under  reasonable load. Note, however, that deflection routing can  be considered as low-cost adaptive routing.  III. Background and Related Work  In ordinary routers, there are buffers in each input port.  When two or more packets compete for an output port, one  of them wins and penetrates through the destined port. The  remaining packets have to stay in the buffers and wait until  next cycle to compete again.  Bufferless deflection routing, on the other hand, does not  have such buffers. When there is no congestion, the flits are  statically routed in the same way as DOR. However, when a  flit loses on arbitration, it is directed to any other port (deflected) instead of being buffered (it actually uses router  pipeline as buffers). Its performance is comparable to ordinary routing under low to medium load. However, as traffic  load gets higher, the flits become more likely to be deflected  and the network is easily congested. Thus the maximum  throughput is degraded. The advantage is in its low complexity. Because this scheme does not require any buffers  except for pipeline registers, it is simple and power efficient  when load is maintained under certain level.   Bless [10] was the first work that eliminated buffers in  NoC and lead to power and area efficiency by using bufferless deflection routing. A number of approaches have been  proposed later to increase efficiency of bufferless deflection  routing. In CHIPPER [11], 4x4 butterfly permutation network is used instead of a crossbar to reduce area and power  overhead. The permutation network is fully connected, but is  a blocking network and thus only supports partial permuta30 1B-2 (cid:1703)(cid:1724)(cid:1748)(cid:1728)(cid:1741)(cid:1659)(cid:1676) (cid:1667)(cid:1697)(cid:1744) (cid:1735)(cid:1735)(cid:1659)(cid:1738) (cid:1729)(cid:1659)(cid:1729)(cid:1735)(cid:1732)(cid:1743)(cid:1742)(cid:1659)(cid:1727) (cid:1728)(cid:1742)(cid:1743)(cid:1732)(cid:1737) (cid:1728)(cid:1727) (cid:1659)(cid:1743)(cid:1738) (cid:1659)(cid:1735)(cid:1724)(cid:1748)(cid:1728)(cid:1741)(cid:1659)(cid:1675) (cid:1668) (cid:1694) (cid:1724)(cid:1737) (cid:1737) (cid:1738) (cid:1743)(cid:1659)(cid:1725) (cid:1728) (cid:1659)(cid:1732)(cid:1737) (cid:1733)(cid:1728)(cid:1726)(cid:1743)(cid:1728)(cid:1727) (cid:1659)(cid:1725) (cid:1728)(cid:1726)(cid:1724)(cid:1744) (cid:1742)(cid:1728)(cid:1659) (cid:1743)(cid:1731) (cid:1728) (cid:1741)(cid:1728)(cid:1659)(cid:1732)(cid:1742)(cid:1659)(cid:1737) (cid:1728)(cid:1745)(cid:1728)(cid:1741)(cid:1659)(cid:1724)(cid:1659)(cid:1729)(cid:1741)(cid:1728)(cid:1728)(cid:1659)(cid:1742)(cid:1735)(cid:1738) (cid:1743) (cid:1703)(cid:1724)(cid:1748)(cid:1728)(cid:1741)(cid:1659)(cid:1675) (cid:1694) (cid:1724)(cid:1737) (cid:1737) (cid:1738) (cid:1743)(cid:1659)(cid:1725) (cid:1728) (cid:1659)(cid:1743)(cid:1741)(cid:1724)(cid:1737) (cid:1742)(cid:1729)(cid:1728)(cid:1741)(cid:1741)(cid:1728)(cid:1727) (cid:1659)(cid:1743)(cid:1738) (cid:1659)(cid:1735)(cid:1724)(cid:1748)(cid:1728) (cid:1741)(cid:1659)(cid:1675) (cid:1693) (cid:1728)(cid:1726)(cid:1724)(cid:1744) (cid:1742)(cid:1728)(cid:1659)(cid:1711) (cid:1710) (cid:1713) (cid:1659)(cid:1735)(cid:1732)(cid:1737) (cid:1734) (cid:1659)(cid:1732)(cid:1742)(cid:1659)(cid:1737) (cid:1728)(cid:1745)(cid:1728)(cid:1741)(cid:1659)(cid:1729)(cid:1741)(cid:1728)(cid:1728) (cid:1697)(cid:1735)(cid:1732)(cid:1743)(cid:1742)(cid:1659)(cid:1727) (cid:1728)(cid:1742)(cid:1743)(cid:1732)(cid:1737) (cid:1728)(cid:1727) (cid:1659)(cid:1743)(cid:1738) (cid:1659)(cid:1724)(cid:1659)(cid:1737) (cid:1738) (cid:1727) (cid:1728)(cid:1659)(cid:1732)(cid:1737) (cid:1659)(cid:1735)(cid:1724) (cid:1748)(cid:1728)(cid:1741)(cid:1659)(cid:1676) (cid:1697)(cid:1735)(cid:1732)(cid:1743)(cid:1742)(cid:1659)(cid:1727) (cid:1728)(cid:1742)(cid:1743)(cid:1732)(cid:1737) (cid:1728)(cid:1727) (cid:1659)(cid:1743)(cid:1738) (cid:1659)(cid:1724) (cid:1659)(cid:1737) (cid:1738) (cid:1727) (cid:1728)(cid:1659)(cid:1732)(cid:1737) (cid:1659)(cid:1735)(cid:1724) (cid:1748)(cid:1728)(cid:1741)(cid:1659)(cid:1675) (cid:1667)(cid:1697)(cid:1744) (cid:1735)(cid:1735)(cid:1659)(cid:1738) (cid:1729)(cid:1659)(cid:1729)(cid:1735)(cid:1732)(cid:1743)(cid:1742)(cid:1659)(cid:1727) (cid:1728)(cid:1742)(cid:1743)(cid:1732)(cid:1737) (cid:1728)(cid:1727) (cid:1659)(cid:1743)(cid:1738) (cid:1659)(cid:1735)(cid:1724)(cid:1748)(cid:1728)(cid:1741)(cid:1659)(cid:1676) (cid:1668) Fig. 2. Flits in TSVs are in deadlock state while flits in 2D layer  experience livelock.  tion occurs, and in turn, no TSV ejection can occur since  TSV links are closed. As a result, all the flits inside the TSV  receive buffers suffer a deadlock, while all the flits in 2D  network experience livelock.   In the situation above, deadlock problem can be solved by  taking two actions at the same time: making TSV links to  accept a flit into their transmit buffers even though they are  closed and injecting flits in the receive buffers to other output ports. Both actions are taken under certain conditions  where deadlock might occur. Below are the conditions:  1. There is no free slot in the 2D input ports.  2. The TSV receive buffer is ready to inject (full).  3. The TSV transmit buffer is empty (receive buffer on  the other side may be full).   When conditions 1 and 2 are met but transmit buffer is  not empty (i.e., a flit transfer is in progress), we can wait.   Deadlock can occur only when all of the above three conditions are met on both sides. Once the actions are taken on  both layers, then the flits in the receive buffers will be emptied at the same cycle, escaping from the deadlock situation.  Sometimes, the three conditions can occur on only one layer,  which is certainly not a deadlock. However, routers decide  whether to take the escape actions based only on its local  information. Thus it is possible that only one side of the  router takes the escape actions while the other does not. Still,  it does not break the functionality of the routing scheme as  explained in the following. Consider the case that the actions  are taken only on layer 0. This implies that   1. there is a free slot on layer 1 and the flit in the receive buffer gets injected, or  2. there is no flit in the receive buffer of layer 1.  In any of the two cases, filling up the transmit buffer on  layer 0 does not cause a trouble since the receive buffer on  the other side of the TSV link is empty or will be emptied.  Note that the transmit buffers are filled up only when the  receive buffers are empty or guaranteed to be emptied,  which is the key to avoid deadlock. Without this strategy,  like the problems handled in sub-section B, the deadlock  problem cannot be solved by just adding more buffers. Regardless of the buffer size, the buffer will become full because of higher input rate than output rate.  MinBD [12] as shown in Fig. 3. The detailed explanation  will be given later in subsection IV.D. There are two directly faced problems in applying deflection routing to 3D network with serialized TSV links. The  first (excess input) problem occurs when the number of incoming flits exceeds that of available output ports. This  comes from the serialization of TSV links. While the crossbar switch in a router can put one flit to a TSV link every  cycle, the TSV link cannot emit flits at the same rate. Consequently, an inter-layer output port can be blocked at a certain time point. At that moment, if flits come into all the  input ports, one of the flits may have nowhere to go because  of the blocked port. This disables the function of deflection  routing. Note that this problem cannot be settled by having  additional buffers. No matter how many buffers there are,  the maximum input rate exceeds the maximum output rate of  TSV links. Then eventually the buffer will become full,  blocking the port.  The second (deflection to TSV link) problem is that deflection to a TSV link can make the performance even worse.  The key point of applying deflection routing is to avoid  crowded TSV links. However, if a flit is deflected to a TSV  link, it is adding redundant traffic overhead to the performance bottleneck. The traffic overhead is actually doubled  because the packet has to go backward eventually2. In addition, the flit will suffer additional two-hop latency going  through the TSV links and router pipeline.  Our solution to the above two problems is treating ports  connected to TSV links (up/down direction) in a special way.  First, when a TSV link has finished receiving a flit into the  receive buffer, it looks for a free slot and only when a free  slot is found, the flit is put into that slot just like an injection  port (TSV injection). Secondly, when a flit is destined upward or downward, it is always put into the TSV link in the  same direction if possible. For this, when such a flit is found,  it is put into the corresponding TSV link just like an ejection  port if the link is open (TSV ejection). If multiple such flits  are destined to the same direction, then it is unavoidable to  select only one. This solution effectively solves/mitigates  both problems of excess input and deflection to TSV link. C. Deadlock avoidance  Original deflection routing is deadlock free, because no  flit is ever stuck or waiting for other resources. However,  because of the TSV ejection/injection scheme, a deadlock  may occur combined with livelock. Consider the example  shown in Fig. 2. In this example, two-layer NoC is assumed.  Layer 0 is full of flits whose destinations are all at layer 1.  Conversely, layer 1 is full of flits whose destinations are all  at layer 0. Additionally, all the TSV links are occupied,  waiting for free slots to come up for injection of the flits in  the receive buffers. Unfortunately, no free slots are available  because the network is full of flits, and thus no TSV injec2 Since we are basically taking ZYX routing, deflection to a TSV  link (in Z direction) means deflection to the unwanted direction  from the destination. Thus the flit should go backward later to  reach the destination. 31 1B-2 (cid:1696)(cid:1733)(cid:1728)(cid:1726)(cid:1743)(cid:1728)(cid:1727) (cid:1659)(cid:1729)(cid:1735)(cid:1732)(cid:1743) (cid:1697)(cid:1735)(cid:1732)(cid:1743)(cid:1659)(cid:1743)(cid:1738) (cid:1659)(cid:1725) (cid:1728)(cid:1659)(cid:1732)(cid:1737) (cid:1733)(cid:1728)(cid:1726)(cid:1743)(cid:1728)(cid:1727) (cid:1700)(cid:1737) (cid:1733)(cid:1728) (cid:1726)(cid:1743)(cid:1732)(cid:1738) (cid:1737) (cid:1674)(cid:1696)(cid:1733)(cid:1728) (cid:1726)(cid:1743)(cid:1732)(cid:1738) (cid:1737) (cid:1659)(cid:1742)(cid:1743)(cid:1724)(cid:1730) (cid:1728) (cid:1677) (cid:1695) (cid:1659)(cid:1707) (cid:1728) (cid:1741)(cid:1736) (cid:1744) (cid:1743)(cid:1728) (cid:1659)(cid:1742)(cid:1743)(cid:1724)(cid:1730) (cid:1728) (cid:1712) (cid:1705) (cid:1696) (cid:1714) (cid:1710) (cid:1695) (cid:1743) (cid:1726) (cid:1733)(cid:1728) (cid:1696) (cid:1743) (cid:1726) (cid:1733)(cid:1728) (cid:1696) (cid:1659) (cid:1713) (cid:1710) (cid:1711) (cid:1743) (cid:1726) (cid:1733)(cid:1728) (cid:1737) (cid:1700) (cid:1659) (cid:1713) (cid:1710) (cid:1711) (cid:1737) (cid:1732)(cid:1738) (cid:1743) (cid:1726) (cid:1728) (cid:1732)(cid:1741) (cid:1727) (cid:1728) (cid:1709) (cid:1743) (cid:1726) (cid:1728) (cid:1733) (cid:1737) (cid:1700) (cid:1741) (cid:1728) (cid:1729) (cid:1729) (cid:1744) (cid:1725) (cid:1728) (cid:1732)(cid:1727) (cid:1710) (cid:1743) (cid:1726) (cid:1733)(cid:1728) (cid:1737) (cid:1700) (cid:1743) (cid:1726) (cid:1728) (cid:1733) (cid:1696) (cid:1741) (cid:1728) (cid:1729) (cid:1729) (cid:1744) (cid:1725) (cid:1728) (cid:1727) (cid:1732) (cid:1710) (cid:1712) (cid:1705) (cid:1710) (cid:1714) (cid:1696) (cid:1695) (cid:1710)(cid:1732)(cid:1727) (cid:1728)(cid:1725) (cid:1744) (cid:1729)(cid:1729)(cid:1728)(cid:1741) Fig. 3. Proposed router architecture.  D. Livelock avoidance  There are numerous ways to avoid livelock. One easy way  is to prioritize old flits [10]. However, it was claimed to be  ineffective [11][14] for its sorting requirement. Instead,  Golden Packet [11] rule was introduced to ensure livelock  freedom. According to the rule, a golden packet ID exists  and it changes every L cycles, where L is the maximum zero-load latency of a packet. During the L cycles, all flits with  the golden ID win on all conflicts. Because of this, the flits  are guaranteed to arrive at the destination within L cycles  before golden ID changes, preventing livelock in a cheap  way.   The golden packet rule can be applied to our work with  slight modification. First, in our architecture, a golden flit  may not be able to be routed toward its minimal direction  because of a closed TSV. This problem can be easily fixed  by using side buffers. When a golden flit is to be routed to a  TSV link but it is closed, the flit is put into the side buffer  instead of being deflected. Because the flit in the side buffer  is forced to be injected within pre-defined threshold cycles,  we can calculate the maximum latency of a golden packet,  and thus ensure livelock freedom.  TABLE 1  System Design Parameters  Simple  buffered  Buffered  Virtual  channel  Adaptive  XYZ  Naïve  deflection  Deflection  1  1  N/A  N/A  N/A  N/A  1  8  O  4  8  O  4  8  O  4 4 x 4 x 4  2 1 4:1  Router  Para-  meters Virtual  channels  Buffers per  VC  Buffer bypass  Packet length  Network size  Pipeline delay Link delay  TSV serialization ratio E. Router architecture: Putting it all together  Considering the routing scheme described above, our  proposed router architecture is shown in Fig. 3. It is based  on MinBD [12], and the only difference is the number of  ports and the TSV ejection/injection logic. The components  in the first stage are placed in the priority order of the operations.  Ejection logic is placed first. TSV ejection/injection  module come second to prioritize TSV links. Sidebuffer redirection/injection is the next. The up/down input port of  sidebuffer redirection logic are connected with output ports  of TSV ejection logic instead of TSV injection. This is because flits to be put into sidebuffer are flits to go out of the  router, not the flits that are waiting to be injected. Injection  logic is placed at the end of the stage. This is because too  much injection will degrade the performance and it’s better  to inject when a free slot exists after all the steps. Second  stage consists of 4x4 permutation network and side buffer  logic. Note that (N, E), (W, S) pairs are changed to (N, S),  (W, E) pairs as in [11], [12] to minimize deflection.  V. Experimental Result  A. Experimental setup  To obtain network performance, we used an in-house,  cycle-accurate simulator. To show the effectiveness of our  router, we compared it with four existing routers: a simple  buffered router without virtual channel, a buffered router  with four virtual channels, a minimal adaptive router, and a  naïve deflection router which performs TSV-link deflection  whenever the port is open. For minimal adaptive router, we  modified adaptiveXYZ in [4] to be used in out architecture.  The design parameters of each router are shown in Table I.  4x4x4 mesh architecture is assumed and simple DOR is used  for non-adaptive buffered routers and as base routing algorithm of deflection routing.   To obtain power consumption, we modified DSENT [8], a  network energy estimator, and integrated into our simulator.  Router components as well as wires and TSVs were modeled for simulation (models for TSVs are from [15]). To be a  fair comparison, buffer bypassing technique was taken into  account in all buffered routers. Also, increased flit-width  overhead of additional information in deflection routing was  32 1B-2   (a) Latency - uniform random traffic.   (b) Throughput - uniform random traffic.  (c) Power efficiency - uniform random traffic.  (d) Latency - hotspot random traffic.  (e) Throughput - hotspot random traffic.  (f) Power efficiency - hotspot random traffic.    (g) Latency – bit complementary traffic.    (h) Throughput - bit complementary traffic.    (i) Power efficiency - bit complementary  traffic.    (j) Latency - tornado traffic.   (k) Throughput - tornado traffic.    (l) Power efficiency - tornado traffic.  Fig. 4. Experimental results. Ranges are adjusted for visibility.  also considered when calculating power consumption.  We tested the networks on four different traffic patterns.  First, uniform random traffic pattern is used as the basic  traffic pattern. All nodes send packets to all other nodes at  equal probability. Hotspot random traffic is similar to uniform random traffic, but more packets are destined to a  ‘hotspot’ of the network. Permutation traffic, on the other  hand, does not send packets to random nodes. Instead, each  node has only one destination, and each node receives packets from exactly one source node. Two kinds of permutation  traffic patterns are used: bit-complementary and tornado.   B. Results  Fig. 4 shows the graphs for experimental results. In the legend, “BUF”, “VC”, “DEF”, “naïve-DEF” and “AD” mean  simple buffered network, buffered virtual channel network,  proposed deflection network, naïve implementation of deflection network and adaptiveXYZ, respectively.  Fig. 4 (a), (d), (g) and (j) show the injection rate-average  latency graph under different traffic patterns. As can be seen,  saturation point of our algorithm is higher than others. It is  sometimes even higher  than adaptiveXYZ algorithm.  Roughly defining saturation load as the load level where the  average latency exceeds 500 cycles, our network performs  25.3% better than simple buffered network and 9.2% better  than adaptiveXYZ in geometric mean. This comes from two  reasons. First, adaptiveXYZ algorithm is a kind of minimal  adaptive routing. Even though it tries to find available TSV  links adaptively, it cannot utilize the ones that do not fall  into the bounding box of the source and the destination.  While flits of other algorithms have to wait in the buffer on  33 1B-2 bandwidth better is more beneficial than taking short routing  path. To avoid performance collapsing on high traffic load,  TSV ejection/injection scheme is used, and corresponding  deadlock and livelock problems are solved in a simple way.  Experiments show that our proposed network shows great  performance while consuming minimal power. When traffic  load from real application is injected, the system might show  different response. Experiments with real workload can be a  better guide for optimizing our network to be used in practice. We leave it as our future work.  Acknowledgements  This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MEST) (No. 2012-0006272) and Ministry of Knowledge Economy (MKE) and IDEC Platform center (IPC) at  Hanyang University.  "
2013,A case for wireless 3D NoCs for CMPs.,"Inductive-coupling is yet another 3D integration technique that can be used to stack more than three known-good-dies in a SiP without wire connections. We present a topology-agnostic 3D CMP architecture using inductive-coupling that offers great flexibility in customizing the number of processor chips, SRAM chips, and DRAM chips in a SiP after chips have been fabricated. In this paper, first, we propose a routing protocol that exchanges the network information between all chips in a given SiP to establish efficient deadlock-free routing paths. Second, we propose its optimization technique that analyzes the application traffic patterns and selects different spanning tree roots so as to minimize the average hop counts and improve the application performance.","A Case for Wireless 3D NoCs for CMPs Hiroki Matsutani1 , Paul Bogdan2 , Radu Marculescu2 , Yasuhiro Take1 , Daisuke Sasaki1 , Hao Zhang1 , Michihiro Koibuchi3 , Tadahiro Kuroda1 , and Hideharu Amano1 1Keio University 3-14-1 Hiyoshi, Kohoku-ku, Yokohama cube@am.ics.keio.ac.jp 2Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh {pbogdan,radum}@ece.cmu.edu 3National Institute of Informatics 2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo koibuchi@nii.ac.jp 1B-1 Abstract—Inductive-coupling is yet another 3D integration technique that can be used to stack more than three known-gooddies in a SiP without wire connections. We present a topologyagnostic 3D CMP architecture using inductive-coupling that offers great ﬂexibility in customizing the number of processor chips, SRAM chips, and DRAM chips in a SiP after chips have been fabricated. In this paper, ﬁrst, we propose a routing protocol that exchanges the network information between all chips in a given SiP to establish efﬁcient deadlock-free routing paths. Second, we propose its optimization technique that analyzes the application trafﬁc patterns and selects different spanning tree roots so as to minimize the average hop counts and improve the application performance. I . IN TRODUC T ION Due to the increase in the design costs of custom System-onChips (SoCs) in recent process technologies, System-in-Packages (SiPs) or 3D ICs that can be used to select and stack necessary known-good-dies in response to given application requirements have become one of promising design choices. Various interconnection techniques have been developed to connect multiple chips in 3D IC packages: wire-bonding, micro-bump [1][2], wireless interconnects (e.g., capacitive- and inductive-coupling) [3][4][5][6] between stacked dies, and through-silicon via (TSV) [3][7] between stacked wafers. Many recent studies on 3D IC architectures have focused on micro-bumps and TSVs that offer the largest interconnect density. However, we consider inductive-coupling that can connect more than three known-good-dies without wire connections to be yet another 3D integration technique, because it provides a large degree of ﬂexibility in building target 3D ICs, such as enabling chips in the package to be added, removed, and swapped after the chips have been fabricated, similar to what is done with building blocks. We propose a novel wireless 3D Chip Multi-Processor (CMP) architecture as a baseline, in which the numbers of processor chips and cache chips in the same package can be customized for a given application set. That is, if the application set requires more cache capacity or bandwidth, we can add more cache chips to satisfy such demands. If the target application set has more threadlevel parallelism, we can add more processor chips to improve performance. Traditional wired Network-on-Chips (NoCs) are used as intra-chip networks in the proposed wireless 3D CMPs, while inter-chip communication is based on wireless inductivecoupling. Since a wireless 3D CMP is a collection of various chips provided by different vendors (e.g., memory, processor, and GPU vendors), we cannot expect to know any pre-determined network topology for intra-chip communications; this makes it difﬁcult to establish routing paths that are free from deadlocks in such ad-hoc 3D CMPs. We ﬁrst propose a routing protocol that exchanges the network information between all chips in a given 3D CMP to establish efﬁcient deadlock-free routing paths. We employ a spanningtree based routing algorithm, in which the average hop count increases depending on the location of the roots of the spanning trees embedded in the target network. To minimize the hop counts, we then propose a technique of optimizing networks, in which the trafﬁc patterns of applications are analyzed and a (a) Baseline 2D CMP (b) Wireless 3D CMP Fig. 1. 2D and 3D CMP architectures. different spanning tree root is selected for each message class to minimize the average hop count and improve the performance of applications. The rest of this paper is organized as follows. Section II presents the proposed wireless 3D CMP architecture. Section III proposes a plug-and-play routing protocol for wireless 3D CMPs. Section IV describes the results we obtained from evaluations and Section V concludes the paper. I I . W IR E L E S S 3D CH I P MU LT I -PROC E S SOR S (CMP S ) A. Baseline 2D and 3D CMP Architectures Figure 1(a) provides an example of a CMP. The chip is divided into sixteen tiles, and each tile has a single processor (gray box) or four shared L2 cache banks (white boxes), in addition to a single on-chip router (red box). The main memory modules and their controllers are connected to four edges of the chip (not shown in the ﬁgure). These tiles are interconnected via their routers. As all L2 cache banks are shared by all processors, a Static NonUniform Cache Architecture (SNUCA) with a cache coherence protocol was assumed in this work. A natural extention of the baseline 2D CMP is the 3D CMP, in which processors, SRAM, and DRAM chips are stacked in a single package, and various organizations have investigated [1][8]. 3D integration enables us to integrate multiple chips fabricated with different process technologies (e.g., logic and DRAMs) into a single package to reduce wire lengths, mitigate the pin count problem, and improve performance. To connect them, 3D NoC architecture [9] is a promising solution as a scalable communication infrastructure. Most studies on 3D CMPs and 3D NoCs to date have assumed micro-bumps or TSVs (i.e., wired solution) for their inter-chip communication; none of these except [10][11] have focused on the wireless approach, which offers a higher degree of ﬂexibility to add, remove, and swap chips after the chips have been fabricated. Note [10] introduces a real implementation of inductive-coupling based wireless 3D NoC and [11] proposes some communication schemes for wireless approaches (e.g., data compression), although these works assume ﬁxed regular topologies, such as ring and 4 × 4 × 2 mesh. 978-1-4673-3030-5/13/$31.00 ©2013 IEEE 23 B. Wireless 3D CMP Architecture Figure 1(b) outlines the proposed architecture for wireless 3D CMP, where the baseline 2D CMP is divided into four chips, each of which has four tiles, i.e., four processor tiles or four cache tiles. Wired links are used for the horizontal network that connects the four tiles on each chip, while wireless inductive-coupling links are used for the inter-chip network that vertically connects four chips. The memories and their controllers are connected to the bottom chip (not shown in this ﬁgure). We have assumed this simple 3D SNUCA to be the baseline for the sake of simplicity. Three types of chips are illustrated in this ﬁgure. The bottom (ﬁrst) chip that consists of sixteen L2 cache banks is a cache chip. The second and third ones that consist of two processors and eight L2 cache banks are hybrid chips. The top (fourth) one that consists of four processors is a processor chip. The performance of applications is typically limited by either computational power or memory bandwidth; thus applications can be classiﬁed into those that are computation-bound and memory-bound. Depending on the target application set, wireless 3D CMP using inductivecoupling enables us to customize the numbers and types of chips stacked in a package after the chips have been fabricated. For example, more processor chips can be added for computationbound applications, while more cache chips will be useful for memory-bound applications. This ﬂexibility is attractive, because designing a new mask pattern for each set of applications has been too costly with recent process technologies. C. Network Requirements for Wireless 3D CMP A wireless 3D CMP is a collection of various hardware components, such as processors, SRAMs, and DRAMs. They are based on different process technologies and fabricated by different vendors. As long as chips satisfy minimum requirements (e.g., performance, chip size, and vertical link pitches), their detailed horizontal architecture (e.g., processors, memories, and horizontal NoC parameters) should be left up to each vendor. A key challenge with wireless 3D CMPs is to stack such post-fabricated hardware components, such as processors and memory chips, provided by different vendors so that these components can cooperate with each other with minimum design rules. The following minimum design rules are required. To connect two arbitrary chips vertically, 1) all chips must have inductors at pre-determined locations. 2) The inductors are connected to input and output ports of on-chip routers to form vertical links, and 3) all the hardware components are topologically reachable to at least an on-chip router that has a vertical link. Note that inductors can be implemented with a common CMOS process and their footprint is small, so the cost could be compensated by the plug-and-play ﬂexibility. In addition, the on-chip routers support the plug-and-play routing protocol proposed in the next section. If the above-mentioned rules are satisﬁed, we do not have to expect any pre-determined network topology for each chip. For example, one chip may have a mesh-based NoC while another may have a ring-based NoC. In an extreme case, it is possible to only have vertical links (i.e., no intra-chip links), if modules on the same chip rarely communicate with one another. Their communication in this case must go through another upper or lower chip via vertical links. Such routing paths cannot be expected to be known at the design time since a chip vendor cannot know the topological structures of upper and lower chips. Thus, conventional static routing strategies for NoCs are not sufﬁcient for wireless 3D CMPs. I I I . P LUG -AND -P LAY ROU T ING W I TH S PANN ING TR E E S O P T IM I ZAT ION A plug-and-play routing protocol that exchanges the topology and trafﬁc information to establish efﬁcient global routing paths 1B-1 is proposed for wireless 3D CMPs to address these problems. The routing paths are established based on spanning trees, which are statically or dynamically optimized for given trafﬁc patterns. Since the network optimization phase is light-weight, it can be carried out at any time if needed, for instance, when the conﬁguration of a SiP, applications running on the SiP, or input data for the applications are changed. A. Overall Procedure and Router Architecture Routing paths are calculated by a single processor inside or outside a 3D CMP, based on the topological information collected from all chips. We assume that every vertical link controller has topology information on its chips. This assumption is reasonable since the horizontal information for chips is known a priori and can be embedded in the vertical link controller at the design time. The topological information on a 3D CMP is exchanged when the system conﬁguration is changed. Then, global routing paths are established or updated if needed. The ﬁve steps in the proposed procedure are given below. • Routing computation node selection: A processor inside or outside the 3D CMP is manually selected as a “routing computation node.” It is in charge of the routing path calculations and optimization. For example, node 7 has been selected as a routing computation node in Figure 2. • Topology request: The routing computation node sends a “topology request” message (blue dotted line) to at least one vertical link controller in every chip to collect all the topological information. • Topology reply: The vertical link controllers reply and send their topological information (red solid line) to the routing computation node. The topological information is represented by an n × n adjacency matrix, in which an element, ai,j , represents a horizontal link between two vertical links, i and j , on the same chip, where n is the number of vertical links in each chip. Figure 2 shows an adjacency matrix for the bottom chip. • Routing computation and optimization: The routing computation node calculates the global paths with a deadlockfree routing algorithm with the optimization techniques for spanning trees discussed in Sections III-B and III-C. • Routing table distribution: The routing computation node distributes the routing table information to every router in the 3D CMP. The routing table information is routed along with the horizontal dimension, then routed along with the vertical one, as seen in Figure 3. This detection-based approach is versatile for both regular and irregular topologies. For example, if a target topology is 3D mesh, the plug-and-play protocol can detect the topological regularity, and it uses the regular routing (e.g., XYZ routing). The topologyagnostic routing is used only when no topological regularity is detected. This can avoid any performance degradation due to the topology-agnostic routing when the topology is regular. Figure 4 illustrates the router and vertical link controller. The router is a conventional VCT or wormhole router with routing tables and trafﬁc counters. It has some horizontal channels depending on a given topology. For example, the example router has X-, Y+, and Y- channels while it does not have X+ channel. It also has local channels connected to a local processor or caches (not shown in this ﬁgure). Its vertical channels (Z+ and Z-) are connected to TX or RX inductors. It maintains trafﬁc counters for dynamic network optimization (mentioned later) that simply count the number of packets injected into each destination. The vertical link controller is attached to the router. It has horizontal topology information embedded at the time of design. It is a simple state machine and is in charge of replying and 24 1B-1 Fig. 2. Topology request and reply. Fig. 3. Routing table update. Fig. 4. Router and link controller. Fig. 5. Optimization ﬂow (a) Root node 0 (b) Root node 7 Fig. 6. Up*/Down* routing paths with different spanning tree roots. sending topology information to the routing computation node and updating the router’s routing table if needed. B. Spanning Tree Based Routing Algorithm To route packets on irregular wireless 3D NoCs, a topologyagnostic routing algorithm without virtual channels is essential. Up*/Down* routing avoids deadlocks in irregular topologies without virtual channels, based on the assignment of direction (up or down) to network channels [12]. Figure 6 has two examples of spanning trees, each of which has a different spanning tree root (node 0 or 7). A legal path must traverse zero or more channels upward followed by zero or more channels downward to guarantee freedom from deadlocks while maintaining network reachability. For example, the red solid line in Figure 6(a) indicates a routing path from nodes 2 to 5. This routing path uses two upward channels followed by two downward channels. Needless to say, the location of a spanning tree root signiﬁcantly affects the hop count and utilization of links. For example, the routing path from nodes 2 to 5 in Figure 6(a) employs a nonminimal path, because the minimal path is prohibited by the spanning tree whose root is located at node 0; thus, four hops are required to send a packet. However, the minimal path in Figure 6(b) is allowed by the spanning tree whose root is located at node 7; thus, only two hops are required. C. Spanning Trees Optimization Two levels of granularity are proposed to optimize spanning trees: message-class-level and ﬁner-level optimizations. The frequency of optimization and its impact are also discussed. Message-class-level optimization: A message class is a group of messages that can share the same virtual channel without introducing protocol deadlocks. Up*/Down* routing generally does not require any virtual channels to avoid structural deadlocks. However, we have assumed a shared cache CMP architecture, in which a cache coherence protocol is running on the NoC. Since such cache coherence protocols typically induce end-toend or request-and-reply protocol deadlocks, they are divided into multiple message classes, each of which uses one or more dedicated virtual channels, to achieve freedom from deadlocks. Table I summarizes an example of virtual channel assignment for a directory-based cache coherence protocol. Three virtual channels are used for the protocol messages so that no end-toend protocol deadlocks are induced. The network resources (i.e., Fig. 7. Optimization result. Red solid lines indicate spanning tree for message class 0, while blue dotted lines indicate that for message class 1. TABLE I A S S IGNM EN T O F V IRTUA L CHANN E L S (VC S ) FOR A CACH E COH ER ENC E PROTOCO L . Message class 0 Request from {L1$} to {L2$ bank} (VC0) Request from {L2$ bank} to {L1$} Message class 1 Request from {L2$ bank} to {directory controller} Request from {directory controller} to {L2$ bank} (VC1) Message class 2 Response from {L1$ or directory} to {L2$ bank} (VC2) Response from {L2$ bank} to {L1$ or directory} buffers) of one virtual channel and those of another virtual channel are logically separated and do not introduce any cyclic dependencies between them; thus, we can assign different spanning trees to them, depending on their trafﬁc patterns. For example, request messages from L1 caches (i.e., processors) to L2 cache banks move in the opposite direction of the reply messages from the L1 caches to the L2 caches. Since Up*/Down* routing introduces non-minimal paths and imbalanced link utilizations depending on the spanning tree roots and trafﬁc patterns, their optimal spanning tree roots that can minimize the hop count will be different. Thus, an optimal spanning tree root should be selected for each message class to improve performance. Finer-level optimization: It is possible to use multiple spanning trees or virtual channels for a certain message class. That is, an optimal combination of multiple spanning trees that can minimize the hop count can be assigned to a message class. The best spanning tree among them is selected for routing, depending on the source and destination pair in this case. Moreover, such a scheme of multiple spanning trees provides additional opportunities to further reduce the hop counts by virtual channel transitions at intermediate nodes. For example, a packet is ﬁrst routed to an intermediate node along with a spanning tree (VC0), and then it is routed to the destination with another spanning tree (VC1). Note that such transitions in virtual channels should be done so that no cyclic dependencies between multiple spanning trees are introduced, e.g., VC transitions are allowed only either in ascending or descending order [13] (e.g., VCi (cid:2)→ VCi−1 (cid:2)→ VCi−2 ). We will evaluate the assignment of two spanning trees for each message class (called Irr6(min)) in Section IV-C. Optimization ﬂow: Figure 5 outlines the optimization ﬂow for spanning trees. First, the trafﬁc patterns of applications on a target 3D CMP are pre-analyzed with system-level simulations or analyzed at run-time for a certain time period to generate a trafﬁc trace. Then, an optimal spanning tree root is selected for each message class to minimize the average hop count of the trace and 25  9  8  7  6  5  4 e e r t i g n n n a p s f o t n u o c p o H Min 6.49 Max 8.99  9  8  7  6  5  4 e e r t i g n n n a p s f o t n u o c p o H Min 4.63 Max 6.48  9  8  7  6  5  4 e e r t i g n n n a p s f o t n u o c p o H 1B-1 Max 7.81 Min 5.94  0  10  20  30  40 Root node ID  50  60  0  10  20  30  40 Root node ID  50  60  0  10  20  30  40 Root node ID  50  60 (a) Message class 0 (b) Message class 1 (c) Message class 2 Fig. 8. Example of hop count distribution with different spanning tree roots (IS application on 64-tile CMP). TABLE II improve the execution time for the application; toward this end, we use the following cost function: (cid:2) (cid:2) C ost = Ws,dHs,d , (1) s d where Ws,d denotes the number of packets from source router s to destination router d in the trace and Hs,d denotes the hop count from s to d. Figure 7 shows an example of optimized spanning trees for a 16-tile 3D CMP that deﬁnes just two message classes for the sake of simplicity. Packets that belong to message class 0 are routed based on the red spanning tree whose root is node 13, while those belonging to message class 1 are routed based on the blue dotted spanning tree whose root is node 2. Note that this cost function only takes into consideration spatial locality, to ﬁt run-time light-weight optimization of spanning trees. A more sophisticated cost function that takes into account temporal locality or trafﬁc burstiness is also possible for off-line optimization of spanning trees. Optimization frequency: The time to calculate the best spanning tree root for each message class is short. The calculation time for target wireless 3D CMPs with three message classes (see Section IV-A) is 0.008sec for 8-tile, 0.020sec for 16-tile, 0.315sec for 32-tile, and 9.376sec for 64-tile with an AMD 2.7GHz Opteron processor. Thus, it is possible to optimize routes at every boot time so that routing paths can be customized for the latest characteristics of applications. More frequent or run-time optimization of routes is also possible if the system has spare or escape virtual channels to remove cyclic dependencies formed across previous and next spanning trees. Dynamic reconﬁguration schemes for interconnection networks, such as the Double scheme [14], can be used for this purpose. Impact of spanning trees optimization: Figure 8 has an example of hop count distributions in a 64-tile wireless 3D CMP when the location of the spanning tree root is varied from nodes 0 to 63. Detailed parameters for the simulations and the evaluation environments will be presented in Section IV-A. There are three message classes each of which uses a single virtual channel in the network. For example, in the message class 0 (Figure 8(a)), the best spanning tree root that minimizes the hop count is node 31 and its average hop count is 6.49, while the worst spanning tree root is node 60 and its hop count is 8.99. There are signiﬁcant differences in hop counts between Min and Max in all message classes. The proposed technique of optimizing spanning trees is signiﬁcant because the optimization of routes enables us to “always” select the best spanning tree root that minimizes the hop count for each message class. We will explain performance beneﬁts obtained with the optimization of spanning trees in Section IV-C. IV. PRAC T ICA L CON S ID ERAT ION S AND EX P ER IM EN TA L R E SU LT S The proposed plug-and-play routing protocol for wireless 3D CMPs is denoted an “irregular approach” since it can cope IRR EGU LAR TO PO LOG I E S TO B E T E S T ED ( PARAM E T ER S ) . 8-tile 16-tile 32-tile 64-tile Topology (2,1,4) (2,2,4) (4,2,4) (4,4,4) #routers 8 16 32 64 #CPUs 4 4 8 8 TABLE III #L2$ banks 16 32 64 128 #Memories 2 4 8 16 S IMU LAT ION PARAM E T ER S ( PROC E S SOR , M EMORY, AND N E TWORK ) . Processor L1 I/D cache size L1 cache latency L2 cache bank size L2 cache latency Memory size Memory latency Router pipeline Buffer size Flit size Protocol # of message classes Control / data packet size UltraSPARC-III 64 KB (line:64B) 1 cycle 256 KB (assoc:4) 6 cycle 4 GB 160 (± 2) cycle [RC/VSA][ST][LT] 5-ﬂit per VC (default) 128 bit MOESI directory 3 (see Table I) 1 ﬂit / 5 ﬂit with any ad-hoc topologies that meet the minimum design rules stated in Section II-C. The prototype system of inductive-coupling reported in [10], on the other hand, uses a single ring-based topology, which signiﬁcantly limits scalability; thus, the irregular approach is compared with 3D mesh networks instead of ring. A. Target CMP Architecture and Evaluation Environments Table II summarizes the conﬁgurations for 8-tile, 16-tile, 32tile, and 64-tile wireless 3D CMPs. Topology (x, y , z ) denotes a wireless 3D CMP that consists of z chips where each chip consists of x×y tiles. The number of chips z is ﬁxed at four in the irregular approach, while their sizes correspond to (2,1), (2,2), (4,2), and (4,4) for 8-tile, 16-tile, 32-tile, and 64-tile CMPs. Table III lists the processor and network parameters. We used a full-system CMP simulator that combines GEMS [15] and Wind River Simics [16] to simulate the wireless 3D CMPs. We modiﬁed a detailed network model of GEMS to accurately simulate the proposed plug-and-play routing protocol. A directory-based MOESI coherence protocol that deﬁnes three message classes was used. We used ten parallel programs from the OpenMP implementation of NAS Parallel Benchmarks (NPB) to evaluate the performance of applications in these communication schemes on the wireless 3D CMPs. Sun Solaris 9 operating system (OS) was running on the CMPs. These benchmark programs were compiled with Sun Studio 12 and were executed on Solaris 9 OS. The number of threads was set to four or eight, depending on the number of processors (e.g., four threads for 8-tile CMP). B. Random Generation of Irregular Topologies A wireless 3D CMP is a collection of various chips whose horizontal architecture is not known by individual chips; the proposed plug-and-play routing protocol can cope with such irregularities. One thousand irregular topologies were randomly generated for 26                          0  50  100  150  200  2  3  4  5  6 Ideal hop count (uniform traffic)  7 N u m e b r o f e n t w o r s k ( 1 , 0 0 0 r t i s a l ) Ave 2.93 (a) 16-tile (b) 32-tile (c) 64-tile Fig. 9. Hop count distribution of randomly generated topologies. 8-tile results have been omitted due to page limitations. Average hop count in this case is 2.29.  0  50  100  150  200  2  3  4  5  6 Ideal hop count (uniform traffic)  7 N u m e b r o f e n t w o r s k ( 1 , 0 0 0 r t i s a l ) Ave 3.92  0  50  100  150  200  2  3  4  5  6 Ideal hop count (uniform traffic)  7 N u m e b r o f e n t w o r s k ( 1 , 0 0 0 r t i s a l ) Ave 4.79 each CMP conﬁguration to evaluate the irregular approach so that each horizontal link appeared with 50% probability while each vertical one appeared with 100%. This is because all chips must have vertical links at pre-speciﬁed locations as deﬁned in the minimum design rules, while they can use an arbitrary or customized topology for their intra-chip network. Note that a network topology that has all horizontal and vertical links is 3D mesh here, in which packets are routed with dimension-order routing (i.e., XYZ routing). Figure 9 shows the hop count distribution of 1,000 randomly generated topologies for 16-tile, 32-tile, and 64-tile wireless 3D CMPs. Out of 1,000 trials for each network size, a network that had the closest hop count value to the average was selected for full-system CMP simulations. Communication latency typically and signiﬁcantly affects the performance of applications on shared-memory CMPs. Thus, it is a good approximation where a network conﬁguration that has the closest hop count value to the average represents the average performance of applications for 1,000 trials. For example, a network conﬁguration whose hop count is 2.93 has been selected for the full-system simulations of the 1,000 random topologies in the 16-tile conﬁguration. C. Performance Improvements by Spanning Trees Optimization One or more spanning tree roots are selected with the proposed technique of optimizing spanning trees for each message class of applications using the cost function (Equation 1) to minimize the average hop count. Four conﬁgurations are compared to ﬁnd improvements to performance by optimizing spanning trees. • Irr3(max): The worst spanning tree root that maximizes C ost is selected for each message class of each application. • Irr3(min): The best spanning tree root that minimizes C ost is selected for each message class of each application. • Irr6(min): The best pair of spanning tree roots that minimizes C ost is selected for each message class of each application. Six VCs are used in total. • Irr(ideal): Minimal paths without Up*/Down* restrictions are selected. They may cause deadlocks. Figure 10 shows the hop counts for NPB applications with Irr3(max), Irr3(min), Irr6(min), and Irr(ideal). Again, if we can take into account the trafﬁc patterns, we can always use the best routing path set. We can reduce the hop counts by up to 31.4% in the 64-tile case compared to the worst case. Also, we found that more than two spanning tree roots for each message class can further reduce the hop counts, but the beneﬁts are small compared to the additional hardware cost of virtual channels. Thus, a single spanning tree root is sufﬁcient for each message class in the CMPs that have 16 and 64 tiles. Finally, Figure 11 shows the execution time for NPB applications with Irr3(max), Irr3(min), and 3D Mesh. 3D Mesh has 100% of horizontal links, while the irregular approach has only 50% of horizontal links. As can be seen, by taking into account trafﬁc patterns, we can always use the best routing path set, and we can reduce the application execution time by up to 15.1% compared to the worst case. In addition, performance gap between Irr3(min) and 3D Mesh is much smaller than that between Irr3(min) and Irr3(max). These can be achieved with the run-time optimization of spanning trees with a light-weight cost function. D. Energy Consumption The irregular approaches with the optimization of spanning trees were evaluated in terms of the average energy consumption needed to transmit a single ﬂit from the source to destination nodes. They are also compared with 3D Mesh. This energy per ﬂit can be estimated as Ef lit = w(E 1hop router hrouter + E 1hop hlink hhlink + E 1hop v link hv link ) = w(Erouter + Ehlink + Ev link ). (2) Here, w represents the ﬂit-width. hrouter , hhlink , and hv link represent the number of routers, horizontal links, and vertical links traversals on average. E 1hop v link correspond to the energy consumed by transmitting single bit data via a router, a horizontal 2mm link, and a vertical link (i.e., inductor). router was set to 0.20pJ in this evaluation, based on the post-layout simulations of on-chip routers when a 65nm CMOS process with a 1.2V supply voltage was used. The E 1hop hlink was set to 0.43pJ, assuming that a semi-global interconnect whose wire capacitance was 0.20pF/mm (from ITRS 2007) was used for the 2mm horizontal links with repeaters inserted. Finally E 1hop set to 0.14pJ from [5]. We calculated the (Erouter + Ehlink + Ev link ) values of Irr(max), Irr(min), and 3D Mesh based on the hop count values (i.e., hrouter , hhlink , and v link ) of all applications extracted from the full-system simulation results. Figure 12 shows the results obtained from evaluation, in which each bar represents its Erouter , Ehlink , and Ev link from the bottom. As shown, the irregular approach with the best spanning tree (Irr3(min)) reduces the energy consumption by up to 24.9% compared to that of the worst spanning tree. In addition, energy efﬁciency of Irr3(min) is relatively close to that of 3D Mesh that has twice number of horizontal links. router , E 1hop hlink , and E 1hop The E 1hop v link was V. CONC LU S ION S As future wireless 3D CMPs, we assume that chips (cardstyle computer components) are inserted to a micro cartridge that provides only power and clock pins to the chips. Packets are transferred between the chips wirelessly without going through the cartridge which typically limits the bandwidth due to pin-count limitation. Since card-style computer components are inserted when needed, we cannot expect any pre-determined topology for the system. Toward this end, we proposed a plug-and-play routing protocol that was able to cope with the irregularity of wireless 3D CMPs. The proposed routing protocol exchanges network information between all chips to establish routing paths that are free from deadlocks based on spanning trees. This approach is speciﬁc to inductive-coupling based 3D NoCs, and it is pointless 1B-1 27                          18  16  14  12  10  8  6  4  2  0 IS DC CG MG EP LU UA Benchmark programs SP BT FT Ave. H p o n u o c t s ( m s e s s a c e g a s s e l 1 0 , , 2 ) -25.2% Irr3(max) Irr3 (min) Irr6 (min) Irr (ideal) (a) 16-tile (b) 64-tile Fig. 10. Hop counts for irregular approach with different spanning trees. Each bar consists of hop counts for message classes 0, 1, and 2, starting from the bottom.  0  5  10  15  20  25  30 IS DC CG MG EP LU UA Benchmark programs SP BT FT Ave. H p o n u o c t s ( m s e s s a c e g a s s e l 1 0 , , 2 ) -31.4% Irr3(max) Irr3 (min) Irr6 (min) Irr (ideal)  0  20  40  60  80  100  120 IS DC CG MG EP LU UA Benchmark programs SP BT FT Ave. E u c e x i t n o i t m e ( o n r m a i l d e z ) -7.9% Irr3(max) Irr3 (min) 3D Mesh (a) 16-tile (b) 64-tile Fig. 11. Application execution times of 3D Mesh and irregular approaches with different spanning trees.  0  20  40  60  80  100  120 IS DC CG MG EP LU UA Benchmark programs SP BT FT Ave. E u c e x i t n o i t m e ( o n r m a i l d e z ) -15.1% Irr3(max) Irr3 (min) 3D Mesh  0  0.5  1  1.5  2  2.5  3 IS DC CG MG EP LU UA Benchmark programs SP BT FT Ave. E r u o t e r E + h i l k n E + v i l k n [ J p / b ] t i -23.4% Irr3(max) Irr3 (min) 3D Mesh (a) 16-tile (b) 64-tile Fig. 12. Energy consumptions of 3D Mesh and irregular approaches with different spanning trees. Each bar consists of Erouter , Ehlink , and Evlink , starting from the bottom.  0  1  2  3  4  5 IS DC CG MG EP LU UA Benchmark programs SP BT FT Ave. E r u o t e r E + h i l k n E + v i l k n [ J p / b ] t i -24.9% Irr3(max) Irr3 (min) 3D Mesh for TSV-based 3D chips since the network topology is ﬁxed at design time and never changed afterwards in wired 3D systems. In addition, we proposed a new technique of optimizing spanning trees that selects the best spanning tree root for each message class to minimize the average hop count and reduce the application execution time. Full-system CMP simulations using parallel applications revealed that the proposed technique of optimizing spanning trees improves the application performance and energy consumption by up to 15.1% and 24.9%, respectively. Acknowledgements This research was performed by the authors for STARC as part of the Japanese Ministry of Economy, Trade and Industry sponsored “Next-Generation Circuit Architecture Technical Development” program. The authors thank to VLSI Design and Education Center (VDEC) and JST CREST for their support. H. Matsutani was also supported by Grant-in-Aid for Research Activity Start-up #23800053. "
2013,Power optimization for application-specific 3D network-on-chip with multiple supply voltages.,"In this paper, a MSV-driven power optimization method is proposed for application-specific 3D NoC (MSV-3DNoC). A unified modeling method is presented for considering both layer assignment and voltage assignment, which achieves the best trade-off between core power and communication power. A 3D NoC synthesis is proposed to assign network components onto each layer and generate inter-layer interconnection. A global redistribution is applied to further reduce communication power. Experimental results show that compared to MSV-driven 2D NoC, the proposed method can improve total chip power greatly.","Power Optimization for Application-Speciﬁc 3D Network-on-Chip with Multiple Supply Voltages ∗ 4C-4 Kan Wang , Sheqin Dong Depar tment of Computer Science & Technology Tsinghua University, Beijing, China 100084 wangkan09@mails.tsinghua.edu.cn ABSTRACT In this paper, a MSV-driven power optimization method is proposed for application-speciﬁc 3D NoC (MSV-3DNoC). A uniﬁed modeling method is presented for considering both layer assignment and voltage assignment, which achieves the best trade-oﬀ between core power and communication power. A 3D NoC synthesis is proposed to assign network components onto each layer and generate inter-layer interconnection. A global redistribution is applied to further reduce communication power. Experimental results show that compared to MSV-driven 2D NoC, the proposed method can improve total chip power greatly. 1. INTRODUCTION 3D integration[1] becomes more and more popular as it succeeds in reducing wire-length, increasing integration density and improving performance. To solve interconnection problem in circuit design, a new communication paradigm, network-on-chip (NoC) has evolved[2]. NoC brings networks into circuit and realizes the communication through on-chip networks, which can greatly reduce communication cost. 3D NoC, which combines both advantages of 3D integration and NoC, has attracted much attention in recent years[3]. However, power consumption is still an issue that is required to be optimized. The power consumption of 3D NoC mainly contains two parts: core power and communication power. The core power is mainly caused by the computing of cores and operating of network components, while the latter is mainly caused by communication between cores and network components. As the number of cores on chip continues to increase, the communication power can take more than 40% of the total chip power[4]. One eﬀective technology for reducing core power is Multiple Supply Voltage (MSV). By assigning lower voltage levels to some blocks, the total power can be reduced[5]. A cluster of cores with the same voltage level is called a voltage island (VI). However, MSV can add to communication cost which is caused by the voltage level conversion between diﬀerent voltage domains. Many previous works paid attention to MSV-driven SoC or NoC design and proposed voltage level assignment methods to improve chip power[4]-[8]. However, all of them just focused on 2D design. At the same time, communication power can be optimized through a good NoC synthesis. Many designers in recent years have proposed 3D NoC synthesis methods[9]-[12]. [9] presented a very eﬃcient 3D NoC synthesis based on a rip-up and reroute algorithm and [10] proposed a partition based ∗ This paper is supported by MOST of China pro ject 2011DFA60290 and NSFC 61176022 1.2V 1.5V Switch NI TSV Figure 1: Ideational MSV-driven applicationspeciﬁc 3D NoC synthesis approach for application-speciﬁc 3D NoC. However, both of them assumed 3-D ﬂoorplan results as inputs. As a result, the communication power cannot be optimized. [11] proposed a synthesis method based on simulated allocation. [12] presented a four-stage synthesis approach to determine power-performance eﬃcient 3-D NoC topology. However, neither of them considered the layer assignment problem. Besides, most of previous work just paid attention to communication optimization and few of them thought of the usage of MSV technology on core power optimization. A MSV-driven 3D NoC which optimizes both core power and communication power is required, as shown in Fig. 1. Unlike traditional 2D or 3D NoC, the design of a MSVdriven 3D NoC involves the issues of layer assignment, voltage assignment, 3D network synthesis as well as 3D ﬂoorplanning. First, layer assignment is important because the assignment result of cores greatly aﬀects the inter-layer communication power. For MSV design, the problem becomes more diﬃcult. On one hand, the power network resources should be optimized[13]. As a result, voltage levels cannot be simply assigned to each layer under the constraint of power network resources. On the other hand, cores cannot be simply assigned to each layer according to the area constraint and core criticality constraint. This means that some performance critical cores such as processors must be assigned with the highest supply voltage level while other functions, such as memories or control logic, can operate on lower supply voltages[14]. However, ensuring the voltage level of critical cores may lead to large communication power and the layer assignment should try to achieve trade-oﬀ between the two. Second, NoC synthesis is also quite important in 3D NoC, because the NoCs can seriously aﬀect the total consumption of communication power. All of previous 3D NoC synthesis methods are based on given ﬂoorplans or during the postﬂoorplan stage. As a result, the chip power is restricted to the initial ﬂoorplan and a bad ﬂoorplan can lead to a bad assignment and hence large communication power. Furthermore, to reduce overlaps, the initial ﬂoorplan may be changed and the total chip size will be enlarged. There978-1-4673-3030-5/13/$31.00 ©2013 IEEE 362 Core Information Core Communication Graph Lowest Allowable Voltage Table Chip Technology 3D Layer Assignment 3D Synthesis and Floorplan Layer Assignment Voltage level Assignment Global Optimization Dead-space realloacation Position Optimization Path Allocation Horizontal Network Components Generation Vertical Interconnection Generation Network Components Integration for Floorplan 3D floorplanning with vertical communication optimization MSV-driven Application Specific 3D NoC Figure 2: Design ﬂow for MSV-driven applicationspeciﬁc 3D NoC fore, a 3D ﬂoorplan with network components assignment is required for power optimization in 3D NoC. In addition, the design cost of inter-layer interconnection is always an indicator of the quality of 3D NoC[9]. However, most of previous work has ignored the inter-layer interconnection planning. A inter-layer communication-aware method is necessary for minimizing the design cost. 1.1 Contributions of Our Work In this paper, a MSV-driven framework (MSV-3DNoC) is proposed for application-speciﬁc 3D NoC. To our best knowledge, this is the ﬁrst work on the design of MSV-driven application-speciﬁc 3D NoC. The contributions of this paper are summarized as follows: 1. A uniﬁed modeling method is proposed for taking into account both layer assignment and voltage level assignment. An ILP formulation with a variable-pruning strategy is employed to solve the problem. 2. 3D NoC synthesis is proposed with consideration of inter-layer communication optimization. 3. 3D NoC ﬂoorplanning with network component assignment considered is presented for communication power improvement. A post-ﬂoorplanning global redistribution is used to further improve the total communication power. The rest of the paper is organized as follows. Section 2 describes the framework of MSV-3DNoC. In Section 3, MSV-driven layer assignment algorithm is introduced. In Section 4, 3D synthesis, inter-connection planning as well as ﬂoorplanning algorithms are proposed. Experiment results are shown in Section 5 and conclusion is shown in Section 6. 2. THE OVERALL DESIGN FLOW The overall design ﬂow of MSV-3DNoC is shown as Fig. 2. Its inputs includes (1) N cores including parameters of widths and heights, (2) a core communication graph, (3) m legal voltage levels, (4) the lowest allowable voltage table for each core and the corresponding power consumptions at diﬀerent voltage levels, and (5) the number of layers n. MSV-3DNoC assigns N cores to n layers with a voltage assignment for each core, assigns network components on each layer, and determines the physical position of each core and component. The output is a generated legal 3D layout with optimized total power consumption of cores and communications, and the design cost including power network resource and total chip area. 4C-4 3. MSV-DRIVEN LAYER ASSIGNMENT Layer assignment plays a key role in the inter-layer communication power optimization of 3D NoC. However, for MSV, the layer assignment has two ob jects of achieving both optimizations of core power and communication power, which are in turn determined by the results of voltage assignment and layer assignment respectively. To achieve tradeoﬀ between the core power and communication power, layer assignment and voltage assignment should be considered together. Furthermore, the constraints of area balance and power network resources will make the problem more difﬁcult. In this section, an ILP based uniﬁed modeling for MSV-driven layer assignment problem is proposed, which aims to optimize the total consumption of chip power. 3.1 Uniﬁed Modeling for Layer Assignment and Voltage Assignment One feasible method to solve the problem is to assign N cores to n layers ﬁrst with the ob jective of communication optimization. Then diﬀerent voltage levels can be assigned into each layer to improve core power. However, this will generate multi-voltages in each layer, which will deﬁnitely add to the design cost of power network. To minimize the design cost of total power network, in this paper, we assume that each layer aims to use the least number of voltage levels, and we call this as power network resource constraint. For simplicity, the problem of m voltage levels to n layers is referred to as M LA (m, n) for short. The mapping between voltage levels and layer levels is referred to voltage-layer mapping (vlmap ). 3.2 ILP Formulation for MSV Driven Layer Assignment According to the relationship between m and n, the problem can be classiﬁed into three kinds: 3.2.1 The case of m = n In the case of m = n, the number of layers equals the number of voltage levels. For simplicity, one single voltage level is assigned to each layer under the power network constraint. Note that more complicated cases that integrate more voltages into layers can be extended through the combination of cases of m = n with m < n and m > n. Then M LA (n, n) can be solved by ILP based algorithm with additional constraint of area balance. Without loss of generality, in this problem, the increasing function of v lmap is used and any other pattern of mapping can be formulated similarly. We use xil to stand for core i assigned to layer level l. Then for each core, the following equation should be satisﬁed: xil = 1 where xil = 0 or 1, ∀1 ≤ i ≤ N n(cid:2) (1) l=1 If core i is assigned to layer l, xil = 1 and otherwise xil = 0. Then the total on-chip core power can be calculated according to: Pcore = n(cid:2) N(cid:2) l=1 i=1 Pil · xil (2) where Pil is the power consumption of core i when assigned to layer l. The inter-layer communications require TSVs[9] to realize, which hence adds to the inter-layer communication cost. 363 Besides, the additional cost of voltage level conversion is required to be considered. Here, we integrate the cost into TSV cost C ostT SV . The inter-layer communication cost can be calculated by communication amounts weighted C ostT SV between cores on diﬀerent layers: Pvcom = C ostT SV · N(cid:2) C omij · xil · xjk N(cid:2) n(cid:2) n(cid:2) (3) i=1 j=1 l=1 k=l+1 where C omij is the communication amount between cores i and j . The formulation can be transformed into linear formulation by replacing xil and vjk with a binary decision variable xil,jk according to [6]. With core criticality constraint considered, the supply voltages of some critical cores are required to be guaranteed. Let vi be the lowest available level for core i. Then for each core, the following inequality should be satisﬁed: xil ≥ 1 where1 ≤ i ≤ N xil = 0, vi−1(cid:2) n(cid:2) (4) l=0 l=vi On the other hand, to minimize the total chip area, the total core area on each layer should be balanced. Therefore, for each layer l, an area-balanced constraint is added: areai · xil ≤ 1 · Area + ξ · Area n Areal = N(cid:2) (5) i=0 Areal and Area are the total core area of layer l and all layers while ξ is a slack variable for an acceptable error on area balance. In this experiments, we set ξ = 0.01. Then the layer assignment problem can be formulated as follows: M inimize P = Pcore + Pvcom Subj ect to (1) and (4) − (5) By solving this ILP problem, both voltage and layer can be assigned for each core, while the area of each layer is balanced well. (6) 3.2.2 The case of m < n In the case of m < n, the number of available voltage levels is less than the layer number. For satisfying the power network constraint, some layers have to share the same voltage level and the key issue of M LA(m, n) is to decide which layers to share the same voltage and which layers to enjoy one single voltage level. In this section, a three-stage algorithm is proposed to solve the problem. First, v lmap candidates generation is used to generate possible mappings from voltage levels to layer levels. After that, layers with the same voltage level are combined to larger ones. Then the problem can be translated into M LA(m, m) with adjusted area balance constraint. Finally, an area balanced partition is performed to partition the large layers into regular ones. More details are shown as follows: (1) N cores are pre-assigned with its lowest allowable voltage levels, from which the proportion of number of cores among m levels can be obtained. A few candidates of combination of v lmap are generated according to the proportion. An example of candidates generation is illustrated in Fig. 3. (2) For each candidate, the problem of M LA(m, m) is solved with adjusted area balance for large layer l as: areai · xil ≤ k · Area + ξ · Area n Areal = N(cid:2) (7) i=0 4C-4 k is the number of layers sharing with voltage level l. Then the candidate with the best power optimization is selected. (3) Finally, an area-balanced k-cut partition is used to divide each large layer into k small ones, with consideration of inter-layer communication cost. 3.2.3 The case of m > n In the case of m > n, some layers use two or more voltage levels and have multiple voltage islands. Then the key issue becomes how to partition the islands and how to place them on each layer with the least power network resources and communication cost. We can use a similar method as 3.2.3 to decide which layers to place the islands. However, in this section, a new method is proposed: (1) First, a voltage assignment method for 2D NoC[17] is used to assign voltage levels to N cores without considering 3D information and area balance constraints. (2) Then a new communication graph is generated. For each edge in the graph, two weights are taken into account. On the one hand, to guarantee inter-layer communication cost, a communication weight is added into each edge. On the other hand, to maintain the original voltage island as much as possible, another cost is added to edges of cores in the same VI. Then an area-balanced n-cut partition can be performed to divide 2D result into n layers. (3) The partition results in previous stage are alternatively adjusted by combining small VIs into large VIs during the same layer to reduce total voltage number on all layers. If there are still multiple islands in each layer, vertical alignment constraints will be considered during ﬂoorplanning. 3.3 ILP Optimization Strategy As a result of area-balanced constraints, the ILP formulation is exact but time-consuming. To solve it, a ILP variablepruning method is proposed. First, two nouns are deﬁned. R corei denotes the increased power of core i from current voltage level to higher level; R comi denotes the potential communication power of core i. R comi is calculated by the following method: for each neighbor core j of core i, if C omij is larger than threshold φ and the voltage level of j is larger than i, then R comi is added by a large number M ; if comij is larger than threshold φ and j is an already assigned core, then R comi is reduced by M ; for any other neighbor core k of core i, R comi is added by C omik multiplied by δ if voltage level of j is larger than i and reduced by C omik multiplied by δ otherwise. If negative number is generated, then it is adjusted to 0. An example is shown in Fig. 4. The number on edge stands for the communication amount of two cores while the number above each core stands for the core power in current layer and higher layer. Assume M = 100, δ = 0.2 and φ = 150, the R core and R com of each core can be calculated as shown in Fig. 4. Since the ob jective of MSV-driven layer assignment is to assign cores to low voltage levels or layers as much as possible without increasing inter-layer communication cost under area-balanced constraint, the following strategy can be used: Initially, the cores are classiﬁed into m groups according to the lowest allowable levels of cores. Then R core and R com are calculated respectively for each core. A sort is performed in each groups according to the ratio of R core and R com diminishingly. The cores in each group are inserted into each layer until the area of layer exceeds a threshold area0 364 4C-4 NoC synthesis method as well as network components based 3D ﬂoorplanning is proposed. 4.1 3D NoC Synthesis 3D NoC is a 3D network including inner-layer networks and inter-layer interconnection. Hence 3D NoC synthesis can be divided into two parts: inner-layer NoC synthesis and inter-layer TSV planning. 4.1.1 Inner-layer NoC synthesis To generate network components, a similar method as [15] is used. Every possible number i of partitions in each voltage island is tried and cores are assigned into i min-cut clusters respectively. Then the solution with lowest communication power will be adopted. After that, the cores with larger communication requirements are assigned to the same cluster and use the same switch for communication. Then network interfaces can be easily generated for cores according to [16]. 4.1.2 Inter-layer interconnection generation Inter-layer interconnections are then generated based on the inner-layer NoC synthesis. First, the pairs of switches to connect through inter-layer interconnection are selected. Then TSV macros can be inserted into the dead-space near the target switches. The relative position of switch pairs and macros can be maintained during the ﬂoorplanning process. Finally, a network ﬂow based algorithm can easily solve the interconnection planning based on available routing resources in dead-space. We don’t discuss in detail here. 4.2 3D NoC Floorplanning In previous researches, the network components were assigned based on given ﬂoorplan or in the post-ﬂoorplan stage. In this case, the network components may not be inserted into the best position and the communication power cannot be optimized. In this paper, a 3D ﬂoorplanning with network components assignment is proposed. The position of network components can be improved through ﬂoorplanning process. Then a global redistribution method is performed for further improvement of communication power. 4.2.1 Floorplanning with network components The generated network components are added into the input of ﬂoorplanning and treated as small dummy cores while the actual cores are referred to real cores. To eliminate the redundancy in the solution space with dummy cores, the disturbance is partitioned into two stages. In the ﬁrst stage, both real cores and dummy cores are treated, while in second stage only dummy cores are thought of. The boundary of two stages can be determined by the dead-space ratio or disturbance times. In 3D NoC, ﬂoorplanning will be performed for each layer. The cost function in ﬂoorplanning of layer i is deﬁned: C osti = α·Areai+β ·V I Areai+γ ·C omi+λ· i(cid:2) j=0 V comij (8) where Areai represents the ﬂoorplan area of layer i, and V I Areai represents the sum of all voltage island areas; C omi stands for the total inner-layer communication power of NoC, while V comij denotes the inter-layer communication cost 1 2 4 3 5 6 1.0V 7 8 9 10 11 1.2V (a) 1.4V 1 7 1 7 1 (1) (2) (3) 2 8 2 8 2 3 9 3 9 3 10 11 6 11 11 6 9 6 L1: 1.0V L2: 1.0V L3: 1.2V L4: 1.4V L1: 1.0V L2: 1.2V L3: 1.2V L4: 1.4V L1: 1.0V L2: 1.2V L3: 1.4V L4: 1.4V 4 5 10 10 4 7 4 5 8 5 (b) Figure 3: Example of candidates generation for M LA(3, 4); (a)An example for voltage level clusters; (b)Three candidates of v lmap generated Level 2 (30,50) 100 2 200 150 Level 1 (30,50) 3 100 (20,30) 1 Assigned Cores Unassigned Cores R_Core1=10 R_Com1=120 R_Core2=20 R_Com2=0 (-150) R_Core3=20 R_Com3=0 (-20) Figure 4: Example of priority calculation for ILP optimization or the ratio is small enough. After that the variables for the already assigned cores are updated and R core and R com are re-calculated. More details for algorithm is shown in Algorithm 1. Similar method can be employed to the case of m > n by pre-assigning some non-critical cores. In this paper, area0 is set as half of the total area of cores in current layer. The eﬃciency of the method is shown in Section 5.1. Algorithm 1 Variable Pruning for ILP 3: 4: 5: 6: 7: 8: 9: Require: N , m, Li, C omij , LAVi ; 1: Sort cores according to lowest allowable level table; 2: for each i ∈ [1, N ] do if core i is assigned then continue; end if checkLargeC om(); R coreC aculation(i); R comC aculation(i); priorityC alculation(i); 10: end for 11: done = 0; 12: while done < m do for each i ∈ [1, s] do s = core number of layer m; if current area is less than max area then Assign core i; Update priority of adjacent cores of core i; end if end for done++; 21: end while 22: ILP solving of the rest unassigned cores; 13: 14: 15: 16: 17: 18: 19: 20: 4. 3D NOC SYNTHESIS AND FLOORPLAN 3D NoC synthesis aims to assign network components on each layer and determine the physical position of each core or component. A good synthesis result can greatly improve the total communication power. Previous works proposed many 3D NoC synthesis methods to optimize the communication power, but all of them are restricted to an input ﬂoorplan. As a result, the communication power cannot be optimized. In this paper, a 3D 365 4C-4 Table 1: Results of MSV-driven Layer Assignment Algorithm Benchmark L# V# E# D 38 tvopd D 36 D 52 D 76 2 3 4 2 3 4 2 3 4 2 3 4 38 38 38 36 36 36 52 52 52 76 76 76 46 46 46 42 42 42 61 61 61 92 92 92 Core Power 47.3 51.8 51.0 48.1 51.0 50.0 80.7 1 ILP Inter-layer Com 44.3 79.7 78.2 33.5 76.5 84.9 45.4 1 Run time(s) 0.68 0.47 64.9 0.49 24.7 21.9 12.64 >1000 >2000 >1000 >3000 >3000 1 Area Balanced Method Core Inter-layer Run Power Com time(s) 74.9 39.7 0.01 74.9 66.1 0.01 74.9 83.5 0.01 73.3 37.0 0.01 73.3 56.6 0.01 73.3 99.6 0.01 137.2 40.0 0.01 137.2 57.4 0.01 137.2 97.4 0.01 149.7 77.6 0.01 149.7 111.4 0.01 149.7 151.6 0.01 1 1 1 Core Power 47.3 51.7 50.8 47.9 50.8 49.8 80.7 94.1 93.2 94.4 104.1 101.8 0.998 0.664 Heuristic ILP Inter-layer Com 44.3 90.6 95.3 37.1 71.5 84.9 45.4 64.6 108.6 81.4 138.6 144.9 1.062 1.098 Run time(s) 0.13 0.02 0.42 0.19 0.48 0.89 1.50 0.02 1.02 1.75 63.0 95.4 0.018 through TSVs between layer i and the ﬁnished layers. The parameters α, β , γ , and λ can be used to adjust the relative weighting between the contributing factors. Graph partitioning tool hmetis [18] and lp solve [19] are used for switches generation and ILP solving. The core power and communication power are all normalized values. 4.2.2 Global redistribution for communication power 5.1 Effect of Layer Assignment Algorithm The ﬂoorplanning with both real cores and dummy cores make sure that enough dead-space is generated for network components. However, as a result of the randomness in disturbance, the position of components may not be optimized. A post-ﬂoorplan global redistribution is proposed for further improvement. First, all the dead-spaces are searched from the given 3D ﬂoorplan. For each network component, the priorities of all dead-spaces are calculated according to communication cost. Assume that component k is assigned to the deadspace i after ﬂoorplanning, then the communication cost can be calculated by the following formula: comC osti,k = N(cid:2) j=0 C omkj ∗ lengthij + N s(cid:2) t=0 C omkt ∗ lengthit (9) where N s is the total number of switches. The priority can be expressed by reciprocal of (9). If the remaining deadspace is not enough, the priorities will be set as 0. Then the best dead-space is selected and network component is reallocated into it. After that, the capacity of all deadspace is updated. The process is repeated until all network components are inserted. After redistribution, the components are inserted into the center of obtained dead-space. Then a simple LP optimization can be employed to obtain the ﬁnal best position of each NC under the constraint of non-overlapping. We don’t discuss in detail in this paper. 5. EXPERIMENTAL RESULTS In this section, three experiments are performed to show the eﬃciency of the proposed algorithms and framework. All experiments are performed on a workstation with 3.0 GHz CPU and 4GB physical memory. Four benchmarks are used in this paper. D 38 tvopd is used from [6] and the other three benchmarks D 36, D 52 and D 76 are derived from D 38 tvopd. Three voltage levels are assumed and the device layer number varies from 2 to 4 for all circuits. It is easy to be extended to more complex cases through combination of the simple cases. The power model of network components and communication is evaluated according to [15]. The core power and lowest allowable voltage level are treated as input. The ob jective of MSV-driven layer assignment is to optimize the total chip power and achieve trade-oﬀ between core power and inter-layer communication power. To make better comparison, an area balanced partition based layer assignment which aims to optimize the inter-layer communication power is also realized. As shown in Table 1, the pure ILP formulation is quite time-consuming. For large benchmarks, ILP can take more than thousands of seconds to solve. With heuristic speed-up strategy, the ILP can be speeded up greatly. Compared to pure ILP formulation, the heuristic ILP can save time by about 98.2%, with only 6.2% more inter-layer communication power for D 38 Vopd and D 36. Even for large benchmarks, Heuristic ILP can still solve in 100 seconds. Compared to the partition method, the proposed MSV-driven layer assignment can improve core power by 33.6% with only 9.8% increase on communication power, which achieves a good trade-oﬀ between core power and inter-layer communication power. 5.2 Effect of NoC Synthesis The eﬀect of NoC Synthesis is shown in Table 2. The results are divided into two patterns: (1) Inner optimization which just optimizes inner-layer communication power during ﬂoorplanning, (2) Inter optimization which optimizes both inner-layer communication power and inter-layer communication power during ﬂoorplanning and (3) Global optimization which further improves total power by post-ﬂoorplan global redistribution. Compared to inner optimization pattern, inter optimization can achieve trade-oﬀ of inner-layer communication power and inter-layer communication power. On average, the inter-layer communication can be improved by about 35.8% with only 3.8% increase on inner-layer communication power and the total communication power can be improved by 16.9%. Furthermore, global optimization can improve inter-layer communication by 60.8% and total communication power by 29.1%. 5.3 Effect of MSV-3DNoC In this section, we will show the eﬀect of MSV-3DNoC compared to MSV-driven 2D NoC. The results are divided into two patterns: MSV-2DNoC: a MSV-driven applicationspeciﬁc 2D NoC design[17]; MSV-3DNoC: The proposed MSV-driven 3D NoC. For each benchmark, ten results are 366 Table 2: Eﬀect of post-ﬂoorplan global redistribution Benchmark L# V# E# Inner Optimization Inter Inner Com Power Power Power 30.60 38.50 69.10 44.21 33.74 77.95 40.12 34.24 74.36 23.88 37.20 61.08 38.09 32.23 70.32 34.91 29.98 64.88 42.92 59.47 102.4 51.69 50.95 102.6 79.84 43.59 123.4 60.05 117.6 186.8 139.4 93.91 233.3 89.34 81.10 170.4 1 1 1 Inter Optimization Inter Inner Com Power Power Power 14.10 43.06 57.15 25.99 34.10 60.09 30.65 36.51 67.15 20.94 38.58 59.52 27.59 34.51 62.10 30.20 29.11 59.30 29.48 65.70 95.18 26.72 50.94 77.65 44.00 51.38 95.38 49.87 108.9 158.8 72.08 103.2 175.2 62.07 81.38 143.4 0.642 1.038 0.831 Global Optimization Inner Com Run-time Power Power (s) 44.24 54.12 37.02 34.40 48.77 48.16 38.00 52.10 53.44 39.85 49.63 39.54 29.07 48.43 46.74 32.41 53.86 47.00 66.25 85.33 59.82 50.81 71.99 53.41 49.03 80.70 59.14 112.6 139.1 65.13 105.1 144.9 60.36 81.57 119.2 82.29 1.047 0.709 Inter Power 9.869 14.37 14.11 9.786 19.35 21.46 19.08 21.18 31.67 26.45 39.77 37.59 0.392 D 38 tvopd 2 3 4 2 3 4 2 3 4 2 3 4 38 38 38 36 36 36 52 52 52 76 76 76 46 46 46 42 42 42 61 61 61 92 92 92 D 36 D 52 D 76 Ratio Table 3: Eﬀect of MSV-3DNoC Benchmark L# V# E# MSV-2DNoC Com Power 95.96 95.96 95.96 108.4 108.4 108.4 135.8 135.8 135.8 264.1 264.1 264.1 1 MSV-3DNoC Com Dead Power Space 54.12 12.65 48.77 18.29 52.10 17.83 49.63 18.02 48.43 17.84 53.86 20.11 85.33 18.16 71.99 16.49 80.70 21.10 139.1 23.83 144.9 13.36 119.2 18.65 0.523 1.034 Core Power 46.2 46.2 46.2 46.8 46.8 46.8 77.8 77.8 77.8 92.4 92.4 92.4 1 Dead Space 14.39 14.39 14.39 11.99 11.99 11.99 19.56 19.56 19.56 23.83 23.83 23.83 1 Core Power 47.3 51.7 50.8 47.9 50.8 49.8 80.7 94.1 93.2 94.4 104.1 101.8 1.098 Run-time (s) 44.7 37.7 53.4 39.5 27.9 48.1 59.8 42.1 59.1 65.1 60.3 82.2 D 38 tvopd 2 3 4 2 3 4 2 3 4 2 3 4 38 38 38 36 36 36 52 52 52 76 76 76 46 46 46 42 42 42 61 61 61 92 92 92 D 36 D 52 D 76 Ratio 0 20 40 60 80 100 120 140 160 0 20 40 60 80 100 120 140 160 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 0 20 40 60 80 100 120 140 160 0 20 40 60 80 100 120 140 160 3 9 11 16 18 22 28 32 34 37 41 47 49 54 56 58 60 66 67 70 72 75 0 20 40 60 80 100 120 30 140 160 0 20 40 60 80 100 120 140 160 1 2 5 6 8 13 15 17 19 20 23 25 27 31 33 39 40 43 44 46 51 53 55 57 61 63 65 68 69 71 Layer 0 Layer 1 Layer 2 NI Switch 1.0V 1.2V 1.5V Figure 5: Floorplan for D 76 selected and the average value is calculated. Fig. 5 shows the generated 3D ﬂoorplan for D 76, which are composed of three voltage levels for three layers. As shown in Table 3, compared to MSV-2DNoC, MSV3DNoC can improve communication power by about 47.7%, due to the reduction of communication distance. Compared to the signiﬁcant improvement, the increase on core power (9.8%) is acceptable. Furthermore, the total chip area will greatly reduced in spite of the increase on dead-space ratio. 6. CONCLUSION In this paper, a MSV-driven framework for applicationspeciﬁc 3D NoC design is proposed. Through a uniﬁed modeling method for simultaneously layer assignment and voltage assignment, 3D NoC synthesis and 3D ﬂoorplanning algorithm, the total power can be optimized. 7. "
2014,A vertically integrated and interoperable multi-vendor synthesis flow for predictable noc design in nanoscale technologies.,"We deliver a design flow for the synthesis and convergence of application-specific networks-on-chip. The flow comes with novel features that can better address nanoscale design challenges: front-end driven floorplanning, dynamic IR-drop minimization, fast and accurate system-level power grid modeling, predictable link design. Above all, such features are addressed by different prototype engines, even from different vendors, that can be smoothly integrated into the flow by means of a common specification format called Communication Exchange Format (CEF), that enables unprecedented tool interactions. This flow is validated by means of an extensive demonstration framework.","A Vertically Integrated and Interoperable Multi-Vendor Synthesis Flow for Predictable NoC Design in Nanoscale Technologies 4B-3 Alberto Ghiribaldi University of Ferrara, Italy alberto.ghiribaldi@unife.it Mikkel Stensgaard Teklatech A/S, Denmark ms@teklatech.com Herv ´e Tatenguem Fankem Federico Angiolini University of Ferrara, Italy herve.tatenguemfankem@unife.it iNoCs S ´aRL, Switzerland angiolini@inocs.com Tobias Bjerregaard Teklatech A/S, Denmark tb@teklatech.com Davide Bertozzi University of Ferrara, Italy davide.bertozzi@unife.it Abstract— We deliver a design ﬂow for the synthesis and convergence of application-speciﬁc networks-on-chip. The ﬂow comes with novel features that can better address nanoscale design challenges: front-end driven ﬂoorplanning, dynamic IR-drop minimization, fast and accurate system-level power grid modeling, predictable link design. Above all, such features are addressed by different prototype engines, even from different vendors, that can be smoothly integrated into the ﬂow by means of a common speciﬁcation format called Communication Exchange Format (CEF), that enables unprecedented tool interactions. This ﬂow is validated by means of an extensive demonstration framework. I . IN TRODUC T ION As of today, there are few (actually, not too many) design methodologies and CAD tool ﬂows for application-speciﬁc networks-on-chip (NOCs) [1, 2, 3]. Especially in industry, interconnect vendors complement their interconnect IP offering with tooling to automate the creation of the system interconnect based on the communication requirements of the SoC at hand. State-of-the-art design methodologies and toolﬂows typically cover a limited range of the whole design process, especially topology synthesis [9, 16, 13, 10, 15, 14], and rely on a library-based approach to NoC design, wherein predesigned soft macros are composed at instantiation time to build arbitrary topologies. In all cases, when we look at current toolﬂows with respect to future system requirements, we can identify the following criticalities: A. EDA ﬂow interoperability. From the system designer’s viewpoint, the integration of various tools is a common requirement throughout the whole NoC synthesis ﬂow. Also, in a design ﬂow that requires largely interdisciplinary skills, it is very common that such tools come from different vendors, each focused and specialized on a speciﬁc design step. If tool interoperability is not properly addressed in a multi-vendor ﬂow, the design cycle may largely prolong. B. Beyond composition: integration. NoC synthesis ﬂows (possibly multi-vendor) today rely on the juxtaposition of different design steps rather than on their full integration, thus missing global visibility and optimization opportunities. This causes, among the other things, poor inter-play between frontend and back-end design, which is prone to a lengthy and possibly inefﬁcient convergence process. In the future, integration of tools into the whole NoC synthesis ﬂow will have to be mastered by a backbone design methodology with global visibility, taking care of smart tool sequencing and of their interplay. C. Evolving technology requirements. Today we are at a main transition point, where new features need to be there in design ﬂows, all related to the intricacy of nanoscale designs. Issues such as process variations, power grid integrity, interconnect delay, etc. cannot be addressed as an afterthought any more, but from the ground up. At the same time, designs should be ranked with respect to these issues not late in the design ﬂow, which may lead to lengthy iterations, but rather early, during a pruning step of the design space relying on technology awareness. With respect to state-of-the-art, we develop a comprehensive and interoperable multi-vendor synthesis ﬂow for applicationspeciﬁc nanoscale NoCs, and we present a structured validation framework of the novel features of this ﬂow. In particular: Claim A: We deliver tool interoperability through the deﬁnition of a common and open speciﬁcation format (named CEF), that provides a consistent representation of design data across all layers of the design process. Through the CEF format, design layers (and associated tools) exchange useful information, design intents, or directives. Therefore, CEF is the backbone of the proposed ﬂow. Validation Means: Tool Interoperability is explicitly proved by running the proposed multi-vendor ﬂow for the design of a multimedia system, and by proving convergence (section IV.B). The CEF format is the actual means of interoperability between mainstream and prototype tools, building up the ﬂow. Claim B: We deliver a vertically integrated design ﬂow, meaning that (i) the entire ﬂow is addressed, and (ii) global scope of the ﬂow allows us to anticipate ﬂoorplanning, a typical back-end design step, in the front-end, thus biasing topology synthesis toward the most efﬁcient physical implementations. In addition, it is not just the length of wires that matters when synthesizing a topology: for the sake of low power, large amounts of trafﬁc should be carried by short links overall; this is achieved by tightly coordinating ﬂoorplanning and topology synthesis, together with use-case speciﬁcation, in the proposed ﬂow. Validation means: Claim B is validated in section IV.C by proving the correlation between the communication cost metric used by the ﬂoorplanner tool to rank ﬂoorplans and the actual NoC dynamic power consumption of synthesized topologies, measured with post-layout analysis. This proves that topology synthesis has actually been driven to the most promising physical implementation. 978-1-4799-2816-3/14/$31.00 ©2014 IEEE 337 Claim C: The proposed ﬂow coherently integrates mainstream industrial tools with new prototype tools addressing the most daunting challenges of NoC design in nanoscale technologies. C1. In existing mainstream ﬂows, analysis of the power delivery network is not available until after physical implementation. At this stage, the turn-around-time is often too long to justify changes to the ﬂoorplan. As a solution, the proposed ﬂow includes a fast, early-phase power delivery network analysis engine and integrates this into the prototype front-end ﬂoorplanner tool. The user can thus create ﬂoorplans that are optimized for system-level communication (see Claim B) as well as for power integrity. C2. NoC links are the major cause of lengthy design iterations between front-end and back-end designers. With the proposed ﬂow it is possible to realistically project the actual length of NoC links, and even insert repeater stages upfront in order to meet predeﬁned timing constraints, thus avoiding lengthy design iterations between front-end and back-end designers. Validation means for claim C: The ﬂow is put at work for the synthesis and physical convergence of an on-chip network for a media-rich mobile embedded system. The validation mean is the ﬁrst-time-right design of this chip on a 40nm industrial technology library. I I . CEF F I L E FORMAT CEF (Communication Exchange Format) is an open design format which speciﬁes SoC-level architecture and communication infrastructure. It is designed to ease tool interoperability and reduce the iterative exploitation of tools at different stages and abstraction levels of the design ﬂow. CEF allows for the expression of both design intent (objectives and constraints) and design implementation, describing for example: (i) system cores, including relevant interface parameters; (ii) communication requirements across cores, with support for multiple use cases; (iii) clock and power domains, including frequency and voltage scaling; (iv) interconnect implementation, including key architectural parameters and routes; (v) rough ﬂoorplan of the design, including wire length annotations. The CEF format allows for iterative and incremental design steps. For example, communication requirements can be provided in a coarse way or modeled accurately. The ﬂoorplan of the design can be omitted, then added when available, and subsequently modiﬁed. New usage scenarios, updated NoC revisions, or even entirely new system cores can be added at any time, either reﬂecting engineering changes or the better understanding of the system as development proceeds. CEF was conceived to ease the interoperability of tools involved in the development of on-chip interconnects. To this end, it has been designed to be expressive enough to capture the key design steps and abstraction layers involved in the design process. CEF does not enforce a strict order in which these tools must be used, instead promoting rich interaction between tools, back-annotation of parameters and successive reﬁnements. CEF has no aim to replace existing formats where established, useful standards exist, nor to model out-of-the-box ev4B-3 Fig. 1. CEF-enabled interoperability between NoC design tools. ery nuance of the system. For example, at the architectural level, CEF does not replace existing NoC models written either in C++, SystemC, Verilog, or other languages, nor it models in detail the architectural parameters of a speciﬁc vendor’s NoC library. Instead, CEF provides generic, broadly applicable, syntax to specify router connectivity, with only some select properties (e.g. virtual channels, ﬂit widths, buffer depths) explicitly included. The set of built-in properties was carefully chosen so as to remain generic and vendor-independent, yet able to express the key parameters that impact in a major way the performance/area/power trade-offs. Rather than as an intermediate representation format, the CEF format is devised as a data exchange format. The need to maximize the interoperability of different tools has motivated the choice of the XML to make the format easily consumable by machines while remaining readable by humans. Figure 1 shows an example of the different interconnect design tools that can interoperate on a single target system speciﬁcation thanks to CEF. The tools operate at different stages of a design process, but can now be seamlessly mixed and matched instead of being forced in a sequential ﬂow, enriching the backannotation and optimization opportunities. For instance: - A NoC architectural simulator can now take into account wire propagation delays and variability effects identiﬁed by backend tools; - High-level exploration tools, optimizers and synthesizers can have a full view of constraints (e.g. communication requirements, block distances) and opportunities (e.g. architectural knobs such as buffering); CEF was developed by the NaNoC consortium, including both academic and industrial partners with key expertise in system interconnect design. Since this paper is focused on the NoC synthesis ﬂow as a whole, the interested reader is referred to [5] for CEF details. I I I . THE F LOW AT A G LANC E The design ﬂow described in this paper revolves around physical design convergence. The main concepts are to (i) automate the ﬂoorplanning of such heterogeneous systems accord338 	           Fig. 2. Proposed ﬂow for the implementation of application-speciﬁc NoCs targeting heterogeneous systems. ing to their communication and power integrity requirements, (ii) perform NoC topology synthesis based on the resulting ﬂoorplan, (iii) a predictable physical synthesis process due to the early modeling and consideration of key technology-level convergence threats in the ﬂow (namely interconnect delay and power grid integrity). A comprehensive overview of the ﬂow is reported in Figure 2. A.Front-End The CEF ﬁle initially includes high-level speciﬁcations of the system, including communication bandwidth annotations between pairs of IP cores, speciﬁed on a use-case basis. A Liberty ﬁle characterizing the dynamic power usage of each IP core is also fed as input to the ﬂow. System ﬂoorplanning is performed before the synthesis of the NoC topology. Knowledge of the communication streams is used to direct the ﬂoorplanner tool to place closer together system blocks that communicate the most. Thus the total power of the NoC, once implemented, is lower, because the routing paths carrying high bandwidth streams are short. To enforce this behaviour, an abstract communication cost metric is deﬁned by multiplying the bandwidth of communication streams by the distance between the associated communicating blocks in the ﬂoorplan. This metric is included in the objective function of the ﬂoorplanning tool. The IR-drop of a design is greatly inﬂuenced by its coarsegrained placement, i.e., its ﬂoorplanning. Hence, in order to ensure power integrity closure, analysis of the power delivery network is warranted as early in the ﬂow as possible [11]. Therefore, in the proposed ﬂow the ﬂoorplanner not only reduces the communication cost function, but is also augmented to include an early-phase, dynamic IR-drop analysis directly in the optimization loop. This analysis leverages a prototype high-level power delivery network speciﬁcation, and is fast enough (less than 50ms) for inclusion into the automated ﬂoorplanning process in the design front-end stage. As a result, the user can create ﬂoorplans that are optimized for system-level communication as well as for power integrity. With respect to commercially available tool ﬂows, ﬂoorplanning frameworks typically miss awareness of system-level design properties. Moreover, IR-drop analysis is not available until after physical implementation. The above novel features of the proposed ﬂow have been incorporated, for the sake of experimentation, into the existing Teklatech FloorDirector tool [4]. Plug-and-play of this tool into the ﬂow was straightforward by making it support CEF as its input and output format. 4B-3 EŽůŽĐŬ >ŝďƌĂƌǇ /ŶƐƚĂŶĐĞ ^ĞůĞĐƚŝŽŶ EŽůŽĐŬ /ŶƐƚĂŶĐĞƐ dĞĐŚ >ŝďƌĂƌǇ ^ĐƌŝƉƚĞĚ ĂĐŬͲŶĚ &ůŽǁ ;^ǇŶƚŚĞƐŝƐ͕ WůĂĐĞ͕ZŽƵƚĞͿ /ƚĞƌĂƚĞŽŶƚŝŵŝŶŐͬZ ǀŝŽůĂƚŝŽŶ EŽƌĞĂ DŽĚĞůƐ ^ǇŶƚŚĞƐŝƐ ƌĞƉŽƌƚƐ /ŶƚĞƌƉŽůĂƚŝŽŶ ;ǆƚƌĂƉŽůĂƚŝŽŶͿ EŽWŽǁĞƌ DŽĚĞůƐ EŽ&ƌĞƋƵĞŶĐǇ EEEEEEEEEEEEE DŽĚĞůƐ Fig. 3. NoC Topology Synthesizer: block characterization ﬂow. After the ﬂoorplanning stage the CEF ﬁle is reﬁned to include the IP core placement. After this, it is handed over to the topology synthesis stage, where the Network-on-Chip is built according to the communication requirements of the system, described in the CEF too. The most relevant requirements for topology synthesis in the proposed ﬂow are back-end modeling (parametric NoC component area and speed models, abstract component power models, interconnect delay models on the target technology), ﬂoorplan awareness for topology synthesis, and RTL automatic generation. These features were to a large extent exposed by the commercial iNoCs NoC synthesis toolchain [3], which was then plugged into the whole ﬂow by making it support the CEF format. The key novelty consisted of extending iNoCs framework to synthesize application-speciﬁc topologies on top of pre-assigned ﬂoorplans, delivered by a tool from a different vendor. The ﬁnal topology synthesis ﬂow relies on NoC components characterization to compare different topology design points and block parameter settings. The characterization (Figure 3) relies on the complete synthesis, placement and routing of some instances of NoC RTL blocks, including Network Interfaces (NI), switches and links. This process yields power, area and maximum frequency values for those instances. Subsequently, by means of interpolation and extrapolation, predictions are made for all other possible instances of the library. The baseline iNoCs’ topology synthesizer was extended to accept a system ﬂoorplan from a CEF description as an input. The increasing impact of wiring, in terms of power and propagation delays, strongly suggests that it is essential to take into account the placement of the system cores in order to better estimate and optimize the interconnect [8, 12]. If such a ﬂow is followed, it is possible to better estimate the power consumption of the interconnect, and to design the topology (including, where needed, automatically instantiating link repeaters to break critical timing paths) to meet timing constraints from the ground up. In particular, the topology synthesis tool: - estimates wire length with Manhattan distance, or more complex routing when hard macros are in the way; - takes decisions based on a pre-characterization of wire performance in the target technology library; - meets timing constraints on links through smart switch ﬂoorplanning and link segmentation, thus making convergence of P&R more predictable. In order to guarantee smooth integration of this step into the whole ﬂow, the insertion of NoC components into the design does not lead to expanding the ﬂoorplan boundaries (Figure 4). In fact, power grid is designed before NoC generation, hence expanding the ﬂoorplan would impair the assumptions of the previous stage. To minimize the concern, while trying to avoid lengthy iterations, optimizations were performed in the iNoCs ﬂoorplanning engine to converge towards more compact ﬂoorplans. 339 4B-3 (a) Input ﬂoorplan for a SoC design (b) Output ﬂoorplan with NoC components instantiated Fig. 4. The NoC synthesizer inserts the NoC blocks into input ﬂoorplan by minimally perturbing block positions. RTL of a complete NoC topology, together with its succinct CEF description, are then handed over to the physical synthesis ﬂow for P&R. B.Back-End The RTL design is translated into a gate-level netlist mapped to the target technology library (tool used: Synopsys Design Compiler), according to the enforced constraints (mainly frequency and operating conditions). Boundary timing constraints have been conservatively set to 25% of the fastest clock in the design in order to take into account the delay of inter-switch links, 100× capacity of biggest inverter in the library has been used as output pin capacitance, and 0.4ns as transition time of signals at input pins. The generated netlist is the starting point of the layout generation process, performed with Synopsys IC Compiler. We implemented a concurrent hierarchical Layout Generation methodology, exploiting a top-down approach. This ﬂow is presented in Figure 5, and reﬂects mature industrial layout ﬂows [6]. After the design has been imported, the ﬂoorplan deﬁnition (in particular die area and architectural blocks’ position) is derived according to speciﬁcations extracted from CEF ﬁle. Then, power and ground grids are generated considering speciﬁcations from ﬂoorplanner tool. Next step is the partitioning of the Network-on-Chip into its fundamental components, thereby making it possible to perform place&route on each block concurrently and independently. This step is necessary to reduce layout generation complexity, machine runtime and to have tighter control on the layout generation process. Each block is then saved as a soft macro, so that only global routing between blocks is required in the top level. Once layout is completed, soft macros are “uncommitted”, i.e. reverted back to their composing standard cells, and ﬁnal global reﬁnements (power and/or timing driven optimizations) are performed. The outcome of this step is a completely placed and routed netlist, together with the associated parasitic effects. At this point, accurate power analysis can be performed, by annotating switching activity on the resulting design. This is done by injecting trafﬁc patterns that reﬂect the predeﬁned use cases using for instance the Modelsim tool, and performing timing and power analysis with proper tools (e.g., the Synopsys PrimeTime suite). IV. EX P ER IM EN TA L R E SULT S The target experimental setting is a multimedia chips for mobile phones (or similar devices), which runs different use Fig. 5. Concurrent Hierarchical Layout Generation Floorplan Best/Worst Best/Best Worst/Worst Worst/Best CommCost 15.83 18.87 43.91 44.52 IR-Drop (mV) 386 166 383 155 FOUR S E L EC T ED FLOOR P LAN S IN EX TR EM E CORN ER S W I TH R EGARD TO COMMCO S T AND IR -DRO P. TABLE I cases. Its technical speciﬁcations reﬂect projected industrial trends of multimedia chip for the near future. This design under test features 25 IP Cores, which include CPU, Graphic Accelerator, various memory banks and peripherals. Further details in [7]. Extrapolating the usage scenarios of existing smart phones, communication requirements of CPU, hardware accelerators and memory for video playback have been scaled up to the high-end HD-TV resolution. The target technology library is an industrial 40nm 1.20V Low-Power CMOS Standard Cell library. The goal of this section is to demonstrate that abstract metrics (the communication cost) and modeling frameworks (IR drops) used in the early steps of the proposed toolﬂow maintain a strong correlation with the physical properties of the design. In other words, we demonstrate that we are not pruning efﬁcient points from the design space as an effect of abstract modeling of design properties or inaccurate/misleading objective functions in early-stage optimization tools. We thus selected 4 ﬂoorplan design points (Table I), representative of the design space, and fed them to the next steps of the ﬂow to prove correlation: promising design points will actually outperform the others as the design is reﬁned. A.Floorplanning&TopologySynthesis,claimsAandB At this level, a correlation study was performed by synthesizing a large number of topology design points for the ﬂoorplans. The outcome can be seen in Figure 6, where dots of different colors represent topologies built on different reference ﬂoorplans. Recall that the communication cost computed by the ﬂoorplanning tool is expected to correlate with power consumption. The “red” and “maroon” dots represent solutions for the two ﬂoorplans with the lowest communication cost, and are very similar in actual power; the “blue” and “dark blue” dots 340 4B-3 ZƵŶϭсǁŽƌƐƚ /ZͬďĞƐƚĐŽŵŵ͕ ĐŽŵŵсϭϱ͘ϴϯ ZƵŶϮсďĞƐƚ/Zͬ ďĞƐƚĐŽŵŵ͕ ĐŽŵŵсϭϴ͘ϴϳ ZƵŶϯсǁŽƌƐƚ /Zͬ ǁŽƌƐƚĐŽŵŵ͕ ĐŽŵŵсϰϯ͘ϵϭ ZƵŶϰсďĞƐƚ/Zͬ ǁŽƌƐƚĐŽŵŵ͕ ĐŽŵŵсϰϰ͘ϱϮ EŽĐϭϰͺϮϰƐǁͺϲϰďŝƚƐͺƌƵŶϮ  z  E d  > ϲϰ/d^ EŽĐϭϯͺϮϱƐǁͺϲϰďŝƚƐͺƌƵŶϮ ƌƵŶϮ EŽĐϭϭͺϮϬƐǁͺϲϰďŝƚƐͺƌƵŶϮ EŽĐϭϭͺϮϬƐǁͺϲϰďŝƚƐͺƌƵŶϬ EŽĐϭϮͺϮϭƐǁͺϲϰďŝƚƐͺƌƵŶϬ EŽĐϭϬͺϭϲƐǁͺϲϰďŝƚƐͺƌƵŶϬ EŽĐϭϱͺϮϱƐǁͺϲϰďŝƚƐͺƌƵŶϬ WKtZ;ŵtͿ NoC Interconnect Timing Convergence Clock Target Domain Frequency clk Audio 100MHz clk CPU 500MHz clk DDR 250MHz clk DMA 200MHz clk DSP 300MHz clk Radio 150MHz clk USB 200MHz clk SPI 128MHz clk SRAM 500MHz clk Video 300MHz Slack Pre-Opt 5,75ns 0,20ns 0,38ns 0,65ns 0,34ns 1,12ns 0,53ns 6,33ns -0,19ns 0,38ns Slack Post-Opt 3,45ns 0,09ns 0,17ns 0,39ns 0,19ns 0,82ns 0,23ns 2,27ns 0,14ns 0,20ns Fig. 6. Topologies generated by the Synthesizer engine for the various ﬂoorplans for different ﬂit widths. T IM ING CONV ERG ENC E FOR A L L C LOCK DOMA IN S W I TH PRO PO S ED M E THODO LOGY, B E FOR E AND A F T ER G LOBA L O P T IM I ZAT ION TABLE III Corner Comm/IR Best /Worst Best /Best Worst /Worst Worst /Best 25 25 16 16 (mm2 ) Switch Flit Reserved NoC Count Width Area Power (bits) (mW ) 64 47.5 64 46.1 64 51.2 64 52.7 1.73 1.72 1.54 1.54 MA IN M E TR IC S O F TH E FOUR S E L EC T ED D E S IGN PO IN T S , ON E P ER IN PU T FLOOR P LAN , A S E S T IMAT ED BY TH E NOC SYN TH E S I Z ER . TABLE II represent solutions for the two ﬂoorplans with the highest communication cost, and are also clustered together. Indeed, there is a measurable difference (typically around 5 mW, or almost 10%) between “good” and “bad” ﬂoorplans. This separation is an average across use cases, and since this includes e.g. the Idle scenario in which the NoC is in fact inactive, it is clearly a conservative estimate; a separation of up to 15 mW or 25% was in fact noticed in trafﬁc-intensive use cases. Moreover, the topologies built on a worse ﬂoorplan exhibit average ﬂow latencies that are slightly higher, due to the need to insert, on average, a few more pipeline stages along links. For the rest of the study, four speciﬁc design points among those produced by the Noc Synthesizer chosen, one per input ﬂoorplan. Without lack of generality, the ﬂit width was ﬁxed to 64 bits. Out of all the possible solutions at this width, we selected representative instances with low latency and typical power for the cloud of solutions based on the same ﬂoorplans. These four solutions are summarized in Table II. Please note that reserved area takes into account a target cell occupancy of 50%. The design points at this stage consist of ﬂoorplans and of application-speciﬁc NoC topologies at the RTL level, customtailored for the ﬂoorplans and for the communication requirements of the application at hand. They have been derived by using tools from different vendors (Teklatech, iNoCs) coherently integrated into the same ﬂow, and they have been showed to maintain correlation of power values across the hierarchy so far. With this respect, claim A is validated, and partly also claim B about correlation. The design points now undergo physical synthesis, involving the interplay with mainstream EDA tools and requiring to preserve correlation across the back-end design steps. B.PhysicalConvergence,claimsAandC2 The target design includes 10 clock domains. Since synchronization is taken care by Dual-Clock Fifos, clock signals have been deﬁned as asynchronous to each other so the tool will not balance the sinks among the clock nets. In addition, a clock uncertainty of 10% has been enforced on every domain, to account for the possible clock skew. During Place&Route, a hierarchical design ﬂow is a must for an interconnect structure which is highly irregular, beyond being distributed. In order to have a tighter control of the results, Hard Bounds are created to deﬁne the areas where NoC building blocks will be placed. Results: Total NoC area after layout turns out to be 946.152μm2 for Best Comm./Best IR drop topology and 898.744μm2 for Worst Comm./Worst IR drop topology, out of a chip area of 6855μm × 6855μm. Aggressive block boundary constraints applied to NoC switches led to timing convergence after the top level integration and global routing of the architectural blocks (3rd column of Table III). Indeed, every clock domain has a positive slack, and only one has a small violation due to the unmatched sizing of a few gates between intra-switch links and switch output cells. This conﬁrms that the link delay prediction and design engine of the topology synthesis tool have done a good job. Given this excellent starting point, the tool does not struggle during the global optimization phase to meet the timing requirements. Therefore, it can afford relaxing the faster paths to reduce area and power consumption (4th column of Table III). Overall, the described methodology actually delivers the fast convergence claimed by our ﬂow (i.e., claim C2, or ﬁrst-time right physical design), and completes the validation of claim A by proving effective integration of front-end and back-end multi-vendor tools. C.CorrelationofDynamicPower,claimB It is indeed possible to prove correlation between Sign-off and Early-phase total NoC power numbers in communicationintensive use cases, such as in “Video Playback”, and “Video Capture” (see Figure 7). Deviations never exceed 8% for both “Best Comm” and “Worst Comm” ﬂoorplans. 341  4B-3 Fig. 7. Correlation of NoC power between early-phase and post-layout analysis. Only in those use cases where the network is heavely underutilized (“LTE modem”), and approaches the idle use case, the power gap can be as large as 25%. This is due to the different choice of ﬂip ﬂops the synthesis tool made during the power model extraction methodology (Figure 3) and the actual physical synthesis of the design at hand. This difference is at such a low level of abstraction that it becomes extremely difﬁcult to account for it upfront. Given the large amount of speciﬁc timing and Design Rule optimizations each topology undergoes, all requiring different effort, and the necessarily more abstract power modeling performed during topology synthesis, these results ultimately validate claim B. D.CorrelationofIRDropMaps,claimC1 This section needs to demonstrate that the IR drop maps derived by the front-end ﬂoorplanner (during the early phase analysis) and the same maps derived by the signoff tool indicate the same trends and critical hotspots. The In-Design Rail Analysis extension of Synopsys ICC was used as the reference sign-off tool. It is worth observing that during early-phase analysis, only the current consumption of the IP cores is considered for IR drop analysis, in that the on-chip network has not been synthesized yet. To match (and later validate) this assumption, in the ﬁrst place a special layout was inferred with the IP cores only. In Figure 8 we report the IR drop maps displayed by Teklatech’s early-phase ﬂoorplanning tool and the one displayed by Synopsys ICC for this design, with reference to the “Best Comm/Best IR drop” ﬂoorplan. The correlation between the two maps is evident: they both agree on indicating the top-left quadrant of the design as the most critical in terms of IR drops. A similar correlation was observed for the “Worst Comm/Worst IR drop” ﬂoorplan. When we compared the two ﬂoorplans (and their IR drop maps) with each other, we noticed that relative rankings between early-phase and signoff are in good agreement (degradations in the order of 2.3x and 1.5x respectively), thus conﬁrming that the front-end ﬂoorplanner would have made the right choice when selecting the “Best Comm/Best IR drop” one. Finally, we derived that the on-chip network has a maximum IR drop which is from 80 to 90% lower than that of IP cores, depending on the baseline ﬂoorplan. If we also look at the IR drop maps for the NoCs in isolation, we can see that they peak in almost the same physical spot of the IP core maps. This means that they do not even change the trend of the IP core maps. Therefore, it was hereby possible to validate the ﬂoor 	    Fig. 8. Floorplan with best communication and best IR drop. plan tool’s assumption to perform early-phase power integrity check by neglecting the on-chip network. V. CONC LU S ION This paper reports a vertically integrated, interoperable and multi-vendor synthesis ﬂow for NoCs. It ranges from application trafﬁc speciﬁcation to layout generation and physical convergence, and integrates prototype tools with mainstream industrial tools. Its main innovations include full vertical integration, global optimization of design steps for technologyaware design, ﬂow extensions to deal with nanoscale designs. The validation strategy of the ﬂow revolved around ﬁrst-timeright design and the proof of correlation of early-phase analysis and design choices with signoff. ACKNOW L EDGM EN T This work was supported by NaNoC and vIrtical European Projects (FP7-ICT-248972 and FP7-ICT-288574). “www.nanoc"
2014,A comprehensive and accurate latency model for Network-on-Chip performance analysis.,"In this work, we propose a new, accurate, and comprehensive analytical model for Network-on-Chip (NoC) performance analysis. Given the application communication graph, the NoC architecture, and the routing algorithm, the proposed framework analyzes the links dependency and then determines the ordering of queuing analysis for performance modeling. The channel waiting times in the links are estimated using a generalized G/G/1/K queuing model, which can tackle bursty traffic and dependent arrival times with general service time distributions. The proposed model is general and can be used to analyze various traffic scenarios for NoC platforms with arbitrary buffer and packet lengths. Experimental results on both synthetic and real applications demonstrate the accuracy and scalability of the newly proposed model.","4B-1 A Comprehensive and Accurate Latency Model for Network-on-Chip  Performance Analysis    Zhiliang Qian1, Da-Cheng Juan2, Paul Bogdan3, Chi-Ying Tsui1, Diana Marculescu2 and Radu Marculescu2  1Electronic and Computer Engineering Department, Hong Kong University of Science and Technology, Hong Kong  2Electrical and Computer Engineering Department, Carnegie Mellon University, Pittsburgh, U.S.A  3Ming Hsieh Department of Electrical Engineering, University of Southern California, Los Angeles, U.S.A  Abstract- In this work, we propose a new, accurate, and  comprehensive analytical model for Network-on-Chip (NoC)  performance analysis. Given the application communication  graph, the NoC architecture, and the routing algorithm, the  proposed framework analyzes the links dependency and then  determines the ordering of queuing analysis for performance  modeling. The channel waiting times in the links are estimated  using a generalized G/G/1/K queuing model, which can tackle  bursty traffic and dependent arrival times with general service  time distributions. The proposed model is general and can be  used to analyze various traffic scenarios for NoC platforms with  arbitrary buffer and packet lengths. Experimental results on  both synthetic and real applications demonstrate the accuracy  and scalability of the newly proposed model.  I Introduction  Network-on-Chips (NoCs) have been proposed as an  efficient and scalable solution to support complex on-chip  communication for a variety of multi-core systems [1]. In  order to cope with tight design constraints, design space  exploration has been used extensively but turned out to be a  major issue in many practical scenarios. More precisely,  detailed simulation can suffer from long evaluation times and  so only be used to estimate a small subset of alternatives [2,3].  Therefore,  it  is crucial  to develop fast and accurate  performance analysis tools that can guide the design space  exploration during the overall optimization process.  Towards this end, several analytical models have been  proposed for NoC performance estimation (Fig. 1) [2,3,4].  Based on the application and the target NoC architecture, the  performance model typically works together with the design  space exploration tool in the inner optimization loop. The  traffic input to the performance model is first extracted from  the current configuration (e.g., task mapping, core floorplan  and routing algorithm). Then, the analytical model estimates  the performance metrics of interest (e.g., latency, throughput)  in order to guide the pruning of the design space.   This paper presents a new delay modeling methodology  that can work for a variety of NoC configurations and traffic  scenarios. More specifically, the proposed analytical model  utilizes G/G/1/K queuing formalism and generalizes the NoC  latency evaluation as follows: (1) The existing traffic arrival  modeling using Poisson approximations [3,5] is extended to a  generalized exponential (GE) packet inter-arrival distribution  which can account for bursty and dependent traffic patterns.  9 8 7 6 5 4 3 2 1 0 7. 7 5 7 . 8 7 .8 5 7. 9 7. 9 5 8 5 x 1 0 Figure 1 The proposed analytical performance model shown in the  synthesis loop for design space exploration.   (2) The packet service process within each router is modeled  by a general distribution to account for the service time  correlation between routers and traffic flows. (3) The NoC  architecture flexibility is enhanced to consider routers with  finite buffer size and enable arbitrary buffer size and packet  length combinations. 4) Finally, by examining the link  dependencies, the proposed framework extracts the core  communication graphs  that can be used  for  further  performance analysis. It is worth mentioning that this  framework is completely generic and it can be applied to any  NoC topology for which both the task mapping and routing  algorithm are known.   The rest of this paper is organized as follows. In section II,  we review related work and highlight the paper contributions.  In section III, we present the proposed analytical model.  Section IV shows the experimental results. Finally, we  conclude by summarizing our main contributions in section V.   II. Related work and paper contributions   For the general class of network-based systems, much of  prior work considers the modeling of wormhole (WH) routers  under the assumption of Poisson arrival time and memoryless  packet service time distributions. For example, in [6], an  M/M/1 approximation for the link delay is used to analyze  capacity and flow allocation. Although generally tractable, the  accuracy of M/M/1 models can be significantly compromised  as the assumption of exponential arrival and service time  distributions may not hold in many real applications [7,9]. To  address this shortcoming, several works have been proposed  to improve the estimation accuracy by generalizing the arrival  and service time distributions. More precisely, in [8], an  M/G/1/K queue based analytical model is proposed to account  for finite size input buffers in local area networks (LANs).  978-1-4799-2816-3/14/$31.00 ©2014 IEEE 323             Table 1 Comparisons of NoC analytical models  Previous NoC analytical models   [6]   [3,4]   [2]   [5]  Queue  M/M/1  Arrival  Poisson  Service  Markov  (cid:143)(cid:3)(cid:4666)(cid:1576) (cid:883)(cid:4667)  Small  Buffer  PB1  Arbiter Round  Robin  Application traffic modeled  M/G/1/K  G/G/1  M/M/m/K  Poisson  General  Poisson  General  General Markov  NoC architecture modeled  K packets  B flits  Small  <1  arbitrary  Round  Fixed  Robin  Priority  (cid:143)(cid:3)(cid:4666)(cid:1576) (cid:883)(cid:4667)  Round  Robin  1PB ratio is defined as the ratio of average packet size ((cid:1865) flits) to the buffer depth ((cid:1828) flits)  However,  the model  is based on  the Laplace-Stieltjes  transform and so is too complicated to be used in a typical  NoC synthesis loop like in Fig.1. In [3], an M/G/1 based  latency model for NoC analysis is proposed. It only assumes a  Poisson process for the arrival rate of header flits (as opposed  to the entire packet). In [2], a fixed-priority G/G/1 based NoC  latency model has been proposed which further attempts at  modeling  the bursty  arrival  times with  a 2-state  Markov-modulated Poisson process (MMPP). However, this  approach targets a specific priority-based router architecture,  while many NoC routers can utilize a more fair arbitration  such as the round robin (RR) scheme. In [5], an M/M/m/K  queue-based analytical model is proposed to analyze the delay  of NoCs with variable virtual channels per link. This approach  assumes negligible flit buffers (i.e., single flit buffer) such that  a packet reaches its destination before its tail leaves the source  host. In [4], an M/G/1/N queuing mode is proposed for both  wormhole and virtual channel NoCs. However, this approach  assumes that the granularity of the buffers is given in terms of  packets instead of flits and therefore a single channel buffer  can hold up to N packets during the analysis. This may not be  the case for NoCs whose buffers are rather small (only several  flits) to save area and power [2].    In this paper, we propose a new NoC latency model which  generalizes the previous work by considering: (i) the arrival  traffic burstiness, (ii) the general service time distribution, (iii)  the finite buffer depth and arbitrary packet length. For clarity  purposes, in Table I, we summarize and compare our proposed  analytical frameworks against other models proposed to date.  The newly proposed model offers a much broader coverage  for a variety of temporal and spatial traffic patterns, as well as  NoC architectures;  this provides more flexibility when  exploring the NoC design space.   III. NoC modeling for performance analysis   destination tile addresses for each specific flow (cid:1858) in the  A. Basic assumptions and notations  We assume that the target applications have been scheduled  and mapped onto the target NoC platform and the source and  application are known. Also, borrowing from the idea of  modeling bursty traffic in hyper-cube multicomputers [16], we  324 This  work  G/G/1/K  General  General  B flits  arbitrary  Round  Robin  4B-1 (cid:1865)  (cid:1828)  (cid:1834)(cid:3046)  (cid:1858)(cid:3046)(cid:481)(cid:3031)  (cid:1864)(cid:3028)(cid:3029)  (cid:1856)(cid:3033)  (cid:1842)(cid:3033) ((cid:1842)(cid:3046)(cid:481)(cid:3031) (cid:4667)  (cid:1864)(cid:3036)(cid:3033)  (cid:2019)(cid:3033) (cid:4666)(cid:2019)(cid:3046)(cid:481)(cid:3031) (cid:4667)  (cid:1829)(cid:3033)(cid:2870)  Table 2 Parameters and notations in analytical modeling  Parameter  Explanation  Service time for header flit (including the link transfer)  Communication flow from source (cid:1871) to destination (cid:1856)   Length (number of hops) of flow (cid:1858)  Average packet size (flits)  Link channel connecting router (cid:1853) and (cid:1854)  Buffer size in each channel (flits)  Set of links that form the routing path of flow (cid:1858)(cid:3046)(cid:481)(cid:3031)   Aggregate set of flows sharing link (cid:1864)(cid:3028)(cid:3029)  Link that resides in the (cid:1861)(cid:3047)(cid:3035) (cid:3)hop of flow (cid:1858)  Mean packet arrival rate of flow (cid:1858)   SCV of the packet inter- arrival time of (cid:1858)  Mean packet arrival rate at link (cid:1864)(cid:3028)(cid:3029)  Delay for a flit to reach the head of buffer in link (cid:1864)(cid:3036)(cid:3033)  Delay for a packet head to acquire the link (cid:1864)(cid:3036)(cid:3033)   Header flit transfer time over the link (cid:1864)(cid:3036)(cid:3033)  Service time for a packet that travels link (cid:1864)(cid:3036)(cid:3033) / (cid:1864)(cid:3028)(cid:3029)  Source queuing time at tile (cid:1871)  Time that a header flit reaches the point where the  Average latency of flow (cid:1858)(cid:3046)(cid:481)(cid:3031) (cycles)  accumulated buffer space can hold the whole packet   (cid:1832)(cid:3039)(cid:3276)(cid:3277)  (cid:2019)(cid:3039)(cid:3276)(cid:3277)  (cid:1869)(cid:3039)(cid:3284)(cid:3281)  (cid:1860)(cid:3039)(cid:3284)(cid:3281)  (cid:2015)(cid:3039)(cid:3284)(cid:3281)  (cid:1871)(cid:3039)(cid:3284)(cid:3281) /(cid:1871)(cid:3039)(cid:3276)(cid:3277)  (cid:1878)(cid:3039)(cid:3284)(cid:3281)  (cid:1874)(cid:3046)  (cid:1838)(cid:3033) (cid:4666)(cid:1838)(cid:3046)(cid:481)(cid:3031) (cid:4667)  assume that the packet inter-arrival times of flow (cid:1858) have  (cid:2019)(cid:3033)(cid:2879)(cid:2869) (cid:3)(cid:3) and a square coefficient of variation (SCV) (cid:1829)(cid:3033)(cid:2870) .  been characterized using a general exponential  Therefore, the ((cid:2019)(cid:3033)(cid:2879)(cid:2869) ,(cid:3)(cid:1829)(cid:3033)(cid:2870) ) characterization of the traffic model is  (GE)  distribution [10] (discussed later in Section III.D) with mean  an  input  to our analytical  framework. Moreover, a  deadlock-free and deterministic routing algorithm is used to  guarantee that no cycles can be formed by link dependencies.  Without loss of generality, in this work we use X-Y routing,  size of (cid:1865) (flits) as in [9]. However, this assumption can be  which can be generalized  to other deadlock-free and  deterministic routing schemes. We also adopt a wormhole  router architecture, where there exists a single buffer at each  input port. For simplicity, we assume that packets have a fixed  extended  to arbitrary packet  length distribution. Other  assumptions include: the traffic source (i.e., the source  Processing elements) has an infinite queue size and the  destination immediately consumes the arriving flits. To  facilitate the discussion, the symbols in Table 2 are used  consistently throughout this paper.   B. End-to-end delay calculation  (cid:15)(cid:2929)(cid:481)(cid:2914) (shown in Fig 2-a) of a specific flow (cid:136)(cid:2929)(cid:481)(cid:2914) (cid:3)consists of the  ((cid:580)(cid:2929)(cid:481)(cid:2914) ) and the path acquisition time ((cid:138)(cid:2929)(cid:481)(cid:2914) (cid:4667):   queuing time at the source s ((cid:152)(cid:2929) ), the packet transfer time  In a wormhole routing network, the end-to-end flow latency  In order to calculate (cid:138)(cid:2929)(cid:481)(cid:2914) , we need to add up the path  acquisition time of every link (cid:142)(cid:2919)(cid:2916) in the routing path of (cid:136) (Fig  2-b shows the links involved for flow (cid:136)(cid:2868)(cid:481)(cid:2876) ) and:  (cid:15)(cid:2929)(cid:481)(cid:2914) (cid:3404) (cid:152)(cid:2929) (cid:3397) (cid:580)(cid:2929)(cid:481)(cid:2914) (cid:3397) (cid:138)(cid:2929)(cid:481)(cid:2914)        (1)    4B-1 Figure 3 Modeling of the GE packet generation process  Therefore, (cid:1864)(cid:2868)(cid:2869) is dependent on (cid:1864)(cid:2869)(cid:2870) . Iteratively, as (cid:2015)(cid:2870)(cid:3033) and  (cid:1860)(cid:2870)(cid:3033) (cid:3)(cid:3)will be affected by the succeeding links of (cid:1864)(cid:2869)(cid:2870) in the same  way, it is required to analyze all subsequent links of link (cid:1864)(cid:2868)(cid:2869)  in the routing path of flow (cid:1858) before deriving the queuing  service and waiting time of link (cid:1864)(cid:2868)(cid:2869) . In this work, we propose  to build the link dependency graph (LDG) first and then  links. The detail of link dependency analysis is presented in  utilize the topological sort algorithm as in [8] to order the  Algorithm 1. Please note that when the routing as well as the  checking every flow (cid:1858) in the application communication set  (cid:1832) . An edge in the LDG is added if there exists a flow (cid:1858)  As shown in lines 7-15 of Algorithm 1, the LDG is built by  task mapping algorithm change, the LDG needs to be rebuilt.  (line 13). After that, the links computation order (cid:1833) can be  whose routing path passes through the two links consecutively  obtained (line 16).   (cid:9)(cid:4666)(cid:150)(cid:4667) (cid:3404) (cid:19)(cid:4666)(cid:27) (cid:3409) (cid:150)(cid:4667) (cid:3404) (cid:883) (cid:3398) (cid:594)(cid:135)(cid:2879)(cid:2980)(cid:2971)(cid:2930) (cid:481) (cid:150) (cid:3410) (cid:882)        (4)  D. GE-type traffic modeling  Many applications in NoCs show bursty patterns of traffic  over a wide range of time scales [7]. Therefore, the  (cid:27) is given by:  Generalized Exponential (GE) distribution is utilized to model  the arrival traffic at the source PEs and the links. The GE-type  cumulative distribution function (cdf) of the inter-arrival time  where (cid:594) (cid:3404) (cid:2870)(cid:2869)(cid:2878)(cid:2887)(cid:3118) and (cid:4666)(cid:585)(cid:2879)(cid:2869) (cid:481) (cid:6)(cid:2870)(cid:4667) are the mean and square  coefficient of variation (SCV) of (cid:27).  shown in the figure, with probability (cid:4666)(cid:883) (cid:3398) (cid:594)(cid:4667) , a packet  (cid:594), the packet needs to traverse a system with an exponentially    We depict the GE packet generation process in Fig. 3. As  distributed service time with mean (cid:883)(cid:512)(cid:4666)(cid:594)(cid:585)(cid:4667) . The bulk of  experiences a zero service time to reach the departure point  (i.e., the point to generate the packet), while with probability  packets consist of a packet which comes from the exponential  branch together with a number of successive packets arriving  through the direct branch. The GE distribution is a versatile  formulation analytically tractable [10]. Moreover, it has been  and simple distribution which helps to make the queuing  demonstrated that the GE distribution also provides an  efficient approximation for both short and  long-range  dependent traffic in supercomputers [10,16] .  WH analytical models, i.e., the path acquisition time (cid:138), the  E. WH router modeling  flit transfer time (cid:580), and the source queuing time (cid:152)(cid:2929) .     1) Flit transfer time: The flit transfer time (cid:580) of link (cid:142)(cid:2911)(cid:2912) is  In this section, we estimate the three key components in  (cid:138)(cid:2929)(cid:481)(cid:2914) (cid:3404) (cid:963) (cid:138)(cid:2922)(cid:3167)(cid:3164) (cid:2914)(cid:3164)(cid:2919)(cid:2880)(cid:2869) (cid:580)(cid:2929)(cid:481)(cid:2914) (cid:3404) (cid:963) (cid:580)(cid:2922)(cid:3167)(cid:3164) calculate the end-to-end delay of flow (cid:2188)(cid:2777)(cid:481)(cid:2785) ,b) the acquisition time (cid:2190) Figure 2 Example showing the flow delay a) the links involved to and transfer time (cid:2241) for link (cid:2194)(cid:2778)(cid:2779)   (cid:2914)(cid:3164)(cid:2919)(cid:2880)(cid:2869) (cid:3397) (cid:4666)(cid:143) (cid:3398) (cid:883)(cid:4667)      (3)  where (cid:138)(cid:2922)(cid:3167)(cid:3164) is the time for a packet header to compete for a  channel with other flows over link (cid:142)(cid:2919)(cid:2916) .  If we denote (cid:580)(cid:2922)(cid:3167)(cid:3164) as the time to transmit the header flit, then                 (2)  the packet transfer time (cid:580)(cid:2929)(cid:481)(cid:2914) can be rewritten as:  notations of the queuing delay (cid:152)(cid:2929) , (cid:138)(cid:2922)(cid:3167)(cid:3164) and (cid:580)(cid:2922)(cid:3167)(cid:3164) in Eqn. 1-3  where the first term denotes the header flit transmission  time over the network, and the second term approximates the  packet serialization time of the body and tail flits. The    In order to derive (cid:15)(cid:2929)(cid:481)(cid:2914) , (cid:580)(cid:2922)(cid:3167)(cid:3164) and (cid:138)(cid:2922)(cid:3167)(cid:3164) for each link need to  are also illustrated in Fig.2-a and -b.   obtaining (cid:580) and (cid:138)(cid:3)of each link. In the following, we first  determine the order of links that can be used for the queuing  be obtained. Two issues hereby arise. First, it is important to  analysis. Second, a detailed queuing model is needed for  to estimate (cid:580) and (cid:138).    present the link dependency analysis to search for the correct  order and then elaborate on the queuing formulation of a link  Algorithm 1 Link dependency analysis  5:     (cid:2168)(cid:2160)(cid:2163)(cid:484) (cid:2183)(cid:2186)(cid:2186)(cid:2196)(cid:2197)(cid:2186)(cid:2187)((cid:2194)(cid:2183)(cid:2184));    1: Input: (cid:2162) the application flow set  3: Container: (cid:2168)(cid:2160)(cid:2163) (cid:3404) (cid:4666)(cid:2178)(cid:481) (cid:2161)(cid:4667) the link dependency graph  2: Output: (cid:2163) the ordered list of links for queuing analysis  4: for any link (cid:2194)(cid:2183)(cid:2184)   7: for each flow (cid:2188) (cid:1488) (cid:3)(cid:2162)                                         6: endfor      // initialize (cid:2168)(cid:2160)(cid:2163), (cid:2161) (cid:3404) (cid:1486)           8:  (cid:2172)(cid:2188) = (cid:2200)(cid:2197)(cid:2203)(cid:2202)(cid:2191)(cid:2196)(cid:2189)(cid:820)(cid:2188)(cid:2203)(cid:2196)(cid:2185)(cid:2202)(cid:2191)(cid:2197)(cid:2196)(cid:4666)(cid:2188)(cid:4667)   //set of links in the path  9:    (cid:2186)(cid:2188) = (cid:2194)(cid:2187)(cid:2196)(cid:2189)(cid:2202)(cid:2190)(cid:4666)(cid:2172)(cid:2188) (cid:4667)           10:   for (cid:2191)=(cid:2778)(cid:483) (cid:2186)(cid:2188) (cid:3398) (cid:2778)(cid:3)(cid:3) 11:      (cid:2194)(cid:2203)(cid:2198) (cid:3404) (cid:2172)(cid:2188) (cid:4666)(cid:2191)(cid:4667);       // the upstream link       12:      (cid:2194)(cid:2186)(cid:2197)(cid:2205)(cid:2196) (cid:3404) (cid:2172)(cid:2188) (cid:4666)(cid:2191) (cid:3397) (cid:2778)(cid:4667);    // the downstream link  16: return (cid:2163) (cid:3404) (cid:2202)(cid:2197)(cid:2198)(cid:2197)(cid:2194)(cid:2197)(cid:2189)(cid:2191)(cid:2185)(cid:2183)(cid:2194)(cid:820)(cid:2201)(cid:2197)(cid:2200)(cid:2202)(cid:4666)(cid:2168)(cid:2160)(cid:2163)(cid:4667);               14:   endfor  15: endfor  13:      (cid:2168)(cid:2160)(cid:2163)(cid:484) (cid:2183)(cid:2186)(cid:2186)(cid:2187)(cid:2186)(cid:2189)(cid:2187)((cid:2194)(cid:2186)(cid:2197)(cid:2205)(cid:2196) ,(cid:3)(cid:2194)(cid:2203)(cid:2198))   C. Link dependency analysis  waiting times (cid:2015)(cid:2870)(cid:3033) and (cid:1860)(cid:2870)(cid:3033) of link (cid:1864)(cid:2869)(cid:2870) (cid:3)(i.e., (cid:1864)(cid:2870)(cid:3033) ) will affect the  To account for the impact of congestion on the succeeding  time to serve a packet in the buffer head of link (cid:1864)(cid:2868)(cid:2869) (cid:3)(i.e., (cid:1864)(cid:2869)(cid:3033) ).  hops, knowing the dependencies among all the links is  important. For example, in Fig. 2-b, due to backpressure, the  325       4B-1 (cid:3397) (cid:138)(cid:2922)(cid:3167)(cid:3126)(cid:3195)(cid:3170)(cid:3167)(cid:3164) (cid:3164) (cid:154) (cid:2922)(cid:3167)(cid:2916) (cid:3) (cid:3404) (cid:580)(cid:2922)(cid:3167)(cid:3164) (cid:3397) (cid:963) (cid:4666)(cid:580)(cid:2922)(cid:3167)(cid:3126)(cid:3168)(cid:3164) (cid:3397) (cid:138)(cid:2922)(cid:3167)(cid:3126)(cid:3168)(cid:3164) (cid:4667) (cid:2947)(cid:3170)(cid:3167)(cid:3164)(cid:2920)(cid:2880)(cid:2869) (cid:149)(cid:2922)(cid:3167)(cid:3164) (cid:3404) (cid:3421) (cid:3427)(cid:143)(cid:3435)(cid:143) (cid:3397) (cid:154) (cid:2922)(cid:3167)(cid:2916) (cid:3439) (cid:3397) (cid:884)(cid:154)(cid:2922)(cid:3167)(cid:2916) (cid:143)(cid:3431)(cid:512)(cid:4666)(cid:143) (cid:3397) (cid:884)(cid:154)(cid:2922)(cid:3167)(cid:2916) (cid:4667)(cid:3)(cid:3)(cid:139)(cid:136)(cid:3)(cid:154)(cid:2922)(cid:3167)(cid:2916) (cid:3407) (cid:1865) (cid:4674)(cid:143)(cid:3435)(cid:143) (cid:3397) (cid:154)(cid:2922)(cid:3167)(cid:2916) (cid:3439) (cid:3397) (cid:884)(cid:3435)(cid:154)(cid:2922)(cid:3167)(cid:2916) (cid:3439)(cid:2870)(cid:4675) (cid:512)(cid:4666)(cid:143) (cid:3397) (cid:884)(cid:154)(cid:2922)(cid:3167)(cid:2916) (cid:4667)(cid:3)(cid:3)(cid:145)(cid:150)(cid:138)(cid:135)(cid:148)(cid:153)(cid:139)(cid:149)(cid:135) (cid:3) (cid:4666)(cid:891)(cid:4667)  shared link (cid:142)(cid:2911)(cid:2912) . For example, in Fig. 4, assume a header flit in  Point A is granted access to the current link (cid:142)(cid:2919) . Then, the  leaves A and ends when the tail flit departs A so to release (cid:142)(cid:2919)  service process of this packet begins when the header flit  service time consists simply of (cid:143) cycles as the entire packet  for other flows. If the downstream links are not congested, this  can traverse downstream links smoothly. However, when a  time (cid:154) (cid:2922)(cid:3167)(cid:2916) (i.e., the delay from Point A to Point B) can be  severe blockage exists along the path, the worst-case scenario  represented as in Eqn. 6 (assume (cid:156)(cid:2922)(cid:3167)(cid:3164) is small in Fig. 4):  needs the packet header to reach Point B (Fig.4) where the  accumulated buffer spaces can hold the whole packet [8]. This   where (cid:550) (cid:2922)(cid:3167)(cid:2916) represents the effective number of hops that a  packet may span. Specifically, if (cid:143)(cid:512)(cid:5) is smaller than the  remaining hops from the current link (cid:142)(cid:2919)(cid:2916) (cid:3)to the destination of (cid:136)      (8)  (i.e., (cid:134)(cid:2926) (cid:3398) (cid:139)), then (cid:550) (cid:2922)(cid:3167)(cid:2916) equals to (cid:1729)(cid:143)(cid:512)(cid:5)(cid:1730); otherwise, (cid:550) (cid:2922)(cid:3167)(cid:2916) will  (cid:149)(cid:2922)(cid:3167)(cid:3164) in Table 2) is then bounded by (cid:143) and (cid:154) (cid:2922)(cid:3167)(cid:2916) under different  be assigned the value (cid:134)(cid:2926) (cid:3398) (cid:139). The service time for flow (cid:136) (i.e.,  congestion conditions and can be approximated according to  [8,16]:     From Eqn. 8-9, once the downstream links transfer time (cid:580)  and contention time (cid:138) are known, the overall link service  it, which yields the mean service time (cid:149)(cid:2922)(cid:3159)(cid:3160) as:    Likewise, the SCV of the service time for link (cid:142)(cid:2911)(cid:2912) can be  time is then the weighted average of all flows passing through         (10)  approximated by:    Based on the mean arrival rate (cid:585)(cid:2922)(cid:3159)(cid:3160) , its SCV (cid:6)(cid:2911) (cid:2922)(cid:3159)(cid:3160)(cid:2870) in Eqn.  Eqn. 10-11, the GE/G/1/K queuing time (cid:138)(cid:2922)(cid:3159)(cid:3160) (i.e., (cid:138)(cid:2922)(cid:3167)(cid:3164) ,  7 as well as the service time ((cid:3)(cid:149)(cid:3365) (cid:2922)(cid:3159)(cid:3160) , (cid:6)(cid:2929) (cid:2922)(cid:3159)(cid:3160)(cid:2870) ) characterized by  (cid:1482)(cid:136) (cid:1488) (cid:9)(cid:2922)(cid:3159)(cid:3160) (cid:4667) can be approximated as:  where (cid:138)(cid:4593) (cid:2922)(cid:3159)(cid:3160) is the waiting time calculated according to the  (cid:10)(cid:8)(cid:512)(cid:10)(cid:512)(cid:883)(cid:512)(cid:955) system [10]. Applying the GE/G/1 formula    3) Source queuing time: The source queue is modeled as a  M/G/1/K queuing formula presented in [15].  (cid:3442) (cid:3398) (cid:3)(cid:149)(cid:3365) (cid:2922)(cid:3177)          (13)  where the arrival process (cid:4666)(cid:585)(cid:2911) (cid:481) (cid:6)(cid:2911)(cid:2870)(cid:4667) and the mean service time  (cid:3)(cid:149)(cid:3365) (cid:2922)(cid:3177) are calculated in a similar way as the channel queues in  Eqn 5-8.  F. The network performance analysis  The proposed analytical flow for NoC latency evaluation is  (cid:1482)(cid:3164)(cid:1488)(cid:3138)(cid:3170)(cid:3159)(cid:3160) (cid:2971)(cid:3164)(cid:3400)(cid:2929)(cid:3170)(cid:3167)(cid:3164)(cid:3118) (cid:1482)(cid:3164)(cid:1488)(cid:3138)(cid:3170)(cid:3159)(cid:3160) (cid:138)(cid:2922)(cid:3159)(cid:3160) (cid:3404) (cid:4666)(cid:2887)(cid:3177) (cid:3170)(cid:3159)(cid:3160)(cid:3118) (cid:2878)(cid:2887)(cid:3159) (cid:3170)(cid:3159)(cid:3160)(cid:3118) (cid:4667) (cid:4666)(cid:2869)(cid:2878)(cid:2887)(cid:3177) (cid:3170)(cid:3159)(cid:3160)(cid:3118) (cid:4667) (cid:138)(cid:4593) (cid:2922)(cid:3159)(cid:3160) (cid:3)(cid:3)         (12)  yields: (cid:152)(cid:2929) (cid:3404) (cid:3)(cid:2929)(cid:3365) (cid:3170)(cid:3177)(cid:2870) (cid:3438)(cid:883) (cid:3397) (cid:3)(cid:3)(cid:3)(cid:2887)(cid:3159)(cid:3118)(cid:2878)(cid:2971)(cid:3159)(cid:3400)(cid:4672)(cid:3)(cid:3177)(cid:3365) (cid:3170)(cid:3177) (cid:3127)(cid:3171)(cid:4673)(cid:3118) (cid:3)(cid:3177)(cid:3365) (cid:3170)(cid:3177) (cid:585)(cid:2916) (cid:4667)(cid:512)(cid:3)(cid:3435)(cid:3)(cid:149)(cid:3365) (cid:2922)(cid:3159)(cid:3160) (cid:3439)(cid:2870) (cid:3398) (cid:883)(cid:3)  (11)  (cid:1482)(cid:2916)(cid:1488)(cid:2890)(cid:3170)(cid:3159)(cid:3160) (cid:6)(cid:2929) (cid:2922)(cid:3159)(cid:3160)(cid:2870) (cid:3404) (cid:2929)(cid:3170)(cid:3159)(cid:3160)(cid:3118)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364) (cid:4672)(cid:3)(cid:2929)(cid:3365) (cid:3170)(cid:3159)(cid:3160) (cid:4673)(cid:3118) (cid:3398) (cid:883) (cid:3404) (cid:4666)(cid:963) (cid:4666)(cid:585)(cid:2916) (cid:3400) (cid:149)(cid:2922)(cid:3167)(cid:3164) (cid:4667)(cid:512) (cid:1482)(cid:2916)(cid:1488)(cid:2890)(cid:3170)(cid:3159)(cid:3160) (cid:2971)(cid:3164) (cid:3)(cid:149)(cid:3365) (cid:2922)(cid:3159)(cid:3160) (cid:3404) (cid:963) (cid:2869)(cid:2879)(cid:2971)(cid:3159)(cid:3400)(cid:3)(cid:2929)(cid:3365) (cid:3170)(cid:3177) (cid:963) (cid:963) (cid:2922)(cid:3159)(cid:3160) (cid:585)(cid:2916) (cid:2916)(cid:1488)(cid:2890)(cid:3170)(cid:3159)(cid:3160) (cid:3404) (cid:143) (cid:3400) (cid:963) (cid:1482)(cid:3164)(cid:1488)(cid:3138)(cid:3170)(cid:3159)(cid:3160)(cid:963) (cid:1482)(cid:3164)(cid:1488)(cid:3138)(cid:3170)(cid:3159)(cid:3160) Figure 4 Illustration of service time of link (cid:2194)(cid:2191) in WH routing  (cid:585)(cid:976)(cid:2922)(cid:2919)(cid:2930)(cid:2922)(cid:3159)(cid:3160) (cid:3404) (cid:143) (cid:3400) (cid:585)(cid:2926)(cid:2911)(cid:2913)(cid:2921)(cid:2915)(cid:2930) (cid:149)(cid:976)(cid:2922)(cid:2919)(cid:2930)(cid:2922)(cid:3159)(cid:3160) (cid:3404) (cid:963) (cid:4686)(cid:2971)(cid:3164)(cid:3400)(cid:4684)(cid:3166)(cid:3170)(cid:3167)(cid:3126)(cid:3117)(cid:3164)(cid:3171) (cid:2878)(cid:2869)(cid:4685)(cid:4687) (cid:2971)(cid:3164) (cid:3)(cid:3)(cid:3) reach the front of the buffer in current link (cid:142)(cid:2911)(cid:2912) (e.g., from  defined as the time taken for the header flit after being granted  link access to leave the buffer head in the upstream node and  of the router and link pipeline stages ((cid:11)(cid:2929) cycles). The second  Point A to Point D in Fig. 4). It consists of two parts: the first  link (cid:142)(cid:2919) (i.e., (cid:147) (cid:2922)(cid:3167) in Table 2, from Point C to Point D). This  to Point C in Fig. 4). For WH routing, this is a constant value  part is the time to leave the upstream router (i.e., from Point A  part accounts for the header flit to arrive at the buffer front of  ((cid:14) (cid:3404) (cid:5)). The mean flit rate arriving at the buffer is   time can be approximated as the waiting time of an M/M/1/K    The mean time to serve a flit ((cid:149)(cid:976)(cid:2922)(cid:2919)(cid:2930)(cid:2922)(cid:3159)(cid:3160) ) in this queuing system is  queue with system capacity being equal to the buffer size B  flows passing through link (cid:142)(cid:2911)(cid:2912) , i.e.:         (5)  calculated as the weighted average of the service time from all   In Eqn. 6, for a packet with (cid:143) flits, it takes (cid:138)(cid:2922)(cid:3167)(cid:3126)(cid:3117)(cid:3164) a header in the buffer to win the next link of flow (cid:136)               (6)  (i.e.,(cid:3)(cid:138)(cid:2922)(cid:3167)(cid:3126)(cid:3117)(cid:3164) (cid:3397) (cid:883) cycles in service). After that, the body and the  tail flits depart the buffer without any additional delay (i.e., (cid:883)   cycles for  packet is (cid:138)(cid:2922)(cid:3167)(cid:3126)(cid:3117)(cid:3164) (cid:512)(cid:143) (cid:3397) (cid:883).      After (cid:3)(cid:585)(cid:976)(cid:2922)(cid:2919)(cid:2930)(cid:2922)(cid:3159)(cid:3160) and (cid:149)(cid:976)(cid:2922)(cid:2919)(cid:2930)(cid:2922)(cid:3159)(cid:3160) are obtained, the M/M/1/B queuing  cycle). Therefore, the mean flit service time over the whole  formulation [15] can be applied to obtain (cid:147) (cid:2922)(cid:3167) and thereby    2) Path acquisition time: The path acquisition time (cid:138) of  flow (cid:136) over its (cid:139)(cid:2930)(cid:2918) hop link (cid:142)(cid:2911)(cid:2912) (i.e., (cid:142)(cid:2919)(cid:2916) ) is the delay for the  header flit to be granted access to the buffers in (cid:142)(cid:2911)(cid:2912) after  More specifically, the system capacity (cid:14) (cid:3404) (cid:141) denotes the  router and forward to the same output link ((cid:141) (cid:3404) (cid:885) as in Fig 4.  modeled as the waiting time of a G/G/1/K queuing system.  contention with other flows sharing the same link. It can be  for (cid:142)(cid:2919)(cid:2916) ). The arrival process is the merging of all traffic flows  number of flows which come from the input ports in the same  that route to (cid:142)(cid:2911)(cid:2912) . Therefore, the mean arrival rate is   , while the SCV can be approximated as:  with (cid:143) flits is split over a limited number of downstream          (7)  links along the path (assume the buffer depth is (cid:5) flits). The    The service time of this queue is illustrated in Fig. 4.  Without loss of generality, we consider the case when a packet  service time accounts for the time that a packet occupies a  (cid:580)(cid:2922)(cid:3159)(cid:3160) (cid:3404) (cid:580)(cid:2922)(cid:3167)(cid:3164) (cid:3404) (cid:147) (cid:2922)(cid:3167) (cid:3397) (cid:11)(cid:2929) .   (cid:585)(cid:2922)(cid:3159)(cid:3160) (cid:3404) (cid:963) (cid:585)(cid:2916) (cid:3)(cid:3)(cid:3)(cid:6)(cid:2911) (cid:2922)(cid:3159)(cid:3160)(cid:2870) (cid:3404) (cid:963) (cid:4666)(cid:585)(cid:2916) (cid:3400) (cid:6)(cid:2916)(cid:2870)(cid:4667)(cid:512) (cid:1482)(cid:2916)(cid:1488)(cid:2890)(cid:3170)(cid:3159)(cid:3160) (cid:4666)(cid:585)(cid:2916) (cid:4667) (cid:2916)(cid:1488)(cid:2890)(cid:3170)(cid:3159)(cid:3160) (cid:2916)(cid:1488)(cid:2890)(cid:3170)(cid:3159)(cid:3160) (cid:963) 326 summarized  each link (cid:142)(cid:2911)(cid:2912) (cid:3)in the ordered list (cid:10) (line 3-15), we calculate  in Algorithm 2. We first apply  the  link  the arrival traffic model (cid:4666)(cid:585)(cid:2922)(cid:3159)(cid:3160) (cid:481) (cid:6)(cid:2911) (cid:2922)(cid:3159)(cid:3160)(cid:2870) (cid:4667) (line 4) first according  dependency analysis presented in section III-B to determine  the correct link ordering for performance analysis. Then, for  the link transfer time (cid:580)(cid:2922)(cid:3159)(cid:3160) only depends on its downstream  link contention time (cid:138)(cid:2922)(cid:3167)(cid:3126)(cid:3117)(cid:3164) to the routing algorithm and applications. As shown in Eqn. 6,  (cid:138)(cid:2922)(cid:3159)(cid:3160) (cid:3)depends not only on its downstream link contention time  the previous loop. On the other hand, the path acquisition time  (Eqn. 8). Therefore, (cid:580)(cid:2922)(cid:3159)(cid:3160) is calculated first in line 5. After   and transfer time (cid:580)(cid:2922)(cid:3167)(cid:3126)(cid:3117)(cid:3164) but also on the current link (cid:580)(cid:2922)(cid:3167)(cid:3164)  , which should have been analyzed in  (cid:580)(cid:2922)(cid:3159)(cid:3160) is obtained, then we calculate the mean and SCV of the  two routers, we utilize GE/G/1/K queue to obtain (cid:138)(cid:2922)(cid:3159)(cid:3160) (line  link service time (cid:4666)(cid:3)(cid:149)(cid:3365) (cid:2922)(cid:3159)(cid:3160) (cid:481) (cid:6)(cid:2929) (cid:2922)(cid:3159)(cid:3160)(cid:2870) (cid:4667) (line 9). If this link connects  11). Otherwise, we calculate the source queuing time (cid:152)(cid:2911) (line  be applied to evaluate the delay for a specific flow (cid:136)(cid:2929)(cid:481)(cid:2914) from  13). Once all the (cid:4666)(cid:138)(cid:481) (cid:152)(cid:481) (cid:580)(cid:4667) variables are known, Eqn. 1-3 can  source (cid:149) to destination (cid:134).  (cid:138)(cid:2922)(cid:3167)(cid:3126)(cid:3117)(cid:3164) Algorithm 2 Analytical working flow  1: Input: (cid:2162) the application flow set; (cid:2163) ordered link list  3: foreach (cid:2194)(cid:2183)(cid:2184) (cid:1488) (cid:2163)  2: Output: (cid:2168)(cid:2201)(cid:481)(cid:2186) latency for each specific flow (cid:2188)  4:    ((cid:2245)(cid:2194)(cid:2183)(cid:2184) (cid:481) (cid:2159)(cid:2183) (cid:2194)(cid:2183)(cid:2184)(cid:2779) (cid:4667)= traffic_model ((cid:2162)(cid:2194)(cid:2183)(cid:2184) )       //Eqn. 5-7  5:    (cid:2241)(cid:2194)(cid:2183)(cid:2184)= calculate_transfer_time((cid:2245)(cid:2194)(cid:2183)(cid:2184) ,(cid:3)(cid:2201)(cid:2188)(cid:2194)(cid:2191)(cid:2202) 6:      foreach (cid:2188) (cid:1488) (cid:2162)(cid:2194)(cid:2183)(cid:2184) and (cid:2194)(cid:2191)(cid:2188) (cid:3404) (cid:2194)(cid:2183)(cid:2184)  7:        (cid:2201)(cid:2194)(cid:2191)(cid:2188) = calculate_link_service_time ( ) //Eqn. 8-9  9:    ((cid:3)(cid:2201)(cid:3365) (cid:2194)(cid:2183)(cid:2184) (cid:481) (cid:2159)(cid:2201) (cid:2194)(cid:2183)(cid:2184)(cid:2779) (cid:4667) = service_time ( )  // Eqn. 10-11    10:   if (cid:2183) (cid:3405) (cid:2184)    // the links between the routers      8:      end    13:    (cid:2204)(cid:2183) = GE_G_1 _queue ((cid:2245)(cid:2194)(cid:2183)(cid:2184) (cid:481) (cid:2159)(cid:2183) (cid:2194)(cid:2183)(cid:2184)(cid:2779) , (cid:3)(cid:2201)(cid:3365) (cid:2194)(cid:2183)(cid:2184) (cid:481) (cid:2159)(cid:2201) (cid:2194)(cid:2183)(cid:2184)(cid:2779) )  12:   else     // the link is the source link  16: foreach (cid:2188) (cid:1488) (cid:2162)  14:   endif  17:    (cid:2168)(cid:2201)(cid:481)(cid:2186)=calculate_flow_latency( )   //Eqn. 1-3  15:endfor  18: end           (cid:2194)(cid:2183)(cid:2184) (cid:481) (cid:2195)(cid:481) (cid:2158))   11:   (cid:2190)(cid:2194)(cid:2183)(cid:2184) = GE_G_1_K_queue ((cid:2245)(cid:2194)(cid:2183)(cid:2184) (cid:481) (cid:2159)(cid:2183) (cid:2194)(cid:2183)(cid:2184)(cid:2779) , (cid:3)(cid:2201)(cid:3365) (cid:2194)(cid:2183)(cid:2184) (cid:481) (cid:2159)(cid:2194)(cid:2183)(cid:2184)(cid:2779) (cid:481) (cid:2193))  IV. Experimental results  A. Basic assumptions and notations  We implement the proposed latency model in MATLAB  flit in a router and one cycle to cross the link (i.e., (cid:11)(cid:2929) (cid:3404) (cid:885)).  and compare its accuracy with a cycle accurate NoC simulator  called Booksim [13]. Without loss of generality, we adopt the  mesh NoC as the target platform with two cycles to transmit a  Both synthetic traffic and real applications are utilized in the  simulation. The real applications include MMS (multimedia  system) [11] and DVOPD (Dual video object plane decoder)  [12] which are mapped onto a (cid:887) (cid:3400) (cid:887) mesh NoC and MPEG4  which are mapped onto a (cid:886) (cid:3400) (cid:886) mesh. Five traces extracted  (MPEG codec), VOPD (Video object plane decoder) [12]  from SPECweb99 [14] are also adopted. They are 16-node  multithreaded workloads for IBM and Oracle database server  (i.e., DVB2 and Oracle), the Apache HTTP server (i.e.,  Apache), scientific workloads of matrix factorization (i.e.,  Sparse) and the ocean dynamic simulation (i.e., Ocean).   B. Evaluation under synthetic traffic patterns  source node. Three packet length (cid:143) and buffer depth (cid:5)  We first compare and evaluate the proposed analytical  model using two synthetic traffic patterns (i.e., random and  Table 1. For these (cid:4666)(cid:143)(cid:481) (cid:5)(cid:4667) configurations, the models in [5,6]  shuffle [13] traffic) with Poisson packet injection rates at each  combinations are evaluated to cover different PB ratios in  (i.e.,(cid:5) (cid:3408) (cid:883)), while the models in [3,4] cannot be used as (cid:5) is  not a multiple of (cid:143). Moreover, because of the round-robin  cannot be applied directly due to the finite buffer size  platform. The results for (cid:890) (cid:3400) (cid:890) and (cid:883)(cid:884) (cid:3400) (cid:883)(cid:884) mesh sizes are  arbitration in the routers, the fixed-priority based model  proposed in [2] is also not suitable for evaluating the target  summarized in Fig. 5. As shown in Fig.5, the proposed  analytical model achieves less than 12% error in predicting the  network saturation point (i.e., the injection rate where the  Figure 5 Comparison between proposed model and simulation  0 0 0.002 0.004 0.006 0.008 0.01 Packet injection rate (packets/cycle) 0.012 100 200 300 400 500 600 700 800 a L t y c n e ( c y c l s e ) S imulation (B=3,m=14) P roposed model (B=3,m=14) S imulation (B=4,m=9) P roposed model (B=4,m=9) S imulation(B=9,m=4) P roposed model (B=9,m=4) (cid:883)(cid:884) (cid:3400) (cid:883)(cid:884) (cid:1865)(cid:1857)(cid:1871)(cid:1860)(cid:481) (cid:1871)(cid:1860)(cid:1873)(cid:1858)(cid:1858)(cid:1864)(cid:1857) 0 0 0.01 0 .02 0 .03 0.04 Packet injection rate (packets/cycle) 0.05 100 200 300 400 500 600 700 800 a L t y c n e ( c y c l s e ) S imulation (B=3,m=14) P roposed model (B=3,m=14) S imulation (B=4,m=9) P roposed model (B=4,m=9) S imulation(B=9,m=4) P roposed model (B=9,m=4) (cid:883)(cid:884) (cid:3400) (cid:883)(cid:884) (cid:1865)(cid:1857)(cid:1871)(cid:1860)(cid:481) (cid:1844)(cid:1853)(cid:1866)(cid:1856)(cid:1867)(cid:1865) 0 0.01 0.02 0.03 0.04 0.05 Packet injection rate (packets/cycle) 0.06 0.07 0 100 200 300 400 500 600 700 800 a L t y c n e ( c y c l s e ) Simulation (B=3,m=14) Proposed model (B=3,m=14) Simulation (B=4,m=9) Proposed model (B=4,m=9) Simulation(B=9,m=4) Proposed model (B=9,m=4) (cid:890) (cid:3400) (cid:890) (cid:1865)(cid:1857)(cid:1871)(cid:1860)(cid:481) (cid:1844)(cid:1853)(cid:1866)(cid:1856)(cid:1867)(cid:1865) 0 0 0.01 0 .02 0 .03 0.04 Packet injection rate (packets/cycle) 0.05 100 200 300 400 500 600 700 800 a L t y c n e ( c y c l s e ) S imulation (B=3,m=14) P roposed model (B=3,m=14) S imulation (B=4,m=9) P roposed model (B=4,m=9) S imulation(B=9,m=4) P roposed model (B=9,m=4) (cid:890) (cid:3400) (cid:890) (cid:1865)(cid:1857)(cid:1871)(cid:1860)(cid:481) (cid:1871)(cid:1860)(cid:1873)(cid:1858)(cid:1858)(cid:1864)(cid:1857)  Figure 6 Comparison of GE-type and Poisson injection a) average injection rate , b) the latency comparison   10 20 30 40 50 60 T ime horizontal index 70 80 90 100 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 P a c e k t i n j c e i t o n r a t e ( p a k c e t s / y c c l e ) Po isson injection (CV=1) GE-type injection (CV=3) GE-type injection (CV=4) 0.01 0.015 0.02 0.025 0 .03 0.035 0.04 0 200 400 600 800 1000 Packe t inje ction rate (packe ts/cycle ) L a t e n c y ( c y c l e s ) Simulation (Poisson injection, CV=1) Simulation (GE-type injection, CV=3) Simulation (GE-type  injection, CV=4) Proposed model (Po isson injection, CV=1) Proposed model (GE-type injection, CV=3) Proposed model (GE-type injection, CV=4) a)  b)  327 4B-1                                             Simulation Proposed model benefit the NoC synthesis and optimization process since  more potential solutions can be evaluated.     4B-1 ) l s e c y c ( l y a e d d n e o t d n E 60 50 40 30 20 10 0 0 5 10 15 Flow index in DVOPD application 20 25 30 V. Conclusions  In this work, we have proposed a GE/G/1/K queue based  analytical model for NoC performance analysis. Packet arrival  burstiness has been taken into account by adopting a GE  traffic model. In addition, different combinations of packet  and buffer lengths have also been modeled. We have evaluated  the accuracy and scalability of the model using both synthetic  and real applications. As shown, the proposed model achieves  less than 13% error for various traffic patterns, while  providing about 70X speedup compared to simulation.   Acknowledgments  Z.-L. Qian and C.-Y. Tsui greatly acknowledge the support  from Hong Kong Research Grant Council under GRF 619813.  R.M. and D.M. greatly acknowledge partial NSF support  under grant CNS-1128624. P.B. acknowledges support from  USC. Finally, the authors would like to thank the anonymous  reviewers for their valuable comments.   "
2014,An evaluation of an energy efficient many-core SoC with parallelized face detection.,"New applications such as image recognition and augmented reality (AR) have become into practical on embedded systems. For these applications, we have developed a many-core SoC that includes two many-core clusters with 32 energy efficient processor cores connected by a low latency tree-based NoC. In this paper, we evaluate performance of many-core SoC by face detection as an example of real image recognition applications and discuss two parallelized implementations on the many-core clusters. By keeping balance of workloads on the cores, the performance scales up to 64 cores and the SoC consumes only 2.21W. The energy efficiency is several tens of times better than that of a high performance desk-top quad-core processor.","4A-4 An Evaluation of an Energy Efficient Many-Core SoC   with Parallelized Face Detection  Hiroyuki Usui, Jun Tanabe, Toru Sano, Hui Xu, Takashi Miyamori    Toshiba Corporation, Toshiba Semiconductor & Storage Products Company, Kawasaki, Japan  {hiroyuki1.usui, jun.tanabe, toru.sano, kei.jo, takashi.miyamori}@toshiba.co.jp  Abstract – New applications such as image recognition and  augmented reality (AR) have become  into practical on  embedded systems. For these applications, we have developed a  many-core SoC that includes two many-core clusters with 32  energy efficient processor cores connected by a low latency  tree-based NoC. In this paper, we evaluate performance of  many-core SoC by face detection as an example of real image  recognition applications and discuss  two parallelized  implementations on the many-core clusters. By keeping balance  of workloads on the cores, the performance scales up to 64 cores  and the SoC consumes only 2.21W. The energy efficiency is  several tens of times better than that of a high performance  desk-top quad-core processor.   I. Introduction  New media processing applications such as  image  recognition, augment reality (AR), and computer vision,  have become  into practical on embedded systems,  automotive, surveillance, digital-consumer, and mobile  systems. These data-intensive applications require high  performance with low power consumption. The power  consumption is limited to roughly under 2-4W for cooling  without fans, for example. One of effective approaches to  meet these requirements is many-core architecture with  energy efficient processor cores.  Accordingly, we have developed a many-core SoC  designed for embedded applications [1]. Unlike many-core  architecture composed of high performance cores [2-3] in  High Performance Computing (HPC) area, our many-core  SoC is composed of energy efficient processor cores. The  many-core SoC consists of two 32-core clusters. The 32  cores in one of the clusters share an L2 cache and they are  connected by a low latency tree-based Network-on-Chip  (NoC). As realistic applications, we have evaluated full-HD  H.264 video decoding and super resolution [1]. The power  consumption of the many-core cluster is less than 1W. The  many-core SoC with a similar concept has been proposed in  [4]. However, they only evaluate the performance with  kernels of  image processing and recognition, corner  detection and SIFT by simulation.  In this paper, we will show performance evaluation results  of face detection, as an example of real image recognition  applications, on actual many-core SoC chips. Face detection  is widely utilized in embedded systems such as digital  camera, automotive, surveillance, human-computer interface,  and so on. Many kinds of algorithm to detect faces have been  proposed  to meet system requirements. For  instance,  surveillance systems need higher detection accuracy than  others. Programmable many-core architecture has  the  advantage of being able to deal with various algorithms,  978-1-4799-2816-3/14/$31.00 ©2014 IEEE 311 even ones proposed in the future, compared with hardware  accelerators, which operate only specific algorithms. We  implemented a face detection based on a method using Joint  Haar-like features [5] that is high accuracy and meets  requirements of surveillance systems.   In  image  recognition, an  image  includes many  Regions-of-Interest (ROIs) and each ROI is generally  processed independently. Therefore, it is efficient to exploit  coarse grain thread level parallelism because overhead of  scheduling thread can be minimized. This characteristic of  image recognition is different from that of the video codec  that has dependencies between macroblocks. However, there  is an issue to parallelize face detection effectively. The  workload of each parallelized task varies dynamically  depending on whether faces exist or not in the ROIs. The  total performance of face detection tends to be limited by the  slowest task. To achieve high scalability, we have to allocate  tasks to many cores balancing workloads among cores.  Considering  this point, we evaluate  two parallelized  methods; allocating scanned image lines cyclically and  dividing an  image equally. We also evaluate power  consumption of the many-core SoC and show its energy  efficiency.  II. Architecture of the Many-Core SoC  A. Overview of the Many-core SoC  Fig. 1 shows the block diagram of our many-core SoC.  Unlike other proposed many-core LSIs [2-3] that consist of  only the array of processor cores and minimum interfaces,  the many-core SoC is highly integrated with two many-core  clusters, two dynamic reconfigurable engines [6], image  recognition and processing accelerators, integrated SRAMs,  two ARM® Cortex™-A9 processors, and various interfaces.   The many-core cluster consists of 32 cores called Media  Processing Engine Block (MPB) and 2MB L2 cache. The  cluster will be explained in more detail in the next section.  The reconfigurable engines are designed for bitstream  processing such as CAVLC and CABAC. The image  recognition and processing accelerators are designed for  commonly used fixed processing in image processing and  image recognition, such as block matching, histogram, affine  transformation, and filter processing [7]. Two channels of  32-bit DDR3 interfaces can provide peak memory bandwidth  at 10.7GB/s. As shown in Fig. 2 and Table I, the chip is  fabricated by 40nm CMOS process and the size of the chip  and the many-core cluster are 15.0mm x 14.0mm and 7.4mm  x 5.7mm respectively. All of the cores operate at 333MHz at                4A-4 TABLE I  Chip Specifications(cid:2) Technology  Interconnect  Chip Size  32-Core Cluster Size  Transistors  Cluster Frequency  (cid:2) 40nm LP Process  8 metal (Cu)  15.0mm x 14.0mm  7.4mm x 5.7mm  87.5M  333MHz, 1.1V  Fig. 1. Block diagram of the many-core SoC   Fig. 2. Chip micrograph and features (cid:2) 1.1V. The total peak performance of the two clusters is  852GOPS.  B. Energy Efficient 32-Core Cluster  Fig. 3 shows the block diagram of the 32-core cluster. A  processor core, Media Processing Engine (MPE), is a 3-way  VLIW processor [8]. It consists of a 5-stage pipelined 32-bit  RISC core and a 64-bit SIMD co-processor. Each core has  private L1 instruction and data caches. The size of the L1  instruction cache is 32KB and the size of the L1 data cache  is 16KB.  The cluster is based on a homogeneous architecture and a  shared memory model. The 32 cores share the L2 cache. The  programming model on the shared memory model is simple;  however, many memory accesses concentrate at the shared  memory when heavily memory-intensive applications are  executed. To resolve this issue, we adopt a multi-banked L2  cache and a low latency tree-based NoC. To minimize the  latency of the NoC router, source routing and a single bit  on/off flow control are introduced. The router can operate at  one cycle  latency. The L2 cache  is 2MB 32-way  set-associative and interleaved to four 512KB banks, and its  peak bandwidth is up to 42.6GB/s.  The NoC architecture of the cluster is also shown in Fig. 3.  Unlike other published many cores based on mesh topology  [2], we choose a tree-based topology. Tree-based topologies  are suitable for shared memory models because all data  transfers go through a shared memory, and there are no direct  data transfers between cores. In case of other topologies like  mesh, access latencies to a shared L2 cache would differ  depending on positions of cores. In our tree-based topology,  the L2 cache accesses from all of the cores have uniform  latency. This is suitable for a homogeneous many-core  architecture in which all cores are equivalent.  Each MPB has its own power switch to reduce leakage  Fig. 3. Cluster architecture and NoC connection (cid:2) power and the supply power can be gated independently of  other cores. In addition, when all of the cores in the cluster  are not in use, the supply power of the L2 cache and NoC  can be gated by the dedicated power switch.  III. Face Detection using Joint Haar-Like Features  We used face detection using Joint Haar-like features [5]  that is high accuracy and meets requirements of surveillance  systems. This method can reduce the detection error by 37%  compared with widely used Viola and Jones’ method [9].  In general object detections, each ROI can be evaluated by  a classifier, which detects a specific object independently. A  classifier operates with a dictionary, which is trained for a  specific object such as face detection.   The Joint Haar-like features are represented by combining  binary values computed from multiple Haar-like features.  Haar-like features are calculated with rectangular regions in  an ROI. The procedure of face detection executed in the  classifier is as follows:  First, the classifier calculates each of the Haar-like  features in a ROI with a dictionary. A Haar-like feature is a  value which is difference in average intensities between two  groups of rectangular regions. Examples of Haar-like feature  are shown in Fig. 4 and two groups of the rectangular  regions are depicted as black and white rectangles. Fig. 5 is a  pseudo code of calculating the leftmost Harr-like features in  Fig. 4. To reduce calculation costs, an integral image is  generally used. An integral image is a derived image whose  pixel represents a sum of pixel values in upper left region of  an original image. Using an integral image, the sum of each  rectangular area (s0, s1) of a Haar-like feature can be  calculated from the only four corners ( e.g. v1, v2, v4 and v5  for s0) of the rectangular and the three arithmetic operations.  312         V2(x1+w,y1)  V1(x1,y1)  V3(x1+2w,y1)  s0 s1  V4(x1,y1+h)  V6(x1+2w,y1+h)  V5(x1+w,y1+h)  Fig. 4. Examples of Haar-like features(cid:2) hs = h * stride; (cid:14967) // stride is width of image(cid:14967) p1 = img + y1 * stride + x1;(cid:14967) // img is pointer of the upper left(cid:14967) // pixel of the image(cid:14967) p2 = p1 + w;(cid:14967) p3 = p2 + w;(cid:14967) p4 = p1 + hs;(cid:14967) p5 = p2 + hs;(cid:14967) p6 = p3 + hs;(cid:14967) v1 = *p1;(cid:14967) v2 = *p2;(cid:14967) v3 = *p3;(cid:14967) v4 = *p4;(cid:14967) v5 = *p5;(cid:14967) v6 = *p6;(cid:14967) s0 = (v5 - v2) - (v4 - v1);(cid:14967) s1 = (v6 - v3) - (v5 - v2);(cid:14967) Fig. 5. A pseudo code of a Haar-like feature(cid:2) Each value is compared with a predefined threshold and is  converted into a binary output.  Then, the classifier combines binary outputs into one  jointed N-bit value, where N is the number of Haar-like  features. Using the jointed value as an index of tables, the  classifier looks up two probabilities in two tables, probability  of including faces and probability of not- including faces. If  the probability of including faces is greater than that of  not-including faces, the result of joint Haar-like features  becomes 1, otherwise 0. These probability tables are  included in the dictionary.  Next, the classifier accumulates the results of joint  Haar-like features with weight. If the accumulated result is  greater than another predefined threshold, the classifier  judges that a face exists in the ROI of the image. During  classification, if it is clear that no face exists in the ROI, the  classifier terminates the remaining calculations.   In addition to lots of coarse grain parallelism based on  ROIs, the algorithm has the following two important  characteristics.  The first characteristic is that the algorithm does not have  enough fine grain data parallelism. As shown in Fig. 5,  accessed memory addresses (from v1 to v6) are not adjacent  and memory accesses instructions are account for large  percentage of the program. It means that it is inefficient to  pack them into a SIMD register of CPUs or GPUs.  The second is imbalanced workloads. The workload of an  ROI where a face exists is larger than that of an ROI without  a face. It takes a longer time to evaluate an ROI with a face  because the classifier needs to calculate more features.  These two characteristics cause random data accesses and  313 4A-4 Fig. 6. How to allocate ROIs to the cores in a single cluster  branch divergence that are inefficient for current GPGPU  architectures. For example, Sharma implements parallelized  face detection based on Viola and Jones’ method on a GPU  using CUDA™ [10]. Although the workload of each thread  is different from each other, all threads in Warp, a group of  threads in a Streaming Processor, need to execute the same  instructions. Therefore, the total performance is limited by  the slowest thread and utilization of ALUs becomes low.   IV. Implementation of Face Detection on the  Many-Core SoC  This section describes how  detection on the many-core SoC.  to parallelize  the face  A. Parallelism of the Face Detection  As mentioned above, the face detection does not have  enough fine grain data parallelism and we do not use the  SIMD co-processor. We focus to exploit coarse grain thread  level parallelism. When the resolution of an image is 4000 x  3200 pixels and the size of ROIs is 25 x 25 pixels and the  step size is 2 pixels, there are about 3M ROIs in the image.  There are lots of thread level parallelism to exploit by  multiple processor cores. To minimize the overhead of  scheduling threads, we decrease the number of threads and  allocate one thread for each core. Because the workload of  each ROI is different from each other, it is a consideration to  allocate sets of ROIs to threads balancing the workload of  each thread.  B. Implementation on the Single Cluster of the Many-Core  We parallelized the face detection and develop two  implementations on the single cluster in the following two  intra-cluster ways.   The first way, Allocating Cyclically, allocates lines to be  scanned to each core cyclically. As shown in Fig. 6 (A), the  scanned line 0 is allocated to the core 0, the scanned line 1 is  to the core 1, and the line 31 is to the core 31. Then, the line  32 is allocated to the core 0 again and so on.   The second way, Splitting Equally, divides the image into  N equal areas horizontally, where N is the number of  operating cores, and the ROIs included in each split area are  allocated to each core. As shown in Fig. 6 (B), when the              4A-4 TABLE II  Images for evaluation  No.  0  1  2  3  4  5  Resolution  4000 x 1440  3000 x 4082  4083 x 3062  4094 x 3107  3568 x 2568  3568 x 2568  # of detected face  30  37  78  148  9  10  im g.0 img.1 im g.2 img.3 im g.4 im g.5 1000 Allocating Cyclically  Splitting Equally ) c e s ( e m i T 900 500 400 300 200 100 0 1 2 4 8 16 32 1 2 The Numbe r of cores 4 8 16 32 Fig. 8.  Execution time of a single cluster  many-core SoC chips. We measured execution times and  power consumption. Execution times to make integral  images and image preprocessing are not included because  these operations can be implemented on the filter accelerator  and other accelerators in the SoC and over-wrapped with a  main part of face detections executed by the many core  clusters.   B. Evaluation Results on a Single Cluster  Fig. 8 shows the execution time in the single cluster of  two intra-cluster ways: Allocating Cyclically and Splitting  Cyclically. The execution times vary greatly depending on  the images. The time of image 1 is under half of that of  image 3 even though the resolutions of the images are almost  the same. As indicated in Table II, the number of faces  included in image 3 is larger than that in image 1. The ROI  where a face exists has larger workload than that where no  face exists. Therefore, it takes a long time to detect faces in  image 3.   Fig. 9 shows the relative performance compared with the  performance in one core. Allocating Cyclically scales up to  32 cores and accomplishes 15.5 times speed-up in 16 cores  and 30 times speed-up in 32 cores. These performance  improvements are close to ideal. On the other hand, in the  Fig. 7. How to allocate ROIs to the cores in two clusters  application is executed by 32 cores, a 4000 x 3200 image is  divided into 32 areas whose size is 4000 x 100.   C. Implementation on the Dual Cluster of the Many-Core  When the application is executed on the two clusters, we  have to take the bandwidth between the L2 cache and the  DDR3 memory into consideration because it is narrower  than that between the L1 and L2 caches. Thinking over it, we  developed two implementations on the two clusters as the  following inter-cluster ways. The first implementation,  Allocating Cyclically, is the same as Allocating Cyclically in  one cluster, and utilizes 64 cores across the clusters as shown  in Fig. 7 (A). The second way, Bisection, divides the image  into two blocks horizontally. As shown in Fig. 7 (B), the  upper and lower blocks are assigned to the cluster 0 and the  cluster 1 respectively. Then, the lines in each block are  allocated to the 32 cores in the cluster by the Allocating  Cyclically method.   D. Execution Platform and Parallel Execution Model  We parallelized the face detection program for the  many-core cluster by using  the  thread-based parallel  execution model which is compatible with the model of the  multi-core architecture [11]. The thread model is much  lighter than other thread models such as POSIX thread. The  application is divided into threads and each thread is  assigned to one core by a thread scheduler dynamically. It  performs effectively on the many-core as well. In this  program, all of the operations that one core executes is  allocated to one thread. Because the thread scheduler  manages the threads at only the beginning and end of the  program, the overhead of synchronization is minimized.  V. Evaluation  A. Parallelism of the Face Detection  We evaluated the face detection with the pictures shown in  Table II as target images. The sizes of the images are from  5.76M to 12.7M pixels, and the number of faces varies from  9 to 148.  The face detection programs are executed on the actual  314                     4A-4 Allocating Cyclically  Splitting Equally Fig. 10. Execution time of the fastest and the slowest cores when 32 cores are running in a single cluster(cid:2) parallelized cases, the utilized bandwidth is maximum  8.5GB/s at 32 cores and about 20% of maximum bandwidth  (42.6GB/s). The NoC and the L2 cache are able to provide  enough bandwidth to the 32 cores. The bandwidth between  the L2 cache and the DDR3 memory in 32 cores is maximum  450MB/s, which is only 4.2% of the theoretical maximum  bandwidth (10.7GB/s). This ensures that the L2 cache  successfully reduces the transactions to the DDR3 memory.  If the amount of transferred data between the L2 cache and  the DDR3 memory  are compared,  the data size of  Allocating Cyclically is from 70% to 85% of that of Splitting  Equally in 32 cores. We confirm that Allocating Cyclically  reduces L2 cache misses because most of the cores access a  smaller area at the same time than that in Splitting Equally.  This is another reason that Allocating Cyclically provides the  better performance.  C. Evaluation Results on Two Clusters  The left graph of Fig. 11 shows the execution time in the  64 cores using two clusters. Allocating Cyclically achieves  higher performance than Bisection in all of the images. The  right graph of Fig. 11 is the relative performance of  Allocating Cyclically normalized by the performance of one  core. It scales up to 64 cores and the performance of 64 cores  is from 56 to 60 times higher than that of one core.  Fig. 12 shows the execution time of the fastest core and  the slowest core. In Allocating Cyclically, the execution time  in the slowest core is at most 1.3 times slower than that in  the fastest core (image 1), while in Bisection, the execution  time in the slowest core is at most 2.8 times slower than that  of the fastest core (image 2 and 5). The processor utilization  of 64 cores is from 87% to 95% in Allocating Cyclically. On  the other hand, in Bisection, the utilization is from 66% to  92%. This evaluation  result ensures  that Allocating  Allocating Cyclically  Splitting Equally Fig. 9. Relative performance of a single cluster  case of Splitting Equally, the speed-up is much lower than  that of ideal case.   Fig. 10 shows the execution times of the fastest core and  the slowest core when 32 cores are running. In Allocating  Cyclically, the execution times of the fastest and slowest  cores are almost equal, and the execution time of the slowest  core is about 1.1 times slower than that of the fastest core. In  Splitting Equally, on the other hand, the execution times of  the fastest and slowest core are quite different, and the  execution time of the slowest core is about 11 times slower  than that of the fastest core. This causes large performance  degradation. In image 2, the performance is much lower than  ideal because the faces are gathered to a certain region of the  image. In image 1, the relative performance is better than the  case of image 2, because the faces are in scattered positions.  However, even in image 1, the performance improvements  are lower than those of Allocating Cyclically.   As  the processor utilization, Allocating Cyclically  achieves higher utilization ratio, from 90% to 95%, than  those of Splitting Equally, from 54.3% to 76%. These results  confirm  that balance of workloads among  threads  is  important and the parallelization by Allocation Cyclically is  more efficient than that by Splitting Equally.   The bandwidth between the cores and the L2 cache is  measured by the on-chip performance monitor. In both  (A)  Execution Time of Two Clusters  (B) Relative Performance (Allocating Cyclically) Fig. 11. Performance of two clusters(cid:2) 315     4A-4 C lus ter s Bus DDR 3 I /F I /O Others ) W m ( r e w o P 250 0 200 0 150 0 100 0 500 0 1 2 4 8 16 The Numb er o f Co res 32 64 Fig. 13. Power consumption of the many-core SoC(cid:2) energy efficient in image recognition applications.  As the next step of this study, we are going to implement  other image processing and recognition applications on the  many-core SoC and evaluate the advantage of the SoC in  more detail. Furthermore, we plan to develop an improved  many-core SoC for embedded applications to pursue higher  performance and lower power consumption.  "
2014,Fuzzy flow regulation for Network-on-Chip based chip multiprocessors systems.,"Flow regulation is a traffic shaping technique, which can be used to improve communication performance with better utilization of network resources in chip multi-processors (CMPs). This paper presents fuzzy flow regulation. Being different from the static flow regulation policy, our system makes regulation decisions fully dynamically according to traffic dynamism and the state of interconnection network. The central idea is to use fuzzy logic to mimic the behavior of an expert that can recognize the network status and then intelligently control the admission of input flows. As the experiment results show, the maximum improvement in average delay reaches 53.0% against static regulation and 37.4% against no regulation. The maximum improvement in average throughput reaches 37.5% against static regulation and 23.8% against no regulation.","4B-4 Fuzzy Flow Regulation for Network-on-Chip based Chip Multiprocessors Systems Yuan Yao and Zhonghai Lu Department of Electronic Systems, School for ICT KTH Royal Institute of Technology, Stockholm, Sweden {yuanyao, zhonghai}@kth.se Abstract— Flow regulation is a trafﬁc shaping technique, which can be used to improve communication performance with better utilization of network resources in chip multi-processors (CMPs). This paper presents fuzzy ﬂow regulation. Being different from the static ﬂow regulation policy, our system makes regulation decisions fully dynamically according to trafﬁc dynamism and the state of interconnection network. The central idea is to use fuzzy logic to mimic the behavior of an expert that can recognize the network status and then intelligently control the admission of input ﬂows. As the experiment results show, the maximum improvement in average delay reaches 53.0% against static regulation and 37.4% against no regulation. The maximum improvement in average throughput reaches 37.5% against static regulation and 23.8% against no regulation. Keywords: Network-on-Chip, Chip Multiprocessor, Flow Regulation, Fuzzy Logic I . IN TRODUC T ION As large uniprocessors are no longer scaling in performance, chip multiprocessors (CMP) become the mainstream to build high-performance computers [2]. CMP chips integrate various components such as processing cores, L1 caches and L2 caches (some also contain L3 caches, for example, in the IBM Power7 multicore processor) together, and multiple CMP chips with external memory banks make up a CMP system. As discussed in [2, 3], CMP systems are rapidly becoming communication limited. Since buses (although long the mainstay of system interconnect) are unable to keep up with increasing performance requirements, network-on-chip (NoC) offers an attractive solution to this communication crisis and is becoming the pervasive interconnection network in CMPs. As explained in [1, 5], in NoC based CMP systems, regulating trafﬁc ﬂows has been shown to be an effective means to improve communication performance and reduce buffer requirements. Fig. 1 illustrates the trafﬁc ﬂow regulation where P0 with $0 Pn with $n regulator 0 FC 0 regulator n FC n ... ... NSR ... Network-on-Chip Fig. 1. Fuzzy ﬂow regulation for CMP regulators are inserted between processing cores and the NoC to perform trafﬁc shaping. To achieve an analyzable performance guarantee, leaky bucket ﬂow control is used and the shaping function is expressed mathematically as (σ , ρ) shaped, with σ bounds a ﬂow’s burstiness and ρ long term sustainable rate. The (σ , ρ) ﬂow regulation is based on network calculus, which has been studied thoroughly in [6, 4, 7]. It stipulates that during any time interval (τ , t], the amount of trafﬁc denoted by Ai (τ , t), which is also called as regulated arrival function, from processor i entering the network is upper-bounded by : Ai (τ , t) ≤ σi + ρi (t − τ ). (1) The constraint in inequality (1) is attractive since it is descriptive enough to model a wide variety of trafﬁc patterns [8]. However, existing ﬂow regulation policies are either static or partially dynamic. With the static regulation [7, 6], the parameters of the regulators are hard-coded during system conﬁguration and remain constant. With the partially dynamic mechanism [1], the trafﬁc dynamism is proﬁled to conﬁgure the regulation parameters online but does not take the dynamism of network congestion state into account. In this paper, we propose a fuzzy ﬂow regulation mechanism for NoC-based CMPs. Being different from the static and partially dynamic ﬂow regulation policy, our system makes regulation decisions fully dynamically according to the trafﬁc dynamism as well as to the state of interconnection network. As Fig. 1 shows, our mechanism uses fuzzy controllers (FCs) and network state recognizers (NSRs) to achieve network status awareness trafﬁc. Based on a sampling window concept, FC, NSR, and the ﬂow regulator form a feedback ﬂow control loop for each processor. The goal of our design is to use fuzzy logic to mimic the behavior of an expert that validly controls the admission of input ﬂows, with the aim of making better use of onchip resources and decreasing communication delays. In our experiments, we show the ﬁdelity and efﬁciency of our fuzzy ﬂow control mechanism with both synthetic ﬂows and traces from benchmark programs SPLASH2. We also demonstrate beneﬁts that our fuzzy ﬂow control brings in terms of average network delay and throughput. The rest of the paper is organized as follows. Section II discusses related work. In Section III, we introduce (σ, ρ)– regulation and the basics of fuzzy logic. Section IV details the fuzzy ﬂow control system from concept to design. In section V, we set up our experiment platform based on Multi-facets General Execution-driven Multiprocessor Simulator (GEMS, [13]) and report the results. Finally, we conclude in Section VI. 978-1-4799-2816-3/14/$31.00 ©2014 IEEE 343 I I . R E LAT ED WORK I I I . (σ, ρ)–R EGU LAT ION AND FU Z ZY LOG IC S 4B-4 Since pioneered by Cruz [7] and Chang [4], network calculus has been successfully applied to design networks such as Internet with integrated/differentiated services [6], WSNs [9], etc. Based on network calculus, ﬂow regulation is proposed to regulate the admission of trafﬁc in order to minimize network overload, worst-case delay and buffer usage. Most traditional networks (for example, WAN, LAN) used window-based ﬂow regulation [11, 12], with advantages of low implementation cost and fast response time. However the effectiveness of this scheme in high speed networks is severely limited due to the increasing of performance requirements today [8]. To solve this problem, rate based ﬂow regulation was introduced [6, 7, 4]. The most popular one of this kind is called leaky bucket ﬂow regulation [7], where the admission of trafﬁc is determined mathematically by a shaping function. Unlike window based ﬂow control, no acknowledgement signals from the far-end or neighbor responser are used, thus this scheme can be implemented efﬁciently in fast speed networks. Works [6, 4] dealt with static regulation, requiring the characterization of trafﬁc ﬂows’ (σ, ρ) values ofﬂine at design time. Though ofﬂine characterization methods are possible, they are inﬂexible and can not capture trafﬁc dynamism. Work in [1] dealt with partially dynamic ﬂow regulation. The regulator can capture and follow the characteristics of varying ﬂows in an on-line fashion, but no network congestion status is considered. The central idea for this work is to make fully dynamic regulation by considering the network congestion status, and then use such information to adaptively control the regulation strength. Unfortunately, there is no formal nor empirical models to precisely describe when a network is congested. Most of the time, network congestion is recognized through measurement metrics such as packet latency, link utilization, or throughput. With a terminal instrumentation measurement approach, Dally and Towles [3] inspected network status by examining average packet latency and average throughput against the offered trafﬁcs. In [2], Kunle et al. proposed a speculative parallel threads method to detect congestion status and to accelerate single-thread applications in CMP. Since these works aim to pinpoint the congestion status through measurement values, they are very difﬁcult to be accurate in some situations. For example, suppose n cycles average packet delay is the indinot saturated if the average packet delay reaches n − 1 cycles? cation for network congestion, can we say that the network is The fundamental fuzzy property of the congestion recognition problem determines that the proposed methods in [2] and [3] are not always plausible. This paper proposes a fuzzy ﬂow regulation technique from concept, design to implementation. Our approach is based on a fuzzy ﬂow regulation system which can recognize the network congestion status and make new ﬂow regulation policies through fuzzy logics. Very different from the others, we aim to mimic the behavior of an expert that validly recognizes the network state and controls the admission of input ﬂows (refer to Section IV). In section V, we implement the fuzzy ﬂow regulation system within GEMS [13], which a full-system CMP simulation environment, and record the experiment results. A. (σ ,ρ)–regulation Function The (σ ,ρ)–regulation function is a powerful characterization which helps to ensure QoS guarantees in networks [6, 4]. In reality, the leaky bucket model [7] can be employed to shape an incoming trafﬁc ﬂow and make it conform to a (σ ,ρ)–regulation function. A (σ, ρ) regulator Rate ρ Token queue σ Waiting queue . . . (cid:2) ρ (σ, ρ) Regulated ﬂow Token matching Fig. 2. Leaky bucket ﬂow regulation Fig. 2 shows a leaky bucket (σ , ρ)–regulator model. There is a token bucket into which tokens are fed at rate ρ. The bucket can hold up to σ tokens. Packet is allowed to depart from the regulator if and only if there are tokens available. For every departed packet, one token is consumed. If packets arrive and there are no tokens, then the data are buffered in the waiting queue until tokens are available. More about leaky bucket (σ , ρ)–regulation can be found in [7, 4, 3]. B. Fuzzy Logic Basics First introduced by L. A. Zadeh [14, 15], fuzzy logic deals with many-valued logic or probabilistic logic problems, with the aim to reason about logics that are approximate rather than ﬁxed and exact. To understand fuzzy logic clearly, we introduce some useful terminologies below. • Fuzzy set: Fuzzy set is a pair (U , M ), where exact values in U can be mapped to degree of truth values in [0, 1] by membership functions in M . • Degree of truth: Degree of truth represents that a proposition might be more or less true, rather than simply true or simply false. For instance, the proposition 1 + 1 = 2 is simply true, while T he network is saturated is neither simply true nor simply false. Degree of truth is in [0, 1], with “1” indicates totally true and “0” totally false. • Membership Function (MF): The MF is a generalization of the indicator function in classical sets. In fuzzy logic, it determines the degree of truth value of a proposition. • Fuzzy Rule: A fuzzy rule is deﬁned as a statement in the form: IF x is A THEN y is B , where x and y are linguistic variables; A and B are degree of truth values of x and y , respectively. The values of A and B are determined by proper membership functions (or consequence membership functions) invoked by x and y , respectively. In each rule, “y is B ” is the consequence of “x is A”. For example, IF average packet delay is H I GH , THEN network is SAT U RAT ED . • Consequence Membership Function (CMF): The CMF indicates that if a rule’s premise is true then the action indicated by its consequence should be quantiﬁed. 344 LOW α β D1 rule 1 EM P T Y M EDI U M 1 H I GH 0.5 M.F. D2 d D3 D4 D5 Average Packet Delay (cycles) rule 2 SAT U RAT ED N ORM AL 1 0.5 C.M.F. S1 S2 S3 C.V. (center of the gray area) S4 S5 S6 Network Congestion Status 1. IF avg . packet delay is LOW , THEN network status is EM P T Y 2. IF avg . packet delay is M EDI U M , THEN network status is N ORM AL 3. IF avg . packet delay is H I GH , THEN network status is SAT U RAT ED Rule Base Fig. 3. Fuzzy network status recognization Fig. 3 depicts the above concepts by showing the relationship between average packet delay and network congestion status. The fuzzy set is a pair (U , M ) where U is a set of exact values (average packet delay ranges from D1 to D5 in this instance) and M the set of MFs with the mapping mi : U → [0, 1], mi ∈ M . In the ﬁgure there are totally three MFs for measurement of low, medium and high delay, respectively. Each MF maps the measured delay value to a degree of truth value in the range [0, 1]. This progress is called fuzziﬁcation. Note that one exact value can invoke multiple MFs. For example average delay d is mapped to two degree of truth values (α and β ) by MFs for low delay and medium delay, correspondingly. These degree of truth values further invoke rule 1 and rule 2 (illustrated by dashed lines), which show the consequences of the delay. For example, IF average packet delay is LOW , THEN network is EM P T Y . This progress is called inferencing using the rule base. To quantify the consequence, each rule is assigned a CMF, which can be treated as the “opposite” function of MF. CMF de-fuzziﬁes all the consequences of the invoked rules back to one Consequence Value (denoted by C.V. in the ﬁgure). This progress is called defuzziﬁcation. CMF can be the same or different shapes as the corresponding MF, but often with both ends closed [16]. There are dozens of approaches to implement defuzziﬁcation, each with various advantages or drawbacks [16]. One of the most popular is the “centroid” method, in which the “center of mass” of the results provide the consequence value. The “mass” (gray area in Fig. 3) of each CMF is determined by the degree of truth value from the corresponding MF. It is shown in Fig. 3 that given the average packet delay d, we can say the network is in EM P T Y state, since the ﬁnal C.V. falls in the EM P T Y CMF. If the C.V. falls in the inter-crossed section [S2 , S3 ], the network status can be interpreted in either way. More about the “centroid” method will be introduced in Section IV. We also illustrate there what difference can be made if network status recognition is based on two metrics (average packet delay, average link utilization) instead of one. IV. FU Z ZY F LOW R EGU LAT ION FOR CMP A. Design Overview To realize our fuzzy ﬂow regulation mechanism requires: 1) module to recognize the network congestion status, and 4B-4 2) module to make new ﬂow regulation decisions and update the old ones. Fig. 1 shows the required modules as network state recognizer (NSR) and fuzzy controller (FC), respectively. Bold lines in the ﬁgure denote data paths and thin lines control paths. The NSR detects network congestion status from the performance statistics of memory transaction (link utilization and packet delay) , and the FC updates new regulation policies based on both network congestion status and on unregulated ﬂows of the local processing core. Note that the NSR is one per CMP chip, but the FC is attached to every core. In this section, we present our fuzzy ﬂow regulation from concept to design. We ﬁrst introduce one problem of on-chip ﬂow regulation, followed by design of the FC and NSR. As to design of leaky bucket ﬂow regulator, please see [7, 1]. B. Concept: Sampling based Fuzzy Flow Regulation One problem we face in CMP ﬂow regulation is control latency, like any other control system introduced in [17] has. Control latency is the time between the generation of a control signal and the control signal begins to take effect. In our fuzzy ﬂow control system, the ﬂow regulation mechanism reacts to network status. This progress consumes time and this time is the control latency. Many factors can effect the control latency, such as the overhead of FC, response time of NSR, the system clock frequency, delays of the on-chip routers, etc. In order to tolerate the control latency problem, we design our fuzzy ﬂow regulation mechanism based on sampling windows. The fuzzy controller does not make new ﬂow regulation policy in every cycle. It samples different input signals during a period of time (called sampling window), and makes new policy after the sampling window is over. The new policy will make inﬂuence on the network status during the next sampling window. I nj ection rate Fuzzy Controller for PN Fuzzy Controller for P1 Fuzzy Controller for P0 n o i t a c ﬁ i z z u F Inference mechanism Rules n o i t a c ﬁ i z z u f e D N etwork state (Empty, Normal, Saturated) PN with $N P1 with $1 P0 with $0 (cid:2) ρ Regulator for P0 ρ Network Regulator for P1 Regulator for PN Statistics Unit Network State Recognizer N etwork inf o. (avg. delay; avg. link utilization) CM P C hip Fig. 4. Structure of fuzzy ﬂow regulation system As shown in Fig. 4, three components: fuzzy controller (FC), network state recognizer (NSR), and ﬂow regulator work together to achieve fuzzy ﬂow regulation. FC is very simple conceptually. At the end of each sampling window, three stages are invoked in each FC: an input stage (fuzziﬁcation), a processing stage (inferencing using rule base), and an output stage (defuzziﬁcation). In the input stage, inputs (injection rate, ρ (cid:2) , of the unregulated data ﬂow from each processor) are mapped by the appropriate membership functions to degree of truth values. In the processing stage, appropriate rules are invoked and a result for each rule generated. In this step, the network status indicator K (has three values : empty , normal, saturated) 345 generated by the NSR inﬂuences the mapping of each rule. The principle is that if the network is empty, the ﬂow regulator casts a relaxing policy; if the network is saturated, the ﬂow regulator tights up the incoming ﬂow; if the network works normally, the ﬂow regulation policy casts no effect. Finally, the output stage converts the combined results back into a new regulation policy. The new ﬂow regulation policy updates the old one inside the regulator, which is leaky bucket based. The regulator forces the output ﬂow to conform to the new injection rate ρ. (cid:2) ) of a data We only restrict the per-window injection rate (ρ source, since in CMP the burst (σ ) is determined by the data block size of memory transaction and that can not be changed. The NSR is also based on fuzzy logic, due to the fact that it is hard to calculate NoC status by mathematical/empirical model. The approach we use is to monitor network state through network statistics information (average packet delay and average link utilization), as it is done in [3]. The network statistics information is collected by the statistics unit shown in Fig. 4, which is provided by GEMS [13]. Similar with the FC, NSR is also realized by three stages: fuzziﬁcation, inferencing using rule base, and defuzziﬁcation. The difference lies in that the NSR makes fuzzy decision on two inputs while the FC on one. C. Design: Fuzzy Controller The aim of the FC is to make ﬂow regulation policy based on the status of network congestion. It takes the unregulated (cid:2) ) as input, and generates the target injection injection rate (ρ rate (ρ) as output. Based on the window mechanism, the sam(cid:2) is straightforward. Within each sampling window, pling of ρ a ﬂow is sampled at each time instant tk for its trafﬁc volume f (tk ), where tk ∈ [L0 , L0 + Lw ] and k ∈ [1, S ], with L0 indicating the start time of a window and Lw the length, (cid:2) is comand S is the total number of samples. At each tk , ρ (cid:2) puted by ρ k = f (tk )/tk to obtain the average rate of the ﬂow. In this paper, we stipulate that all the sampling intervals are equal, which implies that all the tk are distributed evenly within [L0 , L0 + Lw ]. Based on the sampled ρ (cid:2) k , the inferencing of each rule in the rule base is inﬂuenced by the network status indicator generated by NSR. Here we use K to symbolize the indicator, where K ∈ [empty , normal, saturation]. , M ), where M = {mi : i = 1, 2, ..., N }. Let mi denote the ith membership function of the linguistic (cid:2) deﬁned over the universe of discourse P (cid:2) . In our variable ρ (cid:2) denotes “unregulated injection rate” and P (cid:2) denotes design, ρ (cid:2) . If we assume there are N memall the possible values of ρ bership functions, then the fuzzy set S of the fuzzy controller is a pair: S = (P (cid:2) We assume that P (cid:2) = [0, ∞] and N = 7. Similarly, let cj delinguistic variable ρ deﬁned over the universe of discourse P . note the j th consequence membership function (CMF) of the In this paper, ρ denotes “regulated injection rate” and P all the possible values of ρ. Similarly, j ∈ [1, 7] and P = [0, ∞]. As introduced in Section III, the mapping of the inputs to the outputs is characterized by a set of rule base, or formally, in modus ponens (If-Then) form. In FC, rules in the rule base has the form: IF ρ (cid:2) is mi (ρ (cid:2) ), THEN ρ is cj (mi (ρ (cid:2) )), i, j ∈ [1, 2, ..., 7] By iterating i and j , we get a set of linguistic rules that specify on how to regulate the ﬂows. V ery low Low S lightly Low 1 S lightly high H igh V ery high 4B-4 × η 1 7 × η 3 7 × η 5 7 V ery low Low S lightly Low η 1 × η 9 7 × η 11 7 × η 13 7 (cid:2) ρ (packet/cycle) S lightly high H igh V ery high × η(b1 ) 1 7 × η(b2 ) 3 7 × η(b3 ) 5 7 η(b4 ) × η(b5 ) 9 7 11 7 × η(b6 ) × η(b7 ) 13 7 ρ (packet/cycle) k ) k )) k ) to cj (mi (ρ Fig. 5. Membership functions with different mapping (δ = 0, ±1) During f uzz if ication and inf erencing , S samples of ρ (cid:2) (cid:2) are recored and denoted by ρ k , k = 1, 2, ..., S . Further, the degree of truth of each sample is denoted by mi (ρ k ), where mi (cid:2) (cid:2) is the ith membership function. The “mass” of each mi (ρ (cid:2) in the corresponding CMF is denoted by cj (mi (ρ k )), where cj is the j th CMF. Different from static regulation and partly dynamic regulation [1], FC makes new regulation policy based on the network congestion status. The network status indicator K determines the extent to which each rule is relevant to the current situation. The mapping principles are as follows : • When K is normal, mapping of mi (ρ (cid:2) (cid:2) is done by f : mi → cj , i = j , and is shown by the solid lines in Fig. 5. It shows that the network works ﬁne (cid:2) , and the trafﬁc just gets with the current injection rate ρ through without regulation. • When K is empty , mapping of mi (ρ (cid:2) (cid:2) done by f : mi → cj , i = j + δ, δ ≤ 0. It shows that when empty, the network can accept more packets and increas(cid:2) is accepted. One example of δ = −1 is shown ing of ρ by the dashed lines in Fig. 5. • When K is saturated, mapping of mi (ρ (cid:2) (cid:2) is done by f : mi → cj , i = j + δ, δ ≥ 0. It shows that when saturated, the network tights up the injection rate to recover from saturation. One example of δ = 1 is shown by the dotted lines in Fig. 5. Note that η in Fig. 5 denotes the average injection rate of each data source within a sampling window. In a CMP environment, data and control packets are injected if and only if cache miss happens. Thus η is treated as the average cache miss ratio (ACMR). Calculation of ACMR is beyond the topic of this paper. For more information please refer to [18]. During def uzz if ication, the centroid method is used to (cid:2) k )), i, j, ∈ [1, 2, ..., 7] into a single result by combine cj (mi (ρ the following formula: k ) to cj (mi (ρ k )) is k ) to cj (mi (ρ k )) (cid:2)N ,N ,S (cid:2)N ,N ,S j,i,k j,i,k (cid:2) k )) (cid:2) bj cj (mi (ρ cj (mi (ρ k )) ρ = , (2) where bj is the center of cj . The new ρ is passed to update the ﬂow regulator, and the effects of the new regulation policy becomes effective during the next sampling window. D. Design: Network State Recognizer As mentioned in Section III, we also design the NSR with fuzzy logic approach, since the task of constructing and analyzing a mathematical model for complex systems such as network-on-chip is difﬁcult, especially when resource contention issues are considered [1]. The structure of NSR is 346 similar to that of FC (fuzziﬁcation, inferencing with rule base, and defuzziﬁcation), but differences exist in that NSR has two inputs (average link utilization and average packet delay) instead of one. 0 3 ξ 3 ξ avg. link utilization (packet/cycle) M EDIUM (md 2 ) LOW (md 1 ) ξ M EDIU M (ml 2 ) LOW (ml 1 ) H IGH (ml 3 ) ml 1 (lk ) = 0.75 2 4 0.5 1 lk 0 3 ζ avg. packet delay (cycle) ζ H IGH (md 3 ) 2 3 ζ 4 0.5 1 dk md 2 (dk ) = 0.6 Network State Indicator K b2 Empty(cnsr min{0.75, 0.6 } = 0.6 1 ) N ormal(cnsr ) Saturated(cnsr 3 ) 2 b3 b1 Fig. 6. Minimal intersection and defuzziﬁcation Let lk and dk denote the k th samples of linguistic variables “average network link utilization” and “average network packet delay” in the corresponding universe of discourse L and D , where L, D ∈ [0, ∞], k ∈ [1, S ]. Let ml i and md j denote the ith and j th membership functions invoked by lk and dk , respectively. During fuzziﬁcation, the membership functions map lk and dk to degree of truth values ml Further, the fuzzy set for NSR is: S = (D , M d ) × (L, M l ) = i : i = 1, 2, ..., N l }. The “∗” operation symbolizes the intersection of the fuzzy sets, which achieves the mapping M d ∗ M l : D × L → [0, 1]. In this paper, we manipulate that N d = N l = 3, and we use and as the intersection method (“*”) of membership functions. There are many ways to implement the and intersection [16, 19]. In the NSR, we use the It is based on the principle that if we are not very certain about the truth of one proposition, we can not be any more certain about the truth of that proposition “and” the other proposition. As shown in Fig. 6, two samples lk and dk generate four degree of truth values that are non-zero and the intersection of the ones generated by membership functions 1 and md 2 is illustrated in dashed line. It is shown that by using minimal intersection, the degree of truth of the combined proposition is determined by the lower one. The inferencing progress is determined by the rule base. We further manipulate that the network is in one of the three status: empty , normal, saturated. Thus, the number of CMF is also three, and each one is denoted by cnsr mapping M d ∗ M l : D × L → [0, 1], according to the and depicts both the MFs and the CMFs of NSR. Considering the intersection method, rules in NSR has the form: i (lk ) and md j (dk ). (D × L, M d ∗ M l ), where M d = {md j : j = 1, 2, ..., N d } and M l = {ml minimal method. ml x , x ∈ [1, 2, 3]. Fig. 6 IF lk is m THEN K is c l i (lk ) and dk is m d j (dk ), j (dk ))). i, j, x ∈ [1, 2, 3], i (lk ), m nsr x (min(m l d where K is the network status indicator as mentioned in Section IV.B. Since N d = N l = 3, there are totally nine rules for the combined propositions. For an unstable network [3], rule base can be designed pessimistically as shown in Table 1. To avoid performance drops, the unstable network is treated as saturated if any of the two inputs reaches high. For a stable network [3], more optimistic rules can be used. TABLE I RU L E TAB L E O F TH E N E TWORK S TAT E R ECOGN I Z ER N etwork status indicator K P acket delay (cycle) M edium N ormal N ormal Saturated Low Empty N ormal Saturated H igh Saturated Saturated Saturated Link utilization (packet/cycle) Low M edium H igh During def uzz if ication, a consequence value K (network status indicator) is generated using the centroid method: K = (cid:2)N d ,N l ,3,S bx cnsr (cid:2)N d ,N l ,3,S j,i,x,k cnsr j,i,x,k x (min(md j (dk ), ml i (lk ))) x (min(md j (dk ), ml i (lk ))) , (3) where bx is the center of cnsr x . The network status indicator K then records in which section (empty, normal, saturated) itself falls and is passed to the fuzzy controller. V. EX P ER IM EN T AND R E SU LT S A. Experiment Setup Our design is implemented using C++ in the cycle accurate full system simulator GEMS [13], which aims to characterize and evaluate the performance of multiprocessor systems. P L2 R P L2 R P L2 R P L2 R P L2 R P L2 R P L2 R P L2 R P L2 R P L2 R P L2 R P L2 R P L2 R P L2 R P L2 R P L2 R M0 M1 M2 M3 Chip 0 Chip 1 Chip 2 Chip 3 Fig. 7. Experiment CMP architecture The architecture of the experimental CMP is constructed as shown in Fig 7. Due to space limitation, we do not show FC, NCR, and ﬂow regulator. For their placement please refer to Fig. 1. Each CMP chip includes four processors (denoted by “P”, with one L1 cache bank built in) and four L2 cache banks (denoted by “L2”). The L1 cache is private and the L2 cache shared. We simulate four CMP chips, with 16 L1 cache banks (64KB each bank) and 16 L2 cache banks (4MB each bank), totally. Outside the CMP chip exists four external memory banks (denoted by “M”, with 1 GB each). The memory module mimics detailed DDR2/DDR3 SDRAMs. The memory coherence protocol is directory based MOESI protocol. The routing algorithm is “X-Y” routing and ﬂow control type is wormhole. The experiments are done with both open-loop and closed-loop measurements [3]. The open-loop measurement focuses on the performance of NoC, while the closed-loop measurement focuses on the performance of the full system. B. Open-loop Measurement: Results of Synthetic Flows In the open-loop measurement, we use both uniform random trafﬁc (to simulate load balanced situation) and permutation trafﬁc (to simulate load unbalanced situation). Fig. 8 347 4B-4 0.25 0.2 0.15 0.1 0.05 e g a n e c t r e P No Regulation Fuzzy Regulation Static Regulation Max Delay 259 cycles Max Delay 267 cycles Max Delay 498 cycles 0.4 0.3 0.2 0.1 e g a n e c t r e P Fuzzy Regulation Static Regulation No Regulation Max Delay 200 cycles Max Delay 169 cycles Max Delay 498 cycles 0 0 200 400 Total Delay for Uniform Traffic (cycles) 600 0 0 200 400 600 Total Delay for Permutation Traffic (cycles) Fig. 8. Delay histogram for open-loop measurement summarizes the global average packet delay histogram. The fuzzy ﬂow regulation consistently improves the results of static regulation and no-regulation for both trafﬁc patterns. In comparison with static regulation, the improvement in global average packet delay is 68 cycles (48.3% in percentage) for uniform random trafﬁc and 73 cycles (53.0% in percentage) for permutation trafﬁc. In comparison with no regulation, it is 10 cycles (12.6% in percentage) for uniform random trafﬁc and 73 cycles (37.4% in percentage) for permutation trafﬁc. Permutation trafﬁc gets more improvement due to trafﬁc unbalance which stresses the network more than uniform random trafﬁc does. Fig. 8 further shows that the statically regulated ﬂows have a large distribution in delay range from 100 cycles to 500 cycles, which is due to the queueing delay in the regulator’s waiting queue. The unregulated ﬂows have the narrowest delay span (from 50 cycles to 250 cycles approximately), but 95% of the packet delays fall in range from 70 to 100 cycles, which is caused by high network transaction delay (since the ﬂow is unregulated). The delay of fuzzy regulated ﬂow spans from 5 cycles to 300 cycles, and the distribution focuses on 50-150 cycles. This gain is due to fuzzy regulation system makes regulation policy based on the status of the network. Unnecessary queueing delay and large network transaction delay are both avoided. C. Closed-loop Measurement: Results of Benchmarks In the closed-loop measurement, we experimented with the Stanford ParalleL Applications for SHared Memory benchmark suite 2 (SPLASH-2) to conﬁrm the performance beneﬁts brought by the fuzzy ﬂow regulation system. Of all the 14 benchmarks, we choose to report 4 of them as shown in Fig. 9, which depicts the throughput of the whole experiment CMP system. In benchmark Barnes, the improvement of fuzzy regulation against no regulation is 0.04 packet/cycle (22.2%), and against static regulation is 0.06 packet/cycle 0.4 No Regulation Static Regulation Fuzzy Regulation 0.22 0.2 0.18 0.16 0.26 0.21 0.19 ) e l c y c / t e k c a p ( t u p h g u o r h T 0.4 0.33 0.3 0.35 0.3 0.26 0 Barnes Ocean Radiocity Fig. 9. Experiments with closed-loop measurement LU 4B-4 (37.5%). In benchmark Ocean, the improvement of fuzzy regulation against no regulation is 0.05 packet/cycle (23.8%); and against static regulation is 0.07 packet/cycle (36.8%). In benchmark Radiocity, the improvement of fuzzy regulation against no regulation is 0.05 packet/cycle (16.6%), and against static regulation is 0.09 packet/cycle (34.6%). In benchmark LU, the improvement of fuzzy regulation against no regulation is 0.07 packet/cycle (21.2%), and against static regulation is 0.1 packet/cycle (33.3%). V I . CONC LU S ION S The central idea of this work is to make network status aware ﬂow regulation through a fuzzy logic approach. On GEMS, our experiments with both synthetic trafﬁc and SPLASH-2 benchmark traces show that the fuzzy regulation can ﬂexibly adjust regulation strength on demand. As a result, it makes more effective use of the system interconnect, achieving signiﬁcant improvement in average packet delay and throughput. V I I . ACKNOW L EDGM EN T The research is sponsored in part by Intel Corporation through a research gift. "
2014,NoΔ - Leveraging delta compression for end-to-end memory access in NoC based multicores.,"As the number of on-chip processing elements increases, the interconnection backbone bears bursty traffic from memory and cache accesses. In this paper, we propose a compression technique called NoΔ, which leverages delta compression to compress network traffic. Specifically, it conducts data encoding prior to packet injection and decoding before ejection in the network interface. The key idea of NoΔ is to store a data packet in the Network-on-Chip as a common base value plus an array of relative differences (Δ). It can improve the overall network performance and achieve energy savings because of the decreased network load. Moreover, this scheme does not require modifications of the cache storage design and can be seamlessly integrated with any optimization techniques for the on-chip interconnect. Our experiments reveal that the proposed NoΔ incurs negligible hardware overhead and outperforms state-of-the-art zero-content compression and frequent-value compression.","NoΔ: Leveraging Delta Compression for End-to-End Memory Access in NoC Based Multicores 7A-1 Jia Zhan* , Matt Poremba* , Yi Xu† , Yuan Xie* † not be applicable to modern SoC implementation that may comprise several unmodiﬁable hard IP blocks from various vendors. *The Pennsylvania State University, {juz145,poremba,yuanxie}@cse.psu.edu † AMD Research China Lab, {yi1.xu,yuan.xie}@amd.com Abstract—As the number of on-chip processing elements increases, the interconnection backbone bears bursty trafﬁc from memory and cache accesses. In this paper, we propose a compression technique called NoΔ, which leverages delta compression to compress network trafﬁc. Speciﬁcally, it conducts data encoding prior to packet injection and decoding before ejection in the network interface. The key idea of NoΔ is to store a data packet in the Network-on-Chip as a common base value plus an array of relative differences (Δ). It can improve the overall network performance and achieve energy savings because of the decreased network load. Moreover, this scheme does not require modiﬁcations of the cache storage design and can be seamlessly integrated with any optimization techniques for the on-chip interconnect. Our experiments reveal that the proposed NoΔ incurs negligible hardware overhead and outperforms state-of-the-art zero-content compression and frequent-value compression. Alternatively, simple compression/decompression modules can be integrated in the network interface (NI) and triggered to compress network trafﬁc. In this case, there is no need to modify the cache structure, allowing for a plug-and-play capability. Additionally, the whole process is conducted in the NI, and thus it is orthogonal and complementary to any further optimization techniques for the switch microarchitecture or network topology. Recently, data compression is explored in the research domain of Network-on-Chip. For instance, Das et al. [10] proposed to compress network messages based on zero bits in a word. Subsequently, Jin et al. [11] and Zhou et al. [12] adopted frequent value compression and design table-based schemes to manage the compression/decompression process. I. Introduction The last few years we have witnessed a growing trend of accommodating many cores and large caches in chip multiprocessors. Conventional shared-memory multi-core design adopts a single bus with limited bandwidth as the communication backbone. Consequently, the bus bears enormous strain and even becomes the performance bottleneck due to frequent packet transmission. To provide an efﬁcient and scalable interconnect for memory and cache accesses, researchers proposed Network-on-Chip (NoC) [1] to replace conventional busbased architectures. However, NoCs also have limitations such as long end-to-end communication latency and high network power dissipation. Therefore, many previous studies [2]–[4] explored optimization on router microarchitecture, network topologies, routing algorithms, etc. However, these techniques inevitably increase design complexity and even incur signiﬁcant performance or power overhead. Instead of seeking new NoC architectures, an alternative yet complementary approach is data compression, also known as source coding, which compresses data messages into more compact bits before they are stored or transmitted. Data compression has been utilized in hardware design to help enhance system performance as well as diminish power dissipation. For example, cache compression expands the cache capacity by packing in more data blocks than given by the space. Dusser et al. [5] proposed Zero-Content Augmented (ZCA) cache design that is able to represent zero cache lines. Zhang et al. [6] discovered the frequent value locality in many application programs, i.e., a few values appear repeatedly and engage in a large portion of memory accesses. They proposed a compression cache [7] that employs a value-centric mechanism with regard to such a phenomenon. Additionally, Alameldeed and Wood [8] exploited some frequent patterns in programs such as all bytes in a 4-byte word are the same. Recently, Pekhimenko et al. [9] introduced a new compression algorithm called Base-Delta-Immediate (BΔI) compression, which stores cache lines as a set of differences (Δ) to the base values. However, all of these cache compression techniques require fundamental changes of the cache architecture to support variable sizes of cache lines. As a result, cache compression may This work is in part supported by NSF CCF-0903432, CNS-0905365, and SRC grant. 978-1-4799-2816-3/14/$31.00 ©2014 IEEE 586 Delta compression is a method to transmit data in the form of differences rather than the complete data set. It is widely adopted in video compression codecs to considerably reduce frame size, or in the networking domain to reduce Internet trafﬁc by allowing HTTP servers to send updated Web pages based on differences between versions. For memory/cache data, they also display a low dynamic range due to several reasons: programs tend to change register numbers or memory addresses and the changed values are the same across many instructions; similar data values and types are grouped together in the memory; arrays are commonly used to represent large piece of data, etc. This indicates that delta compression can be a promising candidate to compress memory/cache trafﬁc ﬂowing through the on-chip interconnect. Therefore, in this work, we investigate a new compression scheme called NoΔ, which leverages the delta compression technique in NoC-based multi-core by transmitting values of bytes as differences (Δs) between sequential values, rather than the complete values themselves. We demonstrate how delta compression can be applied to the NoC domain with speciﬁc architecture support and evaluate the corresponding performance gain and power savings. Overall, this paper makes the following major contributions: • To the best of our knowledge, this is the ﬁrst work that exploits the usage of delta compression in NoC-based architectures. We run full system simulation with SPEC CPU2006 benchmark suite and analyze data patterns to get the compression ratios of different applications. • A lightweight compression/decompression module is designed which incurs minimum area and power overhead. We integrate these modules in the Network Interfaces and provide detailed synthesis results to validate the architecture design. • We compare the proposed NoΔ technique over previous zerocontent compression and frequent-value compression. Experimental results show that our scheme outperforms the other two in terms of both performance and power. II. Data Compression in On-Chip Networks A. NoC-based Multi-core Architecture The revolution of multicore/Chip-Multiprocessor (CMP) opens up opportunities to circumvent the power wall, but it also poses challenges on the design of the cache hierarchy. Consequently, the last level cache (L2, or where applicable, L3 caches) becomes oversized to accommodate the needs of all cores, and gets fragmented into numerous banks due to the increased capacity. However, it becomes clearly inefﬁcient to demand that every cache access incurs the delay penalty of accessing the furthest bank. To this end, some researchers proposed innovations of network fabric that enable caches to support non-uniform cache bank accesses, which are referred to as NonUniform Cache Access (NUCA) architectures [13]. Fig.1 depicts the high-level view of our NoC-based multi-core system in this paper. This 4×4 tile-based system forms a 2D mesh network interconnected through routers. In this architecture, each node consists of a CPU core, a private L1 instruction and data cache, and a separate bank of a shared last level cache (L2). In this work we adopt a SNUCA [14] architecture for the L2 where the mapping of data blocks is unique and cannot be dynamically moved. These processing elements are coupled with the router through the Network Interface (NI). The main memory is attached to this multi-core chip via memory controllers (MCs) on the corners. Each MC controls a single memory channel with possibly many DIMMs. MC MC 1 L1 5 L2 2 MC 4 3 MC Tile link link NI L2 Core L1-I$ L1-D$ link Router To router link Fig. 1: A 4×4 NoC-based multicore with SNUCA architecture. B. Memory Access and Packet Switching Fig.1 also demonstrates the detailed ﬂow of a memory access that produces network trafﬁc. Speciﬁcally, once a core issues a memory request, its private L1 cache is ﬁrst checked to see whether the data is locally available. If the data is not available in the L1, the request is then injected into the on-chip network and forwarded to an L2 bank (path 1). Subsequently, if an L2 miss is encountered, the request message is further delivered to the appropriate memory controller (path 2), and the off-chip memory access (path 3) is triggered consequently. Correspondingly, the response message from memory goes back on chip and is sent to the L2 cache bank (path 4) for updates. It is ﬁnally transferred to the L1 (path 5) that can be accessed by the core. Note that in this example, a private L1 cache hit would not generate any network trafﬁc, and an L2 cache hit would not further lead to message ﬂow in paths 2 to 4 which are directly used for off-chip memory request/response. The memory request/response messages together with the cache coherence messages infuse into the on-chip network through the Network Interface (NI). A NI is responsible for the packetization/depacketization of these messages, and the ﬂit fragmentation/assembly for ﬂow control. When running some memory-intensive applications 587 7A-1 on the cores, there will be many packets competing for the shared resource in the network. As a result, network latency plays a vital role in memory accesses to deliver the required Quality-of-Service. In addition, network power dissipation will increase signiﬁcantly due to frequent packet accesses. In this work, we analyze the end-to-end memory access patterns in NoC-based multi-core and explore data compression to reduce the packet size, and hence relieve network load for performance improvement as well as power savings. C. NoΔ Mechanism In general, data packets are generated by load and store instruction misses in the processor cache. These misses are converted into read request and write request packets and forwarded into the network. Each read request contains the memory address to be read where data will be fetched in a read reply message. The write request contains not only the address information but also the actual data to be stored. Fig.2 illustrates two packet formats, where the header ﬂit includes the address and other packet identiﬁcation information, and the body ﬂits contain the actual data. Read request/ Write reply Write request/ Read reply Addr Other Addr Other Data Header Body (including tail) ﬂits Fig. 2: The two packet formats required for end-to-end memory accesses through on-chip interconnect The goal of data compression is to explore the internal redundancy of data values and squeeze packets into more compact units. However, data compression in on-chip networks may result in extra queuing delays of packets in the network interfaces as well as hardware overhead from those compression/decompression modules. In this work, we apply a lightweight compression mechanism called NoΔ to the on-chip interconnect domain. It succeeds to achieve a signiﬁcant compression ratio with minimal extra overhead. NoΔ is adapted from delta compression and leverages a key observation: the data values stored in the packets that are traversing the network have a low dynamic range, i.e., when the whole packet is divided into multiple ﬁxed-length units, the relative value differences between different units are small. Therefore, the original data can be reorganized as a common base value of ﬁxed size plus an array of relative differences (Δs). Then the packet size information will be updated in the header ﬂit. Assume that the data portion of a packet has D bytes, the size of base value is B bytes, and the other values can be represented by a set of Δs: {Δ1 , Δ2 , ..., Δn }, where n = D/B . Then we can claim that the packet is compressible only if: (1) (2) max{size(Δi )} < B , ∀i ∈ {1, 2, ..., n} Then the size of delta (Δ) chosen for compression is: size(Δ) = max{size(Δi )}, ∀i ∈ {1, 2, ..., n} For different data patterns, Fig.3 plots the percentage of packets that have a speciﬁc data pattern over the total number of network packets. SPEC CPU2006 benchmark suite [15] is used for our observations . We compare the ratio of zero-content packets (Denoted as ZeroCmpr), frequent value packets (Denoted as FreqCmpr), and The detailed evaluation methodology is described in Section IV. 7A-1 packets that have NoΔ compression capabilities. As the ﬁgure demonstrates, data packets that have “Δ” patterns constitute a big portion of the total network packets. On average, the ratios of compressible packets for ZeroCmpr, FreqCmpr, and NoΔ are 13.5%, 14.6%, and 21.9%, respectively. Therefore, leveraging these “Δ” patterns in the NoC compression can potentially achieve good performance. Note that these results only show the rough ratios of compressible packets that motivate our work; further investigation of how much trafﬁc can be reduced is required. q ZeroCmpr  FreqCmpr NoΔ  o i t a R i n o s s e r p m o C t e k c a P 45%  40%  35%  30%  25%  20%  15%  10%  5%  0%  gob mk  leslie3d  libquantu soplex  strea m  calculix  h264ref  na md  sjeng  specrandi  Ge msFDT xalancb m dealII  specrandf  Geo Mean  Fig. 3: Ratios of compressible packets for different patterns As an example, Fig.4 demonstrates how NoΔ works for a data packet that consists of a header ﬂit and four data ﬂits. Here we assume that a ﬂit is only four-byte long and the ﬁrst data ﬂit is treated as the base. As the ﬁgure indicates, the data ﬂits can be represented using a four-byte base value, 0xC0D45800, and an array of three two-byte deltas. As a result, the deltas can be stored comfortably into one ﬂit and hence the whole packet is compressed into three-ﬂit long. We refer to this compression scheme as ﬂit-based compression because the base size is exactly equal to one ﬂit. Multiple bases: Our observation shows that a single packet may mix different data patterns. Therefore applying delta compression with multiple bases can increase the compression ratio. To improve compression efﬁciency without sacriﬁcing design simplicity, we use two bases: one is always 0; the other is the ﬁrst arbitrary value from the set of ﬁxed-size data values, just like the example in Fig.4. Table I shows an encoding table that summarizes the bases and deltas we analyze in this work. Note that all potential compressed sizes are known statically. The smallest compressed packet size will be chosen in case multiple candidates are applicable for the same packet. The encoding bits are used to identify which base-delta pair has been applied for a compressed packet. For a packet that contains all zeros, only one extra bit “0” will be added to the header. TABLE I: Encoding table for the NoΔ. All sizes are in bytes. Name Zero B16Δ8 B16Δ4 B16Δ2 B16Δ1 Base Δ NA NA 16 8 16 4 16 2 16 1 Enc. 0 0001 0010 0011 0100 Name Base Δ B8Δ4 8 4 B8Δ2 8 2 B8Δ1 8 1 B4Δ2 4 2 B4Δ1 4 1 Enc. 0101 0110 0111 1000 1001 III.Architecture Support To support compression and decompression in a network-on-chip, modules for both functions need to be placed in all of the NIs. When data is injected into the network, the compression module tests if compression is possible. Likewise, the decompression module checks if the ejected data is compressed. Therefore, the data is only compressed when it is injected into the network, and does not need to be modiﬁed between hops. Original packet 4 bytes Header 4 bytes 4 bytes 4 bytes 4 bytes 0xC0D45800 0xC0D45801 0xC0D4580F 0xC0D4583A A. Compression Module Compressed packet Header 0xC0D45800 0x01 0F 3A saving two ﬂits ! 4 bytes 4 bytes 1 byte Fig. 4: Packet compression demonstration Additionally, the above ﬂit-based compression algorithm can be extended in two ways to further improve the compression ratio: Variable-length base: Instead of ﬁxing the size of the base to one ﬂit, a data packet can be represented by different sets of ﬁxedsize values. For example, a 64-byte data can be represented as 4 16-byte, 8 8-byte, or 16 4-byte values. Then the base value can be of size 16B, 8B, or 4B accordingly. In this case, multiple choices are provided to accommodate various data localities. In contrast, the ﬂitbased compression, as indicated in Fig.4, has two major limitations: • The base is always ﬁxed as one-ﬂit long, and thus it would be difﬁcult to ﬁnd another ﬂit whose value has a relative difference that falls within a small range of Δ. • Since a ﬂit is the minimal ﬂow control unit, a packet can be at most encoded into three ﬂits: the header ﬂit, the base ﬂit, and another ﬂit that stores all Δs. Alternatively, variable-length base compression can aggressively squeeze a data packet into two-ﬂit long, or even single-ﬂit long if there is extra space in the header ﬂit that can be utilized. We use a four-byte long ﬂit in the example to save space. In real experiments, our baseline ﬂit size is 16 bytes. A diagram of the compression modules is shown in Fig.5. Here we use the same packet from Fig.4. The size of the base is 4 bytes, and each delta is a 1-byte difference between the base and the subsequent 4-byte segments. The ﬁgure contains 3 subtractors to calculate the differences between each of the four 4-byte packet segments and the base. In general, N-1 subtractors are needed for a packet with N segments. The subtractors must have a bit width of the segment size, since we need to check all of the most signiﬁcant bits to check for sign extensions. In the example, the subtractor would have 32-bit inputs and outputs, and the 24 most signiﬁcant bits are checked for sign extension. Checking for sign extension is done by simply using AND and OR gates on all of the most signiﬁcant bits. The output of the AND is high if all the bits are ones and the output of the OR is zero if all the bits are zero. When all the bits are zeros or ones, the output of the subtractor is considered to be sign extended. If any of the subtractor outputs are sign extended, the packet can be compressed. The outputs of the sign extension are placed in the header ﬂit to be used as enable signals for the subtractors in the decompression unit. 0xC0D45800 0xC0D45801 0xC0D4580F 0xC0D4583A Subtractor 1 Subtractor 2 Subtractor 3 Sign Extended? Sign Extended? Sign Extended? compress Fig. 5: A single NoΔ compression module 588       7A-1 B. Decompression Module TABLE II: Compressed packet sizes and priorities for the NoΔ encodings Decompression modules use subtractors to regenerate the original uncompressed packet via two’s complement addition. An example of a decompression module is given in Fig.6. The base value is passed through directly to the output and each of the subtractors. Each of the following 1-byte segments following the 4-byte base are passed to the subtractors. An enable signal on the subtractor is tied to the sign extension data placed in the header ﬂit by the compression module. When a subtractor is disabled, a value of zero is used in place of the base and the output is the original uncompressed data. The sign extension bits from the header are also used to select the secondary input to the subtractor via a multiplexor. This selects either Δ bits if the segment is compressed, or an entire segment if it is uncompressed. The output of the subtractors is the reconstructed packet data. In this module, the subtractors only need to be the width of the delta. In this example, the subtractor inputs and output are 8-bit signals. The remaining upper bits, 24 bits in the example, can be hard wired to the most signiﬁcant bits of the base when compression is enabled. 0xC0D45800 0x01 0x0F 0x3A en en en Subtractor 1 Subtractor 2 Subtractor 3 0xC0D45800 0xC0D45801 0xC0D4580F 0xC0D4583A Fig. 6: A single NoΔ decompression module C. NoΔ Architecture In order to accommodate compression of different base or Δ sizes, multiple compression modules are needed. Our design includes compression modules for base and delta sizes listed in Table I, organized in parallel as shown in Fig.7. These modules execute simultaneously and output a compress signal specifying whether the packet can be compressed with the given base size. It is possible that multiple modules will indicate that the packet is compressible. In this case, we arbitrarily choose a compression with the smallest compressed number of body ﬂits. Furthermore, we can break ties where the ﬂit length is the same by choosing the encoding with the least decompression power, based on results in section IV-D. The compressed sizes and number of body ﬂits are shown in Table II. The priority in this table indicates the order in which compression techniques are chosen. Lower priority values have larger compression ratios and are chosen ﬁrst, if possible. Compression: From L2 Decompression: To L2 Injected Packet Priority Mux Zero B16Δ8 B16Δ4 B4Δ1 No  Cmpr. Zero B16Δ8 B16Δ4 B4Δ1 No  Cmpr. Priority Mux To Network Ejected Packet From Network Fig. 7: NoΔ architecture with multiple compression and decompression modules IV. Experimental results A. Platform Setup We use GEM5 [16] full system simulator to setup the basic system platform. Ruby [17] and Garnet [18] are enabled to model a detailed Name Zero B16Δ8 B16Δ4 B16Δ2 B16Δ1 Packet Size (Flits) 0 bits (0) 320 bits (3) 224 bits (2) 176 bits (2) 152 bits (2) Priority Name 1 8 5 4 3 B8Δ4 B8Δ2 B8Δ1 B4Δ2 B4Δ1 Packet Size (Flits) 288 bits (3) 176 bits (2) 120 bits (1) 272 bits (3) 152 bits (2) Priority 9 6 2 10 7 memory system and interconnection network. We use DSENT [19] for detailed network power analysis. We model a 16-core multicore architecture at 45nm technology, as shown in Fig.1, Section II. The detailed system conﬁgurations are listed in Table III. Each core has a split private L1 cache (instruction and data cache each 8KB in size). All the cores share a distributed L2 cache that comprises 16 banks. Each bank of the L2 cache has a size of 1MB and is allocated to a tile. From a high-level point of view, these 16 tiles form a 2D-Mesh topology that employs a dimensionorder routing algorithm and wormhole ﬂow control. Each network packet contains ﬁve ﬂits and each ﬂit is set to be 16-byte long. As for the router microarchitecture, we adopt a classic router design with ﬁve-stage pipelines. Each input port of the router contains two virtual channels (VCs). Each VC has a buffer depth of 4 ﬂits. core count clock frequency L1 I & D cache L2 cache memory # memory controllers cache-coherency network topology routing router pipeline buffer Packet length TABLE III: System conﬁgurations 16 2GHz 2-way, 8KB, 1cycle 8-way, 16×1MB, 5 cycles per bank 4GB DRAM 4 (on the corner) MESI protocol 4×4 mesh dimension-order classic ﬁve-stage 2 VCs per port, 4-ﬂit depth per VC 5 ﬂits, each ﬂit 16 bytes B. Performance and Energy Evaluation We use a diverse set of workloads from SPEC CPU2006 [15] benchmark suite for performance and power analysis. Given each workload, the benchmark program is duplicated to 16 instances and mapped to the corresponding 16 cores. Note that most of the workloads we selected are memory-intensive which relatively generate more network trafﬁc. Meanwhile, we also implement two other state-of-the-art compression techniques adopted in the NoC compression domain: zerocontent compression (ZeroCmpr) [10] and frequent value compression (FreqCmpr) [11]. Speciﬁcally, for a zero-content packet, it can be represented by the header ﬂit plus an extra encoding bit “0”; For frequent-value compression, we explore repeated data patterns at different granularity: 16 bytes, 8 bytes, 4 bytes, and 2 bytes. We compare different compression schemes with the baseline that does not utilize any compression techniques. Intuitively, NoC compression decreases the number of ﬂits injected to the network. This can be observed in Fig.8a, which shows the normalized total ﬂit count in the network for different applications. It also reﬂects the compression ratio: the less number of ﬂits traversing the network, the higher the compression ratio. Among the three compression techniques, NoΔ has the highest average compression 589 1.1  1  0.9  0.8  0.7  0.6  0.5  gob mk  leslie3d  libquantu m  soplex  strea m  calculix  h264ref  na md  sjeng  specrandi  Ge msFDT xalancb mk  dealII  specrandf  Geo Mean  N o r m a i l F d e z t i l C n u o t Baseline  ZeroCmpr  FreqCmpr  NoΔ  (a) Total number of ﬂits traversing the network 27  25  23  21  19  17  15  gob mk  leslie3d  libquantu m  soplex  strea m  calculix  h264ref  na md  sjeng  specrandi  Ge msFDT xalancb mk  dealII  specrandf  Geo Mean  A e v r e g a P e k c a t a L t y c n e Baseline  ZeroCmpr  FreqCmpr  NoΔ  (b) Average packet latency comparison 0.14  0.12  0.1  0.08  0.06  0.04  0.02  gob mk  leslie3d  libquantu m  soplex  strea m  calculix  h264ref  na md  sjeng  specrandi  Ge msFDTD  xalancb mk  dealII  specrandf  Geo Mean  A e v r k n L e g a i U i l i t a z i t n o Baseline  ZeroCmpr  FreqCmpr  NoΔ  (c) Average link utilization comparison 1.05  1  0.95  0.9  0.85  0.8  0.75  0.7  gob mk  leslie3d  libquantu m  soplex  strea m  calculix  h264ref  na md  sjeng  specrandi  Ge msFDT xlancb mk  dealII  specrandf  Geo Mean  N o r a n i l d e a z N e t w o r k e n E r y g Baseline  ZeroCmpr  FreqCmpr  NoΔ  (d) Total network energy consumption Fig. 8: Comparisons of different compression techniques on ﬂit count, packet latency, link utilization and network energy. ratio of 21.1% for all applications. However, the compression ratio varies for different applications. For instance, sj eng and stream indicate signiﬁcant space for compression (more than 40%), while some applications like GemsF DT D , xalancbmk , and dealI I display weak compressibility (less than 10%). Note that the number of packets remains the same after compression despite the decrease of ﬂit count. However, since the basic ﬂow control unit is a ﬂit, fewer ﬂits cause less network congestion. As a result, packets will be delivered smoothly without heavily suffering from head-of-the-line blocking. In order to analyze the impact of different compression techniques on the network performance, we collect average packet latencies after running different applications on different compression architectures. As implied in Fig.8b, NoΔ outperforms all the other compression methods. It can lower the packet latency by up to 17.5% (stream) with an average reduction of 10.1%. This phenomenon can be further explained through Fig. 8c: the average link utilization, which reﬂects the network load, drops signiﬁcantly when compression is utilized. NoΔ again reduces the load to the highest degree, i.e, up to 20.8% for sjeng and stream and on average 13.1% for all applications. The decrease of network load reduces network congestion. Therefore, individual packets tend to progressively propagate to their destinations with less contention with each other for shared resources. Furthermore, another primary goal of using data compression in NoC is to conserve energy. Since packet compression will reduce the number of network operations, such as buffer write and read, allocation and arbitration, crossbar traversal, link traversal, etc., the dynamic energy consumption of the communication fabric will be reduced accordingly. On the other hand, the shrink of average packet latency reduces the total system execution time and thus can potentially save the static energy as well. Fig.8d demonstrates the normalized total network energy consumption using different encoding techniques. Compared to the baseline, NoΔ for stream benchmark achieves the highest energy savings — 29.5%. On average, ZeroCmpr, FreqCmpr, and NoΔ cut down the total network energy by 11.1%, 11.9%, and 15.3%, respectively. NoΔ proves to be the most competitive candidate for NoC compression. C. Sensitivity Study Fig.4 demonstrates NoΔ with ten different compression/decompression modules. However, depending on the application, real design may only utilize a few of them to reduce power/area overhead. To evaluate how the selection of encodings affect NoΔ, we test SPEC 2006 with only 16-byte bases, 8-byte bases, or 4-byte bases, respectively. Fig.9 shows the compression ratio of ﬂits. The baseline design (Base all) is to use all the ten different encodings. 50%  45%  40%  35%  30%  25%  20%  15%  10%  5%  0%  Gob mk  leslie3d  libquantu soplex  strea m  calculix  h264ref  na md  sjeng  specrandi  Ge msFDT xalancb mk  dealII  specrandf  Geo Mean  F t i l C o m p r n o s s e i R a i t o Base_all  Base_16  Base_8  Base_4  Fig. 9: Flit compression ratios for a subset of encoders/decoders. Base n means only 16-byte base is utilized. There are three important observations from Fig.9: (1) Different applications may prefer different encodings. (2) For each application, the best encoding design may yield a very close compression ratio as the baseline. (3) On average, Base 4 generates the highest compression ratio, which only incurs 1.0% compression loss compared to the baseline design. Therefore, we may choose Base 4 encodings only while still obtaining satisﬁable compression ratios. 590 7A-1                               7A-1 TABLE IV: Power/Area overhead for different compression schemes Area(mm2 ) Encoder Decoder Total NA NA 0.183 0.00526 0.00523 0.17 NA NA 0.023 0.00443 0.00432 0.14 0.00474 0.00173 0.10 Power(mW) Encoder Decoder Total NA NA 273 NA NA 1626.1 NA NA 220 4.26 3.95 131.36 1.47 0.81 36.48 ZeroCmpr [10] FreqCmpr p [11] FreqCmpr s [11] FreqCmpr a [12] NoΔ V. Conclusion In this paper, we propose NoΔ that conducts data encoding/decoding in the network interface prior to NoC packet injection/reception. The encoder/decoder module incurs minimal area/power overhead. We evaluate NoΔ using a full system simulator with real benchmarks. Experimental results show an average compression ratio of 21.1%, which results in 10.1% performance improvement and 15.3% energy savings. In addition, a comprehensive comparison veriﬁes that our method outperforms other compression techniques like zero-content compression and frequent-value compression.  [2] [5] [7] [1] W. J. Dally and B. Towles, “Route packets, not wires: On-chip interconnection networks,” in DAC, 2001, pp. 684–689. J. Kim et al., “A novel dimensionally-decomposed router for onchip communication in 3D architectures,” ACM SIGARCH Computer Architecture News, vol. 35, no. 2, pp. 138–149, 2007. [3] B. Grot, J. Hestness, S. W. Keckler, and O. Mutlu, “Express cube topologies for on-chip interconnects,” in HPCA, 2009, pp. 163–174. [4] S. Ma, N. E. Jerger, and Z. Wang, “Whole packet forwarding: Efﬁcient design of fully adaptive routing algorithms for networks-on-chip,” in HPCA, 2012, pp. 1–12. J. Dusser, T. Piquet, and A. Seznec, “Zero-content augmented caches,” in ICS, 2009, pp. 46–55. [6] Y. Zhang, J. Yang, and R. Gupta, “Frequent value locality and valuecentric data cache design,” in ACM SIGOPS Operating Systems Review, vol. 34, no. 5, 2000, pp. 150–159. J. Yang, Y. Zhang, and R. Gupta, “Frequent value compression in data caches,” in MICRO, 2000, pp. 258–265. [8] A. R. Alameldeen and D. A. Wood, “Adaptive cache compression for high-performance processors,” in ISCA, 2004, pp. 212–223. [9] G. Pekhimenko et al., “Base-delta-immediate compression: practical data compression for on-chip caches,” in PACT, 2012, pp. 377–388. [10] R. Das et al., “Performance and power optimization through data compression in network-on-chip architectures,” in HPCA, 2008, pp. 215–225. [11] Y. Jin, K. H. Yum, and E. J. Kim, “Adaptive data compression for high-performance low-power on-chip networks,” in MICRO, 2008, pp. 354–363. [12] P. Zhou et al., “Frequent value compression in packet-based NoC architectures,” in ASPDAC, 2009, pp. 13–18. [13] B. M. Beckmann and D. A. Wood, “Managing wire delay in large chipmultiprocessor caches,” in MICRO, 2004, pp. 319–330. J. Huh et al., “A NUCA substrate for ﬂexible CMP cache sharing,” in ICS, 2005, pp. 31–40. J. L. Henning, “SPEC CPU2006 benchmark descriptions,” ACM SIGARCH Computer Architecture News, vol. 34, no. 4, pp. 1–17, 2006. [16] N. Binkert et al., “The gem5 simulator,” ACM SIGARCH Computer Architecture News, vol. 39, no. 2, pp. 1–7, 2011. [17] M. M. Martin et al., “Multifacet’s general execution-driven multiprocessor simulator (GEMS) toolset,” ACM SIGARCH Computer Architecture News, vol. 33, no. 4, pp. 92–99, 2005. [18] N. Agarwal, T. Krishna, L.-S. Peh, and N. K. Jha, “GARNET: A detailed on-chip network model inside a full-system simulator,” in ISPASS, 2009, pp. 33–42. [19] C. Sun et al., “DSENT-a tool connecting emerging photonics with electronics for opto-electronic networks-on-chip modeling,” in NOCS, 2012, pp. 201–210. [14] [15] D. Overhead Estimation We veriﬁed our design by implementing the modules in behavioral Verilog, creating a testbench, and simulating using Mentor Graphics Modelsim. We further investigate the overheads in terms of power, area, and critical path by synthesizing our Verilog using the Synopsys Design Compiler. We choose a 45nm technology node using the worst-case low-power library. The results of individual compression and decompression module pairs are shown in Fig.10. The average dynamic power for one pair of compression and decompression units is just over 1mW. Leakage power is negligibly low, adding only an additional 9.6 μW on average. In contrast, the total power for a single router based on the DSENT power model ranges from 55mW—144mW depending on benchmark. Therefore, a single compression and decompression unit pair consumes 1.81%—0.69% of the power of a router. ) W μ ( r e w o P c i m a n y D 1400  1200  1000  800  600  400  200  0  Compression  Decompression  B16Δ8  B16Δ4  B16Δ2  B16Δ1  B8Δ4  B8Δ2  B8Δ1  B4Δ2  B4Δ1  Fig. 10: Dynamic power breakdown for the compression/decompression modules From the power results we can also observe a trend in the dynamic power for compression and decompression modules. Compression and decompression module power increases as the base decreases. This is due to additional adders needed to calculate the Δ values. The power also increases as the delta shrinks due to larger fan-in AND and OR gates needed for the sign extended check logic. Conversely, the dynamic power for the decompression module decreases as the delta shrinks, since the subtraction units in the decompression module have input sizes of Δ bits. Based on this observation of the decompression power, we can break ties when multiple compressions are possible. Speciﬁcally, we prefer smaller Δ values and larger bases. This decision is reﬂected by the priorities in Table II in Section III-C. NoΔ has modest hardware overhead and implementation complexity because the compression and decompression algorithms involve only simple vector addition, subtraction, and comparison operations. The average area of a compression and decompression module pair is 0.006 mm2 . Based on the area of the network interface and router, this area is relatively small. Furthermore, we analyze the impact of these modules on the critical path. We successfully synthesized our design with a clock rate of 1 GHz, meaning the design can compress a packet in a single network clock cycle in our simulated system. This additional latency is accounted for during simulation by adding extra queuing latency to packets. Finally, we compare the power/area overhead of NoΔ with ZeroCmpr and FreqCmpr under 45nm technology, as shown in Table IV. For frequent-value compression, we consider one design from [12] with adaptive compression (FreqCmpr a) and another two designs from "
2015,A fast and accurate network-on-chip timing simulator with a flit propagation model.,"Network-on-chip (NoC) can be a simulation bottleneck in a many-core system. Traditional cycle-accurate NoC simulators need a long simulation time, as they synchronize all components (routers and FIFOs) every cycle to guarantee the exact behaviors. Also, a NoC simulation does not benefit from transaction-level modeling (TLM) in speed without any accuracy loss, because the transaction timings of a simulated packet depend on other packets due to wormhole switching. In this paper, we propose a novel NoC simulation method which can calculate cycle-accurate timings with wormhole switching. Instead of updating states of routers and FIFOs cycle-by-cycle, we use a pre-built model to calculate a flit's exact times at ports of routers in a NoC. The results of the proposed simulator are verified with NoC implementations (cycle-accurate at RTL) created by a commercial NoC compiler. All timing results match perfectly with packet waveforms generated by above NoCs (with 40-325 times speed up). As another comparison, the speed of the simulator is similar or faster (0.5-23X) than a TG2 NoC model, which is a SystemC and transaction-level model without timing accuracy (due to ignoring wormhole traffics).","A Fast and Accurate Network-on-chip Timing Simulator with a Flit Propagation Model Ting-Shuo Hsu† , Jun-Lin Chiu, Chao-Kai Yu, and Jing-Jia Liou∗ Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan 30013 e-mail: † tingshuo.hsu@gmail.com and ∗ jjliou@ee.nthu.edu.tw Abstract— Network-on-chip (NoC) can be a simulation bottleneck in a many-core system. Traditional cycle-accurate NoC simulators need a long simulation time, as they synchronize all components (routers and FIFOs) every cycle to guarantee the exact behaviors. Also, a NoC simulation does not beneﬁt from transactionlevel modeling (TLM) in speed without any accuracy loss, because the transaction timings of a simulated packet depend on other packets due to wormhole switching. In this paper, we propose a novel NoC simulation method which can calculate cycle-accurate timings with wormhole switching. Instead of updating states of routers and FIFOs cycle-bycycle, we use a pre-built model to calculate a ﬂit’s exact times at ports of routers in a NoC. The results of the proposed simulator are veriﬁed with NoC implementations (cycle-accurate at RTL) created by a commercial NoC compiler. All timing results match perfectly with packet waveforms generated by above NoCs (with 40–325 times speed up). As another comparison, the speed of the simulator is similar or faster (0.5-23X) than a TG2 NoC model, which is a SystemC and transaction-level model without timing accuracy (due to ignoring wormhole trafﬁcs). I . IN TRODUC T ION As a thriving on-chip interconnection architecture, Networkon-chip (NoC) has been studied deeply in past decades and improved by many subtle designs. The design space can contain multiple variables, such as topology, routing algorithm, switching technique, ﬂow control mechanism, and router microarchitecture. Besides the router architecture parameters, other parameters such as channel schemes and buffer size can affect the system performance. Since NoC has such a huge design space and signiﬁcant impacts on performance and cost, an efﬁcient simulator to evaluate architecture and determine the optimized solution is crucial in modern system-level design. Many NoC tools have been proposed for system evaluation in [1] and [2]. In [1], NoC simulators are classiﬁed into regular network simulators [3, 4, 5, 6] dedicated NoC simulators [7, 8, 9, 10, 11, 12, 13, 14], and full-system simulators [15]. Researchers adopt regular network simulators for NoC to avoid tedious work of building infrastructure of simulator. However, many building blocks and parameters of a regular simulator are different from that of NoC. And also the protocol of computer network is general and unnecessarily complex than that of NoC, so some computation efforts might be wasted. While we can create a NoC prototype quickly from a network simulator, the accuracy and speed do not match the later two types. The full-system simulation is usually involved with another simulation kernel, but the general mechanism is similar. All these dedicated NoC simulators differs in implementation languages, trafﬁc models, and some supported architecture parameters including topologies, routing algorithms and switching technology [1]. As any architecture simulation, abstraction level (languages) and implementation features balance the trade off of accuracy and speed. If more architecture features are implemented in detailed designs, better accuracy is obtained at the sacriﬁce of simulation speed. For example, at register transfer level (RTL), we can expect exact timing for all signals. Yet, it will not be suitable for evaluating system performance or investigating hot spots. As another example, many NoC simulators use C/C++, Java or SystemC to emulate the hardware behavior with cycle-by-cycle synchronization (cycle-accurate). Unfortunately, the improvement over RTL is limited. So, techniques like parallel simulation [14] or hardware-acceleration [16] are then developed to speed up simulation. To avoid cycle-based simulation, transaction level modeling (TLM) is proposed to approximate the timings of a NoC. Aimed at trading off accuracy for speed, TLM can be coded in either approximate timed or loosely timed models. Transaction Generator 2 (TG2) [17] is one such implementation, which is about 81 times faster than the RTL version of a 8x8 mesh. The speed up is obtained by simplifying communication behavior and synchronizing only at the starts and the ends of communication transactions (or phases) instead of every clock cycles. Therefore, certain NoC features such as wormhole switching may be ignored. And the errors of timing may be intolerable. In observing above gap of accuracy and speed, we propose a new timing simulator for NoC. The simulator is based on a ﬂit propagation model that calculate ﬂit timings at input and output ports of FIFOs and switches. The calculation is performed with pre-characterized timing equations for available features of a NoC including wormhole switching, pipelining, input/output buffers, ﬂow controls, etc. In experiments, the proposed simulator can be as fast as a TLM model with the same accuracy of a RTL model. The rest of the paper is organized as follows. We give a brief introduction to generic NoC architecture in Section II. In Section III, we present our proposed simulation ﬂow. And two ﬂit propagation models for different router microarchitectures are detailed in Section IV. We show the veriﬁcation results and simulation speed comparisons in Section V and summarize this paper in Section VI. as they arrive in a router. We mark the header arrival time to each router on Fig. 1, where the channel is assumed to be a synchronous FIFO. The trafﬁc congestion is illustrated when packet 1 wishes (but fails) to enter the router 1 at 4 ns, since the router 1 is occupied by packet 0 and can be released at 6 ns when the tail of packet 0 passes through. I I I . PRO PO SED NOC T IM ING S IMU LAT ION F LOW Our NoC timing simulation ﬂow can be divided into four main steps, as Fig 3 shows. First we compute the routing path of injected packets, and secondly arbitrate ownership of channels which is required by multiple packets. After the ﬁrst two steps, all packets are clustered into two groups: one for packets which obtain full ownership of their routing paths and another for packets which are blocked by other packets. We calculate the detailed ﬂit timing of packets in the ﬁrst group (step 3), The done packets will release the occupied channels (step 4) so that more packets take the full path ownership. We can repeat step 2–4 to gradually complete timing computation of all packets. And the details of each step are described as follows. Fig. 1. A 2D mesh NoC. Fig. 2. The hierarchical structure of NoC message. I I . NOC ARCH I T EC TUR E In this section, we brieﬂy introduce the generic NoC architecture and terminology. More comprehensive details of NoC architecture can be referred to [18]. A NoC consists of network interfaces (NI), routers, and channels. The network interfaces connect and perform protocol translation between other IPs and a NoC. And routers are interconnected with each other through channels to form the network infrastructure, as the example NoC shown in Fig. 1. The important design features of NoC include topology, routing algorithm, ﬂow control and router microarchitecture. And channels can be implemented by wires, synchronous FIFOs, or even asynchronous interfaces. Fig. 1 shows that two packets are injected into the NoC at 1 ns and 2 ns respectively. Packets are routed from their source endpoints to their destinations. Fig. 2 shows the structure of a NoC message. A message will be broken into packets to route through NoC. Each packet consists of ﬁxed-length ﬂow control digits (ﬂits), starting with a header ﬂit and ending with a tail ﬂit. A ﬂit is the basic unit for network ﬂow control, and typically with the same size of a physical digit (phit) in the NoC. In the wormhole switching, routers forward a packet as soon as its header arrives. And the successive ﬂits follow the header Fig. 3. The NoC timing simulation ﬂow. Step 1: After the NoC timing simulator load all packets, the ﬁrst step is to determine the routing path for each packet according to the modeled NoC topology and routing algorithm. For each packet, we record the position of each input port and output port on its routing path. Step 2: Conﬂicts may occur among some routing paths determined in step 1. In step 2, we calculate the time when a header ﬂit arrives each routers on it routing paths (detailed in Section IV). If two packet headers need to pass through the same output port of the same router, we arbitrate which header wins the ownership of this output port according to the competitive headers’ arrival time and the modeled arbitration policy. The ownership will be hold in this calculation round until the tail ﬂit leaves and releases the occupied output port (step 4). Step 3: After arbitration, we pick the packets which own all output ports on their routing paths to perform ﬂit timing calculation. All the time points when they arrive in and depart from the passed routers of these packets are solved in this step. Flit timing calculation will be detailed in Section IV. Step 4: According to the tail departure time calculated in step 3, we know when those occupied output ports will be released. In step 4, we update the release times of those output ports, so other blocked packets’ forward propagation timings can be calculated in next round. We will repeat step 2–4 until computations of all packets are done. We use Fig. 1 as an example to illustrate above process. After routing paths of packet 0 and 1 are determined, we found that there is a conﬂict at router 1. Here header of packet 0 will advance to the output of router 1 since it arrives earlier. So in the ﬁrst round of calculation, ﬂit arrival times of packet 0 can be calculated. And the release time of router 1 is updated as 5 ns at router 4. Then in the second round, the ﬂit arrival times of packet 1 can be determined based on the updated router release time of packet 0. Please note that the above example can only work for the implementation in which all simulated packets are injected at one time. If we iteratively inject packets in batches, errors may occur when the newly-injected packets violate previous timing calculations. For example, in Fig. 1, if a new packet is injected at 7 ns, from router 4 to router 7, header of packet 1 is blocked and its ﬂit arrival times are postponed. Therefore, we will have to check all previous packets for conﬂicts when injecting new batch of packets. This will lead to signiﬁcant slow down when number of packets increases. This problem can be solved by keeping a time window for each calculated packet. The time window starts with the packet injection time and end with the expected packet exit time (according to currently injected packets). And we keep a packet on pending status until the simulation time exceeds its window. For the above example, the window end time of packet 0 is 5 ns, and 10 ns for packet 1. So when the new packet is injected at 7 ns, we only examine the packet 1 since the new injection doesn’t change exit time of packet 0. For all pending packets, we check and recompute their timings with new packets if necessary, and the window end time might be extended if the packet is blocked by the newly-injected packets. IV. F L I T PRO PAGAT ION MODE L In the NoC timing simulation ﬂow described above, we need to develop ﬂit propagation models to calculate the accurate timings. The propagation model depends on the operations and architecture of routers and channels. In this section, we illustrate ﬂit propagation models for both single-stage and twostage pipelined routers, to demonstrate the general idea of our method. Based on the discussion, the model can be easily extended to different router microarchitectures. A. Problem formulation The objective of our ﬂit propagation model is to calculate the cycle-accurate packet timings. Given a packet with its injection time, we want to calculate the time when the tail ﬂit of the packet exit the NoC. Since we have determined the routing path at step 2 in Section III, we can label the passing routers on the routing path as stage[0], stage[1], ..., stage[m], ..., to represent the traversal order for routers on path, as shown in Fig. 4. Also, the ﬂits are labeled as f lit[0], f lit[1], ..., f lit[n], etc. Then, we deﬁne the ﬂit arrival time for a f lit[n] at a router stage[m] as the time when the ﬂit is sampled by the last-level pipeline register in this router, and denote it as tstage[m] f lit[n] . The packet exit time is when the tail ﬂit leaves last-stage router. Fig. 4. Wormhole switching on a routing path. B. Channel and ﬂow control timing parameters In our ﬂit propagation model, we deﬁne three timing parameters of the channel and ﬂow control schemes. (1) The buffer capacity (in ﬂits) of a FIFO channel is denoted as buf f er . (2) The channel latency is denoted as ∆tF I F O , which means the latency for a ﬂit to pass through a channel. Fig. 5 illustrates the concept of ∆tF I F O . Assume that f lit[n] is located at the output port of the upstream router at the time = t0 , it can enter the downstream router at the time = t0 + ∆tF I F O at the earliest. (3) The ﬂow control latency is denoted as ∆tF C , which means the latency for a ﬂow control signal to propagate backward to the upstream routers. Fig. 6 presents the concept of ∆tF C . Assume that f lit[0] is blocked at the router 1 due to trafﬁc congestion, and ﬂow control mechanisms stop data transmission to prevent data loss. After congestion is released, the downstream router may need a period of time to inform the upstream router to restart transmission. Fig. 5. The channel latency ∆tF I F O Fig. 6. The ﬂow control latency ∆tF C C. Single-stage router We start our ﬂit propagation model with a 1-stage router, which is the simplest router architecture in modeling. Also, the virtual channels are not employed in our example. The architecture of our target 1-stage router is shown in Fig. 7. The only set of pipeline registers are located in the output controllers for each output port and named the forward pipeline registers (fwdPipe ). In the following paragraphs we will show how to calculate packet timings with this propagation model. Fig. 7. The 1-stage router architecture. C.1 Header arrival time for a 1-stage router First we use Equation (1) to calculate the header (f lit[0]) arrival time, which is denoted as tstage[m] f lit[0] for each router stage[m]. The Equation (1) considers two possible cases: congestion occurs or not. Congestion means that (the output port of) a downstream router has been occupied by another packet. Assume that output port release time is tstage[m] release , which is calculated and updated in the previous round of ﬂit timing calculation. The earliest possible time when the header can arrive in a router is the header arrival to the previous router with an additional latency (∆tF I F O ) to pass the channel. If this earliest possible time is earlier than the time the occupied router is released, congestion has happened, and the header arrival time will be the router release time with an additional cycle to sample the current header (Equation (1a)). Otherwise, the header arrival time is just the earliest possible time (Equation (1b)). Equation (1) Header arrival time for a 1-stage router  tstage[m] f lit[0] = tstage[m] release + clk (1a) if congestion happens tstage[m−1] f lit[0] + ∆tF I F O otherwise (1b) (Congestion happens if tstage[m−1] f lit[0] + ∆tF I F O ≤ tstage[m] release ) C.2 Flit arrival time for a 1-stage router After the header arrival time for each router is calculated, we further process arrival times of the rest ﬂits. We use Equations (2) to calculate the ﬂit arrival time tstage[m] f lit[n] , which must be the maximum between the following two expressions. Expression (2a) deﬁnes the earliest possible ﬂit arrival time (if no congestion occurs). Speciﬁcally, a ﬂit (f lit[n]) can enter a router (stage[m]) after the ﬂit arrives at the previous router (stage[m−1]), plus an additional latency (∆tF I F O ) to pass the channel. Figures 8 illustrates the concept of Expression (2a). Expression (2b) means that the ﬂit can enter a router only after the router is released by the previous packet (if congestion happens). Please note that the router release time depends not only on the previous packet, since the trafﬁc congestion will propagate backward to other routers from the congestion source due to wormhole switching. Figures 8 presents an example of synchronous FIFO channel with buffer capacity of two ﬂits. During congestion, f lit[n] is blocked before the router stage[m]. After congestion, when f lit[n − 3] is forwarded to stage[m + 1] and makes the channel 1 available for f lit[n − 1], this time is denoted as tstage[m+1] f lit[n−3] by deﬁnition. At the same time, channel 1 propagates backward a control ﬂow signal to inform stage[m] and channel 0 to forward ﬂits. So f lit[n] can enter the stage[m] after f lit[n − 3] enter the stage[m + 1] with a latency of ﬂow control signal. Equation (2) Flit arrival time for a 1-stage router tstage[m] f lit[n] = max( tstage[m−1] f lit[n] + ∆tF I F O , tstage[m+1] f lit[n−(buf f er+1)] + ∆tF C ) (2b) (2a) Fig. 8. Illustrate the Expression 2a. Fig. 9. Illustrate the Expression 2b. D. Two-stage router Deeper pipelines can break critical paths and increase operating frequency. A possible two-stage router is shown in Fig. 10. One additional level of pipeline registers, input pipeline registers (inputPipe ), is added. We use this example to show how the ﬂit propagation model changes for alternative router microarchitecture. D.1 Header arrival time for a 2-stage router Compared with the single-stage router, the only difference is that the earliest possible header arrival time calculation (Equation 3b). In addition to the previous-stage header arrival time (tstage[m−1] f lit[0] ) with channel passing latency (∆tF I F O ), an extra clock period is taken to pass the additional register. D.2 Flit arrival time for a 2-stage router Compared with the single-stage router, the difference is the additional clock period in Expression 4a, which is also from the The tool will produce a logic-synthesizable RTL and cycleaccurate pin-accurate SystemC model. Table I shows our NoC conﬁguration options in the experimental setup. As we discussed in Section III, topology, routing algorithm and arbitration can only affect the routing path but not ﬂit timing model. The switching technique and ﬂow control is not conﬁgurable in Arteris NoC. And we choose synchronous FIFOs with 2-ﬂit depth as communication channels. The router microarchitecture is also a critical factor of ﬂit timing model. In NoCcompiler, ﬁve optional pipeline registers can be added to a router. In this demonstration, we choose a two-stage router which is introduced in Section IV–D. Please note that the proposed method can be adapted to other router microarchitectures. TABLE I NoC parameters setting in the veriﬁed model 4 × 4 2D Mesh NoC topology Routing algorithm X-Y Routing Router arbitration policy First In First Out Switching technique Worm-hole Switching Flow control scheme On/Off-like Router pipelining 2-stage router Flit size 32 bits Channel type Synchronous FIFO Channel bandwidth 32 bits Channel FIFO depth 2 ﬂits To present the correctness of the proposed simulator, modeling features are added one by one to show that error rates decrease with each additional feature and ﬁnally match to the RTL golden reference. We have prepared three incomplete versions (denoted as V1–3) and a ﬁnal version (V4) which include all necessary features. V1 considers only the per-hop FIFO and router latency but not the trafﬁc contentions (similar to TG2). In V2, trafﬁc contentions are taken into account based on the header arrival times (Equation (1)). But V2 ignores timing behavior of other ﬂits, which is modeled with Equation (2) in V3. And ﬁnally V4 considers all pipeline features in a router which is detailed in Section IV–D . There are six test cases in our veriﬁcation (due to space limitation, we only show these representative cases). The ﬁrst three tests are synthesized trafﬁc patterns which we try to cover the corner cases of wormhole switching. The fourth test contains randomly-generated 1000 packets. The last two cases are traces from real NoC applications: Case 5 is collected from several message-passing API test programs. Case 6 is a parallel oddeven sorting program. Table II shows the error rates of different simulators, compared to an Arteris RTL NoC model. Comparing V2-4, both the average and maximal error rates gradually decrease (or at least hold constant), and timings of V4 perfectly matches to the RTL reference model. It’s interesting that in some cases the simplest hop latency model V1 outperforms V2 and V3 in maximal error rates. It might be that certain type of errors are masked and averaged out by multiple trafﬁc sources. Besides the four versions of our simulator, we also compare the result with TG2 [17]. TG2 features a fast transaction-level simulation interface, but produces considerable error rates since it does not consider wormhole switching. Fig. 10. The two-stage router architecture. Equation (3) Header arrival time for a 2-stage router  tstage[m] f lit[0] = tstage[m] release + clk (3a) if congestion happens tstage[m−1] f lit[0] + ∆tF I F O + clk otherwise (3b) (Congestion happens if tstage[m−1] f lit[0] + ∆tF I F O + clk ≤ tstage[m] release ) additional pipeline register (Fig. 11). Another difference is the constraint of forward buffered ﬂits (Expression 4b), since now the inputPipe can buffer one more ﬂit (Fig. 12). Equation (4) Flit arrival time for a 2-stage router tstage[m] f lit[n] = max( tstage[m−1] f lit[n] + ∆tF I F O + clk, tstage[m+1] f lit[n−(buf f er+2)] + ∆tF C ) (4b) (4a) Fig. 11. Illustrate the Expression 4a. Fig. 12. Illustrate the Expression 4b. V. EX PER IM EN TA L R E SU LT S A. Veriﬁcation with RTL Implementation To verify our NoC timing simulator, a RTL NoC implementation is used as a golden reference model. The RTL model is compiled by Arteris NoCcompiler [19]. With NoCcompiler, users can conﬁgure, instantiate, and connect with existing IPs (NIs, routers, and FIFOs, etc.) to build a customized NoC. Simulation error rate comparisons (relative to Arteris RTL NoC). TABLE II Test cases Case1 Case2 Case3 Case4 Case5 Case6 V1 V2 V3 V4 TG2 average max average max average max 9.30% 9.30% 9.30% 9.30% 9.30% 9.30% 38.87% 63.55% 3.25% 5.13% 3.25% 5.13% 76.75% 91.84% 1.22% 5.13% 1.22% 5.13% 11.91% 61.54% 8.74% 102.44% 8.47% 102.4% 24.10% 33.18% 22.00% 31.82% 22.00% 31.82% 54.54% 687.8% 24.27% 951.28% 5.01% 14.29% average max average max 0% 0% 9.30% 9.30% 0% 0% 34.30% 54.77% 0% 0% 63.17% 74.76% 0% 0% 11.64% 82.79% 0% 0% 164.87% 205.83% 0% 0% 29.46% 687.83% Simulation time of running each test cases 1000 times. TABLE III Test cases Case1 Case2 Case3 Case4 Case5 Case6 V1 V2 V3 V4 TG2 Speed-up (TG2/V4) 0.29 s 0.30 s 0.31 s 1.45 s 1.69 s 6.26 s 0.35 s 0.34 s 0.44 s 7.06 s 8.24 s 36.06 s 0.48 s 0.50 s 1.07 s 18.81 s 16.64 s 276.03 s 0.41 s 0.42 s 0.85 s 14.17 s 12.04 s 298.38 s 9.36 s 9.46 s 9.65 s 36.95 s 42.36 s 151.51 s 23.0 22.7 11.4 2.6 3.5 0.5 Arteris SystemC 25.64 s 27.25 s 48.88 s 2241.85 s 3911.73 s 12064.34 s Speed-up (Arteris/V4) 63.0 65.3 57.7 158.2 324.9 40.4 B. Simulation Speed Besides correctness, simulation time is also concerned by tool users. Table III shows simulation times taken by different simulators with the six test cases. We repeat each test cases for 1000 times to measure the accumulated CPU time, and compare the results of the four versions of our simulator (V1-4), TG2, and Arteris SystemC model. The Arteris SystemC model (also created by NoCcompiler) is a typical cycle-based simulator, which can achieve about several times speed up than a RTL version. Among our simulators, the simulation time generally grows when more simulation features are added (some small glitches between V3 and V4 may be due to system loading variation). Compared with Arteris SystemC model, our simulator (V4) can achieve 40-325 times speed up. To compare with TG2, our simulator (V4) run 0.5-23 times faster. Therefore, we can expect the proposed simulator to be in the same order of efﬁciency as a regular TLM model (TG2). We are also investigating Case 6 traces for the reason of speed gap. V I . SUMMARY AND CONC LU S ION S In this paper, we have developed a ﬂit propagation model to accurately capture the timings of ﬂits at all components of a NoC. The model considers details of FIFO latency and microarchitectures of a router to construct equations for exact arrival times. We then plug in the developed model in a simulation ﬂow which handles trafﬁcs of packets in the wormhole switching. In experiments, our simulator achieves the same accurate timing as a RTL model while maintaining faster or similar speeds at transactional level. ACKNOW L EDG EM EN T S This work was supported in part by Ministry of Science and Technology, Taiwan, under Contract MOST 103-2220-E-007011 and by Industrial Technology Research Institute of Taiwan. "
2015,A novel approach using a minimum cost maximum flow algorithm for fault-tolerant topology reconfiguration in NoC architectures.,"An approach using a minimum cost maximum flow algorithm is proposed for fault-tolerant topology reconfiguration in a Network-on-Chip system. Topology reconfiguration is converted into a network flow problem by constructing a directed graph with capacity constraints. A cost factor is considered to differentiate between processing elements. This approach maximizes the use of spare cores to repair faulty systems, with minimal impact on area, throughput and delay. It also provides a transparent virtual topology to alleviate the burden for operating systems.","A Novel Approach Using a Minimum Cost Maximum Flow Algorithm for  Fault-Tolerant Topology Reconfiguration in NoC Architectures  Leibo Liu, Yu Ren, *Chenchen Deng,  Shouyi Yin, Shaojun Wei  Institute of Microelectronics  Tsinghua University  Beijing 100084, China  Email: chenchendeng@tsinghua.edu.cn  Jie Han  Department of Electrical and  Computer Engineering  University of Alberta  Edmonton, AB, Canada T6G 2V4  Abstract - An approach using a minimum cost maximum flow  algorithm is proposed for fault-tolerant topology reconfiguration  in a Network-on-Chip system. Topology reconfiguration is  converted into a network flow problem by constructing a  directed graph with capacity constraints. A cost factor is  considered to differentiate between processing elements. This  approach maximizes the use of spare cores to repair faulty  systems, with minimal impact on area, throughput and delay. It  also provides a transparent virtual topology to alleviate the  burden for operating systems.  I Introduction  The advance in VLSI manufacturing technology has made  it possible to integrate thousands of processing elements (PEs)  on a single chip. In terms of communication infrastructure,  Network-on-Chip (NoC)  is considered as a promising  interconnect scheme for manycore processors [1]. With the  increasing circuit density, the reliability of a manycore system  has become one of the most important challenges. Many  solutions have been proposed to sustain the reliability of a  system,  including remapping [2], fault  tolerant routing  algorithms [3] and various topologies for implementing the  communication  infrastructure  [4].  Improving  the  manufacturing process can help to increase the reliability, but  this approach will become increasingly difficult in the future.  A more practical solution is to provide redundant hardware to  construct a fault-free system [5].   A reconfigurable system usually has many free resources.  Due to its flexibility, these redundant resources can be utilized  for improving reliability. In this paper, redundancies at the  core level are considered, i.e. faulty PEs are replaced by spare  ones. The concept of virtual topology [6] is also introduced  because different chips may have different topologies and a  faulty PE may change the underlying topology. A virtual  topology is isomorphic to the topology of the target design.  Topology  reconfiguration  is  implemented by mapping  between the virtual topology and the physical topology. With  limited resources on a chip, an important question concerning  topology reconfiguration is how to improve reliability by  using spare resources with the least overhead.   In this paper, a novel approach using a minimum cost  maximum flow (MCMF) algorithm [7] in graph theory is  proposed for run-time topology reconfiguration. This method  repairs faults in PEs and to improve the reliability of an  NoC-based reconfigurable architecture at the cost of a minor  performance reduction, compared to a fault-free system. The  proposed approach successfully converts  the  topology  reconfiguration problem into a network flow problem by  constructing a directed graph based on the topology. A cost  metric is introduced to model the overhead difference between  PEs. Simulation results show that the success rate to repair all  faulty PEs is increased by up to 40% compared with previous  approaches [13] using the same redundant resources. The  latency is 3.5% smaller and throughput is 4.7% higher than  previous approaches [12]. Besides, the proposed approach has  a polynomial computation time [7], which is suitable for  run-time reconfiguration.  II. Design Consideration and Related Work  A. Design Consideration  Given a set of reliable and defective PEs, an objective is to  obtain a system with the same functionality as the original one.  The design consideration is how to improve the repair rate as  much as possible with minimal impacts on the operational  overhead,  including  the  repair  rate,  the  increase  in  reconfiguration time, the change of topology, and the increase  in area, throughput and latency. As area, throughput and  latency are common metrics for evaluating an NoC, they are  not discussed in detail here. Three additional evaluation  metrics including the repair rate, reconfiguration time and  topology are introduced as follows.  Repair rate  is an  important metric  to evaluate  the  effectiveness of a repair approach. It is defined as the  probability that the faulty PEs in a topology can be  successfully repaired by the spare ones. Different repair  strategies  result  in different  repair  rates. Under  the  circumstances that the hardware resources on a chip are  limited, more options are offered to each defective PE in this  paper so that they can be efficiently repaired.  The reconfiguration time determines whether an approach  can be performed at run-time. It depends on the required  computation of the repair algorithm. If faults are detected at  run-time and  repaired by  reconfiguration,  the overall  performance of the entire system will be improved. On the  other hand, when chips are produced and tested massively,  reconfiguration time is also an important parameter, because it  is closely related to the cost of a chip. Therefore, a faster  reconfiguration approach is preferred.  During configuration,  topology  is also one of  the              considerations. When the faulty cores are replaced by spare  ones, the topology of the target design may become irregular  and would cause performance degradation due to the lack of  prior knowledge of the faulty cores. For example, Fig. 1 (a)  shows a processor with 4×4 2D mesh topology. Suppose 4  spare cores are provided as shown in Fig. 1 (b). When faulty  cores are present, as shown in Fig. 1 (c) and (d), different  chips may have different topologies, and the topologies also  may not be the same as expected. It will be a big burden for  the operating system (OS) to optimize parallel programs on  different topologies. To address this problem, a unified virtual  topology is introduced. Topology is defined as the  topology of the target design, e.g. Fig. 1 (a). Fig. 1 (d) shows a  topology with four spare cores. Physical Topology is the  topology of fault-free cores and their interconnections, as  shown in Fig. 1 (e). A fault-free 4×4 processor can still be  obtained. Its topology is different but isomorphic to the  reference topology. In a reconstructed chip, each core is  considered to be virtually connected to its neighbors. Virtual  Topology is defined as the reconstructed topology. Fig. 1 (f) is  an example of a virtual 4×4 2D mesh topology. The 9th, 12th,  15th and 19th PEs are four virtual neighbors of the 13th PE.  The 9th PE is considered to be virtually located under the 8th  PE, although they are physically located side by side. A virtual  topology appears as unified for the OS and other programs  regardless of the underlying physical topology.  B. Related Work  There are many ways to implement the mapping between  the virtual and physical topologies. One possible solution is to  add a firmware layer to record the mapping information,  similar  to  the CORE_AVAILABLE_REG  used  in  UltraSPARC T1 processor [8]. OS works on the virtual  topology, and the firmware is responsible for transformation.  The idea of virtual topology is also applied in Cray T3E  network [9]. The mapping from physical to virtual numbers is  implemented by changing the routing table in each node and  logically renaming the logical “who am I” register.   Faults can be divided into two main categories: permanent  faults and transient ones. Permanent faults are usually caused  by manufacturing defects, aging effects, and/or physical  damages to the resources that generate or transport data. One  approach to dealing with permanent faults is fault tolerant  routing, which involves isolating the entire router [10] or a  few ports of a router [11]. Another method for tolerating  permanent faults is to use spare components to replace  defective elements [12]. Transient faults are usually caused by  neutron and alpha particles, power supply and interconnect  noise, electromagnetic inference and electrostatic discharge.  Error detecting/correcting codes are pervasively used to  handle these errors. In this paper, only permanent faults are  considered. Faults can occur at links, network interface, router  and processing element levels. A VLSI processor integrates a  large number of PEs on a single chip. As the size of a system  increases and the cost of a single PE becomes relatively  inexpensive compared with the entire system, PE-level  redundancy is considered efficient. In this paper, only faults in  PEs are considered while the communication infrastructure is  assumed to be fault-free.   In [12], for an N×N NoC system, a spare row of routers is  added. Every router within each column shares the common  Fig. 1. (a) The expected target design. (b) The implementation on a  chip. (c) and (d) A chip with faulty PEs. (e) The physical topology. (f)  A virtual topology.  spare router. However, if a column contains more than one  fault, spares in other columns cannot be utilized to repair the  faults which renders a low repair rate. In [13], a repair strategy  using two spares in one group is presented. It can tolerate two  failures within one group. These  two approaches are  straightforward to implement. However, there is room for  improvement in the repair rate. In [14], a novel repair  technique is proposed to improve the yield of through-silicon  vias (TSVs). This technique enables faulty TSVs to be  repaired by redundant TSVs that are far apart. An NoC is used  as the communication infrastructure, so the repair of TSVs is  similar to the problem of repairing faulty PEs. Thus this  approach is applicable to a manycore system.  III. Proposed Topology Reconfiguration Approach  In this section, the reconfiguration from physical to virtual  topology is first introduced. Then reconfiguration algorithm is  detailed. Besides, the overhead of reconfiguration time,  throughput and latency is analyzed.  A. Topology Reconfiguration  Faulty PEs change the target design and the topology is  reconfigured by mapping from various physical topologies to  a unified virtual topology. Each router has a look-up table and  two registers for storing its physical and virtual numbers. The  index into the look-up table is the virtual address, while the  entry in the table is the physical address. The look-up table  provides a mapping from the physical topology to a virtual  topology. The failure information can be obtained through  various testing strategies. No matter what kind of testing  approach is adopted, when faults are captured, the failure  information is sent to the controller. The controller calculates  the virtual topology using the proposed algorithms. Then the  look-up table together with the virtual address register is  logically renamed. OS works on the virtual topology. Fig. 2  shows an example of a virtual topology and the mapping table.        A packet sent from the virtual address #II to #XVI is actually  sent from the physical address 3 to 20. If XY routing is used  with X-axis first, the routing of the packet will be 2 hops to  the right and 4 hops downward.  B. Minimum Cost Maximum Flow (MCMF) Approach  A non-spare PE at location (x, y) is assumed to be faulty. In  a valid repair solution, it is logically replaced by a healthy PE  at location (x’, y’). To be more specific, the PE at location (x’,  y’) will be re-indexed as (x, y) in the reconfigured mesh. The  PE at location (x’, y’) will then be replaced by a healthy PE at  location (x’’, y’’) until the replacement ends at a spare PE. The  ordered sequence of nodes (x, y), (x’, y’), (x’’, y’’)… involved  in the replacement chain is defined as a repair path. It is a  sequence of substitutions that logically replaces a faulty PE  using a spare one. A general methodology to reconfigure a  mesh with faulty PEs is equivalent to determining the repair  paths. The repair paths determine the neighbors of each PE,  and then a virtual topology is obtained. Fig. 3 shows an  example to illustrate the concept of a repair path and the  reconfigured virtual topology. The repair path is a virtual path  indicating the replacement of PEs, and it does not physically  exist. In contrast, the routing path is physically implemented  by the NoC, and it is determined by the source and destination  addresses.   If faulty PEs are detected, a repair path will start from a  faulty PE and end at a spare one. Each PE along the repair  path must be physically next to each other because PEs are  assumed to be replaced by physical neighbors. If multiple  repair paths are present, intersections are not allowed. Because  each PE can only be mapped to one index in the virtual  topology, an intersection means that the PE is mapped to two  locations. In summary, the set of repair paths must meet the  following requirements.  1) Each repair path is continuous;  2) The set of repair paths covers all faulty non-spare PEs;   3) There is no intersection between any repair paths.  Next, a repair algorithm referred to as MCMF is proposed  to analyze whether a mesh is repairable and how to generate a  repair path set. The problem of determining a set of  non-intersecting continuous repair paths can be converted  Mapping Table Physical address 1 3 4 5 6 12 9 10 Virtual  address #IX #X #XI #XII #XIII #XIV #XV #XVI Virtual  address #I #II #III #IV #V #VI #VII #VIII Physical  address 11 13 14 15 16 17 18 20 Fig. 2. Example of a virtual topology and the mapping table.  Fig. 3. (a) A 4×4 mesh with repair paths. (b) The reconfigured virtual  topology.  into an MCMF problem. It is a classical combinatorial  optimization problem, i.e., how to find the maximum flow  between a source and a target in a network with capacity  constraints (on nodes and edges). The relationship between  repair paths and the MCMF is stated as follows (see Fig. 4).   Consider the mesh as a directed graph. Each continuous  repair path can be seen as a unit flow starting from a faulty PE  and ending at a spare PE. The grid then becomes a  multi-source multi-target network. A unit capacity “1” on each  edge and node ensures that an edge or a node can only be  utilized once in the repair paths. By adding a super source  node that points to all the faulty PEs and merging all the spare  PEs into a target node, the grid is converted into a  single-source single-target network. Since each repair path is  defined by a unit flow from a source to a target in the network,  the weight of the maximum flow is equal to the number of  faulty PEs that can be repaired by spare PEs. When all the  faulty PEs find their repair paths, i.e., all the faults can be  repaired, the weight of the maximum flow is equal to the  number of faulty PEs.  In a practical application, PEs differ from each other.  Replacing a PE with another one will cause changes in the  system, so PEs should not be treated equally. As a result,  another variable, cost, is introduced to model these differences.  Cost is used to describe the overhead of replacing a PE with  another one. It can be any metric to model the differences of  the network. For example, in a network with high throughput,  edge delay  is critical  to guarantee  the quality of  communication, so cost is defined as the edge delay. In an area  sensitive chip, cost is defined as the hardware consumption.  Cost can be defined on an edge or a node, according to the  problem requirement. As an example in this paper, the amount  of data transmission on each PE is taken as cost. An H. 264  video-decoding application  [15]  is chosen  to be  the  benchmark. The H. 264 decoding algorithm is computation-  intensive. Subtasks are partitioned clearly, and they work  independently of each other. So they can be mapped onto  different PEs and be executed in parallel. Besides, the various  amounts of computation and communication in each subtask  meet the verification requirements of the proposed approach.  H. 264 decoding algorithm is so widely used that resource  codes and verification data can be found easily, so it is used as  benchmark in this paper. Many other applications can be used  as benchmarks, too. In the example of H. 264 video-decoding  algorithm, cost is defined as the volume of transmitted data on  each PE, because communication cost is an important factor in  this application. An H.264 decoder is mainly composed of  Inverse Discrete Cosine Transform – Inverse Quantization      (IDCT-IQ), Motion Compensation (MC) and Deblocking  blocks. The distribution of data transmission is calculated, as  shown in Table I. Each block is randomly mapped onto one  PE. Fig. 5 (a) shows an example of the weighted graph with  three faulty PEs.  Now,  the NoC system  topology  reconfiguration  is  converted into a minimum cost maximum flow problem in  graph theory. The MCMF algorithm is performed in achieving  the maximum flow of the directed graph while simultaneously  minimizing the cost under predetermined cost constraints. A  mesh is represented as a directed graph G (V, E), where V is  the set of nodes in the mesh, and E is the set of edges between  nodes. F is the set of faulty nodes. Each node represents a PE  and  its corresponding router, while  the directed edge  connecting two nodes is the wire between two routers. Each  edge and each node has a unit capacity. This is described  mathematically as follows.  1. The set of nodes is defined as  VV ' ∪= is the source node, and T is the target node.   2. The set of edges E is defined as follows:  1) For every pair of nodes (i, j) that are adjacent in the grid,  i → and  j → ;   define two edges  ∈ , define an edge  2) For every spare node 3) For every faulty node ∈ , define an edge  3. Define the capacity of every edge to be 1.  4. Define the capacity of every node to be 1.  5. Solve the minimum cost maximum flow problem for the  graph constructed above.  Vvs Fvf Tvs fVS →   , where S  →  },{ TS j i TABLE I  Proportion of data transmission of each block in an H.264 decoder  IDCT-IQ  MC  Deblocking  Others  52%  21%  17%  10%  (a)                               (b)  Fig. 4. MCMF algorithm for determining the repair paths. (a)  Multiple-source multiple-target  network.  (b) Single-source  single-target network with flow and repair path.  (a)                               (b)  Fig. 5. (a) An example for H. 264 video-decoding application.   (b) Reconfigured virtual topology.  A solution to the above problem will return the maximum  flow of the constructed graph, as well as individual flows. The  maximum flow indicates how many faulty PEs can be repaired,  and every flow represents a repair path. According to the  repair path set, a virtual topology can be obtained, as shown in  Fig. 5 (b). If the maximum flow is not equal to the number of  faulty PEs, some faults are not repaired.  C. Reconfiguration Time Analysis  The feature of the maximum flow between source and  target ensures that the faulty PEs are replaced by spare PEs as  much as possible, therefore improving the reliability of the  network. The MCMF problem has polynomial-time solutions,  which runs in O(ElogV(E+VlogV)) [7], where E is the number  of edges and V is the number of nodes. If cost is not  considered, i.e., every PE is considered to be identical, then  MCMF is degraded to a maximum flow (MF) problem. In this  way, the proposed algorithm has polynomial-time solutions.  Apart from the execution of the algorithm, it will also take  some  time  to reconfigure  the  look-up  tables and  the  virtual-address registers. Taking all the reconfiguration bits  into consideration, the refresh time is less than 3% of the  algorithm execution time. Furthermore, in practical situations  not all the routers and all the values in the look-up tables in  one router need refreshing. Only partial reconfiguration is  needed.   D. Throughput and Latency Overhead Analysis  From the viewpoint of the NoC, it is necessary to model the  performance degradation of different virtual topologies. A  metric named Distance Factor (DF) is introduced in [16]. It is  used to describe the average hop count between virtual  neighbors, so it reflects the average delay and throughput of a  network. The distance factor between two nodes m and n is  defined as the physical hops between them (DFmn=Hopsmn).  The distance factor of node n (DFn) is defined as the average  distance factor between node n and all its virtual neighbors.  = 1 DF DF (1)   k The DF of a topology is defined as the average DFn of all  its N nodes in the topology.  1 = N nDF ∑ DF ∑ (2)  mn = 1 N n It is clear that the reference topology has the minimum DF.  DF=1 in a mesh, which means that each pair of virtual  neighbors is exactly one hop away from each other. Smaller  DF indicates shorter communication delay among virtual  neighbors.  Next, experiments are performed to evaluate the DF. Two  repair schemes are used as baseline solutions for comparison,  i.e., “N:1” [12] and “N:2” [13]. The N:1 scheme has one  column of spare PEs on the right border. If there is one  defective PE in a row, shifting is conducted to repair it with  the spare one. This scheme can tolerate one failure in each  row. The N:2 scheme has two spare PE columns, one on the  left and one on the right border. This scheme can tolerate at  most two failures in a row. For a fair comparison, MCMF is  considered to have the same spare resources as the baseline  schemes. In the comparison between MCMF and N:1, both  approaches are conducted on a 4×5 mesh. The MF approach is  k m n = 1         0 0 .02 0 .04 0 .06 Fai l ure Rate (a) 0.08 0 .1 0 .95 1 1 .05 1 .1 1 .15 1 .2 1 .25 D i t s n a e c F c a t r o Mesh 4*5 N:1 MF MCMF 0 0 .02 0 .04 0 .06 Fai l ure Rate (b) 0.08 0 .1 0 .95 1 1 .05 1.1 1 .15 1.2 1 .25 D i t s n a e c F c a t r o Mesh 4*6 N:2 MF MCMF Fig. 6. DF comparison between MCMF, MF, N:1 and N:2  also performed for comparison, in which the data transmission  of each PE is the same. These two approaches are also  compared with N:2 on 4×6 mesh. For each topology, 10,000  different fault patterns are considered, with PE failure rate  ranging from 1% to 10%. DF is calculated only when all the  faulty patterns in the network could be repaired. Fig. 6 shows  the DF comparison results. As shown in the figure, DF  increases with the increase of failure rate. In Fig. 6 (a), the N:1  scheme has the same DF as MF. This is because under the  circumstances that all the test fault patterns can be repaired by  both approaches, the fault patterns that can be repaired by N:1  can also be repaired using MF in the same way. In other words,  N:1 scheme is a subset of MF. The DF of MCMF is a little  higher than MF and N:1. By taking cost into consideration,  MCMF tries to find the maximum flow with the minimum  cost. Because the cost varies among PEs, the reconfigured  topology may become unbalanced resulting in a larger DF  than MF. In Fig. 6 (b), compared with N:2, both MCMF and  MF have smaller DF. Because DF is used to describe the  average hop between virtual neighbors, it is predicted that the  throughput and latency using MCMF and MF is better than  the baseline schemes.   IV. Experimental Results  While reconfiguration time is analyzed in the previous  section, the performance of repair rate, throughput and latency  are evaluated on a cycle-accurate simulator. The proposed  approach is also implemented using Verilog and verified on  FPGA. In the following experiments, the proposed approach  uses the same topology and area as those in the baseline  approach.  A. Experimental Setup  The baseline schemes are the same as in DF calculation, i.e.  N:1 and N:2. The repair rate is obtained from simulation using  Matlab. Throughput and latency are measured in a C++ NoC  platform using Carbon SoC Designer. The packet length is set  to be 16 flits. Each input port has 3 virtual channels and each  channel has a FIFO buffer to store 4 flits. Each PE injects  packets independently and the destination of a packet is  randomly determined. XY routing is used, which routes  packets along  the X-axis first, and  then Y-axis. The  performance measures include system throughput and latency.  Average delay is the time required for a packet to traverse the  network from source to destination. Network throughput is the  packets delivering rate for a particular traffic pattern. Each  scheme may generate a different virtual topology for a given  fault pattern. The mapping between the virtual topology and  physical topology is given by a look-up table. Thus the  difference between the NoC models lies in the look-up tables.  B. Repair Rate  Repair rate is the probability that faulty PEs can be  successfully repaired by spare ones. PEs are assumed to work  independently. All PEs including the spare ones are subject to  failures. The number of faulty PEs is varied from 1 to the  maximum number of spare PEs. For each number of faults,  3000 fault patterns are randomly generated. Fig. 7 shows the  repair rate comparison between different spare topologies and  mesh sizes. With the increase of faults, the repair rate using  the two baseline schemes drops significantly while MCMF  maintains a much higher repair rate. The N:2 scheme performs  well for small meshes with a repair rate of over 90%, but in  the case of large meshes, the repair rate drops by nearly 40%  at the failure rate of 10%. The N:1 scheme can only tolerate  one fault at each row and the N:2 scheme can tolerate at most  two faults at each row. When the number of faults increases,  even if the faulty PEs are fewer than the spare ones, they  cannot be fully repaired. In other words, the utilization  efficiency of the spare hardware is low. The PEs used for  repair in MCMF are not restricted to the faulty row, and it is  more capable of using spare PEs to repair faults.   It can also be observed that the repair rate of MCMF in an  8×9 mesh is higher than that of N:2 in a 4×6 mesh although  they both have 8 spare PEs. This indicates that redundancy is  not the only dominating factor for determining the final repair  rate. It implies that by using the MCMF algorithm, a higher  repair rate with less redundant resources can be achieved.  Hence, this algorithm can reduce the redundant hardware  required to obtain a high repair rate.   C. Throughput and Latency Overhead  The H. 264 video-decoding application is taken as an  example to measure the performance of MCMF. MCMF and  N:1 approaches are implemented on a 4×4 mesh, with 4 spare  PEs in a column on the right border, the same as Fig. 1 (b). 4  out of the 20 PEs are randomly chosen to be faulty. 100   0 0 1 2 Failure Numbe r 3 4 20 40 60 80 100 R e a p i r R a t e [ % ] M e s h 4* 5 MCMF N:1 0 2 4 Failure Numbe r 6 8 0 20 40 60 80 100 R e a p i r R a t e [ % ] M e s h 8*9 MCMF N:1 0 0 2 4 Failure Numbe r 6 8 20 40 60 80 100 R e a p i r R a t e [ % ] M e s h 4* 6 MCMF N:2 0 5 1 0 Failure Numbe r 15 0 20 40 60 80 100 R e a p i r R a t e [ % ] M e s h 8*1 0 MCMF N:2 Fig. 7. Repair rate comparison.                                                    ) s e l c y c ( y c n e t a L 90 80 70 60 50 40 0 0 Error mi nimum-cost MF N:1 ) y t i c a p a c f o % ( u p h g u o r h t T 0 .2 0 .4 0 .6 0 .8 Offered traffi c (% of capaci ty) (a) 1 0.8 0.6 0.4 0.2 0 0 0 .2 0 .4 0 .6 Inject i ng PEs (% o f to tal PEs) (b) 0 .8 Fig. 8. Latency and throughput comparison between MCMF and N:1  approaches.  different faulty patterns are generated. A mesh with no faultsis  also simulated for comparison. The average latency and  throughput are shown in Fig. 8. It can be seen that the average  latency  increases with  the  increase of  traffic volume.  Compared to N:1, the latency of MCMF is decreased by 3.5%.  Compared with the fault-free system, the latency of MCMF is  increased by 7.0%~10.1%. The throughput of MCMF is 4.7%  higher than N:1. Compared to the fault-free system, the  throughput of MCMF is decreased by 4.1%~5.3%. If the  differences of PEs are not considered, MCMF is degraded to a  MF algorithm. The data transmission is the same between PEs.  As an ideal case of MCMF, the latency of MF is 4.5% smaller  than N:1 and 5.3% smaller than N:2. The throughput is 11.3%  higher than N:1 and 6.3% higher than N:2.  V. Conclusions and Future Work  Effective fault-tolerant techniques are critical to ensure the  reliability of integrated circuits. In this paper, an approach  using an MCMF algorithm  is proposed for  topology  reconfiguration to improve fault-tolerance. Cost is used to  model different PEs in practical applications. Experiment  results show that the proposed approach achieves a higher  repair rate, higher throughput and shorter delay than other  approaches with the same topology. In addition to that, a  polynomial reconfiguration time is achieved.  The proposed approaches are not restricted to use in  NoC-based manycore systems; they are also applicable to  many other highly integrated systems. For example, in a  reconfigurable system with many free PEs, how to organize  and utilize these PEs for communication and computation  during  run-time  reconfiguration presents a significant  challenge. The approaches presented in this paper may be  useful for addressing this issue and at the same time,  providing a unified topology for the OS and other programs.  "
2015,ShuttleNoC - Boosting on-chip communication efficiency by enabling localized power adaptation.,"Networks-on-Chip (NoC) gradually becomes a main contributor of chip-level power consumption. Due to the temporal and spatial heterogeneity of on-chip traffic, existing power management approaches cannot adapt the NoC power consumption to its traffic intensity, and hence lead to a suboptimal power efficiency. They either resort to over-provisioned NoC design that only suits for traffic spatial distribution, or coarse-grained power gating that only serves traffic temporal variation. In this paper, we propose a novel NoC architecture called Shuttle Networks-on-Chip (ShuttleNoC). By permitting packets shuttling between multiple subnetworks, localized power adaptation can be achieved. Experimental results show that ShuttleNoC could achieve optimal power efficiency with up to 23.5% power savings and 22.3% performance boost in comparison with traditional heterogeneity-agnostic NoC designs.","ShuttleNoC: Boosting On-chip Communication Efﬁciency by Enabling Localized Power Adaptation † †‡ Hang Lu , Guihai Yan † , Yinhe Han † , Ying Wang † and Xiaowei Li †‡ State Key Laboratory of Computer Architecture, Institute of Computing Technology Chinese Academy of Sciences, Beijing, China University of Chinese Academy of Sciences, Beijing, China {luhang, yan, yinhes, wangying2009, lxw}@ict.ac.cn ‡ Abstract—Networks-on-Chip (NoC) gradually becomes a main contributor of chip-level power consumption. Due to the temporal and spatial heterogeneity of on-chip trafﬁc, existing power management approaches cannot adapt the NoC power consumption to its trafﬁc intensity, and hence lead to a suboptimal power efﬁciency. They either resort to over-provisioned NoC design that only suits for trafﬁc spatial distribution, or coarse-grained power gating that only serves trafﬁc temporal variation. In this paper, we propose a novel NoC architecture called Shuttle Networks-on-Chip (ShuttleNoC). By permitting packets shuttling between multiple subnetworks, localized power adaptation can be achieved. Experimental results show that ShuttleNoC could achieve optimal power efﬁciency with up to 23.5% power savings and 22.3% performance boost in comparison with traditional heterogeneity-agnostic NoC designs. I . IN TRODUC T ION Along with the technology scaling, power consumption has become a top design constraint for manycores. The power consumption of a manycore processor can be roughly breakdown to computation power for cores and communication power for Networks-on-Chip (NoCs). Compared with computation power which is relatively easy to regulate with many core-level techniques such as DVFS, the management for communication power is more sophisticated to hit the power efﬁciency frontier, largely because the sporadic variations of on-chip trafﬁc is hard to be adapted by current power management techniques [1][2][3][4]. However, without power-efﬁcient NoC infrastructures, we have little chance to achieve optimal chip-wide power efﬁciency. Recent studies show that the power consumption of NoC could reach as high as 80 watts, a large slice of total chip power, under 16nm technology node for a mesh connected multicore [5]. The same trend also emerges at commercial designs, i.e. Sun’s “Niagara” processor, the interconnect takes nearly 17% of total power [6]; this percentage reaches up to 28% in Intel’s “Intel80” processor [7]. In future manycore era, NoC power consumption is expected to increase more rapidly. Localized power adaptability is an essential design challenge for NoC power management. NoC should be delivered proportional power quota to each node (router + link) in accordance with its local trafﬁc intensity [2]. This requirement, though intuitively simple, is hard to accomplish, given the sporadic trafﬁc variations in not only temporal, but also spatial dimension. The trafﬁc intensity may vary at nanoseconds magnitude [1], and distribute heterogeneously across different *This work is supported in part by National Basic Research Program of China (973) under grant No. 2011CB302503, in part by NSFC under grant No. (61432017, 61202056, 61100016, 61076037, 60921002). Core Core Core Core Core Core R1 R5 R9 R2 /64bits R3 Core Core R6 R7 Core /256bits Core /256bits R10 R11 Core Core Core Core R4 R8 Core R12 Core R13 R14 /64bits R15 R16 Core R1 R`1 Core R5 R`5 Core R9 R`9 Core R13 R`13 Core Core R2 R`2 R6 R`6 Core /64bits /64bits Core R3 R`3 R7 R`7 Core R10 R`10 Core R11 R`11 Core R14 R`14 Core R15 R`15 Core R4 R`4 Core R8 R`8 Core R12 R`12 Core R16 R`16 Fig. 1. NoC architectures supporting only spatial or temporal heterogeneity. locations [8]. Due to these limitations, traditional NoC designs fail to catch such trafﬁc heterogeneity in both temporal and spatial dimensions, and usually renders the NoC at a suboptimal power efﬁciency state. Some prior studies [8] devote to design a heterogeneous NoC based on trafﬁc “spatial” distribution. For example, in Figure 1(a), big routers are relatively designed for boosting network performance by providing higher bandwidth (256bits), in comparison with small power efﬁcient routers (64bits). This design philosophy assumes that routers in central area of a mesh will handle heavier trafﬁc than those at boundaries, so a larger power consumption of big routers is essentially expected to obtain proportional performance enhancement. Whereas, on-chip trafﬁc distribution is never ﬁxed and hotspot may migrate anywhere in NoC, especially when it handles multiprogram workloads, so small routers like 𝑅5 and 𝑅13 may also encounter intense trafﬁc while big routers are almost idle. Under these circumstances, power efﬁciency will suffer. At the other end of the spectrum, some solutions aim to design a conﬁgurable NoC to capture the trafﬁc “temporal” heterogeneity. Multi-NoC [2][9], as a representative, evenly breaks down the original single NoC into multiple subnetworks (subnets). By the employment of power gating, an entire subnet could be powered on/off according to the temporal trafﬁc variations, as shown in Figure 1(b). However, such temporal-oriented approach ignores the spatial trafﬁc distribution, and is not a comprehensive solution either. For example in Figure 1(b), there are two subnets with each 64bits wide. If a trafﬁc ﬂow intends to traverse from node 5 to node 4 under dimensional order routing, and sub-router 𝑅5 is already a hotspot, the only solution is to wake up all sub-routers along the path in subnet 2 (𝑅′5, 𝑅′ 6, 𝑅′ 7, 𝑅′ 8, 𝑅′ 4) to serve this trafﬁc ﬂow, even though sub-routers 𝑅6, 𝑅7, 𝑅8, 𝑅4 in subnet 1 are sufﬁcient to handle this trafﬁc ﬂow. Hence, such power management approach degrades the power efﬁciency of these nodes. subnet 0 active sleep subnet 1 congested subnet 0 hot spot wakeup wake up subnet 1 (a) after  wakeup subnet 0 active sleep subnet 1 subnet 0 active packets active subnet 1 (b) Fig. 2. Power adaptation based on local trafﬁc intensity. The temporal/spatial heterogeneity yields different network demands and power efﬁciency consequences. Ideally, we would expect each node’s bandwidth to be in line with its local trafﬁc intensity. Power management should be capable to adapt to both temporal and spatial heterogeneity of onchip trafﬁc. For example, Figure 2(a) shows a scenario that packets are blocked by a “hotspot” sub-router. If it could “wake up” the “higher level” subnet of neighboring node, and steer the congested packets into the newly powered sub-router, congestion condition would be effectively alleviated, just as Figure 2(b) shows. By contrast, if an active sub-router is more than required due to light trafﬁc condition, we can ofﬂoad the trafﬁc back into the “lower level” subnet and control the ofﬂoaded sub-router to “sleep” state to save power. This observation motivates a localized power management scheme that takes both temporal and spatial trafﬁc heterogeneity into account. Therefore, this paper proposes a novel NoC architecture, called Shuttle Networks-on-Chip (ShuttleNoC), to achieve optimal power efﬁciency. By monitoring trafﬁc intensity at each node, sub-router and its associate links could be powered on/off to implement localized power adaptation. Without losing connectivity, packets in a subnet are allowed to shuttle into active subnets, rather than waking up sleeping sub-router of the same subnet to proceed. In particular, this paper makes the following contributions: ∙ We propose ShuttleNoC architecture to achieve localized power adaptation. We leverage our insights from the weaknesses of existing heterogeneity-agnostic NoC designs. By providing shuttle ability, it avoids unnecessary activation of sub-routers, so temporal/spatial heterogeneity of on-chip trafﬁc could be better supported. ∙ We propose a localized power management mechanism and associate router microarchitecture to achieve optimal power efﬁciency. By receiving power-gating or wakeup requests from other subnets, the bandwidth could be scaled locally, which provides a unique opportunity to achieve optimal power efﬁciency. I I . SHU T T L E N E TWORK S -ON -CH I P ARCH I T EC TUR E In order to achieve the aforementioned packet shuttling to attain localized power adaptation, we need some modiﬁcations upon traditional NoC architectures. The proposed ShuttleNoC design is shown in Figure 3, without loss of generality, we start describing its microarchitecture using a 4x4 mesh connected NoC with two subnets. The packet shuttling is implemented through two hierarchies: 1) at chip level, apart from temporal-oriented approach, a particular hardware called “Link Reconﬁguration Module” (referred to as LRM hereafter) is added between neighboring nodes, which makes previous separated subnets related to each other. The link of a subnet Core R1 R`1 R5 R`5 R9 R`9 R13 R`13 Link Reconfig  Module R2 R`2 R6 R`6 R10 R`10 R14 R`14 R3 R`3 R7 R`7 R11 R`11 R15 R`15 Core R4 R`4 R8 R`8 R12 R`12 R16 R`16 Fig. 3. Shuttle Networks-on-Chip architecture. Dedicated link reconﬁguration module is responsible for packet shuttling between subnets. Packets are no longer required to stay within only one subnet after injected. This augmented link ﬂexibility provides a unique opportunity to localized power adaptation. is reachable to other subnets after reconﬁgured in LRM; 2) for an individual sub-router, we need additional control paths connected to the LRM to transmit “shuttle requests”, so LRM could reconﬁgure the links accordingly and steer the ﬂit to the desired subnet. ShuttleNoC resolves two power efﬁciency limitations associated with previous heterogeneity-agnostic approaches. First, it eliminates the unnecessary activation of sub-routers, hence avoiding the over-provisioned power consumption at lesscongested nodes. This beneﬁt, unique in ShuttleNoC, stems from the ﬂexible link connectivity provided by LRM. Taking the same packet forwarding example in Section I, only 𝑅′5 is shuttle from 𝑅′5 to 𝑅6, which is a light-loaded sub-router (𝑅′6 necessary to be powered on as Figure 3 shows. Packets could is still sleeping), and proceed to the destination in subnet 1. Thus, we have 4 less router activations. For other passing-by packets, i.e. 𝑅1 → 𝑅9, 𝑅′5 could also be used for shuttling, leaving 𝑅′1 and 𝑅′9 at sleep state. Hence, overall power consumption could be reduced signiﬁcantly for ShuttleNoC. Second, apart from spatial-oriented approach, the bandwidth of a node is never ﬁxed but dynamically changed, so trafﬁc spatial variations could be well adapted to further improve power efﬁciency. Link Reconﬁgure Module. The detail implementation of LRM is shown in Figure 4(a). In order to implement packet shuttling, we need additional control and data paths between neighboring nodes. For example, at east output of 𝑅1 in the ﬁgure, sub-router could issue a shuttle request (shuttle_req) for the destination subnet, i.e. west input of 𝑅′2. The data path is reconﬁgured by controlling four multiplexers in LRM. According to different enabler combinations, we can get different subnet connections. In Figure 4(a), supposing a packet intends to shuttle from 𝑅1 to 𝑅′2, LRM conﬁgures the route as 𝑅1 → 𝑁𝑎 → 𝑁𝑑 → 𝑅′2. Although we adopt LRMs, it only introduces a mild power and area overhead. Detailed evaluation of ShuttleNoC will be shown in Section IV. Subnet  select Network Interface demux Subnet  select Network Interface demux R`1 R1 /64bits shutte_req (East out West in) /64bits Na Nc Nb Nd Packet Steering R2 R`2 M (cid:258) (cid:258) R L o m (cid:258) (cid:258) r f PG/WU_in state_out /8bits /8bits Local State Ctrl Power Metric Computation PG WU Runtime  Parameter  Collection Routing  computation VC allocator input port 4 VC0 VC1 VCn (cid:258) (cid:258) (cid:258) (cid:258) input port 0 VC0 VC1 Switch  allocator to crossbar output port 0 VCn output port 4 (cid:258)(cid:258) (cid:258)(cid:258) to LRM state_in /8bits Ctrl Signal  Propagation N S W E N S W E to subnet 0 to subnet 1 /8bits P G / W U _ o u t (a) (b) Fig. 4. Link reconﬁguration module & dedicated router microarchitecture supporting localized power adaptation. I I I . LOCA L I Z ED POW ER ADA P TAT ION BA S ED ON SHU T T L ENOC In ShuttleNoC, packets are no longer required to stay within one subnet after injected, so opportunity arises that we can obtain optimal power efﬁciency by manipulating such packet shuttling operation. In this section, we ﬁrstly specify the router microarchitecture dedicated for ShuttleNoC, and then detailed power adaptation mechanism is discussed. A. Router Microarchitecture in ShuttleNoC Like previously proposed power management techniques [1][2][10], localized power adaptation also relies on runtime statistics as the reference to power on/off a sub-router. Packet shuttling is implemented between neighboring sub-routers, so their power state must be obtained in real time. In ShuttleNoC, three modules: Local State Ctrl, Power Metric Computation and Ctrl Signal Propagation are added to serve these purposes, as shown in Figure 4(b). Local State Ctrl (LSC). LSC is used to control state transition of its host sub-router. Bandwidth adaptation signals, namely PG/WU_in, are received from neighboring subrouters of 4 directions. “PG” and “WU” indicate a “power in ShuttleNoC, it is hence an 8-bit signal (2 sub-routers×4 gating” or “wake-up” request, respectively. If we use 2 subnets directions) and each bit is possible to be a PG or WU. By analyzing their numerical relationship, LSC decides to power on/off its host sub-router. Power Metric Computation (PMC). In order to quantify trafﬁc intensity, we employ PMC module to compute the microarchitectural parameters at runtime. Previous work [2] has proposed several plausible congestion detection metrics, such as local injection queue occupancy, average or maximum buffer occupancy, etc. These metrics can certainly be used in ShuttleNoC; however, to measure trafﬁc intensity, the metric should be able to pinpoint the precise data path or direction that causes the packet contention. PMC then selects average ﬂits queueing delay for each output direction as the intensity metric (Eq. 1). 𝑄𝐷𝑜𝑢𝑡𝑑𝑖𝑟 = ∑𝑁𝑜𝑢𝑡𝑑𝑖𝑟 𝑖 𝑑𝑒𝑙𝑎𝑦𝑖 𝑁𝑜𝑢𝑡𝑑𝑖𝑟 , 𝑜𝑢𝑡𝑑𝑖𝑟 ∈ (𝐸 , 𝑊, 𝑁 , 𝑆 ) (1) 𝑁𝑜𝑢𝑡𝑑𝑖𝑟 stands for the total number of ﬂits heading direction 𝑜𝑢𝑡𝑑𝑖𝑟 . 𝑑𝑒𝑙𝑎𝑦𝑖 is the queuing delay that ﬂit 𝑖 must be retained in its input virtual channel, due to the failure of virtual channel allocation (VA) or switch allocation (SA). An increasing 𝑄𝐷𝑜𝑢𝑡𝑑𝑖𝑟 may indicate that output virtual channels in 𝑜𝑢𝑡𝑑𝑖𝑟 direction may be limited, and a power adaptation request is supposed to be issued. These parameters can easily be obtained as soon as a packet has ﬁnished certain pipeline stages, without introducing additional overhead. Ctrl Signal Propagation (CSP). Note that once PMC intends to issue a power adaptation request (PG/WU) to a certain output direction, CSP module is designed to inform the target sub-router, in that direction, of its request. state_in includes the on/off status sent from neighbor LSCs. CSP, based on this information, propagates power adaptation request by controlling symmetrically organized MUXes, as Figure 4(b) shows. Each bit of PG/WU_out will connect to the corresponding sub-router’s LSC module. B. Power Adaptation Mechanism State Transition in LSC. Clearly, the efﬁcacy of power management mechanism depends on the dynamic router status, so similar to [1][2], we also use three states to depict a router: Active, Sleep, Wakeup. The state is maintained in LSC module. Active indicates the router is currently working and packet shuttling is applicable, while Sleep and Wakeup means the router is power-gated or waking up, respectively. Packets are not allowed to shuttle into “sleep” and ”wakeup” routers. In Section III-A, we have shown that LSC receives power adaptation requests (PG/WU_in) sent from neighbor CSPs (PG/WU_out), so LSC and CSP are coupled as two sides of handshaking operation. Figure 5 shows such interaction. Note that it only shows two neighboring sub-routers of the same subnet, while LSC also interacts with CSPs of other subnets. For a particular sub-router, state transition from Active to Sleep must satisfy two conditions: 1) Num(WU) equals to 0, which denotes all bits in PG/WU_in are PGs; 2) no packet is remained in local input buffers. LSC can then safely power down this router to reduce power consumption. By contrast, if LSC detects arbitrary PG/WU combinations in PG/WU_in, state transition from Sleep to Wakeup depends on if Num(WU) attains a pre-deﬁned threshold. If so, it means the local bandwidth is more requested to be broadened. LSC will then power up its host router. Once Wakeup, it may take 10 ∼ 20 cycles that Active state will be ﬁnally attained, R state_out R PG/WU_in R R CSP LSC Active Sleep Wakeup PG/WU_in state_out P G / W U _ n i s t a t e _ o u t P G / W U n _ i u o _ e a s t t t after T_wu cycles Num WU threshold ( ) (cid:116) Num WU ( ) 0 buffer empty _ true (cid:32) (cid:173) (cid:174) (cid:175) (cid:32) Fig. 5. LSC handshaking with CSP & router state transition. 0 0.2 0.4 0.6 0.8 1 1.2 a o T t l E e n r y g ( o n r m a i l d e z t o 1 r s e u q e ) t Total Energy 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Request Threshold 0 30 60 90 120 150 180 P e k c a t a L t y c n e ( s e c y c l ) degraded performance large power reduction Packet Latency (cycles) Fig. 6. Request threshold (Num(WU)) impact to ShuttleNoC power and performance. according to [1][2][5]. We use T_wu to indicate this transition delay. Subnet Selection in CSPs. As speciﬁed above, CSP processes the power adaptation requests generated by PMC. It must decide which sub-router is supposed to be activated/deactivated based on their on/off status. Detailed procedure is shown in 1. We stipulate the activation of subnets must be in order. For example, if subnet 1 is already active and subnet 2, 3 and 4 are off, subnet 2 is then chosen as the activation candidate (line 8 to 11). Shutting down, on the contrary, follows a reverse order by starting from the highestlevel active subnet (line 2 to 5). IV. EVA LUAT ION In this section, we evaluate ShuttleNoC and the proposed localized power management approach. First, we introduce the platform and baselines we use. Second, we show various results in terms of performance and runtime power efﬁciency. A. Experimental Setup Platform. We modiﬁed Booksim2.0 [11] simulator to run application traces from full system simulation. The fundamental NoC topology is a 4x4 and 8x8 mesh. On-chip router is conﬁgured with a four-stage pipeline plus one cycle for link traversal or shuttling. We use 4 virtual channels for the input buffer with 5-ﬂit depth each. For the power evaluation, we use DSENT [12] power model, which is fed with the statistics from NoC simulations of PARSEC [13] benchmark. We also employ Synopsis Design Compiler [14] to obtain the area overhead of various NoC designs under SMIC90 technology library. Algorithm 1 Subnet Selection in CSP Input: State of neighboring routers: 𝑠𝑡𝑎𝑡𝑒 𝑖𝑛; Subnetworks: 𝑁 ; PMC requests: 𝑟𝑒𝑞𝑢𝑒𝑠𝑡𝑠; Output: Subnet selected: 𝑛; 1: for each 𝑟𝑒𝑞 < 𝑑𝑖𝑟, 𝑜𝑝𝑎 >∈ 𝑟𝑒𝑞𝑢𝑒𝑠𝑡𝑠 do if 𝑜𝑝𝑎 == 𝑃 𝐺 then 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: for (𝑖 = 𝑁 − 1; 𝑖 >= 0; 𝑖 − −) //shut down from the highest-level do if 𝑠𝑡𝑎𝑡𝑒 𝑖𝑛[𝑑𝑖𝑟][𝑖] == 𝑊 𝑈 then return 𝑛; //power off subnet 𝑛 at output 𝑑𝑖𝑟 end if end for else if 𝑜𝑝𝑎 == 𝑊 𝑈 then for (𝑖 = 0; 𝑖 < 𝑁 ; 𝑖 + +) //wake up from the lowest-level do if 𝑠𝑡𝑎𝑡𝑒 𝑖𝑛[𝑑𝑖𝑟][𝑖] == 𝑃 𝐺 then return 𝑛; //wake up subnet 𝑛 at output 𝑑𝑖𝑟 end if end for end if 15: end for 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 e g n S i l 6 5 2 O S 6 5 2 4 6 / T O 6 5 2 u h S l t t e 6 5 2 e g n S i l 6 5 2 O S 6 5 2 4 6 / T O 6 5 2 u h S l t t e 6 5 2 e g n S i l 6 5 2 O S 6 5 2 4 6 / T O 6 5 2 u h S l t t e 6 5 2 e g n S i l 6 5 2 O S 6 5 2 4 6 / T O 6 5 2 u h S l t t e 6 5 2 e g n S i l 6 5 2 O S 6 5 2 4 6 / T O 6 5 2 u h S l t t e 6 5 2 e g n S i l 6 5 2 O S 6 5 2 4 6 / T O 6 5 2 u h S l t t e 6 5 2 e g n S i l 6 5 2 O S 6 5 2 4 6 / T O 6 5 2 u h S l t t e 6 5 2 e g n S i l 6 5 2 O S 6 5 2 4 6 / T O 6 5 2 u h S l t t e 6 5 2 e g n S i l 6 5 2 O S 6 5 2 4 6 / T O 6 5 2 u h S l t t e 6 5 2 blackscholes fluidanimate streamcluster swaptions x264 vips freqmine bodytrack average P a o w d e e z r B r t 6 5 2 n e o g d n k a e o w ( N o r m i l S i l ) LRM Power Static Power (router+link) Dynamic Power (router+link) Fig. 7. Overall NoC power breakdown (normalized to Single-256). Baselines. We use three baselines to prove the efﬁcacy of ShuttleNoC in power adaptation: (1) the ﬁrst one is a traditional single NoC with no power management involved. We conﬁgure the bandwidth of NoC platform as 256bits, so the ﬁrst baseline is referred to as “single-256”; (2) the second baseline, referred to as “SO-64/256”, is the “spatial-oriented” approach, and two bandwidth conﬁgurations are set as 64bits and 256bits; (3) the third one, referred to as “TO-256”, is the “temporal-oriented” approach with 4 subnets and each one 64bits wide. The same conﬁguration is also set for the proposed ShuttleNoC. Besides, T_wu is set as 20 cycles for a Waking-up sub-router to ﬁnally attain Active state [1][2]. Since both 16bits (4 sub-routers × 4 directions) for ShuttleNoC. Note that even if the bandwidth conﬁguration for the baselines and ShuttleNoC is not exactly the same, the maximum bandwidth (256bits) is equal. we use 4 subnets, state_in/out and PG/WU_out/in are B. Results and analysis 1) Request threshold & ShuttleNoC Responsiveness: We ﬁrstly evaluate the impact of “wakeup” request threshold, Num(WU) in Figure 5, to the ShuttleNoC responsiveness. Speaking of responsiveness, we evaluate the total energy consumption and average packet latency, with Num(WU) tuned from the minimum to the maximum. The energy results are normalized to the minimum-request scenario (1 on X-axis). As can be seen from Figure 6, the packet latency remains almost stable with threshold varied from 1 ∼ 10, while the total energy reduces nearly 50%. This phenomenon proves that larger threshold contributes to the power reduction due to the prolonged sub-router sleeping cycles without compromising performance. However, the latency starts to climb signiﬁcantly to 160 cycles (207.7% degradation) from 10 ∼ 16. NoC per                              0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 b l a c k s c h o l e s a fl u i d n i m s tr e a t e a m c l u s t e r s w a p ti o n s x 2 6 4 v i p s fr e q m i n b e o d y tr a c k A v e r a g e N o r m a i l d e z A e v r e g a P e k c a t a L t y c n e Single−256 SO−64/256 TO−256 Shuttle−256 Fig. 8. Overall NoC performance (normalized to Single-256). formance is suffered under this scenario, because sub-routers become too “lazy” to respond to the increasing WU requests, even if the total energy continues to decrease. Therefore, for the rest of the experiments, we set the request threshold as 10 to acquire a better energy/performance tradeoff, but ShuttleNoC can actually work at other threshold values, based on the total power budget available or the intended NoC power efﬁciency. 2) Overall Power & Performance: ShuttleNoC enables the localized power adaptation, so in this set of experiment, we show how much beneﬁt it brings in terms of overall power reduction in a 8x8 mesh. Figure 7 plots the breakdown of NoC power to examine the impact of each component. Compared to “Single-256”, “SO-64/256” and “TO-256”, ShuttleNoC shows substantial static power reduction by 19.2%, 16.0% and 12.5%. The improvement comes from the localized power adaptation. ShuttleNoC only activates/deactivates sub-router based on the local trafﬁc intensity, rather than ﬁxing the bandwidth as done in “SO-64/256”. Compared to “TO-256”, it does not need to affect neighboring node to maintain the network connectivity, so static power is also reduced. On the other hand, ShuttleNoC employs LRM to achieve packet shuttling, so dynamic power may increase due to the frequent link reconﬁgure operation. As shown in the ﬁgure, it incurs 3.8% power overhead for LRM, and 1.7%, 3.3% dynamic power increase compared to “SO-64/256” and “TO-256”, respectively. However, the abundant static power reduction still renders ShuttleNoC an overall power savings of 23.5%, 14.3% and 9.3%. To further prove the effectiveness of ShuttleNoC, we then show the overall performance enhancement in comparison with three baselines. Average packet latency is used as the performance metric. The result in Figure 8 shows that ShuttleNoC improves network performance by 22.3%, 16.4% and 14.7% on average. The runtime support of trafﬁc heterogeneity in ShuttleNoC effectively avoids potential hotspots. Generally speaking, ShuttleNoC offers improvements in both power and performance. Hence, power efﬁciency is undoubtedly boosted compared to the baselines. 3) Runtime Power Efﬁciency: To explore the implication of power/performance beneﬁt brought by ShuttleNoC, we further evaluate the power efﬁciency variation periodically, by executing canneal benchmark under ShuttleNoC and the two baselines. We use Energy Delay Product (EDP) as the power 0 1 3 5 7 9 11 13 Exection Interval 15 17 19 1 2 3 4 5 6 7 8 9 x 104 E e n r y g D y a e l P r c u d o t TO−256 SO−64/256 Shuttle−256 Fig. 9. Energy Delay Product (EDP) at runtime. efﬁciency representative. The result is shown in Figure 9, in which we observe that EDP values remain almost constant (5% variation on average) for ShuttleNoC. Whereas,TO-256 exhibits large ﬂuctuations during execution, because of the coarse-grained, subnet-level power control. For SO-64/256, it suffers from the severe performance degradation when heavy trafﬁc migrates to small routers. 4) Heterogeneity Adaptation Analysis: The smooth power efﬁciency of ShuttleNoC stems from the effective adaptation of trafﬁc heterogeneity. As evidence, this set of experiment traces the latency variation of every node in a 4x4 NoC, by executing the same canneal benchmark. As shown in Figure 10, SO64/256 exhibits obvious latency variations between 40 ∼ 190 cycles. Due to the large bandwidth of big routers in the center, node 6, 7, 10, 11 show a moderate latency variation compared to nodes at boundaries like 1 or 15, but still around 100 cycles. TO-256 shows a mild latency variation around 110 cycles on latency variation around 30 ∼ 40 cycles. Such near-constant average. By sharp contrast, ShuttleNoC shows a more smooth latency further proves that ShuttleNoC has the unique ability of localized bandwidth adaptation, and thus more possible to achieve optimal power efﬁciency. C. Overhead Analysis ShuttleNoC relies on link reconﬁguration module and dedicated hardware in routers to fulﬁll the power adaptation purpose. We evaluate the implementation cost of ShuttleNoC by comparing its area overhead to other NoC designs, as shown in Figure 11. We analyze several area contributive components. ShuttleNoC incurs a mild increase of total NoC area, due to complicated link layout and dedicated modules. Fortunately, LRM only consists of several multiplexers, and does not introduce signiﬁcant area overhead. Other control logics in total occupy 4.3% of overall NoC area. Note that although ShuttleNoC exhibits more area overhead, it does not consume a larger power, because the proposed power adaption mechanism brings more power savings, and even gives a better play in network performance. V. R E LAT ED WORK The concept of ShuttleNoC is similar to bandwidth scaling techniques proposed in [15] and [16], in which bi-directional links are employed to resolve low resource utilization. However, they do not target power efﬁciency in NoCs. [17] allows                   SO-64/256 TO-256 Shuttle-256 ) l s e c y c ( y c n e a L t 200 100 0 13579111315 Node Index 10 20 30 40 Exection Time ) l s e c y c ( y c n e a L t 200 100 0 13579111315 Node Index 10 20 30 40 Exection Time ) l s e c y c ( y c n e a L t 200 100 0 13579111315 Node Index 10 20 30 40 Exection Time Fig. 10. Heterogeneity adaptation comparison. ) 2 m m ( a e r A C o N 12 10 8 6 4 2 0 LSC+PMC+CSP LRM Link Clock Control Logic Crossbar Buffer Single-256 TO-4x64 Shuttle-4x64 Shuttle-2x64 Fig. 11. Area overhead analysis of multiple NoC designs. concurrent transmission of multiple ﬂits on a link. [8] proposed that ﬂit size can be decoupled from channel width and transmitted on big/little routers respectively. [18] combines multiple ﬂits together if possible and send through a wider channel. These spatial-oriented approaches may encounter complex hardware design and suffer from runtime variations of trafﬁc distribution. Multiple Networks-on-Chip (MultiNoC) proposed in [2] can provide proportional energy consumption in accordance with the in-ﬂight trafﬁc, but it unnecessarily activates an entire subnetwork to adapt to local trafﬁc intensity. [19] uses bandwidth/frequency asymmetric Multi-NoC for latency/bandwidth sensitive workloads; commercial processors like TILE64 [20] and TRIPS [21] also employ multiple NoCs to isolate different message classes. However, these designs do not consider localized power adaptation either. To the best of our knowledge, the proposed ShuttleNoC is the ﬁrst work that targets localized power adaptation to achieve optimal power efﬁciency. V I . CONC LU S ION This paper proposes ShuttleNoC, a novel NoC architecture to enforce optimal power efﬁciency. Unlike previous temporal and spatial-oriented approach, ShuttleNoC achieves localized power adaptation to serve runtime trafﬁc heterogeneity. By precise state transition mechanism and dedicated link reconﬁguration module, packets are allowed to shuttle between multiple subnetworks. Compared with temporal-oriented approach, ShuttleNoC avoids unnecessary activation of sub-routers to obtain lower power consumption. Besides, ShuttleNoC does not resort to ﬁxed bandwidth conﬁgurations as in spatialoriented approach, and hence yields higher network performance. We therefore believe that ShuttleNoC is a promising scheme to achieve optimal power efﬁciency in future manycore processors. "
2015,Adaptive remaining hop count flow control - Consider the interaction between packets.,"The interaction between packets affects performance and global fairness of Network-on-Chip. Preferentially transferring packets with small remaining hop counts (PPSR) can reduce the flying packet amount to improve the performance. Yet, the global fairness is negatively affected. In contrast, preferentially transferring packets with large remaining hop counts (PPLR) can achieve better global fairness with a poorer performance. In this paper, we propose adaptive remaining hop count flow control, which dynamically switches between PPSR and PPLR. In this way, we can achieve higher performance and better global fairness.","Adaptive Remaining Hop Count Flow Control: Consider the Interaction between Packets Peng Wang, Sheng Ma, Hongyi Lu, Zhiying Wang, Chen Li State Key Laboratory of High Performance Computing National University of Defense Technology Email: {pengwang, shengma, hylu, zywang, chenli, weichen}@nudt.edu.cn Changsha, China 410000 Abstract—The interaction between packets aﬀects performance and global fairness of Network-on-Chip. Preferentially transferring packets with small remaining hop counts (PPSR) can reduce the ﬂying packet amount to improve the performance. Yet, the global fairness is negatively aﬀected. In contrast, preferentially transferring packets with large remaining hop counts (PPLR) can achieve better global fairness with a poorer performance. In this paper, we propose adaptive remaining hop count ﬂow control, which dynamically switches between PPSR and PPLR. In this way, we can achieve higher performance and better global fairness. I. Introduction As the core count on a single chip is continually increasing, the on-chip communication mechanism becomes the performance scaling bottleneck [1]. The Network-onChip (NoC) is now the de-facto communication choice for many-core processors. The ﬂow control is one of the most important aspects of NoC design, since it is in charge of allocating buﬀers and channels and directly determines the network latency and throughput. Many novel ﬂow control mechanisms have been proposed [2, 3, 4, 5, 6, 7, 8]. Peh and Dally designed FlitReservation ﬂow control [2] to reduce the credit roundtrip delay. This mechanism can enhance both the buﬀer utilization and network throughput. Through bypassing pipeline stages of several intermediate routers, Kumar et.al [3] proposed Express Virtual Channel to reduce the message transmission latency. Based on this work, Kumar et.al [4] further presented Token Flow Control to broadcast the buﬀer status to nearby routers, which allows messages to directly go to the destination router. However, most of these work focus on the hardware structure of the network, such as router architecture and pipeline, and ignore the interaction between packets with diﬀerent remaining hop counts. This interaction strongly aﬀects the performance when packets compete for the same resource. In this paper, we deﬁne the packets with small remaining hop counts as PSR, and the packets with large remaining hop counts as PLR. In general, PSR need less time to transfer in the network and will occupy smaller buﬀer resource. But, due to the competition between PSR and PLR, PSR may stay longer in the network and continuously occupy the buﬀers, In some worse cases, PLR may block PSR. PLR need more hop counts to transfer in the network and consume large channel resource. But, due to the competition between PSR and PLR, PLR may stay longer in the current router and cannot use the rest of channels in the routing path. In this paper, we deeply research the interaction between packets with diﬀerent remaining hop counts. Based on a nonlinear weight probability arbiter [9], we have explored the performance space between Preferentially transferring packets with small remaining hop counts (PPSR) and preferentially transferring packets with large remaining hop counts (PPSR). PPSR can effectively reduce ﬂying packet amount, which is helpful to enhance the network performance, especially when the network load is high. Yet, the global fairness is negatively aﬀected. In contrast, PPLR can enhance the global fairness with a poorer performance, which is helpful to achieve higher buﬀer and channel utilization when the network load is low. According to the above analysis, we propose adaptive remaining hop count(ARHC) ﬂow control, to dynamically switch between PPSR and PPLR. In this way, ARHC ﬂow control combines the the advantage of PPSR and PPLR. Based on the cycle-accurate interconnection network simulator booksim2.0 [10], we estimate the throughput and fairness. We also evaluate average accepted rate to measure the performance when the the network is over saturated. Furthermore, we also estimate the performance of our designs in the adaptive routing. The throughput in our ARHC ﬂow control is 12% higher than PPLR in T=0 T=1 S1 10 01 S2 D1 12 D2 13 1p 2p S1 10 S1 10 01 S2 01 S2 D1 12 D1 12 D2 13 D2 13 S1 10 S1 10 T=2 01 S2 D1 12 Situation 1 01 S2 D1 12 Situation 2 D2 13 D2 13 T=3 01 S2 01 S2 D1 12 D1 12 D2 13 D2 13 S1 10 S1 10 Fig.1. The interaction between packets with diﬀerent remaining hop counts. the shuﬄe pattern. The global fairness in the ARHC ﬂow control is better than PPSR and round robin routers. Our ARHC ﬂow control also enhances the performance when the network is over saturated. We make the following contribution: • We explore the design space between PPSR and PPLR. • We propose adaptive remaining hop count(ARHC) ﬂow control to combine the advantage of PPSR and PPLR. • We also estimate our designs in the adaptive routing. The remainder of the paper is organized as follows. Section II discusses the interaction between the packets. Section III presents the implementation of our design . Section IV describes the simulator and parameters. Section V gives the experimental results, including the throughput, average accepted rate and global fairness. Finally, Section VI concludes the paper. II. Motivation The remaining hop count is the transmission distance from the current router to the destination router. The interaction between packets with diﬀerent remaining hop counts also inﬂuence the network performance. As shown in Fig. 1, the resource nodes S1 and S2 send packets p1 and p2 to the destination nodes D1 and D2 respectively. At T = 0, p1 and p2 have to compete for the access to the downstream router. The router 01 selects one of them to transfer, but as p1 and p2 have diﬀerent remaining hop counts, the network shows diﬀerent state. • Preferentially transfer p1 , the packet with a small remaining hop count: As shown in Situation 1, the p1 reaches destination node D1 at T = 1 and gets down the network through the interface layer at T = 2. The p1 has a smaller remaining hop count, and it needs smaller time to transfer in the network. In this situation, the network can release the buﬀer occupied by the p1 as soon as possible. The released buﬀer resource can be reused by other packets. At the same time, the total ﬂying packet amount is reduced. For example, at T = 2, there is only 1 packet in the Situation 1. While, there are two packets in Situation 2. This is helpful to avoid the network congestion and enhance the network throughput. • Preferentially transfer p2 , the packet with a large remaining hop count: The Situation 2 can achieve better buﬀer and channel utilization. In the Situation 2, it takes 3 cycles to ﬁnish all packet transmissions. While, in the Situation 1, it takes 4 cycles to ﬁnish transmissions. However, the packets with larger remaining hop counts may block following packets, and make the network congested more easily. For example, if there is no free buﬀers in the destination node D2 , the incoming packet p2 has to stay in the node 02. The packet P1 is also blocked, as it cannot get into its destination node D1. Furthermore, this situation makes that the network contains more packets in a time For example, at T = 2, there is only 1 ﬂying packet in the Situation 1, While, there are two ﬂying packets in Situation 2. This also makes the network congested more easily. If we preferentially transfer packets with small remaining hop counts (PPSR), the ﬂying packet amount is reduced and the performance is improved. In contrast, if we preferentially transfer packets with large remaining hop counts (PPLR), the network can achieve higher buﬀer and channel utilization, with a poorer performance. Adapting to the network status, we dynamically switch between PPSR and PPLR. When the network load is low and is not saturation, we use the PPLR to enhance the resource utilization and achieve better global fairness. When the network load is high and the competition between packets is serious, we use PPSR to reduce the ﬂying packet amount and achieve higher network throughput. In this way, we can combine the advantage of PPSR and PPLR. III. Design Preferentially transferring some packets is unfair and may induces some packets starved. In general, we have to set a threshold value to avoid packet starvation. When the latency of a packet exceeds the threshold value, the packet will get the highest priority to transfer. However, when a vast of packets exceed the threshold value, these packets will get the highest priority. Routers randomly select one from them. This induces that we cannot preferentially transfer packets based on our expectation, especially when the injection rate is high. Therefore, we use the nonlinear weight probability arbiter [9] to avoid the starvation and preferentially transfer some packets as we expect. The detail of the nonlinear weight probability arbiter will be introduced in section A. The design of PPSR and PPLR in deterministic and adaptive routing will be introduced in section B and C. The switching mechanism of ARHC ﬂow control will be introduced in section D. from the source node to the destination node in each dimension. Given, two packets with the hop counts w and w − d respectively, so the w1 = w and w2 = w − d, and the probability of grant g1 and grant g2 is p1 = w 2w−d and p2 = w−d 2w−d . When the hop count is large or hop counts of requests are close to each other w >> d, the probability of grant g1 and grant g2 is close to 1 2 . As a result, the linear probability weight arbiter randomly select one of them and the weights have no eﬀect on the arbitration. In order to overcome this problem, Lee et.al [9] has proposed a nonlinear weight probability arbiter, as shown in Fig. 3. 1c1h 2h 2c 1w EXP EXP 2w 1w Tw + RNG < < 1g 2g w w(cid:14) 1 2 Weight calculation Sum Random weight arbiter A. Nonlinear Weight Probability Arbiter Fig.3. The nonlinear weight probability arbiter. 1r 1w 2r 2w Arbiter 1g 2g Fig.2. The weight probability arbiter. The weight probability arbiter is shown in Fig. 2. The requests r1 and r2 with the weight w1 and w2 apply for the same resource. The probability of each request wins the competition is: P (r1 ) = P (r2 ) = w1 w1 + w2 w2 w1 + w2 (1) (2) The probability of grant g1 and grant g2 are only aﬀected by their weights. But using the linear weight cannot provide an eﬃcient diﬀerentiable probability. For example, adopting the hop count as the weight, w = hx or w = hx + hy , where hx and hy represent the hop count The nonlinear weight probability arbiter consists of three parts, the weight calculation unit, sum unit and random weight arbiter. The weight calculation produces the nonlinear weight at the computation formula w = ch , where h is the hop count, and c is the contention degree or the number of packets contending for the same output port. In this way, even though packets have larger hop counts or similar hop counts, the arbiter can produce diﬀerentiable weights. Given the similar example, two packets with hop counts w and w − d respectively, so the w1 = cw and w2 = cw−d . Even though the hop counts are large or close to each other w >> d, the w1 is still cd times larger than the w2 . So we can avoid the problem in the linear weight probability arbiter. The sum unit adds all the weights together and produces WT . Using WT as a boundary, the random unit produces a random number between 0 to WT − 1. Based on the weight from each request, the random weight arbiter divides the WT into multi-intervals. The request is granted, only when the random number is at its interval . In this way, the nonlinear weight probability arbiter can guarantee that requests with large weights have higher opportunities to win. The packets with small weights also have opportunities to transfer and will not be starve. B. PPSR and PPLR in deterministic routing We adopt the X-Y routing. Packets can’t transfer in Y direction, until they have ﬁnished transmission in the X direction. The packets routing in the X direction only compete with the packets routing in the X direction, and will not be impacted by the packets routing in the Y direction. The packets routing in the Y direction is similar. So the weight w is only determined by the remaining hop count in each direction. In PPSR, the packets with small remaining hop counts have higher weights. The packets with larger remaining hop counts should have smaller weights, so the w = cni−hi , where ni is the number of routers in the routing direction. and i is the direction that the packet is routing. In contrast, in PPLR, we need select the packets with the largest remaining hop counts. The larger remaining hop count produces a bigger weight, so the w = chi i . i cw Furthermore, more requests make the WT bigger. The bigger WT decreases the grant probability of the packet with the highest weight. As a result, the arbiter cannot eﬀectively select the packet with the highest weight. For example, two packets p1 and p2 with hop counts w and w − d respectively, so p1 has the highest weight. The probability of grant g1 is cw +cw−d , which can be simpliﬁed into g1 = 1+c−d . When we add the third packet p3 with a hop count w − d , The probability of grant g1 become cw +cw−d+cw−d(cid:2) , which can be simpliﬁed into g1 = 1+c−d+c−d(cid:2) . The denominator increases and the g1 −d and c becomes smaller. If we increase the c, the c will decrease. So we can increase c to weaken the inﬂuence of the number of requests. There are two input ports contending for the same output port in the X direction and there are three input ports contending for same output port in the Y direction. So we set that c in the X direction to be 2, and c in the Y direction to be 3. −d(cid:2) 1 1 cw (cid:2) xh S yh D Fig.4. The alternative delivery paths in the network. C. PPSR and PPLR in Adaptive Routing In our adaptive path, the router can select one of alternative delivery paths for the packet routing, based on the state of the neighborhood routers. Furthermore, routers prefer to select the downstream router with the largest available buﬀers. In the escape path, we use the X-Y routing to avoid deadlock. Because both the adaptive path and the escape path use the same allocation units(VA and SA), we set the c in the X direction to be 2, and c in the Y direction to be 3. In this way, the c induces small bias in the adaptive path, but we can make our design simpler and reduce the hardware consumption. As shown in Fig. 4, a packet has more than one alternative delivery paths for routing. At each hop, the router need select an available direction for routing. The routing direction of a packet is indeterminacy. When packets contend for the same resource, we need to consider remaining hop counts in both the X direction and the Y direction. Computing weights in PPSR for nonlinear probability is w = cnx−hx + c and the weight in PPLR is w = chx x + c y . ny −hy y x hy D. ARHC Flow Control Switching Machination As shown in Fig. 1, PPLR can achieve higher buﬀer and channel utilization and better global fairness, which is helpful to reduce the average packet latency. But this ﬂow control mechanism may induce the network blocked and saturation easily. In contrast, PPSR can reduce total ﬂying packet amount, which is helpful to achieve higher throughput. But this ﬂow control mechanism may causes some packets to wait a long time and negatively aﬀects the global fairness. Based on the above analysis, PPLR has advantage on the buﬀer and channel utilization. But when the network load becomes higher, PPLR makes the network blocked and saturation easily. So PPLR is appropriate for enhancing the network global fairness when the network load is low. PPSR can reduce the ﬂying packet amount, which is critical to enhance performance with high network loads. But PPSR has a bad globe fairness. So we propose adaptive remaining hop count(ARHC) ﬂow control, which dynamically switches between PPSR and PPLR. In this way, ARHC ﬂow control can take advantage of PPSR and PPLR. The switching mechanism is the key to this design. The switching condition in the DOR network is that: if the input port buﬀer in the downstream route was empty, we use PPLR. Otherwise, we use PPSR. In this way, if we use PPLR, the packets with large remaining hop counts will be placed in the head of the buﬀer queues and will be sent to the next routers as soon as possible. They will not block the following packets and the rest of the buﬀers also can accept the packets. On the other hand, if some buﬀers in the downstream router are occupied, it implies that the resource competition in the downstream router A e v r e k c a p e g a t l a t n e y c [ e c y c l ] s 70 60 50 40 30 20 10 0 0.1 0.15 0.2 0.25 robin PPLR PPSR ARHC Injection rate[flit/cycle/node] (a) uniform random with deterministic routing. Fig.5. Latency throughput curve A e v r e k c a p e g a t l a t n e y c [ e c y c l ] s 70 60 50 40 30 20 10 0 0 0.05 0.1 0.15 0.2 robin PPLR PPSR ARHC Injection rate[flit/cycle/node] (b) shuﬄe with deterministic routing. A e v r e k c a p e g a t l a t n e y c [ e c y c l ] s 0 20 40 60 80 100 0.1 0.15 0.2 0.25 robin PPLR PPSR ARHC Injection rate[flit/cycle/node] (c) uniform random with adaptive routing. A e v r e k c a p e g a t l a t n e y c [ e c y c l ] s 0 20 40 60 80 100 0 0.05 0.1 0.15 robin PPLR PPSR ARHC Injection rate[flit/cycle/node] (d) shuﬄe with adaptive routing. TABLE I Parameters Parameters values topology 2D mesh Network size 64 router per node 1 Virtual channel 4 Buﬀers per input port 16 Packet size 1 ﬂit may be drastic and the incoming packets may have to stall in a long time. The switching condition in the adaptive routing is similar that: if any one of the input port buﬀer in the available downstream router is not occupied, we use PPLR. Otherwise, we use PPSR. IV. Methodology Our evaluation of designs is based on a cycle-accurate interconnection network simulator booksim2.0 [10]. The simulator provides various traﬃc patterns to stress the network. We evaluate the average throughput, through diﬀerent traﬃc patterns including uniform random, shufﬂe and tornado. We also use the hotspot traﬃc pattern to evaluate the global fairness. In this traﬃc pattern, all nodes sent packets to a destination node. Some nodes get more bandwidth and can sent more packets to the destination node. So we measure the rate of sent packets to get the global fairness. The simulator also supports ﬂexible conﬁguration parameters to constitute various network structures. Parameters used in the simulator are described in the Table I. We also use a round robin router as comparison baseline. The detailed results are shown in the next section. V. Result A. Throughput As shown in Fig. 5, PPSR can achieve higher throughput than PPLR and the round robin router. The throughput in PPSR is 4.77% and 7.69% higher than the round robin router in the uniform random and shuﬄe traﬃc patterns. Yet, the throughput in PPLR is 2.4% and 7.69% lower than the round robin router. In the adaptive network, PPSR also can achieve higher throughput than the PPLR. Even though the adaptive routing provides more the available delivery paths and weakens interaction between packets, PPLR is still easy to become saturation and has lower throughput. As PPSR can route packets with small remaining hop counts to their destination as soon as possible, the ﬂying packet amount is reduced and the buﬀer resource is quickly released. But this may lead to that some packets have to wait a long time for routing. This problem will be discussed in the section C. As our ARHC ﬂow control combines PPSR with PPLR, the throughput in ARHC ﬂow control is improved. As shown in Fig. 5d, the throughput in our ARHC ﬂow control is 12% higher than PPLR in the shuﬄe pattern. B. Average Accepted Rate We measure the average network accepted rate to evaluate performance when the network is over saturation. As shown in Fig. 6a, the average accepted rate in the ARHC ﬂow control is 16% higher than PPLR. Even though our ARHC ﬂow control has higher average accepted rate than PPLR, its performance is lower than round the robin arbiter router. It is mainly due to that we adopt the local information to control ARHC ﬂow control, which induces performance loss. For example, if the current router uses PPLR, the packet sent will be placed in the head of the buﬀer queue in the downstream router. However, the current router only gets the information from the neighborhood routers. This can not guarantee that the downstream also uses PPLR to route packets. As a result, that packets with large remaining hop counts may block the following packets, which induces the performance loss. As shown in Fig. 6, comparing with the the round robin router, PPSR can achieve 75% average accepted rate improvement in the tornado traﬃc pattern and 21% improvement in the shuﬄe traﬃc pattern. These performance gains are relatively smaller in the adaptive network. As the available traﬃc increase, the probability of blocking caused by PLR becomes smaller. A e v r p e c c a e g a t d e i l f [ e c y c / s t l ] 0 0.05 0.1 0.15 0.2 0 0.5 1 robin PPLR PPSR APHC Injection rate[flit/cycle/node] (a) tornado with deterministic (b) shuﬄe with deterministic routing. routing. Fig.6. Average accepted rate curve A e v r p e c c a e g a t d e i l f [ e c y c / s t l ] 0 0.05 0.1 0.15 0.2 0.25 0 0.5 1 robin PPLR PPSR ARHC Injection rate[flit/cycle/node] A e v r p e c c a e g a t d e i l f [ e c y c / s t l ] 0.14 0.12 0.1 0.08 0.06 0.04 0.02 0 0 0.5 1 robin PPLR PPSR ARHC Injection rate[flit/cycle/node] (c) tornado with adaptive routing. A e v r p e c c a e g a t d e i l f [ e c y c / s t l ] 0 0.05 0.1 0.15 0.2 0 0.5 1 robin PPLR PPSR ARHC Injection rate[flit/cycle/node] (d) shuﬄe with adaptive routing. 1 3 5 7 0 0.05 0.1 0.15 0.2 1 2 3 4 5 6 7 8 A p e c c u p h e d g o u n o e h t c d y c e / t s t / r t i l f [ l ] (a) round robin. (b) PPSR. (c) PPLR. Fig.7. Global fairness cross all nodes.(The mean square error from a to d is: 0.022, 0.028, 0.011, 0.020) 1 3 5 7 0 0.05 0.1 0.15 0.2 1 2 3 4 5 6 7 8 A p e c c u p h e d g o u n o e h t c d y c e / t s t / r t i l f [ l ] 1 3 5 7 0 0.01 0.02 0.03 0.04 0.05 0.06 1 2 3 4 5 6 7 8 A p e c c u p h e d g o u n o e h t c d y c e / t s t / r t i l f [ l ] 1 3 5 7 0.14 0.12 0.1 0.08 0.06 0.04 0.02 0 1 2 3 4 5 6 7 8 A p e c c u p h e d g o u n o e h t c d y c e / t s t / r t i l f [ l ] (d) ARHC. C. Global Fairness In this section, we explore the global fairness in our designs. As the round robin router can achieve the local fairness, we use it as the baseline. In the hotspot trafﬁc pattern, all routers sent packets to only one router. Across all nodes, we evaluate the mean square error of sending packet rates to assess the global fairness. The bigger mean square error indicates poorer global fairness. In this way to measure the global fairness, there is only one destination router. Packets in the same route have the equal remaining hop count in the hotspot traﬃc pattern. Our design cannot work in this situation. So we adopt the distance from the source router to the hotspot router as our remaining hop count in the global fairness test. As shown in Fig. 7, PPSR have the biggest mean square error (0.028), so the fairness is worst. In the PPSR, the packets closer to the hotspot have higher probabilities for routing. The round robin router has second highest mean square error (0.022). Since the round robin arbiter is a local fairness arbitration mechanism, the routers closer to hotspot can share higher bandwidth. These induce that the routers close to the hotspot router can sent more packets, and harm the global fairness. PPLR has the lowest mean square error and can achieve best global fairness. It is due to that PPLR is similar to age-based arbitration, which is a global fairness mechanism [9, 11]. ARHC ﬂow control combines PPLR with PPSR and has better global fairness than PPSR and round robin router. VI. Conclusions In this paper, we have discuss the mutual interaction between the packets. The remaining hop count of packets has a big inﬂuence on the network performance. Through our analysis and adopting an nonlinear weight probability arbiter, we propose adaptive remaining hop count (ARHC) ﬂow control, dynamically switching between PPSR and PPLR. Based on the cycle-accurate interconnection network simulator booksim 2.0, we estimate the throughput and average accepted rate in the deterministic routing and adaptive routing. We also evaluate the global fairness in the hotspot traﬃc pattern. The throughput in our ARHC ﬂow control is 12% higher than PPLR in the shuﬄe pattern. The global fairness in the ARHC ﬂow control is better than PPSR and round robin arbitration mechanisms. Our ARHC ﬂow control also enhances the performance when the network is over saturated. Even though taking local state information to control ARHC ﬂow control makes some performance loss, our ARHC ﬂow control still can improve the performance at the throughput, average accepted rate and global fairness. VII. Acknowledgement This work is supported by 863 Program of China (2012AA010905), NSFC (61272144, 61303065, 61202121), Research Pro ject of NUDT (JC13-06-02), Doctoral Fund of Ministry of Education (20134307120028), Research Fund for the Doctoral Program of Higher Education of China (20114307120013), and Hunan Provincial Natural Science Foundation (14JJ3002). "
2015,A performance enhanced dual-switch Network-on-Chip architecture.,"Network-on-Chip (NoC) is an attractive solution for future systems on chip (SoC). The network performance depends critically on the performance of packets routing. However, as the network becomes more congested, packets will be blocked more frequently. It would result in degrading the network performance. In this article, we propose an innovative dual-switch allocation (DSA) design. By introducing two switch allocations, we can make utmost use of idle output ports. Experimental results show that our design significantly achieves the performance improvement in terms of throughput and latency at the cost of very little power overhead.","A Performance Enhanced Dual-switch Network-on-Chip Architecture Lian Zeng Takahiro Watanabe Graduate School of Information, Production and Systems, Waseda University 2-7 Hibikini, Wakamatsu-ku, Kitakyushu Fukuoka, Japan lian zeng ips@ruri.waseda.jp Graduate School of Information, Production and Systems, Waseda University 2-7 Hibikini, Wakamatsu-ku, Kitakyushu Fukuoka, Japan watt@waseda.jp Abstract— Network-on-Chip (NoC) is an attractive solution for future systems on chip (SoC). The network performance depends critically on the performance of packets routing. However, as the network becomes more congested, packets will be blocked more frequently. It would result in degrading the network performance. In this article, we propose an innovative dual-switch allocation (DSA) design. By introducing two switch allocations, we can make utmost use of idle output ports. Experimental results show that our design signiﬁcantly achieves the performance improvement in terms of throughput and latency at the cost of very little power overhead. I . IN TRODUC T ION As the technology of semiconductor continues to develop, hundreds of cores will be deployed on a signal die in future Chip-Multiprocessors (CMPs) designs. In order to solve the problem of bus based system such as increasing power consumption, the limitation of bandwidth and scalability, the promising packet-switch Network-on-Chip (NoC) has become an attractive solution which can provide low latency, high throughput and low power [1]. Besides different routing algorithms, a router architecture can signiﬁcantly affect the network performance. However, researchers are continuously confronted by one of major challenges: as the whole network becomes more congested, packets will be blocked more frequently. As a result, the network performance is degraded, and in this situation, how to transfer more packets through a router should be considered. On the other hand, power dissipation is also an important factor which developer should consider when they design. Input buffers alone could consume almost 46% percent of the total power of the whole interconnection network [2]. Therefore, even though simply increasing the size of input buffers will lead to more packets being transmitted and buffered, power increases with the number of buffers. In this paper, we propose a high performance and power modest dual-switch allocation design. In order to make more packets being transmitted and buffered, the design becomes a combination of a primary switch allocation and a secondary switch allocation with no additional buffers, as the power consumption of buffers dominates the whole power of network. At low trafﬁc load, almost all packets use the primary switch allocation to assign their desired output port. Whenever there is a conﬂict, the packet which fails in the primary switch allocation will be assigned to other corresponding idle output port by the secondary switch allocation. The dual-switch allocation design enables blocked packets transmit through router via idle output port as far as possible, thus achieving high throughput and low latency. On the other hand, power overhead is very little, as there are no additional buffers and links in our design. This paper is organized as follows. In Section II, a brief overview of related work is presented. Proposed dual-switch allocation design is presented in Section III. Experimental setup and results will be demonstrated in Section IV. I I . R E LAT ED WORK Various techniques have been proposed to lead to more packets being transmitted and buffered, and then balance the congestion of interconnection network. J. Suseela and V. Muthukumar proposed a loopback virtual channel mechanism to improve the performance of a router [3]. This design can minimize latency by adding additional virtual channel, but with the cost of increased power consumption and complexity. Another approach dividing the router into two subnets is Network Processor Array (NePA) [4], which uses additional input ports including buffers and links in north and south output ports. In this case, NePA can separate and transmit the packets which desire north or south direction. However, its power consumption becomes larger due to increasing buffer and link. Other techniques were also proposed in recent years [5, 6, 7], however some of them were based on additional buffers to improve the network performance, so that power consumption remains as a challenging problem. In order to resolve this problem, we propose a dual-switch allocation design which can make full use of idle output ports to enhance the performance of latency and throughput. At the same time, power overhead will be a few. I I I . D E S IGN O F DSA ROU T ER A. Architecture In this Section, we will introduce the architecture of dualswitch allocation design (DSA). Our design can apply to both conventional and virtual channel router. In order to simplify the discussion, we eliminate the virtual channels in the rest of paper. Fig. 1(a) shows a baseline router which has ﬁve input ports and ﬁve output ports. When there are incoming ﬂits, they will be buffered ﬁrstly, and then the desired output port which              	        (a) Baseline    	        	        Fig. 1. The architecture of (a) Baseline (b) DSA router (b) DSA will be calculated by routing computation. Switch allocation will assign ﬂits to desired output. If the desired direction has been occupied by any other ﬂit, the ﬂit will be standby in buffer until the desired direction is available. At last, crossbar is controlled by the switch allocation for correctly connecting input ports to output ports. Fig. 1(b) demonstrates the router architecture of the proposed DSA. There are two allocations within each router. In order to guarantee the fairness of assigning, each allocation is based on round-robin method [1]. Our design uses a lookahead technique, which calculates the desired route direction for the next router, not for the current router [1, 8]. Firstly, all ﬂits are buffered. And then all ﬂits in each buffer will be assigned to their desired output ports by the primary switch allocation (PSA). If some of them fail allocation in PSA, they will continuously utilize the secondary switch allocation (SSA) to assign its direction according to the lookahead information which is calculated in the current router. B. Router Implementation Fig. 2 demonstrates pipeline stages in different router designs. Fig. 2(a) shows the router pipeline of baseline design whose stages are buffer writing (BW), routing computation (RC), switch allocation (SA) and switch traversal (ST) [1]. When there are incoming ﬂits, they are stored in input buffers at BW stage, and then routing computation is executed at RC stage according to head ﬂits. After that, SA will assign the desired output. Finally crossbar connects input port to output port according to SA at ST stage. In regard to a lookahead (LA) pipeline shown in Fig. 2(b), RC is done at the preceding router, and ﬂits can make SA stage when they are buffered in input ports. So no RC is independently needed for trans(a) Basic four-stage pipeline     (b) Lookahead three-stage pipeline 	    (c) DSA three-stage pipeline Fig. 2. Three pipelines architectures. The stages are buffer writing (BW), routing computation (RC), switch allocation (SA), primary SA (PSA), secondary SA (SSA), and switch traversal (ST). mitting the packet to a router neighboring the current router. In this case, we can improve router performance and reduce power consumption, as reducing pipeline stages from four to three. Fig. 2(c) shows the pipeline of the proposed DSA architecture. By utilizing a lookahead technique, all incoming head ﬂits buffered in input ports will make switch allocation by PSA immediately. If some of these ﬂits fail in PSA, then they will make switch allocation by SSA according to routing information which is calculated by RC at the current router. We adopt existing router components delay model which is described by Peh and Dally [9] to estimate the delay through our pipeline stages. Table I shows the delays of SA and ST pipeline stage achieved by baseline, LA and proposed router, respectively. The stage with the longest critical path delay will set the clock frequency. Since very simple deterministic routing algorithm is implemented in each design, the delay of BW and RC stages are a few and less than ST stage. Therefore, from Table I, we can easy get the delay of ST stage is longest critical path delay. The switch arbitration delay increase in DSA compared with baseline and LA design, as DSA operate two SA allocation (PSA and SSA). However, any clock cycle that accommodates the ST stage will also accommodate the increased SA delay in DSA design. As a result, the penalty of additional SA in DSA will hidden in router pipeline and router delay of DSA will no worse than LA. Furthermore, packets contention delay TH E D E LAY ( IN UN I T S O F τ ) O F P I P E L IN E S TAG E IN D I FF ER EN T D E S IGN S TABLE I Router Baseline(four-stage) Lookahead(three-stage) DSA(three-stage) † SA(τ ) 48.04 48.04 87.08 ‡ ST(τ ) 100 100 100 † ‡ the delay of an inverter with identical input capacitance [10] the tSA of DSA is consist of tP SA and tSSA 	      	 	 Fig. 3. The method of DSA. Outside the bracket is OLD-RI and inside the bracket is NEW-RI. The red arrow indicates the PSA and blue arrow indicate the SSA. enable reduce according to our proposed method (described in later). Fig. 3 shows the method of DSA. Firstly, we deﬁne OLD routing information (OLD-RI) which is calculated in the preceding router and carried by incoming head ﬂits. Then, NEW routing information (NEW-RI) is calculated by the current router and decide which output port is desired in the next router. In Fig. 3, the desired east direction is OLD-RI and north shown in bracket is NEW-RI which is calculated by Router 1. If the desired output port in east direction is occupied by other packet and we know this packet should be sent to north direction at the next router (Router 2) according to NEW-RI, thus in order to resolve the blocking of this packet, this packet is transmitted to north direction at Router 1, and then transmitted to east direction at Router 3. If packet is assigned to its desired direction by PSA, it is transmitted with NEW-RI (north) when leaving the current router. While a packet is assigned by SSA, it is transmitted with OLD-RI (east) when leaving the current router. As a result, the performance of network is improved, because of &$& ""'&$$(% !"" !&$""  !""$!$""'&$""$ ""#$""%%!	 &%!'$  %)&""&""!* ""$!&"" ""%&%%!&""&% %$""'&#'&#""$& *  $!% &&!&%$$*)& 	""'&#'&#""$&% %%!* ""'&#'&#""$&%%%!* &""%$$&""! 	! making utmost of idle output port and making more packets being transmitted and buffered. Fig. 4 shows the ﬂow chart of the proposed DSA. At ﬁrst, the head ﬂits which carries OLD-RI together are buffered in each input buffers. Then PSA assigns each ﬂit to its desired output port according to OLD-RI. During the assignment of PSA, routing computation unit calculates the NEW-RI. If switch allocation complement by PSA, crossbar connects input ports to output ports and then send ﬂits to the next router. In this case, when packet leaves the current router, it will carry NEW-RI information together. At low trafﬁc loads, each head ﬂit may enable to be assigned to their desired output port, as there is no contention between ﬂits. However, as the trafﬁc loads increasing, the preceding packet might block the succeeding packet which desires the same output port. Therefore, some of head ﬂits in input buffer will fail assignment in PSA. In this case, these failed head ﬂits will continuously make switch allocation in SSA according to the value of NEW-RI. If the NEW-RI direction is available for one head ﬂit, SSA assigns this packet to this direction. In other words, although the desired direction of the current router is unavailable, the router knows which direction the packet should be transmitted to in downstream router. Thus we can ﬁrstly transmit the packet to the direction which is desired in the next router if it is available, and then send it to the direction which is desired in the current router. Similarly, if switch allocation complement by SSA, crossbar will connect output port to input port and transmit packet to the next router, carrying OLD-RI. Otherwise, if SSA fails, head ﬂits will continue to be standby in input buffer for PSA, and repeat this process. We use a round-robin technique in PSA and SSA to guarantee the fairness. The result is idle output port can be made full use and improve the network performance as reducing the contention between ﬂits. Meanwhile, power overhead is very little, as only SA consumes a little powers (details are discussed in the Section IV). The timing of an incoming ﬂit is shown in Fig. 5. First, all incoming ﬂits arrive at input port and are buffered in input buffers. Second, PSA make switch allocation according to OLD-RI. Subsequently, SSA will make switch allocation for ﬂits which fail in PSA according to NEW-RI. At the same time, routing computation unit will calculate NEW-RI information for head ﬂit. After that crossbar connects input port to output port and transmit ﬂit to downstream router according to SA results. RC and DSA stage (consist of PSA and SSA) are executed in parallel.  &%)$ %%! !&!) ""!&!'""'%*  ""$!&""	 ""%&%%%!&"" &$%$""'&#'& #""$&*        ""!   ""!      	       	      Fig. 4. Flow chart of the proposed DSA Fig. 5. The timing of a ﬂit                       (a) There is no conﬂict                                    (b) Two ﬂits both desire the same output port                    (c) More than two ﬂits both desire the same output port (d) Flit both fail in PSA and SSA Fig. 6. The examples of DSA method C. Examples There are some different situations in DSA method. And how ﬂits are assigned for each situation is shown in Fig. 6. In our method, we utilize minimal routing algorithm [1], so we dont consider the router sending packet to back. If the routing direction of packet is local, we always assign it by PSA. In other words, if one packet which desires the local output port fails in PSA, SSA will not be used, and this packet will wait for next PSA. In Fig. 6, the letter before bracket is OLD-RI which is carried with incoming head ﬂits and letter in bracket is NEW-RI which is calculated by the current router. Fig. 6(a) shows the situation with no conﬂict. All ﬂits can be assigned to their desired output port by PSA. So when ﬂits leave current router, they will carry with the NEW-RI. For instance, the ﬂit in north input port has been assigned to south output port (S). And when it leaves current router, NEW-RI (E) will be carried with it. In this case, SSA is not utilized. Fig. 6(b) shows the situation when two ﬂits desire the same direction. The ﬂits in south and west input port want to be sent to east direction. Firstly, all ﬂits in each buffer will make allocation by PSA. Based on round-robin technique, if east output port is assigned to west input port (so it is unavailable for ﬂit in south input), the ﬂit in south input will fail in PSA and continuously utilize SSA according to NEW-RI (N). In this case, the NEW-RI direction (N) of ﬂit in south input is available, thus a router ﬁrst assigns this ﬂit to north output port by SSA, and in downstream router, it will desire east direction. Fig. 6(c) shows the situation more than two ﬂits desire the same direction. The OLD-RI of ﬂits in north, south and west input are east direction and east output port is assigned to the ﬂit in north input by PSA. These failed ﬂits will use SSA to complete assignment. In this case, the NEW-RI directions of ﬂits in south and west input (N and S) are available, so current router will transmit ﬂit in south input to north direction, and send ﬂit in west input to south direction. And then in downstream router, they will desire east output port. Fig. 6(d) shows the situation that some of ﬂits fail in PSA and SSA. We can see, ﬂits in north and west, east and south input desire east and west direction, respectively. North input port is assigned to east output port. And east input is assigned to west output port. So ﬂits in south and west input fail in PSA. Within SSA, the NEW-RI of these two ﬂits both are north direction. In this case, based on round-robin, if north output port is assigned to west input port, the ﬂit in south input also fails in SSA and it will wait in input buffer for next PSA. D. Deadlock and Fault Tolerance In order to avoid deadlock, we use a deadlock recovery scheme DISHA [11]. An extra ﬂit buffer is equipped at each router to store the head ﬂit of one of deadlock packet. We set T to keep track of the number of clock cycles. The value of T is increased as head ﬂit cannot be sent out. Whenever T is larger than the threshold Tth , recovery will be executed. The DSA design enables hardware-level fault tolerance [12, 13]. If the link between two routers is fault, PSA will fail according to OLDRI. However, router enables transmit packet by SSA according to NEW-RI. And the packet can forward to its destination. Our approach can deﬁnitely work on other virtual channel designs [1, 14]. As the same way, if all virtual channels in desired input port of downstream router are not available, the blocked packet enables utilize SSA to forward to the direction which is desired in the next router according to NEW-RI information. IV. P ER FORMANC E EVALUAT ION In this section, we make different experiments in order to evaluate the DSA design in terms of area, power consumption and the network performance. We compare our design with basic pipeline router, lookahead pipeline router and NePA [4] which utilizes lookahead method. Dimensional-order-routing (DOR) [1] is deployed in baseline, lookahead and NePA, respectively. All of the designs are implemented in 8×8 2D mesh topology. DOR is deployed in different designs. In order to guarantee the fairness of experiments, we set the total buffer sizes same for different designs. Each buffer size of NePA and DSA are set into 3 ﬂits and 4 ﬂits, respectively. Each input buffer size of baseline and LA are set into 4 ﬂits except local input buffer. And the local buffer size of them is 5 ﬂits. Therefore we can get the same total buffer sizes of different designs. In this experiment, we compare the performance of DSA with other designs in terms of latency, throughput and energy consumption. A. The Network Performance at Synthetic Trafﬁcs We evaluate the performance of network by using an open source simulator Noxim [15]. In our experiments, we ﬁrst execute a consecutive simulation where the injection rate is varied from 0.01 to 0.90 using random trafﬁc pattern in 8×8 2D mesh. Fig. 7(a) shows the performance of latency between different designs. We can see DSA is signiﬁcantly better than others. As trafﬁc loads increases, DSA gets average improvement by 38.8%, 29.6% and 26.5% compared with baseline design, LA and NePA, respectively. Fig. 7(b) shows the throughput of each design. After saturation point, the throughput of DSA reaches 0.5 and outperforms baseline, LA and NePA designs. Fig. 7(c) shows the energy consumption for the whole network at random trafﬁc pattern. Because NePA design adds two additional input ports including buffers, so more buffer operations such as writing and reading will be executed in NePA and then dynamic and leakage power of buffer will increase linearly [16]. From Fig. 7(c), at low trafﬁc loads, DSA consume almost same energy with baseline and LA designs, as PSA is utilized more frequently. However, as trafﬁc loads increasing, more packets blocking happens, in this situation, both PSA and SSA will be used. So the power consumption of DSA becomes a bit larger than baseline and LA designs. Although there is (a) The latency at random trafﬁc (b) The throughput at random trafﬁc (a) Throughput at injection rate = 0.5 (c) The energy consumption at random trafﬁc Fig. 7. The comparison of different designs at random trafﬁc (b) Energy consumption at injection rate = 0.5 Fig. 8. The throughput and energy consumptiom at injection rate = 0.5 of other synthetic trafﬁcs AR EA AND POW ER E S T IMAT ION FOR D I FF ER EN T D E S IGN S TABLE II Designs Baseline Lookahead (LA) NePA DSA 2 ) Area (mm 0.3084 0.3084 0.6016 0.3101 † ‡ only PSA is used both PSA and SSA are used Power (pJ/f lit) 245.18 245.07 460.05 245.09 (min) 247.54 (max) 246.31 (average) † ‡ more energy consumption in DSA, the average overhead is only 6.1%. And we also evaluate our design in other trafﬁc patterns such as bit-reversal, transpose and shufﬂe trafﬁc pattern. Fig. 8(a) shows the resultant throughput at the injection rate 0.5. Obviously, the throughput of DSA is signiﬁcantly better than others. Especially, in bit-reversal and transpose trafﬁc, the improvement of DSA is distinct. Fig. 8(b) shows the energy consumption under different trafﬁcs at 0.5 injection rate. From this ﬁgure we can know, there exist a few overhead of energy consumption in DSA design. B. Area and Power Estimation We use Orion 2.0 simulator [17] to estimate the area and power of router, with the setting of 65nm technology, 1GHz router at 1Vdd, and the ﬂits size is 128 bits. TABLE II shows the evaluation of area and power for different designs. In regard to area, we can see the design of NePA occupies the largest area compared with others, as it has much more input buffers and a crossbar has more complexity. It almost has two times of area compared with baseline and LA router. However, the area of our design has only a bit larger compared with baseline and LA router, because we only add an additional switch allocator which occupies very small area and a ﬂit buffer in our design. On the other hand, obviously, the power of NePA is also the largest one among different designs, because the power is increased with the number of input buffers. The power of buffer dominates the power consumption of whole router. For DSA design, there are two cases. One is that the ﬂit passes through a router assigned only by PSA, that is, SSA is not used. In this case, the power consumption is minimal and it is almost same with LA. Second is the case where the ﬂit ﬁrstly fails in PSA, and then it is forwarded to downstream router by SSA. In this case, both PSA and SSA are used, so it shows the maximum power consumption. From TABLE II, both max power and average power of DSA are larger than that of baseline and LA design, but it is very little. V. CONC LU S ION S In modern Network-on-Chip, more and more cores can be deployed. And as the trafﬁc loads increasing, packets may be blocked more frequently. In order to reduce blocking, additional buffers are utilized to improve latency and throughput. However, the power consumption will be increased obviously. To improve the network performance under the less overhead power, this paper proposes a dual-switch allocation network (DSA). It allows packets assign their desired output port by the primary switch allocation ﬁrstly. If some of packets fail in the primary switch allocation, they enables continuously utilize the secondary switch allocation to assign their desired directions. The router can make utmost use of idle output port, as much as possible, to improve the network performance. Experimental results show that our design has signiﬁcant improvement compared with baseline, LA and NePA designs in terms of latency and throughput, at the cost of a small overhead power. "
2015,Fine-grained runtime power budgeting for networks-on-chip.,"Power budgeting for NoC needs to be performed to meet limited power budget while assuring the best possible overall system performance. For simplicity and ease of implementation, existing NoC power budgeting schemes, irrespective of the fact that the packet arrival rates of different NoC routers may vary significantly, treat all the individual routers indiscriminately when allocating power to them. However, such homogeneous power allocation may provide excess power to routers with low packet arrival rates whereas insufficient power to those with high arrival rates. In this paper, we formulate the NoC power budgeting problem as to optimize the network performance over a power budget through per-router frequency scaling, taking into account of heterogeneous packet arrival rates across different routers as imposed by run time traffic dynamics. Correspondingly, we propose a fine-grained solution using an agile dynamic programming network with a linear time complexity. In essence, frequency of a router is set individually according to its contribution to the average network latency while meeting the power budget. Experimental results have confirmed that with fairly low runtime and hardware overhead, the proposed scheme can help save up to 50% application execution time when compared with the best existing methods.","Fine-Grained Runtime Power Budgeting for Networks-on-Chip Xiaohang Wang∗ † , Tengfei Wang∗ , Terrence Mak∗ ‡ , Mei Yang§ , Yingtao Jiang§ , Masoud Daneshtalab¶ ∥ ∗Guangzhou Institute of Advanced Technology, CAS, China Email: xh.wang@giat.ac.cn †Shenzhen Institute of Advanced Technology, CAS, China ‡The Chinese University of Hong Kong, China §University of Nevada, Las Vegas, USA Email: stmak@cse.cuhk.edu.hk Email: mei.yang@unlv.edu, yingtao@egr.unlv.edu ¶Royal Institute of Technology (KTH), Sweden ∥University of Turku, Finland Email: masdan@utu.ﬁ Abstract— Power budgeting for NoC needs to be performed to meet limited power budget while assuring the best possible overall system performance. For simplicity and ease of implementation, existing NoC power budgeting schemes, irrespective of the fact that the packet arrival rates of different NoC routers may vary signiﬁcantly, treat all the individual routers indiscriminately when allocating power to them. However, such homogeneous power allocation may provide excess power to routers with low packet arrival rates whereas insufﬁcient power to those with high arrival rates. In this paper, we formulate the NoC power budgeting problem as to optimize the network performance over a power budget through per-router frequency scaling, taking into account of heterogeneous packet arrival rates across different routers as imposed by run time trafﬁc dynamics. Correspondingly, we propose a ﬁne-grained solution using an agile dynamic programming network with a linear time complexity. In essence, frequency of a router is set individually according to its contribution to the average network latency while meeting the power budget. Experimental results have conﬁrmed that with fairly low runtime and hardware overhead, the proposed scheme can help save up to 50% application execution time when compared with the best existing methods. INTRODUC T ION I . A highly integrated many-core chip is prone to extremely high power consumption that exceeds the chip’s power budget set forth by cooling, packaging and/or capacity of the power supply. When this situation happens, various parts of the many-core chip will have to be deliberately switched off (darkened) so that the chip is still in compliance with its power budget, which is referred as “dark silicon”. It has triggered a paradigm shift from conventional low power design to power budgeting design that attempts to optimize the system performance under a power budget. In the literature, various online power budgeting approaches [1], [2] have been suggested for many-core chip systems that explore circuit techniques like DVFS and/or power gating. These circuit techniques are particularly applied to one important component of a many-core chip system, the networkon-chip (NoC) [3]–[5], since NoC is one key contributor to This research program is supported by the Natural Science Foundation of China No. 61376024 and 61306024, Natural Science Foundation of Guangdong Province No. S2013040014366, and Basic Research Programme of Shenzhen No. JCYJ20140417113430642. the system power consumption and also one major limiter to the performance [6]. Basically, a few key factors should be considered for power budgeting in NoCs: Trafﬁc ﬂows are unevenly distributed across the network. For example, in Fig. 1, two separate ﬂows with ﬂow rates of 50 Gbps and 100 Gbps are present. Assume ﬂit size is 128 bits, the processors work at 1GHz, and each router can operate at one of the two frequencies, either 1 GHz or 500 MHz. If a coarse grained control scheme is applied, all the routers then have to operate at the same frequency, say 1 GHz in this example, which obviously will cause excess power consumption for the slower ﬂow f1 . On the other hand, if we set the router frequency to 500 MHz, the 100 Gbps bandwidth request for ﬂow f2 is not satisﬁed. Actually, one can immediately conclude from Fig. 1 that an ideal solution would be setting routers 0 and 1 to run at 500 MHz, and routers 2 and 3 at 1 GHz. The trafﬁc variance further causes difference in the packet arrival rate at each router. If we deﬁne the packet arrival rate of a router as the sum of all its output ports’ arrival rates [7], packet arrival rate rarely maintains a constant value as the case in Fig. 1. More likely, due to uneven distributions of the data generated and consumed for a given application, a router’s packet arrival rate can be signiﬁcantly different from those of other routers engaging in the data communications. Fig. 2 shows the distributions of router packet arrival rates for running two benchmarks, ﬂuidanimate and ferret. As of ﬂuidanimate, about 40% of the routers have a packet arrival rate of 0.03 ﬂit/cycle, and only 2% of them have 0.06 ﬂit/cycle. As such, the router service rates should be better tuned according to their individual packet arrival rates to achieve a low network latency. Tuning router service rates can be done by per-router frequency scaling. Existing coarse-grained power budgeting schemes [3]–[5], however, tune the frequencies/voltages and service rates of all the routers homogeneously, ignoring the heterogeneous nature of the router packet arrival rates. One big problem of NoC power budgeting based on perrouter frequency scaling lies in the fact that its complexity grows exponentially with the network size. In a simple word, the number of frequency combinations increases exponentially. For example, in a fairly modest NoC system with only 64 routers, if each router can operate at one of the two available frequency levels, the total number of frequency combinations is as large as 264 (around 2 × 1020 ). Ad-hoc Fig. 1. An example to illustrate that a coarse-grained power budgeting for all routers is inefﬁcient. Each router can work at either 1GHz or 500 MHz. Ideally, routers 0 and 1 can operate at 500 MHz to satisfy the bandwidth request of f1 with low power, while routers 2 and 3 can operate at 1GHz to satisfy the bandwidth request of f2 . Fig. 2. Distributions of the router packet arrival rates (histograms) for two different benchmarks: ﬂuidanimate (left) and ferret (right). approaches like [3], [8] or network-wide power management schemes [4], [5] fail to ﬁnd low latency scenarios in dealing with such a vast solution space. Consequently, they have to live with degraded overall performance. To address the aforementioned challenges, we propose an agile runtime power budgeting scheme for NoC, named Fine-grained Runtime Power budgeting for NoC (FiRuP). This scheme has two major steps: (1) network performance estimation and (2) power budgeting. The key contributions of this work are threefold. • Network performance is estimated using an extended analytical latency model by adding the router frequency scaling into the model used in [7]. • Based on the average latency model, the NoC power budgeting problem is formally formulated as to optimize the network latency under the power budget constraint, • The above problem is solved by an agile dynamic protaking into account per-router frequency scaling. gramming network with linear complexity and low cost. Low network latency can be achieved given the power budgets. This paper is organized as follows. Section II reviews related work. The NoC power budgeting problem is formally deﬁned in Section III. Section IV solves this problem using a dynamic programming network. Section V presents the experimental results. Finally, Section VI concludes the paper. I I . R ELATED WORK As an emerging design paradigm to battle escalating power consumption of complex many-core chips, power budgeting design [1], [9], [10] is gaining its momentum. Targeted to optimize the overall system performance under a limited power budget, this paradigm is a vast departure from the conventional design approaches that merely minimize online power consumption [11]. Most of the above approaches target power budgeting for the processors only. However, NoC has been an increasingly important contributor to the Fig. 3. Illustration of a ﬂow from node S to node D . overall system performance and power consumption [6]. Of the limited NoC power budgeting approaches found in the literature [3]–[5], they can be broadly classiﬁed into two types, 1) network-wide and 2) per-router/link. Given a power budget, methods in the ﬁrst type try to throttle trafﬁc to enter the network or turning down the frequency or voltage of the whole network to meet the network power budget. As these networkwide or even chip-wide approaches overlook the difference in the routers’ packet arrival rates, they might result in poor performance. To improve the control granularity, a few NoC power minimization approaches [12], [13] adopt per-router or per link control. For example, the frequency or voltage of each router can be adjusted, if doing so can reduce the power consumption. Routers/links can even be power-gated to save leakage power, if they are predicted to be inactive for a certain time period [5]. In case of misprediction, a backup path is set up that allows trafﬁc to bypass those inactive routers. However, these approaches have different optimization objectives than the power budgeting design as described in this paper; rather they try to minimize power consumption under a performance constraint. Therefore, they cannot guarantee good performance under a power budget. The optimization objective in NoC power budgeting can be the network performance (e.g., latency). There have been a few studies that attempt to model network latency analytically, based on different queuing models. The worst-case latency analyses can be found in [14], [15], and average latency models can be found in [7], [16], [17]. In this paper, we extend the latency model used in [7] by including router frequency scaling. I I I . THE NOC POW ER BUDG E T ING PROB LEM FORMU LAT ION The objective of the NoC power budgeting is to optimize the average latency under a power constraint, which corresponds to minimizing the overall system execution time. Latency models can be derived from incorporating router frequency scaling effects into an existing analytical latency model in [7]. In essence, the zero load router traversal latencies and router/channel service rates must be updated according to the frequency of each router. A. The NoC Average Latency Model with Router Frequency Scaling As in Fig. 3, given a ﬂow from the source node s to the destination d with a ﬂit arrival rate (cid:21)s!d , the average latency for the head ﬂit is [7], ∑ ∑ Ls!d h = tin + tZL i + W k!j i + te (1) i2Ωs!d i2Ωs!d tZL i where tin and te are the injection and ejection waiting times, is the zero load traversal time through router i, Ωs!d is the routing path of this ﬂow, W k!j is the waiting/queuing time in router i with input port k to output port j . i Proportional to the router frequency fi , zero load traversal time of a router tZL includes the route computing time tr , switching time ts , and wire traversal time ts . The waiting/queuing time at each router i is [7], i W k!j i = (cid:21)i j (C 2 A + C 2 m!j ) S j ) k m=1 (cid:21)i 2 2((cid:22)i j − ∑ where (cid:21)i j is the total arrival rate at the output port j , C 2 A is the coefﬁcient of variance of the arrival rate, (cid:22)i j is the service rate of the output port j , C 2 S j is the coefﬁcient of variance of the service rate of the output port j , (cid:21)i m!j is the arrival rate of packet from input port m to output port j . Suppose the router frequencies can take discrete values fi ∈ {(cid:28)1 ; (cid:28)2 ; : : : ; (cid:28)m }. The average latency can be written as [7], P s!d (cid:2) Ls!d ∑ ∑ ∑ d ai (cid:1) fi+ i k j ((cid:22)i j ∑ i (3) Li (fi ) bijk (cid:1) fi (cid:0) cijk )2 = ∑ ∑ ∑ s L = = i where P s!d is the probability of sending packets from source s to destination d. The notations ai , bijk , cijk , and Li (fi ) in Eqn. (3) are deﬁned as follows. ∑ d ∑ ∑ s ai ≡ ∑ P s!d · tZL i bijk ≡ P s!d× (cid:21)i j (C 2 A + C 2 2 S j ) s d cijk ≡ ∑k ∑ m=1 (cid:21)i m!j Li (fi ) ≡ ai × fi + bijk · fi − cijk )2 ((cid:22)i j j;k (2) (4) (5) (6) (7) Note that ai , bijk , and cijk are independent of the router frequency, and they can be computed periodically. The latency model in Eqn. (3) is updated periodically to capture the variation in the ﬂow trafﬁc rate, corresponding to the arrival rates. B. Problem Formulation Of a total of n routers in a many-core system, the decision variables are the respective frequencies denoted as f1 ; f2 ; : : : ; fn . Assume each router can operate on any of the m frequency levels, fi ∈ {(cid:28)1 ; (cid:28)2 ; : : : ; (cid:28)m }. We assume a router power model similar as in [18], where the dynamic power is related to the frequency. The per-ﬂit router power consumption [18] at each frequency level then is given as b(fi ) = bj , if fi = (cid:28)j , for 1 ≤ j ≤ m. Using the latency model given in Eqn. (3), we can deﬁne the power budgeting problem. The NoC power budgeting problem aims to optimize the overall network performance under the power budget constraint as follows. Given n routers whose frequencies can be selected from a ﬁnite set {(cid:28)1 ; (cid:28)2 ; : : : ; (cid:28)m }, a power budget PN , the network performance given by Eqn. (3), ﬁnd the frequency assignments for all the routers, in order to optimize the network performance. Mathematically, the problem is formulated as, ∑ ∑ ∑ ∑ Fig. 4. Overview of the system. min L = i subject to ai × fi+ n∑ i j k ((cid:22)i j bijk · fi − cijk )2 (8) i=1 (9) wi · b(fi ) ≤ PN − Pl for each fi ∈ {(cid:28)1 ; (cid:28)2 ; : : : ; (cid:28)m } and (cid:28)1 ≤ (cid:28)2 ≤ : : : ≤ (cid:28)m . where wi is the number of ﬂits passing through router i within a given time unit. Eqn. (9) ensures that the total power consumption is within the given budget PN which is a function of time t. Power budgets can be measured leakage power of the whole NoC. We denote PN − Pl as P , or predicted using the methods described in [19]. Pl is the i.e., the upper bound for NoC dynamic power consumption. PN can be determined by performing a system-lvel power budgeting to divide the system power budget into those of processors, caches, and NoC. To solve the above problem efﬁciently with low complexity, we use a dynamic programming network (DPN) for real time computation, as introduced in next section. (cid:3) IV. SOLV ING THE POW ER BUDG ET ING PROB LEM A. Overview The proposed FiRuP approach has two major steps. 1) For the network performance estimation, the necessary parameters in Eqn. (3), e.g. P s!d , (cid:21)s!d , ﬁrst and second moments of the arrival rates/service rates, etc., need to be monitored and computed online. 2) The NoC power budgeting problem is solved using an agile dynamic programming network (DPN) with a linear complexity. Finding the router frequency settings is equivalent to ﬁnding an weighted longest path in the DPN. B. Deﬁnition of the Dynamic Programming Network Deﬁnition 1. Dynamic programming network. A dynamic graph D(V ; A), with V and A representing the sets of vertices programming network for power budgeting is denoted as a and edges, respectively, as in Fig. 5. • D(V ; A) has m × (n + 1) vertices, corresponding to m available frequencies and n routers. Each column of m • Two dummy vertices, s and d are added before the ﬁrst vertices is called a stage, and in total, there are n stages. stage and after the last stage. The edges connecting s to the vertices in stage 1 and those connecting the vertices in stage n to d have a weight of 0. (cid:3)Note that the DPN used in adaptive routing [20] targets a different problem and thus has a different structure. It cannot be used for the purpose of power budgeting directly. The forward pass will be performed only once for an NoC system. In the backward pass, the two dynamic programming values G (vi;j ; d) and V (vi;j ; d) of each vertex vi;j at stage i can be computed by a stage-by-stage manner from d to s. At the i-th stage, the following two recursions are calculated simultaneously, {CP (vi;j ; vi+1;k ) + G(vi+1;k ; d)}; G(vi;j ; d) = min 8vi+1;k V (vi;j ; d) = fCV (vi;j ; vi+1;k ) + V (vi+1;k ; d)g 8vi+1;k AND J (s;vi;j )+CP (vi;j ;vi+1;k )+G(vi+1;k ;d)(cid:20)P min for 1 ≤ j ;k ≤ m. (13) The term J (s; vi+1;k ) + CP (vi;j ; vi+1;k ) + G(vi+1;k ; d) ≤ J (s; vi+1;k ) + CP (vi;j ; vi+1;k ) + G(vi+1;k ; d) is the minimum indicates the power budget constraint. P in Eqn. power consumption of the whole network with fi = (cid:28)k . If it exceeds the power budget P , the path will be discarded, i.e., fi cannot be set to (cid:28)k . The initial conditions at stage n are given as, (12) (13) (14) (15) (16) G (vn;j ; d) = 0; V (vn;j ; d) = 0; for 1 ≤ j ≤ m. If this computation is expanded from d to s, then the dynamic programming value of the whole path can be computed in n steps, (cid:3) V (s; d) = min {i=n(cid:0)1∑ (vi;j ;vi+1;k )2Ω(s;d) AND j=m∑ k=m∑ J (s;vi;j )+CP (vi;j ;vi+1;k )+G(vi+1;k ;d)(cid:20)P CV (vi;j ; vi+1;k )} i=1 j=1 k=1 where Ω(s; d) is the set of all the paths from s to d. The weighted longest path from vi;j to a vertex at stage i can be determined as, (cid:22)(vi;j ) = arg {CV (vi;j ; vi+1;k ) + V min (vi+1;k ; d)} (cid:3) 81(cid:20)k(cid:20)m AND J (s;vi;j )+CP (vi;j ;vi+1;k )+G(vi+1;k ;d)(cid:20)P (17) vi;j will choose the path connecting to vi+1;k = (cid:22)(vi;j ), corresponding to assigning fi (the frequency of the router i) to be (cid:28)k . The pseudo-code in Algorithm 1 shows the parallel computation of the Bellman equations by dynamic programming network given in Eqn. (10)-(17), which is equivalent to ﬁnding the minimum weight path from s to d in parallel. Given that the network size is n and there are m available frequency levels, the forward pass takes n cycles in traversal, and is performed only once. The backward pass also takes n cycles. Thus, the runtime overhead of the approach takes only n cycles, i.e., the time complexity is O(n). Each router i has dynamic programming network vertices from the stage i. The dynamic programming network has a size of n × m vertices done in one cycle. Thus, the space complexity is O(n × m). involving simple add and compare operations, and could be In a many-core system, apart from NoC, both the processor cores and cache banks contribute to the total power consumption. To perform a system-level power budgeting, we can adopt a simple heuristic as adopted in [2]. Fig. 5. A parallel dynamic programming network to solve the power budgeting problem. In total, there are m × (n + 1) vertices, where m and n are the numbers of available frequency levels and network size. Each stage has m vertices. • Each vertex vi;j has three dynamic programming values: ◦ J (s; vi;j ) representing the minimum power consumption from s to vi;j , corresponding to the minimum power consumption by assigning f1 ; : : : ; fi(cid:0)1 ; ◦ G(vi;j ; d) representing the minimum power consumption from vi;j to d, corresponding to the best power ◦ V (vi;j ; d) representing the best network performance consumption by assigning fi ; : : : ; fn ; • For fi ∈ {(cid:28)1 ; (cid:28)2 ; : : : ; (cid:28)m }, each vertex at stage i is (Eqn. (8)) after assigning fi ; : : : ; fn . connected to all the m vertices at the next stage i + 1, corresponding to assigning the m available frequencies to the router i. An edge exists between two vertices, vi;j and vi+1;k for 1 ≤ j ;k ≤ m. Each edge has two weights: ◦ A power consumption weight CP is the power consumption of assigning fi (the fre◦ A performance weight CV quency of router i) to be (cid:28)k ; Eqn. (7)) is the performance after assigning fi (the frequency of router i) to be (cid:28)k . = Li ((cid:28)k ) (see vi;j ; vi+1;k vi;j ; vi+1;k vi;j ; vi+1;k = bk ( ) ( ) ( ) C. Finding Power Budgeting Solutions With the above dynamic programming network, the performance optimization problem in Eqns. (8) - (9) can be deﬁned in the form of Bellman equations recursively. Finding a low latency power budgeting solution (per-router frequency setting) is equivalent to ﬁnding a weighted longest path from s to d in D(V ; A) under the power budget constraint. Two traversals are involved in solving the problem: a forward pass from s to d and a backward one in reverse order. In the forward pass, the dynamic programming value J (s; vi;j ) from s to each vertex vi;j at stage i can be computed by a stage-by-stage manner from s to d. At the i-th stage, the recursion is given as, {CP (vi;j ; vi+1;k ) + J (s; vi;j )}; J (s; vi+1;k ) = min8vi;j for 1 ≤ j ;k ≤ m. The initial condition at stage 0 is given as, (10) (11) for 1 ≤ j ≤ m. J (s; v1;j ) = 0; Algorithm 1: NoC Power Budgeting Input: CP (vi;j ; vi+1;k ), CV (vi;j ; vi+1;k ): the weights edge (vi;j ; vi+1;k ), for 1 ≤ i < n and 1 ≤ j; k ≤ m. of power consumption and performance of each (cid:3) (vi;j ; d): the best dynamic programming value for vi;j . Function: Find an edge connecting each vertex vi;j to a vertex at the next stage, corresponding to the frequency assignment of the router i Output: V begin /* The forward pass (performed only once) */ for each stage i from s to n − 1 do for each vertex vi;j parallel do for each edge (vi;j ; vi+1;k ) parallel do if J (s; vi+1;k ) > J (s; vi;j ) + CP (vi;j ; vi+1;k ) then J (s; vi+1;k ) = J (s; vi;j ) + CP (vi;j ; vi+1;k ); end end end /* The backward pass for each stage i from n − 1 to s do for each vertex vi;j parallel do for each edge vi;j ; vi+1;k ) ( ) ( ( parallel do vi+1;k ; d if G (vi;j ; d) > G + CP (vi;j ; vi+1;k ) then G (vi;j ; d) = G (vi+1;k ; d) + CP (vi;j ; vi+1;k ); if J (s; vi;j ) + CP + G (vi+1;k ; d) ≤ P AND V (vi;j ; d) > + CV (vi;j ; vi+1;k ) then V vi;j ; vi+1;k ( ) ) vi+1;k ; d (cid:22)(vi; j ) = vi+1;k ; ( fi+1 = (cid:28)k ; V (vi;j ; d) = V vi+1;k ; d ) ( + CV vi;j ; vi+1;k */ ) ; end end end end V. EX P ER IM EN TAL R E SU LT S A. Experimental Setup Experiments are performed using an event-driven manycore simulator [21]. Table I lists the simulator conﬁguration. The Orion 2.0, Cacti, and McPAT libraries are integrated into the simulator for computing the power consumption of interconnection, caches, and processors, respectively. The power and area of the DPN are extracted from RTL level synthesis using a 65 nm CMOS technology. We then fed the DPN power consumption data into the simulator. Both run time and power consumption of computing the latency model (Section III) are recorded in the simulation. Table II lists the benchmarks for performance evaluation, and they are selected from PARSEC and SPLASH-2. In the following experiments, we select an 8 × 8 2D mesh with XY routing as the underline NoC architecture. We compare the performance TABLE I. CON FIGURAT ION U SED IN THE S IMU LAT ION Number of processors Fetch/Decode/Commit size ROB size L1 D cache (private) L1 I cache (private) L2 cache (shared) MESI protocol 64 (MIPS ISA 32 compatible) 4 / 4 / 4 64 16KB, 2-way, 32B line, 2 cycles, 2 ports, dual tags 32KB, 2-way, 64B line, 2 cycle 64KB slice/node, 64B line, 6 cycles, 2 ports Main memory size Frequencies available NoC ﬂit size Data packet size Meta packet size NoC latency NoC VC number NoC buffer Routing algorithm 2GB, latency 200 cycles 1GHz, 800MHz, 500MHz, 330MHz On-chip network parameters 72-bit 5 ﬂits 1 ﬂit router 2 cycles, link 1 cycle 4 5 (cid:2) 12 ﬂits XY routing TABLE II. B ENCHMARK S U S ED IN TH E S IMU LAT ION PARSEC SPLASH-2 streamcluster, swaptions, ferret, ﬂuidanimate, blackscholes, freqmine, dedup, canneal, vips barnes, raytrace Fig. 6. Average error of the proposed latency model. of the proposed approach FiRuP with two best existing NoC power budgeting schemes, (1) NoRD [5], where routers with low trafﬁc load are power gated, and (2) SAPP [3], where the ﬂit injection rate at each node is controlled according to the network power budget. NoRD and SAPP are coupled with the same system-level power budgeting. B. Validating the Network Performance Model Fig. 6 shows the error of the proposed latency model. For each benchmark, 10 experiments are run where each router’s frequency is set randomly. The error of a single experiment is deﬁned as, | |LA − LS LS "" = × 100% (18) where LA and LS are the latencies obtained from the proposed latency model and simulation, respectively. Fig. 6 shows the average error of the proposed latency model against the simulation result. All of the errors are within 8 %, which indicates the proposed latency model is a good approximation of the actual network performance. C. Performance Evaluation Fig. 7 demonstrates the performance of different NoC power budgeting methods, given two scenarios, power budgets of 90W and 50W. The execution times are normalized with respect to FiRuP. From Fig. 7, FiRuP records an average of 51 % and 47 % reduction over NoRD and SAPP in execution time, for the case where power budget is 90W. The average packet latencies of NoRD and SAPP are 2:2× and 1:9× over that of FiRuP, respectively. In FiRuP, routers the application execution time by 50% over a power budget, compared to other best known methods. Besides low run time penalty, the proposed approach also introduces a low hardware overhead (only a 0.56 % area increase of the whole NoC), rendering it as a suitable scheme for power budgeting in future NoC based many-core systems. "
2016,"Work hard, sleep well - Avoid irreversible IC wearout with proactive rejuvenation.","Various wearout mechanisms have both a reversible and an irreversible (permanent) part, with some, like BTI and EM having a significant reversible part, while others, like HCI, being mostly irreversible. In this paper we make two contributions. First, we show that the boundary between the reversible and irreversible parts of wearout is not fixed, with the irreversible part becoming at least partially reversible under the right conditions of active accelerated recovery and stress/recovery scheduling. Second, we show that there are certain stress/recovery schedules that can (almost) completely eliminate irreversible wearout, thus allowing significant reductions in necessary design margins. The experiments were done on commercial FPGAs fabricated in a 40nm technology. To fully repair and avoid the irreversible wearout, we propose a biology-inspired sleep-when-getting-tired strategy. The strategy can achieve >60× design margin reduction and ~9% average performance improvement within a 10-year lifetime constraint compared to the no-recovery case. Potential system level implementations (a negative “turbo-boost” like strategy) in multicore and NoC systems are also presented.","Work hard, sleep well - Avoid irreversible IC wearout with proactive rejuvenation  Xinfei Guo and Mircea R. Stan  Department of Electrical and Computer Engineering, University of Virginia, USA  {xg2dt, mircea@virginia.edu}  7A-4 Abstract – Various wearout mechanisms have both a reversible  and an irreversible (permanent) part, with some, like BTI and  EM having a significant reversible part, while others, like HCI,  being mostly  irreversible. In  this paper we make  two  contributions. First, we show that the boundary between the  reversible and irreversible parts of wearout is not fixed, with the  irreversible part becoming at least partially reversible under the  right  conditions  of  active  accelerated  recovery  and  stress/recovery scheduling. Second, we show that there are  certain stress/recovery schedules that can (almost) completely  eliminate  irreversible wearout,  thus allowing  significant  reductions in necessary design margins. The experiments were  done on commercial FPGAs fabricated in a 40nm technology. To  fully repair and avoid the irreversible wearout, we propose a  biology-inspired sleep-when-getting-tired strategy. The strategy  can achieve >60x design margin reduction and ~9% average  performance improvement within a 10-year lifetime constraint  compared to the no-recovery case. Potential system level  implementations (a negative “turbo-boost” like strategy) in  multicore and NoC systems are also presented. I. Introduction Wearout (or aging) has grown to be a huge reliability threat  to the lifetime of circuits and systems as technology scaling  continues [1][6] and emerging devices appear [2]. In the  front-end of line (FEOL), Bias Temperature Instability (BTI)  [3] and Hot Carrier Injection (HCI) [4] are two prominent  wearout effects that shift the threshold voltage of transistors,  thus degrading performance. Similar shifts happen in the  back-end  of  line  (BEOL)  for  interconnect, with  Electromigration  (EM)  [5] being  the main wearout  phenomenon that shortens the lifetime of the metal wire, and  becomes severe due to power delivery challenges for large  integrated circuits [6]. While these physical phenomena that  characterize wearout are at the device/materials levels, their  effects are apparent across the system stack, with timing or  transient faults at the circuit level, errors at the architecture  level and failures at the system level [1][7].  Although each wearout effect has been extensively studied  [3][4][5][7], there are still controversies about the exact  physical mechanisms that lead to the wearout phenomena. It  has long been accepted that BTI [3] and EM [5] are partially reversible wearout effects that could be partially recovered  when stress (e.g. voltage stress or current stress) is removed.  HCI is viewed as an irreversible wearout phenomenon since  the recovery within a reasonable time is negligible [4], so it  will accumulate during the lifetime of the system. The partial recovery property of wearout phenomena can be used to  improve the lifetime and other metrics (e.g. performance) of  the systems. At the circuit level, several methods [8][9][10]  were proposed to rebalance the signal probabilities to  maximize the recovery time. At the architecture level, [11]  introduced a proactive approach by shutting down the cache  bank periodically to recover memory cells. Several novel  schemes were proposed to exploit the idle time of busy  functional units for out-of-order processors and superscalar  processors [12]. An alternative method was to adaptively tune  the performance according to the degree of wearout so that  certain blocks could start the recovery phase [13][14]. Also,  several recovery boost techniques were proposed to accelerate  the recovery of the reversible wearout by applying high  temperatures [15][16] and/or negative voltages [11][16].  While the fact is that even for these reversible wearout  mechanisms, like BTI, there is still an irreversible part that is  hardly recovered within a reasonable time [4][7][17][18]. Fig.  1 shows a typical threshold voltage shift due to BTI under AC  stress, when  the  transistor  is switched ON and OFF  periodically, the net threshold voltage shift that is caused by  the irreversible component accumulates cycle by cycle, and  this worsens performance and requires larger design margins.   ON r t t i o e s a s n S a t OFF r T h t V (cid:168) St re ss Reco very St re ss Reco very St re ss Reco very Vth Net Increase T ime Fig. 1. Illustration of threshold voltage shift during AC Stress Several works [17][18] have studied the irreversible  component of wearout (especially BTI) at the device level.  However, these only focus on demonstrating and modeling  the permanent component,  thus a solution  that could  fundamentally repair the irreversible wearout is still missing  in the field. In this paper, we aim to answer two questions -  The first one is whether the boundary between reversible and  irreversible wearout is fixed or not, and if not, how to shift  the boundary and allow (part of) the irreversible wearout to  become reversible; the second one is how to run the system in  such a way as to avoid the irreversible wearout so that it can  always keep almost “fresh”. To explore  these  issues,  extensive stress tests on actual hardware (40nm FPGAs) are  conducted. Test results show that the boundary between  reversible and irreversible wearout is indeed soft and can be  tuned by changing temperatures and/or voltages, thus part of the irreversible wearout can be recovered by applying several  accelerated rejuvenation techniques [16]. To fully recover the  chip irreversible stress needs to be totally avoided, a  sleep-when-getting-tired  strategy  that  combines  the  accelerated rejuvenation techniques and proactive periodic  scheduling methods is proposed for this purpose. We show  that the irreversible part of the wearout can be almost fully avoided and delayed explicitly even under accelerated stress  conditions. With the system being rejuvenated periodically  and fully recovered to a near fresh state, the design margin  can be significantly reduced. Based on the model extracted  from the measurement results, the proposed strategy can  achieve a more than 60x design margin reduction and a ~9%  978-1-4673-9569-4/16/$31.00 ©2016 IEEE 649   average performance improvement with a 10-year lifetime  constrain under normal operation conditions.   II. Fast Traps vs. Slow Traps – A Physics Perspective It has been widely accepted that BTI is caused by trap  generations at the Si-SiO2 interface or in gate dielectrics of  the transistors when under stress [3][4][7][15][16]. HCI is a  similar phenomenon where charge carriers at drain side gain  sufficient kinetic energy to overcome a potential barrier  necessary to break an interface state [4][7]. The presence of  such mobile charges triggers numerous physical damage  processes that can drastically affect the device parameters,  such as increase in threshold voltage Vth, thus decrease in  circuit performance and reduction of noise margin, and might  lead to timing errors and failures [3][7][9]. When the stress is  removed, some of the traps are de-trapped (annealed), and the  number of occupied traps reaches a new equilibrium, thus  leading to recovery. Illustrated in Fig. 2 is the process of  trapping and de-trapping.  Ga te Ox ide Traps Source Cha rg e  Car r ie r Trapping De-Trapping Drain Body (cid:2) Fig. 2. Illustration of the trapping and de-trapping process  Here we define fast traps as those traps that have a high  probability of trapping the charge carrier. These traps have a  relatively lower trap energy barrier and are easier to be filled  in a shorter time. On the contrary, slow traps are those have a  higher trap energy barrier that is difficult for charge carriers  to overcome. The principle of physics for detrapping  (recovery process) is that the trapped charge carriers (e.g.  electrons for NMOS) have a certain probability to escape,  with the probability being higher if their energy is higher and  the trap energy barrier is lower, and vice-versa.  Based on  the statistical mechanics theory, the distribution of kinetic  energies is proportional to the product of density of state and  the Boltzmann distribution [20]. The 3-dimensional density of  state is proportional to E , therefore the energy distribution of  electron is given by  − E kT ) )1( kT 2/3 × × E ×= A ) exp( Ef E (            (1)  where E is the energy of the electrons, k is Boltzmann  constant, T is temperature in Kelvin and A is a normalization  factor. Since the de-trapping rate is proportional to the  number of electrons at the energy of consideration, the  de-trapping rate  is proportional  to  fE(E). The energy  distribution of electrons at room temperature (300K) is  plotted in Fig. 3(a), which shows that majority of the  electrons are at low energy in meV range, whereas the center  energy of even the lowest energy of the trap is in order of  several kT (~ 0.026eV) [4][21]. This means that only a small  fraction of electrons at the tail of the distribution could  participate in the de-trapping process.   Shown in Fig. 4 is the illustration of the trapping and  detrapping process for two types of the traps. Since fast traps have lower trap energy barrier, so it is easier for charge  carriers to escape from them, and this leads to fast recovery,  or reversible part of wearout. For the slow traps, the charge  carriers need to overcome a higher trap energy barrier. As a  7A-4 result, it is very slow or even impossible for them to escape  within a reasonable time, so these traps will cause the  irreversible wearout. To give a first order estimation, if the  trap energy is 100meV higher, the probability distribution  goes down by a factor of exp(100mV/26mV) ≈ 50. This  indicates that the time it takes to de-trap goes up by about a  factor of 50 for every 100meV increase in trap energy.  ~ kT (0.026eV) at  room temperature Energy that causes  detrapping (a) (b) Fig. 3. Energy distribution of electrons (a) at room temperature   (b) at different temperatures  Fa st  Tr aps S low  Tr aps Empty Traps Filled Traps Tr app ing De-t ra ppin g Fig. 4. Illustration of Fast traps vs. Slow traps  (cid:2) From the above discussion we postulate that the boundary  between the reversible and irreversible wearout highly  depends on the trap energy and the energy of charge carriers  during  recovery periods. Equation  (1)  indicates  that  temperature plays an important role of determining energy of  electrons. Fig. 3(b) shows that by increasing the temperature,  the energy distribution shifts to the right, so the probability of  detrapping increases. Besides temperature, voltages also  strongly affect the detrapping process by the additional  electrical field which helps the detrapping process. In the next  section, we will further the boundary between reversible and  irreversible wearout experimentally by applying different  recovery conditions to FPGA chips. III. Reversible Wearout vs. Irreversible Wearout  A. Test Setup  A group of 2-input Look-up-table (LUT) based 40nm  commercial FPGAs within the same family is chosen as the  test platform. The test structure is a ring oscillator chain that  consists of 75 inverters implemented in LUTs, followed by a  16-bit counter to capture the output frequency (~20MHz) of  the ring oscillator. An enable signal is used to switch between  DC stress (constant stress) and AC stress. The FPGA chips  are heated up or cooled down in a thermal chamber, with a  (cid:114)0.5°C accuracy. Core voltage is provided by an external  DC power supply with a nominal 1.2V. An interface board is  used for communications between the computer (for data  sampling) and the FPGA chip.   B. Natural Recovery vs. Accelerated Recovery  In most literature [8]-[14], recovery refers to the condition  when stress is removed (e.g. Vgs=0) and the temperature is  the environment  temperature, which usually  is  room  temperature. In this paper, we refer to this passive recovery  condition as natural recovery. We consider accelerated  650 recovery conditions as when several active recovery boost  techniques (e.g. high temperature [15][16] and negative  voltages [11][10][16]) are applied to accelerate the recovery  process. Different from these recovery boost literature, which  considers only the reversible wearout, this paper deals with  both reversible and irreversible wearout, with a focus on the  irreversible part. During the accelerated recovery period, we  apply both high temperature (110°C) and negative voltage  (-0.3V) as the active accelerated conditions.  During the wearout phase, two chips are stressed under the  same accelerated stress conditions (110°C, constant stress  with nominal voltage) for the same 6-hour duration, which is  followed by a 6-hour recovery phase when natural recovery and accelerated recovery conditions are applied to two chips  separately. During recovery, both chips are enabled every 20  minutes for data sampling. The data sampling time is less  than 3s, which has a negligible effect on wearout and  recovery considering a long lifetime span (days and years)  [16].   7A-4 recovery speeds up the recovery process and even recovers  some parts that would otherwise be considered irreversible. In  the time domain this can be roughly seen as a steep slope  followed by a saturation (zero slope) once all the reversible  part and part of irreversible part were recovered. Also, it is  worth to mention that the Recovered Wearout is larger than  the Reversible Wearout as shown in the figure, and this  further demonstrates that the active accelerated recovery recovers some of the irreversible parts. (cid:2) Fig. 6. Frequency change under two accelerated stress conditions  (cid:2) Fig. 5. Irreversible part under two recovery conditions  (cid:2) Fig. 5 shows the measurement results for two different  recovery conditions, the frequency under natural recovery condition is normalized to the accelerated recovery condition.  It is clear that under natural recovery condition (27°C and  Vgs=0V), the recovery process is much slower than in the  accelerated recovery case. But in both cases, recovery  saturates to some values below fresh state frequency, and the  irreversible part of wearout after natural recovery is much  larger than the accelerated recovery case. This shows that  with the active accelerated recovery techniques (e.g. high  temperature and/or negative voltages), not only the reversible  the recovery process is sped up, but, more importantly, some  of the irreversible part becomes reversible. This further  demonstrates that the boundary between two is not fixed, and  can even be controllable.  C. Sequentiality of reversible and irreversible wearout  As discussed  in Section I, reversible wearout and  irreversible wearout are mostly determined by fast traps and  slow traps respectively. So there will be sequences when  wearout happens due to the different trapping rate of the two.  To further investigate this, a group of stress tests is conducted.  Shown in Fig. 6 is the frequency change under two  accelerated stress conditions with different stress voltages. It  illustrates that both conditions follow similar wearout patterns.  Firstly, reversible wearout kicks in, then the effect of  reversible wearout levels off and irreversible wearout takes  over – in time domain this is roughly seen as a steep slope  followed by shallow slope during wearout.   Fig. 7 shows a test case when a 6-hour accelerated  recovery process follows a 6-hour stress. The accelerated  Fig. 7. Sequentiality of reversible and irreversible wearout  (cid:2) D. Irreversible wearout during accelerated recovery  As shown in Fig. 5 and Fig. 7, the zero slope during later  recovery indicates that there are still some parts of wearout  that are irreversible, and these parts will keep accumulating  throughout the system lifetime. Fig. 8 is a test case for several  cycles of stress and recovery. In each cycle, a 6-hour  accelerated stress is followed by a 6-hour accelerated  recovery (6 hours vs. 6 hours). IRn refers to the amount of  irreversible wearout for the nth cycle. It shows in the figure  that the recovery under accelerated conditions saturates in  each cycle, and the irreversible wearout (IR) increases for the  first few cycles and settles afterwards. A possible explanation  for this behavior is that in the later cycles, some of the  irreversible wearout from previous cycles starts to recover,  and the accelerated recovery and stress can fully compensate  each other. But it will not be fully recovered to the fresh state  even with accelerated recovery techniques.  Fig. 8. Accumulation of the irreversible wearout  (cid:2) 651 IV. Sleep when getting tired for FULL recovery A. Sleep with accelerated rejuvenation when getting tired  The whole process of wearout and accelerated recovery  can be compared to the biological world. Humans for  example are active during daytime, with the body conducting  activities and experiencing fatigue. During night time sleep,  the body goes through several active processes that are  essential for the recovery of its full capabilities for the next  day. If some organs experience heavy fatigue without in-time  rest, part of the fatigue will be translated into some potential  harms to the body, and will be hardly recovered. This is well  known for athletes that need scheduled recovery periods after  intensive workouts, with their athletic performance actually  getting better after the rest periods. These biological fatigue  and recovery schedules are not unlike those illustrated in Fig.  8. We thus borrow these ideas and see how they apply to  electronic systems. The key idea is to stop the stress before  irreversible effects get a chance to accumulate. The ideal  strategy is thus to keep the circuit active only during the  reversible phase of wearout until the irreversible wearout  kicks in; thus the irreversible wearout becomes almost  unobservable even in accelerated stress cases. To validate  this idea, a set of tests with different “circadian rhythms” is  conducted, and is summarized in Table I. Each cycle is  defined as the total time of the sleep duration (accelerated  recovery) and the wearout duration (accelerated stress). All  chips start from the fresh state. The total test time is 3 days  for all cases.  TABLE I Summary of periodic accelerated rejuvenation test cases  Case Name  6 hrs vs. 6 hrs  4 hrs vs. 4 hrs  2 hrs vs. 2 hrs  1 hr vs. 1 hr  Chip  No.  1  2  3  4  Cycle stress  time   6 hours  4 hours  2 hours  1 hours  Cycle accelerated  recovery time   6 hours  4 hours  2 hours  1 hours  # of  cycles  6 9 18 32 Shown in Fig. 9 are the measurement results. For all test  cases except the 1 hr vs. 1 hr case, the accelerated recovery  has a period of saturation which indicates the irreversible  parts of the wearout, and the irreversible parts accumulate in  the first several cycles and settle down in the following cycles.  For the 1 hr vs. 1 hr case, alternating phases of stress and  accelerated recovery can completely compensate for each  other, and after each accelerated recovery phase the chip can  indeed start fresh. The irreversible part of wearout is totally  avoided explicitly. Fig. 10 shows the accumulated irreversible  wearout for the first 6 cycles under above four test conditions.  It shows  that  the earlier  the accelerated rejuvenation  techniques are applied, the slower the irreversible wearout  accumulate, which results in less permanent component.  There is an optimal balance of stress and accelerate recovery  (e.g. 1hr vs. 1hr in this accelerated case) which leads to  almost no irreversible wearout. To estimate the actual  frequency degradation during normal operation, a first-order  experiment-based model is extracted in this paper. Based on  the BTI trapping-detrapping model [7] and HCI models [4],  the temperature affects the slope of the degradations by  introducing an exponential coefficient  , where B is extracted based on the measurement conducted at  two temperatures (110°C and 100°C). With this model, the  frequency degradation at any operating conditions (voltages  and temperatures) can be estimated. Assume that the amount  Degradatio n exp( − TB / ) × 7A-4 of frequency degradation under normal condition is the same  as the accelerated case for 1 hour, the equivalent duration is  about 31 hours under nominal voltage and room temperature.  So the translated optimal condition could be that, for example,  the chip is active under normal operating conditions for less  than 31 hours, and followed by a 1 hour or longer accelerated  recovery duration.  Fig. 9. Measurement results for different “circadian rhythms”  (cid:2) Fig. 10. Irreversible wearout for the first 6 cycles  (cid:2) B. Reduction of Design Margin   The biggest advantage of the proposed strategy is to  significantly reduce the design margins. At design time, to  meet the timing requirement throughout the whole lifetime,  guardbands need to be added (e.g. by oversizing transistors).  Recent work has shown that the necessary margins need to be  more than 20% over the system lifetime (several years)  [1][19]. The added design margins mean large timing slacks  and therefore wasteful power consumption during the initial  lifetime of the system. Additionally, it is hard to predict how  much the performance degrades during the lifetime due to the  uncertain switching activities and operating conditions.  Without the proposed sleep-when-getting-tired solution, the  margin needs to cover both reversible and irreversible wearout, and the irreversible part has to cover long time  periods (typically years). Since the proposed strategy starts  recovery before the irreversible wearout kicks in, the design  margin only need to cover reversible wearout. Assume that  the irreversible wearout at room temperature is the same as  the one at the accelerated stress case (at 110°C), and we  define AC stress as the case when transistors switch between  ON and OFF with a 50% duty cycle, which gives the  balanced stress and recovery during operation. Fig. 11 shows  the estimated design margin of 5 year and 10 year lifetime  spans under DC and AC stress at room temperature. The  proposed solution gives a design margin reduction of at least  60X for all cases. In the AC stress case for a 10-year lifetime  constraint, the design margin reduction is more than 100X. It  also shows that as the lifetime constraint increases, the  guardbands need to be expanded correspondingly, while with  the proposed strategy, the design margin stays the same.   652 Fig. 11. Design margin estimation under different stress conditions  (cid:2) The alternative solutions for dealing with wearout are  adaptive techniques at the circuit level [14] or dynamic  reliability management techniques at the architecture level  [22], where wearout sensors are used to track during the  whole period of the lifetime. This means more tracking power.  With the proposed strategy, the time for recovery is known  ahead, wearout sensors only need to track during a short time  (e.g. days) for the reversible part of wearout. Numerically the  difference for the tracking power is between O(ln(days)) and  O(ln(10 years)) for a 10-year lifetime constraint, or about  ln(3650) ~ 8x reduction.  Fig. 12. Average performance improvement (IMP) for 1 day and 2  days from measurement  (cid:2) Fres h y c n e u q e r F 7A-4 no recovery strategy is applied, the average performance  decreases dramatically, and this leads to that the average  performance improvement achieved by the proposed strategy  will increase with time (1.6x from 1 day to 2 days). Fig. 13 is  the predicted average performance  improvement under  nominal operation conditions (room temperature, nominal  voltage) based on the same model used for previous design  margin estimation. It also shows the average performance  improvement increases with the lifetime constraint, and for a  10-year lifetime span, the improvement can be as large as ~  9%.  V. System-level Implementation  The proposed sleep-after-getting-tired solution has been  demonstrated to almost fully rejuvenate the chip in the  previous sections. In this section, we will discuss the details  of implementing the strategy at the system level.  A. Right balance of wearout and accelerated rejuvenation  A key part of the proposed strategy is to employ a right  balance of wearout and accelerated recovery so that the  circuit could keep active as long as possible but could be  rejuvenated back to the fresh state within a short sleep  duration. Based on experiment results shown in this paper,  wearout is a relatively slow process under normal operating  condition (without being accelerated), even the reversible part  of the wearout usually takes days, so the sleep period could  be scheduled roughly in a daily (or several days) base - the  accelerated rejuvenation follows a 1-day (or several days)  period of being active.   Acc ele rated  Rejuv ena tion Irre ve rsi bl e W ear out Se nsor Fee dback Opt ima l Acc ele rated Rejuv ena tion Ti me 1 Day 2 Day s Op tima l Act iv e Time Time Fig. 14. Illustration of the “training” process  (cid:2)  To actually measure the optimal balance of active vs.  sleep for a certain system, small embedded wearout sensors  (e.g. [23]) need be spread over the on-chip test structures (e.g.  ring oscillator), these sensors can feed the degradation  information back to the system scheduler which enables the  accelerated rejuvenation techniques. Shown in Fig. 14 is a  training-like process, during which  the  accelerated  rejuvenation techniques can be applied incrementally (e.g. 1  day, and then 2 days) during the initial lifetime. At the time  when irreversible wearout starts to show up, the optimal  active and accelerated rejuvenation duration could be finally  determined. (cid:2) B. Negative “Turbo-boost” for periodic rejuvenation  For applications like cell phones, the scheduled active vs.  sleep pattern could even follow human beings’ circadian  rhythms, like 12 hours vs. 12 hours. During sleep, instead of  being fully turned off, devices could still work in low-power  mode, where recovery still be accelerated by other factors,  like high temperatures. For multicore systems (e.g. server  applications) or Network-on-Chip (NoC) systems which  consist of hundreds or even thousands of identical cores, due  to the TDP limitations, a significant amount of cores cannot  be operated at the same time, and this leads to the so-called  Fig. 13. Estimated average performance under different stress  conditions  (cid:2) C. Average Performance Improvement  With the periodic accelerated rejuvenation, the circuit is  guaranteed to run faster than the case when no recovery is  applied. In this paper, the average performance refers to the  average of all performance values during operating time  (wearout period). Fig. 12 shows the average performance  improvement (IMP) calculated from the measurement results  for 1 day and 2 days for the same chip. Under 1hr vs. 1hr case,  when irreversible wearout is almost completely avoided, it  gives the best average performance, which is close to the  fresh status. As operation time increases (e.g. 1 day to 2 days),  the average performance will keep almost the same for 1hr vs.  1hr case, while for other test cases, especially the case when  653 dark silicon problem. Recent research [23][25] pointed out  that even with the latest FinFET technology and novel  processor architectures, dark silicon issues still stays as big  challenges. The proposed sleep-when-getting-tired solution  schedules  the active and sleep  time (ON and OFF)  periodically, so this made it a perfect fit in dark silicon era.  Shown in Fig. 15 is a potential implementation, where a NoC  system has many identical cores, each of which includes  several wearout sensors that monitor the core behaviors and  feed the information back to the system scheduler that  determines how many cores and which core to disable based  on the wearout information and power/thermal constraints, it  also enables the accelerated rejuvenation blocks to apply the  accelerated recovery to these sleep cores. The external  negative voltage generating circuitry (e.g. [25]) could be used  as part of the accelerated rejuvenation control blocks. One big  potential is to assign the sleep cores in the way that they are  surrounded by the active cores, so that the sleep cores could  be rejuvenated by the heat generated by those active cores as  shown in the figure. Since most of the existing already  include the wearout sensors and employ power gating  techniques for power and thermal optimization purposes,  there is a good opportunity to combine the proposed strategy  with  the existing real-time scheduling  techniques and  algorithms  to achieve power,  thermal and wearout  optimization together. The strategy of employing shorter  accelerated rejuvenation duration (hours) after a longer span  (days) of reversible wearout is like a negative “Turbo-boost” as illustrated in Fig. 16. With this method, the overall design  margins can be significantly reduced, and the system can  perform for more of the time at higher levels of performance  and power efficiency.   Sleep Cor e Acti ve Core Apply to sl eep  cores /bl ocks Accelerated  Rej uvenation  Co ntro l Sensor  ou tputs Real-time Schedu ler To cor es From  Schedu ler Sensor  ou tput sensors Co re Accelerated Rejuvenatio n Fig. 15. Implementation of the proposed solution in a NoC system  (cid:2) Fresh Nega tiv e  “Turbo-boost” Wear out y c n e u q e r F Ave rage  Freque ncy Hours Act iv e Sl eep No rec ove ry End of li fe Day s i n g r a M n g i s e D Time Fig. 16. Illustration of the negative “Turbo-boost” strategy  Ye ars (cid:2) VII. Conclusions and Future Work In this paper, we explored the theory behind reversible and  irreversible wearout, and demonstrated with measurements  that the boundary between the two is soft and can be shifted  by applying active accelerated recovery techniques (e.g. high  temperature and/or negative voltage). While these techniques  in general still cannot fully rejuvenate the chip back to the  7A-4 fresh  state, we  further propose a biology-inspired  sleep-when-getting-tired strategy by scheduling the sleep  before irreversible wearout kicks in. The proposed solution  achieves a more than 60x design margin reduction and ~9% average performance improvement with a 10-year lifetime  span. The solution fits perfectly in the dark silicon era by  employing a negative “turbo-boost” like scheduling strategy  for multicore or NoC systems. For future work we plan to  explore the optimized scheduling method (both hardware and  algorithms) that considers power, thermal and wearout  budgets together to optimize the design space and maximize  the benefits of the proposed solution.  (cid:2) Acknowledgements  This work was supported  in part by NSF grant  CCF-1255907, SRC task 2410.001 and the Center for Future  Architectures Research (C-FAR), one of the six SRC  STARnet Centers, sponsored by MARCO and DARPA. "
2016,Dynamic admission control for real-time networks-on-chips.,"Networks-on-Chip (NoCs) for real-time systems require solutions for safe and predictable sharing of network resources between transmissions with different quality-of service requirementrs. In this work, we present a mechanism for a global and dynamic admission control in NoCs designed for realtime systems. It introduces an overlay network to synchronize transmissions using arbitration units called Resource Managers (RMs), which allows a global and work-conserving scheduling. We present a formal worst-case timing analysis for the proposed mechanism and demonstrate that this solution not only exposes higher performance in simulation but, even more importantly, consistently reaches smaller formally guaranteed worst-case latencies than TDM for realistic levels of system's utilization. Our mechanism does not require modification of routers and therefore can be used together with any architecture utilizing non-blocking routers.","8A-2 Dynamic Admission Control for Real-Time Networks-On-Chips Adam Kostrzewa, Selma Saidi, Leonardo Ecco, Rolf Ernst Institute of Computer and Network Engineering {kostrzewa, saidi, ecco, ernst}@ida.ing.tu-bs.de Technische Universit¨at Braunschweig Abstract— Networks-on-Chip (NoCs) for real-time systems require solutions for safe and predictable sharing of network resources between transmissions with diﬀerent quality-of service requirementrs. In this work, we present a mechanism for a global and dynamic admission control in NoCs designed for realtime systems. It introduces an overlay network to synchronize transmissions using arbitration units called Resource Managers (RMs), which allows a global and work-conserving scheduling. We present a formal worst-case timing analysis for the proposed mechanism and demonstrate that this solution not only exposes higher performance in simulation but, even more importantly, consistently reaches smaller formally guaranteed worst-case latencies than TDM for realistic levels of system’s utilization. Our mechanism does not require modiﬁcation of routers and therefore can be used together with any architecture utilizing non-blocking routers. I. Introduction When used in safety critical applications, NoCs must assure spatial and temporal independence between different data streams, e.g. suﬃcient isolation required by avionics safety standard DO-178B. Therefore, NoCs have the need for mechanisms providing safe and predictable sharing of interconnect resources (e.g. links and buﬀers) in order to bound or avoid interference between concurrent transmissions. There exist two established solutions to tackle this problem: non-blocking routers with rate control [1] and Time-Division Multiplexing (TDM) [2]. The ﬁrst mechanism is based on a local arbitration performed independently in routers and conducting dynamic scheduling between transmissions competing for the same output port. Although this approach is capable of providing worst-case guarantees, in particular when using virtual channels for isolation, it comes at high hardware cost and does not scale well with the number of isolated streams. TDM oﬀers a diﬀerent solution according to which each transmission receives, in a cyclic order, a dedicated time slot to have an exclusive access to the NoC. TDM allows an easy implementation and provides timing guarantees, but also results in average latencies which are very close to the worst case even when the system is not highly loaded [3]. This is mainly due to the traﬃc from generalpurpose applications that hardly ever follows a constant, predictable pattern assumed by TDM schemes. Hence, an eﬃcient execution is only possible for a single selected use-case with a known and static behavior for which the TDM scheme is fully optimized. The contribution of this work is an alternative mechanism for providing eﬃcient and safe sharing of resources in NoCs designed for real-time systems. We introduce a global and dynamic admission control based on scheduling units, called resource managers (RMs), with which applications have to negotiate their accesses to the NoC. Transmissions are scheduled using round-robin arbitration, and are granted an exclusive access to network resources. Synchronization is achieved using control messages and a dedicated protocol. The proposed mechanism allows to overcome the drawbacks of the previously described solutions. It decreases hardware-overhead compared to non-blocking routers due to the global arbitration. Moreover, it reduces temporal overprovisioning compared to TDM due to the work-conserving scheduling. The presented solution does not require modiﬁcation of routers and therefore can be used in conjunction with any architecture utilizing nonblocking routers. The rest of the paper is structured as follows: Section II provides a detailed discussion of related work. Section III describes the ﬂow of the mechanism. Based on that, a formal timing analysis is presented in Section IV. Finally, Section V presents an experimental evaluation and validation of timing properties along with the corresponding overheads. II. Related Work Multiple research eﬀorts investigated the problem of ensuring worst-case guarantees in NoCs. The most frequently deployed solution is enforcing isolation with timedivision multiplexing (TDM), such as [2, 4]. According to this scheme, resources are shared in time and each communication participant has its own static time slot during which it acquires exclusive access to the NoC. TDM-based systems are easy to implement and analyze, and the global TDM-based scheduling allows to guarantee the absence of contention and therefore reduces the amount of necessary hardware resources e.g. buﬀers or logic in routers. However, TDM-based arbiters oﬀer static non workconserving scheduling. If an application does not have any pending transfer then its time slot is wasted. The eﬃcient utilization of such architectures is only possible when the system is highly loaded, i.e continuously requested. This implies dedicated solutions, such as an oﬄine generated schedule statically applied to the whole NoC [5]. Consequently, average latencies from non optimized applications are close to the worst-case, even in lightly loaded systems introducing a ma jor temporal overhead [3] . Furthermore, the overhead of TDM depends only on the duration of the cycle, i.e. the number of applications and the size of their time-slots, and not on the frequency of their accesses to the interconnect. Using large slots increases the latency of all applications [2]. Small slots lead to a distribution of longer transmissions over several 978-1-4673-9569-4/16/$31.00 ©2016 IEEE 719 TDM-cycles, even when the remaining TDM-cycles are not used, which drastically decreases performance. Moreover, too short TDM-slots may cause underutilization of other peripherals and modules e.g. too short transmissions towards DRAM memory may cause a drastic increase in command overhead [3, 6]. In order to mitigate these eﬀects, multiplexing of time slots between several channels with independent TDMschedules was proposed in [3]. The eﬀectiveness of such approaches depends directly on the number of independent channels and their utilization. If there are few heavily utilized channels the performance improvement will be low. Moreover, they rely on static budgets which leads to the same problems as in case of TDM-slot’s granularity. Other approaches, such as SurfNoc [4], employ optimized TDM scheduling to minimize the latency overhead. This is performed by replacing the cycle-by-cycle TDMschedule with more ﬂexible solutions e.g. domain oriented waves. Although this allows to decrease negative sideeﬀects of TDM-arbitration, it does not fully eliminate them, providing only a more optimized solution. Moreover, these solutions require much more complex routers thereby drastically increasing costs of the hardware and power consumption. The alternative solution to TDM is based on nonblocking routers with rate control where arbitration is performed locally in routers e.g. QNOC[8]. The guarantees are provided through dynamic scheduling between transmissions mapped to diﬀerent virtual channels (VCs) trying to acquire a shared output port in a router. Although this approach oﬀers worst-case guarantees and work-conserving scheduling, it introduces high hardware overhead. Firstly, the arbitration is based on the assumption that packets can be forwarded as they arrive [9] i.e. there is no back pressure and no correlation between routers which requires larger buﬀers than TDM-based approaches [2]. Secondly, the ma jor challenge emerges from the constant increase in the number of applications integrated into a single chip, such as the Flight Management System [10], encompassing multiple tasks with diﬀerent importance to the system’s safety (criticality levels ). In case of non-blocking routers, the number of VCs must be equal to the number of criticality levels and therefore must increase accordingly, otherwise the system is not predictable [2]. In this work, we consider a system in which applications must be mapped to the same VC due to insuﬃcient hardware resources, therefore excluding solutions based on non-blocking routers. The proposed solution allows to overcome the drawbacks of previously described mechanisms. It reduces hardware overhead compared to non-blocking routers due to the global arbitration which decreases the blocking and the size of the necessary buﬀers in routers. Moreover, it supports safe and predictable sharing of the same VC between diﬀerent transmissions. In comparison to TDM, our mechanism drastically decreases average latencies, i.e. temporal overprovisioning, due to the workconserving arbitration. Finally the proposed mechanism allows to maintain the locality of memory accesses [11] through the isolation of a whole transmission. This improves performance and decreases power consumption of DRAM memory modules which constitute the most common hot-module in MPSoCs. Mechanisms for global arbitration in NoCs were traditionally introduced only to increase the average per8A-2            	  Fig. 1. Global admission control at the top-layer before sending data on the bottom-layer. formance of the interconnect through blocking avoidance e.g. [12, 13]. III. Dynamic Admission Control in NoCs In this work, we propose a global and dynamic mechanism for admission control in real-time NoCs. The proposed solution introduces an overlay network built over the existing NoC architecture, as depicted in Figure 1. In order to comply with the isolation requirement, the toplayer is used to arbitrate between interfering transmissions, i.e sharing the same VC and whose paths overlap in at least one physical link. Each transmission must ﬁrst acquire the right to access the NoC from the top-layer before eﬀectively start sending data on the bottom-layer. In real-time NoCs, synchronization between interfering transmissions is essential since the eﬀects of both back pressure and head-of-line blocking (resulting from wormhole switching) in the bottom-layer, where switch arbitration between packets/ﬂits is performed, may endanger the system safety [12]. Therefore, the proposed mechanism has the following advantages: 1. Predictability through the isolation of the entire transmission composed of multiple packets. Hence, timing guarantees are provided to the whole transmission instead of a single packet. 2. Eﬃciency since it becomes possible, given the global view, to dynamically adapt to the current state of the system and perform optimization. 3. Preserve the locality of accesses which is in particular very suitable to memory traﬃc performed using DMA. Indeed, in safety-critical MPSoCs platforms, memory traﬃc constitutes the main bottleneck for both performance and predictability [14, 15]. The proposed mechanism can be applied to diﬀerent NoC architectures and is independent from the actual implementation of the bottom-layer. However, we assume the following about the underlying baseline NoC architecture: a 2D mesh network with wormhole switching, prioritized virtual-channels (VCs) (see [8]) and a deterministic XYrouting. A. Mechanism description We refer to a set of interfering transmissions as a synchronization scenario arbitrated/synchronized using a scheduling unit - Resource Manager (RM). We assume a Round Robbin arbitration policy, which is workconserving. Each sender belonging to a synchronization scenario must, for each transmission, request a permission from the RM. The communication between processing nodes and the RM is protocol-based and is realized with special control messages transmitted on a dedicated virtual channel. The transmission of data starts only after being acknow ledged by the RM. Later, when the transmissions completes, the sender must inform the RM in order 720 Sender  Client  RM  clnt_req()  reqMsg  time  clnt_rel()  clnt_strt()  ackMsg  rm_proc()  execution  relMsg  rm_rel()  Fig. 2. Workﬂow of the RM-based admission control in a NoC. to release the resources. Note that when sender is granted an access to the NoC, this access is granted for the entire transmission, composed of multiple packets. When, in the system, multiple disjoint sets of interfering applications (possibly resulting from multiple hotmodules) exist, the designer may use diﬀerent RMs to synchronize each of them. This permits to increase the system’s scalability and to decrease the synchronization overhead. Each RM is then responsible of synchronizing transmissions belonging only to its associated synchronization scenario. B. Protocol and Implementation Standard NoC architectures do not provide any hardware mechanism enforcing admission control, therefore we introduce local supervisors called clients at each node. Clients trap all outgoing transmissions requests and negotiate their synchronization with the RM before they can obtain physical access to NoC. The communication between the RM and the clients is performed with a protocol using three control messages: reqMsg (request), relMsg (release) and ackMsg (acknowledge). The workﬂow (cf. Fig. 2) is explained in the scope of conducted operations. Whenever a sender is trying to start a transmission its request is trapped by the client. The corresponding client then sends a request message to the RM to obtain access to the network (clnt req ()). the RM is equipped with a queue for storing pending requests from clients. If the queue is empty, the RM must wait for a new request to arrive. Otherwise, the scheduler decides about which request from the queue should be served ﬁrst (rm proc ()). Each RM is sequential and serves one request at a time. The selected request is removed from the queue and from this moment on the resource is considered to be occupied. After that, the RM must notify the selected sender for service with the ackM sg . After receiving the ackMsg, the communication may start (clnt strt ()). Once granted, the connection holds until the end of the transmission or the abortion through the client based on a predeﬁned timeout used to prevent unbounded connection times. When the client detects the end of the transmission (e.g. based on its time-budget or injection of the last ﬂit) it issues a relMsg to the RM (clnt rel ()). As soon as the relMsg arrives, the RM considers the resource to be free again (rm rel ()). The latency of control messages is crucial for the performance of the proposed mechanism. We use prioritized VCs and assign a dedicated VC (with the highest priority) for control messages in order not to be blocked by any other traﬃc. More generally, control messages can be allocated to any available VC capable of giving latency guarantees or a dedicated independent control NoC or signal lines for maximum performance. In order to evaluate the hardware overhead required for the implementation of clients, we implemented a client in the IDAMC architecture [16] on a Virtex-6-LX760 Xilinx FPGA. The client required only 200 Look-up tables (LUTs), which corresponds only to 3% of the area of a 8A-2 Network Interface (NI) module. Overall, the area overhead of clients is comparable with the TDM-based solution [17]. A TDM-arbiter must also trap and distinguish between diﬀerent transmissions and monitor their duration to detect the end of a time-slot. In the described setup, the scheduler used by the RM has the size of a standard round-robin arbiter, whereas the main area overhead of the central RM module comes from the request queues. Our implementation in the IDAMC architecture on a Virtex-6-LX760 Xilinx FPGA for eight synchronized senders resulted in 800 LUTs which is only 12 % of the area of the NI module. Low hardware requirements of RMs permits implementation of multiple units in the same NoC to decrease the overhead and increase scalability. IV. Predictability of the mechanism In order to prove the predictability of the mechanism, we compute a bound on the worst case latency for each transmission synchronized with RM taking into consideration other interfering senders and the overhead of the protocol. The timing relations of the individual transmissions are abstracted by event models [18] that capture the worst-case and best-case behavior of every possible transmission arrival/activation pattern. Therefore, we may use temporal-analysis frameworks, such as applied in this work Compositional Performance Analysis (CPA) [19], which apply event models to capture the dynamics of the system’s behavior. Note that the provided analysis concerns only the toplayer. We assume that the bottom-layer architecture of a particular NoC is analyzable (e.g. [9]) and provides Basic Network Latency Bounds, which constitute the input to the top-layer analysis, The bottom-layer latency depends on the following factors: the source/destination routing distance, packet size, link bandwidth, additional protocol overheads and other ongoing communication. Deﬁnition 1 (Basic Network Latency Bounds). Let C and C + i denote the minimum and maximum time required to transfer all packets of a transmission i in speciﬁc VC when no contention and maximal NoC contention are considered respectively. Therefore, the time during which the packets from the transmission i are physically present in i ; C + the network can be bounded by [C i ]. − − i A. Analysis of the Top-Layer In CPA [18], we consider every RM as a resource under round-robin scheduling and every transmission belonging to the synchronization scenario as a task. The latency of a transmission under RM control can be decomposed into 3 phases, the release time denoting the time between the client issuing a request and its arrival at the RM, the blocking time which a request has to wait in the RM queue before being granted access to the NoC by the RM, and the task execution denoting the transfer and processing of the acknowledge message, the maximum time necessary to transmit all packets belonging to the transmission and the time necessary to send and process the release message. Deﬁnition 2 (Request arrival functions). Let η i (Δt) and η+ i (Δt) deﬁne the minimum and maximum number of requests (events), i.e transmissions which can be issued by a sender to the RM, within a time window of size Δt. i (n) and δ+ Their pseudo-inverse counterparts δ i (n), denote the minimum and maximum time interval between − − 721 the ﬁrst and the last event in any sequence of n event/request arrivals [18]. The release time includes the propagation delay of the RM’s control messages through the NoC along with the interference with other synchronized senders. It introduces a delay and jitter, for the requests arriving at the RM. The jitter can be captured using the method from [20]. Deﬁnition 3. The worst-case propagation jitter of a transmission i, Ji,ctrl , is a time interval i,ctrl − C Ji,ctrl =C + i,ctrl and C + − − i,ctrl (1) − − − − − − (2) i,ctrl } where, C i,ctrl denote the best- and worst-case network latencies of a control message for transmission i. Based on this, we derive (similarily to [19]) the RM input requests arrival functions for each transmission i, which are identical to the output applications/senders requests arrival function including the propagation jitter. Deﬁnition 4. The output event model δ i , out(n) for arrivals of request messages for a transmission i, is denoted as: i,out (n)=max{δ i (n) − Ji,ctrl , (n − 1) · C δ where, δ i (n) denotes the minimum time interval necessary for n activations of transmission i by a sender and C i,ctrl minimum network latency of a single request [19]. We analyze the RM with the busy window approach (see [19]). It constructs a critical instant, which marks the beginning of the busy window time interval and considers the worst-case arrival sequence of events, event’s duration and the scheduling policy to compute the maximum delay for a transmission (task) under analysis. It maximizes the response time i.e. the duration between the activation of the transmission and its completion. Deﬁnition 5. Let the minimum and maximum q-event busy windows ω i (q) and ω+ i (q) describe the minimum and maximum time interval required for sender i to conduct q consecutive transmissions belonging to the same synchronization scenario protected by a RM. Corollary 1. The minimum q-event busy window can i (q)≥ q · C always be bounded by ω i . Theorem 2. The worst-case time necessary to conduct q transmissions i belonging to a synchronization scenario protected by a RM is bounded by: i (q)≤ q · C + i + q · 3 · C + ω+ where C + i,ctrl denotes maximum latency of the control messages for transmission i and Bi (ω+ i (q)) the maximum blocking resulting from scheduling of other transmissions belonging to the same synchronization scenario. i,ctrl + Bi (ω+ i (q)) (3) − − − Proof. The theorem directly results from the description of the mechanism and protocol (see Sec. III). The busy window of q consecutive transmissions mi is bounded by the time necessary to send all packets belonging to these transmissions (q · C + i ), the time (3q · C + i,ctrl ) necessary to exchange control messages for each transmission (reqMsg, ackMsg, relMsg), plus the maximum time interval during which a particular request can be blocked due to other ongoing transmissions in the top-layer. Note that the ω+ i (q) appears on both sides of Eq. 3 forming a ﬁxed-point iteration problem. It can be solved 8A-2 i (q)= q · C + i + 3q · C + iteratively starting with ω+ two consecutive iterations produce the same result. Lemma 3. The blocking time which q requests experience in a time window Δt can be bounded by j + 2 · C + j,ctrl ) · min{q , η+ j,out (Δt)} (4) (C + i,ctrl until Bi (Δt)= (cid:2) ∀j∈S where, S is a set of al l transmissions belonging to the same synchronization scenario as transmission i, C + j denotes the maximum latency of the transmission j , 2 · C + notes the maximal latency of the acknow ledge and release messages for j and η+ j,out (Δt) is the maximum number of requests for transmission j within interval Δt taking into consideration the worst-case propagation jitter resulting from the transmission of control messages. j,ctrl deProof. According to the assumptions, the considered RM uses round-robin scheduler. The sum in Eq. 4 computes the interference from all other transmissions belonging to the same synchronization scenario. Following this arbitration policy, each transmission j may block only once the transmission i. However, q activations of i can be blocked only q times and also no more than η+ j,out (Δt) times. Finally, we assume that the ﬁrst requests from all streams arrive exactly at the same moment in order to construct the critical instance. Therefore, it is a conservative overestimation of the actual interference. − − B. Derived metrics Based of the computed busy-window ω+ i (q), we derive the following QoS metrics, commonly used in the analysis of real-time systems: worst-case response time and worstcase backlog . Let Ri be the worst-case response time of the transmission i i.e. the longest time interval between its activation and completion: {ω+ i (q) − δ i (q)≥ δ (q + 1)} (q) , ω+ Ri =max∀q≥1 The response time Ri is represented by the diﬀerence between the busy window ωi (q) and the earliest possible activation δ (q). Later the schedulability test has to conﬁrm if the constraint Ri ≤Di is satisﬁed, where Di deﬁnes the the transmission’s deadline for every transmission in the system. If not, then system is not schedulable given the constrains. Let bi be the worst-case backlog of a transmission i i.e. the maximum number of pending, unserved requests. bi can be computed as follows: {η+ i (q)) − (q − 1)} bi =max∀q≥1 i (ω+ Calculation of the backlog provides the conservative upper-bound on the size of the buﬀer required by the RM. (5) (6) − V. Experimental Results In this section, we evaluate the proposed mechanism. The analysis from Sec. IV is implemented in the pyCPA framework [19] using the approach presented in [9] for deriving the bottom-layer latencies (C and C Simulations are carried out with OMNeT++ event-based simulation framework and HNOCS library [21]. The evaluation is done through comparison with TDM-based solutions (cf. Sec. II) utilizing long TDM-slots, adjusted to the maximum network latency of a transmission (C + ) taking +/− i,ctrl ). +/− i 722                                                                                                                                      !            !           ""!        #  $%&   '(   () *  +      )   ,   )   Fig. 3. Analytical comparison of worst-case latency guarantees for applications (A1-A4) generating diﬀerent NoC load. A3 and A4 have the same settings. TDM-Long TDM-Short RB 020406080100120140160180  2  4  6  8  10  12  14  16 Num. of   Applications 0.120.250.380.500.620.750.881.00 Interfering Load (L) a L t y c n e [ s e c y c l 0 0 0 1 x ] (a) (b) Fig. 4. Worst case guarantees for a burst of 16 transmissions with jitter =10%P (a) Transmission latency (b) Protocol overhead resulting from RM. 32kB 64kB 128kB 256kB  20  15  10  5  0  2  4  6  8  10  12  14  16 Num. of   Applications 0.120.250.380.500.620.750.881.00 Interfering Load (L) O e v r d a e h [ % o f r t n a s ] . into consideration possible interference with other VCs, as well as short TDM-slots, adjusted to the transmission of a single packet. We assume as synchronized transmissions memory transfers which constitute the ma jority of the NoC traﬃc (cf. [22, 15]). They allow to exploit locality of transfers in the memory context and to challenge the performance and the scalability of the system since memories are the most common hot module in a systemon-chip. A. Evaluating Worst-Case Latency Guarantees In experiments, we consider synchronization scenarios with x senders performing a burst of y transmissions per riod P = 16 · x · C + and a small jitter J equal to 10% of activation. Senders are activated periodically with a pea period. We assume that all synchronized transmissions are of equal length i.e. they have the same C + . However, we vary the system’s load L deﬁned as the total number of transmissions from synchronized applications per period (yx · C + )/P . P i.e. L= First, we analyze a system with four (x= 4) applications (A1-A4) where we vary the burst size to generate diﬀerent loads L on the network (L is equal to 15%, 65% and 90% of P). We measure for each application the worstcase latencies obtained per burst, see Fig 3. We observe that synchronization with the RM always results in better guarantees than TDM (even when short slots are considered) despite the additional communication protocol overhead. This overhead increases with the load but remains very low compared to the transmission time (4,1% of C + for L=90%). Fig. 4 illustrate the same results , i.e transmission latency and protocol overhead in the worst case, as we vary the number of synchronized applications (x= [2..16]) and interfering load (yx = [2..16]). The results concern an application transferring periodically a burst of 16 transmissions. (cid:3) x DRAM SRAM1 SRAM2  AU  0.5  VU  600  190  0.5  RAST  MED  CPU  IDCT  RISC  500  BAB  910  UP  SMAP  ADSP  40  60  250  500  40  32  670  (a) (b) Fig. 5. MPEG-4 average communication demands speciﬁed in MB/s (a) and mapping (b). RAST  RM  AU  SRAM1  SRAM2  IDCT  MED  CPU  VU  ADSP  RISC  BAB  UP  SMAP  R0  R1  R2  R3  R4  R5  R6  R7  R8  R9  R10  R11  R12  R13  R14  D R A M  RM  RM  The worst-case latencies, depicted in Fig. 4(a) for both TDM and RM increase along with the size of the synchronization scenario. However, they remain constant for TDM and independent from the system’s load. This allows our approach to signiﬁcantly outperform TDM (up to 80%), due to the applied work-conserving scheduling. Note that TDM with short slots performs better than TDM with long slots as it is less sensitive to the jitter. The protocol overhead, depicted in Fig. 4(b) is presented as a percentage of the transmission’s length. It increases proportionally to the number of synchronized senders. This eﬀect can be mitigated by implementing multiple RMs in the same NoC. Moreover, the protocol overhead depends directly on the frequency and number of synchronized transmissions i.e. system’s load. Finally it decreases, as an absolute ratio, with increasing length of transmissions as the protocol overhead is constant w.r.t the transmission length. B. Application and Benchmark-Based Results We evaluate the average performance of the mechanism using the MPEG-4 video decoder application [23]. This comparison is relevant for all cases in which synchronized application not only requires the worst-case guarantees but also proﬁts from a faster execution. We identify three modules with high communication requirements: DRAM (the target of 7 senders), SRAM2 (target of 4 senders) and SRAM1 (target of 2 senders), see Fig 5(a). Requests to each memory module constitute independent synchronization scenario. We map diﬀerent scenarios on independent VCs and protect each of them with an independent RM mapped on a diﬀerent node. Each module of the MPEG-4 decoder is modeled with traﬃc generator conducting 8kB long DMA transfers what allows to maximize the beneﬁt from DDRAM3 2133N [6]. Transmissions are performed periodically and periods are calculated based solely on the required bandwidth including some release jitter (J=10% of P). Fig. 6 presents the achieved, in the simulation, averagelatencies of a single transmission in the system with TDMand RM-based arbitration. The depicted values include both network and memory latencies to assess the eﬀect of memory locality on the described mechanisms. Latencies of the DRAM memory are modeled after the speciﬁcations of DDRAM3 2133N [6]. As explained previously, TDM with short slots performs better than TDM with long slots in the network. However, when considering the memory eﬀect, then the memory latency for TDM with long slots is better than with short slots since the long slots allow to maintain the locality of memory accesses to the DRAM. Indeed, DRAMs have an internal level of caching, which in standard DDR3 modules amounts to 8kB. Consequently, contiguous and aligned 8kB long 8A-2 723 	      	      	      	      	      	      	      	      	                                                                                                                                            ! ""#$ !  !  %  ! !  & Fig. 6. Eﬀect of memory locality on the total transmission latencies for MPEG-4 module using TDM and RMs,                                                                                                                                      !    ""                 #    $     #   % Fig. 7. Latencies of CHSTONE benchmark with TDM and RMs. transfers fully beneﬁt from the caching. The RM also allows to maintain the locality of memory accesses, since applications are granted access to the NoC for the entire transmission. Overall, RM performs better than TDM for memory traﬃc traversing the NoC and accessing main DRAM memory. Finally, we evaluate the average performance with CHSTONE benchmark [24]. The traces of benchmarks are extracted using the Gem5 simulator. We use an ARMv7a core with a 32 kB L1 cache and 64 Bytes long cache-line (10 packet long transmissions). Compilation is performed without any optimization with respect to RM nor TDM (standard gcc compiler ver. 4.7.3). Next, we establish diﬀerent scenarios resulting from possible mappings and number of senders assuming a constant placement of the RM and one sender per node. Fig. 7 presents the average latencies for diﬀerent sizes of synchronization scenarios. In this case, it is also visible that the RM signiﬁcantly outperformed other solutions. However, the diﬀerence compared to TDM in case of small synchronization scenarios e.g. 2 senders, is rather low (around 8%). This has two reasons: short duration of TDM-cycles and relative high RM overhead (three control messages per transmission). In case of bigger scenarios, the solution based on RMs is up to 60% better than TDM. Note that the activation patterns of senders are not necessarily synchronized with respect to the TDM-cycle. Tailoring TDM schedule in order to optimize such systems, which are not fully loaded, requires dedicated solutions and introduces additional hardware overhead e.g. SurfNoC [4]. RM allows eﬀective arbitration without additional eﬀort. VI. Conclusion Designing predictable embedded multicores for safetycritical and real-time systems is a ma jor challenge. In this context, managing shared resources such as NoCs and main DRAM memory is an unavoidable obstacle. In this paper, we propose an eﬃcient and a predictable solution for safely sharing NoC resources, which constitute the main communication backbone of modern MPSoCs platforms. The solution provides signiﬁcant improvement over well established TDM-based solutions due to the work-conserving arbitration, which decreases both average latencies and provided service guarantees. This is performed at the cost of small hardware implementation and protocol latency overheads. Acknowledgment This work was funded within the EMC2 pro ject by the German Federal Ministry of Education and Research with the funding ID 01—S14002O and by the ARTEMIS Joint Undertaking under grant agreement n 621429. The responsibility for the content remains with the authors. ◦ "
2016,STLAC - A spatial and temporal locality-aware cache and network-on-chip codesign for tiled many-core systems.,,"STLAC: A Spatial and Temporal Locality-Aware Cache and Network-on-Chip Codesign for Tiled Many-core Systems 1A-3 ∗,† †,§ † § ∗ Mingyu Wang and Zhaolin Li Institute of Microelectronics, Tsinghua University, Beijing, China Tsinghua National Laboratory for Information Science and Technology, Beijing, China Research Institute of Information Technology, Tsinghua University, Beijing, China E-mail: wang-my12@mails.tsinghua.edu.cn ∗ Abstract— The spatial and temporal locality of workloads are the root causes for cache designs to overcome the memory wall problem. However, few existing state-of-the-art designs exploit both the two locality features to optimize the memory hierarchies in the area of tiled many-core systems, which losses the opportunities to explore more performance improvement. To address this problem, an adaptive spatial and temporal locality-aware cache and networkon-chip (NoC) codesign (STLAC) is proposed, which dynamically partitions the last level cache (LLC) as data prefetch buﬀer or victim cache for locality prediction and exploits a hybrid burst-support NoC for fast data prefetch. The data prefetch buﬀer speculates the data blocks in subsequent addresses to exploit the spatial locality, while the victim cache collects the evicted data blocks from the upper memory hierarchy to exploit the temporal locality. By combining the proposed adaptive cache partition with the hybrid burst-support NoC, the oﬀ-chip misses and onchip network usage are greatly reduced. Experimental results demonstrate that the proposed STLAC reduces up to 43% oﬀ-chip misses and improves 15% performance on average compared with the traditional shared LLC design. I. Introduction With the advancement of modern integrated circuit (IC) technology, tiled many-core architectures with a large shared last level cache (LLC) and a meshed network-onchip (NoC) are widely used in multimedia applications and scientiﬁc computing workloads due to the high parallel processing ability, good power distribution and scalability [1, 2], such as Intel’s 64-core Single-Chip-Cloud (SCC) processor [3, 4] and Tilera’s 72-core TILE-Gx72 processor [5]. However, the real memory access behavior of these applications can be very diﬀerent, which leads to diﬀerent cache organization requirements. Hence, it gives the optimization opportunities to reduce the oﬀ-chip misses and on-chip network usage for performance improvement. Although the traditional shared LLC avoids unnecessary data replications and maximizes the utilization of cache resources, the cache capacity interference problem 978-1-4673-9569-4/16/$31.00 ©2016 IEEE 37 is introduced at the same time, which may have much negative impact on the performance while the running threads have signiﬁcant diﬀerent cache requirements [6]. For example, stream-like workloads have fairly low temporal locality on data access, which not only take few advantage of the traditional cache designs but also lead to performance degradation by potentially evicting the data blocks that may be reused in the future by other running threads. Therefore, it is necessary to give a new memory hierarchy design that can provide some kind of intelligent control to distribute cache resources more fairly and even take advantage of the diﬀerences among the locality of the running workloads. Prior research has made much eﬀort on cache organization optimization in single-core systems [7, 8], or intelligently distributing the private or shared part of LLC for adaptive cache designs [9, 10]. However, few existing state-of-the-art designs exploit both the spatial and temporal locality to optimize the memory hierarchies for tiled many-core systems. For example, CloudCache [6] dynamically changes the private cache size for each thread to satisfy the diﬀerent cache capacity requirements. This method is proven to be especially beneﬁcial for highly heterogeneous workloads. However, since each thread is assigned a private LLC, the utilization of LLC can not be maximized, and the coherence protocol is also complicated because the size of the private LLC for each thread is dynamically changeable during the runtime. Elastic cooperative caching (ECC) [9] and victim replication (VR) [10] are also recently presented state-of-the-art designs. Similar as CloudCache, ECC dynamically adjusts the size of cache to be private or shared for the spilled blocks, and VR exploits the local LLC slice as a victim cache for the data evicted from the private cache. All these prior studies have made some improvement on performance by distributing the private or shared caches, but all of them lack considerations on the locality of the running workloads. Note that, for stream-like applications, neither extending the amount of private cache nor placing the local primary cache victims into LLC slice is eﬃcient to achieve performance improvement, because most of these data will not be used in the future. Therefore, they loss the opportunities to further explore the performance improvement. (cid:258) Core 4 L1I/D$ / $ LLC Controller Victim Prefetch Core 14 L1I/D$ / $ LLC Controller Network 1A-3 1 5 9 0 4 8 5 1 4 12 13 2 6 10 14 3 7 11 15 3 2 Victim Prefetch Memory Bank Fastprefetchpath (cid:258)(cid:258) Network Memory Controller Memory Bank Memory Controller Fig. 1.: Overview of data access process in STLAC. In this paper, we propose an adaptive spatial and temporal locality-aware cache and NoC codesign (STLAC), which dynamically partitions the LLC as data prefetch buﬀer or victim cache for locality prediction and exploits a hybrid burst-support NoC for fast data prefetch. The data prefetch buﬀer speculates the data blocks in subsequent addresses to exploit the spatial locality, while the victim cache collects the evicted data blocks from the upper memory hierarchy to exploit the temporal locality. Compared with the traditional shared LLC design, the proposed STLAC reduces up to 43% oﬀ-chip misses and improves 15% performance on average. The rest of this paper is organized as follows. Section II introduces the proposed adaptive cache design and the proposed burst-support NoC in detail. Section III gives the results and comparisons on network and full system simulations, respectively. Finally, Section IV concludes this work. II. Design of STLAC We begin with a high-level description of the proposed system, where 1(cid:2) - 5(cid:2) schematically explain an example of STLAC. Fig. 1 depicts an overview of the tiled many-core the data access process in STLAC. Each tile is composed of a core, a private upper level private cache, a LLC bank and the directory controller. At ﬁrst, we assume that three threads are executing in core 1, 4, 7, and access the required data from the lower memory hierarchy of a remote node 14 via the packet-switched NoC, because all the cache misses occur in the victim part shown as 1(cid:2), 2(cid:2) and 3(cid:2). After a period of time, the prefetch part of the LLC in node 4 is expanded due to the high spatial locality of the running thread, and a cache miss exactly occurs in the prefetch part in node 4. Then, a fast data prefetch is issued between node 4 and node 14 via the burst-support NoC, shown as 4(cid:2). Once the data prefetch is ﬁnished, the threads can access the data from the corresponding LLC instead of the lower memory hierarchy to improve 5(cid:2). Particularly, if the prefetched data are not in a remote the cache hit rate and reduce the network usage, such as node, no node-to-node data prefetch path via the network is needed, which further reduces the network usage. Core and upper level cache side Addr Data Cache Contoller Prefetch part Cache Partition Unit Victim part LLC Prefetch Controller Prefetch Engine Addr Way (cid:258) Addr Data Prefetched data Lower level memory side Fig. 2.: Hardware structure of adaptive cache partition. A. Adaptive Cache Resource Partition A notable feature of the proposed adaptive cache partition strategy is to dynamically adjust the ratio between the victim part and prefetch part of LLC at runtime according to the real spatial and temporal locality features for diﬀerent workloads. Fig. 2 illustrates the hardware structure of the adaptive LLC design, in which the cache partition is under the control of the cache partition algorithm (CPA). Our cache partition is operated at waygranularity and executed periodically at a speciﬁc time interval TΔ . Cache miss rates (M Rs) are calculated for each TΔ , and the locality analysis for the running workloads is completed by comparing the diﬀerence of the cache M Rs between the prefetch part and the victim part with a pre-set threshold, instead of tracking the data access behavior for some amount of instructions by a Stream Detect Unit used in [8] for saving the hardware overhead. For example, if the M R in the victim part becomes more than that in the prefetch part, the capacity of the victim part C apacityv will be shrunk while the capacity of the prefetch part C apacityp will be expanded. On the contrary, if the M R in the prefetch part becomes more than that in the victim part, the cache partition unit then asserts an opposite cache partition trend. As a result, a 38 5: 4: 3: Algorithm 1 Cache Partition Algorithm (CPA) 1: set C apacityv = C apacityp ; reset all Counters and M R Proﬁlers {cache conﬁguration and counter initiate} 2: if M Rv − M Rp ≥ threshold then C apacityv − −; {shrink the capacity of victim cache} C apacityp + +; {expand the capacity of prefetch buﬀer} conﬁgure the LRU way of the victim cache as 6: else if M Rv − M Rp ≤ threshold then prefetch data buﬀer; {expand the capacity of victim cache} C apacityv + +; conﬁgure the LRU way of the prefetch data buﬀer as victim cache; C apacityp − −; {shrink the capacity of prefetch buﬀer} 10: else 11: Return; {optimal cache conﬁguration achieved} 12: end if 9: 7: 8: dynamic and optimal ratio between the victim cache and data prefetch buﬀer is achieved during the whole life cycle of the running workloads. Compared with the traditional direct prefetch technique [7], the Cache pol lution eﬀect that the data possibly to be used in the future are replaced by the prefetched data, can be minimized in the proposed adaptive design, because the prefetched data are limited to be ﬁlled in the prefetch part other than the entire cache space based on a local least recently used (LRU) replacement policy. The spatial and temporal locality of the running workloads can be exploited by the prefetch part and the victim part as much as possible, respectively. B. Fast Data Prefetch using Burst-support NoC In order to guarantee the advantage of the prefetch technique, the prefetched data from remote nodes should be transferred as fast as possible without breaking the continuity. Otherwise, it may lead to degradation on performance, because the length of the prefetched data ﬂits is much longer than normal coherence request or response ﬂits, and it increases the possibility of network congestion. Also, if the prefetched data can not arrive at the destination node earlier than the time when the corresponding load/store instruction wants it, the advantage of the prefetch technique may be weakened. If we assume that the prefetched data ﬂit is stalled at an intermediate node for some time, and then an equivalent data accessed by a single transfer has arrived at the destination node via another datapath due to the path diversity of the network. In this situation, this ineﬀective prefetch not only makes no performance improvement, but also may introduce performance loss by increasing network congestion. Therefore, we propose a hybrid burst-support NoC for STLAC, where the coherence protocol requests/responses 1A-3 VA SA Burst Configuration n-1 (cid:22127) Output Port (cid:22127) RC (cid:22127) 0 VC state 0 (cid:258)1 VC state VC state (cid:258)1 n-1 0 VC state 0 VC state 1 (cid:258)1 VC n-1 0 VC VC0 VC VC0 VC1 VC1 (cid:22127) (cid:22127) VC (cid:22127) (cid:22127) VCn-1 VCn-1 (cid:22127) Stalled Stalled Input Port Burst packet Fig. 3.: The microarchitecture of the burst-support router. Data Request PS Flit Data Response Burst Flits NodeD Dest Addr Addr Length 4 NodeS Dest Data Data Addr Addr(0) Length 4 H/T H/T Node S I (cid:258) I D H/T H/T H B B B B T H (cid:258) Dest Data Addr(3) 4 H B B B B T T H T Time (cid:258) 0 3 6 9 12 15 166 167 168 169 170 171 172 173 (cid:258) Cycles Fig. 4.: Time-space diagram of a 4-beat fast prefetch via the NoC, where S , I and D represents the source, intermediate and destination node, respectively. H , B and T represents the head, body and tail ﬂits, respectively. and single data accesses are transferred as traditional packet-switched (PS) ﬂits, and the prefetched data are transferred as long burst packets. The detailed microarchitecture of the burst-support router is shown in Fig. 3. An n-ﬁeld virtual channel (VC) state register is added for each input port to record the states of all the n VCs. Once one of the VCs is set to the burst state, all the other VCs in this input port will be set to the stal l state to guarantee the physical NoC link corresponding to this input port is exclusive for the burst packet. The head ﬂit of a burst packet takes a higher priority in the virtual channel allocation (VA) and the switch allocation (SA) stages than other normal PS packets. If it successfully wins SA, the switch will be reserved for the entire burst packet until the tail ﬂit passes this router. Otherwise, the burst packet will be stalled in this router and will compete for the switch in the next cycle. More speciﬁcally, the fast prefetch for multiple data blocks in continues locations are realized by sending one data request PS ﬂit with the baseline address and the prefetch length ahead, and then the required data blocks in these subsequent addresses are transferred as a long burst packet, shown as Fig. 4. To give an illustration about the advantage on network usage reduction of the proposed burst-support NoC over the traditional PS and 39 12 0 4 8 1 5 9 13 2 6 10 14 3 7 11 15 1 2 3 4 (a) Baseline PS 12 0 4 8 1 5 9 13 2 6 10 14 3 7 11 15 1 2 3 4 Blocked (b) Hybrid CS 12 0 4 8 1 5 9 13 2 6 10 14 3 7 11 15 1 2 3 4 Blocked (c) The pro. Fig. 5.: An illustration of the hybrid data access in a 4×4 meshed NoC under a simple traﬃc. 1(cid:2) represents a 4-beat data prefetch with PS, CS and the proposed burst-support NoC connections, respectively. 2(cid:2), 3(cid:2) and 4(cid:2) represent other normal PS connections. 0 10 20 30 40 50 60 0.10 0.12 0.14 0.16 0.18 0.20 0.22 0.24 0.26 0.28 0.30 0.32 0.34 0.36 A e v r e k c a p e g a t l a t n e y c Injection rate  PS CS Pro. (a) uniform 0 10 20 30 40 50 60 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 A e v r e k c a p e g a t l a t n e y c Injection rate  PS CS Pro. (b) transpose 0 10 20 30 40 50 60 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0.22 0.24 A e v r e k c a p e g a t l a t n e y c Injection rate  PS CS Pro. (c) bitcomp 0 10 20 30 40 50 60 0.010 0.015 0.020 0.025 0.030 0.035 0.040 0.045 0.050 0.053 A e v r e k c a p e g a t l a t n e y c Injection rate  PS CS Pro. (d) hotspot Fig. 6.: Average packet latency in a 4×4 meshed NoC under diﬀerent traﬃc workloads with 5% burst injection rate. hybrid circuit-switched (CS) NoCs [11], Fig. 5 shows a schematic example under a simple traﬃc with these three diﬀerent connections. Compared with the normal PS NoCs, the network usage is saved by speculating the data blocks of incremental addresses in destination nodes, other than frequently sending separate request/response ﬂits from source/destination nodes. On the other hand, compared with CS NoCs, no extra circuit setup ﬂits [11] are required for the burst packets, and the network congestion is relieved because only part of the physical NoC link is exclusive other than the entire physical NoC link from the source node to the destination node. III. Evaluation and Results A. Burst-support NoC Simulation Results To evaluate the proposed burst-support NoC, we modify the cycle-accurate NoC simulator Booksim2.0 [12] to realize the proposed burst-support router. And, in order to compare the proposed design to some typical designs, we have also realized burst transfers in the CS NoC with two physical circuit planes and the baseline PS NoC, respectively. For fair comparisons, the burst requests are transferred as PS ﬂits in all these three networks and are responded by burst packets, CS packets and four subsequent single data transfer ﬂits in the proposed NoC, CS NoC and baseline PS NoC, respectively. erage packet latency results in a 4×4 meshed NoC under As a result, with a given burst injection rate, the avdiﬀerent traﬃc workloads are shown in Fig. 6, respectively. It is obvious that the CS NoC suﬀers a relative high network congestion in general because the entire transfer path (from source to destination) is exclusive. For example, in the hotspot traﬃc pattern, the CS NoC takes a signiﬁcant faster speed to reach the network saturation point because all nodes send packets to a destination node. Hence, it may be a good choice on network latency reduction for the slight burst workloads, but it is not suitable for the relative frequent fast data prefetch transfers in the proposed design. On the other hand, compared with the baseline PS NoC, the proposed design shows some 1A-3 40                                         1.2 1 0.8 0.6 0.4 0.2 0 a b l k c c s h . d o b y . c a e n n a l s w p a . r d a i x x 4 6 2 t f f l u . w a t e r . r y a r t a c e e v a r e g a Shared STLAC Fig. 7.: Normalized oﬀ-chip misses results. advantages on packet latency reduction because the network usage of the proposed burst-support NoC is saved by speculating the data blocks of incremental addresses in destination nodes, other than frequently sending separate request/response ﬂits from source/destination nodes in the baseline PS NoC. TABLE I: Conﬁguration Parameters CPU type 16-core, x86 OS Fedora 64-bit L1I/D cache 16kBytes, 4-way, private L2 cache 128kBytes/slice, 8-way, STLAC CPA threshold 10% Block size 64Bytes Prefetch length 4 Coherence protocol MOESI Distributed Directory 4×4, 2D Mesh NoC topology Routing algorithm dimension-order Baseline router 3-stage pipeline Main memory 512MBytes, 150 cycles latency B. Full System Simulation Results To model the proposed STLAC, we ﬁrst modify the cycle-accurate NoC simulator Booksim2.0 [12] to realize the hybrid burst-support NoC and integrate it into a multiprocessor simulator FeS2 [13] to model the tiled manycore system. FeS2 is improved from the GEMS multiprocessor simulator [14] and implemented as a module for Virtutech Simics [15]. And then, STLAC is implemented by modifying the ruby multiprocessor memory system module. The PARSEC [16] and SPLASH-2 [17] benchmarks with 16 threads on the 16-core platform are taken to analyze the oﬀ-chip misses, the network usage to evaluate the performance improvement. The main system conﬁguration parameters are given as Table I. All the benchmarks take the simsmal l inputset to save the simulation time. For quantitative comparisons, the total number of injected ﬂits into the network and the runtime (cycles) is taken as the metric for network usage and performance evaluation, respectively. We evaluated the proposed STLAC with the cache partition executed every 1000k cycles based on the real run100% 90% 80% 70% 60% 50% 40% 30% 20% 10% 0% a b l k c c s h . d o b y . c a e n n a l s w p a . r d a i x x 4 6 2 t f f l u . w a t e r . r y a r t a c e Victim Prefetch Fig. 8.: Hit distributions for diﬀerent benchmarks. 1.02 1 0.98 0.96 0.94 0.92 0.9 0.88 0.86 0.84 0.82 4 h y a p 6 c d e a d 2 s o n a x b n a a b l k c . . l . c r s w i x t . . f f l e e u c g e a a a e y v a a w t r r t r r Shared STLAC Fig. 9.: Normalized network usage results. time in general. Fig. 7 gives the oﬀ-chip misses results from the full system simulation. Obviously, due to the adaptive partition of the LLC based on diﬀerent spatial and temporal locality requirements of the running workloads, a great advantage on oﬀ-chip misses reduction over the traditional design has been achieved. The prefetch technique is proven to be especially beneﬁcial for the stream-like workloads, because it takes a high hit probability for the prefetched data blocks of the subsequent addresses in the LLC although most of the data will not be accessed again in the future. On the other hand, for the workloads which have good temporal locality, in order to minimize the cache pollution eﬀect, the proposed STLAC manages the prefetch part and victim part separately, and the prefetched data are limited to be ﬁlled in the prefetch buﬀer other than the entire cache space based on a local least recently used (LRU) replacement policy. Hence, the spatial and temporal locality of the running workloads can be exploited by the prefetch part and the victim part as much as possible, respectively. The oﬀ-chip misses are reduced by up to 43% on average and this reduction rises to more than 50% if the running workloads have good spatial and temporal locality, such as bodytrack, ﬀt, raytrace. To further investigate the eﬃciency of the adaptive cache design, Fig. 8 shows the cache hit distributions for each part in STLAC to show the diﬀerent memory access behavior of diﬀerent workloads. Then, the spatial and temporal locality diﬀerences among diﬀerent workloads are illustrated by the ratios of the hit rate (H R) between the prefetch part and the victim part directly, which are also the key basis for STLAC to distribute cache resources fairly for performance improvements. For exam1A-3 41 1.4 1.2 1 0.8 0.6 0.4 0.2 0 Shared STLAC . y d o b l a e n n a c . h c s k c l a b . p a w s x i d a r 4 6 2 x t f f . u l . r e t a w e c a r t y a r e g a r e v a Fig. 10.: Normalized performance results. ple, the workloads with high H R in the prefetch part have high spatial locality and achieve more performance gains from the prefetch technique, such as bodytrack ﬀt, raytrace. However, on the contrary, the workloads with high H R in the victim part have high temporal locality and achieve more performance gains from the victim cache, such as canneal, x264. Moreover, to present the advantage on network usage reduction due to the proposed burstsupport network, Fig. 9 shows the network usage results for diﬀerent workloads from the full system simulation. Due to the use of hybrid burst-support network for fast prefetch, the network usage is saved by 7.6% on average, which further helps the performance improvement. Fig. 10 shows the performance comparisons. It is obvious that by combining the proposed adaptive cache resource partition for spatial and temporal locality prediction with the burst-support network for fast data prefetch, STLAC can achieve signiﬁcant performance improvement. It improves 15% performance on average, and this improvement even rises to nearly 30% for these workloads with good spatial and temporal locality compared with the traditional shared LLC design. IV. Conclusion In this paper, an adaptive spatial and temporal localityaware cache and NoC codesign for tiled many-core systems is proposed. It optimizes the memory hierarchy by taking advantage of the diﬀerences of the spatial and temporal locality among the running workloads and exploiting a hybrid burst-support network for fast data prefetch. The spatial and temporal locality of the running workloads can be exploited by the prefetch part and the victim part as much as possible, respectively. This method is proven to distribute the cache resources more fairly and to have signiﬁcant advantages on oﬀ-chip misses reduction and performance improvement. It has good potentiality for multimedia and scientiﬁc computing applications. Acknowledgements This work is supported by the National Key Science and Technology Pro jects of China, and special thanks are due to the referees that have made several suggestions to signiﬁcantly improve the paper. 42 1A-3 "
2016,A high performance reliable NoC router.,"Aggressive scaling of CMOS process technology allows the fabrication of highly integrated chips, and enables the design of multiprocessors system-on-chip connected by the network-on-chip (NoC). However, it brings about widespread reliability challenges. Aiming to tackle the permanent faults on the router components, we propose a high performance, high reliability and low cost router design based on a generic 2-stage router. Four fault tolerant strategies are added in our reliable router. We exploit a double routing strategy for the routing computation(RC) failure, a default winner strategy for the virtual channel allocation (VA), a runtime arbiter selection strategy for the switch allocation (SA) failure and a double bypass bus strategy for the crossbar failure. Different from previous reliable routers, our design leverages the feature of pipeline optimization and routing algorithm to maintain the performance in fault tolerance especially under heavy network loads. Besides, our proposed router provides higher reliability with lower hardware consumption than previous reliable router designs.","8A-1 A High Performance Reliable NoC Router Lu Wang, Sheng Ma , Zhiying Wang State Key Laboratory of High Performance Computing, National University of Defense Technology College of Computer, National University of Defense Technology, Changsha, China luwang@nudt.edu.cn, masheng@nudt.edu.cn, zywang@nudt.edu.cn ∗ Abstract— Aggressive scaling of CMOS process technology allows the fabrication of highly integrated chips, and enables the design of multiprocessors system-on-chip connected by the networkon-chip (NoC). However, it brings about widespread reliability challenges. Aiming to tackle the permanent faults on the router components, we propose a high performance, high reliability and low cost router design based on a generic 2-stage router. Four fault tolerant strategies are added in our reliable router. We exploit a double routing strategy for the routing computation(RC) failure, a default winner strategy for the virtual channel allocation (VA), a runtime arbiter selection strategy for the switch allocation (SA) failure and a double bypass bus strategy for the crossbar failure. Diﬀerent from previous reliable routers, our design leverages the feature of pipeline optimization and routing algorithm to maintain the performance in fault tolerance especially under heavy network loads. Besides, our proposed router provides higher reliability with lower hardware consumption than previous reliable router designs. I. Introduction Moores Law scaling yields higher transistor density with each succeeding process generation, leading multiprocessors system-on-chip connected by the network-onchip (NoC). However, the extreme down scaling trends of CMOS technology have rendered transistors more susceptible to both permanent and transient faults[1]. It has been predicted that future designs will consist of hundreds of billions of transistors, where upwards of 10% of them will be defective due to wear-out and variation [2]. Thus, building a fault tolerant NoC system is indispensable, and designing a reliable NoC router is an important part of it. In order to address the permanent faults in NoC routers, previous work proposes some strategies to maintain the correction of the operation by modifying the baseline router [3, 4, 5]. BulletProof [3] employs N-modular redundancy techniques to provide fault tolerance. However, it brings heavy hardware overheads. Vicis router [4] exhibits great fault tolerance at a low area cost and incorporates error detection with system recovery. Recently, Pavan Poluri et al. [5] proposed a fault tolerant ∗ Corresponding Author router capable of tolerating multiple permanent faults in its pipeline. It employs a low cost correction circuitry and exploits idle time of existing resources to accomplish fault tolerance. Their design makes use of the inherent redundancy. However, borrowing the idle time of existing non-faulty resources results in poor performance under heavy network traﬃc. All these work aims to provide higher reliability with lower hardware consumption, but they neglect the performance degradation during the process of tolerating faults. By leveraging the feature of pipeline optimization and routing algorithm, we solve the performance loss problem and avoid the long waiting in the heavily-loaded network. In this article, we propose a high performance, high reliability and low cost NoC router. Our proposed reliable router is capable of detecting multiple permanent faults at real time and tolerating them through minor modiﬁcations on the generic 2-stage router [6, 7]. It contains diﬀerent fault tolerant strategies on the 4 main pipeline units, including the routing computation (RC), the virtual channel allocation (VA), the switch allocation (SA) and the crossbar. One interesting and dominant idea in our work is that the pipeline optimization can not only be used for improving performance, but also be used for fault tolerance. Lookahead routing determines the route of a packet one hop in advance [8]. The faulty RC can result in stall at the downstream router rather than the current router. Through our double routing strategy in the downstream router, the fault in the RC can be tolerated. Aggressive speculation performs the VA and SA in the same cycle [9]. It uses two parallel switch allocators for non-speculative requests and speculative requests respectively. Our proposed runtime arbiter selection strategy exploits the redundancy to guarantee the non-speculative requests can be normally granted with few hardware overheads. These strategies can result in some ﬂits being processed without the pipeline optimization. However, the performance of the entire work is still good even with heavy network loads because these strategies avoid long waiting for available resources. Our double bypass bus mechanism makes use of the feature of XY routing algorithm to tolerate the crossbar failure and it can bypass two ﬂits simultaneously to reduce 978-1-4673-9569-4/16/$31.00 ©2016 IEEE 712 8A-1 the performance loss. Furthermore, the default winner strategy for the VA uses a register to store a default winner VC if the arbiter is faulty. It brings little additional latency and maintains the performance with failure. In the experiment part, we compare the impact of latency caused by fault tolerance in our reliable router with the one proposed by Poluri [5]. The result shows that our design obtains better performance especially with heavy network loads. We also evaluate the extra area consumption and use Silicon Protection Factor (SPF) [3] to compare the reliability of our proposed router with other existing fault tolerant NoC routers. The overhead of the correction circuitry in our router is 16% lower than Poluri’s design [5] and SPF is 44.7% higher than that one. These results reveal that our proposed router achieves higher reliability and lower hardware consumption. This paper makes the following primary contributions: • Observes the feature of pipeline optimization and routing algorithm can be used for maintaining performance in fault tolerance. • Proposes a high performance, high reliability and low cost fault tolerant router design based on a generic 2-stage router. • Compares the performance, hardware consumption and reliability of our proposed reliable router with other existing fault tolerant NoC routers. II. Proposed reliable NoC router In order to address multiple permanent faults on a generic 2-stage router, we separately propose fault detection and tolerance mechanism on the RC, the VA, the SA and the crossbar. Our strategies make use of the advantage of pipeline optimization and the feature of XY routing algorithm. They can maintain the performance even with heavy network loads. A. Double Routing strategy for RC failure A 2-stage NoC router exploits the lookahead routing mechanism. The RC is used to compute the output port of the next hop in the ﬁrst stage. A permanent fault in the routing unit may result in deadlock in the deterministic routing algorithm. This fault has no eﬀect on the current router. In contrast, the ﬂit would be forwarded to the wrong output port in the downstream router. In order to provide fault tolerance, we add an extra stage in the downstream router to compute the current output port. Our double routing strategy is explained in Fig. 1. If the RC in router k is faulty, the ﬂit should use two cycles to compute its current output port and next output port in router k+1. Subsequent ﬂits will be stalled if the RC is busy. The main advantage is that no additional resources are required. Only when faults happening, an extra cycle is needed. Lookahead  routing Lookahead  routing Current node  routing Lookahead  routing RC  unit k-1 Fault RC  unit k RC  unit k+1 Fig. 1. Double routing strategy for the RC. input4 output4 input0 output0 input1 output1 input2 output2 input3 output3 Er ror1 input==output Fig. 2. Fault detection circuitry for the RC. Er ror2 illegal turn In this work, we employ the dimension order (XY) routing algorithm in the NoC. And we add a detection circuitry based on the hardware assertion mechanism [10]. The key idea is to have a simple hardware checker module. It compares inputs and outputs of the protected component and checks whether any functional rule is broken during the component’s operation. Fig. 2 describes our detection circuitry. Error1 signal is used to detect the invalid output direction. Error2 signal is used to detect the illegal Y to X turn in XY routing algorithm. B. Fault tolerant VA design B.1 VA fault scenario The VA is composed of two steps. In the ﬁrst step, every input with a head ﬂit arbitrates for an empty VC at the downstream router. When an arbiter associated with an input VC is faulty, the head ﬂit in this VC would not be allocated a downstream VC, resulting in the ﬂit being blocked. Hence, we have to provide fault tolerant mechanism to avoid this problem. The second step is used for arbitrating for an output VC. A faulty arbiter will result in that downstream VC not being allocated to any ﬂits in the current router. However, if the ﬂits could be re-allocated to other VCs in the output port, they are not blocked. So we do not need to tolerate this fault. B.2 Default winner strategy According to the VA fault scenario, we must tolerate the faulty input VC arbiter in the ﬁrst stage of VA. As shown in Fig. 3, we propose to add a register as the bypass path for an input VC arbiter. When the arbiter is faulty, we select the output VC in the register as the default winner. The output VC in the register is chosen from the current requests. The comparison unit compares the available buﬀers of diﬀerent output VCs. The VC with the most free buﬀers will be stored in the register. A 2:1 713 8A-1 available buffers  for outputVC1  (cid:266) available buffers  for outputVCn  QSET S R CLR Q QSET S R CLR Q InputVC arbiter is  faulty? Comparison Unit   defaul t winner VC Regis ter req.1 req.2 req.n InputVC Arbiter ... OutputVC Arbiter 2:1 mux Fig. 3. Default winner strategy for the VA. multiplexer chooses from the output of the arbiter and the identiﬁcation of VC in the register. By providing the VC in the register as the arbitration result, this mechanism can perform well even though all the input VC arbiters are faulty. Also, our strategy guarantees that all the ﬂits are allocated to diﬀerent output VC fairly. Although we do not need to tolerate the faults in the second stage, we need to decrease the performance loss caused by re-allocation. We restricts only the requests which output VC arbiter is non-faulty could be added to the VA. This approach avoids the cases where the ﬁrst stage of the VA is successful while the second stage arbiter is faulty. So no additional latency is caused by this method. Through the detection circuitry of arbiters, we can know whether a VC arbiter is faulty. As shown in Fig. 4, the hardware assertion mechanism [10] is used to detect three kinds of arbiter faults. If there is at least one request for the arbiter, but no one is granted, Error 1 signal will be set high. It violates the basic function of an arbiter. Similarly, Error 2 signal is used to detect the fault of a grant without a request. Error 3 signal is used to detect whether multiple bits are set to be high. C. Fault tolerant SA design C.1 SA fault scenario Switch allocation is composed of two steps. The ﬁrst step arbitrates between multiple requests from one input port. The second step arbitrates for one output port. If an arbiter in the ﬁrst stage is faulty, the ﬂits in this input port will be blocked. If the arbiter for an output port is faulty, the output port will be unreachable. C.2 Runtime arbiter selection strategy In a generic 2-stage NoC router, there are two parallel switch allocators. One allocator handles non-speculative requests, while the other one handles speculative requests [6]. Our reliable SA design leverages this hardware redundancy. The basic idea is to select arbiters of each allocator at runtime to ensure the ﬂits with non-speculative requests will not be blocked and each output port will be reachable. client  request Arbiter (cid:3304) (cid:3304) Error1: grant to nobody output grant vector Error3:multiple bits  set to be high Error2: grant without   a request Fig. 4. Fault detection circuitry for the arbiter. 2:1 mux 2:1 muxx SA_1_IN_i Arbiter control 2:1 mux 2:1 muxx SA_1_OUT_ j Arbiter SA_2_IN_i SA_2_OUT_ j 2:1 mux 2:1 muxx Arbiter Arbiter 2:1mux 2:1mux 1mux non-spec reques t spec  reques t Fig. 5. Runtime arbiter selection strategy for the SA. Correction circuitry is in the red rectangular box. Fig. 5 shows our reliable SA design. The correction circuitry is in the red rectangular box. In a baseline 2-stage router, non-speculative requests arbitrate for SA 1 IN i and SA 1 OUT j while speculative requests arbitrate for SA 2 IN i and SA 2 OUT j. If the arbiters in the nonspeculative switch allocator are faulty, some ﬂits would be blocked and thus inﬂuence the entire system. Our reliable SA design adds some 2:1 multiplexers to choose appropriate arbiters at runtime. For example, when arbiter SA 1 IN i is faulty, we can select arbiter SA 2 IN i to proceed the non-speculative requests. Then SA 2 IN i is used as a component in the non-speculative switch allocator. Subsequent speculative requests add to SA 1 IN i. They cannot be granted because SA 1 IN i is faulty. Then the ﬂits enter the non-speculative switch allocation stage in the next cycle and can be handled successfully. A similar strategy can be used for output port arbiters. Control signals come from the the current state of each arbiter. It can be obtained through the detection circuitry of arbiters described in Section B.2. Our reliable SA design can realize detection and selection of arbiters at runtime to avoid system stall. It brings few area overheads. For a 5-input port router, only 20 2:1 multiplexers are added. And it maintains the high performance even with heavy network loads because nonspeculative requests can always be handled successfully. 714 8A-1 3:1mux control Horizontal bypa ss bus in0 in1 in2 in3 in4 2:1 mux Vertical bypass bus out 0 out 1 out 2 out 3 out 4 Fig. 6. Double bypass bus strategy for the crossbar. D. A crossbar with double bypass bus The crossbar connects the input and output ports to facilitate a ﬂit to traverse from an input VC to an output port [9]. If the crossbar is faulty, the ﬂits will not be transferred to the downstream router. In order to solve this problem, we propose a crossbar with double bypass bus. As shown in Fig. 6, we add two bypass bus to allow ﬂits to bypass the faulty crossbar. Based on the features of XY routing algorithm, one of the bypass bus is used to traverse ﬂits from x-dimension or the local router and it connects all the output ports. The other one only bypasses the ﬂits from Y-dimension and it connects the Y-dimension output port or the ejection port. In a XY routing algorithm, most ﬂits transfer along one dimension. Only some ﬂits need to turn from X-dimension to Y-dimension. Hence, in most cases, there are no conﬂicts between these two bypass bus. And two ﬂits can be proceeded at the same time to decrease the performance loss especially for the heavily-loaded network. If multiple input ports connected to one bypass bus have ﬂits, they need to arbitrate and only one ﬂit can transmit through the bypass bus. The arbitration is performed in the second stage of switch allocation. If the failure of crossbar is detected, the second stage of switch allocation changes. We reuse two non-faulty output arbiters to arbitrate for double bypass bus. Requests from X-dimension are added to one arbiter while requests from Y-dimension are added to the other one. If there are conﬂicts between two bypass bus, we choose the ﬂits from X-dimension to transfer ﬁrst. Diﬀerent from Poluri’s design [5], our strategy can work normally even though all the multiplexers are faulty. And it has better performance than the single bypass mechanism in Vicis router[4]. Our basic crossbar is composed of multiple multiplexers. We use hardware assertion mechanism [10] to detect the faulty multiplexers. If the selection signal for one input port is set high and no ﬂits are leaving from the output port, the multiplexer is faulty. III. Performance analysis In order to compare the performance of our reliable router design with Poluri’s [5], we modify the booksim simulator [11] to support these two kinds of fault tolerant routers. And we compare the extra latency caused by the simulations are conducted on a 8 × 8 2D mesh network, correction circuitry for achieving fault tolerance. All the with 4VCs per port and 16 ﬂits per VC. Faults are injected randomly with each individual component having a fault rate of 0.1. To be fair, two kinds of reliable routers are injected with the same faults which can be handled properly in each design. A. Saturation throughput comparison We analyze the impact of average latency under the uniform random traﬃc pattern with the faults injected. Fig. 7a shows the average latency of Poluri’s reliable NoC router [5] with fault-free, the crossbar fault injected, the SA fault injected and the VA fault injected scenarios. We do not analyze the scenario of injecting faults on the RC because the duplicating RC strategy [5] has no impact on latency. It can be seen that the saturation throughput is only half of the fault-free scenario when the faults are injected to the SA and the crossbar. The saturation throughput decreases 18% when the faults are injected to the VA. Hence, Poluri’s design [5] shows poor performance under higher injection rates. The result of our proposed router is shown in Fig. 7b. Except for tolerating crossbar faults, other fault tolerant strategies bring minor performance decrease. The decrease of saturation throughput caused by the crossbar faults is due to that multiple ﬂits may arbitrate for one bypass bus with heavy network loads. More importantly, each strategy in our design can obtain higher saturation throughput than Poluri’s [5]. Poluri’s design [5] leverages resources sharing mechanism. Multiple ﬂits compete for a few non-faulty hardware resources. With the increase of injection rates, the competition aggravates and the performance degrades signiﬁcantly. Our reliable router has suﬃcient resources for the non-speculative SA, the RC and the VA. The fault tolerant strategy for the crossbar uses the feature of XY routing algorithm to decrease the competition. Hence, there is less performance loss in our design especially for the heavily-loaded network. B. Extra latency comparison B.1 Fault tolerant strategy for the VA Fig. 8a compares the average extra latency of our proposed default winner VA fault tolerance mechanism with the share arbiter strategy described in Poluri’s article [5]. The share arbiter strategy borrows the idle time of nonfaulty arbiters to proceed the requests for a faulty input VC arbiter. Our design brings little extra latency before the injection rate of 0.4 ﬂit/cycle. However, the average 715 20 25 30 35 40 45 50 55 60 0 0.1 0.2 0.3 0.4 injection rate(flit/cycle)  0.5 e g a r e v a l a t e n y c ( c y c l e ) fault-free SA faults injected VA faults injected crossbar faults injected (a) Average latency in Poluri’s proposed reliable NoC router 60 55 50 45 40 35 30 25 20 0 0.1 0.2 0.3 0.4 injection rate(flit/cycle)  0.5 e g a r e v a l a t e n y c ( c y c l e ) fault-free crossbar faults injected SA faults injected RC faults injected VA faults injected (b) Average latency in our proposed fault tolerant router Fig. 7. Impact of average latency with the faults injected in our design and Poluri’s. 0 0.05 0.5 1 1.5 2 2.5 3 0.15 0.25 0.35 0.45 e g a r e v a x e t a r l a t e n y c ( c y c l e ) injection rate (flit/cycle)  default winner share arbiter (a) Average extra latency comparison for diﬀerent strategies in the VA. 5 4.5 4 3.5 3 2.5 2 1.5 1 0.5 0 0.05 0.15 0.25 0.35 injection rate(flit/cycle)  0.45 e g a r e v a x e t a r l a t e n y c ( c y c l e ) runtime arbiter selection bypass faulty arbiter (b) Average extra latency comparison for diﬀerent strategies in the SA. 8 7 6 5 4 3 2 1 0 0.05 0.15 0.25 0.35 injection rate(flit/cycle)  0.45 e g a r e v a x e t a r l a t e n y c ( c y c l e ) double bypass bus share multiplexers single bypass bus (c) Average extra latency comparison for diﬀerent strategies in the crossbar. Fig. 8. Average extra latency caused by fault tolerant strategies. extra latency of share arbiter mechanism increases dramatically with the increase of injection rates. One reason is that our reliable design restricts only the requests which output VC arbiter is non-faulty could be added to the VA. It saves the additional 1 cycle for VA re-allocation in Poluri’s design [5]. The other reason is that the share arbiter strategy may result in waiting an available nonfaulty input VC arbiter for a long time under high injection rates. B.2 Fault tolerant strategy for the SA We compare the extra fault tolerance latency of the SA between our runtime arbiter selection strategy and the bypass faulty arbiter mechanism proposed in Poluri’s work [5]. Bypass faulty arbiter mechanism uses a register as a bypass path to tolerate the fault in the input port arbiter and another arbiter as a secondary path to tolerate the fault in the output port arbiter. As shown in Fig. 8b, when the injection rate is under 0.2 ﬂit/cycle, the bypass faulty arbiter strategy shows better performance than our strategy because it has no inﬂuence on pipeline. However, with the increase of injection rate, our design could maintain the extra latency in 1 cycle while the extra latency of the bypass faulty arbiter increases quickly. In Poluri’s design [5], multiple ﬂits with diﬀerent output port share one non-faulty arbiter in the second stage of the SA. It results in the increase of extra latency with the increase of injection rates. Our design has suﬃcient non-faulty arbiters to handle the non-speculative SA requests, so it only causes one extra cycle latency even when more ﬂits are injected to the network. B.3 Fault tolerant strategy for the Crossbar In order to evaluate our double bypass bus mechanism, we compare our design with the single bypass bus mechanism in Vicis router [4] as well as the share multiplexers strategy in Poluri’s design [5]. The single bypass bus strategy uses a single bypass bus to transfer ﬂits when the crossbar is faulty. The share multiplexers strategy selects another multiplexer as a second path to reach the output port if one multiplexer is faulty. According to Fig. 8c, our double bypass bus strategy improves the performance greatly comparing with the single bypass bus strategy [4] and decreases the extra latency by 5 cycles. This is due to that our design can bypass two ﬂits in most cases while the single bypass mechanism only transfer 1 ﬂit per cycle. Our design has poorer performance than Poluri’s design [5] when the injection rate is under 0.2 ﬂit/cycle because more than 2 ﬂits can be transferred in the share multiplexers mechanism. However, the selection of the second 8A-1 716                             8A-1 ) 2 m μ ( a e r a 6000 5000 4000 3000 2000 1000 0 area overhead baseline baseline+fault tolerance baseline+fault tolerance +fault detection RC SA VA XB Fig. 9. Area overhead of each unit with respect to the baseline router. path in share multiplexers mechanism does not consider the features of XY routing algorithm. And most ﬂits arbitrate for one non-faulty multiplexer when the injection rate is high. Our strategy avoids this problem and shows better performance than Poluri’s design [5] with high network loads. IV. Reliability Analysis A. Hardware consumption analysis We develop the generic 2-stage router and our proposed reliable router in Verilog HDL and synthesize in NangateOpenCell 45nm library under 1.0V and 0.5GHz, with Synopsys Design Compile. Fig. 9 shows the area overhead of each unit with respect to the baseline router. The area of SA, VA and crossbar increase by 8.6%, 39.9% and 51% respectively when our proposed fault tolerant circuitry is added. No additional fault tolerant circuitry is needed for the RC. The area overhead of the entire router is 9.8% comparing with the baseline router. And it increases to 27% when incorporating the detection circuitry. B. Reliability comparison using SPF We discuss the reliability improvement of our proposed router in comparison to Poluri’s design [5], BulletProof [3] and Vicis [4] using Silicon Protection Factor (SPF). SPF is deﬁned as the ratio of mean number of faults required to cause a failure and the area overhead incurred due to the correction circuitry [3]. In the RC unit, our strategy can tolerate a maximum of 5 faults. Each fault aﬀects the functionality of one RC component per input port. It requires the RC of the adjacent router to be non-faulty. On the other hand, a minimum of 1 fault can cause failure if the adjacent router has faulty RC. The SA can tolerate at most 10 faults, with all the arbiters in one switch allocator being faulty. The other one can still handle non-speculative switch allocation requests normally. However, if the two arbiters for the same port in two switch allocators are both faulty, the switch allocation will be blocked. Thus, the minimum TABLE I SPF comparison of our proposed reliable router with other fault tolerant router designs Architecture BulletProof Vicis Poluri’s design Our reliable router Area Faults to cause failure 52% 3.15 42% 9.3 31% 15 27% 21 SPF 2.07 6.55 11.4 16.5 number of faults to cause failure is 2. The VA can tolerate a maximum of 20 faults, with each input VC arbiter being faulty. A minimum of 2 faults can result in failure if the input VC arbiter and its default winner path both have faults. In the crossbar, a minimum of 2 faults can cause failure with one faulty bypass bus and one faulty multiplexer. It can tolerate 5 faults at most. Hence, the minimum number of faults to cause failure is calculated as min {1(RC), 2(SA), 2(VA), 2(XB)}, which is 1 fault. The maximum number of faults tolerated by the router is the sum of maximum faults tolerated by each unit, which is 5(RC) +20(VA)+10(SA)+5(XB)=40 faults. An additional fault would result in failure. So the maximum number of faults to cause failure is 41. The mean number of faults to cause failure is 21. Area overhead is 27%. SPF of our proposed reliable router is 21/1.27=16.5. Table I shows SPF comparison of out proposed reliable router with other fault tolerant router designs. The SPF value of other designs are obtained from related work [4, 3, 5]. Comparing the SPF values, we can conclude that our proposed router is more reliable than the previous designs. V. Conclusion We propose a low cost reliable router design based on a generic 2-stage router. It uses diﬀerent fault tolerant strategies on the RC, the VA, the SA and the crossbar respectively to provide high performance and high reliability. The simulation result shows that our design decreases the extra latency and improves performance in the heavily-loaded network. More importantly, the synthesis result reveals that our design only induces a few hardware overheads. By using SPF as the evaluation metric, our proposed reliable router obtains better reliability compared with other fault tolerant routers. The fault tolerant strategy for each unit can be used on other routers. The idea of using the feature of pipeline optimization and routing algorithm can be applied for future fault tolerance designs. VI. Acknowledgements This work is supported by the National Natural Science Foundation of China (No.61572058, No.61303065, 717 8A-1 No.61272144, No.61202121), National High Technology Research and Development Program of China (No.2012AA010905) and Research Fund for the Doctoral Program of Higher Education of China (No.20114307120013). "
2016,FoToNoC - A hierarchical management strategy based on folded lorus-like Network-on-Chip for dark silicon many-core systems.,"In this dark silicon era, techniques have been developed to selectively activate nonadjacent cores in physical locations to maintain the safe temperature and allowable power budget on a many-core chip. This will result in unexpected increase in the communication overhead due to longer average distance between active cores in a typical mesh-based Network-on-Chip (NoC), and in turn reduce the system performance and energy efficiency. In this paper, we present FoToNoC, a Folded Torus-like NoC, and a hierarchical management strategy on top of it, to address this tradeoff problem for heterogeneous many-core systems. Optimizations of chip temperature, inter-core communication, application performance, and system energy consumption are well isolated in FoToNoC, and addressed in different design phases and aspects. A cluster-based hierarchical strategy is proposed to manage the system adaptively in several different control levels. Compared with mesh-based systems on a set of synthetic and real benchmarks, FoToNoC can achieve on average 39.4% performance improvement when similar temperature conditions are maintained, and the proposed strategy can further reduce the total energy consumption by up to 42.0%.","8A-3 FoToNoC: A Hierarchical Management Strategy Based on Folded Torus-Like Network-on-Chip for Dark Silicon Many-Core Systems ∗ Lei Yang, Weichen Liu, Weiwen Jiang, Mengquan Li, Juan Yi, Edwin Hsing-Mean Sha College of Computer Science, Chongqing University, Chongqing, China 400044 {leiyang, wliu, jiang.wwen, mengquan, jenni1987, edwinsha}@cqu.edu.cn pattern 1 pattern 2 pattern 3 pattern 4 Random Abstract— In this dark silicon era, techniques have been developed to selectively activate nonadjacent cores in physical locations to maintain the safe temperature and allowable power budget on a many-core chip. This will result in unexpected increase in the communication overhead due to longer average distance between active cores in a typical mesh-based Network-on-Chip (NoC), and in turn reduce the system performance and energy efﬁciency. In this paper, we present FoToNoC, a Folded Torus-like NoC, and a hierarchical management strategy on top of it, to address this tradeoff problem for heterogeneous many-core systems. Optimizations of chip temperature, inter-core communication, application performance, and system energy consumption are well isolated in FoToNoC, and addressed in different design phases and aspects. A cluster-based hierarchical strategy is proposed to manage the system adaptively in several different control levels. Compared with mesh-based systems on a set of synthetic and real benchmarks, FoToNoC can achieve on average 39.4% performance improvement when similar temperature conditions are maintained, and the proposed strategy can further reduce the total energy consumption by up to 42.0%. I . IN TRODUC T ION Given aggressive Multiprocessor System-on-Chip (MPSoC) design targets, Network-on-Chip (NoC) deliveres low latency, high performance and low power with the scaling technology. Continuous technology scaling leads to a utilization wall challenge, silicon chips cannot be fully utilized that rises Dark silicon problem [1][2][3]. Dark silicon refers to the phenomenon that a fraction of many-cores has to become “dark” or “dim” in order to maintain the systematic allowable power budget and safe temperature reliability of the chip, which induces new design challenges in NoC-based many-core architectures, such as the management on performance, energy and chip thermal. Facing challenges brought by dark silicon, the tradeoff on system performance, energy efﬁciency and chip thermal control is increasing difﬁcult for systematical management. Applications prefer to be assigned on adjacent cores to decrease communication delay, while running cores prefer to be surrounded by dark cores for better heat dissipation. The increased communication overhead induced by dark cores would require cores running faster to mitigate the performance loss and in turn increases energy consumption. Therefore, hardware and software studies, separately performance improvement and thermal management, are impractical for deep optimization on many-core systems. ∗Corresponding author: Weichen Liu. Email: wliu@cqu.edu.cn. 978-1-4673-9569-4/16/$31.00 ©2016 IEEE 725 8*8 Alpha  21264 core array Amount of Dark  Silicon is 50% Tsafe = 81.8 oC(cid:13)(cid:1)(cid:1)(cid:1) #active cores = 32, 22 nm technique, @2.6 Ghz, 64GB Memory, McPAT[4]. peak=80.00 peak=82.17 peak=82.15 peak=72.81 peak=75.67 Fig. 1. Impact of ﬁve different dark silicon decisions on the thermal proﬁles. Contradiction: - Pattern 1, 2, 3  have better  performance than 4. - Pattern 4 is the  best dark silicon  pattern while with a  nearly worst  performance. Pattern 1 Pattern 2 Pattern 3 Pattern 4 Random Fig. 2. Impact of ﬁve different dark silicon decisions on system performance. Motivated: As a mesh-based NoC shown in Fig.1, ﬁve patterns result in extremely different thermal states after computation. As conﬁrmed in [3][5], activating adjacent cores as pattern 1, 2 and 3 leads to thermal hotspots and high peak temperature (82.17◦C) to threaten chip temperature reliability. In pattern 4, active cores are evenly distributed on the chip that the heat conducting effect is alleviated and the peak temperature is 9.3◦C lower. However, dark silicon patterns have different effects on the overall performance. Long distance communication in pattern 4 increases delay which consequently diminishes system performance (37.4% worse) in Fig.2. It is crucial to trade-off application performance and thermal in the dark silicon era. In this paper, we present FoToNoC, a novel Folded Toruslike NoC based hierarchical management strategy for dark silicon many-core systems. It is a superposition of regular and application-speciﬁc NoC topology reconﬁguration utilized for dark silicon chip, such that physically distributed cores are interconnected with reduced intercommunication cost and organized in logically condensed virtual processor clusters. Summarizing, the main contributions of this paper are: i) Isolate and address the mixed design concerns of hierarchical hardware software co-design in the dark silicon era. ii) Propose a physical-logical isolated architecture organization to keep safe regarding temperature reliability of the chip via distributed core activation, and reduce communication overhead through folded torus-like “dark silicon-friendly” architecture. iii) Propose efﬁcient strategies to maximize the beneﬁts of FoToNoC in the dark silicon era, and manage heterogeneous cores in clusters with DVFS for optimized chip temperature, application performance and system power consumption. The rest of this paper is organized as follows: Section II presents the related works; Section III presents hierarchical organization of many-cores and Section IV presents cluster management strategies; Experimental results and conclusion are presented in Section V and Section VI, respectively. I I . R ELATED WORK S To decrease communication latency, physical express links and virtual express channels (EVCs) were proposed to tackle the long distance in many-core architectures. Applicationspeciﬁc long-range links added in [6] signiﬁcantly reduced average packet latency, which exploited the beneﬁts of complete regular and partial topology customization. Inserting longrange links introduces overhead due to the additional wires, extra ports and repeaters, which are also application-speciﬁc and not for dissimilar applications. Skip-links [7] and SMART NoC [8] dynamically reconﬁgured topologies based on the trafﬁc, and set up crossbars at intermediate routers to reduce latency. Long physical links connect far away routers, leading to reduction in network diameter, thereby saving latency and power, however, requiring additional router ports, larger crossbars and extra physical channels [9]. Application-speciﬁc links are specialized but not compatible for applications with various requirements. They need non-trivial changes to adapt to the emerging new requirements brought by dark silicon, in which patterns of cores on the chip are further complex with temperature, power, scalability, and even more considerations. Works addressed dark silicon problems [5][10][11][12], based on different design philosophies. Authors in [10] proposed a power management framework to obtain the powerperformance tradeoff in asymmetric many-cores. Zhan et al. [13] investigated topological/routing support and thermalaware ﬂoorplan for sprinting process with an efﬁcient network power management scheme. Authors in [14] combined heterogeneous thread-to-core mapping, dynamic work and power partition on a runtime scheme to improve performance. Shaﬁque et al. [3] addressed challenges including heterogeneous architecture synthesis, design space exploration, NoC design and run-time power with thermal management hoping to fully exploit the abundance of transistors in dark silicon era. Performance efﬁciency is greatly enhanced in these works. However, run-time inter-chip communication, which is relevant to application performance, is not addressed combined with chip thermal management. New challenges are brought by dark silicon in many-cores architectures, system performance optimization is not straightforward parallel operations, cooperation with power management is the ultimate solution for hardware/software co-design. We focus on a superposition of architecture organization, FoToNoC in physical-logical view, neither restricted to application speciﬁc nor regular mesh, to perceive the respective responsibilities of hardware/software in different hierarchies. I I I . H I ERARCH ICA L ORGAN I ZAT ION O F FOTONOC Hoping to fully exploit the available transistors in dark silicon era, the principal goal is to save power consumption, maximize system performance and keep safe temperature. On 8A-3 regular NoCs, however, long distance communications of distributed active cores have impact on performance and series of problems. This induces an apparent contradiction: i) Applications are mapped onto logically adjacent cores with the minimum communication delay for achieving high performance; ii) To keep the safe temperature, tasks should be executed on physically decentralized active cores for better heat dissipation. We hierarchically separate it in physical view and logical view to improve performance and minimize temperature. A.MotivationalExampleonPhysicalandLogicalView We design FoToNoC, a clustered Folded Torus-like NoC, to appropriately cooperate with dark silicon pattern for achieving high performance and low power consumption for its inherent potential. A motivational example of clustered management strategy on FoToNoC is illustrated in Fig.3. and independent. A task graph T = (V , E , te, tc, r , d), is an Applications, modeled as task graphs, are run-to-completion edged weighted Directed Acyclic Graph . V is a set of task nodes; E is a set of weighted edges. For v ∈ V , te(v) is the e = (u, v), e ∈ E , tc(e) represents the communication latency execution time of task v at the nominal frequency. For edge from node u to v . For T , r(T ) is the release time and d(T ) is the strict deadline. Applications are mapped to FoToNoC, which is based on the generation of ARM big.LITTLE architecture. H = (I P , CS , F ) is a heterogeneous folded torus. I P , I P = {c1 , c2 , ..., cN } is a set of cores connected as folded torus topology. Cluster set CS = {cs1 , cs2 , ..., csM } contains M types of cores. Within a cluster, cores are the same type and directly connected. fi ∈ F is a frequency regulator for cluster csi that each core in csi runs at the same frequency fi . Fig.3 is an example of mapping T to H with four clusters, b (big cores with high frequency), M1 , M2 and L (LITTLE cores with low frequency). According to the strategy (Section IV), tasks are mapped decentralized on the chip in physical chip, such as tasks t1 and t2 are mapped to c1 and c3 , respectively. However, they are directly connected in logic view. Physically distributed and logically adjacent are exactly what we pursue in physical view and logical view. Physical view Different mappings on active cores in physical arrangement generate different thermal proﬁles. We theoretically determine dark silicon pattern, which is a layout of cores distribution, to directly minimize temperature via the consideration of conductance between two heatsink elements based on Fourier’s law [15]. We choose the dark silicon patterns as Fig.1 pattern 4 and Fig.3 in terms of the theoretical foundation of Fouriers law, in which active cores are distributed every other one that the thermal states are averagely better than irregular solutions. By analogy, dark silicon patterns of large-scale NoCs can be obtained and conﬁrmed as the best power budget utilization [3][5][11][13]. However, the performance is worse due to the long distance communication (e.g., 37.4% worse in Fig.2 pattern 4), especially for communication intensive applications. As we stated, application performance is determined by topologies of multiple cores, hence we continue the exploration on the common used Mesh, Unfolded Torus and Folded Torus topologies illustrated in Fig.4 (b), (c), (d), for intensive study [16][17]. Notice 726 t2 t1 Cluster with big cores High Performance    (big core) Low Performance (LITTLE core) Logical Mapping Physical Locations Application T Logical View Physical View b M2 L 64-core System 1 3 5 7 49 33 17 53 55 37 39 51 35 19 21 23 active core dark core running core M1 t2 t1 t2 t1 t3 t3 t4 t3 t4 t5 t5 t5 t6 t6 t4 t6 Fig. 3. Application mapping on FoToNoC in physical and logical views. C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 C11 C12 C13 C14 C15 C16 C17 C18 C19 C20 C21 C22 C23 C24 C25 C26 C27 C28 C29 C30 C31 C32 C33 C34 C35 C36 C37 C38 C39 C40 C41 C42 C43 C44 C45 C46 C47 C48 C49 C50 C51 C52 C53 C54 C55 C56 C57 C58 C59 C60 C61 C62 C63 C64 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 35 36 37 38 39 40 43 44 45 46 47 48 51 52 53 54 55 56 59 60 61 62 63 64 (b) 33 34 41 49 42 50 57 58 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 1 2 3 4 5 6 7 8 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 45 46 47 48 41 42 43 44 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 (a) (d) (c) Fig. 4. (a) Physical views of many-core systems; (b) Mesh; (c) Unfolded Torus; (d) Folded Torus topologies. that 64 cores on the chip are arranged in 8 × 8 array of tiles as shown in Fig.4 (a). On physical platform, cores are numbered from c1 to c64 for uniﬁcation in the following discussion. Logical view Logical connections of cores directly inﬂuence performance and power consumption in many-core systems. Folded torus has better performance by reducing communication overhead with the increasing packet injection as shown in Fig.5. Additionally, cores are linked to every alternate cores in both horizontal and vertical directions, that applications can be mapped physically distributed and logically adjacent, on which communication latency is reduced with lower temperature. To explore the inherent properties of physical construction and topology connection of folded torus, we logically simplify it into an equivalent connections in Fig.6. Cores having minPacket Injection Rate  (pkt/s) P c a k e t L a t e n c y Fig. 5. Comparison of average network latency among different topologies. imum hop count are logically arranged closely to each other for easy observation. We virtually remove curve links between cores (such as (1, 2), (9, 10),...,(57, 58)), which are physically adjacent on the chip and may lead worse temperature. Consequently, the clustered FoToNoC in Fig. 7 is constituted with four virtual clusters and 16 cores within a cluster are dispersed in physical construction, e.g., four clusters in Fig.3. Tasks are preferentially mapped to ambient cores on FoToNoC with less communication delay, like cores 1, 3 and 17 in Fig.7. Meanwhile, those cores are distributed on ﬂoorplan in Fig.4 (d) that the chip would be in a proper condition of heat distribution. The novel Physical-logical view is vividly reﬂected by FoToNoC with dark silicon, which perceives the respective responsibilities of hardware and software hierarchically as the example in Fig.3. Target on mapping a set of task graphs to H with the safe temperature constraint, we propose cluster management strategy on FoToNoC, such that the maximum utilization of available cores, the maximum system performance and the minimum energy consumption can be achieved. IV. C LU S T ER MANAG EM EN T S TRAT EGY FoToNoC can beneﬁt the dark silicon and address the tradeoff between application performance and chip temperature in terms of the special physical arrangement and logical interconnection. On top of it, we propose a hierarchical Cluster Management Strategy (CMS), Algorithm 1, to maximize the beneﬁts for application performance, chip temperature and system power consumption. It contains two phases including intercluster management and inner-cluster mapping on clustered heterogenous architecture to select a deadline satisﬁed cluster for run-time application, and then map it onto cores for efﬁcient execution by DVFS (Each type of cores have the maximum frequency value as in Table I, under which there are 9 frequency levels greater than the lowest frequency threshold). 1 3 5 7 49 33 17 53 55 37 39 51 35 19 21 23 57 41 59 61 63 25 9 43 45 47 29 31 27 11 13 15 2 4 6 8 50 34 18 54 56 38 40 52 36 20 22 24 58 42 60 62 64 26 10 44 46 48 30 32 28 12 14 16 Fig. 6. Equivalent topology of the folded torus based 64-core system. 1 3 5 7 49 33 17 37 39 53 55 51 35 19 21 23 57 41 59 61 63 25 9 29 31 43 45 47 27 11 13 15 2 4 6 8 50 34 18 38 40 54 56 52 36 20 22 24 58 42 60 62 64 26 10 30 32 44 46 48 28 12 14 16 Fig. 7. The folded torus managed in four isolated virtual clusters. 8A-3 727     In CMS, applications are mapped to virtual cluster which can satisfy the deadline constraint. Within a cluster, the efﬁcient algorithm [18] is employed to map tasks to cores by Dominant Sequence Clustering (DSC) method [19] for the maximum parallelism, that the optimized application performance can be achieved. Running cores within virtual clusters are physically distributed on the plantform that the chip will be in a ﬁne thermal state to provide more application execution in the future. Consequently, system energy consumption will be minimized in CMS since the reduced communication delay allows more processor computation time and in turn allows lower processor frequency scaling and consequently lower energy consumption on processing. Inspired by ARM big.LITTLE architectures, heterogenous virtual clusters are managed in CMS which can further keep the chip in low power mode by mapping applications on little cores to reduce energy consumption. Basically, CMS deﬁnes two event handlers that deal with the management of FoToNoC when: 1) an application arrives and requests for execution; 2) an application ﬁnishes and releases system resources. It is composed of two main functions: C luster M anager (line 1-23) and M ap to C lutster (line 24-41). Function C luster M anager is triggered by a released application T , and determines mapping from T to cluster csID at f req . Function M ap to C luster checks if T can execute at release time r(T ) and ﬁnish before deadline d(T ) Algorithm 1 Cluster Management Strategy (CMS) (bool, ID , f req , tstart ) = C luster M anager(AC, T , Γ) else if f lag = 0 then end if 10. end for (f lag , freq , tstart ) = M ap to C luster(T , id(cs), Γ); if f lag = 1 or (f lag = 0 and freq < fid(cs) ) then return (T RU E ,id(cs),max{fid(cs) min , freq },tstart ); T M P ID = id(cs); break ; 1. Sort active cluster set AC in ascending order by frequency; 2. csmin ← the cluster in CS − AC with the lowest frequency; 3. for cs in ordered list AC do 4. 5. 6. 7. 8. 9. 11. for cs in ordered list CS − AC do (cid:3) , f 12. 13. 14. 15. 16. 17. 19. if f lag = 0 then 20. (cid:3) (cid:3) (cid:3) = 0 then (cid:3) = 1 then else if f lag end if 18. end for (f lag if f lag break; return (T RU E , id(cs), f start ) = M ap to C luster(T , id(cs), Γ); return (T RU E , T M P ID , fID min , tstart ); start ); req , t req , t (cid:3) (cid:3) 21. else 23. end if 22. return (F ALSE , -1, -1, -1); (f lag , f req , tstart ) = M ap to C luster(T , ID , Γ) freq ← map in cluster(T , csID , d(T ), r(T )); if freq ≤ fID max and C alcT (T , ID , freq ) < Γ then 24. if csID is dark then 25. 26. return (1, freq , r(T )) 27. 28. return (-1, freq , r(T )) 29. 30. 31. else if csID is active then tf in ← the time that all tasks on csID have ﬁnished; 32. 33. 34. freq ← map in cluster(T , csID , d(T ), tf in ); if freq ∈ [fID min , fID max ] and C alcT (T , ID , freq ) < Γ else end if then 35. 36. 37. 38. 39. 40. else end if 41. end if else if freq < fID min then return (1, freq , tf in ); return (0, freq , tf in ); return(-1, freq , tf in ); 8A-3 calling M ap to C luster . 1) Application T arrives. 2) Application T ﬁnishes on cs at f req . frequency freq is obtained by map in cluster(T , csID , d(T ), on cluster csID under temperature Γ. The minimum required r(T )), such that T can execute on csID without deadline miss. We use MatEx [20] in C alcT (T , ID , f req) to track the power trace of cores and calculate temperatures of cores when T is executed on cluster csID at frequency f req . MatEx [20] is an efﬁcient transient and peak temperature computation tool for compact thermal models which provides run-time thermal supervision in our strategy. It will be mapped to active cluster cs in AC (line 3-10) or dark cluster cs in CS -AC (cs will be powered on if the application is mapped) (line 11-18) by If T can be mapped to an active (Line 31-32). Otherwise, T will be mapped to a new cluscluster, it will start after currently running applications ﬁnish ter at the available lowest frequency for energy saving. The C luster M anager tries to ﬁnd one cluster for T until it is failed due to deadline miss on all clusters (line 19-23). tions keep running on cs, then turn off cs; ii) application T (cid:3) If: i) no applicais running on cs, then T (cid:3) keeps running at f req or at a lower frequency by scaling. In this strategy, we calculate transient temperatures to guarantee the validation of new application mapping at run-time by [20], and dynamically scale frequency for energy saving. Since an application is mapped to a cluster with the minimized energy consumption, it will maintain the optimality when a new application requests to execute. The new application will either be accepted by CMS if assigning it to a different cluster keeps the safe temperature threshold, or rejected and scheduled in the future if it violates the dark silicon restriction. Whether new applications could be mapped is based on the temperature calculated by the re-evaluation of all other clusters on the chip. Actually, if the new application cannot wait for executing after the current application, it can be executed simultaneously by frequency scaling. We do not present the ﬁne-grained method of overlap execution in this algorithm, but it is implemented for run-time applications in evaluation experiment. V. P ER FORMANC E EVA LUAT ION In this section, we conduct two comprehensive sets of experiments on a large group of synthetic benchmark applications (Task Graph For Free (TGFF) suit [21]) and real benchmark applications (H.264 video decoder) to evaluate the proposed management strategy in dark silicon many-core systems. Computation power of four types of cores on the heterogeneous FoToNoC are detailed in Table I. We obtain chip’s thermal steady states by HotSpot v 5.02 [22] with the conﬁguration in grid model mode, and run-time transient temperatures and peak temperature of the chip are obtained from the recently released MatEx [20]. We use the default parameters with an ambient temperature 45◦C, and a threshold temperature 81.8◦C. We build a FoToNoC system simulator in Python to simulate deadline-constrained application executions and evaluate CMS strategy and run experiments on a workstation with Intel(R) Xeon(R) E5-2650 at 2.6GH z and 64GB memory. 728 C LU S T ER TY P E S AND COR E CON FIGURAT ION S TABLE I Cluster type Freq (GH z ) Cache level Power (W ) Area (mm2 ) L 3.0 L2 3.715 0.816 M1 3.5 L2 6.544 1.046 M2 4.0 L2 19.691 1.086 b 4.5 L2 28.808 1.178 Technology node: Alpha 21264 cores in 22-nm technology. Power: Per-core power from McPAT [4]. A.Comparison of thermal distribution and application performanceonFoToNoCandMesh-basedNoC In mesh platform shown in Fig. 4 (b), 64 cores are divided into four symmetrical clusters: cluster 1 (numbered 1-4, 9-12, 17-20 and 25-28), cluster 2 (5-8, 13-16, 20-24 and 29-32), and similar for cluster 3, 4. Results on steady state heat distribution of mesh and FoToNoC are shown in Fig. 8. Generally, tasks are mapped next to each other on mesh for achieving higher performance while the heat cannot be dissipated effectively. Though mesh obtains similar performance as FoToNoC, the mapping induces hotspots on the chip. It is mitigated in FoToNoC on FoToNoC could also decrease temperature reaching 9.3◦C as which tasks are physically distributed on the chip (Section III). observed in Fig. 1, meanwhile achieve better performance. We then compare application performance with groups of applications, in which applications have the same mapping on physical cores on both of Mesh-based and Folded Torus-based NoCs. The chip temperatures of mesh and FoToNoC are expected to be similar, and we compare results on application scheduling in both systems. As shown in Fig.9, applications generated by TGFF with the number of task nodes increasing from 5 to 39 are scheduled on mesh and FoToNoC. Applications are executed on the same set of cores on physical locations, the performance in terms of schedule length on FoToNoC is on average 39.44% better due to the logical connection (Section III). It convincingly proves that FoToNoC organization can achieve greatly reduced communication overheads and in turn transform to signiﬁcantly improved performance for dark silicon many-core systems. Combine this set of experiments, the proposed FoToNoC system organization could result in consistent improvement in both chip temperature and application performance with physical and logical optimization. It outperforms mesh-based systems for addressing new challenges in the dark silicon era. B.Evaluationofapplicationperformanceandenergyconsumptionon theclustermanagementstrategy Next, we evaluate the proposed cluster management strategy (CMS) on FoToNoC architecture compared with a management (a) Thermal states on Mesh Fig. 8. Comparison of thermal distribution on Mesh and FoToNoC. (b) Thermal states on folded torus (apps/s) 0.0017 0.0020 0.0025 0.0031 0.0036 0.0040 0.0045 0.0050 0.0056 0.0059 0.0077 0.0100 0.0125 0.0200 0.0250 0.0500 0.1000 729 8A-3 5000 Folded Torus Mesh 4000 3000 h t g n e L e l u d e h c S 2000 1000 0 Fig. 9. Comparison of performance on Mesh and FoToNoC dark silicon. TGFF application strategy (NCMS) that has the same architectural arrangement in four clusters and mapping algorithms but does not apply our cluster-based management. In NCMS, cores run at ﬁxed frequency without ﬁne-grain dynamic frequency scaling, and the deadline miss of new applications will trigger new cluster to be powered on immediately if possible. Under the safe temperature validation by MatEx [20], CMS and NCMS are extensively evaluated on the efﬁciency of the proposed strategy for application performance and energy consumption. In our strategy, the energy consumption will be minimized due to three reasons. First, folded torus results in lower average communication cost due to less routers and links passed. Second, reduced core-to-core communication delay allows more processor computation time on average with the same deadline constraint, which allows lower processor frequency scaling and consequently lower energy consumption on processing (certainly lower temperature). Third, inspired by the ARM big.LITTLE architecture, we can even keep the chip in low power mode by mapping applications on little cores to reduce energy consumption. As shown in Table II, with the increasing injection rate (generated by Negative Exponential Distribution [23]) for a set of 17 applications, the deadline miss rate (R) is signiﬁcantly reduced (on average 10.38%) by the proposed CMS compared to NCMS. For fair comparison on energy consumption, we compare the total energy consumed by the same set of successfully executed applications in both CMS and NCMS, since the two strategies may drop different applications. Results show CMS provides signiﬁcant energy saving of 45.02% on average compared to NCMS. TABLE II COM PAR I SON O F NCMS (NC) AND CMS (C) ON D EAD L IN E M I S S RATE (R) AND EN ERGY CON SUM P T ION (E ) FOR TGFF A P P L ICAT ION S W I TH VAR IOU S IN J EC T ION RATE S Rate in EN C /J EC /J E Impv Rate in NCMS CMS RN C RC 0.00% 0.00% 0.00% 5.88% 5.88% 11.76% 11.76% 11.76% 23.53% 5.88% 23.53% 35.29% 35.29% 47.06% 52.94% 70.59% 70.59% 17988.72 25361.99 20313.29 23527.44 22876.37 24390.85 19487.82 22117.87 21357.88 26297.28 18347.37 21134.16 22930.42 20199.93 14456.66 12052.38 14001.3 0.00% 10844.15 0.00% 11893.03 0.00% 12337.59 0.00% 12482.4 0.00% 17523.37 0.00% 9626.71 5.88% 16675.62 5.88% 15743.38 5.88% 14129.43 5.88% 10278.83 5.88% 14645.48 23.53% 14653.93 29.41% 7054.49 23.53% 9867.29 41.18% 7895.71 47.06% 8460.49 52.94% 8751.58 Average energy consumption reduction (%) 39.72% 53.11% 39.26% 46.95% 23.40% 60.53% 14.43% 28.82% 33.84% 60.91% 20.18% 30.66% 69.24% 51.15% 45.38% 29.80% 37.49% 45.02%   TABLE III COM PAR I SON O F NCMS (NC) AND CMS (C) ON D EAD L IN E M I S S RATE (R) AND EN ERGY CON SUM P T ION (E ) FOR R EA L I S T IC H .264 B ENCHMARK A P P L ICAT ION S W I TH VAR IOU S IN J EC T ION RATE S Rate in NCMS EN C /J RN C CMS EC /J RC Rate in (apps/s) 0.0017 0.0020 0.0025 0.0031 0.0036 0.0040 0.0045 0.0050 0.0056 0.0059 0.0077 0.0100 0.0125 0.0200 0.0250 0.0500 0.1000 13.33% 6.67% 13.33% 26.67% 20.00% 20.00% 33.33% 53.33% 26.67% 40.00% 40.00% 53.33% 60.00% 60.00% 60.00% 66.67% 73.33% 55627.40 50405.13 48461.13 45303.41 53926.83 39837.72 49068.54 21497.72 45303.41 40445.12 40445.12 35586.83 18340.00 25262.84 26963.41 22105.12 22105.12 0.00% 35240.47 0.00% 35677.66 6.67% 30517.30 13.33% 38423.93 13.33% 48683.68 13.33% 30454.75 0.00% 19022.54 13.33% 13117.73 0.00% 29721.64 0.00% 30607.98 0.00% 25209.63 6.67% 20859.35 20.00% 12417.68 60.00% 19610.16 33.33% 12290.21 60.00% 11370.27 60.00% 10369.75 E Impv (%) 36.65% 29.22% 37.03% 15.19% 9.72% 23.55% 61.23% 38.98% 34.39% 24.32% 37.67% 41.38% 32.29% 22.38% 54.42% 48.56% 53.09% 35.29% 8A-3 Results show signiﬁcant advantages of the proposed approach compared to traditional mesh-based NoC and state-of-the-art management strategies. Heterogeneous cores with different behaviors can be integrated within one cluster, and enable ﬁnegrain energy-efﬁcient scheduling and system control, which will be an interesting extension to FoToNoC in the future work. ACKNOW L EDG EM EN T S This work is partially supported by Natural Science Foundation Of China (NSFC) No.61402060, National 863 Program 2013AA013202 and 2015AA015304, and Chongqing HighTech Research Program cstc2014yykfB40007, China. "
2017,Detailed and highly parallelizable cycle-accurate network-on-chip simulation on GPGPU.,"As the number of processing elements in modern chips keeps increasing, the evaluation of new designs will need to account for various challenges at the NoC level. To cope with the impractically long run times when simulating large NoCs, we introduce a novel GPU-based parallel simulation method that can speed up simulations by over 250×, while offering RTL-like accuracy. These promising results make our simulation method ideal for evaluating future NoCs comprising thousands of nodes.","8A-4 Detailed and Highly Parallelizable Cycle-Accurate Network-on-Chip  Simulation on GPGPU  Amir Charif, Alexandre Coelho, Nacer-Eddine Zergainoh, Michael Nicolaidis  TIMA Laboratory  Université Grenoble Alpes, France  {amir.charif, alexandre.coelho, nacer-eddine.zergainoh, michael.nicolaidis}@imag.fr  Abstract – As the number of processing elements in modern  chips keeps increasing, the evaluation of new designs will need  to account for various challenges at the NoC level. To cope with  the impractically long run times when simulating large NoCs, we  introduce a novel GPU-based parallel simulation method that  can speed up simulations by over 250x, while offering RTL-like  accuracy. These promising results make our simulation method  ideal for evaluating future NoCs comprising thousands of nodes.  I. Introduction  NoCs (Networks-on-Chips) [1] are the new preferred onchip communication medium for modern CMPs (Chipmultiprocessors) and MPSoCs (Multiprocessor Systems-onChips), as they do not suffer from the scalability limitations  of traditional buses. Ever since their introduction, NoCs have  been extensively researched and various solutions are being  proposed to meet the needs of emerging applications in terms  of performance, quality of service, power consumption, and  fault-tolerance.   At an early stage, novel NoC designs are usually evaluated  and validated by means of cycle-accurate simulation. While  simulating NoCs at the RTL (Register Transfer Level) can  produce very accurate results, popular cycle-accurate  simulators such as Booksim [2] are often preferred due to  their shorter simulation run times and easier programmability.  However, with the tremendous increase in the number of  processing nodes in modern and emerging chips, new  proposals will have to be validated against increasingly large  NoCs, which can take impractical simulation times, even  when simulating at a high level of abstraction.   Parallelization is one obvious solution to the issue and  unsurprisingly, several works have attempted  to  take  advantage of modern many-core processors to speed up NoC  simulations. However, due to the significant synchronization  overhead in CPU-based multithreading, such solutions often  need to sacrifice cycle-accuracy to obtain decent simulation  performance [6]. More importantly, achieving high speedups  usually requires CPUs with a very high core count, which can  be rather costly.   By contrast, GPGPUs  (General-Purpose Graphics  Processing Units) have been widely adopted in the HPC  (High-Performance Computing) field as a more cost-effective  parallel processing solution. GPUs have been gaining in  popularity in various domains, including cycle -accurate  simulation [9], [10]. Unfortunately, prior attempts at  simulating NoCs on GPU [9] assume an overly simplified  NoC architecture and propose a static parallelization method  that is very hard to extend and generalize. To the best of our  knowledge, no general and scalable method for performing  realistic NoC simulations on GPU has yet been proposed.   In this paper, we introduce the first detailed and modular  NoC simulator design  targeting GPU platforms. The  contribution of this paper is twofold. First, a flexible task  decomposition approach, specifically geared towards high  parallelization is proposed. Our approach makes it easy to  adapt the granularity of parallelism to match the capabilities  of  the host GPU. Second, all  the GPU-specific  implementation  issues  are  addressed  and  several  optimizations are proposed. Our design is evaluated through  a reference implementation, which is tested on an NVidia  GeForce GTX980Ti graphics card and shown to speed up  simulations by over 250 times.    The rest of this paper is organized as follows: A short  survey on related works is presented in section II. Section III  presents our task decomposition method. In Section IV, we  discuss the implementation of a reference simulator on GPU.  The results that were obtained in terms of speedup are  presented in section V before concluding in section VI.  II. Related works  The vast majority of NoC simulators available today are  single-threaded. General-purpose NoC simulators such as  Booksim [2], Noxim [3], and Garnet [4], are very widely used  in NoC research. In spite of being very good tools, all of these  simulators are overly slow at performing long running  simulations of large NoCs, comprising over 256 routers.   Several attempts have been made to build scalable multithreaded NoC simulators that leverage modern multicore  machines. A popular example is Hornet [6], which distributes  simulation tasks evenly among CPU threads to speed up  simulations. To avoid per cycle thread synchronization, which  is the major showstopper when it comes to CPU-based  parallelism, Hornet performs synchronization periodically,  which results in a certain loss in timing accuracy. Authors in  [7] propose a task distribution based on thread pools, in which  idle threads execute the next available task in order to better  balance the work load. However, because threads may access  different locations in memory throughout the simulation, it  978-1-5090-1558-0/17/$31.00 ©2017 IEEE 672                     8A-4 each responsible of accomplishing a specific action every  cycle. For the sake of illustration, we will assume, throughout  the rest of this paper, a canonical VC (Virtual Channel) router  architecture, as it is the one that most simulators adopt as their  baseline model [2], [4]. However, note that our proposals are  general and are not architecture-specific. A VC router  typically includes the following modules:    x  Input Unit (one per input port): This module reads a  flit from the corresponding input port, writes it to the  appropriate virtual channel FIFO, and, if it is a head flit,  performs route computation to select the next output port  to forward the packet to. It is also responsible of updating  the states of its associated virtual channels and sending a  credit upstream when a flit is read from one of its queues.   x VC Allocator: allocates one free output VC to each input  VC that is waiting for VC allocation.   x Switch Allocator: decides which input VCs can transmit  a flit through the crossbar in the next cycle.   x Output Unit (one per output port): This module reads  x  a flit from the input VC that won the switch allocation in  the previous cycle (corresponding to crossbar traversal),  and writes it to the corresponding output port. It also  updates credit counters and output VC states upon  receiving credits from the downstream input unit.    Injector: Although not part of a real router, this module  is necessary for simulation. Its main task is to generate  new packets and schedule them for transmission.   x Ejector: This module receives flits at their destination  and updates simulation statistics such as packet latency,  number of received packets, etc.   Every module instance in the network is identified by its  router ID ݎ , and its module type ID ݉ . Both are unique  positive integers. For instance, module type 0 may refer to  VC allocators, module type 1 to switch allocators, etc. In  this case, ID (15, 0) would identify the VC allocator of  router 15. The execution of module (ݎ, ݉) at cycle ܿ is  performed  using  a  global  function  called  ݁ݔ݁ܿܯ݋݀ݑ݈݁((ݎ , ݉), ܿ).   In order to obtain a model that behaves exactly like  hardware in terms of timing, every value written by a module  during one cycle cannot be visible to other modules until the  next cycle. We also need modules to be executable  concurrently and in any order. Many simulators solve this  issue by adopting an Evaluate/Update mechanism, in which  data is first evaluated in a temporary data set before becoming  visible to other modules.   We propose a method that is inspired by the odd/even  scheme used in [7]. This method consists in defining two data  sets. During even cycles, all the modules read from the first  data set and write to the second data set, while in the odd  cycles accesses are done on the opposite data sets. In this  work, every module is defined as a function, and a set of  registers. We incorporate the odd/even idea into our definition  of registers.   A register is defined as a tuple (o, e) with two associated  exhibits less cache affinity than the approach used in Hornet .  More recently, in [8], the authors have proposed a new  simulation method that combines uniform task distribution,  and a GALS (Globally Asynchronous Locally Synchronous)  approach to avoid frequent synchronization.   A few works have attempted to perform NoC simulations  on GPU. Authors in [10] managed to simulate a thousand core  system, including a NoC on an NVidia GeForce GTX 480  GPU. However, since the focus is not on the network level,  this solution uses a simple NoC model, which cannot be used  for detailed network-only simulation.   The closest related work that we are aware of is [9]. In this  solution, every GPU thread executes the tasks of one  bidirectional link in a router and speedups of up to 17x were  reported. In practice, however, we need to be able to simulate  a detailed router architecture, which consists of several  modules, including potential extensions by the end user, all of  which can be quite complex. Including all of these modules  in the actions of input/output links severely limits the level of  parallelism, which heavily impacts performance. At the same  time, if we were to simulate a very large network, the link  count of which cannot be accommodated by the GPU , then  the simulation would simply be infeasible and the core of the  simulator would have to be reprogrammed.   In this paper, we provide a generic task definition that does  not suffer from these limitations and an implementation that  yields much higher speedups than those reported in related  works.  III. Generic Task Decomposition  The design of a parallel simulator requires an appropriate  decomposition of the simulation into elementary tasks. These  tasks are then mapped onto threads for parallel execution. The  definition of simulation tasks is crucial as it determines the  granularity, and consequently the performance of the parallel  simulation. For instance, in [7], a task is defined as the set of  actions performed by a single router. Due to the significant  synchronization overhead of CPU-based multithreading, this  coarse-grained decomposition is appropriate. By contrast, in  GPU based parallelism it is usually preferable to run as many  threads as possible. In [9], a task corresponds to the actions  performed by one output and one input port of a router. This  finer-grained decomposition is more appropriate to the  inherently parallel architecture of GPUs,  in which  synchronization among threads is relatively cheap .   We propose in this work a new task definition that makes  it easy to configure the granularity of parallelism to match the  capabilities of a given GPU. This section will consist of three  parts. First, we provide a definition of modules, which are the  basic building blocks of our simulator design. Then, we  introduce a new abstraction called module group. Finally, we  give our definition of what a single simulation task is in terms  of module groups.  A. Modules  In hardware, every NoC router comprises several modules,  673         8A-4 Fig. 1. Two example module groupings.   C. Tasks  Given the above definitions, we are now able to define a  simulation task as the actions of one module group associated  with one router. Every task is identified by one router ID ݎ  and one module group ID ݃. The execution of task (ݎ , ݃) at  cycle ܿ is done via the function ݁ݔ݁ܿܶܽݏ݇((ݎ , ݃), ܿ), which  is presented in Algorithm 1.  IV. GPU-based implementation  We now show how we can leverage the previously defined  task decomposition to develop a very fast parallel simulator  on GPU using CUDA C [11]. A GPU has an architecture that  is fundamentally different from that of a CPU, and without  proper understanding of the numerous challenges and  performance pitfalls related to this architecture, it is very easy  to produce a naïve implementation that yields suboptimal  performance. In this section we start off by explaining the  general mode of operation of a GPU. We will then point out  the key  implementation challenges and performance  considerations and how they can be addressed. Note that even  though we will be using CUDA-specific terminology, the  same concepts apply to other GPU platforms as well.  A. Overview on the GPU architecture  At the software level, programming a GPU starts by  defining a Kernel, which is simply a function to be executed  by several GPU threads. These threads are organized in  Blocks. When launching a kernel, the programmer needs to  specify the number of blocks, as well as the number of threads  within each block.   Algorithm 1: Task execution routine  1. ܌܍܎ ݁ݔ݁ܿܶܽݏ݇(ݐܽݏ݇ܫ݀ , ܿݕ݈ܿ݁) {   ࢒ࢋ࢚ ݎ, ݃ ← ݐܽݏ݇ܫ݀    ࢌ࢕࢘ ݉ ∈ ݃݁ݐܩݎ݋ݑ݌ܯ݋݀ݑ݈݁ݏ(݃) {         ݁ݔ݁ܿܯ݋݀ݑ݈݁((ݎ , ݉), ܿݕ݈ܿ݁)  2.  3.  4.  5.  6.     }   }  operations: ܴ݁ܽ݀ and ܹݎ݅ݐ݁ . Given ܿ the cycle at which  the register is accessed, the two operations can be defined as  follows:  ܴ݁ܽ݀൫(݋, ݁), ܿ൯ = ݋ ݂݅ ܿ ≡ 0[2], ݁ ݋ݐℎ݁ݎݓ݅ݏ݁  ܹݎ݅ݐ݁((݋ , ݁), ݔ , ܿ) = ݁ ← ݔ ݂݅ ܿ ≡ 0[2], ݋ ← ݔ ݋ݐℎ݁ݎݓ݅ݏ݁  In C, this can be efficiently implemented by declaring  every register internally as a double entry array. For example:  ࢛࢔࢙࢏ࢍ࢔ࢋࢊ ࢙ࢎ࢕࢚࢘ ݒܿ_ݏݐܽݐ݁ݏ[2];  The read and write operations can be defined as follows:  #ࢊࢋࢌ࢏࢔ࢋ ݎ݁݃_ݎ݁ܽ݀(ݎ݁݃, ݌ܽݎ) (ݎ݁݃)[(݌ܽݎ)]  #ࢊࢋࢌ࢏࢔ࢋ ݎ݁݃_ݓݎ݅ݐ݁(ݎ݁݃ , ݒ , ݌ܽݎ) (ݎ݁݃)[! (݌ܽݎ)] = ݒ  Where the parity ݌ܽݎ is evaluated at every cycle ܿ as  ܿ & 1. Note that this very simple mechanism solves all timing  inconsistencies at almost no cost in performance, as these are  still simple memory accesses.   A problem can still arise if two modules try to write a  register at the same time. However, this issue can easily be  solved by ensuring that every module is allowed to read any  other module’s registers, but can only write to its own  registers. This restriction on the programming model removes  the need to use expensive atomic operations or other  synchronization primitives. More importantly, it makes it  very easy to define modules that behave very similarly to the  simulated hardware.   B. Module groups  While it is possible to execute all modules in parallel with  proper mapping, such fine granularity is not always desirable  and feasible. Therefore, we provide a means to group modules  to fine tune the granularity as necessary.   A module group is simply a unique collection of module  types. No two module groups contain the same module type  and every module type belongs to exactly one module group.  Given a module group identifier ݃ , assume that the set of  module types contained in ݃ can be accessed using a  function called ݃݁ݐܩݎ݋ݑ݌ܯ݋݀ݑ݈݁ݏ (݃).   To help make the idea of module groups clearer, fig. 1  shows two possible groupings of modules. In fig. 1 (a), 8  groups were defined. Each of the first 5 groups contains one  input unit and one output unit of the same direction (East,  West, North, South, Local), assuming a mesh-like topology.  The sixth group includes the injector and ejector and the final  two groups contain the switch and VC allocators respectively.  In fig. 1 (b), modules were reorganized to fit in 4 groups.  Module groups are meant to be run in parallel. Therefore,  the number of module groups affects the number of threads  used for simulation. The module grouping method allows us  to adjust the number of threads to match the capabilities of a  given GPU. As we will see in Section V, this indirection is  necessary for simulating very large networks.  In practice, module groups can be represented as a 2dimensional array, in which the row numbers correspond to  group IDs and the values in each row to module types.   674                              8A-4 that can be used by one block is usually very limited (typically  48KB), so fitting all the simulation data in this memory space  can be rather challenging. Some obvious optimizations are  possible nonetheless. For instance, the states of several virtual  channels can all be stored in one integer. This has the double  benefit of both saving memory space and allowing for  constant time round-robin arbiter implementations using a  series of bitwise operations. The most important optimization,  however, lies in our implementation of flit FIFOs.   Our representation is based on two observations. First,  assuming atomic VC allocation, one virtual channel can only  ever be storing flits of the same packet. Second, apart from  the head flit, which stores useful information used for routing,  the flits of a packet are of little interest to the simulator, as we  are usually only interested in knowing whether they are  present or not. Thus, we propose the representation shown in  fig. 3. This structure, which can fit in one integer (32 or 64  bits depending on the simulation parameters), is used to  represent a sequence of flits (from ݏݐܽݎݐ to ݁݊݀ ) of the  same packet of size ݏ݅ݖ݁ + 1. It can also represent an entire  packet when ݏݐܽݎݐ = 0 and ݁݊݀ = ݏ݅ݖ݁ , a single flit when  ݏݐܽݎݐ = ݁݊݀ , and an empty queue when ݏݐܽݎݐ > ݁݊݀ . The  ݁݊ݍݑ݁ݑ݁ operation is implemented by incrementing the  ݁݊݀ field, and ݀݁ݍݑ݁ݑ݁ by incrementing the ݏݐܽݎݐ field.  The type of an individual flit can be deduced from the values  of ݏ݅ݖ݁ , ݁݊݀ and ݏݐܽݎݐ . The “packet data” field contains  information such as the destination of the packet, its allocated  VC at every hop, and latency. Thanks to this representation,  we were able to fit enough data to simulate a 64 node network  with 4 virtual channels in less than 40KB of shared memory.  D. Simulating large networks  Due to the limited amount of shared memory and number  of threads per block, there is a maximum number of routers  that can be simulated in one block using a given module  grouping. Simulating  larger networks may  therefore  necessitate the creation of multiple blocks. Since shared  memory is only visible to threads within the same block, the  structures used to transfer flits and credits between routers  must be stored in global memory.   To ensure cycle-accuracy, all threads of all blocks must be  synchronized at the end of each cycle. This is done using the  lock-based synchronization method introduced in [12], but  with a slight modification. Our synchronization function is  presented in Algorithm 2, where __ݏݕ݊ܿݐℎݎ݁ܽ݀ݏ() is a  native barrier used to synchronize all threads within a block.   Algorithm 2: Synchronization pseudo-code  2.  1. ܌܍܎ ݏݕ݊ܿℎݎ݋݊݅ݖ݁(ݐℎݎ݁ܽ݀ܫܦ , ݐܽݎ݃݁ݐ) {   ࢒ࢋ࢚ ݔ , ݕ ← ݐℎݎ݁ܽ݀ܫܦ  3.  __ݏݕ݊ܿݐℎݎ݁ܽ݀ݏ()  4.     ࢏ࢌ (ݔ, ݕ = 0,0) {  5.        ࢇ࢚࢕࢓࢏ࢉ ܵݕܸ݊ܿܽݎ ← ܵݕܸ݊ܿܽݎ + 1   6.        ࢝ࢎ࢏࢒ࢋ (ܵݕܸ݊ܿܽݎ ≠ ݐܽݎ݃݁ݐ) {  7.        }  8.     }  9.     __ݏݕ݊ܿݐℎݎ݁ܽ݀ݏ()  10.  }  Threads can access several memory spaces in the GPU, the  most important of which are Global Memory, which is a slow  off-chip DRAM, and Shared Memory, which is a very fast  on-chip cache. Each block can use a limited portion of Shared  Memory (typically 48KB), which is visible only to threads  within the same block. At the hardware level, GPU resources  are organized in units called Stream Multiprocessors (SMs).  Each SM contains several processing cores, a Shared Memory  cache to be used by blocks, and registers to be used by threads.  During execution, threads are divided into fixed sized groups  called Warps. All threads within a warp execute the same  instruction at once, following the SIMT (Single Instruction  Multiple Threads) paradigm. More architectural details will  be revealed as needed in the remainder of this section.  B. Warp-friendly task mapping  Since all threads of a warp execute the same instruction,  they cannot branch to different locations simultaneously. If  threads of the same warp happen to execute different branches  of code, then their executions are serialized. While this is not  a major issue for simple conditional statements within  modules, having threads of the same warp execute different  module groups can be highly inefficient. Clearly, task  mapping cannot be performed in an arbitrary manner.  Fortunately, the way thread IDs are assigned to warps is  deterministic. For instance, assuming an architecture with ܹ  threads per warp, the first ܹ thread IDs are guaranteed to be  in the same warp, the same applies to threads of IDs between  ܹ and 2ܹ − 1, and so on.  We propose a mapping that minimizes the divergence  between threads of the same warp. Assuming that ܹ is the  number of threads per warp in a given architecture (typically  32), and ܴ the number of simulated routers, we identify the  threads using two coordinates (ݔ , ݕ) such that the absolute  thread ID is equal to ݕ ܴ + ݔ . Every task (ݎ, ݃) is then  mapped to thread (ݎ, ݃) . Since the ݕ coordinate is the  slowest changing coordinate in the thread ID, the module  group only changes every ܴ threads, which means that there  is a lower chance that different module groups, which  correspond to different ݕ coordinates, are contained within  the same warp. Moreover, if the number of simulated routers  is a multiple of ܹ , then all the threads within one warp are  guaranteed to be executing the same module group, i.e. the  same code. For network sizes that are not multiples of ܹ ,  padding the network using dummy nodes that do not receive  or generate packets can offer better performance.  C. Compact flit queue implementation  The next issue that needs to be addressed is that of memory  accesses. Accessing global memory is very costly. While  some optimizations are possible by playing on alignment and  coalescing, reorganizing the simulation data to meet all of  these constraints can be tedious. It would be much more  convenient if we could store all simulation data in the very  fast, shared memory cache and not have to worry about the  way data is accessed. However, the amount of shared memory  675                   8A-4 V. Experimental results  In  this  final section, we evaluate our simulator  implementation in terms of performance and accuracy.  Fig. 2. Flit queue representation. P is the maximum simulated packet  size. The “ready” field is used to signal a new flit presence.  A. Speedup  Compared to the solution in [12], we have one extra call to  __ݏݕ݊ܿݐℎݎ݁ܽ݀ݏ() before the inter-block synchronization.  This is to prevent cases where thread (0, 0) gets to the  synchronization point and allows other blocks to proceed to  the next cycle while threads of its own block are still running  in the current cycle.   This synchronization method only works if all blocks are  executed concurrently. If block execution is serialized, then a  deadlock will inevitably occur. Generally, if the number of  executed blocks is at most equal to the number of SMs  (Stream multiprocessors) of the GPU, then each block is  scheduled in a separate SM and the deadlock situation can  never occur. However, if the number of blocks exceeds the  number of SMs, then several blocks have to be scheduled on  the same SM. In this case, blocks can still be executed in  parallel provided that SM resources (mainly the amount of  shared memory and the number of thread registers) are  sufficient to accommodate all the blocks simultaneously.   Luckily, module grouping already solves the cases where  the number of threads needs to be reduced either due to an  insufficient number of registers or to an excess in the number  of threads per SM. However, we should in addition make it  possible to evict some of the simulation data from shared  memory. We have achieved this in our implementation by  providing compile time options to disable shared memory  caching for certain modules. Of course, this only affects the  way some pointers are initialized and does not change the way  modules are programmed. By carefully setting module  grouping and shared memory caching, it is possible to fit very  large networks in the limited resources of a GPU.  E. Simulation kernel and final notes  After addressing various design issues, we can now define  the simulation kernel as  in Algorithm 3.  In our  implementation, constant memory space was used to store the  module grouping  table. For pseudo random number  generation, which is essential for the injection process, we  have used a CUDA library [11] called cuRAND.   Algorithm 3: Simulation kernel pseudo-code  1. ܌܍܎ ݇݁ݎ݈݊݁(ݐℎݎ݁ܽ݀ܫܦ) {   ࢒ࢋ࢚ ݔ , ݕ ← ݐℎݎ݁ܽ݀ܫܦ  ࢒ࢋ࢚ ܿݕ݈ܿ݁ ← 0  2.  3.  4.  5.     ࢝ࢎ࢏࢒ࢋ (ܿݕ݈ܿ݁ < ܵ݅݉ݑ݈ܽݐ݁݀ܥݕ݈ܿ݁ݏ) {  6.     ݁ݔ݁ܿܶܽݏ݇((ݔ , ݕ), ܿݕ݈ܿ݁)  7.        ܿݕ݈ܿ݁ ← ܿݕ݈ܿ݁ + 1  8.        ݏݕ݊ܿℎݎ݋݊݅ݖ݁(ݐℎݎ݁ܽ݀ܫܦ , ܿݕ݈ܿ݁ × ܰݑ݉ܤ݈݋ܿ݇ݏ)   9.        }  10.  }  As a performance metric we consider the execution  speedup, which is obtained by dividing the execution time of  a sequential simulation, in which all modules are executed  iteratively, by that of the GPU-based parallel simulation.   The sequential version was compiled with level 2 compiler  optimizations enabled and run on an AMD A8-6500  @3.5GHz under Ubuntu Linux. For the parallel version, we  use an NVidia GeForce GTX980Ti graphics card, which  features 22 SMs (Stream Multiprocessors), each including  96KB of shared memory.  Each simulation was run 10 times and the execution times  were averaged. For all simulations, we have only counted the  time spent in the actual network simulation, not including  initialization and memory allocation.  Several mesh sizes were tested (16x16, 24x24 and 32x32)  with 2 or 4 VCs, using the network parameters presented in  table 1. In all tested configurations, we were able to fit an 8x8  mesh network in a single block of threads. Therefore, a 16x16  mesh was simulated using 4 blocks of 8x8, a 24x24 mesh  using 9 blocks, and a 32x32 mesh using 16 blocks.   We have used 8 module groups organized exactly as shown  in fig. 1 (a). It should be noted that we have not noticed any  improvement in speedup when using more module groups,  mainly because some modules have very different  complexities and running them in parallel forces faster  modules, such as output units, to spend more time waiting for  more complex modules, such as the VC allocator, to finish  executing.  The results are presented in table 2. First, notice the very  high speedups obtained in all simulations, especially for  larger networks. Simulating a 32x32 network is 100 times  faster on GPU. This reflects the high level of parallelism that  is possible using our method. Assuming a perfectly scalable  CPU-based parallel simulator, achieving such high speedups  would require a machine comprising at least 100 threads.  Thus far, we have tested networks that required a number  of blocks that is smaller than the number of SMs, which is  why we were able to run the simulations at the desired  granularity with no compromises. In order to push the GPU  to its limits, we now try to simulate larger networks (40x40 to  64x64).   TABLE I  Network configuration  Parameter  Value  Flit buffers per VC 4  Packet size  5 flits  Routing  XY  Traffic pattern  Uniform Random  Injection rate  0.001 packets/node/cycle  Simulated cycles  100000  676                            8A-4 VI. Conclusions  In this paper, a novel GPU-based NoC simulation method  was introduced. We have presented a module specification  that offers high hardware fidelity, thread safety, and easy  extensibility. These properties are suitable for parallel NoC  simulation in general, not only on GPU. Our module group  abstraction, which decouples the definition of modules from  their mapping on the platform, has proven extremely useful  for simulating very large networks (~4K nodes) on GPU and  excellent speedups were obtained (over 250x). We have  addressed GPU-specific implementation challenges such as  thread divergence, memory usage and synchronization to  implement a simulator capable of producing very accurate  statistics at ultra-high speeds, making it an ideal tool for  modeling and validating future NoC designs consisting of  thousands of nodes.    [1] W. J. Dally and B. Towles, ""Route packets, not wires: on-chip  interconnection networks,"" Design Automation Conference, 2001.  Proceedings, 2001, pp. 684-689.  [2] N. Jiang et al., ""A detailed and flexible cycle-accurate Networkon-Chip simulator,"" Performance Analysis of Systems and Software  (ISPASS), 2013 IEEE International Symposium on, Austin, TX,  2013, pp. 86-96.  [3] V. Catania, A. Mineo, S. Monteleone, M. Palesi and D. Patti,  ""Noxim: An open, extensible and cycle-accurate network on chip  simulator,"" 2015  IEEE 26th  International Conference on  Application-specific Systems, Architectures and Processors (ASAP),  Toronto, ON, 2015, pp. 162-163.  [4] N. Agarwal, T. Krishna, L. S. Peh and N. K. Jha, ""GARNET: A  detailed on-chip network model inside a full-system simulator,""  Performance Analysis of Systems and Software, 2009. ISPASS 2009.  IEEE International Symposium on, Boston, MA, 2009, pp. 33 -42.  [5]  “Netmaker.”  [Online].  Available:  http://wwwdyn.cl.cam.ac.uk/~rdm34/wiki.  [6] P. Ren et al., ""HORNET: A Cycle-Level Multicore Simulator,"" in  IEEE Transactions on Computer-Aided Design of Integrated  Circuits and Systems, vol. 31, no. 6, pp. 890-903, June 2012.  [7] M. Eggenberger and M. Radetzki, ""Scalable parallel simulation  of networks on chip,"" Networks on Chip (NoCS), 2013 Seventh  IEEE/ACM International Symposium on, Tempe, AZ, 2013, pp. 1 -8.  [8] M. Eggenberger, M. Strobel and M. Radetzki, ""Globally  Asynchronous Locally Synchronous Simulation of NoCs on ManyCore Architectures,"" 2016 24th Euromicro International Conference  on Parallel, Distributed, and Network-Based Processing (PDP),  Heraklion, 2016, pp. 763-770.  [9] M. Zolghadr, K. Mirhosseini, S. Gorgin and A. Nayebi, ""GPUbased NoC simulator,"" Formal Methods and Models for Codesign  (MEMOCODE), 2011 9th IEEE/ACM International Conference on,  Cambridge, 2011, pp. 83-88.  [10] C. Pinto et al., ""GPGPU-Accelerated Parallel and Fast  Simulation of Thousand-Core Platforms,"" Cluster, Cloud and Grid  Computing  (CCGrid), 2011 11th  IEEE/ACM  In ternational  Symposium on, Newport Beach, CA, 2011.  [11] NVIDIA. ""CUDA Toolkit Documentation v7.5."" [Online].  Available: http://docs.nvidia.com/cuda.  [12] S. Xiao and W. c. Feng, ""Inter-block GPU communication via  fast barrier synchronization,"" Parallel & Distributed Processing  (IPDPS), 2010 IEEE International Symposium on, Atlanta, GA,  2010, pp. 1-12.  Since the number of required blocks is higher than the  number of SMs, several blocks need to share SM resources.  As indicated in the results presented in table 3, it was  necessary to reduce the number of module groups to 7 in order  to be able to simulate 40x40 and 48x48 networks, and we  could only use up to 4 module groups to simulate 56x56 and  64x64 networks. Moreover, since thread blocks also need to  share Shared Memory, global memory had to be used to store  some modules when simulating 56x56 and 64x64 ne tworks  with 4 VCs (see Section IV-D). In this case, only the injectors  were removed from shared memory. We can see that in spite  of these restrictions, the speedup continues to scale with the  number of nodes, exceeding 250x when simulating a 4K node  network.  B. Hardware fidelity  Finally, we evaluate our simulator in terms of how  precisely it can mimic hardware behavior, by comparing the  results produced by our reference implementation to those  obtained using an RTL model based on Netmaker [5], which  is a library of synthesizable NoC routers written in  SystemVerilog. We have simulated an 8x8 mesh network with  2 VCs for 100000 cycles. Single flits were injected at varying  rates and the average network latency results are presented in  fig. 3. We can see that our simulator and the RTL model  exhibit identical behaviors and the difference in latency was  less than 5% for all the tested injection rates. These results  indicate that our simulator is not only ultra-fast, but also  matches RTL accuracy.   TABLE II  Simulation speedup (1)  16x16  2vc  26,53  16x16  4vc  28,60  24x24  2vc  55,07  24x24  4vc  59,65  32x32  2vc  95,53  32x32  4vc  102,59  TABLE III  Simulation speedup (2)  # groups  40x40 2vc  40x40 4vc  48x48 2vc  48x48 4vc  7  139,92  146,75  190,55  193,37  # groups  56x56 2vc  56x56 4vc  64x64 2vc  64x64 4vc  4  210,33  182,03*  277,27  252,90*  (*) Injectors were evicted from shared memory.  ) s l e c y c ( y c n e t a L k r o w t e N . g v A 70 60 50 40 30 20 10 0 "
2017,Communication driven remapping of processing element (PE) in fault-tolerant NoC-based MPSoCs.,"In this paper, the capacitive value used in a structure of artificial magnetic conductor (AMC) is investigated for achieving dual-band frequency responses of microstrip patch antenna. The implemented structure is a planar structure which consists of four identical square patches arranged in 2×2 array incorporated with capacitors placed between adjacent patches. The proposed antenna takes a total dimension of 52mm × 52mm. The AMC structure and antenna are deployed on FR4 Epoxy dielectric substrate with relative permittivity of 4.4 and total thickness of 3.2mm. From the characterization result, it is shown that the incorporation of surface-mount capacitor on AMC structure could produce dual-band frequency responses at 1.42GHz and 1.59GHz. However, the gain of antenna with capacitive AMC at the resonant frequency is lower than of the antenna without capacitive AMC. Meanwhile, the bandwidth is also narrower as the use of capacitive AMC.","Communication Driven Remapping of Processing Element (PE) in Fault-tolerant NoC-based MPSoCs 8A-3 Chia-Ling Chen, Yen-Hao Chen and TingTing Hwang Department of Computer Science, National Tsing Hua University, Taiwan s102062567@m102.nthu.edu.tw, yhchen@cs.nthu.edu.tw and tingting@cs.nthu.edu.tw AB STRACT We propose a remapping algorithm to tolerate the failures of Processing Elements (PEs) on Multiprocessor System-on-Chip. A new graph modeling is proposed to precisely deﬁne the increase of communication cost among PEs after remapping. Our method can be used not only to repair faults but also to improve the communication cost of given initial mapping results. Experimental results show that under multiple failures, the communication cost by our method is 43.59% less on average compared with that by previous work [1] using the same number of spare PEs. Moreover, the communication cost is further reduced by 4.16% after applying our method to initial mappings produced by NMAP [2]. In this paper, the topology reconﬁguration is adopted to repair PE failures. We propose a remapping algorithm for homogeneous NoC-based MPSoCs with spare PEs. Our algorithm minimizes the total communication cost and achieves 100% repair rate. Moreover, our method can be applied not only to repairing the faults but also improving the performance of initial mapping. The rest of the paper is organized as follows. Chapter II describes the motivation for this work. Chapter III presents our proposed remapping approach in detail. Chapter IV shows the extension of the algorithm to produce initial mapping. Experimental results are reported in Chapter V, followed by conclusions in Chapter VI. I . INTRODUCT ION With the advance in semiconductor technology, a large number of Processing Elements (PEs) can be integrated on a single chip. Such implementation is commonly known as Multi-processor System-on-Chips (MPSoCs). Applications, consisting of several tasks, are mapped onto PEs of a chip to perform various functions in high performance. As the demand for high bandwidth and scalability increases, Network-on-Chip (NoC) is considered a feasible communication infrastructure among PEs [3]. The NoC is composed of routers connecting other routers, and each PE is attached to its individual router. With the number of PEs integrated on the MPSoC increasing, reliability has become an important design concern [4]. Fault tolerant schemes are required to maintain the functionality of a system when some of its components break down. Faults in NoC-based MPSoCs may occur in the communication network (i.e., links, network interfaces and routers) and the PEs. In this paper, we focus on the failures in PEs, and assume that the communication infrastructure can be handled by other techniques. There has been several previous work targeting on solution of tolerating faulty PEs. One solution is to move tasks of application running on a failed PE to other normal ones when a failure is detected. C. Ababei and R. Katti [5] proposed a 2-step heuristic approach to address single and multiple PE failures. This approach searches a new mapping region which is fault-free in ﬁrst step, and then remaps the tasks on faulty cores onto this new region in second step. It tries to minimize overall migration of each task. O. Derin et al. [6] proposed a fault-tolerant task mapping which is formulated as an Integer Linear Programming (ILP) problem. Optimal mappings for all singlefault scenarios are found at design time, which are used by the online task remapping heuristic with the objective of minimizing communication trafﬁc in the system and total execution time of the application. The above approaches inevitably cause the degradation of the system performance due to extra workload added onto the remaining fault-free PEs. A more practical solution to tolerate failures is to use hardware redundancy. When an element of system fails, redundant element can take over the role assigned to the failed one and the whole system will run normally. The idea of providing redundant resources to improve reliability is widely applied in various hardware components, including PE, physical links, via, router, etc. In the work of L. Liu, et al. [1], spare PEs are provided to construct a reconﬁgurable architecture. The topology reconﬁguration problem is converted into ﬂow problem in graph theory, and a repair approach is proposed to tolerate PE failures with minimal impact on area, throughput and delay. This method reassigns tasks based on the current locations. Hence, the reconﬁguration cost is very low. Moreover, the modeling is very effective and produces results of high repair rate in polynomial time. 978-1-5090-1558-0/17/$31.00 ©2017 IEEE 666 I I . MOT IVAT ION Communication cost is an important metric to measure the performance of NoC-based systems. A commonly used metric [2], [7], [8] is given by: commcost = (amounti × hopcounti ) (1) (cid:2) i∈C , y , y where C is the set of the communication between pairs of tasks, amounti is the transferred data bytes of communication i, i ∈ C , and hopcounti is the minimum number of hops between the mapped PE locations of source task and target task of communication i. L. Liu, et al. [1] has proposed an elegant modeling of repairing failed PE. A non-spare faulty PE at location (x, y) is replaced by a healthy PE at (cid:3) (cid:3) ), and then the PE at location (x (cid:3) (cid:3) ) is replaced by other location (x healthy PE, and so on, until the replacement ends at a spare one. Such a replacement chain is deﬁned as a repairing path. To ﬁnd a repairing path, a mesh architecture is represented as a topology graph, a directed graph, where each node denotes a PE on the mesh and each directed edge connecting two nodes denotes the replacement of two PEs. To ensure the minimal impacts on communication distance (hopcount), a PE can only be replaced by its neighbour PEs. However, the cost of the amount of data transmission is deﬁned on each node rather than edge, which takes the communication cost into consideration indirectly. An example is shown in Figure 1. In Figure 1(a), a 3 × 4 architecture with application tasks mapped onto its non-spare PEs (R1-R3, R5-R7, R9-R11), and the weight on edge between a pair of tasks denotes the corresponding communication amount. According to the above modeling, it is converted into the topology graph, as illustrated in Figure 1(b). Note that the communication cost is accumulated on each node. Let R5 be a faulty PE. A repairing path R5 → R1 → R2 → R3 → R4 can be seen as a unit ﬂow starting from a source node—faulty PE R5 and ending to a target node—spare PE R4. The repair method using minimum cost maximum ﬂow (MCMF) algorithm is used to repair multiple faults. The modeling is very effective and produces results of high repair rate in polynomial time. However, there are two problems in this approach. One is that the communication cost modeled on node by [1] is not able to describe the actual communication cost and thus unsatisfactory results may be produced. Figure 2(a) shows a 3 × 4 architecture with one faulty PE to be repaired. Figure 2(b) shows that a minimum cost repairing path based on the cost deﬁned by [1] is obtained. After task remapping, as shown in Figure 2(c), the total communication cost becomes commcost = (6× 2+ 30× 2+ 10× 1+ 5× 1+ 5× 2+ 24× 1+ 30× 1+ 5× 2) = 161. But choosing another repairing path, as shown in Figure 2(d), less amount of communication cost after remapping can be obtained as shown in Figure Zϭ ZϮ Zϯ Zϰ Zϱ Zϲ Zϳ Zϴ Zϵ ZϭϬ Zϭϭ ZϭϮ     & / , ' ϯϬ ϲ ϱ ϯϬ Ϯϰ ϭϬ ϱ ϱ &ĂƵůƚͲĨƌĞĞW ^ƉĂƌĞW dĂƐŬ ŽŵŵƵŶŝĐĂƚŝŽŶ (a) &ĂƵůƚͲĨƌĞĞW >ŝŶŬ ^ƉĂƌĞW Zϭ Zϱ Zϲ ZϮ Zϯ Zϳ Zϵ Zϰ Zϱ ZϭϬ Zϭϭ ZϭϮ ϱ ϭϬ ϱ ϯϲ ϱϵ Ϯϵ ϭϲ ϰϬ ϯϬ ZĞƉĂŝƌŝŶŐƉĂƚŚ ĨĂƵůƚǇW (b) Fig. 1. (a)Task mapping on 3x4 architecture (b)Topology graph modeled by [1].     & / , ' ϯϬ ϲ ϱ ϯϬ Ϯϰ ϭϬ ϱ ϱ &ĂƵůƚͲĨƌĞĞW ^ƉĂƌĞW &ĂƵůƚǇW dĂƐŬ ZĞƉĂŝƌƉĂƚŚ ŽŵŵƵŶŝĐĂƚŝŽŶ (a) ϱ ϭϬ ϱ ϯϲ ϱϵ Ϯϵ ϭϲ ϰϬ ϯϬ     & / , ' (b)     & / , ' ϱ Ϯϰ ϱ ϱ ϯϬ ϲ ϯϬ ϭϬ (c)     & / , ' ϯϬ ϲ ϱ ϯϬ Ϯϰ ϭϬ ϱ ϱ (d)     & / , ' ϯϬ ϭϬ ϱ ϯϬ ϱ Ϯϰ ϱ ϲ (e) Fig. 2. (a)Initial mapping with 1 faulty PE. (b)The minimum cost repairing path obtained by [1]. (c)Remapping result from [1], commcost = 161. (d)Another repairing path. (e)Remapping result with lower communication cost, commcost = 131. 2(e), where commcost = 131 . Clearly, the deﬁned cost in [1] is not precise enough to represent the communication cost. The second problem is that previous work [1] does not guarantee to fully repair a faulty NoC-based chip even when there are enough spare PEs. The reason is that each PE can be replaced by its neighbour PE only, which means a repairing path should be a continuous replacing chain. Figure 3 shows examples of 3×4 and 4×5 architecture with nonrepaired faulty PE and un-used spare PE. In Figure 3(a), the faulty PEs are R1, R2 and R5. R2 and R5 can be repaired by the replacing chains R2 → R3 → R4 and R5 → R6 → R7 → R8 respectively. Yet R1 cannot be repaired since all the neighbour PEs of R1 are faulty. Similar situation occurs in another example shown in Figure 3(b) with faulty PEs R6, R11, R12 and R17. In this case, R11 is blocked by other 3 faulty PEs and cannot ﬁnd out a continuous replacing chain to a spare PE, R5. Motivated by the above two observations, we propose a communication driven remapping algorithm to repair faulty PEs. We consider Zϭ ZϮ Zϯ Zϰ Zϱ Zϲ Zϳ Zϴ Zϵ ZϭϬ Zϭϭ ZϭϮ (a) Zϭ ZϮ Zϯ Zϰ Zϱ Zϲ Zϳ Zϴ Zϵ ZϭϬ Zϭϭ ZϭϮ Zϭϯ Zϭϰ Zϭϱ Zϭϲ Zϭϳ Zϭϴ Zϭϵ ZϮϬ (b) Fig. 3. Examples of unfully repaired system. the communication demands between pairs PEs directly to minimize the total communication cost. Moreover, we allow each PE can be replaced by PE hops away from it, and guarantee 100% repair rate when the number of faulty PEs are not more than that of spare ones. Lastly, in this paper we assume faults occur one by one rather than simultaneously. Our assumption is more reasonable than previous work [1], since the probability of PEs simultaneously breaking down is extremely low. I I I . RE PA IR ING ALGOR ITHM BY REMA P P ING Our objective is to tolerate the PE failures by remapping while minimizing the communication cost. We ﬁrstly present the problem formulation of the remapping problem in Section III-A. Then, we describe the proposed repairing algorithm in detail in Section III-B. A. A New Graph Modeling We follow the idea of repairing path presented in previous work [1]. A repairing path can be seen as a ﬂow starting from a faulty PE and ending at a spare PE, and the problem of obtaining the optimal repairing path can be converted into a minimum cost ﬂow (MCF) problem. As mentioned in Chapter II, the communication cost can be expressed as commcost in Equation (1). For precisely modeling the impact on commcost, we will introduce a new topology graph. Our core idea is to model the variation of commcost for all possible task re-allocation scenarios in the graph. The new topology graph is formally deﬁned as following. The topology graph is a directed graph, G = (V , E , C ost). Each vertex vi ∈ V represents a PE in the NoC-based MPSoC, and each directed edge vi → vj denoted as ei,j ∈ E , represents the PE replacement of vi by vj , which means the task on vi is remapped onto vj . The cost of each edge ei,j , denoted as costi,j ∈ C ost, represents the increment in commcost, Δcommcost, when the task on vi is remapped onto vj assuming unchanged locations of other tasks. Figure 4(a) shows an example of a 2 × 3 architecture with initial mapping. If we remap task B from R2 onto R3, Hence, costR2,R3 on edge R2 → R3 is 15 in the topology graph as shown in Figure 4(b). Similarly, the commcost will decrease 10 if we remap task C from R4 onto R2. Hence, costR4,R5 is -10. A repairing path in the topology graph is composed of a set of continuous directed edges, the total cost of these edges is exactly the Δcommcost, which is the commcost after remapping compared to the commcost before mapping. Although our cost deﬁnition of each edge is modeled in the local view assuming unchanged locations of other tasks, global optimal solution can be obtained by accumulating cost of edges in repairing path. We will explain it later. In order to solve the non-fully repairing problem of previous work [1] mentioned in Chapter II, we allow a PE can be replaced by non-neighbour PE. That is, an edge ei,j is constructed between vi and vj even when vi on edges accurately, not all pairs of vertices ∈ V have edges between and vj are not adjacent to each other. However, in order to model cost them. Consider an example shown in Figure 5. Figure 5(a) shows a 2 × 3 architecture. R1, R2, R4 and R5 are non-spare PEs, while R3 and R6 are spare ones. One fault occurs in R1. The commcost of initial mapping is 50. The complete topology graph is constructed as shown in Figure the commcost will increase 15, i.e., (10×2+5×3)−(10×1+5×2) = 15. 8A-3 667       Zϭ ZϮ Zϯ Zϰ Zϱ Zϲ     ϭϬ ϭϬ ϮϬ ϱ (a) Zϭ Zϰ Zϱ ZϮ Zϯ Zϲ ϮϬ ϱ Ϭ ϭϱ Ͳϭϱ ϮϬ ͲϭϬ ϰϬ ϮϬ Ϯϱ ϮϬ (b) (c) Fig. 4. The 2 × 3 topology graphs with our deﬁned cost on each edge. Zϭ Zϰ Zϱ ZϮ Zϯ Zϲ ϮϬ Ϭ Ϭ ϭϱ Ͳϭϱ ϮϬ Ϭ ϮϬ ϱ ϮϬ ϮϬ Zϭ ZϮ Zϯ Zϰ Zϱ Zϲ     ϭϬ ϭϬ ϮϬ ϱ Y X (a) Zϭ Zϰ Zϱ ZϮ Zϯ Zϲ ϮϬ Ϭ Ϭ ϭϱ Ͳϭϱ ϮϬ Ϭ ϮϬ ϱ ϮϬ ϮϬ ϱ ͲϭϬ ϮϬ ϰϬ Ϯϱ ͲϮϬ Ͳϭϱ Ϭ ͲϭϬ Y X (b) Zϭ ZϮ Zϯ Zϰ Zϱ Zϲ     ϭϬ ϭϬ ϮϬ ϱ (c) Zϭ ZϮ Zϯ Zϰ Zϱ Zϲ     ϮϬ ϭϬ ϭϬ ϱ (d) on repairing path R1 → R4 → R5 → R6. (d)Remapping result based on Fig. 5. (a)Initial mapping. (b)Complete topology graph. (c)Remapping result based repairing path R1 → R4 → R2 → R3. 5(b), where each non-spare node has directed edges going to the other nodes including spare ones. Consider the repairing path X , R1 → R4 → R5 → R6, with total cost, 0 + (−15) + 20 = 5. The remapping result based on the repairing path X is shown in Figure 5(c). The commcost becomes 55 from 50 after remapping. Δcommcost = 5 represents the exact cost of repairing path X . However, consider another repairing path Y , R1 → R4 → R2 → R3, with total cost, 0 + (−10) + 15 = 5 in the graph shown in Figure 5(b). The remapping result based on the repairing path Y is shown in Figure 5(d), where commcost becomes 75 from 50. Hence the cost of path Y is Δcommcost = 25, rather than the incorrect value Δcommcost = 5. This inconsistent costs between repairing path and actual remapping result is caused by the down-and-up (back-andforth) moves as shown in Figure 5(a). Consider the communication link where data amount is 10 between task A and task B in Figure 5(a). When only remapping task A from R1 to R4, the distance between these two tasks become zero hop and hence the communication cost between them decreases 10. When only remapping task C from R4 to R2, the distance between task A and task B is still one hop, and hence the communication cost between them increases 0. However, the increment of communication cost is not (−10) + 0 if we perform both actions, remapping the task A from R1 to R4 and task C from R4 to R2, at the same time. The distance between A and C becomes two hops and the communication cost increases 10 actually. When a repairing path is not moving monotonically increasing (decreasing) in x or y -direction, incorrect cost is modeled. The following Lemma 1 gives conditions to build the topology graph. Lemma 1. Let R0 → R1 → ... → Rn be a given repairing path and Rk be the k th PE of the path located at (xk , yk ), where k is a positive integer. To obtain the correct cost, the given repairing path should subject to the following conditions: (cid:3) x0 ≤ x1 ≤ ... ≤ xn or x0 ≥ x1 ≥ ... ≥ xn y0 ≤ y1 ≤ ... ≤ yn or y0 ≥ y1 ≥ ... ≥ yn Proof. The communication between pairs of tasks can be classiﬁed into three cases based on whether the tasks are on the PEs of repairing path. Three cases are that both tasks are not on the PEs of the path, only one of them is on the PEs of the path, and both are on the PEs of the path. Since we locally deﬁne the cost on topology graph assuming that locations of other tasks are unchanged, clearly our cost modeling is correct for communication of the ﬁrst and second cases. Let us consider the communication of third case. Assume there is a communication link between task A and task B with communication is on Rj located at (xj , yj ), where i < j and i, j ∈ 0, 1, ..., n. After volume amountA,B . Task A is on Ri located at (xi , yi ) and task B remapping based on the given repairing path, task A is remapped onto Ri+1 located at (xi+1 , yi+1 ) and task B is remapped onto Rj+1 located at (xj+1 , yj+1 ). The number of hops between task A and task B , represented (cid:3) A,B after remapping. Hence, communication cost is also changed according to Equation (1). We obtain as hopcountA,B , is changed to hopcount ΔcommcostA,B = amountA,B × (hopcount(cid:3) A,B − hopcountA,B ) = amountA,B × [(|xj+1 − xi+1 | + |yj+1 − yi+1 |) − (|xj − xi | + |yj − yi |)] = amountA,B × [α(xj+1 − xi+1 ) + β (yj+1 − yi+1 ) − α(xj − xi ) − β (yj − yi )] = amountA,B × [α(xi − xi+1 − xj + xj+1 ) + β (yi − yi+1 − yj + yj+1 )] (2) where α = (cid:2) (cid:2) 1, −1, if x0 ≤ x1 ≤ ... ≤ xn if x0 ≥ x1 ≥ ... ≥ xn if y0 ≤ y1 ≤ ... ≤ yn if y0 ≥ y1 ≥ ... ≥ yn β = 1, −1, In our modeling, ΔcommcostA,B on edge Ri → Ri+1 is modeled as ΔcommcostA,B (Ri → Ri+1 ) = amountA,B × [α(xi − xi+1 ) + β (yi − yi+1 )], and ΔcommcostA,B on edge Rj → Rj+1 is modeled as ΔcommcostA,B (Rj → Rj+1 ) = amountA,B × [α(xj+1 − xj ) + β (yj+1 − yj )]. Hence, total ΔcommcostA,B on the given repairing path is ΔcommcostA,B = amountA,B × [α(xi − xi+1 ) + β (yi − yi+1 )] + amountA,B × [α(xj+1 − xj ) + β (yj+1 − yj )] = amountA,B × [α(xi − xi+1 − xj + xj+1 ) + β (yi − yi+1 − yj + yj+1 )] (3) As we can see, the results of Equation (3.1) and (3.2) are equal, which clearly indicate that our modeling can obtain the solution with correct total communication cost. Following Lemma 1, we limit topology graph by removing all the edges that cause down-and-up (back-and-forth) moves. In this paper, the spare PEs are assumed to place at the right side of the architecture. Therefore we ﬁrstly construct all the edges going to right in x-direction. Next, up edges and down edges can not coexist in the same topology graph. But if the edges are limited to going only up or down in the y -direction, the solution space will signiﬁcantly be reduced. Therefore, we construct two topology graphs. They are the right-up one and the right-down one, which include all the possible repairing paths without down-and-up (back-andforth) moves. An example of the 2× 3 architecture shown in Figure 4(a) is modeled to two topology graphs illustrated in Figure 4(b) and (c), where 8A-3 668 Figure 4(b) shows the 2 × 3 right-up topology graph, and Figure 4(c) shows the 2 × 3 right-down one. B. Repairing Algorithm The ﬂow of our proposed repairing algorithm is illustrated in Figure 6. Given an initial mapping. When a new faulty PE occurs, we start repairing procedure. First, the corresponding topology graphs are constructed. Then MCF is applied to ﬁnd a repairing path. Finally, tasks are remapped following the repairing path. We describe each step in detail as follows. Step 1: Construct the topology graphs Once a PE failure is detected, the right-up topology graph and the rightdown topology graph are constructed. Meanwhile, the cost of each edge is deﬁned according to the communication among PEs, as mentioned in Section III-A. Step 2: Apply MCF to obtain the repairing path For each topology graph, deﬁne two additional nodes S and T , where S is the source node and T is the target node. Deﬁne an edge from S pointing to the faulty PE. For each spare PE, deﬁne an edge from it to T . The cost of these edges are deﬁned as 0. Then, apply the minimumcost-ﬂow algorithm on these two topology graphs respectively. Clearly, we will obtain two ﬂows, one is from the right-up topology graph and the other is from right-down topology graph. Simply choose the one with lower cost as the repairing path. Step 3: Remap the tasks Remap the involved tasks based on the repairing path produced by step 2. A new mapping result is produced. Note that if a new faulty PE occurs, the procedure is repeated. New topology graphs are constructed based on the remapping result produced in the last iteration and failed nodes in the previous iteration are removed from topology graphs. /ŶŝƚŝĂůŵĂƉƉŝŶŐ ZĞƉĂŝƌŝŶŐĂůŐŽƌŝƚŚŵ ŶĞǁĨĂƵůƚǇ zĞƐ ŽŶƐƚƌƵĐƚƚŚĞ WŽĐĐƵƌƐ͍ ƚŽƉŽůŽŐǇŐƌĂƉŚƐ EŽ ƉƉůǇD&ƚŽŽďƚĂŝŶ ƌĞƉĂŝƌŝŶŐƉĂƚŚ ZĞŵĂƉƚŚĞƚĂƐŬƐ Fig. 6. Flow diagram of the proposed repairing algorithm. Figure 7 shows an example of proposed repairing algorithm. A fault occurs in PE, R1, of the 2 × 3 architecture, as shown in Figure 7(a). The right-up and right-down topology graphs are constructed and then MCF algorithm is applied. A repairing path R1 → R2 → R3 is obtained from the right-up topology graph illustrated in Figure 7(b) and the total cost is 15, while a repairing path R1 → R4 → R5 → R6 is obtained from the right-down topology graph illustrated in Figure 7(c) and the total cost is 5. Therefore, we select the repairing path in right-down topology graph. Finally, the tasks are remapped by the repairing path as shown in Figure 7(d) and Δcommcost = 5. That is, the communication cost of remapping result increases 5 compared to that of initial mapping. IV. EXTEN S ION TO IN IT IAL MA P P ING Many task-mapping methods have been proposed [9]. Among them, balance of work load and communication cost are commonly used as mapping metrics. Our remapping technique is to move the complete task assigned to one PE to the other one. Hence, it affects communication cost rather than load balance. 8A-3 Zϭ ZϮ Zϯ ϭϬ   ϭϬ ϱ ϮϬ   Zϰ Zϱ Zϲ (a) ϮϬ ϮϬ Ϭ ϭϱ Ϭ ϭϱ Zϭ ZϮ Zϯ Zϭ ZϮ Zϯ ϱ ͲϭϬ ϰϬ Ϭ Ϭ ϮϬ ϮϬ ϮϬ ϱ Ϯϱ Zϰ Zϱ Zϲ Zϰ Zϱ Zϲ Ͳϭϱ ϮϬ Ͳϭϱ ϮϬ ϮϬ (b) ϮϬ (c) Zϭ ZϮ Zϯ ϭϬ  ϱ ϭϬ ϮϬ    Zϰ Zϱ Zϲ (d) Fig. 7. (a)Initial mapping. (b)A min-cost repairing path obtained from the rightup topology graph. (c)A min-cost repairing path obtained from the right-down topology graph. (d)Remapping result. . Besides repairing faulty PEs, our method can also be applied to further improve the communication cost of a given initial mapping. A two-step algorithm is proposed to perform a re-mapping: • Step 1: Placement of faulty PEs • Step 2: Application of our repairing algorithm To utilize our method to further improve the communication cost of a given initial mapping. The ﬁrst step is to assume the locations of faulty PEs and spare PEs. Then, the second step is to call our algorithm to repair the faulty PEs by spare PEs. An example is illustrated in Figure 8. Figure 8(a) is a given initial mapping with 9 tasks mapped onto a 3×3 NoC. First, we assume PEs on the left column are faulty and create a column of spare PEs on the right side, as illustrated in Figure 8(b). Next, the repairing algorithm mentioned in Section 3.2 is applied, and the remapping result is obtained as illustrated in Figure 8(c). Except the column of faulty PEs, we take the relative position of tasks mapped on remaining 3×3 region as the new mapping result. The new initial mapping is shown in 8(d). Since we assume faults occur one by one in our approach, the speciﬁed locations of faults in Figure 8(b) will give 3! remapping results due to different sequence of fault occurrence. Besides faulty PEs on the left side, faulty PEs can also be located on the other three sides, i.e., right, top and bottom sides. Figure 9(a), (b), (c) and (d) shows 3×3 NoC with faults on on the opposite side of the faulty PEs. Therefore, at least ( n!×4 ) mapping the right, bottom, top and left respectively, and their spare PEs are added results can be obtained for an n × n architecture. In our experiment, n is less than 10 and hence we could try all possible sequences and select the result with the lowest commcost as our new initial mapping result. V. EX PER IMENTAL RE SULT S In this chapter, we presents the benchmarks, environment setting and experimental results. Our proposed approach is implemented using C and veriﬁed on a MPSoC evaluation tool called Transaction Generator (TG) [10]. Eight application models are used in the experiment as shown in Table I. av bench is from literature [11], while others: RS enc, RS dec, H264-720p dec, H264-1080p dec, Fpppp, FFT-1024 complex and Sparse 669     & ' , / ^ƉĂƌĞW &ĂƵůƚǇW (a)     & ' , / (b)    & , '  / (c)    & , '  / (d) Fig. 8. Extension to initial mapping.     & ' , / ^ƉĂƌĞW &ĂƵůƚǇW (a) faults on the left     & ' , / (b) faults on the right     & ' , / (c) faults on the top     & ' , / (d) faults on the bottom Fig. 9. PE failures are assumed on 4 different sides. are from the realistic NoC trafﬁc benchmark suite MCSL [12] based on real applications. The mapping of application tasks is on a mesh 4 × 5 architecture, where 4 PEs on the right most column are spare PEs, and XY deterministic routing is used. Hence, tool in MCSL partitions tasks to 16 groups and each group is mapped to one non-spare PE. As shown in Table I, column labelled #C ommunicationLinks among tasks is the number of communication links among tasks in the original spec. After mapping, we have column labelled #C ommunicationLinks among P E s to represent the total number of communication links among PEs which are related to the commcost. In the following experiments, we ﬁrstly perform the result of repairing PE failures, and then followed by the effect on initial mapping. Finally, the overall impact of applying our approach to both initial mapping and repairing is presented in the last experiment. A. Results on Repairing In repairing experiments, we present the evaluation results of repair rate and communication cost. Our repairing algorithm, called Ours for (cid:3) short, is compared with Liu’s work [1] called Liu s. Repair rate is the probability that faulty PEs can be repaired by spare ones. To conduct experiment on repair rate, we randomly generate 50000 faulty patterns for the number of faults varying from one to four on 4 × 5 and 8 × 9 mesh NoC respectively. Liu’s and our repairing algorithm are then called to repair faulty PEs. The results of repair rates are shown in Figure 10. It (cid:3) shows that as the number of faulty PEs increases, repair rate of Liu fallen rapidly in both 4× 5 and 8× 9 NoC architectures. In contrast, Ours s has remains 100% repair rate for all cases. For experiment of communication cost, initial mapping is produced by NMAP. 10000 faulty patterns are randomly generated. Different applications have different characteristics, including communication amount and number of communication links among tasks. For fair comparison of different applications, we use average hopcount each data byte traverses as the metric of communication cost instead of the commcost. Figure 11 shows the average communication cost of all applications with different number of faulty PEs. Let us take number of faulty PEs = 0 as the base. When number of faulty PEs = (cid:3) 1, communication cost of Liu s raises 0.119 hopcount/byte averagely while that of Ours raises only 0.064 hopcount/byte averagely. That is, Ours is improved by (0.1199 − 0.0643)/0.1199 = 0.463 = 46.3%. We compute the improvement for all cases in different number of faulty PEs and then compute the average. The communication cost of Ours achieves (cid:3) 43.59% less on average than that of Liu s. ϵϳ͘Ϭй ϵϳ͘ϱй ϵϴ͘Ϭй ϵϴ͘ϱй ϵϵ͘Ϭй ϵϵ͘ϱй ϭϬϬ͘Ϭй Ϭ ϭ Ϯ ϯ ϰ Ğ ƚ Ă Z ŝƌ Ă Ɖ Ğ Z EƵŵďĞƌŽĨĨĂƵůƚǇWƐ DĞƐŚϰǆϱ >ŝƵΖƐ KƵƌƐ ϵϳ͘Ϭй ϵϳ͘ϱй ϵϴ͘Ϭй ϵϴ͘ϱй ϵϵ͘Ϭй ϵϵ͘ϱй ϭϬϬ͘Ϭй Ϭ ϭ Ϯ ϯ ϰ ϱ ϲ ϳ ϴ Ğ ƚ Ă Z ŝƌ Ă Ɖ Ğ Z EƵŵďĞƌŽĨĨĂƵůƚǇWƐ DĞƐŚϴǆϵ >ŝƵΖƐ KƵƌƐ Fig. 10. Comparison of repair rate of Liu’s [1] versus Ours under different number of faults. 1.70 1.80 1.90 2.00 2.10 2.20 2.30 0 1 2 3 4 C o m m n u i a c i t n o o c s t ( g v a . n u o c p o h t / y b t e ) Number of faulty PEs Liu's Ours Fig. 11. Comparison of communication cost of Liu(cid:3) s versus Ours under different number of faults. B. Results on Initial Mapping As mentioned in Chapter IV, our method can be applied to improve initial mappings. We choose two initial mapping results, from Ori and NMAP respectively, as our baseline mappings. Ori is the default mapping of application models provided by TG, while NMAP [2] is a mapping algorithm that minimizes the average communication delay under the bandwidth constraint. With or without applying our proposed remapping algorithm to Ori and NMAP results, we have four combinations as shown in Table II. Figure 12 shows the comparison of communication cost of 4 initial mapping results for each application. Except FFT-1024 complex, either results of Ori or NMAP are improved after applying our remapping algorithm. Average improvement of Ori+Ours over Ori is 10.91% and that of NMAP+Ours over NMAP is 4.16%. 8A-3 670               8A-3 TABLE I B ENCHMARK S . Application av bench RS enc RS dec H264-720p dec H264-1080p dec Fpppp FFT-1024 complex Sparse Description a video codec pair and an audio codec Reed-Solomon code encoder Reed-Solomon code decoder H.264 video decoder with a resolution of 720p H.264 video decoder with a resolution of 1080p Chemical program performing multi-electron integral derivatives Fast Fourier transform with 1024 inputs of complex numbers Random sparse matrix solver # Tasks 40 262 182 2311 5191 334 16384 96 # CommunicationLinks among tasks among PEs 57 25 348 18 392 71 3461 65 7781 65 1145 120 25600 116 67 34 COM PAR I SON O F 4 COMB INAT ION S FOR IN I T IA L MA P P ING . TABLE II Mapping Algorithm Ori Ori + Ours NMAP NMAP + Ours Description default mapping that benchmarks provide applying our approach on default mapping a mapping technique proposed in [2] applying our approach on NMAP result Ori Ori+Ours NMAP NMAP+Ours 2.5 2 1.5 1 0.5 0 ) e t y b / t n u o c p o h g v a ( t s o c n o i t a c u n u m m o C Fig. 12. Comparison of communication cost of 4 methods for initial mapping. C. Overall Effect Finally, we conduct the experiment to see the overall effect by adopting our approach to both initial mapping and repairing. Three combinations of algorithms for initial mapping and repairing are: Nmap–Liu, NmapOurs– Liu and NmapOurs–Ours. In Nmap–Liu, the initial mapping is produced by NMAP and the repairing by Liu’s [1]. In NmapOurs–Liu, the initial mapping is produced by NMAP+Ours and the repairing by Liu’s [1]. In NmapOurs–Ours, the initial mapping is produced by NMAP+Ours and the repairing by Ours. The experimental result is shown in Figure 13. By comparing the Nmap–Liu and NmapOurs–Liu, we can observe that communication cost by NmapOurs–Liu decreases an average of 3.94% as compared with that by Nmap–Liu due to applying our method to initial mapping. When comparing the NmapOurs–Liu and NmapOurs–Ours, we can see the increment of communication cost caused by increasing faults is smaller in NmapOurs–Ours than in NmapOurs–Liu. The result of NmapOurs–Ours achieves 39.08% improvement on average compared with the result of NmapOurs–Liu. V I . CONCLU S ION S In this paper, we have presented a repairing algorithm to tolerate PE failures by remapping the application tasks. Our proposed graph modeling can precisely describe the the increment of communication cost resulting from task remapping. Our mapping result is 43.59% better than previous work [1]. As long as the number of faulty PEs are not more than that of spare ones, we can achieve 100% repair rate for all cases. Moreover, the proposed algorithm can also be applied to the initial mappings to improve results. We have performed a set of experiments to demonstrate that our proposed algorithm can indeed obtain better results in terms of communication cost. 671 ) e t y b / t n u o c p o h . g v a ( t s o c n o i t a c i n u m m o C 2.25 2.15 2.05 1.95 1.85 1.75 1.65 Nmap-Liu NmapOurs-Liu NmapOurs-Ours 0 1 2 3 4 Number of faulty PEs Fig. 13. Comparison of overall effect. "
2017,Islands of heaters - A novel thermal management framework for photonic NoCs.,"Silicon photonics has become a promising candidate for future networks-on-chip (NoCs) as it can enable high bandwidth density and lower latency with traversal of data at the speed of light. But the operation of photonic NoCs (PNoCs) is very sensitive to temperature variations that frequently occur on a chip. These variations can create significant reliability issues for PNoCs. For example, microring resonators (MRRs) which are the building blocks of PNoCs, may resonate at another wavelength instead of their designated wavelength due to thermal variations, which can lead to bandwidth wastage and data corruption in PNoCs. This paper proposes a novel run-time framework to overcome temperature-induced issues in PNoCs. The framework consists of (i) a PID controlled heater mechanism to nullify the thermal gradient across PNoCs, (ii) a device-level thermal island framework to distribute MRRs across regions of temperatures; and (iii) a system-level proactive thread migration technique to avoid on-chip thermal threshold violations and to reduce MRR tuning/trimming power by migrating threads between cores. Our experimental results with 64-core Corona and Flexishare PNoCs indicate that the proposed approach reliably satisfies on-chip thermal thresholds and maintains high network bandwidth while reducing total power by up to 64.1%.","Abstract— Silicon photonics has become a promising candidate for future networks-on-chip (NoCs) as it can enable high  bandwidth density and lower latency with traversal of data at the  speed of light. But the operation of photonic NoCs (PNoCs) is  very sensitive to temperature variations that frequently occur on  a chip. These variations can create significant reliability issues  for PNoCs. For example, microring resonators (MRRs) which  are the building blocks of PNoCs, may resonate at another wavelength instead of their designated wavelength due to thermal variations, which can lead to bandwidth wastage and data corruption in PNoCs. This paper proposes a novel run-time framework  to overcome temperature-induced issues in PNoCs. The framework consists of (i) a PID controlled heater mechanism to nullify  the thermal gradient across PNoCs, (ii) a device-level thermal island framework to distribute MRRs across regions of temperatures; and (iii) a system-level proactive thread migration technique to avoid on-chip thermal threshold violations and to reduce  MRR tuning/trimming power by migrating threads between  cores. Our experimental results with 64-core Corona and Flexishare PNoCs indicate that the proposed approach reliably satisfies on-chip thermal thresholds and maintains high network  bandwidth while reducing total power by up to 64.1%.  I. INTRODUCTION  The increasing core-density of chip-multiprocessors (CMP) requires high bandwidth to support extensive inter-core communication. Electrical NoCs cannot offer such a large bandwidth while maintaining an acceptable level of power dissipation [1]. On-chip photonic  links provide several advantages over traditional metallic counterparts, such as near light speed data transfer, higher bandwidth density,  and low power dissipation [2]. Moreover, photonic links have several  times lower data-dependent energy consumption compared to electrical wires, enabling the design of high-radix networks that are easier  to program [3]. Silicon photonics is thus becoming an exciting new  option for on-chip communication, and has catalyzed much research  in the area of photonic NoCs (PNoCs) for manycore systems [4].  However, photonic interconnects suffer from susceptibility to thermal fluctuations, which impacts correctness and performance. Hence,  PNoCs are yet to be widely adopted.   Microring resonators (MRRs) and waveguides are the basic building blocks of a PNoC. MRRs are used as modulators/demodulators at  the source/destination node. MRRs also perform photonic switching  operations to route an optical signal in PNoCs. However, photonic  components and especially MRRs are extremely susceptible to thermal fluctuations. Fig.1 depicts the impact of thermal variation on  MRRs. MRRs R1-Rn have been designed to resonate on wavelengths  ૃ1-ૃn respectively at temperature T1. As the temperature increases,  due to the resulting variations in refractive index, each MRR now  resonates with a different wavelength towards the red end of the visible spectrum (i.e., red-shift). This red-shift is shown in the figure  where, at temperature T2, MRR Ri will now be in resonance with ૃi1. This phenomenon reduces transmission reliability and results in  wastage of available bandwidth, e.g., MRRs are unable to read or  write to wavelength ૃn at temperature T2.  Maintaining a uniform temperature across all the MRRs is a must  for reliable data transmission in PNoCs. But thermal fluctuations and  gradients are common in CMPs. 3D-ICE [5] simulations of PARSEC  [6] and SPLASH-2 [7] benchmarks indicate a 15-20K peak thermal  gradient in a 64-core CMP as shown in Fig. 2. Such a huge gradient  causes a mismatch of resonant wavelengths of MRRs, leading to unreliable data transmission and PNoC performance degradation.   Recently, few techniques have been proposed to address thermal  issues in PNoCs. At the device-level, a trimming mechanism is proposed in [8] that induces a blue shift (decrease) in the resonance  wavelengths of MRRs using carrier injection. A tuning technique was  demonstrated in [9] where a red-shift (increase) in the resonance  wavelengths is induced by using a localized heater. Further several  athermal photonic devices have been presented to reduce the localized tuning/trimming power in MRRs. These design time solutions  include using cladding to reduce thermal sensitivity [10] and using  heaters as well as temperature sensors for thermal control. While  these device-level techniques are promising, they either possess a  high power overhead or require costly changes in the manufacturing  process (e.g., much larger device areas) that would decrease network  bandwidth density and area efficiency. At the system-level, a thread  migration framework was presented in [11] to avoid on-chip thermal  threshold violations and also reduce trimming/tuning power for MRs.  In [12], a ring aware thread scheduling policy was proposed to reduce  on-chip thermal gradients in a PNoC. A proportional-integral-derivative (PID) heater mechanism was proposed in [13] that minimizes  the effect of thermal variation on PNoC’s performance and power.  However, all these system-level techniques do not consider the impact of run-time workload variations and also result in considerable  power performance overheads.   Fig. 1: Impact of thermal variations on MRRs.  Fig. 2: Peak thermal gradient (in Kelvin) across a 64-core chip running  48-threaded PARSEC [6] and SPLASH-2 [7] benchmarks.   Our goal in this paper is to minimize thermal variations with reduced localized thermal tuning and trimming in PNoCs, thereby reducing key overheads and ultimately easing the adoption of PNoCs  for future CMP systems. We propose a novel low-power thermal  management framework that integrates an adaptive heater mechanism at the device-level and a dynamic thread migration scheme at  the system-level. This paper makes the following contributions:  •  A novel temperature island framework with adaptive heater based  MRR to handle thermal gradients across PNoC;     •  An islands of heaters based dynamic thread migration (IHDTM)  scheme in conjunction with a support vector regression based temperature prediction mechanism. Such a scheme nullifies on-chip  thermal threshold violations and also reduces trimming/tuning  power for MRRs;  •  The evaluation of the proposed framework on a 64-core CMP with  a system-level simulator shows: (a) 70% improvement in trimming  Islands of Heaters: A Novel Thermal Management  Framework for Photonic NoCs  *Dharanidhar Dang ‡ , *Sai Vineel Reddy Chittamuru † , Rabi Mahapatra ‡ , and Sudeep Pasricha † ‡ Department of Computer Science and Engineering, Texas A&M University, College Station, TX, U.S.A.  † Department of Electrical and Computer Engineering, Colorado State University, Fort Collins, CO, U.S.A.  {d.dharanidhar, rabi}@tamu.edu,{sai.chittamuru, sudeep}@colostate.edu  ______________________________________________________________________________________________________________________________________________________________________  * Dharanidhar Dang, Sai Vineel Reddy Chittamuru contributed equally to this  work.   This research is supported in part by grants from SRC, NSF (CCF-1252500),  and AFOSR (FA9550-13-1-0110). The authors are thankful to Luceda Photonics for their device-level experimental support.  978-1-5090-1558-0/17/$31.00 ©2017 IEEE 4A-2 306                     power dissipation over the most recent prior work, (b) 64.1% improvement in total power dissipation compared to a state-of-the-art  thermal management technique, (c) 13.72K improvement in peak  temperature, and (d) these improvements are achieved while maintaining full network-bandwidth.  The rest of the paper is organized as follows. Section II explains  the proposed thermal management framework in detail. Experiments,  results, and comparative analysis are demonstrated in Section III followed by conclusions in Section IV.  II. ISLANDS OF HEATERS BASED DYNAMIC   THERMAL MANAGEMENT (IHDTM)  The proposed IHDTM framework enables variation-aware thermal  management by integrating device-level and system-level enhancements. A high-level overview of the framework is shown in Fig. 3.  At the device-level, the entire PNoC layer is divided into ‘k’ regions  or islands, namely: TIS1-island, TIS2-island, TIS3-island, and so on. All  MRRs in the TISi-island (i ≤ k) are designed to operate at TISi; similarly, MRRs in the other islands are designed to operate at their respective temperatures. We use our device-level technique to overcome small deviations (±10K) in TISi whereas the system-level technique is used to adapt to larger variations (> ±10K). The device-level  technique aims to adapt to the changing on-chip thermal profile,  maintaining maximum bandwidth and correct MRR operation while  minimizing trimming and tuning power in the PNoC. At the systemlevel, the dynamic thread migration scheme maintains acceptable  core-temperatures for each island. The following sections explain the  proposed (i) device-level island framework and (ii) system-level  thread migration scheme in detail.  Fig. 3: IHDTM framework with device-level thermal islands and system-level temperature-aware thread migration mechanism (TATM)   A. Thermal Islands  The thermal distribution across a 64-core PNoC chip running  PARSEC and SPLASH-2 benchmarks (using 3D-ICE simulation)  shows three major zones of temperature: 363K, 343K, and 323K.  Also, the average thermal gradient in the PNoC chip is found out to  be approximately 15-20K. To reduce this gradient, the proposed device-level framework adopts three islands (as shown in Fig. 3) each  of which are maintained at a unique temperature by assigning TIS1,  TIS2, and TIS3 to 363K, 343K, and 323K respectively. As mentioned  in the previous section, MRRs in the 363K-island (TIS1-island) are  designed to operate at 363K with a variation range of ±10K. MRRs  employ thermal tuning and electrical trimming when they are operated below and above their designed temperatures respectively. Similarly, MRRs in other islands are designed to operate at the respective  temperatures. For PNoCs of other sizes (e.g. 16-core, 25-core, 36core, 128-core, 256-core), there can be slight variations in the number  of islands and their respective temperature zones. Accordingly, the  numbers of islands and their temperatures can be fixed at design time.                            (a)                                                         (b)   Fig. 4: (a) MRR with adaptive heater (b) Thermal tuning of MRR  To manage localized temperature variation below designed temperature, each MRR is integrated with a PID controller [15] based  heater as shown in Fig. 4(a). The PID controller is tuned with proportional band Kp=50, integral cycle-time Ki=1 millisecond (ms), and  derivative coefficient Kd=0. An open source PID tuning software [15]  is used to determine optimal values of Kp, Ki, and Kd.   Algorithm 1: Thermal management of MRR  Input: Temperature (T) around the MRR detected by thermal sensor  //Controller converts T to appropriate heater current as follows:  1:     dT = |Tisland -T|        2:     PHeat =  ௗ் ఘ × Heff  3:     if (T ≤ T island) then   iHeat = iMax-ට௉ ಹ೐ೌ೟ ோ ಹ೐ೌ೟ 4:     else     iHeat = iMax+ට௉ ಹ೐ೌ೟ ோ ಹ೐ೌ೟ Output: current (i Heat) to be fed to heater  Algorithm 1 depicts the control algorithm for the heater in each  MRR to stabilize thermal variations. In the algorithm, T represents  the temperature across an MRR as detected by the corresponding  thermal sensor, Tisland is the fixed temperature of the island in which  the MRR resides (Tisland = TISi), PHeat is the heater power, iHeat represents heater current, and Heff stands for the transfer function of the  heater. With any local temperature change dT, there is an equivalent  shift in resonance for the MRR. To undo this resonance shift in an  MRR, an equivalent amount of heat must be radiated by the heater  integrated with that MRR. As per the algorithm, the controller collects temperature data T from the local thermal sensor as input. In  step 1, the absolute value of the difference between T and Tisland is  calculated followed by determining the required heater power PHeat  in step 2. T is compared with Tisland in step 3 and accordingly the required heater current iHeat is computed either in steps 3-4. The evaluated value of iHeat is fed to the heater coil. This amount of current is  needed by the heater to maintain the fixed temperature Tisland around  the MRR. Our analysis shows that a maximum of 1 ms of time is  needed for the heater element to bring the surrounding temperature to  the desired value of Tisland. We account for this time delay in our simulations. Fig. 4(b) shows the tuning process of an MRR with injected  heater current as explained in the algorithm. The control algorithm is  invoked after every 1ms for each MRR.  This heater-based technique helps to stabilize thermal fluctuations  in each temperature island with reduced tuning power. However, if  the power footprint of a workload on a core associated with a 363Kisland is very low, its core temperature may fall below the lower thermal limit (i.e. smaller than 353K). This thermal gradient can significantly increase tuning power consumption of an associated MRR.  Similarly, if the power footprint of a workload on a core associated  with a 323K-island keeps increasing beyond a threshold, then its core  temperature might reach beyond the control of the MRR-trimmer (i.e.  greater than 333K). This will in turn permanently shift the resonance  of the MRR, inducing errors during communication. To address these  issues, we propose a system-level temperature-aware thread migration (TATM) technique that performs thread migration to idle cores  to maintain temperatures of corresponding MRRs close to the designtemperatures of their respective islands. By intelligently migrating  threads, this technique reduces device-level tuning/trimming power  in MRRs. TATM also aims to proactively reduce thermal hotspots,  which in turn will reduce instances of irrecoverable drift in MRRs.  B. Temperature-Aware Thread Migration Scheme (TATM)  1) Objective: The primary goal with TATM is to maintain the  temperature of all the cores in an island on a die below a specified  thermal threshold (Tt) and above a thermal limit (Tl), i.e., for a core i  in the TISj-island, Tlj ≤ Ti ≤ Ttj where Ti is the temperature of core i, Ttj  is threshold temperature of TISj-island, and Tlj is thermal limit of TISjisland. TATM maintains the core temperatures such that the  temperature of all the MRRs within an island is close to their design  temperature, to reduce tuning power consumption in adaptive heaters  as explained in the previous section.   4A-2 307                     We utilize support vector based regression (SVR) to predict the  future temperature of a core. This predicted temperature of a core is  compared with the corresponding island’s thermal threshold (upper  limit) and thermal limit (lower limit) to determine the potential for a  thermal emergency. If such a potential exists, then TATM initiates  thread migration. Inter-island thread migration (Inter-island cores  (IEIC)) is preferred over intra-island thread migration (Intra-island  cores (IAIC)). This step has a twofold benefit. Firstly, by moving the  thread away from a core that could suffer a thermal emergency, we  avoid instances of irrecoverable drift in the MRR groups of that core.  Secondly, by moving the thread to a core in different island, we ensure that the temperature of the island and its corresponding ring  blocks remains between the island’s thermal threshold (Tt1, Tt2, and  Tt3) and thermal limit (Tl1, Tl2, and T13) to conserve trimming/tuning  power. If a thermal emergency occurs due to exceeding the thermal  threshold, then it is preferred that the thread is migrated to a core in  an island whose MRR design temperature is higher. If a thermal  emergency occurs due to temperature falling below the thermal limit  then it is preferred that the thread is migrated to a core in an island  whose MRR design temperature is lower. The parameters used to describe TATM in this section are shown in Table I.  Table I: List of TATM parameters and their definitions  Symbol  Definition  IPCi  Instructions per cycle of i th  core  Ti  Current temperature of i th  core  TNi  Average temperature of immediate neighboring cores of  i th  core; if this core is on chip periphery and missing  neighbors, then we consider virtual neighbor cores at  ambient temperature in lieu of the missing cores  PTi  Predicted temperature of i th  core  Ttj  Thermal threshold of TISj-island  Tlj  Thermal limit of TISj-island  IEICtj  Inter-island cores for TISj-island whose island MRRs design temperature is greater than TISj  IEIClj  Inter-island cores for TISj-island whose island MRRs design temperature is smaller than TISj  IAICj  Intra-island cores for TIS j-island  C  Regularization parameter  W  Weight vector for regression  xi and y i   Input and outputs in training and test data  ξi  Slack variables  ε  Error function  b  Bias for cost function  2) Temperature Prediction Model: We designed a support vector regression (SVR) based temperature predictor that accepts input parameters reflecting the workload for a core i, in terms of instructions per  cycle (IPCi), temperature (Ti), and surrounding core temperatures  (TNi), and predicts the future temperature for core i.  Architecture: A typical SVR [21] relies on defining a prediction  model that ignores errors that are situated within the ε range of the  true value. This type of a prediction model is called an ε-insensitive  prediction model. The variables (ξ and ε) measure the cost of the errors on the training points. These are zero for all points that are inside  the ε-insensitive band. SVR is primarily designed to perform linear  regression using a cost function (CF) as depicted in equation (1).       ܥܨ ൌ ݉݅݊	 ଵ ଶ 	ܹ ் . ܹ ൅ ܥ ෌ ሺߦ ௜ ൅ ߦ ௜ ∗ሻ ௡ ௜ୀଵ 																							 ሺ1ሻ  Subject to:  		ݕ ௜ െ ܹ ்ߔሺ	ݔ ௜ ሻ െ ܾ	 ൑ ߝ ൅ 	 ߦ ௜ 		ሺߦ ௜ ൒ 0, ݅ ൌ 1, 2, … , ݊ሻ		ሺ2ሻ ܹ ்ߔሺ	ݔ ௜ ሻ ൅ ܾ െ 	ݕ ௜ ൑ ߝ ൅ 	 ߦ ௜ ∗ 	ሺߦ ௜ ∗ ൒ 0, ݅ ൌ 1, 2, … , ݊ሻ	ሺ3ሻ ߢ൫	ݔ ௜ , 	ݔ ௝ ൯ ൌ ߔሺݔ ௜ ሻ்ߔሺݔ ௝ ሻ																																													ሺ4ሻ SVR performs linear regression in this high-dimension space using  ε-insensitive loss and, at the same time, tries to reduce model complexity by minimizing W T .W. This can be described by introducing  (non-negative) slack variables ξi and ξi *  (i = 1 to n), to measure the  deviation of training samples outside the ε-insensitive band. Thus  SVR is formulated as minimization of the cost function (CF) in equation (1) with constraints shown in equations (2) and (3).   To handle non-linearity in data, SVR first maps the input xi onto  an m-dimensional space using some fixed (non-linear) mapping denoted as Ȱ, and then a linear model is constructed in this high-dimensional space as shown in equations (2) and (3). This allows it to overcome drawbacks of linear and logistic regression towards handling  non-linearity in data. This class of SVRs is called kernel based SVRs  which use kernel κ as shown in equation (4) for implicit mapping of  non-linear training data into a higher dimensional space.  As on-chip temperature variation data is non-linear in the original  space, our SVR model employs a kernel based regression which uses  a Radial Basis Function (RBF) [14] as shown in equation (5):  ߢሺ	ݔ ௜ , 	ݔ ௝ ሻ ൌ ݁ݔ݌ ቀെߛห	ݔ ௜ െ 	ݔ ௝ ห ଶ ቁ 																																						ሺ5ሻ The RBF kernel improves the accuracy of SVR when data has nonlinearity in the original space. We performed a sensitivity analysis  (SA) to determine regularization parameter (C) and gamma (γ) values  of the kernel based SVR (see Section III.A for chosen values). This  SA overcomes the possibility of over fitting of training data and improves accuracy further. The definition of each of the variables used  in equations (1) to (5) are mentioned in Table I.  Training and Accuracy: We trained our SVR model using a set of  multi-threaded applications from the PARSEC [6] and SPLASH-2  [7] benchmark suites, specifically: blackscholes (BS), bodytrack  (BT), vips (VI), facesim (FS), fluidanimate (FA), swaptions (SW),  barnes (BA), fft (FFT), radix (RX), radiosity (RD), and raytrace (RT)  with different thread counts: 2, 4 and 8. We considered different combinations of thread mappings on a 9-core (3×3) floorplan, to train our  predictor to determine the temperature of the center (target) core. The  threads mapped to a 9-core floorplan represents a generic mapping  and can be applied to 64-core, 128-core, and 256-core floorplans.   As the future temperature of a target core is dependent on the average temperature of its immediate neighboring cores, we trained our  SVR model with temperature inputs from the target core running a  single thread, as well as its surrounding cores running a variable number of threads. Simulations with various mappings of these threads  allowed us to obtain data to train our SVR model. This data included  temperature for the target core and its neighboring core temperatures,  as well as instructions per cycle (IPC) for the target core. IPC is very  useful to determine if there is a phase change in an application and  plays a crucial role in maintaining future temperature prediction accuracy especially when temperatures of a target core and its neighbors are similar at a given time. Our training algorithm involved an  iterative process that adjusts the weights and bias values in the SVR  (equations (1)-(3)) to fit the training set.                               (a)                                                      (b)  Fig. 5: Actual and predicted maximum temperature variation with execution time for (a) fluidanimate (FA) and (b) radiosity (RD) benchmarks  run on a 64-core platform executing 32-threads  We verified the accuracy of our SVR model for multi-threaded  benchmark workloads (we considered 6000 floorplans, with 70% of  input data for training and 30% for testing) and found that it has an  accuracy of over 95%. Fig. 5(a) and (b) show actual and predicted  on-chip temperature variations for a 64-core platform executing 32  threads of the FA and RD benchmarks. From these figures it can be  seen that our temperature predictor tracks temperature quite  accurately. When predicted temperature is beneath thermal limit or  exceeds the thermal threshold our thread migration mechanism  (which is discussed next) migrates threads between cores to reduce  4A-2 308                                  tuning/trimming power and keep overall maximum temperature  below the threshold.   3) Thermal Management Algorithm: Fig. 6 illustrates the entire  TATM technique. For each core, we periodically monitor the IPC  value from performance counters and temperature from thermal sensors. If a thermal emergency is predicted for a core by the SVR predictor, then TATM initiates a thread migration procedure, otherwise  no action is taken. In this work we have considered the thermal  threshold of an island to be equal to maximum allowable temperature  in that island i.e. Ttj = TISj + 10K to avoid instances of irrevocable  drift in MRs and thermal limit of an island is minimum allowable  temperature in that island i.e. Tlj = TISj - 10K to reduce tuning power.  Fig. 6: Overview of TATM technique with support vector regression  (SVR) based temperature prediction model.  Algorithm 2: TATM thread migration algorithm  Inputs: Current core temperature (Ti), average neighboring core temperature (TNi), current core IPC (IPCi)  1:      for each core i do  // Loop that predicts future temperature        2:            PTi = SVR_predict_future_temperature (Ti, TNi, IPCi)  3:      for each core i do // Loop that checks for free IAICs   4:            j = Find island of core (i)  5:            if IPCi == 0 then  List_IAIC j = Push i  //add core to IA ICj list  6:      for each island j do // Loop that create IEIC list for TISj-island  7:           for all islands m do  8:                 if TIsm > TIsj then IEICtj = push IAICm      9:                 else if TIsm < TIsj then IEIC lj = push IAICm      10:    for each core i do   // Loop that performs thread migration (TM)  11:            j = Find island of core (i)  12:         if PTi > Ttj then // Check predicted temp exceed thermal threshold  13:               if List_IEIC tj ≠ ሼሽ // Do inter-island TM   14:                   Migrated_core = Find_lowest_T IS_core(List_ IEIC tj)  15:                   Thread_migration(core_i → Migrated_core)   16:                   n = island of Migrated_core  17:                   List_ IAIC n and List_IEIC tj = Pop Migrated_core  18:               else if List_ IAICj ≠ ሼሽ then // Do intra-island TM   19:                   Migrated_core = Find_min_temp_core(List_ IAIC j)  20:                   Thread_migration( core_i → Migrated_core)  21:                   List_ IAIC j and List IEIC = Pop Migrated_core  22:         else if PTi < Tlj then // if predicted temp is below thermal limit  23:               if List_IEIC lj ≠ ሼሽ // Do inter-island TM   24:                   Migrated_core = Find_highest_TIS_core(List_ IEIC lj)  25:                   Thread_migration(core_i → Migrated_core)   26:                   n = island of Migrated_core  27:                   List_ IAIC n and List_IEIC lj = Pop Migrated_core  28:               else if List_ IAICj ≠ ሼሽ then // Do intra-island TM   29:                   Migrated_core = Find_min_temp_core(List_ IAIC j)  30:                   Thread_migration( core_i → Migrated_core)  31:                   List_ IAIC j and List IEIC = Pop Migrated_core  Output: Thread migration to IAIC or IEIC cores  Algorithm 2 shows the pseudo-code for the TATM thread migration procedure. Firstly, future temperature (PTi) of the i th  core is predicted using the SVR based predictor with inputs: core temperature  (Ti), core IPC (IPCi), and temperature of neighboring cores (TNi) in  steps 1-2. The list of available free cores (IAICj) in TISj-island (i.e.,  those that are not currently executing any thread) is obtained in steps  3-5. In steps 6-9, a loop iterates over islands to generate a list of free  cores IEICtj and IEIClj in other islands whose TIS is higher and lower  than current island respectively. In step 10, a loop iterates over all  cores to perform thread migration. Step 12 and 22 checks for possible  thread migration conditions (i.e., thermal emergency cases where current core predicted temperature (PTi) in TISj-island is greater than  thermal threshold (Ttj) or smaller than thermal limit (Tlj)). If a thread  migration is required as PTi > Ttj, then in steps 13-21, we check for  free IEICtj, and if they are available then we migrate the thread from  the current core to the IEICtj core with the lowest TIS (inter-island  migration), else we migrate the thread to a free IAICj with the lowest  temperature (intra-island migration). On the other hand, if a thread  migration is required as PTi < Tlj, then in steps 23-31, we check for  free IEIClj, and if they are available then we migrate the thread from  the current core to the IEIClj core with the highest TIS (inter-island  migration), else we migrate the thread to a free IAICj with the lowest  temperature (intra-island migration). This TATM thread migration  technique is invoked at every 1ms (epoch) and the sample frequency  of SVR is considered as 0.1 ms (10 times lower compared to the  epoch for thread migration). This sampling frequency is sufficient to  monitor on-chip temperature variations [20].  III. EXPERIMENTS, RESULTS, AND ANALYSIS  A. Experimental Setup   The IPKISS [16] tool was used for the design and simulation of  heaters, MRRs, and other silicon photonic components. This tool allows photonic component layout design, virtual fabrication of components in different technologies, physical simulation of components,  and optical circuit design and simulation. The circuit-level results obtained from IPKISS were used for system-level simulation.  We target a 64-core CMP system for evaluation of our IHDTM  framework. Each core has a Nehalem x86 [17] microarchitecture with  32 KB L1 instruction and data caches and a 256 KB L2 cache, at  32nm and running at 5GHz. We evaluate our framework on two wellknown PNoC architectures: Corona [3] and Flexishare [22]. Corona  uses a 64×64 multiple write single read (MWSR) crossbar with token  slot arbitration. Flexishare uses 32 multiple write multiple read  (MWMR) waveguide groups with a 2-pass token stream arbitration.  Each MWSR waveguide in Corona and each MWMR waveguide in  Flexishare is capable of transferring 512 bits of data from a source  node to a destination node.  Table II: Properties of materials used by 3D-ICE tool [5]  Material  Thermal Conductivity Volumetric Heat Capacity   Silicon  1.30e-4 W/µm K  1.628e-12 J/µm 3  K  Silicon dioxide  1.46e-6 W/µm K  1.628e-12 J/µm 3  K  BEOL  2.25e-6 W/µm K  2.175e-12 J/µm 3  K  Copper  5.85e-4 W/µm K  3.45e-12 J/µm 3  K  BEOL: Back end of line fabrication material  We modeled and simulated these architectures with the IHDTM  framework for multi-threaded applications from the PARSEC [6] and  SPLASH-2 [7] benchmark suites (Section II.B). Simulations were  performed with an execution period of one billion cycles. Power and  instruction traces for the benchmark applications were generated using the Sniper 6.0 [17] simulator and McPAT [18]. We used the 3DICE tool [5] for thermal analysis. We considered a three layered 3Dstacked CMP system as advocated in existing PNoC architectures [3],  [22] with a planar die area footprint of 400mm 2 , where the top layer  is the core-cache layer, the middle layer is the analog electronic layer  [3] which contains control circuits for modulator and photodetector  and also the trans-impedance amplifiers of detectors, and the bottom  layer is the photonic layer with MRRs, waveguides, ring heaters, and  ring trimmers for carrier injection. Some of the key materials used in  the construction of the 3D-stack in the 3D-ICE tool and their properties are shown in Table II. We used a heat sink adjacent to the corecache layer for heat dissipation to the ambient environment.   The MRR thermal sensitivity was assumed to be 0.11nm/K [9].  For PNoCs, we considered 64 dense-wavelength-divisionmultiplexing (DWDM) waveguides sharing the working band 15301625 nm. The MRR trimming power is set to 130µW/nm [8] for  current injection (blue shift) and tuning power is set to 240µW/nm  [9] for heating (red shift). To compute laser power, we considered  detector responsivity as 0.8 A/W [2], MRR through loss as 0.02 dB,  waveguide propagation loss as 1 dB/cm, waveguide bending loss as  0.005 dB/90 0 , and waveguide coupler/splitter loss as 0.5 dB [2]. We  4A-2 309                     4A-2 calculated photonic loss in components using these values, which sets  maximum  temperature. For most of  the cases, maximum  the photonic laser power budget and correspondingly the electrical  temperatures with PDTM and IHDTM are below the thermal  laser power. For energy consumption of photonic devices, we adapt  threshold. On average, IHDTM has 2.37K and 1.56K lower  parameters from [19], with 0.42pJ/bit for every modulation and  maximum temperature compared to the PDTM policy for 48 and 32  detection event, and 0.18pJ/bit for modulator/detector driver circuits.   threads, respectively. IHDTM prefers to migrate threads within  The ambient temperature was set to 303K for our analysis and the  islands (inter-island) of cores based on the power consumption of  for TIS1-island, TIS2-island, and TIS3-island thermal thresholds were  running thread, which facilitates reduction in its peak temperature  set to 373K, 353K, and 333K respectively and the thermal limits were  compared to PDTM.  set to 353K, 333K, and 313K respectively. Based on our sensitivity  analysis we get the best accuracy for our SVR-based temperature predictor when parameters C and γ are set to 1000 and 0.1 respectively.  We also considered thread migration overhead in our simulations that  ranged from 500-1000 cycles to account for startup latency (extra  cache misses, branch miss predictions) in the migrated core. Further,  in the simulation we considered a 250-500 cycles overhead towards  migration of threads for writing dirty cache lines from the write back  caches, flushing the pipeline in the source core, and also PNoC latency to transfer data from architectural registers from the source core  to the migrated core.  B. Experimental Results  We compared the performance of our IHDTM framework with  two prior works on multicore thermal management: a ring aware policy (RATM) [12] and a predictive dynamic thermal management  (PDTM) framework [20]. To compare these frameworks, we consider  Corona and Flexishare PNoC architectures. RATM distributes  threads uniformly across cores that are closer to PNoC nodes first and  then distributes the remaining threads in a regular pattern from outer  cores to inner cores. PDTM uses a recursive least square based temperature predictor to determine if the predicted temperature of a core  exceeds a thermal threshold, and if so then thread migration is performed from that core to the coolest free core.   (a)  (b)  Fig. 8: Normalized power (Laser Power (LP), Trimming and tuning  power (TP) and modulating and detecting Power (MDP)) comparison of  IHDTM with RATM and PDTM for (a) 48 and (b) 32 threaded applications of PARSEC and SPLASH-2 suites executed on Corona PNoC architectures for a 64-core multicore system. Results shown are normalized  w.r.t RATM.  (a)  (b)                                       (a)                                               (b)  Fig. 9: Normalized average power (laser power (LP), trimming and tuning power (TP) and modulating and detecting power (MDP)) comparison  of IHDTM with RATM and PDTM for (a) 48 and (b) 32 threaded applications of PARSEC and SPLASH-2 suites executed on Flexishare PNoC  for a 64-core system. Power results are normalized wrt RATM results.  Bars represent mean values of power dissipation; confidence intervals  show variation in power across PARSEC and SPLASH-2 benchmarks.  Fig. 8 and Fig. 9 show the power consumption comparison for the  Fig. 7: Maximum temperature comparison of IHDTM with RATM and  three thermal-management techniques across multiple 48-threaded  PDTM for (a) 48 and (b) 32 threaded PARSEC and SPLASH-2 benchmarks executed on 64-core CMP with Corona PNoC.  and 32-threaded applications for the Corona and Flexishare PNoC  architectures, respectively. One of the main reasons why IHDTM has  Fig. 7 shows the maximum temperature obtained with the three  lower power consumption than RATM and PDTM is that it more  frameworks across eleven applications from the PARSEC and  aggressively reduces thermal tuning and trimming power in both  SPLASH-2 benchmarks suites with 48 and 32 thread counts executed  Corona and Flexishare PNoCs. It is evident from Fig. 8(a)-(b) that  on a 64-core system with the Corona PNoC architecture. From Fig.  IHDTM running 48 threads has 61.6% and 62.5%; and IHDTM  7(a) it can be observed that for the IHDTM framework the FFT  running 32 threads has 67.3% and 68.5% lower thermal tuning and  application with 48 threads exceeds the threshold (363K) by 0.4K as  trimming power on average compared to RATM and PDTM for  there are insufficient number of free cores in the 363K-island on the  Corona PNoC architecture respectively. Similarly, from Fig. 9(a) and  chip whose temperature is below the thermal threshold to migrate  (b), it can be seen that IHDTM running 48 threads has 62.8% and  threads. However, in Fig. 7(b) our IHDTM framework avoids  63.9%; and IHDTM running 32 threads has 68.5% and 70% lower  violating thermal thresholds for all the benchmark applications with  tuning and trimming power on average compared to RATM and  32 threads. On average, IHDTM has 13.27K and 13.72K lower  PDTM for Flexishare PNoC respectively. The IHDTM framework  maximum temperature compared to the RATM policy for 48 and 32  intelligently conserves tuning/trimming power compared to RATM  threads, respectively. Along with local thermal stabilization by PID  and PDTM by performing intelligent intra-island and inter-island  controlled heaters, IHDTM migrates threads from hotter cores to  thread migration.  cooler cores to control maximum temperature, whereas RATM does  IHDTM saves considerable thermal tuning and trimming power to  a simple thread allocation that is unable to appropriately control  ultimately reduce total power. From the power analysis in Fig. 8 and  310                               Fig. 9, it can be observed that IHDTM with Corona running 48  threads has 45.5% and 46.8%; and IHDTM with Corona running 32  threads has 51.6% and 52.3% lower total power consumption compared to Corona with RATM and PDTM respectively. Further, Flexishare with IHDTM running 48 threads has 55.9% and 57.2%; and 32  threads has 63.5% and 64.1% lower power consumption compared to  Flexishare with RATM and PDTM respectively.   (a)  (b)  Fig. 10: Normalized execution time comparison of IHDTM with RATM  and PDTM for (a) 48 and (b) 32 threaded applications of PARSEC and  SPLASH-2 suites executed on Corona PNoC for a 64-core system. Results  shown are normalized w.r.t RATM.                                            (a)                                      (b)  Fig. 11: Normalized average execution time comparison of IHDTM with  RATM and PDTM for Flexishare PNoC running (a) 48; and (b) 32  threaded applications from PARSEC and SPLASH-2 suites executed on  64-core system. Results are normalized wrt RATM results. Bars represent mean values of execution time; confidence intervals show variation  in execution time across PARSEC and SPLASH-2 benchmarks.  Fig. 10 shows the average execution time comparison between the  three frameworks across the 11 48-threaded and 32-threaded applications from the PARSEC and SPLASH-2 suites, for the Corona PNoC  architectures respectively. From Fig. 10(a) and (b) it can be seen that  Corona with IHDTM running 48 and 32 threads has 12.8% and 7.4%  higher execution time respectively compared to Corona with RATM.  Corona with IHDTM needs extra execution time to migrate threads  between cores whereas the RATM policy simply schedules threads  without any migration, and thus does not possess such overheads. The  execution time overhead of Corona with IHDTM running 32 threads  is lower compared to 48-threaded version, as it lowers traffic congestion in the Corona PNoC which in turn reduces overall latency. Further, Corona with IHDTM running 48 and 32 threads has 2.6% and  4.3% higher execution time respectively compared to PDTM.  IHDTM has more number of thread migrations compared to the number of thread migrations in PDTM, as IHDTM performs intra-island  and inter-island thread migrations when the thermal emergencies are  predicted by the SVR predictor. Similarly, from Fig. 11(a) and (b),  the Flexishare with IHDTM running 48 and 32 threads has 9% and  5.9% higher execution time compared to RATM and 3.4% and 4.4%  higher execution time compared to Flexishare with PDTM. From the  execution time results it can be seen that Flexishare has lower execution time overhead compared to Corona as it uses a faster MWMR  crossbar instead of slower MWSR crossbar in Corona.  Lastly, from the power consumption and execution time results,  we can obtain energy consumption results for the three frameworks.  On an average, for Corona, energy consumption of IHDTM running  48 threads is 38.5% and 45.4% lower compared to RATM and  PDTM, respectively. Further energy consumption of Corona with  IHDTM running 32 threads is 48.1% and 50.3% lower compared to  RATM and PDTM, respectively. On the Flexishare architecture,  IHDTM running 48 threads has 52.2% and 56% lower energy consumption compared to RATM and PDTM respectively; and IHDTM  running 32 threads has 61.4% and 62.6% lower energy consumption  compared to RATM and PDTM, respectively. From the energy consumption results IHDTM has better energy savings for the optimized  Flexishare compared to the Corona.  IV. CONCLUSIONS  We have presented the IHDTM framework that exploits devicelevel on-chip thermal islands and system-level dynamic thread migration scheme TATM for the reduction of maximum on-chip temperature and also conserves trimming and tuning power of MRRs in  DWDM-based PNoC architectures. The proactive thermal management scheme used in IHDTM results in interesting trade-offs between  performance and power/energy across two different state-of-the-art  crossbar-based PNoC architectures. Our experimental analysis on the  well-known Corona and Flexishare PNoC architectures has shown  that IHDTM can notably conserve total power by up to 64.1% and  thermal tuning power by up to 70%.  "
2017,DLPS - Dynamic laser power scaling for optical Network-on-Chip.,,
2017,Adaptive load distribution in mixed-critical Networks-on-Chip.,"Modern Networks-on-Chip (NoCs) must accommodate a diversity of temporal requirements e.g. provide guarantees for real-time senders with the minimum impact on performance sensitive best-effort (BE) traffic. In this work, we propose a protocol-based adaptive load distribution which by selectively detouring BE traffic i.e. load balancing, allows to significantly improve NoC's performance without costly hardware extensions. The introduced method offers, during runtime, safe and efficient integration of mixed-critical workloads through the coupling of the flow control with the path selection based on the global NoC state. The requested real-time reliability of the interconnect is achieved through predictable synchronization with control messages supported by a formal analysis and an experimental evaluation.",
2017,A tighter recursive calculus to compute the worst case traversal time of real-time traffic over NoCs.,"Network-on-Chip (NoC) is a communication subsystem which has been widely utilized in many-core processors and system-on-chips in general. In this paper, we focus on a Round-Robin Arbitration (RRA) based wormhole-switched NoC which is a common architecture used in most of the existing implementations. In order to execute real-time applications on such a NoC based platform, a number of given real-time requirements need to be fulfilled. One of the most typical requirements is schedulability which refers to determining if real-time packets can be delivered within the given time durations. Timing analysis is a common tool to verify the schedulability of a real-time system. Unfortunately, the existing timing analyses of RRA-based NoCs either provide too pessimistic estimates which results in overly allocated resources or require a large amount of processing which limits the applicability in reality. Therefore, in this paper, we present an improved timing analysis, aiming to provide more accurate estimates along with acceptable computation time. From the evaluation results, we can clearly observe the improvement achieved by the proposed timing analysis.","3C-3 A Tighter Recursive Calculus to Compute the Worst Case Traversal Time of Real-Time Trafﬁc over NoCs Meng Liu, Matthias Becker, Moris Behnam, Thomas Nolte Email: {meng.liu, matthias.becker, moris.behnam, thomas.nolte}@mdh.se M ¨alardalen University, V ¨aster ˚as, Sweden Abstract—Network-on-Chip (NoC) is a communication subsystem which has been widely utilized in many-core processors and system-on-chips in general. In this paper, we focus on a Round-Robin Arbitration (RRA) based wormhole-switched NoC which is a common architecture used in most of the existing implementations. In order to execute real-time applications on such a NoC based platform, a number of given real-time requirements need to be fulﬁlled. One of the most typical requirements is schedulability which refers to determining if real-time packets can be delivered within the given time durations. Timing analysis is a common tool to verify the schedulability of a real-time system. Unfortunately, the existing timing analyses of RRA-based NoCs either provide too pessimistic estimates which results in overly allocated resources or require a large amount of processing which limits the applicability in reality. Therefore, in this paper, we present an improved timing analysis, aiming to provide more accurate estimates along with acceptable computation time. From the evaluation results, we can clearly observe the improvement achieved by the proposed timing analysis. I . IN TRODUC T ION Many-core processors are gaining more and more attention due to their high computing capability along with limited hardware cost. Communication between computing cores are typically accomplished by a Network-on-Chip which is also a communication subsystem used in System-on-Chips (SoC) in general. In order to achieve high throughput as well as low hardware cost, the wormhole-switching technique is favored for most implementations of NoCs [14]. Under such a technique, each packet is divided into a number of transmission units (called ﬂits). A router can transmit a received ﬂit without waiting for the arrival of the complete packet. Therefore, this technique incurs much lower buffer cost compared to a store-and-forward mechanism (e.g. Ethernet), which suits the design principles of NoCs. An access control of each output-port at a router is necessary such that contentions between communication ﬂows1 can be well-manageable. Round-Robin Arbitration (RRA) is one of the most utilized arbitration mechanisms for NoCs. RRA provides relatively fair service to all the ﬂows and requires simple implementation, thus it has been used in many existing implementations (e.g. [22][1][3]). The work presented This work has been supported by the Knowledge Foundation (KKS) through the project PREMISE and DPAC. 1A ﬂow is a series of packets with the same characteristics. in this paper focuses on executing real-time applications in RRA-based wormhole-switched NoCs. Real-time applications typically have a number of speciﬁc timing requirements. One of the most important requirements is schedulability which refers to determining if all the real-time packets can be delivered within given time durations (called deadlines). Guaranteeing the fulﬁllment of such a requirement can typically be accomplished by two types of approaches: 1) performing timing analysis to verify if packets can be delivered in time; 2) using different mechanisms to enforce the control on packet transmission (e.g. contention control) such that the end-to-end traversal delay of each packet is guaranteed not to exceed its deadline. This paper addresses the ﬁrst aspect, and we target applications with hard real-time requirements where deadlines of packets need to be strictly respected. A number of analysis methods have been proposed in the literature targeting RRA-based NoCs, such as Network Calculus based analyses (e.g. [16][5]), Compositional Performance Analysis (CPA) based approaches (e.g. [7]) and Recursive Calculus (RC) based solutions (e.g.[4][2]). This paper focuses on the RC based analysis. The existing RC based approaches either provide too pessimistic estimates due to the usage of approximations to simplify the analysis (e.g. [4]) or require a long processing time because of the exploration of a huge search space (e.g. [2]). Such limitations hinder the application of those solutions in practice. Therefore, in order to overcome the above limitations, in this work we propose an improved timing analysis for RRA-based NoCs. A. Contribution This paper presents the following contributions. • An RC-based time analysis targeting RRA-based NoCs is presented. Compared to the related solutions, the proposed analysis involves less pessimism and requires relatively low processing time. • A number of extensive experiments have been generated to evaluate the performance of the above solution, from where we can clearly observe the improvement of the proposed analysis. B. Related Work Several performance analysis approaches for NoCs have been proposed in the literature aiming to guide system design and to optimize system performance, such as queueing theory 978-1-5090-1558-0/17/$31.00 ©2017 IEEE 275 based methods (e.g. [11][12]) and machine learning based approaches (e.g. [17]). However, these solutions mostly focus on the analysis of average latency, thus they cannot be directly applied on applications with hard real-time requirements where the temporal behavior in the worst-case scenario is of the most importance. Targeting hard real-time applications, the authors in [20] have presented a response time analysis for NoC messages which is based on the well-known timing analysis [9] for task scheduling on single-core processors. Recently, the authors in [10] propose a stage-level latency analysis, which can provide more accurate estimates compared to the analysis presented in [20]. However, all these analyses target NoCs using a ﬁxedpriority based scheduling, which requires implementation of sufﬁcient virtual-channels as well as logic for priority-based arbitration. Unfortunately, to the best of our knowledge, many COTS (commercial-off-the-shelf) many-core processors do not support the above design. On the other hand, most of the existing COTS many-core processors simply use a round-robin based arbitration (e.g. Tilera TILE64 [22], Epiphany E64G401 [1]). Targeting this type of design, several Network Calculus (NC) [13] based analyses have been proposed (e.g. [16][5]). However, due to the fact that a packet may span over a number of routers at the same time in a wormhole-switched NoC, using a NC based approach becomes complicated and involves signiﬁcant pessimism for some cases [6]. The authors in [18] present an analysis approach based on Compositional Performance Analysis (CPA) [7] targeting NoCs with 2-stage iSLIP arbitration which can be used to achieve a similar analysis purpose. However, this approach does not consider the back-pressure effect. In [4], the authors present the RC method to compute end-to-end delay of a packet over RRA-based wormholeswitched networks. Unfortunately, this approach does not take minimum inter-arrival times between packets from the same ﬂow into account, which results in pessimistic estimates especially when the network utilization is low. Thus, in [2], the authors propose an improved analysis based on the work in [4] called the Branch and Prune (BP) algorithm. In this analysis, the minimum inter-arrival times between packets are taken into account, thus the pessimism involved in the results is signiﬁcantly reduced. Unfortunately, the whole BP algorithm is based on an exhaustive search, whose complexity increases exponentially as the number of ﬂows in the network goes up. Therefore, a second version of the BP algorithm, called Branch, Prune and Collapse (BPC), is presented in [2] to reduce the processing time. In BPC, a threshold for the number of examined scenarios is given as an input from users, which is used to limit the amount of computation (more details will be explained in Section III). Consequently, in BPC, the accuracy of the analysis highly relies on the given threshold. However, the threshold is not straightforwardly tunable for users to achieve any speciﬁcally expected accuracy, because the relation between the selected threshold and the involved pessimism is uncertain. The analysis proposed in this work is also an RC based approach, however, we solve the over3C-3 d e s Input buffer Input W E N S L n Output  Link t i i r t a b o n a b o R b d n A u o R r i Buffer Control Fig. 1. The abstracted architecture of an example NoC. pessimism problem in a different way compared to BP/BPC. The processing time of the proposed analysis is signiﬁcantly lower than that of BP/BPC, and the results will not be biased by any user input (e.g. the selected threshold in BPC). In this paper, we compare our proposed approach with the methods presented in [4][2], since these solutions fall into the same category (i.e. all are RC based). The remainder of the paper is organized as follows. In Section II, we present the system model considered in this paper. We recapitulate RC and BP/BPC in Section III. Section IV presents the details of the proposed timing analysis. In Section V, we show the evaluation results of the proposed solutions, and Section VI concludes the paper along with some ideas about future work. I I . SY S T EM MOD E L The system considered in this paper consists of m × m computing cores which are connected by a wormhole-switched NoC. Figure 1 shows an abstraction of the architecture. The NoC uses a 2D mesh-based topology and the XY-routing scheme which have been widely utilized in many research works (e.g. [20][15][8]) as well as commercial implementations (e.g. [22][1]). The access of an output link at a certain router is controlled by a round-robin arbitration. Under the RRA mechanism, only one packet is transmitted from each input buffer within one round-robin cycle. Each pair of adjacent routers are connected by a full-duplex link. The bandwidth of each physical link is denoted by bw. The size of a single ﬂit is σ. We use d f to represent the transmission time of a single ﬂit over one physical link (i.e. d f = σ/bw). The network contains a set of n periodic or sporadic ﬂows F = { f1 , fn}. A ﬂow f i is characterized by (Li , Ci , Ti , Ji , Di , Ri ). Each ﬂow includes an inﬁnite number of packets (also called instances hereinafter) with the same characteristics. Li is the packet size of f i (including the payload and the header). Ci denotes the basic transmission time of f i over one physical link (i.e. Ci = Li · d f ). Ti represents the period of a periodic ﬂow or the minimum inter-arrival time of a sporadic ﬂow. In other words, a new packet/instance of f i can be generated after every Ti . Due to the variation in the execution time of the source task of f i (i.e. the task that generates f i ), the arrival of f i may not strictly follow a periodic pattern. The maximum time deviation between the actual arrival time of a certain packet of f i and its expected arrival time according to its period is deﬁned as a release jitter denoted by Di (Di ≤ Ti ). A ﬂow is deﬁned as schedulable (denoted by Ji ). Each ﬂow has a constrained relative deadline f2 , ..., 276     ݂݀ + ݀(݂ହ , ܮ݅݊݇(ܣܤ)) ݀(݂ହ , ܨ݅ݎݏܮ݅݊݇) ݂݀ + ݀(݂ଶ , ܮ݅݊݇(ܤܥ)) ݂݀ + ݀(݂ଷ , ܮ݅݊݇(ܤܥ)) ݂݀ + ݀(݂ହ , ܮ݅݊݇(ܤܥ)) ݂݀ + ݀(݂ଷ , ܮܽݏݐܮ݅݊݇) ݂݀ + ݀(݂ସ , ܮ݅݊݇(ܥܦ)) ݂݀ + ݀(݂ଶ , ܮ݅݊݇(ܥܦ)) ݂݀ + ݀(݂ହ , ܮܽݏݐܮ݅݊݇) ݂݀ + ݀(݂ସ , ܮܽݏݐܮ݅݊݇) ݂݀ + ݀(݂ଶ , ܮܽݏݐܮ݅݊݇) ܮହ/ܾݓ ܮଷ/ܾݓ ܮସ/ܾݓ ܮଶ/ܾݓ max + + ݂݀ + ݀(݂ଵ , ܮ݅݊݇(ܣܤ)) ݀(݂ଵ , ܨ݅ݎݏܮ݅݊݇) ݂݀ + ݀(݂ଶ , ܮ݅݊݇(ܤܥ)) ݂݀ + ݀(݂ଷ , ܮ݅݊݇(ܤܥ)) ݂݀ + ݀(݂ଵ , ܮ݅݊݇(ܤܥ)) ݂݀ + ݀(݂ଷ , ܮܽݏݐܮ݅݊݇) ݂݀ + ݀(݂ସ , ܮ݅݊݇(ܥܦ)) ݂݀ + ݀(݂ଶ , ܮ݅݊݇(ܥܦ)) ݂݀ + ݀(݂ଵ , ܮܽݏݐܮ݅݊݇) ݂݀ + ݀(݂ସ , ܮܽݏݐܮ݅݊݇) ݂݀ + ݀(݂ଶ , ܮܽݏݐܮ݅݊݇) ܮଵ/ܾݓ ܮଷ/ܾݓ ܮସ/ܾݓ ܮଶ/ܾݓ max + + ܹଵ + Fig. 2. The computation of W1 in Example 1. if its worst-case traversal delay (denoted by Wi ) is no larger than its deadline (i.e. Wi ≤ Di ). The whole network is deﬁned as schedulable if all the ﬂows in the network can meet their deadlines. Moreover, each ﬂow has a ﬁxed path/route Ri . I I I . R ECA P I TU LAT ION O F RC AND BP /BPC First, we recapitulate the original Recursive Calculus [4], and the BP/BPC approach [2]. Assume that f i is the ﬂow under analysis. The main idea of the analysis is to compute the maximum delay on each link along Ri from its source node to the destination. We use Li = {l 1 } to represent all the links within Ri , where l 1 i and l ni i are the ﬁrst and the last links respectively (i.e. Ri contains ni links in total). The maximum delay of f i at a certain ∈ Li ) is represented by d ( f i , l k link l k i (l k ). Alg. 1 illustrates how to calculate d ( f i , l k ). i , l 2 i , ...,l ni i i i i Alg. 1 Recursive Calculus - Computing d ( f i , l k i ) 1: d ( f i , l k i ) ← 0 i and lx (cid:5)= l k−1 2: for all lx where lx is an upstream link next to l k cMaxBl ocking ← 0 for all f j coming from lx and going to l k d el ayFrom f j ← 0 /*l k is the last link of f j */ then d el ayFrom f j ← L j d el ayFrom f j ← d f + d ( f j , next Link( f j , l k cMaxBl ocking ← MAX (cMaxBl ocking, d el ayFrom f j ) )+ = cMaxBl ocking i do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: i do if l k i = l n j j i bw else i )) end if end for d ( f i , l k )+ = d f + d ( f i , l k+1 ) i 14: end for 15: d ( f i , l k i i ) 16: return d ( f i , l k i We use Sr(l k ) to represent the input router of l k i , and Ds(l k to denote the receiving router. While computing the maximum delay of f i on l k i , we need to consider the worst-case scenario ) (i.e. l k−1 that all the other input links of Sr(l k , where f i comes from, is excluded) have pending packets to transmit i (Alg. 1, line 2 − 14). d ( f i , l k on l k ) thus consists of the delays caused by the trafﬁc from these input links of Sr(l k ) and the transmission of f i itself over l k i . According to the RRA policy, each of these input links can transmit at most one packet ahead of f i . Therefore, for a certain input link lx (Ds(lx ) = Sr(l k and lx (cid:5)= l k−1 ), we need to ﬁnd the ﬂow coming from lx that can cause the largest blocking to f i (Alg. 1, line 3 − 14). i i ) i i i i i ) i Due to the fact that each NoC router typically has a quite limited buffer (i.e. that can hold only several ﬂits), a packet is commonly spanned over multiple routers. As a result, when the output link of Ds(l k ) (i.e. l k+1 ) is occupied by other ﬂows, even if f i already gets access to l k i , f i still cannot continue its transmission since the buffer at Ds(l k ) is full. This phenomenon is known as back-pressure. Therefore, the delay of any ﬂow f j over l k i also depends on its delay over the next link (Alg. 1, line 9). If l k is already the last link of f j , the delay can be directly acquired by computing the traversal i (Alg. 1, line 6 − 7). time over l k As presented above, the maximum delay of f i over l k i can be acquired by recursively computing the delays of a number of ﬂows over their downstream links starting from l k i . The WorstCase Traversal Time (WCTT) of f i can thus be obtained by computing the largest delay over its ﬁrst link (i.e. d ( f i , l 1 )). The complete calculation is achieved by traversing the whole computation tree (e.g. Figure 2). i i i i i A B C D ݂ଵ = {3, 50, A Æ C} ݂ଶ = {2, 40, B Æ D} A B C D ݂ଷ = {1, 20, B Æ C} ݂ସ = {5, 20, C Æ D} ݂ହ = {2, 20, A Æ C} ݂௜ = {ܥ௜ , ܶ௜ , ℜ௜ } Fig. 3. An example showing the computation using RC. Example 1. A NoC consists of ﬁve real-time ﬂows as shown in Figure 3. Assume that f1 is the ﬂow under analysis. To simplify the presentation, we set d f to 1. Figure 2 shows the recursive computation of W1 . If we apply Alg. 1, we will get the result of W1 = 33. The main drawback of RC is that it does not take periods (or minimum inter-arrival times) of ﬂows into account. In other words, the analysis is based on an approximation that within any time duration, each ﬂow always has inﬁnite pending instances. As shown in Example 1, one instance of f2 is considered during the computation of d ( f5 , Link(AB)). While computing d ( f1 , Link(AB)), a second instance of f2 is taken into account. Consequently, W1 contains a delay caused by two instances of f2 . However, within a time duration of W1 (i.e. 33), at most one instance of f2 can be released according to T2 (i.e. T2 = 40). Therefore, the worst-cases considered in RC can be pessimistic in reality. In order to reduce such type of pessimism, in [2], the authors propose an improved analysis BP/BPC based on RC where the minimum inter-arrival times between packets are 3C-3 277 taken into account. In the BP approach, the algorithm investigates all the possible interfering scenarios. An interfering scenario is basically a blocking trace where the ﬂow under Example 1, { f3 , f5 , f1} (i.e. f3 is transmitted before f5 , then f1 analysis is the last one to be transmitted. For example, in is transmitted after f5 ) is one of the interfering scenarios, and { f4 , f2 , f5 , f3 , f1} is also a possible scenario. In other words, the interfering scenarios refer to all the subtrees, that contain the ﬂow under analysis, from the original computation tree. While computing the transmission delay of each scenario, the algorithm computes and tracks the number of arrivals of each involved ﬂow based on their characteristics. Therefore, the calculated estimates are tighter than the ones given by RC. However, the BP algorithm does not scale well, because the number of investigated scenarios increases exponentially as the number of ﬂows goes up. Therefore, a revised version of the BP algorithm, called BPC, is presented in [2] to reduce the computation complexity. In BPC, a threshold for the number of examined scenarios is given as an input from users. Once the number of examined scenarios reaches the given threshold, a dummy scenario is created which is used to replace a set of original scenarios (i.e. a number of scenarios are collapsed into one single approximated scenario). The complexity of the algorithm can thus be controlled by the number of investigated scenarios. On the other hand, the accuracy of the BPC approach highly relies on the given threshold. A lower threshold can make the algorithm terminate faster but results in more pessimism, and a higher threshold can prolong the processing time but brings tighter estimates. However, the threshold is not easily tunable for users to achieve any expected accuracy, because the relation between the selected threshold and the involved pessimism is uncertain. IV. T IGH T ER R ECUR S IV E CA LCU LU S In this section, we propose another approach called the Tighter Recursive Calculus (TRC) to reduce pessimism involved in the original RC. Our solution takes minimum interarrival times of ﬂows into account to provide more accurate results. Compared to BP/BPC [2], TRC is more scalable due to the lower computation time and it is simpler to implement. TRC is presented in Alg. 2 and 3. The main idea of this analysis is to perform iterations on top of the recursive computation. The ﬁrst iteration is the same as the original RC, where we can get an upper bound of the traversal delay for each ﬂow (Alg. 2, line 1). First, we present how TRC acquires tighter estimates in the second iteration using those upper-bounds computed in the previous iteration (Alg. 2, line 4-14). Then we extend the calculation to multiple iterations. Assume that f i is the ﬂow under analysis, and preWi is the maximum traversal delay computed by the previous iteration (i.e. RC). Within the time duration of preWi , an upper-bound of the number of arrivals of a ﬂow fq (denoted by η( fq , f i )) can be computed by η( fq , f i ) = (cid:6) preWi + Jq Tq (cid:7) (1) 3C-3 Alg. 2 Tighter Recursive Calculus - Computing Wi 1: Wi ← RC( f i ) 2: preWi ← Wi 3: while True do for all fq in SI i do η( fq , f i ) ← (cid:6) preWi+Jq Tq (cid:7) 4: 5: 6: end for 7: Wi ← 0 for all f p from the same source node as f i do p ) /*using Alg. 3*/ Wi+ = d ( f p , l 1 end for 8: 9: 10: 11: Wi+ = d ( f i , l 1 12: 13: 14: 15: ) /*using Alg. 3*/ if Wi = preWi then break preWi ← Wi end if i 16: end while 17: Wi + = Ji 18: return Wi (cid:6) 33 40 Therefore, while computing Wi , for any ﬂow fq ( fq ∈ SI i , where SI i represents all the ﬂows which can be involved during the computation of Wi ), at most η( fq , f i ) instances need to be taken into account. For example, in Example 1, the calculation of W1 only needs to consider one instance of f2 (i.e. η( f2 , f1 ) = (cid:7) = 1). Note that η( fq , f i ) is also used to represent the remaining available arrivals of fq in Alg. 2 and 3. In other words, Eq. 1 computes an initial value of η( fq , f i ) based on the Wi computed in the previous iteration (Alg. 2, line 5). As the second iteration proceeds, the value of η( fq , f i ) will be decreased accordingly. The main procedure of the second recursive computation is similar to the original RC. However, while calculating the blocking of f i caused by f j (i.e. denoted as d el ayFrom f j in Alg. 3), TRC checks if there are available arrivals of f j (Alg. 3, line 7). If there is no available arrivals of f j (i.e. η( f j , f i ) = 0), f j cannot cause any more blocking to f i . The blocking delay of f i caused by f j is thus 0. On the other hand, if there is at least one available arrival of f j (i.e. η( f j , f i ) > 0), TRC performs the same computation as RC (Alg. 3, line 8-19). During the computation of d el ayFrom f j , TRC ﬁrst creates a temporary copy of the current available arrival counts for all the ﬂows in SI i (Alg. 3, line 6). This temporary copy is used to monitor the changes of arrival counts during the computation of the current branch. According to RRA, if multiple ﬂows, which can block f i at l k i , come from the same upstream link, only one of them can actually block f i . The analysis thus chooses the maximum blocking from each upstream link. Therefore, only if f j causes the largest blocking from a certain upstream link, the corresponding changes of the arrival counts of f j during the computation of d el ayFrom f j become effective (Alg. 3, line 16-18 and 21). When the transmission of f j is considered to cause blocking to f i , the available arrival count of f j will be decreased by 1 (Alg. 3, line 11). As the computation proceeds, the available arrival counts of involved ﬂows will be decreased gradually. By checking the arrival counts, a number of branches of the computation tree constructed by RC can be pruned. The 278 i Alg. 3 Tighter Recursive Calculus - Computing d ( f i , l k ) i i i do i do 2: d ( f i , l k 1: Input: η( fq , f i ), ∀ fq ∈ SI ) ← 0 3: for all lx where lx is an upstream link next to l k i and lx (cid:5)= l k−1 cMaxBl ocking ← 0 for all f j coming from lx and going to l k tm p η( fq , f i ) ← η( fq , f i ), ∀ fq ∈ SI if η( f j , f i ) > 0 /*there is at least one arrival*/ then d el ayFrom f j ← 0 / ∗ l k is t he l ast l ink o f d el ayFrom f j ← L j tm p η( f j , f i )− = 1 d el ayFrom f j ← d f + d ( f j , next Link( f j , l k cMaxBl ocking ← MAX (cMaxBl ocking, d el ayFrom f j ) if f j causes the cMaxBl ocking then rtm p η( fq , f i ) ← tm p η( fq , f i ), ∀ fq ∈ SI f j ∗ / then end if = l n j if l k else bw )) i j i i i i 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: end if end if end for η( fq , f i ) ← rtm p η( fq , f i ), ∀ fq ∈ SI )+ = cMaxBl ocking d ( f i , l k )+ = d f + d ( f i , l k+1 ) and η( fq , f i ), ∀ fq ∈ SI ) i i i i i 23: end for 24: d ( f i , l k 25: return d ( f i , l k i pessimism can thus be reduced. Since Wi computed by RC is a guaranteed upper-bound of the traversal delay for f i . The actual number of instances from any other ﬂow f j that can block f i thus cannot exceed the maximum arrival count η( f j , f i ) computed by Eq. 1. Therefore, if Wi computed by RC contains x (x > η( f j , f i )) occurrences of f j , TRC only takes η( f j , f i ) occurrences into account (i.e. x − η( f j , f i ) occurrences will be removed). Below, the selection of the η( f j , f i ) out of the x we prove that occurrences does not impact the total blocking that f j can cause to f i . First, we present four properties that can be directly inferred from the principle of the XY-routing. Property 1. The transmission route of f i (i.e. Ri ) can change direction at most once in a XY-routing based NoC. Property 2. Two ﬂows can meet at most once in a XY-routing based NoC. Property 3. A ﬂow can pass a certain router at most once in a XY-routing based NoC. Accordingly, a ﬂow can pass a certain column or a certain row at most once. Property 4. A ﬂow cannot transmit on multiple parallel columns or rows in a XY-routing based NoC. Theorem 1. Assume that f i and f j meet at router Sr(la ) and depart at router Ds(lb ). After f i and f j meet, f i meets a third ﬂow f p at Sr(lm ), and f j meets f p at Sr(ln ). Then lm and ln must be the same link (i.e. lm = ln ), and they must be upstream links of Ds(lb ). 3C-3 Upstream links of ܦݏ(݈௕ ) ܵݎ(݈௔ ) ݈௔ ܵݎ(݈௠ ) ݈௠ ܵݎ(݈௡ ) ݈௡ ܦݏ(݈௕ ) ݈௕ ݂௜ ݂௣ ݂௝ Downstream links  of ܦݏ(݈௕ ) Fig. 4. An example to illustrate Theorem 1. Proof. First of all, we know that f i and f j share all the links between Sr(la ) and Ds(lb ). If lm is one of those links, when f p meets f i , f p can also meet f j at the same node since the output link is shared by both f i and f j . In this case, lm and ln must be the same link which is one of the upstream links of Ds(lb ). Then we need to examine the case where lm and ln are downstream links of Ds(lb ) (i.e. f p meets f i and f j after f i and f j have departed). First, we prove lm = ln in this case by contradiction. Assume that lm (cid:5)= ln . Since f i and f j depart at Ds(lb ), at least one of them must change its direction at Ds(lb ). Without loss of generality, we assume that f j is the ﬂow that changes the direction. According to the principle of the XY-routing, f j can only turns to a vertical direction (i.e. to the North or to the South). According to Property 1, ln must be in the same direction and the same column as the vertical links in R j . This implies that ln and Ds(lb ) are in the same column. The proof is then given in two cases. Case 1: lm is in a horizontal direction. According to Property 1 and 4, lm must be in the same direction and the same row as the horizontal links in Ri . This implies that lm and Ds(lb ) are in the same row. Since f p passes both lm and ln , we can have the following two cases. Case 1.1: f p meets f i ﬁrst. f p meets f i after Ds(lb ) implies that when f p meets f i , f p has already passed Ds(lb ) and the column where Ds(lb ) locates. According to Property 3, f p cannot go back to the column where Ds(lb ) locates. Therefore, f p cannot meet f j at all in this case. Case 1.2: f p meets f j ﬁrst. Similar to Case 1.1, f p meets f j after Ds(lb ) means that when f p meets f j , f p has already passed Ds(lb ) and the row where Ds(lb ) locates. According to Property 3, f p cannot go back to the row where Ds(lb ) locates. Consequently, f p cannot meet f i at all. Based on the discussion of the above sub-cases, we can conclude that lm cannot be in a horizontal direction. Case 2: lm is in a vertical direction. According to Property 1 and 4, lm must be in the same direction and the same column as the vertical links in Ri . Property 4 also implies that lm and ln must be in the same column. This means that f i also changes its direction at Ds(lb ). However, f i and f j can only turn into opposite directions. Consequently, lm and ln are in the same column but different directions. According to Property 3 and 4, a ﬂow can never transmit in different directions within in the same column. Therefore, this case is not valid either. 279 By summarizing the above two cases, we can conclude that lm = ln . According to Property 2, since f i and f j meet before Ds(lb ), they cannot meet again after Ds(lb ). Therefore, lm (= ln ) must be an upstream link of Ds(lb ). Based on the RRA policy, a ﬂow f p ( f p ∈ SI i ) can directly cause blocking to f i at most once. Therefore, if f p has multiple occurrences involved in the computation of Wi , f p must also cause direct blocking to a third ﬂow f j ( f j ∈ SI i ) then indirectly block f i . For example, in Example 1, two instances of f2 are considered in the computation of W1 (see Figure 2), because f2 can directly block both f1 and f5 where f5 can also directly block f1 . According to Theorem 1, if f p can meet both f i and f j where f i can also meet f j , f p must meet f i and f j at the same router (e.g. in Example 1, f2 meets both f1 and f5 at Router B). Accordingly, the delay caused by any occurrence of f p during the computation of Wi can always be acquired by d ( f p , lx ) (computed using Alg. 3) where lx is the output-link of the router where f p meets f i and f j . any ﬂow f p ( f p ∈ SI Therefore, the selection of the remaining valid occurrences of i ) does not affect the total blocking that f p can cause to f i . For example, as shown in Figure 2, the child-branches of d ( f2 , Link(BC)) (underlined in Figure 2) are always the same, and the parent-branches of d ( f2 , Link(BC)) (i.e. d ( f1 , Link(AB)) and d ( f5 , Link(AB))) always have the same child-branches. Consequently, if we need to consider only one instance of f2 while computing W1 , no matter which d ( f2 , Link(BC)) we keep, the ﬁnal result remains the same. According to the above discussion, in the second iteration, TRC can reduce a certain amount of pessimism which is included in the ﬁrst iteration (i.e. the original RC). Now we prove that by iteratively apply the above recursive computation, we can still obtain a safe upper-bound of the traversal delay of each ﬂow. Theorem 2. The actual worst-case traversal delay of (denoted as W A i ) cannot exceed the Wi computed by Alg. 2. Proof. This theorem can be proven by induction. We use Wi (k) to represent the Wi computed by the kth iteration. We know that the ﬁrst iteration in TRC can always provide a guaranteed upper-bound of traversal delay (i.e. Wi (1)) for any ﬂow f i ( f i ∈ F ), since the computation is exactly the same as the original RC. Assume that xq (1) instances of fq are considered in the computation of Wi (1). Therefore, at most (cid:6) Wi (1)+Jq (cid:7) (i.e. Eq. 1) instances of fq (∀ fq ∈ SI i ) can actually cause blocking to f i due to separation caused by the minimum inter-arrival times (i.e. Tq ). Accordingly, removing the effects caused by the (cid:7)) instances of fq (i.e. which is what TRC does in the second iteration) can still provide guaranteed estimates. The Wi (2) computed by the second iteration is thus still safe (i.e. Wi (2) ≥ W A i ). Now assume that Wi (k) is a safe upper-bound of Wi (i.e. Wi (k) ≥ W A i ) calculated in the kth iteration, and xq (k) instances of fq (∀ fq ∈ SI actual number of instances of fq (∀ fq ∈ SI i ) are considered while computing Wi (k). The i ) is thus no more (cid:7). Thus, removing the effects caused by the extra (xq (1) − (cid:6) Wi (1)+Jq than (cid:6) Wi (k)+Jq Tq Tq 3C-3 (xq (k) − (cid:6) Wi (k)+Jq Tq (cid:7)) instances of fq (i.e. which is what TRC does in the (k + 1)th iteration) can still provide guaranteed estimates. Therefore, the Wi (k + 1) calculated by the k + 1 iteration is still guaranteed (i.e. Wi (k + 1) ≥ W A i ). This completes the proof. Now we prove the termination of iterative computation in TRC. Theorem 3. As the number of iterations increases in TRC, the computed Wi (∀ f i ∈ F ) can either decrease or remain the same. Proof. As discussed earlier, after the kth iteration, TRC computes an estimated Wi (k). According to the TRC algorithm, in the (k + 1)th iteration, the number of instances of a ﬂow fq (∀ fq ∈ SI i ) considered in the computation of Wi (k + 1) can either be reduced or remained unaltered. In other words, while computing Wi (k + 1), the number of involved instances of fq can never be increased compared to the calculation of Wi (k). If fq contributes to Wi (k) (i.e. fq causes the maximum blocking at a certain branch in the computation tree), reducing the instances of fq may result in a lower estimate (i.e. Wi (k + 1) ≤ Wi (k)) since the considered blocking can potentially be reduced. On the other hand, if fq does not contribute to the Wi (k), reducing the instances of fq can never achieve an estimate higher than Wi (k). Therefore, the computed Wi (k + 1) can either be lower than or equal to the estimate computed in the previous iteration (i.e. Wi (k + 1) ≤ Wi (k)). As presented in Alg. 2, if Wi (k) = Wi (k + 1), the algorithm terminates since the estimates computed in the following iterations will always remain unchanged. On the other hand, if TRC cannot reach a convergence of Wi , more iterations will be performed. According to Theorem 3, the computed estimates will keep decreasing as the number of iterations increases. According to Eq. 1, the number of instances of fq (∀ fq ∈ SI i ) considered in the computation (i.e. η( fq , f i )) will also be decreased. We know the fact that η( fq , f i ) has a lower bound of 1 (i.e. at least one instance can be considered). Once the computation reaches the scenario where the number of involved instances of all the ﬂows in SI i becomes 1 (i.e. i ), the algorithm will terminate since the estimates calculated in the following iteration will become stable. η( fq , f i ) = 1, ∀ fq ∈ SI V. EVALUAT ION We have generated a number of experiments to evaluate the proposed analysis TRC. First, we compare the performance of TRC with BP and BPC [2] using a case study as well as a number of general experiments. Then we generate a number of more extensive experiments to evaluate TRC comparing with the original RC. A. Comparison between TRC and BP/BPC In this section, we show the results of experiments comparing TRC and BP/BPC. f i Tq 280 a) Case Study: The case study used in this paper is based on an autonomous vehicle application, which has been used in [8][21]. The considered system contains a number of tasks which perform different functionalities including obstacle detection, navigation control, stability control and so on. The inter-task communications are achieved by 38 realtime ﬂows. The application uses a 4 × 4 2D mesh-based NoC. Packets are routed using the XY routing algorithm, and each router uses Round-Robin Arbitration. The frequency of the NoC is 100 MH z, and the link bandwidth is 3.2 Gbit /s. More details of the ﬂow parameters and the task mapping can be found in [19]. We apply RC, TRC, BP and BPC on the above application. In the following experiments, the thresholds of investigate scenarios of BPC are set to 200 and 1000 (denoted as BPC-200 and BPC-1000 respectively). TH E IM PROV EM EN T S OB S ERV ED FROM TH E CA S E S TUDY. TABLE I Flow ID 3, 8, 14 17, 38 25 26, 30 TRC BP 1.08% 1.08% 4.49% 4.49% 28.56% 28.56% 38.5% 38.5% BPC-1000 BPC-200 1.08% 1.08% 4.49% 4.49% 28.56% 28.56% 38.5% 38.5% Table I shows the results regarding the observed improvement2 of TRC, BP and BPC compared to the original RC. According to the results, improvements can be observed in 8 out of 38 ﬂows, where the maximum observed improvement is 38.5% (i.e. f26 and f30 ). BP and BPC provide the same results. This is mainly because the total number of scenarios is low (i.e. BP and BPC both investigate all the possible scenarios). Moreover, we can observe that TRC can give the same estimates as BP and BPC. On the other hand, for 30 out of 38 ﬂows, all the four approaches produce the same results (i.e. TRC, BP and BPC do not achieve any improvements for these ﬂows). This is mainly because the ﬂows are allocated sparsely in the network. As a result, while analyzing a ﬂow f i , only one instance of f j ( f j ∈ SI i ) is considered in RC. In this case, the period of f j does not affect the traversal delay of f i at all. Therefore, all the four methods give the same estimates. TH E PROC E S S ING T IM E ( IN second ) O F ANA LY Z ING TH E CA S E S TUDY. TABLE II RC 0.002 TRC 0.135 BP 1.69 BPC-1000 BPC-200 1.68 1.67 Table II shows the processing time of the evaluated approaches. As shown in the results, RC is faster than the other approaches since it ignores the periodicity of ﬂows. TRC, BP and BPC takes into account the periodicity to provide tighter estimates with a price of longer processing time. While achieving the same estimates, TRC is signiﬁcantly faster (i.e. more than 10 times faster) than BP and BPC. b) General Evaluation: In addition to the case study, we also generate a number of general evaluations where the ﬂow 2 In this section, the improvement of solution A compared to solution B is represented by VB −VA · 100%, where VA and VB are the estimates computed by solution A and B respectively. VB 3C-3 parameters are randomly selected. For each network setting, the result is acquired from 200 generated experiments. The considered network uses a 4 × 4 2D-meshed topology. The frequency of the NoC is 100 MH z, and the bandwidth is 3.2 Gbit /s. The period of each ﬂow is randomly3 selected from [5, 33] ms. The utilization of each ﬂow is randomly selected from [0.003, 0.01]. The source and destination of each ﬂow are randomly selected (but not assigned to the same node). Figure 7 shows the results regarding the improvement of TRC and BPC compared to RC. As shown in Figure 7, as the number of ﬂows increases, the maximum and average observed improvement goes up for both TRC and BPC. When the number of ﬂows is low (e.g. 10 and 15), the maximum improvement of TRC is the same as BPC-1000 and BPC200, and the average improvement of TRC is slightly lower than BPC-1000 but higher than BPC-200. When the number of ﬂows is small, the total number of scenarios is low. As a result, BPC-1000 has a higher chance to be able to investigate all the scenarios, thus it achieves the highest average improvement. However, BPC-200 has to include approximations for certain cases since the threshold is frequently exceeded. When the number of ﬂows becomes larger (e.g. 30 and 35), TRC obviously achieves greater maximum and average improvement than BPC-1000 and BPC-200. As the number of ﬂows increases, the number of scenarios goes up accordingly. For these experiments, the threshold is frequently exceeded for BPC-1000, the achieved improvement is thus decreased. As discussed earlier, the performance of BPC highly relies on the selected threshold. A higher threshold can achieve tighter estimates, but it results in longer processing time. On the other hand, TRC does not have the above limitation. Therefore, as the number of ﬂows goes up, TRC obviously gives tighter estimates compared to BPC. t n e m e v o r p m I 100% 80% 60% 40% 20% 0% TRC BPC-1000 BPC-200 t n e m e v o r p m I 10 15 20 25 Nr. Of Flows (a) Maximum Improvement 30 35 TRC BPC-1000 BPC-200 40% 30% 20% 10% 0% 10 15 20 25 Nr. Of Flows (b) Average Improvement 30 35 Fig. 7. Comparison between TRC and BPC regarding the improvement in the computed estimates. ݏ n i e m i T g n i s s e c o r P RC TRC BPC-1000 BPC-200 100 80 60 40 20 0 15 10 20 25 Nr. Of Flows (a) Max. processing time per flow set 30 35 ݏ n i e m i T g n i s s e c o r P RC TRC BPC-1000 BPC-200 25 20 15 10 5 0 15 10 20 25 Nr. Of Flows (b) Avg. processing time per flow set 30 35 Fig. 8. Comparison between RC, TRC and BPC regarding the processing time. 3 In our experiments, all the randomly generated parameters follow a uniform distribution within a speciﬁc range. 281                 100% 80% 60% 40% 20% 0% 20 40 60 80 100 Nr. Of Flows 120 140 160 90%~100% No Imrpovement 80%~90% 0%~10% 70%~80% 10%~20% 60%~70% 20%~30% 50%~60% 30%~40% 40%~50% 40%~50% 30%~40% 50%~60% 20%~30% 60%~70% 70%~80% 10%~20% 0%~10% 80%~90% No Imrpovement 90%~100% 3C-3 max mean 20 40 60 80 100 120 140 160 Nr. Of Flows 10 8 6 4 2 0 s n o i t a r e t I f o . r N s w o l f f o e g a t n e c r e P Fig. 5. Improvements achieved by TRC w.r.t. the number of ﬂows. Fig. 6. The number of iterations using TRC. Figure 8 presents the results regarding the processing time of RC, TRC and BPC. It shows that RC is always faster than the other approaches. TRC is slightly slower than RC, but signiﬁcantly faster than BPC. According to Figure 8, as the number of ﬂows goes up, the processing times of all the approaches increase, where the processing time of BPC grows obviously faster than the others. B. Comparison between TRC and RC In this section, we present more extensive evaluation of TRC comparing with the original RC. The considered network uses an 8 × 8 2D-mesh based topology. The ﬂow generation is the same as utilized in the previous section. The number of ﬂows is selected from 20 to 160 with a step of 20. Figure 5 shows the percentage of ﬂows for each improvement range. As shown in the results, when the network only contains 20 ﬂows, around 80% of the ﬂows do not show any improvements. As the number of ﬂows goes up, such a percentage decreases dramatically. When the number of ﬂows is 160, improvements can be observed in 96% of the ﬂows. Moreover, when the number of ﬂows is low, the observed improvement is also small. However, as the number of ﬂows increases, more and more ﬂows show higher improvements. For example, when the number of ﬂows is 20, the percentage of ﬂows, which can show improvement more than 50% (i.e. 50% − 100%), is only 1%. When the number of ﬂows becomes 60, such a percentage increases to 20.55%. When the number of ﬂows goes up to 100, this percentage becomes 49.4%. Figure 6 shows the number of iterations required by TRC in the above experiments. When the number of ﬂows increases from 20 to 160, the maximum number of required iterations goes up from 3 to 8, and the average number linearly increases from 1.15 to 3.1. As shown in Alg. 2 and 3, the time complexity of one iteration is close to the original RC. Based on the above results, we can conclude that the extra time cost of TRC is quite acceptable given the decreased pessimism. V I . CONC LU S ION AND FU TUR E WORK S In this paper, we present an improved timing analysis (TRC) to compute worst-case traversal delays of real-time packets over RRA-based wormhole-switched NoCs. According to the evaluation results, the proposed TRC can provide signiﬁcantly tighter estimates compared to the original Recursive Calculus especially when the network contains a large number of ﬂows. On the other hand, compared to BP/BPC, which can also provide tighter estimates, TRC requires much shorter processing time. As future work, we will take buffer size into account in order to provide further accurate estimates. Moreover, we would also like to perform general comparison between the RC based approaches and the CPA based methods.  [1] Adapteva Inc. Epiphany Architecture "
2017,Using segmentation to improve schedulability of RRA-based NoCs with mixed traffic.,"Network-on-Chip (NoC) is the interconnect of choice for many-core processors and system-on-chips in general. Most of the existing NoC designs focus on the performance with respect to average throughput, which makes them less applicable for real-time applications especially when applications have hard timing requirements on the worst-case scenarios. In this paper, we focus on a Round-Robin Arbitration (RRA) based wormhole-switched NoC which is a common architecture used in most of the existing implementations. We propose a novel segmentation algorithm targeting RRA-based NoCs in order to improve the schedulability of real-time traffic without modifying the hardware architecture. Additionally, we also address the problem of transmitting both real-time traffic and best-effort traffic in the same NoC. The proposed solutions aim to provide timing guarantees to real-time traffic and achieve low latency for best-effort traffic. According to the evaluation results, the proposed segmentation solution can significantly improve the schedulability of the whole network.","9A-4 Using Segmentation to Improve Schedulability of RRA-based NoCs with Mixed Trafﬁc Meng Liu, Matthias Becker, Moris Behnam, Thomas Nolte Email: {meng.liu, matthias.becker, moris.behnam, thomas.nolte}@mdh.se M ¨alardalen University, V ¨aster ˚as, Sweden Abstract—Network-on-Chip (NoC) is the interconnect of choice for many-core processors and system-on-chips in general. Most of the existing NoC designs focus on the performance with respect to average throughput, which makes them less applicable for realtime applications especially when applications have hard timing requirements on the worst-case scenarios. In this paper, we focus on a Round-Robin Arbitration (RRA) based wormhole-switched NoC which is a common architecture used in most of the existing implementations. We propose a novel segmentation algorithm targeting RRA-based NoCs in order to improve the schedulability of real-time trafﬁc without modifying the hardware architecture. Additionally, we also address the problem of transmitting both real-time trafﬁc and best-effort trafﬁc in the same NoC. The proposed solutions aim to provide timing guarantees to real-time trafﬁc and achieve low latency for best-effort trafﬁc. According to the evaluation results, the proposed segmentation solution can signiﬁcantly improve the schedulability of the whole network. I . IN TRODUC T ION The complexity of todays embedded applications is steadily increasing. Many industrial domains face the shift away from single-core platforms towards multi-and many-core platforms in order to fulﬁll the applications growing demand for computational power, while still satisfying the requirements on low power consumption. The interconnect of choice on such platforms is the Network-on-Chip (NoC) [3]. Typically a many-core platform is arranged into a number of nodes, where each node can have one or multiple cores as well as local memory. A node contains further a network interface which connects to a NoC router. These routers are in turn connected to each other and they thus comprise the NoC. Wormhole-switching is implemented in most of the existing NoC designs [11]. In contrast to store-andforward switching, wormhole-switching requires signiﬁcantly smaller buffers on each router. This is the case since a packet is transmitted in so called ﬂow control digits (ﬂits), the elementary unit of transmission on a NoC. A header ﬂit is transmitted ﬁrst through the network. As long as the next link on the path is free and the buffer on the next router can accommodate at least one ﬂit, the header continues its transmission. The remaining ﬂits follow in a pipelined manner. During this transmission, the ﬂits of one packet can span over multiple routers, hence the name wormhole-switching. In this work we target applications with mixed timing requirements. A subset of tasks and packets has strict timing requirements (called real-time trafﬁc), while others have no This work has been supported by the Knowledge Foundation (KKS) through the project PREMISE and DPAC. timing requirements at all (called best-effort trafﬁc). One industrial example for such a system is a Programmable Logic Controller (PLC) which is used to control processes in factory automation. Apart from the actual control algorithms which are time critical, many such devices allow a user to connect to the system via web-services in order to parameterize or observe the process variables. These parts do not face any timing requirements. However, since the NoC is shared among real-time and best-effort trafﬁc, the timing analysis, which is required to obtain the Worst-Case Traversal Time (WCTT) of real-time packets, must consider all packets [7]. This means that a system designer must specify parameters such as packet size and period for best-effort trafﬁc as well. Due to the system characteristics, prior knowledge of such information about best-effort trafﬁc might be difﬁcult or even not possible. Thus, a method to integrate both real-time and best-effort trafﬁc on the NoC while satisfying all the timing requirements is of importance for many applications. In order to provide real-time guarantees, many network/ﬂow control mechanisms have been proposed (e.g. [8][12][13][6]). However, most of these solutions require support from speciﬁc hardware designs. Consequently, for many of the existing implementations, those solutions cannot be directly applied. In this paper, we propose a novel segmentation algorithm for RRA-based NoCs, which aims to improve the schedulability of all the real-time trafﬁc in a NoC. The proposed algorithm can be used at software level such that no modiﬁcation of hardware is required. Additionally, we also present a solution to select proper segment sizes for best-effort trafﬁc, in order to provide low latency to these trafﬁc while all the real-time packets are guaranteed to fulﬁll their requirements. The contributions of this work are: • A novel packet segmentation algorithm is proposed. The algorithm aims to improve the schedulability of real-time trafﬁc without modifying the hardware design. • Based on the introduced concept of segmentation, we show how to select the maximum segment size for besteffort trafﬁc such that the timeliness of real-time trafﬁc is still guaranteed. • Extensive evaluations, using synthetic experiments as well as a case study of an autonomous vehicle application, have been performed. According to the evaluation results, applying the proposed segmentation approach can significantly improve the schedulability of the whole network. 978-1-5090-1558-0/17/$31.00 ©2017 IEEE 744 A. Related Work The predictability of timing behavior is important for realtime applications. Targeting such type of applications, a number of research works have been presented in the literature, such as time-triggered NoCs (e.g. [8][12][13]), the Back Suction ﬂow-control scheme [6], ﬂow regulation [5], and ﬁxed-priority based NoCs [14][15]. These solutions resolve the scheduling problem of real-time trafﬁc in NoCs from different aspects, however, they either require speciﬁc hardware support or runtime monitoring of trafﬁc. For example, a time-division multiplexing based solution typically requires routers that can support time-slot based transmission, and synchronization among all the nodes in the network has to be carefully considered. A ﬁxed-priority based NoC requires implementation of multiple virtual-channels in order to achieve preemptions between different priority levels, which can also result in high buffer cost. In this paper, we address the timeliness issue in NoCs from another perspective - packet segmentation. The proposed segmentation algorithm is an off-line solution, and it can be simply applied on most of the existing CommercialOff-The-Shelf (COTS) implementations without requiring any hardware modiﬁcation. The remainder of the paper is organized as follows. In Section II the system model is introduced. In Section III, we illustrate the segmentation approach for real-time trafﬁc in RRA-based NoCs. Then in Section IV, we present an extended segmentation approach, where best-effort trafﬁc is integrated in NoCs together with real-time trafﬁc. The evaluations are presented in Section V, and Section VI concludes the paper. I I . SY S T EM MOD E L In this paper, we focus on an m × m 2D-mesh based wormhole-switched NoC using XY-routing (as shown in Figure 1). Such type of design has been utilized in many existing NoC implementations (e.g. [2][17]). At each router, a RoundRobin Arbitration (RRA) mechanism is used to control the link access. Under such a mechanism, each input buffer can deliver at most one packet to the output link within one round-robin cycle. The network contains a set of real-time ﬂows (denoted as Srt ) and a set of best-effort ﬂows (denoted as Sbe ). A ﬂow is a series of packets with the same characteristics. Each real-time ﬂow f i is generated periodically or sporadically, and it can be characterized by f i = {Li , Ti , Di , Sri , Dsi }. Li represents the size of a complete packet of f i . Ti is the period of a periodic ﬂow or the minimum inter-arrival time of a sporadic ﬂow. Each real-time ﬂow has a relative deadline Di . A ﬂow is deﬁned as schedulable if its WCTT (denoted as Wi ) is no larger than its deadline (i.e. Wi ≤ Di ). The network is deﬁned as schedulable if all the real-time ﬂows meet their deadlines. Moreover, each ﬂow has a ﬁxed path/route which starts from its source node Sri and ends at its destination node Dsi . On the other hand, a best-effort ﬂow does not have any speciﬁc characteristics regarding packet size, period or deadline. However, in order to provide timing guarantees to real-time trafﬁc, a certain level of control on the best-effort trafﬁc is mandatory. In this work, we ﬁx the route of each best-effort trafﬁc, and we identify the maximum segment size of each ﬂow under speciﬁc mappings. 9A-4 Input buffer Input W E N S L d e s i r t a b o n a b o R b d n A u o R r i n Output  Link t i Buffer Control Fig. 1. The abstracted architecture of an example NoC. As long as the utilized segment size of each best-effort ﬂow does not exceed the identiﬁed bound, the real-time trafﬁc is guaranteed to be schedulable. At each router, the transmissions of real-time trafﬁc and best-effort trafﬁc are treated in the same manner (i.e. there is no need to distinguish different types of trafﬁc on each router). Alternatively, all the control is implemented at the software level on each computing core. One solution is to add a middleware (an intermediate software program) between software partitions and network interfaces on each core. The middleware divides each packet into a number of sub-packets based on selected segment sizes. It also guarantees that realtime trafﬁc is always prioritised over best-effort trafﬁc on a computing core. I I I . S EGM EN TAT ION FOR NOC S W I TH R EA L -T IM E TRA FFIC First, we focus on RRA-based NoCs containing only realtime trafﬁc. A motivating example is shown in Figure 2, where the NoC contains 5 real-time ﬂows. Now we consider f1 as the ﬂow under analysis1 . We notice that, f1 can get direct blocking from f2 , f3 and f5 because of shared links, and indirect blocking from f4 because of back-pressure on f2 . Therefore, the transmission of all the other 4 ﬂows can affect the WCTT of f1 (i.e. W1 ). Assume that f1 misses its deadline (i.e. W1 > D1 ). In order to make f1 meet its deadline, W1 , which consists of the basic transmission time (i.e. the transmission time without any blocking) and blocking delay from other ﬂows, has to be reduced. For most applications, the basic transmission time of a ﬂow is difﬁcult to decrease, since it requires reduced data payload. Therefore, reducing the blocking caused by other ﬂows is more practical. In this section, we propose a packet segmentation based solution to reduce the blocking that a ﬂow may experience during its transmission, such that its worst-case traversal delay can be decreased accordingly. A B C D A B ݂௜ (ܵݎ௜ Æܦݏ௜ ) ݂ଵ (A Æ C) ݂ଶ (B Æ D) C D ݂ଷ (B Æ C) ݂ସ (C Æ D) ݂ହ (A Æ C) Fig. 2. An example of ﬁve real-time ﬂows in a NoC. 1Note, that the solution presented in this paper is independent of the utilized timing analysis. Typical timing analysis targeting RRA-based NoCs (e.g. [7][4][1][10]) are valid for this work. 745     9A-4 1: Input: f i Alg. 2 Segmentation to make f i meet its deadline (cid:4)= ∅ do i do 2: while SIA i j LS while T rue do for all f j in SIA k ← L j pre k ← k I ncrease(k) LS j if LS ← L j k ← L j pre k < σ then j j LS break val id ← T rue anal yze fm using t he current LS val id ← Fal se break end if for all fm in Θ j ∪ { f j } do if Wm > Dm then j end if end for if val id = Fal se then LS break ← L j pre k j anal yze f i using t he current LS j ret urn SCH E DU LABLE end if if Wi ≤ Di then end if end while end for u pd at e SIA i 32: end while 33: ret urn U N SCH E DU LABLE 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: Assume that f i is the ﬂow under analysis, and that k occurrences of f j are involved in Wi . An RRA-based NoC typically uses a non-preemptive policy. The length of each occurrence of blocking from f j then depends on the size of a complete packet of f j . In other words, the packet size of f j can directly affect Wi . Under the RRA policy, without changing the period of f j , the number of occurrences of f j considered in Wi is ﬁxed. Therefore, if we can decrease the length of each occurrence of blocking from f j , Wi can be reduced accordingly. To achieve this, we propose to apply packet segmentation on f j . Packet segmentation means dividing a packet into a number of sub-packets. After the segmentation, f j may cause less blocking to f i because the length of each blocking occurrence is reduced. However, since each subpacket has to wait for one round-robin cycle at a router, f j itself may thus experience more blocking which can result in a larger W j . Thus, a smaller sub-packet of f j can provide shorter Wi by inducing less blocking, which on the other hand can result in a larger W j by containing a higher number of subpackets. Therefore, the selection of sub-packet sizes needs to be considered carefully. Alg. 1 Segmentation for real-time ﬂows 1: Input: Srt 2: U nsched ul abl eF l ows ← ∅ 3: for all f i in Srt do 4: Wi ← Com put eWorstCaseLat ency( f i ) ad d f i int o U nsched ul abl eF l ows if Wi > Di then end if 8: end for 9: for all f j in U nsched ul abl eF l ows do result ← Al g. 2 ( f j ) if result = U N SCH E DU LABLE then ret urn U N SCH E DU LABLE end if 14: end for 15: ret urn SCH E DU LABLE 5: 6: 7: 10: 11: 12: 13: The complete segmentation approach is presented in Alg. 1. First, the algorithm checks the schedulability of the ﬂow set without applying the segmentation process (Alg. 1, line 3-8). Note that, to compute a packets WCTT, the applied schedulability test must consider that a packet may be transmitted in multiple segments. If all the ﬂows can meet their deadlines, the algorithm can directly terminate since the timing requirement is already fulﬁlled while each packet keeps its original payload size. On the other hand, if the timing analysis shows that certain ﬂows cannot meet their deadlines, the algorithm starts to apply a segmentation process in order to save these ﬂows from missing their deadlines (Alg. 1, line 9-14). The segmentation process is presented in Alg. 2, where f i is the ﬂow which may miss its deadline without segmentations. In order to reduce the blocking caused to f i , we only need to consider the ﬂows in SI i (representing all the ﬂows which have potential to affect Wi ). Furthermore, not all the ﬂows in SI actually contribute to Wi . For example, in Figure 2, both f2 and f3 are included in SI 1 since both of them can cause blocking to f1 . However, f2 and f3 come from the same source node, therefore, they cannot cause blocking to f1 at the same time i i i due to the RRA policy. An analysis thus only takes the ﬂow which causes the maximum blocking into account. We use SIA , which is a subset of SI i , to denote the set of ﬂows which can actually contribute to the current Wi . Accordingly, in order to reduce the blocking involved in Wi , we only need to apply the segmentation process on the ﬂows in SIA While segmenting a ﬂow f j ( f j ∈ SIA (Alg. 2, line 3 - 30). i ), we increase the number of segments gradually, and the size of one segment decreases accordingly (Alg. 2, line 7 - 8). The increasing function can be a step function to keep it simple or a binary search to make it more efﬁcient. For each given segment size, we need to check if it is acceptable (Alg. 2, line 9 to 24). When the size of one segment2 (denoted by LS j ) becomes smaller than the size of a single ﬂit (denoted by σ), such a segment size is not acceptable since a ﬂit is already the minimum transmission unit in wormhole-switched NoCs (Alg. 2, line 9 - 11). As discussed earlier, increasing the number of segments of f j may increase the blocking caused to f j . If the scheduling policy used at the output-port of the source node is also RRA, we only need to recheck the schedulability of f j itself since other ﬂows cannot get increased blocking by the reduced LS j . 2Note that a new header is added to each segment so that a segment can be transmitted as normal packets. A header is typically one single ﬂit, which is much smaller than the data payload. To simplify the presentation, the size of the header ﬂit is included in LS j . 746 i i However, such a situation may not be practical for most cases. When a packet is generated by a source task, it is inserted into the buffer at the source node which typically uses a FIFO mechanism. The RRA policy, where the access of the outputlink switches between different packets within one local buffer, is difﬁcult to implement and causes extra overhead. In a more general case, the output-port at the source node simply uses a FIFO mechanism. Consequently, the real-time ﬂows which are generated from the same source node of f j (denoted by Θ j ) may also be affected by the segmentation of f j . This is because the extra blocking caused to f j can also delay the transmission of these ﬂows if they are pipelined behind f j at the source node. Therefore, given a new segment size of f j , we need to recheck the schedulability of both f j and all the ﬂows from the same source node of f j (Alg. 2, line 14 - 20). If the new segment size makes any of the above ﬂows miss its deadline, such a segment size is not acceptable. If the given segment size is not acceptable, the algorithm stops the segmentation process on f j and starts to investigate another ﬂow in SIA (Alg. 2, line 11, 23). On the other hand, if the given segment size is acceptable, we can use it to reanalyze f i . If f i becomes schedulable, the algorithm terminates with a success (Alg. 2, line 25 to 27). Otherwise, the algorithm continues by increasing the number of segmentations of f j or other ﬂows in SIA . Note that the ﬂows included in SIA i may vary during the segmentation process. For example, before the segmentation, f j causes the maximum blocking to f i at a certain router. After the segmentation, the blocking caused by f j decreases. be caused by another ﬂow fm ( fm ∈ SI Consequently, the maximum blocking at the same router can i ). In this case, in the following segmentation process, fm which is not considered in the initial SIA should also be taken into account. Therefore, the ﬂow set SIA i needs to be updated continuously (Alg. 2, line 31). While updating SIA , the ﬂows which cannot be segmented any more will be removed. When there is no ﬂow remaining in SIA , the algorithm terminates since no more ﬂows can be segmented. In this case, the algorithm returns a result of failure, which means that segmentation cannot save f i from missing its deadline (Alg. 2, line 33). As presented earlier, while segmenting a certain ﬂow, the schedulability of all the ﬂows which may be affected by the segmentation is examined. Only if all the affected ﬂows are schedulable, the segmentation becomes effective. Therefore, we can guarantee that if a ﬂow is schedulable in a NoC without segmentation, it remains schedulable after applying the segmentation policy; on the other hand, if a ﬂow misses its deadline in a framework without segmentation, it is potentially schedulable using the segmentation mechanism. In other words, the proposed segmentation algorithm achieves either better or equal but never worse performance compared to NoCs without segmentation. i i i IV. S EGM EN TAT ION FOR NOC S W I TH M IX ED TRA FFIC In this section, we investigate how to add best-effort trafﬁc into a NoC framework with segmentations. The target is to guarantee that real-time ﬂows remain schedulable while achieving relatively low latency of best-effort trafﬁc. 9A-4 In a general NoC design, real-time trafﬁc and best-effort trafﬁc are not transmitted in separated networks (i.e. each router treats real-time and best-effort trafﬁc in the same manner). In this case, the transmission of best-effort trafﬁc can deﬁnitely block the transmission of real-time trafﬁc under the RRA mechanism. Similar to the discussion in Section III, if a best-effort ﬂow f i causes blocking to a real-time ﬂow f j , a larger packet size of f i results in more blocking on f j . If we apply segmentation on best-effort packets, the size of each segment of f i is decreased accordingly, and the blocking on f j can thus be reduced. On the other hand, with more segments, f i itself may suffer from more blocking. Since there is no speciﬁc deadline for a best-effort ﬂow, we only need to reduce the latency of a best-effort packet as much as possible while guaranteeing the timeliness of all real-time ﬂows. Alg. 3 Segmentation considering best-effort ﬂows 1: Input: Srt , Sbe 2: Initialize: Li ← 1, ∀ f i ∈ Sbe 3: result ← Al g.1 (Srt ) 4: if result = U N SCH E DU LABLE then ret urn U N SCH E DU LABLE 5: 6: end if 7: while Sbe (cid:4)= ∅ do for all f i in Sbe do 8: 9: 10: 11: 12: 13: 14: 15: result ← Al g.1 (Srt ) I ncreaseSegment Size( f i ) if result ← U N SCH E DU LABLE then Rest oreSegment Size( f i ) remove f i f rom Sbe end if end for 16: end while 17: ret urn SCH E DU LABLE We propose a heuristic algorithm (i.e. Alg. 3) to select segment size of best-effort trafﬁc where the schedulability of real-time trafﬁc is still guaranteed. In the beginning, each best-effort packet is conﬁgured with a segment size of the minimum payload size (Alg. 3, line 2). The algorithm ﬁrst checks the schedulability of the real-time trafﬁc with the initial setting (Alg. 3, line 3). If Alg. 1 returns a result of U N SCH E DU LABLE , at least one of the real-time ﬂows may miss its deadline. As discussed earlier, increasing segment size of best-effort trafﬁc cannot decrease the blocking caused on real-time ﬂows. Consequently, further increasing segment size of best-effort trafﬁc cannot save the unschedulable real-time ﬂows. In this case, Alg. 3 can be terminated directly (Alg. 3, line 4-5). On the other hand, if the real-time trafﬁc is schedulable with the initial setting, Alg. 3 tempts to increase segment size of best-effort packets gradually (Alg. 3, line 9). Similar to Alg. 2, the increasing function can be a step function or a binary search solution. For each increased segment size, the algorithm rechecks the schedulability of the real-time trafﬁc in order to verify if the current change is acceptable or not (Alg. 3, line 10). If the updated segment size makes any realtime ﬂow miss its deadline, this change is considered as invalid and the corresponding segment size is restored to the previous value. This ﬂow is then removed from Sbe since its segment 747 9A-4 size cannot be increased any more (Alg. 3, line 11-13). On the other hand, if the updated segment size is valid, the algorithm continues increasing the segment size. The above process is repeated until all the best-effort ﬂows have been removed from Sbe (Alg. 3, line 7-17). In this case, no best-effort ﬂow can have a further increased segment size (i.e. each best-effort ﬂow is assigned with its maximum segment size). As long as the segment size of each best-effort ﬂow does not exceed the selected bound, the real-time ﬂows are always guaranteed to be schedulable. V. EVA LUAT ION We have generated a number of experiments to evaluate the proposed segmentation solutions. First, we show the evaluation results regarding applying segmentation on NoCs with only real-time trafﬁc, then we present the results regarding segmentation on NoCs with mixed trafﬁc. A. Evaluation of Segmentation on NoCs with Real-Time Trafﬁc In this section, we present the evaluation of the proposed segmentation algorithm for real-time packets (i.e. Alg. 1). The evaluation uses an 8 × 8 2D-meshed NoC with the XYrouting. The source and destination of each ﬂow are randomly selected3 . The overhead incurred by the segmentation policy (i.e. extra headers) have been taken into account. For each experiment, we create two frameworks with the same setting (including network architecture and ﬂow set), and the only difference between these frameworks is that one framework uses packet segmentation while the other one does not. Then we examine the schedulability of both frameworks. The timing analysis is achieved using the method proposed in [10]. The evaluation results are represented by the schedulability ratio4 achieved by the two frameworks with respect to the number of ﬂows in the network. Six groups of experiments have been generated. In the ﬁrst group, the packet size of each ﬂow is randomly5 generated from a range of [5, 25] ﬂits. The utilization of each ﬂow is randomly selected from [0.003, 0.1]. The number of ﬂows in the network are selected from 5 to 65 with a step of 5. For each setting, we generate 100 experiments. The results are shown in Figure 3. When the network only contains 5 ﬂows, the schedulability ratio achieved by the framework with the segmentation policy (marked by W it hSeg in the ﬁgure) is 100%, while the framework without segmentation (marked by N oSeg in the ﬁgure) achieves the same schedulability ratio. As the number of ﬂows goes up, the schedulability ratios of both frameworks decrease since the total utilization of the ﬂow set6 increases (e.g., as shown in Figure 3, as the number of ﬂows goes up from 5 to 65, the maximum observed total utilization increases from 0.73 to 5.67). However, the schedulability ratio of the framework without segmentation decreases obviously 3 The source and destination of each ﬂow are not set to the same node, otherwise communication through NoC is unnecessary. 4Given an experiment setting, the schedulability ratio is the percentage of schedulable ﬂow sets among all the generated sets. 5 In our experiments, all the randomly generated values are selected from the given range following a uniform distribution. 6 The total utilization of a ﬂow set is the summation of the utilizations of all the included ﬂows. faster compared to the other framework. When the number of ﬂows is 30, the schedulability ratio of the framework with segmentation is 81%, which is 29% higher than the ratio achieved by the framework without segmentation. When the number of ﬂows reaches 50, the schedulability ratio of the framework with segmentation decreases to 14%, while the ratio achieved by the other framework drops to 0. In the second group of experiments, we evaluate the frameworks with higher utilization. The generation of packet sizes remain the same as in the ﬁrst group. Now the utilization of each ﬂow is randomly selected from [0.01, 0.2]. The number of ﬂows increases from 5 to 50 with a granularity of 5. The results are presented in Figure 6. Similar to the observation from the ﬁrst group of experiments, as the number of ﬂows increases, the schedulability ratio achieved by the framework with segmentation declines obviously slower than the framework without segmentation. When the number of ﬂows goes up from 5 to 30, the schedulability ratio of the framework with segmentation decreases from 99% to 18%, while the schedulability ratio of the other framework drops from 96% to 1%. When the number of ﬂows is 35, the schedulability ratio of the framework without segmentation becomes 0, while the framework with segmentation still has a schedulability ratio of 6%. By comparing the ﬁrst two groups of experiments, we can observe that changing utilization of each real-time ﬂow does not really affect the beneﬁt (in the sense of improving schedulability) yielded by the segmentation approach. In the third group of experiments, we investigate NoCs with larger packet sizes. Now the packet size of each ﬂow is randomly generated from [5, 50] ﬂits, and the utilization of each ﬂow is randomly selected from [0.003, 0.1]. The number of ﬂows increases from 5 to 60 with a step of 5. As shown in Figure 4, similar to the previous results, as the total network utilization increases, the schedulability ratio achieved by the framework without segmentation decreases obviously faster than the framework with segmentation. When the number of ﬂows is 30, the schedulability ratio of the framework with segmentation is 66%, which is 35% higher than the ratio achieved by the framework without segmentation. When the number of ﬂows reaches 50, only 1% of the generated ﬂow sets are schedulable for the framework without segmentation, while the other framework still has a schedulability ratio of 10%. The schedulability ratio of the framework with segmentation becomes 0 when the number of ﬂows reaches 60. The fourth group of experiments utilize the same setting as used in the third group to generate packet sizes of realtime ﬂows, while the utilization of each ﬂow is randomly selected from [0.01, 0.2]. As shown in Figure 7, the maximum difference between the schedulability ratios achieved by these two frameworks is 38%. When the number of ﬂows reaches 30, the schedulability ratio achieved by the framework without segmentation becomes 0. However, for the other framework, schedulable ﬂow sets can be observed until the number of ﬂows reaches 45. Additionally, we also generate two groups of experiments with further increased packet sizes. In these two groups of experiments, the packet size of each ﬂow is randomly selected from [5, 100] ﬂits. The ﬂow utilization is generated in the same 748 35 Nr. Of Flows c S b a u d e h l i l i t y a R t i o 0 1 2 3 4 5 6 0% 20% 40% 60% 80% 100% 5 15 25 45 55 65 NoSeg WithSeg MaxU M x a . o T t a l U t i l i a z t i n o Fig. 3. NoSeg vs. WithSeg. Packet size from [5, 25] ﬂits. Flow utilization from [0.003, 0.1]. 25 35 Nr. Of Flows c S b a u d e h l i l i t y a R t i o 0 1 2 3 4 5 0% 20% 40% 60% 80% 100% 5 15 45 55 NoSeg WithSeg MaxU M x a . o T t a l U t i l i a z t i n o Fig. 4. NoSeg vs. WithSeg. Packet size from [5, 50] ﬂits. Flow utilization from [0.003, 0.1]. 35 Nr. Of Flows c S b a u d e h l i l i t y a R t i o 0 1 2 3 4 5 0% 20% 40% 60% 80% 100% 5 15 25 45 55 65 NoSeg WithSeg MaxU M x a . o T t a l U t i l i a z t i n o Fig. 5. NoSeg vs. WithSeg. Packet size from [5, 100] ﬂits. Flow utilization from [0.003, 0.1]. 25 Nr. Of Flows c S b a u d e h l i l i t y a R t i o 0 3 6 9 0% 20% 40% 60% 80% 100% 5 15 35 45 NoSeg WithSeg MaxU M x a . o T t a l U t i l i a z t i n o Fig. 6. NoSeg vs. WithSeg. Packet size from [5, 25] ﬂits. Flow utilization from [0.01, 0.2]. 25 Nr. Of Flows c S b a u d e h l i l i t y a R t i o 8 7 6 5 4 3 2 1 0 0% 20% 40% 60% 80% 100% 5 15 35 45 NoSeg WithSeg MaxU M x a . o T t a l U t i l i a z t i n o Fig. 7. NoSeg vs. WithSeg. Packet size from [5, 50] ﬂits. Flow utilization from [0.01, 0.2]. 25 Nr. Of Flows c S b a u d e h l i l i t y a R t i o 0 3 6 9 0% 20% 40% 60% 80% 100% 5 15 35 45 NoSeg WithSeg MaxU M x a . o T t a l U t i l i a z t i n o Fig. 8. NoSeg vs. WithSeg. Packet size from [5, 100] ﬂits. Flow utilization from [0.01, 0.2]. manner as used in the previous experiments. The results are given in Figure 5 and Figure 8, from which we can obtain very similar observations as acquired in the other experiments regarding the improvement accomplished by the segmentation approach. We notice that the maximum difference between the schedulability ratios achieved by these two frameworks is 48% (when the NoC contains 35 ﬂows) in the ﬁfth group (see Figure 5) and 46% (when the NoC contains 15 ﬂows) in the sixth group (see Figure 8). By comparing all six groups of experiments, we can observe that the improvement achieved by the segmentation approach increases as packet size goes up. For example, for the settings with ﬂow utilization from [0.003, 0.1] (i.e. Figure 3, Figure 4 and Figure 5), as the maximum packet size increases from 25 to 100 ﬂits, the average improvement raises from 11% to 20% and the maximum observed improvement increases from 29% to 48%. Generally, according to the above evaluation outcomes, we can conclude that using the proposed segmentation approach can signiﬁcantly improve the schedulability of NoCs with realtime trafﬁc. We can also observe that the improvement can always be clearly observed regardless of different packet sizes and ﬂow utilizations. Furthermore, the segmentation solution achieves more improvement when ﬂows have larger packets. B. Evaluation of Segmentation on NoCs with Mixed Trafﬁc In this section, we evaluate the proposed segmentation algorithm targeting NoCs with mixed trafﬁc. The evaluation includes a case study using an automotive application as well as a number of general experiments. 1) Case Study: First, we evaluate the proposed solution using a case study based on an autonomous vehicle application which has been utilized in [16][9]. The application includes a number of tasks performing different functionalities. 38 realtime ﬂows are utilized for the inter-task communication. We deploy the application on a many-core platform using a 4 × 4 2D-meshed NoC. The NoC uses the XY routing and the RRA mechanism. The task mapping and ﬂow characteristics are adapted from [9] with slight modiﬁcation. Besides the realtime ﬂows, we generate 8 best-effort ﬂows in the network. The maximum packet size of the best-effort ﬂows are set to 500 ﬂits. In the beginning, we examine the schedulability of the whole NoC without applying any segmentation (i.e. best-effort ﬂows are transmitted based on their maximum packet size). The result shows that 2 out of the 38 real-time ﬂows ( f11 and f27 ) may miss their deadlines. The worst-case transmission latencies of the best-effort ﬂows are shown in Table 1 (the left three columns). Then we apply the segmentation process on the best-effort ﬂows. After selecting proper segment sizes of best-effort ﬂows, the result shows that the given realtime ﬂow set becomes schedulable. Table 1 (the right two columns) shows the maximum segment sizes of the best-effort ﬂows selected by the algorithm, as well as the corresponding WCTTs of these ﬂows based on the chosen segment sizes. As discussed in Section III, when we decrease the segment size of a certain ﬂow f i , f i has a high chance to experience more blocking. This is why f42 and f45 get increased WCTTs. Since f46 and f45 are generated from the same source node, an increased delay of f45 makes the WCTT of f46 become larger as well. On the other hand, the ﬂows which may experience blocking from the segmented ﬂows can thus get reduced WCTTs. Generally, the algorithm sacriﬁces the throughput of best-effort ﬂows to save real-time ﬂows from missing their deadlines. Therefore, after applying the segmentation process, the average WCTT of the best-effort ﬂows increases from 0.619 ms to 0.625 ms (i.e. around 0.9% increase). 2) General Evaluation: Additionally, we have also generated a number of general experiments. Similar to Section V-A, these experiments focus on the comparison between a frame9A-4 749                         R E SU LT S R EGARD ING TH E WCTT O F B E S T- E FFORT FLOW S . TABLE I No Seg Flow ID SegSize WCTT (ﬂits) (ms) 500 0.507 500 1.024 500 0.522 500 1.582 500 0.076 500 0.076 500 0.594 500 0.573 39 40 41 42 43 44 45 46 With Seg SegSize WCTT ( ﬂits) (ms) 500 0.507 500 1.019 500 0.518 160 1.620 500 0.076 500 0.076 160 0.603 500 0.587 work employing our proposed segmentation solution (Alg. 3) and a framework without using segmentation. The comparison is represented by schedulability ratio as well. The experiments are deployed on an 8 × 8 2D-mesh based NoC, which uses the XY-routing and RRA mechanism. The number of ﬂows in each ﬂow set increases from 10 to 35 with a step of 5. Each ﬂow set contains 5 best-effort ﬂows, and all the other ﬂows are real-time trafﬁc. The packet size of each real-time ﬂow is randomly generated from [5, 25] ﬂits. The maximum packet size of each best-effort ﬂow is randomly generated from [5, 100] ﬂits. The source and destination nodes of each ﬂow are randomly selected. Two groups of experiments have been generated, where the utilization of each real-time ﬂow is randomly selected from [0.003, 0.1] and [0.01, 0.2] respectively. The results are presented in Figure 9. For the group with ﬂow utilization generated from [0.003, 0.1] (marked with dash lines), as the number of ﬂows increases from 10 to 35, the schedulability ratio achieved by the framework with segmentation decreases from 100% to 69%, while the schedulability ratio of the other framework drops from 99% to 47%. A similar observation can also be obtained from the second group where ﬂow utilization is selected from [0.01, 0.2] (marked with solid lines). 100% 80% 60% 40% 20% 0% NoSeg-[0.01,0.2] WithSeg-[0.01,0.2] NoSeg-[0.003,0.1] WithSeg-[0.003,0.1] o i t a R y t i l i l b a u d e h c S 10 15 20 25 30 35 Nr. Of Flows Fig. 9. NoSeg vs. WithSeg (with mixed trafﬁc). According to the above results, we can observe that using the proposed segmentation mechanism can signiﬁcantly improve the schedulability of real-time ﬂows compared to the NoC design where packet segmentation is not applied. On the other hand, the throughput of best-effort trafﬁc is partially sacriﬁced in order to provide guarantees to real-time trafﬁc. 9A-4 V I . CONC LU S ION AND FU TUR E WORK S In this paper, we introduce a segmentation-based approach in order to improve the schedulability of real-time trafﬁc in RRA-based NoCs. The proposed solution is also used to address the problem of transmitting both real-time trafﬁc and best-effort trafﬁc in the same NoC. The solution aims to provide low latency for best-effort trafﬁc while the schedulability of all the real-time trafﬁc is still guaranteed. According to the evaluation results, the proposed segmentation solution can signiﬁcantly improve the schedulability of the whole network. In this work, we assume that all the ﬂows are already mapped in the NoC. We can observe that the mapping of ﬂows deﬁnitely affect the WCTTs of ﬂows which further affects the schedulability of the whole network. In our future work, we would like to address the mapping problem in the context of NoCs with segmentation.  [1] L. Abdallah, M. Jan, J. Ermont, and C. Fraboul. Wormhole networks properties and their use for optimizing worst case delay analysis of many-cores. In 10th IEEE International Symposium on Industrial Embedded Systems (SIES), June 2015. [2] Adapteva Inc. Epiphany Architecture "
2017,A tool for synthesizing power-efficient and custom-tailored wavelength-routed optical rings.,"Out of all the optical network-on-chip topologies, the ring has been proved to be far superior to its competitors: the contention-free all-to-all communications offer the lowest latency possible, while its clean physical design with few crossings and ring resonators provides unmatchable power results. The ring implements simultaneous communications by using a communication matrix that sets a distinctive waveguide-wavelength pair for each of them. That communication matrix has a high impact on energy consumption, but so far there have been very few efforts towards optimizing and automating its design. As far as we know, we propose the best optical ring design algorithm, which produces rings with the lowest number of wavelengths and waveguides in the literature. The algorithm is completed with a layout-aware and fully automated laser power calculation framework to help the user choose the most power-efficient design point.","4A-1 A Tool for Synthesizing Power-Efﬁcient and Custom-Tailored Wavelength-Routed Optical Rings Marta Ort´ın-Ob ´on University of Zaragoza Luca Ramini University of Ferrara V´ıctor Vi ˜nals-Y ´ufera University of Zaragoza Davide Bertozzi University of Ferrara Abstract—Out of all the optical network-on-chip topologies, the ring has been proved to be far superior to its competitors: the contention-free all-to-all communications offer the lowest latency possible, while its clean physical design with few crossings and ring resonators provides unmatchable power results. The ring implements simultaneous communications by using a communication matrix that sets a distinctive waveguide-wavelength pair for each of them. That communication matrix has a high impact on energy consumption, but so far there have been very few efforts towards optimizing and automating its design. As far as we know, we propose the best optical ring design algorithm, which produces rings with the lowest number of wavelengths and waveguides in the literature. The algorithm is completed with a layout-aware and fully automated laser power calculation framework to help the user choose the most power-efﬁcient design point. IN TRODUC T ION I . The recent remarkable advances in silicon photonics pave the way for the implementation of optical networks-on-chip (ONoCs) as an enabling technology for the integration of hundreds of cores in the same silicon die [1]. Compared with their electronic counterparts, they offer higher bandwidth, lower latencies, and reduced energy consumption [2]. The inherent characteristics of optics have forced researchers to come up with new network designs radically different from the electronic ones, which can be classiﬁed in two groups: spacerouted and wavelength-routed. Space-routed optical networks must set up a path in the optical network before they can start transmitting data, which introduces an unpredictable delay [3]. In those networks, all wavelengths are routed together to increase the bit parallelism of communications. In this work, we focus on wavelength-routed networks, which rely on wavelength-division multiplexing: wavelengths are not used to increase bit parallelism, but to provide contention-free all-to-all communications [4], [5]. This is a popular approach adopted in many optical network designs [6]–[9] and is well suited to deliver predictable communication performance in smallto-medium networks [10], or to deliver control trafﬁc in large networks. Among all the wavelength-routed topologies, the optical ring stands out as the preferred alternative, offering low latency communications along with reduced energy consumption [11]. Grani et al. present a comprehensive design-space exploration for optical rings and demonstrate their outstanding power consumption and performance results over electronic topologies [12]. A distinctive feature of optical rings is their ﬂexible implementation, which opens up opportunities for their optimization and customization for speciﬁc interconnect requirements. In practice, by varying the number of wavelengths and the number of waveguides, the ring design can be tuned to produce inﬁnite variations with very different power consumption values. To the best of our knowledge, the literature on the design space exploration of optical rings and on its automation is extremely poor. Grani et al. brieﬂy explore the trade-off between the number of waveguides and wavelengths and conclude that it is a topic worth studying [12]. LeBeux et al. propose the ﬁrst algorithm to generate ring communication matrices [13]. Their main goal is to come up with a design methodology capable of materializing ring designs that meet the connectivity requirements of the system at hand. However, they do not report a power modeling framework, hence missing a fundamental quality metric to assess the efﬁciency of their designs. This paper shares the same goal of automatically instantiating optical rings with the minimum allocation of resources; however, power optimization is the primary concern during the synthesis process. In practice, our tool searches for the right balance between number of waveguides and number of wavelengths based on their implications on total network power, and builds upon a power modeling framework with layout accuracy. As a result, this paper presents a tool that automates optical ring generation driven by power efﬁciency requirements. As an additional beneﬁt, the tools is also capable of customizing optical rings for the communication requirements of the system at hand, thus avoiding unnecessary overdesign. Automatically calculating the power consumption of the optical network becomes essential in order to prune the design space towards the most promising solutions. Chan et al. explore insertion loss, crosstalk, and power consumption for the TorusNX and Square Root topologies [14]. PhoenixSim is a computer system simulator than includes optical network power calculations [15]. These proposals study power consumption for several existing topologies, but do not use that information to improve the topology design or choose the best among several topology variations. With respect to previous work, we implement an automatic power consumption calculator for the ring that takes into account physical design constraints, such as core placement directives and the optical power losses in the power distribution network. In particular, we use the layout-aware power models not only for power analysis of generated ring designs, but also to drive the synthesis process toward the most power-efﬁcient ring conﬁgurations. I I . SY S T EM ARCH I T EC TUR E AND O P T ICA L N E TWORK S -ON -CH I P Our algorithm to generate the optical ring matrices and calculate the network power can be applied to communicate any number of elements in any tiled system architecture. Therefore, we do not impose any restrictions on the architecture, and the elements we communicate can be simple cores, large clusters, or other components. Besides, we can introduce as an input the communications that have to be implemented (connectivity matrix), enabling the use of the algorithm for more complex architectures such as the 3D design proposed in [13]. 978-1-5090-1558-0/17/$31.00 ©2017 IEEE 300 The optical ring is a wavelength-routed optical NoC, and relies on the principle of wavelength-selective routing. This means that every initiator can communicate with every target at the same time by using different wavelengths. The communication matrix (waveguide-wavelength pair to implement each communication) has to be chosen to ensure that wavelengths will never interfere with each other on the network optical paths. As long as we stay within the restrictions imposed by the technology and the place-and-route constraints, the number of waveguides and wavelengths of the design can be tuned in order to implement all the required communications with minimal power. Including more waveguides on the ring increases the number of crossings (as we will show in Section IV), which leads to an interesting trade-off: multiplexing a lot of wavelengths on a few waveguides means the insertion loss of the wavelengths will be lower, but we will need to add up the power for a lot of them; on the other hand, increasing the number of waveguides will allow us to reduce the amount of wavelengths, but each one of them will have higher insertion loss. Our framework generates the connectivity pattern ﬁnding the right balance in the number of waveguides and wavelengths to obtain the conﬁguration with lowest power. I I I . G EN ERAT ING TH E O P T ICA L R ING COMMUN ICAT ION MATR IC E S We propose an optimized algorithm to communicate any number of elements through an optical ring. We follow the basic design ideas already proposed in [13]: a wavelength can be used to implement several communications on the same waveguide, and we alternate clockwise and counterclockwise waveguides. Our objective is to minimize the number of waveguides and wavelengths needed to implement all the communications without contention. The mechanism we propose to build the ring communication matrix is detailed in Algorithm 1. As an input, the algorithm needs the number of waveguides of the ring, a maximum number of wavelengths, and the connectivity matrix. The latter allows us to indicate which speciﬁc nodes we want the ring to connect and, therefore, to use ring for any platform. The output consists of two communication matrices: one for the waveguides and one for the wavelengths that will have to be used for each communication. For a given number of waveguides, which may be determined by the place-androute constraints, the algorithm generates the ring design with minimal number of wavelengths. For each communication that needs to be implemented in the ring (loop in line 5 of the pseudocode), the algorithm ﬁrst tries to set the connection on the minimal path between the two nodes reusing a wavelength already present in the design (lines 6 to 16). If that is not possible because some of the required ring sections are not free in any waveguide with any of the existing wavelengths, a new wavelength will be added to set the communication (lines 18 to 25). If the maximum number of wavelengths had already been reached, then the algorithm will try to set the communication on the non-minimal path, going around the ring in the other direction (lines 27 to 36). If it is also not possible to do that, the algorithm will ﬁnish its execution unable to generate the ring design with the given input (lines 38 to 41). The complexity of the algorithm is polinomial: O(n3 ), n being the number 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: 40: 41: 301 4A-1 Algorithm 1 Generate optical ring communication matrices. 1: Input Data: num waveguides, max num wavelengths, connectivity matrix 2: Output Data: waveguide matrix, wavelength matrix 3: ring ← generate ring(num waveguides, 0 wavelengths) 4: used wavelengths ← 0 5: for communications f rom connectiv ity matrix COM do (cid:2) First try to reuse a wavelength to set the communication on the short path. success ← f alse for used wavelengths wl do wg ← ring .get f ree wg short path(COM , wl) if wg exists then store communication short path(COM , waveguide matrix, wavelength matrix, wl, wg) ring .store use short path(COM , wl, wg) success ← true break end if end for (cid:2) If it did not work, try adding a new wavelength if N OT success && used wavelengths < max num wavelengths then ring .add wavelength() used wavelengths + + store communication short path(COM , waveguide matrix, wavelength matrix, new wl, f irst wg) success ← true ring .store use short path(COM , new wl, f irst wg) end if (cid:2) If we could not add more wavelengths, try setting the communication on the long path for used wavelengths wl do wg ← ring .get f ree wg long path(COM , wl) if wg exists then store communication long path(COM , waveguide matrix, wavelength matrix, wl, wg) ring .store use long path(COM , wl, wg) success ← true break end if end for if N OT success then ERROR : U nable to generate ring break end if 42: end for of nodes, which guarantees that it will scale efﬁciently as we increase the number of nodes. The ﬁrst difference of the algorithm in [13] with respect to ours is that they ﬁx the number of wavelengths to use and utilize all of them in the same waveguide until it is not possible to set any of the remaining communications, at which point they add a new waveguide. The beneﬁt of this idea is that the number of wavelengths that can be multiplexed in the same waveguide is given by the technology, so it is a very reasonable input. The drawback is that using up all the sections of the ﬁrst waveguide before adding a new one forces the algorithm to use non-minimal paths for several communications that could have found a shorter path on a second waveguide. Having longer paths has a negative impact on power because it increases the number of crossings and the propagation loss, as we will explain in Section IV. To avoid this problem, we ﬁx the number of waveguides in our ring and reuse the same wavelength on all of them as much as we can before adding a new one, always trying to set communications on the shortest path ﬁrst. The inputs of the algorithm allow us to run it several times for a given number of waveguides to get different communication matrices by gradually reducing the maximum number of wavelengths. We can generate a ﬁrst design without a restriction on the number of wavelengths, which will have only minimal paths. After that, we can try reducing the maximum number of wavelengths to generate rings with fewer wavelengths but more non-minimal paths, which yields a very interesting power trade-off. A ﬁnal and very important detail not mentioned in [13] is the order in which communications are set. We explore two different options: Setting long-path communications ﬁrst. The objective of this choice is to have the shorter communications ﬁll the gaps left in the ring by the longer communications. Setting short-path communications ﬁrst. The interesting results obtained from these options will be discussed in the evaluation section. • • IV. CA LCU LAT ING LA S ER POW ER Total optical network static power comes from the laser power, the thermal tuning of microring resonators, and modulators and receivers. In this paper, we focus only on laser power as the key differentiator among the ring designs under test. In ring topologies, the number of nodes determines the number of modulators and receivers. For that reason, modulator and receiver static power and the power associated to the thermal tuning applied to microring resonators [7] are the same for every generated ring design, and are therefore not included in this paper. The second step of the ring design consists in calculating laser power taking into account layout constraints and physical-level parameters. As a novelty, we include the power of the laser distribution network, which brings the power from the laser source to all the nodes. The laser source generates the optical power, which then has to reach all the nodes and the waveguides in each node. To send the same wavelength to several paths at the same time we need to use splitters. To illustrate the complexity of the laser distribution scheme, we present an example in Figure 1 for a system with three nodes and three wavelengths, assuming the ring has one single waveguide. For each node, the power needed for each wavelength has been calculated from the insertion loss of the path that uses that wavelength and starts at that node. When designing how to distribute the laser to all the nodes, we need to set the appropriate splitting ratio so that the the required optical power reaches every node while minimizing the waste. Ideally, we would need selective splitters to apply the required splitting ratio to each individual wavelength in order to bring the exact power needed to every node. However, more pragmatic solutions do exist, which pose less stringent requirements on splitter technology: • Using a separate distribution waveguide for each wavelength would allow the splitters to have the ideal splitting ratio at every node for every wavelength, but 4A-1 • • it would involve a very large number of crossings at every node to inject the laser into all waveguides, as we will show in Figure 2. Using the same power for all the wavelengths at each optical network interface (ONI), corresponding to the worst-case power across all wavelengths in the ONI, would allow us to use the same ratio for all the splitters. In the example from Figure 1, we would need to bring 15 mW to all the wavelengths in ONI0, 20 mW for all the wavelengths in ONI1, and 30 mW for ONI2. The drawback of this method is the power waste. For example, wavelength 1 in ONI 0 would be getting 15 mW when it actually needs only 5 mW. Using a ﬁxed splitting ratio for all the splitters in the laser distribution network. With a 50% splitting ratio, the same power will be sent to both branches for each wavelength. For example, the splitter directly above ONI1 needs to send 7 mW of power to ONI1 and 2 mW to ONI2 for wavelength 1. Since the ratio is 50-50, it will send 7 mW to both branches. For wavelength 2 it will send 12 mW, and for wavelength 3, 30 mW. Similarly to the previous option, we would also be wasting power. Without lack of generality, we decide to choose the third option. From each node we will likely need to set paths of different lengths with very different power needs. Therefore, the second option that proposes to use the same power for all the communications starting at each node would be more wasteful. Besides, as we will show in Figure 2, the laser distribution network is implemented as a perfect binary tree, which will reduce the difference in the power that needs to be sent to each branch at every splitter. laser source laser distribution network ONI 0 1   5 mW 2   10 mW 3   15 mW ONI 1 1   7 mW 2   12 mW 3   20 mW ONI 2 1   2 mW 2   5 mW 3   30 mW Fig. 1: Example of the optical power needed at every ONI for each wavelength. Our algorithm takes into account details that have a direct impact on power, not only by favouring the use of minimal paths, but also by reducing the waste of power in the chosen laser distribution network. It sets up communications in pathlength order, and uses the same wavelength as much as possible before including a new one. As a result the same wavelength will be used to implement paths of similar lengths, thus minimizing the differences across nodes for every wavelength and reducing wasted power in the laser distribution network. These details of the algorithm that have a direct impact on power were not considered in [13]. The laser distribution network is implemented as a perfect binary tree (or as close as possible with the given number of nodes) to reduce the number of splitters, as shown in Figure 2, but it is an input that can be easily modiﬁed. Inside every ONI, the laser needs to reach all the waveguides, which generates crossings typically overlooked when designing optical ring topologies (Figure 2). When calculating the power we consider 302 4A-1 0 15 14 1 6 7 2 5 8 3 4 9 13 12 11 10 HUB laser laser source PD PD PD PD PD PD PD PD wg3 wg2 wg1 wg0 G H F A E B D C G H E F 1 3 DCB A  A 1 0 B 2 C 3 D E F G H  -  -  -  -  -  -  -   -  -  -  -  -  -  -   -  -  -  -  -  -  -   -  -  -  -  -  -  -   -  -  -  -  -  -  -   -  -  -  -  -  -  -   -  -  -  -  -  -  -   -  -  -  -  -  -  -  -0 2 3 0 3 2 0 1 3 4 Modulators (,) Couplers and lters (,)  PD Photodetector Crossing (a) Waveguide 0, clockwise Fig. 2: Laser power distribution tree in a 16-node ring (left) and detail of the distribution of the laser to all the waveguides inside an ONI (right). TABLE I: Physical level parameters. Chip size Crossing loss Propagation loss Bending loss Splitter loss Receiver sensitivity 16x16 mm 0.15 dB 0.15 dB/mm 0.005 dB 0.2 dB -20 dBm Modulator loss Coupler loss Filter drop loss Photodetector loss Coupler efﬁciency Laser efﬁciency 1 dB 1 dB 1 dB 1 dB 90% 8% G H F A E B D C E F 0 G H 3 1 2 DCB A  A B 3 C D 0 E F G H  -  -  -  -  -  -  -   -  -  -  -  -  -  -   -  -  -  -  -  -  -   -  -  -  -  -  -  -   -  -  -  -  -  -  -   -  -  -  -  -  -  -   -  -  -  -  -  -  -   -  -  -  -  -  -  -  0 4 0 3 3 1 3 2 1 2 physical level parameters that can also be introduced as an input. For this paper, we use the values shown in Table I. Total laser power is calculated with a recursive algorithm that traverses the laser distribution tree adding the power from every branch. In order to focus purely on the power required by the generated communication matrices without the effect of the chosen laser distribution network, we will also calculate the laser power with an ideal power distribution network. We will simply add up the required power for all the paths in the ring assuming the splitters do not impose any restrictions and the insertion loss of the laser distribution network is zero. V. EVALUAT ION This section compares our algorithm with the only other existing algorithm for ring design [13], shows the number of wavelengths and waveguides needed to communicate chips of different sizes, and analyses the power consumption. A. Detailed Example We start by applying our algorithm to a detailed example. To facilitate comparison, we have reproduced the 3D architecture with two layers and four ONIs per layer presented in [13], and built an optical ring using two waveguides like they do. Figure 3 represents the communications implemented with each wavelength on the two available waveguides (note that the proposed two-layer system does not need all-toall communications). Our algorithm manages to build all the communications with ﬁve wavelengths instead of the six needed in [13]. Also, the need of the algorithm from [13] to completely ﬁll the ﬁrst waveguide before adding the second forces the use of non-minimal paths on the ﬁrst waveguide for communications that could have been implemented with minimal paths on the second waveguide. We have also compared the power needed for the rings obtained with both algorithms, applying the power calculation described in Section IV to both of them. Our optical ring requires 91.2 mW, while the ring from [13], (b) Waveguide 1, counterclockwise Fig. 3: Wavelength assignment in two waveguides to connect 8 nodes distributed in two layers, reproducing the detailed example presented in [13]. with more wavelengths and longer paths, needs 117.3 mW, a 27% more. With the ideal laser distribution network, our design needs 15.2 mW, while the one in [13] needs 15.4 mW. The difference in this case is smaller because the penalization of the longer paths is not as relevant. Besides, as a side-effect of their algorithm, they use the outer waveguide (with crossings at the hubs) less frequently, thus reducing insertion loss. This unexpected beneﬁt would not have a big impact in larger systems with more waveguides. B. Exploration of the Number of Wavelengths and Waveguides We run our algorithm for rings of different sizes and get the minimum number of wavelengths needed to implement allto-all communications with different number of waveguides. Figure 4 shows our results, along with the available data from the algorithm designed by LeBeux et al. [13]. As expected, when increasing the number of nodes to communicate we need more waveguides and/or wavelengths to implement all the communications. Also, increasing the number of waveguides allows us to reduce the number of wavelengths because each one of them can be used for more communications. We note that, given the same number of waveguides, our algorithm is able to build the ring with fewer wavelengths than [13] in every case, and the differences become more prominent with larger systems. C. Laser Power Analysis Figure 5 shows the power for the different rings our algorithm generates to implement all-to-all communications for 16 nodes, both with the realistic and the ideal power distribution networks described in Section V-A. It includes the results obtained with the two options regarding the order in 303 70 60 50 40 30 20 10  0 2 w w w g 4 g 6 g 4 w w w g 6 g 8 1 1 0 3 g w w g g 1 1 2 2 3 0 6 0 6 3 w w w w w g g g g g 2 3 4 5 6 2 0 0 2 5 w w w w w g g g g g 2 3 4 6 6 6 8 0 w w w w g g g g 2 3 3 5 0 5 0 0 0 w w w g g g # w g n e e v a l t s h 256nodes 64nodes 36nodes 25nodes 16nodes 9nodes Fig. 4: Number of wavelengths needed to implement all-toall communications with optical rings with different number of waveguides (wg) in systems with increasing number of nodes. The red dots represent the number of wavelengths required to implement the same communications with the same number of waveguides by the only other existing ring design algorithm [13]. Please note that these numbers have been extracted from a graph and there may be small imprecisions.   10    9    8    7    6    5    4    3    2   40   10   15   25   20 # wavelengths   30   35 P o w e r ( W ) 2wg 3wg 4wg 5wg 6wg 7wg 8wg 8wg 9wg 10wg 10wg 2wg 3wg 4wg 5wg 6wg 7wg 9wg long first short first (a) Realistic power distribution network  170  175  180  185  190   10   15   25   20 # wavelengths   30   35   40 P o w e r ( m W ) 2wg 3wg 4wg 5wg 6wg 9wg 10wg 9wg 8wg 7wg 10wg 8wg 2wg 3wg 4wg 5wg 6wg 7wg long first short first (b) Ideal power distribution network Fig. 5: Laser power to implement all-to-all communications on a 16-node chip with varying number of wavelengths and waveguides. It also includes the two versions of the algorithm: setting long-path communications ﬁrst (blue) and setting shortpath communications ﬁrst (red). which communications are set: setting longer-path communications ﬁrst (blue) and setting shorter-path communications ﬁrst (red). We leave out of the graph the conﬁgurations with only one waveguide because they have extremely high power consumption and require a lot of wavelengths, since they all need to share the same waveguide and all communications have to follow the same clockwise direction. For a given number of waveguides, comparing the long and short-path ﬁrst versions we notice that the ﬁrst conﬁguration the algorithm can build, where there are only minimal paths, is always better with long paths ﬁrst: it has fewer wavelengths and consumes less power. This is because, as we already anticipated in Section III, if we leave the short paths for the end they will be able to ﬁll the gaps in the ring left by the longer paths. Focusing on the conﬁgurations built with the same number of waveguides with long paths ﬁrst, we notice that, as the algorithm manages to reduce the number of wavelengths, the power surprisingly increases. This is because the reduction in the number of wavelengths is achieved by introducing non-minimal paths, which increases insertion loss (due to propagation distance and number of crossings). However, when building short-paths ﬁrst with the realistic power distribution network, the power actually decreases as we build conﬁgurations with fewer wavelengths. The non-minimal paths to maximize wavelength reuse are set when a lot of the ring sections are already occupied, corresponding to the last communications implemented on the ring. With short paths ﬁrst, those last communications are the ones with longest paths. Therefore, the difference in length between the minimal and the non-minimal path is small and will not penalize insertion loss as much. For example, in a 16-node ring a minimal path of 7 hops corresponds to a non-minimal path of 9 hops, which is a small difference. In contrast, a minimal path of 1 hop corresponds to a non-minimal path of 15 hops. This power reduction is true with the realistic power distribution network, which heavily relies on the worst-case power for each wavelength. For the ideal network, all the paths are considered, so power increases with the number of non-minimal paths also in the short-paths ﬁrst version. We also notice that when the number of waveguides is even results are much better than when it is odd. That is because there are the same number of clockwise and counterclockwise waveguides, which helps build a more balanced ring with minimal paths only. We also notice that with an even number of waveguides a design that minimizes the number of wavelengths by implementing non-minimal paths can never be found. In contrast, this is common for conﬁgurations with an odd number of waveguides because the extra waveguide than unbalances the design provides additional room on that direction for communications to be built on the non-minimal path. With higher number of waveguides it is more uncommon to ﬁnd additional conﬁgurations with reduced number of wavelengths because the number of wavelengths is already small, and they are used across a higher number of waveguides. It is also worth mentioning that these trends do not continue when generating rings for larger platforms. With more nodes, it is normally not possible to generate ring conﬁgurations with non-minimal paths and fewer wavelengths than the ﬁrst conﬁguration obtained by the algorithm. This means that the best conﬁguration is always obtained by setting the communications of the longest paths ﬁrst. Finally, the best ring conﬁguration power-wise is the one with only two waveguides, both with the realistic and ideal power distribution networks. As we explained in Section IV, the laser power injected in the hub generates crossings in all the waveguides except the innermost one. Including more waveguides means that there will be more communications in the waveguides with crossings, therefore increasing the insertion loss of those paths. Also, in the realistic power distribution network, it involves including more splitters to bring the laser to all the waveguides. D. Customizable Ring Designs When communicating complex systems, we may need to build a ring for more complicated connectivity matrices instead of setting basic all-to-all communications. In this 4A-1 304       experiment, we start with a fully connected 16x16 system and randomly remove some of the required connections to test if our algorithm is still able to come up with efﬁcient designs. Figure 6 shows the power of the ring designs for 16 nodes with two waveguides using the realistic power distribution network, starting from full-connectivity and gradually decreasing the number of required paths. We can clearly see that the algorithm is very successful in optimizing power when reducing the number of needed connections, which makes it a suitable option for systems with speciﬁc connectivity requirements that need a customized design. ) W ( r e w o P  2.0  1.5  1.0  0.5  250  200  150 # communications  100  50 Fig. 6: Power for ring designs to connect 16 nodes with two waveguides using the realistic power distribution network, starting from full-connectivity and gradually decreasing the number of required paths. Paths are randomly removed and the average power of 20 runs has been calculated for each point of the graph. Note that removing one path removes the communication between the two nodes in both directions. E. Computation Time To analyse the execution time of the algorithm we run it on an Intel Xeon E5606 that runs at 2.13 GHz and has 8 GB of RAM. Table II presents execution time for our algorithm to generate rings and calculate power for systems of different sizes. We observed that for a given number of nodes, execution time does not signiﬁcantly change for different number of waveguides, thus we present results only for one case per chip size. We can se that the execution time increases with chipsize, but it is small even with the largest sizes. V I . CONC LU S ION We present a tool to generate ring communication matrices with minimum number of waveguides and wavelengths, while optimizing for power efﬁciency. We include automatic power calculations with physical constraint awareness including the contribution of the laser distribution network. Our algorithm is able to generate ring designs with fewer waveguides and/or wavelengths than any other existing proposal for any number of nodes. We demonstrate the importance of the order in which communications are set and show that an even number of waveguides allows for more balanced designs with reduced power. With a given number of waveguides, reducing the number of wavelengths does not necessarily mean saving power, because it enforces the use of non-minimal paths, which increases insertion loss. Both with ideal and realistic power distribution networks, we ﬁnd out that the best design point is the ring with only two waveguides, pointing out that adding extra wavelengths is more cost-effective than adding extra waveguides. There is a large margin between results with the ideal and realistic power distribution networks, indicating that power not only depends on an efﬁcient communication matrix, but also on a good laser distribution network design, which should be the focus of further optimizations. 4A-1 TABLE II: Execution time of the algorithm Conﬁguration 9nodes, 2wg 16nodes, 4wg 25nodes, 10wg 36nodes, 22wg 64nodes, 26wg 256nodes, 250wg Generate Ring (ms) 0.14 1.3 8.7 25.9 387.2 660.5 Calculate Power (ms) 0.06 0.3 0.5 0.79 4.0 135.5 ACKNOW L EDG EM EN T This work was supported in part by grants TIN201346957-C2-1-P, Consolider NoE TIN2014-52608-REDC (Spanish Gov.), and gaZ: T48 research group (Aragn Gov. and European ESF). [4] [8] "
2017,Hybrid analysis of SystemC models for fast and accurate parallel simulation.,"Parallel SystemC approaches expect a thread-safe and race-condition-free model from the designer or use a compiler which identifies the race conditions. However, they have strong limitations for real world examples. Two major obstacles remain: a) all the source code must be available and b) the entire design must be statically analyzable. In this paper, we propose a solution for a fast and fully accurate parallel SystemC simulation which overcomes these two obstacles a) and b). We propose a hybrid approach which includes both static and dynamic analysis of the design model. We also handle library calls in the compiler analysis where the source code of the library functions is not available. Our experiments demonstrate a 100% accurate execution and a speedup of 6.39× for a Network-on-Chip particle simulator.",
2018,A lifetime-aware mapping algorithm to extend MTTF of Networks-on-Chip.,"Fast aging of components has become one of the major concerns in Systems-on-Chip with further scaling of the submicron technology. This problem accelerates when combined with improper working conditions such as unbalanced components' utilization. Considering the mapping algorithms in the Networks-on-Chip domain, some routers/links might be frequently selected for mapping while others are underutilized. Consequently, the highly utilized components may age faster than others which results in disconnecting the related cores from the network. To address this issue, we propose a mapping algorithm, called lifetime-aware neighborhood allocation (LaNA), that takes the aging of components into account when mapping applications. The proposed method is able to balance the wearout of NoC components, and thus extending the service time of NoC. We model the lifetime as a resource consumed over time and accordingly define the lifetime budget metric. LaNA selects a suitable node for mapping which has the maximum lifetime budget. Experimental results show that the lifetime-aware mapping algorithm could improve the minimal MTTF of NoC around 72.2%, 58.3%, 46.6% and 48.2% as compared to NN, CoNA, WeNA and CASqA, respectively.","2C-4 A Lifetime-aware Mapping Algorithm to Extend MTTF of Networks-on-Chip Letian Huang∗ , Shuyu Chen∗ , Qiong Wu∗ , Masoumeh Ebrahimi† , Junshi Wang∗ , Shuyan Jiang∗ , Qiang Li∗ ∗University of Electronic Science and Technology of China, Chengdu, China, 611731 †Royal Institute of Technology (KTH), Stockholm, Sweden huanglt@uestc.edu.cn Abstract—Fast aging of components has become one of the major concerns in Systems-on-Chip with further scaling of the submicron technology. This problem accelerates when combined with improper working conditions such as unbalanced components’ utilization. Considering the mapping algorithms in the Networks-on-Chip domain, some routers/links might be frequently selected for mapping while others are underutilized. Consequently, the highly utilized components may age faster than others which results in disconnecting the related cores from the network. To address this issue, we propose a mapping algorithm, called lifetime-aware neighborhood allocation (LaNA), that takes the aging of components into account when mapping applications. The proposed method is able to balance the wearout of NoC components, and thus extending the service time of NoC. We model the lifetime as a resource consumed over time and accordingly deﬁne the lifetime budget metric. LaNA selects a suitable node for mapping which has the maximum lifetime budget. Experimental results show that the lifetimeaware mapping algorithm could improve the minimal MTTF of NoC around 72.2%, 58.3%, 46.6% and 48.2% as compared to NN, CoNA, WeNA and CASqA, respectively. Keywords—many-core system; Network-on-Chip; mapping algorithm; lifetime reliability I . IN TRODUC T ION Nowadays, Network-on-Chip (NoC) has been widely used in Multi-Processor System-on-Chip (MPSoC) but the aging issue of NoC, similar to other platforms, is emerging as a major research concern. Escalating device defects, shrinking feature-size and growing transistor density have negatively impacted the reliability which can be seen in the increase of the failure rate (both permanent and transient) [1]. Permanent faults reduce the system lifetime [2] and therefore, techniques are proposed to improve the systems lifetime in terms of mean time to failure (MTTF). Currently, there are mature methods and techniques to tolerate failures in cores due to aging. For instance, researchers have already proposed architectures that can gracefully tolerate up to a few hundred (>500) processorlogic permanent faults, in a 64-node CMP [3]. Even if a certain core is broken, it can be isolated because the core is an independent part, then the rest of MPSoC will continue to work. However, such well-protected components, cores, are connected to a less-protected infrastructure, NoC. Faults in a router can cause disabling a well-functioning core along with the router [4]. As another consequence, the connectivity of NoC will be devastated and the performance of MPSoC will be severely reduced. So, enhancing the lifetime of NoC has the same level of importance as cores in MPSoC. LaNA tries to manage NoC reliability at run-time through task mapping as a low overhead approach. Mapping algorithms try to allocate applications to the cores in an optimal way and aim to minimize the overall data latency and/or the power dissipation of the network. Mapping algorithms are mostly evaluated based on their deﬁned cost functions. An application is composed of a set of communication tasks. To map an application onto the NoC-based multicore system could be deﬁned as a one-to-one mapping function from a set of application tasks to a set of cores. Mapping algorithms can decide the allocation of an application to the cores based on the usage of cores and routers in the NoC-based multi-core system. The activity rate of one circuit is directly related to the aging of this circuit. Since the trafﬁc in NoC is unbalanced, the activity of each part of a router is quite different. So, the activity of the routers should be represented by the activity of different paths. As shown Fig. 1, a path between the crossbars of two routers is composed of a mux connected with output registor, an output registor, a switch allocator for controlling this mux and thisregister, the wires between two routers, an input buffer, a routing computation unit and a virtual channel allocator. For example, Fig. 1 shows the activated circuit as a result of delivering a packet from Router A to Router B. So, the wear condition of one path is positively correlated with the wear condition of the wires subordinate to this path. In this paper, we model and refer to the aging of wires which represents the aging of paths as well. Fig. 1. The activity of one router’s different parts are represented by the activity of links Most of the dynamic mapping algorithms do not consider NoC aging, so that some wires could be aged much faster than the others. Fig. 2(a) and (b) show a case study that 978-1-5090-0602-1/18/$31.00 ©2018 IEEE 147 presents the distribution of the NoCs’ MTTF under two mapping algorithms as CoNA [5] and LaNA. The case study is evaluated in 8 × 8 2D mesh NoC. The trafﬁc pattern is random and the injection rate is 0.05 ﬂits/cycle. For example in CoNA, the minimum MTTF is 0.2 while the maximum is 1 which means that the links (a group of wires) with minimum MTTF are aging 5 times faster than the links with maximum MTTF. The unbalanced MTTF distribution would become a bottleneck for system reliability. In our proposed LaNA mapping, the minimal MTTF and the average MTTF are enhanced, which indicates the positive impact of aging consideration in mapping algorithms. In 8×8 NoC, the normalized MTTF of links is evaluated under Fig. 2. different mapping algorithms This paper presents a method for wear-leveling in NoCs to balance the usage and extend the overall lifetime. The main contributions of this paper are as follows: (1) Studying the effect of different mapping algorithms on unbalanced wires utilization and analyze the factors that cause changes on the MTTF values. (2) Modeling the aging of NoC’s wires using the analyzed factors and then present a lifetime budget for the wires. (3) A lifetime-aware mapping algorithm based on the lifetime budget metric to enhance the NoC’s lifetime. I I . R E LAT ED WORK Due to the adverse impact of the deep submicron technology, the system reliability have received a lot of research attention over the past few decades. Task mapping and scheduling-based system-level design techniques can provide a low overhead approach for enhancing the reliability. The existing aging-aware task mapping techniques suffer from the following limitations: First, as the reliability of system highly depends on temperature, most prior works solely consider mapping tasks on an MPSoC platform with the objective of balancing the temperature of the cores [6] [7]. However, these methods neglect other factors of reliability such as switching activity and operating frequency. Second, previous works have completely ignored the role of routers in their reliability analysis, focusing only on the cores. A lifetime-aware task mapping technique based on ant colony optimization (ACO) is proposed in [8]. A wear-based heuristic method is proposed in [9] that is combined with runtime task mapping. These techniques improve the cores’ lifetime but they ignore about the aging of the NoC. When applications are mapped to the cores, data is communicated on 2C-4 the NoC platform. If the aging of NoC is not considered in the application mapping, the NoC components might be soon aged which gradually results in isolating the cores from the system. If NoC is not equipped with proper fault-tolerant techniques, then the whole system may crash. Even by assuming such techniques, disabling NoC components (and consequently the cores) not only can impact the system performance, but also the lifetime of other cores due to the increased load. A dynamic lifetime aware adaptive routing is proposed in [10]. This work introduced a metric for each router called lifetime budget. The adaptive routing aims to balance the lifetime distribution based on the lifetime budget metric. However, the capability of routing algorithms in enhancing NoC lifetime is small due to the limited path diversity and lack of global knowledge. I I I . AG ING EVALUAT ION A. The Simulation Framework To measure aging, we need to extract the temperature values under which the NoC is working. We set up a simulation framework for observing the temperature values on each component of a tile (i.e. core, router, cache, tags, and MC) to see the relations between the temperature values in different components. The simulation system used in this part is mainly composed of two mainstream open-source tools: McPAT [11] and Hotspot [12]. To understand the temperature distribution of NoC during mapping, we quantiﬁed the impact of the mapping process on the core temperature. We consider two different NoC ﬂoorplans similar to the ones in [13], illustrated in Fig. 3(a) and (d). we examine two workloads and depict the temperature of the tile components in a 3×3 mesh NoC. In this ﬁgure, the horizontal axis shows nine PEs with each PE including L2 cache data, core, router, L2 cache tags, and MC. The vertical axis represents temperature of each component and the ambient temperature is 45◦C. Fig. 3(b) and (e) show the temperature of different tile components when all PEs are active. As expected, in both scenarios, the temperature of the cores is around 67◦C which is the highest among all the other components. The router temperature is however around 47◦C on average. In Fig. 3(c) and (f), we turn off four PEs (i.e. the PE number 3, 5, 6, and 9) to observe the effect of PE’s activity on the routers’ temperature. As can be seen from these ﬁgures, turning off the PEs has a small effect on the the routers’ temperature. In other words, the router temperature mostly depends on its own activity. For this reason, in modeling the router aging, we only consider the router temperature which is nearly a constant value close to 47◦C. In the prior work [10], the entire tile temperature is assumed as the router’s temperature. However, our observation shows that this approach is not accurate because the router’s temperature is much lower than that of the core. This is because the distance of the core and router is relatively far in the NoC ﬂoorplan. Therefore in this paper, we use the ambient temperature as the router’s temperature when modeling the 148 2C-4 Fig. 3. The temperature of different tile components in a 3 × 3 mesh network with two different ﬂoorplans. aging of routers. We assume the temperature as a constant value of 47◦C. B. Aging Formulation Devices are aged for different reasons such as electromigration (EM), Negative Bias Temperature Instability (NBTI), and Time Dependent Dielectric Breakdown (TDDB) [14]. In this paper, EM is used as the wear-out related permanent faults when modeling the system lifetime. The aging rate is derived from [15]: (cid:3) (cid:2) exp( −Q kTt kTt ) r (t) = j (t) where Q is the activation energy (e.g. 1.5eV for copper), j (t) is current, and kTt is the temperature. As was already discussed, the temperature is assumed to be a constant value, (Ta : ambient temperature), and thus Tt can be replaced by Ta . In this equation, r(t) is a continuous value while under the discrete condition, the aging rate can be expressed by r(n) where n refers to the nth time interval. The current j (t) is proportional to links’ activity rate and can be measured by [2]: j (t) = C Vdd W H × f × p where C , W and H are the capacitance, width, and thickness of the wire, respectively. f is the clock frequency and Vdd is the working voltage and both are constant values. p is the switching activity that is proportional to the incoming ﬂit rate at the nth time interval which is the only stimuli to the wires. Using Equation 1 and Equation 2, MTTF under time-varying current density and temperature stresses can be calculated by [15]: T f = A E [r(n)] where A is a constant related to the system structure and E[·] is expectation. Based on these equations, the wires lifetime budget under the discrete monitored condition can be expressed by [10]: (cid:4) LB (n) = 0 LB (n − 1) + rn − r(n) if n is 0 otherwise (4) where LB (n), rn , and r(n) are the lifetime budget, the normal failure rate and the actual failure rate of wires, respectively at the nth time interval. We use rn to denote the lifetime consumption rate under the nominal condition and rn is the inverse of the expected MTTF, i.e. rn · T f = A. At runtime, we monitor the actual operating condition regularly, calculate the consumption rate r(n), and compare it with the normal failure rate rn . When r(n) < rn , the wires are consuming their lifetime budget slower than the nominal rate, and vice versa. Lifetime is modeled as a wire resource consumed over time and a lifetime budget of the wires indicates the maximum allowed workload in a given time. The wires with a small lifetime budget means faster aging. IV. LANA : L I F E T IM E -AWAR E N E IGHBORHOOD A L LOCAT ION The proposed mapping algorithm, named Lifetime-aware Neighborhood Allocation (LaNA), is composed of two novel contributions. The ﬁrst contribution is to exploit an efﬁcient mechanism for selecting the ﬁrst node to map. The second contribution is to select some available nodes around the ﬁrst node for mapping with the consideration of the lifetime metric. The details of these contributions are described in following subsections. (1) (2) (3) 149 2C-4 tion mapping algorithm is proposed which aims at ﬁnding an optimal placement of nodes for task mapping. The algorithm based on communication ﬂow and the lifetime budget of wires (PLB). The formulated problem is as follows. Let us ﬁrst assume a directed graph G = (V, A) where V represents the set of nodes and A refers to the set of wires. By assuming two nodes s, d ∈ V, then Fs,d denotes the communication ﬂow from the source node s to the destination node d according to XY routing. Between the node s and d, there might be several hops. The minimum value of these lifetime budgets will be used to present the lifetime budget of the ﬂow Fs,d . The lifetime budget of the ﬂow Fs,d is: {LBi } i={s,··· ,d} F LBs,d = min (6) where LBi is the lifetime budget of the wire and {s, · · ·, d} are the paths between the routers from node s to node d. With lifetime budget as the cost, the F LBs,d is to select the fastest aging wire as the aging cost of the ﬂow. There is usually a situation where a node has communication with several other nodes, thus the method evaluates the lifetime budget of a node by selecting the related minimal F LBs,d , that is the fastest aging ﬂow lifetime metric. F LBs,m and F LBm,d are the node m as a starting node and end node of ﬂow lifetime budget. The worst cost of nodes’ lifetime budget is represented as the following equation: P ELBm = min s,d {F LBs,m , F LBm,d } (7) A. First-Node Selection Strategy For minimizing the probability of congestion, applications should be preferably mapped to the square-shaped regions. The selection of an optimal ﬁrst node has to be spatially available and contiguous. However, since applications enter and exit the platform at different times, ﬁnding a free square area is not always possible. Hence, our approach is to search the nodes with the largest number of available neighbors. Several nodes might be available and these nodes located in the center of a square-based region which ﬁt the application will be the options. Then, in this case the region with the maximum lifetime budget will be selected. The maximum lifetime budget is calculated by summing up the lifetime budgets of all the nodes within the region. As was mentioned, several available squares could accommodate the incoming application. To select a proper region, we assign a cost, called PLB, to each square based on the Equation 4 and choose a region with the maximum lifetime budget. The cost is presented by the following equation: P LB = LBi (5) (cid:5) i∈S quare where LBi is the lifetime budget of each node within the region. A higher PLB indicates that wires in the chosen square area have experienced a relatively lower workloads and thus they have a longer service lifetime. The ﬁrst node to map is the node with the maximum PLB in the square area. To balance Algorithm 1 First Node Selection Input: appSize: Size of the entering application; Ap : Task graph of the application; Output: Cf n : The selected ﬁrst node for the mapping; 1: ns ← the node set with the largest number of free neighbors; 2: P LBmax = 0; 3: for each node i in ns do Calculate P LB ; if PLB > P LBmax then Cf n ← i; P LBmax ← P LB ; 4: 5: 6: 7: 8: end if 9: end for aging over the platform, we propose a ﬁrst-node selection method for the lifetime-aware mapping algorithm, as described in Algorithm 1. First, all the nodes with the maximum number of free neighbors are chosen as the candidates. Second, depending on the size of the entering application, the radius factor is determined with the candidate nodes in the center. Third, PLB for each square area is calculated, i.e. the sum of the wire lifetime budget LB(n) for all the nodes in the square area. Finally, the node with the maximum PLB is selected as the ﬁrst node among all the available nodes. B. LaNA Mapping Algorithm To improve the system’s minimal MTTF and balance the lifetime distribution, the lifetime-aware neighborhood allocaFig. 4. An example of the LaNA mapping algorithm The pseudo-code of the proposed mapping algorithm with lifetime improvement is given in Algorithm 2. The mapping is started by initialization the ﬁrst node, ﬁrst task and task mapping order (line 1-3). Then, the available nodes in the closest distance from the ﬁrst node are explored. If there is only one available node, the corresponding task is mapped to it (line 5-7). If not, according to the position of the local and destination node, the cost of F LBs,d and P ELBi are calculated (line 8-9). Finally, the optimal choice of node is the one that has the maximum P ELB (line10-12). As an example, let us follow the mapping process of Fig. 4, where an 150 application with 6 tasks is going to be mapped to the system. According to Algorithm 2, the ﬁrst task (t4 ) with maximum {t2 ,t1 ,t5 } are mapped to the free neighbors of the ﬁrst node. degree is mapped onto the ﬁrst node and its connected tasks The sequential choose order of neighborhood node is based on the value of P ELB . Then, three nodes (a,b,c) can be selected for t0 . The value of F LBt0 ,t1 and F LBt0 ,t2 should be calculated because t0 has communication with t1 and t2 . Then the P ELB of these three nodes (a,b,c) are calculated. The value of F LB and P ELB of three nodes are shown in Fig. 4. According to LaNA, task t0 is mapped onto the node b which has the maximum P ELB . Algorithm 2 Lifetime-aware Mapping Input: Ap : Task graph of the application; tp : the predecessor task of current task Output: map result[]: mapping result, map result[i]=j means task i is mapped to P Ej ; 1: Initialize: ﬁrst node ← by Algorithm1; 2: Initialize: ﬁrst task ← the task with maximum degree; 3: Initialize: M ← task order by WeNA; 4: map result[ﬁrst task] = ﬁrst node; 5: for each task tc in M do P Ep ← map result[tp ] N ← the free PEs with the minimum distance from 6: 7: if more than one PE is selected then Calculate P ELB for each P Ei in N for each PE P Ei in N do select the P Ei that has the maximum PELB map result[tc ] =P E i ; P Ep ; 8: 9: 10: 11: 12: 13: 14: end for end if 15: end for V. R E SU LT S AND D I SCU S S ION A. Experiment Setup Experiments are performed on the many-core simulator, which is an open source NoC simulator [16]. Table I shows the conﬁgured parameters in the simulator. TABLE I S IMULAT ION SETUP Parameters NoC size NoC frequency Packet size Buffer size Routing algorithm Total simulation time Values 8 × 8 1GHz 5 ﬂits 12 ﬂits XY 10 million cycles Several sets of applications with 4 to 20 tasks are generated using TGFF tool [17], where the communication volume is randomly selected between 6 and 14 packets of data. We employ Electromigration as the failure model. In our experiments, the mapping tables and lifetime budgets are updated after each 2C-4 1000 cycles.The following parameters are used for computing the MTTF and LB metrics: A = 1, C = 268f F /mm, Vdd = 1.5V , W = 0.6μm, H = 0.6μm and the ambient temperature of 45◦C. In the experiments, we compare the lifetime-aware mapping algorithm with NN [18], CoNA [5], WeNA [19], and CASqA [20]. B. Minimum MTTF Evaluation The wires with smaller MTTF wear sooner than those with higher values. In this section, the minimum MTTF for different mapping methods observe how the proposed method can improve the minimum MTTF. Three conﬁgurations are as: the system utilization of 60%, 80%, and 100%. It means that cores are dynamically enabled an disabled but ensuring that for example 60% of all cores are active at a time. Fig.5 shows the minimum MTTF values for different mapping algorithms. As can be seen from this ﬁgure, LaNA improves the minimal MTTF for 72.2%, 58.3%, 46.6%, and 48.2% than NN, CoNA, WeNA, and CASqA, respectively. Fig. 5. Minimal MTTF evaluation C. Average MTTF Evaluation Fig. 6 (a) shows that the average MTTF of NoC based on LaNA has improved by 12.3%, 13.5%, 12.8% and 11.7% over NN, CoNA, WeNA and CASqA. (a) (b) Fig. 6. MTTF evaluation: (a) average MTTF (b) variance of MTTF Obviously, the more balanced lifetime distribution could improve the reliability of the whole NoC. The normalized variance of MTTF is shown in Fig. 6 (b). The proposed mapping algorithm decreases the variance of MTTF for 36.8%, 28.8%, 29.9% and 39.1% than NN, CoNA, WeNA and CASqA, respectively. In all situations, the MTTF ﬁrst-node selection mapping algorithm improves the variance, showing its efﬁciency in achieving a more balanced lifetime distribution. 151 D. Average Latency and AWMD The average latency of different mapping algorithms are shown in Fig. 7 (a). Compared with NN, CoNA, and CASqA, the average latency of NoC is decreased by 8.5, 0.6 and 3.1 cycle(s) when the lifetime-aware mapping algorithm is adapted. The avearge latency of lifetime-aware mapping algorithm is only 1.5 cycle more than WeNA. The result shows that the proposed method leads to a longer lifetime with negligible effect on the average latency. Fig. 7 (b) shows the Average Weighted Manhattan Distance (AWMD) metrics experiments over different mapping algorithms by regulating the injection rate. As can be seen, the AWMD of LaNA is smaller than NN, CoNA, CASqA and is close to that of WeNA. The smaller AWMD value, the lower power consumption is expected, since the communicating nodes are placed closed to each other. (a) (b) Fig. 7. NoC evaluation (a) average latency (b) AWMD V I . CONC LU S ION In this paper, we proposed a mapping algorithm to improve and balance MTTF over the NoC platform. First, we investigated MTTF for different mapping techniques which highlighted the problem of unbalanced MTTF on NoC. Second, we analyzed the temperature values on different components of a tile. This analysis conﬁrmed that the router temperature could be considered as a constant. Third, we model the aging of NoC wires, representative of a router activity, based on a router temperature. Finally, we proposed a ﬁrst-node selection strategy and a lifetime-aware mapping algorithm to improve MTTF by considering the lifetime budget metric. Experimental results showed that our mapping algorithm leads to improvements on minimum, average, and variance of MTTF. ACKNOW L EDGM EN T This paper is supported by the National Natural Science Foundation of China under grant (NSFC) No.61534002, No.61471407, Central Universities under Grant ZYGX2016J042. It is also supported by VINNOVAMarieCurie. "
2018,Optimizing dynamic mapping techniques for on-line NoC test.,"With the aggressive scaling of submicron technology, intermittent faults are becoming one of the limiting factors in achieving a high reliability in Network-on-Chip (NoC). Increasing test frequency is necessary to detect intermittent faults, which in turn interrupts the execution of applications. On the other hand, the main goal of traditional mapping algorithms is to allocate applications to the NoC platform, ignoring about the test requirement. In this paper, we propose a novel testing-aware mapping algorithm (TAMA) for NoC, targeting intermittent faults on the paths between crossbars. In this approach, the idle links are identified and the components between two crossbars are tested when the application is mapped to the platform. The components can be tested if there is enough time from when the application leaves the platform and a new application enters it. The mapping algorithm is tuned to give a higher priority to the tested paths in the next application mapping. This leaves enough time to test the links and the belonging components that have not been tested in the expected time. Experiment results show that the proposed testing-aware mapping algorithm leads to a significant improvement over FF, NN, CoNA, and WeNA.","3C-1 Optimizing Dynamic Mapping Techniques for On-Line NoC Test Shuyan Jiang∗ , Qiong Wu∗ , Shuyu Chen∗ , Junshi Wang∗ , Masoumeh Ebrahimi† , Letian Huang∗ , Qiang Li∗ ∗University of Electronic Science and Technology of China, Chengdu, China, †Royal Institute of Technology (KTH), Sweden huanglt@uestc.edu.cn Abstract—With the aggressive scaling of submicron technology, intermittent faults are becoming one of the limiting factors in achieving a high reliability in Network-on-Chip (NoC). Increasing test frequency is necessary to detect intermittent faults, which in turn interrupts the execution of applications. On the other hand, the main goal of traditional mapping algorithms is to allocate applications to the NoC platform, ignoring about the test requirement. In this paper, we propose a novel testing-aware mapping algorithm (TAMA) for NoC, targeting intermittent faults on the paths between crossbars. In this approach, the idle links are identiﬁed and the components between two crossbars are tested when the application is mapped to the platform. The components can be tested if there is enough time from when the application leaves the platform and a new application enters it. The mapping algorithm is tuned to give a higher priority to the tested paths in the next application mapping. This leaves enough time to test the links and the belonging components that have not been tested in the expected time. Experiment results show that the proposed testing-aware mapping algorithm leads to a signiﬁcant improvement over FF, NN, CoNA, and WeNA. Keywords—Network-on-Chip; mapping algorithm; intermittent fault; on-line testing I . IN TRODUC T ION With the development of fabrication process technology and computing architectures, Multi-Processor Systems-onChip (MPSoCs) are likely to have some hundreds of cores integrated into the same chip. However, the performance of a busbased SoC does not scale with a number of cores. Networkon-Chip (NoC) emerges at a historic moment, which proposes a modular and scalable communication architecture [1], [2]. In such an aggressive trend, reliability has become one of the most important challenges. In fact, the shrunken transistors are prone to the variability phenomena while easily inﬂuenced by internal defects, aging process, and wear-out [3]. Furthermore, parameters are more difﬁcult to control when the transistors’ size is reduced, which can lead to tiny defects in the process of production [4]. These tiny defects lead to intermittent faults under the combined effect of voltage, temperature, circuit behavior and other factors. The latest research shows that intermittent faults have accounted for more than 40% of the processor’s faults, showing the importance of handling them [5]. Mapping algorithms decide the location of application tasks on the NoC-based multi-core system. Different methods are proposed to allocate the tasks onto the cores in an optimal way. Depending on the underlying routing algorithm and the mapping strategy, some links are idle during the execution of an application. Traditional methods usually ignore about these idle times while in this paper we utilize them for the purpose of testing. Intermittent faults have a distinct characteristic that they occur repeatedly in ﬁxed positions while having a certain randomness on the time of occurrence. One solution to detect intermittent faults is increasing the test frequency. The periodic Built-In Self-Test (BIST) is used to detect and diagnose the intermittent faults [6]. BIST has the advantages of ﬁne-grained diagnosis and high fault coverage. However, increasing the test frequency comes at the cost of interrupting the applications which are not desired. To address this issue, we combine the test and mapping so that the idle times in mapping can be utilized for injecting test vectors. In this paper, we target the intermittent faults that if not well-treated they may develop to permanent faults. We propose a mapping strategy with embedded testing, called TAMA. The motivation behind this work is that the mapping strategy and the routing algorithm directly affect the link utilization. Thereby, the idle links can be recognized and the components on the idle path can be tested in free times without interrupting the running applications. In short, TAMA tests unused links and components while an application is executing and then tries to map the next application on the tested links/components so that the other idle links can be tested. The remainder of this paper is organized as follows: Section II presents the related work about the NoC test and mapping algorithms. The mapping problems along with the concept of idle links in mapping are formally modeled in Section III. In Section IV, we propose an optimized testing-aware mapping algorithm. Section V presents and discusses the simulation results. Finally, the conclusion is given in the last section. I I . R E LAT ED WORK Due to the high pressure to reduce time to market, the high failure rate of modern chips, and the complexity of large multi-core architectures, the testing process of intermittent faults is becoming increasingly important. However, most fault detection methods focus only on permanent or transient faults [7]–[9], ignoring the intermittent faults. On the other hand, increasing the number of processors in a single chip demands an efﬁcient run-time task mapping algorithm. In the case of dynamic mapping, the task assignment and ordering 978-1-5090-0602-1/18/$31.00 ©2018 IEEE 227 are performed during the execution of an application [10]– [12]. Research has been very limited when considering testing and mapping at the same time. Most researches on mapping algorithms simply focus on improving some essential performance indicators such as trafﬁc and congestion while putting less attention on the system reliability. For example, the CoNA mapping algorithm [10] mainly targets the reduction of internal and external congestion through keeping the mapped region contiguous and placing the communicating tasks in a close neighborhood. The WeNA mapping algorithm [11] primarily aims at decreasing inter-processor communication overhead by arranging the order of task mapping based on the communication volume. Among different test strategies [7], [9], periodic built-in self-test (BIST) is usually applied to detect and diagnose the intermittent faults [6]. However, traditional BIST methods signiﬁcantly inﬂuence the NoCs’ throughput because the circuit under test should be disabled for testing and isolated from the rest of the circuit by wrappers. TARRA [13] tries to reduce the negative impact of BIST methods on performance by introducing a reconﬁgurable router architecture combined with a test strategy. However, TARRA does not take into account the idle time during mapping. The test infrastructure in this paper is derived from [14] that presents the on-the-ﬁeld test and conﬁguration infrastructure for a 2D-mesh NoCs. Unlike the traditional BIST methods [15], [16], a controlled BIST strategy is used to diagnose and locate the faults in the components of the path between two crossbars. The goal of this paper is to integrate the test procedure in application mapping. In this way, it is possible to ensure the reliability of NoC without interrupting the running applications to perform the test. However, if a link is highly utilized during the application mapping, it should be tested with a higher frequency as it is under stress and thus prone to faults. On the other hand, the under-utilized links should not be tested too frequently in order to avoid wasting resources. Therefore, a proper test scheduler and a reasonable mapping strategy should be designed using the information of link utilization. We propose an efﬁcient method to identity the free links and test the path between two crossbars during the task mapping and give priority to the tested links when mapping the next application. I I I . PROB L EM D E FIN I T ION An application includes a set of communication tasks which can form an application graph. The mapping algorithm is a process of mapping an application graph (Fig. 1(a)) to a topology graph (Fig. 1(b)) in an optimal way [17]. In order to simplify the comparison and reduce the problem size, we consider a uniform mesh-based NoC in our deﬁnitions and experiment. A. Testing the Paths The occupied cores with busy and free links can be easily found by considering a uniform mesh-based NoC with the 3C-1 underlying XY routing (as we assumed in this paper). Fig. 1(b) shows a case study that presents the result of mapping an application (Fig. 1(a)) to the cores of NoC using the CoNA mapping algorithm. As can be seen from this ﬁgure, the red and green links stand for the occupied and free links, respectively. The free links and the related components can be tested when the application is running without interrupting it or increasing the execution run. The occupied links will be tested as soon as the application exits the platform. The test lasts until either the test period ends or the paths are needed by a new application. It should be emphasized that the test is immediately stopped when the path is requested by a new application. To reduce these conﬂicting situations, the mapping strategy tries to utilize the links that have been recently tested than those of under the test or requiring a test. (a) (b) Fig. 1. (a) An application with 8 tasks and 9 edges. (b) free (green) and occupied (red) links. B. BIST Strucure Fig. 2 shows the test structure between two routers, which is derived from [14]. Router A generates the test packets in the TPG unit and delivers them toward the TPA unit of Router B for analysis. Test packets pass through the crossbar, output registers, wires and the input buffer controlled by a switch arbiter, routing calculation and state machines. Thus the test procedure is able to capture intermittent faults on data paths as well as the control-units. For simplicity, in this paper we use a link/path test that actually means testing all the components between the TPG unit of one router to the TPA unit of the next router. Fig. 2. The BIST structure. C. On-the-ﬁeld Test Strategy The test strategy aims at a speciﬁc test method to meet the maximum test frequency. Intermittent faults may suffer from the detection delay that is the difference between the time a fault is triggered and the time it is detected. One solution to reduce this delay is to increase the test frequency. On the other hand, the application execution time should not be affected by the test procedure. So, the test scheduling algorithm should be developed by taking into account the three issues: 228 1) The test start time: one of the test constraints is that it should not affect the application mapping, so once a path is not occupied by any application, the path test starts immediately, like path1 in Fig. 3. 2) Identifying and testing the free paths: we utilize the deterministic routing algorithm XY that enables identifying the free paths as soon as an application is mapped to the platform. 3) Handling the conﬂicting situation of test and mapping: when an application is entering the platform, the requested paths should be ready and thus the test should be stopped immediately. In other words, mapping has an absolute priority for the use of paths which ensures that the test does not interrupt or delay the execution of an application, like path2 in Fig. 3. Fig. 3. The path test time under two conditions. D. Reliability Evaluation Metrics We deﬁne a reliability evaluation metric called average test time. The test time is the time from when an application enters the platform until a path is tested (Fig. 3). The average test time is computed by considering the test time of all paths. Lower average test time means a higher test frequency. We also use average test time to indirectly evaluate system reliability. By increasing the test frequency, intermittent faults can be detected, and appropriate fault-tolerate methods can be adopted on time [18] [13]. Therefore, packets will not be affected by faults if faults are detected and tolerated early. IV. T E S T ING -AWAR E MA P P ING A LGOR I THM We analyze the link utilization under four mainstream mapping algorithms as FF [12], NN [12], CoNA [10], and WeNA [11], shown in Fig. 4. The analysis is performed under two situations where the experiment environment of situation1 and situation2 are adapted from [10] and [11], respectively. Link utilization stands for the number of cycles that links are utilized over all simulation run. The link utilization (Lutilization ) can be modeled as: Lutilization = Ts (cid:2) t=1 Nc (t) Nl × Ts 3C-1 reasonable mapping algorithm can provide a balance between test and mapping. It not only improves the link utilization, avoiding the waste of resources, but also increases the test frequency. Fig. 4. The link utilization for different mapping algorithms under two different situations in 8×8 NoC. We claim that there is a quest for a testing-aware mapping algorithm which can achieve the goal of application mapping while detecting intermittent faults in NoCs. It will not degrade the overall system performance. In other words, the paths can be tested without interrupting the mapping procedure. To realize the testing-aware mapping technique, we optimize the mapping algorithm while satisfying the requirements of online testing. The proposed mapping algorithm, named testing-aware mapping algorithm (TAMA) takes the status of on-line testing to exploit an efﬁcient mechanism for mapping. Based on this method, the tasks are ﬁrst ordered and then mapped to the platform. A. Sorting tasks Tasks should be ordered before mapping to the platform. The ordering method of TAMA is similar to WeNA [11]. First, the tasks with the largest number of edges are selected as candidates. For example on the task graph of Figure 5(a), four tasks have three edges (i.e. t2, t3, t4, and t5). Then, the one with the largest number of communication volume is chosen as the ﬁrst task to map (i.e. t3). To sort the remaining tasks, we follow the breadth-ﬁrst traversal technique similar to [11]. In the example of Figure 5(b), the breadth-ﬁrst traversal technique starts from the ﬁrst task (t3) and explores the ﬁrst-level neighbor tasks (t1, t2, and t5), and sorts them based on their communication volume (t5 (18), t1 (16), and t2 (4)) from the father task, t3. In the next step, the neighbor tasks of t5 are explored and sorted (t4(11) and t7(7)), as shown in Figure 5(c). The procedure is repeated for t1 (Figure (d)) and t2, and ﬁnally the neighbor task of t4 (i.e. t6) is examined in the last level (Figure (e)). As a result, the tasks’ order will be t3, t5, t1, t2, t4, t7, t0, and t6. B. Mapping tasks to nodes For mapping tasks to the platform, The CoNA and WeNA algorithms starts with selecting the ﬁrst node by taking the situation of the neighboring nodes into account. This prevents the area fragmentation to a large extend while decreasing the congestion probability [10], [11]. For the remaining tasks, CoNA randomly maps them to one of the closest neighbors [10]. WeNA, on the hand, takes the communication where Nc stands for the number of utilized paths at each simulation cycle; Nl represents the number of paths in the network; and Ts is the total simulation cycles. For all four mapping algorithms, the link utilization is less than 10%, verifying the fact that the paths are mostly in an idle mode. Therefore, an opportunistic on-line test scheduling method can take advantage of such situations to test the paths when free. A (1) 229 3C-1 Fig. 5. An example of TAMA mapping algorithm. volume into account [11] in the mapping decision. Similar to other mapping algorithms, TAMA starts by selecting the ﬁrst node to map and then continue with the rest. The ﬁrst node selection of TAMA is based on three consecutive steps, given in Algorithm1: Step1: the nodes with the maximum number of free neighbors are selected as candidates. This selection prevents area fragmentation and decreases the congestion probability. Fig. 5(f) shows an example of the TAMA mapping algorithm where an application with 8 tasks and 9 edges is going to be mapped to a 3 × 4 mesh NoC with the node ˜n0,0 as the manager. The recently-tested nodes are identiﬁed by highlighted arrows. By following Step 1, the maximum number of available neighbors belongs to the nodes ˜n1,1 and ˜n2,1 , and thus selected as candidates. Step2: from the candidates, the ones with the maximum number of tested links are chosen as new candidates. This avoids selecting the non-tested links as much as possible to give them free time to test in the next round. It can also balance the link utilization and improve the reliability by choosing the links that are recently tested. In the given example, since both nodes have four tested links, both of them are chosen again. Step3: from the new candidates, the node that is closer to the manager (˜n0,0 ) is selected as the ﬁrst node. This is to reduce the system latency. If there are more than one candidates, a node is chosen randomly. Since the node ˜n1,1 is closer to the manager, it is selected as the ﬁrst node to map. Thereby, the ﬁrst task, t3, is mapped to the node ˜n1,1 . The nodes for mapping the remaining tasks are chosen based on the following three steps, given in Algorithm2: Step1: the nodes that are closest to the father task node are selected. As shown in Fig. 5(f), there are four available nodes to map t5 (˜n0,1 , ˜n1,0 , ˜n2,1 , and ˜n1,2 ). Step2: the nodes with the maximum number of tested links are chosen with regard to the location of the father node and the data ﬂow direction. Considering the fact that the ﬂow of data is from t3 to t5, then both ˜n1,0 , ˜n2,1 are considered as the nodes with 1 tested link and thus suitable to map the Algorithm 1 Selecting the ﬁrst node to map Input: Ap : The given application task graph; Output: F N : The selected ﬁrst node; 1: N ← all nodes in NoC except the manager; 2: for node i in Ndo S 1 ← select the nodes with the maximum number of free neighbors; 3: 5: for node i in S 1 do S 2 ← select the nodes with the largest number of tested links; 6: 4: end for 7: end for 8: for node i in S 2 do F N ← select the node with the smallest Manhattan distance to node n(0,0) ; 9: 10: end for task. In this example we map t5 to the node ˜n2,1 , which is chosen randomly. t1 and t2 are also mapped to ˜n1,0 and ˜n0,1 , respectively, as shown in Fig. 5(g). If the father node is located some hops away from the candidate nodes, then the XY routing is considered to ﬁnd the number of tested links on the path. Step3: the nodes with the maximum number of tested links are chosen considering the location of other mapped tasks and the data ﬂow direction. For mapping t4 in Fig. 5 (g), there are three available nodes close to t5 (˜n2,0 , ˜n3,1 , and ˜n2,2 ); two of which with one tested link. On the other hand, as can be seen from the task graph, t4 has data communication with t2 as well which is already mapped to the platform. So, in this step the number of tested links on the path from t2 to t4 is counted to decide for a better node to map. The path can be found by considering a static routing like XY. From two possible options (˜n3,1 and ˜n2,2 ), the node with the maximum number of tested links (i.e. ˜n2,2 ) is selected to map t4 (Fig. (h)). Following Step1 to Step3, t7, t0, and t6 are mapped to the platform, shown in Fig. (h) to (j). 230 Algorithm 2 Selecting other nodes to map Input: Ap : The given application task graph; T Q: The ordered task queue; P: The given platform for mapping; Output: M AP : T→ P: Mapping tasks to the platform; t and n: The task to map and the selected node; 1: N ← all nodes in NoC except the manager; 2: map nalg1 ← t1 ; //map t1 to the node selected by Alg.1 3: for i=2→ |T Q| do for node j in Ndo S 1 ← select nodes that are closest to father node; for node j in S 1 do S 2 ← select the nodes with the largest number of tested links to/from the father node; for node j in S 2 do S 3 ← select node with the largest number of tested links on the path to/from the other nodes; end for end for 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: end for map ns3 ← ti 14: end for V. R E SU LT S AND D I SCU S S ION A. Experiment Setup We compare TAMA with four different mapping algorithms as First Free (FF), Nearest Neighbor (NN) [12], Contiguous Neighborhood Allocation (CoNA) [10] and Weighted-based Neighborhood Allocation (WeNA) [11]. The algorithms are implemented in the ESY-net simulator [19]. Several sets of applications are generated using TGFF [20] with the parameters listed in Table I. Apart from the parameters listed in this table, we set the system usage rates to 1, 0.8, 0.6, and 0.4. TABLE I TH E S E TU P PARAM E T ER S Parameters Network size Number of tasks Max communication volume Application injection rate Simulation length Number of applications Values 8 × 8 4-20 10-30 10 10,000,000 2,000 B. Maximum Test Time Fig. 6(a) shows the maximum test time under different system usage rates for ﬁve mapping algorithms. As can be seen from this ﬁgure, TAMA decreases the maximum test time than the WeNA algorithm by more than 28.98%, 34.6%, 39%, 43.8% under system usage rate of 1, 0.8, 0.6 and 0.4, respectively. The beneﬁt of TAMA is more signiﬁcant in lower usage rates that is because of testing paths at idle times and thus shortening the test time. C. Average Test Time The average test time is evaluated and compared in Fig. 6(b). As can be seen from this ﬁgure, TAMA leads to the 3C-1 lowest average test time which enables a higher test frequency and thus a better reliability against intermittent faults. If a faulty path is detected, methods can be applied to either tolerate or ﬁx it, which is out of the scope of this paper. Similar to the analysis of the maximum test time, TAMA is more advantages in lower usage rates with the maximum increase of 38.1% against WeNA when the system usage rate is 0.4. D. Interrupted Test Rate Fig. 6(c) illustrates the interrupted test rate on all paths during the whole execution time. The interrupted test rate (Tinterrupted ) can be modeled as: Tinterrupted = Tc + Tic Tc (2) where Tc stands for the total number of tested paths during the application mapping over all simulation run. Tic represents the number of tests that are interrupted due to the request on using the path. As can be seen from this ﬁgure, the interrupted test rate is lowest in TAMA as compared to other methods under all conﬁgurations. This is due to the fact that TAMA identiﬁes the idle paths based on the underlying XY routing algorithm and the location of the mapped application. Thereby the idle paths are tested meanwhile the application runs in the platform. The remaining paths are tested when the application leaves the platform, and thus the interrupted test rate decreases. It should be mentioned that the mapping strategy selects the tested paths with a higher probability than untested ones. This may leave the busy paths unallocated this time and thus allowing them to be tested during the execution of a new application. E. AWMD metric Fig. 7(a) shows the results of Average Weighted Manhattan Distance (AWMD) metric under different system usage rates. The AWMD of TAMA is always lower than all the other examined methods. The only exception is when the system usage rate is 1 and the AWMD of WeNA is by 0.00085 lower than TAMA. F. Average Latency Evaluation The average latency follows the same trend as AWMD. As shown in Fig. 7(b), the average latency of TAMA, CoNA, and WeNA are nearly the same while it is signiﬁcantly lower than FF and NN under different system usage rates. In sum, TAMA can reduce the maximum test time, average test time, and interrupt rate at no compromise of AWMD and average latency. V I . CONC LU S ION In this paper, we proposed a combined approach of mapping algorithm and on-line testing, called testing aware mapping algorithm (TAMA). TAMA targets at increasing test frequency by taking advantage of free time slots during the application execution for testing. First, On-the-ﬁeld test infrastructure and strategy are proposed, which guarantee the test program cannot effect the process of application mapping by making use of free paths in mapping. Second, tasks are ordered and 231 3C-1 (a) (b) (a) Max test time (b) Average test time (c) Interrupted test rate. Fig. 6. (c) [5] L. Rashid, K. Pattabiraman, and S. Gopalakrishnan, “Characterizing the impact of intermittent hardware faults on programs,” IEEE Transactions on Reliability, vol. 64, no. 1, pp. 297–310, 2015. [6] H. Al-Asaad, B. T. Murray, and J. P. Hayes, “Online bist for embedded systems,” IEEE design & Test of Computers, vol. 15, no. 4, pp. 17–24, 1998. [7] Q. Yu and P. Ampadu, “A dual-layer method for transient and permanent error co-management in noc links,” IEEE Transactions on Circuits and Systems II: Express Briefs, vol. 58, no. 1, pp. 36–40, 2011. [8] A. Ghofrani, R. Parikh, S. Shamshiri, A. DeOrio, K.-T. Cheng, and V. Bertacco, “Comprehensive online defect diagnosis in on-chip networks,” in VLSI Test Symposium (VTS), 2012 IEEE 30th. IEEE, 2012, pp. 44–49. [9] X. Chen, Z. Lu, Y. Lei, Y. Wang, and S. Chen, “Multi-bit transient fault control for noc links using 2d fault coding method,” in Networkson-Chip (NOCS), 2016 Tenth IEEE/ACM International Symposium on. IEEE, 2016, pp. 1–8. [10] M. Fattah, M. Ramirez, M. Daneshtalab, P. Liljeberg, and J. Plosila, “Cona: Dynamic application mapping for congestion reduction in manycore systems,” in Computer Design (ICCD), 2012 IEEE 30th International Conference on. IEEE, 2012, pp. 364–370. [11] L.-T. Huang, H. Dong, J.-S. Wang, M. Daneshtalab, and G.-J. Li, “Wena: Deterministic run-time task mapping for performance improvement in many-core embedded systems,” IEEE Embedded Systems Letters, vol. 7, no. 4, pp. 93–96, 2015. [12] E. Carvalho, N. Calazans, and F. Moraes, “Heuristics for dynamic task mapping in noc-based heterogeneous mpsocs,” in Rapid System Prototyping, 2007. RSP 2007. 18th IEEE/IFIP International Workshop on. IEEE, 2007, pp. 34–40. [13] L. Huang, J. Wang, M. Ebrahimi, M. Daneshtalab, X. Zhang, G. Li, and A. Jantsch, “Non-blocking testing for network-on-chip,” IEEE Transactions on Computers, vol. 65, no. 3, pp. 679–692, 2016. [14] Z. Zhang, D. Refauvelet, A. Greiner, M. Benabdenbi, and F. Pecheux, “On-the-ﬁeld test and conﬁguration infrastructure for 2-d-mesh nocs in shared-memory many-core architectures,” IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 22, no. 6, pp. 1364–1376, 2014. [15] S.-Y. Lin, W.-C. Shen, C.-C. Hsu, C.-H. Chao, and A.-Y. Wu, “Faulttolerant router with built-in self-test/self-diagnosis and fault-isolation circuits for 2d-mesh based chip multiprocessor systems,” in VLSI Design, Automation and Test, 2009. VLSI-DAT’09. International Symposium on. IEEE, 2009, pp. 72–75. [16] C. Grecu, P. Pande, A. Ivanov, and R. Saleh, “Bist for networkon-chip interconnect infrastructures,” in VLSI Test Symposium, 2006. Proceedings. 24th IEEE. IEEE, 2006, pp. 6–pp. [17] P. K. Sahu and S. Chattopadhyay, “A survey on application mapping strategies for network-on-chip design,” Journal of Systems Architecture, vol. 59, no. 1, pp. 60–76, 2013. [18] M. R. Kakoee, V. Bertacco, and L. Benini, “At-speed distributed functional testing to detect logic and delay faults in nocs,” IEEE Transactions on Computers, vol. 63, no. 3, pp. 703–717, 2014. [19] J. Wang, Y. Huang, M. Ebrahimi, L. Huang, Q. Li, A. Jantsch, and G. Li, “Visualnoc: A visualization and evaluation environment for simulation and mapping,” in Proceedings of the Third ACM International Workshop on Many-core Embedded Systems. ACM, 2016, pp. 18–25. [20] R. P. Dick, D. L. Rhodes, and W. Wolf, “Tgff: task graphs for free,” in Proceedings of the 6th international workshop on Hardware/software codesign. IEEE Computer Society, 1998, pp. 97–101. Fig. 7. (a) AWMD metric (b) Average latency evaluation. a network node which has a maximum number of available neighbors and a largest number of tested paths is selected as the ﬁrst node to map the ﬁrst task. It can signiﬁcantly improve the reliability of system, decrease the congestion probability and prevent area fragmentation. As the third part, TAMA maps the remaining tasks according to the number of tested path and nearest neighborhood, trying to form the most tested and contiguous region. Experiment results showed that TAMA leads to signiﬁcant improvement on test frequency and reliability over traditional mapping algorithms. ACKNOW L EDGM EN T This paper was supported by the National Natural Science Foundation of China under grant (NSFC) No.61534002, No.61471407, Central Universities under Grant ZYGX2016J042. This work is also supported by VINNOVAMarieCurie. "
2018,Neu-NoC - A high-efficient interconnection network for accelerated neuromorphic systems.,"A modern neuromorphic acceleration system could consist of hundreds of accelerators, which are often organized through a network-on-chip (NoC). Although the overall computing ability is greatly promoted by a large number of the accelerators, the power consumption and average delay of the NoC itself becomes prominent. In this paper, we first analyze the characteristics of the data traffic in neuromorphic acceleration systems and the bottleneck of the popular NoC designs adopted in such systems. We then propose Neu-NoC - a high-efficient interconnection network to reduce the redundant data traffic in neuromorphic acceleration systems and explore the data transfer ability between adjacent layers. A sophisticated neural network aware mapping algorithm and a multicast transmission scheme are designed to alleviate data traffic congestions without increasing the average transmission distance. Finally, we explore the sparsity characteristics of fully-connected NNs. Simulation results show that compared to the most widely-used Mesh NoC design, Neu-NoC can substantially reduce the average data latency by 28.5% and the energy consumption by 39.2% in accelerated neuromorphic systems.","2C-3 Neu-NoC: A High-efﬁcient Interconnection Network for Accelerated Neuromorphic Systems∗ ∗ Xiaoxiao Liu1 , Wei Wen1 , Xuehai Qian2 , Hai Li3 , Yiran Chen3 1University of Pittsburgh, Pittsburgh, USA 2University of Southern California, Los Angeles, USA 3Duke University, Durham, USA {xil116, wei.wen}@pitt.edu, xuehai.qian@usc.edu, {hai.li, yiran.chen}@duke.edu ABSTRACT A modern neuromorphic acceleration system could consist of hundreds of accelerators, which are often organized through a network-on-chip (NoC). Although the overall computing ability is greatly promoted by a large number of the accelerators, the power consumption and average delay of the NoC itself becomes prominent. In this paper, we ﬁrst analyze the characteristics of the data traﬃc in neuromorphic acceleration systems and the bottleneck of the popular NoC designs adopted in such systems. We then propose Neu-NoC – a high-eﬃcient interconnection network to reduce the redundant data traﬃc in neuromorphic acceleration systems and explore the data transfer ability between adjacent layers. A sophisticated neural network aware mapping algorithm and a multicast transmission scheme are designed to alleviate data traﬃc congestions without increasing the average transmission distance. Finally, we explore the sparsity characteristics of fully-connected NNs. Simulation results show that compared to the most widely-used Mesh NoC design, Neu-NoC can substantially reduce the average data latency by 28.5% and the energy consumption by 39.2% in accelerated neuromorphic systems. 1. INTRODUCTION Neural Network (NNs) have been widely utilized in many cognitive applications and achieved remarkable successes in the scenarios where the volume of the data to be processed is beyond the capability of a human being, such as robotics [13], self-driven car [17], etc. To better infer the input data, the scale of the adopted NN is usually very large. As a recent example, Google uses a database of about 30 million moves to teach AlphaGo to play Go and beat human champions [16]. Compared to conventional solutions, various hardware accelerators were recently proposed as an energy-eﬃcient alternative to perform NN computations [4]. Network-on-chip (NoC) is widely used in modern multicore systems for on-chip data transferring [6]. Neuromorphic systems can also beneﬁt from NoC to manage the data movement between the accelerators. However, when the scale of the NN is large, the data traﬃc in a neuromorphic system can be very busy, making the NoC become the bottleneck of the system performance and energy. For instance, the data communication may count for more than 30% computation cost in a deep learning accelerator and grows up rapidly with the increase of the system scale [4]. Meanwhile, about 26% chip area and more than 10% total energy consumption comes from the NoC [4]. The non-uniformity of the transmission data size over the connections, the redundancy of the data transmission between the NN layers, and the local congestions all signiﬁcantly degrade the eﬃcacy of the NoC This work is supported in part by NSF-1725456, NSF1615475, and DOE DE-SC0018064 ∗ in a neuromorphic acceleration system. In [3], Carrillo el at. implemented a star-mesh-based NoC for spiking NNs to replace the conventional shared bus solution. However, considering the constraints of spiking NNs and spiking signals, it is diﬃcult to apply the start-mesh-based NoC onto other NNs that have diﬀerent topologies. Ring topology has been recently used in the NOC design of some multicore systems for its simplicity, i.e., Intel Nehalem [14]. But its implementation in a large system is doubtful by taking into account the increased hop count and long wire delay. In this work, we propose a hybrid ring-mesh NoC architecture (namely, NeuNoC) that can adapt to the unique communication patterns in neuromorphic acceleration systems to achieve better performance, less energy, and smaller area. Compared to the prior-arts, our key contributions are: • We perform comprehensive evaluations of traditional NoC in neuromorphic acceleration systems • We design a novel hybrid NoC for neuromorphic to identify design bottlenecks and limitations; acceleration systems based on the characteristic of NN data traﬃc, oﬀering signiﬁcant performance and power eﬃciency improvement; • We propose a sophisticated NN-aware mapping algorithm to reduce the distance of the data communication and alleviate the congestions on neuromorphic acceleration systems as the same time; • We develop a multicast type of data transmission to reduce the amount of packets with same data in each routing path; • We design a new type of ﬂit to leverage the sparse feature of feature map to further mitigate the data traﬃc congestion with an acceptable accuracy loss. A set of popular NN and machine learning applications is adopted in the evaluation of Neu-NoC. Simulation results show that Neu-NoC can reduce the transaction latency up to 28.5% compared to the Mesh NoC baseline and averagely achieve 39.2% energy saving. 2. BACKGROUND AND MOTIVATION 2.1 Neural Networks (NN) In this work, we mainly focus on Multilayer Perceptron (MLP), which is a feedforward artiﬁcial neural network that widely utilized in classiﬁcation algorithms such as the classiﬁcation layer in deep neural networks (DNN) [8] and convolutional neural networks (CNN) [15], and approximate computing [9]. MLP presents not only the basic computation patterns of DNN, i.e., matrix multiplication followed by nonlinear activation functions (e.g., sigmoid etc.) at each layer, but also the intensive communications within the layers. Figure 1 depicts the hardware utilization of Alexnet [12] running on Nvidia GeForce GTX TITAN X GPU [1] where 978-1-5090-0602-1/18/$31.00 ©2018 IEEE 141 2C-3 Passedthroughtrafficduringonecomputation 80 70 60 50 40 30 20 10 0 1 6 1 1 6 1 1 2 6 2 1 3 6 3 1 4 6 4 1 5 6 5 1 6 NetworkChannels 6 6 1 7 6 7 1 8 6 8 1 9 6 9 1 0 1 6 0 1 1 1 1 Uniquedatatraffic Duplicatedatatraffic s ƚ e k c a p a  t a d  f o  . o N 1 0.8 0.6 0.4 0.2 0 Figure 3: (a) The volume of data packets transmitted over diﬀerent channels; (b) The ratio of duplicated data packets. in distributed local memories and fetched through a speciﬁc high-throughput memory bus within a short distance. The matching relations between nueron outputs and weights are stored as the source address information in head ﬂit and also the sequential order of ﬂits in a packet. In MLP, the neurons in one layer only communicate with the neurons in the next adjacent layer. As an initial study, we stimulate the data traﬃc of 6 selected NN benchmarks running on the NoC of the neuromorphic acceleration system depicted in Figure 2. More details about these bencharmks can be found in Table 1 in Section 4. Figure 3(a) shows the result of running mnist mlp 1 that on average, 57.6% of the total data packets travel through only 29% of total channels during the NN computation. The data traﬃc is particularly concentrated on the transmission channels that connecting two adjacent layers while the channels between the neurons in one layer are idle most of the time. Obviously such communication pattern leads to very unbalanced traﬃc patterns and causes congestions over the NoC. In addition, every neuron in a layer of the NN sends the same value to its connected neurons in the next layer. It means that a node of a NoC could send multiple packets containing the same data to a set of nodes. We compare the number of the packets with unique data and the number of the packets with the same data but sent to diﬀerent destination nodes. The results in Figure 3(b) show that a signiﬁcant amount of packets are indeed used to deliver the same data to diﬀerent nodes. Such traﬃc pattern is rare in a conventional computing model and oﬀers a great improvement opportunity for the NoC design in neuromorphic acceleration systems. 3. IMPLEMENTATION OF NEU-NOC 3.1 Hierarchical Structure of Neu-NoC Our initial analysis of the traﬃc patterns on traditional mesh NoC in neuromorphic systems inspired us to propose Neu-NoC – an eﬃcient NoC architecture that can suppress unnecessary data transfers of the same data and reduce the bandwidth consumption by consolidating neurons on the compute memory 100% 80% 60% 40% 20% 0% CŽŶǀŽůƵĂƚŝŽŶ PŽůůŝŶŐ ZĞ>h ůĂƐƐŝĨŝĐĂƚŝŽŶ Figure 1: Hardware utilization of each layer in DNN. MLP processes the largest memory-to-computing ratio. As such a memory-to-computing ratio directly links to the trafﬁc intensity of the NoC on a neuromorphic system, we choose MLP as the target in our study: the performance of MLP on each NoC design reﬂects the worst-case eﬃcacy of the NoC. 2.2 Neuromorphic Acceleration System On a neuromorphic acceleration system, data are stored in a distributed manner (i.e., the weights of synapses) and participates in the computation through local neurons. Although these architectures greatly mitigate the requirement of memory bandwidth, long-range connectivity across cores emerges as a new challenge in hardware development. For example, in IBM TrueNorth system, the local neuronal execution within a neurosynaptic core is extremely eﬃcient, while the energy consumption of core-to-core communication increases rapidly with the distance between source and sume 224× energy of an intra-core one (i.e., 894pJ vs. 4pJ destination [2]: an inter-core data transmission could conper spike per hop). As the scale and density of the NN increase, more inter-core interconnections are introduced; the eﬀect of long-range connectivity and communication quickly becomes a severe design challenge. To overcome this challenge in NN acceleration, both hardware and software solutions are explored. The hardware approaches attempt to build a specialized circuit and/or architecture including some customized network models. These new models, however, are often inconsistent with their state-of-the-art version, resulting in low inference accuracy [2]. On the contrary, the software approaches mainly focus on reducing the scale and connectivity of DNN models while still retaining the accuracy [10]. However, implementing such pruned models on hardware can be very challenging due to the scattered memory access patterns. 2.3 Motivation of Our Work NoC is a critical part in neuromorphic acceleration system designs because the data traﬃc pattern is signiﬁcantly diﬀerent from that in conventional multicore systems. Figure 2 depicts a neuromorphic acceleration system that is composed of 648 processing engines (PEs) [4] connected with a 26×26 Mesh NoC as our baseline. The weights are stored ... NBin Nbout Figure 2: Baseline design of a neuromorphic acceleration system with NoC. 142 Unpacking inject in_0 eject out_0 in_1 out_1 Allocation Packing H P P PP No Unused Data Data No.of“0” ... T Unused H:head(01),P:payload(10), PP:packed(11),T:tail(00) dest P/PP Mesh Router PE inject eject channel_0 channel_1 PE PE PE (b) (c) (d) Ringrouter Meshrouter (a) …… …… Figure 4: (a) Hierarchical Neu-NoC; (b) ring bus; (c) ring router; (d) packet format. same NN layer into local nodes. Figure 4 (a) presents the overview of Neu-NoC architecture. The topology of Neu-NoC is a hybrid ring-mesh NoC structure. It consists of local rings and a global mesh that interconnects all the local rings. Figure 4 (a) shows a conﬁguration example with 8 PEs connected in one ring bus which is communicated to others through a Mesh NoC. In Neu-NoC, we use rings to cluster the neurons in the same layer to reduce the number of the data packets contenting the same data that need to be transferred: the neurons connected by one ring only send one copy of their data to the rings that connect the neurons in the next layer. The local ring topology consists of two channels – a channel to receive data and a channel to pass the neuron output to the next layer, as shown in Figure 4(b). As a result, the output transferring of the neurons does not need to wait until the ring is idle. The traﬃc stalls are greatly reduced. Neu-NoC consists of two types of routers – a ring router connected to the PEs in each local ring and a mesh router connected to each local ring and other four mesh routers on its four neighbour directions. A block diagram of the router’s microarchitecture is shown in Figure 4(c). The ring router consists of a 2-to-1 multiplexers, buﬀers, and function blocks for pack/unpacking data packets. Additionally, the ring router supports the arbitration with diﬀerent priorities in Al location function block, i.e., data packets in-ﬂight always have a higher priority than the newly-injected packets. We adopt the typical mesh router design with wormwhole ﬂit-based ﬂow control [6] for the global mesh network to maintain high ﬂexibility of mapping diﬀerent NN topologies. 3.2 NN-aware NoC Mapping 3.2.1 Effect of NoC mapping As the data transmission requirement (e.g., the source and destination neurons, amount of data) in a NN will not change during the computation, the data routing path is decided by NN-to-NoC mapping and routing algorithm [6]. However, normal direct-mapping or random-mapping may not be optimal: the distance between two connected neurons and the congestion ratio in each NoC channel are more 7 1 2 9 3 10 13 8 6 4 1 4 9 12 13 5 6 7 8 2 3 12 5 11 10 11 (a)NNͲawaremapping (b)Sequentialmapping Figure 5: Diﬀerent PE group placement in Neu-NoC for mnist_mlp_2 (MLP topology after mapping: 8-4-1). 2 2 2 2 2 2 1 1 4 3 3 2 1 1 1 1 1 1 1 1 1 1 2 1 7 1 2 9 5 3 8 6 4 12 10 11 13 Original 1Ͳ>7Ͳ>9 1Ͳ>7Ͳ>9>10 1Ͳ>12 1Ͳ>12Ͳ>11 9Ͳ>12Ͳ>5Ͳ>13 12Ͳ>5Ͳ>13 10Ͳ>11Ͳ>13 11Ͳ>13 Multicast 1Ͳ>7Ͳ>9>10 1Ͳ>12Ͳ>11 9Ͳ>12Ͳ>5Ͳ>13 12Ͳ>5Ͳ>13 10Ͳ>11Ͳ>13 11Ͳ>13 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1Ͳ>7Ͳ>9>10: RouterID PacketHeader Figure 6: Multicast and Packet Header Example. likely to enlarge the transmission latency. Here we use an example to illustrate how diﬀerent mapping schemes inﬂuences the average hop count and the congestion of the data traﬃc. Fig. 5 shows the comparison between a random mapping and our proposed NN-aware mapping based on the NN topology. Here each circle represents a local ring network and the number in each circle labels the NN layers that the ring network resides on. The arrows show the path between routers, and the number on each arrow represents how many packets pass through the path at a moment of MLP computation. In direct mapping scheme, the layers are placed sequentially on the NoC, which may lead to a very high number of hops between the PEs in the adjacent layers. Our NN-ware mapping scheme, however, allows more PEs to access the PEs in adjacent layers with fewer hops. To obtain the minimum hop counts and a more balanced NoC, we design a NN-aware mapping which leads to the mapping solution with signiﬁcantly reduced routing distance and distributed data traﬃc among the NoC for obtaining latency reduction. 3.2.2 Problem formulation and deﬁnition The problem of mapping a NN onto a NoC is a NP problem. We introduce the following deﬁnitions to help to formulate the problem that our proposed NN-aware mapping algorithm is facing to ﬁnd the optimal NoC mapping solution of the NN: Deﬁnition 1: A Neural Network Communication Graph (NNCG) is a directed graph denoted by G(N , A), in which each vertex ni represents one neuron in NN, and each directed arch ai,j models a communication ﬂow from one neuron ni to one of its connected neurons nj in the next layer. (ARCG) G(cid:3) Deﬁnition 2: An architecture characterization graph (U, L) is also a directed graph that represents the physical NoC, where each vertex ui denotes a node in the NoC and each edge li represents a physical link. Deﬁnition 3: For an ARCG G(U, L), a deterministic routing function (cid:2) : R → P maps ri,j to one routing path 2C-3 143 2C-3 header of packet by introducing a bit string encoding scheme [5] to carry multiple destination nodes and add a decoding and bit reset function in the router, as shown in Figure 6. In bit string encoding, each destination can be represented by only one bit. Figure 6 depicts an example of a multicast and its corresponding packet header. The length of the encode bit string in the header equals the number of nodes in the Mesh NoC; each bit represents a node and setting the bit to 1’b1 means that the node is one of the destinations. When a packet arrives at one of its destination, the bit corresponding to the destination will be reset to 1’b0. Among all the selected benchmarks, the longest routing path is 16 links, which exists in alexnet cnn cla. 3.4 Sparsity-aware Trafﬁc Reduction As many prior-arts have proven, the highly abstracted feature map in DNN can be very sparse [7]. This sparsity can also help to reduce the traﬃc on the NoC. We introduce a new type of ﬂit, namely, all-zero ﬂit, to present a sequence of 0‘s in the data. The all-zero ﬂit can be sent in any order between the head ﬂit and the tail ﬂit in a packet. Such a scheme allow only one ﬂit indicate multiple sequential ﬂits of 0‘s in the original ﬂit designs: We add one bit as a ﬂag to denote if the packet is packed with all zero data, and use the payload area to denote the number of the continuous 0‘s. We implement a function block at each input/output port that connecting the mesh and the local ring to pack/unpack all-zero ﬂits, as shown in Figure 4. In the packing block, we use a counter to detect the sequence of 0‘s and a “pack” signal will be asserted if packing is needed. Similarly, in the unpacking block, as soon as an all-zero ﬂit is detected, the router will unpack the all-zero ﬂits and restore the sequence of 0’s. The packing/unpacking blocks only introduce 3.1% area overhead to the router design. Figure 4(d) depicts the format of a data packet. We add a new ﬂit type - ‘PP’ to represent the all-zero ﬂit in which the 2-bit head is 2’b11 and the “body” gives the number of 0‘s. The formats of the other ﬂits are similar to that in conventional wormhole ﬂit-based ﬂow control NoC. 4. EXPERIMENTAL METHODOLOGY In our experiments, we use MLP on MNIST database and the classiﬁcation layer of AlexNet on ImageNet as our NN examples. MLPs on MNIST are trained without data augmentation and AlexNet [12] is trained using Caﬀe 1 . During the forwarding of the NNs, we zero out the activations propagated to the hidden layers when their absolute values are smaller than a predetermined threshold. We deﬁne the accuracy as the mean square error (MSE) between the actual and target outputs under the training vectors. Table 1 gives the benchmark information such as the implementation details, the initial training accuracy, and other informations after feature maps are pruned. We modify Booksim [11] – a cycle-accurate NoC simulator, by adding NN types of traﬃc module to mimic the data transfer in neuromorphic acceleration systems during the operations. Each Node is assumed to be a processing engine (PE) that has the design and computing ability similar to the one in [4]. A constant delay is added as the calculation delay before the output packets are generated when all the input data from the previous layer are collected. Table 2 summarizes the parameters of our simulation platforms. The 1 https://github.com/BVLC/caffe/tree/master/models/ bvlc_alexnet Algorithm 1 NN-aware mapping algorithm 1: procedure NN-aware-mapping 2: input : NNCG(N,A) ← NN communication graph 3: ARCG(U,L) ← architecture characterization graph 4: 5: output : map ← fromNNCG(N,A)toARCG(U,L) 6: for ni ∈ NNCG(N,A) do 7: map ni to ux ∈ ARCG(U,L) 8: Generate map(ni ) ∈ U 9: 10: for each ai,j in A do H ← (cid:2) |rowi − rowj | + |coli − colj | 11: if H ≤ Hmin then 12: Hmin ← H 13: mapmin ← map(ni ) 14: for each pi,j ∈ PHmini,j do 15: ltraf f ic ← li + · · · + lj , li , . . . , lj ∈ pi,j 16: 17: 18: 19: recodelmax f oreachmap(ni ) 20: output map(ni ) with smallest lmax if ltraf f ic ≥ lmax then lmax ← ltraf f ic pi,j , where pi,j ∈ Pi,j . Based on these deﬁnitions, the problem of the NN-aware mapping can be formulated as follow: Give an NNCG and an ARCG, we need to ﬁnd a mapping function map() which satisﬁes: min{TN oC = Q(cid:2) P(cid:2) y=1 x=1 |rown+1,y − rown,x | + |coln+1,y − coln,x | M 2 (M − 1) + P acksum } (1) We assume that the row and the column numbers of the xth PE are rown,x and coln,x , respectively. Correspondingly, rown+1,y and coln+1,y are the row and the column numbers of the y th connected PE in the adjacent layer. such that: 3∀ni ∈ N , map(ui ) ∈ U ∀ni (cid:9)= nj ∈ N , map(ui ) (cid:9)= map(uj ) ∀ai,j ∈ A, pi,j ∈ Pi,j (2) (3) (4) The proposed algorithm is shown in Alogrithm 1. The results of the hop count and max data load of the selected 7 benchmarks using the proposed NN-aware mapping algorithm are summarized in Table 1 and discussed in Section 5. 3.3 Multicast Transmission Since each neuron sends the same data to all its connected neurons in the next layer and some of the data packets share the same routing path, we design a multicast type of data transmission to combine the data packets from the same source node into one data packet, which will take same path during routing. For simplicity of the hardware, only the data packets that are completely contained by a other packet will be merged to the latter one. Table 1 shows the total number of the hops that all the data packets pass through in one NN computation before and after multicasting is applied. To support the multicast transmission, we redesign the 144 Table 1: The description and implementation details of the selected benchmarks (Ran: Random, Mult: Multicast) 2C-3 Benchmark mnist mlp 1 mnist mlp 2 mnist mlp 3 mnist mlp 4 mnist mlp 5 alexnet cnn cla Topology mapping in PEs NoC Initial Zero ﬂit Hops No. Max load Packet No. Mesh Neu accuracy traﬃc Ran NN Ran NN NN Mult 300-100-10 19-7-1 6x6 3x3 98% 63.8% 12 4 3 1 1000-500-10 63-32-1 10x10 4x4 98.27% 55.8% 124 76 16 4 36 20 1500-1000-500-10 94-63-32-1 14x14 5x5 98.28% 58.3% 550 421 24 14 132 87 2000-1500-1000-500-10 125-94-63-32-1 18x18 7x7 98.32% 53.2% 1774 1308 36 11 324 224 2500/2000/1500/1000/500/10 157-125-94-63-32-1 22x22 8x8 98.22% 56.8% 3958 3074 27 16 644 419 4096-4096-1000 256-256-63 24x24 9x9 56.60% 48.3% 8218 7024 90 54 1260 563 Table 2: The system simulation conﬁguration PE Mesh Fattree Mesh [4] Neu NoC 256 16-bit multipliers, 256 16-bit adders 26×26 mesh topology, XY routing, wormhole switching, 4-stage pipeline, 4 virtual channels per port 4-ary fat-tree with a height of 3 in local, 6×6 mesh in global connection, nearest-common ancestor routing for fat-tree, XY-routing for mesh 8 nodes in local ring topology, 9×9 mesh in global with XY-routing, 1 cycle router delay for passing through, min 3 cycle router delay for inject/eject energy consumption of the NoC is estimated by also Booksim with 45nm technology. 5. EXPERIMENTAL RESULTS 5.1 Impact of Concentration Degree Allocating more PEs on each ring network not only reduces the redundant data packets sent to the PEs of the next NN layer but also reduces the latency and energy overhead of the routing on the global mesh. However, increasing the length of the ring network will also increase the wire delay. Figure 7 compares the execution times of all the NN benchmarks using a Neu-NoC with ring and mesh conﬁgurations. Here the total number of the PE is set to 648. When the number of PEs located in the same ring increases from 1 to 8, the NoC performance keeps improving due to the reduction of the global hops count. However, such trend stops when the scale of the ring network is too large (i.e., 64 PEs) so that the increase of the local wire delay has surpassed the reduction of the global hops count. Among all the tested benchmarks, alexnet_cnn_cla is most sensitive to the scale of the ring network because its large hidden layers (4096 neurons) fully occupies all the PEs in each ring. In the following experiments, we choose 8 PEs as our default conﬁguration which demonstrates the best balance between the local wire latency and global hops count. 5.2 Impact of Feature Map Sparsity We also evaluate the impact of the NN sparsity on the eﬃcacy of Neu-NoC. Here the sparsity is deﬁned as the percentage of 0’s in the feature map. It is known that increasing the sparsity will degrade the accuracy of the NN. However, as shown in Figure 8, the accuracy degradation of all the benchmarks can be maintained at a very low level even the 2.5 2 1.5 1 0.5 0 k=1,n=24 k=4,n=12 k=8,n=9 k=16,n=6 k=64,n=4 mnis_mlp_1 mnis_mlp_2 mnis_mlp_3 mnis_mlp_4 mnis_mlp_5 alexnet_cnn_cla Figure 7: Impact of concentration degree (k : the number of the nodes in ring; n: the numbers of columns/rows of mesh). 145 corresponding NN sparsity is very high: In alexnet cnn cla, which is the worst case, the average sparsity is 78.6% across three feature maps while the incurred accuracy is less than 1%. In Neu-NoC, we can dynamically sparsify the NN by zeroing out any features smaller than 1%. The column of “Zero ﬂit” of Table 1 shows the ratios between the corresponding traﬃc and the baseline traﬃc. 5.3 Effectiveness of NN-aware Mapping Figure 9 illustrates the impacts of diﬀerent NN mapping schemes on the average latency of packet transmission in Neu-NoC in the simulated neuromorphic acceleration system. Here x−axis is the data injection rate of the input neurons of the ﬁrst layer, which is deﬁned as the number of packets are input in each node in every cycle. The injection rate between the NN layers is triggered after the neuron collects all the inputs from the previous layer. As shown in Figure 9, NN-aware mapping always achieves the best performance in all 6 tested benchmarks as well as the best tolerance to the traﬃc load (data injection rate) increase. For example, in 5 out of 6 benchmarks, the average packet latency of NN-mapping maintains low until the data injection rate exceeds about 0.09 while that of the other two mapping schemes starts to rockets when the data injection rate reaches about 0.05. In addition, NN-aware mapping demonstrates great scalability by showing more signiﬁcant average packet latency reduction when the network scale is large, e.g., in alexnet_cnn_cla (see Figure 9(f )). 5.4 Effectiveness of Multicast Figure 10 compares the average packet latencies before and after applying multicast in 5 benchmarks: mnist_mlp_2 ∼ mnist_mlp_5 and alexnet_cnn_cla. Note that here we did not include mnist_mlp_1 because mnist_mlp_1 does not have any data packet can be represented by other packets which share the same source node and contain its routing path. The result shows multicast successfully reduces that the average packet latency in all 5 benchmarks, especially in Alexnet_cnn_cla and mnist_mlp_2 which have less layers mnist_mlp_3 ∼ mnist_mlp_5. The packet counts before and and hence, less complicated mapping and routing paths than after applying multicast are depicted in Table 1. 5.5 Evaluation of Neu-NoC We compare the execution time and energy consumption of three NoC designs: 2D Mesh, Fattree Mesh [4], and NeuNoC (including the design with and without zero ﬁlt design) for the 6 benchmarks, as depicted in Figure 11. All the results have been normalized to the baseline Mesh NoC design. Compared to Mesh NoC, Neu-NoC averagely reduce the average packet latency and energy consumption of the NoC by 23.2% and 31.1%, respectively. Compared to FattreeMesh NoC [4] which has been used in a neuromorphic acceleration system, Neu-NoC achieves on average 6% average packet latency reduction and 13% energy consumption saving, respectively. If we allow 1% accuracy degradation of NN 0 0.2 0.4 0.6 0.8 1 0.92 0.94 0.96 0.98 0.85 1 fc2 fc3 (a) mnist mlp 1 0 0.2 0.4 0.6 0.8 1 0.90 0.95 1.00 fc2 fc3 (b) mnist mlp 2 0 0.2 0.4 0.6 0.8 1 0.88 0.9 0.92 0.94 0.96 0.98 1 fc2 fc3 fc4 (c) mnist mlp 3 0 0.2 0.4 0.6 0.8 1 0.9 0.92 0.94 0.96 0.98 1 fc2 fc3 fc4 fc5 (d) mnist mlp 4 0 0.2 0.4 0.6 0.8 1 0.9 0.92 0.94 0.96 0.98 1 fc2 fc3 fc4 fc5 fc6 (e) mnist mlp 5 0 0.2 0.4 0.6 0.8 1 0.48 0.50 0.52 0.54 0.56 0.58 fc6 fc7 fc8 (f ) alexnet cnn cla Figure 8: Accuracy impact of feature map sparsity (y-axis: sparsity). 3500 3000 2500 2000 1500 1000 500 0 0.01 0.03 0.05 0.07 0.09 0.11 0.13 s e c y C l random seq nnͲaware (a) mnist mlp 1 4000 3500 3000 2500 2000 1500 1000 500 0 0.01 0.03 0.05 0.07 0.09 0.11 0.13 random seq nnͲaware (b) mnist mlp 2 0 500 1000 1500 2000 2500 0.01 0.03 0.05 0.07 0.09 0.11 0.13 random seq nnͲaware (c) mnist mlp 3 0 500 1000 1500 2000 2500 0.01 0.03 0.05 0.07 0.09 0.11 0.13 random seq nnͲaware (d) mnist mlp 4 0 500 1000 1500 2000 2500 3000 0.01 0.03 0.05 0.07 0.09 0.11 0.13 random seq nnͲaware (e) mnist mlp 5 4500 4000 3500 3000 2500 2000 1500 1000 500 0 0.01 0.03 0.05 0.07 0.09 0.11 0.13 random seq nnͲaware (f ) alexnet cnn cla Figure 9: Average packet latency of diﬀerent mappings (x-axis: injection rate). mnist_mlp_3 mnist_mlp_4 mnist_mlp_5 0 500 1000 1500 2000 0.01 0.03 0.05 0.07 0.09 0.11 0.13 e c y C l s mnist_mlp_2 Original multicast 0 500 1000 1500 0.01 0.03 0.05 0.07 0.09 0.11 0.13 Original Multicast 0 500 1000 1500 0.01 0.03 0.05 0.07 0.09 0.11 0.13 Original Multicast 0 500 1000 0.01 0.03 0.05 0.07 0.09 0.11 0.13 Original Multicast 0 500 1000 1500 0.01 0.03 0.05 0.07 0.09 0.11 0.13 Alexnet_cnn_cla Original Multicast Figure 10: Average packet latency of before and after applying multicast (x-axis: injection rate). 1 0.8 0.6 0.4 0.2 0 1 0.8 0.6 0.4 0.2 0 Mesh FatͲtree NeuͲNoC NeuͲNoCwithzeroflit ;ĂͿ ;ďͿ Figure 11: (Normalized (a) average packet latency and (b) energy of all the NoC designs. (See Table 1), Neu-NoC with zero ﬁlt design can further reduce the average packet latency and energy consumption by 11.7% and 21.19%, respectively, compared to Fattree-Mesh. 6. CONCLUSION & FUTURE WORKS In this work, we propose Neu-NoC, a novel NoC design to optimize the traﬃc in neuromorphic acceleration systems. A set of techniques, i.e., NN-aware mapping, multicast, and sparsity-aware traﬃc reduction, are proposed to reduce average hops account and redundant traﬃcs. Our results show that Neu-NoC can eﬀectively reduce the average packet latency and energy consumption in the tested 6 MLP benchmarks by on average 23.2% and 31.1%, without incurring any accuracy degradations. Moreover, slightly relaxing the accuracy requirement of the benchmarks by 1% can further save the average packet latency and energy consumption by 28.5% and 39.2%, respectively. Our future work will focus on applying Neu-NoC to accelerate other components of DNNs such as convolutional and pooling layers. 7. "
2019,Routing in optical network-on-chip - minimizing contention with guaranteed thermal reliability.,"Communication contention and thermal susceptibility are two potential issues in optical network-on-chip (ONoC) architecture, which are both critical for ONoC designs. However, minimizing conflict and guaranteeing thermal reliability are incompatible in most cases. In this paper, we present a routing criterion in the network level. Combined with device-level thermal tuning, it can implement thermal-reliable ONoC. We further propose two routing approaches (including a mixed-integer linear programming (MILP) model and a heuristic algorithm (CAR)) to minimize communication conflict based on the guaranteed thermal reliability, and meanwhile, mitigate the energy overheads of thermal regulation in the presence of chip thermal variations. By applying the criterion, our approaches achieve excellent performance with largely reduced complexity of design space exploration. Evaluation results on synthetic communication traces and realistic benchmarks show that the MILP-based approach achieves an average of 112.73% improvement in communication performance and 4.18% reduction in energy overhead compared to state-of-the-art techniques. Our heuristic algorithm only introduces 4.40% performance difference compared to the optimal results and is more scalable to large-size ONoCs.","Routing in Optical Network-on-Chip: Minimizing Contention with Guaranteed Thermal Reliability Mengquan Li1 , Weichen Liu2;(cid:3) , Lei Yang1 , Peng Chen1 , Duo Liu1 , Nan Guan3 1College of Computer Science, Chongqing University, China 2School of Computer Science and Engineering, Nanyang Technological University, Singapore 3Department of Computing, Hong Kong Polytechnic University, Hong Kong ABSTRACT Communication contention and thermal susceptibility are two potential issues in optical network-on-chip (ONoC) architecture, which are both critical for ONoC designs. However, minimizing conﬂict and guaranteeing thermal reliability are incompatible in most cases. In this paper, we present a routing criterion in the network level. Combined with device-level thermal tuning, it can implement thermal-reliable ONoC. We further propose two routing approaches (including a mixed-integer linear programming (MILP) model and a heuristic algorithm (CAR)) to minimize communication conﬂict based on the guaranteed thermal reliability, and meanwhile, mitigate the energy overheads of thermal regulation in the presence of chip thermal variations. By applying the criterion, our approaches achieve excellent performance with largely reduced complexity of design space exploration. Evaluation results on synthetic communication traces and realistic benchmarks show that the MILP-based approach achieves an average of 112.73% improvement in communication performance and 4.18% reduction in energy overhead compared to state-of-the-art techniques. Our heuristic algorithm only introduces 4.40% performance difference compared to the optimal results and is more scalable to large-size ONoCs. 1 INTRODUCTION ONoC architecture [2] provides an innovative solution for longhaul data transmission to satisfy the communication bandwidth and latency requirements with low power dissipation. State-of-the-art ONoC contains a photonic network for bulk message transmission and an electronic overlay network for the control of the photonic network. The inability of ONoCs to perform inﬂight buffering and processing suggests the use of optical circuit switching: optical messages are transmitted end-to-end in the photonic network, once an optical path is established by a control packet routed in the electronic network. This approach provides excellent communication performance for unconﬂicted message transmissions, in which dedicated optical circuits guarantee the full bandwidth of the channels and (cid:3) Corresponding author: Weichen Liu. Email: liu@ntu.edu.sg. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and /or a fee. Request permissions from permissions@acm.org. ASP-DAC 2019, Jan. 2019, Tokyo, Japan © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6007-4/19/01. . . $15.00 https://doi.org/10.1145/3287624.3287650 remain connected for the duration of the communication sessions. However, conﬂicted messages have to be blocked for guaranteed delivery with extra communication delays and energy overheads, which would degrade the communication performance and energy efﬁciency of ONoCs. Most works [5, 14] utilize dedicated ONoC architectures and bypath links to avoid contention. They possess high efﬁciency while incur additional hardware overheads. Contention-aware adaptive routing [7] is an attractive solution for ONoCs, which mitigates communication conﬂict by utilizing its inherent ﬂexibility. It requires no extra hardware but leverages the existing generic network architecture with lightweight software computation. Existing routing techniques generally make routing decision for every communication pair individually, which does not facilitate reducing communication contention. Large-scale ONoCs typically have multiple communications at any moment. Therefore, we focus on contention-aware adaptive routing considering multiple communications in this work. Thermal reliability is another potential issue in ONoC designs due to the intrinsic thermal sensitivity of nanophotonic devices, which has aroused great concern in recent years [6, 9, 13, 17, 18]. On every routing path in ONoC, multiple optical switches switch signals in stages to implement data transmission. Micro-ring resonator (MR), performing as a wavelength-selective optical switch in every stage, is especially susceptible to temperature variations. The resonant wavelength of a MR red-shifts with increasing temperatures and blue-shifts with decreasing ones. The undesired mismatch between signal wavelength and the resonant wavelength of the MR will result in additional optical power loss, potentially causing performance degradation and even functional failures. For guaranteeing the thermal reliability of ONoCs, device-level thermal tuning is widely employed for switches to compensate their temperaturedependent wavelength shift. However, this technique is energy-hungry, such that the energy consumption for thermal regulation would possibly null the beneﬁts of ONoC in energy efﬁciency [13, 18]. The problem will be more critical in the ONoCs who have large temperature variations and a large number of switching stages. Separately performance improvement and thermal management are impractical for deep optimization on ONoCs. However, there are apparent contradictions between optimizing communication performance, energy efﬁciency and guaranteeing reliability. The local thermal tuning guarantees the thermal reliability of generic ONoCs while introduces extra energy overhead. In order to minimize the regulation energy consumed by thermal tuning, messages prefer to be routed along the path that has the lowest thermal gradient, while communication pairs prefer to select different routing paths without contention for better communication performance. In this paper, we address the above trade-off problem by developing novel routing techniques. Analyzing the thermal effects in ONoC, we theoretically formulate the boundary condition of the number of switching stages in routing paths for reliable data transmissions and present a routing criterion at the network level based on the current silicon photonic fabrication technology. The criterion, combined with recognized device-level thermal tuning, can implement thermal-reliable ONoC. We further propose two contentionaware adaptive routing approaches, including a MILP model and a heuristic algorithm (CAR), to minimize communication conﬂict based on the guaranteed thermal reliability, and meanwhile, mitigate the energy overheads of thermal regulation in the presence of chip thermal variations. By applying the criterion, our approaches can achieve excellent performance with largely reduced complexity of design space exploration. Compared to state-of-the-art techniques on synthetic communication traces and realistic benchmarks, our MILP-based approaches improves communication performance by 112.73% and reduces energy overhead by 4.18% on average. The heuristic algorithm achieves a satisfying performance comparable to the MILP model with only 4.40% difference and is more scalable to large-size ONoCs. The rest of this paper is organized as follows: Sec. 2 analyzes the thermal effects in ONoC and presents a routing criterion to guarantee thermal reliability. In Sec. 3, we formulate the problem addressed in this paper and propose two routing approaches. The evaluation results are shown in Sec. 4 and Sec. 5 draws the conclusions. 2 THERMAL RELIABLE ONOC An ONoC (the electronic network is omitted here for simplicity) and the logical view of an optical router are illustrated in Fig. 1(a). We provide a speciﬁc switching network design of a nonblocking 5 (cid:2) 5 optical router [3] in Fig. 1(b), in which the wavelength-selective optical switches play a key role in implementing high-level routing policies in ONoCs. There are two different switch designs shown in Fig. 1(c): parallel switching element (PSE) and crossing switching element (CSE). When a switch is conﬁgured to be switched off, the optical signal from the Input port will be delivered to the Through port. Otherwise, it is resonated into the ring and delivered to the Drop port. (a) An ONoC and an optical router in logical view. (b) Router Cygnus [3] (c) Switching elements Figure 1: ONoC and its functional components. The resonant wavelength of switching elements is sensitive to ambient temperature. Given the initial resonant wavelength, l0 , at initial temperature, T0 , the formula of the resonant wavelength of a switching element, lSE , and its ambient temperature, TSE , can be expressed as follow: lSE = l0 + r(cid:1) (TSE (cid:0) T0 ) (1) where r is the temperature-dependent resonant wavelength shift coefﬁcient of the switching element and is about 0.06 nm/K at the 1550 nm wavelength range [18]. The undesired deviation from the peak resonant wavelength caused by temperature variation would result in additional power loss. According to the traveling wave theory [10], the optical power loss due to a switching element can be formulated as follow: ( ( )2 )2 ) (  LSE = 10l og 2k2 + k2 2k2 p 1 + lin (cid:0) l0 (cid:0) r(TSE (cid:0) T0 ) 4 q2 (2) i=1 (3) LSEi where k2 is the fraction of power coupling between the waveguides and the ring. k2 p is the fraction of intrinsic power loss per round-trip in the ring. lin is the wavelength of optical signals. q is the -3dB bandwidth of the power transfer spectrum. To ensure that ONoC function properly, the signal power reaches to the receiver of each routing path should be guaranteed. Otherwise, the ﬂuctuations in received power may lead to dramatic reliability degradation and even failure [12]. We formulate the condition in (3), where PT X is the input optical power of the transmitter on a path; (cid:229)m i=1 LSEi is the total optical power loss inserted in the m active switches (i.e., the number of switching stages) in the path; LW G includes the power loss due to the passive switches and the waveguides, and SRX is the receiver sensitivity. Compared to the switches in the ON state, the thermal sensitivity of the passive switches and the waveguides is negligible [18]. PT X (cid:0) m(cid:229) (cid:0) LW G (cid:21) SRX The above model indicates that there are two methods to implement reliable data transmission for ONoCs in presence of temperature ﬂuctuations. One is to increase the input power of the transmitter to guarantee enough optical power reaching the receiver under a high loss in the path. The other way is to control the total power loss of optical switches by employing device-level thermal tuning [13], which is more energy-efﬁcient than the former method in current silicon photonic technologies [18]. By utilizing local microheaters and photodetectors (PDs) that are readily available in a typical photonic platform, thermal tuning technique dynamically maintains the local temperature and the resonant wavelength of the switches throughout the duration of their operation. The major concern of this technique is the power consumption, which may signiﬁcantly decrease the beneﬁts of ONoCs in energy efﬁciency. Formally, the regulation power consumed by thermal tuning in a routing path is formulated as follow: e(cid:1) r(cid:1) (TSEi E o (4) where e is the tuning efﬁciency in mW/nm; TSEi is the temperature of the i-th switch; td ur is the duration time of the switches’ operation. We apply thermal tuning in this work for ONoC reliability and further optimize the regulation energy overhead in Sec. 3. However, even in the cases where every switch works stably and properly through thermal tuning, the system reliability of ONoC is t hermal(cid:0)t uning = (cid:0) T0 ) (cid:1) td ur m(cid:229) i=1 SE (cid:21) SE also affected by the number of switching stages in routing paths. A large number of switching elements in a routing path would increase the total power loss of the path, resulting in a risk of unreliability. To ensure that the signal power received by the receiver on an optical path is not lower than its sensitivity, we deduce a boundary condition for the limit of the number of switching stages in routing paths from (3) as follow: m (cid:20) (PT X (cid:0) LW G (cid:0) SRX )=Lid eal m (cid:1) Lid eal (5) where Lid eal is the optical loss of switches at their desired temperature. The power loss due to the switches under temperature ﬂuctuations is always greater than the loss of ideal switches ((cid:229)m SE ). To show the impact of the number of switching stages on the power loss of routing paths more intuitively, we conduct experiments to simulate the transmission processes of optical signals along the routing paths that have different number of switching stages based on the current manufacturing technology of silicon photonic devices. We employ the professional photonic component & circuit simulations [1], which are widely used by the nanophotonics community for design and veriﬁcation. A recognized optical router Cygnus (see Fig. 1(b)) is employed for ONoCs in the simulations, in which at most one switching element is active for a speciﬁc transmission. We assume that all the switches in routers work at their desired temperature by applying thermal tuning. i=1 LSEi (a) Effect of # of switching stages on optical power loss. (b) Three types of routing paths. Figure 2: Analysis on the # of switching stages in routing paths. As shown in Fig. 2(a), the total power loss of a routing path is proportional to the number of switching stages in this path. The state-of-the-art PDs in the receivers achieve a sensitivity of -14.2 dBm [18]. Assumed the input power is 1 mW , we can observe that if the number of switching stages in a routing path is more than 4, the received power of the PD is lower than its sensitivity. Our results are consistent with those reported in [15]. This problem will be more serious with large temperature variations. Therefore, in this paper, we restrict the number of switching stages in routing paths to 4 for guaranteeing system reliability and, as a side beneﬁt, energysaving1 . The less number of switching stages a route has, the less input power the transmitter requires for reliable transmission. Besides, the energy consumed by thermal tuning decreases with the decreasing number of switching elements. There are three types of shortest routing paths2 for every communication pair: I-shape, Lshape, and Z-shape routes, as illustrated in Fig. 2(b). 1As the development of silicon photonic technology, this restriction can be relaxed due to low-loss optical device fabric. 2Minimal routing considers route length and route utilization, which will facilitate to mitigate the risk of contention. Based on the theoretically formulated boundary condition of the number of switching stages in routing paths, we present a routing criterion at the network level considering the current silicon photonic fabrication technology. The criterion, combined with device-level thermal tuning, can implement thermal-reliable ONoC. In addition, it helps to exponentially reduce the search space when making routing decisions, which improves the efﬁciency of our approaches proposed in Sec. 3. 3 CONTENTION-AWARE ROUTING Utilizing local thermal tuning and restricting the number of switching stages in routes, we guarantee the thermal reliability of generic ONoC, while it also brings a challenge: reducing the energy consumed for thermal tuning. In addition, we aim at mitigating communication contention by leveraging the inherent ﬂexibility of adaptive routing in this paper. While the ﬂexibility would decrease when the number of switching stages is restricted, which is adverse to contention reduction. Considering the contradiction between communication performance, energy efﬁciency and thermal reliability, we formulate the problem as follow: Given the ﬂoorplan of a thermal-reliable ONoC that is implemented by using the solutions presented in Sec. 2, and a communication demand represented by a set of pairs that require data transmission, determine a route for every pair to minimize communication conﬂict, and meanwhile, mitigate the energy overheads of thermal regulation in presence of chip thermal variations. 3.1 MILP-based routing approach We ﬁrst model the problem by MILP formulations. Our model includes two phases: the major objective is to minimize communication conﬂict; based on it, we minimize the total energy consumption with considering chip thermal variations. The notations used in MILP formulations are summarized with deﬁnition in Tab. 1. Table 1: Variables used in MILP model Notation Deﬁnition D rPi L reqli;k Tr SEi;k Xi;k Nc(cid:2) Ei The communication demand. The set of available routes (I/L/Z-shape paths by applying the routing criterion) of pair i. The set of optical links. The set of required links for constructing the routing path k of pair i. The ambient temperature of the router r. The set of switching elements in the routing path k of pair i. Binary variable, equals to 1 if communication pair i routes along the routing path k. The number of communication conﬂicts. The energy consumption of comm. pair i. In the ﬁrst phase, we minimize communication contention. Ob ject ive 1 : minimize(Nc(cid:2) ) The key constraints are described as follows: Constraint for communication demand: Equation (7) guarantees that at most one routing path is selected for every pair. Xi;k (cid:20) 1 (7) Constraint for route overlap: For each optical link l 2 L, it can be occupied by zero or one routing path. If link l is occupied by a 8i 2 D k2rPi (cid:229) (6) As shown in Algorithm 1, we can ﬁrstly obtain the set of all the available routing paths for every communication pair by applying the routing criterion proposed in Sec. 2, which constitutes its communication region (denoted as cRegion) (Line 2–4). For a pair (src; d es), there are at most jd es:x (cid:0) src:xj + jd es:y (cid:0) src:yj paths in total. The route of each pair is selected from its communication region. For the pairs whose communication regions are independent of other regions, we are free to select a routing path based on the routes’ temperature distribution, without the risk of contention (Line 5–7). The function GetRouteWithMaxEnergyEfficiency() is designed to ﬁnd the route that maximizes energy efﬁciency among the communication region. For the pairs that there are overlaps between their regions, we should consider the risk of link conﬂict, potentially resulting in communication contention. To handle the potential conﬂict regions, we sort pairs according to the sizes of their communication regions, from small to large (Line 9). The pairs with small communication regions are prior to the pairs with large regions because it has less alternatives. It is easier for the pairs with large communication regions to ﬁnd feasible routes even if some of links are occupied by other communications. In the most extreme case, no feasible route can be obtained for the pairs that there is only one available routing path and it has been occupied by others. We make routing decisions for pairs in the sorted order. For every communication pair, we select the route that achieves maximal energy efﬁciency on the basis of no communication contention (cRegion Xi;k (cid:229) (cid:229) i2D k2rPi k2rPi 8i 2 D route, Cl = 1. Then (cid:229)k2rPi Xi;k (cid:3) reqli;k (cid:20) 1, which guarantees no route overlap. Xi;k (cid:3) reqli;k (cid:20) Cl (8) Constraint for communication conﬂict: Equation (9) calculate the number of conﬂicts. Nc(cid:2) = D (cid:0) (cid:229) (9) In the second phase, based on the minimum number of communication conﬂicts obtained from the ﬁrst phase, we minimize total energy overhead with consideration of the regulation energy consumed by thermal tuning, shown as follow: i 2 D Ob ject ive 2 : minimize(sum(Ei )) (10) In addition to the constraints (7)-(8), we add two key constraints in this phase as follows: Constraint for minimum conﬂict: Equation (11) guarantees the minimum contention, based on which we can further optimize the energy overhead in presence of chip thermal variations. Xi;k = D (cid:0) Nc(cid:2) (11) (cid:229) i2D (cid:229) k2rPi Constraints for thermal-induced energy consumption: In this paper, we consider a set of tasks running in a relatively stable power proﬁle, resulting in steady temperature states of the processor cores. Thus, the ONoC is in a steady temperature state in the concerned period of time. The derived Equation (12) estimates the total energy consumption of each pair, by taking thermal variations into account. ( Ei =Xi;k (cid:3) + m (cid:1) E o E e int (cid:1) W e ct rl (cid:1) h + E e oeeo (cid:1) W o MR + E o payl oad ) cu (cid:1) (h + 1) ) (( ( )) (12) + (cid:229) r2SEi;k e(cid:1) r(cid:1) (Tr (cid:0) T0 ) (cid:1) (W o payl oad =Roeeo + d (cid:1) n=c) ct rl and W o payload =Roeeo + d (cid:1) n=c) where W e payload refer to the size of control packet in the electrical network and the size of data packet in the optical network, respectively; h denotes the number of hops from the source to the destination; Roeeo is the E-O and O-E interface data rate; d is the travel distance of a data packet; n is the refractive index of silicon waveguide; c is light speed in vacuum; (W o denotes the duration time of the switches’ operation; E e int is the average energy required to transfer a single bit through an electrical interconnection; E e cu denotes the average energy required by the control unit to make a decision for a single packet; E o MR is the energy consumed by an active MR to transfer a data packet; E o oeeo is the energy required by a single bit for O-E and E-O conversions. The optimal result obtained by our MILP model can be converted into a equivalent routing solution. We optimize communication performance by minimizing contention, and explore the design space to ﬁnd the optimal routing solution with minimized energy consumption based on the minimum contention. 3.2 Heuristic algorithm As the problem addressed in this paper is NP-hard, the running time of the proposed MILP model increases exponentially with the increasing sizes of network and communication demand. To effectively solve this complex issue, we further propose a heuristic contentionaware routing (CAR) algorithm. has smaller region (Pair2 in this example). Assume the route rP2;2 achieves maximum energy efﬁciency for Pair2 , the link (R7 ! R3 ) is occupied. Thus the available communication region of Pair3 is reduced to frP3;1 ; rP3;2 ; rP3;3 g and we can select one of routing paths based on their temperature distributions. algorithm proposed in [16]. Experiments are run on a workstation with Intel(R) Core(TM) i5-4590 at 3.30 GHz and 8 GB memory. Evaluation results on average communication energy consumption, communication latency, network throughput, and link utilization are compared with two state-of-the-art techniques, including a thermal-sensitive routing algorithm proposed in [17] and an efﬁcient contention-aware routing approach (DyXY) [7]. Work [17] focuses on the effect of thermal variations on the energy efﬁciency of ONoCs and the proposed routing mechanism is able to ﬁnd the routing solution that has the minimum total energy consumption. Figure 3: An example of the CAR algorithm. Complexity analysis: The complexity of Algorithm 1 is O (M (cid:2) N ), where M is the number of communication pairs and N 2 is the ONoC size. The complexity of looping through the jd es:x (cid:0) srx:xj + jd es:y (cid:0) src:yj alternative paths to ﬁnd a route with maximal energy efﬁciency for a pair is O (N ) in the worst case, and there are M communication pairs in total. Our approaches can be integrated into the centralized thermalaware task scheduler in the operating system (OS). The scheduler gathers chip temperature information when making decisions for task assignment and scheduling. The communication requests during the period of task scheduling are known by the scheduler in advance. Therefore, the scheduler can calculate the routes for every pairs and give the routing information to the source nodes of the pairs during the period of task mapping. The setup process of routing path is conducted in the electronic control network. The electronic routers can transfer a control packet that contains route information to establish the desired routing path. We can achieve deadlock-free electronic control network with additional efforts, such as using virtual channel ﬂow control. Both of our approaches are suitable to be applied for multiple communication requests, while we do not restrict the heuristic CAR algorithm to be applied for the cases with single communication request. 4 PERFORMANCE EVALUATION We consider 2D mesh-based ONoCs with size range from 8 (cid:2) 8 to 15 (cid:2) 15 as target platforms. A simulator in MATLAB is built to evaluate our approaches on the target platforms. Considering the DVFS capability of modern processors, we model computation power under different voltage/frequency levels by McPAT v1.0 [8] for out-oforder Alpha 21346 cores in 22 nm technology. The generated processor power traces are used to estimate chip thermal distribution with HotSpot v5.02 [4]. We perform the approaches based on the generated chip thermal variations. Gurobi optimization with CVX v2.1 is employed as the MILP solver in this paper, which is designed to be the fastest, most powerful solver available for MILP problems. We base our experiments on a large number of synthetic communication traces and a set of realistic benchmarks that includes well-known StreamIt benchmarks [11] (aud iobeam, t d e p p, f mrad io, f ilt erbank, and beam f ormer) as well as DSP-stone benchmarks [16] (I IR f ilt er and 4 st agel at t ice). The communication traces of the benchmarks are generated by using the task mapping (a) Avg. energy consumption. (b) Avg. comm. latency. (c) Avg. network throughput. (d) Avg. link utilization. Figure 4: Comparison of evaluation results obtained by different approaches based on synthetic communication traces. Fig. 4 illustrates the result comparison based on synthetic communication traces. We conduct 100 groups of experiments for each size of ONoC. In every group of experiment, the sources, the destinations and the volume of communications among processors in a communication demand are randomly generated. The voltage/frequency levels of processors are randomly assigned, consequently resulting in a randomly-generated chip thermal distribution. Our approaches signiﬁcantly improve communication performance in virtue of the minimized contention. The MILP-based approach achieves an average of 32.44% and 12.15% reduction in communication latency than work [17] and the DyXY, respectively. Our heuristic only increases the latency by 4.40% compared to the MILP model. Regarding to the network throughput that denotes the rate of optical packet transmitted through ONoC (measured in packets per second), the MILP-based approach achieves an average of 112.73% and 56.78% improvements than work [17] and the DyXY, respectively. Compared to the two techniques, our heuristic also averagely improves the network throughput by 76.04% and 29.74%. We denote the average ratio of busy links to the total links in an ONoC during a period of time as the average link utilization. Our MILP-based approach increases average link utilization by 71.89% and 24.87% compared to work [17] and the DyXY, respectively. The heuristic only has 6.52% utilization difference compared to the MILP model. In addition, compared to the work [17] that achieves excellent energy efﬁciency with the minimum power consumption, our MILP-based approach and the CAR algorithm only consume an average of 6.49 pJ/pck and 11.1 pJ/pck more energy, resulting in 0.62% and 1.06% extra energy overhead, respectively. Compared to the DyXY, our MILP model reduces 4.18% energy overhead on average. Sec. 2, the time cost of our MILP model is acceptable. Meanwhile, our proposed polynomial-time algorithms are highly efﬁcient with negligible running time (< 1s) compared with the MILP method, especially in large-scale ONoCs. 5 CONCLUSION In this paper, we propose a routing criterion and two routing approaches that are designed for ONoC to guarantee reliability and improve communication performance and energy efﬁciency. Conﬂictinduced communication performance degradation and the energy overheads resulted from thermal tuning are minimized through our approaches based on guaranteed thermal reliability. Evaluations based on synthetic communication traces and realistic benchmarks show the effectiveness of our approaches, compared to state-of-the-art techniques. ACKNOWLEDGMENT This work is supported by NTU NAP M4082282 and SUG M4082087, HP-NTU Digital Manufacturing Corporate Lab, Singapore and NSFC 61772094, China. "
2019,A high-level modeling and simulation approach using test-driven cellular automata for fast performance analysis of RTL NoC designs.,,"A High-Level Modeling and Simulation Approach Using Test-Driven Cellular Automata for Fast Performance Analysis of RTL NoC Designs Moon Gi Seok, Hessam S. Sarjoughian Daejin Park ACIMS lab., Arizona State University, Tempe, USA mseok@asu.edu, Hessam.Sarjoughian@asu.edu Sch. of EE., Kyungpook National University, Rep. of Korea boltanut@knu.ac.kr Abstract—The simulation speedup of designed RTL NoC regarding the packet transmission is essential to analyze the performance or to optimize NoC parameters for various combinations of intellectual-property (IP) blocks, which requires repeated computations for parameter-space exploration. In this paper, we propose a high-level modeling and simulation (M&S) approach using a revised cellular automata (CA) concept to speed up simulation of dynamic ﬂit movements and queue occupancy within target RTL NoC. The CA abstracts the detailed RTL operations with the view of deciding a cell’s state of actions (related to moving packet ﬂits and changing the connection between CA cells) using its own high-level states and those of neighbors, and executing relevant operations to the decided action states. During the performing the operations including connection requests and acceptances, architecture-independent and user-developed routing and arbitration functions are utilized. The decision regarding the action states follows a rule set, which is generated by the proposed test environment. The proposed method was applied to an open-source Verilog NoC, which achieves simulation speedup by approximately 8 to 31 times for a given parameter set. I . IN TRODUC T ION The network-on-chip (NoC) paradigm has been an important design methodology using on-chip communications among large numbers of intellectual properties (IPs) to mitigate the design complexity and scalability issue of a system on a chip (SoC) due to its advantages including communication throughput, ﬂexibility, and scalability. Designers have commonly implemented various NoCs using hardware description languages (HDLs) at register-transfer level (RTL) for automatic synthesis to circuits. Generally, RTL NoC design requires iterative simulations to ﬁnd acceptable communication performance using a set of parameters for the participation of potential IPs or to optimize the design parameters (e.g., input buffer size) for designated IPs. However, slow RTL simulation speed causes reducing the parameter set space or limits having a sufﬁcient parameter exploration for optimization. To increase the simulation speed, the high-level modeling can be a solution. Typically, the highlevel NoC modeling approaches can be divided into two main categories: queuing modeling and architectural- and algorithmlevel hardware modeling using high-level modeling languages, such as SystemC. The queueing modeling studies have tried to develop various parametric equations to observe the average service time and queue utilization based on the queueing theory [1], [2]. The previous studies show relative sound accuracies (≈ 90-92%) Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. ASPDAC '19, January 21–24, 2019, Tokyo, Japan © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6007-4/19/01…$15.00 https://doi.org/10.1145/3287624.3287648 Fig. 1: Proposed CA modeling process using RTL NoC. and negligible computation overhead due to their analytic characteristics comparing with the RTL simulation, which iteratively processes events for signal value transitions at every simulation step. However, the previous studies have limitations in evaluating dynamic changes in packet transmission, such as buffer status, so it is not able to monitor hotspot situations at a speciﬁc time. Moreover, the statistical approach of queueing models cannot guarantee the exact communication performance results compared to the RTL simulation. The architectural- and algorithm-level NoC models are usually developed using SystemC at the early stage of NoC designs. The high-level models have loosely-/approximatedtimed or cycle-level accuracy, and utilized to explore design spaces and verify functionalities for assisting implementation of RTL models as intermediate models [3], [4]. Due to the algorithm-level abstraction of RTL operations, the models increase simulation speed with reduced modeling efforts. However, some operations of ﬁnal RTL models and their high-level intermediate models might differ, so it cannot guarantee matched packet transmission results between two models. Moreover, the high-level modeling requires in-depth knowledge about detailed and speciﬁc operations of target RTL design. In this paper, we propose a high-level and cycle-accurate modeling and simulation (M&S) method for the fast evaluation of packet transmissions of target RTL NoC design using an extended concept of cellular-automata (CA) and the proposed process, as shown in Fig. 1. The proposed methods abstracts detailed RTL operations into 1) cell’s action-state decision based on high-level state (that are ﬂit and capacity information) and 2) executes operations (that are ﬂit fetching and connection change) corresponded to decided action state using a built-in ﬂit-fetching function and user-developed global functions. Compared to the previous algorithm-level modeling, the CA model is reversely derived from ﬁnal RTL design, not an intermediates model toward the RTL design. Without indepth knowledge of all RTL operations, functions of actionstate decision (related to the action-execution timing) reference the derived rules by the proposed test environment using the user-speciﬁed information of CA network and RTL probing signals of cells’ high-level states, and the implementation of global functions requires the knowledge about the routing and arbitration algorithm of target RTL design. The CA network structure is identiﬁed by the designer using the information of RTL arbitration units, FIFO queues, or other units that cause propagation delay due to the ﬂit-storing ﬂipﬂops (FFs). Similar to the conventional CA concept, each cell determines the action state based on neighbors’ and own state at each time step according to a rule [5]. The central CA extensions consist of two parts: 1) neighbors are dynamically determined in runtime to reﬂect the NoC arbitration and 2) each cell has an inﬂuence function, which enables the simulation engine to ﬁnd and schedule only state-changeable neighbors after a cell’s action state is changed in order to prevent unnecessary executions of CA to achieve scalability. A decision rule consist of multiple coupled states of the target cell and its neighbors and associated action states. For the decision-rule generation, the proposed test environment observes RTL signals that are related to cells’ high-level state and action state and stores the pair of high-level and action states during the RTL NoC simulation of randomly injected packets. Moreover, the CA network is validated by checking next position of ﬂits to be moved during the test. After the test, the environment generates compact decision rules through checking a state dependency. The rest of the paper is organized as follows. Section II describes the proposed CA modeling and simulation method. Section III describes the proposed test environment and rule generation. Section IV applies the proposed approach to an open source RTL NoC model. Section V concludes the paper. I I . C EL LU LAR AU TOMATA MODE L ING AND S IMU LAT ION O F NOC RTL D E S IGN A. CA Modeling Perspectives and Formal Speciﬁcations We propose high-level CA modeling for RTL NoC designs, which have the following characteristics. • NoC designs consist of synchronous routers. • NoC designs have a ﬂit-based transmission mechanism, which breaks a large packet into small ﬂits for low latency. All CA can be classiﬁed into two types: a buffer cell (BC ) and a coupling cell (C C ). Each BC has a FIFO buffer to store one or multiple ﬂits. The BC represents queues or intermediate units where each FFs holds a ﬂit and causes cycle-level delay, as shown in Fig. 2(a). The C C is responsible for establishing the connection or disconnection between two speciﬁc BC s among two or multiple adjacent BC s. The cell Fig. 2: Cell allocation, and ff and q value of a BC (BC 0 ). coupling between C C and BC can be established using input and output information of the representing arbiter unit of the target RTL NoC. The speciﬁc high-level states of BC and C C are deﬁned as below. Deﬁnition 1: A high-level state (Sb ) of BC is represented using a 4-tuple < id, F , Q, F T > and a high-level state (Sc ) of C C is represented as a 3-tuple < id, C, P >, where • id : the cell identiﬁcation index; • F : the set for the ﬂit-fetching action states (fs ) of BC , F = {fi , ff }, fi : idle, ff : state of ﬂit fetching at the next clock trigger; • Q : the set of possible number of ﬂits (q ) in the FIFO buffer within BC ; • F T : the set of the front ﬂit types, F T = ∅ ∪ {header, payload, last, single}; • C : the set of connectivity action states (cs ) of C C , C = {cc , cd , cc(0) , cd(1)}, cc : connected state, cd : disconnected state, cc(0) : newly connected state at the current clock cycle, cd(1) : to be disconnected state at the next clock trigger; • P : the id pair of connected BC s, ∅ ∪ {BC.id, BC.id}. The ff (∈ F ) means to bring a ﬂit from previous BC (BC −1 ) to current BC (BC 0 ) at the next clock trigger, and the q(∈ Q) is related to the number of valid output ﬂits, as shown in Fig. 2(b)(c). The RTL signal of a cell’s ff is related to write-enable conditions of an input ﬂit at the clock trigger. The action state of a cell, fs and cs , is decided by checking the current values of its own and neighbors’ state based on the described mapping condition in its decision rule. Depending on the existence the connection states of adjacent C C s, neighbors of a cell can be variously coupled, as Fig. 3(b). For a BC , if an adjacent C C is disconnected, the BC cannot reach another BC across the C C as a neighbor, and an adjacent C C can be a neighbor after requesting a connection. The connection request to next C C is executed by the simulation engine when a ﬂit is fetched and a next C C is not designated. Depending on the number of next reachable Fig. 3: Various neighbor couplings of BC and C C . C C s, multiple connection requests can be performed based on the destination information of the front ﬂit, which is obtained using the global routing function. The current BC (BC 0 ) and current C C (C C (cid:104)−1,0(cid:105) , connecting BC −1 to BC 0 and BC 0 ) can reach BC −1 and BC 1 at most as neighbors. The background philosophy referring three BC s at most is to ﬁnd the relationship between status of sender (BC −1 ) and availability of receiver (BC 0 ), and the receiver’s availability sometimes depends on the action of BC 1 . For example, a BC 0 represents an intermediate unit that has a FF to store a ﬂit. A new ﬂit arrival is possible when the current FF’s output ﬂit is ready to exit at the next clock trigger to prevent the ﬂit loss, the output-ﬂit exit is related to the fs of BC 1 . In general, two cells should be referred to decide the availability. Based on the deﬁned high-level states and neighbor couplings, we deﬁne the decision function of the action state and an inﬂuence function for BC and C C , as shown below. Deﬁnition 2: The simulation engine invokes the BC functions δa and η as well as the C C functions δa and η , where (cid:122) (cid:125)(cid:124) (cid:123)×F 1 → F 0 , the fs decision function; Q−1× • δa : prev . next (cid:122) (cid:125)(cid:124) (cid:123) (cid:122) cur. (cid:122)(cid:125)(cid:124)(cid:123) (cid:125)(cid:124) Q0 (cid:123) (cid:122) (cid:125)(cid:124) (cid:123) C (cid:104)−1,0(cid:105) cur. s (cid:122) (cid:125)(cid:124) (cid:123) next states × (Q0 × F 1 )n , the cs the number of previous/next prev . states (F × Q × F T )−1 )p × • δc : decision function, p/n: neighbor BC s; (cid:122)(cid:125)(cid:124)(cid:123) cur. b/c × (cid:122)(cid:125)(cid:124)(cid:123) prev ./next S 0 Sm b/c ) b/c → {id}r , the inﬂuence function, • η : m: the number of neighbors, r: the number of neighbors whose action state can be changeable due to the current state (S 0 Each of the δa , δc , and η functions has an if-then structure based on corresponding rule-describing conditions of previous, current and next states, and the abstracted structures of those functions are shown in Fig. 4(a)(b)(c). In the decision rules of C C and BC , default conditions are deﬁned for trivial actionchanging situations. After simulation time has advanced, the fs s of all BC s are set to fi before making decisions. Depending on whether previous, current and next states meet any condition of ff , the fs can be newly updated to fs . The cs of C C s can be also updated to cd(1)/c(0) or remain unchanged depending on the Fig. 4: Abstract structures of δa/c and η . current and neighbor’s states. The cc(0) affects the fs state of a BC 1 , but the cd(1) does not affect any neighbor cells. Due to the setting that the fs of all BC s are initialized into fi before action decisions, the cc(0) decision is performed conservatively. In detail, the cc(0) decision is related to the receiver’s availability of a BC 0 , and the availability can depend on the fs of BC 1 . In this situation, if a BC 0 of a CC is actually available at the time, and the δc of the C C is invoked before δa of BC 1 during simulation, cs would be unchanged as cd . This conservative decision leads that the cs as cc(0) would be retained at the clock cycle. On the other hand, cd(1) is determined aggressively, so cs can be set as cd(1) to cc interchangeably at the current time. After the decision of fs or cs and the execution of staterelated operations by the engine, the changed current state can affect some neighbors’ state decisions, and η is deﬁned to provide the information of inﬂuenced neighbor cells from the current change. If the updated current cell state meets any pre-/next conditions of the opposite action of a next/previous cell, the cell is considered as a changeable cell, as shown in Fig. 4(c), and the group of changeable cells is informed to the simulation engine. B. Scheduling-based Simulation Algorithm The CA high-level states, except for cs or fs , are updated by the simulation engine after performing operations of the ﬂit fetching and the connection change. The overall algorithm for the CA state transition at each iteration, is described in Algorithm 1. After the action state decisions of CA end at a current time, the time advances to the next clock-trigger time, and the action transitions of CA are repeated. Algorithm 1: Simulation algorithm at each time step. 1 Lp : a list of previously scheduled CA 2 Ln : a list of newly scheduled CA for decisions 3 Li : a list of inﬂuenced CA /* Move each flit or reflect connection changes */ 4 foreach cell in Lp do if cell type is BC then BC 0 .buffer.insert(BC −1 .buffer.pop()) BC 0 .fs ← fi , BC −1 .fs ← fi if BC 0 .q > 0 and next CC is not designated then request connection(s) to next CC (s) using the global routing function schedule next CC (s) to to Ln schedule BC 0 and BC −1 to Ln else if CC (cid:104)−1,0(cid:105) .cs = cc(0) then CC (cid:104)−1,0(cid:105) .cs ← cc else CC (cid:104)−1,0(cid:105) .cs ← cd /* Decide new fs and connection state 15 Lp ← ∅ 17 18 19 20 16 foreach cell in Ln do if cell is not executed as an inﬂuenced cell then if cell type is BC then execute BC 0 .δa if BC 0 .fs = ff then sched. cell to Lp execute CC (cid:104)−1,0(cid:105) .δc if CC (cid:104)−1,0(cid:105) .cs = cd(1) then sched. cell to Lp else if CC (cid:104)−1,0(cid:105) .cs = cc(0) then update CC (cid:104)−1,0(cid:105) .p using an arbitration function, then schedule cell to Lp 21 22 23 24 25 else schedule cell.η to Li /* Process the influenced cells while Li is not empty do celli ← Li .pop() /* Repeat upper sequential sentences from line No. 17 to 25 */ exec. celli .δa/c , ... and sched. celli .η to Li 5 6 7 8 9 10 11 12 13 14 26 27 28 29 */ */ Fig. 5: Overall test process and environment structure. while executing operations in the ﬁrst phase, is decided if the cell is not already executed as an inﬂuenced cell. The execution priority of the inﬂuenced cells is higher because those cells are determined as changeable cells using η , so early detection of active cells whose action state are ff or cc(0) can eliminate useless iterations. Therefore, the inﬂuenced cells are executed in the while-loop. Depending on the decided action state, cells are scheduled to Lp for the execution of action operations at the next time. I I I . T E S T-DR IVEN RU LE S ET G ENERAT ION U S ING RTL NOC D E S IGN S The purpose of the test environment is to validate the CA network and to provide rules for the CA action-state decision. The overall test process is shown in Fig. 5(a). First of all, the designer identiﬁes the RTL modules that hold the ﬂits using FFs for BC s and arbiter modules for C C s by analyzing a simulation result of a few routers for a small sample of packet transmissions. Second, the designer constructs networks using the simulation result and the input and output information of arbiter modules. Using codes of the RTL modules for BC s, the designer identiﬁes probing RTL signals that correspond to the CA states. Based on the obtained CA network and probing RTL signal information, a network validation and saving pairs of conditions and their associated actions are performed. These are achieved using the proposed test environment with runtime NoC simulation for randomly injected packets. After the simulation, the environment generates decision rules for each cells after reducing the saved condition-action pairs. The overall structure of the test environment is illustrated in Fig. 5(b). To intervene during RTL simulation, the test library should be developed using a programming language interface (PLI) of the simulator, such as the Verilog procedure interface (VPI). The PLI enables an external library to access RTL signals and register functions of the library as callbacks to be invoked during the runtime simulation. The overall algorithm can be divided into two phases: the ﬁrst phase to perform ﬂit-fetching operations and reﬂect the connect change and the second phase to decide fs and cs state. Regarding the ﬂit ﬂow, the ﬁrst phase is to move ﬂits at the clock trigger and the second phase is to identify the ﬂit-fetching BC s at the next clock trigger. After fetching a ﬂit, fs s of related BC s are initialized and connection requests to reachable C C s are performed. In the perspective of connection, the ﬁrst phase removes the connection for new connectivity and stabilizes cc(0) by setting cc to prevent the change of connected BC pair until cs is set to cd(1) . Since the execution order of BC can be different at each iteration, the buffer of each BC has one redundant slot because a ﬂit can arrive before an existing ﬂit being fetched. The fs or cs of each cell in Ln , which is a list of state-changed cells Fig. 6: Overall CA network of target RTL NoC and test-driven state conditions for action decision. To achieve the test purpose, the proposed library references three types of time-related callbacks: a callback called at a speciﬁc time before any RTL signal updates, a callback called after all RTL signals are updated at the time, and a callback called at the end of the simulation. For Verilog, the callback types are cbAtStartOfSimTime, cbReadOnlySynch and cbEndOfSimulation, respectively. At every clock cycle before RTL simulation, a callback in the test library is invoked to check the existence of scheduled packets at the invoked time based on the input scenario. If any scheduled packets exist, the callback requests packet generations to relevant RTL generators by manipulating their input signals. After the current time RTL simulation, a callback is invoked to save states of BC s and connected BC pairs of C C s to identify state conditions of actions. The state of BC s can be determined by checking the values of probing signals, and the connected BC pairs can be obtained by comparing the valid output and input ﬂit values between candidate BC pairs. The connection pair enables us to validate the CA network. The test library manages the current and previous states and connected BC pairs. To detect conditions of cc(0) and cd(1) , the test library compares current BC pairs with the pairs at the previous clock cycle. If the previous connection pair of the C C (cid:104)−1,0(cid:105) does not exist or does not match the current connection, the connected state of the C C is newly changed and the neighbors’ states at the clock cycle are one of the cc(0) conditions. If the previous pair does not match the current pair, the previous neighbor states are one of the cd(1) conditions if the previous fs of the BC 0 was ff , which means that a last transmission has happened at the current clock trigger. If the previous fs of the BC 0 was fi , we do not consider the current neighbor states of the C C as a recent disconnection. After updating the connection pairs of C C , if the default pre-condition of a BC is met, related current and next states are saved as one of the conditions of the BC action. The conditions are managed by multiple rule sets that are shared by multiple cells based on the symmetry of representing RTL units. After testing until the number of conditions of each rule is converged, we can reduce conditions using the following principle: if one or multiple state values of conditions are identical and all possible values of rest states are observed, then the rest states do not need to be considered during the action-state decision. For example, in the situation that two conditions (0, fi ) and (0, ff ) are observed, and fi and ff are all possible cases when the ﬁrst state is 0, the action decision only reference the ﬁrst state. IV. EX PER IM EN TAT ION We applied the proposed M&S approach to an open source Verilog NoC, CONNECT [6]. In this experiment, we targeted an NoC with a 4-by-4 routers topology. We identiﬁed a CA network validated by the test environment, as shown in the Fig. 6(a). There are three kinds of BC s, which represents an input queue, an output queue, and an intermediate unit and two types of C C s, which serve an arbiter to resolve contention between the input ports and another arbiter to resolve contention between the virtual channels (VCs) at output ports. We developed three global functions: a routing function based on the XY routing scheme and two round-robin arbitration functions for the two arbiters. To track the states of BC s, we identiﬁed RTL signals; the input XML for the signals is described in Fig. 6(b). The input and output queue instances of the routers share the same module. The test library was developed using the VPI provided by the ModelSim 10.5c in Quartus®Prime Pro 17.1. Using the test environment with input XML and various packet Fig. 7: Comparison of simulation results between Verilog and CA NoCs. generation scenarios, we obtained various conditions for CA actions, and parts of the conditions are shown in Fig. 6(c)(d). After sufﬁcient random testing and the convergence for the conditions, overall rules of each cell were obtained after the reduction, as shown in Fig. 6(e). Based on the rule set, we developed the high-level CA and simulation engine following the proposed method using C++. To check and debug the equivalence of ﬂit transmission between the Verilog and the CA NoC model, we enabled the NoC CA simulator to share packet generation scenarios with the Verilog test environment and to visualize the F T states of all BC s. For various packet generation scenarios, we conﬁrmed identical arrival times of the packets between the Verilog and CA model, and an example of the matched output ﬂit values of a queue is illustrated in Fig. 7(a). For the simulation speed comparison, we measured the execution time of Verilog and CA NoC models by increasing the number of injecting packets per node, keeping the 0.4 average ﬂit-injection rate per cycle to prevent packet loss caused by trafﬁc congestion. We used an experimental machine whose CPU is Intel®Xeon®E3-1505M 3GHz and memory size is 64 GB. Using the proposed M&S approach, we obtained the speedup from 8.4 to 31.2×, as shown in Fig. 7(b). We observed that the number of increased packets and increased execution time have a linear relationship. Based on the linearity, we deﬁned two parameters, which are ∆t200 to measure the runtime speedup and tinit to measure initialization speedup. Based on the obtained values of ∆t200 and tinit , we can expect that if the number of injected packets goes zero, the CA speedup reaches about 24.6/40.7/55.4× (for the number of VCs is 2/3/4). If the number of injected packets grows arbitrarily large, the speedup will be converged to about 4.8/5.6/6.5×. The proposed M&S method requires the designers to develop the routing and arbitration functions. Depending on the target NoC, various routing and arbitration functions, such as adaptive routing and quality-of-service supporting arbitration algorithms can be implemented. We will validate the ﬂexibility by applying various RTL NoCs. Moreover, we will improve the test environment to support the automatic identiﬁcation of the CA network. V. CONC LU S ION We proposed a high-level M&S method using a newly extended CA to reduce the RTL simulation execution time focusing on packet transmissions. The proposed CA deﬁnes detailed RTL operations in two steps: First, the action states are determined (that are related to the ﬂit-fetching and connection changes) using the deﬁned high-level states of current and neighbor cells based on test-driven rule sets. Second, the detailed operations corresponding to the action states are simulated. For the scalable simulation, we proposed an inﬂuence function to schedule changeable cells to be mainly activated by the simulation engine. Our evaluation showed various CA speedup (up to 31 times) relative to the ﬁxed NoC size and the number of injected packets with valid transmission results. "
2020,Contention Minimized Bypassing in SMART NoC.,"SMART, a recently proposed dynamically reconfigurable NoC, enables single-cycle long-distance communication by building single-bypass paths. However, such a single-cycle single-bypass path will be broken when contention occurs. Thus, lower-priority packets will be buffered at intermediate routers with blocking latency from higher-priority packets, and extra router-stage latency to rebuild remaining path, reducing the bypassing benefits that SMART offers. In this paper, we for the first time propose an effective routing strategy to achieve nearly contention-free bypassing in SMART NoC. Specifically, we identify two different routes for communication pairs: direct route, with which data can reach the destination in a single bypass; and indirect route, with which data can reach the destination in two bypasses via an intermediate router. If a direct route is not found, we would alternatively resort to an indirect route in advance to eliminate the blocking latency, at the cost of only one router-stage latency. Compared with the current routing, our new approach can effectively isolate conflicting communication pairs, greatly balance the traffic loads and fully utilize bypass paths. Experiments show that our approach makes 22.6&#x0025; performance improvement on average in terms of communication latency.",
2020,Maximizing the Communication Parallelism for Wavelength-Routed Optical Networks-On-Chips.,"Enabled by recent development in silicon photonics, wavelength-routed optical networks-on-chips (WRONoCs) emerge as an appealing next-generation architecture for the communication in multiprocessor system-on-chip. WRONoCs apply a passive routing mechanism that statically reserves all data transmission paths at design time, and are thus able to avoid the latency and energy overhead for arbitration, compared to other ONoC architectures. Current research mostly assumes that in a WRONoC topology, each initiator node sends one bit at a time to a target node. However, the communication parallelism can be increased by assigning multiple wavelengths to each path, which requires a systematic analysis of the physical parameters of the silicon microring resonators and the wavelength usage among different paths. This work proposes a mathematical modeling method to maximize the communication parallelism of a given WRONoC topology, which provides a foundation for exploiting the bandwidth potential of WRONoCs. Experimental results show that the proposed method significantly outperforms the state-of-the-art approach, and is especially suitable for application-specific WRONoC topologies.",
2020,Broadcast Mechanism Based on Hybrid Wireless/Wired NoC for Efficient Barrier Synchronization in Parallel Computing.,"Parallel computing is essential to achieve the manycore architecture performance potential, since it utilizes the parallel nature provided by the hardware for its computing. These applications will inevitably have to synchronize its parallel execution: for instance, broadcast operations for barrier synchronization. Conventional network-on-chip architectures for broadcast operations limit the performance as the synchronization is affected significantly due to the critical path communications that increase the network latency and degrade the performance drastically. A Wireless network-on-chip offers a promising solution to reduce the critical path communication bottlenecks of such conventional architectures by providing hardware broadcast support. We propose efficient barrier synchronization support using hybrid wireless/wired NoC to reduce the cost of broadcast operations. The proposed architecture reduces the barrier synchronization cost up to 42.79% regarding network latency and saves up to 42.65% communication energy consumption for a subset of applications from the PARSEC benchmark.",
2020,Reutilization of Trace Buffers for Performance Enhancement of NoC based MPSoCs.,"The contemporary network-on-chips (NoCs) are so complex that capturing all network functional faults at presilicon verification stage is nearly impossible. So, on-chip design-for-debug (DfD) structures such as trace buffers are provided to assist capturing escaped faults during post-silicon debug. Most of the DfD modules are left idle after the debug process. Reuse of such structures can compensate for the area overhead introduced by them. In this work, the trace buffers are reutilized as extended virtual channels for the router nodes of an NoC during in-field execution. Optimal distribution of trace buffers among the routers is performed based upon their load profiling. Experiments with several benchmarks on the proposed architecture show an average of 11.36% increase in network throughput and 13.97% decrease in average delay.",
2020,Co-Exploring Neural Architecture and Network-on-Chip Design for Real-Time Artificial Intelligence.,"Hardware-aware Neural Architecture Search (NAS), which automatically finds an architecture that works best on a given hardware design, has prevailed in response to the ever-growing demand for real-time Artificial Intelligence (AI). However, in many situations, the underlying hardware is not pre-determined. We argue that simply assuming an arbitrary yet fixed hardware design will lead to inferior solutions, and it is best to co-explore neural architecture space and hardware design space for the best pair of neural architecture and hardware design. To demonstrate this, we employ Network-on-Chip (NoC) as the infrastructure and propose a novel framework, namely NANDS, to co-explore NAS space and NoC Design Search (NDS) space with the objective to maximize accuracy and throughput. Since two metrics are tightly coupled, we develop a multi-phase manager to guide NANDS to gradually converge to solutions with the best accuracy-throughput tradeoff. On top of it, we propose techniques to detect and alleviate timing performance bottleneck, which allows better and more efficient exploration of NDS space. Experimental results on common datasets, CIFAR10, CIFAR-100 and STL-10, show that compared with state-of-the-art hardware-aware NAS, NANDS can achieve 42.99% higher throughput along with 1.58% accuracy improvement. There are cases where hardware-aware NAS cannot find any feasible solutions while NANDS can.",
