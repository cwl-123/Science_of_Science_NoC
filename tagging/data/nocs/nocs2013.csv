title,abstract,full_text
"SNet, a flexible, scalable network paradigm for manycore architectures.","A scalable communication paradigm for manycore architectures, called SNet (Scalable NETwork), is presented. It offers a wide range of flexibility by exploring the routing paths in a dynamic way, taking into consideration the network load. It is then followed by the data transmission phase through the chosen path.","SNet, a ﬂexible, Scalable NETwork paradigm for manycore architectures Celine Azar, Stephane Chevobbe, Yves Lhuillier Embedded Computing Laboratory, CEA, LIST, Gif-Sur-Yvette, F-91191, France; celine.azar, stephane.chevobbe, yves.lhuillier@cea.fr Jean-Philippe Diguet Lab-STICC, Universite de Bretagne Sud, Lorient, France; jean-philippe.diguet@univ-ubs.fr Abstract—A scalable communication paradigm for manycore architectures, called SNet (Scalable NETwork), is presented. It offers a wide range of ﬂexibility by exploring the routing paths in a dynamic way, taking into consideration the network load. It is then followed by the data transmission phase through the chosen path. Categories and Subject Descriptors. General Terms. Design Keywords. Manycore architectures, Scalable network I . IN TRODUC T ION Integrating thousands of IPs onto the same die becomes feasible with the 10 nm technology [1], which makes the computational resources cheaper considering their availability. One among the limits of these gigascale SoCs (System-On-Chip) will be the ability to efﬁciently interconnect pre-designed functional blocks and to accommodate their communication requirements in a scalable manner [2]. We propose to dedicate a set of Processing Elements (PE) to achieve routing tasks and a new PE co-processor, called DMC (Direct Management of Communications), to handle data transfers. With simple software libraries uploaded in PE memories, the user deﬁnes a speciﬁc topology, which is based on existing communication links between PEs. Then the routing is dynamic and implemented in a distributed manner in order to create paths from producer to consumer according to data dependencies.The whole concept is called Snet. I I . TH E SN E T CONC E PT The routing strategy in SNet, implemented in the software library, is based on a distributed ACO (Ant Colony Optimization) algorithm in which dynamic paths exploration is achieved in parallel for all communicating tasks. Paths search begins at the destination tasks to prevent round-trips. Destinations broadcast ant agents in the routing network to explore all courses to reach the source tasks following a priority order. If the ant reaches a dead end course, the corresponding path is dropped and the ant erases its traces back from the network. The routing strategy prohibits cyclic paths: a routing node is visited once by an ant, thus preventing deadlocks. All ants compute a cost function, that depends on the network load, and are compared regarding their costs at the source task. The optimal one is kept for data transmission, and the others erase their paths back to clean the network. The ﬁrst occurrence of the paths creation stage is considered as an initialization task that can be sought again at runtime to adapt the routing paths according to the actual system state and application needs. SNet is designed to be applied to any interconnection topology (mesh, torus, fat tree, etc...). In a considered topology, changing the conﬁguration of computing/routing nodes does not affect the implemented programs since the ACO algorithm is totally decorrelated from the physical placement of tasks. To validate the SNet concept, we have developed a manycore platform, which consists of an array of homogeneous RISC cores interconnected with their four neighbor cores in a mesh fashion. More details on the platform and the ACO algorithm are found in [3]. The SNet features are as follows: • Communication is achieved by loading a routing task, running the ACO algorithm, on an arbitrary set of cores. • The ACO algorithm handles dynamically paths creation among remote communicating tasks without usersupervision. • Once routing paths are dynamically created, data transfers are launched and handled by the DMC module. Multitasking in SNet may increase the utilization rate of the cores. Once the routing PEs are done of paths creation, they may switch to run either computing tasks or other control tasks, while the DMCs handle data transfers. They may also be switched off in order to reduce the power consumption. In the actual implementation of SNet, multitasking is not handled, but will be in future work. The DMC co-processor features a link table which is dynamically ﬁlled by the ACO algorithm during paths creation. It manages multiple requests arriving simultaneously while applying a round robin checking scheme. It requires one cycle per data transfer. The router area for SNet is equal to 0.014 mm2 at 40 nm TSMC. It includes one core area, to which we add the limited area of the DMC co-processor equal the overhead of the four shared buffers, linking each core to its four nearest neighbors. In the actual implementation of the platform, the chosen cores have a small footprint of 16 KGates and include a 4 KB instruction memory, and a 4 KB local data memory. When comparing it to state-of-art to 0.162 µm2 , and routers, it takes a middle range value between smaller ones such as XPIPES with 0.0104 mm2 , and larger ones such as AEthereal with 0.0234 mm2 at 40 nm TSMC. We conclude that the advantage of SNet is having a programmable, highly ﬂexible communication network scheme, while presenting a relatively small footprint router. I I I . R E SU LT S We follow hereby an evaluation methodology developed in [4] to extract the characteristics of different routing topologies applied to SNet, by setting ﬁxed simulation parameters to all tested topologies and modeling trafﬁc with Poisson injection distribution. 16 computational nodes are mapped to the platform, as shown in Figure 1, with a varying number of routing nodes. While this work addresses large systems, including thousands of cores, the limited platform dimensions considered hereby serve to characterize the SNet concept with acceptable simulation time. Each node of the platform injects 640 messages of n ﬂits in the network, the ﬂit width being 32 bits. In order to test the limits of the network saturation, n varies from 10 to 1000 f lits, which is equivalent to 25 KB and 2.44 M B respectively. The simulations are conducted on our ISS simulator, which is based on an ISS array of RISC processors and model one instruction per cycle. disposal of the routing tasks. When the computing tasks are distributed in a more-or-less uniform way, it results in a balanced network load. (a) Spatially Uniform Distribution (a) Square (b) Column (c) Cross (d) L (e) U (f) W (g) H (h) Bridge Fig. 1: Routing networks tested on the manycore platform. Dark gray refer to routing nodes, light gray to computing nodes, and white to unassigned cores. Measured latencies for the studied routing topologies under spatially uniform trafﬁc distribution are plotted in Figure 2a, whereas Figure 2-b shows results for best mapping in which each two communicating tasks are placed as close as possible. In the actual implementation, the network presents a bandwidth of 9.6 Gbps at 300 M H z , considering that one transmission can be achieved simultaneously by the DMC module. The minimum message latency depends on the routing paths computed at runtime. The starting value for the message latencies is quite the same for the two distributions, around 25 cycles. Maximum injection rates for spaced mappings range between 0.2 and 0.55, and between 0.4 and 0.9 for the best mappings. The different scored results are mainly due to the number of assigned routing cores. The higher this number is, the more possibilities are available to create routing paths, which reduces contentions. Another criteria lies on the spatial distribution of the computing tasks, which depends on the (b) Best Possible Distribution Fig. 2: Latency variation following the Poisson injection distribution. IV. CONCLU S ION S SNet efﬁciently implement a novel routing paradigm designed for scalability, ﬂexibility, and ease of programmability. It performs path exploration before data transmissions using the ACO distributed algorithm, without interrupting the application running. Results show that, though offering high ﬂexibility and scalability, SNet manages to score high injection rate values reaching 0.9, and low message latencies. In the next project steps, time reduction of the ACO paths creation stage will be feasible by introducing speciﬁc hardware modules for ant agents propagation. Multitasking execution will increase computing efﬁciency and pave the way of dynamic migration. "
3D logarithmic interconnect - Stacking multiple L1 memory dies over multi-core clusters.,"In this paper we propose two synthesizable 3D network architectures: C-LIN and D-LIN, which allow modular stacking of multiple L1 memory dies over a multi-core cluster with a limited number of processing elements (PEs). Two Through Silicon Via (TSV) technologies are used: the state of the art Micro-bumps and the promising and dense Cu-Cu Direct Bonding, with consideration of the ESD protection circuits. Our results demonstrate that, in processor-to-L1-memory context, C-LIN and D-LIN perform significantly better than traditional network on chips and simple time-division multiplexing buses, and they achieve comparable speed vs. their 2D counterparts, while enabling modularity: from 256KB to 2MB L1 memory configurations with a single mask set.","3D Logarithmic Interconnect: Stacking Multiple L1 Memory Dies Over Multi-Core Clusters Erfan Azarkhish, Igor Loi, and Luca Benini DEI, University of Bologna Bologna, Italy erfan.azarkhish@unibo.it, igor.loi@unibo.it, and luca.benini@unibo.it Abstract—In this paper we propose two synthesizable 3D network architectures: C-LIN and D-LIN, which allow modular stacking of multiple L1 memory dies over a multi-core cluster with a limited number of processing elements (PEs). Two Through Silicon Via (TSV) technologies are used: the state of the art Micro-bumps and the promising and dense Cu-Cu Direct Bonding, with consideration of the ESD protection circuits. Our results demonstrate that, in processor-to-L1-memory context, CLIN and D-LIN perform signiﬁcantly better than traditional network on chips and simple time-division multiplexing buses, and they achieve comparable speed vs. their 2D counterparts, while enabling modularity: from 256KB to 2MB L1 memory conﬁgurations with a single mask set. IN TRODUC T ION AND R E LAT ED WORK S I . Cluster-based many core design with a limited number of processors in each cluster, sharing tightly-coupled L1 data memories (TCDMs) has gained interest recently in programmable accelerators and embedded systems such as ST-Microelectronics Platform 2012/STHORM [1]. While network-on-chips (NoCs) are suitable for inter-cluster communication, cross-bar based interconnects seem to perform much better for intra-cluster communication since they provide a uniform and ultra-low latency memory access [2], [3]. However, since the reusability of custom designed crossbars such as [3] is limited across different technology nodes, synthesizable IPs are highly desirable in this context. Moreover, the advent of 3D technology has provided new opportunities to increase design modularity and reduce latency as well as manufacturing cost. In this paper, we take advantage of 3D technology to increase the shared L1 memory size in a modular fashion, i.e. stacking memory dies on top of a logic die, without the need to re-spin silicon. We focus on a single cluster with a typical size (16 PEs) sharing a tightly coupled multi-banked L1 memory, and propose two synthesizable 3D network architectures: C-LIN and D-LIN. While simulation results conﬁrm the superiority of LIN-based interconnects over traditional network on chips (NoC) and buses in processor-to-L1-memory context, post-layout results show that the these architectures increase design modularity over their 2D counterpart (2D-LIN), and provide the opportunity for further cost optimizations, while achieving comparable speed. 3D EX T EN S ION S TO 2D -L IN I I . The basic 2D Logarithmic Interconnect (2D-LIN) illustrated in Fig.1a) is designed following the Mesh Of Trees approach [4]. Centralized 3D-Logarithmic-Interconnect (C-LIN) and Distributed 3D-Logarithmic-Interconnect (D-LIN) are two extensions of the 2D-LIN to be integrated in a 3D-stacked Chip multiprocessor and to overcome the 2D limitation by splitting the design into one logic layer and several memory layers, and stacking them over each other. As illustrated in Fig.1b, in CLIN, the 2D design is cut at the memory interface, therefore, PEs and the interconnection network are placed on the logic layer, while memory modules along with small decoding logics are placed on memory layers. This way the logic and memory elements are completely separated and different technologies and optimizations may be utilized for design of the logic and memory dies. In the other alternative, D-LIN, 2D design is cut at the PE interface (See Fig.1c), therefore, interconnect is distributed among the layers. This will reduce the number of TSVs, since the number of slave channels is usually more than master channels (in order to reduce memory access conﬂicts). I I I . EX P ER IM EN TA L R E SU LT S Our baseline 2D-LIN conﬁguration is composed by 16 (STmicroelectronics xP70 ASIP RISC) PEs, sharing on-chip TCDM with 32 memory banks. Our design ﬂow is based on the STM CMOS-28nm Low Power technology library, with a Multi VT H synthesis ﬂow with Synopsys Design Compiler Graphical (2011.09), place and route with Cadence SoC Encounter Digital Implementation (10.1), and the sign-off tasks in Primetime (2011.09). Micro-bumps (with 40μm×50μm pitch) and Cu-Cu Direct Bonding (with 10μm × 10μm pitch) have been adopted from [5] (Capacitive load of TSVs is assumed to be 30fF), and ESD protection, glitch removal, and boot-time conﬁguration circuits to allow for identical layouts have been implemented as well (See Fig.2). Fig.3 compares execution time between LIN and a highperformance NoC [6] under random trafﬁc patterns. When WorkingSetSize is very small NoC performs slightly better than LIN. However, as PEs start to access remote banks, NoC’s execution time increases rapidly, while LIN starts to beneﬁt from load distribution among multiple banks. Table I compares performance between LIN, aforementioned NoC, and a simple bus, while some executing image processing benchmarks. In all cases LIN performs better than NoC, and bus results are an upper bound to the execution time. Post layout areas are also shown in table II. Fig.4a illustrates the silicon area (mm2 ) for 2D and 3D implementations. As can be seen, the 2D die size increases with the embedded on-chip SRAM, except for the ﬁrst two conﬁguration where PE obstructions and design geometry dominate the total design area. The resulting layouts after full placement and routing for D-LIN (Cu-Cu direct bonding) are depicted in Fig.5. Timing results are depicted in Fig.4b. It can be seen that, CLIN and D-LIN improve the performance over 2D-LIN with Logic Layer Mem Layer 1 Mem Layer 2 PE C Data MUX D Data MUX MM Bank1 Layer  Dcd T I Data D O Data Arbitration O Data MUX Data MUX MM Bank1 Layer  Dcd T I PE C Data MUX D Data MUX MM Bank2 Layer  Dcd T I Data O Data Arbitration O Data MUX Data MUX MM Bank2 Layer  Dcd T I 1 2 3 4 5 6 7 W I R I N G W I R I N G W I R I N G Request Response Data MUX Tree Response Block Address  Decoder Req Data MUX Tree Response Block Address  Decoder Req Test  &  Set Address Decoder Resp DMA PE Arbitration Tree Arbitration Tree Request Block l k Flow  Control Test  &  Set Address Decoder Resp DMA PE Arbitration Tree Arbitration Tree Request Block l k Flow  Control W I R I N G PE1 PEp Bank1 MM1 Bankm MMm a) b) c) Data MUX Data MUX Arbitration + Data Arbitration + Data MM Bank1 Layer  Dcd MM Bank2 Layer  Dcd 5 C C O O T T I I MM Bank1 Layer  Dcd T I MM Bank2 Layer  Dcd T I Logic Layer Mem Layer 1 Mem Layer 2 1 2 3 4 6 PE PE W I R I N G W I R I N G Request Response Fig. 1. Block diagrams of 2D-LIN (a), C-LIN (b), and D-LIN (c) e c a F t e c a F o Carrier c i e g t o a r L t s b u S D i O e To BGA Pads 0 Configuration MemLayers +1 LayerID T S V T S V T S V +1 LayerID T S SS V T S S V T S S V +1 LayerID T S V T S V T S V Req Block O C Resp Block T S V e c a F t k c a B o M e O m D i t a r 1 t s e b u S M e O m D i t a r 2 t s e b u S M e O m D i t a r 3 t s e b u S T S V T S V T S V Operation e S i 2 e e S i 2 S i 2 S i 2 T S V T S V I TMM Dcd T S V T S V I TMM Dcd T S V T S V I TMM Dcd PE Eni D I i t c a n e v e e v l t i E c A y a e Layer i  Data D l m n e t Enj Layer j  Data D V S T Glitch Removal Circuirty Fig. 2. Structure of the 3D stacking and conﬁguration circuitry TABLE I. B ENCHMARK EX ECU T ION T IM E COM PAR I SON Benchmarks  LIN Execution Time (ms)  AMAT (ns)  NoC Execution Time (ms)  AMAT (ns)  Bus Execution Time (ms)  AMAT (ns)  FAST  5.59  6.54  8.21  7.53  46.40  81.30  CT  SIFT  79.89  4464.07  6.47  6.46  106.92  4943.43  7.09  6.57  730.51 30799.99  82.40  81.90  the same memory size by small factors of 6.7% and 3.7% respectively, which is due to the large capacitive load of TSVs and overheads of the protection circuitry. This experiment shows that current TSVs are not yet scaled enough to provide a major performance boost over 2D planar designs. IV. CONC LU S ION We presented two synthesizable network architectures, CLIN and D-LIN which can be integrated with 3D Stacking technology to provide access to tightly coupled shared memory banks stacked over multi-core clusters. Our results demonstrated that in processor-to-L1-memory context, LIN outperforms both traditional NoCs and simple time-division multiplexing buses. Our modular design strategy allowed for stacking of multiple memory dies without the need for different masks, and offered better scalability with a similar performance compared to the 2D designs. It can be concluded that, even though the current TSVs are still not much better in terms of speed than global on-chip wires, they can provide more freedom in heterogeneous integration of dies with costoptimized technologies, since they are deﬁnitely much better than traditional off-chip links. ACKNOW L EDGM EN T This work has been supported by VIRTICAL EU project (CA n. 288574) and Phidias EU project (CA n. 318013). 120  110  100  90  80  70  60  50  40  0  50  100  150  Working Sets Size (KBytes)  200  250  o T t a l c e x E u i t T n o i m e ( S μ ) LIN  NoC  16KBytes  Fig. 3. Performance comparison between NoC and LIN TABLE II. PO S T- LAYOU T AR EA COM PAR I SON Interconnect  LIN  NoC-3.6GHz [8]   MIRA (3DM) [7]  MIRA (3DM-E) [7]  NoC-5.1GHz [6]  Cardinality Area (mm2)  (p=16,m=32)  0.09  4x4  0.29  4x4  0.40  4x4  0.98  4x4  1.02  18.0 16.0 14.0 12.0 10.0 8.0 6.0 4.0 2.0 0.0 256K 512K 1M 2M 256K 256K 256K 256K L Die M Die L Die M Die L Die M Die L Die M Die uBUMP Cu-Cu uBUMP Cu-Cu C-LIN D-LIN 2D 3D A r a e [ m m 2 ^ ] L Die: Logic Die M Die: Memory Die Memory Size 500 450 400 350 300 250 200 150 100 50 0 256K 512K 1M Memory Size 2M Up to 2M Up to 2M Up to 2M Up to 2M uBUMP Cu-Cu uBUMP Cu-Cu C-LIN D-LIN (8 Mem Layers) 2D 3D M x a i m u m [ y c n e u q e r F M H ] z a) b) Fig. 4. Silicon area (mm2 ) (a), Maximum frequency (MHz) (b)       	           	        	                      Fig. 5. Physical implementation of D-LIN: Landing pads (RDL), Logic Die (Cu-Cu Direct bonding), Memory die "
Backward probing deadlock detection for networks-on-chip.,"To accurately detect deadlocks in Network-on-Chip (NoC) as early as possible, a novel deadlock detection mechanism called Backward-probing Deadlock Detection (BDD) is proposed in this work, which can detect and resolve all existing deadlocks. It was realized using probe systems that generate probes for deadlock detection. A probe system includes a probe System Manager (SM) for turning on probe system, a probe Generator (GEN) for generating probes, a Link Selection (LS) connected to a Switch Allocation (SA), which is used for copying the generated probes, transmitting probes backward, and discarding probes when the probes find that the traversal path is just a congestion not a deadlock or when probe congestion occurs. There is also a TB Calculation (TBC) in LS for TB settings. Finally, a probe comparator (PB Comparator) is used for claiming deadlocks. Note that each port except the local one in a router has its own probe system.","Backward Probing Deadlock Detection for Networks-on-Chip Yean-Ru Chen†, Zi-Rong Wang†, Pao-Ann Hsiung‡, Sao-Jie Chen†, and Meng-Hsun Tsai§ (cid:41) To accurately detect deadlocks in Network-on-Chip (NoC) as early as possible, a novel deadlock detection mechanism called Backward-probing Deadlock Detection (BDD) is proposed in this work, which can detect and resolve all existing deadlocks. It was realized using probe systems that generate probes for deadlock detection. A probe system includes a probe System Manager (SM) for turning on probe system, a probe Generator (GEN) for generating probes, a Link Selection (LS) connected to a Switch Allocation (SA), which is used for copying the generated probes, transmitting probes backward, and discarding probes when the probes ﬁnd that the traversal path is just a congestion not a deadlock or when probe congestion occurs. There is also a TB Calculation (TBC) in LS for TB settings. Finally, a probe comparator (PB Comparator) is used for claiming deadlocks. Note that each port except the local one in a router has its own probe system. Router 1 i GEN SM ii iii LS SA A B Router 2 S A L S PBvc_1 SA iv C LS SA 4 _ c v B P A S A S S L G E N S M D L S S A S A P B v c _ 2 SA Switch Allocation GEN PB Generator SM LS PB System Manager Link Selection Data packets which  contributes Deadlock Data packets from  opposite direction Data request direction Probe transferred direction SA PBvc_3 SA LS SM GEN M S N E G Router 3 Router 4 Fig. 1. Example of Probe Trace. Suppose that there exists a deadlock cycle currently in a NoC. As shown in Figure 1, the data requesting (cid:129) †Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan, 10617, R.O.C. (cid:129) ‡Department of Computer Science and Information Engineering, National Y.-R. Chen is the corresponding author. E-mail: d95943037@ntu.edu.tw (cid:129) §Department of Computer Science and Information Engineering, National Chung Cheng University, Chiayi, Taiwan, 621, R.O.C. Cheng Kung University, Tainan, Taiwan, 701, R.O.C. direction in the deadlock is counterclockwise. There are 4 steps from i to iv to show the probe trace from generation to backward transmission. Step i shows that the probe system is turned on by SM in Router 1, and then a probe is generated by GEN. In step ii, the generated probe is sent to the LS of port A. Then LS sends the probe to the SA which includes arbiters to compete with other packets in step iii. If a probe transmission is requested, it will be ﬁrst sent to the probe buffer named PBvc 1 in Router 2. In stepiv , the LSs make copies of the probe and transmit the copied probes in the backward direction to the direction of A’s channel-requestor buffers which are all full, that is, transmitting along the directions of B and D , respectively. Note that C is also A’s channelrequestor buffer but it is not full, thus the probe should not be backward transmitted along the direction of C . In the following description, we will introduce the rules for BDD. We ﬁrst introduce the channel waitfor graph (CWG) proposed by Warnakulasuriya and Pinkston [2], which is useful for describing and proving our proposed method. The authors of [2] have proved that if there is a deadlock in the network, there is a cycle in the corresponding CWG. Here we assume that there already exists a deadlock cycle in a NoC. In the corresponding CWG, a blocked packet held by input channel c is requesting to be transmitted to an output channel c(cid:2) . In other words, c is one of c(cid:2) ’s requestors in CWG. Let Creq = {c|c ∈ requestors of c(cid:2) }, and Pset ={p|p ∈ Creq ∧ p’s buffers are all full}. The rules for BDD are described as follows. 1) Rule 1: Turn on probe system by SM if all of the following three conditions are satisﬁed. a) C on1.1 : The input buffers in c which contain packets requesting to for output via c(cid:2) are all full. b) C on1.2 : Packets in input buffers mentioned above are all blocked. c) C on1.3 : When C on1 and C on2 hold for longer than a given threshold time TT O . 2) Rule 2: After turning on the probe system, when either one of the following conditions is satisﬁed, then probes are generated by GEN. a) C on2.1 : If this port does not send any probe during the period of the probe system turned-on, BACKWARD PROBING DEADLOCK DETECTION FOR NETWORKS-ON-CHIP 2 TABLE 1 Improvements from Comparing BDD with Lee and Cycle-Based in Conventional NoC and FA-BiNoC. R-Sat P.(%) c.Lee c.cycle I-Sat T.(%) c.Lee c.cycle Pattern Ti = 16 Uniform NoC FA-BiNoC NoC Hotspot FA-BiNoC Regional NoC FA-BiNoC 98.9 99.2 0.0 99.6 90.0 87.1 99.1 99.7 98.9 99.7 92.6 89.9 6.2 3.1 0.1 5.7 6.3 2.8 6.2 5.9 3.4 5.7 11.3 3.2 R-Ave P.(%) c.Lee c.cycle 46.2 49.4 12.6 43.5 32.5 44.5 51.7 53.5 24.9 51.7 41.7 53.6 a probe will be generated at threshold time Ti . b) C on2.2 : Otherwise, the probe system will send a probe at threshold time Ti2 . 3) Rule 3: Copy probes by LS for backward transmission. The number of probe copies is equal to |Pset |. 4) Rule 4: Backward transmit the generated and/or copied probes by LS to the directions of the channels in Pset according to the following two conditions. a) C on4.1 : When c receives a probe from c(cid:2) , check if the probe system is turned on by Rule 1. b) C on4.2 : The port of channel c has either never claimed deadlock or has not claimed deadlock in the past Twait period of time. 5) Rule 5: Discard probes by LS when either one of the following conditions is satisﬁed. a) C on5.1 : The probe will not be transmitted to the directions of those channels which are not in Pset . This is a concept of probe cancellation. In real implementation, the probe will not be copied at those directions in this case. b) C on5.2 : If the downstream probe buffer is full, the incoming probe will be discarded. 6) Rule 6: Assume that the dimension of c is dim1 and the dimension of c(cid:2) is dim2. When the port generates a probe, the TBs of the probe will be set by TBC in LS according to the following two conditions. a) C on6.1 : When dim1 = dim2, TBs are set to be zeros. b) C on6.2 : When dim1 (cid:4)= dim2, both T B{dim1,dir(c)} and T B{dim2,dir(c(cid:2) )} will be set 1 (true), where dir(c) denotes c’s direction and dir(c(cid:2) ) denotes c(cid:2) ’s direction. 7) Rule 7: Each port has a PB Comparator. This PB Comparator will claim that a deadlock is detected when the following two conditions hold. a) C on7.1 : Four turn bits are set 1. b) C on7.2 : When all the considered buffers satisfy Rule 1, SM will send the satisfaction information to PB Comparator. We have proved that if there is a real deadlock in a NoC, BDD can detect it. We implemented a cycleaccurate simulation environment in HDL language, Verilog, and then synthesized it into gate level using Synopsys Design Compiler under UMC 90 standard cell library at a ﬁxed clock speed for both the conventional NoC and BiNoC router architectures with a fully adaptive routing algorithm. On the conventional NoC, each port in a router has 4 virtual channels (VCs), while each VC contains an 8-ﬂit space. There are 10 ports for data transmission on a router, which includes 5 input ports and 5 output ports. Buffer size of each virtual channel is 32 ﬂits. There is one virtual channel for probe transmission on each port, and it contains 8-ﬂit space, the same as the data VC. Each router has 4 probe VCs (PBvc). On the FABiNoC, channel direction is reconﬁgurable, so there are totally 10 bidirectional channels in a router. We set TT O to have 8 clock cycles, Ti to have four different cycles: 16, 32, 64 and 128. Ti2 has 128 clock cycles, Twait has 8 clock cycles, and TR has 16 clock cycles. Note that TR is the time for deadlock resolution. The simulation environment was comprised of 8 × 8 nodes, connected as a mesh array. Each packet had a constant length of 8 ﬂits. The total simulation time for each injection was 50000 clock cycles. In uniform trafﬁc, a node can be every other node’s destination with equal probability. In hotspot trafﬁc, 4 nodes are set to be hotspots, where 20% of the total packets are sent from these 4 nodes. In regional trafﬁc, 90% of total packets are sent to the destination at distance less than 3 hops, and most transmission occurs among two neighboring routers. Table 1 is used to show the improvements calculated from comparing BDD with Lee [1] and cyclebased methods in conventional NoC and fully adaptive routing algorithm BiNoC (FA-BiNoC). BDD can achieve an average of 53.6% reduction in deadlocked packets and 5.9% increase in throughput at the trafﬁc saturation point. ACKNOW LEDGMEN T This work was partially supported by National Science Council, ROC, under grant NSC 101- 2220-E-002-008 and NSC 101-2221-E-006-235. "
Leveraging the geometric properties of on-chip transmission line structures to improve interconnect performance - A case study in 65nm.,"Implementation of low energy, low latency transmission line interconnects on a network-on-chip presents the circuit designer with a variety of structural design choices. This work presents a study of the comparative effects of changing the wire geometries on the latency, energy dissipated, area, and noise properties of the transmission lines. These results will aid the engineer in the design and performance analysis of the global interconnect and foster a quantitative understanding of the wave signaling properties in the RLC regime.","Leveraging the geometric properties of on-chip transmission line structures to improve interconnect performance: A case study in 65nm Shomit Das1 , Georgios Manetas2 , Kenneth S. Stevens1 , Roberto Suaya2 1University of Utah, 2Mentor Graphics Corporation Abstract— Implementation of low energy, low latency transmission line interconnects on a network-on-chip presents the circuit designer with a variety of structural design choices. This work presents a study of the comparative effects of changing the wire geometries on the latency, energy dissipated, area, and noise properties of the transmission lines. These results will aid the engineer in the design and performance analysis of the global interconnect and foster a quantitative understanding of the wave signaling properties in the RLC regime. Energy dissipation in wires has become a primary bottleneck for the continued scaling of future interconnect networks. Packet switching on high radix network topologies exacerbates this problem due to the energy and latency overhead contributed by switches and routers. Alternative network fabrics utilizing low latency, low energy buses have shown promise in circumventing this problem [1]. Transmission lines (TL) are possibly the most suitable building blocks for bus-based topologies since they provide fast, low energy long distance communication using present generation CMOS devices and techniques. Reliable on-chip implementation of TL interconnects requires substantial analysis and careful design, often with the help of complex full-wave ﬁeld solvers. Characterization of transmission lines is a well studied subject [2] [3] [4]; however, a direct relation between the physical and geometric properties of a TL link design and its effectiveness as a communication medium is not available. This work aims at providing an insight into the various trade-offs between the performance of global RLC wires and their geometric properties. The case study presented here is intended to serve as a guide for link designers to better understand the available design choices supplemented with numerical data. RC representations of long wires are rendered inaccurate due to high frequency inductance effects. Electromagnetic ﬁeld solvers are required for analyzing the behavior of transmission lines in order to include second order effects such as substrate coupling. Since accuracy trumps solution time in this study, a 3D full wave solver is employed for the analyses [5]. Scattering (S) parameters are extracted from the TL geometries for a frequency range of 100 MHz to 100 GHz and written to a Touchstone ﬁle for over 50 different transmission line structures. Transient analyses are performed on these extracted values in Synopsys HSPICE using CMOS drivers and loads. The back end of the line (BEOL) arrangement from the Global Foundries 65nm LPE process, complete with a hierarchical dielectric structure, low resistivity substrate and dense lower metal layers, is used as the guide to set up the simulation environment. Tab. I summarizes the parameters corresponding to this BEOL stack. The coplanar and microstrip transmission line conﬁgurations as shown in Fig. 1 are studied. The design parameters in this study are the widths of the signal and TABLE I: BEOL stack information Parameter Metal layers Top metal thickness Permittivity lower dielectric Permittivity higher dielectric Values 6 (4 normal + 2 thick) 1 µ m 3.0 3.6 ground paths, and the spacing between them. All evaluations are made for 5mm links, which is the typical length for medium to long distance communication on a modern chip. Transient simulations are performed at frequencies of 5 GHz, 10 GHz, 15 GHz, and 20 GHz. Energy consumption in a transmission line link is reduced due to the absence of repeaters. They are eliminated by the wave signaling nature of data propagation. Our results show that increasing the frequency of data transfer does not increase the average power dissipated in the link; therefore operating at higher speeds results in lower energy per transferred datum. The simulations also show that changing the signal wire width has very little effect on the energy dissipated in the link as seen in Fig. 2a. Therefore, the width of the signal line can be increased to boost the signal integrity or decreased for area considerations without having to worry about the energy budget. Fig. 2d shows that wider ground wires bring better coupling between the signal and return paths and therefore marginally reduce the energy dissipation. Also to be noted is the fact that the microstrip conﬁguration of transmission lines demonstrates better energy performance as compared to the coplanar conﬁguration. A major win in the argument for adoption of TLs as global interconnects is the near speed-of-light velocity of signal propagation. The propagation delay depends on the effective permittivity of the dielectric surrounding the wires, which relates to the arrangement of the wires and therefore differs in the cases of coplanar and microstrip topologies. The effective permittivity changes signiﬁcantly for microstrip lines when the signal width is varied, therefore changing the latency. Microstrip lines have nearly identical latencies for all signal width values under consideration as shown in Fig. 2b. Conversely, Fig. 2e shows that when the ground wire width changes, the latency of both conﬁgurations only decreases slightly. Simulations show that coplanar lines outperform microstrip lines in the matter of link latency. One of the fundamental issues with implementing transmission lines over low resistivity silicon substrates is their S G G S G Fig. 1: Cross section of microstrip and coplanar waveguides Energy Per Transfer E e n r y g e p r r t n a f s e r ( p ) J 0 2 4 2 4 6 Conductor Width (µ m) 0 8 (cid:3) (cid:3) (cid:3) (cid:3) (cid:3) (cid:3) (cid:3) ◦ ◦ (cid:52) (cid:52) ◦ ◦ (cid:52) ◦ ◦ (cid:52) ◦ (cid:52) (cid:67) (cid:67) (cid:67) (cid:67) (cid:67) (cid:67) (cid:67) (cid:52) 20G (cid:67) 15G ◦ 10G (cid:3) 5G Coplnr µ strip (a) Energy versus conductor width scaling Transmission Line Latency L a t y c n e e p r m m ( p ) s 8 13 18 2 4 6 Conductor Width (µ m) 0 8 (cid:3) (cid:67) (cid:3) (cid:67) (cid:3) (cid:67) (cid:67) (cid:3) (cid:3) (cid:67) (cid:3) (cid:67) (cid:52) ◦ (cid:52) ◦ ◦ (cid:52) ◦ (cid:52) ◦ (cid:52) ◦ (cid:3) (cid:67) ◦ (cid:52) 20G (cid:67) 15G ◦ 10G (cid:3) 5G Coplnr µ strip (b) Latency versus conductor width scaling Transmission Line Eye Height E e y H e i h g t ( V ) 0 0.3 0.6 2 4 6 Return Path Width (µ m) 0 8 (cid:3) (cid:3) (cid:3) (cid:3) (cid:3) (cid:3) (cid:67) ◦ (cid:52) ◦ (cid:52) ◦ ◦ (cid:52) (cid:52) ◦ ◦ (cid:52) (cid:52) (cid:67) (cid:67) (cid:67) (cid:67) (cid:67) (cid:52) 20G (cid:67) 15G ◦ 10G (cid:3) 5G Coplnr µ strip (c) Eye height versus ground width scaling Energy Per Transfer E e n r y g e p r r t n a f s e r ( p ) J 0 2 4 2 4 6 Return Path Width (µ m) 0 8 (cid:3) (cid:3) (cid:3) (cid:3) (cid:3) (cid:3) ◦ (cid:52) ◦ (cid:52) ◦ ◦ (cid:52) (cid:52) ◦ ◦ (cid:52) (cid:52) (cid:67) (cid:67) (cid:67) (cid:67) (cid:67) (cid:67) (cid:52) 20G (cid:67) 15G ◦ 10G (cid:3) 5G Coplnr µ strip (d) Energy versus ground width scaling Transmission Line Latency L a t y c n e e p r m m ( p ) s 8 12 16 2 4 6 Return Path Width (µ m) 0 8 (cid:67) (cid:3) (cid:3) (cid:67) (cid:3) (cid:67) (cid:67) (cid:3) ◦ (cid:52) (cid:52) ◦ ◦ (cid:52) (cid:52) ◦ (cid:3) (cid:67) (cid:3) (cid:67) (cid:52) ◦ (cid:52) ◦ (cid:52) 20G (cid:67) 15G ◦ 10G (cid:3) 5G Coplnr µ strip (e) Latency versus ground width scaling Transmission Line Eye Width E e y W i d t h ( p ) s 0 100 200 2 4 6 Return Path Width (µ m) 0 8 (cid:3) (cid:3) (cid:3) (cid:3) (cid:3) (cid:3) ◦ (cid:52) ◦ (cid:52) ◦ ◦ (cid:52) (cid:52) ◦ ◦ (cid:52) (cid:52) (cid:67) (cid:67) (cid:67) (cid:67) (cid:67) (cid:67) (cid:52) 20G (cid:67) 15G ◦ 10G (cid:3) 5G Coplnr µ strip (f) Eye width versus ground width scaling Fig. 2: Energy, latency, and eye height and width as conductor and ground widths are scaled sensitivity to noise. In this work, eye diagram analysis is used to evaluate the channel noise characteristics. The eye height is the measure of the amplitude ﬁdelity of the channel. On the other hand, the eye width captures the effect of frequency dependent losses on the rise and fall times of the transmitted signal. Simulations show that the eye height progressively decreases as bit rate increases. Dielectric losses become dominant close to 20 GHz and result in the closing of the eye. Eye height values remain fairly constant with variations in conductor width. Changing the width of the return path has a remarkable effect on the eye height. For example, a 65% gain in eye height is observed when the ground wire width of a microstrip line is increased from 2 µ m to 8 µ m at 15 GHz, as seen in Fig. 2c. Microstrips perform decidedly better than coplanar wires in the timing ﬁdelity metric. They exhibit a wider eye as well as more robustness to changes in conductor width as compared to coplanar lines. Wider return paths provide better coupling and consequently improve the eye width of the received signal, as observed in Fig. 2f. In addition to requiring thick, top level metal layers for reliable implementation, TL wires have larger pitches when compared against minimum pitch RC wires. This is due to the minimum constraints imposed on signal and return path dimensions along with spacing requirements to ensure correct transmission line behavior [6]. Coplanar lines have return paths on either side of the signal path on the same metal layer which results in wider pitches. Such a conﬁguration provides excellent noise rejection from aggressor lines on the same layer, but suffer from crosstalk induced by wire currents on vertically adjacent layers and the substrate. Conversely, microstrip lines have return paths on a separate metal layer than the signal paths. The pitch of this conﬁguration depends on the return path width. Smaller pitch microstrip lines are more susceptible to crosstalk from neighboring wires on the same metal layer. Also, this arrangement necessitates the use of two consecutive top metal layers running in the same direction on the chip, which is a high price to pay for global interconnects. The purpose of this work is to aid the designer in making suitable choices from the available design space to achieve link performance goals. If energy is the primary concern, a microstrip line running at a high data rate is the optimal choice. To decrease the propagation delay value, the correct choice is to employ coplanar wires with wider return paths. Wide return paths also provide better noise resistance to the TL medium. If there are a number of aggressor lines on the same metal layer, a coplanar wire is more suitable; whereas microstrips provide better resistance from substrate coupled noise as well noise from dense, lower layer wires. These design guidelines enable the network-on-chip architect to make informed choices in order to maximize the performance of the low latency, low energy transmission line channel. "
Energy-efficient adaptive wireless NoCs architecture.,"With the increasing number of cores in chip multiprocessors, the design of an efficient communication fabric is essential to satisfy the bandwidth and energy requirements of multi-core systems. Scalable Network-on-Chip (NoC) designs are quickly becoming the standard communication framework to replace bus-based networks. However, the conventional metallic interconnects for inter-core communication consume excess energy and lower throughput which are major bottlenecks in NoC architectures. On-chip wireless interconnects can alleviate the power and bandwidth problems of traditional metallic NoCs. In this paper, we propose an adaptable wireless Network-on-Chip architecture (A-WiNoC) that uses adaptable and energy efficient wireless transceivers to improve network power and throughput by adapting channels according to traffic patterns. Our adaptable algorithm uses link utilization statistics to re-allocate wireless channels and a token sharing scheme to fully utilize the wireless bandwidth efficiently. We compare our proposed A-WiNoC to both wireless/electrical topologies with results showing a throughput improvement of 65%, a speedup between 1.4-2.6X on real benchmarks, and an energy savings of 25-35%.","Energy-efﬁcient Adaptive Wireless NoCs Architecture Dominic DiTomaso, Avinash Kodi, David Matolak, Savas Kaya, Soumyasanta Laha, and William Rayess School of Electrical Engineering and Computer Science Ohio University, Athens, OH 45701 E-mail: dd292006, kodi, matolak, kaya@ohio.edu Abstract—With the increasing number of cores in chip multiprocessors, the design of an efﬁcient communication fabric is essential to satisfy the bandwidth and energy requirements of multi-core systems. Scalable Network-on-Chip (NoC) designs are quickly becoming the standard communication framework to replace bus-based networks. However, the conventional metallic interconnects for inter-core communication consume excess energy and lower throughput which are major bottlenecks in NoC architectures. On-chip wireless interconnects can alleviate the power and bandwidth problems of traditional metallic NoCs. In this paper, we propose an adaptable wireless Network-on-Chip architecture (A-WiNoC) that uses adaptable and energy efﬁcient wireless transceivers to improve network power and throughput by adapting channels according to trafﬁc patterns. Our adaptable algorithm uses link utilization statistics to re-allocate wireless channels and a token sharing scheme to fully utilize the wireless bandwidth efﬁciently. We compare our proposed A-WiNoC to both wireless/electrical topologies with results showing a throughput improvement of 65%, a speedup between 1.4-2.6X on real benchmarks, and an energy savings of 25-35%. I . IN TRODUC T ION The shrinking of silicon technology has given rise to chip multiprocessors (CMPs) that integrate hundreds to thousands of cores on a single chip. The traditional bus-based networks which connect these cores do not scale well due to high energy and latency bottlenecks. Additionally, with higher clock frequencies, the dissipated power rises and more clock cycles are required for data to traverse the bus. Network-onChip (NoC) designs are the response to the limitations of bus-based networks [1]. NoCs can provide high bandwidth communication for CMPs. However, the metal wires that connect cores in conventional NoC designs suffer from high energy costs and long propagation delays due to routers and intermediate hops, respectively. Additionally, the multi-hop communication of traditional NoC topologies such as a mesh or torus further increase power and latency [2]. Even though metal wires have limitations at long distances, they can still be highly effective and suitable for short distance communication. A 1 mm metal wire in 32 nm complementary metal-oxidesemiconductor (CMOS) technology consumes a low energy of 0.18 pJ/bit. One potential solution is wireless interconnects that can alleviate the limitations of metal wires by providing low latency and energy efﬁciency [3], [4], [5], [6], [7]. The unique beneﬁts of wireless interconnects include: (1) high energy efﬁciency for long, one-hop communication, (2) reduced complexity compared to systems with waveguides or wires, and (3) compatibility with complementary metal-oxide-semiconductor (CMOS) wireless technology designs. Wireless interconnects can be used to transmit data across the chip in one-hop with low energy. The work by Chiang et al. [4] used wireless interconnects operating at 2 pJ/bit to create long distance (30 cm) links between computing chassis. The RF-Interconnect [7] placed an radio frequency (RF) transmission line in a zig-zag pattern to transmit packets quickly across the chip at 1.2 pJ/bit. Ganguly et al. [3] created a hybrid network that organized cores into subnets in which communication within a subnet was wired and communication between subnets used 0.33 pJ/bit wireless links with a total 512 GHz bandwidth. The hybrid WCube design [5] used a wired mesh on one tier and a wireless hypercube network on the second tier with wireless transceivers operating at 4.5 pJ/bit with a 512 GHz bandwidth. Lastly, iWISE [6] was a hybrid network which distributed 1 pJ/bit wireless links to avoid additional hops to centralized hubs. These related works used long links to quickly propagate data across the chip at very low energies and designed hybrid networks to provide an additional 512 GHz wireless bandwidth without the area overhead and complexity of metal wires. However, each of these related NoCs used ﬁxed wireless links that cannot adapt to dynamic trafﬁc patterns during runtime. With the limited wireless spectrum, it is critical that wireless links be fully utilized by adapting them to trafﬁc patterns. Therefore, we propose to use energy efﬁcient transceivers in our Adaptable Wireless NoC Architecture (A-WiNoC) to create a one-hop, low power design while improving network performance through adaptable transceivers. With the limited wireless spectrum, we use wired links for local communication while reserving wireless links for global communication. Wireless interconnects have been proven to be a low energy alternative to prior work; however, the inherent adaptability of wireless links have not been utilized before. Often, channel bandwidth can be under-utilized in real applications due to dynamic trafﬁc patterns. We use adaptable wireless links to improve performance by adapting them to trafﬁc demands, thereby, efﬁciently utilizing network resources. The major contributions of this work include: (1) Adaptability: We use adaptability to give more bandwidth to hot spots caused by dynamic trafﬁc patterns. Our adaptive algorithm reallocates transmission time slots to these high trafﬁc spots to lower contention and improve performance. (2) Energy Efﬁcient Devices: We show that trends in various emerging fabrication technologies such as sub-50nm RF-CMOS and SiGe BiCMOS are moving towards wireless transceivers with the energies and data rates near the NoC requirements of ∼1 pJ/bit and ∼32 Gbps. (3) Evaluation on Real Benchmarks: We evaluate our novel A-WiNoC architecture compared to other wired/wireless networks on the benchmarks PARSEC, SPLASH-2, and SPEC CPU2006 by collecting traces from SIMICS and GEMS [8]. We evaluate the network throughput and show an improvement of up to 65% as well as a speedup between 1.4X and 2.6X. Using the Synopsys Design Compiler, A-WiNoC was estimated to have an energy savings of 25% over a wireless network and up to 35% over electrical networks. I I . ADA P TAB L E W IR E L E S S NOC ARCH I T EC TUR E A. NoC Design Architecture: The trends of wireless transceivers (Section III) show very low energies and high data rates making them ideal for our NoC architecture. The proposed architecture called AWiNoC: Adaptable Wireless NoC Architecture is shown in Figure 1(a). The architecture has a total of N cores where N=64 in this paper. To minimize energy dissipation and reduce packet latency, we concentrate four cores by connecting to a single router [9] (for N=64, N/16 cores are concentrated). Routers are organized into sets in order to systematically distribute static and dynamic wireless links. Figure 1(a) shows the set organization. Each set has N/4 cores - Set 0 has cores 1 to N/4, Set 1 has cores N/4+1 to N/2, Set 2 has cores N/2+1 to 3N/4, and Set 3 has cores 3N/4+1 to N (Also seen in the simpliﬁed Figure 1(b)). This creates four sets, each with four routers. Each router has four transmitters: Tij , which indicates a transmitter from Set i to Set j. All the routers in each set share these four wireless transmitters. As explained in [6], the choice of four routers and four sets optimizes wireless channel sharing by giving a set an opportunity for every router to transmit to a different set. Additionally, since we have 16 wireless channels available, the choice of four total sets each with four transmitters was made to evenly distribute wireless bandwidth. Therefore, the four routers share four transmitters for wireless communication between sets. Figure 1(a) also shows the wired/wireless connections between routers. These routers are placed on the chip in a grid-like fashion. Wired links connect the routers in a mesh topology. Wired links are, therefore, used for short distances because short metal wires consume low energy and have lower propagation delays compared to long metal wires. Additionally, diagonal wired links are used to fully connect routers within a set. This reduces the total wireless spectrum requirement while still maintaining a single hop network. Deadlocks: Our network avoids deadlocks by routing packets to their destination in one hop. If a packet’s source node is exactly one wired hop away from its destination node, then a wired link is used. Otherwise, if the source is farther than one wired link, then a single wireless hop is used in order to reduce packet latency and power. Therefore, a packet will always take at most one hop from source to destination (wired or wireless) and deadlocking can be avoided as there is no circular dependency for packet transmission. Wireless Communication  Tij = Transmitter from            Set i to Set j on            frequency fij  i,j ϵ {0, 1, 2, 3}  Adaptable  Transmitter from  Set 0 to Set j  Wired Communication  Router  Metal Wire  T0j  Set 2  Set 0  Set 1  Set 3  T02 T03  T01  T0j  T02 T03  T01  T0j  T02 T03  T01  T0j  T02 T03  T01  T0j  T12 T13  T1j  T10  T12 T13  T1j  T10  T12 T13  T1j  T10  T12 T13  T1j  T10  T32 T3j  T31  T30  T32 T3j  T31  T30  T32 T3j  T31  T30  T32 T3j  T31  T30  T2j T23  T21  T20  T2j T23  T21  T20  T2j T23  T21  T20  T2j T23  T21  T20  Router 0  Router 1  Router 2  Router 3  …  N/16 Cores  (a) Detailed Architecture Set 2  Set 0  Set 1  Set 3  0  …  1  …  2  …  3  …  4  …  5  …  6  …  7  …  8  …  9  …  10  11  12  13  14  15  Wireless  Transmitter  Static  allocation  Dynamic  allocation  Router  T02 T03  T01  T0j  Tij = Transmitter from            Set i to Set j on            frequency fij  i,j ϵ {0, 1, 2, 3}  N=total number of  cores  T13  T10  T1j  T12  T2j  T21  T20  T23  T31  T30  T3j  T32  Core  …  …  …  …  …  …  N/16  0  N/4  N  N/2  N/4+1  3N/4  N/2+1  3N/4+1  (b) Wireless Communication Fig. 1: Adaptable wireless architecture showing (a) router and transceiver organization and (b) the logical wireless communication between sets. Communication: The proposed adaptable wireless NoC architecture uses statically and dynamically conﬁgured wireless channels for communication between routers. The architecture uses 16 wireless channels as there are 16 routers. Each wireless channel has their own unique set of carrier frequencies. With a total available bandwidth of 512 GHz, each wireless channel has a data rate of 32 Gbps. There are 12 static wireless channels which are used to transmit packets at low energy (see Figure 1(b)). Static channels allow the network topology to be connected at all times. An additional, four adaptable wireless channels can be adapted based on trafﬁc patterns to give additional bandwidth to certain portions of the chip. Four adaptable wireless channels are used so that each set has at least one adaptable channel. More than four adaptable channel can be used; however, this will unnecessarily add to the complexity of the network. The total 16 wireless channels are shared among multiple transceivers which are replicated at each router (see Figure 1(a)). However, to avoid interference, a time division multiplexing (TDM) scheme is used to ensure that multiple transceivers do not use the same wireless channel simultaneously. This virtually creates more wireless links from   the 16 wireless channels without increasing the total wireless bandwidth. Therefore, multiple transceivers are distributed at each router to share wireless communication and improve network performance by reducing hot spots. For wireless communication, each set has four transmitters. Three transmitters are used as static communication and one transmitter can be adapted to any set. For example, in Set 0 of Figure 1a, T01 , T02 , T03 are statically allocated from Set 0 to Set 1, Set 2, and Set 3, respectively. T0j can be adapted to any Set 1-3. The transmitters are replicated at each router in the set to avoid additional hops to a centralized wireless hub. Transmitters T01 , T02 , T03 , and T0j are replicated at routers 0-3 in Set 0. Figure 1(b) shows a simpliﬁed version of A-WiNoC to illustrate the wireless communication. Each set has four shared transmitters. The notation Tij is used to indicate a transmitter from Set i to Set j. For example, Set 0 in Figure 1(a) uses the four transmitters: T01 , T02 , T03 , and T0j . For each transmitter, Tij , a unique set of frequencies, fij , is allocated to avoid interference. Therefore, with four sets the total number of wireless channels is 16. Three transmitters are statically conﬁgured to the other three sets which are shown as solid arrows. This ensures that all the sets are always connected. One transmitter is adaptable, shown as a dotted arrow, and can transmit to any set depending on the trafﬁc pattern. The thin black lines in Figure 1(b) show that each router has all four transmitters available for transmission. However, only one router can use a single transmitter at a time. For example, in Set 0, router 0 can use any of the four transmitters in Set 0, but not at the same time as routers 1-3. Tokens: Since multiple routers in a set have transmitters tuned to the same wireless channel, time division multiplexing (TDM) is used to assign time slots to a router. Time slots indicate when a router can use a certain transmitter in order to avoid interference. Time slots are assigned by implementing a token sharing scheme. Tokens are passed between routers and represent the right to transmit on a certain wireless channel. When a router possesses a token, it is immediately given a time slot and starts transmitting data. If no data needs to be transmitted, it passes the token to the next router. Tokens were used because they can be quickly passed between routers so that routers do not wait long to transmit data. There are 16 tokens representing the 16 wireless channels. Since each set shares four wireless channels, only four tokens need to be passed between the routers within a set. Figure 2 shows one example of communication for Set 0 and Set 1 across two cycles. For Set 0, the four tokens, 01, 02, 03, and 0j are passed between routers 0-3 where j indicates a adaptable token that can be used to send to any set 1-3. For this example, Router 3 has the token to transmit to Set 3. Router 3 will transmit to every router in Set 3. Each router will look into the packet header, compare the packet destination with its own address, and either accept or reject the packet. This is called single write multiple read (SWMR). Likewise for router 2, the packet will be transmitted to all routers in Set 2 and the correct destination will accept the packet. This approach will consume more power; however, it will reduce the number of hops for the packet. Router 0 in Figure 2 has heavy trafﬁc going to Set 1. Therefore, it can use the token for its static transmitter as well as the token for its adaptable transmitter to double the data rate to Set 1. After each transmitter sends a packet in cycle 0, it will immediately pass on the token. Routers which need the token will capture it and transmit in cycle 1. In Figure 2, during cycle 1, the tokens 01 and 0j are captured and used. Tokens 02 and 03 are idle since no router currently requires transmission to Sets 2 and 3. For Set 1 in Figure 2, routers 4, 6, and 7 use their tokens to transmit to Sets 0, 2, and 3, respectively. During cycle 1, routers 5 and 6 capture tokens to transmit to Sets 0 and 2, respectively. Router 7 can capture tokens 13 and 1j again in the next cycle if no other routers require the tokens. To scale A-WiNoC to a higher number of cores, such as 256 or 512, more cores per set can be added. As the maximum wireless spectrum is being used, the number of wireless channels will remain at 16. Therefore, the set organization and number of transmitters remains the same while the number of cores attached to the transmitters will increase. Wireless communication with tokens and the reconﬁguration algorithm (explained in the next Section) is the exact same as the 64 core version. For example, at 256 cores, there will be 64 cores in each set connected via a wired mesh. Four wireless transmitters will be shared by 16 cores via a direct wired connection. Four cores are concentrated to a single router as before; however, each router is directly connected to a wireless router. Flow control for all network sizes will be managed by credits which can be piggybacked onto packets. B. Adaptability We adjust the duty cycle, or the duration in which our wireless links transmit. When signal duty cycle increases for any link, if the signaling rate stays constant, throughput increases proportionally. The increased throughput afforded by a duty cycle increase comes at the expense of a proportional increase in dissipated power, and of course at the expense of throughput for other links that are time-multiplexed on the same frequency channel. Unlike previous wireless NoC architectures, we take advantage of the inherent adaptability of wireless interconnects. Adaptability is used in our architecture to give more bandwidth to sets with the most trafﬁc. The A-WiNoC architecture reconﬁgures time slots to the adaptable transmitter. Time slots are deﬁned as cycles in which a transmitter can send data and are allocated by the passing of tokens. The global controller (GC) determines to which set an adaptable transmitter should allocate its resources. The local controller (LC) collects statistics on each link utilization and indicates to which set the adaptable transmitter should reconﬁgure. Each LCi is attached to one of the four wireless transmitters. Each LCi uses hardware counters to collect historical statistics. Each time a packet is sent, each LCi updates the counter, Linkutil . At the end of the reconﬁguration window, Rw , each LCi sends Linkutil to the GC. Rw equals 100 cycles in this paper. Set 2  Set 0  Set 1  Set 3  Router 2  Router 3  01  0j  2X data rate to Set 1  03  02  Router 0  Router 1  X  X  X  X  X  X  X  X  X  X  X  X  SWMR  Set 0:  ij  i3  i0  i1  i2  Token from  Set i to Set 0  Set i to Set 1  Set i to Set 2  Set i to Set 3  Adaptable -  Set i to Set j  Cycle 0  Set 2  Set 0  Set 1  Set 3  Router 2  Router 3  01  02  0j  03  Router 0  Router 1  X  X  X  X  X X  Cycle 1  Set 1:  Set 2  Set 0  Set 1  Set 3  10  2X data rate  to Set 3  13  1j  12  SWMR  X  X  X  X  X  X  X  X  X  X  X  X  Router 6  Router 7  Router 4  Router 5  Set 2:  Set 3:  Set 2  Set 0  Set 1  Set 3  10  13  1j  12  X  X  X  X  X  X  X  X  X  X  X  X  Router 6  Router 7  Router 4  Router 5  Fig. 2: Example of the token scheme for communication in Set 0 and Set 1 for two cycles. The GC compares the data and determines which Set has the highest utilization. GC then communicates with LC3 , which is attached to the adaptable transmitter, to reconﬁgure to the set with the highest utilization. The pseudo code for the adaptive algorithm is shown in Algorithm 1. GCs evaluate statistics and re-allocate resources for the current reconﬁguration window, Rw , based on the previous Rw . After Rw , in Step 2, the GC will send a LinkRequest control packet to all LCi , requesting utilization data. In Step 2a, each LCi will update the ﬁeld in the LinkRequest packet with the Linkutil information. The Linkutil information is the number of link traversals on the outgoing links and will be reset to zero to prepare for the next Rw . The LinkRequest packet is returned back to the GC. In Step 3, GC receives LinkRequest packet containing the utilization information for all outgoing links for the previous Rw . In Step 3a, GC separates each Linkutil for each outgoing set: S et0util , S et1util , S et2util , and S et3util . The GC is able to separate utilization into different sets by knowing which set each transmitter sends to, including the adaptable transmitter. In Step 3b, GC ﬁnds the highest set utilization by using comparators and the utilization information from each set. The set with the maximum utilization should be the set to which the adaptable transmitter reconﬁgures. In Step 4, GC sends a LinkResponse control packet to Algorithm 1 Adaptive Algorithm Step 1: Wait for reconﬁguration window, Rw Step 2: GC sends LinkRequest control packet to all LCi Step 2a: Each LCi computes the Linkutil for previous Rw and updates the ﬁeld in the LinkRequest packet and returns back to GC Step 3: GC receives LinkRequest packet containing information for all outgoing links Step 3a: GC separates each Linkutil for each outgoing set: S et0util , S et1util , S et2util , and S et3util , Step 3b: GC ﬁnds max[S et0util , S et1util , S et2util , S et3util ] Step 4: GC sends LinkResponse control packet to adaptable transmitter, Tij . LinkResponse ∈ 00, 01, 10, 11, where 00 indicates maximum utilization is Set 0, 01 is Set 1, 10 is Set 2, and 11 is Set 3. Step 4a: Transmitter Tij reallocates time slots to set with maximum utilization by only accepting packets for that outgoing set Step 5: Go to step 1 LC3 which is attached to the adaptable transmitter. The LinkResponse packet requires two bits which will contain 00 if Set 0 has the highest utilization, 01 for Set 1, 10 for Set 2, and 11 for Set 3. In Step 4a, the adaptable transmitter , Tij , reads the LinkResponse packet and reallocates time slots to the set with the maximum utilization. By reallocating time slots, Tij will only accept packets destined for the reconﬁgured set during at least the time frame of Rw at which the algorithm is repeated. Fig. 3: Trends found in RF-CMOS transceivers designed for lowpower and short-range links for WiNoC system requirements. Data adapted from [10], [11], [12]. Fig. 4: Power ampliﬁer trends in integrated transmitters implemented using compound (III-V) and silicon-based (SiGe HBT and CMOS) devices. Data collected from [17], [18], [19], [20] I I I . TR END S O F W IR E L E S S TRAN SC E IV ER S As wireless NoC (WiNoC) is an emerging technology, the most practical guideline to assess the viability of WiNoC technology is to refer to trends in important ﬁgures of merits measured for ultra-low power and short range CMOS transceivers in literature (See Figure 3). In this summative plot, both data rate and link distance are plotted as a function of modulation energy efﬁciency, which must be lower than 1 pJ/bit for WiNoC systems to be able to compete with wired links. It appears that both ﬁgures can be extrapolated with systems, i.e. a typical link distance ≤1 cm and data rates ≥30 an acceptable certainty to meet the requirements for WiNoC Gbps. While these objectives are not trivial to achieve, it is reassuring to note that they are within the reach of general trends in RF-CMOS, especially when the closest data points are considered for the link distance that use 65nm CMOS generation. For low-power CMOS integration on silicon, the ongoing adaptation of sub-90nm RF-CMOS back-end solutions for vehicular radar at 77 GHz will be a critical starting point as it will provide a complete technology base with on-chip antennas as well as compact transceivers that can reach massmarkets [13]. Encouraged by recent demonstration of a 410 GHz oscillator based on 90nm CMOS devices [14] and empowered by ongoing device scaling, RF-CMOS circuitry will play a central role in the ultra low power integration up to 600GHz [15]. For the acceptable noise and gain performance beyond 150 GHz the use of SiGe BiCMOS technology, which integrates ultrafast SiGe heterojunction bipolar transistors (HBT) with sufﬁcient gain performance, will be crucial in an otherwise purely CMOS architecture [16]. Such hybrid SiGe BiCMOS solutions, already popular for high-throughput optical modulators operating around 30 Gbps, is the most practical route to surmounting the impasse between ultra-low power performance and high frequency operation. To illustrate this trend, we refer to Figure 4 which shows measured DC power dissipation at state-of-the-art PAs based on highperformance III-V devices (high electron mobility transistors HEMTs), SiGe HBTs and RF-CMOS technology, as a function of carrier/modulation frequency. SiGe HBTs are more suitable for WiNoCs due to their power levels and material engineering techniques on silicon bipolar transistors compared to high performance III-V HEMTs with poor integration potential. While CMOS devices do not yet match the frequency response needed for LNA/PA designs around 500GHz, the ongoing device scaling and process reﬁnement appears to scale up the frequency response exactly at the right direction. Additionally, circuit engineering and better understanding of devices in a a given technology generation can bring about signiﬁcant reduction in power levels, thus making CMOS circuits a very strong contender for WiNoC implementation in the long term. The trend lines in Figure 3 show that CMOS circuits are moving towards target WiNoC data rates near 32 Gbps and energies near 1 pJ/bit. IV. AN T ENNA CON S ID ERAT ION S The THz ’performance gap’ is especially evident in antenna design [21]. On one side of the gap, the existing WLAN RFCMOS radios use off-chip antennas because of their relatively low frequency of operation (≤5.4GHz). On the other side, on-chip optical networks utilize infra-red free-space solutions with nanostructures used as efﬁcient nanoantennas for resonant absorption. Also, the dominant applications in the 1 THz range have been medical and security imaging technologies that can do without on-chip integration. However, thanks to the vehicular anti-collision radar and multi-media driven indoor wireless network applications in the 60-90 GHz range, onchip antenna solutions have recently gained momentum [13]. Thus, while much less developed compared to other pieces of the WiNoC puzzle, compact on-chip antenna solutions also appear to be within the reach of silicon mm-wave integration as will be discussed in more detail below. The easiest case for the design will be when frequency is large enough to employ conventional antenna theory. Even in this case, analysis of mutual coupling and pattern deformation due to the WiNOC landscape may still be needed, which would require a rigorous 3D FDTD simulation that can only happen after actual digital ﬂoor design and wireless router placement. For low/moderate operating frequencies, additional power must be transmitted to compensate for the reduced (l (cid:5) λ). For an example, a patch antenna of area 0.9 mm2 , antenna efﬁciency when the antennas are “electrically small” mounted on a CMOS substrate and operating at 60 GHz, was analyzed and measured in [22] with gains ranging from ∼ 7 dB to -9 dB. Use of such an antenna at both Tx and Rx would require from 14 to 18 dB larger transmit power than if an omnidirectional antenna of gain 0 dB were used. Thus increasing antenna gain (directivity) is a prime concern which cannot be tackled via traditional approaches such as large aperture antennas or arrays due to size limitations. Moreover, high gain antennas are also needed for time-frequency resource reuse that rely on spatial isolation. Luckily, several novel solutions can be adapted for compact high gain antennas including special materials as in [21], where a micro-strip patch antenna design with gain ∼ 8 dB was obtained with approximately 70% radiation efﬁciency in the THz band, using a metamaterial substrate. V. P ER FORMANC E EVA LUAT ION In this section, we compare A-WiNoC to electrical NoC designs including mesh, Concentrated Mesh (CMesh), and Flattened Butterﬂy (FB) architectures and the wireless network WCube. A packet size of four 64 bit ﬂits was used. For a fair comparison, the bisectional bandwidth for all networks was kept the same. Additional cycle delays were added for wired links longer than 5 mm. We assume a total wireless bandwidth of 512 GHz [5], [6]. Therefore, with the 16 channels in AWiNoC, each wireless link is 32 Gbps. For open-loop measurement, we varied the network load from 0.1-0.9 of the network capacity. The simulator was warmed up under load without taking measurements until steady state was reached. Then a sample of injected packets were labeled during a measurement interval. The simulation was allowed to run until all the labeled packets reached their destinations. For closed-loop measurement, the full executiondriven simulator SIMICS from Wind River with the memory package GEMS was used to extract trafﬁc traces from real applications. The Splash-2, PARSEC, and SPEC CPU200 workloads were used to evaluate the performance of 64core networks. We assume a 2 cycle delay to access the L1 cache, a 4 cycle delay for the L2 cache, and a 160 cycle delay to access main memory. The energy and area results for the NoC components were estimated using the Synopsys Design Compiler with the 40 nm TSMC technology library. In the following sections, we will compare A-WiNoC to other networks by providing energy and area estimates along with speedup and throughput simulation results. A. Throughput and Latency Figure 5 shows the throughput and latency for the 64 core networks for four different mixes of synthetic trafﬁc. Mix 0 is a mix of non-uniform (NUR), matrix transpose (MT), and neighbor (NBR) trafﬁc. Mix 1 is NUR, bit reversal (BR), and perfect shufﬂe (PS). Mix 2 is uniform (UN), butterﬂy (BFLY), and MT. Lastly, mix 3 is UN, BR, complement (COMP), and PS. For each mix, the trafﬁc randomly switches between the different patterns every 500 cycles. 4T-1A is A-WiNoC as described earlier with 4 transmitters per set; 1 of which is adaptable (4T-1A). R=100 indicates that the reconﬁguration window is 100 cycles. 4T serves as our non-adaptable baseline in which 4T is A-WiNoC with 4 transmitters per set; none of which are adaptable. For mix 0, 4T-1A shows an increase in throughput between 7% and 65%. For mix 1, 4T-1A shows an increase in throughput between 7%-46%. Both of these mixes use NUR trafﬁc which creates a hot spot. The main reason for the increase in throughput is mainly due to the reconﬁguration algorithm which gives more bandwidth to hot spots. For mix 2, 4T-1A shows a decrease of 11% in throughput compared to FBﬂy and mesh. This is due to the more uniform mix of trafﬁc patterns which is beneﬁcial for the long links of FBﬂy and the non-concentrated mesh network. A uniform mix balances the load across all links, thereby having few under-utilized links. However, 4T-1A still increases throughput by at least 29% over 4T, CMesh, and WCube due to the BFLY and MT patterns in the mix. For mix 3, 4T-1A shows a throughput higher than all other networks. As the trafﬁc changes between four patterns, the reconﬁguration algorithm adapts the network accordingly. The latency plots show the networks saturating at a similar point as the throughput plots. However, the low load latencies show that the wireless links of 4T and 4T-1A consistently have a lower latency than the other networks. Networks such as mesh have high hop counts and networks such as FBﬂy have long wired links which cause high latencies. B. Speedup Figure 6 shows the speedup on real applications for a miss status handling registers (MSHR) that allow 2 requests at a time per core. Simulations for a MSHR=4 and 8 were also evaluated, but not shown. A core sends a 1 ﬂit request to another core which will send back a 4 ﬂit response. For a MSHR of 2, 4T-1A has an average speedup of 2.59X over mesh as well as a 48% improvement over WCube. This is mainly because of the one-hop diameter of A-WiNoC which is possible due to our architecture utilizing long wireless links and our fair token scheme. The performance of 4T-1A and 4T are similar due to the overall uniform pattern and low trafﬁc load of many of the benchmarks. The uniform nature of the Splash-2 benchmarks leave few links under-utilized. On the other hand, the adaptability of 4T-1A improves the performance over 4T for the slightly less uniform PARSEC and SPEC CPU2006 benchmarks. As the MSHR increases from 2 to 8, the network load will be increasing. This results in 4T-1A improving its average speedup over 4T from 4.4% (MSHR=2) to 8.5% (MSHR=4) to 11.1% (MSHR=8). Although the improvement of the reconﬁguration is increasing with network load, the improvement of A-WiNoC relative to the other networks is decreasing. The speedup of 4T-1A over mesh decreases from 2.59X (MSHR=2) to 2.17X (MSHR=4) to 1.4X (MSHR=8). This decrease in improvement may be due to the type of utilization used in the reconﬁguration algorithm. 0.1 0.12 0.14 0.16 0.18 0.2 0.22 0.24 0 0.2 Offered Load  0.4 h T r u p h g u o t t i l f ( o c e c y c s / l / r e ) 4T-1A Mesh FBfly 4T WCube CMesh 0 0.01 50 100 150 200 250 300 350 400 0.11 Offered Load  0.21 a L t y c n e ( s e c y c l ) FBfly CMesh Mesh 4T-1A 4T WCube (a) Mix 0 0.1 0.12 0.14 0.16 0.18 0.2 0.22 0 0.2 Offered Load  0.4 h T r u p h g u o t t i l f ( o c e c y c s / l / r e ) 4T-1A 4T FBfly WCube Mesh CMesh 0 0.01 50 100 150 200 250 300 350 400 0.11 Offered Load  0.21 a L t y c n e ( s e c y c l ) FBfly CMesh Mesh 4T-1A 4T WCube (b) Mix 1 0.1 0.12 0.14 0.16 0.18 0.2 0.22 0 0.2 Offered Load  0.4 h T r u p h g u o t t i l f ( o c e c y c s / l / r e ) FBfly Mesh 4T-1A 4T CMesh WCube 0 0.01 50 100 150 200 250 300 350 400 0.06 0.11 Offered Load  0.16 a L t y c n e ( c y c l s e ) FBfly CMesh Mesh 4T-1A 4T WCube (c) Mix 2 0.1 0.12 0.14 0.16 0.18 0.2 0.22 0 0.2 Offered Load  0.4 h T r u p h g u o t t i l f ( o c e c y c s / l / r e ) 4T-1A WCube FBfly Mesh 4T CMesh 0 0.01 50 100 150 200 250 300 350 400 0.06 0.11 Offered Load  0.16 a L t y c n e ( s e c y c l ) FBfly CMesh Mesh 4T-1A 4T WCube (d) Mix 3 Fig. 5: Throughput and Latency for different mixes of trafﬁc with trafﬁc changing every 500 cycles. 0 0.5 1 1.5 2 2.5 3 3.5 B a r s e n T F F O n a e c R s o d a i t i y R y a r t e c a W a t e r b z i p e s a b c c g h m m e r o h c s k c a b l . l l f n a d u i i m . r f q e m i e n s w p a i t s n o A e v r e g a p u d e e p S 4T-1A Mesh FBfly 4T CMesh WCube Fig. 6: Speedup on real applications. C. Energy Figure 7 shows the energy of each network for each of the trafﬁc patterns. The energy consumption of a whole ﬂit traversing a wireless link, a 5 mm wired link, a 5x5 crossbar and a buffer are shown in Table I. 4T-1A has an average energy savings of 35% over CMesh. The main reason for the savings is due to the use of the low energy wireless links. A-WiNoC shows a reduction in electrical wire energy dissipation for all trafﬁc patterns. Furthermore, 4T-1A has an average energy savings of approximately 25% over WCube. This savings is due to the higher ratio of wireless transmission compared wired transmissions in A-WiNoC. By using a token sharing scheme, more wireless links can be used compared to the centralized wireless hubs of WCube. However, the many wireless links of A-WiNoC increases the router inputs and outputs, thereby, increasing the crossbar size and energy. This causes A-WiNoC to have the largest router energy dissipation for most trafﬁc patterns. However, the one-hop nature of AWiNoC reduces the number of crossbar traversals. Overall, the slight increase in router energy can be compensated for by the large savings in link energy. Across different trafﬁc patterns, A-WiNoC improves energy over FBﬂy between 7% for BFLY trafﬁc and 58% for MT. The differences across different trafﬁc patterns is due to the total number of wired link traversals in each network. In trafﬁc patterns such as MT and COMP, there is a high percentage of long distance trafﬁc. With many packets traversing from one edge of the chip to the other, the energy dissipation due to wired links will be high in the electrical networks. However, in A-WiNoC the low energy wireless links can be utilized more and there will be a large energy savings. WCube is also a wireless network, but the centralized wireless hubs create more electrical hops as packets must route from the source to the wireless hub then from another wireless hub to the destination. In trafﬁc patterns such as BFLY, there is less long distance trafﬁc. This type of trafﬁc causes the energy dissipation of the electrical networks to be lower and more competitive with A-WiNoC and WCube. D. Area Table I shows the area estimates for a wireless link, a 5 mm wired link, a 5x5 crossbar used in mesh, and a buffer for a ﬂit. For the wireless transceiver area, from our study of existing                                     ) J n ( t e k c a P r e p y g r e n E 2.5 2 1.5 1 0.5 0 Wireless Router Wired l f y h h e A s s b 1 B e e u M M T F C 4 C W l f y h h e A s s b 1 B e e u M M T F C 4 C W l f y h h e A s s b 1 B e e u M M T F C 4 C W l f y h h e A s s b 1 B e e u M M T F C 4 C W l f y h h e A s s b 1 B e e u M M T F C 4 C W l f y h h e A s s b 1 B e e u M M C T F 4 C W l f y h h e A s s b 1 B e e u M M T F C 4 C W UN  NUR  BR  BFLY  COMP  MT  PS  Fig. 7: Energy breakdown for different trafﬁc patterns for A-WiNoC and other wireless/wired networks. TABLE I: Power and Area estimates from Synopsys Design Compiler with the 40 nm TSMC library for a 64 bit ﬂit. Wireless Link 5 mm Wired Link 5x5 Crossbar Packet Buffer Energy (pJ) 64 102 7.5 4.0 Area (mm2 ) 0.05-0.10 0.0394 0.0273 0.002949 trends, we estimate the transceiver area to be between 0.05 mm2 and 0.1 mm2 . A-WiNoC will have a total network area increase of 1.7-2.2X over the mesh network and an increase between 1.8-2.4X over FBﬂy. This increase is due to the area of the wireless links and the increase in router size. A router in A-WiNoc will be between 11x11 to 13x13 ports depending on its location in the topology. This area increase is the trade-off for the throughput, speedup, and energy beneﬁts. V I . CONC LU S ION S The trends in wireless technologies have shown that onchip wireless interconnects are a potential solution to alleviate the higher power and latency of metallic NoCs. We proposed a one-hop, hybrid architecture called A-WiNoC which uses adaptable wireless transceivers with low energies (∼1 pJ/bit) and high data rates (∼32 Gbps). We design a reconﬁguration algorithm to adapt to trafﬁc patterns and a token sharing scheme to fully utilize wireless bandwidth. Our results on real applications show a 1.4-2.6X speedup and our energy estimates from the Synopsys Design Compiler show an energy savings of 25-35% over wireless and electrical networks. ACKNOW L EDG EM EN T S We thank the anonymous reviewers for their excellent feedback. This work was partially supported by the National Science Foundation grants CCF-0915418, CCF-1054339 (CAREER) and ECCS-1129010. "
Minimal-path fault-tolerant approach using connection-retaining structure in Networks-on-Chip.,"There are many fault-tolerant approaches presented both in off-chip and on-chip networks. Regardless of all varieties, there has always been a common assumption between them. Most of all known fault-tolerant methods are based on rerouting packets around faults. Rerouting might take place through nonminimal paths which affect the performance significantly not only by taking longer paths but also by creating hotspot around a fault. In this paper, we present a fault-tolerant approach based on using the shortest paths. This method maintains the performance of Networks-on-Chip in the presence of faults. To avoid using non-minimal paths, the router architecture is slightly modified. In the new form of architecture, there is an ability to connect the horizontal and orthogonal links of a faulty router such that healthy routers are kept connected to each other. Based on this architecture, a fault-tolerant routing algorithm is presented which is obviously much simpler than traditional fault-tolerant routing algorithms. According to this algorithm, only the shortest paths are used by packets in the presence of fault. This results retains the performance of NoCs in faulty situations. This algorithm is highly reliable, for an instance, the reliability is more than 99.5% when there are six faulty routers in an 8×8 mesh network.","Minimal-Path Fault-Tolerant Approach Using Connection-Retaining Structure in Networks-on-Chip Masoumeh Ebrahimi, Masoud Daneshtalab, Juha Plosila, Hannu Tenhunen Department of Information Technology, University of Turku {masebr, masdan, juplos, hanten}@utu.fi Abstract—There are many fault-tolerant approaches presented both in off-chip and on-chip networks. Regardless of all varieties, there has always been a common assumption between them. Most of all known fault-tolerant methods are based on rerouting packets around faults. Rerouting might take place through nonminimal paths which affect the performance significantly not only by taking longer paths but also by creating hotspot around a fault. In this paper, we present a fault-tolerant approach based on using the shortest paths. This method maintains the performance of Networks-on-Chip in the presence of faults. To avoid using non-minimal paths, the router architecture is slightly modified. In the new form of architecture, there is an ability to connect the horizontal and orthogonal links of a faulty router such that healthy routers are kept connected to each other. Based on this architecture, a fault-tolerant routing algorithm  is presented which is obviously much simpler than traditional faulttolerant routing algorithms. According to this algorithm, only the shortest paths are used by packets in the presence of fault. This results retains the performance of NoCs in faulty situations. This algorithm is highly reliable, for an instance, the reliability is more than 99.5% when there are six faulty routers in an 8×8 mesh network. INTRODUCTION I. As the number of processing elements integrated into a single chip is increasing, traditional bus-based architectures in Many-Core Systems-on-Chip (MCSoCs) are inefficient and a new communication infrastructure is needed. Networks-onChip (NoC) has emerged as a promising solution for on-chip interconnection in MCSoCs due to its scalability, reusability, flexibility, and parallelism  [1] [2] [3]. Transient and permanent faults are two different types of faults that can occur in on-chip networks  [4]. Transient faults are temporary and unpredictable. They are often difficult to be detected and corrected. Permanent faults are caused by physical damages such as manufacturing defects and device wear-out. These faults should be recovered or tolerated in a way  that  the network continues  functioning. Routing techniques provide some degrees of fault tolerance in NoCs. They can be categorized into deterministic and adaptive approaches  [5] [6]. A deterministic routing algorithm uses a fixed path for each pair of nodes resulting in increased packet latency especially in congested networks. In adaptive routing algorithms, packets are not restricted to a single path when traversing from a source to a destination router. So, they can decrease the probability of routing packets through congested areas and thus improve the performance.  In minimal adaptive routing algorithms, the shortest paths are used for transmitting messages between routers. In contrast, performance can severely deteriorate in non-minimal methods due to taking longer paths by packets. Moreover, non-minimal methods are usually more complex with a larger number of virtual channels. Deadlock-free non-minimal routing schemes are the most common approaches to tolerate faults in the network  [7]. They are used in order to reroute packets around faults  [8]. Some fault-tolerant algorithms are proposed to support special cases of faults, such as one-faulty routers, convex or concave regions. These algorithms either disable  the healthy components or require a large number of virtual channels to avoid  deadlock.  There  are  other  fault-tolerant approaches  [9] [10] [11] which do not require any virtual channels. However, these algorithms are partially adaptive and very limited in supporting faults. In addition, fault-tolerant algorithms are relatively complex due to considering different fault models in order to find a path for a packet. A common behavior in fault-tolerant approaches is that packets are routed normally in the network until they face a fault. At this time, turn models or other techniques are used to reroute packets around the faults such that no cycles be formed in the network. The performance analysis in  [11] indicates that in a 4×4 mesh network, the average packet latency can be increased by almost twice when a single router is faulty in the central part of the network. This is due to the fact that the latency is usually forgotten when designing a fault-tolerant algorithm. Faults might exist forever or it might be recovered after a long period of time. Now, imagine how many packets should be misrouted around the fault and how it can affect the performance. What if there are several faults in the network, existing for a long time? In this paper, we present a method named MiCoF (Minimal-path Connection-retaining Fault-tolerant approach). It is a lightweight fault-tolerant approach that maintains the performance of NoCs in the presence of faulty routers. In this approach, packets are never misrouted around the faulty routers since an alternative path is chosen prior reaching the fault. To keep the connectivity and avoid misrouting packets, when a router becomes faulty, the belonging links will be connected in appropriate directions. In other words, a faulty router can be seen as a wire, connecting the surviving routers to each other. The distinguishing feature of MiCoF from the other proposals is in considering the performance as the main goal besides tolerating faults. The rest of this paper is organized as follows: Section II reviews the related work. Preliminar ies are given in Section III. The proposed fault-tolerant routing algorithm is presented in Section IV. Multiple faulty cases are discussed in Section V. The results are investigated in Section VI while we summarize and conclude in the last section. II. RELATED WORK A number of studies present solutions to tolerate faulty links  [12] [13] or routers  [11] [14] in the network. A large numbers of faults can be supported by the proposed method in  [15] without using virtual channels. However, this method takes advantage of a routing table at each router and an offline process to fill out the tables. FTDR  [16] and HARAQ  [17] are Q-learning approaches requiring routing tables. The routing tables  in HARAQ are used  to collect  the congestion information while the routing tables in FTDR indicate the possibility of routing packets through the neighboring routers to different destination nodes. The presented algorithm in  [18] does not require any routing  tables, but packets  take unnecessary non-minimal paths. In this algorithm, an output hierarchy is defined for each position in the network. According to the current and destination positions of a packet, the routing algorithm scans the hierarchy in a descending order and selects the highest available direction. BFT-NoC  [19] presents a different perspective to tolerate faulty links. This approach tries to maintain the connectivity between the routers through a dynamic sharing of surviving channels. Zhen Zhang et al. present an algorithm  [11] to tolerate a single faulty router in the network without using virtual channels. The main idea of this algorithm is to route packets through a cycle free contour surrounding a faulty router. Each router should be informed about the faulty or healthy status of eight direct and indirect neighboring routers. This algorithm is extended in  [20] to tolerate two faulty links or routers in the network. It utilizes two virtual channels along both X and Y dimensions. Each router needs to know the fault statuses of twelve surrounding links or eight neighboring routers. DBP  [21] approach uses a lightweight approach to maintain the network connectivity among non-faulty routers. In this approach, besides the underlying  interconnection  infrastructure, all routers are connected to each other via an embedded unidirectional cycle (e.g. a Hamiltonian cycle or a ring along a spanning tree). A default back-up path is used at each router to connect the upstream to the downstream router. This algorithm is based on taking non-minimal routes. In sum, some of the proposals require to know the fault statuses of all routers or links in the network. To collect this knowledge, online or offline techniques are employed. In some approaches, for making a correct decision, the fault statuses of direct and indirect neighboring routers or links are needed. In few numbers of approaches, the routing decision is based on the minimum knowledge about the faults in the network (i.e. four neighboring routers or connected links). As the fault might occur on additional resources, the simpler approach is always preferred. The fault-tolerant algorithms based on turn models are very limited in rerouting packets and thus they are usually unable to tolerate multiple faults in the network. Deactivations of healthy resources or employing virtual channels are commonly used solutions to support multiple faults. Fault-tolerant algorithms are usually very complicated due to considering various conditions, such as the location of faults, the number of faults, turn model rules, etc. That is why almost all of them are based on deterministic method, to be able to con trol different conditions. Most importantly, the best known algorithms are based on rerouting packets around faults which may result in taking unnecessary non-minimal paths. These approaches affect the performance significantly not only by taking longer paths but also by creating hotspot around a fault. The presented algorithm (MiCoF) has many advantages over traditional methods: 1- it maintains the performance level of NoCs by choosing only the shortest paths for each pair of source and destination router in the presence of faults. 2- the routing unit only requires the fault status of the adjacent routers, that is the minimum knowledge needed by a faulttolerant routing algorithm. 3- the algorithm is very simple such that it can be implemented in few lines of code.  This small piece of code covers all positions of faulty routers and does not set any exceptional rule for borderline routers. 4- it requires only one and two virtual channels along the X and Y dimensions, which is the minimum amount of virtual channels to design a fully adaptive routing algorithm. Moreover, it does not require any routing table. 5- unlike traditional methods, when a router becomes faulty, its belonging links can still be utilized. 6- it is highly reliable such that on average 99.5% of packets successfully reach their destinations when there are six faulty routers in the network. By another metric of reliability, when six faults occur in the network, with the probability of more than 50%, the network functions normally without any message loss. Reachability is also another highlighting point of this method as a router with all neighbors faulty is still reachable. Besides the mentioned advantages, we need to consider that MiCoF may result in a formation of long wires in the network when supporting a large number of faults. In addition, Nonminimal methods or adding virtual channels are necessitated when multiple faults are going to be supported by 100%. We will investigate these issues in our future work. III. PRELIMINARIES Fig. 1(a) shows a typical router in the XY network. In this figure, each input channel is paired with a corresponding output channel. By adding two virtual channels per physical channel, a double-XY network is obtained (Fig. 1(b)). The virtual channels in each dimension are differentiated by vc1 and vc2. Fig. 1(c) shows the double-Y network in which one and two virtual channels are used along the X and Y dimensions, respectively. Each router in the double-Y network has seven pairs of channels, i.e. East(E), West(W), Northvc1(N1), North-vc2(N2), South-vc1(S1), South-vc2(S2), and Local(L). The idea of this paper is developed upon a double-Y network. However, it can be implemented on a double-XY network or a network with more virtual channels. Fig. 1. A router in (a) XY (b) double-XY (c) double-Y network The proposed fault-tolerant routing algorithm is based on a fully adaptive routing algorithm. In this paper, one and two virtual channels are used along the X and Y dimensions. This is the minimum number of virtual channels that can be employed to provide  fully adaptiveness.  In double-Y networks, commonly the following method is used to guarantee the deadlock freeness. The network is partitioned into two subnetworks called +X and –X, each having half of the channels in the Y dimension. Eastward packets are routed through +X subnetwork (i.e. by using the first virtual channel (vc1) in the Y dimension) while westward packets are propagated within –X sub-network (i.e. by using the second virtual channel (vc2) along the Y dimension). IV. THE MICOF APPROACH A. The Router Architecture of MiCoF In NoCs, faults may occur in cores (such as processing elements and memory modules), links or routers. When a core is faulty, the connected router and links can continue functioning. When a link is faulty, the approaches similar to BFT-NoC  [19] retain the connectivity by a dynamic sharing of surviving channels. Thereby, cores and routers perform functioning normally. The most severe case causes by a faulty router. In this case, not only the connected core cannot send or receive packets, but also the packets from the other cores cannot be transmitted through this router. The most common solution is to reroute packets around the faulty router or faulty region. This may imply a non-minimal routing algorithm based on turn models. However, turn models are very limited to tolerate multiple faulty routers. In sum, when a router is faulty, the corresponding core and links are also tagged as faulty and become out of use. The core cannot start working until the fault is recovered. In this work, we show that the links can still be used to retain the performance by a simple modification in the router architecture. Fig. 2 shows the router architecture in the MiCoF approach. Normal router architecture includes input buffers, a routing unit, a virtual channel allocator, a switch allocator and a crossbar switch. In our modified architecture, in a case of faults, the east input channel is directly connected to the west output channel while the west input channel is connected to the east output channel. Similarly, the packets coming from the north or south input channels are directly connected to the south or north output channels, respectively. So, no processing takes place in the router and packets are not stored in the input buffer . So, the flow control is normally performed between the surviving routers. In other words, the whole router acts as a wire, connecting the input channels to output channels in specific directions. Compared with normal router architecture, this architecture needs a few amounts of multiplexers and demultiplexers at input and output ports plus a small wiring overhead. Fig.3 (a) shows a 4×4 mesh topology with five faulty routers. A faulty router itself and the core connected to it are disconnected from the network while the links are used to connect the neighboring routers in appropriate directions. Using the MiCoF architecture, the resulted network is illustrated in Fig. 3(b). Fig. 2. Router architecture u sing MiCoF Fig. 3. (a) five fautly routers in a 4×4 mesh topology (b) the resulted network by using the MiCoF approach B. Fault-Tolerant Routing Algorithm Supporting the MiCoF Architecture In this subsection, we present a fault-tolerant routing algorithm based on the presented architecture. This algorithm is able to support all locations of a single faulty router without taking non-minimal routes. Using the characteristics of fully adaptiveness, packets usually have an alternative choice when facing a fault in one direction. Let us follow a northeast-ward packet when there is a single faulty router in the network. As shown in Fig. 4(a), when the destination is to the east or north of the current router, the packet can bypass the faulty router and reach the destination router without taking any nonminimal path. This is achieved as the faulty router can be considered as a wire connecting the links in the horizontal and orthogonal directions. In Fig. 4(b), the packet is one hop away from the destination router in both the X and Y dimensions (Delta_x=1 and Delta_y=1). By default, the packet is sent to the Y direction (patterns 1 and 2). However, when the neighboring router in the Y direction is faulty, the packet is delivered to the X direction instead (pattern 3). Fig. 4(c) indicates the cases where the distances are two (or it can be greater than two) hops and one hop along the X and Y dimensions  (Delta_x>=2 and Delta_y=1),  respectively. Similar to the previous case, the packet is sent to the Y direction unless the neighboring router in this direction is faulty (patterns 1, 2, and 3). By delivering the packet to the Y direction, the packet is placed in the same row as the destination router where it can reach the destination regardless of faults in the path. When the neighboring node in the Y dimension is faulty (pattern 4), the packet is delivered to Fig. 4. Tolerating all one-faulty routers using only shortest paths the X direction. In the next hop, the packet stands in one of the positions of Fig. 4(a) or Fig. 4(b). In other words, in all positions of a faulty router, the packet could reach the destination by using the shortest paths. In Fig. 4(d), the distance is one and two (or greater than two) hops along the X and Y directions (Delta_x=1 and Delta_y>=2), respectively. The rule is as simple as avoiding to send the packet to the Y direction when the neighboring router in the X direction is nonfaulty. Using this rule, all positions of faults can be covered by patterns 1, 2, 3, and 4.  In the next hop, the packet stands in one of the positions of Fig. 4(a) or Fig. 4(b). When the distances along both directions are two or greater than two hops (Delta_x>=2 and Delta_y>=2), the packet should be sent to a non-faulty neighboring router (Fig. 4(e)). By routing the packet with this policy, the packet reaches one of the positions of Fig. 4(c) or Fig. 4(d). In a case where both neighboring routers are non-faulty, the packet is sent through the less congested direction. If the network is designed to tolerate a few numbers of faults, packets can be routed adaptively in the network as long as the remaining distance along both directions is equal or greater than two hops. To sum up, it is guaranteed that all situations of a single faulty router are covered by MiCoF taking only the shortest paths between each pair of source and destination routers. In addition, fully adaptiveness is provided when the packet is not close to the destination router (i.e. the packet is close to the destination when the distance along one dimension is at most one hop). However, for better reliability, the adaptiveness of MiCoF can be limited. The limitation is applied to a situation when the distances along both directions are two or greater than two hops. In this case, a packet is sent to a (non-faulty) larger-distance direction (Fig. 4(f)). The packet can be sent through either direction when the distances along both directions are equal. Obviously, this adaptivity limitation does not affect the behavior of the algorithm in supporting all single faulty routers without misrouting packets. Another interesting point is that, as illustrated in Fig. 4(g), this algorithm only requires  the fault  information of four neighboring routers (i.e. it is normally provided for any faulttolerant method). Thereby, this algorithm does not impose any area overhead due  to collecting  the fault  information throughout the network. Taking into account that faults might occur on additional resource, the less used resources offers the more reliable method. As it is already mentioned, fault-tolerant routing algorithms are usually very complicated. In contrast with them, the MiCoF algorithm is very simple with a negligible area overhead. The whole MiCoF routing algorithm is shown in Fig. 5. In this figure, the adaptiveness is limited to support more faulty switches. However, the adaptiveness can be increased when the performance is more preferred than the high reliability. Definit ions: Xd,Yd: X and Y of destination switch              Xc,Yc: X and Y of current switch ngbr: neighboring router **-----------------------------------------------** x_d ir <= E when Xd>Xc else W; y_d ir <= N when Yd>Yc else S; Delta_x <= (Xd -Xc) when Xd>Xc else (Xc-Xd ); Delta_y <= (Yd -Yc) when Yd>Yc else (Yc-Yd ); if (Delta_x =0 and Delta_y =0) then select <= local; elsif (Delta_x >=1 and Delta_y =0) then select <= x_d ir; elsif (Delta_x =0 and Delta_y >=1) then select <= y_dir; elsif (Delta_x >=1 and Delta_y =1) then if ngbr(y_d ir)=healthy then select <= y_dir; else select <= x_d ir; end if; elsif (Delta_x =1 and Delta_y >=2) then if ngbr(x_d ir)=healthy then select <= x_dir; else select <= y_d ir; end if; else if (ngbr(x_d ir)=faulty and ngbr(y_dir)=faulty) or (ngbr(x_d ir)=healthy and ngbr(y_d ir)=healthy) then if (Delta_x > Delta_y) then select <= x_d ir; elsif (Delta_x < Delta_y) then select <= y_dir; else select <= x_dir or y_dir; end if; elsif ngbr(y_dir)=healthy then select <= y_d ir; elsif ngbr(x_dir)=healthy then select <= x_d ir; end if; end if; Fig. 5. Pseudo VHDL code of MiCoF V. RELIABILITY UNDER MULTIPLE FAULTY ROUTERS A. Two Faulty Routers By the MiCoF approach, all positions of one faulty router can be tolerated using only the shortest paths. The MiCoF algorithm does not change in order to support more faults. In this subsection, we investigate how two faulty routers can be Fig. 6. Tolerating two faulty routers by the MiCoF approach tolerated using the same routing algorithm. As shown in Fig. 6(a), when the current and destination routers are located in the same row or column, the packet can reach the destination regardless of the faulty routers in the path. Fig. 6(c) indicates all the six positions of two faulty routers when the distances are two and one hops along the X and Y dimensions. According to the MiCoF algorithm, by default, the packet is sent to the Y direction. If the neighboring router in the Y direction is faulty, the packet is sent to the X direction. In patterns 1, 2, and 3 of Fig. 6(c), the packet is sent to the neighboring router in the Y direction as it is healthy. In the next hop, the packet faces to the similar situation as in Fig. 4(a). Patterns 4, 5, and 6 cover the cases when the neighboring router in the Y direction is faulty, and thus the packet is sent to the X direction. In the next hop, the packet stands in one of the positions of Fig. 4(b) or it simply passes through the faulty router. A similar approach is applied when the distance is one and two hops along the X and Y dimensions (Fig. 6(d)). Fig. 6(e) shows the cases in which the distance is two hops along both directions. If the neighboring router in one of the directions is faulty, the packet is sent through the non-faulty direction (pattern 1 and 2). From the next hop to the destination router, there might be at most one faulty router in the path that can be supported according to Fig. 4(c) or Fig. 4(d). If both neighboring routers are healthy (position 3 in Fig. 6(e)), in the remaining path from the next hop to the destination router, the packet might face two faulty routers. As indicated in Fig. 4(c), Fig. 4(d), Fig. 6(c), and Fig. 6(d), all one and two faulty routers are supported by MiCoF. If both neighboring routers are faulty, the packet can be sent through either direction. This packet will not face any fault in the remaining path as both faults are already bypassed. So far, all two faulty routers are supported by MiCoF using only the shortest paths. There is only one position in which two faulty routers cannot be supported using the shortest paths. This is the case when the distance from the source to the destination router is one along both dimensions while the neighboring routers in both directions are faulty (Fig. 6(b)). These positions of faults are called diagonal positions. The source router still can send and receive packets to/from every other router in the network except the destination router. In other words, only the packets from this specific source router position cannot reach to this specific destination router position (or vice versa). All the other packets can be normally routed in the network. If the source router is farther away from the destination router, the packet never stands in this unsupported position as the packet already chooses other routes prior reaching this position (e.g. similar to the pattern 2 of Fig. 6(c) and Fig. 6(d)). B. Reliability Analysis of Two Faulty Routers In this paper, we use two reliability metrics named reliability1 and reliability2 in our measurements. Reliability1 shows the probability that the network can successfully deliver any packet under the existence of fault. Reliability2 is the probability that a packet can be successfully delivered under fault. These metrics can be calculated as follows: · Reliability1: According to MiCoF, if two faults are located in diagonal positions, the network may fail. At first, we calculate the number of total combinations of two faulty switches in the network. Then, we measure the number of combinations in which two faults occur in diagonal positions. By dividing these two numbers, the reliability value is obtained. The number of different combinations of two faulty switches in an n×n mesh network can be measured by: 2 (cid:1840)(cid:3028)(cid:3039)(cid:3039)_(cid:3030)(cid:3042)(cid:3040)(cid:3029)(cid:3036)(cid:3041)(cid:3028)(cid:3047)(cid:3036)(cid:3042)(cid:3041)(cid:3046) = (cid:4672)(cid:1866)(cid:2870)2 (cid:4673) = (cid:1866)(cid:2870) ((cid:1866)(cid:2870) − 1) (cid:1840)(cid:3031)(cid:3036)(cid:3028)(cid:3034)(cid:3042)(cid:3041)(cid:3028)(cid:3039) _(cid:3030)(cid:3042)(cid:3040)(cid:3029)(cid:3036)(cid:3041)(cid:3028)(cid:3047)(cid:3036)(cid:3042)(cid:3041)(cid:3046) = 2((cid:1866) − 1)(cid:2870) Fig. 7 shows all combinations in which two faulty switches are  located  in diagonal positions. By extending the idea to an n×n mesh network, the number of diagonal combinations can be calculated by: Fig . 7. A couple indicates a diagonal position (i.e. nine diagonal positions in each figure) Finally, the Reliability1 can be calculated by: (cid:1866)(cid:2870) ((cid:1866)(cid:2870) − 1) (cid:1844)1 = 1 − (cid:1840)(cid:3031)(cid:3036)(cid:3028)(cid:3034)(cid:3042)(cid:3041)(cid:3028)(cid:3039) _(cid:3030)(cid:3042)(cid:3040)(cid:3029)(cid:3036)(cid:3041)(cid:3028)(cid:3047)(cid:3036)(cid:3042)(cid:3041)(cid:3046) (cid:1840)(cid:3028)(cid:3039)(cid:3039)_(cid:3030)(cid:3042)(cid:3040)(cid:3029)(cid:3036)(cid:3041)(cid:3028)(cid:3047)(cid:3036)(cid:3042)(cid:3041)(cid:3046) = 1 − 4 ((cid:1866) − 1)(cid:2870) According to this formula, for example in an 8×8 mesh network, with the probability of 95.2%, two faults will not be located in diagonal positions, and thus the network functioning normally without dropping any message. · Reliability2: The second definition is mostly used in literature to report the reliability value. Let us assume that the network is examined under all combinations of twofaulty switches. Thereby, the number of examinations is equal to the combinations of two faulty switches (Nall-combinations). Per examination, each healthy switch delivers one packet to every other healthy switch in the network (i.e. total of n2-3 packets, except itself and two faulty switches). As faulty switches do not send or receive any packets (i.e. total of n2-2 switches are able to deliver packets), the total number of delivered packets per combination is: Therefore, the total number of delivered packets in the whole examinations is: (cid:1840)(cid:3031)(cid:3032)(cid:3039)(cid:3036)(cid:3049)(cid:3032)(cid:3045)(cid:3032)(cid:3031) _(cid:3043)(cid:3032)(cid:3045)_(cid:3030)(cid:3042)(cid:3040)(cid:3029)(cid:3036)(cid:3041)(cid:3028)(cid:3047)(cid:3036)(cid:3042)(cid:3041) = ((cid:1866)(cid:2870) − 2)((cid:1866)(cid:2870) − 3) (cid:1846)(cid:1867)(cid:1872)(cid:1853)(cid:1864) = (cid:1840)(cid:3031)(cid:3032)(cid:3039)(cid:3036)(cid:3049)(cid:3032)(cid:3045)(cid:3032)(cid:3031)_(cid:3043)(cid:3032)(cid:3045) _(cid:3030)(cid:3042)(cid:3040)(cid:3029)(cid:3036)(cid:3041)(cid:3028)(cid:3047)(cid:3036)(cid:3042)(cid:3041) × (cid:1840)(cid:3028)(cid:3039)(cid:3039)_(cid:3030)(cid:3042)(cid:3040)(cid:3029)(cid:3036)(cid:3041)(cid:3028)(cid:3047)(cid:3036)(cid:3042)(cid:3041)(cid:3046) (cid:1830)(cid:1857)(cid:1858)(cid:1857)(cid:1853)(cid:1872)(cid:1857)(cid:1856) = 2 × (cid:1840)(cid:3031)(cid:3036)(cid:3028)(cid:3034)(cid:3042)(cid:3041)(cid:3028)(cid:3039) _(cid:3030)(cid:3042)(cid:3040)(cid:3029)(cid:3036)(cid:3041)(cid:3028)(cid:3047)(cid:3036)(cid:3042)(cid:3041)(cid:3046) = 4((cid:1866) − 1)(cid:2870) On the other hand, per diagonal position, two packets must be dropped (those from the source to the destination switch or vice versa), so that the total number of defeated packets is calculated by: Therefore, the reliability2 can be measured by: (cid:1844)2 = 1 − (cid:1830)(cid:1857)(cid:1858)(cid:1857)(cid:1853)(cid:1872)(cid:1857)(cid:1856) (cid:1846)(cid:1867)(cid:1872)(cid:1853)(cid:1864) According to this formula, in an 8×8 mesh network, 99.998% of packets reach their destination considering all combinations of two faulty routers. C. Three Faulty Routers In this subsection, we take a quick look at three faulty routers in the network. If three faults are well distributed over the network, there are easily supported according to Fig. 4 and Fig. 6. However, when there are close to each other, the fault situations shown in Fig. 8 are obtained. If the locations of three faults are similar to the patterns 1 or 2 of Fig. 8(a) and Fig. 8(b), then it is supported by the MiCoF routing algorithm (Fig. 5). In patterns 3 and 4, a few percentages of packets cannot reach the destination. Even under these severe fault conditions, the rest of the packets can be routed to their destinations through the shortest paths. The focus of this work is to tolerate faults by only using the shortest paths without any performance loss. However , nonminimal paths or virtual channels can be used to support the remaining cases. Fig. 8. Three faulty routers in the network which are close to each other VI. EXPERIMENTAL RESULTS To evaluate the efficiency of the proposed approach, a NoC simulator is developed with VHDL to model all major components of the on-chip network. For all the routers, the data width is set to 32 bits. Each input buffer can accommodate 8 flits in each virtual channel. Moreover, the packet length is uniformly distributed between 5 and 10 flits. As a performance metric, we use latency defined as the number of cycles between the initiation of a message issued by a Processing Element (PE) and the time when the message is completely delivered to the destination PE. The simulator is warmed up for 12,000 cycles and then the average performance is measured over another 200,000 cycles. MiCoF is designed based on using one and two virtual channels along the X and Y dimensions. We implemented two other methods, called ReRS  [11] (Reconfigurable Routing for Tolerating Faulty Switches) and RAFT  [20] (Reconfigurable Routing for Tolerating Faulty Links). Unlike MiCoF, both methods are based on a detour strategy and thereby packets may take unnecessary longer paths to reach destinations. ReRS requires one virtual channel along each dimension and is able to tolerate all single faulty routers. RAFT utilizes two virtual channels to support all two faulty routers. A. Reliability Evaluation under Uniform Traffic Profile In the uniform traffic profile, each processing element (PE) generates data packets and sends them to another PE using a uniform distribution  [22]. The mesh size is considered to be 8×8. To evaluate the reliability of MiCoF, the number of faulty routers increases from one to six. All faulty routers are selected using a random function. The results are obtained using 10,000 iterations when the traffic is uniform random. MiCoF RAFT ReRS ) % ( y t i l i b a i l e R 100 80 60 40 20 0 ) % ( y t i l i b a i l e R 100 50 0 1-faulty node 2-faulty node 3-faulty node 4-faulty node 5-faulty node 6-faulty node Fig. 9. Reliability measurment based on the first definition MiCoF RAFT ReRS 1-faulty node 2-faulty node 3-faulty node 4-faulty node 5-faulty node Fig. 10. Reliability mea surment ba sed on the second definition 6-faulty node Reliability is measured based on two metrics. Using the first reliability metric, we measure the number of combinations with no packet loss over the total number of combinations. For the second metric, the average number of successful packet arrivals at destinations into the total number of delivered packets is calculated. The reliability values based on the first metric is shown in Fig. 9. All three approaches are reliable by 100% when there is a single fault in the network. RAFT is the only approach that can guarantee the reliability by 100% under the cases of two faulty routers. However , this method utilizes one more virtual channel than MiCoF. As illustrated in this figure, the reliability of ReRS and RAFT abruptly decreases with more faulty routers. MiCoF is highly reliable, for an instance, in 50% of all combinations of six faulty routers, the network is functioning normally without any packet loss. Fig. 10 shows the reliability measurement based on the second metric. The important point is that the reliability of MiCoF is more than 99.5% under six faulty routers in the network. B. Performance Analysis under Uniform Traffic Profile The performance analysis under uniform random traffic is shown in Fig. 11. The average communication latency of ReRS and RAFT are obtained under the cases of single and two faulty routers as more faulty routers are not well supported. The average communication latency of MiCoF is measured under one to six faulty routers in the network. To have a fair performance comparison we use two virtual channels in all three methods. The extra virtual channels are used for the performance purposes. In a fault-free network, the performance of all methods is comparable while RAFT outperforms others because of its better adaptiveness. As the number of faults increases, the performance of ReRS and RAFT significantly decreases. We increase the number of faulty routers to six faults and measure the performance of MiCoF. Surprisingly, the performance gradually starts growing under the same traffic load. ) l e c y c ( y c n e a L t e g a r e v A 350 300 250 200 150 100 50 0 MiCoF: 1-fault MiCoF: 6-fault ReRS: 1-fault ReRS: 2-fault RAFT: 1-fault RAFT: 2-fault 0 0,1 0,2 0,3 0,4 Injection Rate (f lits/node/cycles) Fig. 11. Performance under uniform random traffic This improvement is from the communication point of view while the whole system performance will be obviously decreased by occurring faults in the network. This is due to the fact that the routing does not take place in faulty routers and the total number of hops is decreased. For clarity, the performance curves of two- to five- faulty routers are omitted, but they are distributed between the curves of one- and sixfaulty routers. C. Performance Analysis under Hotspot Traffic Profile Under the hotspot traffic pattern, one or more routers are chosen as hotspots receiving an extra portion of the traffic in addition to the regular uniform traffic. In simulations, given a hotspot percentage of H, a newly generated message is directed to each hotspot router with an additional H percent probability. We simulate the hotspot traffic with a single hotspot router at (4,4) in an 8×8 mesh network. The performance of each network under different numbers of faulty routers and H=10% is illustrated in Fig. 12.         ) l e c y c ( y c n e a L t e g a r e v A 350 300 250 200 150 100 50 0 MiCoF: 1-fault MiCoF: 6-fault ReRS: 1-fault ReRS: 2-fault RAFT: 1-fault RAFT: 2-fault 0 0,05 0,1 Injection Rate (f lits/node/cycles) 0,15 0,2 0,25 Fig. 12. Performance under hotspot traffic D. Hardware Analysis To assess the area overhead and power consumption, the whole platform of each method is synthesized by Synopsys Design Compiler. We compared the area overhead and power consumption of MiCoF with ReRS and RAFT. In this set of analysis, ReRS has no virtual channel, MiCoF uses one virtual channel in the Y dimension, and RAFT utilizes two virtual channels along both directions. The power consumption of all methods is measured in a non-faulty network. For each scheme, we  include network  in terfaces,  routers, and communication channels (MiCoF uses additional resources for connection retaining purposes (Fig. 2)). For synthesizing, we use the UMC 90nm technology at the operating frequency of 1GHz and supply voltage of 1V. We perform place-and-route, using Cadence Encoun ter, to have precise power and area estimations. The power dissipation  is calculated using Synopsys PrimePower in a 6×6 mesh network. The layout area and power consumption of each platform are shown in Table 1. As indicated in the table, MiCoF has a lower area overhead than the RAFT and higher one than ReRS. This is mostly because of using different numbers of virtual channels. Table 1. Details of hardware implementation Power (W) dynamic & static Area (mm2) Network platforms MiCoF ReRS RAFT 6.886 6.513 7.295 2.40 2.10 2.85 VII. CONCLUSION In this paper, we proposed a fault-tolerant approach named MiCoF. In the presen ted approach, all packets are routed through the shortest paths, maintaining the performance of NoC in the presence of faults. To be able to route packets through the shortest paths, the router architecture is slightly modified. The purpose of this modification is to maintain the connectivity among the surviving routers. For th is to happen, when a router becomes faulty, the links are simply connected to each other along the horizontal and orthogonal directions. MiCoF is a very simple, lightweight, and adaptive approach which takes advantage of only one and two virtual channels [7] [5] along the X and Y dimensions. The high reliability provided by this simple approach is a final conclusion of this work. "
Scalable parallel simulation of networks on chip.,"With continuing miniaturization, NoCs with 1024 nodes will become realistic around the year 2020. The design of such NoCs requires efficient simulation techniques to evaluate design alternatives and to validate functional correctness. The current state of the art, sequential simulation, will no longer provide acceptable simulation time. Parallel simulation exploiting multicore and multithreading capabilities of simulation computers is a potential solution. However, current parallel techniques suffer from limited scalability due to the need to synchronize simulation time and the access to shared data structures. This work presents a new approach based on an explicit ordering of simulation tasks so that a maximum of independent tasks are simulated between any dependent tasks. This enables efficient synchronization and, together with dynamic load balancing, reduces blocking time. A near-linear simulation speedup of up to 15.5 is achieved on a 16 core simulation machine.","Scalable Parallel Simulation of Networks on Chip Marcus Eggenberger, Martin Radetzki Institut f ¨ur Technische Informatik Universit ¨at Stuttgart Pfaffenwaldring 5b 70569 Stuttgart, Germany Email: {marcus.eggenberger,martin.radetzki}@informatik.uni-stuttgart.de Abstract—With continuing miniaturization, NoCs with 1024 nodes will become realistic around the year 2020. The design of such NoCs requires efﬁcient simulation techniques to evaluate design alternatives and to validate functional correctness. The current state of the art, sequential simulation, will no longer provide acceptable simulation time. Parallel simulation exploiting multicore and multithreading capabilities of simulation computers is a potential solution. However, current parallel techniques suffer from limited scalability due to the need to synchronize simulation time and the access to shared data structures. This work presents a new approach based on an explicit ordering of simulation tasks so that a maximum of independent tasks are simulated between any dependent tasks. This enables efﬁcient synchronization and, together with dynamic load balancing, reduces blocking time. A near-linear simulation speedup of up to 15.5 is achieved on a 16 core simulation machine. I . IN TRODUC T ION As traditional on chip communication structures, such as busses, do not scale with the number of connected processing cores, networks on chips (NoC) [1] have been proposed as a means of coping with the ever-increasing communication demands of large scale chip multiprocessors (CMP). These NoCs must be designed such that the required performance is delivered while resources are kept as low as possible. However, because of their cost, design changes at late product development stages must be avoided. Avoiding them requires early and efﬁcient system simulation, something that is usually achieved by raising the level of abstraction whilst losing simulation precision. But there are limitations to this approach, as a severe degradation of simulation accuracy has been shown in CMP simulations that focused only on computational aspects and neglected communication [2]. Furthermore, as shown on a wormhole switched network, communication not only cannot be ignored, but even has to be simulated at cycle level accuracy or large inaccuracies are still to be expected [3]. As technology nodes continue to decrease, the number of transistors per die increases and the prospected transistor count [4] makes NoCs of 1024 nodes realistic around the year 2020. However, with the limited increase in single core performance of recent years, the gap between the performance of single threaded simulators and the complexity of simulated models continues to increase, making detailed design evaluation more time consuming and therefore less feasible. To narrow this gap, we present a novel decomposition paradigm for fast, parallel system simulation of NoCs, leveraging today’s commonly available shared memory, multi processor system. Experimental results obtained using a reference implementation demonstrate the applicability of the proposed approach as achieved speedups surpassed a factor of 15 on a 16-core (two 8-core packages) machine. The rest of this paper is organized as follows: Section II introduces to related work in the ﬁeld of NoC simulation, Section III identiﬁes the targeted scope of this work, and deﬁnes the problem statement, in Section IV we propose our novel decomposition approach and show its application to different topologies, Section V shows the performance of our proposed approach using a reference implementation, and ﬁnally, Section VI concludes. I I . R E LATED WORK Early NoC simulations were performed using general network simulators such as ns-2 [5], and general network simulators such as OPNET [6] and OMNeT++ [7], [8] are still employed for NoC evaluation. Special NoC simulators were developed focusing on different aspects including, but not limited to, fault simulation to evaluate error recovery methods [9], power estimation [10], and easy means for graphical debugging of RTL NoC models [11]. However, these usually single threaded simulators rarely focus on simulation performance, which, with the ever-increasing core count per die, will no longer be negligible in the future. The Transaction Level Modelling (TLM) paradigm has been proposed to increase simulation performance by modelling communication as function calls (transactions). TLM models have been employed for NoC simulation using different levels of abstraction. While [12], [13] model switch-to-switch communication as single transactions, [3] raises the abstraction level further by modelling multi-hop end-to-end communication as single transactions enabling even higher simulation performance at the loss of simulation accuracy. However, these TLM based models are simulated using a single threaded SystemC kernel, thus constraining CPU power to that of a single core. A highly specialized NoC simulation concept has been proposed in [14], where the NoC is simulated on an FPGA controlled by a host processor. This is achieved by a single, synthesized Switch that performs a sequential simulation of the whole NoC, switch by switch. For that, the switch is connected to on-chip RAM containing data for all switches. However, this solution requires a synthesizable switch model, making it unsuitable for early design space exploration. Acknowledging the need for parallel NoC simulation, HORNET [15], a highly conﬁgurable, multithreaded system level NoC simulator, was recently introduced. Parallel simulation is achieved by splitting the NoC into equal parts matching the number of simulator threads, where each colocated switch and processing element are assigned to a ﬁxed thread. Read and write operations on the input buffers are protected by individual mutexes for each input buffer head and tail. Threads are synchronized periodically, with a frequency depending on the communication accuracy. An even ﬁner grained parallelization approach was proposed in [16], where NoC simulation was performed on GPUs. In GPU programming, each hardware thread executes a fairly small computation kernel. In this case, each kernel is responsible for the necessary simulation steps of one input port and one output port. However, the ﬁne-grained parallelization comes at the cost of increased synchronization effort, consuming a signiﬁcant amount of simulation time. Another parallel NoC simulator targeting distributed memory architectures and focusing on the evaluation of error control was introduced in [9]. Using the message passing interface (MPI), communication between switches is modelled using MPI send and -receive calls, allowing a wide distribution of up to one single switch or PE per physical processor. Graphite, a multithreaded and distributed simulator for many-core systems, was introduced in [17]. While focusing on the performance evaluation of software for distributed manycore systems, Graphite uses lax synchronization of the individual simulator kernels, and on-chip communication cannot be modelled at cycle accuracy, which makes it infeasible for precise evaluation of on-chip network performance. Our work differs in that we propose a novel, parallel decomposition paradigm suitable for common multiprocessor systems that scales well, even with different workloads. I I I . PR EL IM INAR I E S AND PROBL EM S TAT EM EN T In this work, we focus on synchronous NoCs with ingress queued buffers. An NoC is composed of an interconnected set of tiles, where each tile consists of a colocated switch and processing element. A common and intuitive approach for parallel simulation of on-chip networks, as employed in [15], is static, spatial decomposition of the workload. In this case, the NoC is divided into equal parts such that each tile is assigned to a ﬁxed thread and all threads are responsible for the same number of tiles. Each thread is then executed by an individual logical or physical CPU core. To ensure that there are no causality errors, the next cycle of a switch must not be simulated before all of its neighboring switches have ﬁnished their current simulation cycle. Therefore threads must be synchronized at the end of each simulation cycle. The ideal scenario, where computation time required for the simulation is the same for all tiles, is depicted in Fig. 1 (Top) using the example of a 16 tile NoC. Fig. 1. Effect of balanced (top) and unbalanced (bottom) workloads in spatial decomposition However, things change if the workload is not the same for all switches. For example, if a switch is located at a trafﬁc hot spot, it is likely to require more time to be simulated than a switch located on routes of few ﬂows. This leads to an unbalanced workload assignment to threads, which, in turn, leads to underutilization of CPU cores, as threads having less workload are waiting for completion of threads with higher workload. This case is shown in Fig. 1 (Bottom) and raises two questions: First, why are threads 0 and 3 not helping out thread 1 and 2? And second, why does thread 0 have to wait for the ﬁnished simulation of switch 11? Assuming switch 0 has neighboring switches 1 and 4, the simulation of switch 0 could already advance to the next simulated cycle without causing a violation of causality. In this work, we provide a solution that minimizes idle times and distributes the workload evenly among threads, thus enabling an efﬁcient parallel simulation that scales well with the number of threads. IV. FA IR AND E FFIC I EN T WORK LOAD D I STR IBU T ION To achieve an ideal speedup, workload must be spread evenly among threads. However, in the case of static, spatial decomposition, this even spread requires advanced knowledge of the individual workloads, making the approach unfavorable. And if workload distribution varies during simulation, underutilized cores might be unavoidable. An improvement would be to change the mapping of tiles to threads at runtime if workload is unbalanced. But this approach also has drawbacks as the necessary monitoring of thread utilization results in an overhead and changing the mapping of a tile requires an additional synchronization step, or otherwise a simulation cycle might be skipped or executed twice when a tile needs to be moved. We introduce a workload decomposition scheme for onchip network simulation based on the individual subtasks of the simulation. Instead of mapping tiles to threads, we deﬁne a total order over all subtasks, and a worker thread pool processes the subtasks in order. Simulation correctness is ensured by dependencies between individual subtasks and simulation efﬁciency is achieved by deﬁning the order such that two dependent subtasks are far apart with respect to the order. To a large degree, this approach allows unhindered execution of tasks, even if one task requires more CPU time Fig. 2. First tasks of an 8 × 8 mesh NoC simulation than others. In the rest of this paper, we will refer to a simulation subtasks simply as a task, which is not to be confused with a task that is part of a simulated application. A. Simulation Decomposition The simulation of a synchronous NoC consists of the repeated simulation of all individual switches and processing elements for all simulated cycles. We group the simulation of one colocated switch and processing element for one cycle as a single task. Thus, for an NoC of N tiles, we have N tasks per cycle and a total of N × C tasks must be performed for a simulated duration of C cycles. Each of these N × C tasks is uniquely identiﬁed by a task identiﬁer (ID) that encodes the affected tile and respective simulation cycle. Task IDs are assigned to tiles in a ﬁxed order over the NoC’s topology, and this process is repeated for each cycle. As an example, Fig. 2 shows the ﬁrst 48 tasks for a simulation of a 4 × 4 mesh topology NoC. Each of the three blocks of 16 tasks comprises the simulation of one cycle, in which task IDs are assigned to tiles in x-ﬁrst dimension order. For a given task ID t we can now calculate its corresponding cycle and affected tile using simple arithmetic operations as cyc(t) = (cid:98)t/N (cid:99) and tile(t) = t mod N , respectively. Sticking to the example of Fig. 2, task ID 21 corresponds to the simulation subtask for tile 5 (21 mod 16 = 5) in cycle 1 ((cid:98)21/16(cid:99) = 1). The task ID is represented as an unsigned 64bit integer value. Since it encodes both the simulation time and the location of the affected switch, the total simulation time is constrained by the size of the simulated NoC to max cycle(N ) = 264/N . However, given a 64 × 64 = 4096 switch NoC, this still leaves room for over 4e15 simulation cycles or a simulated time of over 8 years, assuming a clock period of 1 ns. An important property of the unique task ID is that it enables expressing dependencies between tasks, even across cycle boundaries, by a simple scalar metric distance that is independent of NoC topology. B. Assigning Tasks to Threads To avoid the aforementioned load balancing issues of spatial decomposition and to reduce the synchronization overhead introduced by mutexes and barriers, we do not use a ﬁxed mapping of tasks to threads. Thus, running the same simulation twice may result in a speciﬁc task being executed by different threads in the two simulation runs. In our approach, a thread pool, where every thread is capable of processing any task, takes care of the workload. Whenever Fig. 3. Parallel task execution for balanced (top) and unbalanced (bottom) workloads a thread is ready to perform work, it simply takes the next task and processes it. To avoid complex list management requiring mutexes to ensure list integrity, we employ a single, global counter to keep track of the next task to be processed. Since the task ID encodes both affected switch and cycle, all necessary information to perform the task is immediately available from the counter value, and a thread simply takes the current counter value as next task to perform and increments the counter so that no other thread will perform the same task. While reading and incrementing the counter is subject to race conditions, it is also a very simple operation that can be implemented without the need for synchronization mechanisms provided by the operating system (e.g. mutexes). Instead we directly rely on the CPU’s hardware support to perform this operation atomically, in our case using the exchange-and-add operation (XADD) of the x86 64 architecture with a lock preﬁx, thus ensuring atomic operation. While our simulator was written for x86 64 architectures, this approach is not limited to x86 64 platforms, as similar hardware support can be found in many current CPUs. And the recently approved C++11 standard removes the need for architecture speciﬁc inline assemblies as it exposes hardware support for atomic integer operations to user space applications via the new std::atomic data type, provided that compiler support is available. The main beneﬁt of the thread pool approach, where each thread is capable of performing any task without ﬁxed task or workload assignment, is its inherent load balancing ability as it provides good core utilization even if individual tasks require more CPU time than others. In the case of workload being uniformly distributed over tasks, the thread pool will more or less seemingly “move” as a group over the list of tasks as shown in Fig. 3 (Top): at ﬁrst, threads 0 to 3 process tasks 0 to 3 respectively, and as soon as the ﬁrst thread ﬁnishes its current task, it starts working on the next available one. For example, in the depicted case, thread 0 immediately starts processing task 4 after task 0 has been ﬁnished, thread 1 takes on the work of task 5 after ﬁnishing task 1, and so on for the remaining threads and time. As a result, there is no stalling between tasks, and CPU cores are fully utilized. Fig. 3 (Bottom) shows a scenario of unbalanced workload matching the earlier example of Fig. 1. When thread 3 ﬁnishes its ﬁrst task, thread 1 is still occupied with task 1 and has not yet been able to take on the work of task 5. However, Fig. 4. Task dependencies across cycle borderes determining the safe distance. Grey marked tasks can be executed before task 21 has ﬁnished, representing the minimum safe distance and degree of independent work. since there is no strict assignment of task 5 to any of the 4 threads, thread 3 is able to start working on this task, so that overall progress can continue unhindered. This leads to the inherent load balancing property of the proposed approach. The example also shows that tasks of cycle 1 can already be processed even if not all tasks of cycle 0 have been ﬁnished, as long as there is no dependency to unﬁnished tasks, which would lead to a causality violation. Thread 2 is still busy processing task 15, the last task of cycle 0, when threads 0, 1, and 3 start processing tasks 18, 17, and 16 respectively, tasks that belong to cycle 1. C. Simulation Correctness In the targeted synchronous NoCs, the outcome of a switch or PE’s simulation for a given cycle must not be visible to neighboring switches until simulation advances to the next simulation cycle. To achieve this, we employ special, mutex free input buffers with two sets of data managing structures such as read and write pointers. During simulation of an even cycle, values are only read from the “even” data set and only written to the “odd” data set, and vice versa for odd cycles. This solves intra cycle dependencies of tasks without the need for mutex based protection. Furthermore, to achieve a correct simulation result, a task must not be executed if it depends on the outcome of another task that has not yet been ﬁnished. While within a single cycle all tasks can be executed independently of each other, there are dependencies between tasks of subsequent cycles: That is, a task must not be executed until all tasks of the previous cycle have been retired for the affected tile and all adjacent tiles. Taking on the earlier example of the 4 × 4 NoC, task 37 corresponds to the simulation of tile 5 in cycle 2, having dependencies as depicted in Fig. 4. It can be seen that task 37 must not be executed before tasks 17, 20, 21, 22, and 25 have been ﬁnished. These tasks correspond to the simulation of the same switch (task 21) and its neighbors (tasks 17, 20, 22, 25) in cycle 1. If task 37 was to execute before task 25 has ﬁnished, the input buffer changes resulting from task 37 would already be visible to tile 9 in cycle 1 and not, as intended, in cycle 2, thus leading to an erroneous simulation outcome. To ensure that these task dependencies are met, we deﬁne a safe distance sd for each task, determining up to which previous tasks the simulation must have at least advanced. That is, task t must not be executed until all tasks t(cid:48) with t(cid:48) ≤ t − sd(t) have been completed. During simulation, we keep track of the maximum task ID tsmax for which all tasks t with t ≤ tsmax have been completed. Let Tactive be the set of tasks that are currently processed by a worker thread, then the monotonic order in which tasks are assigned yields that tsmax = min(Tactive ) − 1. Checking whether it is safe to execute a task or not, means simply checking whether t − sd(t) ≤ tsmax . To simplify things, we deﬁne the minimum safe distance sdmin as the minimum of all safe distances for all tasks, and only check for t − sdmin ≤ tsmax . Note that due to the repetitive order of tasks with each cycle, the minimum safe distance only depends on the order of tasks within a single cycle and is independent of the simulation duration. In the previous example, task 37 may be executed if all tasks with a task ID of 25 or less have been ﬁnished, and thus sd(37) = 12, which is also equal to the minimum safe distance, and task 37 may also be executed if 37 − 12 = 25 ≤ tsmax . If the safe distance constraint is violated, the executing thread is suspended until tsmax has caught up to the required task ID. Suspending the thread is a viable solution as there is no other work that can be performed. Assuming task t fails the safe distance check, then the monotonic task order ensures that all tasks t(cid:48) with t(cid:48) < t either have already been ﬁnished or are currently being processed by another thread, and tasks t(cid:48)(cid:48) with t(cid:48)(cid:48) > t will also fail the safety check, as t(cid:48)(cid:48) − sdmin > tsmax immediately follows t − sdmin > tsmax . As a result, no other task can be executed. D. Efﬁcient Task Order For parallel simulation, it is crucial that to a large degree threads can perform their work unhindered, which in our case translates to the minimum safe distance being as large as possible. Only if the minimum safe distance is large, can many consecutive tasks be executed without any interdependence, and if this criteria is met, threads are rarely suspended, and CPU core utilization is thus high. Therefore the minimum safe distance can be seen as an indicator of the degree of independent work. Since the safe distance depends only on the order of tasks within a single cycle, it directly depends on the maximum distance of two tasks belonging to neighboring nodes within a single cycle. That is, the safe distance of a task t is equal to the size of the NoC minus the maximum distance between task t and task t(cid:48) , where t(cid:48) is the last task of any neighboring node of task t in the same cycle as t. As a result, the problem of ﬁnding a task order with a large minimum safe distance is the same as ﬁnding a numbering of NoC nodes where the maximum neighbor distance is small. In the previous example, the only neighboring tasks of task 21 with larger task IDs are tasks 22 and 25, and therefore the maximum distance to a succeeding neighbor is 4, and sd(21) = 16 − 4 = 12, which is also to the safe distance of task 37, the simulation task of switch 5 in cycle 2. For common, regular topologies such as meshes and cubes, such an order can be found using a constructive approach by traversing the network in dimension order. As our example already showed, ordering the nodes of a K × K 2D-mesh network results in a maximum neighbor distance of K or a minimum safe distance of (K − 1) × K . Generally for ndimensional meshes with K nodes in each dimension, this order yields a maximum neighbor distance of K n−1 and a minimum safe distance of (K − 1) × K n−1 . While for meshes in each dimension a start and endpoint can be identiﬁed, this is not the case for tori where each dimension is connected in a ring like manner. Therefore, traversing a torus in strict dimension order results in an inefﬁcient task order. Instead, for an n-dimensional torus, we traverse only n − 1 dimensions in strict dimension order and apply a breadth ﬁrst traversal to the last dimension (cf. Fig. 5 (left)). Applying this modiﬁed dimension order to a k-ary n-cube results in a maximum neighbor distance of 2×K n−1 and a minimum safe distance of (K − 2) × K n−1 . As shown, an appropriate order can be found for common, regular topologies in a constructive way. However, our decomposition approach is not limited to these topologies. Given a large enough NoC and a limited number of links per switch, an efﬁcient order can be found for arbitrary topologies, for which we will show two solutions: First, we introduce breadth ﬁrst order as a simple, constructive enumeration, providing feasible solutions and a lower bound on achievable safe distances. Second, we deﬁne the search for the ideal order as an integer linear programming (ILP) problem. Since efﬁcient task orders rely on the maximum neighbor distance being small, breadth ﬁrst order comes as a rather obvious solution. When processing a node in breadth ﬁrst order, all of its neighboring nodes are added to the order before any other node is visited, which limits the distance of neighboring nodes within the order. However, the resulting maximum neighbor distance varies depending on both the selected starting node and the order in which neighboring nodes are added. In the following, we will provide a worstcase estimation for the maximum neighbor distance and safe distance. 1) Breadth First Order: Let G(V , E ) be a connected, undirected, simple graph representing the topology of an NoC, with V the set of vertices or nodes, and E the set of edges. We write |G| = |V | for the number of nodes in the graph. For any v ∈ V , d(v) is the degree of node v , and ∆(G) = max{d(v)|v ∈ V } the maximum degree of graph G. For any given vertex v , N (v) is the set of v ’s neighbors, that is the set of vertices, adjacent to v : and ∀i, j ∈ [0, n − 1], i (cid:54)= j : vi (cid:54)= vj be a breadth ﬁrst order of all nodes in the graph, starting at an arbitrary but ﬁxed v0 . We deﬁne the maximum neighbor of a vertex v with respect to the breadth ﬁrst order as: N (v) = {v (cid:48) |(v , v (cid:48) ) ∈ E }. Let v0 , v1 , . . . vn−1 with n = |V | Nmax (v) = max{j |vj ∈ N (v)} (1) Since the ﬁrst vertex in the breadth ﬁrst order cannot have any previous neighbors, the maximum neighbor of vertex v0 is equal to v0 ’s degree, which is also limited by the maximum degree of the graph: Nmax (v0 ) = d(v0 ) ≤ ∆(G) (2) And because G is connected, all succeeding vertices vi must have at least one predecessor. Therefore, the number of unvisited neighbors of vi is less or equal to its degree minus 1. Also the indexes of the unvisited neighbors must be larger than the maximum neighbor of vi ’s predecessor, therefore Nmax (vi ) is limited to: Nmax (vi ) ≤ d(vi ) − 1 + Nmax (vi−1 ) ≤ ∆(G) − 1 + Nmax (vi−1 ) Using induction results in: Nmax (vi ) ≤ (i + 1) × ∆(G) − i (3) (4) (5) It can be seen that, while progressing in breadth ﬁrst order, maximum neighbor distance growth is limited by G’s maximum degree. Obviously Nmax is also constrained by the total number of vertices in the graph. We are interested in the last node vimax for which Nmax (vimax ) = |G| − 1 since from this point on, the relative distance to neighboring nodes can no longer grow. This is the case for (imax + 1) × ∆(G) − imax = |G| − 1. If no integer solution exists, vimax has less than d(vimax ) − 1 unvisited, neighboring vertices. But since the index of unvisited neighbors of vimax must have at least an index of Nmax (vimax−1 ) + 1, the relative distance of vimax to its neighbors is also at least as large as the relative distance of vimax−1 to its neighbors. Therefore we are looking for the smallest imax , for which the following holds: Solving for imax yields: (imax + 1) × ∆(G) − imax ≥ |G| − 1 (cid:24) |G| − ∆(G) − 1 ∆(G) − 1 imax = (cid:25) (7) Note that this requires ∆(G) ≥ 2, which is always the case for connected NoCs with more than 2 switches. Since we know that the largest possible relative distance occurs for imax , we can conclude that the maximum distance between any two neighboring nodes vi , vj ∈ V , (vi , vj ) ∈ E is less than or equal to Nmax (vimax ) − imax : (cid:24) |G| − ∆(G) − 1 |i − j | ≤ Nmax (vimax ) − imax ∆(G) − 1 = |G| − 1 − (cid:25) This imposes a limit to the maximum neighbor distance. Examining the minimum safe distance sd(G) = |G| − that sd(G) ≥ |G|/(∆(G) − 1) for sufﬁciently large enough max distance in relation to the size of the NoC, we ﬁnd |G| − (cid:16)|G| − 1 − (cid:108) |G|−∆(G)−1 NoC: (6) (8) (9) (10) (11) (12) (cid:109)(cid:17) (cid:25) |G| ∆(G)−1 (cid:24) |G| − ∆(G) − 1 ∆(G) − 1 sd(G) |G| = 1 = |G| + |G|→∞−−−−−→ 1 |G| × 1 ∆(G) − 1 2) Efﬁcient Task Order as ILP Problem: The problem of ﬁnding an ideal task order, where the safe distance is as large as possible, can also be deﬁned as an ILP problem. The beneﬁt of this is twofold. First, it allows for veriﬁcation of the previously proposed orders against the optimal solution. Second, it can be employed to run an ILP solver for a given topology to ensure the highest degree of independent tasks and the highest core utilization in all subsequent simulation runs. Given a topology of N arbitrarily ordered tiles, we deﬁne the ILP problem as ﬁnding a permutation of the initial order, representing the execution order during simulation, and an optimization goal of minimizing the overall maximum distance to neighboring tiles. The topology is represented as an adjacency matrix A of size N × N , with ai,j = 1 iff there is a link connecting tile i and tile j , or 0 otherwise. Subject to optimization is the permutation matrix P of size N × N , with pk,l = 1 iff task k is to be executed at position l, or 0 otherwise. To ensure the bijection property of the permutation, P is constrained to having exactly one 1 in each row and N −1(cid:88) column: N −1(cid:88) ∀k ∈ [0, N − 1] : pk,i = 1 (13) i=0 ∀l ∈ [0, N − 1] : pi,l = 1 (14) i=0 The properties of the permutation matrix P allow us to express the maximum distance of the topology by applying the following constraint to every two tasks i, j with ai,j = 1: N −1(cid:88) pi,k × k − N −1(cid:88) pj,l × l ≤ max distance (15) k=0 l=0 To avoid negative solutions, max distance is constrained to be of non-negative integer values. The ILP solver’s optimization goal is then set to minimize max distance in order to ﬁnd the optimum solution. With the ILP problem deﬁnition, we were able to verify the applicability of the proposed, constructive orders. In the case of mesh topologies, the optimum solution found by the ILP solver was the same as the proposed dimension ordered enumeration. In the case of two-dimensional tori, the results depended on the size of the torus. For example, in the case of 4 × 4torus, both breadth ﬁrst order and ILP problem found a solution with a maximum neighbor distance of 7, while the modiﬁed dimension order had a maximum neighbor distance of 8. However, in the case of 5×5 tori or larger, ILP was able to ﬁnd better solutions than the breadth ﬁrst order (max. neighbor distance of 9 instead of 10). Fig. 5 shows resulting orders on the example of a two-dimensional 5 × 5 torus for the proposed orders, with safe distances of 15 (mod. dim. order), 15 (breadth ﬁrst), and 16 (ILP). We also evaluated the performance of breadth ﬁrst order using randomly generated topologies. For topologies having a size of 50 tiles, the safe distance could be improved by up to 8, which equals 16% of the NoC’s size. Fig. 5. Task orders for 5-ary 2-cube topology: modiﬁed dimension order (left), breadth ﬁrst order (middle), ILP solution (right) V. EVALUAT ION Simulation performance evaluation was performed in two ways: First, we implemented a reference simulation kernel along with a basic, conﬁgurable NoC model, and compared its simulation performance to HORNET [15], a state of the art parallel NoC simulator that employs a traditional static decomposition scheme and is suitable for shared memory architectures. Second, we evaluated the performance of our approach on the example of randomly generated topologies using synthetically generated workloads. A. Performance of Our Implementation The wormhole switched NoC model used in our reference implementation supports mesh topologies of conﬁgurable size; different routing schemes such as XY-, Up/Down- [18], and O1TURN routing [19]; and ingress queued buffers of conﬁgurable size and number of virtual channels. Performance evaluation is based on synthetically generated, complement trafﬁc, which allowed exact reproduction of the simulated scenarios with HORNET. All simulations ran for a simulated duration of 50,000 cycles, as larger simulation periods were found to have no impact on simulation performance. A 16 core simulation machine having two processors of 8 cores each and a total of 64GB RAM was used to execute the simulations. All simulated scenarios were repeated using 1 to 16 threads, each ﬁxed to a dedicated CPU core. Fig. 6 (a) plots the simulation speedup (top) and relative speedup (bottom) for simulated NoCs of sizes 8 × 8, 16 × 16, and 32× 32. It can be clearly seen that simulation performance scales nearly linearly with the number of threads; however, with an efﬁciency depending on the simulated NoC’s size. Using 16 simulation threads, speedups of 8.9, 12.8, and 14.4 respectively were achieved. And in the case of the 32 × 32 NoC, speedups were close to the theoretical maximum throughout the simulation runs, having a relative speedup of at least 0.9 in all cases. While the speedup of 8.9 in the smallest case still yields a good result, the very basic and lightweight implementation of the NoC model limits the scalability. The single threaded simulation of the 8×8 NoC ﬁnishes in less than 20 seconds, corresponding to an average simulation duration of only 6 µs per tile and cycle. The resulting unfavorable computation to synchronization ratio prohibits achieving higher speedup. It is important to mention that simulation speedups increase monotonically and are very stable, i.e. using more threads never resulted in lower simulation performance. per tile and simulated cycle provides higher speedup when parallelized, we expect our approach to scale even better when applied to a complex simulator. B. Performance for Arbitrary Topologies We used a set of 60 randomly generated, connected graphs of average sizes 50, 100, and 250 to evaluate the applicability of our approach to arbitrary topologies. Graph sizes varied by +/- 1 vertex. We simulated each graph using synthetically generated workloads. As our approach would beneﬁt from high mean workloads µ per task, we carefully selected µ not too high yet still representing a realistic workload. Therefore, we chose µ such that the simulation duration of a 50 tile NoC under uniform workload distribution would require approximately 100 seconds, which is less than, but still comparable to the simulation duration of a 8 × 8 NoC using HORNET, taking about 170 seconds. To evaluate the impact of unbalanced workloads, we simulated each topology four times with tasks having different Gaussian distributed workloads, using variances of σ = {0.5, 1, 2, 4} × µ. Negative workloads were saturated to 0, corresponding to an empty task. For all topologies, we performed simulations with safe distances determined using the breadth ﬁrst order, and repeated simulations for topologies of size 50 with the optimum safe distance determined using an ILP solver. Fig. 7 plots the simulation performance for the simulated scenarios. The results show that if workload does not vary too much (σ = 0.5 × µ), our decomposition approach works well for all simulated sizes and even for small safe distances, as speedups of 14.5 to 15.5 were achieved. While this concurs with our earlier described simulation behavior under uniformly distributed workloads (c.f. Fig. 3), it is notable that even for σ = 0.5 × µ, about 1/3 of all workloads are either 0 or ≥ 2 × µ. As expected, the simulation performance decreases if workloads vary too much and the safe distance is too low to compensate the unbalanced workload. In the cases of σ = 2 × µ and σ = 4 × µ, the large number of empty tasks (30% and 40%) impacted performance depending on the ratio of safe distance to the number of simulation threads. While for the smallest sizes, the degree of independent work (26 independent tasks vs. 16 worker threads) limited the speedup to 9.0 (σ = 2 × µ) and 8.0 (σ = 4 × µ), the effect is mitigated with increasing safe distance. For same-sized topologies, ILP was able to provide solutions raising speedups to 11.7 and 10.3 respectively. Increasing the topology’s size to 100 and 250 vertices resulted in speedups of 12.6 and 15.5 for σ = 4 × µ, conﬁrming that our proposed decomposition approach is unimpaired by non-uniform workload, provided that topologies are sufﬁciently large. V I . CONCLU S ION For parallel NoC simulation, we have introduced a novel decomposition methodology that scales linearly with number of simulation threads and that, due to its inherent load balancing property, is to a large degree unaffected by workload imbalances. These improvements are achieved by explicitly (a) Implementation (b) HORNET [15] Fig. 6. Comparison of parallelization speedups between our own reference implementation (a) and HORNET [15] (b) We used HORNET (cf. Fig. 6 (b)) to compare our approach to traditional barrier based decomposition. It can be seen that the barrier-based approach requires a high workload to scale signiﬁcantly. In the case of the smallest NoC, speedups were not able to surpass a factor of 4.6 and employing more than 6 threads did not notably increase simulation performance. By default HORNET randomly assigns tiles to threads, assignations that remain ﬁxed throughout the simulation. This approach seems to balance workload reasonably well, as long as the NoC is large enough. For the 16 × 16 NoC, very good speedups were achieved for up to 7 threads (speedup 6.8), and speedups increased for up to 9 threads (speedup 8.0) from where on no signiﬁcantly higher simulation performance was possible, resulting in a maximum speedup of 9.5. Not surprisingly, the 32 × 32 NoC yielded the best speedup, with a peak at 11.9 and a relative speedup of 0.74. Interestingly, for the two larger NoCs, increasing the number of threads from 15 to 16 signiﬁcantly improved the speedup from 8.2 to 9.5 and 10.2 to 11.9, respectively. This improvement indicates that HORNET is susceptible to cases where the number of cores does not divide the number of tiles, making the scalability of the approach questionable for arbitrary topologies. In direct comparison, our achieved speedups were higher for all simulated scenarios, which is especially remarkable in the smallest case. While the barrier based decomposition was limited to a speedup of 4.6, our approach was able to achieve a speedup of 8.9, which is fairly close to the simulation speedup of 9.5 achieved for the 16 × 16 NoC simulated using HORNET. Similarly, our 16×16 performance with a maximum speedup of 12.8 surpassed the 11.8 speedup achieved for HORNET’s 32 × 32 performance. It is worth mentioning that the feature rich simulation of HORNET has a signiﬁcantly higher workload compared to our reference implementation, e.g. in the single threaded case of the 8 × 8 NoC, the runtimes for HORNET and our reference implementations were 164.79 seconds and 19.19 seconds, respectively. As more computation (a) avg. size: 50, avg. safe dist.: 26 (b) avg. size: 50, avg. safe dist.: 36 (ILP) (c) avg. size: 100, avg. safe dist.: 49 (d) avg. size: 250, avg. safe dist.: 95 Fig. 7. Average simulation performance for arbitrary topologies of sizes 50, 100, and 250, evaluted using synthetic workloads. ordering simulation subtasks, thus reducing thread blocking to an unavoidable minimum, and by trimming down synchronization efforts to a bare atomic counter increment, leveraging hardware support. The applicability of our approach was shown using a reference implementation of a simple NoC model for differently sized NoC, and its performance for arbitrary topologies was evaluated on randomly generated graphs with synthetic workloads of different distributions. Achieving speedups of 15.5 on a 16 core simulation machine conﬁrmed the highlighted beneﬁts of our approach. While we targeted synchronous NoCs, future research will include evaluation of the application on other types of NoCs such as GALS, and plesiochronous systems. ACKNOW L EDGM ENT This work has been supported by the German Research Foundation (Deutsche Forschungsgemeinschaft, DFG) under grant Ra 1889/4-1. "
An accurate and scalable analytic model for round-robin arbitration in network-on-chip.,"Due to continuously increasing performance requirements of embedded applications, today's multi-processor system-on-chips will evolve towards many-core system-on-chips with thousands of processors on a single chip. Accurate, fast and flexible (i.e., parameterizable) simulation models are necessary to be able to analyze and optimize these large systems. Network-on-chip is a common solution for the interconnection of large processor arrays. Existing analytic models for the performance analysis of network-on-chip often possess a lack of accuracy, if applied for the popular round-robin arbitration scheme. It turns out to be challenging to find an appropriate analytic representation for this apparently simple scheme. In this paper, we propose an accurate service time estimation model that is designed for round-robin arbiters. It is further employed to a queueing model for network-on-chip. The comparison with cycle-accurate simulation proves the accuracy of the proposed service time model, which is essential for predicting key performance indicators, such as network throughput or latencies.","An Accurate and Scalable Analytic Model for Round-Robin Arbitration in Network-on-Chip Erik Fischer and Gerhard P. Fettweis Vodafone Chair Mobile Communications Systems Technische Universit ¨at Dresden Dresden, Germany Email: {erik.ﬁscher, fettweis}@ifn.et.tu-dresden.de Abstract—Due to continuously increasing performance requirements of embedded applications, today’s multi-processor system-on-chips will evolve towards many-core system-on-chips with thousands of processors on a single chip. Accurate, fast and ﬂexible (i.e., parameterizable) simulation models are necessary to be able to analyze and optimize these large systems. Networkon-chip is a common solution for the interconnection of large processor arrays. Existing analytic models for the performance analysis of network-on-chip often possess a lack of accuracy, if applied for the popular round-robin arbitration scheme. It turns out to be challenging to ﬁnd an appropriate analytic representation for this apparently simple scheme. In this paper, we propose an accurate service time estimation model that is designed for round-robin arbiters. It is further employed to a queueing model for network-on-chip. The comparison with cycleaccurate simulation proves the accuracy of the proposed service time model, which is essential for predicting key performance indicators, such as network throughput or latencies. Keywords—network-on-chip; noc; round-robin; queueing theory; analytic model. I. Introduction The recent trend in embedded processor design makes it obvious that more and more parallelism will be necessary to fulﬁll the continuously increasing performance and reliability requirements of modern embedded applications. Therefore, multi-processor system-on-chips (MPSoCs) scale to many-core SoCs with thousands of processors on a single chip in the near future [1]. Network-on-chip (NoC) has emerged as ﬂexible and suitable design approach to solve the interconnection problem for MPSoC during the last decade [2][3]. NoC is a packet switched network where router nodes are used to propagate a packet from a source module to a target module. The routers can be arranged in an arbitrary topology and are connected with an arbitrary number of modules. Furthermore, a ﬂit (the basic transmit unit in NoCs) can be propagated using diﬀerent routing, switching and arbitration schemes. On the one hand, this large variety of parameters is the essence for high ﬂexibility. On the other hand, it spans a very large design space [4]. This makes the optimization of the interconnection infrastructure challenging [5]. With respect to many-core SoCs, the network-on-chip will become a bottleneck. Therefore, parameters, such as network topology, routing strategy, etc., must be investigated carefully for this next generation of SoC. Fast and ﬂexible (i.e., parameterizable) analytic models are necessary for a comprehensive design space exploration (DSE). The DSE usually starts with a very large design space (e.g., many diﬀerent topologies, routing schemes, core mappings, etc.). That’s why the design space has to be reduced iteratively by discarding alternatives with the worst performance. A high accuracy is essential to acquire reliable information for the design optimization already in an early design phase. The authors of this paper observed that existing analytic models are not able to provide a suﬃcient accuracy. This is especially the case for NoC routers using the popular round-robin arbitration scheme. This arbitration scheme oﬀers a low complexity and local fairness [6] and is therefore used in many existing network-on-chip designs for handling best-eﬀort traﬃc [7][8][9]. The arbitration scheme has a strong impact on the whole network throughput, bottlenecks and path latencies. Therefore, it inﬂuences design decisions signiﬁcantly. Wrong design decisions can lead to over-provisioning, i.e., waste of chip area. On the other hand, performance requirements may not be fulﬁlled, which is even worse. Thus, it is essential to employ arbitration models that oﬀer a high accuracy. Figure 1 gives an impression of the complex traﬃc ﬂow within routers. Therein, we see splitting and merging of traﬃc ﬂows. Collisions can occur, which must be resolved by an arbitration algorithm, such as roundrobin. Furthermore, the collision resolution is dependent on the current state of the arbiter and it diﬀers between the inputs. Though, round-robin is an apparently simple arbitration algorithm, existing analytic approaches cannot reﬂect the internal collision and collision resolution behavior precisely. In this paper, we introduce an analytic service time model that is speciﬁcally designed to reﬂect the behavior of roundrobin arbiters and oﬀers a high accuracy. Following, the service time model is used to extend an existing queueing model for NoC routers [10]. The model provides information about the steady-state distribution of the routers. Based on this, many important network performance metrics, such as mean latencies or network throughput, can be derived. The remainder of this paper is structured as follows. In n i b o R d n u o R n o i t a r t i b r A Router collision Fig. 1. Collisions in routers with merge and split of traﬃc ﬂows controlled by round-robin arbitration.   Section II, related work is summarized. Section III shows the system model that underlies the analytic model. In Section IV, the analytic service time model for round-robin arbiters is discussed in detail. In Section V, it is applied to extend an existing queueing model for further NoC performance analysis. The accuracy of the proposed approach is demonstrated by comparison with a cycle-accurate simulation and existing analytic performance models in Section VI. Finally, Section VII concludes the work. II. Related Work In 1998, Lysne proposed an empirical model for the service time estimation of round-robin based routers in wormhole routing networks [11]. Many diﬀerent approaches have been proposed recently to employ analytic models for the NoC performance analysis. In 2009, Nikitin et al. proposed an empirical model to estimate contention delays for constant service time routers [12]. This approach considers the special characteristics of the output ﬂows of a constant service time router, which causes a de-randomization of the Poisson inputs. Also in 2009, Bakhouya et al. proposed an analytic model based on Network Calculus [13] for the estimation of performance bounds in NoCs [14]. Ogras et al. introduced a low complexity analytic approach in 2010 for the mean value performance analysis in NoCs [15]. Therein, virtual channel ﬁrst-come ﬁrst-serve (FCFS) input buﬀered routers are assumed. An arbitrary topology and service time distribution are supported. In 2012, Kiasari et al. proposed a ﬂexible G/G/1 queueing model for estimating the average packet latency in NoCs with arbitrary network topology and deterministic routing [16]. The approach also consists of a channel service time estimation model. Also in 2012, an interesting approach has been published for the analysis of the mean waiting time in a weighted round-robin service system [17]. Though it is a more general approach, the assumed service time model is very simple and does not consider contention of multiple concurrent inputs or forwarding to diﬀerent outputs. Therefore, this model is not suﬃcient to reﬂect the situation in a NoC router. In 2012, another approach was published based on queueing theory [10] to derive the steady-state distribution of the network routers assuming M/M/1 queues. The approach is very ﬂexible in terms of topology, routing scheme, and traﬃc pattern and it allows to derive arbitrary performance metrics. However, by investigating existing models, it has been found that the special characteristics of the common roundrobin arbitration scheme are not well reﬂected [11][12][15][17] or neglected [10][14][16]. The accuracy is not satisfying for several examined scenarios. In this paper, we propose an analytic model for the service time estimation that is especially designed for round-robin arbiters and therefore provides a high accuracy. III. System Model In this paper, we assume routers to be connected in an arbitrary topology. An arbitrary number of modules (including zero) can be connected to a router. Furthermore, the routing scheme is not restricted. We assumed packet-by-packet switching, due to the inﬁnite buﬀer assumption of the queueing model, as stated in the following. However, the switching discipline only aﬀects the queueing model. The service time model is independent of the chosen switching discipline. In addition, round-robin arbitration scheme is employed. h c t i w S . . . ... . . . ... r a b s s o r C Input FIFOs Routing &  Arbitration Fig. 2. System model assumption of router architecture. Figure 2 gives an overview of the assumed router architecture. Each router r consists of a certain number of buﬀered inputs N r i and unbuﬀered outputs N r o . The mean arrival rates λr i at the router inputs i and the mean service rate µr must be known for each router r. Note that the mean service rate µr is assumed to be equal for all router inputs i. However, this is no general restriction, since it is straightforward to extend the model for input speciﬁc service rates. The forwarding probability f r i,o denotes the probability that a packet, which arrives at input i, is propagated to output o. f r i,o must also be known for each router. Note that the index r will be omitted for the purpose of convenience when performing local analysis on router level. Further assumptions are necessary in order to derive the extended queueing model in Section V. An unlimited space is assumed for all input buﬀers. All router inputs are driven by Poisson processes [18]. They have exponentially distributed inter-arrival times that possess the Markov (”memoryless”) property. This allows to approximate realistic (random) network traﬃc while simultaneously reducing the model complexity. Indeed, concrete packet arrival patterns are often not known in an early stage of the DSE, or would restrict the application area too much. Therefore, the assumption of random Poisson traﬃc is reasonable for this purpose. Finally, the Markov property must also hold for the distribution of the service times. Here, the router service times include the delay for arbitration, switching, and forwarding of a packet over the link. Note that these restrictions only apply for the employed queueing model and do not relate to the service time model, which is presented here. The service time model is independent of the distribution of the arrival process or buﬀer capacities. It can therefore be employed for other, more general, queueing models as well. Figure 3 illustrates the method that is used for the analytic NoC performance estimation. Thereby, we follow an approach to decouple the service time estimation and the actual queueing model. This facilitates the queueing model signiﬁcantly, since the traﬃc situation within a NoC router can be quite complex, as shown in Figure 1. By means of the service time estimation, the queueing model can consider each input queue in isolation, i.e., independent of the concurrent queues. Furthermore, this brings the advantage of a high ﬂexibility, since the arbitration model can be altered without changing the queueing model. Figure 3 depicts how the service time estimation is integrated into the analytic NoC model. Based on the model inputs   Model Input Topology Traffic Routing Network Model Router Inputs Router Model Router Model instance 1 instance r Service Time  Estimation Service Times Queueing  Model .  . . . .  . Fig. 3. Structural overview: method for analytic NoC performance estimation. (topology, traﬃc, and routing scheme), the network model derives local router parameters, such as the arrival rates λi and forwarding probabilities fi,o . The network model is explained in detail in [10]. The router model is executed for each router instance and starts with the service time estimation, which is introduced in Section IV. Finally, the estimated service times xi,o are employed by the queueing model to compute performance indicators, such as network throughput or latencies. A short summary of the queueing model is given in Section V. More details can be found in [10]. IV. Service Time Estimation The service time estimation model follows an empirical approach. It has been derived by behavioral analysis of a cycle accurate arbiter simulation model. The analytic model consists of two parts that are derived in the following. 2) 1) We determine the probability that contention occurs when a packet is requesting service to be routed from input i to output o (Section IV-A). The contention resolution behavior is considered, which can be characterized by the probability that a packet is forced to wait under a certain contention situation (Section IV-B). TABLE I. Notation overview B = (cid:2)Bm,n (cid:3) Binary ”truth table” matrix with contention case m and competing input n ci,o di,o,m fi,o λi λi,o NC,o Ni No ri,o ρi ρi,o wi,o x xi,o Collision probability for routing a packet from input i to output o Decision probability: probability that input i has to wait for access to output o under contention situation m Forwarding probability from input i to output o Arrival rate at input i Traﬃc intensity from input i to output o Number of competing inputs for output o Number of router inputs Number of router outputs Collision resolution distribution Buﬀer utilization of input i Path utilization from input i to output o Waiting probability for input i accessing output o Mean packet service time Mean service time (including contention delay) for packet propagation from input i to output o A convenient overview of the notation that is used in following equations is provided in Table I. We assume that we have knowledge about the mean packet arrival rates λi and forwarding probabilities fi,o for all router inputs i and router outputs o. We can determine the traﬃc intensities from i to o according to (1): λi,o = λi · fi,o . (1) Given the mean service time xi,o for serving a packet from input i to output o, the corresponding path utilization ρi,o within the router switching fabric is derived as follows: ρi,o = λi,o · xi,o . (2) Equation (2) follows the common deﬁnition of the utilization factor for a single server system in queueing theory [18]. We deﬁne path utilization ρi,o as the probability that input i is either requesting access for output o or currently using this connection. The buﬀer utilization factor ρi of input i can be deﬁned based on the mean service time xi , as shown in (3) ρi = λi · xi , (3) with xi = PNo o=1 xi,o · fi,o . Furthermore, the following relation can be obtained: ρi = NoX o=1 ρi,o . (4) The contention resolution is the very speciﬁc part, which is determined by the characteristics of the arbiter. Here, we focus on round-robin arbitration. Equation (4) must hold, since every packet that is requesting a routing path from input i to output o must also occupy the buﬀer of input i. A. Contention Probability Let us examine the collision probability by means of a small example. Assume a router with three inputs. Now, the collision probability c1,o , when routing a packet from input 1 to an arbitrary output o, is determined: c1,o = P (cid:2)path 1 → o used(cid:3) · P (cid:2)path 2 ∨ 3 → o used(cid:3). In (5), we see that a collision occurs, if inputs 1 and simultaneously input 2 or 3 are requesting access to output o. This is expressed by the following short form notation: (5) B =  1 0 0 1 1 1  . (10) The ﬁrst two rows in (10) represent the collision case with one competing input, respectively. The third row represents the collision case with both competing inputs. The example in (10) is directly related to the case in (7). By means of (9), a generalization for (8) can be derived: c1,o = P1→o P2∨3→o . (6) ci,o = 2Ni −1 −1 X m=1 ρi,o · Ni −1 Y n=1 hBm,n ρi n,o + (cid:0)1 − Bm,n (cid:1) (cid:16)1 − ρi n,o (cid:17)i. (11) The probability P2∨3→o that a path from input 2 or 3 is requested to o can be decomposed into ”(either 2 or 3) or (2 and 3)”. Accordingly, (6) is expanded to (7): c1,o = P1→o P2→o P3→o + P1→o P2→o P3→o |                                      {z                                      } 2x collision + P1→oP2→o P3→o |             {z             } 3x collision . (7) Clearly, we can represent the probability Pi→o , that the path from input i to output o is used, by the path utilization factor ρi,o (as deﬁned in (2)). Therefore, we can rewrite (7): c1,o = ρ1,o · (cid:0)ρ2,o (1 − ρ3,o ) + (1 − ρ2,o ) ρ3,o + ρ2,o ρ3,o (cid:1) . (8) In (8), a general pattern can already be recognized. Therein, the path utilization probability of the considered input ρ1,o has to be multiplied with every combination of the path utilization probabilities and inverse path utilization probabilities (i.e., the probability for the path to be unused) of every competing input. The sum over all these combinations yields the total collision probability. Equation (8) shows similarities to the binomial distribution. However, in contrast to the binomial distribution, the random variables (i.e., path utilization) are not identically distributed. If we target just at the computation of the collision probability, it would not be necessary to expand the sum in this way to consider every possible contention situation separately. However, as it is shown later on, the contention resolution is diﬀerent for each case. To derive an expression for the general case, we introduce a binary ”truth table” matrix B: Therein, ρi o denotes a subvector of ρo that consists of the path utilization factors of all competing inputs of i for output o. The ﬁrst summand within the product (Bm,n ρi n,o ) contributes the path utilization factors while the second summand contributes the inverse path utilization factors. Finally, the sum over all possible contention cases (index m) is computed. B. Contention Resolution Equation (11) yields information about the collision probability. The remaining question is: Given a contention for output o, what is the probability of a considered input i that it has to wait for an access to the output? For this purpose, we ﬁrst derive the collision resolution distribution. It can be characterized by the probability mass function ri,o . The function speciﬁes the average probability of an input i to be selected at ﬁrst to be granted access to the output o by the round-robin arbiter (i.e., no waiting for service is necessary). The collision resolution distribution is independent of the concrete contention situation. Assume two competing inputs for the same output. Input 1 has a high traﬃc intensity to the output, Input 2 has a comparatively low traﬃc intensity. Therefore, the roundrobin arbiter is quite often selecting input 1. Now, consider a contention situation between both inputs. The chance is high that the last selected input was input 1. According to the roundrobin scheme, the next input (input 2) is selected (i.e., gets the highest arbitration priority). Finally, we can conclude that the higher the traﬃc intensity from a certain input to an output (in comparison to the competing inputs), the higher the chance that it has to wait in case of a collision. This idea motivates following equation for the collision resolution distribution ri,o : (9) Bm,n = (cid:18)(cid:22) m 2n−1 (cid:23) mod 2(cid:19) , 1 ≤ m < 2Ni −1 , 1 ≤ n < Ni . The matrix covers all possible collision scenarios, where a row of the matrix represents a single contention case of the Ni − 1 competing inputs. Therein, the row index m is used to identify the contention case and the column index n identiﬁes the competing input. Bm,n is 1, if n is considered as competing input for contention case m and 0 otherwise. Every row of B contains at least a single 1-element. Otherwise, this row would represent the non-collision case, which does not contribute to the collision probability. Following, a small example is provided for the case of three inputs (Ni = 3), i.e., two contending inputs: ri,o = 1 NC,o PNi n=1,n,i ρn · fn,o PNi n=1 ρn · fn,o . (12) The enumerator of (12) accumulates the path utilization (ρn · fn,o ) for output o over all competing inputs n. The denominator accumulates the path utilization of all inputs, including the considered input i. Thus, the higher the path utilization (which is directly related to the traﬃc intensity) of input i compared to the path utilization of the competing inputs, the smaller this fraction will be. This behavior reﬂects the idea mentioned above. Furthermore, we ﬁnd in (12) the factor NC,o , which is the number of competing inputs for output o. It can be determined by following equation: NC,o = NiX i=1 (cid:6)λi,o (cid:7). (13) (due to contention) can be derived. By adding the mean service time for the packet itself x, the mean service time including contention delay xi,o for packets propagated from input i to output o is computed: xi,o = x + wi,o λi,o . (16) In (15), we see that the path utilization factors ρi,o must be known for estimating the mean service times xi,o . On the other hand, the path utilization factors depend on the mean service times, according to (2). Consequently, (15) characterizes a system of equations. Since the closed form solution would be quite cumbersome, we propose an iterative solution algorithm, following the idea of Lysne [11]. The pseudo code of the proposed algorithm is illustrated in Algorithm 1. Therein, all necessary equations are referenced. Algorithm 1 Iterative Service Time Estimation Eq. (9)(1) Eq. (2)(4) Eq. (13)(12)(14)(15) Eq. (16) Require: λ, f , x, thre shold 1: Precompute B, λi,o 2: Initialize xi,o ← x 3: for each iteration do Calculate ρi,o , ρi if (∃ ρi > 1) then exit with error end if for each output port do Calculate NC , r, d, w Calculate xnew end for Check convergence ∆i,o ← | xnew if (∀ ∆i,o < thre shold) then ﬁnish last iteration end if Update xi,o ← xnew 17: end for 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: i,o i,o i,o − xi,o | First, the binary matrix B and traﬃc intensities λi,o are precomputed (line 1). The mean service times xi,o are initialized with x (the mean service time without contention) (line 2). The for-loop (line 3) is executed until a suﬃcient accuracy is reached, i.e., all estimated service times are stable, according to the convergence criterion in line 12-13. At the beginning of each iteration the utilization factors (ρi,o and ρi ) are updated (line 4). The check for ρi > 1 (line 5) ensures the stability and convergence of the algorithm. For each router output the number of contending inputs NC , the collision resolution distribution r, the decision probabilities d, and the waiting probabilities w are calculated (line 9). Finally, the mean service times xi,o are updated (lines 10 and 16) and the algorithm continues with the next iteration. The convergence of the algorithm depends on the considered traﬃc scenario, the number of router inputs, and the selected threshold. By examining a series of diﬀerent traﬃc scenarios for a router with ﬁve inputs, using a threshold of 0.001, an average of 6 iterations and a maximum of 12 iterations has been observed. These results are intended to give a general idea of the convergence behavior. However, they represent no upper bound. In (13), every input i with a traﬃc intensity λi,o > 0 counts as competing input for output o. This is expressed by means of the ceil operation (⌈·⌉). Note that in (12) we used ρn · fn,o instead of ρn,o in order to describe the path utilization. This is motivated by following idea. Assume a situation where the total traﬃc of input i is propagated to a single output o with traﬃc intensity λi,o = λi . Now, assume a second case where the traﬃc is split onto a second output with a 50% chance. The arrival rate in turn is twice as high. Consequently, the traﬃc intensity via output o is the same as in the ﬁrst case: λi,o = 2 · λi · 0.5 = λi . However, it has been found that the collision resolution is nevertheless inﬂuenced by the blocking traﬃc ﬂow that is propagated to the second output. Therefore, using ρn · fn,o instead of ρn,o as path utilization is a better estimator for the collision resolution probability in order to reﬂect the behavior for the case mentioned above. Note that in general ρn · fn,o , ρn,o . With (12) the average collision resolution probabilities ri,o can be estimated. For a speciﬁc contention situation, however, we must also take into account which inputs are actually part of the contention in order to determine the instantaneous collision resolution factor for each input. I.e., we have to blank out non-contending inputs and rescale the collision resolution distribution ri,o for the remaining inputs according to a concrete contention case m (w.r.t. the row index of the binary matrix B that covers all possible contention cases). This yields the decision probability di,o,m that corresponds to the probability that input i has to wait for access to output o under a concrete contention situation identiﬁed by the index m. The decision probability is computed as follows: di,o,m = PNi −1 n=1 Bm,n · r i n,o ri,o + PNi −1 n=1 Bm,n · r i n,o . (14) In (14), the binary matrix B is used to blank out noncontending inputs for the concrete contention case m. The enumerator in (14) accumulates the collision resolution probabilities r i n,o of the competing inputs n (r i o denotes a subvector of ro that consists of the collision resolution probabilities of all competing inputs of i for output o). This is scaled to the overall sum of the collision resolution probabilities for the concrete contention situation, including the collision resolution probability of the considered input itself (ri,o ). The decision probability represents the probability that input i has to wait for access to output o, given a concrete contention situation. Therefore, by inserting the decision probability into (11) for computing the collision probability, an expression for estimating the waiting probability for input i accessing output o can be derived: wi,o = 2Ni −1 −1 X m=1 ρi,o · Ni −1 Y n=1 hBm,nρi n,o + (cid:0)1 − Bm,n(cid:1) (cid:16)1 − ρi n,o (cid:17)i · di,o,m . By scaling the waiting probability wi,o with the corresponding traﬃc intensity λi,o , the average waiting time per packet The presented service time model is applied to extend an existing queueing model [10]. The model represents the (15) V. Application to Router Queueing Model state of the router input queues by a multidimensional Markov chain. Therein, a single dimension represents the ﬁll state of a single input queue, i.e., the number of packets contained in the input queue. Therefore, a state in the Markov chain is speciﬁed by a vector of length Ni . The state transitions in the Markov chain are characterized by the arrival rates λi and mean service rates µi for each input i. The arrival rates are given as input parameter to the model. The mean service rates, however, have to be estimated. In [10], a service time estimation approach is proposed that is well suited to reﬂect the behavior of ﬁrst-come ﬁrst-serve (FCFS) arbiters. However, as the next section reveals, this is no good approximation for round-robin arbiters. Therefore, we replace the service time estimation by the approach described in Section IV. The mean service rates µi per input i can be derived from (16) by weighting the mean service time xi,o from input i to output o with the corresponding forwarding probability fi,o and computing the sum over all outputs o: 1.4 1.2 1 0 1.2 1.1 1 0 1.4 1.2 No rthern router input Sim RRMod RefMod 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Eastern router input Sim RRMod RefMod 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Router input o f module Sim RRMod RefMod e m i T e c v i r e S e m i T e c v i r e S e m i T e c v i r e S 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Injection rate (f lits/cycle/module) 0.35 0.4 0.45 µi = 1 PNo o=1 xi,o · fi,o . Fig. 4. Service times of north (top), east (middle) and module (bottom) input of a 5-port center router in an 8x8 NoC with uniform traﬃc. (17) The reciprocal in (17) is used to convert the service time into a service rate. Based on known arrival rates and the computed service rates, we can follow the approach in [10] in order to ﬁnd the macro state probabilities σ( y) for the multidimensional Markov chain. Based on the macro state probabilities, performance metrics, such as the mean number of packets in the queue E[ xi ], can be derived: E[ xi ] ≈ X y ρi ( y) 1 − ρi ( y) σ( y). (18) Note that in (18) xi represents the (ﬁll) state of input queue i. The buﬀer utilization ρi can be computed according to (3) and y corresponds to the router macro state. For more detailed information about this queueing model, refer to [10]. By applying Little’s law [18], we can ﬁnally compute the mean queueing delay Wi for input i: E[ xi ] λi which is applied for estimating path latencies in the following section. Wi = , VI. Performance Evaluation We analyze the performance of our proposed analytic model for the service time estimation in two steps. First, we compare the estimated service times on router-level. Therefore, a cycle-accurate round-robin arbiter model is employed to serve as reference for the analytic model. In addition, it is compared to the performance of the analytic round-robin model proposed by Lysne [11]. Following, the mean path latencies are investigated on network level in order to demonstrate the suitability of our proposed approach for the NoC performance analysis on a large scale. A cycle-accurate NoC simulator [19], as well as diﬀerent analytic models [10][11][15], serve as reference and benchmark, respectively. For the analysis on router level, a 5-port router has been considered. The run time of the cycle-accurate reference is 100, 000 cycles. The communication pattern is generated according to the traﬃc that a center router of an 8 × 8 2Dmesh NoC (at location [4, 4]) would experience in case of uniform network traﬃc conditions. Note that the local arrival rates and forwarding probabilities are not uniform for this case. We assume Poisson input ﬂows and single-ﬂit packets. Module injection rates between 0.01 and 0.42 ﬂits/cycle/module have been simulated (0.42 ﬂits/cycle/module is around the network saturation point of an 8 × 8 2D-mesh NoC). The injection rate corresponds to the arrival rate at the module input ports of the router. The results of the simulation series are presented in Figure 4. For the purpose of clarity, only the results of three of the ﬁve router inputs are depicted. In all three cases, the estimated mean service times xi of our proposed analytic model (RRMod) are very close to the results of the cycleaccurate simulation (Sim) for all considered injection rates. The reference model (RefMod) [11] shows a slight underestimation of 3% (northern router input) or overestimation of 7.5% (router input of module) of the service times, respectively, that increases with higher injection rates. In contrast, the mean relative error of our proposed approach is much less than 1% in all three cases. After investigating the mean service times on router level, the accuracy of latency estimation is investigated on a large scale in the following. Therefore, an 8x8 2D-mesh topology is employed. Referring to Section V, our proposed service time estimation was applied to extend an existing NoC queueing model [10]. A deterministic dimension ordered (XY) routing and round-robin arbitration has been applied. A packet size of 1 ﬂit was chosen to emphasize the inﬂuence of the queueing delay. In order to analyze the behavior under realistic application conditions, an application-speciﬁc traﬃc pattern was applied. Three types of multi-media applications were used: Video Object Plane Decoder (VOPD), MPEG4 Decoder (MPEG4), and Multi-Window Displayer (MWD) [20]. Each application consists of twelve tasks. Then, ﬁve multi-media applications were selected (1×MPEG4, 2×VOPD, 2×MWD) and their tasks were randomly assigned to processors. I.e., 60 of the 64 available processors are busy. The communication intensity between the tasks ﬁnally determines the application                  speciﬁc traﬃc pattern that is used for the NoC. As mentioned before, a cycle-accurate NoC simulator is used as reference [19]. Therefor, a total amount of 500, 000 ﬂit injections has been simulated in each run. The simulator employs routers with input buﬀers that can store 128 ﬂits each. The large buﬀer size is used to approximate the inﬁnite buﬀer queueing model. In addition to the cycle-accurate simulation, three analytic reference models have been considered as benchmarks: • • • Model 1: the analytic mean value approach for latency analysis in NoC proposed by Ogras et. al. [15], Model 2: the queueing model proposed in [10] without the round-robin service time extension, and Model 3: the queueing model proposed in [10] using the round-robin service time estimation proposed in [11]. The results of this simulation series are depicted in Figure 5. The ﬁgure clearly shows the high accuracy of our proposed model (Round-Robin Model) compared to the simulation reference (Cycle-accurate Simulation). The mean error of our proposed analytic model for all considered injection rates (below network saturation) is less than 6%. The three reference models exhibit a mean error of 17%, 13%, and 34%, respectively. At low injection rates (up to 6.5 packets/cycle), all reference models are able to estimate the average latency accurately, since the queueing delay is negligible in this case. When the injection rate is reaching the point of network saturation, the queueing delay rapidly increases. It can be observed that the considered reference models are not able to estimate the latency with a suﬃcient accuracy around this region. Thus, it is essential to estimate the network saturation point with high accuracy. Figure 5 demonstrates that the accuracy of the service time estimation has a strong inﬂuence on the estimation of the network saturation point. Though the relative accuracy gain seems not to be so big, it’s essential for the DSE to estimate the network saturation point very precisely. By underestimating the saturation point, the designed system will be over-provisioned. By overestimating the saturation point, we risk to select false candidates, i.e., network conﬁgurations, which are actually not able to fulﬁll the performance requirements. Figure 5 yields a highly aggregated representation of the results for several injection rates. However, sometimes a high aggregation can hide inaccuracies on a ﬁne-grained level. E.g., critical paths are not recognized correctly, even though the average network latency is quite close to the reference. To study the behavior of the presented model on a more ﬁnegrained level, we focus in the following on two selected injection rates close to the network saturation: 6.9 packets/cycle and 7.12 packets/cycle. Further investigations of the model behavior require some aggregation of the results. Therefore, we aggregate all path latencies of traﬃc ﬂows that span the same distance, i.e., have to pass the same number of routers (called number of ”hops”). Since packets are never routed back to the source, the minimum distance is two hops. The maximum distance for the considered 8 × 8 NoC is 15 hops. However, for the application-speciﬁc traﬃc pattern, the largest routing distance is 12. In the absence of contention, we expect a linear interdependency between routing distance and latency. ] l s e c y c k c o c l [ y c n e a t l t e k c a p e g a r e v A 50 45 40 35 30 25 20 15 10 5 Cycle-accurate Simulation Round-Robin Model Model 1 Model 2 Model 3 5.5 6 6.5 7 Packet injection rate (pkt/cycle) 7.5 8 Fig. 5. Average latencies for 8 × 8 2D-mesh NoC with application-speciﬁc traﬃc at diﬀerent injection rates. Every hop requires two clock cycles in the considered NoC conﬁguration. This can be observed at an injection rate of 6.9, as shown in Figure 6(a). The queueing delay is still negligible at this injection rate. From the results, we can conclude that the behavior of the cycle-accurate simulation is accurately estimated by our proposed analytic model (Round-Robin Model). The reference models 1 and 3 also demonstrate a good accuracy at an injection rate of 6.9. Only reference models 2 shows a slight overestimation of the aggregated path latencies of up to 32%. This is due to the underestimation of the network saturation point, as it can be seen in Figure 5. We conclude that the service time model has no big inﬂuence far below the network saturation point. The situation looks completely diﬀerent, if the injection rate is increased to 7.12 (which is very close to the network saturation). In this case, the queueing delay gets dominant for some critical paths. Thus, the linear interdependency between routing distance and latency is hardly observed anymore. Referring to Figure 6(b), the most critical paths have distances of 3, 4, 5, 8, and 10 hops, respectively. Figure 6(b) shows that the critical paths are exactly identiﬁed by the our analytic model (Round-Robin Model) and the queueing delay is estimated with a good accuracy. The relative error is less than 22%. The marginal underestimation of the queueing delays for the critical paths results from a slight overestimation of the network saturation point. This relation can also be observed in Figure 5. In contrast, the reference models 1 and 3 are not able to accurately estimate the increased latencies for the critical paths. This error is caused by the strong overestimation of the network saturation point. For these two models, we still recognize the linear interdependency between routing distance and latency, which is contrary to the results of the cycle-accurate simulation. Concerning reference model 2, an injection rate of 7.12 already exceeds the network saturation point, as depicted in Figure 5. Therefore, the estimated latencies of the critical paths tend towards inﬁnity in this case, as shown in Figure 6(b). Although the actual path latencies might be of less interest so close to the network saturation, this study, nevertheless, is a good approach for the veriﬁcation of the correct functionality of the service time model. We can conclude from the ﬁgure that the presented service time model works precisely, also on             ] l s e c y c k c o c l [ y c n e t a l t e k c a p e g a r e v A ] l s e c y c k c o c l [ y c n e t a l t e k c a p e g a r e v A 50 40 30 20 10 0 50 40 30 20 10 0 Cycle-accurate Simulation Round-Robin Model Model 1 Model 2 Model 3 2 3 4 5 6 7 8 9 Routing Distance (# hops) 10 11 12 (a) Injection rate: 6.9 Cyc le-accurate Simulation Round-Rob in Model Mode l 1 Mode l 2 Mode l 3 2 3 4 5 6 7 8 9 Routing Distance (# hops) 10 11 12 (b) Injection rate: 7.12 Fig. 6. Average latencies aggregated according to the routing distance (# hops) at two injection rates close to network saturation. a ﬁne-grained level. The path latencies are estimated accurately and the critical paths are identiﬁed correctly. Finally, note that we studied the accuracy for diﬀerent traﬃc patterns and topologies as well. This is not presented here due to the limited amount of space. Certainly, the mean error of the estimated latencies varies slightly depending on the scenario. However, the general trend and the high accuracy of the estimated network saturation point could be reproduced for every considered scenario. VII. Conclusion and Future Work In this paper, we proposed an analytic model for the accurate service time estimation of round-robin arbitration in NoC. Existing analytic models for the performance analysis of NoC often possess a lack of accuracy, especially w.r.t. the popular round-robin arbitration scheme. A service time estimation, speciﬁcally designed for this arbitration scheme, can signiﬁcantly increase the accuracy of the performance estimation. The service time estimation is intended as an extension for an existing analytic NoC model based on queueing theory [10], which allows for fast and ﬂexible early design space exploration of NoC-based many-core SoC. The performance evaluation demonstrates the high accuracy of our proposed approach by comparing it with cycle-accurate simulation and three analytic reference models. It proves that an accurate service time model is essential for predicting key performance indicators, such as network throughput or latencies. The latency estimation showed a mean error of less than 6% for the considered 8 × 8 2D-mesh case using an application-speciﬁc traﬃc pattern. Several model extensions are possible. A modiﬁed service time estimation to support prioritized arbitration (e.g., weighted round-robin) is an interesting extension concerning quality of service. This should be feasible by modiﬁcation of the collision resolution distribution ri,o (refer to Section IV-B). A ﬁnite buﬀer model should be employed in order to model network acceptance and back pressure eﬀects. Considering frequency scaling techniques via multiple clock domains is another interesting extension towards NoC-based many-core SoCs. "
Accelerating atomic operations on GPGPUs.,"General purpose computing on GPUs (GPGPU) has experienced rapid growth over the last several years as new application realms are explored and traditional highly parallel algorithms are adapted to this computational substrate. However, a large portion of the parallel workload space, both in emerging and traditional domains, remains ill-suited for GPGPU deployment due to high reliance on atomic operations, particularly as global synchronization mechanisms. Unlike the sophisticated synchronization primitives available on supercomputers, GPGPU applications must rely on slow atomic operations on shared data. Further, unlike general purpose processors which take advantage of coherent L1 caches to speed up atomic operations, the cost and complexity of coherency on the GPU, coupled with the fact that a GPU's primary revenue stream - graphics rendering - does not benefit, means that new approaches are needed to improve atomics on the GPU. In this paper, we present a mechanism for implementing low-cost coherence and speculative acquisition of atomic data on the GPU that allows applications that utilize atomics to greater extents than is generally accepted practice today, to perform much better than they do on current hardware. As our results show, these unconventional applications can realize non-trivial performance improvements approaching 20% with our proposed system. With this mechanism, the scope of applications that can be accelerated by these commodity, highly-parallel pieces of hardware can be greatly expanded.","Accelerating Atomic Operations on GPGPUs Sean Franey and Mikko Lipasti Electrical and Computer Engineering, University of Wisconsin - Madison sfraney@wisc.edu, mikko@engr.wisc.edu Abstract—General purpose computing on GPUs (GPGPU) has experienced rapid growth over the last several years as new application realms are explored and traditional highly parallel algorithms are adapted to this computational substrate. However, a large portion of the parallel workload space, both in emerging and traditional domains, remains ill-suited for GPGPU deployment due to high reliance on atomic operations, particularly as global synchronization mechanisms. Unlike the sophisticated synchronization primitives available on supercomputers, GPGPU applications must rely on slow atomic operations on shared data. Further, unlike general purpose processors which take advantage of coherent L1 caches to speed up atomic operations, the cost and complexity of coherency on the GPU, coupled with the fact that a GPU’s primary revenue stream - graphics rendering does not beneﬁt, means that new approaches are needed to improve atomics on the GPU. In this paper, we present a mechanism for implementing low-cost coherence and speculative acquisition of atomic data on the GPU that allows applications that utilize atomics to greater extents than is generally accepted practice today, to perform much better than they do on current hardware. As our results show, these unconventional applications can realize non-trivial performance improvements approaching 20% with our proposed system. With this mechanism, the scope of applications that can be accelerated by these commodity, highly-parallel pieces of hardware can be greatly expanded. I . IN TRODUCT ION Over the past several years, the previously rigid, ﬁxed function units making up graphics processing units (GPUs) have evolved to meet the ever-more-general demands of advanced shaders for rendering graphics. Through this evolution, the GPU has changed to look much more like a highly parallel general purpose processor, not unlike expensive, low volume supercomputers. Correspondingly, early adopters motivated the evolution of the programming environment for these processors to make their capabilities available to a broader audience. This ecosystem of advanced programming environments for commodity, highly parallel processors has led to the emergence of general purpose computing on GPUs (GPGPU). Through this emergence, many applications that were previously limited to the costly realm of supercomputing, have now been ported to the GPU. While signiﬁcant work in both the environment and hardware continues to expand the application scope for these parts, many highly parallel applications remain out of reach. One large portion of the application space that remains unrealized on the GPU is that of parallel workloads that utilize atomic operations to update globally shared data, particularly those that utilize these updates to perform coarse, global synchronization. This is due to the fact that modern GPUs do not efﬁciently support synchronization outside of very local work units. In NVIDIA terminology for instance, there is no direct support for synchronizing threads across computational blocks (CTAs). While in the past, some applications have had access to specialized hardware [1] or message passing techniques [2] [3] for synchronization, GPGPU application have had to rely solely on atomic operations to global data. While this is a common technique utilized in general purpose CPUs, the lack of L1 cache coherence in GPUs make these operations signiﬁcantly slower. Further, though coherence in fused CPU/GPU chips appears inevitable to support the desired tight coupling between the two units, it seems unlikely that the L1 coherence between processing units within the GPU, necessary to signiﬁcantly improve these synchronization operations, will be seen on the GPU. This is due to the traditionally high design and veriﬁcation cost of coherence, coupled with the fact that the primary source of GPU revenue - rendering of graphics - does not beneﬁt from it. Therefore, it seems apparent that opening up this application class to the GPU will require new, inexpensive mechanisms. In this paper, we will discuss two such mechanisms to provide high speed atomic operations to applications on the GPU. The ﬁrst is a rather simple adaptation of Atomic Coherence [4] that allows the GPU to implement L1 cache coherence on atomic data in a complexity-effective way. Atomic Coherence contains complexity by mimicking a traditional blocking bus, thereby preventing races and the associated state explosion in the coherence controller, while retaining the performance of a non-blocking interconnect. It accomplishes this by mandating a node acquire a mutex corresponding to the data triggering the coherence action. Further, only the node holding the mutex can utilize the interconnect for the associated data, preventing races and leaving the interconnect available for non-conﬂicting activities. The serialization penalty of mutex acquisition is mitigated by a ultra-low-latency nanophotonic ring. Even without such an interconnect, Section II-A adapts this approach over a novel electrical interconnect that performs reasonably well by providing coherence only for atomic data. This approach reduces the cost of providing L1 coherence to a level low enough to merit adoption on a cost-sensitive GPGPU, while providing substantial improvements in the performance of synchronizing operations. The second approach we present for improving atomic performance builds on the ﬁrst with very little incremental cost (to be quantiﬁed in Section IV-G). The intuition behind it is that the state maintained by acquisition of mutexes for Atomic Coherence itself, is nearly enough to ensure safe access to atomic data. By removing the coherence controller altogether and merely extending the information tracked at the nodes responsible for the mutexes and maintaining some information at the requester, we can determine if a node can safely access the associated data from its local cache without initiating an action beyond acquiring the mutex. Therefore, if enough locality exists in the access stream of atomic data (i.e., the same nodes are modifying the same data over a window of time), this mechanism can improve performance by eliminating coherence activity in the common case and fallback to a global access as a failsafe when mutex acquisition indicates multiple nodes are sharing the data. The rest of this paper will elaborate on these mechanisms, systematically building them from a baseline Atomic Coherence implementation on a novel electrical interconnect topology. I I . CACH ING ATOM IC DATA As mentioned in the introduction, L1 cache coherence is a feature lacking in GPUs and one that is not likely to ﬁnd support in the foreseeable future. Unfortunately, for atomic operations to approach the performance available on current general purpose CPUs, something akin to coherence for these operations is imperative. In order to understand the importance of this support, one must have an understanding of the current state-of-the-art regarding atomic operations on the GPU. Unfortunately, to the best of our knowledge, implementation of atomic operations on a GPU have not been publicly described. While it might be most intuitive to assume that atomic instructions are executed like non-atomic instructions in the shader core, some have suggested that these operations actually occur at the memory interface by the extension of alpha blending hardware to perform them [5]. This scenario is not only plausible, but corroborated by microbenchmarks [6]. Under this assumption, atomic requests are both ordered and performed remotely, after traversing the interconnect from the shader core to the corresponding memory bank. With the introduction of Fermi, an L2 cache is available which can greatly improve atomic performance [7] by performing them at the L2 bank, rather than at memory. This still requires a traversal of the interconnect to the L2 bank, however. Therefore, according to our understanding of current stateof-the-art systems, an atomic operation is generated in the shader core and traverses the interconnect to the appropriate L2 bank. Once at the L2 bank, the operation is ordered, data is acquired, and the operation is performed. The new data is written back and a response is sent back to the core containing the previous value of the data (to be consistent with the CUDA programming guide [8]). In order to achieve any signiﬁcant latency reduction over this conﬁguration then, the atomic operations must be performed locally - at the shader core itself - with local data, in order to avoid the latency of traversing the interconnect. This, in turn, requires the ability to cache atomic data at the shader core, and to do so coherently, and in a manner which supports their atomic semantics. The following section describes our proposed lightweight schemes, which are utilized strictly for atomic data to provide this support. W H t n e r r u C r e v o p u d e e p S  10%  5%  0% -5% -10% -15% -20%  18  16  14  12  10  8  6  4  2  0 ) n s n i K 1 r e p ( e t a R c i m o t A -6X -30% >-10X -2.25X canneal threadFence GPUSort hypercolumn fluidanimate AtomNaive (onebusy) AtomNaive (busy) AtomDir Atomic Rate Fig. 1. Atomic Coherence Results. A. Atomic Coherence As previously described, Atomic Coherence is a mechanism that reduces design and veriﬁcation complexity of coherence controllers by preventing race conditions, but it does so by leveraging a nanophotonic ring that is unique to the system it is evaluated on [4]. Therefore, a na¨ıve ﬁrst-pass implementation of Atomic Coherence on a GPU with an electrical interconnect would involve imposing a ring topology on the nodes in the system that would need to acquire mutexes (i.e., shader cores). We can limit cost by the realization that this ring need not be a physical set of wires, but can be mimicked more simply by a rotating token (what we will call a “talking stick” to avoid confusion between the similar terms ’token’ and ’mutex’). This way, acquisition of a mutex involves waiting for the talking stick to reach the requesting node, acquiring the mutex (assuming it is available), and marking it unavailable for subsequent requesters. Release of the mutex would then again involve waiting for the talking stick to return and releasing the mutex (i.e., marking it available to the rest of the system). In this system, the additional required structures are mutex status tables at each node to track the state of the mutexes in the system and buffers to hold requests until the talking stick arrives. The talking stick incurs no additional cost as it can be implied, essentially providing time-division multiplexed access to it. In other words, a node knows that it is the holder of the talking stick based on how many timesteps have occurred since it last had it (one could imagine a modulo operation on the cycle count). Cost can be further limited by utilizing the underlying interconnect to transmit updates. Given that the underlying interconnect is likely to exhibit congestion or otherwise prevent the updates from travelling at the speed of the talking stick, we utilize a mechanism termed a “busy wire” to indicate to nodes when an update is in ﬂight. This busy wire is a point-to-point wire in the interconnect, conceptually running along with the data links, that allows a node to indicate when an update is in ﬂight. When it is in one state (nominally, ’1’) it indicates an inﬂight update and all mutex acquisitions are stalled, while the other state indicates no update is inﬂight and subsequently allows acquisitions to occur. While a busy wire is conceptually a single wire spanning all nodes in the system, since it is implemented point-to-point, it does not have the power and latency concerns such long global wires would have. Instead, each node is               responsible for keeping the busy wire asserted as indicated by its upstream node (as determined by talking stick rotation) until such time as that node indicates it no longer needs to be asserted. The state of the busy wire then propagates through the system at a rate at least equal to the talking stick to ensure no mutexes are acquired until the update completes. The same assertion/deassertion process occurs when a mutex is released. Not surprisingly, this simple mechanism leads to a significant false conﬂict problem, where non-conﬂicting requests are stalled waiting for an update to traverse all nodes in the system. Figure 1 shows the results of this with the “AtomNaive (onebusy)” bars, showing that many applications are severely impacted. Therefore, for the small cost of additional pointto-point wires, analogous to adding a few control signals between nodes, we can reduce this false conﬂict problem by incorporating more busy wires. The number of busy wires then becomes a design time decision based on expected mutex acquisition interleavings, but as the number increases beyond one, has the effect of segmenting the mutex space that they correspond to. For instance, with two busy wires, one could correspond to “odd” mutexes while the other was associated with “even.” Then when a node acquires a mutex, it asserts the associated busy wire to indicate that a node may not acquire a mutex associated with it. In this way, we are able to signiﬁcantly reduce the false conﬂict rate in the system to improve performance. Figure 1 shows the results for both na¨ıve implementations of Atomic Coherence (with AtomNaive (busy) corresponding to the conﬁguration with busy wires) with 8,192 mutexes. For the AtomNaive (busy) results, 16 busy wires are used as each benchmark was rather sensitive to the number, with performance being strongly penalized for fewer. We will show in Section IV how we can design a more robust system that is less sensitive to the number of busy wires. While much better than a single busy wire, performance is still not signiﬁcantly better in most cases than the current state-of-the-art approach, and indeed much worse for certain applications. This is due to the fact that this approach ignores the signiﬁcant latency characteristics of an electrical interconnect compared to the fast communication of nanophotonics. As such, performance of applications that are highly sensitive to the latency of atomic operations may experience the signiﬁcant slowdown that canneal exhibits. The applications evaluated in this graph will be explained in greater detail in section IV-B, but represent GPGPU applications that utilize atomic operations to varying degrees, with their rate of atomic instructions shown on the secondary y-axis as atomic operations per 1,000 instructions. We can improve on this baseline, by avoiding the sort of steep penalties experienced by some applications, by adapting techniques used in directory-based cache coherence and replacing the ordering achieved through the talking stick with “owner” directories. Whenever a node wishes to acquire a mutex in this system, instead of waiting for a circulating talking stick, it simply requests the mutex from the owner. The owner then responds back to the requester with the mutex if it is available or queues or NACKs the request if it is not. Utilizing this mechanism, we free ourselves from waiting for the talking stick by replacing that wait with a round-trip communication with the owner. This results in an average, uncontended, no congestion mutex acquisition latency reduction from 7.5 cycles to 5 cycles in a 4x4 mesh, assuming each “hop” (traversal through one node to the next) is 1 cycle. The results for this approach are presented in Figure 1 (labeled AtomDir). I I I . FA S T EXC LU S IV E ACC E S S TO R E SOURC E S As can be seen, a simple adaptation of Atomic Coherence that takes into account the limitations of an electrical interconnect allows it to perform reasonably well, and without the potential for signiﬁcant penalty exhibited by the na¨ıve case. We note in this system, however that the owner of the mutex has a signiﬁcant amount of information on the acquisition patterns in the network that is nearly sufﬁcient for it to conclude if a requesting node has exclusive access to the data. If the owner were merely capable of retaining the identity of the node that most recently acquired a mutex, it could also unambiguously indicate to the requester whether or not it had potentially stale cached data. Starting with this insight, we develop a more robust system that combines coherence information in the same structure as mutex management and improves latency to acquire a mutex through management distribution and speculative data acquisition. The rest of this section will develop this more robust system through a series of evolutions from Atomic Coherence’s directory-inspired baseline. A. Requester-Side Lookup Tables To start, we extend the Atomic Coherence, directory-based system to maintain simple Modiﬁed/Invalid (MI) coherence on atomic data in the mutex directory itself, doing away with a separate coherence controller altogether. We accomplish this by ﬁrst extending the mutex status tables present at each directory to also include the identity of the node that most recently acquired the mutex. Therefore, the table increases from a simple boolean per entry to identify the state (available/unavailable) to the boolean plus log2 (# of nodes), resulting in 5 bits per entry in a 16-node system. Further, to prevent aliasing at the requester, we introduce a table for the requester to associate an address with each mutex it acquires. This support is necessary because we assume a similar mapping of addresses to mutexes that Atomic Coherence does, which allows multiple addresses to map to the same mutex. Therefore, acquisition of the same mutex for two separate accesses could erroneously result in the requester trusting its local data when it should not. Structurally, for our simulations, we allow this table to have a power-of-two number of entries equal to or less than the number of mutexes in the system, allowing it to be simply indexed by the appropriate number of address bits. While performance could certainly be improved by utilizing a more sophisticated hash or making the tables associative, we did not ﬁnd performance to be greatly impacted by the size of these tables (to be discussed in further detail in  8  7  6 t n u o  5 C  4 p o  3 H  2  1  0 Rot Latency Xport Latency 5 4 0.5 2 0 1 n o d e 1.5 3.5 2.5 1.5 4 8 n o d e s n o d e s n o d e s Ring Size 0 7.5 1 6 n o d e s Fig. 2. Ring Layout. Fig. 3. 4x4 Mesh Average Hop Count. Section IV) and leave evaluation of other implementations to future work. B. Multiple Rings While replacing the Atomic Coherence controller with simple lookup tables and logic beneﬁts both design and veriﬁcation complexity and likely improves area and power, the performance results are unaffected. Given the performance of Atomic Coherence with directories, this adaptation alone is likely insufﬁcient to provide a compelling solution to speeding up atomic operations on the GPU. Therefore, we undertake the task of developing mechanisms for improving latency that are motivated by the observation that mutex acquisition latency is optimal when a node is able to satisfy its own mutex requests in the same cycle the requests are generated (i.e., an immediate self-satisﬁed request). In the AtomDir case, this occurs only when the requester happens to also be the owner of the mutex. In AtomNaive, this occurs whenever the talking stick is present at the requester on the same cycle of the request. For both conﬁgurations in a 16-node network, this is a 1 in 16 chance, given a uniform distribution of mutex requests. Therefore, we attempt to increase the number of self-satisﬁed mutex requests - that is maximized in AtomNaive - without imposing the severe latency overhead of the talking stick. As Figure 3 shows, this challenge is addressed by effectively ﬁnding a middle point between the two conﬁgurations. These results are obtained by evaluating a system that is a hybrid of the single owner directory of AtomDir and the fully-distributed directory in AtomNaive. In this system, shown as a 4x4 mesh in Figure 2, mutex state is distributed across some number of logical rings in the system, where each member of a ring maintains and responds to requests for the same mutexes and nodes in different rings maintain different mutexes. The “Xport Latency” (Transport Latency) numbers for Figure 3 corresponds to the latency of communicating a request to a node that maintains a mutex’s state and are generated by iterating over all requester-responder combinations in a system, accumulating the round-trip hop counts between them, and averaging this sum over the possible combinations. “Rot Latency” (Rotational Latency) is simply: N −1 i X i=0 N where N is the number of nodes in the ring and the result is the average time a node has to wait for the talking stick. Since the growth rate for a talking stick to circulate is slower initially than the reduction in latency to communicate a mutex to a directory as ring sizes increase, these ﬁgures show that the selection of a proper ring size can improve latency over either Atomic Coherence conﬁguration. While we are able to gain some of the beneﬁts of both Atomic Coherence conﬁgurations with this mechanism, we also gain some of their shortcomings. For one, we have the same coherency issues between replicated mutex status tables that AtomNaive had. To address this, we once again utilize updates on the existing interconnect along with busy wires. Further, just as in AtomNaive, the talking stick is implied, however since more than one ring can exist in this system, more than one talking stick exists, one for each ring. Before continuing, it is important to take a moment to discuss the potentially non-intuitive nature related to how waiting for the talking stick to circulate to a responding node is an improvement over merely forwarding the request on to a single owner. This is because FEAR has essentially replaced the ∆x plus ∆y latency to transmit a request to an owner by replacing one dimension (∆x, nominally) with the stick latency. This is beneﬁcial because average stick latency is half ∆x since it is always a one-way trip while ∆x is round-trip. C. Speculative Fetches In current GPU systems, uncontended atomic operations simply travel to the L2 and are immediately satisﬁed, while in FEAR they incur the additional mutex acquisition penalty. If frequent enough, even this small penalty can result in overall performance degradation. Therefore, we introduce one ﬁnal optimization to FEAR to support speculative fetches. With this optimization, memory fetches are issued in parallel with a mutex request to proactively acquire data in the case that mutex acquisition indicates it needs new data. This naturally creates issues if the mutex is contended as a node needs to know if it fetched stale data due to a racing request. To address this issue, we employ a virtual timekeeping system based on the concept of epochs. The epoch concept is similar to a Lamport clock, and provides a low cost way for differentiating mature and immature events in the system [9]. An epoch consists of a ﬁxed number of cycles. At the boundary of each epoch, all responders indicate that their mutex releases (i.e., available mutexes in the mutex status table) are mature and all requesters indicate that their outstanding mutex requests are stale. Therefore when a responder sends a mutex to a requester, it indicates whether the last release was mature or not. When the requester receives the mutex, if it is mature, it checks whether or not its request is stale. If both the release is             mature and the request is not stale, the requesting node knows that no update could have occurred to the data associated with a mutex between its speculative fetch and mutex acquisition (i.e., both conditions of the release being done in a previous epoch and the request being made in the current epoch are satisﬁed). Note that in cases where the requesting node can use locally cached data, the speculative fetch data is ignored. Figure 4 highlights just the sort of problematic situation a speculative fetch can encounter and how FEAR’s epoch method ensures correctness. In this example, a requester (Node A) requests a mutex from the nearest eligible responder (Node B) and sends a speculative fetch to the L2 in parallel. Meanwhile, the current holder of the mutex (Node C) completes and sends its writeback and mutex release to the appropriate L2 slice. Piggy-backing the release to the writeback in this way is necessary to prevent race conditions that could allow a subsequent mutex acquisition to get data from the L2 before the previous writeback has been received at the L2. While Node A’s mutex request is queued up at Node B, the L2 receives the speculative fetch request and creates a response to Node A with data (that is stale with respect to Node C’s update). Once the L2 receives Node C’s update, it then sends the mutex release on to its nearest responsible node (Node B in this example, but it could be any other node in the ring). Upon receipt of the mutex, Node B is now free to give it to Node A. To exacerbate the problem, this mutex response actually arrives at Node A prior to the speculative fetch response. The system is now in a situation where Node A has speculative data that is stale, and a mechanism is needed to ensure it gets a fresh copy from the L2 before proceeding. The epoch method ﬁlls this role, because no matter where the epoch boundary is placed on the diagram, either the mutex release will be considered immature (Case 1 & 2) or the request will be stale (Case 2), indicating to Node A that it cannot use its speculatively fetched data. Two subtle cases for epoch boundaries are shown, but careful examination reveals that placing a boundary anywhere on the timeline still maintains correct behavior. While ensuring correctness, this method can certainly have a negative impact on performance when an otherwise safe speculative fetch is deemed unsafe. This can be due to excessively long delays in a responder giving a requester a mutex or due to excessively long epochs that indicate a sufﬁciently old release is immature. As we will show in Section IV, however, proper selection of epoch length can almost completely eliminate any penalty and indeed greatly improve performance over a system that does not allow speculative fetches. D. Summary To summarize, FEAR is composed of multiple nonoverlapping logical rings imposed on the nodes in a system. All nodes within a ring are responsible for the same subset of the mutex space. When a node wishes to acquire a mutex, it makes a request of the node nearest it that is a member of the set (i.e., ring) responsible for the space the mutex resides. The node that receives this request then responds when: Fig. 4. Epoch Example. 1) it is their turn (i.e., they have the “talking stick”) 2) they have up-to-date mutex information (as indicated by the appropriate busy wire being clear) 3) their mutex status table indicates that available. the mutex is Once they grant the mutex to a requester, the responder then generates an update message on the underlying interconnect to inform the other nodes in the ring that the mutex is unavailable and they assert the busy wire. Finally, the responder clears the busy wire when their update message gets back to them (indicating all other nodes have seen the update). Release of a mutex follows a similar path in that the requester sends a release message to the nearest node in the ordering ring. This nearest node then indicates in their mutex status table that the mutex is now available, generates another update message to pass this information along to the other nodes in the ring, and asserts the busy wire. Once again, the busy wire remains asserted until the generator of the update message receives its update. IV. EVA LUAT ION We perform two phases of evaluation for FEAR. In the ﬁrst, we perform sensitivity analysis on the FEAR system’s parameters such as the number of mutexes and busy wires, to see how it responds to varying the number of resources available to the system. In the second, we evaluate the relative performance improvement of FEAR’s different mechanisms such as speculative fetches, to determine their impact on overall performance. A. Simulation We evaluate FEAR on the GPU using GPGPU-Sim [10] to simulate execution of a set of CUDA benchmarks that utilize atomics to varying degrees. GPGPU-Sim is a cycleaccurate GPU simulator that incorporates a network model which is critical to the evaluation of FEAR - that is itself a cycle-accurate simulator from Dally and Towles [11]. Since GPGPU-Sim does not properly model the serialization and read-modify-write semantics of atomic operations currently, we had to add support for them in the model along with support for features of FEAR. As discussed in Section II, we implement memory-side computation of atomics for our baseline to be consistent with state-of-the-art hardware, while FEAR conﬁgurations use shader-side computation to be able to take advantage of cached data. Core & Memory No. Shader Cores No. Mem Interfaces Topology No. Registers / Core Shared Mem. / Core L1 Cache L2 Cache Compute Core Clock Interconnect Clock Memory Clock Routing Function Priority No. VCs Buffer size / VC 28 8 6x6 Mesh 16K 16KB 32KB 4-way set assoc., 64B lines, LRU 512KB 8-way set assoc., 64B lines, LRU 1300 MHz 650 MHz 800 MHz Interconnect Dimension-ordered Class-based 2 4 TABLE I GPGPU-Sim Conﬁguration. Benchmark ﬂuidanimate hypercolumn canneal GPUSort threadFenceReduction Best Speedup 19.5% 7.19% 3.73% 1.07% 0.30% IPC 22.4 163.7 23.5 60.9 12.1 Atom Rate 17.1 0.008 0.74 0.67 0.01 TABLE II Result Summary. (Atomic Rate is per 1K instructions) B. Benchmarks For exercising FEAR, we encountered a chicken-and-egg problem of a dearth of applications that utilize atomic operations, due to the fact that GPUs do not perform them well. Therefore, we accumulated a small suite of benchmarks from various sources, with the goal of compiling a set that has a range of atomic instruction reliance. The suite consists of threadFenceReduction from the CUDA SDK [12] and GPUSort from Rodinia [13], along with less traditional GPU benchmarks such as a port of ﬂuidanimate to CUDA [14], a brain learning model (hypercolumn) [15], and our own port of canneal. The ports of ﬂuidanimate and canneal are based upon the implementations in the PARSEC benchmark suite [16]. Together, this suite represents a range of applications that utilize atomics on the GPU only rarely (threadFenceReduction) to quite frequently (ﬂuidanimate). In Table II’s ﬁnal column the rate at which each benchmark executes atomics per 1,000 instructions is shown. The range is quite broad, and generally correlates with reported performance gain (to be discussed in Section IV-D). The notable exception to this trend is hypercolumn. Despite its low atomic rate, hypercolumn actually utilizes atomic operations to perform global synchronization of work units by utilizing them to index into a queue to coordinate work among the blocks. As opposed to the other workloads that largely utilize atomics to protect potential racing updates, hypercolumn’s explicit serialization and synchronization through these operations make it much more sensitive to atomic performance as reﬂected in the performance results. For the rest of this section, we report our results as speedup relative to the baseline - with memory-side computation of atomic operations - that is meant to model a state-of-the-art GPU system. For each benchmark, we report these speedups for complete execution of the kernel that uses them. C. Conﬁguration Table I shows the conﬁguration used in GPGPU-Sim to approximate that of a Fermi architecture GPU that performs atomic operations at the L2 interface. As previously mentioned, functionality was added to that of publicly-available GPGPU-Sim in the form of support for atomic operations consistent with assumptions described in Section II along with support for the features of FEAR. Request-response deadlock that could be introduced by FEAR’s requests and releases is avoided by prioritizing releases over requests in the classbased, 2 virtual channel interconnect. D. Overall Performance Table II summarizes the results obtained for different conﬁgurations for each benchmark as speedup over the baseline. For each, the best speedup is ﬁrst presented followed by characteristics of the application. The characteristics presented are the absolute performance of each benchmark in terms of instructions executed per cycle, followed by the rate at which atomic operations are performed by the application per 1,000 instructions. As these results show, performance with FEAR is very much related to the function and rate at which the application utilizes atomics, lending credence to the claim that a system utilizing FEAR can improve the performance of applications that utilize atomics in ways and at rates that are poor ﬁts to today’s GPUs. Further, we see that FEAR does not, as it should not, adversely affect applications that utilize atomics sparingly, threadFenceReduction being a prime example. While its restrained use of atomic operations makes it a good ﬁt for current hardware, a FEAR-enabled system would allow it to beneﬁt from more liberal use of them. E. Sensitivity Analysis Figures 5 through 8 show how sensitive each application is to varying design-time conﬁgurable parameters of the system. One feature of note on these graphs is the lack of sensitivity for any application to the size of the lookup tables. Despite the simple, direct mapping of mutexes to it, once it gets above 1K entries (and even less than that for all but canneal) there is only negligible performance impact. Another interesting feature is the fact that, with relatively few busy wires, each application quickly achieves very close to the performance they could achieve if updates were ideal (i.e., all nodes in a ring were instantaneously updated with each acquisition and release). Finally, a feature present in these graphs is one that will become a recurring theme; the inﬂuence system parameters p u d e e p S p u d e e p S  25%  20%  15%  10%  5%  0% -5% -10% 1K fluidanimate canneal hypercolumn gpusort threadFence 4K 8K 16K Mutexes Fig. 5. Number of Mutexes Sensitivity. fluidanimate canneal hypercolumn gpusort threadFence  25%  20%  15%  10%  5%  0% -5% -10% p u d e e p S p u d e e p S  25%  20%  15%  10%  5%  0% -5% -10% 1K fluidanimate canneal hypercolumn gpusort threadFenceReduction 4K 8K 16K Entries Fig. 6. Lookup Table Size Sensitivity. fluidanimate canneal hypercolumn gpusort threadFenceReduction  25%  20%  15%  10%  5%  0% -5% -10% 1 2 4 8 16 ideal 20 80 160 320 640 Busy Wires Fig. 7. Busy Wire Sensitivity. Length Fig. 8. Epoch Length Sensitivity. have on ﬂuidanimate, with the exception of lookup table size, that highlights the affect FEAR has on non-traditional GPGPU applications that frequently use atomic operations to update global data. F. Deconstructing FEAR In order to evaluate the contribution of different components of FEAR, we systematically simulate the system, building features on top of one another. Through this evaluation, we seek to measure the performance contributions of 1) allowing atomic data to be cached, 2) distributing mutex ownership (i.e., creating multiple rings of mutex maintainers), and 3) issuing speculative fetches along with mutex requests. Figure 9 breaks down these elements for each application and are normalized to the standard memory-side atomic computation model that is assumed as the current state-of-the-art. “AtomDir” shows the inherent beneﬁt of being able to cache atomic data, “Topology” shows the beneﬁt of distributing ownership, and “SpecFetch” shows the advantage of issuing speculative memory fetches along with mutex acquisition. From this ﬁgure we not only see that each element of FEAR is important to achievable performance for one or more benchmarks, but also that application behavior can have a signiﬁcant effect on how important each is. GPUSort for instance, values caching of atomic data over any other as evidenced by its performance in the very simple caching scheme, AtomDir. On the other hand, the increased trafﬁc FEAR introduces with its update messages has a somewhat negative impact on the system that is largely recovered with speculative fetching. Canneal on the other hand requires speculative fetches to achieve signiﬁcant improvement. Finally, ﬂuidanimate and hypercolumn exhibit monotonically increasing performance for each feature. Given the non-traditional reliance on atomics each of these benchmarks exhibit, these results again highlight FEAR’s potential to broaden the application scope of GPGPU. W H t n e r r u C r e v o p u d e e p S  25%  20%  15%  10%  5%  0% -5% -10%  18  16  14  12  10  8  6  4  2 ) n s n i K 1 r e p ( e t a R c i m o t A canneal  0 threadFence GPUSort hypercolumn fluidanimate AtomDir Topology SpecFetch Atomic Rate Fig. 9. FEAR Contributions to Performance. G. Power and Area Implications In order to quantify the power and area impact of FEAR, we used fabmem to evaluate the additional structures needed to support it [17]. These structures consist of the mutex status tables and the requester-side lookup tables. Note that these two structures along with the busy wires, are the only additional costs of FEAR as it does not require tagging memory locations as some other synchronizing systems do. The mutex availability table is replicated across all nodes that participate in mutex acquisition in the system (cores and memory interfaces), while the lookup tables must be present at all nodes that acquire mutexes (cores). Busy wires exist along with the data links between nodes and due to the small number required to closely approximate ideal updates - with respect to the link widths themselves - we did not quantify their power or area effects. Tables III and IV show both the per-node and chip-wide impact of various sizes of these structures. With our 6x6 mesh system, we estimate chip-wide impact by multiplying the area and power results for the structures by 36 for the mutex availability table and 28 for the lookup tables. This is due to the fact that 8 nodes in the system are memory interfaces and only require a mutex availability table since they               Per-Node Mutexes Entries Area (µm2 ) 8,496 14,270 28,077 55,868 2K 4K 8K 16K 512 1K 2K 4K Power (mW) 3.13 5.48 13.86 29.88 Chip-Wide Area (µm2 ) 305,857 566,880 1,064,264 2,065,122 % Area 0.06% 0.11% 0.21% 0.41% TABLE III Mutex Availability Table Impact. Per-Node Entries Area (µm2 ) 121,417 251,465 473,525 988,566 2K 4K 8K 16K Power (mW) 5.54 10.76 20.33 39.36 Chip-Wide Area (µm2 ) 3,400,000 7,041,007 13,258,705 27,679,839 % Area 0.68% 1.41% 2.65% 5.54% TABLE IV Requester-Side Lookup Table Impact. will never make a mutex request and act only as responders. We also assume a 500 mm2 die based on GPU die reports, though values can be easily scaled for larger or smaller dies. For instance, assuming a more typical CPU die size of 200 mm2 , one would multiply each percentage and power value by 2.5. From these tables, it is clear to see that FEAR imposes trivial overhead, particularly in a state-of-the-art GPU context. V. R E LAT ED WORK In the realm of distributed mechanisms for enforcing global synchronization, recent years have provided MP-LOCKS [2], GLocks [3], and TLSync [18], which bear signiﬁcant similarities to FEAR. These approaches streamline the acquisition of locks in systems that can afford the integration of substantial additional hardware for this purpose. In contrast, FEAR emphasizes low cost, in both complexity and hardware, to minimize the impact on the cost-efﬁciency of 3D rendering, the primary driver of revenue for the product in the ﬁrst place. Another recent proposal is Synchronization State Buffers (SSB) by Zhu, et. al. [19]. This proposal also succeeds at maintaining the lock status of a large memory in a space efﬁcient way, but it does not have the unique topology to improve time to acquisition and contention reduction that FEAR has, nor is it binary compatible as FEAR is. V I . CONC LU S ION We have presented FEAR; a low-latency mechanism for acquiring and releasing mutexes in a system of multiple nodes, applied to improve the performance of atomic operations on a GPU. With this mechanism, we ﬁnd that traditional GPGPU workloads can achieve modest improvement, while certain workloads that have not traditionally been applied to the GPU can achieve substantially better improvement. Further, FEAR contains no GPU-centric elements, lending itself to many application domains outside of GPU atomics. It is general enough that it is easily applied to enforcing mutually exclusive access to any shared resource and could be easily applied to more traditional CPU problems. In fact, our motivating example of Atomic Coherence could leverage FEAR on a traditional CPU in order to realize its beneﬁts without relying on an optical interconnect. ACKNOW L EDGM EN T We would particularly like to thank Dana Vantrease for her input in developing the mechanisms presented in this paper. This work was supported in part by NSF grant CCF-1116450. In addition, we would like to thank the anonymous reviewers for their insight and feedback. "
"Centralized buffer router - A low latency, low power router for high radix NOCs.","While router buffers have been used as performance multipliers, they are also major consumers of area and power in on-chip networks. In this paper, we propose centralized elastic bubble router - a router micro-architecture based on the use of centralized buffers (CB) with elastic buffered (EB) links. At low loads, the CB is power gated, bypassed, and optimized to produce single cycle operation. A novel extension to bubble flow control enables routing deadlock and message dependent deadlock to be avoided with the same mechanism having constant buffer size per router independent of the number of message types. This solution enables end-to-end latency reduction via high radix switches with low overall buffer requirements. Comparisons made with other low latency routers across different topologies show consistent performance improvement, for example 26% improvement in no load latency of a 2D Mesh and 4X improvement in saturation throughput in a 2D-Generalized Hypercube.","Centralized Buffer Router: A Low Latency, Low Power Router for High Radix NOCs Syed Minhaj Hassan School of Electrical and Computer Engineering Georgia Institute of Technology Email: minhaj@gatech.edu Sudhakar Yalamanchili School of Electrical and Computer Engineering Georgia Institute of Technology Email: sudha@ece.gatech.edu Abstract—While router buffers have been used as performance multipliers, they are also major consumers of area and power in on-chip networks. In this paper, we propose centralized elastic bubble router - a router micro-architecture based on the use of centralized buffers (CB) with elastic buffered (EB) links. At low loads, the CB is power gated, bypassed, and optimized to produce single cycle operation. A novel extension to bubble ﬂow control enables routing deadlock and message dependent deadlock to be avoided with the same mechanism having constant buffer size per router independent of the number of message types. This solution enables end-to-end latency reduction via high radix switches with low overall buffer requirements. Comparisons made with other low latency routers across different topologies show consistent performance improvement, for example 26% improvement in no load latency of a 2D Mesh and 4X improvement in saturation throughput in a 2D-Generalized Hypercube. I . IN TRODUC T ION A general design goal of an on-chip processor-memory network is to provide low latency, low power communication. Since wires are present in abundance, the networks are not bandwidth limited. This is fundamentally different from offchip networks where efforts seek to improve network throughput under a constraint wiring density. In those cases, abundant wiring is typically utilized by increasing the router radix and reducing the network hop count per request, resulting in reduced latency. The problem in using the increased number of wires to reduce latency in on-chip networks is the large amount of buffering associated with high radix router. The minimum depth of each of the buffers on a router port has to be equal to the credit round-trip latency (to avoid bubbles between successive ﬂits and maximize link utilization). This size grows with link length. Furthermore, buffer utilizations are typically low due to routing constraints, number of single ﬂit packets (e.g., in coherent shared memory processors), and bursty behavior of trafﬁc. This results in an over provisioned network with large buffer space that is underutilized. Buffers consume a signiﬁcant amount of static power and area in onchip networks degrading the energy efﬁciencies. Increasing router radix to reduce latency ampliﬁes buffer needs and exacerbates energy inefﬁciency. The pressure has been towards low radix routers with focus on reducing the latency within the router, e.g., speculation [18], bypassing [5], lookahead routing [6], etc. In this paper, we propose the use of a centralized buffer (CB) in on-chip wormhole routers to decouple the required buffer space per router from its radix. Furthermore, we propose to use this router in conjunction with Elastic Buffers (EB) on the links [16]. The CB is power gated so that at low loads trafﬁc bypasses the CB and it operates as a single cycle router, while at higher loads it operates as a buffered router. A novel extension to bubble ﬂow control is used to realize deadlock freedom. The same mechanism avoids both routing deadlock as well as message dependent deadlock using a constant CB size per router independent of the number of message types. The result is a compact, energy and area efﬁcient physical channel router whose low load performance approaches that of buffer-less routers and high load performance approaches that of buffered routers. This paper makes the following contributions. 1) Propose a new energy-efﬁcient router architecture with • a centralized buffer (CB) and elastic buffer (EB) links • optimizations to produce single cycle operation at low loads • load dependent power gating of the CB 2) Provide an efﬁcient deadlock freedom mechanism that realizes both routing deadlock and message dependent deadlock with a ﬁxed buffer size independent of the number of message types The remainder of the paper is organized as follows. Section II provides a brief background. Section III gives the detailed discussion of our router micro-architecture and the associated optimizations. Finally, section IV compares power and performance of our router with different state of the art low latency routers for various topologies. I I . BACKGROUND & MOT IVAT ION A. The Problem Traditional virtual channel based routers use buffers for deadlock freedom and performance optimization. While total storage is optimized for performance, actual buffer occupancies can be very low. The critical importance of energy efﬁciency has motivated several approaches to reduce buffering requirements with minimal compromises in performance. Buffer-less routers [17] represent one such approach. However, as load increases these routers increase both network trafﬁc and average packet latency as packets are routed to nonminimal directions. Also, they are fundamentally throughput limited designs. This is because congestion at any node propagates quickly in these networks causing other packets to stall or take non-minimal routes. Elastic buffer networks [16] have been proposed which retain the minimal buffering requirement without the use of deﬂection routing. However, EB networks face obstacles in integration with standard performanceoptimized router architectures. For example, virtual channels cannot be integrated in the normal manner, and multiple physical channels are recommended for routing and message dependent deadlock avoidance, further increasing pressure on router radix and hence buffering. Ideally, we would like to make use of the advantages of both buffer-less routers and EB links in a power efﬁcient way. Section III describes our approach towards that goal. B. Elastic Buffer Channels and Bubble Flow Control An elastic buffer adds simple control logic to the masterslave latches of a D ﬂip-ﬂop to make them 2 independent storage locations (2 slot FIFO). EBs use a ready-valid handshake to move a ﬂit forward. Pipeline bubbles created with ready-valid handshake are avoided by providing 2 slots per EB within a single clock cycle delay. A channel consisting of multiple such pipeline buffers, instead of repeaters, is called an elastic buffer channel that makes it similar to a distributed FIFO. Elastic buffered links are used to provide link level ﬂow control. A fundamental problem with elastic buffer ﬂow control is that they face challenges in providing multiple virtual channels. The main reason of this is pipelined EB links (Although, we can have multiple virtual channel buffers in the router, link pipelining without the knowledge of buffer space in the next router creates dependencies among ﬂits of different VCs within the link. Thus, we must pursue deadlock freedom by other means. This is achieved, in this paper by proposing a novel extension to bubble ﬂow control. Bubble ﬂow control [19] avoids deadlocks in a packet-based ring by ensuring that at least one empty buffer exists in the ring, so that every packet in that dimension can (eventually) make progress. In a multidimensional tori, any packet entering the ring in a new dimension must not violate this property. A simple way to ensure this locally is to permit injection into the ring (or dimension traversal) only if at least two empty packet buffers are available. Clock cycles that span multiple dimensions are avoided by dimension order or turnmodel [8] based routing. A fundamental problem with bubble ﬂow control is that it has been proposed with packet based SAF and VCT schemes. For these schemes, the worst case bubble requirement for packet insertion is 2 packets per port per node, which is very high for small buffers in on-chip networks. We extend this approach to the ﬂit level and CB router to be able to utilize EB links with reduced bubble penalty. I I I . C EN TRA L I ZED E LA S T IC BUBB L E ROU TER In this paper, we propose the use of a centralized buffer (CB) in on-chip wormhole routers with EB links. There is minimal buffering at the inputs/outputs. At low loads, the CB is power-gated off, and the packets bypass the router taking 2 cycle bypass path. At high loads, the ﬂits are streamed through the router taking 4 cycles (buffered path). We further proposed lookahead switch allocation which reduces these paths to 1 cycle and 3 cycles only. This section describes the router micro-architecture in detail and the novel deadlock freedom application of bubble ﬂow control that can support both routing and message-dependent deadlocks. A. The Baseline Centralized Elastic Bubble Router Fig. 1. Router (a) Micro-architecture (b) Pipeline Stages Router Micro-architecture: Figure 1(a) shows the internal router design. It consists of a large crossbar with single ﬂit input and 2-ﬂit output staging-buffer per port. It also consists of a centralized buffer (CB), which is only utilized when a packet from input buffers cannot make progress to the corresponding output buffers. The control and data information are split in the links. This separation will be explained in section III-B. Central buffer allocator (CBA) performs allocation to the central buffer, while input buffer switch allocator (IBSA) and central buffer switch allocator (CBSA) performs output port allocation for input buffers and central buffers, respectively. The central buffer is a DAMQ [23] style multi-ported output queue in which ﬂits destined to different outputs are kept separate from each other. It can be considered as multiple output queues (1 per port) which share each others space. We kept the number of read and write ports of the CB to 1. This means that if 2 or more ﬂits need to travel in or out of the CB in a single cycle, they need to be serialized. This serialization is achieved by the corresponding switch allocation stages. The performance overhead of this serialization is small (CBs do not require very high throughput), however, the power reduction by reducing the number of ports is signiﬁcant. Furthermore, having a single port at the input of the CB keeps the number of output ports of the crossbar in check. The new crossbar is I nports × (Outports + 1) switch with small area and power dissipation. Pipeline Stages: Figure 1(b) illustrates the different pipeline stages of the baseline CEB router. A ﬂit or packet entering the input buffer can take 2 different paths in the router. 1) Bypass Path - A ﬂit traversing this path encounters 2 stages within the router. IBSA (the switch allocation stage for ﬂits in the input buffer) and ST stage. 2) Central Buffered Path - A ﬂit traversing this path will encounter 4 pipeline stages within the router. Allocation (CBA) and traversal (CBT) to the central buffer, and allocation (CBSA) and traversal (CBOT) from the central buffer to the output port. In both the cases, lookahead routing [6] is used which perform RC in parallel with IBSA or CBA. At low load, path 1 will be chosen. If the corresponding output port is busy servicing another packet from a different input port, path 2 will be selected. Since ﬂits within a packet need to arrive in order, if a path is chosen for head ﬂit of a packet, all subsequent body and tail ﬂits will follow the same path. Furthermore, since interleaving of ﬂits of different packets is not allowed, once an output port is picked by either CB or any of the IBs, it is not released until the whole packet is traversed. Every cycle, 3 allocation operations (IBSA, CBSA and CBA) are performed simultaneously. The IBSA tries to allocate a ﬂit at an input buffer to the output port, and if granted set the necessary crossbar and mux signals. The CBSA in the mean time will try to allocate a ﬂit in the central buffer to the corresponding output port. Among the 2 allocations, CBSA is given higher priority, since these packets arrived earlier than the packet in IB stage. In parallel to these allocations, CBA will also try to allocate central buffer space to packets in the input buffers (1 packet per output port at a time). A packet will be allocated to a CB only if the CB has enough space to hold the whole packet. However, if IBSA wins in allocation, CBA will be ignored. Based on which allocation wins, 1 or 2 of the 3 traversals will be performed in the next cycle. B. Lookahead SA Baseline CEB encapsulates an EB router reducing the input buffering requirements. However, since allocation and traversal are 2 different stages, the minimal buffering requirement is 2 ﬂits for 100% utilization. We further reduce the input buffers to single ﬂit by performing the switch or CB allocation (IBSA/CBA) in parallel with the last LT/IB stage. This will also reduce the latency within the router to 1 cycle. Performing allocation in parallel with IB is achieved by separating the data and control information of a single ﬂit and sending the control information 1 cycle ahead of the data. Note that lookahead routing decides the output port of the next router in the previous one and sends this information along the data (other control information includes ﬂit type, etc). If we can forward this information one cycle ahead of the actual data e.g., during the ST cycle, it will arrive at the downstream router earlier, allowing it to contest for allocation 1 cycle earlier. Thus, when a ﬂit reaches its downstream input buffer, it will perform the ST or CBT stages immediately in the next cycle without waiting for the IBSA or CBA stage to complete. Since route computation can also be done in parallel with allocation, we can again send the next router output port information during the ST stage i.e., one cycle ahead. Note that this is different from prediction router [14], as allocations are deterministic and not predictive. Also, note that the ﬂit control information is already sent out-of-band in most on-chip routers. Even if it is sent in-band, it can be sent with the ﬂit information of the previous ﬂit. Thus, there is no extra wiring overhead of this scheme. We will assume out-of-band control information in this paper. Guaranteeing 1 cycle lookahead: A problem with lookahead SA is to guarantee that the control information always arrive 1 and only 1 cycle ahead of the corresponding data. This is a necessary condition because if SA wins earlier than the actual data arrival in the input buffer, random data will be propagated forward from that buffer. Note that in general, this is not guaranteed because data and control can get misaligned along the pipeline. We achieved this goal by utilizing the ready-valid Fig. 2. Guaranteeing 1 cycle ahead - Ready Valid handshake signals of data & Control path handshake signal of the previous pipeline stage in the data path to traverse the next pipeline stage in the control path. This can be seen from Figure 2. The ready out signal of the data path is also routed to the ready in signal of the same pipeline stage in the control path. Similarly, the valid in signal of this pipeline stage in the data path will be sent to valid in of the next pipeline stage in the control path. This will ensure that once the control information is in 1st pipeline stage of the link and the corresponding data is next to leave the output buffer, both will progress across the link, with control information always moving 1 cycle ahead. The control and data information also needs to be split in the input output buffers. The input control buffer needs to be 1 ﬂit larger than its data counterpart. On the other hand, the output control buffer can to be 1 ﬂit smaller than the data output buffer. Further details of why different buffer size is required is given in [11]. This technique will guarantee that control information always arrive 1 and only 1 cycle ahead of the data information. Thus IBSA and CBA can be performed 1 cycle earlier. C. Deadlock Avoidance Routing and message dependent deadlock avoidance is achieved by extending the bubble ﬂow control technique to ﬂit level using central buffers. This technique keeps the routing minimal thus ensuring minimum no load latency as well. Before explaining our scheme, we like to reiterate 3 conditions that are required for bubble ﬂow control to work. 1) Every ring or cycle must have a bubble 2) If there is a bubble in the ring, packets within the ring cannot wait indeﬁnitely on any other condition within the ring i.e. they have to make progress. 3) External packets entering the ring are not allowed to destroy the bubble. Avoiding Routing Deadlock: The idea of avoiding routing deadlock is simple. For every ring, even having a single ﬂit bubble is enough to ensure forward progress of ﬂits. For ﬂits entering the ring, we need to ensure that the whole packet will be allowed to enter the ring along with maintaining the original bubble of the ring. This is a necessary condition because of the following reason. If the whole packet is not allowed to enter the ring, even having a multi ﬂit bubble in the ring, e.g., in the input buffer, will not guarantee forward progress, i.e., condition 2 above will not be satisﬁed. This means that a bubble of packet length+1 is required when changing dimensions to ensure deadlock freedom. This bubble can be provided with the output based CBs without increasing the size of input and output buffers. Furthermore, since packets in the central buffer of the current router are part of the overall ring (corresponding to that output port), looking at the space of the next router’s CB (which will require CB credit information to ﬂow upstream) is not required. This is because if there is enough space in current router’s CB, it is guaranteed that ﬂits from the previous routers will move forward to create at least an equal amount of space in the next router. Thus the condition to avoid deadlock only requires looking at empty buffer space of itself and no credit information of the downstream router is needed which makes our technique perfectly suitable for EB-based channels. A formal proof is given in [11]. This condition is checked during allocation of both OBs and CBs i.e. during IBSA and CBA to ensure a bubble is maintained in the ring. Furthermore, checking full packet space is not required during CBSA as the packet has already entered the ring and therefore, same dimension condition will be applied here. This makes the minimum CB buffer size requirement to 2 ∗ dim ∗ P ktLength + 1 ﬂits. In practice, we can reduce the CB size to 2 ∗ dim ∗ (P ktLength − 3) + 1 by leveraging the fact that the 2-ﬂit output and 1-ﬂit input buffer is part of the overall ring. Starvation is also possible with CBs. We ensured starvation does not occur by round robin allocation of central buffers to each port. We would also like to mention here that this solution is feasible for on-chip networks where packet size is not large. In fact, all bubble ﬂow control techniques except worm-bubble [3] are not good solutions for networks with large packet sizes. Variable packet sizes are allowed as long as each ring keeps a bubble of the maximum packet size. Fig. 3. Message Dependent Deadlock Avoiding Message Dependent Deadlock: Message dependent deadlocks are usually solved by providing separate virtual networks as explained in [13]. The basic idea is to provide separate virtual network (channels) to every class of a message dependent chain. We propose the use of bubble ﬂow control technique to avoid message dependent deadlocks as well. Note that every reply of a request message (e.g in request reply networks) can be considered a 180 degree turn of the same packet, allowing the possibility of cycles between 2 or more different request-reply pairs. The packet source injecting new requests can be considered as an external entity that inserts new packets in this cycle (Figure 3(a)). These cycles will be deadlock free as long as we ensure that the 3 conditions of deadlock avoidance mentioned earlier are satisﬁed. Condition 1 and 3 can be easily satisﬁed by inserting request messages in the injection queue of the network interface (NI) only when there is a space of at least 2 packets, (Figure 3(a)). Satisfying condition 2 means that if there is only 1 empty space left in the injection queue, the reply message of a request will still be generated, even if there is a new request message pending to get inserted in the injection queue (Figure 3(b)). Thus a packet present within the message cycle (request message in the ejection queue) is allowed to progress to the next buffer (generate a reply message) with space of only 1 packet downstream (injection queue). External packets (pending request messages in the message sources) have to wait. Implementation of this scheme in network terminals means having the ability to accept new request messages and generate the corresponding reply, if there is an empty space in the injection queue. If it has no empty space, (Figure 3(c)), request messages can wait in the ejection queue but since there will be a bubble in the message cycle somewhere, this bubble will always propagate back to the injection queue allowing the request messages to get serviced. Note that this scheme is valid for any number of message classes without adding VCs. The use of bubble ﬂow control in the preceding manner makes it possible to deal with routing deadlock and message dependent deadlock with the same mechanism, e.g., there is no need for additional storage such as separate request and reply networks. In particular, the cost of dealing with message dependent deadlock is ﬁxed independent of the number of message classes. Overall, the cost of deadlock freedom at a router is independent of the network size or the number of message types. D. Power Gating of CB Since CBs are utilized only at high loads, we applied a simple coarse grained power gating technique to it. Power gating CB is simpliﬁed as it does not interfere with the main path of the router. Deadlock avoidance will be guaranteed as long as it will turn on in some ﬁnite amount of time when a packet is blocked. Initially, the CB is kept off. Whenever 2 packets collide for an output port, a counter starts counting the wait cycles of the unallocated packet. When the wait becomes X cycles, the CB is turned on. It takes 3 cycles for the CB to turn on completely. Once on, unallocated packets can be pushed into it allowing the blocked packets to move ahead. When an on CB is empty, and minimum on-time (set to 10 cycles) has passed, it is turned off. At low loads, this simple power gating technique keeps the CB off. At high loads, since we wait for X number of cycles before turning it on, this technique can potentially reduce performance. Sensitivity to value of X is explored in [11]. We ﬁxed the value of X to be 500 in all our power gating simulations. Dynamically adjusting the wait time at different loads can further reduce power reduction and improve throughput but is left for future. IV. R E SU LT S A. Simulation Setup We have developed 4 different router micro-architecture models to understand the latency and throughput impact of our CEB design. The baseline router consists of a standard 2 stage pipeline router with 2VCs per virtual network. The other 2 routers implemented are 2 stage EB router and a simple ﬂit deﬂection (FD) router similar to Flit BLESS from [17]. Parametric conﬁgurations of each of the routers is given in Table I. The DAMQ based central buffer is organized into 6 slots of 3 ﬂits each. Better implementation of the central buffer is left for future work. The default wait time before turning on the power gated CB is 500 cycles. Furthermore, to reduce the latency and buffering requirements of the deﬂection router, we retire the packets as soon as the tail ﬂit arrives without waiting for head and all body ﬂits to reach the destination. This makes the deﬂection router very optimistic. Since EB requires duplicate physical channels, we have assumed its links to be half wide with twice number of ﬂits per packet. We have Parameter Pipeline Depth InBuf Size (per port) OuBuf Size (per port) CBuf Size Inj/Ej Que Size Baseline 2 5*VC 2 na 20 FD 1 1 2 na 20 EB 2 2*Virt. Net 2 na 20 CEB 1 1 2 18 20 SY ST EM CON FIGURAT ION S O F VAR IOU S ROU TER S TABLE I implemented mesh, torus and generalized hypercube (GHC) topologies for both 2D and 3D networks. The 2D networks are 8x8 while 3D networks are 4x4x4. Number of ports for mesh and torus are 2 ∗ dim + 1 and (k − 1) ∗ dim + 1 for GHC topologies. Thus 2D-GHC, with k = 8 has the highest number of ports and therefore has the highest power consumption. The torus and mesh networks have single cycle link delays between adjacent routers. The GHC models multi-cycle links equal to the number of routers between the source and destination i.e. the link delay for node 2 and 3 from node 0 in the x dimension will be 2 and 3 cycles respectively. The packet size is kept ﬁxed (= 5 ﬂits) except EB routers which are 10 ﬂits as discussed earlier. All links are assumed to be 128 bits wide. All designs except deﬂection routers use minimal dimension order routing. For torus topologies with single VC and no central buffering, tranc routing from [20] is used which is an up down style non-minimal routing technique that does not use VCs. 4 different synthetic trafﬁc patterns (random, bit complement, bit reversal and tornado) are evaluated. Unless otherwise stated, all results present an average of all 4. Random distributes the trafﬁc evenly and has high throughput. All others are adversarial trafﬁc patterns with relatively low throughput. Tornado travels equal or more than k/2 hops in each dimension and thus has the highest no load latency. All simulations are performed for 50 million cycles. Applications traces are taken by running 64 threaded version of PARSEC and SPLASH benchmarks with 64 cores, 16 MC conﬁguration using an inhouse simulator with DRAMSim2 [22] as the main memory model. The traces are generated at the back side of L1 and messages are classiﬁed into read/write/coherence type requests. A reply of 5 ﬂits is generated from the destination every time a read request is received. Read requests and coherent messages for all networks including EB consists of 2 ﬂits and write messages are 5 ﬂits except in the EB network in which they are 10 ﬂits wide. This allows us to test our scheme for variable size packets as well. For power modeling, Orion 2.0 [25] is used which calculates the router power as the sum of the power in its buffers, crossbar, arbiters and allocators along with the link power. We modiﬁed Orion to get more accurate results. As a conservative estimate, EB links are modelled to take 3x more device power and 3x more leakage power in routing logic than non-EB links. Similarly the CEB router which has 3 arbiters takes 3x more power in arbiters. VC allocator power is assumed to be negligible for all cases. Segmented crossbars with 2 segments are used. For GHC topologies, partitioned crossbars are used. Baseline and EB routers are assumed to have 2 message classes, with 2 VCs per message class. FD and CEB do not model any VC or message class. CEB has an additional component of power due to its central buffer. All buffers are assumed to be register based. The network is modeled to be running at 2GHz with Vdd = 1.0V and 45nm technology. Activity for different components such as crossbar and input output buffers etc. are taken directly from performance simulations and fed as activity of different components of Orion. Power Gating a CB is assumed to reduce its leakage to 20% of the original. B. Performance with Synthetic Traces Comparison with Other Routers: Figure 4 compares throughput and average packet latency of CEB routers with that of other routers with different network conﬁgurations. Note that throughput is deﬁned as retired ﬂits per node per cycle. At low loads, CEB network has the latency equal to that of deﬂection (FD) router. This is because of the single cycle latency within the router. Both baseline and EB has 2 cycle latency within the router resulting in increased no load latency. Furthermore, EB has higher serialization latency since each link is narrower than the other routers. Baseline and deﬂection routers have the lowest saturation throughput. For deﬂection routers, the greater the number of ports per router, more numerous the deﬂections are, and thus saturation throughput does not increase with the number of ports. This can be seen in the case of GHC where deﬂection router saturates quickly compared to others. The baseline router has higher throughput than deﬂection in most cases but because of extra bubbles created due to credit based ﬂow control, their throughput is low as compared to routers that use elastic links even with multiple VCs. This difference increases with longer links in GHC topologies. Both EB and CEB have much higher throughput due to the use of elastic links. CEB has higher saturation throughput due to the removal of head of line blocking made possible through the central buffering. However, since travelling to the central Fig. 4. Throughput (Retired Flits/Node/Cycle) vs Average Latency (Cycles) for different network conﬁgurations buffer increases latency within the router, this is not always true. This can be seen in Figure 5 which shows the same graph of 3D torus but with individual trafﬁc patterns. Note that for Tornado trafﬁc EB performs better than CEB. Since Tornado is an adversarial trafﬁc pattern, it requires larger number of packets to traverse the central buffer and thus increased latency within the router and lower throughput. A similar behavior can be seen for the 3D Mesh topology in Figure 4 where the CEB curve starts going backwards. This means very high utilization of CB is also not desirable as it increases the latency within the router. be also used to compare the results of different topologies for CEB routers. Note that they have different link bandwidth and buffer requirements and therefore different area and power. The no-load latency of GHC topologies are the lowest. Their saturation throughput is close to 1. Mesh topologies have the highest no-load latency due to greater number of hop counts and lowest throughput. In general, the greater the number of ports, the lower will be the hop count and lower will be the noload latency with higher saturation throughput. This is not true in the case of other routers like deﬂection and baseline which has slow increase in performance with increase in number of ports. This makes CEB well suited for high radix routers. RowNo 1 2 3 4 5 6 7 8 9 Parameter Baseline-M1 EB-M1 Baseline-M4 EB-M4 FD-P4 FD-P20 CEB-GATE CEB-NOGATE CEB-NOSA 2D-Torus 3D-Torus 3D-GHC 2D-GHC 100 60 280 120 55 135 55 73 78 124 68 376 152 61 141 61 79 86 110 60 320 120 70 150 70 88 98 145 70 460 160 85 165 85 103 118 Fig. 5. Throughput (Retired Flits/Node/Cycle) vs Average Latency (Cycles) with different trafﬁc patterns BU FFER S PAC E (KB ) W I TH D I FFEREN T CON FIGURAT ION S TABLE II Impact of Individual Optimizations: CEB uses various optimizations for different purposes e.g. lookahead SA for latency reduction, power gating for power reduction and bubble ﬂow control for deadlock avoidance. We compare the advantages of individual optimizations in Figure 6 for various network topologies. In the ﬁgure, NOBUBBLE represents the case without any optimization and no bubble ﬂow control. NOSA adds bubble ﬂow control to the NOBUBBLE case. NOGATE adds lookahead SA to the NOSA case without power gating. It can be seen that NOGATE and GATE cases which have single cycle latency in the router by adding lookahead SA optimization has signiﬁcantly low no load latency. Their throughput, therefore, is higher in general. The torus topologies with NOBUBBLE have higher no load latency due to non-minimal routing (remember we use tranc routing for these cases). However, the saturation throughput of 3D torus with non-minimal routing is higher which shows the overhead of having bubbles in the network. Note that both NOSA and NOBUBBLE case has 2 ﬂit input buffer as opposed to single ﬂit in other cases. Lastly it should be noted that power gating closely tracks the case with no power gating specially in the case of GHC topologies, thus its performance overhead is low. Comparison of different topologies The above ﬁgures can C. Buffer Space Reduction Analysis Since our main goal is to reduce the buffering requirement of the network, we perform buffer space analysis for different routers and optimizations of CEB. Table II gives the total buffer space requirements of different routers with different topologies. The formula for calculating the buffer space is [(P ∗ (F ∗ V C + O) ∗ M ) + C + I + E ] ∗ L, where L is the link width, P is the number of ports per router, F , O , C is number of ﬂits in input, output and central buffer respectively and I and E are the injection and ejection queue size in ﬂits. M is the number of virtual networks required to support different message classes. For this analysis, torus topologies use 2 physical channels or 2 VCs in EB or baseline router respectively and GHC use 1 VC. We can see that the baseline router requires large buffer space even with single message class (row 1). EB with 1 message class requires less storage but it increases significantly with the increase in number of message classes as can be seen by row 4 with 4 message classes. Since GHC topologies use only 1 VC or virtual network, the storage requirement is reduced, however this will reduce throughput as well (not simulated). FD (row 5) requires the least buffering Fig. 6. Performance Impact (Throughput vs Latency) of individual optimizations space. However, if we consider that it has to re-organize ﬂits coming out of order at the network interface, which requires larger storage, the buffer space requirement of FD will also increase. If we increase the ﬂits space in ejection queue to 5 times, the buffering requirement of FD easily surpass most other networks (row 6). This is because the total ejection queue size aggregated over all NIs is 20K which will be increased to 100K. Thus reorganization overhead of FD is high both in buffer space and latency (which we have not model). Baseline Fig. 7. Static and Dynamic power distribution (a) varying injection rate, (b) varying topology CEB requires more buffer space than EB for single message class due to the presence of central buffer (row 8). But since, it does not require extra buffer storage to handle message classes, the storage requirement does not increase. When the gating is turned off, CEB has equal amount of buffer power as that of FD with no re-organization overhead. With gating on, the storage requirement increases slightly, however the increase is small specially for high radix topologies. On the other hand, as discussed earlier the throughput will be extremely high as compared to FD. Finally, the last row shows that lookahead SA saves 5K and 15K of buffer space for 2D torus and GHC topologies respectively. D. Power Analysis Figure 7 (a) compares the static and dynamic power dissipation of different routers conﬁgured in a 2D torus topology and normalized to the baseline case at injection rate of 150 cycles per packet. The different bars represent different injection rates (maximum time between 2 successive packets from a node) as given by the text in Figure 7(a). i.e., bar 1 represents injection rate of maximum 150 cycles per packet, bar 2 represents injection rate of 20 cycles per packet and so on. As obvious, baseline and FD have the maximum and minimum static power, respectively (the bottom component of each bar). Among the routers with elastic links, EB has more static power than CEB. This is because of the minimum requirement of having 4 physical networks (2 for each message class). This also results in EB having the highest dynamic power specially at high loads. Note that EB routers are 64 bit and individual networks have lower power. Elastic links have high power compared to others. This is because of both high activity and larger unit power (power required to traverse a ﬂit). The dynamic power of CEB is low compared to baseline and EB routers due to its small buffering space. Power Gating of the central buffer, although, reduces static power but its advantages at high loads are small. Figure 7 (b) shows the same plot with various topologies. This time it is normalized to the baseline case with 2D torus topology. CEB router has static power increase comparable to EB router. The dynamic power of CEB, however, increases rapidly. This is because of the very high saturation throughput and thus high activity of the CEB routers. Small increase of dynamic power in EB routers is attributed to thinner channels and crossbars. Although, this along with high saturation throughput makes EB routers a good candidate for NOCs, they loose on no-load latency. Furthermore, their power increases dramatically with increased number of message classes. Fig. 8. (a) Normalized average packet latency (b) Normalized throughput per unit power for real application traces E. Results with Real Benchmarks Figure 8 (a) shows average packet latency of 2D-Mesh network with different routers normalized to the CEB case. In general, FD has the maximum average latency while CEB has the least. In few of the benchmarks, this latency is extremely high. This is due to unnecessary deﬂections and lack of starvation avoidance in FD routers. In these routers, packets at the injection queue are prioritized lower than the packets already present in the network guaranteeing availability of ports. However, this can potentially lead to starvation at very high load and thus increased latency. Average latency of baseline and EB routers increase by 1.4-1.6x than CEB due to increased no load latency and lower throughput. The trends are similar across different benchmarks. Figure 8 (b) shows the throughput per unit power of the same conﬁguration normalized to the CEB case. CEB performs better than all other routers. Again, FD routers perform the worst because of its large packet latency. All other routers have similar throughput as they retired almost equal number of packets in a ﬁxed amount of time. However, the power consumed by baseline and EB router is higher than the CEB case. This behavior is directly attributed to higher latency and larger buffering requirements of both the baseline and EB routers. We conclude that CEB reduces power at ﬁxed load and decreases average packet latency. If further reduction in latency is required, high radix topologies can be used. Under a ﬁxed load CEB will perform better. V. R E LATED WORK Reducing latency within the router has been explored in a number of works. Scarab [12] and Chipper [7] uses buffer-less routing similar to BLESS. [9] uses packet dropping to remove deﬂections in buffer-less routers. Roshaq [24] and IBMSP2 [1] uses the same concept of having shared central buffers along with a fast bypass path for low load common case. Both use credit based ﬂow control and has inherent limitations with longer pipelines. ’High Throughput Shared Buffer NOC’ router [21] uses multiple shared buffers and 2 crossbars and are power hungry. Our design performs better than these cases due to minimal deterministic routing, EB links and single central buffer. Prediction routers [14] speculatively perform SA stage in parallel with IB. The overhead of speculation, however, is high. Our lookahead SA is deterministic, thus does not have any prediction overhead. Some other designs have used EB links to reduce pipeline bubbles. Hybrid EB-VC [16] adds VC buffers to avoid deadlocks in EB. They use a technique similar to on-off ﬂow control for drainage of ﬂits into VC buffers. Again, on-off ﬂow control requires these buffers to be large enough increasing their area and power. Kilo-Noc [10] uses EB for its MECS topology. They fall back to VC based buffering space in the routers to avoid ﬂits of different virtual channels to deadlock each other. Furthermore, their approach is tailor-made for MECS topology. The scope of our router is much broader as it can work with many different topologies and favours high radix networks with reduced buffering requirement. Chen et. al [4] recently proposed Critical bubble and Worm bubble ﬂow control [3]. These needs to be incorporated with our design to increase throughput further. Dimensional bubble ﬂow control [2] uses bubble to provide adaptivity in Mesh networks using single VC. We have not explored adaptive routing with CEB. But providing adaptivity using similar approach is a key next step. Finally, power gating [15] has been explored but its scope is limited in input based VC routers. Central buffers are a natural component for gating. V I . CONC LU S ION S In this paper, we present CEB, a novel low latency, low power router micro-architecture and compared it with other low latency designs. We have shown that a small central buffer in an EB channel based router can be used to avoid deadlock and improve throughput without the need of having separate physical networks. We further presented lookahead SA which is used to achieve no load latency comparable to wire latency. Our results show an average improvement of 3x in throughput / unit power and 1.6x in average latency for PARSEC and SPLASH benchmarks conﬁgured in a 2D Mesh topology. The improvements get higher with higher radix topologies. The key next step is to provide support for adaptivity and QoS guarantees (generally provided using VCs) in CEB networks that does not have multiple VCs. ACKNOW L EDG EM EN T S This research was supported in part by the National Science Foundation under grant CNS 0855110 and Sandia National Laboratories. We also acknowledge the detailed and constructive comments of the reviewers. "
Headfirst sliding routing - A time-based routing scheme for bus-NoC hybrid 3-D architecture.,"A contact-less approach that connects chips in vertical dimension has a great potential to customize components in 3-D chip multiprocessors (CMPs), assuming card-style components inserted to a single cartridge communicate each other wirelessly using inductive-coupling technology. To simplify the vertical communication interfaces, static Time Division Multiple Access (TDMA) is used for the vertical broadcast buses, while arbitrary or customized topologies can be used for intra-chip networks. In this paper, we propose the Headfirst sliding routing scheme to overcome the simple static TDMA-based vertical buses. Each vertical bus grants a communication time-slot for different chips at the same time periodically, which means these buses work with different phases. Depending on the current time, packets are routed toward the best vertical bus (elevator) just before the elevator acquires its communication time-slot. Network simulations show that Headfirst sliding routing reduces the communication latency by up to 32.7%, and full-system CMP simulations show that it reduces application execution time by 9.9%. Synthesis results show that the area and critical path delay overheads are modest.","Headﬁrst Sliding Routing: A Time-Based Routing Scheme for Bus-NoC Hybrid 3-D Architecture Takahiro Kagami1 , Hiroki Matsutani1 , Michihiro Koibuchi2 , and Hideharu Amano1 1Keio University 3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Japan blackbus@am.ics.keio.ac.jp 2National Institute of Informatics 2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan koibuchi@nii.ac.jp Abstract—A contact-less approach that connects chips in vertical dimension has a great potential to customize components in 3-D chip multiprocessors (CMPs), assuming card-style components inserted to a single cartridge communicate each other wirelessly using inductive-coupling technology. To simplify the vertical communication interfaces, static Time Division Multiple Access (TDMA) is used for the vertical broadcast buses, while arbitrary or customized topologies can be used for intra-chip networks. In this paper, we propose the Headﬁrst sliding routing scheme to overcome the simple static TDMA-based vertical buses. Each vertical bus grants a communication time-slot for different chips at the same time periodically, which means these buses work with different phases. Depending on the current time, packets are routed toward the best vertical bus (elevator) just before the elevator acquires its communication time-slot. Network simulations show that Headﬁrst sliding routing reduces the communication latency by up to 32.7%, and full-system CMP simulations show that it reduces application execution time by 9.9%. Synthesis results show that the area and critical path delay overheads are modest. I . IN TRODUC T ION The three-dimensional integration is a promising VLSI architecture that stack several smaller wafers or dies in order to reduce the wire length and wire delay, and three-dimensional Network-on-Chip (3-D NoC) [28] has been extensively studied in terms of its network topology [18][7][23], router architecture [12][14][22], and routing strategy [25]. Various interconnection techniques have been developed to connect multiple chips in a 3-D IC package: wire-bonding, micro-bump [2][13], wireless (e.g., capacitive- and inductivecoupling) [6][10][20][21] between stacked dies, and throughsilicon via (TSV) [6][3] between stacked wafers. These 3-D IC technologies are compared in [6]. Many recent studies on 3D IC architectures focus on micro-bump and TSV techniques that offer the highest level of interconnect density. On the other hand, as another 3-D integration technique, the inductivecoupling can connect more than two examined dies without wire connections. The wireless contact-less approach that connects chips in vertical dimension has a great potential to customize components in 3-D chip-multiprocessors (CMPs), assuming cardstyle components inserted to a single cartridge communicate each other wirelessly using inductive-coupling technology. Although power supplies are provided by bonding wires at this moment, wireless power transmission techniques using inductive-coupling have been improved recently [30][29][24]. The inductive-coupling power transmission can be used for these card-style components inserted to a cartridge [4]. In this case, adding, removing, and swapping chips in a package after the chips have been inserted to a cartridge are possible, which will bring us a great ﬂexibility of “ﬁeld stackable systems” using the card-style components in the future. Toward this purpose, the vertical communication interfaces should be simpliﬁed, while arbitrary or customized topologies should be used for intra-chip networks; thus, we focus on static Time Division Multiple Access (TDMA) buses for the inter-chip communication. In this paper, we propose the Headﬁrst sliding routing scheme to overcome the simple static TDMA-based vertical buses. The static TDMA-based vertical buses grants a communication time-slot for different chips at the same time periodically, which means they are working with different periodic scheduling. For example, at a certain moment, vertical bus 0 gives a time-slot for chip 1, vertical bus 1 allows chip 2, and vertical bus 2 allows chip 0. At the next phase, vertical bus 0 gives a time-slot for chip 2, vertical bus 1 allows chip 0, and vertical bus 2 allows chip 1. Each vertical bus behaves just like an elevator in an ofﬁce building. Fortunately, a waiting time to obtain the time-slot of vertical bus (elevator) is predictable for each chip, thus a key design of packet routing is to select the best elevator that minimizes the waiting time. The best elevator to route packets to a destination depends on the current time. The proposed Headﬁrst sliding routing routes packets toward the best elevator so that the packets acquire their communication time-slot just when they arrive at the elevator. In this paper, Headﬁrst sliding routing and a conventional minimal routing are compared in terms of the zero-load latency, network performance (latency vs. offered workload), and application execution time using a full system CMP simulator. The area and critical path delay overheads are also evaluated. The rest of this paper is organized as follows. Section II overviews the 3-D bus-NoC architecture that uses buses for vertical dimension and NoC for horizontal dimension. It also introduces wireless inductive-coupling technology. Section III proposes Headﬁrst sliding routing for wireless 3-D NoC-Bus architecture. Section IV analyzes the zero-load latency and Section V evaluates the performance and cost. We conclude the paper in Section VI. I I . R E LAT ED WORK A. Wired 3-D Interface and Bus Structure Micro-bump [2][13] and Through-silicon via (TSV) [6][3] have been mature techniques utilized in actual products. Although the micro-bump is mostly limited for face-to-face connections of two dies, the TSV is used to connect a number of chips and so 3-D bus structure is utilized. Especially, the 3-D bus is efﬁcient for connecting processors and memory systems in 3-D multi-core systems[15]. Dynamic Time Division Multiple Access (dynamic TDMA) [26][1] bus is introduced for distribution of bus mastership between multiple chips, while point-to-point Network-on-Chips (NoCs) are used inside the chip. HIBS[5] adds paralleism to the bus structure. Such 3-D architecture is called Bus-NoC Hybrid architecture, and it has become a conventional way to build 3-D NoCs. However, as described later, these architectures is difﬁcult to be used in inductive coupling bus. B. Wireless Inductive Coupling Techniques on wireless chip interconnection are classiﬁed into two: capacitive-coupling and inductive-coupling. Here, we concentrate inductive coupling, since capacitive coupling is mainly used for face-to-face connecting with two chips. Inductive-coupling [20] [19] [24] uses square or hexagon coils as data transmitters.The coils can be implemented with common metal layers of the chip, and no special process technology is required for building them. Inductive-coupling has potential as an interconnection technology for custom building-block SiPs, since addition, removal, and swapping of chips become possible after the chips have been fabricated and stacked in a package with a low cost. A contact-less interface without ESD protection device has been shown to be able to handle bit rate of more than 1GHz with a low energy dissipation (0.14pJ per bit) and a low bit-error rate (BER < 10−12 ) [20]. In the inductive-coupling approach, data modulated by a driver are transferred between two coils that are exactly superimposed on each other. The driver and inductor pair for sending data is called the TX channel, while the receiver and inductor pair is called the RX channel. Since a coil can be used both for transmitter and receiver, the TX channel and RX channel can be switched quickly, that is, a half-duplex bi-directional channel can be formed by using a single coil. Also, data multicast can be used if a TX channel is placed at the same location of multiple RX channels in different chips. By switching of TX/RX channel on a multicast channel, an inductive coupling bus can be formed on multiple chips. Although a lot of practical systems are available by using point-to-point networks using inductive coupling, there is few report to use bus. Since it takes relatively large latency for arbitration of multiple chips, a simple TDMA is preferred for inductive coupling bus. In MuCCRA-Cube [27], TDMA bus is used for 3-D links between PE (Processing Element) array of dynamically reconﬁgurable processors. In this system, four time-slots are assigned into each chip, and a chip can send the data in the term of the assigned time-slot. The 3-D links were not efﬁciently used because of the low utilization ratio for 3-D direction [27]. I I I . W IR E L E S S 3 -D NOC -BU S HYBR ID ARCH I T EC TUR E Toward the practical wireless 3-D ICs that allow us to add, remove, and swap the chips in the ﬁeld, hardware complexity of vertical communication lines (e.g., number of inductors) should be minimized. Thus, static TDMA buses are preferred for the inter-chip communication compared to the dynamic one that requires additional control lines (i.e., dedicated inductors) for dynamic arbitration. To ﬁll in the gap between static and dynamic TDMA schemes while keeping the hardware simplicity of the static scheme, the following two ideas are combined in this paper. ∙ Phase-shift static TDMA control for multiple vertical ∙ Headﬁrst sliding routing scheme that routes packets to the broadcast buses best vertical bus (elevator) depending on the current time, in order to minimize the waiting time at the elevator Note again that arbitrary or customized topologies can be used for intra-chip networks. A. Phase-Shifted Static TDMA Buses The major performance bottleneck of the static TDMA scheme is the waiting time at the bus to acquire a time-slot. The waiting time for a time-slot increases as the number of chips increases. The negative impact will also increase when state-of-the-art low-latency single-cycle routers are employed in the horizontal dimension. To minimize the waiting time at the buses, all the vertical buses grant a communication timeslot for different chips at the same time periodically, which means they are working with different periodic scheduling(See Figure 1). Let 𝑛 is the number of chips and 𝑇𝑠𝑙𝑜𝑡 is the length of a time-slot. At time 𝑇 , vertical bus 𝑖 gives a time-slot for chip (𝑇 /𝑇𝑠𝑙𝑜𝑡 + 𝑖)𝑚𝑜𝑑 𝑛, When the number of vertical buses is greater than or equal to 𝑛, at least one vertical bus is available waiting time from 𝑛𝑇𝑠𝑙𝑜𝑡 − 1 to 𝑇𝑠𝑙𝑜𝑡 − 1 assuming no packet for sending on each chip, which can reduce the worst-case contentions at the selected elevator. B. Routing Schemes To route packets in the wireless 3-D NoC in which vertical links employ the phase-shift static TDMA buses while horizontal networks employ arbitrary or customized topologies, we propose to use two routing schemes: Minimum-hop (MH) routing and Headﬁrst sliding (HS) routing. MH routes packets using a minimal path between a source and a destination via an elevator. Our observation is that MH routing achieves a high saturated throughput while its zero-load communication latency is longer than that of the dynamic TDMA (ideal case) due to the waiting time at elevators. Another observation is that the best Inductor Cartridge Chip2 TX RX RX Chip1 RX RX TX RX Chip0 RX RX RXTX y c n e a L t Fig. 1. Phase-shifted static TDMA Buses Current Arrival Current Arrival A B C Chip0 Chip1 Chip1 Chip2 Chip0 Chip2 Chip0 Chip1 Chip1 Chip2 Chip1 Chip1 Chip2 Chip0 Chip1 Chip1 A A B B C C Chip0 Chip1 Chip1 Chip2 Chip0 Chip2 Chip0 Chip1 Chip1 Chip2 Chip1 Chip1 Chip2 Chip0 Chip1 Chip1 Time Time Timeslot B Chip1 A C B Chip1 A C On-Chip Router Vertical Bus Fig. 2. Concept of Headﬁrst sliding routing MH HS dynamic TDMA Offered Workload Fig. 3. Contribution of this work S1 D3 D1 S2 S3 D2 elevator to route packets to a destination is depending on the current time. HS routes packets toward the best elevator, based on the current time, so that they acquire their communication time-slot when they arrive at the elevator (see Figure 2). To ﬁll in the gap between static and dynamic TDMA schemes while keeping the hardware simplicity of the static scheme, MH and HS routing schemes are used as follows. ∙ When a offered workload is less than a certain threshold value, HS routing is used to further reduce the communica∙ When a offered workload is more than the threshold, MH tion latency. routing is used to achieve a higher saturated throughput. Figure 3 illustrates the contribution of this work. The xaxis shows the offered workload and the y-axis shows the communication latency. By using MH and HS routing schemes depending on the workload, the expected throughput vs. latency curve is emphasized with a bold line. Note that there is no throughput degradation compared to MH routing and its communication latency can be close to the ideal dynamic TDMA scheme when the workload is not high by using the time-based HS routing scheme. The following sections will illustrate MH and HS routing schemes. 1) Minimum Hop Routing: Packets are routed based on the following rules. ∙ Transfer rule 1: If the source and destination are on the Fig. 4. An example of deadlock situation same chip, packets are routed based on arbitrary deadlockfree routing on the chip (e.g., XY routing on 2-D mesh ∙ Transfer rule 2: topology). If the source and destination are on different chips, packets are ﬁrst routed to an elevator on the source chip, moved to the destination chip, and routed to the destination. An elevator is selected so that the hop count is minimized. MH routing does not guarantee deadlock-freedom without situation. Each chip employs 4 × 4 2-D mesh topology, in virtual channels. Figure 4 illustrates an example of deadlock which XY routing is used for intra-packet transfers. In this case, S1 sends a message to D1, S2 sends a message to D2, and S3 sends a message to D3; thus they cause the cyclic dependency which introduces deadlocks. To avoid such structural deadlocks, two VCs are required for all the routers, and the following rule is imposed to MH routing. ∙ Transfer rule 3: If the source and destination are on different chips, packets are transferred with VC-0 on the source chip, while VC-1 is used on the destination chip after an elevator is used. If the source and destination are on the same chip, only VC-1 is used for the packet transfer. 2) Headﬁrst Sliding Routing: Depending on the current time, packets are routed toward the best elevator so that an expected transfer time is minimized. The above-mentioned Transfer rule 2 is updated as follows. ∙ Transfer rule 2’: If the source and destination are on different chips, packets are ﬁrst routed to an elevator on the source chip, moved to the destination chip, and routed to the destination. An elevator is selected so that the expected transfer time 𝑇 is minimized. 𝑇 is formulated as follows. 𝑇 = 𝑅𝐻𝑠𝑑 + 𝑇𝑤𝑎𝑖𝑡 (1) where 𝑅 is a ﬂit transfer time at a router, 𝐻𝑠𝑑 is the number of hops from source to destination, and 𝑇𝑤𝑎𝑖𝑡 is an expected waiting time at an elevator. 𝑇𝑤𝑎𝑖𝑡 is calculated as follows. First the arrival time of a packet to an elevator 𝑇𝑎𝑟𝑟𝑖𝑣𝑒 is calculated as follows, assuming no packet contentions. 𝑇𝑎𝑟𝑟𝑖𝑣𝑒 = 𝐶 𝑢𝑟𝑟𝑒𝑛𝑡𝑇 𝑖𝑚𝑒 + 𝑅𝐻𝑠𝑏 , (2) where 𝐻𝑠𝑏 is the number of hops from source to elevator, which is depending on the routing algorithm. The transfer start and ﬁnish times can be estimated based on this 𝑇𝑎𝑟𝑟𝑖𝑣𝑒 . Let 𝑇𝑎𝑙𝑙𝑜𝑐 is a time-slot allocation time and 𝑇𝑠𝑙𝑜𝑡 is the length of a time-slot. If a packet transfer start time is greater than or equal to 𝑇𝑎𝑙𝑙𝑜𝑐 and a packet transfer ﬁnish time is less than 𝑇𝑎𝑙𝑙𝑜𝑐 + 𝑇𝑠𝑙𝑜𝑡 , 𝑇𝑤𝑎𝑖𝑡 is zero. Otherwise, 𝑇𝑤𝑎𝑖𝑡 is set to the next time-slot allocation time. C. Run time Routing Policy Switching In order to switch HS and MH routing at run time, each on-chip router employs both routing policy at the local input port. The router selects either routing policy depending on the offered network load. For measurement of the network load, the number of packets the local input buffer receives is counted with a packet counter which is reset to zero for the every m cycles. Here, the following simple selection algorithm is adopted. First, the HS routing is used. The counter is incremented when a header ﬂit arrives at the local input buffer. If the counter value reaches the threshold before resetting to zero, the MH routing is selected. Otherwise, the HS routing is selected. Threshold is selected depending on the simulation as described later. IV. ANA LY S I S First, we analyzed the zero-load latencies for MH routing and HS routing when the source and destination are different chips. For obtaining baseline results, uniform trafﬁc is assumed. Given that a packet which consists of 𝐿 ﬂits goes through 𝐻 routers, its zero-load latencies, 𝑇0 , is calculated as 𝑇0 = 𝐻 (𝑇𝑟𝑜𝑢𝑡𝑒𝑟 + 𝑇𝑙𝑖𝑛𝑘 ) + 𝑇𝑏𝑢𝑠 + 𝐿/𝐵𝑊 + 𝑇𝑏𝑙𝑜𝑐𝑘 , (3) where 𝑇𝑟𝑜𝑢𝑡𝑒𝑟 , 𝑇𝑙𝑖𝑛𝑘 , and 𝑇𝑏𝑢𝑠 are latencies for transferring a header ﬂit on a router, a link, and a bus, and 𝑇𝑏𝑙𝑜𝑐𝑘 is the waiting time in the case when a packet misses the time-slot. CON FIGURAT ION S O F D I FF ER EN T BU S P LAC EM EN T PO L IC I E S TABLE I Pattern 𝐵 (Number of buses) sparse2 2 dense2 2 sparse4 4 dense4 4 sparse8 8 dense8 8 Placement method sparse dense sparse dense sparse dense MH and HS routing schemes show different zero-load latencies since their 𝑇𝑏𝑙𝑜𝑐𝑘 differs. Under uniform trafﬁc,𝑇𝑏𝑙𝑜𝑐𝑘 for MH (𝑇𝑏𝑙𝑜𝑐𝑘,𝑚ℎ ) is calculated as follows 𝑇𝑏𝑙𝑜𝑐𝑘,𝑚ℎ = ∑𝑇𝑚𝑎𝑥 𝑡=1 𝑀 𝑇𝑠𝑙𝑜𝑡 𝑡 , (4) where 𝑀 is the number of stacked chips and 𝑇𝑚𝑎𝑥 is the maximum time of waiting for the valid time-slot assignment. On the other hand, 𝑇𝑏𝑙𝑜𝑐𝑘 for HS, 𝑇𝑏𝑙𝑜𝑐𝑘,ℎ𝑠 is described as 𝑇𝑏𝑙𝑜𝑐𝑘,ℎ𝑠 = 𝑓 (𝑋𝑏𝑢𝑠 , 𝑌𝑏𝑢𝑠 , 𝑇𝑠𝑙𝑜𝑡 , 𝑀 ), (5) where 𝑋𝑏𝑢𝑠 and 𝑌𝑏𝑢𝑠 are 2-D coordinates of a bus, 𝑇𝑠𝑙𝑜𝑡 is the length of a time-slot, 𝑀 is the number of chips. Based on Equations 4 and 5, the zero-load latencies for MH and HS routing schemes are analyzed by using a ﬂitlevel simulator. The 3-D stacked chips each of which uses a 4 × 4 mesh topology are selected as targets. The number of chips M is assumed as 2 ≤ 𝑀 ≤ 8. Six conﬁgurations of bus placement are summarized in Table I. The results are depending on location of 3-D buses. Here, two placement methods, sparse and dense are examined. In sparse method, the buses are distantly located along the edges of the chip. On the other hand, in dense method, they are located near the center of the chip locally. As an example, Figure 5(a) and 5(b) show sparse and dense, respectively given that 𝐵 = 4. Figure 6 shows the zero-load latencies for MH and HS, assuming 𝑇𝑟𝑜𝑢𝑡𝑒𝑟 = 2, 𝑇𝑙𝑖𝑛𝑘 = 1, 𝐿 = 5, and 𝑇𝑠𝑙𝑜𝑡 = 8. Here, dense4 is adopted as the bus location pattern. The similar tendency is observed when other bus location patterns are used. The result shows that 𝑇𝑏𝑙𝑜𝑐𝑘 has a great impact on overall latency. HS reduces the zero-load latency compared with MH, since it can reduce 𝑇𝑏𝑙𝑜𝑐𝑘 drastically. As the number of stacked chip increases, the reduction of 𝑇𝑏𝑙𝑜𝑐𝑘 increases. In contrast, it slightly increases 𝐻 compared with MH, since it is not a minimal routing. V. EVA LUAT ION S In this section, the hardware overhead of the router with the HS routing is evaluated ﬁrst. Then, we compare the MH routing with the HS routing in terms of their communication latency and impact to application performance. (a) sparse4 (b) dense4 Fig. 5. An example of bus placement method ] ] l l e e c c y y c c [ [ y y c c n n e e a a L L t t  70  70  60  60  50  50  40  40  30  30  20  20  10  10  0  0 tx block(MH) tx block(MH) routing(MH) routing(MH) tx block(HS) tx block(HS) routing(HS) routing(HS)  2  2  3  3  4  4  5  5  6  6 Number of Chips Number of Chips  7  7  8  8 Fig. 6. Zero-load latency TABLE II BA S E L IN E ROU T ER Port Buffer Routing Switching Pipeline Stage Flit Size 5 input/output port 5 ﬂit XY Routing Wormhole 2 VCs [RC/VSA] [ST] [LT] 128 bit A. Hardware Overhead Here, we designed the HS router based on a 5-port common router with 3-stage pipeline structure shown in Table II, and evaluated the hardware amount and critical path delay. The HS routing algorithm is implemented on the RC stage of the baseline router. The design is described in Verilog HDL, and synthesized at operating frequency of 500MHz by using the Synopsys Design Compiler version D-2010.3. Fujitsu’s eshuttle 65nm CMOS process with 12-layer, and a standard cell library CS202SZ are used. Figure 7 compares the area of the baseline router and the HS router. The area overhead of the HS is only 2.3% compared with the baseline router. Note that if the number of VCs or the size of the buffer increases, the relative overhead becomes small. The critical paths of both routers are on the LT stage, and their maximum delays are the same, although the RC Stage of the HS router is heavier than that of the baseline router. Thus, the operational frequency is not inﬂuenced. ] ] 2 2 m m u u [ [ a a e e r r A A r r e e t t u u o o R R  100000  100000  80000  80000  60000  60000  40000  40000  20000  20000  0  0 Baseline Router Baseline Router HS Router HS Router Fig. 7. Hardware amount B. Network Performance In order to measure the baseline network performance, we used a ﬂit-level simulator written by C++ [9]. In the simulation, HS, MH and the case of switching the HS and the MH (HS + MH) are compared. Also, the network performance on the ideal dynamic TDMA scheme is evaluated. The target 3-D stacked chips are as described in Section IV, and in the case of HS + MH, the counter reset interval is 512 cycles. The threshold value of switching routing policys are determined based on the cross point of HS and MH shown in Figure 8 later. Let 𝑇 𝐻 be the offered workload of cross point, 𝑚 be the reset interval of packet counter, and 𝑆 be the packet size. The threshold value is given as 𝑚𝑇 𝐻/𝑆 . Figure 8 shows simulation results under the uniform trafﬁc, matrix trafﬁc and reversal trafﬁc. Here, sparse4 (4-chip) and dense4 (4-chip) are adopted as the bus location pattern. The similar tendency is observed when other bus location patterns are used. As shown in ﬁgures, the HS improves latency, compared with the MH when the offered workload is low. In paticular, the dense8 (8-chip) reduces the latency by up to 32.7% compared with MH. As the workload increases, the difference becomes gradually small, and when it is more than a certain threshold value, the latency of the HS is larger than that of the MH. This tendency is just the same as expected in Section III-B. Also, the results demonstrate that the HS + MH latency is close to the HS routing scheme when workload is not high, and in the region of high workload, the latency of the MH is close to the HS + MH routing scheme. Moreover, the HS + MH latency is close to the dynamic scheme in the region of low workload. Thus, by switching the HS and the MS     ] ] l l e e c c y y c c [ [ t t y y c c n n e e a a L L e e g g a a r r e e v v A A  80  80  70  70  60  60  50  50  40  40  30  30  20  20  0  0 MH MH HS HS HS + MH (m = 256) HS + MH (m = 256) HS + MH (m = 512) HS + MH (m = 512)  0.01  0.01  0.02  0.02  0.03  0.03  0.04  0.04 Injected Traffic [flit/cycle/node] Injected Traffic [flit/cycle/node]  0.05  0.05  0.06  0.06 Fig. 9. Change the reset interval schemes depending on the workload, we can further reduce the communication latency compared to MH routing even by using the simple static TDMA buses. The interval of counter reset is important to select the routing scheme. As shown in Figure 9, the latency of the MH + HS is close to the MH routing scheme when the short interval is used because of the low resolution of the workload measurement. As the interval increases, the resolution is improved. However, the large interval value makes the switching slow. Thus, there is a trade-off between the measure resolution and switching speed. C. Application Performance In order to measure the impact to real application performance, full system simulations of wireless 3-D CMPs are performed. Table III shows target 3-D shard-memory CMPs consisting of four chips. They are connected with multiple vertical buses as shown in Table I. SNUCA[11] is adopted as a cache architecture, and four memory controllers are placed in the four corners of the bottle chip. Other parameters are listed in Table IV. For simulation, we used a full system multi-processor simulator: GEMS[17] and Wind River Simics[16]. We modiﬁed a detailed network model of GEMS, called Garnet, in order to precisely simulate behavior of the MH and HS. A directory-based MOESI coherence protocol that uses three message classes is used. Six VCs are, thus required for each input port since each message class requires two VCs to avoid structural deadlocks. To evaluate the application performance of these routing schemes, we used eight parallel programs using OpenMP from NAS Parallel Benchmarks[8] on Sun Solaris 9 operating system. These benchmark programs are compiled by Sun Studio 12 and executed on Solaris 9. The number of threads is set to eight. Figure 10 shows the application execution cycles of eight benchmark programs in the case of four buses. The application execution time (Y-axis) is normalized to the execution time using the MH. As shown, HS improves execution cycles by 5.9% - 9.9%, compared with those of the MH. This comes from that in the assumed CMPs and application programs, the TABLE III CH I P CON FIGURAT ION Topology Routing Processor/L1Cache L2Cache Router 4×4 mesh XY Routing 2 16 16 TABLE IV S IMU LAT ION PARAM E T ER Processor L1 I/D cache size L1 cache latency L2 cache bank size L2 cache latency Memory size Memory latency Router pipeline Buffer size Flit size Protocol # of Message Classes Control / data packet size UltraSPARC-III 64 KB (line:64B) 1 cycle 256 KB (assoc:4) 6 cycle 160 (± 2) cycle 4 GB [RC/VSA][ST][LT] 5-ﬂit per VC (default) 128 bit MOESI directory 3 1 ﬂit / 5 ﬂit workload is not so heavy. In this evaluation, the advantages of the HS is large when the location dense4 is used. V I . CONC LU S ION S Toward the practical wireless 3-D ICs that allows us to add, remove, and swap the chips in the ﬁeld, hardware complexity of vertical communication lines (e.g., number of inductors) should be minimized. Thus, static TDMA buses are preferred for the inter-chip communication compared to the dynamic one that requires additional control lines (i.e., dedicated inductors) for dynamic arbitration. To ﬁll in the gap between static and dynamic TDMA schemes while keeping the hardware simplicity of the static scheme, we propose Headﬁrst sliding routing scheme, which routes a packet toward the best vertical bus (elevator) just before the elevator acquires its communication time-slot, depending on current time. In addition, we propose to switch Headﬁrst sliding routing and a conventional minimal routing (Minimum hop routing) in accordance with a offered workload. In this paper, we analyzed zero-load latencies of Headﬁrst sliding routing and Minimum Hop routing, and evaluated them in terms of the network performance (latency vs offered workload) and the application performance. According to the analysis, we conﬁrmed that the time when the packet transfer wait at buses have a heavy impacts on the whole latency, and Headﬁrst sliding routing can drastically reduce it. The result of the network performance showed that Headﬁrst sliding routing reduces latency up to 32.7% at a low workload, although Minimum hop performs better than Headsliding ﬁrst at a high workload. It present that the selection between Headﬁrst sliding and Minimum hop in accordance with a offered workload is the best approach in static TDMA-based bus architectures. Moreover, at a low workload, Headﬁrst sliding performs nearly as good as a minimal routing with          20  20  30  30  40  40  50  50  60  60  70  70  80  80  0  0  0.01  0.01  0.02  0.02  0.03  0.03 Injected Traffic [flit/cycle/node] Injected Traffic [flit/cycle/node]  0.04  0.04  0.05  0.05 A A e e v v r r y y c c n n e e a a L L e e g g a a t t [ [ e e c c y y c c l l ] ] MH MH HS HS HS + MH HS + MH dynamic TDMA dynamic TDMA (a) sparse4 (4-chip) Uniform  20  20  30  30  40  40  50  50  60  60  70  70  80  80  0  0  0.01  0.01  0.02  0.02  0.03  0.03 Injected Traffic [flit/cycle/node] Injected Traffic [flit/cycle/node]  0.04  0.04  0.05  0.05 A A e e v v r r y y c c n n e e a a L L e e g g a a t t [ [ e e c c y y c c l l ] ] MH MH HS HS HS + MH HS + MH dynamic TDMA dynamic TDMA (b) dense4 (4-chip) Uniform  20  20  30  30  40  40  50  50  60  60  70  70  80  80  0  0  0.01  0.01  0.02  0.02  0.03  0.03 Injected Traffic [flit/cycle/node] Injected Traffic [flit/cycle/node]  0.04  0.04  0.05  0.05 A A e e v v r r y y c c n n e e a a L L e e g g a a t t [ [ e e c c y y c c l l ] ] MH MH HS HS HS + MH HS + MH dynamic TDMA dynamic TDMA (c) sparse4 (4-chip) Matrix  20  20  30  30  40  40  50  50  60  60  70  70  80  80  0  0  0.01  0.01  0.02  0.02  0.03  0.03 Injected Traffic [flit/cycle/node] Injected Traffic [flit/cycle/node]  0.04  0.04  0.05  0.05 A A e e v v r r y y c c n n e e a a L L e e g g a a t t [ [ e e c c y y c c l l ] ] MH MH HS HS HS + MH HS + MH dynamic TDMA dynamic TDMA (d) dense4 (4-chip) Matrix  20  20  30  30  40  40  50  50  60  60  70  70  80  80  0  0  0.01  0.01  0.02  0.02  0.03  0.03 Injected Traffic [flit/cycle/node] Injected Traffic [flit/cycle/node]  0.04  0.04  0.05  0.05 A A e e v v r r y y c c n n e e a a L L e e g g a a t t [ [ e e c c y y c c l l ] ] MH MH HS HS HS + MH HS + MH dynamic TDMA dynamic TDMA (e) sparse4 (4-chip) Reversal  20  20  30  30  40  40  50  50  60  60  70  70  80  80  0  0  0.01  0.01  0.02  0.02  0.03  0.03 Injected Traffic [flit/cycle/node] Injected Traffic [flit/cycle/node]  0.04  0.04  0.05  0.05 A A e e v v r r y y c c n n e e a a L L e e g g a a t t [ [ e e c c y y c c l l ] ] MH MH HS HS HS + MH HS + MH dynamic TDMA dynamic TDMA (f) dense4 (4-chip) Reversal Fig. 8. Average packet latency  0.8  0.8  0.85  0.85  0.9  0.9  0.95  0.95  1  1  1.05  1.05  1.1  1.1 Ave. Ave. DC DC LU LU FT FT CG CG SP SP BT BT Benchmark programs Benchmark programs MG MG IS IS E E u u c c e e x x i i t t n n o o i i t t m m e e ( ( o o n n m m a a i i l l a a z z i i t t n n o o ) ) MH MH HS HS (a) sparse4 (4-chip)  0.8  0.8  0.85  0.85  0.9  0.9  0.95  0.95  1  1  1.05  1.05  1.1  1.1 Ave. Ave. DC DC LU LU FT FT CG CG SP SP BT BT Benchmark programs Benchmark programs MG MG IS IS E E u u c c e e x x i i t t n n o o i i t t m m e e ( ( o o n n m m a a i i l l a a z z i i t t n n o o ) ) MH MH HS HS (b) dense4 (4-chip) Fig. 10. Application execution time                                                                 dynamic TDMA-based buses, although Headﬁrst sliding works on static TDMA-based buses. Thus, Headﬁrst sliding ﬁll in the gap between static and dynamic TDMA schemes, while keeping the hardware simplicity of the static scheme. As the result of the application performance, Headﬁrst sliding routing improves by 9.9%, compared by Minimum hop routing. We also conﬁrmed that the area and critical path delay overheads are modest. As a future work, we are planning to investigate routing schemes to improve the saturated throughput at a high workload. We are going to evaluate the thermal In addition, we will extend Headﬁrst sliding routing scheme to improve the latency within a range of low to middle workload. "
A deadlock-free routing algorithm requiring no virtual channel on 3D-NoCs with partial vertical connections.,"Elevator-first routing algorithm has been introduced for partially connected 3D network-on-chips, as a low-cost, distributed and deadlock-free routing algorithm using two virtual channels. This paper proposes Redelf, a modification of the elevator-first routing algorithm on a 3D mesh topology. The proposed algorithm requires no virtual channel to ensure deadlock-freedom."," A Deadlock-Free R Routing Algorithm Requirin ng No Virtual  Channel on 3D-N NoCs with Partial Vertical C Connections  Jinho Lee and Kiyoung Choi  School l of Electrical Engineering and Computer Science  Seoul National University  icarosj@dal.snu.ac.kr, kchoi@snu.ac.kr Abstract— Elevator-first routing algorithm h has been introduced  for partially connected 3D network-on-chip ps, as a low-cost,  distributed and deadlock-free routing algorith m using two virtual  channels. This paper proposes Redelf, a m modification of the  elevator-first routing algorithm on a 3D m mesh topology. The  proposed algorithm requires no virtual c channel to ensure  deadlock-freedom.  IndexTerms—NoC, deadlock, routing algorit thm, 3D stacking  I. INTRODUCTION  Among the efforts to deal with high c cost of TSVs, [1]  introduced elevator-first algorithm for an n architecture that  connected layers of 2D NoCs with arbitrary  partial connections  of TSV links. Proof was shown that, with any y network topology  and any deadlock-free 2D routing algorithm ms, a deadlock-free  algorithm could be built using two virtual ch annels. Later in [2],  its detailed architecture was described and it s cost analysis was  given.  In this work, we present a routing algorith hm named “Redelf,”  and show that when we focus on layered m mesh architecture, it  can be deadlock-free even without using vir rtual channel while  maintaining the original benefits of elev vator first routing  algorithm.   II. ORIGINAL ELEVATOR-FIRST AL LGORITHM  Because our algorithm is based on  the elevator-first  algorithm [1], knowledge about the algorit thm is essential to  understand ours. The algorithm has been n proposed as an  algorithm that connects several layers of 2D D networks that use  deadlock-free algorithms. The key point of th he algorithm is that  it is deadlock-free even though there are onl ly partial, arbitrary  vertical connections, while only two virtual c channels are needed.    In elevator-first routing, when a pack ket is destined to a  node located on another layer, it is first route ed to a vertical link  (an elevator) using the routing algorithm insi ide the 2D network  (e.g., XY routing). When it arrives at the elev vator, the packet is  sent upward or downward (towards the dest tination). When the  packet reaches the next layer, the destination n is checked. If the  destination is on the current layer, then i it is routed to the  destination using the routing algorithm of the e new layer. If it is  not, the same process is repeated until it reach hes the layer of the  destination node. The routing is deadlock-fre ee with two virtual  channels only inside the 2D networks (ver rtical links do not  require virtual channels).  III. PROPOSED ALGORITHM M   A. Target architecture   The elevator-first algorithm allows low w cost distributed  routing for 3D NoC where planar deadlock k-free networks are  Fig. 1. 3D mesh with random parti ial connections and its down  elevator selection.  partially connected by vertica al channels. In this work, we  decided to focus only on mesh h topology for planar networks,  which is probably the most pop pular one among wide variety of  NoC topologies [3][4][5]. Thu us, it would be meaningful to  focus on layers of meshes inst ead of arbitrary topologies. We  use dimension ordered (XY) ro outing as in examples of [1]. In  the remainder of this section, we e show that, by composing some  rules for choosing elevators (v vertical channels), elevator-first  algorithm can be made deadloc ck-free even without any virtual  channel.   B. Proposed routing algorithm f for 3D mesh with partial  irregular vertical links   Our algorithm uses the origi inal elevator-first algorithm (Elf)  as baseline. In Elf, the algorithm m can choose any elevator when  the destination and the source a are located in different z planes.  In our algorithm, certain rules ar re applied when an elevator is to  be taken. Before introducing t the rules, we first define some  special nodes called “pivots”.   Definition 1: An up (down) ) pivot elevator of a plane is the  up (down) elevator that has  no more upward (downward)  elevator to the south-or-due-east t of it in the plane.  By the definition, only one up  pivot and only one down pivot  exist for each layer. Using this d definition, the rules for selecting  elevators are as follows.  • Rule 1: When the destinat tion is on a different layer, take  the nearest elevator amon ng the south-or-due-east ones of  the source (including the se elf-node).  • Rule 2: If there is no so outh-or-due-east elevator of the  source in the desired direct tion, take the pivot elevator.               Redelf Redelfv2 Elf Cycles 1000 800 600 400 200 0 0 Latency  Flits/Cycle 14 12 10 8 6 4 2 0 Throughput  Redelf Redelfv2 Elf Energy/flit Redelf Redelfv2 Elf J/Flit 2E-09 1.5E-09 1E-09 5E-10 0 0.1 0.2 (a) Latency  0.3 0 0.1 0.2 0.3 0.4 0.5 (b) Aggregate Throughput  (c) Energy per flit  Fig. 2. Experimental result. X axis represents injection ratio (Flits/node/cycle).  0 0.2 0.4 0.6 • Rule 3:  Assume a down elevator is chosen by Rule 1 and  it is not at the self-node. Also assume that it is located to  the south or due east of the up pivot elevator. Then,  instead of taking that down elevator, take the down pivot  elevator on the same plane. Same rule applies when up  direction and down direction are switched.  The thick arrows in Fig. 1 show an example of elevator  selection. Normally, packets having node “S” as their source  would take node “A” as its down elevator by Rule 1. However,  because node “P” has an up pivot elevator and “A” is located to  the south of “P”, the down pivot elevator at node “B” is selected  instead by Rule 3. On the other hand, packets having “A” as  their source takes the down elevator at the self-node “A” by  Rule 2. This routing algorithm is deadlock-free without any  virtual channel. Detailed description and proofs are omitted due  to space reason.   IV. EVALUATION  A. Experimental setup  To examine performance of our routing algorithm, we used  an in-house, cycle-accurate NoC simulator integrated with  DSENT [7], an NoC power modeling  tool. 3D NoC  architectures consisting of four layers of 4x4 mesh with various  vertical connections (25%, 50%, and 75%) were used. The  routers are 3-stage pipelined and each port has 8-flit buffers  with buffer-bypass technique applied.   Our algorithm was compared with the original elevator-first  algorithm. However, because  the original elevator-first  algorithm uses two VCs (virtual channels) for the planar links,  direct comparison with our algorithm using no virtual channel  wouldn’t be fair. So we also tested our algorithm on the same  architecture with two planar VCs.  Four kinds of traffic patterns were used: uniform random,  hotspot random, bit complementary, and tornado. Under the  traffics, three metrics, average latency, aggregate throughput,  and energy consumption per flit were measured. The results are  shown in Fig. 2. For space reason, only the result under uniform  random traffic with 25% of vertical connections is shown. In  the legend, “Elf” means the baseline original elevator-first  routing. “Redelf” means the proposed algorithm with no virtual  channel and “Redelfv2” means the proposed algorithm using  two planar VCs as in the baseline elevator-first algorithm.   B. Performance comparison.   To make a quantitative comparison, we can define  “saturation point” as the point where the latency reaches over  500 cycles. On average1, saturation point of “Redelfv2” is 8.4%  higher than “Elf”, and that of “Redelf” is 4.5% lower than “Elf”.  In average peak throughput, “Redelfv2” is 13.7% higher than  “Elf” and “Redelf” is 6.0% lower than “Elf”. In average  saturated throughput, “Redelfv2” is over 31.2% higher than  “Elf” and “Redelf” is only 10.6% lower than “Elf”. In terms of  average energy per flit at the lowest point, “Redelf” consumes  27.9% less energy than “Elf” and “Redelfv2” consumes 8.0%  less than “Elf”. For saturated energy per flit, “Redelf” is 17.7%  lower than “Elf” and “Redelfv2” is 27.7% lower than “Elf”.  This implies that, when designing for low-power, “Redelf” has  advantage in energy efficiency when the network is expected to  work at relatively low load, and “Redelfv2” should be better  when there is high stress in the network.   V. CONCLUSION   In this paper, we have proposed a routing algorithm for 3D  NoC with partial vertical connections. Compared to elevatorfirst algorithm, the proposed algorithm achieves lower power  consumption, higher performance, or both.   Acknowledgements  This work was supported by the National Research Foundation of  Korea (NRF) grant funded by the Korea government (MEST) (No.  2012-0006272) and Ministry of Knowledge Economy (MKE) and  IDEC Platform center (IPC) at Hanyang University.   "
An NoC and cache hierarchy substrate to address effective virtualization and fault-tolerance.,"In future many-core chip systems, virtualization of chip resources will become mandatory in order to get the maximum chip utilization and provide the maximum possible service to demanding applications. Also, failures of the chip will need to be managed to keep high yields of chips manufacturing. In this paper we provide a novel substrate for the on-chip interconnect and for the memory coherence protocol. We take a radical approach when designing the network and memory, by effectively co-designing both. We take into account the visibility of the whole chip resources to the memory controller, which is in charge of providing the appropriate support for virtualization and memory-level fault-tolerance. Then, the network is designed taking into account the memory coherence protocol and providing solutions for the critical communication requirements of memory modules (caches) and processors in a virtualized domain. The coherence protocol is also designed in order to allow its effective use in a virtualized scenario. With our approach, the chip can be fully virtualized on application demand providing total partitioning of core resources and smart use of memory resources. Results demonstrate that our scheme effectively optimizes the utilization of chip resources, allowing the implementation of techniques which can outperform a first-touch policy up to a 6%, reducing LLC misses and enabling LLC fault tolerance.","An NoC and Cache Hierarchy Substrate to Address Effective Virtualization and Fault-Tolerance Mario Lodde, Jos ´e Flich Parallel Architectures Group Universitat Polit `ecnica de Val `encia email: mlodde@gap.upv.es, jﬂich@disca.upv.es Abstract—In future many-core chip systems, virtualization of chip resources will become mandatory in order to get the maximum chip utilization and provide the maximum possible service to demanding applications. Also, failures of the chip will need to be managed to keep high yields of chips manufacturing. In this paper we provide a novel substrate for the on-chip interconnect and for the memory coherence protocol. We take a radical approach when designing the network and memory, by effectively co-designing both. We take into account the visibility of the whole chip resources to the memory controller, which is in charge of providing the appropriate support for virtualization and memory-level fault-tolerance. Then, the network is designed taking into account the memory coherence protocol and providing solutions for the critical communication requirements of memory modules (caches) and processors in a virtualized domain. The coherence protocol is also designed in order to allow its effective use in a virtualized scenario. With our approach, the chip can be fully virtualized on application demand providing total partitioning of core resources and smart use of memory resources. Results demonstrate that our scheme effectively optimizes the utilization of chip resources, allowing the implementation of techniques which can outperform a ﬁrst-touch policy up to a 6%, reducing LLC misses and enabling LLC fault tolerance. I . IN TRODUC T ION Industry has turned its attention to chip multiprocessor systems (CMPs) [1] [2] to face the power consumption challenge while keeping performance increase constant. As technology advances, simpler cores (possibly in-order cores) provide a better performance/power ratio, making possible to build many-core systems where tens or hundreds of cores are included in the same chip. To reduce design and validation processes, the chip can be designed with a tile-based approach, replicating the same basic module (tile) in the chip area. Each tile usually includes a core, its private caches, a bank of last-level cache (LLC) and a switch to connect the tiles through a network on chip (NoC), typically a 2-D mesh. This organization is appealing since it reduces design time and provides a modular, easily scalable system. Designers and test teams can focus major efforts in the tile architecture and functionality. Figure 1 shows the tilebased design this paper assumes, in which a memory controller is added to access external memory. The cache hierarchy is kept coherent through a cache coherence protocol, typically directory-based, meaning that a data structure, the directory, is used to keep track of the cores which have a copy of the block in their private caches. This information is used by the LLC to locate the tiles to which it has to communicate when some coherence action must be performed. As far as the LLC is concerned, it can be private to the core which lies in the same tile or shared by all the tiles. In the latter case, each LLC bank is a slice of the whole shared and distributed LLC. Fig. 1. Tiled CMP system. Shared LLCs are usually preferred because, although this organization has higher access latencies, it provides higher cache capacity, thus avoiding expensive off chip accesses that are more frequent when private LLCs are used. The higher access latency is due to the mapping of memory blocks to the LLC banks. The address space is divided into subsets, each one mapped to one bank, which is called home bank for that subset of addresses. If a cache access misses in the private caches of a core, a request must be sent to the home bank. Since the home bank may lie in a different tile, the miss latency will be higher than the latency of private LLCs, where all requests are sent to the bank located in the same tile. In this paper we focus on CMP systems with shared LLC. The address mapping policy to the LLC banks is crucial and has an impact on performance which grows with the system size, due to the higher number of hops to reach a distant tile where a frequently accessed block may have been mapped. The simpler mapping policy assigns blocks to the LLC banks basing on the less signiﬁcant bits of the block address. This policy is simple to implement and, at least in theory, evenly distributes the blocks over the LLC banks1 . However, it does not minimize the hop count in the network as the requestor is not taken into account by the mapping policy. Various alternative mapping policies have been proposed to solve this issue, based on static or runtime optimizations made at compiler, OS or hardware level. First-touch [3], for instance, is an OS-based proposal in which the address mapping to the LLC banks is still static but the OS is in charge to load the pages in the main memory depending on the threads which access those data. The goal is to map the pages to addresses 1 This happens only if all the blocks are uniformly accessed by the cores, which is not the common case in real applications. a) Partitions. b) LLC partitioning following a static LLC bank mapping. c) LLC partitioning following a dynamic LLC bank mapping. Fig. 2. Virtualized CMP system to three applications. Resources are assigned to different applications. which are statically mapped to the L2 cache bank located in the tile where the thread is running. Other proposals follow a totally dynamic home mapping approach. Runtime Home Mapping (RHM) [4] is a hardwarelevel mapping policy in which the memory controller is in charge to map a block to an LLC bank as close as possible to the block requestor.. Using RHM, a block can be mapped to any LLC bank, and the home can change dynamically at runtime: if a block is replaced and requested again, the second time it can be mapped to a different bank. The use of alternative mapping policies is particularly appealing in a virtualized environment. In a virtualized system, resources are assigned to different applications providing the view of exclusiveness of resources usage. For instance, in Figure 2 a 16-tile CMP system is virtualized to three applications, each running in a disjoint set of cores. In such situation, it is mandatory to prevent each application from affecting the performance of the others. Indeed, we need a perfect partitioning not only of cores but also of memory and NoC resources. In such environment, a static home mapping approach would spread the blocks over all the LLC banks, thus destroying the exclusiveness in the use of cache and NoC resources. A dynamic home mapping approach instead can keep all the blocks in LLC banks located within the partition of the application. Figure 2.b shows the virtualized system when a static home mapping is used. As can be seen, the LLC layer is not partitioned and every core can request blocks allocated in any LLC bank. However, with a dynamic approach (Figure 2.c) both the core and the memory layers are partitioned and thus trafﬁc originated from different applications don’t mix. 2 In this paper we propose a co-design of the network and memory hierarchy to achieve an effective partitioning of chip resources, enabling an efﬁcient implementation of a virtualized system. In order to achieve such partitioning, we provide the following novelties: • A management policy where the memory controller is 2 The only exception is when L2 misses require accessing the memory controller, which, can be located in a corner of the chip thus needing to cross other partitions. aware of the applications running in the chip and their partitions. The memory controller will be in charge of performing the home LLC bank mapping taking into account the resources assigned to each application. • A routing framework, combined with the efﬁcient LBDR approach [5]. The framework will allow an efﬁcient partitioned-based search of the home banks when private caches accesses result in a miss. • An infrastructure to collect control messages, embedded in the NoC, that will be bounded to each partition, improving the latency of private misses. The collecting system will be integrated into the routing framework. • A method to allow applications with large memory footprints to get more LLC banks. In this case chip resources are not assigned at tile-level: an application may use LLC banks which are located in tiles where another application is running. • A strategy that allows applications to share some LLC banks if they share code or data. The ﬁnal solution is a redesign of recently proposed, efﬁcient methods in NoCs and in memory coherence protocols. In particular, the ﬁnal solution will complement the LBDR routing mechanism[5], the RHM mapping approach[4] and the gather network solutions, proposed in [6]. In addition, our proposal provides support to LLC banks failures. The memory controller will keep into account LLC banks which have been detected as failed. Those banks will be avoided when the mapping is performed. The whole mechanism is termed pNC, meaning it supports partitioning both at NoC level and at Cache hierarchy level. The obtained results show pNC is the only one that achieves perfect system partitioning, and efﬁcient memory failure coverage. Also, results highlight the vital need to partition resources in a virtualized scenario. The rest of the paper is organized as follows. In Section II, the NoC-memory coherence co-designed solution is shown, focusing on the NoC components and the memory coherence components. Section III shows how these methods are codesigned to support the virtualized scenario in combination of a redesign of the memory controller. Evaluation results are discussed in Section IV. Related work is discussed in Section V. Finally, the paper provides conclusions in Section VI. I I . NOC AND CACH E H I ERARCHY SUB STRATE pNC partitioning solution is based on the combination of two previous methods. The ﬁrst one, Logic-Based Distributed Routing (LBDR), is used at NoC level and allows the efﬁcient (simple implementation and low hardware requirements) codiﬁcation of most routing algorithms. The second one, the Runtime Home Mapping strategy, allows to map cache blocks to the LLC banks at run-time. RHM maps each cache block as close as possible to the requestor. LBDR and RHM are orthogonal techniques. We describe how they can be integrated to provide a NoC and cache substrate in which both cooperate with the hypervisor to effectively partition chip resources when virtualization is enabled. Next we summarize LBDR and RHM. For further details please read the reference papers. A. Logic Based Distributed Routing LBDR relies on the deﬁnition of a small set of conﬁguration bits at the switches. For a 2-D mesh, LBDR requires 3 bits per output port at each switch. With these bits, multiple routing algorithms can be encoded. A small logic block with seven logic gates per output port is needed. Figure 3.c shows the logic and required bits for the east (E ) port of a switch. The Ren (and Res ) bit encode the turn prohibition that may exist at the next switch for packets leaving the E port and taking at the next switch the north N (south S ) port. If the Ren (Res ) bit is zero, then packets are not allowed to turn north (south). In this way, routing restrictions (pair of links that can not be used by the same message in order to avoid network deadlocks) can be encoded in the routing bits. The Ce bit is used to indicate whether the E port can be used or not. It sets the boundary of the network to propagate messages. Fig. 3. LBDR logic and conﬁguration bits for east output port. Recently the mechanism has been extended to support nonminimal paths in presence of failures. Also, a basic broadcast mechanism has been proposed in [5]. We do not take into account the non-minimal path support but we use the broadcasting mechanism. In such broadcast, the connectivity bits limit the broadcast , constraining it into a partition. Later we will see how this mechanism is used for LLC home search. B. Runtime Home Mapping RHM is a cache hierarchy-level technique that enables the mapping of cache blocks to LLC banks at runtime, in order to place each cache block as close as possible to the requestor, exploiting at the same time the whole on chip cache capacity. Whenever an LLC miss occurs, a request is sent to the memory controller (MC) to retrieve the requested block from main memory. In RHM, the mapping of cache blocks to the home node is performed by the MC. A small data structure is included in the MC to keep information about the utilization of LLC banks. The mapping is performed depending on the block requestor and the LLC banks utilization. If there is available space in the requestor tile’s LLC bank, then the block is mapped on that bank, else neighboring banks are candidates and the block is placed in the nearest bank which has available space or the bank with the lowest utilization. Figure 4.a shows the MC table. It keeps record of the utilization of each set at each LLC bank (percentage of ways used in that set). Since a block can be mapped to any LLC bank, a search phase is needed. In case of miss in the private cache, a request is sent to the LLC bank of the same tile. Due to the mapping policy, the probability of ﬁnding the block in the same tile is high. If the request, however, misses in the tile’s bank, all other banks must be checked, so a request must be broadcasted and an answer from every bank is expected with an acknowledgement message (meaning that the request has been received) and the data block, in case of hit in a bank. Broadcast and acknowledgement phases are expensive in terms of network resources and performance impact. Thus, two NoC optimizations are supported in RHM. The ﬁrst one is an NoC-level broadcast implementation in which a broadcast is sent as a single message which replicates at switches to ﬂood the network. Broadcast messages avoid the routing restrictions encoded in Rxy bits, thus being compatible with LBDR. Figure 4.b shows how switch 5 broadcasts a message on the network. The message reaches all nodes in four hops. The second optimization is a Gather Control Network (GCN) to enable a fast transmission of ACK messages through a tiny circuit. From a logical point of view, the GCN is a set of AND trees, one for each tile. Figure 4.c shows the AND tree for the upper left tile of a 4 × 4 mesh. The tree has its root in the upper left node and its leaves in all the other nodes. When the upper left tile sends a broadcast request, it waits to receive an acknowledgement from each tile. When a tile receives the request, it triggers its output signal for the AND tree of the sender. Once all the tiles do the same, the AND tree output will be set, thus notifying the sender of all the acknowledgements. The GCN has been proved to be very effective in this purpose, enabling fast many-to-one communication and having a very low overhead in terms of hardware resources. In [7] two implementations are presented: a combinational one, which structure is similar to the logical AND tree described above, with low latency but compromised scalability, and a sequential one, which employs encoders and decoders at both ends of links to reduce the wiring requirements, thus having better scalability but higher latency. We use the latter in this work. With these two NoC-level optimizations, trafﬁc generated by RHM is considerably reduced, thus scaling to a higher number of cores. a) Occupation table at MC. b) Broadcast steps in LBDR. Fig. 4. RHM mechanism. c) Gather Control Network to collect ACKs. I I I . PNC : LBDR AND RHM SU P PORT TO V IRTUAL I ZAT ION Now we integrate LBDR and RHM to provide a NoC- and cache-level support to virtualization: pNC. The basic idea is to allow RHM to work at the partition-level, where partitions are deﬁned by LBDR. With the connectivity bits, different partitions at the NoC level can be deﬁned, some of them overlapped. This is achieved by using a vector of connectivity bits per output port. Packets injected by applications incorporate in their headers an ID ﬁeld that identiﬁes the partition. With 3 partition bits up to 8 overlapped partitions can be deﬁned. Notice that two disjoint partitions can use the same ID but routing packets at different partitions, so the combinations of partitions is much larger. For the sake of simpliﬁcation, we consider the 64-tile CMP shown in Figure 1, where tiles are organized in an 8 × 8 mesh. We assume the default partitioned regions outlined with thicker lines. Thus, the chip is logically divided in four partitions, each including 4 × 4 tiles. Notice that other partitions are possible and they can be reprogrammed on-the-ﬂy in the system by just changing the connectivity bits at the switches. Connectivity bits prevent trafﬁc from leaving the partitions. This is especially useful for broadcast messages, thus avoiding unnecessary ﬂooding of other partitions. However, the GCN infrastructure, such as it is presented in [6], cannot work properly if broadcasts are to be limited within a partition. Consider the example where the upper left tile (Tile 0)3 sends a broadcast message due to an L2 bank miss. Tiles in the same column will collect their ACKs, which in turn will be collected in the ﬁrst row. Tile 0 is notiﬁed when all the tiles trigger their signals. However, if broadcasts are limited within a partition due to LBDR connectivity bits, the tiles located outside the partition will not receive the broadcast and thus will not trigger the GCN signal for Tile 0. The GCN modules of the tiles at the partition boundary will indeﬁnitely wait for signals coming from outside the partition. LBDR connectivity bits can be used to solve this problem, as they deﬁne the boundaries of the partition. We need to link the connectivity bits approach with the GCN infrastructure. Figure 5.a shows how the GCN module can use the connectivity bits to this purpose. The complemented value of the 3 Tiles are numbered from 0 to 63, starting from the upper left corner and descending row by row to the lower right corner of the CMP. a) pNC switch design b) Example of GCN connected with LBDR bits Fig. 5. GCN-LBDR mechanism. connectivity bit is ORed with the input GCN signal at the partition boundary. Thus, that signal will be masked. Figure 5.b shows a detailed example, where the partition boundary, represented by the CS and CE bits set to zero, set all the input signals to one for every GCN destination, so no notiﬁcations is expected from outside the partition. Notice that this solution adds a delay of a single logic gate. Partitions deﬁned by LBDR can be overlapped, thus allowing a tile to belong to different partitions. This is another useful feature with partitioning/virtualization for various reasons. First, two or more applications in different partitions can share several LLC banks (we will not use this possibility in this work). Second, it allows to deﬁne a global partition, including all the tiles, so that all of them can reach the memory controller (which is a shared resource). Third, a partition can steal LLC banks from underutilized partitions (Home Stealing, HS). For instance, an application which is executed in a 16-core partition may have a working set which does not ﬁt in the LLC capacity offered by the partition. The opposite may also happen. The LLC banks may be underutilized by the running application in that partition, which is then using resources that are actually not needed and possibly could be useful to an application which is executed in a neighboring partition. In our ﬁnal design, three connectivity bits per port are used. The ﬁrst one deﬁnes the partitions to which applications are mapped to. We refer this to as Processor Partition (PP). Unless stated, we assume four different non-overlapping PPs, each with 16 tiles4 . The second bit must allow each node to reach a shared memory controller. If a single memory controller is used, then the second bit must deﬁne a region as large as the whole chip.5 A third bit is used to deﬁne Home Partitions (HP), thus enabling LLC partitions to differ from the shapes and sizes of PPs, enabling Home Stealing. HPs may increase or reduce the default LLC capacity deﬁned by the partition. Also, home stealing can be used if two applications share parts of their virtual address space. HPs of the two applications can be conﬁgured so that they partially overlap, and shared blocks can be mapped in the banks that belong to both partitions, at the boundaries. Figure 6 shows an example with four partitions with different shapes. PP0 uses HS taking some LLC banks from PP2. In addition, PP2 and PP3 share 4 LLC banks by overlapping their HPs (HP2 and HP3). In Summary, PPs set the cores on which an application is running, while HPs deﬁne the LLC banks which can be used by those cores. The MC must be aware which tiles belong to the PP and HP of an application to perform home mapping. Fig. 6. Processor Partitions and Home Partitions example. It is worth to note that the conﬁguration of LBDR bits to create partitions, and the deﬁnition of PP and HP partitions has to be taken by a higher entity layer, the hypervisor. Based on current application demands, the hypervisor, running in the OS, conﬁgures the chip appropriately. These dynamics are not explored in this paper and are left for future work. 4 If overlapped PPs were used, then more connectivity bits were required. 5Notice that with several MCs, the partitioning support will need deﬁning disjoint MC partitions. However, we assume just one MC. A. Memory Controller Design To perform home mapping, the MC must be aware of the partitioning of the system, both for processors and home banks. Two small tables containing this information are initiated by the OS hypervisor, one table deﬁning PPs and another deﬁning HPs. Each table has as many entries as the number of tiles in the system, each entry containing one bit per possible partition ID (four in our case). For our 8 × 8 system, with up to 4 partitions, each table has a size of 256 bits to store the partitioning information. When a request is received at the MC, both tables are checked to compute the home bank. Regardless the sizes of PP and HP partitions, the home bank is selected among those belonging to the HP. Blocks are mapped to the bank in the requestor’s tile until it is full. Then, neighboring banks are checked within the HP set. Optimizations can be derived by using different strategies on shared HP partitions. We consider only one memory controller. However, in CMP conﬁgurations with different MCs, partitions of the chip need to be partitioned between the different MCs. In all the cases, a general global partition needs to be deﬁned in order to allow the full utilization of the chip by a single application. In case of multiple MCs, the tiles belonging to a given partition must forward LLC misses to the proper MC. This can be achieved by a small register associated to each LLC bank. B. Fault Tolerance LBDR provides fault-tolerance at NoC level. Orthogonal to LBDR’s NoC-level fault-tolerance, pNC provides faulttolerance at LLC-level, allowing the CMP to still work even when some LLC banks are faulty. This is achieved by an extra bit used in the MC for each tile. When this bit is set then the LLC bank is set as faulty and avoided when mapping blocks. Notice that any combination of faults is supported. Fault-tolerance support can not be achieved if static mapping is used, neither with ﬁrst-touch nor with the SNUMA approach. Both techniques rely on static block mappings of address to home banks, thus in the presence of a faulty bank, all the blocks that should be mapped to that bank can not be accessed. The ﬂexibility of pNC evicts faulty LLC banks, distributing the blocks in neighboring banks. C. Mapping Algorithm The mapping algorithm, including the access to the partition tables and fault-tolerance support, is executed in parallel with the off-chip access, so its latency is hidden. function S EL EC T T I LE(int T) if (HP[Tr ] == PP[T]) && !Faulty[T] && Free[T, set] for i = 0 → N − 1 do distance = 2 × N selected = -1 if HP[i]==PP[T] && distance ≤ dist(T, i) && Free[i, set] then distance = dist(T, i) then return T else selected = i end if end for end if return selected end function IV. EVALUAT ION To evaluate pNC in a virtualized environment we used gNoCsim, a cycle-accurate trace-driven network and cache simulator platform. We fed gNoCsim with a set of memory access traces obtained running applications from SPLASH2 [8] and PARSEC [9] benchmark suites in Graphite[10] simulator. gNoCsim can be embedded in Graphite but unfortunately Graphite is not able to run multiprogrammed workloads, thus the choice to use memory access traces. Synchronization events between threads (barriers) have been included in the traces. We checked single application performance and they deviated less than 10% in accuracy when represented in traces. We implemented a MESI directory-based coherence protocol and three block mapping policies. First, a static mapping policy (STATIC), where the home is computed basing on the less signiﬁcant bits of the block address. Second, a ﬁrst-touch policy (FT), in which memory pages are loaded depending on the ﬁrst accessed block of a page which is requested to main memory. The page is mapped to the same tile of the L1 cache which issued the request. Third, the pNC policy, in which a block is mapped to any LLC bank bank within the HP partition of the application requesting the block (previous algorithm). We assume an 8 × 8 CMP system divided in 4 × 4 regions. Network and cache parameters are shown in Table I. The MC is connected to Tile 0. As we use one MC, one region deﬁned by the connectivity bits must cover the whole chip, thus allowing all the tiles to reach the memory controller. Routing Flow control Flit size Switch model Switching Buffer size: Virtual channels: GCN hop delay LBDR credits 8 byte 4-stage pipelined virtual cut-through 9 ﬂit deep 4 1 cycles TABLE I L1 cache size L1 tag latency L1 data latency L2 bank size L2 tag latency L2 data latency Cache block size Memory controllers 32 kB 1 cycle 2 cycles 256 kB 1 cycle 4 cycles 64 B 1 N ETWORK PARAM ET ER S switch. The GCN also requires 6 additional wires per port per direction in our 8 × 8 system. B. Performance in Fault-Free Systems We analyze ﬁrst how the three mapping policies behave when the same application is running in the four regions. Notice that the four regions do not share any code or data. Figure 7(a) shows the execution time when the thee mapping policies are used, normalized to the RHM case. First touch and RHM have similar performance results, while static mapping performs much worse. When L2 cache banks are not excessively stressed, the performance of static mapping is just slightly degraded (FFT, LU, LUNC and CANNEAL) compared to RHM and First-touch. However, as soon as many frequently accessed blocks are mapped to a single bank exceeding its capacity, performance of static mapping quickly degrades and execution time can rise up to 50-60X (BARNES and WATER-NSQ, not shown as the scale is limited to 8). Figure 7.b shows the number of LLC misses for the previous applications, normalized to the RHM case. Again, RHM and First-touch have similar behavior while the number of misses with static mapping is generally higher and can rise up to 700 times (e.g. BARNES), as some LLC banks are highly demanded by the four applications (many frequently used blocks are mapped to the same bank, causing the replacement of blocks which are actually still being used). Also, the average miss latency (not shown) is 81% higher than in the pNC case. LLC banks are further away from requestors. Let us see now what happens with an heterogeneous workload. Since different applications have different needs in terms of resources, we can now enable the home stealing property of pNC (pNC-steal). With home stealing enabled, some cache banks of a region can be used to increase the L2 cache capacity of another region with higher cache space demands. Figure 8 shows the conﬁguration we assume for home stealing. A. pNC overhead Compared to the basic CMP design with static mapping, pNC has an overhead due to the tables at the memory controller and to the network optimizations. The MC has three tables: the ﬁrst two are PP and HP tables, each with 64 four-bits entries (512 bits in total); the third table records the utilization stats of each LLC bank. This table is used by RHM, and contains a counter for each set of each LLC bank. For a 16 × 16 tile system with 16-way 256KB LLC banks, the minimum memory requirements for this table is 2KB. For fault-tolerance, a 64-bit register is used (one bit per tile). At network-level, in each switch the GCN is used. According to [7], this logic introduces a 2.3% area overhead at each Fig. 8. Cache stealing conﬁguration. We use the three application scenarios shown in Table II. Regions are numbered from 0 to 3 starting from the upper left and descending to the lower right. The sets are built mixing an application which has high cache capacity demand, which we map it in P0, and three applications with lower demands. Figure 9 shows execution time when each set of applications is executed using STATIC, FIRST, pNC, and pNC-steal. The Figure shows the execution time of the application with the higher execution time (Ocean for Set 1, Ocean non-cont. for Set 2, Canneal for Set 3). This time STATIC has much lower performance degradation. The heterogeneity in applications leads to a better balance of L2 bank sets utilization (indeed, (a) normalized execution time (b) normalized L2 misses Fig. 7. Performance and L2 mises (same application in all the regions). with the third set, it achieves even better performance than the rest). FIRST and pNC have similar average performance, one performing slightly better that the other for different sets. Cache stealing, however, contributes to reduce execution time by 6% in Set 2 and 7% in Set 3, while it has a negligible impact on Set 1. Set 1 Set 2 Set 3 P0 Ocean Ocean non-cont. Canneal P2 LU non-cont. Barnes Barnes P1 FFT LU LU TABLE II P3 Cholesky Water-nsq Blackscholes S ET S O F A P PL ICAT ION S EX ECU T ED IN TH E CMP. To take a deeper look to each set, Figure 10 shows the execution time for each application of Sets 1, 2 and 3 respectively. Notice that the execution time in Figure 9 corresponds to the ﬁrst set of bars in Figure 10 (application mapped in partition P0 takes always more time to execute as it is larger). Looking at each single application, we can again notice the performance degradation of STATIC, while performance of FIRST and pNC are similar, with pNC performing slightly better except for CANNEAL, FFT and LUNC. The aim of cache stealing is to expand the LLC capacity of the application running in partition 0 without affecting the performance of the applications running on other partitions, especially on P1 and P2, where LLC capacity is reduced by a quarter. This is achieved both for Set 2 and Set 3, while in Set 1 the FFT mapped into P1 suffers the reduced LLC capacity. In this case, the hypervisor have chosen a wrong partitioning for the applications and a wrong home stealing policy. Figure 9 shows also the number of L2 misses for the three sets of applications, normalized to the pNC case. pNC is effective in reducing the number of L2 misses, and Cache Stealing allows a further reduction: on average, the number of misses with Static Mapping is 58% higher than with pNC, and 34% higher with First-Touch, while pNC-stealing reduces the number of misses by a 17% compared to normal pNC. C. Performance in Faulty Systems Finally, we evaluate the performance of pNC when some LLC banks are faulty. We run the sets of applications shown in Table II on a CMP system with 10% randomly-selected faulty LLC banks. Figure 9(b) shows the execution time for the three sets of applications, normalized to the case of a fault-free CMP. The performance degradation ranges from a maximum of 15% (Set 1) to a minimum of 1.5% (Set 3), with an average of 7%. Figure 9(b) shows also the normalized number of misses. When faults are present, the number of misses increases from a minimum of 2,2% (Set 3) to a maximum of 7,8% (Set 1), with an average of 5,8%. pNC performs home mapping even with some faulty banksy, and distributes the load between banks minimizing the impact of faults. Notice that for STATIC and FIRST, these cases are not supported. V. R ELAT ED WORK Virtualization and partitioning becomes an effective way to optimize resource utilization in CMPs. A NUCA [11] organization avoids the high latency of a single and large LLC, but introduces a variable interconnect delay, depending on the location of the accessed LLC bank. Virtualization introduces new challenges when a NUCA LLC is used, mainly due to the coherence protocol and to the block mapping performed. OSbased techniques like First-Touch [3] are effective in reducing the LLC access latency, although they are built on top of a static hardware mapping. pNC substrate relieves the OS and provides a more ﬂexible substrate providing fault tolerance. Several proposals at hardware level reduce the LLC latency in a NUCA organization. D-NUCA[11] migrates data blocks within a bank set. Although is effective in a single-processor system, its performance improvements are limited in CMPs. CMP-NuRapid [12] decouples data and tag of a cache line allowing a ﬂexible block mapping. However, it relies on a bus, which is not suitable for a many-core system. Other works proposed data migration and replication in a NUCA[13][14]. Marty et al. [15] propose a two-level directory-based coherence protocol to address virtualization, with one level acting inside the region allocated to a Virtual Machine and a second level dedicated to manage the coherence between VMs. Inside each region block mapping is static, based on a table conﬁgured for each VM. This solution, based on static mapping, does not minimize the block access latency as pNC does. It also relies on a second-level directory at DRAM for inter-VMs coherence and assigns resources at tile level, while our scheme decouples LLC banks and cores, allowing to deﬁne HP partitions different from PP partitions. Other proposals partition LLC cache into regions [16] [17], but they assume a single LLC bank shared by all the processors. Pods [18] is an alternative to tiled CMPs. Each pod includes a set of cores connected to a small LLC through a low-latency (a) with no LLC bank failures (b) with LLC bank failures Fig. 9. Normalized execution time and L2 misses (mixed applications). (a) Set 1 (b) Set 2 (c) Set 3 Fig. 10. Normalized execution time for each application of the three sets. interconnect with the aim to improve throughput and avoid the high latency of LLCs due to the interconnect. Notice that there are many works related to coherence protocols and miss latency reduction, however, none of them focus on partitioning support and virtualization, which is the topic of pNC. V I . CONC LU S ION Virtualization is an efﬁcient way to exploit CMP resources. Common design choices used in tiled CMPs however do not guarantee the isolation of virtualized partitions, leading to performance degradation due to a non-optimal use of chip resources. A smart NoC and coherence protocol co-design enables the implementation of pNC, in which LBDR and RHM are combined to provide a ﬂexible substrate which allows to independently allocate processors and LLC banks to the applications running on different VMs. Performance improves up to 60x if compared to a baseline system using static mapping, and up to a 6% if compared to First-Touch. Furthermore, pNC enables LLC-level fault tolerance. V I I . AGRE EM EN T S This work has been supported by the VIRTICAL project (grant agreement n 288574) which is funded by the European Commission within the Research Programme FP7. "
Dynamic traffic distribution among hierarchy levels in hierarchical Networks-on-Chip (NoCs).,"As the number of modules grows, performance scalability of planar topology Networks-on-Chip (NoCs) becomes limited due to the increasing hop-distances. The growing hop-distance affects both end-to-end network latency and overall network saturation. Hierarchical topologies provide better traffic hop distance and therefore are more adequate for large systems. However, the introduction of hierarchical NoCs offers new challenges. In particular, how to distribute the traffic among the hierarchy levels to effectively utilize the hierarchical structure. In this paper we propose a dynamic traffic distribution scheme that adapts traffic distribution among the hierarchy levels to the changing traffic conditions. We evaluate our scheme with packet-accurate simulations and show that it enables to realize the potential of hierarchical NoCs in latency reduction under both light and heavy traffic loads.","  Dynamic Traffic Distribution among Hierarchy  Levels in Hierarchical Networks-on-Chip (NoCs)  Ran Manevich1  Israel Cidon2  Avinoam Kolodny2  Electrical Engineering Department, Technion – Israel Institute of Technology, Haifa, Israel.  ranman@tx.technion.ac.il1,  {cidon, kolodny}@ee.technion.ac.il2  Abstract— As the number of modules grows, performance  scalability of planar topology Networks-on-Chip (NoCs) becomes  limited due to the increasing hop-distances. The growing hopdistance affects both end-to-end network latency and overall  network saturation. Hierarchical topologies provide better traffic  hop distance and therefore are more adequate for large systems.  However, the introduction of hierarchical NoCs offers new  challenges. In particular, how to distribute the traffic among the  hierarchy levels to effectively utilize the hierarchical structure.       In this paper we propose a dynamic traffic distribution scheme  that adapts traffic distribution among the hierarchy levels to the  changing traffic conditions. We evaluate our scheme with packetaccurate simulations and show that it enables to realize the  potential of hierarchical NoCs in latency reduction under both  light and heavy traffic loads.         Keywords—Hierarchical networks on chip; Adaptive routing;  I.   INTRODUCTION   Thousand-module Systems-on-Chip (SoCs) and ChipMulti-Processors (CMPs) are expected to emerge in the near  future [20]. Extending the usage of planar Network-on-Chip  (NoC) topologies in such systems is likely to suffer from  excessive end-to-end hop-counts [1], which in turn introduce  high latency. Hierarchical NoCs can dramatically reduce hopdistances and therefore are preferred in large SoCs and CMPs  [9-19]. Hierarchical NoCs are comprised of two or more planar  hierarchy levels. An example of a hierarchical NoC with 4x4  bottom 2D mesh and two upper hierarchy levels of 2x2 and  1x1, is presented in Figure 1. The average hop-distances in  hierarchical NoCs are smaller because long distance (global)  packets are routed over the higher hierarchy levels that span  longer physical distances per a single hop. However,  hierarchical NoCs are usually designed under traffic locality  assumptions where most of the packets are exchanged between  adjacent modules [9-14, 17-19]. Under such assumptions, the  upper hierarchy levels include less network resources and are  designed to serve limited amounts of traffic. Consequently,  without special precautions, excessive volume of global traffic  might form a bottleneck at the top levels and saturate the  network due to wormhole back-pressure.   In clustered hierarchical NoCs [9-14], modules at different  clusters are connected only through the upper hierarchy levels.  In non-clustered hierarchical NoCs [15-19],  the upper  hierarchy levels are added to a fully connected bottom network  and provide alternative, shorter paths between distant nodes.   Routing policy in non-clustered hierarchical systems defines  Figure 1. Example of a hierarchical 2D mesh NoC [19] with 4x4 bottom mesh  and 2 upper levels (2x2, 1x1). Shortest path between top left and bottom right  corners in 4x4 2D Mesh: 6 hops; in the presented hierarchical NoC: 4 hops.   the distribution of traffic among the hierarchy levels. In Static  Traffic Distribution (STrD), packets of each flow (defined by  source-destination pair) are always routed through the same  hierarchy levels and pass between levels at the same locations.  Hierarchical NoCs that employ STrD (e.g. [17] and [19])  usually excel in either light load latency or latency under heavy  loads, but not both. Different traffic distribution policies are  required to optimize performance for these two opposite  scenarios. For optimal performance at light load, the policy  should direct upwards any packet that can take a shortcut  through the higher levels. For optimal performance under  heavy load, traffic distribution policy should yield balanced  utilization of all the hierarchy levels. Under this policy, a much  smaller fraction of global packets is routed over the upper  levels of the hierarchical structure.   In this paper we introduce the Dynamic Traffic Distribution  (DTrD) scheme for hierarchical NoCs. Our scheme adaptively  modifies  the  traffic distribution among hierarchy  levels  according to the varying traffic conditions. With DTrD, under  light load, most of the packets are routed over the shortest  possible paths. Under heavy traffic load, traffic load is  balanced among the levels to prevent bottlenecks at the top of  the hierarchy. Utilizing DTrD enables to realize the potential  performance of a given hierarchical NoC in latency reduction  under both light and heavy traffic loads.   The paper  is organized as follows: Our research  methodology is presented in section 2. Section 3 presents an  analysis of the potential benefits of employing DTrD in various  hierarchical NoCs. Section 4 describes the architectural  concepts and the implementation of DTrD in detail. Packet  accurate simulations of hierarchical NoCs employing STrD and  DTrD are presented in Section 5. Finally, sections 6 and 7  present the related work and conclude the paper.                                            II. METHODOLOGY  In this section we describe the NoC traffic model and  baseline hierarchical architecture used throughout the paper.   A. NoC Traffic Model  The bandwidth version of Rent's rule [1] relates the  communication bandwidth (B) between a cluster of modules  and the rest of the system with the number of modules in the  cluster (G) (Eq. 1, k – average bandwidth of a single module, R  – Rent's exponent):  B kG   = R       (1)  Heirman et al. [2] showed experimentally that traffic  patterns of popular CMP benchmarks follow the bandwidth  version of Rent's rule with Rent's exponent of ~0.7 on average.  Based on the observations in [2], we use synthetic traffic  patterns that follow the bandwidth version of Rent's rule (a.k.a  Rentian traffic). Synthetic patterns are formed similarly to [19].  Rentian traffic patterns are useful because they provide the  means to model various degrees of traffic locality simply by  modifying Rent's exponent R. Low exponents (<0.6) represent  ""localized"" traffic patterns where the vast majority of packets  are exchanged among nearest neighbors. High exponents  (>0.8) represent lower locality patterns where many packets  traverse significant distances. Although the average Rent's  exponent observed in [2] was ~0.7, [2] showed that traffic  locality in CMP benchmarks is ""time-varying"" and that Rent's  exponent R changes at different application phases. We use  Rentian traffic patterns with Rent's exponent values of 0.6, 0.7  and 0.8 to evaluate the performance of DTrD and static traffic  distribution policies at the different phases. Packets hopdistance histogram of Rentian traffic patterns (with R=0.6, 0.7,  and 0.8) running on 32x32 2D-mesh are shown in Figure 2.a.  Figure 2.b presents average hop-distances of the same patterns  in 16x16 and 32x32 systems.               B. PyraMesh – The Baseline Hierarchical Architecture  We use PyraMesh [19] as a representative hierarchical NoC  topology.  PyraMesh is a family of 2D mesh NoCs that are  stacked in multiple levels, as illustrated in Figure 3. The first  level is a standard mesh with a single hop connection for every  pair of nearest neighbors. The higher levels contain meshes  with longer links and fewer routers, forming a pyramid-like  hierarchy. The structure  is described by  the following  architectural parameters:   K    - Size of the bottom mesh (i.e. K describes KxK mesh).  NL - Number of levels, including the bottom mesh.   αi - Ratio between dimensions (in nodes) of levels i and i+1.  Ci - Concentration of level i, i.e. how many routers in level i  are connected to a router in level i+1 along a single dimension.  Routing in PyraMesh is composed of three phases. In the first  phase, packets are routed towards the highest hierarchy level  that they are supposed to reach. In the second phase, they travel  to the switch at that level which has the shortest connection to  the destination module. Finally, at the third phase, packets  descend across the hierarchy levels towards the destination.  XY routing is utilized at each of the levels separately. The  phases are illustrated in Figure 3.a. We use the notation δ for  the mapping between flows (defined by source-destination  pairs) and the highest hierarchy level they reach. We term δ as  Figure 2. (a) – Packet hop-distance histograms in 32x32 2D Mesh for Rentian  traffic with Rent's exponents R of 0.6, 0.7 and 0.8. While only 1.3% of the  packets travel more than 26 hops for R=0.6, as much as 4.8% traverse such  distances for R=0.8. (b) The relation between average hop distance and Rent's  exponent in 16x16 and 32x32 2D mesh NoCs.   Traffic Distribution Mapping. In PyraMesh, flows are  classified according to the Manhattan distance between the  source and the destination at the bottom mesh. Each hierarchy  level i has a corresponding Distance Threshold (termed DThi)  that indicates the longest Manhattan distance of packets that  reach this level. Accordingly, traffic distribution mapping δ is  defined with DThi as follows (PDistance stands for packet's  Manhattan travel distance at the bottom mesh):  P Distance ≤ DTh 1 DTh 1 < P Distance ≤ DTh 2 (2)  1 2 M δ    =     NL DTh NL − 1 M < P Distance Examples of two PyraMeshes with K = 8 (i.e. 8x8 bottom level  1 mesh) are presented in Figure 3.    Hierarchical topologies were found in [19] to be more costeffective than planar 2D mesh starting at system size of 16x16.  [19] allows optimization of topology and traffic distribution  configuration that maximizes different design goals. In this  work, we evaluate DTrD using 16x16 and 32x32 hop-distance  optimized   PyraMesh   configurations  (i.e.  configurations that  minimize  the average hop-distance of packets). Traffic  distribution parameters and the levels structure of these  PyraMeshes are shown in Table I and Figure 4. Note that the  DThi values (i.e. the mapping δ) of these systems ensure that  almost any packet that can be routed over a shorter path in hops  at the higher hierarchy levels (as compared with its hopdistance at the first level) is directed upwards in the hierarchy.  While this routing policy provides the lowest possible average  latency under light traffic loads, a bottleneck might be formed  at   the  upper   levels  as  the   load  increases.  This  is likely to             w e i v r e p p U w e i v e i d S (a)  (b)  Figure 3. Two Examples of PyraMesh, upper view (top) and side view  (bottom). (a) – 3-levels PyraMesh with [K = 8, NP = 1, NL = 3, αi = 2, Ci =  1] and an illustration of a routing path between a source at the upper left  corner and a destination at the bottom right. The path composed of 3 routing  phases, phase 1 – the way up until the highest level; phase 2 – the way at the  highest level; phase 3 – the path down from the highest level to the  destination. (b) – 2-levels PyraMesh with [K = 8, NP = 1, NL = 2, α = 4, C  = 2]. The upper view figures are taken from [19].  happen since the upper levels are much sparser than the bottom  mesh and can serve smaller traffic volumes. Other systems in  [19] were optimized for performance under heavy loads. In  these systems a much lower fraction of packets is directed over  the upper hierarchy levels such that the load is roughly  balanced along the hierarchical structure, but average hopdistance is much higher.    TABLE I.   LATENCY OPTIMIZED PYRAMESHES [19]   System Size  Architecture Parameters  16x16  32x32  NL=3, αi=[4,4], Ci=[2,4], DThi=[5,8]  NL=4, αi=[4,4,2], Ci=[2,4,2], DThi=[4,10,50]  Figure 4. (a) – 16x16, 3-levels (NL=3) PyraMesh with 2 upper levels of 4x4  and 1x1. (b) 32x32, 4-levels PyraMesh with 3 upper levels of 8x8, 2x2 and 1x1.  The sizes of the upper levles are deduced form the values of the parameter αi.  For instance, in (b) 32/ α1  = 32/4 = 8.  III.       THE CASE FOR DYNAMIC TRAFFIC DISTRIBUTION  AMONG HIERARCHY LEVELS  DTrD leverages the ability to alter traffic distribution  among the hierarchy levels dynamically during the operation of  the system and adapts it to the varying traffic conditions.  In  this section we analyze the potential benefits of employing  dynamic traffic distribution among hierarchy levels in the  systems depicted in Table I and Figure 4. The following  notation is used henceforth:  δHop-Distance – Traffic distribution mapping δ that minimizes              packet hop-distance.   δLoad-Balance – Traffic distribution mapping δ that balances              the traffic load among hierarchy levels.  We use the average packet hop-distance as a metric for the  performance at light load. Performance under heavy load is  predicted using the average router bandwidth metric RBW (i.e.  number of flits that pass through the router per cycle) [19].  RBW at level i is defined with the following notation:   iI  - Average injection rate in packets/cycle/router into level i.  LP  - Average packet length in flits.  iD  - Average Manhattan packet distance in level i.  iK  - Mesh size of level i.  RBW i = I P D L + i 1 ⋅ i ) ( ⋅ 2 K i (3)  We set the average injection rate at the bottom level to 1 ( 1I =  1) since we are interested to compare between different system  configurations at identical injection rates;  1iI > are accordingly  obtained based on the topology and traffic model of each  configuration. We use the bottom mesh, without the additional  levels, as a reference and define Congestion Immunity as:  RBW Level Mesh 1 Congestion Immunity = (4)  max( RBW i )  Congestion Immunity > 1 implies that the hierarchical scheme  has higher injection saturation rate than the bottom level mesh.  RBWi is directly related to the traffic distribution among the  levels. If traffic is distributed according to δLoad-Balance, RBWi is  expected to have values that are roughly homogeneous. In the  case of δHop-Distance, RBWi sharply increase up the hierarchy  since much more traffic is directed toward the sparse upper  hierarchy levels.       In PyraMesh, traffic distribution mapping is defined by traffic  distribution values DThi (Eq. 2). DThi,Hop-Distance and DThi,LoadBalance are respective threshold  sets that define δHop-Distance and  δLoad-Balance. We use the DThi,Hop-Distance values that minimize  average hop-distance (Table I). As distribution according to  δHop-Distance tend to direct packets over the shortest paths, the  calculation of DThi,Hop-Distance is based only on the network  topology and does not have to take the traffic characteristics  into account. In contrast, the calculation of DThi,Load-Balance  requires having prior knowledge or making assumptions  regarding the degree of traffic locality (i.e. Rent's exponent R).  δLoad-Balance is supposed to equalize RBWi  (Eq. 3) across the  levels to avoid bottlenecks. RBWi depends on the average   packet hop-distance  at  level i, which  depends  on the degree  of traffic locality. To achieve load balance among hierarchy  levels in the average case, we used Rentian traffic with R = 0.7  and found DThi,Load-Balance utilizing simulated annealing. Both  DThi,Load-Balance  and DThi,Load-Balance in 16x16 and 32x32 systems  are presented in Table II. We measured average hop-distance  and Congestion Immunity for the systems of Table II with  Rentian traffic and R = 0.6, 0.7 and 0.8. The results,  normalized to average hop-distance of a same size flat 2D  mesh are presented in Figure 5.   While traffic distribution according to δHop-Distance yields the  minimum average packet hop-distance,  the congestion  immunity  under this distribution  policy is  significantly  lower                                           composed of three functional modules: feedback, control, and  routing. The purpose of the feedback module is to monitor the  traffic load. The control module adjusts the mapping δ to  optimize traffic distribution among hierarchy levels to the  measured load. Finally, the routing module determines the  packets paths such that traffic is distributed according to δ.  A. Network Load Monitoring (Feedback)  Average end-to-end latency and traffic load are directly  correlated. Although increased average packet latency is the  most credible indication for high load, its real-time monitoring  is complex. In our scheme, traffic load is measured by average  buffers occupancy, that is much simpler to sample. If too many  packets are directed towards the upper levels under heavy load  or vice versa (i.e too few packets under light load) there is a  mismatch between the active traffic distribution mapping δ and  the traffic load in the system. In the first case, the network  might saturate because of congestion at the upper levels. In the  second case, the potential light load latency reduction in not  achieved due to underutilization of the hierarchical structure.  Both scenarios can be identified by monitoring of buffer  occupancy at the upper hierarchy levels. In order to ensure  reliable and fast traffic distribution control, it is desirable to  decouple the feedback mechanism from the network itself [3].  Each of the monitored routers contains logic that produces Nbits wide measurement of  the  real-time  input-buffers  occupancy ratio. The output of this logic is connected by N-bits  point-to-point links to the centralized feedback module. The  output of the feedback module yields the maximum among the  average buffers occupancies of each level:  Feedbakc OUT = max Average Average ( (       Buffers Occupancy M Buffers Occupancy ) ) Level 2 Level NL        (5) This definition of feedback enables to have an indication if one  of the levels becomes congested due to improper distribution of  traffic. Similarly, it can imply that the upper levels are  underutilized and can serve more traffic. The architecture of  the feedback module and it's placement in the entire DTrD  system are presented in Figures 6 and 8 respectively.    There is a tradeoff between the number of routers that are  sampled at the upper levels and accuracy of real-time load  measurements. Connecting all the routers of the upper levels to  the  feedback module would yield  the most accurate  measurements but demands appropriate resources.   Sparse feedback sampling might be incredible if injection  rate changes are not homogeneous among all communicating  source-destination pairs. Although designers should be aware  of this tradeoff, in this work we connect all the routers of the  upper levels to the feedback module since we find the hardware  costs of feedback aggregation from all the routers above the  bottom mesh as tolerable. The upper levels in hierarchical  NoCs are sparse (Figure 4); therefore feedback should be  connected only to 6.2% and 6.3% of the routers in 16x16 and  32x32 systems, respectively. The feedback module itself is a  simple combinatorial  logic block. To estimate  its area  demands, we implemented the feedback circuit (assuming  measurement resolution N = 4 bits) of 32x32 system using the  Xilinx ISE  environment  with the  xc5vlx30 VIRTEX 5 FPGA  Figure 5.(a),(b) – Average hop-distance of packets  for different values of Rent's  exponent and traffic distribution according to δHop-Distance and δLoad-Balance,  normalized to the average hop-distance in the level-1 mesh. (c), (d) –  Congestion Immunity (Eq. 4) at the same system and traffic configurations.   than under δLoad-Balance. For this reason, in systems with static  traffic distribution, designers ought  to select a single  distribution mapping δ which is a compromise between the  potential performance under light and heavy loads. DTrD  provides the ability achieve the best performance for the whole  range of traffic loads. By nature, the contrast between δHopDistance and δLoad-Balance, in both the average hop-distance and  congestion immunity metrics increases with the size of the  system and Rent's exponent R. Note that the congestion  immunity metric ignores the fact that routers at the upper  hierarchy levels are usually higher in radix 1 than the 5x5  routers at the bottom mesh. Such routers are capable of  accommodating many more packets per time unit (RBW) if  traffic is uniformly distributed among their ports. However, the  ports to the neighbors and the port to the upper level are likely  to be more utilized than the multiple ports to the lower level in  the presence of many long distance packets. Having these two  contradictory aspects in mind, we predict that our δLoad-Balance,  which maximizes the congestion immunity metric, yields lower  average latency than δHop-Distance under heavy loads. We  evaluate the quantitative differences at the simulation section.  TABLE II.   TRAFFIC DISTRIBUTION THRESHIOLDS   Topology  DThi,Hop-Distance  DThi,Load-Balance  16x16  32x32  [5,8]  [4,10,50]   [11,19]  [23,42,61]  IV.  IMPLEMENTATION OF DYNAMIC TRAFFIC DISTRIBUTION  In this section we describe a possible implementation of  dynamic traffic distribution mechanism in hierarchical NoCs.  We use PyraMesh NoCs for illustration purposes, but similar  implementations can be easily devised for other hierarchical  schemes. The dynamic traffic distribution mechanism is  1 Up to 21x21 for level Concentration C = 4; 16 ports to the lower level, 4  ports the neighbors around and one port to the upper level.                                                                            Figure 6 – Architecture of the feedback module.  as  target device. The equivalent gate count of  the  implementation was 3855 NAND gates - totally negligible at  the scope of the entire system. We investigate the relation  between injection rate and buffer occupancy at different levels  in Section V. Moreover, we show in Section V that the  packets are routed in each level towards the router that is  occupancy measurements resolution N can be as low as 4 bits.                       closest to the destination (in destination's North-East quarter as  well). XY routing is employed in each level separately.  Dynamic traffic distribution does not affect the routing of  packets that are already on their way. When active DThi set is  changed, it affects only new packets that enter the source  queue. The scheme of the entire DTrD system in a 4x4  hierarchical NoC is presented in Figure 8.      Figure 8.The architecture of DTrD components in 4x4 hierarchical NoC.  V. SIMULATION RESULTS  We  implemented PyraMesh with dynamic  traffic  distribution mechanism using the OMNET++ [22] based  HNoCs open-source NoC simulation framework [21]. We used  this implementation to evaluate the performance of DTrD  under various  traffic scenarios and  to characterize  its  components. Our evaluation assumes that all the links in the  system are traversed within a single clock cycle. In [23] we  show that this assumption is valid for a wide variety of  hierarchical NoCs in present and future technology nodes. In  systems with tight timing constrains, the longest links at the  upmost hierarchy levels might have to be pipelined. We expect  that adding 1-2 cycles to each hop at the upmost levels would  not significantly affect our simulation results and leave this  issue for future work. Simulations were performed on 16x16  and 32x32 NoCs (Table I). Table III summarizes simulator,  system, and traffic parameters that we use in our simulations.   We divide this section into 3 sub sections. In the first subsection we present an evaluation of our feedback criteria (Eq.  5) and it's relation to traffic load. In sub-section B we compare  DTrD with static traffic distribution (STrD, according to δHopDistance and δLoad-Balance) and planar 2D mesh at steady state for  different injection rates.  Finally, the third sub-section presents  dynamic behavior of DTrD, STrD and 2D mesh for timevariant traffic patterns.  TABLE III.   HNOCS AND TRAFFIC PARAMETERS  Virtual channels per input port  Input buffer size [flits]  Packet size [flits]  Simulation clock period  2  4  8  2ns  Hierarchical NoC sizes  16x16, 32x32  Traffic Patterns  Rentian (R=0.6, 0.7, 0.8)  B. Control and Routing  The control module is responsible for adapting traffic  distribution mapping δ to the measured traffic load. Finding  and applying optimal δ mapping to every real time load  measurement demands complex real-time computations and  excessive hardware resources. To simplify the implementation  of DTrD, we limit δ to vary between the previously presented  δHop-Distance and δLoad-Balance. Accordingly, we define two system  modes: light load mode and heavy load mode. δHop-Distance and  δLoad-Balance are applied under light and heavy traffic loads,  respectively. Control decisions are based on the output of the  feedback module (i.e. the highest among average buffer  occupancies of the hierarchy levels, Eq. 5). To avoid control  fluctuations, transition between the modes is done according to  two thresholds as depicted in Figure 7.  Traffic distribution mappings δHop-Distance and δLoad-Balance are  implemented with the respective DThi,Hop-Distance and DThi,LoadBalance sets (Eq. 2) that are hard-coded in the routing logic in  each router.   The control module uses a single bit wire, which  connects it with all the network interfaces (NI's) of the sources,  to define the active set. We dedicate a single bit at the header  of each head-flit to indicate the traffic distribution mapping that  was active when the packet entered the source queue. Similarly  to feedback, this lightweight off-band connection enables to  separate the NoC from its control.    In PyraMesh with static  traffic distribution among  hierarchy  levels,  packets in  level  i  are  directed  towards  the  nearest terminal to a higher level (at the North-East quarter) as  long as PacketDistance>DThi. When  the  highest level is reached,    Figure 7. DTrD control loop              A. Relation between injection rate and buffers occupancy    The feedback in the proposed scheme is based on measurement  of buffers occupancy at the upper hierarchy levels. We used  our simulator to study how our control feedback metric reflects  the actual traffic conditions in the network. For example, we  present our observations of buffers occupancy and average  packet latency in 32x32 PyraMesh with Rentian traffic and  Rent's exponent R = 0.8 (Figure 9). The results in Figure 9  follow our aforementioned assumptions. When traffic is  distributed according δHop-Distance, the upper levels are much  more loaded compared to the first level mesh (Figure 9.a).  δLoad-Balance yields balanced buffers occupancy across the levels  (Figure 9.b). Comparable results were observed in the  simulations of the other system/traffic configurations.     We used these results to set DTrD control loop thresholds  (High->Low threshold, Low->High threshold - Figure 7).     Since the saturation knee occurs for upper levels buffers  occupancy of ~10%, we set the Low->High threshold (i.e. the  feedback value that indicated congestion at the upper levels) to  0.1; the High-Low threshold is set to 0.01 (1%).  B. Seatdy-State  Simulations  We generated Rentian traffic patterns with time invariant  properties (i.e. Rent's exponents R) and measured steady sate  head-of-packet average latency, including the source queuing  period, at different injection rates. We tested DTrD, STrD with  δHop-Distance and δLoad-Balance, and planar 2D mesh. Rent's  exponents of 0.6, 0.7 and 0.8 were used. These exponents  represent application phases with high, moderate and low  degrees of traffic locality. The results are presented in Figure  10. As expected, traffic distribution according δHop-Distance yields  the lowest average latency under light loads in all the scenarios.  Moreover, it's evident that DTrD performs well in selecting the  better alternative between δHop-Distance and δLoad-Balance at all the  injection rates. The difference between both traffic distribution  policies increases as the traffic becomes less local (i.e. with the  increment    of   Rent's   exponent  R).    Moreover,  there   is   a   Figure 9. Average buffers occupancy ratio vs. load.  The presented occupancy  is a long-term average after simulation worm-up at each injection rate.  significant gap between the results of 16x16 and 32x32  systems. In 16x16, although traffic distribution according to  δHop-Distance did cause the network to saturate at lower injection  rates than δLoad-Balance, the difference between the rates was  much lower that what we would expect based on the  Congestion Immunity metric (Figure 5.c). The upper levels in  16x16 PyraMesh are very sparse (4x4, 1x1) and comprise large  routers, each of them with 16 ports to the routers of the level  beneath. Unless Rent's exponent is very high (>0.8), most of  Figure 10. Steady state average latency vs. Injection rate for DTrD, planar 2D Mesh and static traffic distribution according to δHop-Distance and δLoad-Balance in  16x16 and 32x32 systems(Table I)and Rentian traffic with Rent's exponents R of 0.6, 0.7 and 0.8.                          the packets that are directed to the upper levels are routed  through a single router. Therefore the inter-router ports are not  getting congested. The result of 32x32 system matches our  expectations much better as there are much more packets that  traverse multi-router paths at the upper levels. We observed  that the gap between δHop-Distance and δLoad-Balance grows even  more in simulations that we performed on larger systems.  C.   Dynamic Behaviour of DTrD  We studied the dynamic behavior of DTrD in systems with  time-dependant traffic patterns. We generated Rentian traffic  with periodic phases; each phase was defined by different  Rent's exponents and injection rates. We collected end-to-end  packets delays on the fly and measured average latency with a  narrow sliding time-window. Average latency of STrD with  δHop-Distance and δLoad-Balance, and 2D planar mesh was measured  as well. Real-time average latency and a recording of the  operation of DTrD control mechanism throughout a single  400us-long simulation run in a 32x32 system are presented in  Figure 11. The results illustrate the efficiency, the remarkable  speed and the stability of DTrD's control mechanism. DTrD  introduces the minimum average latency at all the traffic  phases. When buffer occupancy of δHop-Distance and δLoad-Balance is  comparable (e.g. the phases during 25us-50us and 300us350us), DTrD fluctuates between the modes and yields a  slightly lower latency than δLoad-Balance, on average.      VI. RELATED WORK  In clustered hierarchical NoCs [9-14], inter-cluster packets  are typically routed through the upper hierarchy levels and no  flexibility is given to manage the utilization of hierarchy levels  in real-time. Such systems perform well as long as most of the  traffic is within the clusters. However, the network is likely to  saturate in application phases with low traffic locality, similarly  to static traffic distribution according to δHop-Distance in Figure  10. Non-clustered hierarchical topologies combine a fully  connected network at the bottom level with higher hierarchy  levels that provide alternative shorter paths (in hop-counts).  The hierarchy structure in non-clustered topologies can be  regular or irregular.   In systems with irregular hierarchy structure (e.g. Longrange links insertion [15], Flattened Butterfly [16]), extra links  with different lengths are supplemented to the bottom level.  Both [15] and [16] utilize shortest path routing with local  mechanisms that avoid over-utilization of the long-range links.  [15] proposes a tailor-made hierarchical structure based on  advance knowledge of the data communication flows; this  methodology is not suitable for CMP systems where traffic  patterns are not known a-priori. [16] is not feasible for  thousand-modules systems due to the quadratic increase in cost  of the high radix routers and the number of long links.  Therefore, DTrD can't be directly compared to the long links  utilization management mechanisms in these approaches.  Dynamic  traffic distribution among hierarchy  levels  (DTrD) is directly applicable in non-clustered systems with a  regular hierarchical structure (e.g. [17-19]). The hierarchical  structure in these systems has distinct levels, usually with a  descending radix. [17] presents a 2-level 2D mesh topology  and utilizes static traffic distribution (STrD) that minimizes  hop-distances (i.e. each packet takes the shorter alternative  between routing a path that includes the second level or just  XY route over at bottom mesh). The performance of such  distribution is likely to follow to our observations for δHopDistance in PyraMesh.   [18] employs minimal hop-distance STrD  as well, but with an adaptive distributed mechanism that directs  packets to their destinations over the bottom 2D mesh if their  access point to the higher hierarchy levels is congested. Packets  that are mapped to the upper hierarchy levels are first routed to  Figure 11. DTrD control operation and average real-time packet latency of DTrD, planar 2D mesh and static traffic distribution according to δHop-Distance and  δLoad-Balance in 32x32 hierarchical NoC (Table I) with time-varying traffic pattern. There are 8 different 50us traffic phases. Each phase is defined by Rent's exponent  R and injection rate (I. Rate in the figure) in flits/source/ns.      their access points upwards. At high injection rates, most of the  packets would not manage to get to the congested upper levels.  Therefore, their attempt to ascend is redundant, imposes  additional latency and causes congested hot-spots around the  terminals to the higher levels. DTrD prevents congestion at the  access points to the upper levels by reducing the number of  packets that are directed upwards and balance load among the  hierarchy levels. [19] provides a STrD configuration that  optimizes performance at either low of high injection rates, but  not both at the same time.   DTrD can be perceived as a sort of adaptive routing scheme  since it adjusts routing of packets as a response to varying  traffic conditions.  Previous adaptive routing schemes employ  nearest neighbor [4-5], regional [6] or global [7-8] congestion  information to define routes of individual packets or switch  between routing modes in distinct routers. DTrD differs from  the previous schemes as it does not control the routing of each  packet directly but switches the entire system between global  routing modes.  To the best of our knowledge, DTrD is the first  scheme that employs a centralized mechanism to dynamically  optimize the utilization of hierarchy levels to the varying traffic  conditions in hierarchical NoCs.                 VII. CONCLUSIONS  In this paper, we introduced the challenge of proper traffic  distribution among hierarchy levels in hierarchical NoCs. We  devised a novel dynamic traffic distribution scheme termed  DTrD and showed that its implementation demands are  negligible even in thousands-module systems. We studied  DTrD's latency performance in 16x16 - 32x32 systems with  various traffic patterns and injection rates and compared it to  static traffic distribution (STrD) policies.   We showed that STrD can optimize performance under  either light or heavy traffic loads, but not both at the same time.  Light load optimized STrD introduced a lower average latency  than high load optimized STrD by up to 22%. High load  optimized STrD introduced higher injection saturation rates by  up to 400%. We presented that with DTrD, the best of both  worlds can be achieved.   Finally, we evaluated the dynamic behavior DTrD and  demonstrated how its control mechanism reacts to traffic load  variations at periods of the order of tens of clock cycles.    ACKNOWLEDGMENT  We would like to thank Mr. Viktor Kulikov for his valuable  help and support.  "
Quadrisection-based task mapping on many-core processors for energy-efficient on-chip communication.,"Network-on-chip (NoC) promises better scalability and power efficiency compared to traditional on-chip interconnects. But in order to fully exploit the benefits offered by the new paradigm, especially as the number of cores in the network increases, challenging resource management questions need to be addressed. Of particular interest and the subject of our study is the question of how to map applications to processors (network nodes) in a NoC so as to minimize the dynamic power consumption of the NoC.","Quadrisection-Based Task Mapping on Many-Core Processors for Energy-Efﬁcient On-Chip Communication Nithin Michael, Yao Wang, G. Edward Suh, Ao Tang School of Electrical and Computer Engineering Cornell University Ithaca, NY 14853, USA I . IN TRODUC T ION Network-on-chip (NoC) promises better scalability and power efﬁciency compared to traditional on-chip interconnects. But in order to fully exploit the beneﬁts offered by the new paradigm, especially as the number of cores in the network increases, challenging resource management questions need to be addressed. Of particular interest and the subject of our study is the question of how to map applications to processors (network nodes) in a NoC so as to minimize the dynamic power consumption of the NoC. I I . PROBL EM S E TU P More speciﬁcally, we assume that each application consists of multiple parallel tasks, which communicate in a message passing fashion through an on-chip network. Each communication ﬂow has a throughput requirement associated with it, which speciﬁes how often a packet is sent between the corresponding source-destination pair in the network. The goal is to determine a mapping of application tasks to network nodes in a way that minimizes the communication energy while maintaining performance. We call this a “task mapping” problem. We will use the following assumptions and notations. The NoC is assumed to have sufﬁcient capacity to handle the applications that will be mapped to it assuming minimal routing. Minimal routing is assumed because it is commonly used and delivers good performance given enough capacity. Thus, we are left with the problem of mapping communicating tasks to processors to minimize dynamic power consumption. Suppose that there are V nodes in the NoC and that there are N tasks to be mapped for a given application. We also assume that N ≤ V , as we are interested in the mapping aspect of the problem and not scheduling. Let A be the communication task graph of an application where A(i, j ) represents the communication rate from task i to task j . We will use B (i, j ) to represent the manhattan distance between node i and node j of the NoC. We want to minimize the communication cost, which given our assumptions, can be calculated as, (cid:88) C ost = A(i, j )B (p(i), p(j )) (1) i,j where p(i), p(j ) are the processors to which tasks i and j are mapped. The idea is that minimizing communication cost Figure 1: Quadrisection Method translates to minimizing dynamic power and leads to better energy-efﬁciency. Unfortunately, the task mapping problem is exactly the quadratic assignment problem which is not only NP-Hard but also difﬁcult to approximate well in general [1]. However, for the special case of networks-on-chip, given the importance of the problem, multiple heuristic schemes have been developed for task mapping. We compare our work to three of these schemes NMAP [4], BMAP [5] and bisection [6] as we feel that they are representative of the ideas that are commonly used to tackle this problem in NoCs. We were motivated by the insight that mapping onto a NoC, particularly, one with a regular topology like the mesh topology, is a two-dimensional problem and should be approached as such. Unsurprisingly, a two-dimensional mapping approach had already been studied for VLSI layout [7]. While we adopt a similar approach, we also take advantage of the fact that we are mapping discrete tasks to individual processors to further reﬁne the algorithm. I I I . QUADR I SEC T ION As shown in Figure 1, the idea is to divide the tasks into four groups of the same size and map the groups to four smaller mesh networks (obtained by bisecting the original mesh network horizontally and vertically) in an attempt to minimize the communication crossing between the groups. We repeat this procedure recursively till the mapping problem becomes small enough to apply exhaustive search. We use the Fiduccia-Matthyses algorithm [3] to assign the tasks to each group. Given an initial assignment of the tasks to four groups, the Fiduccia-Matthyses algorithm moves one task at a time from one group to another in order to minimize the communication cost. To prevent inﬁnite loops, once a task is moved, it is locked and cannot be moved (a) Total Communication Cost (Normalized to Quadrisection) (b) Costs of Different Mappings for Benchmarks (Normalized) (c) Dynamic Power (Normalized) Figure 2: Evaluation results again until the next iteration. At each iteration, for each task group that is further divided into sub-task groups, we take advantage of the discrete nature of our problem to perform exhaustive search in order to determine sub-task group to mesh assignments that minimize the communication cost to the other task groups. The high-level description of the core of the quadrisection algorithm is provided in Algorithm 1. In the algorithm, Mi refers to the coordinates that determine a mesh, Si refers to a task group and AssignLoc is used to assign sub-task groups to ‘good’ mesh locations with respect to external task groups. Algorithm 1 Quadrisection(V, A) Input: V , A map ← [(0, 0), . . . , (0, 0)] map ←RECURSIVEMAP(M , S, A, map) procedure R ECUR S IV EMA P(M , S, A, map) if M is a 2 × 2 mesh then (cid:46) Do exhaustive search for 2 × 2 mesh & AssignLoc else map(S1 , S2 , S3 , S4 ) (M1 , M2 , M3 , M4 , S1 , S2 , S3 , S4 ) ← F M (M , S, A) &AssignLoc (cid:46) Initialize map (cid:46) Compute map (cid:46) Use Fiduccia-Matthyses to obtain sub-task groups (cid:46) Use AssignLoc to determine mesh assignments For each i = 1, 2, 3, 4 do RECURSIVEMAP(Mi , Si , A, map) (cid:46) Do the mapping for each smaller mesh recursively end for end if end procedure IV. EVALUAT ION The numerical evaluation was performed for synthetically generated random trafﬁc patterns as well as for common benchmark patterns (Table I). As evident from Figure 2a, Quadrisection performed noticeably better than the other algorithms on almost all synthetic trafﬁc patterns. Figure 2b shows that quadrisection outperformed again for the benchmark trafﬁc patterns, coming very close to the optimal value for the cases for which we were able to calculate Table I: Benchmarks Benchmark DSP MWD MPEG4 VOPD Ericsson Radio A/V [2] # of tasks 6 12 12 16 15 21 # of ﬂows mesh size 8 13 26 21 26 33 4 × 4 4 × 4 4 × 4 4 × 4 4 × 4 8 × 8 it. The benchmarks were also simulated using Darsim and the dynamic power evaluated using the power models from Orion 2. The results shown in Figure 2c were found to mirror our numerical predictions (Figure 2b). It should also be noted, particularly for online mapping tasks, that the runtime of these algorithms plays an important role in their selection. Promisingly, on average, quadrisection and bisection took on the order of 0.06 seconds like BMAP while NMAP took around 3.3 seconds. As can be seen from our evaluations, Quadrisection generates very good mappings quickly and should be a useful addition to the NoC designer’s toolkit. "
Physical planning for the architectural exploration of large-scale chip multiprocessors.,"This paper presents an integrated flow for architectural exploration and physical planning of large-scale hierarchical tiled CMPs. Classical floorplanning and wire planning techniques have been adapted to incorporate layout constraints that enforce regularity in the interconnect networks. Routing is performed on top of memories and components that underutilize the available metal layers for interconnectivity. The experiments demonstrate the impact of physical parameters in the selection of the most efficient architectures. Thus, the integrated flow contributes to deliver physically-viable architectures and simplify the complex design closure of large-scale CMPs.","Physical Planning for the Architectural Exploration of Large-Scale Chip Multiprocessors Javier de San Pedro, Nikita Nikitin, Jordi Cortadella and Jordi Petit Universitat Polit `ecnica de Catalunya Abstract—This paper presents an integrated ﬂow for architectural exploration and physical planning of large-scale hierarchical tiled CMPs. Classical ﬂoorplanning and wire planning techniques have been adapted to incorporate layout constraints that enforce regularity in the interconnect networks. Routing is performed on top of memories and components that underutilize the available metal layers for interconnectivity. The experiments demonstrate the impact of physical parameters in the selection of the most efﬁcient architectures. Thus, the integrated ﬂow contributes to deliver physically-viable architectures and simplify the complex design closure of large-scale CMPs. I . IN TRODUC T ION Chip multiprocessing is rapidly evolving and increasing the power-performance efﬁciency of computing systems by exploiting the parallelism inherent in applications. Tile replication [1] and hierarchical CMP organizations [2] are two widely adopted methodologies for designing many-core CMPs under time-to-market pressure. Figure 1 shows an example of a hierarchical tiled CMP. The chip consists of identical tiles (clusters) organized in a mesh. Every cluster includes a local interconnect (bus or ring), cores (C) with private caches (L2), a shared cache (L3) and a router (R) that connects the cluster to the top-level mesh. The problem of architectural exploration consists of automatically ﬁnding values for all of the system-level design parameters (number of clusters, interconnect topology, number of cores per cluster, amount of L2 and L3, etc.) to maximize chip performance under design constraints, typically area [3], [4]. A trade-off exists between the area dedicated to cores, caches and interconnect, depending on the CMP workload. While various works exist that explore system-level parameters, signiﬁcantly less research has been carried out in the ﬁeld of physical planning for CMPs [5], [6]. Physical aspects have a signiﬁcant impact on chip area and routability and may introduce important deviations in the area estimations performed during architectural exploration. Both the tendecy towards wire links and the use of over-the-cell routing (in which interconnect wires are routed on top of memory com(cid:1) (cid:1) (cid:1) (cid:1) (cid:2) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:2) (cid:1)(cid:2) (cid:1) (cid:1) (cid:1) (cid:1)(cid:2) (cid:6) (cid:7) (cid:8)(cid:9) (cid:1) (cid:2) (cid:3)(cid:4) (cid:2) (cid:3)(cid:4) (cid:3)(cid:4)(cid:5) (cid:3)(cid:5) Fig. 1. Architecture of a hierarchical tiled CMP. (cid:30)(cid:2)(cid:13)(cid:6)(cid:21)(cid:9)(cid:12)(cid:20)(cid:13)(cid:21)(cid:6) (cid:37)(cid:3)(cid:4)(cid:14) (cid:41)(cid:10)(cid:3)(cid:2)(cid:24)(cid:34)(cid:10)(cid:12)(cid:24)(cid:18) (cid:42)(cid:2)(cid:43)(cid:4)(cid:3) (cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6) (cid:7)(cid:8)(cid:4)(cid:9)(cid:10)(cid:2)(cid:9)(cid:11)(cid:12)(cid:13)(cid:14)(cid:4)(cid:15)(cid:8)(cid:2)(cid:16)(cid:4)(cid:9)(cid:17) (cid:1)(cid:2)(cid:3)(cid:4)(cid:5) (cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:9)(cid:14)(cid:9)(cid:10)(cid:4)(cid:5) (cid:6)(cid:15)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:16)(cid:4)(cid:16)(cid:2)(cid:3)(cid:11)(cid:4)(cid:5) (cid:17)(cid:7)(cid:18)(cid:4)(cid:3)(cid:9)(cid:2)(cid:7)(cid:7)(cid:4)(cid:9)(cid:18)(cid:5) (cid:19)(cid:2)(cid:3)(cid:20)(cid:21)(cid:2)(cid:14)(cid:22)(cid:5) (cid:27)(cid:19)(cid:28)(cid:6)(cid:20)(cid:14)(cid:12)(cid:5)(cid:23)(cid:20)(cid:13)(cid:10)(cid:2) (cid:1)(cid:2)(cid:3)(cid:4)(cid:5) (cid:1)(cid:14)(cid:9)(cid:10)(cid:4)(cid:5) (cid:17)(cid:7)(cid:18)(cid:4)(cid:3)(cid:9)(cid:2)(cid:7)(cid:7)(cid:4)(cid:9)(cid:18)(cid:5) (cid:18)(cid:9)(cid:14)(cid:19)(cid:20)(cid:21)(cid:4)(cid:14)(cid:21)(cid:22)(cid:9)(cid:12)(cid:5)(cid:23)(cid:14)(cid:2)(cid:13)(cid:24)(cid:25)(cid:22)(cid:9)(cid:12)(cid:21)(cid:20)(cid:2)(cid:13) (cid:23)(cid:24)(cid:16)(cid:25)(cid:4)(cid:3)(cid:13)(cid:2)(cid:26)(cid:13)(cid:9)(cid:2)(cid:3)(cid:4)(cid:5) (cid:1)(cid:21)(cid:24)(cid:5)(cid:18)(cid:4)(cid:3)(cid:13)(cid:5)(cid:11)(cid:27)(cid:4) (cid:28)(cid:29)(cid:30)(cid:28)(cid:31)(cid:13)(cid:9)(cid:14)(cid:9)(cid:10)(cid:4)(cid:13)(cid:5)(cid:11)(cid:27)(cid:4) (cid:17)(cid:7)(cid:18)(cid:3)(cid:14)(cid:8)(cid:9)(cid:21)(cid:24)(cid:5)(cid:18)(cid:4)(cid:3)(cid:13)(cid:11)(cid:7)(cid:18)(cid:4)(cid:3)(cid:9)(cid:2)(cid:7)(cid:7)(cid:4)(cid:9)(cid:18) (cid:17)(cid:7)(cid:18)(cid:4)(cid:3)(cid:8)(cid:9)(cid:21)(cid:24)(cid:5)(cid:18)(cid:4)(cid:3)(cid:13)(cid:11)(cid:7)(cid:18)(cid:4)(cid:3)(cid:9)(cid:2)(cid:7)(cid:7)(cid:4)(cid:9)(cid:18) (cid:1)(cid:21)(cid:24)(cid:5)(cid:18)(cid:4)(cid:3)(cid:8)(cid:21)(cid:4)(cid:32)(cid:4)(cid:21)(cid:13)(cid:33)(cid:2)(cid:2)(cid:3)(cid:12)(cid:21)(cid:14)(cid:7) (cid:19)(cid:11)(cid:3)(cid:4)(cid:13)(cid:21)(cid:4)(cid:7)(cid:34)(cid:18)(cid:10)(cid:13)(cid:4)(cid:5)(cid:18)(cid:11)(cid:16)(cid:14)(cid:18)(cid:11)(cid:2)(cid:7) (cid:18)(cid:9)(cid:14)(cid:19)(cid:20)(cid:21)(cid:4)(cid:14)(cid:21)(cid:22)(cid:9)(cid:12)(cid:5)(cid:23)(cid:4)(cid:26)(cid:8)(cid:5)(cid:2)(cid:9)(cid:12)(cid:21)(cid:20)(cid:2)(cid:13) (cid:35)(cid:4)(cid:7)(cid:4)(cid:3)(cid:14)(cid:18)(cid:11)(cid:2)(cid:7) (cid:2)(cid:26)(cid:13)(cid:9)(cid:2)(cid:7)(cid:36)(cid:34)(cid:24)(cid:3)(cid:14)(cid:18)(cid:11)(cid:2)(cid:7)(cid:5) (cid:37)(cid:7)(cid:14)(cid:21)(cid:38)(cid:18)(cid:11)(cid:9)(cid:14)(cid:21) (cid:16)(cid:2)(cid:22)(cid:4)(cid:21)(cid:11)(cid:7)(cid:34) (cid:1)(cid:2)(cid:2)(cid:3)(cid:4)(cid:2)(cid:5) (cid:6)(cid:7)(cid:2)(cid:8)(cid:9)(cid:10)(cid:9)(cid:11)(cid:12) (cid:13)(cid:2)(cid:11)(cid:14)(cid:12)(cid:10) (cid:15)(cid:16)(cid:17)(cid:7)(cid:13)(cid:18)(cid:4)(cid:19)(cid:9)(cid:7)(cid:16)(cid:13)(cid:20)(cid:9)(cid:2)(cid:11) (cid:39)(cid:11)(cid:16)(cid:24)(cid:21)(cid:14)(cid:18)(cid:11)(cid:2)(cid:7) (cid:19)(cid:11)(cid:3)(cid:4)(cid:13)(cid:12)(cid:21)(cid:14)(cid:7)(cid:7)(cid:11)(cid:7)(cid:34) (cid:40)(cid:21)(cid:2)(cid:2)(cid:3)(cid:12)(cid:21)(cid:14)(cid:7)(cid:7)(cid:11)(cid:7)(cid:34) (cid:29)(cid:12)(cid:5)(cid:20)(cid:3)(cid:12)(cid:21)(cid:20)(cid:2)(cid:13) (cid:27)(cid:19)(cid:28)(cid:6)(cid:20)(cid:14)(cid:12)(cid:5)(cid:23)(cid:8)(cid:5)(cid:12)(cid:13)(cid:13)(cid:20)(cid:13)(cid:25) Fig. 2. The ﬂow for architectural exploration of CMPs. ponents) increase these deviations. In this work, we propose a architectural exploration ﬂow that incorporates ﬂoorplanning and wire planning for more accurate area and physical viability estimation. Our proposal takes into account the use of overthe-cell routing and hierarchical tiled CMPs. I I . EX P LORAT ION FLOW Figure 2 presents the interface and steps of the exploration ﬂow. The input data consists of a set of models and constraints. Examples of the models that can be used are presented in [4]. Different models of cores can be considered with various implementation aspects. The models for on-chip and off-chip memories deﬁne area and access latency. The workload is characterized by deﬁning the memory access patterns and expected throughput for each core type. For physical planning, the aspect ratios and the number of metal layers used by each component are deﬁned. The output of the exploration ﬂow is a system-level description of a conﬁguration (or a set of conﬁgurations). This description includes all system-level parameters, a cluster-level ﬂoorplan and a interconnect wire plan. A. Architectural exploration The objective of architectural exploration is to efﬁciently reduce the design space and generate a moderate number of candidate conﬁgurations with the highest performance satisfying the area and power constraints. The efﬁciency in exploration is achieved by a heuristic search (both Simulated Annealing and Extremal Optimization are used) that uses an analytical model [4] for performance evaluation. This analytical model captures the contention component in the interconnect. Power consumption is also estimated using analytical models. B. Physical planning This stage is the main contribution of this work. The objective of this stage is to obtain accurate estimations of chip area for all candidate conﬁgurations obtained in the previous stage, considering the physical parameters of the components, e.g. aspect ratios and number of metal layers. As it can be seen from Fig. 1, the regularity of the top-level mesh reduces the ﬂoorplanning problem from chip to cluster level. To solve the cluster-level ﬂoorplanning problem, a heuristic search (Simulated Annealing) is also used. Only slicing ﬂoorplans are considered in order to reduce the search space. Bounding curves are used to efﬁciently model components with variable aspect ratios [7]. In the cost function, we estimate wire length using Lee’s algorithm [8] instead of half-perimeter wire length to ﬁlter unroutable ﬂoorplans before trying to perform full wire planning. In order to allow the abutment of different clusters without any wiring overhead in the connections, a symmetry constraint is introduced during wire planning for the global interconnect wires. Additional constraints are also deﬁned to enforce the adjacency between cores and their private caches, thus avoiding performance penalties caused by accessing distant caches. Wire planning is performed only at the link level, and not for the individual wires of each link. We create a 3D grid where each horizontal plane represents a metal layer of the cluster, and the cell size is equivalent to the minimum width of a link. Blockages are then inserted on this grid according to the metal layers occupied by each CMP component. A SAT solver is then used to ﬁnd the required routes according to the architectural conﬁguration and the constraints. For every promising candidate obtained during the architectural planning, a set of the ﬂoorplans with a minimized combination of area and wire length is generated. Afterwards, wire planning is performed to verify the routability of every ﬂoorplan and generate a more accurate estimation of the wire length. Only the best routable ﬂoorplans are selected for performance validation. C. Validation The validation stage uses simulation to improve the performance estimations of the selected candidates. In our implementation, we used a modiﬁed version of BookSim. I I I . EX P ER IM EN TA L R E SU LT S In this experiment, we show the impact of physical planning on architectural exploration. An exploration was performed with constraints on total chip area (350 mm2 ) and power (200 W). The area and performance models for cores were obtained by scaling publicly available Intel Core 2 Duo data. For the workload model, we characterized two SPEC2006 benchmarks (namd and soplex) as described in [4]. In Fig. 3, we plot the conﬁgurations with the highest throughput. The x-axis indicates the estimated performance of each conﬁguration. The y-axis shows three approximations of the chip area for every conﬁguration: block area is the sum of areas from the individual components; best ﬂoorplan  360  355  350  345  340  335 ] 2 m m [ a e r A  330  74  74.5  75 (a) (b) Block area Best floorplan Best routable floorplan  75.5  76  76.5 Throughput [IPC]  77  77.5  78 Fig. 3. Comparison of area estimations with and without physical planning indicates the minimum possible ﬂoorplan area, even if it has routability problems; best routable ﬂoorplan shows the area for the smallest routable ﬂoorplan. In order to satisfy the area requirement, the designer will not be able to select the best conﬁguration (a) but rather (b), with a 1.6% throughput loss. Even though area and throughput are similar, the architectural parameters are rather distinct. This result gives an insight for the designer to consider an alternate group of architectures with improved physical parameters. Figure 4 shows (b) after the full exploration ﬂow. ACKNOW L EDGM EN T S This research has been funded by a gift from Intel Corp., project CICYT TIN2007-66523, and FPI grant BES-2008004612. "
GCA - Global congestion awareness for load balance in Networks-on-Chip.,"As modern CMPs scale to ever increasing core counts, Networks-on-Chip (NoCs) are emerging as an interconnection fabric, enabling communication between components. While NoCs provide high and scalable bandwidth, current routing algorithms, such as dimension-ordered routing, suffer from poor load balance, leading to reduced throughput and high latencies. Improving load balance, hence, is critical in future CMP designs where increased latency leads to wasted power and energy waiting for outstanding requests to resolve. Adaptive routing is a known technique to improve load balance, however, prior adaptive routing techniques either use local or regionally aggregated information to form their routing decisions. This paper proposes a new, light-weight, adaptive routing algorithm for on-chip routers based on global link state and congestion information, Global Congestion Awareness (GCA). GCA uses a simple, low-complexity route calculation unit, to calculate paths to their destination without the myopia of local decisions, nor the aggregation of unrelated status information, found in prior designs. In particular GCA outperforms local adaptive routing by 26%, Regional Congestion Awareness (RCA) by 15%, and a recent competing adaptive routing algorithm, DAR, by 8% on average on realistic workloads.","GCA:Global Congestion Awareness for Load Balance in Networks-on-Chip Mukund Ramakrishna, Paul V. Gratz and Alexander Sprintson Department of Electrical and Computer Engineering, Texas A&M University Email: mukund.r@tamu.edu, pgratz@gratz1.com, spalex@tamu.edu Abstract—As modern CMPs scale to ever increasing core counts, Networks-on-Chip (NoCs) are emerging as an interconnection fabric, enabling communication between components. While NoCs provide high and scalable bandwidth, current routing algorithms, such as dimension-ordered routing, suffer from poor load balance, leading to reduced throughput and high latencies. Improving load balance, hence, is critical in future CMP designs where increased latency leads to wasted power and energy waiting for outstanding requests to resolve. Adaptive routing is a known technique to improve load balance, however, prior adaptive routing techniques either use local or regionallyaggregated information to form their routing decisions. This paper proposes a new, light-weight, adaptive routing algorithm for on-chip routers based on global link state and congestion information, Global Congestion Awareness (GCA). GCA uses a simple, low-complexity route calculation unit, to calculate paths to their destination without the myopia of local decisions, nor the aggregation of unrelated status information, found in prior designs. In particular GCA outperforms local adaptive routing by 26%, Regional Congestion Awareness (RCA) by 15%, and a recent competing adaptive routing algorithm, DAR, by 8% on average on realistic workloads. I . IN TRODUC T ION Current architectures are moving towards multi- and manycore designs, such as the Intel Single Chip Cloud Computer [1], and the recently announced Intel Many Integrated Core (MIC) Knights Corner chip [2], as a means to achieve higher performance from increasing transistor densities, while addressing the concerns of rising power envelopes and wire delays. As these designs continue to grow, there is a critical need for a scalable and high-bandwidth on-chip communication fabric to serve as an interconnection network for these components. Networks-on-chip (NoCs) have emerged to fulﬁll this role. Na¨ıve scaling of NoCs for large multi-core designs, however, exponentially increases the latency of program data accesses and sharing. High interconnect latencies translate into idle processor core cycles and wasted power, eschewing the primary motivation for multi-core designs. Therefore, reducing interconnect latency is critical to achieve scaling performance in future CMP designs. Since multiple paths between source and destination exist, NoCs must implement a routing algorithm to route packets to their destination. This algorithm impacts the latency and throughput that the trafﬁc experiences. Many current NoC proposals implement oblivious routing algorithms [1], such as dimension ordered routing (DOR), which route packets irrespective of the state of the congestion in the network. Although these algorithms have low complexity, they often perform poorly because they produce load imbalances in the network, particularly when under realistic workloads that are often inherently imbalanced. Figure 1 illustrates the load imbalance due to oblivious routing. The ﬁgure shows a snapshot of the state of an NoC implementing DOR when executing the fft benchmark from the SPLASH-2 suite [3]. In the ﬁgure, link utilization is shown for a 100 cycle window during a congested program phase. The size and darkness of the arrows indicates the degree of link utilization during the time window. We see here that workload imbalances and the DOR routing algorithm conspire to direct a signiﬁcant fraction of the trafﬁc to a subset of the nodes in the network, thereby disproportionately loading the links of the network. Ultimately, extra request latencies lead to less effective utilization of CMP resources, eschewing the efﬁciency beneﬁts of CMP architectures. Adaptive routing exploits the path diversity of the topology to select a less congested path for every sourcedestination pair. Such techniques are efﬁcient at routing trafﬁc around congested hotspots, balancing load across the network, thereby providing better performance over oblivious routing. This paper proposes a novel globally-aware routing policy with low implementation complexity, that efﬁciently reacts to changing network conditions. This algorithm: • Uses “back-annotated piggybacking” to propagate congestion information across the network in an efﬁcient manner. • Contains a light-weight, shortest path route computation technique, optimized for easy hardware implementation. • Two versions of the algorithm are presented, one (GCA) designed for small to medium sized networks (up to 64nodes), another (LGCA) designed for large networks. I I . R ELAT ED WORK A. Locally Aware Adaptive Routing Locally adaptive routing techniques make decisions based on local congestion information. Dally and Aoki make use of the number of free VCs as a congestion metric and pick the port with the higher number of free VCs [4]. Kim et al [5] use buffer availability at adjacent nodes as a congestion metric, while Singh et al [6], [7] use output queue lengths for the same purpose. DyXY [8] and DyAD [9] switch between deterministic and minimal adaptive routing based on local congestion information. These schemes use buffer occupancy as their metric of congestion. As these techniques use readily available local information only, they have low implementation complexity and have no secondary impact on network trafﬁc due to status information propagation. Such schemes, however, often make sub-optimal routing decisions at a global level due to a myopic view of network congestion. They also react slower to congestion at further links as they rely on network back pressure for information propagation. Figure 2 highlights these limitations. If we route a packet from S to D using local adaptive routing, at each hop the link with a lower congestion value will be chosen. In this manner, the path S → T → U → X → D on Fig. 1: Network link utilization for fft under DOR. Fig. 2: Motivation for global awareness. Fig. 3: Back-annotated piggybacking. which the sum of congestion values is 1 + 3 + 13 + 11 = 28 would be selected1 . For this packet, however, a better path is S → V → W → Z → D as the total sum of the congestion values on this path is 9 + 2 + 1 + 2 = 14. Thus, local adaptive routing picks a more congested path due to greedy decision making based on local lightly loaded links to nodes T and U; once it reaches node U, it has only one minimal direction to traverse, and that direction is heavily congested. Global congestion awareness enables complete visibility of the network state and alleviates the issue of greedy decisions. As the decisions are based on a global state, there is no reliance on network back pressure for information. B. Regionally Aware Adaptive Routing Regional Congestion Awareness (RCA) was the ﬁrst work to explore adaptive routing based on congestion visibility beyond the adjacent nodes [10]. RCA gathers congestion information from nodes beyond the adjacent one by propagation over a light-weight monitoring network. At each hop, the incoming congestion information is aggregated with the local congestion information and then propagated further into the network. RCA weights the congestion values by distance from the local node, such that distant links have less impact upon route selection. DBAR is a regional adaptive routing algorithm, which factors in information about the destination node while selecting the route, reducing the noise aggregation adds [11]. DBAR exploits the high degree of locality many trafﬁc patterns exhibit, by propagating congestion information in the X and Y dimensions without aggregation. This technique is particularly useful for partitioned many-core architectures, where distant partition congestion information is not useful for routing decisions in the local partition. In unpartitioned networks, however, DBAR’s performance is very similar to RCA and hence we do not explicitly compare GCA’s performance against it. Although these regionally aware techniques provide a better view of the network than locally adaptive schemes, due to the congestion weight aggregation, the resolution of the information can be quite poor, leading to degraded performance. The relevance of congestion of a downstream link is highly dependent on whether or not that link will eventually be traversed by the packet. If we route a packet from S to D in Figure 2 using a regional adaptive algorithm like RCA1D [10], we can see these limitations. At node S, it compares between the aggregate congestion of all links in the EAST direction against the SOUTH direction. This comparison leads it to pick the EAST direction. Similarly at downstream nodes, 1We only consider minimal adaptive routing here, due to the complexity and generally higher latencies of non-minimal adaptive routing it picks EAST at T to reach U from where it has only one admissible dimension to traverse. It selects the same path as local adaptive, S → T → U → X → D , and the sum is 28. This route is chosen because RCA considered links in the SOUTH and EAST direction that were 2 hops away from the current node. With minimal routing, for a packet from S to D, these links would never be used as they fall out of the minimal region, however, their congestion status adversely affects the route selection. Global awareness overcomes the issue of aggregated information by maintaining ﬁne-grained congestion information about the network, helping it avoid considering links outside the minimum path range for a given source-destination pair. C. Globally Aware Adaptive Routing Two recent works have proposed globally aware adaptive routing techniques. Adaptive Toggle Dimension Ordered Routing (ATDOR) is a NoC architecture that implements a secondary network to transmit congestion information by each node to a dedicated node, using it to pick between XY or YX DOR for every source destination pair in the network [12]. DAR adds a separate network, used to communicate their local congestion information with the rest of the network [13]. Every node then determines the amount of trafﬁc that it has to split for a particular destination amongst the candidate output ports. Both these approaches introduce a sideband network for gathering and disseminating congestion information, adding to implementation overheads. In the case of DAR, the monitoring network’s congestion information updates are very slow. As a result DAR is slow to respond to changing congestion status of the network. GCA removes the need for a sideband network by embedding (“piggybacking”) status information in packet headers. We also implement a low-complexity, low-latency route computation unit, providing a much lower response time to the changing network congestion. I I I . G LOBA L CONG E ST ION AWAR EN E S S (GCA ) Global Congestion Awareness (GCA) is a novel, globallyaware, adaptive routing scheme for NoCs that provides each router a timely and complete view of the congestion status of the whole network. The two main contributions of this technique are in the method of dissemination of the information across the network and the route computation technique that each node employs. A globally aware algorithm can determine the exact amount of congestion that the packet will encounter along the candidate paths instead of a rough estimate of the congestion in a given direction. Thus, it can pick a better path as it takes into account each individual link that can be traversed and does not include links which lie outside Fig. 4: Composition of the header ﬂit the proposed route. The minimal adaptive model guarantees livelock freedom and we implement deterministically routed escape channel VCs for deadlock freedom. A. GCA Model Ideally, at every hop of the packet’s traversal, output port selection would be based upon a perfect view of the network’s congestion and thus always pick the best path towards its destination. In a realistic implementation, however, a perfect view is impossible due to the latency incurred in transferring congestion information from one point of the network to another. Furthermore, for packets traversing a considerable number of hops from their source, network congestion close to the destination might change by the time the packet reaches that region. Thus, the decision made at the source might turn out to not be the best. These issues are speciﬁcally addressed in our route computation technique. The GCA algorithm consists of the following: 1) A new ﬁeld in the header ﬂit called the “trafﬁc vector” is introduced, which consists of “back-annotated” congestion information as seen by the packet as it traverses the network, along with bits recording the path taken. 2) Every node extracts this trafﬁc vector ﬁeld from the header ﬂit and updates its own current view of the network based upon this new information. 3) Each node maintains a congestion map of the link status of all links in the network. Upon extraction of the trafﬁc vector from an incoming packet, the node performs a route computation of the affected sub-network to determine the best output port for every node in that subnetwork. These routes are maintained in a pre-route table. 4) Every node also appends congestion information about its own link into the header before sending out the packet. 5) Packets are routed to the appropriate output port in each node by looking up the pre-route information embedded in the ﬂit. The current node also adds in output port information for the next hop by looking up its routing table; thus employing per-hop lookahead routing. In the following sections, we present a detailed description of the GCA architecture at a packet-level and router-level. B. Trafﬁc Vector Prior globally aware techniques employ a separate sideband network, which is dedicated to congestion information. This approach, however, introduces signiﬁcant extra hardware overhead and increases the complexity of the design. GCA proposes to embed congestion status information in the packet header, “piggybacking” it for propagation in the network. Typically, a signiﬁcant fraction a packet’s header goes unused due to the wide bit-widths of typical NoC links, as shown in Figure 4. The header ﬂit contains source and destination node identiﬁers, and the physical memory address of the cache block. We assume a ﬂow-control overhead of ﬁve bits, 3 bits for virtual channel ID (VC) and 2 bits for ﬂit type (FT). Given an assumed link width of 128 bits, the √ remaining ten bytes in the header are unused. We propose to use some of these bits to encode the congestion information from nodes that the header ﬂit has visited. This eliminates the complex overhead of a sideband network, making better utilization of bits that were being wasted. In Figure 4, the new ﬁeld required in the header ﬂit is highlighted in gray. At every hop, each node appends information into the trafﬁc vector ﬁeld of the header ﬂit and then sends it out on its speciﬁed output port. The added information consists of two parts: congestion information and input port information. The congestion metric we use is the number of free VCs and corresponds to the link that is connected to the current node. The input port information, denoting the ﬂit’s arrival port, is appended in order to assist with information extraction that downstream nodes will perform on the trafﬁc vector embedded in this ﬂit. At every hop, the trafﬁc vector ﬁeld is appended with 5 new bits. The trafﬁc vector for a 4x4 2D-Mesh is shown in Figure 4, where the maximum size of the trafﬁc vector could be 25 bits. Here this leaves 55 unused bits in a network with 128-bit wide links. Generally, the maximum bits required scales as 2 ∗ ( N − 1) where N is the total number of nodes in the network. Thus, the 80 bits available in the header would support networks up to 64 nodes. For larger networks or where the header has fewer free bits, we ﬁnd that constraining the information to a smaller window provides sufﬁcient information for GCA to successfully route packets. We term this technique as Limited GCA (LGCA) and discuss it in further detail in Section IV-C. In network designs where available header bits are scarce, an easily implementable alternative is to add an extra, “monitor tail ﬂit” to the packet, at the cost of some increase in the network load. This method does imply that the visibility of the network is limited to the regions that experience the trafﬁc. Most of on-chip trafﬁc, however, is request-response packets, thus the links loaded with “upstream” trafﬁc are candidates for transmission of “downstream” trafﬁc too. To reduce the effect of lack of information for unvisited nodes, links for which a node does not receive direct information are approximated to a nominal congestion level, this trade-off helps in keeping the solution realistic (further details are provided in Section III-C). Piggybacking congestion information in the ﬂit’s direction of travel (solid links in Figure 3), would pass information about links that are in the ingress direction to all downstream nodes. This would be useless as the downstream nodes can only send trafﬁc in the opposite direction (dashed links in Figure 3). Therefore, we “back-annotate” the ﬂit by embedding information about the links opposite the direction of travel. C. Congestion Map The congestion map provides each node an accurate representation of the network’s link congestion to utilize in routing decisions. It contains values denoting the congestion of links in the network downstream from the current node. For every incoming header ﬂit, the router reads the trafﬁc vector ﬁeld and inserts this information into its local map. It back-traces the ﬂit’s path by using the output port information embedded by previous hops. The router simply overwrites the older value in the map with the new value. In parallel, the router appends the local congestion information and also adds the input port information into the trafﬁc vector ﬁeld before sending the ﬂit back out into the network. the congestion state of links is Initial State: Initially, unknown before its entries can be ﬁlled up through the piggybacked information. If, as in Figure 1, we signify a congested link as black and an uncongested link as white, we will assign the initial, unknown state of all links as an intermediate or nominal value, which is between the extremes (e.g. gray). Over time, as information about links is received, the gray links would subsequently be changed to a known state of a value lying between black and white. We experimented with initial states at the extremes, but either case caused a degradation in performance, relative gray. Congestion Information Staleness: It is always possible that some cycles after congestion state information about a given link is received, the condition of the network could change, despite a lack of new information about that link. Then the value present in the local congestion map would be stale, potentially leading to wrong decisions. To address information staleness, the congestion map employs a fading mechanism. If an entry in the congestion map has not been updated in the last n cycles, we fade (increment or decrement) it towards the gray status in steps of x. Intuitively, the link’s knowledge is now considered gray because we have not received any information about it over a considerable period of time. Discussion of the actual values used for n and x is provided in Section IV-B. D. Route Computation So far, we have described how the algorithm makes every node “globally aware” by employing a combination of piggybacking and the congestion map structure. The router employs a specially tailored shortest path algorithm to compute the best possible output port for the incoming trafﬁc using this information. Shortest Path Algorithm: Every link in the network is assigned a value which represents its congestion. A path between nodes is made up of multiple links and its congestion is the sum of the congestion of all those links. GCA picks the path with least congestion from the current node to every other node in the network from the set of minimal paths that are admissible. We can easily abstract this problem to ﬁnding the shortest path in a graph. Each edge weight is a congestion metric and the shortest path would denote the path whose sum congestion is the least. The graph analogy would make sure that all possible options are evaluated. We note, however, that minimal routing greatly reduces the size of the graph to be considered. Also, from each node’s perspective, the network can be partitioned into four parts (quadrants), such that each quadrant can be processed in parallel. Further, the graph is acyclic and directed with non-negative edge weights. Finding shortest paths in a graph is well documented with standard solutions (cf. Dijkstra [14]), which are widely used in network routing protocols like IS-IS [15]. As we need to implement route computation in hardware, however, we take advantage of the constraints described above to come up with a light-weight, hardware implementable algorithm. Congestion Information Scaling: Due to less frequent and high-latency congestion updates, the congestion map view will be less accurate for distant nodes. Furthermore, the likelihood that congestion status would change before a packet gets to the link increases considerably for distant nodes. To address both these issues, we assume that the choice of the ﬁnal path should be more inﬂuenced by the congestion status of nearby links than far away links. We achieve this by scaling every link’s congestion value with a scaling factor Si , where i denotes the distance from the source node in hops and w (w < 1) is an empirically determined constant, according to Formula 1. Si = 1 − (w ∗ i) (1) In this manner, the node’s own output links are not scaled (Si = 1) and the scaling increases in steps of w as you move away from the node. For all the links with i ≥ 1/w , the value of Si is ﬁxed at w as we do not allow negative congestion metrics. Route Computation Example: Here we illustrate the GCA algorithm. Consider the 4x4 mesh in Figure 5a. In the ﬁgure, the number in the top left corner of each node represents the node ID. We take node 5 to be the source node, searching for the preferred output port for packets destined to every other node in the network. The congestion values from the congestion map (Section III-C), for every link, are shown in the numbers on the links. We assume they have already been scaled as shown. The local node has no cost associated with it, therefore the cost of reaching itself is assigned zero. For all other nodes, we assign a value of inﬁnity. We also deﬁne an optimal output port with respect to the local node. This output port corresponds to the calculated path from the local node to that node. The number inside the node is the shortest path’s congestion value in reaching that node while the letter in the node denotes the optimal output port from the local node (N:North, E:East, W:West, S:South). Nodes on the same X and Y dimension as the source node are termed dimension nodes (shaded in Figure 5a). Trafﬁc within each quadrant is restricted within that quadrant by the minimal routing constraint. Thus we may perform parallel computation of the paths in each quadrant as they are independent of each other. We ﬁrst describe the route computation algorithm at a high level and list the “rules” it enforces in order to keep the implementation simple. • Every step of the routing algorithm, we update a set of nodes with the congestion value of their shortest path from the source node and the output port, which corresponds to this path. • The congestion value of the shortest path to a node is used in computing the shortest path of the neighboring nodes accessible from it. • For every node X, we can deﬁne at most two feeder nodes (i.e. the nodes that can send trafﬁc to node X ). • The orthogonal dimension nodes form a special case because they have only one legal output port from the source node and the congestion of their shortest path is simply the sum of the scaled congestion values of the links connecting such a node to the source. • Due to the straightforward, static nature of computation for the dimension nodes, their output ports are hard coded and their shortest path distances are updated whenever their feeder links are updated. • For all other nodes, route computation consists of a simple add and compare step. As every node has at most two options, the algorithm compares the congestion seen by those two paths and picks the lesser one. Given these rules, the initial starting state of the computation mesh is as shown in Figure 5a. We now illustrate the route computation technique step by step for the mesh in Figure 5: (a) Initial starting state of the mesh (b) After step 1 (c) After step 2 (d) End of computation Fig. 5: Step by step illustration of the route computation technique for a 4 X 4 2-D Mesh 1) In the ﬁrst step, all the nodes which can be reached from the dimension nodes only are “ready”. Consider the case of node 10 in the Figure 5a, which can be reached through either node 6 or node 9. • The cost of reaching node 10 through node 6 is: Cost(6) = 2 + 2 = 4. Similarly, for the path through node 9: Cost(9) = 6 + 3 = 9. Comparing C ost(6) and C ost(9), the cost to reach node 10 is least through node 6. We therefore, assign a cost of 4 for reaching node 10 and the EAST output port is carried over from node 6. A similar operation is carried out for all the other “ready” nodes and at the end of this step, we have Figure 5b. 2) For the next step, we have a new set of nodes, which are ready to be evaluated using the results of the previous step. Continuing in the same quadrant for our example, we illustrate the same operation for node 14 whose feeder nodes are node 10 and node 13. • For the path through node 10, the cost to node 14 is: Cost(10) = 4 + 4 = 8. Similarly, for the path through node 13: Cost(13) = 8 + 4 = 12. Comparing C ost(10) and C ost(13), the cost to reach node 14 is least through node 10. We therefore, assign a cost of 8 for reaching node 14 and the EAST output port is carried over from node 10. At the end of this step, we have Figure 5c. 3) At this point, we only have one node left (node 15) whose path still needs to be computed. Repeating the operation described above, we calculate that the minimum cost to reach it is 8 through node 11 and it is assigned an output port of EAST. Figure 5d shows the ﬁnal state after the algorithm has computed the optimal output ports for all the nodes in the network. steps is (2 ∗ √ For a mesh of N nodes, the maximum number of required N /2(cid:99), otherwise it is (cid:98)√ N ) − 3. If N is even, the lower bound is N /2(cid:99) − 1. Whenever the node updates one of the links in the map, our route computation module only needs to recompute a subgraph of the network. This is because the trafﬁc ﬂow from the source node is unidirectional and thus the change can potentially affect only the nodes downstream of that link. (cid:98)√ √ IV. IM P LEM EN TAT ION A. Baseline Adaptive Router We use the baseline adaptive router architecture proposed by Kim et al. [5]. This design reduces the router pipeline to three stages by pre-computing the optimal output port for the Fig. 6: GCA router microarchitecture incoming ﬂit. The congestion information from neighboring nodes at each output port is stored in a structure called the Congestion Value Registers (CVRs), which are used in the port preselect stage to pick the least congested option. B. GCA Router Microarchitecture Here, we present the GCA router microarchitecture. The GCA router is shown in Figure 6, with the GCA speciﬁc additional hardware components highlighted in gray. At the skeletal level, the router’s microarchitecture is the same as that of the baseline adaptive model. Instead of using CVRs to store local congestion status, however, the router needs a larger structure to store information about the complete network. Despite the larger structure, look-ahead routing ensures that the router pipeline is not adversely effected as we will discuss in the next section. In addition to introducing new storage elements, the algorithm also introduces a route computation module in order to use the congestion map and build the optimal output port case for all the other nodes. 1) Storage Requirement The congestion map holds congestion status information about all the links in the network. We normalize the value of the congestion metric to a scale of 0-7, using 3 bits. Since it is possible that up to four packet headers with trafﬁc vectors could arrive at any given cycle, a ﬁve entry queue is placed at the input to this structure. In the unlikely event packet headers come in faster than the vectors can be processed, the new vectors are dropped, with some degradation in the congestion map quality. For a network with N nodes, this structure would N − 1) bits. use 6 The result of the route computation is stored in an optimal output port table. The computation of the complete route helps in implementing the look-ahead route computation, which √ √ N ( TABLE I: Summary of number of bits of storage overhead for a 64-node network. Storage Element GCA LGCA Congestion Map Optimal Output Port Table Flag Array 336 49 112 72 49 24 TABLE II: Summary of values chosen for the design parameters for the purpose of performance evaluation. Design Parameter Size of window for fading (n) Increment/Decrement step (x) Scaling constant (w) Value 100 cycles 1 unit 0.1 √ √ √ N ∗ ( N + 1 − 2 enables pre-routing in the next hop. To do this, we store the output port decision that corresponds to the next hop and embed this information in the packet. As GCA employs perhop minimal routing, we only need one bit per destination. We do not save anything for the dimension nodes as they have only one valid output port. The overhead of this structure is N bits for an N node network. This structure is replicated per port to allow parallel reads. The last storage structure the ﬂag array, used to implement the fading mechanism (Section III-C). To avoid making decisions based on stale information, we use a ﬂag bit for every entry in the map, which denotes whether or not this value was modiﬁed in the last n cycle window. At the end of every n cycles if the ﬂag bit is set for any entry in the map, that entry is faded towards the value of 4 in steps of x. For large networks, performing the fading operation for all entries in one go could be a slow process. We propose to stagger the fading operation by applying the above algorithm for one entry in the map every n/l cycles where l is the number of links in the network. The values of n and x were empirically determined to be 100 cycles and 1 respectively. This ﬂag array adds 2 N − 1) bits. Table I summarizes the storage overheads required for the GCA technique for a 64-node 2D-Mesh network. 2) Route Computation The route computation hardware consists of a very simple, basic logical block that uses the stored values in the congestion map to compute the best case output port for every node. As described in section III-D, the route computation algorithm performs two additions to determine the possible path costs for reaching a particular node and then compares the results to pick the optimal one. For nodes that are feeders to other nodes in the network, the resultant optimal path cost and optimal output port choice feed through to the computation basic block of its downstream nodes. For any node of the mesh, which is not a dimension node, the block uses adders to add the cost of reaching its feeder nodes to the cost of the link connecting the node to its feeder. As all the evaluated nodes have two feeders, we end up with two cost values that are compared to pick the lower one and also to pick the corresponding winner’s output port to carry forward as the output port for this node. One copy of this basic block is instantiated for every node for which route computation has to be performed, such that two of these blocks feed into the downstream block as its inputs. As the whole circuit is combinatorial, there is no requirement √ of storage except for the port decisions that are made. If we consider a NxN mesh, we need to do the route computation for N − (2 N − 1) nodes and would require as many basic blocks. Crucially, the route computation in GCA is not part of the router pipeline, but rather operates in parallel to the router’s critical path. New, incoming information is not used for route computation of the packet that carried it, as this would be useless since it covers the reverse direction. Incoming packets are routed via previously computed routes, stored in the route table, based upon prior incoming status information. Thus GCA adds no latency to the trafﬁc and can be implemented in aggressive router designs. The weight w for the scaling factor as deﬁned in Equation 1 was empirically determined to optimally be 0.10. Whenever we update a link’s congestion value in the congestion map, we can scale and store it because it is predetermined due to a static network topology. The different design variables introduced for GCA act as tuning knobs for the algorithm. We performed experiments spanning the range of legal values for all the design parameters over different workloads and the ﬁnal values that we selected provide the best performance. Table II summarizes these optimal values for a 64-node network employing GCA. C. Limited GCA (LGCA) The hardware overhead incurred by GCA is a function of the number of nodes in the mesh. As the network grows, the storage requirements increase, as does the size of the route computation hardware and the size of the trafﬁc vector. To address the scalability of GCA, we propose a constrained GCA for large networks (greater than 64-node), Limited GCA (LGCA). LGCA maintains congestion information for a limited window of j hops around the current node. This works because the link state of nodes closer to the current node is both more likely to be accurate and more likely to be used by a packet going through the current node; a factor GCA implicitly acknowledges in the algorithm’s scaling technique; which assign a lower importance to congestion information which is far away from the node. We further note that as the packet traverses the network, subsequent nodes will have congestion information in their windows, missing in the current node to assist in routing. The scaling factor from Equation 1 simply uses a Si = 0 for all links that lie outside this window. Table I also shows the overheads of an LGCA implementation where j = 7. D. Synthesis Experiment To evaluate the feasibility of this technique, we performed synthesis for a 64-node network with j = 4 for an LGCA implementation using Synopsys Design Compiler. We built in all the storage and computation elements for implementing GCA in the baseline router and found that the incurred area overhead is negligible (less than 1%) on a 45nm process technology. The synthesized route computation unit is capable of running at 2GHz. For designs where the size of the route computation unit exceeds our experimental 16-node design, the design is easily pipelined by adding latches at clock boundaries to stagger the computation over multiple cycles. V. EVALUAT ION A. Methodology This section presents the performance results of the GCA routing algorithm on different synthetic trafﬁc patterns and TABLE III: Simulation parameters Parameter Network Size Router uarch Per hop latency Virtual channels/port Flit buffers/VC Link width Trafﬁc workload Duration of simulation Synthetic trafﬁc SPLASH-2 8x8 2D Mesh Two stage speculative 3 cycles:2 in router; 1 to cross channel 8 5 128b Uniform random, Transpose and Bit-complement 10000 warmup cycles followed by 100000 packets 7x7 2D Mesh Same Same 8 5 128b SPLASH-2 traces 10 million cycles or end of trace (a) SPLASH-2 benchmark results (b) Multiple region performance a suite of realistic workloads. The results are compared against oblivious routing as well as local and regional adaptive techniques. We also present our results from the network sensitivity studies that we performed. We employ a C++ based cycle-accurate on-chip network simulator [16] that models the two-stage baseline router microarchitecture as described in section IV-A. All the new structures as described in section IV-B were built into the simulator and the congestion of the links is denoted by the amount of free virtual channels in the downstream node of the link. In designs with low VC count, the number of free buffers could be used for greater resolution (as in RCA [10]). There are three algorithms that GCA competes against: (1) DOR, an oblivious routing algorithm for 2-D meshes; (2) Local, a local adaptive routing technique; (3) RCA-1D, a regional congestion aware adaptive routing technique (all further reference to RCA refer to RCA-1D). As all competing algorithms present their results in terms of average packet latency, we use the same metric to be able to perform a fair comparison. GCA is also compared against another globally aware routing scheme, DAR [13] under realistic workloads. Table III details all the parameters that are used to conﬁgure the network for our simulations. B. Realistic Trafﬁc To demonstrate the effectiveness of GCA on realistic workloads, we simulate traces of the SPLASH-2 suite of benchmarks [3]. These trafﬁc patterns represent typical scientiﬁc workloads. The traces correspond to a 49-node shared memory CMP organized in a 7x7 2D Mesh topology [17]. Figure 7a shows the average packet latency for the seven benchmarks. The numbers are normalized to the average packet latency of the DOR algorithm. Using Gratz et al.’s [10] methodology, we group the SPLASH benchmark trafﬁc into contended and uncontended benchmarks. The results show a 44% overall beneﬁt on average packet latency for GCA over DOR, 26% over local, 15% over RCA and 8% over DAR. The best case is seen for the benchmark water-spatial where GCA performs 51% better than RCA and 53% better than DAR on average packet latency. More imporFig. 7: Performance evaluation on SPLASH-2 tantly, GCA improves latency ∼27% on water-nsquared and by 4% on lu over RCA. These two contended benchmarks do not see much improvement for RCA over local; GCA shows the real beneﬁt that is derived by increasing the awareness that every node has about the congestion status of the network. Comparing the performance of DAR and GCA over the four benchmarks - fft, radix, ocean and barnes, we ﬁnd some interesting details. DAR outperforms GCA on fft and radix while GCA comfortably beats DAR on the other two. In fact, DAR performs worse than local adaptive on ocean and barnes. These results highlight the difference between the two globally-aware techniques. fft and radix both have stable trafﬁc patterns, in these cases it appears DAR’s stochastic trafﬁc distribution improves latency by reducing route thrashing. Radix and ocean have more rapidly changing trafﬁc patterns where DAR’s slow route calculation and propagation can not keep up. GCA’s faster congestion propagation and route computation ensures that it is competitive on all workloads and has a better mean performance. C. Multiple Region Performance As described in section II, regionally aware adaptive techniques suffer from aggregating excess information and are unable to offer isolation for consolidated workloads. Here we evaluate the performance of GCA in a scenario where there are different trafﬁc patterns in different partitions of a larger network (i.e. multiple of virtual machines). We simulated SPLASH trafﬁc on a 14x14 mesh, subdivided into four 7x7 meshes. Each sub-mesh runs a different trace and the trafﬁc from one sub-mesh does not cross over to another. GCA naturally constrains route calculation to only links relevant to the packet destination. Hence, it should avoid interference from congestion values in the different partitions. Figure 7b shows the results from one of the combinations, contrasted with the performance of the same benchmarks in isolation. The performance of local adaptive and GCA remains unaffected relative the isolated versions of the same benchmark as expected. RCA-1D, however, is adversely affected in this scenario as it would always consider congestion information of other sub-meshes thereby degrading its performance. GCA (a) Uniform random on 8x8 mesh (b) Transpose on 8x8 mesh (c) Transpose on 16x16 mesh Fig. 8: Load latency graphs for synthetic workloads. performs better than RCA by 20% on this combination of benchmarks due to the fact that it takes into consideration the locality of trafﬁc in the sub-meshes. It still outperforms local by 9% due to the fact that it is globally adaptive in that submesh. D. Synthetic Trafﬁc We evaluate the techniques under uniform random and transpose trafﬁc patterns. The load latency graphs for these trafﬁc patterns are presented in Figure 8 and we consider saturation bandwidth to be three times of zero load latency. For the transpose trafﬁc pattern, DOR causes load imbalances that are greatly corrected by introducing adaptive routing. We see local shows a huge improvement in the saturation bandwidth and it further improves as the adaptive routing algorithm’s awareness increases. GCA is able to achieve a 5% improvement in throughput over RCA without sacriﬁcing latency. In uniform random trafﬁc, adaptivity does improve performance over oblivious routing but the amount of improvement that can be exploited by increasing awareness is limited and that’s why there is not much of a difference between local, RCA or GCA. However, GCA still offers the best throughput without sacriﬁcing latency. E. Sensitivity to Network Design Point In this section, we evaluate the GCA algorithm’s sensitivity to network size. We choose the transpose trafﬁc pattern as it shows a clear distinction in performance for GCA and compare it against local and RCA. As Figure 8c shows, GCA outperforms local by 21% in a 16x16 mesh as compared to 15% in a 8x8 mesh. As the size of the network increases, locally adaptive algorithms base their decisions on a much smaller subset of links thereby making even more sub-optimal routing decisions as they would in a medium sized network. GCA shows a greater performance advantage than on 8x8 network by providing a complete view of the network. V I . CONC LU S ION In this paper we present a novel routing adaptive routing technique for on-chip networks, which aims to make routing decisions based on global knowledge of network state, Global Congestion Awareness (GCA). GCA is a light-weight, lowcomplexity adaptive routing algorithm, which makes per-hop routing decisions based upon awareness of the congestion of links throughout the network. It differs from other globally aware routing schemes in that it utilizes the existent packets within the network to convey (“piggyback”) congestion information instead of requiring a sideband network dedicated to congestion status propagation. This makes it a more scalable solution than other existing techniques. Our experiments show that GCA consistently performs better than Regional Congestion Awareness (RCA-1D) on a variety of workloads. On SPLASH-2 trafﬁc, GCA is 51% better in latency over RCA in the best case and 15% better on average. It also betters a competing globally aware routing algorithm, DAR, by 8% on average on realistic workloads. "
A greedy approach for latency-bounded deadlock-free routing path allocation for application-specific NoCs.,"Custom network-on-chip (NoC) structures have improved power and area metrics compared to regular NoC topologies for application-specific systems-on-a-chip (SoCs). The synthesis of an application-specific NoC is a combinatorial problem. This paper presents a novel heuristic for solving the routing path allocation step. Its main advantages are the support of realistic nonlinear cost estimation and the ability to handle latency constraints, which guarantee high performance of processing elements sensitive to communication delays. Additionally, the method generates deadlock-free routing by avoiding cycles in the channel dependency graph. The NoC is constructed sequentially in a greedy manner by selecting the routing path for each communication flow in such a way that the additional NoC HW resources are kept minimal. The routing path is found using a binary search cheapest bounded path (BSCBP) algorithm. The method is highly efficient and provides a NoC routing path allocation for a smart phone SoC with 25 processing elements and 96 flows in less than a minute.","A Greedy Approach for Latency-bounded Deadlock-free Routing Path Allocation for Application-speciﬁc NoCs Amit Verma†, Pritpal S. Multani†, Daniel Mueller-Gritschneder†, Vladimir Todorov‡ , Ulf Schlichtmann† † Institute for Electronic Design Automation, TU Muenchen ‡ Intel Mobile Communications GmbH amit.verma.nitrkl@gmail.com, daniel.mueller@tum.de, vladimir.todorov@intel.com, ulf.schlichtmann@tum.de Abstract—Custom network-on-chip (NoC) structures have improved power and area metrics compared to regular NoC topologies for application-speciﬁc systems-on-a-chip (SoCs). The synthesis of an application-speciﬁc NoC is a combinatorial problem. This paper presents a novel heuristic for solving the routing path allocation step. Its main advantages are the support of realistic nonlinear cost estimation and the ability to handle latency constraints, which guarantee high performance of processing elements sensitive to communication delays. Additionally, the method generates deadlock-free routing by avoiding cycles in the channel dependency graph. The NoC is constructed sequentially in a greedy manner by selecting the routing path for each communication ﬂow in such a way that the additional NoC HW resources are kept minimal. The routing path is found using a binary search cheapest bounded path (BSCBP) algorithm. The method is highly efﬁcient and provides a NoC routing path allocation for a smart phone SoC with 25 processing elements and 96 ﬂows in less than a minute. I . IN TRODUC T ION Networks-on-chip (NoCs) provide better scalability for future systems-on-a-chip (SoCs) than traditional bus-based communication architectures. Much research has been conducted on the topic of routing algorithms for regularly structured NoCs such as meshes, tori, stars or fat-trees. These regular structured NoC topologies are well suited for homogeneous many-core general-purpose systems. However, SoCs, for which the application is known at design time, beneﬁt from application-speciﬁc NoC structures, which outperform the regular topologies in terms of area and power consumption. These application-speciﬁc SoCs can supply communication between heterogeneous cores such as general purpose processors, DSPs, HW accelerators, memories, modems or serial interfaces in a cost efﬁcient way. Synthesizing application-speciﬁc NoC structures requires design choices such as selecting the number of routers for the network, connecting the cores to the appropriate routers, and allocating routing paths for the communication ﬂows. These choices can be made at design time by the use of a predeﬁned communication proﬁle of the application. Such a proﬁle contains multiple different use cases, which model the real life scenarios of the SoC usage. For a smart phone SoC, one use case may describe Internet surﬁng by using an LTE connection, another a simple phone call, yet another video capturing with the phone’s camera. Each use case may result in different communication patterns between the SoC’s cores. The synthesized NoC structure must support the required bandwidth to route all these communication ﬂows. It also has to be optimal in terms of area or/and power. Additionally, performance of many cores is very sensitive to communication delays in the network (latency). For example, a processor’s performance drops rapidly with increasing latency for loading data from memory after a cache miss. Therefore, latency constraints must also be included in the communication proﬁles. This paper proposes a novel heuristic approach for the allocation of routing paths for the communication ﬂows in an application-speciﬁc NoC with deterministic routing. It selects a routing path for each ﬂow in a greedy manner, such that the additional hardware cost is kept minimal, thus optimizing power and area of the NoC. This step requires that the system has already been partitioned into groups of cores, connected to individual routers. This partitioning can be performed on the basis of communication bandwidth, latency, physical distance and power domains. This work utilizes spectral clustering as proposed in [1]. The routing is then constructed using the binary search cheapest bounded path (BSCBP) algorithm inspired by the LARAC algorithm [2], such that paths with minimal cost that also satisfy the latency constraints (e.g. number of hops) are chosen. This approach has several advantages: • It runs in a polynomial time because of its greedy manner. • It supports any nonlinear costs estimation based on the used NoC router technology. • It handles latency constraints together with NoC cost during path allocation. • It avoids deadlocks due to cycles in the channel dependency graph by rerouting ﬂows. The routing paths for 96 ﬂows in a smart phone SoC with 25 cores can be generated in less than a minute. For this test case, the nonlinear HW cost computation is based on a router model for wormhole ﬂow control. It includes analytical buffer size estimation based on the ﬂow bandwidths, packet arrival rates and possible congestion at output ports of the router. This part of the cost is essential because the buffers are the main contributor to the NoC’s area. The proposed algorithm is able to ﬁnd a Routing Allocation that lead to about 20% estimated cost improvement compared to straight forward shortest path routing. The remainder of the paper is structured as follows: Section II gives an overview over related work, Section IV brieﬂy explains how the cores are mapped to different routers. Section V describes the proposed novel routing path allocation algorithm, Section VI shows experimental results for randomly generated SoCs of different complexity as well as an industrial smart phone chip, and Section VII presents the conclusions. I I . R ELAT ED WORK Routing Path Allocation (RPA) during application-speciﬁc NoC synthesis is an N P -hard problem. Different approaches exist to compute a good quality solution in reasonable time. In [3], methods for energy-aware system partitioning and RPA are presented. That RPA method is based on dynamic programming and uses energy consumption as cost metric. It does not consider any latency bounds. In [4], [5], a linear cost model is used to apply mixed integer linear programming (MILP) to allocate routing paths. In [6] a Modiﬁed Shortest Path (MSP) algorithm is presented as part of a NoC synthesis based on a Genetic algorithm (GA). It also routes ﬂows sequentially taking into account the routing paths of previous ﬂows. It does not guarantee that latency constraints are met. The method also does not explicitly avoid deadlocks, instead it inserts virtual channels if required. Another greedy RPA method for mesh topologies is described in [7]. The paper presents an idea that a constrained shortest path problem can be solved by a linear combination of the latency and cost. The same idea is used in our approach to achieve routing paths under latency constraints for application-speciﬁc NoCs by using an algorithm inspired by the Lagrangian Relaxation Based Aggregated Cost (LARAC) algorithm. In [8] and [9] the authors use linear programming (LP) and mixedinteger linear programming to approximate the solution of the topology synthesis problem. These approaches, however, do not consider delay constraints and multiple use cases. An algorithm with latency constraints is presented in [10]. In [11] a deadlock-free routing algorithm is presented that already assigns marginal cost to the edges in the network graph. It uses a similar restriction to the routing to avoid deadlocks. In [12] the authors use tabu search to ﬁnd a solution to the application speciﬁc NoC synthesis problem. The method presented there uses a single use case speciﬁcation and achieves deadlock avoidance by insertion of virtual channels and takes care of satisfying latency constraints. The method iteratively partitions the available routers until it ﬁnd a feasible solution. In contrast, the approach presented in this paper utilizes explicit spectral clustering to partition the system and assign the cores to routers. Furthermore, it uses an iterative algorithm to ﬁnd the cheapest delay constrained paths in the routing. The search is performed on a channel routing graph and deadlocks are avoided by referencing a channel dependency graph, which indicates the occurrence of a deadlock. The channel routing graph allows the assignment of router cost for different pairs of input/output channels and to set continuous latency constraints on the ﬂows. I I I . SOC S PEC I FICAT ION Firstly, the application-speciﬁc SoC is speciﬁed by a set of communication patterns. This speciﬁcation can be given in the form of Core Communication Graphs (CCGs). Each CCG C CGx (Cx , Fx ) describes a use case, where ∀ci ∈ Cx is a core and ∀fk ∈ Fx is a directed communication ﬂow. Each ﬂow is described with a tuple fk = (sk , dk , βk , k , δk ), where sk is its source core, dk is its destination core, βk is the average bandwidth in MBps, k is the maximal tolerated latency ( in ns, number of hops...), and δk is the ﬂow’s packet size distribution. An example speciﬁcation of an SoC with 8 cores is illustrated in Fig. 1. The speciﬁcation contains two use cases described by two distinct CCGs. The ﬁrst one (Fig.1(a)) involves the cores c0 , c1 , c2 , c3 , and c5 and the ﬂows f0 to f5 which form the communication pattern between the cores. The second use case (Fig.1(b)) involves the cores c0 , c1 , c3 , c4 , c5 , c6 , and c7 and seven ﬂows f6 to f12 . c2 f3 c3 f0 c0 f1 f2 f4 c1 c5 f5 (a) s f8 c4 f6 f7 c5 f12 c0 f11 f10 c6 (b) c3 f9 c7 Fig. 1. Two use cases specifying an SoC IV. SY ST EM PART I T ION ING The ﬁrst step of application-speciﬁc NoC generation requires associating the cores with speciﬁc routers. This is done by partitioning the cores into groups G = G1 , ..., Gw . The cores inside each group Gw are connected to the local ports of one network router. Often, min-cut max-ﬂow methods based on communication bandwidth are used to ﬁnd a suitable partitioning. Additionally, latency, position on the ﬂoorplan and voltage-frequency islands might be considered during system partitioning. In this work, the spectral clustering approach from [1], [13] is used. It ﬁnds a grouping of the cores, such that the local communication between cores inside one group is maximized and the trafﬁc between the groups is minimized, while taking the latency constraints into account. Thus, it tries to construct the grouping in such a way that Eq.1 is minimized for every two groups Ga and Gb . In the equation cut(Ga , Gb ) is the inter-group communication and vol(G) is the communication inside a group. N cut(Ga , Gb ) = cut(Ga , Gb ) vol(Ga ) + cut(Ga , Gb ) vol(Gb ) (1) In addition, the method can handle multi-use case speciﬁcations and can automatically estimate the number of partitions in the SoC. Fig. 2 illustrates an example system partitioning and routers derived from the speciﬁcation in Fig.1. Based on this partitioning the set of ﬂows can be divided into two non-intersecting subsets. The ﬁrst set Fin = {fk |sk , dk ∈ Gv } contains all intra-router ﬂows. The second one Fout = {fk |sk ∈ Gv , dk ∈ Gw , v (cid:54)= w} contains all inter-router ﬂows. Routing a ﬂows in fk ∈ Fin is trivial and it is done only via the router in the corresponding group where fk occurs. Routing of the ﬂows G0 r0 c0 c1 c2 c7 G2 r2 c6 c4 r1 G1 c5 c3 Fig. 2. Example partitioning of an SoC and the corresponding routers The directed edge ex,y = (lx , ly ) between two channels lx and ly represents a routing step, for which the ﬂow fk is forwarded from lx to ly by the router between them. Each cycle-free path from the source channel ls to the destination channel ld represents a possible routing path for the ﬂow. The number of possible routing paths increases exponentially with the number of routers. Fig. 4 shows the CRG for ﬂow f10 of the example from Fig.1. There are two possible routing paths for f10 . The channels on those paths are labeled in Fig. 3. in Fout , however, is a much harder task as multiple possible paths between the routers have to be considered. ls l0 l2 l5 l4 l3 l1 ld c5 l5 c4 r1 l1 l4 l2 l3 c3 l0 c2 r0 c1 c0 c6 r2 c7 Fig. 3. Example of a system with all possible channels between the routers V. GR EEDY ROUT ING PATH A LLOCAT ION The task of RPA is to ﬁnd a routing path for all ﬂows fk ∈ Fout such that the latency constraints are met, the routing is deadlock-free and the cost of the NoC structure is minimal. This paper proposes an RPA method that greedily inserts the ﬂows into the NoC such that the cost of the required additional hardware is minimal. For each ﬂow fk ∈ Fout , a cheapestbounded-path algorithm based on the Dijkstra’s [14] and LARAC [2] methods is used to ﬁnd the optimal routing path in the channel routing graph (CRG). The weight of each edge and node in the CRG is a linear combination of the delay on the NoC channels or routers and the hardware cost. Additionally, the channel dependency graph (CDG) is analyzed to avoid any deadlocks. The CDG is a directed graph which contains all channels as nodes, but initially no edges between them. An edge between two channels in the CDG indicates a dependency between them. Thus, a cycle in the graph indicates a deadlock. Based on this the proposed algorithm checks whether selecting a link will cause a deadlock. After a ﬂow fk is routed the CDG is updated by inserting the edges corresponding to the path of fk . The details of the proposed greedy RPA method are described in the following subsections. A. Channel Routing Graph (CRG) In the CRG CRGk (Nk , Ek ) of a ﬂow fk , a node lv ,w represents the inter-router channel between routers rv and rw as depicted in Fig. 3. The node ls represents the local channel from the source core of the ﬂow and ld the local channel to the destination core. Fig. 4. CRG graph for ﬂow f10 from Fig.1(b) B. Edge Labeling In order to select the optimal routing path for a ﬂow fk , each edge ex,y =(lx ,ly ) of the CRG is labeled with a delay αx,y and a cost ∆κx,y . The value αx,y of an edge ex,y is the sum of the predeﬁned delay (in hops or seconds) on the channel lx and the router ri , to which it serves as an input: αr,k = α(er,k ) = α(lx , ly )) = αlx + αri (2) The value of ∆κx,y is the cost of the additional hardware that needs to be added to support the ﬂow fk on the lx and on the subsequent router ri as well as the cost to route the ﬂow to the next channel ly using the router between the two channels. Therefore, it is expressed as the difference between the cost having the already inserted the new ﬂow and the cost without the new ﬂow ( the old cost ): ∆κx,y = ∆κ(ex,y ) = ∆κ(lx , ly ) = = κnew (lx , ly ) − κold (lx , ly ) (3) The contributors to the cost are the switching matrix size of the router, the estimated buffer sizes of the different ports and the port bitwidth (channel bitwidth) of the router. Additional hardware resources may be required because: • the input or output channel were not yet used and need to be added to the router increasing the switching matrix size, • the buffer size on the used input channel must be increased, • the buffer size on another input channel needs to be increased due to congestion on a shared output channel or • the port size of the router must be increased to support the new maximal bandwidth on the input or output channel. For example, if the depicted edge e0,5=(l0 ,l5 ) is included in the routing path, l0 is used as an input channel and l5 as an output channel for router r1 . To calculate the additional cost, ﬁrst the old cost hardware cost κold (l0 , l5 ) is computed from all ﬂows previously routed on channel l0 and all ﬂows Router r1 to c4 B1 B2 B3 to c3 to c5 B4 to l5 from l4 (a) 4-por t router to c4 Router r1 to c3 B1 B5 from l0 to l1 B2 B3 to c5 to l5 from l4 B4 (b) 5-por t router Fig. 5. Channel construction - only if ﬂows use it that use the router r1 . Next, the ﬂow f10 is added to the existing ﬂows that use the channel and the router. Thus, a new hardware cost κnew (l0 , l5 ) ≥ κold (l0 , l5 ) is derived. ∆κ(l0 , l5 ) is computed from these two according to Eq.3. If the channel l1 has not yet been used, the cost is equal to κnew (l0 , l5 ), which represents the complete hardware to create the channel and route the ﬂow because κold (l0 , l5 ) equals zero. For example, in Fig. 5(a) the port interfaced with the channels l0 and l1 has not been created. Thus, the cost ∆κ of routing a ﬂow through it will equal the complete hardware needed to instantiate this port for the ﬁrst time. In Fig. 5(b), where the port is already present, ∆κ will represent only the additional hardware for routing the ﬂow through the port interfaced with l1 and l0 . C. Buffer Estimation Producing adequate buffer estimates is not a trivial task. For example, ﬂows that use the same router may affect each other, thus, increasing the requirements of buffer space of the router as a whole. Figure 5(b) show the ﬂows coming on different input ports, which interfere with each other while passing through the router. In the ﬁgure, a 5-port router is depicted, where B1 to B3 are the input buffers of the different ports. The ﬂows coming on B0 and B2 are interfere with each other, while accessing c4 . This contention contributes to the increasing both the sizes of B1 and B3 . Thus, adding a ﬂow to one input channel of the router may or may not increase the buffers on multiple input channels of the same router, depending on the output channel chosen by the ﬂow. To cope with this problem, this work uses the the proposed apporach from [15] to estimate the buffer requirements and the bitwidth of the ports for each ﬂow that is to be routed. The authors in [15] present an analytical method for estimating the average buffer sizes of NoC routers using queuing theory. To estimate the buffer sizes of the router, the method uses the clock frequency of the router, the port bitwidth of the router w , the ﬂows on the router, together with their input and output channels and packet distributions δ . The buffer cost is combined with the cost of the switching matrix size to estimate the cost of routing a ﬂow and to properly scale the port sizes. D. Cost computation The advantage of the proposed method is that it can be used to compute incrementally the non-linear cost estimate of the buffers. Furthermore, it allows computation of the tradeoff between buffer size and port bitwidth, which allows comparing the cost of the switching matrix and the estimate buffer sizes. This is due to the fact that the switching matrix cost increases with increases port width, but the buffer requirements decrease due to increased throughput. In this work, the buffer estimates, port bitwidths and switching matrices are recomputed for each ﬂow that needs to be routed. The cost of a switching matrix (κSM ) can be obtained from the bitwidth of the port (w), the number of output channels (out) and the number of input channels (in) as shown in Eq. 4. It roughly represents the number of gates needed to construct the matrix. The cost of the buffers κBU F is computed according to Eq. 5, where sizeBU F is the buffer size in number of bits and the factor 10 is an industrial rule-of-the-thumb used to estimate the number of gates needed for a ﬂip-ﬂop. κSM = out × w × (in − 1) + (w − 1) × out × (in − 1) ≈ ≈ 2 × out × in × w κBU F = 10 × sizeBU F (4) (5) Equations 4 and 5 are used to compute the cost κ(lx , ly ) = κSM (lx , ly ) + κBU F (lx , ly ). As both the estimation of the buffer size and the switching matrix use the bitwidth of the port in the cost computation, a tradeoff between the two can be computed. This is done by constantly increasing w until min(κSM + κBU F ) = min(κ(lx , ly )) is reached. Such an iteration is done when computing the new cost κnew (lx , ly ). It allows ﬁnding the minimum required ∆κ(lx , ly ) because it minimizes κnew (lx , ly ). E. BSCBP Algorithm The computation of the cost or delay on the channels and routers is done independently for each edge of the CRG for every ﬂow fk . The total cost or delay of any valid routing path of the ﬂow fk from its source channel ls to its destination channel ld is the sum of the edge costs or delays respectively along the path, which includes the required hardware and delays of all used channels and routers. The optimal routing path of one single ﬂow fk is the one that leads to minimal cost while assuring that the total delay on the path is smaller than the maximal tolerated latency k on the ﬂow. This is a constrained shortest path problem, which can be solved by our binary search cheapest bounded path (BSCBP) algorithm, which is inspired by the LARAC algorithm [2]. The pseudocode of the used method is shown in Algorithm 1. The BSCBP algorithm uses the linear combination of multiple edge weights to ﬁnd a total edge weight wx,y : wx,y (a) = a × dx,y + (1 − a) × ∆κx,y (6) The variable a is the scaling factor of the algorithm. The value of dx,y represents the delay of the edge ex,y , which in this work has the value of 1 hop. The algorithm, however, can work with an arbitrary delay measure. BSCBP ﬁrst runs Dijkstra’s algorithm with a = 0 to ﬁnd the shortest path for wx,y (0) = ∆κx,y between the source and destination channel in the CRG of the ﬂow. This will result in the cheapest routing path in terms of cost. If the total delay on this routing path is lower than the latency constraint, the path is optimal and the algorithm terminates. If the latency constraint is not met, it reruns Dijkstra’s algorithm with a = 1 such that wx,y (1) = dr,k . This will result in the fastest routing path for the ﬂow. If the latency constraint is not met, then the system partitioning must be changed because no valid routing path exists for this ﬂow. Otherwise, there exists a range of a values that will give the cheapest path that meets the latency constraint using Dijkstra’s algorithm with edge weights according to (6). Unlike the standard implementation of LARAC in [2], BSCBP uses binary search to ﬁnd an a value in this range as described in Algorithm 1(line 15-25). f10 in Fig. 6(a) and Fig. 6(b), representing respectively the CRG and CDG graphs of the ﬂow. There ﬂows f6 and f11 have already been routed. The routing path for f6 is l5 → l3 and for f11 it is l3 → l0 . Both lead to the dependencies in Fig. 6(a). Thus, when trying to route f10 the transition l0 → l5 will be prohibited because it will cause a deadlock. Hence, this transition and the subsequent ones origination after it will be invisible to the BSCBP algorithm, leaving only channel l2 as an option. This avoidance is achieved by labeling the nodes of the CRG with their predecessor nodes from the CDG. As the exploration of nodes proceeds with the BSCBP algorithm, labels are also propagated onto successor nodes(channels). The BSCBP algorithm is modiﬁed such that it checks the label of of the current node if it includes any of its successor nodes. Whenever a match occurs, the successor node is ignored. Once the a routing path is found, it is also used to update the CRG. The introduction of the deadlock constraint causes the path allocation algorithm to ﬁnd suboptimal paths as it is fundamentally based on Dijkstra’s algorithm. Algorithm 1 BSCBP Cheapest Bounded Path 3: a ← 0, i ← 0 1: PROC EDURE : BSCBP (CRGk , k , Imax ) 2: B EG IN 4: path ← Dijkstra(CRGk ,wx,y (a)) 5: if delay(path) ≤ k then return path 7: end if 6: 8: a ← 1 9: path ← Dijkstra(CRGk ,wx,y (a)) 10: if delay(path) > k then 11: return FAIL 12: end if 13: al ← 0, au ← 1 14: pathold ← path 15: κ(path) = −1 16: while i < Imax ∧ κ(path) (cid:54)= κ(pathold ) do a ← (al + au )/2 path ← Dijkstra(CRGk ,wx,y (a)) if delay(path) ≤ k then au ← a pathold ← path al ← a i ← i + 1 17: 18: 19: 20: 21: 22: 23: 24: 25: end if else 26: end while 27: return pathold 28: END F. Deadlock Avoidance To avoid deadlocks in the NoC, a channel dependency graph (CDG) is considered for each of the different use cases. It captures the sequence of link utilization by the different ﬂows when they are routed through the network. A cycle in the graph indicates the occurrence of a deadlock and, hence, must be avoided. The nodes in the CDG are the same as the nodes of the CRG, except for the local channels ls and ld , which are not present. Whenever a ﬂow uses two channels in sequence, a directed edge is added between the ﬁrst and the second of them, indicating a dependence. An example is given for ﬂow ls l0 l2 l5 l4 l3 l1 ld (a) CRG for f10 given the CRG of f10 l0 l2 l5 l4 l3 l1 (b) CDG for f10 with f6 and f11 already routed Fig. 6. Example of a routing restriction for ﬂow f10 G. Greedy Flow Insertion The previous subsections described how to construct the deadlock free routing under latency constraints for an individual ﬂow. To conduct the complete routing path allocation, the ﬂows are inserted into the NoC in sequence as shown in Algorithm 2. The allocation of routing paths for all intrarouter ﬂows fk ∈ Fl is trivial. The routing path consists of the router input channel coming from the source core and the output channel leaving to the destination core. The intra-router routing paths are saved in the set of routing paths Pr of already routed ﬂows Fr . Then, the NoC structure is updated such that it can support intra-router ﬂows. First, the ﬂows in Fg are sorted in descending order by their bandwidth requirements. The ﬂows are then routed in order. A CRG is generated for each ﬂow and the costs are computed for all edges in the CRG as described in the previous part of the paper. Its CRG is generated and the edge costs are computed based on the current NoC structure. The BSCBP Cheapest Bounded Path algorithm is run to ﬁnd the cheapest routing path that meets the latency constraint and is deadlock free. In this way, the routing path is chosen that leads to minimal additional costs. The found routing path is added to the set of routing paths Pr and the ﬂow to the set of already routed ﬂows Fr . The CDG of the corresponding use case is updated appropriately. This leads to a greedy approach that does not ﬁnd the optimal global routing allocation for all ﬂows, but leads to a very good solution in a very short time. The quality of the solution depends also on the choice of the order of the ﬂow insertion. In this paper, we investigate ﬂow ordering by latency constraints and bandwidth requirement. Algorithm 2 Greedy Flow Insertion 4: for ∀f ∈ S do 1: PROC EDURE : GF I (Fg ) 2: B EG IN 3: S ← sort(Fg ) for ∀ex,y ∈ CRGf do κold (lx , ly ) ← cost(CRGf ) κnew (lx , ly ) ← cost(CRGf , f ) ∆κ(lx , ly ) ← κnew (lx , ly ) − κold (lx , ly ) end for pathk = BSCBP (CRGf (Nf , Ef ), f , Imax ) Pr ← Pr ∪ pathk update(CDGf , pathk ) 5: 6: 7: 8: 9: 10: 11: 12: 13: end for 14: END H. Complexity Analysis For a partitioning with R routers the number of possible channels (nodes) in the CRG grows in the order of O(R2 ). Dijkstra’s algorithm has a complexity of O(N 2 ), where N is the number of nodes in the graph. The BSCBP algorithm, which is based on Dijkstra’s one, is run a maximum of Imax times. There are K ﬂows describing the SoC that need to be routed. Therefore, the total complexity of the presented greedy routing path allocation method is polynomial of the order O(K ImaxR4 ). V I . EX PER IM EN TA L R E SU LT S In this section, experimental results are presented to validate the presented routing path allocation (RPA) method. We compare the results to shortest path RPA. Shortest path RPA results in the fastest routing paths. These paths should always meet the latency constraints. The drawback of shortest path RPA is that it does not consider NOC cost, thus, leading to a possible rather expensive solution. In contrast, the presented greedy RPA using the BSCBP algorithm always looks for the cheapest route that still meets the latency constraint. With these cheap routing paths, the method is able to reduce the NoC cost at the price of a slightly overall higher bandwidth-hop product. As the experimental results show, the method ﬁnds a far better compromise than shortest path RPA. A. Testcases To evaluate the effectiveness of the RPA method several random system speciﬁcations have been constructed. Each of them is constructed with different complexity in terms of number of cores, ﬂows between the cores and number EX PER IM EN TA L FOR RANDOM LY GENERAT ED S PEC I FICAT ION S TABLE I Case: 5 Cores, 15 Flows, 3 Routers, 1 Usecase Algorithm Sort BW-hop prod. Cost Time [s] Shortest path Greedy B L 64.4 66.2 70.3 69.2 17136 16968 16968 16288 < 1 < 1 < 1 < 1 Case: 10 Cores, 30 Flows, 5 Routers, 2 Usecases Algorithm Sort BW-hop prod. Cost Time [s] Shortest path Greedy B L 51.0 56.4 56.0 53.0 17280 17064 18296 16408 < 1 1.53 2.33 1.82 Case: 15 Cores, 45 Flows, 7 Routers, 3 Usecases Algorithm Sort BW-hop prod. Cost Time [s] Shortest path Greedy B L 61.6 72.1 72.1 71.3 21880 20736 18472 19184 < 1 6.51 6.43 4.96 Case: 20 Cores, 80 Flows, 9 Routers, 4 Usecases Algorithm Sort BW-hop prod. Cost Time [s] Shortest path Greedy B L 65.5 80.0 78.4 86.3 44776 40176 38592 34544 < 1 48.36 46.7 44.1 Case: 40 Cores, 160 Flows, 15 Routers, 5 Usecases Algorithm Sort BW-hop prod. Cost Time [s] Shortest path Greedy B L 62.2 83.3 80.0 82.6 82224 61360 62608 49528 < 1 727 662 541 of routers. The results obtained from running the proposed routing method are compared against a shortest path approach, where inter-router ﬂows are routed directly between the communicating routers. Furthermore, different ﬂow orderings have been taken into account. Table VI-A shows the results obtained from the random speciﬁcations. In the table there are 5 random cases presented, starting from a very small one with 5 cores, 15 ﬂows and 3 routers to a complex one having 40 cores, 160 ﬂows and 15 routers. Both the combined switching matrix and buffer costs and the bandwidth-hop product are listed in the table. The bandwidth hop-product is a measure, which shows what amount of data is contained in the NoC on average at any point in time. Additionally, ”B” and “L”, show respectively sorting of ﬂows according to ﬂow with max. bandwidth ﬁrst and ﬂow with tightest latency constraint ﬁrst. As it can be seen from the table the proposed routing method is always better than shortest path approach except once. In this case the rerouting of ﬂows leads to higher cost. This shows that different orderings of the ﬂows should be investigated to ﬁnd always a better solution. However, as the complexity increases the proposed RPA method produces signiﬁcant cost gains with little increase on the bandwidth-hop product. Furthermore, in most cases sorting the ﬂows by either bandwidth or latency is more advantageous as compared to no sorting at all. Finally, the running time increases with the increase of complexity, but it is still in the order of several minutes. B. Smart Phone Speciﬁcation To test the feasibility of the method on a real SoC designs, it is applied to the speciﬁcation of an industrial smartphone SoC,which features 25 cores and 96 ﬂows. The SoC speciﬁcation includes ﬁve different use cases: Full HD Video Playback, Video Capture, LTE Mobile Internet Access, WiFi Mobile Internet Access, Video Streaming via LTE or WiFi and Idle. The NoC architecture must support all ﬂows of any of these use cases simultaneously. The SoC only operates one use case at a time, such that the NoC must not support the ﬂows of two or more use cases simultaneously. The frequency of the NoC is chosen to 500 MHz. The partitioning is done as described in Section IV. It ﬁnds nine groups of cores, thus 9 routers are used for the NoC. Table II shows the results for the routing path allocation step. Shortest path RPA ﬁnds the solution with the optimal bandwidth-hop product. However, all ﬂows are directly routed to the channel that leads to the destination router regardless of the NoC cost. This leads to minimal latencies for the ﬂows, but results in high HW costs due to the many required inter-router channels. The proposed greedy RPA approach leads to slight degradation in bandwidth-hop product compared to shortest path routing at a signiﬁcant improvement in the hardware cost of about 23% of the shortest path NoC cost. This is achived by rerouting ﬂows to channels that require minimal additional cost. As can be seen, the greedy algorithm with latency bounds gives a very good compromise between bandwidth-hop product and NoC cost. The resulting topology of the NoC is shown in ﬁgure 7. The topology presented is the one with the least cost from Table II. As can be seen, there is one central router that connects to the memory and the CPU. It is strongly connected with the remaining network because ﬂows from these two cores are most sensitive to latencies. This router would need to be designed carefully. EX PER IM EN TA L RE SU LT S FOR SMART PHONE S PEC I FICAT ION TABLE II Algorithm Sort BW-hop prod. Cost Time [s] Shortest path Greedy B L 154 156 158 163 31008 28024 23704 27672 < 1 47.8 28.3 36.7 V I I . CONC LU S ION S This work proposes a novel greedy algorithm for the routing path allocation problem in the design of custom NoCs. The work uses a multi-use case speciﬁcation of a SoC to produce and route a NoC, which satisﬁes all the use cases. It uses a greedy algorithm, which is deadlock free and satisﬁes latency constraints, combine with a cost function considering both switching matrix and buffer sizes to insert the links between the routers in a fast and cost-effective way. [1] "
On self-tuning networks-on-chip for dynamic network-flow dominance adaptation.,"Modern networks-on-chip (NoC) systems are required to handle complex run-time traffic patterns and unprecedented applications. Data traffics of these applications are difficult to be fully comprehended at design-time so as to optimize the network design. However, it has been discovered that the majority data flows in a network are dominated by less than 10% of the specific pathways. In this paper, we introduce a method that is capable of identifying critical pathways in a network at run-time and, then, can dynamically reconfigure the network to optimize for the network performance subjected to the identified dominated flows. An online learning and analysis scheme is employed to quickly discover the emerged dominated traffic flows and provides a statistical traffic prediction using regression analysis. The architecture of a self-tuning network is also discussed which can be reconfigured by setting up the identified point-to-point paths for the dominance data flows in large traffic volumes. The merits of this new approach are experimentally demonstrated using comprehensive NoC simulators. Compared to the conventional network architectures over a range of realistic applications, the proposed self-tuning network approach can effectively reduce the latency and power consumption by as much as 25% and 24%, respectively. We also evaluated the configuration time and additional hardware cost. This new approach demonstrates the capability of an adaptive NoC to handle more complex and dynamic applications.","On Self-tuning Networks-on-Chip for Dynamic Network-Flow Dominance Adaptation Xiaohang Wang Guangzhou Institute of Advanced Technology, CAS Email: xh.wang@giat.ac.cn Terrence Mak The Chinese University of Hong Kong Email: stmak@cse.cuhk.edu.hk Mei Yang and Yingtao Jiang University of Nevada, Las Vegas Email: mei.yang@unlv.edu, yingtao@egr.unlv.edu Masoud Daneshtalab University of Turku, Email: masdan@utu.ﬁ Maurizio Palesi University of Enna, Kore, Italy Email: maurizio.palesi@unikore.it Abstract—Modern networks-on-chip (NoC) systems are required to handle complex run-time trafﬁc patterns and unprecedented applications. Data trafﬁcs of these applications are difﬁcult to be fully comprehended at design-time so as to optimize the network design. However, it has been discovered that the majority data ﬂows in a network are dominated by less than 10% of the speciﬁc pathways. In this paper, we introduce a method that is capable of identifying critical pathways in a network at run-time and, then, can dynamically reconﬁgure the network to optimize for the network performance subjected to the identiﬁed dominated ﬂows. An online learning and analysis scheme is employed to quickly discover the emerged dominated trafﬁc ﬂows and provides a statistical trafﬁc prediction using regression analysis. The architecture of a self-tuning network is also discussed which can be reconﬁgured by setting up the identiﬁed point-to-point paths for the dominance data ﬂows in large trafﬁc volumes. The merits of this new approach are experimentally demonstrated using comprehensive NoC simulators. Compared to the conventional network architectures over a range of realistic applications, the proposed self-tuning network approach can effectively reduce the latency and power consumption by as much as 25% and 24%, respectively. We also evaluated the conﬁguration time and additional hardware cost. This new approach demonstrates the capability of an adaptive NoC to handle more complex and dynamic applications. Keywords—networks-on-chips, self-tuning, regression, reconﬁgurable INTRODUCT ION I . A large number of on-chip many-core systems have been designed for a wide range of applications, including scientiﬁc computing, the Internet-based services, the newly emerging applications of recognition, mining, and synthesis (RMS) [1], among many others [2]. One of the key components of an on-chip many-core system is its on-chip network (OCN) or network-on-chip (NoC), which has to provide efﬁcient communication bandwidths for the processor cores and other resources with low latency and low power. Modern architectural optimization techniques applied to NoCs in many/multi-core systems [3]–[7] assume a general purpose packet-switching fabric where packets are transmitted through complex router pipelines in a hop-by-hop manner. Such scheme, however, incurs high communication latency and power consumption due to the contention for the shared channels, buffering, and long pipeline stages [8] [9]. Some of the existing approaches like the express virtual channel (EVC) [8] and the duo [10] try to bypass part of or full router pipeline stages so as to optimize data transmission. Another big problem of most of the NoC architectural designs is that they fail to exploit the applications’ trafﬁc behavior, although applications running in these systems exhibit stable and predictable trafﬁc behaviors [10]. A couple of noticeable exceptions to this kind of NoC architectural designs is the work in [11], which investigates the self-similarity of trafﬁc and the duo approach in [10], which analyzes the spatiotemporal distribution of trafﬁc ﬂows. A more subtle alternative to the conventional general purpose packet-switching fabric rests on the on-line learning of the applications’ trafﬁc behavior and dynamically change the network conﬁguration. Thus, channel contention, excessive buffering, and complex pipeline stages could be avoided. The potential beneﬁt of having such a self-tuning NoC is attributed to the fact that a small percentage of the ﬂows account for a disproportionally large number of the packets transmitted; this phenomenon is therein referred as ﬂow dominance [10]. As an example, let us consider a 64-core system running benchmarks through its NoC with 64 threads1 . Here a ﬂow is deﬁned as the data trafﬁc ﬂowing between a source-destination pair. With the distribution function of the packets sent by the ﬂows, Fig. 1 shows the ﬂow dominance observed in different benchmarks. The Y-axis in Fig. 1 indicates the percentage of the packets injected by the corresponding ﬂow divided by the sum of the total packets. We can see that ﬂow dominance in some applications is more apparent than that in the others. For example, for barnes, the top 20 ﬂows (only 7% of the total ﬂows) inject about half of the total packets. Here we deﬁne the dominant ﬂows as the ones with the highest data volume. Optimizing the network for the dominant ﬂows will obviously bring in the biggest performance improvement. The above mentioned ﬂow dominance feature shall be explored to help build intelligent self-tuning NoC to optimize both latency and power. In this paper, we propose a design method to help building such application-aware intelligent 1 The readers can refer to Section III.A for detailed conﬁguration of the experiments. 2) At the master core, the regression models of the ﬂows are collected. (cid:15) Phase II: reconﬁguration 1) The master core2 predicts the ﬂows’ trafﬁc volume based on the parameters of the regression models. After sorting, the dominant ﬂows are found. 2) Point-to-point connections are set up by reconﬁguring the network to optimize the dominant ﬂows. In this phase, only the master core is involved. Algorithm 1 shows the overall ﬂow of framework in an algorithmic manner. The input is the ﬂow trafﬁc of the NoC running applications, and the output is the point-to-point paths set up for the identiﬁed dominant ﬂows. Algorithm 1: Self-tuning NoC Input: F : NoC ﬂow trafﬁc of each node (Table I) Output: Point-to-point paths for the predicted dominant ﬂows. Function: Identify dominant ﬂows & ﬁnd point-to-point paths for them. begin /* Phase I: dominant flow identification (Section II.C) */ for each node i do for each destination j do /* flow f<i;j> */ 1) get the parameters of ARMA and polynomial models; 2) select the model with min error (the best model); 3) send the parameters of the best model to the master core; end end /* Phase II: reconfiguration (Section II.D) sort and ﬁnd the predict dominant ﬂows DF in the next time interval; */ HeuristicBasedPathSetup (DF ) ; /* point-to-point path setup (Algorithm 2). */ end B. System architecture and deﬁnitions The reconﬁgurable NoC structure is composed of two parts, (i). the basic NoC layer based on an existing (packet-switching) NoC system, e.g., 2D mesh, 2D concentrated mesh, or even 3D NoC topologies, etc., and (ii). the reconﬁgurable layer (Fig. 2 (a)) which offers low latency and low complexity. The reconﬁgurable layer can be realized with the lightweight conﬁgurable switches (CS). Each router in the packet switching NoC has one additional port connecting to the corresponding CS by the through-silicon vias (TSVs). This port only requires data buffers, and no crossbar switching or arbitration logic is needed. The switches in the reconﬁgurable layer can be conﬁgured to provide point-to-point connections/paths for the dominant ﬂows. In a simple word, the dominant ﬂows can 2Without losing generality, the master core is assumed to be the top-left core in the mesh like topologies. Fig. 1. Dominant ﬂow analysis. Flows are sorted according to their trafﬁc volume in the descending order. The X-axis represents the ﬂow IDs (each ﬂow is associated with a source-destination pair), while the Y-axis represents the ratio of the packet number injected by the corresponding ﬂow over the total number of the packets in the network. This ﬁgure is the distribution function of the packets sent by the ﬂows. NoC (AIN) with online learning capability. AIN works in two phases. 1) The dominant ﬂow identiﬁcation phase, in which identiﬁes the behavior of the ﬂow trafﬁc and exploit the ﬂow dominance with predictable patterns. Data regression models will be used to predict the ﬂows’ trafﬁc and dominant ﬂows are also identiﬁed in this phase. 2) The reconﬁguration phase, in which a reconﬁgurable NoC structure is employed to customize point-to-point paths for the identiﬁed dominant ﬂows. To the best of our knowledge, this work is the ﬁrst to introduce a framework in building intelligent self-tuning NoC architectures. The rest of the paper is organized as follows. Section II presents overall framework, including the dominant ﬂow identiﬁcation and the description of the reconﬁgurable NoC structure. Section III discusses the experimental results. Section IV reviews related work. Section V concludes the paper. I I . S EL F - TUN ING NOC FRAM EWORK A. Overview of the self-tuning NoC framework The framework of the self-tuning NoCs operates in two phases. There is a master core performing the path setup. During the dominant ﬂow identiﬁcation phase, data regression models with their parameters are identiﬁed for prediction purpose at each node. In the reconﬁguration phase, the ﬂows’ trafﬁc can be predicted according to the regression models, after which the dominant ﬂows can be sorted out. Correspondingly, the network can be reconﬁgured to optimize the dominant ﬂows. The basic steps performed in each phase include: (cid:15) Phase I: dominant ﬂow identiﬁcation 1) Each source node records the ﬂow’s trafﬁc volume at each time interval. At the end of this phase, the regression model for each ﬂow’s trafﬁc volume is built. The parameters of the models of each ﬂow are sent to the master core. TABLE I. NOTAT ION S U SED IN TH E PA P ER F = {Ft },where Ft = {f<1;1>;t , ...,f<K;K>;t } = DF ={dfi } {(s; d) }, 0<i<S mpi;k ∈ M Pi M Pi dfi;k P AT Hi;j K f<s;d>;t represents the trafﬁc volume of the ﬂow from source node s to destination node d at interval t. K is the network size after concentration. Ft is the set of the ﬂows’ trafﬁc volume at interval t. F is the set of the ﬂows’ trafﬁc volume over the whole execution time. The top S ﬂows after ranking (dominant ﬂows). dfi corresponds to the i-th dominant ﬂow. All of ﬂow dfi ’s minimal paths. The k-th minimal path of ﬂow dfi . A binary variable indicating if the k-th minimal path of ﬂow dfi is set up or not. The set of reconﬁgurable layer links in the j -th minimal path taken by ﬂow dfi . The network size after concentration. traverse in the reconﬁgurable layer as if it provides these ﬂows with dedicated wires. As a result, the router pipelines, excessive buffering, and channel contention, are all avoided for these dominant ﬂows. As shown in Fig. 2(b), each CS consists of 5 4-to-1 multiplexers (MUXs) for bidirectional transmission. A global line connecting the master core and the CSs controls the conﬁguration of each CS. The master core sends the control signals to the conﬁgurable switches via the global control line after performing the path setup algorithm in Phase II. Each control signal has the format of <CS ID, MUX ID, MUX select>. For the concentrated mesh (CMesh) network where a total of 16 conﬁgurable switches is used, the global control line thus has a width of log2 16+ log2 5+ 2 = 8 bits. The power and area can be estimated based on this circuit. The latency of the reconﬁgurable layer is assumed to be 1 cycle/hop. Table I lists the notations used in this section. At each source node, the trafﬁc volume of the ﬂows is recorded. Assume the network size is K after concentration. At node i, f<i;j>;t represents the trafﬁc volume of ﬂow from node i to node j at time instance t, where 0 (cid:20) j < K . The set of the ﬂows’ trafﬁc volume at interval t is represented as Ft = ff<1;1>;t , ..., f<K;K>;t g. C. Phase I: Dominant ﬂow identiﬁcation In this phase, two data regression models, the autoregressive moving averaging (ARMA) model [12] and the polynomial model [13], are considered in predicting the ﬂows’ trafﬁc. These two models are popular for their simple structures and high precision in prediction. Algorithm 1 summarizes the major tasks performed by each node and the master core during the two phases as described in the previous section. 1. The autoregressive moving averaging (ARMA) model Variable f<s;d>;t is used to denote the trafﬁc volume from node s to d at time instance t. In the ARMA model, the objective is to predict f<s;d>;t from a linear combination of its past values (e.g., f<s;d>;t−1 , f<s;d>;t−2 , ...). An ARMA(p, q ) model can be written as ^f<s;d>;t = φ1 f<s;d>;t−1 + ... + φp f<s;d>;t−p +ωt + θ1ωt−1 + ... + θq ωt−q (1) where fφ1 , ..., φp g and f θ1 , ..., θq g are regression parameters and f ωt , ..., ωt−q g is a Gaussian white noise. The predicted value of f<s;d>;t is denoted as ^f<s;d>;t . To estimate the parameters fφn , θm jn = 1, ..., p, m = 1, ..., qg with a given (p, q ) pair, methods like the Yule-Walker estimator or maximum likelihood estimator can be used, which are detailed in [12]. 2. The polynomial regression model In the polynomial regression, the trafﬁc volume of a ﬂow f<s;d>;t should be modeled by a polynomial function of time t given below [13] M∑ ^f<s;d>;t (w) = ω0 + ω1 t + ω2 t2 + ... + ωM tM = ωj tj (2) j=1 where M is the order of the polynomial and the coefﬁcients ω0 , ..., ωM are collectively denoted as a vector w. The value of the coefﬁcients can be determined by ﬁtting the polynomial to the training data. This can be done by minimizing an error function that measures the misﬁt between ^f<s;d>;t (w) and the training set data points. Methods like maximum likelihood could be used to estimate the coefﬁcient vector w for a given order of M [13]. The regression coefﬁcients (e.g., fφn , θm jn = 1, ..., p, m = 1, ..., qg or fω0 , ..., ωM g) are obtained such that they could be used to predict the ﬂows’ trafﬁc volume. D. Phase II: Reconﬁguration After discovering the dominant ﬂows in Phase I, pointto-point paths could be created in the reconﬁgurable layer to expedite the dominant ﬂows. As this layer is made of simple switches only, they don’t have any ﬂow control and routing computation capabilities. Thus, the key issue in path setup in this layer is to avoid path overlap. To facilitate this request, the reconﬁgurable layer itself could be expanded to increase the path diversity (i.e., to increase the number of minimal paths from the same source to the same destination) with additionally added switches. Expansion of the reconﬁgurable layer is regulated by a parameter, E . For example, if initially the reconﬁgurable layer has a size of N (cid:2)N , then the expanded layer can eventually be as large as (2N (cid:0) 1) (cid:2) (2N (cid:0) 1) with E = 2. The path setup problem could be described as follows: Set up the paths for the dominant ﬂows such that, the paths have the minimum overlap and the total distance of all the involved paths is minimized. This problem can be formally formulated as follows. Suppose the dominant ﬂows are sorted in DF (see Table I). For a given dominant ﬂow dfi , all of its minimal paths are enclosed in set M Pi with jM Pi j representing the total number of minimal paths of ﬂow dfi . mpi;k 2 M Pi represents the k-th minimal path of ﬂow dfi . A binary variable dfi;k is used to indicate if the k-th minimal path of ﬂow dfi is set up or not. Let P AT Hi;j be the set of links on the j -th minimal path taken by ﬂow dfi in the reconﬁgurable layer. Let V be the maximum number of ﬂows sharing one link. Each conﬁgurable switch p has a set of binary variables, fC p g. C p X Y represents the connection from the input port X to the output port Y , where X and Y correspond to elements in E , W, S, N , and L. In each conﬁgurable switch, one port can only be connected to one of X Y Fig. 2. The reconﬁgurable layer. (a) The general architecture, including an existing packet-switching NoC and the reconﬁgurable layer proposed in this paper. (c) The design of a conﬁgurable switch. Fig. 3. Possible interconnection schemes of input and output ports in a conﬁgurable switch. Two connected ports are called a pair of ports. Q denotes the number of pairs of connected ports. The maximum value of Q is 5. the other ports. These two connected ports are called a pair of ports. Let Q be the number of pairs of ports that are connected within one conﬁgurable switch. Fig. 3 shows examples of connected pairs of input/output ports with different values of Q. The maximum value of Q is 5 as in this ﬁgure. M in(V + Q), Subject to:∑ The problem could be formulated as follows. ∑ dfi;k = 1 ∑ k∈|M Pi | dfi;k (cid:20) V , 8 link l (3) (4) (5) ∑ Y ̸=X C p X Y l∈P AT Hi;j ;k∈|M Pi | i (cid:20) Q, 8p, 8 X, Y 2 fE , W, S, N , and Lg C p X Y (cid:21) dfi;j , 8p, if mpi;j makes an X to Y turn in p (6) Eqn. (3) ensures exactly one path is established for each dominant ﬂow. Eqn. (4) tries to minimize the overlap among the links in the reconﬁgurable layer. Eqn. (5) tries to minimize the overlap inside each conﬁgurable switch, e.g., an input should not be connected to more than one output. Eqn. (6) ensures that if a ﬂow’s path makes a turn from direction X to Y inside a conﬁgurable switch p, C p X Y will be set. Different from the work in [14] which only sets the constraint of the link bandwidth capacity, the key idea of phase II is to minimize both V and Q, corresponding to the overlap of links and switches. When solving the problem, V is typically larger than 1, i.e., more than one ﬂows share a single link. This will cause the paths in the reconﬁgurable layer to overlap, which should be avoided as only simple point-to-point connections are allowed in this layer. To deal with the overlapping problem, a simple solution is to increase the number of channels/links between two conﬁgurable switches; that is, if a link is shared by V ﬂows, it is extended to include V links between the two conﬁgurable switches. However, this approach demands a lot hardware resource which varies from application to application. As an alternative, if we reformulate the problem by setting V less than 2, we shall be completely free from the overlapping concerns. Under this condition, the problem objective becomes M in(Q), sides the ones given in Eqn. (3)(cid:24)(6). The following two additional constraints are included beV < 2 Q < 6 (7) (8) Eqn. (7) ensures non-overlapping paths among the ﬂows. Eqn. (8) set limit on the maximum value of Q as in Fig. 3. This new problem formulation implies lower hardware resource required. However, in most cases, we fail to ﬁnd such solutions. The reason is that, due to minimal route overlapping, some ﬂows may end up using the same links/conﬁguration switches no matter which minimal paths are actually selected. For example, consider two ﬂows f<0;1> and f<0;2> in a 4 (cid:2) 4 reconﬁgurable layer, i.e. ﬂows from node 0 to 1 and from 0 to 2 (Fig. 4). The paths of the two ﬂows will always overlap if minimal routing is assumed. To overcome this problem, a heuristic search algorithm is used to ﬁnd out the solutions with relaxed constraints. In the example, only one path actually will be selected for one ﬂow (e.g., f<0;1> ) in the reconﬁgurable layer, leaving the other ﬂow (e.g., f<0;2> ) to be routed in the packet-switching NoC. Heuristic search based path setup (Algorithm 2) TABLE II. PARAM ET ER S U SED IN THE S IMU LAT ION 64 (MIPS ISA 32 compatible) 4 / 4 / 4 64 16KB, 2-way, 32B line, 2 cycles, 2 ports, dual tags 32KB, 2-way, 64B line, 2 cycle 64KB slice/node, 64B line, 15 cycles, 2 ports Number of processors Fetch/Decode/Commit size ROB size L1 D cache (private) L1 I cache (private) L2 cache (shared) MESI protocol Main memory size NoC ﬂit size Data packet size Meta packet size NoC latency NoC VC number NoC buffer PARSEC SPLASH-2 2GB On-chip network parameters 72 bits 5 ﬂits 1 ﬂit latency: router 2 cycles, link 1 cycle 4 5 × 12 ﬂits Benchmarks streamcluster, swaptions, ferret, ﬂuidanimate, blackscholes, freqmine, dedup, canneal barnes, raytrace I I I . EX P ER IM EN TAL EVALUAT ION S A. Experimental setup We use an event-driven many-core simulator to model the NoC architecture that is designed following the selftuning framework described in previous section. Table II lists the NoC parameters that were plugged into the many-core simulator. The ORION 2.0 power library [15] is integrated with our simulator. Evaluation is performed over a suite of benchmarks adopted from SPLASH-2 [16] and PARSEC [17]. The benchmarks are cross-compiled into MIPS-compatible binaries. In our experiment, 8 benchmarks in PARSEC were cross-complied. All the benchmarks in SPLASH-2 could be compiled, and all of them were included into our experiments. Of these compiled benchmarks, we deliberately picked two benchmarks from SPLASH-2, (i) barnes, whose data trafﬁc shows a clear ﬂow dominance feature, and (ii) raytrace with less ﬂow dominance, for reporting and analysis. In the experiments, we select the 2D concentrated mesh (denoted as CMesh) as the basic topology which can be augmented following the proposed AIN. In this paper, we assume that each node or tile has a processor, an L1 cache, an L2 bank with directory (or just a memory controller), and an NI/router. In the concentrated mesh, four such nodes are considered to be a “meta-node”. In the experiments, the sampling interval is set to be 5M cycles, which balance between the computation time and precision of the regression models. We compare the performance improvement of architectures augmented with AIN, EVC and duo [10]. The baseline router is assumed to have two pipeline stages. We also compare the result with that obtained from an ideal router with only one pipeline stage. B. Expansion factor of the reconﬁgurable layer In Section II, we see that the reconﬁgurable layer can be expanded by inserting more switches to help increase path diversity that is needed for accommodating more ﬂows in the reconﬁgurable layer. It is interesting to see how the size of the reconﬁgurable layer affects the number of non-overlapping paths built by Algorithm 2. Fig. 5 shows the sensitivity of the Fig. 4. The search tree example in the heuristic based path setup optimization. In this algorithm, a search tree is built where each tree node keeps a record of some of the ﬂows passing through it. Each ﬂow record has two ﬁelds: <ﬂow ID, the minimal path taken by the ﬂow>, as shown in Fig. 4. Each ﬂow in the sorted dominant ﬂow list DF is scanned and checked to ﬁnd all its minimal paths. A new node is inserted if one of the minimal paths does not overlap any of the ﬂows recorded in the parent node. Note that, if all of the minimal paths of a new ﬂow are found to conﬂict with at least one of the ﬂows in the parent node, the new ﬂow is skipped and the next ﬂow will be checked. This process continues until all of the ﬂows in DF are checked. For example, in Fig. 4, only two dominant ﬂows are considered, f<0;5> and f<0;6> . Two tree nodes A and B are created with two minimal paths (i.e. paths (0, 1, 5) and (0, 4, 5)) for the ﬁrst ﬂow f<0;5> . These two tree nodes are attached to the root. For the second ﬂow f<0;6> , the ﬁrst tree node C in the last level with the path (0, 4, 5, 6) does not conﬂict with the path of f<0;5> in node A. However, the second tree node D contains the path (0, 1, 5, 6) which conﬂicts with f<0;5> ’s path (0, 1, 5) in node A, in that they both use the links (0, 1) and (1, 5). Thus, the second tree node D of f<0;6> will not be attached to node A. Algorithm 2: HeuristicBasedPathSetup Input: Dominant ﬂow list DF = fdf1 , ..., dfS g, where S is the number of dominant ﬂows (Table I) Output: Non-overlapping point-to-point paths for the dominant ﬂows. Function: Find point-to-point paths for dominant ﬂows begin 1) sort the dominant ﬂows in descending order according to their trafﬁc volumes; 2) set the root of tree as empty set; for each ﬂow dfi in the sorted list do ﬁnd all the minimal paths M Pi between the source and destination; for each minimal path mpi;k of this ﬂow do if no conﬂict with parent tree nodes then add the < dfi , mpi;k > as a new node to the tree; end end end Fig. 5. The packet coverage with different expansion factors. E is the expansion factor (Section II.D). The packet coverage is deﬁned as the packet number of the ﬂows whose paths could be setup by Algorithm 2 over the total packet number. expansion factor with respect to the size of the reconﬁgurable layer for benchmarks. The ﬂow packet coverage is deﬁned as the packet number of the ﬂows whose paths could be setup by Algorithm 2 over the total packet number. For streamcluster, we can setup non-overlapping pointto-point paths for 16 ﬂows (accounting for 18% of total packets) without exercising network expansion (i.e., expansion rate = 1). The path setup algorithm runs very fast without expansion, with cycles much less than 105 cycles. Consider the sampling interval of 5M cycles, the run time without expansion is negligible. Together 22 ﬂows can ﬁnd their point-to-point paths (accounting for 23% of total packets) with an expansion rate of 2 within one sampling interval. We can setup 24 ﬂows (accounting for 24% of total packets) with expansion = 3, but this comes with longer run time. We can see that, expansion rate of 1 is sufﬁcient for most of the applications with run time neglectably smaller than the sampling interval. C. Performance comparison of AIN against the original NoC, EVC and duo 1. Comparison of AIN against the original CMesh Fig. 6 shows the reductions of latency and network power with (1) AIN proposed in this paper, (2) EVC and (3) duo over the original CMesh. If CMesh is augmented with AIN, on average, the reductions in latency and network power are 14% and 13 %, respectively. For benchmarks like barnes, the reductions could be as much as 25% and 24%. Among the benchmarks, barnes, swaptions, freqmine, and blackscholes, latency/network power are reduced by more than 10% with AIN, e.g., the latency of barnes is reduced by 25% compared against the original CMesh. To investigate the difference in the latency/network power reduction among the benchmarks, a recall to Fig. 1 is necessary. From Fig. 1, we can ﬁnd that the above applications have “narrow” peaks, which means the ﬂow dominance is more obvious, i.e., a small percentage of the ﬂows account for a very large number of the packets. For the remaining applications, their curves in Fig. 1 are more “ﬂat”, indicating that ﬂow dominance is not so obvious, i.e., the trafﬁc is more evenly distributed, instead of centered on a few ﬂows. Thus, we deﬁne the kurtosis of the ﬂow distribution curve (see Fig. 1) as a metric of the ﬂow dominance, which is the fourth moment about the mean divided by the standard Fig. 6. The (a) latency improvement and (b) network power saving of CMesh+AIN, CMesh+EVC, and CMesh+duo against the original CMesh with 2-stage pipeline routers. Fig. 7. The kurtosis of the ﬂow distribution of the benchmarks as the metric of ﬂow dominance. deviation. Fig. 7 plots the kurtosis of the benchmarks in the descending order. As we can see that, the kurtosis of swaptions, blackscholes, freqmine, and barnes are larger than those of the remaining ones. Correspondingly, the reductions in terms of the latency/network power are more obvious for these applications. Fig. 8 further compares the reductions in terms of latency and network power of AIN against the CMesh assuming the routers have only one pipeline stage, i.e., the ideal routers. We can see from Fig. 8 that, the average reductions in terms of latency and power of AIN over the ideal CMesh are 8% and 7%, respectively. For the benchmarks with larger kurtosis values, the reduction in each metric is greater, e.g., for barnes, swaptions, and freqmine, the reductions in term of latency and network power of AIN against the ideal CMesh are over 10%, respectively. 2. Comparison of AIN against EVC Fig. 6 also shows the performance of EVC in terms of latency and network power. In all the cases, AIN achieves lower latency/network power than the EVC schemes. The performance difference between AIN and EVC is also obvious in the benchmarks with larger kurtosis values, i.e. whose ﬂow dominance is more obvious. We can see that AIN reduces more than 10% latency/network power against EVC in the cases of barnes, swaptions, and freqmine, which have large kurtosis values. On average, AIN reduces 8% latency and 7% network power against EVC. As EVC uses a heuristic way to setup interval, which is 5M cycles, this time penalty is fairly small because the learning ends after the training phase, i.e., no regression model building in Phase II, Phase I only takes once. In Phase II, only path setup occurs whose run time is less than 105 cycles. Thus the path setup takes negligible time compared to the time interval. During the path setup process, only the packet switching NoC will operate. This process takes 105 cycles, which happens in every 5M cycles (reconﬁguration happens every 5M cycles). The hardware cost of the reconﬁgurable layer is related to the design of conﬁgurable switches and wires. Assume the packet-switching NoC is 1 (cid:2) 1mm2 . The length of a link working frequency is 1GHz and the size of each tile in the (assumed to be the same width as the links in the packetswitching NoC shown in Table I ) in the reconﬁgurable layer is 2mm as four tiles are connected to be a “meta-node” in the concentrated mesh. Each of the conﬁgurable switches (CS) has an area of 1075 um2 and consumes 6.25uW dynamic power (switching activity is 0.5) using Synopsys Design Compiler under 45nm TSMC library. The area and dynamic power of a link are about 86606 um2 and 0.09 W (switching activity is 0.5) respectively with the 45nm technology available from the ORION 2.0 simulator [15]. From the above analyses, we can draw some insights. (cid:15) For applications whose ﬂow dominance is more obvious, more performance beneﬁt can be achieved from the (cid:15) The major cost of this learning/analysis framework comes intelligent framework. from the memory to record the parameters of the models. IV. R ELATED WORK Studies of trafﬁc modeling and prediction have been focused on the Internet. For example, neural network [18] is used to predict trafﬁc ﬂows. However, the behavior of Internet trafﬁc is substantially different from the on-chip case. In [11], the Hurst parameter is estimated to exploit the selfsimilarity in NoC trafﬁc running MPEG-2 video applications. However, this work cannot be used to model and predict online trafﬁc dynamically. In [10], the spatiotemporal distribution of NoC ﬂows is analyzed, but this work does not include prediction models for the ﬂow. Machine learning methods, like reinforcement learning, are used in NoC routing algorithm to choose less congested channels. For example in [6], an adaptive routing algorithm augmented by the reinforcement learning is proposed to selected output channels based on global congestion information. This approach could be complementary to our approach proposed in this paper which optimizes the end-to-end ﬂows. In the literature, several NoC topologies are proposed including concentrated mesh, ﬂattened butterﬂy, and multidrop express cube structure [19], where express channels can be used to optimize communication. However, these topologies add substantial complexity to the router architecture. In addition, these are ﬁxed topologies without the capability of being reconﬁgured online. An application-aware topology reconﬁguration [14] is proposed which sets up paths for the communications depending on the applications trafﬁc. As demonstrated in their experiments, this approach is only suitable for multiprocessor system-on-chip (MPSoC) applications Fig. 8. The latency improvement and network power saving of AIN against the ideal CMesh with only one pipeline stage. Fig. 9. Packet coverage of AIN and duo. non-overlapping point-to-point paths for the ﬂits, AIN learns from the global information before setting up point-to-point paths. Thus, the more ﬂow dominance, the better AIN can optimize the ﬂows. 3. Comparison of AIN against duo Finally, the performance of AIN and duo is compared, which in also shown in Fig. 6. On average, AIN reduces 7% latency and 7% network power against duo. For benchmarks with larger kurtosis values, e.g., barnes, swaptions, freqmine, AIN reduces more than 10% latency, network power against duo. Fig. 9 plots the packet coverage of AIN and duo. This ﬁgure shows the percentage of the total packets being optimized by customized circuits, e.g. the reconﬁgurable layer in this paper and the S-channel in the duo case. We can see that the packet coverage of AIN is higher than that of duo which means more packet transmissions are optimized in the reconﬁgurable layer. Thus, AIN achieves lower latency and network power than duo. D. Cost of AIN The cost of AIN includes three parts: memory to store model parameters, run time of the dominant ﬂow identiﬁcation /reconﬁgurable phases, and hardware cost of the reconﬁgurable layer. For the ﬁrst part, an ARMA (p, q ) in a K -sized network after concentration requires 2K 2 (cid:2) (p + q) parameters in the master core and 2p + 2q parameters in each of the other cores. These include the parametersfφn , θm jn = 1, ..., p, m = 1, ..., qg and the p past values of the ﬂow variable. For a 64(cid:2)64 concentrated mesh, assume an ARMA(11, 0), the master core has to store 5280 parameters, corresponding to about 10KB memory. For a polynomial model with order M , M parameters need to be stored. The total run time for regression model calculation, path setup is in the order of 105 cycles. Compared with the sampling which have less frequent trafﬁc. For communication-intensive CMP applications, not all the communications can ﬁnd a path using the approach in [14]. Besides, [14] only has constraints on the link bandwidth capacity, while our approach tries to minimize the overlap of links as stated in Section II.D. Work has also been proposed to make the NoC more ﬂexible, which enables dynamic conﬁguration. For example, express virtual channels (EVC) [8] are used to bypass router pipeline stages utilizing communication locality as in [8] [20]. Another scheme, virtual point to point connection in [9] sets up virtual point-to-point connection between senders and receivers. Both of the two methods optimize communication by bypassing router pipeline stages. However, neither of the two schemes includes a learning process which could adjust the NoC structure with global trafﬁc info, i.e., only heuristics are used without the knowledge of application trafﬁc. In the experiments, our framework reduces 8% latency and 7% network power against EVC, on average. For applications with more obvious ﬂow dominance (larger kurtosis values) the reductions against EVC are more than 10%. The duo approach in [10] analyzes the characteristics of trafﬁc ﬂows and use heuristics to conﬁgure the multi-drop channels to set up express channels. Our framework differs from that approach in the following three aspects. (1) The work in [10] focuses on trafﬁc distribution analysis, while our work emphasizes a framework combing regression model and non-overlapping point-to-point path setup. Our framework is more general and targeted at supporting online reconﬁguration of NoC. (2) In our approach, regression models are used to predict the trafﬁc ﬂow, such that, the point-to-point paths can be set up in advance in each time interval. In contrast, in [10] express paths are set up after collecting trafﬁc ﬂow values in current interval, whereas the trafﬁc distribution could be different from that during the express path setup. Thus, the express paths might not be optimal for the real trafﬁc. (3) In our approach, dominant ﬂows are preferred to be optimized while in [10] ﬂows with longer communication distance are biased. Thus the packet coverage of the two approaches differs as in Section III.C. More packets are optimized in our approach than that in [10]. Overall, our approach reduces 7% latency and 7% network power against the work in [10] on average. V. CONC LU S ION In this paper, we proposed a self-tuning NoC framework to help build intelligent NoC that can deliver higher performance at the presence of changing data trafﬁc. This framework identiﬁes the ﬂow dominance of the applications’ ﬂow trafﬁc and optimizes the dominant ﬂows in two phases. First, ﬂows are recorded and modeled. Next, the master core identiﬁes the dominate ﬂows which are optimized by reconﬁguring the network layer made of conﬁgurable switches. An efﬁcient algorithm sets up non-overlapping point-to-point paths for the dominant ﬂows on the reconﬁgurable layer such that these ﬂows can traverse through the path with latency as low as the wire delay. The reconﬁgurable layer can be augmented to any existing NoC topologies. Our experiments showed that, existing topologies like concentrated mesh augmented with our framework can reduce as much as 25% latency and 24% network power compared against the original NoC system for both PARSEC and SPLASH-2 benchmarks. We expect that such type of self-tuning NoC systems can be used to improve the performance for a wide variety of applications. [3] "
Per-flow delay bound analysis based on a formalized microarchitectural model.,"System design starting from high level models can facilitate formal verification of system properties, such as safety and deadlock freedom. Yet, analyzing their QoS property, in our context, per-flow delay bound, is an open challenge. Based on xMAS (eXecutable Micro-Architectural Specification), a formal framework modeling communication fabrics, we present a QoS analysis procedure using network calculus. Given network and flow knowledge, we first create a well-defined xMAS model for a specific application on a concrete on-chip network. Then the specific xMAS model can be mapped to its network calculus analysis model for which existing QoS analysis techniques can be applied to compute end-to-end delay bound per flow. We give an example to show the step-by-step analysis procedure and discuss the tightness of the results.","Per-Flow Delay Bound Analysis Based on a Formalized Microarchitectural Model Xueqian Zhao and Zhonghai Lu Department of Electronic Systems, School for ICT KTH Royal Institute of Technology, Stockholm, Sweden {xueq, zhonghai}@kth.se Abstract—System design starting from high level models can facilitate formal veriﬁcation of system properties, such as safety and deadlock freedom. Yet, analyzing their QoS property, in our context, per-ﬂow delay bound, is an open challenge. Based on xMAS (eXecutable Micro-Architectural Speciﬁcation), a formal framework modeling communication fabrics, we present a QoS analysis procedure using network calculus. Given network and ﬂow knowledge, we ﬁrst create a well-deﬁned xMAS model for a speciﬁc application on a concrete on-chip network. Then the speciﬁc xMAS model can be mapped to its network calculus analysis model for which existing QoS analysis techniques can be applied to compute end-to-end delay bound per ﬂow. We give an example to show the step-by-step analysis procedure and discuss the tightness of the results. I . IN TRODUC T ION Top-down system design starts from high level models. One major advantage of using a high level model is to abstract away low-level details and capture only relevant information, which can be used for formal veriﬁcation and for design space exploration. By verifying basic system properties and design decisions, system designers can gain not only insights but also conﬁdence for the follow-up implementation. One example of such models for designing communication fabrics is the eXecutable Micro-Architectural Speciﬁcation (xMAS) model [6] developed by Intel researchers. xMAS is a formal framework modeling communication fabrics. The xMAS library consists of a small set of formally deﬁned building blocks or primitives modeling trafﬁc source and sink, arbitration, communication and synchronization. A communication fabric can be modeled by instantiating and connecting components from the xMAS library, resulting in a structural representation. The representation is an executable model expressed with well-speciﬁed semantics using synchronous equations. Also, the representation is a graphical diagram which intuitively captures both data ﬂow and control ﬂow. xMAS was initially deﬁned in [6]. Its use in verifying the safety and deadlock freedom properties of communication fabrics can be found in [5] and [8], respectively. In practice, xMAS has been used to model and validate the micro-architecture of a number of complex industrial designs. Nevertheless, performance analysis based on xMAS models remains an open problem. This paper presents a worst-case communication time analysis technique based on xMAS models, with focus on ﬂowwise end-to-end delay bound. Here, a ﬂow refers to a unicast stream of data from a source to a destination. Speciﬁcally, given a network model plus necessary trafﬁc characteristics, the methodology deﬁnes a step-by-step procedure in order to compute per-ﬂow delay bound. Starting from the microarchitecture diagram of a canonical on-chip virtual channel (VC) router, we show how the xMAS primitives can be used to construct a formalized model, which can integrally rather than separately capture both the data ﬂow and the control ﬂow. Then based on the xMAS model, we show how it can be mapped to produce its QoS (Quality-of-Service) analysis model. Since the QoS analysis is based on Network Calculus (NC), a mathematical formalism for reasoning about performance, backlog and throughput bounds in communication networks [2][3][7], we naturally call it an NC model. After obtaining the NC model, previous analysis techniques can be applied or extended to compute an end-to-end delay bound for each ﬂow in the network. The paper’s contributions can be summarized as follows. • We describe how to express a router’s micro-architecture in xMAS (Section IV). • Given an xMAS model, we propose a scheme to map the xMAS model to its NC model, upon which QoS analysis can be performed (Section V). • We exemplify the methodology, validating its correctness and tightness (Section VI). The remainder of the paper is structured as follows. In Section II, we discuss related work. Section III describes the xMAS modeling framework. In Section IV, we show how to create an xMAS model for a canonical VC router. Then in Section V, we present the QoS analysis methodology. We use a tutorial example in Section VI to illustrate and justify our approach. Finally, we conclude in Section VII. I I . R E LAT ED WORK The network calculus theory originated from macronetworks in the context of offering performance guarantees in Internet and ATM. The seminal paper was written by Cruz in 1991 [7]. Since then, a large number of researchers have contributed to developing this theory, for example, relating it to min-plus, max-plus algebra [2] and to ﬁltering theory [3], generalizing it to stochastic network calculus [10], as well as applying the theory for practical use, for example, in offering QoS in rate-guaranteed services in ATM, and differentiated and integrated services of Internet [2]. As network calculus develops, it has also been applied to realtime embedded systems, SoCs, and wireless sensor networks. Its application in realtime systems can be found in realtime calculus [16] where it is extended to compute pernode worst-case delay bound under various scheduling polices, such as ﬁxed priority, TDMA, EDF, etc. As a result of pernode guarantee, realtime calculus makes system-wide worstcase performance analysis compositional. In SoCs, SymTA/S (Symbolic Timing Analysis for Systems) [9] applies the idea of trafﬁc shaping to increase the minimum distance between events and thus reduce peak workload bursts so as to improve worst-case performance. In [14], through a probabilistic study on multimedia playout buffer, stochastic network calculus is applied to show the potential of saving on-chip memory resources when relaxing hard QoS requirements. Recently network calculus is applied to on-chip micronetworks. In this regard, the application of network calculus for NoCs has taken a clear-box rather than a black-box approach since the simple on-chip hardware router enables to analyze the ﬂow contention, buffering and multiplexing, and ﬂow control behaviors much more closely. In [13], Qian et al. identiﬁes three basic ﬂow contention patterns, analyzes perﬂow equivalent service curve under these patterns, and further extends the contention analysis to cover arbitrary combinations of on-chip network contention scenarios. Furthermore, they propose a worst-case delay bound analysis technique [12] which considers sophisticated resource sharing scenarios, such as link sharing, buffer sharing and control sharing. In [1], a network calculus based approach is presented to analyze and evaluate on-chip interconnects in terms of performance and cost, such as latency, energy consumption and area requirements. While inheriting the analysis techniques from [12][13], this work differs from previous works in that it is grounded on a formalized communication model. Via this model, we see the light of a formalized holistic approach, which integrates a router’s micro-architectural details to generate a well-deﬁned structured intermediate model (in this case, the xMAS model). Furthermore, an NC model can be constructed from the xMAS model. As such, the xMAS model seamlessly bridges the gap between an informal router micro-architecture diagram and a formal NC analysis model, thus making the analysis approach integrated and well founded. I I I . TH E XMAS MOD E L ING FRAM EWORK [6 ] A. The xMAS primitives The xMAS modeling framework hosts a concise set of primitives for modeling concurrent communication and synchronization behavior, namely, source, sink, queue, function, merge, switch, fork, and join. Figure 1 shows the symbols of the primitives. Their basic functions can be explained as follows: • Source: A source is a component which attempts to send a packet non-deterministically. It has only a single outport. f k source sink function queue merge switch fork join Fig. 1. The xMAS modeling primitives • Sink: A sink is the dual of a source. It is a component which non-deterministically consumes a packet. It has only one inport. • Function: A function is a primitive to model transformations on the input data, generating the output data. For example, a memory turns requests into responses after some access latency. • Queue: A queue is a FIFO buffering component with a standard interface comprising a read port and a write port. The queue has two parameters: size k (the number of elements it can contain) and a type (the type of elements it can contain). • Merge: A merge models multiplexing and arbitration, selecting one packet among multiple competing packets. A merge has multiple input ports and one output port. A special merge is one at which data from different inputs never arrive at the same time, and thus no arbitration is needed. The special merge functions as an aggregator. • Switch: A switch is a primitive to route incoming packets to exactly one of the outputs, modeling de-multiplexing. It consists of one input port and two or multiple output ports. • Fork: A fork is a primitive with one inport and two outports. Intuitively, a fork takes an input packet and creates a packet at each output. It ensures that a transfer only occurs when the input is ready to send and both the outputs are ready to receive. • Join: A join is the dual of a fork. It has two inports and one outport. Intuitively, a join takes two input packets (one at each input) and produces a single output packet. It ensures that a transfer only occurs when the two inputs are ready to send and the output is ready to receive. B. Example of formal deﬁnitions Each of the xMAS primitives has a formally deﬁned semantics. We take the fork and the join as two examples. A detailed coverage of the primitives and other formal deﬁnitions can be found in [6]. i a i.irdy i.data i.trdy a.irdy a.data a.trdy b.irdy b.data b.trdy (a) The fork primitive (left) and its signal equivalent (right) b a b o a.irdy a.data a.trdy b.irdy b.data b.trdy (b) The join primitive (left) and its signal equivalent (right) o.irdy o.data o.trdy Fig. 2. Fork and Join primitives with their full signals Figure 2 shows the fork and join primitives and their full signals. Each input and output channel, x, in a primitive encapsulates exactly three signals when unfolded: x.data, x.irdy, and x.trdy, which represent the data, initiator ready (the data is valid or invalid), and the target ready (whether the target is ready to receive data or not). The fork primitive has one input, i, producing two outputs, a and b. By deﬁnition, the signal relations can be formally deﬁned using the following equations: a.irdy := i.irdy and b.trdy a.data := f(i.data) b.irdy := i.irdy and a.trdy b.data := g(i.data) i.trdy := a.trdy and b.trdy where f and g are two parameters which deﬁne the two mapping functions, one from i.data to a.data, and the other from i.data to b.data, respectively. The join primitive takes two inputs, a and b, producing one output o. The formal deﬁnitions are as follows: a.trdy := o.trdy and b.irdy b.trdy := o.trdy and a.irdy o.irdy := a.irdy and b.irdy o.data := h(a.data, b.data) where h is the parameter which deﬁnes the mapping function from a.data and b.data to o.data. C. Example of macroblock: Modeling a credit logic One can use the primitives to construct macros. To exemplify the usage of the primitives, we show how to model a credit counter as a macro, which can be used to realize a common link-level ﬂow control function to avoid buffer overﬂow. k i o i token k o Fig. 3. An xMAS model for a credit counter Figure 3 illustrates the credit counter (the box on the righthand side) and its macro-block symbol (the left-hand side symbol). The credit counter has one inport, i, for producing credits and one outport, o, for consuming credits. Internally, it has ﬁve components: one queue with size k , one fork, one join, one source and one sink. The fork connects its inport with a token source and its two outports with the queue and the outport of the credit counter. The join connects its outport with a sink and its two inports with the queue and the inport of the credit counter. Initially, since the queue has a size of k , it can absorb up to k tokens (one token for one credit). This means that the credit counter can output up to k credits. A signal at the inport of the credit counter will trigger the sink to absorb one token from the queue, thus making one room for producing one more credit. In the following, we show how the basic building block of NoCs, a typical virtual channel (VC) router, can be neatly modeled using the xMAS primitives. IV. ROU T ER MOD E L ING IN XMAS A. Router’s micro-architecture diagram Credit Credit Flow Flow VC Credits 1 2 3 4 Arbiter Credit Management Routing VC Allocator vc1 vc2 vc3 vc4 D M X D M X M U X M U X D M X D M X Credit Credit Flow Flow M U X M U X queuing unit 2×2 crossbar Fig. 4. Micro-architecture of a 2-by-2 router with 2 VCs per inport Figure 4 draws the micro-architecture of a typical packetswitching VC router. For simplicity, we show a 2×2 router, VCs, which share an input channel to the 2×2 crossbar. On which has two inports and two outports. Each input has two the data path, it has a queueing unit and a crossbar. On the control path, it has a Routing unit, a VC allocator, an Arbiter and a Credit Management unit. The Routing block computes the outport through checking the packet header with a routing algorithm. The VC Allocator block allocates a speciﬁc VC in the downstream router to packets. The Arbiter arbitrates on ﬂows’ requests to use and thus share the input channel to the crossbar and the output Physical Channel (PC) bandwidth. Depending on how the VCs are sized, it performs either wormhole switching or virtual cut-through switching, speciﬁcally, ﬂit-size VCs for wormhole and packet-size VCs for virtual cut-through switching. With wormhole switching, the VCs are allocated per-packet while the PCs per-ﬂit (ﬂit is the ﬂow control unit). With virtual cut-through switching, both VCs and PCs are allocated per-packet. The router performs the conventional credit-based link-level ﬂow control, meaning that a ﬂit or packet can be sent out downstream via the data forward channel only if there is a buffer space available in the allocated VC of the downstream router. Whenever the downstream router transmits the ﬂit or packet forward, it will feed a credit (indicating one free buffer) back to the upstream router through the credit feedback channel. The Credit Management block enables the link-level ﬂow control to avoid buffer overrun. It owns an array of credit counters. Each credit counter corresponds to a speciﬁc VC in the downstream router, and counts the number of free buffers in the VC. It counts down by 1 upon sending out a packet from the current router to the VC, and counts up by 1 upon the 2×2 router, the array has a size of 4, as there are 4 VCs receiving a credit for the VC from the downstream router. With in the downstream router. The counters have an initial value equal to the VC capacity. As can be observed, the router micro-architecture can accurately capture the data ﬂow, i.e., how packets are moved into VCs from inports and how they go through the crossbar to outports. Drawing such a graph is a common practice to convey the router’s micro-architecture. However, it is difﬁcult and somewhat awkward for such micro-architecture models to capture the control ﬂow, i.e., how the packet movement is explicitly controlled by the credit availability. In the following, we create an xMAS model to express the router’s micro-architecture, capturing both data ﬂow and control ﬂow in integration. B. Router’s xMAS model Credit management 4_to_2 Credit Credit demux cq1 cq2 cq3 cq4 cc1 cc2 cc3 cc4 2_to_4 mux vc1 vc2 vc3 sq1 Flow Flow mq1 sc1 mc1 sq2 vc4 mq2 sc2 mc2 queuing unit crossbar j1 j2 k1 k2 synchronization Fig. 5. An xMAS model for the VC router Credit Credit Flow Flow Figure 5 depicts an xMAS model for the VC router. The data ﬂow is modeled by the switch, the queue and the merge. Speciﬁcally, the switches, sqi (i = 1, 2) and sci (i = 1, 2) model the de-multiplexers (DMX) of the queuing unit and crossbar, respectively. The merges, mqi (i = 1, 2) and mci (i = 1, 2), model the multiplexor (MUX) of the queuing unit and crossbar, respectively. There exists a direct one-toone correspondence between the data-ﬂow components in the micro-architecture diagram and that in the xMAS model. The arbitration policies can be viewed as being embedded in the merges mqi (i = 1, 2) and mci (i = 1, 2). The more interesting part is the modeling of the control ﬂow and its interaction with the data ﬂow, particularly, how the credit-based ﬂow control affects the data ﬂow. Two synchronization primitives, join and fork, nicely serve this purpose. As can be seen in the synchronization block in Figure 5, there is a pair of join and fork, ji and ki (i = 1, 2), connected to each output channel. The join ensures that a buffer space in the allocated VC at the downstream router is available whenever dispatching a ﬂit or packet from one of the four VCs. The fork produces a credit backward to the upstream router for a corresponding credit counter whenever a ﬂit or packet leaves the router. The credit management block can be explicitly modeled with the xMAS primitives, as illustrated in Figure 5. There are four credit counters, cci (i = 1, 2, 3, 4), each corresponding to a VC in the downstream router. A credit is received from the downstream router through one of the two credit channels and the 2-input-4-output macro-block, and then distributed to one of the four credit counters as a ’colored’ token. The four queues cqi (i = 1, 2, 3, 4) connected with the four credit counters are used to store credits when credits are received but cannot be consumed immediately by the two join components through the 4-input-2-output macro-block. The two join components connect the four credit counters through the 4-input-2-output macro-block. This means that any of the four VCs in the current router are allowed to transmit packets to any of the four VCs in the downstream router. As can be observed from this example, the xMAS model can explicitly capture the control ﬂow (through the credit counter) and its interaction with the data ﬂow (through join and fork). In fact, we can see that the two pairs of synchronization primitives (join, ji , and fork, ki , where i = 1, 2) for ﬂow control lie on the data movement path. C. Discussion on the router’s xMAS model Note that mapping the router’s microarchitecture to its xMAS model is not unique, since, in principle, the ﬂow control behavior may be modeled at different places on the data path. The modeling alternatives reﬂect different design decisions. In Figure 5, we model it after the crossbar (see the positions of j1 and j2 ). This means that the router requires credit availability after the crossbar arbitration. Alternatively we could also model it after the queuing unit and before the crossbar. This captures another design intention which requires credit availability before entering the crossbar arbitration. Since a credit is only needed at the moment when a packet is sent out, i.e., after the crossbar arbitration (mc1 and mc2 ), we model it after the crossbar. The routing unit and VC allocator are not explicitly modeled, but should be implicitly reﬂected in the ﬂow path. The routing unit determines the delivery path for each ﬂow. The VC allocator determines which VC to use per-packet for each ﬂow. In general, a routing algorithm may dynamically choose different paths for a ﬂow to deliver packets, exploiting the diversity of network connectivity. A VC allocator may dynamically allocate different VCs to transmit the packets of a ﬂow according to the VC availability, i.e., per-packet VC allocation. However the dynamic path selection and the dynamic VC allocation makes the complex performance analysis problem even more complicated. Also, to understand the dynamic case, we have to start with understanding the static case. Therefore, we assume in the following (1) deterministic routing, i.e., each ﬂow has a single path which can be pre-determined, given its source and destination addresses; (2) deterministic VC allocation, meaning that the set of VCs which a ﬂow passes in order is pre-determined, i.e., per-ﬂow VC allocation. In implementation, a ﬂow ID embedded in packets can be used to realize the per-ﬂow VC allocation. With these two deterministic assumptions, we know exactly the path for each ﬂow’s traversal inside the routers and outside the routers (the physical channels). V. QOS ANA LY S I S M E THODO LOGY A. Overview Network-Flow Diagram xMAS Model Network Calculus (NC) Model Worst-case Performance Analysis Delay Bound Fig. 6. The xMAS based QoS analysis ﬂow Figure 6 gives an overview of the QoS analysis methodology. It starts from a network-ﬂow diagram which illustrates both network (topology, router micro-architecture) and ﬂow information (passing PCs and VCs). Then a speciﬁc xMAS model is created. Based on the speciﬁc xMAS model, we can further derive its NC model for interested ﬂows. Finally, QoS analysis is performed on the NC model applying and extending existing analytic techniques. B. Modeling a network-ﬂow diagram in xMAS To develop an xMAS model for a speciﬁc application on a speciﬁc communication platform, we need to know both the network information and the ﬂow information (routing path), which are captured in a network-ﬂow diagram. As discussed in Section IV-C, to facilitate analysis, we assume that each ﬂow has a deterministic routing path, and when passing a VC router, it always uses the same VC. At a ﬁrst glance, the deterministic routing assumption appears too strong since network routing should enjoy the path diversity offered by the network. However, in practical SoC designs, when compared to oblivious and adaptive routing algorithms, deterministic routing algorithms, for example, the source routing and dimension-order routing, are often preferred, because they have less implementation complexity, and can help to achieve deadlock freedom, simplify validation and debugging. By assuming speciﬁc communication patterns with deterministic paths (PCs plus VCs), the routers can be much more simpliﬁed because redundant VCs, components and unnecessary connections can be pruned. This highlights the beneﬁts of application-driven NoC designs [11], which could largely simplify the resources and their connections. As a consequence, we will obtain a specialized xMAS model with only relevant primitives and their connections after modeling a network-ﬂow diagram in xMAS, as to be exempliﬁed in Section VI-A. C. Mapping an xMAS model to an NC model Naturally we ﬁrst model each xMAS primitive in NC. In Figure 1, there are eight primitives, which can be paired into four groups: source-sink components (source and sink), function and queue components (function and queue), blocking n-to-1 components (merge and join), nonblocking 1-to-n components (switch and fork). Merge and join are blocking because packets may be blocked due to arbitration and waiting for credits. Typically, there is a queue for temporary packet storage at the ingress channels of a merge or join component directly or indirectly. Switch and fork are nonblocking, because, once the input data move to a switch or a fork, they can be sent to one or multiple channels without blocking. • Modeling source-sink in NC: The xMAS source is nondeterministic. Here we assume that the trafﬁc injection is upper-bounded by a linear arrival curve, α(t) = b + rt, where b constrains trafﬁc burstiness and r the sustainable rate. In case that trafﬁc ﬂows do not conform to a linear arrival curve, the leaky-bucket trafﬁc shaping technique [4] may be used to enforce the conformance. The xMAS sink provides sufﬁcient bandwidth to drain data but may incur nondeterministic delay. We consider a latency-rate type of sinks whose service can be modeled as a latencyrate server [15] βR,T , where T denotes the maximum processing delay and R the average service rate. • Modeling function-queue in NC: The function primitive models data transformation. In a communication fabric, it can be used to model the behavior from memory request to response. For the NC analysis, it serves as a cut point to turn one round-trip communication into two unicast communications so that the request and response ﬂows can be analyzed separately. The xMAS queue can be used for two purposes: (1) It models nonblocking transport delay. Then it can be modeled as a δ -delay element [2], which has a burst delay function δT (t) = +∞ for t > T , and 0 otherwise. (2) It is an explicit queueing component, which can be directly mapped to a buffer in NC. • Modeling blocking n-to-1 components in NC: The blocking components, merge and join, result in delay, backlog and rate control. Therefore they are modeled as a server or service element in NC. Speciﬁcally, the merge can be modeled as an arbitration server (arbiter), β σ , while the join component as a ﬂow control server, β τ . • Modeling nonblocking 1-to-n components in NC: The nonblocking components, switch and fork, do no incur delay and rate control function in general, and thus are not treated as a service element. Rather they serve as a connector in the NC model to provide connectivity, allowing trafﬁc or credit splitting and aggregation. Beyond xMAS primitives, xMAS macros may be often used in designs. This necessitates a mapping from xMAS macros to NC elements. For example, the credit counter in xMAS could be mapped to a delay element in NC, since it incurs a control function for when a packet can be forwarded. We call the two levels of functional mapping, i.e., from xMAS primitives to NC elements and from xMAS macros to NC elements, the xMAS-component to NC-element mapping. With the xMAS-component to NC-element mapping built up, we can map each xMAS component (primitive or marcoblock) in an xMAS model to its NC element in a one-to-one correspondence. After the component mapping, we map the connections among the components. In most cases, it is a straightforward mapping, i.e., after making a one-to-one mapping of the xMAS primitives and macros to their NC counterparts, the connections can be directly inherited from the xMAS model. However, there is an exception. At a fork whose outports are connected to a sink and a credit counter, the sink service should occur before the credit generation. Thus the sink should be modeled before the fork in the NC model, as we shall show in the example in Section VI. D. QoS analysis based on the NC model After obtaining the NC model, we conduct the per-ﬂow endto-end delay bound analysis with three major steps: 1) Transform the closed-loop NC model into an open-ended NC model using the method in [12]. The credit-based ﬂow control can be treated as providing a control service to the data ﬂow, thus a loop-less service element, ﬂow controller τ , can be inserted into the data path. 2) Derive per-ﬂow end-to-end ESC from the open-ended model, using the techniques presented in [13][12] and [2]. At this step, we mainly investigate the ﬂow interference patterns concerning link sharing and buffer sharing, taking direct and indirect contention into account. In the end, we obtain an end-to-end Equivalent Service Curve (ESC) for a ﬂow. 3) Finally, together with the ﬂow’s arrival curve, we can use the basic results of network calculus [2] to compute its end-to-end delay bound. V I . AN I L LU S TRAT IV E EXAM P L E A. From network-ﬂow diagram to xMAS model We give a full example illustrating the procedure from a network-ﬂow diagram, to its xMAS model, to its NC models (initial model and reﬁned model), and exhibiting how the perﬂow end-to-end delay bound can be computed. Credit Credit Credit Management VC Allocator Arbiter Routing Credit Credit Credit Management VC Allocator Arbiter Routing Credit Credit vc1 vc2 vc3 vc4 D M X D M X M U X M U X D M X D M X M U X M U X vc1 vc2 vc3 vc4 D M X D M X M U X M U X D M X D M X M U X M U X queuing unit 2×2 crossbar queuing unit 2×2 crossbar Fig. 7. Network-Flow diagram with ﬂows passing buffers and channels Figure 7 draws a network-ﬂow diagram where two ﬂows f0 and f1 pass two concatenated routers, R1 and R2 . Entering into two VCs at R1 (f1 , f2 use V C1 and V C3 , respectively), merged into one aggregate ﬂow {f0 , f1 }. Then the aggregate the two ﬂows multiplex over one shared output channel and are ﬂow shares the ingress VC, V C1 , at R2 . Finally, the aggregate ﬂow goes through the same output channel of R2 . Though the example is simple, it captures all three relevant resource sharing scenarios: link (PC) sharing at R1 , buffer sharing at R2 and credit feedback control sharing between R1 and R2 . Note that, in the example, the un-used components and connections are drawn in dashed lines. As such, we see ’customized’ routers and speciﬁc ﬂows. This specialization leads to a speciﬁc and simpliﬁed xMAS model. cq1 cc1 Credit Feedback vc1 Credit Feedback vc1 vc3 mc1 j1 k1 k2 Fig. 8. The xMAS model for the example According to the general model represented in Section IV-B, Figure 8 depicts the xMAS model for the example. In order to give meaningful results, two trafﬁc sources are added for ﬂows f0 and f1 , and one sink to show their delivery end. B. Mapping the xMAS model to its NC model ff0 ; f1g Fig. 9. The initial NC model for the example Following the mapping scheme described in Section V-C, we obtain the initial NC model depicted in Figure 9 for the example. The merge mc1 maps to the arbiter server with a service curve, β σ = γ0,C ⊗ δ0 , where C is the link (PC) capacity; βRf c is the service function of the ﬂow controller (join j1 ). β σs = γ0,Cs ⊗ δTs is the latency-rate service curve of the sink in R2 with latency Ts and service rate Cs . The credit logic (cc1 and its associated queue, cq1 ) maps to a delayer β ν = δTc , where Tc models the credit feedback delay. The two forks, k1 and k2 , do not provide any service but provide connections for the data ﬂow and the credit ﬂow. One note to make here is that, not like its xMAS model, the sinking server in the NC model is placed before the credit generation. The reason is that a credit is only generated after a packet is served. This is why we have the service ﬁrst and the credit second. C. Per-ﬂow delay bound analysis We ﬁrst give assumptions, symbols and notations used in the analysis. We assume that ﬂow fi (i = 0, 1) conforms to an afﬁne arrival curve αi (t) = γbi ,ri (t), where bi is the burstiness and ri the trafﬁc rate. Function γbi ,ri (t) = ri t + bi for t > 0, and 0 otherwise. The routers with FIFO queues use weighted round-robin (WRR) for sharing links. The output arrival curve of fi at router Rj is denoted as αRj . The service curve of router Rj is βRj and the ESC that Rj offers to ﬂow fi is denoted as ˆβRj . The burst delay function δT (t) = +∞ for t > T , and 0 otherwise. Notation [x]+ = x if x > 0, and 0 i i otherwise. Operations ⊗, (cid:4) and ∧, ∨ represent convolution, de-convolution and minimum, maximum in min-plus algebra, respectively. For the VCs, we assume ﬂit-size buffers. For the ﬂows, we assume single-ﬂit packets. Let f0 be the tag ﬂow for which we shall derive its delay bound, and f1 the interference ﬂow. The delay bound formula for f0 can be deduced by the following ﬁve ﬁne steps. • Step 1: Break the closed feedback loop in Figure 9, and calculate the ESC of the closed loop subsystem. The service curve provided by R2 to the aggregate ﬂow ⊗ δTs . We use the method in [12] to eliminate the feedback loop and graphically turn Figure 9 into Figure 10. The service function of the ﬂow controller τ is calculated by the sub-additive closure as {f0 , f1 } is β σs = γ0,Cs β τ = βR2 ⊗ β ν + B = γ0,Cs ⊗ δT + B , where T = Ts + ν , B is the size of the VC in router R2 . ⎧⎨ ]+ 0 = [ ˆβ loop − αloop ˆβ loop ⎩γ0,Cs−r1 ⊗ δTs+b(cid:2) 1 {(γ0,Cs−r1 ⊗ δTs+nT +b(cid:2) 1 /Cs 1 /Cs n≥0 (cid:5) = , B ≥ CsT + nB )}, B < CsT . • Step 4: Derive the end-to-end ESC for f0 . Then the ESC that the end-to-end system provides to f0 is ⎧⎨ (cid:6) 0 = ˆβ0 ⊗ ˆβ loop ˆβ sys (cid:5) = ⊗ δTsys 0 ⎩γ0,Csys n≥0 B ≥ CsT , {(γ0,Csys ⊗ δTsys+nT + nB )}, B < CsT , where Csys = (w0C )∧ (Cs − r1 ), and Tsys = Ts + b(cid:3) 1 /Cs +φ1 . • Step 5: Deduce the closed-form delay bound formula for the tag ﬂow f0 . The delay bound for ﬂow f0 is computed by ﬁnding the greatest horizontal distance between the arrival curve α0 and . With Csys ≥ r0 , we have the system ESC ˆβ sys 0 Tsys + b0 Tsys + b0 Csys Csys , + (T − B Csys B ≥ CsysT )(cid:8) b0 B (cid:9), B < CsysT . Opened Loop Service ff0 ; f1g ¯D0 = Fig. 10. The reﬁned NC model removing the feedback loop Then we calculate the ESC of the subsystem (highlighted in the dashed red line), which is obtained after breaking the closed feedback loop. We have ⎧⎨ ˆβ loop = β τ ⊗ β σs ⎩γ0,Cs {(γ0,Cs n≥0 ⊗ δTs (cid:5) = B ≥ CsT , ⊗ δTs+nT + nB )}, B < CsT . This formula also gives the boundary condition of the VC size, B < CsT , for the back-pressure occurrence as a result of ﬂow control. • Step 2: Calculate the ESCs at the arbiter β σ . Let φ0 and φ1 be the weights in processing time at the WRR arbiter for f0 and f1 , respectively. Thus the ESCs for f0 and f1 before the ﬂow controller are ˆβ0 = w0β σ ⊗ δφ1 , ˆβ1 = w1β σ ⊗ δφ0 , where allocated bandwidth ratios w0 = φ0 /(φ0 + φ1 ), and w1 = φ1 /(φ0 + φ1 ). • Step 3: Calculate the left-over service of the opened loop for f0 . We ﬁrst calculate the arrival curve of f1 before the ﬂow controller, as 1 = γb1 ,r1 (cid:4) ˆβ1 = γb(cid:2) αloop , 1 ,r1 where b(cid:3) 1 = b1 + r1φ0 is the burst of f1 after the arbiter. Then we obtain the ESC of the opened loop for f0 as 0 = ¯D0 + Dt . Note that this delay bound is for queuing delay and does not include transport delay Dt . To include the transport delay, the total delay bound becomes ¯Dt D. Validation and Tightness To validate the analytical results, we compare them against RTL simulation results. The xMAS primitives are implemented at RTL, which is cycle-accurate bit-accurate. Based on the RTL library of the xMAS primitives, the example is constructed. Given trafﬁc ﬂows and different buffer sizes, we simulated with sufﬁcient time to ﬁnd the maximum delays, which are then compared to the calculated delay bounds to validate the correctness and tightness of the analytical results. We deﬁne tightness as the percentage of the maximum simulated delay to the corresponding analytical bound. We looked into two cases. In both cases, the ﬂow control loop is present. But due to different VC sizes, the ﬂow control effect is different. Case 1 has no back-pressure, as the VC has large enough buffer size to tolerate worst-case trafﬁc burstiness. Case 2 has back-pressure, as the VC’s buffer size is not large enough and thus back-pressure may occur. We studied ﬂow f0 . The transport delay is 3 cycles. router R2 , B ≥ 92. The arrival curves of the two ﬂows are Case 1: Without back-pressure, the ingress buffer size in 3+ 0.2t. The service curve of the sink is β σs = γ0,0.9 ⊗ δ100 = 0.9[t − 100]+ . The service curve of the arbiter σ is β σ = γ0,1 ⊗ δ0 = 1[t − 0]+ . As a special case of WRR, the arbiter uses RR policy, i.e., φ0 = φ1 = 1 cycle (to process 1 packet). The credit feedback delay is 2 cycles, namely β ν = δ2 . With these parameters, the boundary condition for backpressure occurrence is B < 92. Since B ≥ 92, the backpressure does not occur. The simulated maximum delay of f0 : α0 (t) = γ3,0.2 (t) = 3 + 0.2t and f1 : α1 (t) = γ3,0.2 (t) = f0 is 108 cycles, while its calculated delay bound equals 114 cycles. Thus the tightness of the delay bound is 94.7%. Case 2: With back-pressure, the ingress buffer size B = 6. The arrival curves of the ﬂows are f0 : α0 (t) = γ5,0.3 (t) = 5 + 0.3t and f1 : α1 (t) = γ2,0.4 (t) = 2 + 0.4t. The service curve of the sink is β σs = γ0,0.9 ⊗ δ500 = 0.9[t − 500]+ . The arbiter and the credit feedback are the same as in Case 1. Bound: 517 Max Delay: 511 500 450 400 350 300 250 200 150 100 50 ) l s e c y c ( y c n e a L t 0 0 0.5 1 1.5 Packet ID 2 2.5 3 x 104 Fig. 11. The analytical bound versus the simulation result when B = 6 With these parameters, the boundary condition for backpressure occurrence is B < 452. Since B = 6, the backpressure occurs. Figure 11 illustrates a sequence of 30,000 packet samples with simulated delays for this case, where the analytical bound is also drawn. As can be seen, the analytic delay bound constrains all simulated packet delays, verifying the analytical correctness. The simulated maximum delay of f0 is 511 cycles, while its calculated delay bound equals 517 cycles. Thus the delay bound has a tightness of 98.8%. Discussion: We would give a note on the validation approach used here. We use simulation to validate analysis. This should not be treated as a formally sound approach. Rather it is used as a best-effort, practical approach for approximation to increase conﬁdence. There are mainly two reasons: (1) Simulation is good at dealing with the average case but difﬁcult, if not impossible, to cover worst-case scenarios. Exhaustive simulation is generally prohibitive. (2) The analysis considers worst-case resource sharing for all ﬂows under any conditions. This may lead to pessimistic results, and may or may not happen during the actual ﬂow transmission in simulation. V I I . CONC LU S ION Based on the xMAS model, we have presented a QoS analysis technique comprising three steps. First a speciﬁc and simpliﬁed xMAS model is created starting from a network and ﬂow diagram. Then an NC model is derived from the xMAS model following a mapping scheme. Finally existing analysis techniques can be applied to obtain per-ﬂow end-to-end delay bound. Compared with the analysis ﬂow without using the intermediate xMAS model, the xMAS-based analysis makes the mapping from a network-ﬂow diagram to its NC model well deﬁned due to the expressiveness strength of xMAS in integrating both data and control ﬂows. The analytical approach has been validated through an illustrative example, showing very good tightness. From this work, we can see that the mathematical derivation can be quite lengthy and complicated. Considering the complexity, the hand-calculation approach is hard to scale as the system size increases. This motivates a future work for us to automate the QoS analysis procedure. We believe that this is achievable thanks to the well-deﬁned steps. Another future work is to give in-depth studies on the tightness of the analysis. ACKNOW L EDGM EN T The research is sponsored in part by Intel Corporation through a research gift. "
A speculative arbiter design to enable high-frequency many-VC router in NoCs.,"High-performance network-on-chip routers usually prefer a large number of Virtual Channels (VC) for high throughput. However, the growth in VC count results in increased arbitration complexity and reduced router clock frequency. In this paper, we propose a novel high-frequency many-input arbiter design for many-VC routers. It is based on the speculation on short and thus fast arbitrations in case of high VC occupancy. We further enhance it to reduce arbitration latency and promote speculation opportunity. Simulation results show that using the proposed arbiter, a 16-VC router achieves almost the same performance as an ideal design, showing improvements of around 48% on zero-load latency and 100% on network throughput over a naive 16-VC design.","A Speculative Arbiter Design to Enable High-Frequency Many-VC Router in NoCs Bo Zhao∗ Youtao Zhang† Jun Yang∗ ∗Dept. of ECE, University of Pittsburgh ∗{boz6, juy9}@pitt.edu †Dept. of CS, University of Pittsburgh † zhangyt@cs.pitt.edu Abstract—High-performance network-on-chip routers usually prefer a large number of Virtual Channels (VC) for high throughput. However, the growth in VC count results in increased arbitration complexity and reduced router clock frequency. In this paper, we propose a novel high-frequency many-input arbiter design for many-VC routers. It is based on the speculation on short and thus fast arbitrations in case of high VC occupancy. We further enhance it to reduce arbitration latency and promote speculation opportunity. Simulation results show that using the proposed arbiter, a 16-VC router achieves almost the same performance as an ideal design, showing improvements of around 48% on zero-load latency and 100% on network throughput over a naive 16-VC design. I . IN TRODUC T ION Chip multiprocessors (CMP) have emerged as the de facto standard of modern processor architectures. With fast technology scaling, the number of on-chip cores increases fast, making it one of the most critical challenges to design Networkon-Chip (NoC) for efﬁcient core-to-core communication. To avoid congestion and achieve high throughput, NoC routers use Virtual Channel (VC) ﬂow control — a large VC count improves efﬁciency of resource allocation by allowing more packets/ﬂits to participate in arbitration [5], [10]. Unfortunately, increasing VC count complicates routing control. In particular, a router needs one v:1 arbiter for each input port, where v is number of VCs per input port. As a result, a manyVC router tends to operate at lower frequency, which results in lower overall network throughput and higher latency. However, while a robust router needs to accommodate the worst case arbitration scenario, we observed that most arbitration decisions ﬁnish early due to high VC occupancy. First, when trafﬁc load is high, more VCs are occupied for buffering. Second, large number of single-ﬂit packets [8] are spread into many VCs even under low trafﬁc. As a result, arbitration can ﬁnish early due to short distance of searching for next winners. In this paper, we propose speculative arbitration to enable the design of high-frequency many-VC routers. It consists of two techniques — Arbiter Timing Speculation (ATS) and Group Priority Bypass (GPB). ATS breaks arbitration into multiple cycles such that frequency is boosted while majority of fast arbitration can ﬁnish in one cycle. GPB further halves arbitration latency by skipping empty VCs. We also prioritize the routing of body and tail ﬂits to increase the percentage This work is supported in part by NSF under CAREER awards CNS0747242 and CSR-1012070. of single-cycle arbitration. With each technique, we discuss in detail the observations and opportunities, and demonstrate that they coordinate well under various trafﬁc conditions. Combining the proposed designs, our experimental results showed that a 16-VC router achieves almost the same performance as an ideal design (no frequency penalty), which represents 48% latency and 100% throughput improvements over a naive 16-VC design. The proposed arbiter design is a straightforward and light-weight circuit technique. It is orthogonal to and thus easily applicable to routing control techniques (e.g. ViChaR [10] and SA-folding [11]), singlecycle pipelines [7], and high-radix switches, to form more powerful router implementations. I I . BACKGROUND A. NoC Router Micro-Architecture Network-on-Chips (NoC) are built from routers and the links between them. A generic router consists of input buffers organized as Virtual Channels (VC), a crossbar to connect input ports to output ports, and control logic. Routing control is realized in a pipelined manner [12], [14], which typically includes Routing Computation (RC), VC Arbitration (VA), Switch Allocation (SA) and Switch Traversal (ST) stages followed by Link Traversal (LT). In this paper we adopt a router pipeline without VC Arbitration [16]. Instead it performs SA and then assigns available VCs from a free VC list/queue [7], [9], [11], [14] to global SA winners. Selecting VCs from the list is fast and is performed in parallel with SA [7], [16]. This design helps to simplify VC selection and RC, and reduce pipeline depth and delay. Fig. 1. Switch Allocation (SA) implementation The router pipeline is known to be imbalanced. For the above router, the majority of the work falls in SA arbitration. As shown in Fig. 1, SA consists of 2 sub-stages. The ﬁrst Fig. 2. Worst case of SA1 round-robin arbitration (H = Head, B = Body, T = Tail) sub-stage picks one out of v input VCs that share a single crossbar port. This requires a v:1 arbiter for each input port. The second sub-stage arbitrates globally among SA1 winners from each input port (p inputs) for each output channel. Since SA has a longer latency than other stages, it determines the router clock frequency [10], [11], [14], [16]. B. The Need for Large Number of VCs In virtual channel ﬂow control, the organization of VCs within a router plays a decisive role in overall system performance. The number of VCs per physical port and the VC depth are two parameters to partition the storage resources and tradeoff between buffer utilization, throughput and latency. Previous studies demonstrate that for low trafﬁc intensity, a small number of VCs can sufﬁce. However with high network loads, increasing the number of VCs is a more effective way of improving performance than simply increasing the VC depth [10], [13]. This is mainly because that additional VCs can alleviate Head-of-Line (HoL) blocking [5], [10] and output resource conﬂict. Fig. 3 compares 3 VC organizations with the same storage resource of 32 ﬂits per input port: 4 VCs × 8 ﬂits, 8 VCs × 4 ﬂits, and 16 VCs × 2 ﬂits. As can be seen, the increased VC count is more efﬁcient with higher trafﬁc loads, and thus substantially pushes the network saturation point further [5]. Fig. 3. Network performance in a 8×8 mesh with 3 VC organizations at same frequency (NOTE: injection rate and delay are based on cycles) Therefore, many-VC would be a preferable design choice in the upcoming many-core era, where intense trafﬁc on each NoC router is expected because: 1) In a distributed cache architecture, a signiﬁcant portion of cache accesses become network trafﬁc, and there are also huge amount of coherence messages [15]. 2) It becomes increasingly expensive to equip each core with a dedicated router than to share a router among multiple cores [1], [15]. For example, in a 48-core prototype chip [14], every two cores share a 8-VC router. The concentrated trafﬁc will be even heavier with more cores sharing a single router, placing higher demand on VC count. Moreover, short packets, especially single-ﬂit packets, also demand for large number of VCs per input port. Single-ﬂit packets are usually message packets that carry cache access requests, coherence commands, table lookup queries, etc. They can contribute to a great portion of totally network trafﬁc in a tile-based NoC. For instance, S. Ma et al. [8] identiﬁed that in the PARSEC benchmark set, on average 80% of network trafﬁc are single-ﬂit packets. In such circumstance, few and deep VCs will suffer much severer HoL blocking because each VC is now able to accommodate much more packets that are heading to different output ports. I I I . MOT IVAT ION & OB SERVAT ION A. Effect of VC Count on Router Frequency and Performance Although a large number of VCs can lead to substantial throughput enhancement, one downside is the increased complexity of the arbitration logic. In Fig. 1, the ﬁrst stage of Switch Allocation (SA1) consists of several v:1 arbiters in parallel. They are typically implemented as round-robin arbiter due to its simple structure and good fairness [2], [4], [16]. Fig. 2 illustrates an example of round-robin arbitration in SA1. It represents the low VC occupancy case where only one packet is present in the entire port. In the beginning, all ﬂits of the packet reside in VC 0, the same position of the priority pointer. In cycle 1, the arbiter ﬁrst looks into the priority position. As it sees the request from the head ﬂit (H) in VC 0, it generates a grant to this ﬂit. As deﬁned by the round-robin priority, the winner will be assigned the lowest priority for next arbitration, so the priority pointer moves to VC 1. In cycle 2, as there is no request in VC 1, the arbiter checks the next position VC 2. As long as no request is met, the priority will traverse through VCs one by one, until in this case, it reaches the last VC v-1, then moves to the lowest priority point VC 0, and grants the body ﬂit (B) there (head is already dispatched). Again, the winner VC 0 will get the lowest priority, and priority points to VC 1. This example demonstrates a worst-case arbitration, where in a physical port with v VCs, the priority is advanced by v-1 steps before a request is met, a grant is generated, and the new priority is set. This worst case corresponds to the critical path in circuit implementation. At circuit level, a round-robin arbiter possesses a circular chain to propagate the priority token [4] (details in Section IV). Each step of priority advancement corresponds to a certain amount of gate delay. Therefore, increased number of VCs will lead to longer priority chain and thus longer critical path latency. Because a clock cycle has to accommodate the worst-case critical path delay, this in turn translates into longer clock cycle time, and lower clock frequency [16], as summarized in Table I. SA D ELAY AND CYC LE T IM E W I TH D I FFER ENT NUMB ER O F VC S TABLE I Design @ 45nm 4-VC 8-VC 16-VC 16-VC ATS (s=4) SA1 delay 210 ps 350 ps 630 ps 210 ps SA2 delay 250 ps Cycle time 460 ps 600 ps 880 ps 460 ps The lowered clock frequency in a many-VC router hurts the network performance, as illustrated in Fig. 4. Here, each design has the clock cycle time deﬁned in Table I while the designs in Fig. 3 assumed the same clock cycle time. Clearly, the negative impact of cycle time surpasses the beneﬁt gained from increasing VC count. The zero-load latency is increased proportionally to the cycle length, and the network saturates much earlier. From this perspective, a many-VC router seems like a bad idea for either throughput- or latency-oriented NoC design. Fig. 4. Network performance in a 8×8 mesh with 3 VC organizations at different frequencies (NOTE: injection rate and delay are based on absolute time) B. Statistics of SA1 step counts The worst case happens if majority of VCs are empty. However at runtime, such worst case may rarely happen. Fig. 5 reports the statistics of SA1 step counts, assuming a 16-VC design. Here the trafﬁc consists of 80% single-ﬂit message packets [8] and 20% 3-ﬂit data packets each of which is capable of carrying a 32-Byte cache line [14]. Such observations motivate us to speculate on that most SA1 arbitrations can be ﬁnished with small step counts and thus short time, so the router is not necessary to be clocked at much lower frequency to accommodate the worst case. More speciﬁcally, if we can break SA1 into several cycles, most arbitrations will likely to ﬁnish with one cycle. This will help in both boosting frequency and reducing latency. To exploit such opportunity, we next propose our Arbiter Timing Speculation (ATS) design. IV. ARB I TER T IM ING S P ECULAT ION (ATS ) Breaking SA1 arbitration into multiple cycles requires segmenting its critical path of priority traversal. This can be realized with proper changes to the existing arbiter circuit design. The basic idea is to limit the maximum number of steps that a priority token can travel within one cycle. If it did not meet a request within that distance, its progress should be saved before each cycle ends. Next we will ﬁrst describe a baseline arbiter design, and then derive our proposed timingspeculative arbiter from it. A. Baseline Arbiter Implementation A generic round-robin arbiter is built from arbiter cells [4]. Each cell is responsible to take one request signal and generate the corresponding grant, if it has a priority token. The priority token can be in 2 forms. First of all, a pointer is the original form of priority token, indicating the priority position in current arbitration. If no request is seen by the pointer, a carry is generated and propagates along cells starting from the pointer location, until it meets a request and grants it. So, along the path of carry, priority decreases and the cell prior to the pointer has the lowest priority. Fig. 5. Statistics of SA1 step counts with mix of 1- and 3-ﬂit packets As illustrated in Fig. 5, a signiﬁcant percentage of SA1 arbitration can be done with small step counts, especially 0step in which there is a request at the priority pointer location. This is observed under low (Flit Injection Rate, FIR = 0.03) to high (FIR = 0.39) trafﬁc loads, and at different congestion regions (center > side > corner) of the mesh network (detailed analysis in Section IV-C). In total, under high trafﬁc, 90%, 82% and 69% of SA1 arbitration can be done within 4 steps in center, side and corner nodes respectively. Fig. 6. Baseline circuit implementation of round-robin arbiter Such circuit implementation is shown in Fig. 6 with 4 arbiter cells. The carry out (COU T ) of each cell is the carry in (CIN ) of its next neighbor. The last cell sends its COU T back to the ﬁrst one, forming a circle of carry propagation. The pointer (P ) is held by ﬂip-ﬂops for the duration of arbitration, and in each cycle only one P signal can be 1 indicating the pointer location. P (cid:48) is the new pointer for next cycle arbitration, generated based on current cycle conditions. A grant (G) is generated on a valid request input (R) when a cell possess either form of priority. On the other hand, if there is no request, a cell should pass its priority to next neighbor as COU T . If a cell grants its request, its priority should be the lowest in next cycle, so its grant will set pointer to its next neighbor. If there is no request to the entire arbiter, the carry Fig. 7. The proposed Arbiter Timing Speculation (ATS) implementation will eventually travel back to the pointer location to reset the pointer. So the arbiter priority remains unchanged in case of idle arbitration. To summarize, the logic functions of an arbiter cell are as follows: G = R · (P + CIN ) COU T = R · (P + CIN ) P (cid:48) = Gi−1 + P · CIN (1) B. Proposed Timing-Speculative Arbiter Implementation To limit the priority traversal distance, we add an explicit connection from the pointer location to the cell with the maximum distance, shown as the gray bold wire in Fig. 7. In this example the maximally allowed step count s is 3. Because the pointer can be at any cell, a connection is needed from any cell i to i+s (mod by v if i+s>v). In case no request is present from pointer location i to cell i+s (but requests exist beyond i+s), cell i+s will eventually get priority token as CIN . Because it also sees a valid Pi−s input from i, it must stop propagating the token (disable COU T ) and set its own P (cid:48) as new pointer for next cycle. If there is no request to the entire arbiter, priority should remain unchanged. In baseline implementation this is achieved by propagating the token back to the pointer location. However this is not possible in our multi-cycle token chain. Instead, we can simply NOR all request inputs to detect if this is an idle arbitration, shown in Fig. 7 as the gray gate and wires. In case of idle (I ), all carries are disabled and P (cid:48) will get previous cycle’s P through the additional mux. Notice that the idle detection is done at the beginning of each cycle with a wide dynamic NOR gate in very short latency, before releasing the precharge on the dynamic gates of COU T signals. In summary, the logic functions of the proposed arbiter cell are listed below (no change on grant G): G = R · (P + CIN ) COU T = R · (P + CIN ) · Pi−s · I P (cid:48) = Gi−1 + Pi−s · CIN + I · P (2) C. Opportunity Analysis As previously mentioned, the opportunities demonstrated in Section III-B and Fig. 5 come from high VC occupancy, which can be usually observed in the following 2 cases: (1) Heavy trafﬁc and/or trafﬁc congestion When network trafﬁc load is heavy, most VCs in a port will likely be allocated. This is more evident for the routers located in the more congested center region of a mesh. The example in Fig. 8 illustrates SA1 with heavy load. Pointer advancement induces grants on consecutive VCs and thus arbitrations ﬁnish in 0-step. When not all VCs are occupied, or ﬂits in some VCs are not eligible for SA due to no free output resource in the downstream router [9], it is still likely that arbitration can be done within a small number of traversal steps. Fig. 8. SA1 arbitration example under heavy trafﬁc or congestion Assuming that we have only 3-ﬂit packets, we plotted the distribution of SA1 step counts in Fig. 9. Under very low trafﬁc (FIR = 0.03), in all three regions nearly 2/3 of arbitrations need v-1 (16-1=15) steps (Fig. 2). As trafﬁc load increases, the distribution starts to migrate to small step counts. The routers in the center region tend to have more small step counts than those in side and corner regions. However, with only 3ﬂit packets, the opportunity for our timing-speculative arbiter design is inferior to what is shown in Fig. 5, especially with light trafﬁc. Fig. 9. Statistics of SA1 step counts with only 3-ﬂit packets (2) Many single-ﬂit packets, even in low trafﬁc With worm-hole routing, VCs are allocated at packet level. Once a head ﬂit is assigned an output VC, all following ﬂits will be sent to the same VC. On the other hand, when a new packet comes, it will try to ﬁnd a new output VC for it, unless all VCs are unavailable [4]. As recent study has shown that single-ﬂit packets dominates NoC trafﬁc in real workloads [8], the VC occupancy is often high because the same amount of ﬂits now belong to more packets which are dispatched to as many VCs as possible. As a result, the percentage of worstcase arbitration is greatly reduced especially with lower trafﬁc loads (Fig. 9 vs. Fig. 5). This is illustrated as the consecutive grants in Fig. 10, which assumes the same low trafﬁc load as in Fig. 2: only 3 ﬂits are in this input port. Fig. 10. SA1 arbitration example under light trafﬁc with only single-ﬂit packets (S = Single) To summarize, with both (1) and (2) considered, we expect high VC occupancy under various trafﬁc load cases, which presents the opportunity that can be harvested by ATS as discussed in Section III-B and Fig. 5. D. Evaluation Results For a 16-VC ATS arbiter to achieve same frequency as the 4-VC arbiter, we pick the maximally allowed step count per cycle s to be 4 (Table I). s = 4 also implies a 4-cycle loop latency on the priority traversal path. Our proposed ATS design is compared to the baseline 4-VC design with 460ps cycle time, the realistic 16-VC design with 880ps cycle time, and an ideal 16-VC design with same cycle time as the 4-VC one, to purely demonstrate the advantage of better VC organization. As shown in Fig. 11, 16-VC with ATS signiﬁcantly outperforms the lower-frequency 16-VC design, and regain the throughput beneﬁt of 16-VC organization over the 4-VC one. This is not only because of the high frequency, but also due to exploiting the speculation opportunities with heavy trafﬁc. Moreover, speculations under low trafﬁc successfully reduces zero-load latency comparing to the 16-VC result. However, a delay gap is still observed from 16-VC-ideal. As frequency is already identical to the ideal case, such delay increase must come from rise in cycle count. Fig. 11. Network performance in a 8×8 mesh (Uniform Random trafﬁc, X-Y routing) V. FURTH ER LATENCY IM PROV EM EN T S A. Group Priority Bypass (GPB) While ATS greatly improves router performance, the latencies under low to medium trafﬁc loads still show notable slowdown over the ideal router design (Fig. 11). This is due to the fact that the arbitrations requiring more than 4 arbitration steps now need multiple cycles to ﬁnish. For example in Fig. 2, ATS will grant the tail ﬂit in cycle 9 instead of cycle 3. To alleviate this problem, we propose Group Priority Bypass (GPB) that partitions VCs into groups and fast-forwards priority pointer to VCs containing valid request inputs. In Fig. 12, arbiter cells are organized into 4 groups with 4 cells each. The solid black wires are the carry signals (within-group wires not shown). The carry output from group A can either connects to the carry input of group B, or directly bypass group B to group C, depending on the bypass signal of group B. Similarly, it may also bypass group C and even group D, looping back to its own group. This is most likely the case of the worst-case arbitration, where the valid request is at the lowest priority position. Fig. 12. A simple illustration of the proposed Group Priority Bypass (GPB) implementation A group should be bypassed if no valid request is seen on all its cells. This can be simply implemented as a group idle detection which NORs all request signals in the group, same as the arbiter idle in Fig. 7. However, if priority pointer resides in an idle group, it should be able to propagate the token as carry out. Therefore, pointer detection is also implemented using NOR function, and the ﬁnal group bypass signal is generated by ANDing the outputs of the two NOR gates, as bypass = (group idle) · (no pointer) In our ATS design we utilize explicit wiring to limit the travel distance of priority token. In presence of priority bypass, the i+s position is varying. Therefore, a connection is needed from i to all possible positions i+s, i+2s and i+3s. This is shown as the example on cell 0, 4, 8 and 12 with the dashed black wires in Fig. 12. The wire implements a buslike interface. For instance, P signal of cell 0 is the bus driver if group A holds the pointer, and cell 4 is its consumer. If group B (and C) is bypassed, cell 8 (12) becomes consumer. In case group B, C and D are all bypassed, no consumer is attached and the bus is actually not needed. This is because the token will ﬂow back to group A and grants a request there. For example, if arbiter is not idle, pointer is at 3 and group B, C, D are bypassed, there must be at least one request at 0, 1, and 2, which is guaranteed to be granted within one cycle. Of course, there also exist other 3 buses 1-5-9-13, 2-6-10-14 and 3-7-11-15. The above controls apply to all 4 buses together. Such bus control is based on pointer detection and bypass signals (B ) of all groups, and is handled by simple combinational logic and tri-state drivers. To be the bus driver, one group simply need to have the pointer inside, indicated by the low logic level of no-pointer (N ) signal. On the other hand, a bus driver or a bypassed group should not be a bus consumer. A valid consumer can be in 3 cases: 1) its previous group j-1 has pointer; 2) its previous group is bypassed but the one before it (j-2) has pointer; 3) its previous 2 groups are bypassed but the one before them (j-3) has pointer. Therefore, the ﬁnal consumer logic is shown below. In case of not a consumer, it will disable the tri-state drivers to all Pi−s inputs in the group, and pull Pi−s inputs to ground voltage. consumer = N ·B · (Nj−1 +Nj−2 ·Bj−1 +Nj−3 ·Bj−1 ·Bj−2 ) All above mentioned control signals are generated at the beginning of each cycle in parallel with arbiter idle, before releasing the precharge of COU T signals. So when priority token starts to traverse, all muxes and buses are already conﬁgured. Also notice that the GPB design requires no change to ATS arbiter cell functions as deﬁned by equation (2). The only difference with GPB is the conﬁgurability of Pi−s signal which is not visible to cells. B. Body/Tail Priority (BTP) As VC occupancy increases, a round-robin arbiter takes longer time to ﬁnish one round of service. For a many-VC router, it results in longer delay for especially data packets with more than one ﬂit. For example in Fig. 8 with 16 VCs, the tail ﬂit in VC0 will be served at cycle 33. To mitigate long packet delay due to such waiting time, we utilize Body/Tail Priority (BTP) that prioritizes the arbitration of body and tail ﬂits. That is, priority pointer is not advanced on granting head and body ﬂits, resulting in consecutive grants to all ﬂits in the same data packet. Notice that in a 2-ﬂit-deep VC the tail will not be accommodated. However it can still beneﬁt from BTP because its body is dispatched early. The circuit implementation is based on the 2-bit ﬂit-type encoding carried by each ﬂit: 00 - Single, 01 - Tail, 10 - Body, 11 - Head. The higher bit of encoding (F ) can be employed as pointer control: F =1 indicating H/B type and pointer should remain unchanged; F =0 indicating T/S type and pointer should advance to next arbiter cell. Therefore, the logic functions of the ATS arbiter cell with BTP are listed below (no change on G and COU T ). In P (cid:48) the G · F term corresponds to a grant on H/B at current cell, and the Gi−1 · Fi−1 term implies a grant on T/S at previous cell. Notice that BTP requires no change to GPB design and they can be combined naturally. BTP is also applied to SA2 arbitration to assist consecutive dispatch. G = R · (P + CIN ) COU T = R · (P + CIN ) · Pi−s · I P (cid:48) = Gi−1 · Fi−1 + G · F + Pi−s · CIN + I · P (3) Such biased priority on Body/Tail ﬂits may bring forth fairness concern. Notice that round-robin arbiters are serving at ﬂit granularity. However such ﬂit-level fairness is unfair to longer packets. BTP actually guarantees packet-level fairness which is the true meaning of fairness in presence of mix-length trafﬁc. Moreover, BTP will not incur starvation because at any time, there are only limited number of VCs and each packet has only limited number of ﬂits. Hence, a ﬂit can always become a SA1 winner within ﬁnite number of arbitrations. C. Opportunity Analysis Under light trafﬁc loads our GPB design will enjoy plenty of bypassing chances. With high trafﬁc and VC occupancy the opportunity decreases but it will not hurt performance, because this in turn implies more opportunities for singlecycle arbitration. Therefore we expect our GPB design to be a perfect enhancement to the ATS design. Fig. 13. Statistics of SA1 step counts with GPB and BTP (mix of 1- and 3-ﬂit packets) More importantly, our GPB design actually upper-limits the arbitration step count to 7. For example, priority pointer is at 0 and a request is at 7. In the ﬁrst cycle the pointer will advance to 4, and in next cycle it grants the request at 7. All larger step counts will be converted down to 1∼7 steps by group bypassing. Therefore, every arbitration will be ﬁnished in at most 2 cycles. In addition, BTP actively promotes 0-step grants, further increasing probability of single-cycle arbitration. With joint effect of GPB and BTP, the statistics of SA1 step counts is shown in Fig. 13. In all conditions >95% of arbitrations can be done within 1 cycle (4 steps). D. Evaluation Methodology and Results The baseline and proposed arbiter circuits are implemented at transistor level with 45nm PTM [18] device models. We utilize dynamic domino-logic circuit style, and precharge of pointer (P (cid:48) ) and grant (G) signals is delayed to the beginning of next cycle for ﬂip-ﬂops to capture them. All results are obtained from SPICE-level simulations at SSLH corner with slow devices, 0.9V VDD (90% of nominal 1V), and 90 ◦C. To model and compare different network designs, we use a cycle-accurate, SystemC based NoC simulator which is heavily modiﬁed from Noxim [19]. An 8×8 mesh topology with 4-stage pipelined (RC, SA, ST, LT) routers is modeled in this paper. Each router has 5 ports (N, E, S, W, and local), and all designs are compared with equal size of 32 buffer slots per input port. Each ﬂit contains 128 bit data payload and 16 bits of sideband control signals [14]. Sideband information is updated at each hop based on the assigned VC. Flow control is credit based at ﬂit granularity. A data packet consists of a head ﬂit carrying routing information, and a body and tail ﬂit carrying a 32-Byte cache line [14]. Different message packets consist of a single ﬂit carrying routing information and commands. 80% of network trafﬁc are message packets and 20% are data packets [8]. All simulations are warmedup with 50000 cycles and then ran 200000 cycles for data collection. Simulation results with different trafﬁc patterns are summarized in Fig. 14. In all delay plots, the proposed GPB+BTP (a) Uniform Random (b) Transpose (c) Butterﬂy (d) Bitreversal (e) Shufﬂe (f) Throughput with Uniform Random Fig. 14. Average network latency and throughput results with different trafﬁc patterns (8×8 mesh, X-Y routing) design on top of ATS is able to effectively remove the extra cycle counts, and thus achieves nearly identical latency as the ideal case. The small difference from the ideal one is due to the small percentage of 2-cycle arbitration. This corresponds to improvements of around 48% on zero-load latency and 100% on network throughput over a naive 16-VC design in Fig. 14(a). Fig. 14(f) compares network throughput. As expected, ATS is as good as 16VC-ideal on throughput, while GPB and BTP are mainly for latency improvement. For different trafﬁc patterns, our design achieved consistent latency and throughput improvements, indicating it is a robust design. E. Overhead Base on our circuit implementations, Table II compares the power and area of 3 round-robin arbiter designs. Although the proposed arbiter shows large overhead from existing 16-input design, it is still negligible because power and area in a router is dominated by crossbar and buffers [11], [12], [14], [16]. The overall contribution of all instances of the proposed arbiter in a router is about 4% area and 2% power, which can be well justiﬁed given the signiﬁcant performance beneﬁts. Also, it is still less power and area hungry than wavefront-based allocator [2] and higher level speculation techniques using duplicated resources [11], [12]. AR EA AND POW ER COM PAR I SON O F ARB I T ER IM PL EM EN TAT ION S Static power 0.64 µW 2.44 µW 3.18 µW Area 100 µm2 380 µm2 530 µm2 Design 4:1 16:1 16:1 ATS+GPB+BTP TABLE II Dynamic power 0.43 mW 0.81 mW 1.36 mW V I . R ELAT ED WORK S Virtual channel ﬂow control [5] has became a widely adopted design choice in NoC routers. Dividing the buffer storage of a physical channel into multiple Virtual Channels (VC) can substantially improve network throughput by alleviating the Head-of-Line (HoL) blocking: active ﬂits have chance to pass blocked ﬂits using network bandwidth that would otherwise be left idle. It was then identiﬁed that the optimal VC organization is trafﬁc dependent [13]. Following this observation, ViChaR [10] took advanages of its dynamically allocatable uniﬁed buffer to dispense a variable number of VCs on demand. The VC count can range anywhere from f/p to f, where f is the total buffer size in ﬂits, and p is the number of ﬂits per packet. As a result, the arbiters in the SA1 stage need to be as wide as f :1. However, evaluations in this paper were on a cycle basis and failed to take into account the negative impact of wide arbiters on router frequency. Fortunately, our circuit design can be seamlessly integrated into ViChaR, making viable its powerful routing control in a high-frequency router implementation. At micro-architecture level, there have been several speculation designs that reinterpret the critical switch allocation. SA-folding [11] was proposed to handle the process variation induced SA1 delay variation which may cause glitches or erroneous grants at SA2 outputs. It splits SA1 and SA2 into two pipeline stages to make sure inputs of SA2 are always synchronized, with a side bonus of halving the clock cycle time. To overcome the additional pipeline stage, two sets of SA2 arbiters are equipped where one set executes SA2 speculatively in parallel with SA1. The SA1 result is discarded if the speculation succeeds, otherwise it goes through the normal SA2 in the next cycle. In contrast, in [12] the entire SA stage is duplicated for a speculative execution in parallel with normal SA, which removes the dependency between VA and SA by assuming a successful VA. To avoid any impact on throughput, the non-speculative results are prioritized over speculative ones. In both these designs, signiﬁcant area and power overheads are expected due to largely doubled resources, however the overall effect is negligible because the SA unit represents a very small portion of the entire router [11], [12]. Nevertheless, they still suffer from reduced frequency and throughput with increasing VC count, and again our circuit techniques can be easily applied as an efﬁcient enhancement. At circuit level, a recent paper [2] studied allocator implementations and compared their delay, power, area, and matching quality. A separable allocator generates a valid matching by decomposing allocation into arbitration across input requesters (SA1) and arbitration across output resources (SA2). Executing the two arbitrations in input-ﬁrst order (used in our study) generally yields lower delay than in outputﬁrst order. On the selection of arbiter circuit, matrix arbiters are faster than simpler round-robin arbiters but such delay reduction usually does not outweigh the associated area and power penalty. The main drawback of separable allocators is that they are not guaranteed to produce maximal matchings because input and output arbitrations are performed independently. In contrast, wavefront-based allocators possess excellent matching quality by effectively considering requesters and resources simultaneously. Although the wavefront circuit for output selection is slow, the overall allocation delay can be comparable to its separable counterpart because the input preselection for each potentially granted output is off the critical path. On the other hand, wavefront-based allocators are very expensive in terms of power and area because: 1) the wavefront circuit is much more complex than SA2 circuits in separable allocators, and its area grows quadratically with the number of inputs; 2) the input selection requires p v:1 round-robin arbiters per input port instead of only one in separable allocators. Finally, investigations on network-level performance suggest that despite the differences in matching quality, the advantage on saturation rate of wavefront-based over separable allocators is negligible in a 8×8 mesh network. Beyond above mentioned implementations, some other circuit efforts have been carried out to reduce the arbiter latency. The design in [6] divides all input requests into two groups which are handled by two separate priority encoders in parallel. It requires a huge thermometer decoder to generate the request division vector, which is too complicated for the simple round-robin scheme. In another simple tree structure design [3], the round-robin rule is guaranteed only when all requests are present, otherwise it causes severe unfairness. Nevertheless, none of these designs can avoid increased clock cycle time as a result of longer delay from increased number of inputs, no matter what their delay-radix dependency is. Therefore every single arbitration will be punished by the longer cycle time. On the other hand, our proposed design can keep clock cycle time unchanged regardless of the number of inputs and the full round-trip delay, and majority of arbitrations still enjoy the short 1-cycle operation. This is based on and demonstrated by our opportunity study, and supported by our active effort in boosting the percentage of single-cycle arbitration. Another recent work [17] implemented a bi-modal arbitration that dynamically switch to a biased mode in which a fast-forward path is created for one incoming channel. The switching is based on local recent trafﬁc history, and such design is well suited for a mesh-of-trees topology. V I I . CONCLU S ION In this paper we proposed and elaborated a novel highfrequency many-input arbiter design for many-VC routers. It fully exploits, and further enhances, the high possibility of short and fast arbitrations in the conditions of high VC occupancy. With the proposed arbiter design, a many-VC router is able to run at same frequency as a router with much less VCs, and still enjoys the beneﬁt of better VC organization, without penalty in network latency. We also provided detailed study and analysis on the speculation opportunities under various trafﬁc conditions. "
PROBE - Prediction-based optical bandwidth scaling for energy-efficient NoCs.,"Optical interconnect is a disruptive technology solution that can overcome the power and bandwidth limitations of traditional electrical Networks-on-Chip (NoCs). However, the static power dissipated in the external laser may limit the performance of future optical NoCs by dominating the stringent network power budget. From the analysis of real benchmarks for multicores, it is observed that high static power is consumed due to the external laser even for low channel utilization. In this paper, we propose PROBE: Prediction-based Optical Bandwidth Scaling for Energy-efficient NoCs by exploiting the latency/bandwidth trade-off to reduce the static power consumption by increasing the average channel utilization. With a lightweight prediction technique, we scale the bandwidth adaptively to the changing traffic demands while maintaining reasonable performance. The performance on synthetic and real traffic (PARSEC, Splash2) for 64-cores indicate that our proposed bandwidth scaling technique can reduce optical power by about 60% with at most 11% throughput penalty.","PROBE: Prediction-based Optical Bandwidth Scaling for Energy-efﬁcient NoCs Li Zhou and Avinash Karanth Kodi School of Electrical Engineering and Computer Science Ohio University, Athens, OH 45701 E-mail: { lz792711, kodi }@ohio.edu Abstract—Optical interconnect is a disruptive technology solution that can overcome the power and bandwidth limitations of traditional electrical Networks-on-Chip (NoCs). However, the static power dissipated in the external laser may limit the performance of future optical NoCs by dominating the stringent network power budget. From the analysis of real benchmarks for multicores, it is observed that high static power is consumed due to the external laser even for low channel utilization. In this paper, we propose PROBE: Prediction-based Optical Bandwidth Scaling for Energy-efﬁcient NoCs by exploiting the latency/bandwidth trade-off to reduce the static power consumption by increasing the average channel utilization. With a lightweight prediction technique, we scale the bandwidth adaptively to the changing trafﬁc demands while maintaining reasonable performance. The performance on synthetic and real trafﬁc (PARSEC, Splash2) for 64-cores indicate that our proposed bandwidth scaling technique can reduce optical power by about 60% with at most 11% throughput penalty. I . IN TRODUC T ION As chip multiprocessors (CMPs) have become an important approach to achieve better performance and power efﬁciency in the many-core era, tens and even hundreds of cores will be integrated on a single chip [1]. An energy and area efﬁcient NoC architecture is becoming increasingly important to meet the multicore performance requirements. Recent research has explored optical interconnect as an alternative to traditional electrical signaling, due to high bandwidth density, low propagation latency, and distance-independent power consumption [2], [4], [5], [6], [7]. Research has shown that optical NoCs could signiﬁcantly improve the achieved bandwidth by 2-6X, and reduce the power consumption by more than 10X when compared to electrical networks [4], [5], [10]. Although the beneﬁts of optical interconnects are signiﬁcant, there are several challenges [3], [9]. Thermal sensitivity and process variations of silicon devices are two potential issues that directly affect the operation of optical devices and impact the performance and reliability of optical NoCs. Different from traditional NoCs which consume large dynamic power, static power consumption induced by external laser, and microring resonators (MRRs) tuning dominate the power budget of optical NoCs. Research has shown that 74% of network power can be attributed to the laser and tuning power in a conventional radix-32 optical crossbar [11]. Reducing static power consumption of optical channels will require latency and bandwidth trade-off in future designs. Prior work has reduced power consumption by exploring the non-uniform trafﬁc and unbalanced resource utilization commonly seen in multicore applications. By gathering the information on past resource usage, the network predicts link and buffer utilization for each channel, and dynamically re-allocates the on-chip resources or tunes the voltage or frequency of the link [12], [13], [14], [15], [16]. For those channels that have low link utilization, the under-utilized bandwidth will be re-allocated to those channels that request more bandwidth, or save power by decreasing the voltage and frequency levels of the link. Recently proposed R-3PO architecture also beneﬁts from 3D stacking of multiple layers and reconﬁgurable interconnects [15]. The effectiveness of dynamic resource re-allocation largely depends on the effectiveness of trafﬁc prediction and reconﬁguration implementation. Current research on optical NoCs has not yet solved the issue of static power consumption when bandwidth demands are low across the network. In optical NoCs, low channel utilization leads to the problem of over provisioned bandwidth. The external laser is always switched on even under low network trafﬁc that consumes unnecessary static power which can reduce the beneﬁts of nanophotonics. As network trafﬁc is often unbalanced and vary with different applications over time, future multicore applications require the network to provide sufﬁcient bandwidth for trafﬁc burstiness and ﬂuctuation under high network load, while consuming reasonable power under low network load. To address the static power problem and improve the bandwidth efﬁciency in optical NoCs, we propose PROBE: Prediction-based Optical Bandwidth Scaling for Energyefﬁcient NoCs, a history-based dynamic bandwidth scaling (DBS) scheme based on past link utilization. We tune the bandwidth of each channel according to the network trafﬁc to meet the performance requirements by dynamically shuttingoff portions of the network. We design an efﬁcient scheme with a lightweight trafﬁc predictor to increase the channel utilization and reduce static optical power. The DBS scheme is evaluated using synthetic trafﬁc and traces from Splash2 [26], PARSEC [27] and SPEC CPU2006 [28] benchmarks. Our simulation results show that the DBS scheme achieves about 60% optical power saving with at most 11% throughput penalty. In summary, the major contributions of this paper include: • We propose a dynamic bandwidth scaling (DBS) scheme that tunes the channel bandwidth adaptively to reduce the optical power consumption. • We present a two-level bandwidth control mechanism to set the channel bandwidth globally while collecting resource utilization and tuning ring resonators locally. • We design a lightweight trafﬁc prediction scheme, and propose three different modes (Performance, Balanced and Power-aware) which provide latency/bandwidth trade-off. I I . HARDWAR E ARCH I T EC TUR E AND IM P LEM EN TAT ION A. Architecture Design We initially discuss the optical architecture that will be used to test and evaluate our proposed DBS scheme. Figure 1 shows the layout of our network design. We combine four cores together and connect them with a shared L2 cache, which we call a tile. The layout consists of 16 tiles in a grid fashion with 4 tiles in x and y-directions. All tiles in each direction are fully connected. Instead of using a single off-chip laser, the ith-tile is assigned with a ith laser that provides the optical signal for all the output channels of that tile in either x or y-direction. Each laser is associated with an off-chip voltage regulator (VR) that tunes the power supplied to the waveguide. For example, in Figure 1, L0 is assigned to Tile 0 to provide the optical signal for the output channel of Tile 0 in x and y-directions, and V R0 is used to tune the supply power of L0 . B. Tunable Silicon Nanophotonic Device and Components 1) Tunable Power Ratio Splitter: Optical power splitters are essential components that distribute optical power. The passive splitter has a ﬁxed power ratio which leads to low power efﬁciency. It is desirable to tune the power ratio dynamically to improve the network utilization. Using multimode interference (MMI) devices, tunable power splitters can be implemented by inducing phase change on top of the multimode section. Varying power-splitting ratios can be achieved by only slightly biasing the refractive index [20], [21], [22]. The driving voltage of the tunable power splitter is 0.9 V, tuning speed is 6 ns, and the access waveguide is only 5 µm pitch, thereby making these devices compatible with 22 nm Complementary metal-oxide-semiconductor (CMOS) process [21], [22]. In this paper, we use the tunable 1x2 power splitters with 3 dB (50 : 50) power ratio under unbiased driving voltage, and a tuning range up to 20 dB (∼99% input power outputs in either of the channels). 2) Three-level Binary-tree-based Waveguide Design: First, we introduce our dynamic optical channel design which is based on the binary-tree waveguide prototype proposed in [19]. Figure 2(a) shows a channel between any two tiles. The channel consists of a three-level binary-tree waveguide with four branch waveguides. Optical signal is split into two parts at each Y-branch by using the tunable power splitter with an on-chip voltage regulator. αi is the power ratio of the splitter at ith-level Y-branch, βi is the output power of ith branch. By varying the driving voltage of each splitter, the power ratio αi can be changed at run time. Assuming the excess loss for ith Y-branch is ei , so (1 − ei ) is the effective input power that is split into two parts. We assume the same excess loss e for each Y-branch. The equations for each βi and the power loss due to m − 1 times splitting are given below [19]: i−1(cid:89) m−1(cid:89) (1 − αk ), 1 ≤ i ≤ m − 1, βi = αi (1 − e)i βm = (1 − e)m−1 (1 − αk ) (1) k=1 k=1 power loss = −10 lg βi (2) m(cid:88) Fig. 1. Layout of the network design(Ri : Router connected to Tile i, L: laser, VR: voltage regulator). Inter Tile Communication: The inter tile communication is same in either x or y-direction. One waveguide is assigned to each tile. By using multiple splitters, each waveguide splits into six channels that connect to the other six tiles in x and y-direction [7]. This design requires three channels per tile or a total of 12 channels for each direction. The waveguides are implemented in a U-shape, which allows for optical data to be modulated on the channels on the ﬁrst pass and for the optical data to be received on the second pass. i=1 where m is the number of branch waveguides which is 4 is our case. With Dense Wavelength Division Multiplexing (DWDM) technique, up to 64 wavelengths can be transmitted within the same waveguide. The effective bandwidth of a optical interconnect is given by Bw = WN ∗ WgN ∗ BR , where WN is the number of wavelengths, WgN is the effective number of waveguide branches, and BR is the effective bit rate of the channel. With WN = 64, WgN = 4, and BR = 5 Gb/s, we obtain a channel bandwidth of 1.28 Tb/s. By dynamically changing the driving voltage at the third splitter (from Figure 2(a)), we can transfer all the power from branch 4 to branch 3. Therefore, with WN = 64, WgN = 3, and BR = 5 Gb/s, the channel bandwidth is tuned to 960 Gb/s. With varying (a) (b) (c) Fig. 2. Tunable channel and the three-level binary-tree-based waveguide. (a) Asymmetrically optical splitting of the channel. (b) Four power states of the channel using different power ratios of the splitters to accommodate the different bandwidth demands. (c) The structure of the waveguide assigned to tile 0, and the power states of the channels at cycle k . number of WgN , we deﬁne four power states of the channel: Pstate 1 (WgN = 4, Bw = 1.28 Tb/s), Pstate 2 (WgN = 3, Bw = 960 Gb/s), Pstate 3 (WgN = 2, Bw = 640 Gb/s), and Pstate 4 (WgN = 1, Bw = 320 Gb/s). The proposed power states are based on the effective bandwidth of the channel and the implementation of the four power states is shown in Figure 2(b). As shown in Figure 2(b), a cross mark on a branch indicates no output power in such a branch. Reducing the number of branches requires less power from the laser. Moreover, the power loss due to splitting decreases as more branches are shut-off and the depth of the tree decreases. To further reduce the optical power, the microring resonators along the closed branches that are used to modulate/detect the signal can be temporarily powered off. Table I shows the channel bandwidth, splitter ratios and power loss for each of the four power states assuming a same access loss e (= 0.2 dB). TABLE I POW ER STATE S CON FIGURAT ION W I TH BANDW IDTH AND CORR E S POND ING S P L I T T ING RAT IO S . Pstate 1 2 3 4 Bandwidth(Tb/s) α1 α2 α3 1.28 1/4 1/3 1/2 0.96 1/3 1/2 1 0.64 1/2 1 NA 0.32 1 NA NA power loss(dB) 0.49 0.39 0.30 0.2 Based on the tunable channel, we design a three-level binary-tree-based waveguide that includes all output channels of a tile in x and y-directions. We present the structure of the waveguide including all output channels of the tile in Figure 2(c). As shown in Figure 2(c), laser 0 is used to provide the optical signal. At level 1, the waveguide splits into two direction waveguides, x and y-direction. At level 2, each x or y-direction waveguide splits into 3 channels, each connecting to a tile in the same direction. At level 3, each channel splits into 4 branch waveguides. In each level, we use α(i,j ) to indicate the power ratio of the splitter of j th Y-branch in ith-level, each splitter is associated with a voltage regulator to tune the power ratio at run time. We take x-direction as an example to calculate the power ratio of each splitter. At level 1, α(1,1) is calculated by the output power in x-direction waveguide over effective input power from the laser. At level 2, α(2,1) is calculated by the output power in channel 1 over the effective input power from x-direction waveguide, and α(2,2) is the ratio of the output power in channel 2 over the total power in channels 2 and 3. As we assume the same receiver sensitivity for each receiver, we estimate the required power by the number of effective branches. At level 3, each channel is in one of the four power states, and the power ratio of each splitter is set according to its power state and pre-deﬁned in Table I. The total splitting power loss is given below: (cid:80)m i=1 oni ∗ (1 − e)li (cid:80)m power loss = −10lg i=1 oni (3) where m is the total number of leave branches of the threelevel binary-tree, oni equals 0 when branch i is closed, otherwise it equals 1, li is the level of the leave branch in the three-level binary-tree. In this paper, we apply the worstcase splitting power loss which is 0.98dB for the optical power estimation. I I I . DYNAM IC BANDW ID TH SCA L ING (DBS ) SCH EM E We introduce the scheme in two parts, ﬁrst we depict our trafﬁc statistics and prediction model, and second we discuss the dynamic tuning algorithm. A. Trafﬁc Statistics and Prediction We adopt two trafﬁc indicators such as link utilization (Linkutil ) and buffer utilization (Buf f erutil ) to track the network trafﬁc load, and take measurements every cycle by using hardware counters [12]. Each hardware counter is associated with an optical transmitter. For each tile, there are three counters (one for each tile in each direction) to monitor the trafﬁc utilization and provide the link and buffer information to a local Bandwidth Tuning Controller (BTC). All the statistics are measured over a sampling time called Reconﬁgurable Windows, Rt w , where t presents the reconﬁguration time. This sampling window impacts performance, as reconﬁguring ﬁnely incurs latency penalty and reconﬁguring coarsely may not adapt in time for trafﬁc ﬂuctuations [15]. In our performance section, we show that by utilizing network simulations to determine the optimum size for Rw . For calculation of Linkutil and Buf f erutil at conﬁguration time t, we use the following equations [12], [15]: (cid:80)Rt w t=1 Activ ity(t) Rt w Link t util = (cid:80)Rt (4) (5) Buf f ert util = w t=1 Occupancy(t)/Buf f ersize Rt w the Link t where Activity(t) is 1 if ﬂit traverses the link in cycle t, or is 0 if no ﬂit is transmitted on the link for a given cycle. Occupancy(t) is the number of buffer occupied at each time t, and Buf f ersize is the total number of buffers for the given link. As the bandwidth of the channel will vary over the time, util is normalized to the full bandwidth when storing, and adjusted based on current bandwidth when predicting. 1) Design of Trafﬁc Prediction: We implement two predictors, and choose the prediction from either predictors according the network trafﬁc. First Predictor: The ﬁrst predictor is used under low trafﬁc variation, named weighted trafﬁc predictor. By taking a weighted average of current network statistics (Linkutil and Buf f erutil ) with past network statistics, the network will gradually tune the bandwidth to prevent temporal and spatial trafﬁc ﬂuctuations affecting performance. We calculate the link follows: and buffer prediction (Linkutil,pred and Buf f erutil,pred ) as P astutil ∗ weight + C urrentutil weight + 1 P redictutil = (6) where weight is a weighting factor and we set this to three in our simulations [23]. Second Predictor: However the trafﬁc burstiness and sudden ﬂuctuations will make the trafﬁc trends hard to predict [17]. With the observation of that application behavior is repetitive, in this paper we extend the prior work in [16] to apply a two-level predictor which is called history patternbased trafﬁc predictor to obtain better prediction for the link utilization. The prediction of channel trafﬁc will be chosen from the second predictor under high trafﬁc variation, or from the ﬁrst predictor under low trafﬁc variation. Implementing the second predictor, each local BTC tracks the Linkutil of corresponding channels from previous k time intervals, and uses them to index a history pattern look-up table for trafﬁc prediction. We ﬁrst deﬁne ﬁve trafﬁc load levels for the Linkutil by equally dividing the scale 0.0 ∼ 1.0 into 5 parts. Each part indicates a trafﬁc load level, from the lowest level 1 (Linkutil between 0.0 ∼ 0.2) to the highest level 5 (Linkutil between 0.8 ∼ 1.0). We set k to ﬁve in our history pattern table design. The two-level predictor has two tables as shown in Figure 3. In each entry of history trafﬁc pattern Fig. 3. Two-level history pattern trafﬁc predictor. Channel 0(i, x, 0) denotes channel 0 in the x-direction of Tile i. H5 : H1 = (5, 3, 1, 5, 4) is the index for locating the predicted trafﬁc load in prediction table (tag = 0, index = 53154, and p = 3 which predicts Linkutil between 0.4 ∼ 0.6) in last time interval. H0 is the current link statistic level which equals 4 (Linkutil between 0.6 ∼ 0.8). The predictor made a wrong prediction, so update the value P of the index(= 53154) with 4. Then make a left shift to H5:H0 as the arrows show in the ﬁgure, use new index H5 : H1 = (3, 1, 5, 4, 4) to ﬁnd the predicted trafﬁc load level (tag = 0, index = 31544, p = 2 (Linkutil between 0.2 ∼ 0.4)) for the next time interval. w w table, channel i(j, x/y, k) denotes channel i (i = 1, 2, 3, ..., 6) which is j th-channel (j = 0, 1, 2) in the x or y-direction of Tile i, and i is used as the tag in prediction table. H5 ∼ H1 are the quantized Linkutil levels for previous 5 time intervals Rt−1 to Rt−5 for the given link. H0 is the current Linkutil of the time interval Rt w . In each entry of prediction table, tag is used to differentiate the channels. Index is used to look up the table for trafﬁc prediction, and is a combination of H5 ∼ H1. P is the predicted Linkutil . The replacement policy used in the prediction table is LRU. After each time interval, local BTC ﬁrst compares the H0 of the channel with the predicted Linkutil for a given index(H5 : H1), update P if the predictor made a wrong prediction. Then the numbers from H5 to H0 make a left shift to get a new index for look-up in the prediction table to ﬁnd the predicted trafﬁc load for the next time interval Rt+1 w . If the entry is not found in prediction table, then H0 is used as the predicted trafﬁc load instead. An example is shown in Figure 3. 2) Predictor Selector: The local BTC automatically choose the predicted link utilization from either the weighted trafﬁc predictor or the history pattern trafﬁc predictor according the trafﬁc ﬂuctuation. The ﬁrst predictor, weighted trafﬁc predictor is chosen under low trafﬁc variation, and the second level history pattern trafﬁc predictor is chosen under high trafﬁc variation. We apply the saturating counter which is used in branch prediction for each link to select the trafﬁc predictor according to the level of trafﬁc variation. When current predictor gives wrong predictions twice, the local BTC will start to choose the prediction from the other predictor. B. Algorithm Implementation Based the predict link and buffer utilization, we introduce our dynamic bandwidth scaling algorithm as below: we design a two-level control system including 1) local BTC which is R ECON FIGURAT ION A LGOR I THM FOR DBS . TABLE II w Wait for Reconﬁguration window, Rw Each local BT Ci send a request packet to its local tile hardware counters for Linkutil and Buf f erutil from previous Rt−1 Each hardware counters sends Linkutil and Buf f erutil for previous Rt−1 w to its local BT Ci Each local BT Ci classiﬁes the link statistic for each hardware counter as: if Linkutil,pred < Linkutil,lower Under -Utilized: Decrease bandwidth if Linkutil,lower ≤ Linkutil,pred ≤ Linkutil,upper and Buf f erutil,pred ≤ Buf f ercon Normal-Utilized: Remain bandwidth if Linkutil,pre > Linkutil,upper or Buf f erutil,pred > Buf f ercon Over-Utilized: Increase bandwidth Each local BT Ci sends bandwidth adjustment information to the corresponding off-chip global BT Cj . Power off parts of the ring resonators if it decides to lower the bandwidth, or remains current conﬁguration until a respond arrives Each global BT Ci checks the current power state of each corresponding local BT Ci , decides to upper or lower the bandwidth of corresponding channel according to requests from local BT Ci s and the next power state each corresponding local BT Ci should be Each global BT Ci looks up the power ratio conﬁguration table, then tunes the power supply of each corresponding laser and sends a response packet to each corresponding local BT Ci Each local BT Ci enable portion of the ring resonators according to the response from the corresponding global BT Ci Go to Step 1 Step 1: Step 2: Step 3: Step 4a: Step 4b: Step 5a: Step 5b: Step 6: Step 7: located at each tile and 2) global BTC which is located near each off-chip laser. Each global BTC controls six local BTCs of a tile whose output channels are fed by the laser assigned to the global BTC. When varying the bandwidth of the channel, local BTC is responsible to enable/disable the MMRs and signal conversion back-end circuitry. Each global BTC maintains the power states of all corresponding channels, and collects the bandwidth requests from local BTCs. According to the bandwidth requests, each global BTC calculates the power ratio of each splitter, and is responsible to tune the laser and splitters. Table II shows the reconﬁguration algorithm in PROBE. IV. P ER FORMANC E EVALUAT ION In this section, we evaluate the performance, power efﬁciency and overall overhead of proposed DBS scheme through simulations using synthetic trafﬁc patterns and traces form real trafﬁc. A. Simulation Setup A cycle-accurate network simulator is developed based on the Booksim [18] and modiﬁed to implement the baseline architecture and the proposed DBS algorithms. An aggressive single cycle electrical router is applied in each tile and the ﬂit transversal time is one cycle from the local core to electrical router [24]. The total delay of Electrical/Optical (E/O) and Optical/Electrical (O/E) conversion is reported less than 100ps [5], and is modeled as part of the link traversal. The nanophotonic transmission latency is amount to 1-2 cycles based on the physical location of source/desination pair. We assume a supply voltage Vdd of 1.0 V and a router clock frequency of 5 GHz [5], [6]. We assume an input buffer of 16 ﬂits organized with 2 virtual channels, and each ﬂit consisting of 256 bits. Dimension order routing algorithm and wormhole ﬂow control are employed in our simulation. For the throughput analysis, packets are assumed to be single-ﬂit packets. We use full execution-driven simulator SIMICS with GEMS [25] integrated to extract trafﬁc traces from real application in Splash-2, PARSEC and SPEC CPU2006 benchmarks. For Splash-2 trafﬁc, the assumed kernels and workloads are as follows: FFT (16K particles), LU (512x512 with a block size of 16x16), Radiosity (Largeroom), Raytrace (Teapot), Radix (1 Million integers), Ocean (258x258), FMM (16K particles), and Water (512 Modules), seven PARSEC applications with medium inputs (blackscholes, facesim, ﬂuidanimate, freqmin, streamcluster, ferret, and swaptions) and three workloads from SPEC CPU2006 (bzip, gcc base, and hmmer). We ran several simulations to determine the optimum reconﬁguration window size of Rw by varying the size from 100 to 5000 simulation cycles. We evaluated the latency and normalized power dissipation for random uniform trafﬁc. While initially the performance improved with increasing window size as the decreasing inﬂuence of the transition latency penalty and more statistics are available which enable better prediction; at very large window sizes, the performance diminishes as the insensitive of the trafﬁc load change. Our simulation results show that 1000 cycles for Rw showed the best performance. We assume a 100 cycles latency for the reconﬁguration to take place after each Rw . It should be noticed that the reconﬁguration latency only brings bandwidth delay when the channel request more bandwidth, and the local BTC waits for the signals from the global BTC indicating the laser tuning is done. B. Power Model The optical power budget is the sum of the laser power and the power dissipated in the MRRs. We adopt the nanophotonic devices and loss values from [4], [8], [9] and listed in Table III to calculate the required optical power. Based on the parameters, we layout the waveguides and estimate the required laser power. In this paper, we assume a ﬂat thermal model that requires ring resonator heating power. The total laser power for the full bandwidth is 10.8 W , for ring heating is 25.85 W . The power consumption by an external laser is: Plaser = η ∗ Ibias ∗ Vbias where η is the wall-plug laser efﬁciency, Ibias is the average driving current, and Vbias is the driving voltage [12]. According to the variable total optical power loss along the waveguide and the receiver sensitivity requirement, we provide power of the laser by varying the driving voltage using off-chip voltage regulator and power sensor. O PT ICAL POW ER LO S SE S FOR SE LEC T O P T ICA L COM PON EN T S . TABLE III Component Value Laser efﬁciency 30% Coupler(Fiber to Waveguide) 1 Waveguide 1 Splitter(Total in Worst-case) 0.98 Non-Linearity 1 Ring Insertion & scattering 1e-2∼1e-4 Ring Drop 1.0 Waveguide Crossings 0.05 Photo Detector 0.1 Ring Heating 26 Ring Modulating 500 Receiver Sensitivity -26 Unit dB dB/cm dB dB dB dB dB dB µW/ring µW/ring dBm C. Synthetic Workload We deﬁned three PROBE modes to provide the ﬂexibility to improve the latency/bandwidth trade-off. Figure 4 presents the power-latency product under light (0.1 pakcets/node/cycle), medium (0.3 packets/node/cycle), and heavy (0.45 packets/node/cycle) injection rates for uniform random trafﬁc with ﬁve different target bandwidth boundaries (Linkutil,lower ∼ Linkutil,upper in Table II). Intuitively, higher bandwidth boundary lead to more aggressive scaling of channel bandwidth, which result in larger delays and lower optical power consumption. Our objective is to increase the resource utilization by providing sufﬁcient bandwidth and obtain acceptable performance. Based on the results, we deﬁne three bandwidth utilization boundaries as Performance Mode (0.2 ∼ 0.4), Balanced Mode (0.4 ∼ 0.6), and Power-aware Mode (0.6 ∼ 0.8). Fig. 4. Three modes for latency/bandwidth trade-off. In Figure 5, we compare the latency/throughput and normalized optical power consumption under bit complement, uniform random, and transpose. It shows that our proposed scheme saves signiﬁcant static optical power while only losing a moderate performance. When the network works at low injection rate, DBS scheme provides minimum bandwidth to obtain the maximum power saving for all trafﬁc patterns under the three modes. However, the zero-load latency only increases a little bit due to the optical signaling. For example, under very low network trafﬁc, most channels work under Pstate 4, the zero-load latency of the three trafﬁcs only increases 5 cycles but the network obtains about 75% optical power saving. With the increasing trafﬁc load, DBS scheme provides more bandwidth gradually to mitigate the increased latency. The states of the channels switch from Pstate 4 to Pstate 1 step by step due to the increasing bandwidth demand. We take uniform random trafﬁc (from Figure 5(b)(e)) as an example to explain the differences between the three modes. Performance and balanced modes start to increase the channel bandwidth to reduce the latency after 0.05 and 0.1 ﬂit/node/cycle respectively, while power-aware mode still maintains the channel bandwidth to optimize the optical power saving until 0.15 ﬂit/node/cycle. After 0.21 ﬂit/node/cycle, performance mode provides high bandwidth and ensures the network performance without any latency penalty. Balanced mode saves at most 50% optical power before 0.45 ﬂit/node/cycle with less then 3 cycles latency penalty when compared with performance mode. Power-aware mode achieves higher bandwidth utilization and more power saving by sacriﬁcing minor performance. The non-smooth latency curve of power-aware mode is due to the ﬂuctuation of the Linkutil near the bandwidth boundary. With heavy network trafﬁc closing to the congestion point, performance and balanced mode has the same latency and throughput when compared to the baseline, while power-ware mode saves 25% optical power with about 11% throughput penalty. For trafﬁc with little locality such as bit complement and transpose permutation trafﬁc as shown in Figure 5(a)(d)(c)(f), the load/latency curves have the same trend with uniform random trafﬁc, but more optical power savings. DBS scheme provides minimum bandwidth to those channels that are idle. For example, in bit complement trafﬁc performance and balanced modes save about 55% and 58% optical power respectively even after network congestion without any performance penalty. D. Trace-Based Workload Network traces from Splash-2, PARSEC, and SPEC CPU2006 benchmarks are used to evaluate the performance of proposed scheme. We compare the execution time and optical power consumption for our proposed scheme with full channel bandwidth. In Figure 6, the execution times of performance mode among all the benchmarks are quite close to the baseline. However, performance mode provides optical power savings due to the long-term low network trafﬁc. For f acesim, swaps, barnes and lu which have almost the same execution time with the baseline, they still save about 59% ∼ 71% optical power. Balanced mode achieves about 70% average optical power saving over all the benchmarks with acceptable 11% execution time penalty on average, except for f erret which takes 20% more time to execute. Power-aware mode obtains about 72% average optical power saving, however it takes 25% more execution time on average. For f erret, streamcluster and hmmer trafﬁc, although the optical power savings are over 70%, the 40% more execution time penalty are too much. This could be improved by deﬁne the high trafﬁc load channels (a) Bit Complement Trafﬁc (b) Uniform Random Trafﬁc (c) Transpose Trafﬁc (d) Bit Complement Trafﬁc (e) Uniform Random Trafﬁc (f) Transpose Trafﬁc Fig. 5. Load-latency curve (a, b, c) and normalized optical power consumption (d, e, f) for Bit Complement, Uniform Random and Transpose trafﬁc. with balanced or performance modes to reduce the latency. We use Geometric Mean (GM) to evaluate all the three modes of our prosed scheme. Performance, balanced and poweraware mode has 3.5%, 10.1% and 24.3% execution penalty respectively, and optical power saving for the three modes are 64%, 69% and 72% respectively. E. Design Cost of PROBE The design cost of the two-level predictor has been reported in [16]. It takes around 0.017% of space in TILE64 and the estimated energy consumption per access is about 0.0039 nJ which is quite small and tolerable. On-chip voltage regulator [29] is applied to tune the driving voltage of the power splitters and lasers. Since the power ratio of the splitters in the last level have four pre-deﬁned ratios, so only four voltage regulators are required. We assign a voltage regulator for each power splitter in the ﬁrst and second level. Given the 0.9 V tuning range, we estimate the power consumption for all 52 voltage regulators to be 0.8 W, and the total area is 0.0312 mm2 by scaling to 22 nm. The tuning speed of voltage regulator is assumed to be 20 ns which is 40 networks cycles. The power tuning of off-chip lasers require 16 voltage regulators with 0.25 W estimated power consumption. So the total power consumption of voltage regulator modules and tunable splitters are estimated to be 1.05 W which only introduces about 3% power overhead compared to the optical power consumption. The scalability of PROBE design could be achieved in different ways. One method of scaling the design is to group the lasers and combine their waveguides by increasing the number of levels (see in Figure 2(c)). For example, by grouping two lasers together, the total number of lasers could be cut in half. We leave this for future work. V. CONCLU S ION S In this paper, we propose a dynamic bandwidth scaling method that reduces the optical power consumption of optical channels based on past network trafﬁc and resource utilization. To implement the proposed scheme, we design a binary-treebased waveguide based on the tunable power splitter. We evaluate our scheme with synthetic trafﬁc pattern and traces from Splash-2, PARSEC and SPEC CPU2006 benchmarks. The power is greatly reduced with acceptable performance penalty. For the synthetic trafﬁc patterns, the power saving up to 75% compared with full bandwidth network and 11% loss in throughput at most. For the real trafﬁc traces under balanced mode, it could save at average 68% optical power with at average 12% increase in execution time. Our design is cost-efﬁcient and can be applied to other optical NoCs by re-designing the waveguide structures. ACKNOW LEDGM ENT We thank the anonymous reviewers for their excellent feedback. This work was partially supported by the National Science Foundation grants CCF-0915418, CCF-1054339 (CAREER) and ECCS-1129010. "
