title,abstract,full_text
SMART++ - reducing cost and improving efficiency of multi-hop bypass in NoC routers.,"Low latency and low implementation cost are two key requirements in NoCs. SMART routers implement multi-hop bypass, obtaining latency values close to an ideal point-to-point interconnect. However, it requires a significant amount of resources such as Virtual Channels (VCs), which are not used as efficiently as possible, preventing bypass in certain scenarios. This translates into increased area and delay, compared to an ideal implementation. In this paper, we introduce SMART++, an efficient multi-hop bypass mechanism which combines four key ideas: SMART bypass, multi-packet buffers, Non-Empty Buffer Bypass and Per-packet allocation. SMART++ relies on a more aggressive VC reallocation policy and supports bypass of buffers even when they are not completely free. With these desirable characteristics, SMART++ requires limited resources and exhibits high performance. SMART++ is evaluated using functional simulation and HDL synthesis tools. SMART++ without VCs and with a reduced amount of buffer slots outperforms the original SMART using 8 VCs, while reducing the amount of logic and dynamic power in an FPGA by 5.5x and 5.0x respectively. Additionally, it allows for up to 2.1x frequency; this might translate into more than 31.9% base latency reduction and 42.2% throughput increase.","SMART++: Reducing cost and improving efficiency of multi-hop bypass in NoC routers Iván Pérez, Enrique Vallejo, Ramón Beivide University of Cantabria, Santander, Spain. ABSTRACT Low latency and low implementation cost are two key requirements in NoCs. SMART routers implement multi-hop bypass, obtaining latency values close to an ideal point-to-point interconnect. However, it requires a significant amount of resources such as Virtual Channels (VCs), which are not used as efficiently as possible, preventing bypass in certain scenarios. This translates into increased area and delay, compared to an ideal implementation. In this paper, we introduce SMART++, an efficient multi-hop bypass mechanism which combines four key ideas: SMART bypass, multi-packet buffers, Non-Empty Buffer Bypass and Per-packet allocation. SMART++ relies on a more aggressive VC reallocation policy and supports bypass of buffers even when they are not completely free. With these desirable characteristics, SMART++ requires limited resources and exhibits high performance. SMART++ is evaluated using functional simulation and HDL synthesis tools. SMART++ without VCs and with a reduced amount of buffer slots outperforms the original SMART using 8 VCs, while reducing the amount of logic and dynamic power in an FPGA by 5.5× and 5.0× respectively. Additionally, it allows for up to 2.1× frequency; this might translate into more than 31.9% base latency reduction and 42.2% throughput increase. KEYWORDS SMART; SMART++; multi-hop bypass ACM Reference Format: Iván Pérez, Enrique Vallejo, Ramón Beivide. 2019. SMART++: Reducing cost and improving efficiency of multi-hop bypass in NoC routers. In International Symposium on Networks-on-Chip (NOCS ’19), October 17–18, 2019, New York, NY, USA. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/ 3313231.3352364 1 INTRODUCTION Low latency in NOCs for a wide range of traffic loads is critical for multiprocessor performance. Different approaches have been considered for this goal, including very large crossbars (such as [19, 21]), low-diameter topologies based on high-radix routers (such as [1, 2]) or aggressive lookahead routing, speculative stages and router bypass mechanisms (such as [11, 12, 14]). SMART [11], which Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’19, October 17–18, 2019, New York, NY, USA © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6700-4/19/10. . . $15.00 https://doi.org/10.1145/3313231.3352364 belongs to the last group, is a very effective solution which implements multi-hop bypass, this is, it skips several intermediate transit routers in a single hop to dramatically reduce latency. SMART combines the simplicity and regularity of traditional 2D tiled designs with near-optimal latency (close to an ideal point-to-point interconnect) and very high throughput. However, practical implementations of SMART result in overly large, power-hungry and slow router designs, for several reasons. First, the Virtual Channel (VC) reallocation scheme employed requires the corresponding buffer to be empty to reassign any VC buffer. This is often required in several contexts such as wormhole (WH) networks using fully-adaptive routing protocols [9, 16]. However, SMART neither employs WH nor is fully adaptive. Additionally, obtaining good performance using this reallocation scheme requires a large number of VCs, each of them holding a whole packet. This large number of VCs makes allocators more complex, which increases the critical path latency, and drastically increases router area and power consumption. Second, the buffers to bypass must be empty; otherwise the packet would not be forwarded. With a limited amount of VCs, this increases Head-of-Line Blocking (HoLB), reducing performance. Finally, even though traffic is sent following Virtual Cut-Through (VCT) flow control, flit-by-flit arbitration collisions may make a packet spread through multiple routers, blocking the buffers in the intermediate routers. This work introduces SMART++, an efficient multi-hop bypass mechanism that avoids the main limitations of SMART and allows for much simpler implementations. SMART++ combines SMART bypass [11], multi-packet buffers, Non-Empty Buffer Bypass (NEBB, [20]) and per-packet allocation using grant-hold circuits. SMART++ supports efficient configurations with a small amount of deeper buffers, rather than the large number of individual VCs required in SMART, which results in much better area, power and critical path delay. SMART++ is evaluated using both the functional simulator Booksim [18] and an HDL implementation based on OpenSMART [15]. SMART++ outperforms SMART requiring only simple changes, provides high performance using a single buffer of limited size per port, and is both area- and power-efficient. Specifically, the main contributions of this paper are: • SMART++, an efficient multi-hop bypass mechanism that outperforms the original SMART with much lower requirements on VCs, area and power. • A performance evaluation by simulation, which proves that: SMART++ without VCs has similar performance than SMART with VCs for single-flit packets and the same buffer space; better performance for multi-flit packets; and 2x buffer reduction for the same performance for bimodal traffic. • Resource utilization and power evaluations using HDL synthesis, showing the high cost of VCs, and the extensive and feasible variety of buffer configurations of SMART++. NOCS ’19, October 17–18, 2019, New York, NY, USA Iván Pérez, Enrique Vallejo, Ramón Beivide Figure 1: SMART bypass setup and flit traversal overview with priority to local flits. Section 2 presents the required background. Section 3 details the SMART++ foundations, while Section 4 discusses implementation details. Section 5 evaluates the proposal. Finally, Section 6 compares to related work and Section 7 concludes the paper. 2 BACKGROUND 2.1 NoC Router bypass Router bypass [12, 13] is a mechanism that reduces latency by skipping some pipeline stages of the router. This type of NoC has additional communication signals denominated LookAheads. LookAheads contain the routing information of packets and are sent one cycle before the transmission of packet flits. With the routing information, the next router allocates the crossbar one cycle before the arrival of flits. If the allocation succeeds, the flit takes a bypass path to the crossbar, avoiding the allocation stages and buffer write, saving time and energy. Multiple LookAheads from different sources and local flits, may compete for the same output port in a router, so a new unit called LookAhead Conflict Check or LookAhead Arbiter is defined to arbitrate them in case of conflict. 2.2 Non-Empty Buffer Bypass (NEBB) Switch allocation in traditional LookAhead router bypass is done flit by flit. Therefore, part of a packet might bypass a router while the remaining is buffered. When multiple packets are allowed to be written in the same buffer, flits of different packets might interleave in the buffer, corrupting data. Requiring empty buffers to forward packets avoids this issue in a conservative way. NEBB [20] is an alternative bypass policy that removes the empty buffer limitation, as its name implies, maximizing the utilization of the bypass. Different variants of NEBB are defined for different flow controls, allowing to bypass a buffer which is non-empty, but not advancing a packet to an output port. In general, they allow the bypass of a non-empty buffer for single-flit packets in any case, or when both the bypass and destination buffers have room for the whole packet and VCT is assumed. Maximizing the bypass utilization reduces dynamic power consumption and the amount of VCs required to obtain the maximum throughput from the NoC. 2.3 SMART: multi-hop router bypass SMART (Single-Cycle Multihop Asynchronous Repeated Traversal) [11] is a NoC router bypass that allows flits to cross multiple routers in a single cycle. In this design, LookAheads are called SMART-hop Setup Request (SSR). When flits are ready to be transmitted in the next cycle, SSRs are broadcast to the next routers in the path. H PCM a x defines the maximum number of hops per cycle allowed, limited by the operation frequency of the NoC. Two variants are defined: SMART_1D only broadcasts SSRs in a row or column of the NoC mesh; SMART_2D employs additional lines to broadcast SSRs in both mesh dimensions, allowing for dimension change in a single multihop at the cost of much higher complexity. SSRs request access to the bypass in each of the downstream routers, in a Switch Allocator Global (SA-G) function. In SA-G, SSRs from different sources and local buffered flits may conflict. If a conflict occurs, flits may suffer a premature stop and be buffered in an intermediate router of the desired multihop path. A single priority policy is enforced in all the network to guarantee correctness. In this work, local flits always have priority over bypass flits as it attains the best performance [11]. Figure 1 shows an example of bypass setup and flit transmission. In the first cycle, the green packet (first router) and the red packet (last router) broadcast the SSRs in their route direction. The SSR of the green packet setups the bypass path of the second and third routers because there are no conflicts with other flits or SSRs. However, it loses against the local red flit in the last router. In the second cycle, the green packet crosses the first router crossbar and the bypass paths of the second and third routers to get to the last router. Routers in SMART follow VCT requirements to send a packet: the destination buffer needs to hold the complete packet, and all the flits of the packet are sent consecutively. However, the flits of the packet are not always received consecutively at the destination of the multi-hop. Indeed, global arbitration is performed per-flit, not per-packet, in all routers in the multi-hop. Thus, a new packet to be transmitted in one of the intermediate routers may receive higher priority and cause a premature stop of part of the flits of another packet. For this reason, flits from a packet may be received with gaps, and flits from different packets may be interleaved in the physical links between routers, similar to a WH network. SMART++: Reducing cost and improving efficiency of multi-hop bypass NOCS ’19, October 17–18, 2019, New York, NY, USA There are two alternatives for the bypass path in SMART NoCs: buffer bypass and router bypass. With buffer bypass [11], flits only bypass the input buffers in the input unit, but they pass through the crossbar. Buffer bypass is required for SMART_2D and for ejection router bypass. Router bypass, depicted in Figure 1 and used in this paper, was introduced in OpenSMART [15]. In this case, flits take a dedicated path from the input port to the output port of the router following the same dimension. Router bypass is more suitable for SMART_1D because avoids conflicts between SSRs and local flits that share the same input port but request different output ports. 3 SMART++ This section introduces SMART++, which supports multi-hop bypass even when inter-router buffers are not completely empty, as long as no packet interleaving occurs in any buffer. SMART++ targets simple designs with only one buffer, or just a few ones (one per virtual network required by the coherence protocol), leading to high frequency and reduced area and power consumption. SMART++ combines three improvements over the baseline SMART: i) Multi-packet buffers, ii) Non-Empty Buffer Bypass for single-flit packets and iii) packet-by-packet arbitration to support multi-flit NEBB bypass. These mechanisms are detailed in subsections 3.1-3.3. Section 3.4 compares the different mechanisms, detailing in which cases each of them supports packet bypass. 3.1 Multi-Packet Buffers (MPB) SMART requires buffers sized for the largest packet in the network, since it implements VCT flow control, but it only holds a single packet due to its VC reallocation policy. SMART++ allows to hold multiple consecutive packets in router buffers and can exploit buffers larger than a single packet size. Such approach is similar to previous proposals for NoCs [4, 7, 17, 22, 23]. The implementation is similar to Whole Packet Forwarding (WPF [17]). WPF implements an aggressive VC reallocation mechanism, which allows to reallocate a given VC if it has enough buffer slots to hold the whole packet and the tail of the previous packet has been already sent. According to [17], WPF can be viewed as applying packet-based flow control in a wormhole network. Note that in SMART flit-by-flit arbitration behaves similar to a WH network. The use of multi-packet buffers allows to employ a lower amount of VCs with deeper buffers, leading to simpler memory organizations that exchange width (#VCs) by length (deeper FIFOs). Such VC reduction simplifies allocation and reduces overall chip area even though the total storage remains the same. Additionally, combining multiple packets in the same buffer increases its efficiency, particularly with different-size packets (bimodal traffic), which often occurs in NoCs. This is evaluated in Section 5.2. 3.2 Non-Empty Buffer Bypass (NEBB) SMART requires an empty buffer in all of the routers to be bypassed. Such policy is forced by its conservative VC reallocation scheme (if no free buffer exists, packet is not sent to the bypass router in the first place), but is also overly conservative, and reduces performance, particularly when the amount of VCs is low. SMART++ employs NEBB [20] to bypass a buffer even when it is not empty. Such bypass is allowed as long as no two packets are interleaved in a given buffer. For single-flit packets, this never occurs as long as the destination buffer has already received the tail of the previous packet. For this reason, SMART++ relies on NEBB to bypass single-flit packets even when the input buffer is not empty. This only requires a policy change in bypass conditions. For multi-flit packets, the flit-by-flit allocation mechanism in SMART implies that the bypass operation might be interrupted at any cycle, if a higher-priority SSR is received at an intermediate router. When this happens, the remainder of the packet is stored in the intermediate router buffer, and if it is not empty, packets would be interleaved and corrupted. For this reason, NEBB does not support multi-flit packet bypass with non-empty buffers when flit-by-flit allocation is employed. 3.3 Packet-by-packet arbitration (PPA) As discussed in Section 3.2, flit-by-flit allocation prevents using NEBB with multi-flit packets. SMART++ solves this issue using packet-by-packet arbitration, which is implemented using a granthold circuit [6] coupled to the round-robin arbitration stages (SA-G and SA-L, discussed in Section 4.1). Grant-hold circuits hold the arbiter outcome for a certain amount of time. When a multi-flit packet header wins arbitration, SMART++ logic locks the arbiter to the winning packet. However, winning SAG does not guarantee that a flit will be transferred in the following cycle: the flit could suffer a premature stop in an upstream router in the multi-hop. To cover this case, SMART++ releases the grant in two cases: when the packet tail is received, or when no flit is received. Grant holding is not required for single-flit packets. Effectively, this makes SMART++ behave exactly as VCT, receiving all packets without holes from upstream channel interleaving. Only packet headers generate SSRs. When a higher-priority SSR (lower distance, using local priority) is received in an intermediate router while a packet is being bypassed, it loses arbitration and is stored in the router buffers. This behavior does not conflict with the single priority enforced in the network requirement of SMART because it does not introduce false positives (flit received when it is not expected), only premature stops. Additionally, these premature stops do not reduce performance: they always occur because other packet is actually being transferred on the desired output1 . 3.4 Comparative analysis of the mechanisms Table 1 summarizes the different cases in which bypass is supported in SMART and SMART++, detailing the specific contribution of each of the mechanisms SMART++ comprises. SMART can only forward data to and bypass empty buffers. MPB supports forwarding packets to non-empty buffers, but not bypassing. NEBB adds support for single-flit packet bypass of non-empty buffers, and the complete SMART++ design including PPA supports bypass of non-empty buffers for any packet size. Figure 2 presents two examples of the conditions presented in Table 1, for both single- and multi-flit packets, sending a packet from R 0 to R 4 . The original SMART mechanisms only sends data when empty VCs are available, so packets stop at R 1 , which is the only transit router with empty buffers. MPB allows to employ non-empty VCs, but not bypass them, so packets stop at R 2 , which is the first 1 There are no cascading invalidations, as occurs with SSRs using Prio=bypass [11]. NOCS ’19, October 17–18, 2019, New York, NY, USA Iván Pérez, Enrique Vallejo, Ramón Beivide Table 1: Allowed bypass depending on the buffer status. Bypass and Dest. buffer refers to the buffers in the bypass router and in the next router. When multiple routers are bypassed, intermediate buffers are both bypass and dest. buffers. They may need to be completely empty, or may accommodate at least a whole packet. Bypass mechanism SMART (Baseline) SMART+MPB SMART+MPB+NEBB SMART++ (SMART+MPB+NEBB+PPA) ✓ ✓ ✓ ✓ Bypass buffer: empty Dest. buffer: empty Dest. buffer: packet X Bypass & Dest. buffer: packet 1-flit packet Multi-flit packet X X X X X ✓ ✓ ✓ ✓ ✓ ✓ Figure 2: Stop router of each mechanism in SMART++ for single-flit (up) and multi-flit (bottom) packets. R 4 is the destination of the blue packet in R 0 . Routers only have one buffer. non-empty buffer in the path. MPB+NEBB allows to bypass nonempty VCs, but only for single-flit packets, so the packet reaches the destination only in the first (upper) case of single-flit packet. Finally, SMART++ allows to bypass non-empty VCs for any size of the packet, so in both cases the packet reaches the destination. 4 SMART++ IMPLEMENTATION DETAILS This section details the organization of the input units, and the buffer backpressure mechanisms. 4.1 Pipeline and input buffer architecture SMART++ implements a three stage pipeline as depicted in Figure 3. In the first stage, flits are written in the VC selected in VC Selection (VS). In parallel, the router performs Route Computation (RC) for the next multi-hop and Switch Allocation Local (SA-L). SA-L grants access to SA-G to local flits. In the second stage, SA-L winners broadcast their SSRs. In parallel, Switch Allocator Global (SA-G) grants access to the crossbar. In the third stage, flits traverse the crossbar (ST) and link (LT) of the routers until finding the first bypass disabled. SMART++ targets designs with few input buffers, ideally one. Such organization may introduce idle cycles, or bubbles, imposed by the architectural dependencies between the operations of consecutive packets reusing the same buffer, as described in [8]. Note that the three stages may need to access flit information in the same cycle, as presented in Figure 3a, generating architectural dependencies and stalls. Specifically, this would occur if flits were dequeued from the input buffer when they win SA-G. In a design without VCs, this would interrupt the transmission of packets sharing the same input buffer, because the second packet in the queue cannot place the request in SA-L while the front packet is doing SA-G. This pipeline bubble increases latency and reduces throughput. SMART++ implements the input unit represented in Figure 3b to address this issue. The input unit has three pipeline registers R1-R3. Using R2, flits are dequeued when winning SA-L, transferring the following flit to the front position of the buffer. In this organization, (a) Router pipeline. (b) Input unit organization Figure 3: Router input unit organization and pipeline. VS and BW in stage 1 read the flit from the first register (1A), whereas RC and SA-L also in stage 1 read their data from 1B, i.e. the front of the input buffer (if it is not empty) or register R1 (if the buffer is empty). In the latter case, the flit advances directly to R2. SA-G reads the flit from register R2, while ST reads the flit from R3. SMART++ implements VCT as defined in Section 3.3, so buffer credits may be handled per-packet. For single-flit packets, the credit is sent back to the upstream router when the flit advances to R2. However, flits may wait indefinitely in R2. For multi-flit packets, we notice that when the header advances to R3, it is sure that the packet flits will be transferred consecutively. For this reason, credit handling for multi-flit packets may be optimized as follows: when a packet header advances to R2, one credit is generated; when the first body flit advances to R2, the remaining credits are generated. 4.2 Buffer backpressure and VC selection This section discusses different backpressure mechanisms supported by SMART routers, mainly credits and free_VC signals, and their extension to support SMART++ routers. The buffer backpressure mechanism notifies the availability of buffer slots in the downstream router to receive a packet or flit. Credits and ON/OFF signals are frequently used mechanisms that notify the availability of each VC individually. Both of them allow the upstream router to track VC availability and select the destination VC for a given packet in the VC allocator (VA). When conservative VC reallocation is used, the ON/OFF mechanism is reduced to a single ON signal per VC, sent when its packet is completely forwarded. By contrast, when multiple packets are allowed per buffer and multiple packet sizes are supported using VCT, credits track the amount of free slots per VC. The implementation in OpenSMART [15] relies on credits, and SMART++ also supports this mechanism directly, with the optimization described in Section 4.1. The use of ON/OFF signals is only supported for single-size packets. The original SMART implementation in [11] employs a different approach based on free_VC signals, depicted in Figure 4a. Each cycle, routers activate a free_VC 1-bit signal on those ports that have at least one free VC to receive a packet. Upstream routers do not know which VC is free, and instead send the packet blindly without VC Allocation (VCA); downstream routers assign a VC upon reception, in a VC selection (VS) operation. SMART++: Reducing cost and improving efficiency of multi-hop bypass NOCS ’19, October 17–18, 2019, New York, NY, USA Table 2: Simulation parameters. (a) free_vc signals in SMART (b) Availability signals in SMART++ with packet sizes of 1 and 5 flits. Figure 4: Buffer signaling mechanisms. The free VC mechanism requires minimal changes to support SMART++ routers. Instead of a single-bit flag, SMART++ employs one independent 1-bit signal per packet size allowed in the network, as depicted in Figure 4. This supports a case with free space for a single-flit packet but not enough space for a multi-flit packet. These signals are denoted avail_i to indicate the availability of some VC with enough free slots to hold packets of i flits (for example, avail_1 and avail_5 for 1 and 5 flits respectively). Obviously, when avail_j is set, avail_k will be also set ∀k < j . SMART++ works with either credits or availability signals. In both cases, it needs to know in advance the size of the packet to be received, for the SA-G stage to decide to send the packet, if there is space for it, or store it in a buffer. The SSR signal is extended to indicate the packet size, using ⌈l oд2 (N )⌉ additional lines for N packet sizes. For N = 2, this implies that SMART++ SSRs will employ only one additional line. In NOCs supporting a single packet size, the SSR network in both models is the same. 5 EVALUATION This section evaluates SMART++. Section 5.1 describes the simulation infrastructure; Section 5.2 presents the cycle-accurate performance results of SMART++; Section 5.3 shows synthesis estimations of power, resource utilization and maximum frequency. 5.1 Simulation Infrastructure Two development platforms compose the simulation infrastructure: BookSim [18] and Bluespec SystemVerilog (BSV). BookSim is an open-source functional simulator written in C++. We have implemented cycle-accurate models of SMART and SMART++, including the partial versions detailed in Table 1. The model implemented supports variable size packets. We have also implemented SMART++ in BSV based on OpenSMART [15]2 . We had to make significant modifications to the SMART model provided by OpenSMART, for it to compile and work properly (e.g. move ST from the second pipeline stage to the third). Like OpenSMART, this model is limited to single-flit packets and works with credits. Router bypass (Section 2.3) is implemented in both platforms. The BSV implementation is used to validate the latency and throughput results of the BookSim model, through BSV functional simulations. To estimate power, resource utilization and frequency, the BSV compiler is used to generate Verilog code, and Quartus 2 Based on OpenSMART’s most recent official commit 84aa93b on 27 Sep 2017. Parameter Topology Bypass mechanism Bypass type Router size VC number & backpressure Buffer size Packet size (flits) Routing VC selection policy SSR policy H PCM AX Flit size BookSim Bluespec System Verilog 4x4 and 8x8 meshes 4x4 mesh SMART, SMART++ and partial versions SMART and SMART++ SMART_1D with router bypass 5 ports 1, 2, 4 or 8 VCs using credits 1, 2, 4, 5, 8, 10, 15 or 20 1, 5 or bimodal (80% of 1 + 20% of 5) DOR XY 1, 2, 4 or 8 1 Shortest queue First available VC One dimension (SMART-1D) + Priority to local flits 4 or 8 4 128 bits 32 bits Prime 18.1 Lite Edition to synthesize and measure the desired metrics on an Arria II EP2AGX45DF29I5 FPGA. Performance simulations on BookSim evaluate 8x8 meshes with H PCM a x = 8. Validation simulations are evaluated in 4x4 meshes with H PCM a x = 4 due to the large requirements of BSV compiler. In both cases local flits have priority over bypass. All the simulations use synthetic traffic. Three traffic patterns are evaluated: random uniform, bit-reversal and transpose. Moreover, we evaluate three packet sizes: single-flit packets, 5-flit packets, and bimodal traffic that combines single-flit and 5-flit packets following a distribution of 80% and 20%, respectively. It emulates the packet distribution observed in full-system simulations of the PARSEC benchmarks [17]. Table 2 gathers the most relevant simulation parameters. 5.2 Cycle-level Performance Results 5.2.1 This section evaluates SMART++ without VCs, then with VCs, and finally the contribution of each of its mechanisms. SMART++ without VCs. Figure 5 compares SMART with multiple VCs and SMART++ without VCs (or 1 VC), showing the average packet latency using single-flit packets, 5-flit packets and a combination of single-flit packets and 5-flit packets (bimodal traffic). The total buffering size (VCs × Buffer size) is the same in both cases, injecting random uniform traffic. With single-flit packets and the same total amount of buffering the performance of SMART++ is similar to SMART. In the case of 5-flit packets SMART++ achieves better performance when the buffer space is low, 5 and 10 slots, and similar for 20 and 40. With a 5 slots buffer, SMART++ outperforms SMART throughput by 48.7% with 1 VC. For bimodal traffic SMART++ is even better, requiring half of the buffer space of SMART to practically obtain the same performance. Figure 6 depicts the packet latency using transpose and bitreversal traffic patterns, with bimodal packet size distribution. Results with both traffic patterns are very similar, requiring buffer of only 10 slots to reach the maximum throughput, while SMART requires 4 VCs of 5 slots. Additionally, SMART++ with a buffer size of 5 slots improves the throughput of SMART with 2 VCs by 18.3% and 10.9% for transpose and bit-reversal. SMART++ with multiple VCs. Figure 7 compares the packet latency of SMART and SMART++ with the same number of VCs and buffer depth. The results of SMART++ with multiple VCs are slightly better than without VCs for the same buffer space, beating SMART in every configuration. The cause of the improvement is the reduction of HoLB. 5.2.2 5.2.3 Partial implementations of SMART++. Figure 8 depicts a breakdown of the improvements of the partial implementations of NOCS ’19, October 17–18, 2019, New York, NY, USA Iván Pérez, Enrique Vallejo, Ramón Beivide (a) Single-flit packet latency. (b) 5-flit packet latency. (c) Bimodal traffic packet latency. Figure 5: SMART vs SMART++ packet latency for different packet sizes. SMART++ only employs 1 buffer (no VCs). P SM a x stands for maximum Packet Size in the simulation, e.g. 4 × P SM a x with bimodal traffic means 4 VCs of 5 flits. (a) Transpose. (b) Bit-reversal. Figure 6: Latency of SMART and SMART++ without VCs for transpose and bitreversal traffic, with bimodal packets. (a) 5-flit packet latency. (b) Bimodal traffic latency. Figure 7: SMART vs SMART++ packet latency with multiple VCs and minimal buffer size per VC. SMART++ (SMART+MPB and SMART+MPB+NEBB). Figures 8a and 8b show throughput and buffer utilization, respectively, injecting bimodal traffic. The buffer utilization represents the percentage of flits that performs BW when arriving a router. It shows that: SMART-MPB uses slightly more the bypass path than SMART before saturation; SMART-MPB+NEBB reduces the buffer utilization of SMART-MPB for every load achieving more throughput; SMART++ has the lowest buffer utilization and the highest throughput. Regarding the throughput obtained with buffers of 10 flits, SMART-MPB almost achieves the same throughput of SMART++: SMART+MPB increases SMART’s throughput by 39.7%, SMART+ MPB+NEBB by 45.1% and SMART++ by 48.5%. Figure 8c depicts the maximum throughput injecting bimodal traffic, for different combinations of VCs and buffer depths. There are three remarkable conclusions. First, SMART requires a big amount of VCs to reach the limit of the NoC (0.5 flits/node/cycle in an 8 × 8 mesh). Secondly, SMART++ and its intermediate versions are very close to the maximum performance of SMART, but (a) Throughput. (b) Buffered flits. (c) Maximum throughput. Figure 8: Performance metrics of SMART++ intermediate versions using bimodal traffic. without requiring VCs and with half of the total buffer size. Particularly, SMART++ with one buffer of 20 slots achieves only 3.0% less throughput than SMART with 8 VCs of 5 slots. Thirdly, the main source of throughput improvement is forwarding packets to partially empty buffers (MPB) instead of empty. The improvement of SMART++ over its intermediate versions is more significant when the amount of resources is low. 5.3 SMART++ synthesis results 5.3.1 Model Validation. We first validate the models implemented in Booksim and BSV by comparing their results. The network is a 4× 4 mesh with H PCma x = 4, single-flit packets and random uniform traffic. Figure 9 shows the packet latency and maximum throughput of both implementations for multiple buffer configurations. The packet latencies obtained form both models are equal until reaching the saturation region where there is a negligible difference. In terms of maximum throughput the highest relative error between models is only 3.53%, when using 2 VCs of 1 slot. Hence, the SMART (buffer size 1) and SMART++ functional models implemented in BookSim cycle accurately simulate the router architecture and the pipeline, according to the HDL implementation. SMART++: Reducing cost and improving efficiency of multi-hop bypass NOCS ’19, October 17–18, 2019, New York, NY, USA (a) Buffer size: 1 slot. (b) Buffer size: 4 slots. (a) ALUTs employed. (b) ALMs employed. (c) Registers employed. (d) Memory employed. Figure 10: FPGA resources employed by each configuration. (c) Maximum throughput. Figure 9: Comparison of packet latency and throughput of the SMART++ models implemented in BSV and BookSim. 5.3.2 Resource Analysis. This and next sections analyze the resource requirements, the maximum frequency and the power consumption of a single SMART++ router. Figure 10 shows FPGA resources used by the synthesized routers, represented by the number of Adaptive Look-Up Tables (ALUTs), Adaptive Logic Modules (ALMs), dedicated registers and internal block memory bits. The results shows the high impact of the amount of VCs on resource demands as they directly affect to the input units, credit units (credit handling logic) and VA. When duplicating the number of VCs, the number of resources is almost doubled. For example, the configuration with 2 VCs of 1 slot increases the number of ALUTs by 86.9%, ALMs by 82.3% and registers by 77.63% with respect to 1 VC of 1 slot. Alternatively, when using deeper buffers, the resource utilization grows in a much lighter way. For example, using 1 VC of 8 slots requires 22.3%, 31.4% and 63.14% more ALUTs, ALMs and registers, respectively, than using 1 VC of 1 slot. The compiler employs block memory (internal FPGA RAM) when buffers larger than 10 slots are instantiated. This occurs in the credit unit for configurations with more than 8 buffer slots. The credit units of OpenSMART has two FIFO structures per port to store credits pending to be transmitted. Each one has as many entries as buffer slots has an input port. 5.3.3 Timing and Power Analysis. Figure 11 depicts the maximum operation frequency and the dynamic power consumption for multiple router configurations. To obtain dynamic power results, we feed the power analysis tool with VCD (value change dump) files generated from ModelSim functional simulations with a clock frequency of 50MHz, which fits in all configurations and is consistent with the cycle-level results in Section 5.2. In both cases, the results reveal that the number of VCs is a critical design factor. Doubling the number of VCs decreases frequency by 18% to 29% in each step. In terms of dynamic power, it almost doubles when duplicating the number of VCs. For example, using 2 VCs of 1 slot multiplies the power of 1 VC of 1 slot by 1.99. Increasing buffer depth has a negligible impact on frequency and moderately increases dynamic (a) Max. operation frequency. (b) Dynamic power. Figure 11: FPGA frequency and dynamic power results. power, 75.84% in the worst case (2 × 8 compared to 2 × 4 slots). Comparing the 8 × 1 SMART and the 1 × 8 SMART++ configurations, resources are reduced by 5.49× on average and dynamic power by 4.99×. Notice that when using more than 8 slots in total, there is an abrupt increase as a consequence of using block memory bits in the credit units as depicted in Figure 10d. 5.3.4 Scaled SMART++ performance results. Section 5.2 presents cycle-accurate performance results of SMART and SMART++. However, Figure 11a shows that frequency in SMART++ may be significantly higher, further improving performance. Frequency is determined by the first router stages. When a given H PCma x value is considered, the delay of LT increases, and it might lower the router frequency. In such case, frequency in SMART and SMART++ would be similar, and their performance would be proportional to the figures in Section 5.2. However, for moderate H PCma x and in FPGA evaluations, this typically does not occur because propagation delay is significantly lower than logic delay. In this case, performance will be determined by the maximum frequency in Figure 11a. Figure 12 presents frequency-scaled latency results of SMART and SMART++. The simpler SMART++ design using a single buffer with 4 packets (4 to 20 flits) clearly outperforms any SMART implementation. Comparing to a competitive SMART using 4 VCs, base latency is reduced at least by 31.9% and throughput increased by 42.2% in all cases. 6 RELATED WORK Sections 1 and 2 have already discussed several alternatives to reduce latency on NoCs, including SMART details. OpenSMART [15] NOCS ’19, October 17–18, 2019, New York, NY, USA Iván Pérez, Enrique Vallejo, Ramón Beivide (a) Single-flit packet latency. (b) 5-flit packet latency. (c) Bimodal traffic packet latency. Figure 12: Frequency-scaled latency of SMART and SMART++ using different packet sizes. is a NoC generator that generates verified RTL of SMART. We have extended it to support SMART++, within its limitations. An SSR network is proposed in [5] to replace SSR broadcast wires and complex allocators to reduce wire and energy overheads. The approach is particularly relevant in SMART-2D, which we do not study for its complexity. WiSMART (wireless-enabled SMART [10]) is a hybrid of SMART and wireless NoC (WiNoC). This combination allows to operate at high frequencies independently of H PCma x , using wireless communication for long distances. The combination with SMART++ is possible, but left for future work. Task mapping techniques to reduce conflicts between packets in SMART are presented in [24]. They focus on communication contention, rather than communication distance, as contention degrades bypass utilization. An analytical model of SMART is presented in [3] to speed up simulations, reducing simulation time in two orders of magnitude with respect to cycle-accurate simulators. These works can be adapted to SMART++. 7 CONCLUSIONS Power and area efficiency are essential features of NoC design. Their optimization is crucial for integrating NoCs in many-core processors. So far, SMART achieves the lowest latency in meshes. However, it requires a large amount of VCs to exploit the advantages of the mechanism due to its conservative VC reuse policy. This work proposes SMART++, a multi-hop bypass mechanism that does not require VCs. SMART++ targets VC and bypass utilization, to allow multiple packets share the same buffer and bypass routers when their buffers are not empty. SMART++ exhibits high performance without VCs, reducing drastically power, area and critical path delay compared to configurations of SMART with multiple VCs. Moreover, SMART++ presents a more efficient utilization of buffers and bypass. Contrary to traditional wisdom, the use of multiple VCs in SMART++ provides just a marginal improvement. For the same frequency and similar performance, SMART++ reduces area and power by 5.5× and 5.0×. Selecting the maximum frequency, SMART++ may reduce base latency by up to 31.9% and increase throughput by up to 42.2%. Altogether, the simple design in SMART++ simultaneously provides near-optimal performance with a small footprint and reduced implementation cost. ACKNOWLEDGMENTS This work was supported by the Spanish Ministry of Science, Innovation and Universities, contract TIN2016-76635-C2-2-R (AEI/FEDER, UE) and FPI grant BES-2017-079971, and the HiPEAC Network of Excellence. Bluespec Inc. provided access to Bluespec tools. "
Analyzing networks-on-chip based deep neural networks.,"One of the most promising architectures for performing deep neural network inferences on resource-constrained embedded devices is based on massive parallel and specialized cores interconnected by means of a Network-on-Chip (NoC). In this paper, we extensively evaluate NoC-based deep neural network accelerators by exploring the design space spanned by several architectural parameters. We show how latency is mainly dominated by the on-chip communication whereas energy consumption is mainly accounted by memory (both on-chip and off-chip).","Analyzing Networks-on-Chip based Deep Neural Networks John Jose Indian Institute of Technology Guwahati Guwahati, India johnjose@iitg.ac.in Giuseppe Ascia, Vincenzo Catania, Salvatore Monteleone, Maurizio Palesi, Davide Patti∗ University of Catania Catania, Italy first.last@dieei.unict.it ABSTRACT One of the most promising architectures for performing deep neural network inferences on resource-constrained embedded devices is based on massive parallel and specialized cores interconnected by means of a Network-on-Chip (NoC). In this paper, we extensively evaluate NoC-based deep neural network accelerators by exploring the design space spanned by several architectural parameters. We show how latency is mainly dominated by the on-chip communication whereas energy consumption is mainly accounted by memory (both on-chip and off-chip). KEYWORDS Deep Neural Network, Network-on-Chip, Performance and energy evaluation, Design space exploration ACM Reference Format: Giuseppe Ascia, Vincenzo Catania, Salvatore Monteleone, Maurizio Palesi, Davide Patti and John Jose. 2019. Analyzing Networks-on-Chip based Deep Neural Networks. In International Symposium on Networks-on-Chip (NOCS ’19), October 17–18, 2019, New York, NY, USA. ACM, New York, NY, USA, 2 pages. https://doi.org/10.1145/3313231.3352375 1 INTRODUCTION The inference phase of Deep Neural Networks (DNNs) is still over the computational capabilities provided by traditional microcontroller based devices. To tackle with this gap, several neural network accelerators have been proposed aimed at improving performance and power metrics, including, latency, throughput, and energy efficiency while executing DNN inferences [5], [7], [2], [4]. As the complexity of DNNs increases, one can expect the emergency of scalable DNN accelerator platforms in which many DNN accelerators are connected into the same chip by means of an onchip communication network [6], [3], [1], [8]. In the previous works, the aspects related to the NoC as communication fabric for supporting the data movement among the NN processing elements is often only marginally investigated. This paper presents an experimental analysis aimed at identifying the main elements of a NoC-based DNN accelerator which mostly impact its performance and energy metrics. The analysis is focused on exploring the design space spanned by a set of architectural elements, including, number of memory interface, local memory size, and links size. VGG-16 [5] (approx 138 M parameters) is considered in the experiments. The ∗ This work was supported in part by the Piano per la Ricerca 2016/2018 DIEEI Universitá degli Studi di Catania. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’19, October 17–18, 2019, New York, NY, USA © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6700-4/19/10. . . $15.00 https://doi.org/10.1145/3313231.3352375 Figure 1: Traffic generated for a convolutional layer. outcome of the analysis is that the on-chip communication account for a relevant faction of the total inference latency whereas energy consumption is dominated by the memory sub-system (both on-chip and off-chip). 2 NOC-BASED DEEP NEURAL NETWORK We consider as reference NoC-based DNN accelerator architecture a mesh-based NoC in which a node can be either a memory interface (MI) or a processing element (PE) which computes one the three types of layers which form the DNN, namely, convolutional layer, max/avg layer, and fully connected layer. A convolutional layer takes in input the input feature map and a set of filter to generate a channel of the output feature map. The left part of Fig. 1 shows the traffic generated to load the input feature map from the main memory. The MI sends the input feature map to the PEs involved in this layer. Each filter is sent to a specific PE (middle part of the figure). Finally, each PE computes a channel of the output feature map that is store back to the main memory (right part of the figure). The output feature map, will be the input feature map for the next layer. It should be pointed out that, the last phase shown in Fig. 1 can be skipped. In fact, each of the PEs active at a generic layer has in its local memory a number (one in the example) of channels of the output feature map. Thus, the PE involved in the layer i can obtain the input feature map from the PEs that computed the layer i − 1. Based on this, with exception of the first layer, the access to the main memory for loading the input feature map can be avoided. Please note that, the underlying assumption behind the above discussion is that, the local memory into the PEs is enough large to store at least one channel of the feature map. If this hypothesis is not met, the output feature map is stored back to the main memory. For a pooling layer, a PE can process multiple feature map channels. In this case, there is no PE to PE traffic as each PE works on the input feature map channel currently stored in its local memory. NOCS ’19, October 17–18, 2019, New York, NY, USA Giuseppe Ascia, Vincenzo Catania, Salvatore Monteleone, Maurizio Palesi, Davide Patti and John Jose Figure 2: Fraction of time spent and energy consumed in each layer. Figure 4: Normalized latency and energy per inference for different number of MIs. Figure 3: Normalized latency and energy per inference for different local memory size. If the local memory is not large enough to store the entire feature map channel, the latter is fetched from the main memory resulting in memory to PEs traffic. In a fully connected layer, the output neurons have a number of inputs equal to the size of the input feature map. A PE can process multiple neurons. Thus, each PE needs to fetch from the main memory a number of weights corresponding to the size of the input feature map. 3 EXPERIMENTAL ANALYSIS The experimental platform is a simulated parameterized NoC-based Deep Neural Network that allows to assess different architectural configurations in terms of performance and energy. VGG-16 [5] is considered in our experiments. It counts approx 138 M parameters and 20 layers. Fig. 2 shows the fraction of time spent and energy consumed in each layer for VGG-16. The platform is configured as a 8 × 8 mesh in which four MIs are located into the four corners of the NoC, links are 256 bits wide and PE local memory is 32 KB. The fraction of time is broken into its three components, namely, communication, computation, and memory. As it can be observed, the communication dominates the latency. For VGG-16 the communication latency is low in the inner convolutional layers. This is due to the fact that, the feature size after the second max-pool layer drastically decreases. Although FC layers account for a significant fraction of the latency, this is dominated by the second and third layer where the size of the feature maps is much larger than that of filters. The energy consumption is dominated by the memory, both local memory and main memory. Energy spent in local memory dominates in CONV layers. The main memory energy contribution is localized in the first layers in which the feature size is too big to fit the local memory size. Fig. 3 shows the normalized latency and energy per inference for different local memory sizes.As it can be observed, the local memory requirement demanded by VGG-16 is larger than 16 KB to obtain important latency improvements but the latter becomes not relevant above 128 KB. As the local memory size increases, the total energy consumption decreases till an inversion point after that it start to increase. The inversion point is at 16 KB for VGG-16. It is due to the fact that, although as local memory size increases the main memory accesses decrease, the energy per access to the local memory increases. Thus there is a optimal local memory size beyond which the main memory energy saving is less than the local memory per access energy. Fig. 4 shows the normalized latency and energy per inference for different number of MI, from 1 to 8. As the number of memory interfaces increases the average distance between PEs and MIs decreases. Thus, the communication component of the latency for memory accesses decreases. As it can be observed, the latency reduction passing from 1 MI to 8 MIs is almost 80% VGG-16. As the number of MIs increases, the energy consumption decreases. For VGG-16 the energy consumption is dominated by local memory. 4 CONCLUSION In this paper, we have evaluated NoC-based deep neural network accelerators in terms of inference latency and energy consumption for two convolutional neural networks when several architectural parameters are made to vary. Overall, we found that, the NoC is the main responsible for inference latency whereas memory (both local and main memory) is the main contributor for energy consumption. "
Power efficient photonic network-on-chip for a scalable GPU.,"In this paper, we propose an energy efficient and scalable optical interconnect for GPUs. We intelligently divide the components in a GPU into different types of clusters and enable these clusters to communicate optically with each other. In order to reduce the network delay, we use separate networks for coherence and non-coherence traffic. Moreover, to reduce the static power consumption in optical interconnects, we modulate the off-chip light source by proposing a novel GPU specific prediction scheme for on-chip network traffic. Using our design, we were able to increase the performance by 17% and achieve a 65% reduction in ED2 as compared to a state-of-the-art optical topology.","Power Efficient Photonic Network-on-Chip for a Scalable GPU Janibul Bashir Indian Institute of Technology, Delhi janibbashir@cse.iitd.ac.in Khushal Sethi Indian Institute of Technology, Delhi ee1160556@iitd.ac.in Smruti R. Sarangi Indian Institute of Technology, Delhi srsarangi@cse.iitd.ac.in ABSTRACT In this paper, we propose an energy efficient and scalable optical interconnect for GPUs. We intelligently divide the components in a GPU into different types of clusters and enable these clusters to communicate optically with each other. In order to reduce the network delay, we use separate networks for coherence and non-coherence traffic. Moreover, to reduce the static power consumption in optical interconnects, we modulate the off-chip light source by proposing a novel GPU specific prediction scheme for on-chip network traffic. Using our design, we were able to increase 2 as the performance by 17% and achieve a 65% reduction in ED compared to a state-of-the-art optical topology. CCS CONCEPTS • Networks → Photonic Network on chip. KEYWORDS Nanophotonics, Graphics Processing Unit (GPU) ACM Reference Format: Janibul Bashir, Khushal Sethi, and Smruti R. Sarangi. 2019. Power Efficient Photonic Network-on-Chip for a Scalable GPU. In International Symposium on Networks-on-Chip (NOCS ’19), October 17–18, 2019, New York, NY, USA. ACM, New York, NY, USA, 2 pages. https://doi.org/10.1145/3313231.3352370 1 INTRODUCTION 1 GPUs are now the default platforms for high performance machine learning and large scale computing. In this paper we present the design of an optical NoC for a GPU, which is power-efficient and scalable. By analyzing the RODINIA [3] benchmarks, we observed that last level cache units are the major contributors to the on-chip traffic, and hence should be given priority while accessing the on-chip network. Moreover, we observed that by delaying some messages originating from the symmetric multiprocessors (SMs), the effect on the overall performance of the system is minimal – this effect can be used for proposing some optimizations. Based on these observations, we build a GPU specific optical interconnect. In addition, we propose to modulate the off-chip light 1 This work has been sponsored in part by the Semiconductor Research Corporation (SRC). Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’19, October 17–18, 2019, New York, NY, USA © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6700-4/19/10. . . $15.00 https://doi.org/10.1145/3313231.3352370 source based on a novel traffic prediction scheme, thereby reducing the static power consumption to a large extent. We evaluated the design in Section 3 and finally conclude the paper in Section 4. 2 DESIGN OF GPUOPT In this paper, we propose a GPU specific topology aiming at ❶ enhancing performance, and ❷ decreasing the static power consumption. To achieve these goals, we propose ❶ an efficient optical topology, called GPUOPT, and ❷ a novel power scaling technique. We incorporated our power scaling technique on GPUOPT and developed an efficient NoC for GPUs, called PS_GPUOPT. Topology : Figure 1 shows our proposed 3D optical NoC. The chip has two layers: logical layer – containing SMs, L2 banks, and memory controllers (MC), and a photonic layer – containing optical components (powered by an off-chip laser array). The logical layer is divided into 16 clusters: 8 SM_Clusters and 8 LLC_Clusters. Each SM_Cluster has 8 SMs, and each LLC_Cluster has an L2 bank tied to a MC. The intra-cluster communication is done electrically, whereas for inter-cluster communication, we incorporate a separate silicon photonics layer underneath the logical layer. The optical layer has optical stations: one for each SM_Cluster (called SM_station), and LLC_Cluster (called LLC_station). These stations are connected together using two different optical crossbars (with a separate serpentine layout): C_network and NC_network. The C_network is used to carry the coherence messages between the SMs, whereas the NC_network is used to carry the noncoherence messages (L1-to-L2 and L2-to-L1). We assume a singlewriter-multiple-reader (SWMR) topology for the C_network and a multiple-writer-single-reader (MWSR) topology for the NC_network that requires token based arbitration [2]. Moreover, to further improve the performance, we allow any optical station to source power from any power waveguide (arbitration is required). Power Scaling : To decrease the laser power consumption in optical NoCs, we: ❶ divide the execution time into fixed size durations, called epochs, ❷ predict the laser power requirement for the next epoch , and then ❸ modulate the off-chip laser (reconfiguration) [1]. Every SM_station uses a function, Ψ, to predict the laser power requirement in the next epoch. It takes three inputs: waiting time of a station (W ), messages received (M R ), and messages sent in the current epoch (M S ), and produces a 1-bit output (station active/inactive in the next epoch) to be sent to the Laser Controller (L_Cntrlr) at the end of every epoch. The rules are: 1 Ψ (M R , M S , W ) = (M R ≥ RT ∧ M S ≤ α ∗ RT ) ∨ (W ≥ WT ) ∨ (M R ≤ RT ∧ M S < α ∗ MR )  0 de f aul t Here, RT , WT , α , and β are hyper-parameters (generated empirically). Similarly, LLC_stations also send a 1-bit prediction to the L_Cntrlr based on the rules given below. NOCS ’19, October 17–18, 2019, New York, NY, USA Janibul Bashir, Khushal Sethi, and Smruti R. Sarangi  0 de f aul t Figure 1: Topology of the optical NoC Laser Power Consumption: As compared to GPUOPT and Prior_Opt, PS_GPUOPT results in 59% and 67% reduction in laser power consumption respectively. The main reason is the modulation of the off-chip laser in PS_GPUOPT. The lower laser power consumption in GPUOPT as compared to Prior_Opt is attributed to its ability to allow on-chip optical stations to share the available optical power. 2 Comparison: In Figure 3 we compare the energy-delayED 1 (M R ≥ α ∗ RT ) ∨ (W ≥ α ∗ WT ) δ (M R , PE , W ) = ∨ (PE ≥ β ∗ M R ) Here, PE is the number of pending events at the LLC_station. In the reconfiguration phase, the L_Cntrlr collects 16 1-bit predictions sent by the 16 optical stations. Based on these recommendations, the L_Cntrlr calculates the amount of laser power required in the next epoch and accordingly modulates the off-chip light source. 3 EXPERIMENTAL RESULTS To evaluate our design, we used a cycle-accurate GPU simulator GPUTejas [4], and extended it to model optical networks. We used an in-house thermal simulator to calculate the temperature variations on the die and accordingly calculated the amount of tuning power required [2]. We used the workloads from the RODINIA benchmark suite for simulations and compared our design with a state-of-the-art photonic network proposed by Ziabari et al. [5] (Prior_Opt ). This is the baseline design. Performance Comparison: Figure 2 compares the perforFigure 2: Performance comparison mance (inverse of simulated execution time) of three different optical NoC configurations. Form the plot, we conclude that GPUOPT is the best configuration that performs 17% better than Prior_Opt. This is because the GPUOPT allows all the optical stations to share the available optical power. In addition, separating the coherence and non-coherence messages, further increases the performance of the overall system. In comparison, the PS_GPUOPT scheme does not perform so well (11% better) because it sometime produces less power than what is needed – stations need to wait. In GPUOPT the optical power is available all the time since it does not use any laser modulation scheme. Figure 3: Energy-Delay-Square product comparison square (ED 2 ) (energy of entire system) product of the three optical configurations. From the plot we observe that PS_GPUOPT is the best configuration. As compared to Prior_Opt and GPUOPT, it has a 65% and 29% reduction in ED 2 respectively. The lower ED 2 in the case of GPUOPT as compared to Prior_Opt is attributed to its higher performance, whereas in the case of PS_GPUOPT the large reduction is due to a significant reduction in laser power consumption. 4 CONCLUSION In this paper, we designed an efficient optical NoC for GPUs where we use a combination of SWMR and MWSR topologies to decrease the contention. In addition, we propose to use a GPU specific laser modulation scheme in order to reduce the static power consumption. Using these set of techniques, we were able to reduce the laser 2 values as power consumption by 67% with a 65% reduction in ED compared to a state-of-the-art optical topology. "
CDMA-based multiple multicast communications on WiNOC for efficient parallel computing.,"In this work, we introduce an hybrid WiNoC, which judicially uses the wired and wireless interconnects for broadcasting/multicasting of packets. A code division multiple access (CDMA) method is used to support multiple broadcast operations originating from multiple applications executed on the multiprocessor platform. The CDMA-based WiNoC is compared in terms of network latency and power consumption with wired-broadcast/multicast NoC.","CDMA-based Multiple Multicast communications on WiNOC for efficient parallel computing Navonil Chatterjee chatterjee.navonil@univ-ubs.fr Lab-STICC, UBS Rodrigo Cataldo rodrigo.cataldo@acad.pucrs.br Lab-STICC, UBS Hemanta Kumar Mondal hemanta.mondal@ece.nitdgp.ac.in NIT Durgapur Jean-Philippe Diguet jean-philippe.diguet@univ-ubs.fr Lab-STICC, CNRS ABSTRACT In this work, we introduce an hybrid WiNoC, which judicially uses the wired and wireless interconnects for broadcasting/multicasting of packets. A code division multiple access (CDMA) method is used to support multiple broadcast operations originating from multiple applications executed on the multiprocessor platform. The CDMAbased WiNoC is compared in terms of network latency and power consumption with wired-broadcast/multicast NoC. CCS CONCEPTS • Networks → Network on chip; • Computer systems organization → Interconnection architectures; • Computing methodologies → Parallel computing methodologies . KEYWORDS Wired/Wireless Network-on-Chip, Broadcast and Multicast operation, Parallel computing, Code Division Multiple Access (CDMA) 1 INTRODUCTION WiNoC has emerged as a viable solution for the implementation of broadcast [1] communications on large manycores. Also, multiple applications can be executed simultaneously on a given WiNoC platform designed for large manycore architectures. In such a case, broadcast operation in one application may interfere with another one running in the same WiNoC, which might lead to contention and increase in power consumption as a single wireless channel is used by multiple application for broadcast communication. In this work, we propose an original strategy to mitigate the aforementioned problems. We present a WiNoC topology which is divided into uniform clusters where each cluster is associated with one wireless interface. As, all the PEs are not associated with wireless interface, we use hybrid mechanism where both wired and wireless links are used for broadcast and multicast operations. For unicast packet transmission, only wired NoC is used. We have used code division multiple access (CDMA) based wireless packet Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’19, October 17–18, 2019, New York, NY, USA © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6700-4/19/10. . . $15.00 https://doi.org/10.1145/3313231.3352371 transmission to support parallel broadcast communication in two or more applications executing in a WiNoC based multiprocessor platform. The proposed method helps to improve the overall system performance by providing efficient broadcast/multicast communication for single and multiple applications. 2 SYSTEM ARCHITECTURE We consider a 2D Mesh based Wireless NoC which is divided into equal sized clusters as shown in Figure 1a. Each cluster consists of 16 routers which are arranged in 4 × 4 fashion. Every cluster has a centrally placed wireless hub with a wireless interface (WI) providing inter-cluster wireless links. The wireless hub is connected to one router in the cluster, which is known as hub connection (HC) router. Thus, all packets communicated using the wireless hub, flow though the HC router. For unicast communication, we have used table based routing while broadcast packets are communicated using WHIRL routing algorithm [3].The Wireless Interface (WI) connects the router with the wireless medium. The WI consists of 3 parts: (1) Antenna, (2) Analog domain and (3) Digital domain, as shown in Figure 1b. The digital domain consists of Serializer/ Deserializer, CDMA encoder/decoder and channel access controller. The main component of the analog domain is the On-Off Keying (OOK) transceiver module. zigzag metal antenna is used for signal transmission and reception. A MAC mechanism is used to share wireless channels among multiple users. We based our choice on a contention free MAC protocol as used in OrthoNoC[1] and modified it to suit our requirements. 3 BROADCAST AND MULTICAST OPERATIONS In this work, we have assumed that a single application or multiple applications may be executed in parallel on the multicore WiNoC. In case of single application, during broadcast operation only one hub works as the source and all other hubs as the destination. There is no conflict with another broadcast, if two of them do not start at the same time. Thus no code is required as all the hubs receive the message. However, in case of multiple applications are executed in parallel, a conflict may occur if both the applications launch a broadcast operation at the same time. In such as case, different CMDA codes will be assigned to the applications. In this work, we have used two type of CMDA based code-channel: (i) two codechannels and (ii) four code-channels, where maximum of two and four applications are executed in parallel, respectively. NOCS ’19, October 17–18, 2019, New York, NY, USA Chatterjee, et al. (a) 12 × 12 Wireless NoC (b) Wireless Interface Architecture Figure 1: Overview of the system Model Comparison with NoC+WHIRL, there is 4.25%, 4.12%, and 2.65% improvement in PDP for scenario 1, scenario 2 and scenario 3, respectively. In scenario 4, we observe a degradation in PDP. Figure 3: Analysis of (a) Network Latency and (b) PDP for multicast operation using different synthetic applications 5 CONCLUSION In this paper, we present a solution to efficiently implement broadcast and multicast operations on future large manycore architecture with the proposed Wireless NoC. We have implemented a CDMAbased transmission protocol for multicast operations in parallel applications. The proposed WiNoC supports maximum of four code-channels which allows execution of four parallel applications. ACKNOWLEDGEMENT This work is a part of BBC project supported by Labex CominLABS. "
NoC-based DNN accelerator - a future design paradigm.,"Deep Neural Networks (DNN) have shown significant advantages in many domains such as pattern recognition, prediction, and control optimization. The edge computing demand in the Internet-of-Things era has motivated many kinds of computing platforms to accelerate the DNN operations. The most common platforms are CPU, GPU, ASIC, and FPGA. However, these platforms suffer from low performance (i.e., CPU and GPU), large power consumption (i.e., CPU, GPU, ASIC, and FPGA), or low computational flexibility at runtime (i.e., FPGA and ASIC). In this paper, we suggest the NoC-based DNN platform as a new accelerator design paradigm. The NoC-based designs can reduce the off-chip memory accesses through a flexible interconnect that facilitates data exchange between processing elements on the chip. We first comprehensively investigate conventional platforms and methodologies used in DNN computing. Then we study and analyze different design parameters to implement the NoC-based DNN accelerator. The presented accelerator is based on mesh topology, neuron clustering, random mapping, and XY-routing. The experimental results on LeNet, MobileNet, and VGG-16 models show the benefits of the NoC-based DNN accelerator in reducing off-chip memory accesses and improving runtime computational flexibility.","NoC-based DNN Accelerator: A Future Design Paradigm Special Session Paper Kun-Chih ( Jimmy) Chen National Sun Yat-sen University, Kaohsiung, Taiwan kcchen@mail.cse.nsysu.edu.tw Masoumeh Ebrahimi KTH Royal Institute of Technology, Stockholm, Sweden mebr@kth.se Ting-Yi Wang Yuch-Chi Yang National Sun Yat-sen University,Kaohsiung, Taiwan ABSTRACT Deep Neural Networks (DNN) have shown significant advantages in many domains such as pattern recognition, prediction, and control optimization. The edge computing demand in the Internet-ofThings era has motivated many kinds of computing platforms to accelerate the DNN operations. The most common platforms are CPU, GPU, ASIC, and FPGA. However, these platforms suffer from low performance (i.e., CPU and GPU), large power consumption (i.e., CPU, GPU, ASIC, and FPGA), or low computational flexibility at runtime (i.e., FPGA and ASIC). In this paper, we suggest the NoC-based DNN platform as a new accelerator design paradigm. The NoC-based designs can reduce the off-chip memory accesses through a flexible interconnect that facilitates data exchange between processing elements on the chip. We first comprehensively investigate conventional platforms and methodologies used in DNN computing. Then we study and analyze different design parameters to implement the NoC-based DNN accelerator. The presented accelerator is based on mesh topology, neuron clustering, random mapping, and XY-routing. The experimental results on LeNet, MobileNet, and VGG-16 models show the benefits of the NoC-based DNN accelerator in reducing off-chip memory accesses and improving runtime computational flexibility. CCS CONCEPTS • Computing methodologies → Artificial intelligence; • Computer systems organization → Architectures ; KEYWORDS Network-on-Chip (NoC), Deep Neural Network (DNN), CNN, RNN, Accelerators, Routing Algorithms, Mapping Algorithms, Neural Network Simulator ACM Reference format: Kun-Chih ( Jimmy) Chen, Masoumeh Ebrahimi, Ting-Yi Wang, and YuchChi Yang. 2019. NoC-based DNN Accelerator: A Future Design Paradigm. In Proceedings of International Symposium on Networks-on-Chip, New York, NY, USA, October 17–18, 2019 (NOCS ’19), 8 pages. https://doi.org/10.1145/3313231.3352376 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’19, October 17–18, 2019, New York, NY, USA © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6700-4/19/10. . . $15.00 https://doi.org/10.1145/3313231.3352376 1 INTRODUCTION Deep Neural Networks (DNNs), as a subset of Neural Networks (NNs), have shown enormous advantages in various domains. The number of applications benefiting from DNNs is increasing with the most popular ones in the area of pattern recognition, image classification, and computer vision [1][2]. The main advantage of DNN is the offered high accuracies that come at the cost of high computational complexity and power consumption. The recent DNN models, such as AlexNet [3] and VGG-16 [4], consist of hundreds of layers and millions of parameters that are too complex to be efficiently run on the existing hardware platforms. There are several algorithmic and architectural solutions to reduce the number of parameters and computational complexity. Pruning [5] and bit-width reduction [6] are in this category. Another solution is to exploit parallelism through specialized DNN accelerators and ASIC design. In spatial architectures, the accelerators are composed of DNN processing units, arranged in a 2D array to flow the data from one PE to another [7]. To be integrated with other algorithmic and architectural solutions, these ASIC-based architectures demand new processor designs and connectivity patterns. This trend has led to several heuristic solutions and specific architectures. However, these architectures lack flexibility, and this trend impose huge design cost. In addition, these accelerators are optimized for a specific DNN application such as image classification while showing a poor performance under differing applications such as speech recognition. Another major problem is the long data communication latency mainly due to frequent read/write accessed from/to off-chip memory. FPGAs are an alternative for the fast prototype of DNN accelerators due to their re-programmable attribute [8]. Compared with the ASIC-based DNN accelerators, FPGAs provide a flexible design space for various DNN accelerator designs. However, FPGA-based designs still suffer from low computational flexibility as they are configured for a specific DNN model and a particular application. To enhance the design flexibility of the current DNN accelerators and reduce the interconnection complexity, a modular structure similar to Networks-on-Chip (NoC) can be exploited. NoC is a packet-switched network which enables a large number of PEs to communicate with each other. NoC consists of routers and links, where each router is connected to a PE (or a group of PEs), and links connect the routers to each other. Topology determines the overall arrangement of routers and links which can be in the form of mesh and torus. In NoCs, resources are shared and distributed among PEs, e.g., memory is distributed among PEs, and packets may utilize distributed network resources in their paths toward the destination. The superiority of NoC has inspired researchers to utilize it in the modeling of large-scale spiking neural networks (such as SpiNNaker NOCS ’19, October 17–18, 2019, New York, NY, USA K.C. Chen and M. Ebrahimi, et al. 2 CONVENTIONAL DNN COMPUTING PLATFORMS In recent years, deep neural networks (DNNs) have become the foundation for many modern artificial intelligent (AI) applications. While DNN delivers stunning accuracy in many AI applications, it comes at the cost of intensive communication and extensive computation. In this section, we review the conventional hardware platforms to execute and accelerate DNN operations. 2.1 Common Hardware Platforms for Neural Network Computing Central Processing Units (CPUs) typically consist of multi-core processors (usually from 2 to 10 cores), employed on desktop and mobile devices. Advanced CPUs, such as 48-core Qualcomm Centriq 2400 [14] and 72-core Intel Xeon Phi [15], are composed of more cores to improve the computation efficiency. CPUs are basically designed to compute general-purpose computations, so they have the main advantage of high computational flexibility, and they are capable of executing complex operations. Nevertheless, they are not suitable for DNN computing, which involves intensive parallel while simple computations (such as multiply and accumulate in convolution). While it has been some attempts to exploit CPU clusters (such as Intel BigDL [16]) to optimize deep learning libraries [17], they cannot still satisfy the efficiency demands in DNN computing. Nowadays, Graphics Processing Units (GPUs) are popular devices to compute DNN operations. Originally, GPUs were designed for computer graphic tasks. Due to the intrinsic parallel computing features in GPUs, they can efficiently execute the essential DNN operations in parallel, such as matrix multiplication and convolution. Also, thanks to the powerful parallel computing platforms (such as Tesla V100 by NVIDIA) and application programming interfaces (such as Compute Unified Device Architecture (CUDA)), GPUs are being extensively used for DNN acceleration in many applications [18]. Although GPUs provide high parallel computing capability to speed up the DNN computing, there is a general concern about the increasing power consumption [19]. Furthermore, GPUs practically require additional communication channels (such as PCIe or NVLink) to connect with the sampling devices. Therefore, the data transfer latency is the other issue in this kind of computing platforms [8]. 2.2 Neural Network Accelerators Because of the intrinsic characteristic of parallel computation in DNN operations, it is intuitive to exploit parallelized multicore hardware to accelerate the operations. However, due to the bandwidth limitation between off-chip memory and processing cores, it is not power and performance efficient to frequently transfer highly parallelized data into the parallelized multicore hardware. To achieve a better trade-off between performance and power consumption in DNN computing, Application Specific Integrated Circuits (ASIC) are used to realize the DNN hardware. In ASIC-based designs, to reach high performance and power efficiency, on-chip computing units are optimized for a particular application [20][21][22][23]. As a result, ASIC-based accelerators achieve superior performance and power advantages over their CPU and GPU counterparts. Figure 1: Flexibility and reconfigurablity of current DNN accelerator design paradigms. [9][10] and CuPAN [11]) and a few DNN accelerators (such as Eyeriss-v2 [12] and Neu-NoC [13]). Fig. 1 summarizes the aforementioned DNN accelerator design paradigms. CPUs and GPUs offer a very high reconfigurability feature at runtime, which enable them to support diverse DNN applications. However, these platforms suffer from high power consumption and data transfer latency between PEs and off-chip memory. In contrast, the ASIC-based DNN accelerators are specifically designed for a particular DNN model in order to achieve optimal performance and power efficiency. In return, these designs limit the computational flexibility; and as a result, the ASIC-based DNN accelerators are not reconfigurable at design time. As shown in Fig. 1, although the FPGA-based designs improve the design flexibility, the computational flexibility is not still sufficient to support reconfigurability at runtime. For example, the data path is fixed for a particular RNN or DNN model at design time, and no further reconfiguration can be done at runtime. Among different DNN accelerator design methodologies, a NoCbased DNN accelerator could be an appropriate choice, which offers power efficiency, computational flexibility, and reconfigurability. The reason is that the NoC-based design methodology decouples the DNN operation into computation and data transmission. Regarding the data transmission part, the NoC interconnection can efficiently process various data flows for different DNN models. Regarding the computation part, different DNN computing models can be executed independent of the data flow. Furthermore, flexible communication reduces the frequent accesses to the off-chip memory by handling the data transfer from one core to another on the chip. Reducing memory accesses leads to significant power saving. The main contributions of this paper are summarized as follows: • We investigate the state-of-the-art DNN accelerator design methodologies and analyze the pros and cons of each. • We suggest Network-on-Chip as a new design paradigm to implement future flexible DNN accelerators. • We analyze the number of memory accesses in the conventional and NoC-based designs under different DNN models . • We analyze the performance of the NoC-based DNN accelerator under different design parameters. NoC-based DNN Accelerator: A Future Design Paradigm NOCS ’19, October 17–18, 2019, New York, NY, USA Figure 2: A PE array design usually adopts the data reuse feature in DNN operations. As mentioned before, the massive data communication latency between the off-chip memory and the processing units has become the system performance bottleneck in advanced DNN models. Because the DNN operation can be treated as a matrix multiplication [7], many ASIC-based DNN designs have improved the conventional unicast or systolic array architecture (Fig. 2(a) and (b)) and developed the multicast or broadcast architecture (Fig. 2(c) and (d)). In these architectures, the DNN computation (i.e., the matrix multiplication) is accelerated using the characteristics of data reuse and data sparsity. In [24], Esmaeilzadeh et al. proposed a hardware called Neural Processing Unit (NPU) to approximate program functions with approximate algorithmic transformation. DianNao series, including DianNao [25], DaDianNao [26], and ShiDianNao [27], use Neural Functional Unit (NFU) to perform both neuron and synapse computations. Different from these platforms, PuDianNao [28] employs Functional Unit (FU) as the basic execution entity. Unlike the accelerators using NFU, PuDianNao can support additional machine learning algorithms such as k-means, linear regression, multi-layer perceptron (MLP), and support vector machine (SVM). In [29], Yin et al. proposed a hybrid-NN processor that is composed of two types of PEs and can support configurable heterogeneous PE arrays. By exploiting the characteristic of data reuse in the conventional CNN models, Chen et al. proposed the Eyeriss processor [7] that can optimize the neural network computation for a specific dataflow. In spite of the fact that the ASIC-based DNN accelerators can achieve better performance with high power efficiency, the design flexibility is very low as the design is optimized based on the specification of one or few DNN models. To mitigate this design challenge, the FPGA-based DNN design methodology has become attractive in recent years [8]. FPGA is an integrated circuit that allows reconfiguration of interconnects between essential components, such as simple logic gates and memory. In any aspect of the application domain, it can properly reconfigure the programmable gates and find a specific architecture to reach the design constraint within a short period. Although the design flexibility of the FPGA-based designs is better than the ASIC-based designs, the computational flexibility is still limited during the runtime operation (e.g., data paths are fixed at design time). 3 NOC-BASED NEURAL NETWORK DESIGN 3.1 NoC in Deep Learning As mentioned before, CPU, GPU, ASIC, and FPGA can accelerate DNN operations, but each comes with a drawback when dealing with large-scale and hybrid DNN models. CPUs and GPUs are power-hungry and require additional complexity around their compute resources to facilitate software programmability [30]. ASIC and FPGA devices are more power-efficient, but they are configured and optimized for a specific model, and thus they lack computational flexibility at runtime. To solve the problems mentioned above, Network-on-Chip (NoC) can be integrated into the DNN accelerator [13]. The NoC-based DNN accelerator separates the platform into data computation and communication. Such separation brings the advantages of better computational flexibility (e.g., different DNN models can be efficiently executed on the platform) and design simplicity and scalability (e.g., computation and computation can be separably designed and optimized) [12]. In the NoC-based design, after executing the neuron operations by a processing element, the obtained result is propagated through packets to the next PE. This process is repeated until the final result is obtained. In this way, no specific dataflow is required to be considered in order to run the target DNN model in the DNN accelerator efficiently. The important point is that the flexible communication between PEs enables handling different DNN models with varying data flows using an identical architecture. Some aspects of designing an NoC-based DNN accelerator have already been investigated in the literature. For example, an efficient mapping algorithm was proposed in [13], and a proper NoC topology was discussed in [31] and [32]. In summary, the offered high flexibility and regular structure make the NoC-based DNN accelerator a new and attractive design paradigm. With proper mapping and routing algorithms, the computational power and performance can be adjusted according to the underlying DNN models. As shown in Fig. 1, the NoC-based designs suggest a better trade-off between the reconfigurability feature and power consumption compared with the conventional design choices (i.e., CPU, GPU, ASIC, and FPGA). NOCS ’19, October 17–18, 2019, New York, NY, USA K.C. Chen and M. Ebrahimi, et al. 3.2 NoC-based ANN/DNN Simulator NoC can reduce the interconnection complexity between PEs in DNN accelerators. To facilitate the NoC-based DNN design, a highlevel NoC-based neural network simulator is needed. However, most of the current simulators focus either on the traffic behavior on the NoC [33][34][35] or the DNN computation [36][37]. One of the first tools to simulate the NoC-based ANN is proposed by Chen et al. [38] and is called NN-Noxim. This simulator facilitates the NoC-based ANN evaluation in the system level. NN-Noxim is an extension of Noxim [33], which is a popular NoC simulator to simulate the traffic behavior on the NoC system. To improve the hardware utilization, the computation of multiple neurons has been assigned into one Processing Element (PE). In other words, the original ANN is first clustered, and then the clustered ANN is mapped to the target NoC platform. NN-Noxim reports classification accuracy as well as the hardware results such as power and latency. Although NN-Noxim has embedded the fundamental functions of ANN, it is not supporting convolution and pooling calculations. As a result, it cannot compute most CNN models, which include convolution and pooling operations. This issue has been addressed by the further development of the simulator in [39]. 3.3 NoC Design Parameters in ANN/DNN Accelerators Advanced DNN models usually consist of thousands of neuron computations, which results in huge computation, inter-neuron communication, and memory accesses. Designing optimized accelerators to solve these issues has become the core of many recent research efforts. Each PE has to execute neuron computations at high frequency, and thus fast and energy-efficient PE designs are highly demanded. The results of each PE should be delivered to another PE in the next layer. This data transmission requires massive inter-neuron communication, and thereby, an efficient interconnection platform is required to offer a flexible and scalable platform. On the other hand, PE outputs and partial sums are usually stored in the off-chip memory. Loading and offloading this data lead to massive memory accesses, which significantly increases the power consumption and data transfer latency. This issue also requires proper solutions in different design levels. In this section, we investigate the solutions that NoC can contribute to addressing. Mapping and Clustering Algorithms: Mapping algorithms define the way in which neurons are allocated to the processing elements. Since a single neuron computation is very simple, it is a cost-efficient solution to assign multiple neurons to a PE [38]. So, clustering algorithms determine how neurons are grouped. The mapping and clustering algorithms should be designed in such a way that to reduce the overall data communication, packet latency, and power consumption. Different mapping and clustering algorithms have been introduced so far in the area of many-core systems [40][41]. However, it has been few contributions investigating the mapping of ANN applications into the NoC-based platform. To reduce traffic congestion and the data delivery latency, Liu et al. proposed an NN-aware mapping algorithm to map a neural network to the NoC platform [13]. By analyzing the communication flow in a neural network and the topology of the adopted NoC, authors apply an exhaustive search approach to find the optimal mapping results with the smallest data communication latency. This mapping algorithm reduces the average packet latency in comparison with sequential and random mapping. In another work, Firuzan et al. [42], proposed an approach to vertically or horizontally slice the neural network in order to map a large-scale neural network to the target NoC platform. In the vertical slicing, neurons in the same layer are grouped while in the horizontal slicing, neurons in different layers are grouped. Each group is then assigned to one PE in the NoC platform. Topology : Topology determines the arrangement of PEs, routers, and links in the network. In [12], a hierarchical mesh topology is proposed to connect PEs. The proposed topology helps in reducing data communication in DNN computation. In [42], Firuzan et al. proposed a cluster-based topology with a reconfigurable architecture to construct an NoC-based neural network accelerator. Since a cluster handles most of the data communication locally, this kind of cluster-based NoC architecture can mitigate traffic congestion in the network. The clos topology is proposed by Yasoubi et al. [11] where PEs are connected through a direct path. The main motivation behind this work is to keep the data communication latency scalable with respect to the mesh size. Routing Algorithms and Router Architecture : Routing algorithms define the possible routes to deliver a packet from a source to a destination. Routing algorithms in the NoC platform have been extensively investigated in literature [43][44]. However, the type of data flow in neural networks is rather different and is mostly one-to-many (i.e., data multicasting/broadcasting) and many-toone traffic (i.e., data gathering), so proper routing algorithms are demanded to support these types of traffic patterns. Regarding the router architecture, some works have utilized traditional 5-port router with wormhole switching [26]. However, most of the current ANN designs are based on circuit-switching, where the paths are fixed at design time [7]. This is mainly to avoid the overhead of routing at runtime. Packet-switched NoC, on the other hand, brings an advantage of flexibility, which may contribute to reducing the memory accesses. Memory Bandwidth: As mentioned before, load and off-loading data from memory leads to massive memory accesses that is counted as one of the leading design challenges in DNN accelerators. To alleviate this problem, Kwon et al. proposed a tree-based NoC interconnect to provide a higher bandwidth between memory and PEs [45]. Some data flow modes are also proposed to efficiently process different actions between the memory and the PEs such as scatter and gather. Due to employing the tree-based structure, the complexity of the data communication between the memory and PEs is reduced from O (n2 ) to O (nl oдn) in comparison with the mesh-based topology. As a result, this approach decreases the memory access latency. 4 NOC-BASED DNN ACCELERATOR DESIGN PARADIGM To implement a DNN network, such as Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN), the network should be first flattened into an ANN-like network. As shown in NoC-based DNN Accelerator: A Future Design Paradigm NOCS ’19, October 17–18, 2019, New York, NY, USA Figure 3: A CNN is flattened into ANN-like CNN: (a) the convolution layer and (b) the pooling layer can be expanded into a partially-connected layer; (c) the interconnection structure of a fully-connected layer is maintained. (a) (b) (c) Figure 4: (a) The flattened DNN model (e.g., ANN-like CNN); (b) clustering neurons; (c) mapping the clustered ANN model to the target NoC platform [38]. Fig. 3, a DNN network is usually composed of different types of layers, such as convolution layer, pooling layer, and fully-connected layer. In order to have a flexible platform, capable of executing different DNN models, we need to reach a unified computation model. For this purpose, similar to [39], we express the operations in different types of layers by Multiply-And-Accumulate (MAC) operations as follows: In the convolution layer , an output map is produced by performing a convolution on the input data and by using a kernel. Thereby, a partial convolution output (Oc 1 ) can be expressed by MAC operations:  Oc 1 = K1 × I1 + K2 × I2 + K3 × I6 + K4 × I7 (Kl × I j ). = l =1, 2, 3, 4; j =1, 2, 6, 7 (1) By extending this equation to calculate other partial outputs, a semi fully-connected layer can be obtained, as illustrated in Fig. 3(a). In the pooling layer , the popular maximum pooling method is applied to capture the most important features [46]. To reuse the MAC operations and increase the hardware utilization, we multiply each input to the weight of 1 and apply the argmax function to find the maximal value. Thereby, a partial pooling output (Op 1 ) in the sub-sampling operation can be calculated by: Op 1 = Max (Oc 1 , Oc 2 , Oc 5 , Oc 6 ) = Pi =1, 2, 5, 6 (W × Ii ), (2) where W is 1 and the pooling operator (P N i =1 I j ) is equal to [39]: P N i =1 Ii = ar дmax (Ii ). (3) By repeating this operation, all outputs of the pooling layer can be computed, as shown in Fig. 3(b). In the fully-connected layer , each neuron in one layer is fully connected to all neurons in the next layer. As illustrated in Fig. 3(c), a partial output in the fully-connected layer O f 1 can be expressed by: 4 O f 1 = W1 × Op 1 + W2 × Op 2 + W3 × Op 3 + W4 × Op 4 (Wk × Op k ). k =1 = (4) Using a similar equation, the final outputs (i.e., O 1 and O 2 ) can be obtained. NOCS ’19, October 17–18, 2019, New York, NY, USA K.C. Chen and M. Ebrahimi, et al. Based on the analysis, we can conclude that most operations in CNN (such as convolution and pooling) can be executed using MAC operations. As shown in Fig. 3, by this procedure, a flattened CNN model is obtained, that is called ANN-like CNN. After obtaining the ANN-like CNN (Fig. 4(a)), mapping algorithms [38] should be applied to map the flattened model to the NoC platform. As shown in Fig. 4(b), the first task is to divide neurons into different execution groups. We cluster neurons layer by layer, and the number of neurons per group depends on the computational power of each PE. As shown in this figure, neurons in Layer 1 are divided into Group 0 and Group 1 and their computations are assigned to PE0 and PE1, respectively (Fig. 4(c)). Similarly, neurons in Layer 2 are divided into Group 2, Group 3, and Group 4 and their corresponding PEs are PE2, PE3, and PE4, respectively. When the computations in a PE is completed, the partial results are packaged and sent to all PEs in the next layer. For example, the obtained results from PE0 and PE1 are transmitted to all PEs in Layer 2 (i.e., PE2, PE3, and PE4). 5 EVALUATION RESULTS AND ANALYSIS In this section, we compare the conventional DNN design with that of the NoC-based design. The comparison is made in terms of performance and the number of memory accesses. UNPU [47] is selected as the representative of the conventional DNN design. The reason for this selection is that UNPU provides a look-up table-based PE (LBPE) to support matrix multiplication and MAC operation. Thereby, UNPU offers better computational flexibility in calculating various DNN operations, in comparison with other DNN designs such as Eyeriss [7]. To evaluate the UNPU design, we have described its computing behavior in SystemC while for the NoC-based DNN design, we utilized the CNN-Noxim simulator [39]. To have a fair comparison, the latency model of UNPU is used for the analysis of both conventional and NoC-based designs. In addition, for the sake of simplicity, we employed mesh topology, random mapping, and XY routing in the simulation process. However, the performance can be improved by employing other advanced topologies [42][45], mapping [40], and routing techniques [48]. For the memory bandwidth, we assume that the bandwidth is enough to access 16 bits of data within one cycle (i.e., similar to the assumption in UNPU [47]). With regards to the target DNN networks, we evaluated LeNet [49], MobileNet [50], and VGG-16 [4] as the representative of a small, medium, and large-scale DNN model, respectively. As shown in Fig. 5, the ANN-like CNN model of LeNet, MobileNet, and VGG-16 involves 286K, 269M, and 17B MAC operations, respectively. 5.1 The Number of Memory Accesses In conventional DNN accelerators, the overall system performance is highly affected by the off-chip memory accesses [45]. These memory accesses are necessary as the partial results are stored in the off-chip memory for further processing by other PEs. In other words, PEs communicate with each other, mainly through read/write operations from/to the off-chip memory. In the NoC-based design, however, PEs can communicate with each other through the network, and thus the off-chip memory accesses can be significantly reduced. Moreover, the cache-coherence technique is an important Table 1: Comparison between conventional and NoC-based designs regarding the number of off-chip memory accesses. LeNet [49] MobileNet [50] VGG-16 [4] Conventional [47] NoC-based Design 884,736 47,938 (-94.6%) 1,061,047,296 4,360,616 (-99.6%) 1,165,128,192 138,508,072 (-88.1%) aspect in NoC to reduce memory accesses. The data synchronization can be achieved with mature memory-coherent techniques in NoC [51]. Using the synchronization techniques, for example, a weight can be loaded from the off-chip memory into one PE and then synchronized with other PEs that need this weight. Hence, the DNN-based NoC design not only reduces the number of off-chip memory accesses but performs the neuron computing distributively. In this section, we compare the number of memory accesses in the NoC-based versus the conventional design. We assume that the resources on the platform are enough to transfer all parameters of the given model from the off-chip memory to the NoC platform at once. This assumption is aligned with recent NoC-based architectures (such as Intel’s Skylake [52]) that reduce the global memory size and increase those of the local ones. TABLE 1 shows the benefit of the NoC-based DNN design, assuming that sufficient distributed on-chip local memory is available. For example, in the LeNet network, the NoC-based design first read all network parameters from the off-chip memory through 47, 938 read accesses. It should be noted that the local memory accesses have not been counted in TABLE 1 as their imposed latency and power consumption are much lower than off-chip memory accesses. The conventional design, on the other hand, requires 884, 736 memory accesses due to frequent off-chip read and write operations. Similarly, the number of off-chip memory accesses can be reduced from approximately 106M to 4M in MobileNet and from 10B to 138M in VGG-16 . In sum, the results show that the distributive neuron computing in the NoC-based design, on average, can reduce the memory accesses by 94%, 99%, and 88% in the LeNet, MobileNet, and VGG-16 network, respectively. This memory access reduction can reduce the power consumption and increase the system performance significantly. 5.2 Performance under Different NoC Sizes In this section, we study the performance of the selected DNN models under different NoC sizes as well as different group sizes. For the performance analysis, we report the PE computational latency, the NoC data delivery latency, and the total latency. The PE computational latency is the total execution time to compute neuron operations in PEs, taking into account the parallel computing. The data delivery latency is the total time that packets spend in NoC. For the evaluation, we consider three DNN models as LeNet, MobileNet, and VGGnet-16; and three different NoC sizes as 6 × 6, 8 × 8, and 12 × 12. The involved packet injection rate (PIR)depends on the execution time of a single PE, which further relies on the group size (i.e., the PIR is equal to the reciprocal of the group size in this work). To show the impact of different group sizes, we also investigate the performance under three different group settings (i.e., the number of neuron computations per PE). NoC-based DNN Accelerator: A Future Design Paradigm NOCS ’19, October 17–18, 2019, New York, NY, USA Figure 5: The performance evaluation of three DNN models under different NoC and neuron group sizes. Let us first assume the LeNet network with 284K operations under the 6 × 6 NoC size. We consider a configuration where the maximum computational capacity of a PE is 3456 neuron computations. To assign neurons to PEs, we start with the first PE in the platform and allocate the computation of as many neurons as to utilize the maximum PE computational capacity (e.g., 3,456 neurons or 10,368 MAC operations by assuming that each neuron involves 3 MAC operations). Then we continue with the next PE in the platform and so on. If all PE assignments for one layer is completed, the assignment in the next layer will be started using a new PE. As can be seen in all sub-figures, by reducing the computational capacity of a PE, the PE latency is reduced. On the other hand, by a smaller group size setting, more transaction data is generated and delivered through the NoC platform. Hence, the NoC data delivery latency will be increased. The growth of the NoC latency is smaller in small network sizes (e.g., 6 × 6 and 8 × 8) while the difference becomes more significant in larger network sizes (e.g., 12 × 12). The main reason is the longer paths in larger networks. It can also be observed that in smaller networks (e.g., LeNet and MobileNet), the total latency is dominated by the NoC communication latency while in large-scale networks (e.g., VGG-16), the total latency is dominated by the PE computational latency. Therefore, the neuron clustering should be done in such a way that neither the NoC communication latency nor the PE computation latency becomes the system performance bottleneck. Therefore, besides improving the PE design, efficient topology arrangement, routing, and mapping algorithms are also essential to improve the performance of the NoC-based DNN accelerators. 6 CONCLUSION In this paper, we first investigated conventional platforms for DNN computing (i.e., CPU, GPU, ASIC, and FPGA) and discussed the pros and cons of each platform. Then, we introduced the NoCbased DNN accelerator which brings the main benefits of reducing off-chip memory accesses and enhancing runtime computational flexibility. For the execution of a DNN model in a NoC-based platform, the model was first flattened into an ANN-like network. Then it was clustered, and each group was mapped to a PE by using random mapping. The XY-routing was used to route packets in the network. Compared with the conventional DNN design, the number of memory accesses in NoC-based design was reduced by 88% up to 99% under three different DNN models. The performance analysis showed a direct relationship between the neuron clustering and its effect on the PE and NoC latency. Finally, we came to the conclusion that efficient PE and NoC designs are both critically important to keep the system performance high in the NoC-based DNN accelerators. ACKNOWLEDGMENTS This work was supported by the Ministry of Science and Technology under the grant MOST 108-2218-E-110-010 and MOST 108-2218-E110-006, TAIWAN; the STINT and VR projects, SWEDEN. NOCS ’19, October 17–18, 2019, New York, NY, USA K.C. Chen and M. Ebrahimi, et al. "
Detection and prevention protocol for black hole attack in network-on-chip.,"Network-on-Chip (NoC) has become exposed to security threats. It can be infected with a Hardware Trojan (HT) to degrade the system performance and apply a denial of service attack. In this paper, we proposed a new HT-based threat model, known as Black Hole Router (BHR), where it deliberately drops packets from the NoC. We proposed a detection and prevention protocol to such BHR attack with reasonably low overhead. The results show 10.83%, 27.78%, and 21.31% overhead in area, power, and performance, respectively. However, our proposed protocol not only detects the BHR attack but also avoids it and assures packet-delivery.","Detection and Prevention Protocol for Black Hole Attack in Network-on-Chip Luka Daoud, and Nader Rafla LukaDaoud@u.boisestate.edu,nrafla@boisestate.edu Boise State University Boise, ID 83725 USA ABSTRACT Network-on-Chip (NoC) has become exposed to security threats. It can be infected with a Hardware Trojan (HT) to degrade the system performance and apply a denial of service attack. In this paper, we proposed a new HT-based threat model, known as Black Hole Router (BHR), where it deliberately drops packets from the NoC. We proposed a detection and prevention protocol to such BHR attack with reasonably low overhead. The results show 10.83%, 27.78%, and 21.31% overhead in area, power, and performance, respectively. However, our proposed protocol not only detects the BHR attack but also avoids it and assures packet-delivery. KEYWORDS Black Hole Router, Hardware Trojan, Network-on-Chip. 1 INTRODUCTION Network-on-Chip [1] was proposed to address the complexity of increasing the interconnect of multicore systems to enhance the systme’s performance, where applications are dynamically allocated [2, 4, 8] and run simultaneously sharing the network resources which had made NoC vulnerable to security threats [3]. By experiencing outsourcing design from an untrusted third party, the original design may be modified at any stage of the design and infected with a Hardware Trojan (HT) to degrade the system performance [5], surreptitiously delete data, or leave a backdoor for secret key leaking [3]. Detection of HT may fail due to the complexity of the design and the nature of the HT of being so small and activated under very specific conditions. Unlike faulty nodes, where they are inoperative nodes and can be detected through post-silicon tests, a malicious node interacts with the system and silently applies its payload. A considerable amount of literature has been published on infected NoC. Most of them focused on snooping sensitive information and applying denial of service (DoS) attacks [3, 5, 6, 9]. In [9], authors addressed HT issues in NoC links. In [5], authors proposed a HT model that applies DoS attack by misrouting the packets to degrade the NoC performance causing deadlock and virtually link failure. In [10], authors studied the effect of sinkhole attack in NoC but power analysis and a detailed distribution of HTs in the NoC was not provided. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner /author(s). NOCS ’19, October 17–18, 2019, New York, NY, USA © 2019 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-6700-4/19/10. https://doi.org/10.1145/3313231.3352374 Since the NoC could be infected with a HT, it can not be trusted any longer and in-field HT-detection is needed at runtime. The goal of this paper is to detect and prevent a Black Hole Router (BHR) attack. BHR is a malicious node in the network that silently discards packets that are passing through, where it seems to the upstream router that packets have been successfully forwarded. In [6], we provided analysis of the BHR attack and its effect on a Mesh NoC. To the best of our knowledge, no solution has been presented to detect a black hole router in Network-on-Chip. In this paper, we filled this gap and proposed a security scheme to not only detect a black hole router but also prevent their effect on the NoC. 2 DETECTION AND PREVENTION OF BHR In order to confirm that a packet is not dropped and moving forward from one hop to another in NoC, a switch-to-switch (s2s)-based acknowledgement (ACK) is required. Therefore, a single notification is sent back to one hop away router confirming that the packet is moving forward towards the destination. Figure 1 shows a process of forwarding a packet from a source (S) to a destination (D) including an acknowledge message sent back to the routers (dashed lines). For example, a packet is sent from router (R2) to router (R3). R3 forwards it to R4 and R4 should send a notification message to R2 that R3 had forwarded the message successfully. In case R3 does not forward the packet, R2 will never be notified that the packet has been forwarded to the next hop. In case R2 doesn’t receive an ACK that the packet has been delivered to R4 within a certain amount of time, a flag is raised indicating a potential of malicious behaviour at R3. The time needed for R2 to wait for a response from R4 has a threshold value that depends on some factors such as the processing time to prepare the ACK, the traffic status, and the NoC congestion. However, when a malicious node exists in the NoC, some certain securities must be applied since the ACK of R4 to R3 may pass through a malicious node that may forge the ACK and it will sound Figure 1: Process of Forwarding a Packet from Source (S) to a Destination (D). NOCS ’19, October 17–18, 2019, New York, NY, USA Daoud and Raf la to R2 that the packet has been forwarded successfully. Therefore, a secure acknowledgment is a must. One way to maintain the authenticity of the ACK is to use a shared key between R2 and R4. Then, a secure ACK is signed along with the received packet by R4 using the shared key. In this case, R3 never forge the signature unless it has access to the shared key. One way to use a simple authentication method is by XORing the data with a one-time pad number that is known only by the two communicating routers. One technique to generate different number every time is by using a pseudorandom number generator (PRNG) at both sides of the communicating routers. In order to synchronize the generated numbers and keep them unpredictable, the shared key between them is used as a seed for the PRNG. Additionally, the ACK should be prepared in a trusted and secure unit. Therefore, the authenticated ACK is performed in the SoC firmware at the interface of the processing core with the NoC, which is assumed to be designed in house by a trusted team. Once a BHR is detected, packets detour around it. There are several routing methods, inherited from fault-tolerant routing, to avoid routing around a specific node. In this paper, we implemented the proposed technique in [5] to avoid a malicious node in NoC. 3 EVALUATION AND RESULTS In our previous work [6], BHR attack was analyzed and evaluated. The malicious router area and power overhead increased by 1.98% and 0.74%, respectively, of the baseline router which is too small to be revealed during the post-silicon test. In this paper, a moderate size five-ports router with 8-flit depth FIFO was designed in C/C++ language targeting high-level synthesis (HLS)[7]. Then, it was synthesized using 45nm TSMC technology (Cadence Design Compiler) for area and power analysis. The area and power overhead of our proposed router scheme increased by 10.83% and 27.78%, respectively. Extensive experiments were run to assess the proposed secure router and NoC under different network traffic, buffer sizes and injection rate. In this paper, the system performance and overhead are analyzed for different buffer sizes and injection rate. The network traffic was set to uniform random distribution for 8 × 8 NoC. Figure 2 shows the throughput overhead of the proposed scheme for several packet injection rate (PIR) to avoid packet dropping. We notice that as the injection rate increases, the overhead slightly increases with average 21.31% due to the traffic of many packets in the NoC. In the second experiment, we studied the waiting time for the delivering-confirmation in the NoC for diffident buffer sizes. We Figure 2: Performance Overhead of the proposed scheme Figure 3: Maximum waiting time for a confirmation for different buffer sizes set the buffer sizes to 2, 4, and 8 packets at each input port. Figure 3 shows the maximum waiting time for NoC of different buffer sizes. It is noticed that when the input FIFO buffer size increases the waiting time for the confirmation ACK decreases because the NoC becomes less congested and packets move faster. 4 CONCLUSIONS In this paper, we proposed a threat model of a Black Hole Router (BHR). We proposed a secure protocol for the NoC-router with runtime detection and protection of BHR. Our proposed router scheme detects BHR at runtime and detours around the infected node once it is detected with overhead 10.83%, 27.78%, and 21.31%, in area, power, and performance, respectively. Future work could include developing a power-efficient technique to detect BHR focusing on the detection of such infected nodes that are colluding together to apply their potent attack. "
A 7.5-mW 10-Gb/s 16-QAM wireline transceiver with carrier synchronization and threshold calibration for mobile inter-chip communications in 16-nm FinFET.,"A compact energy-efficient 16-QAM wireline transceiver with carrier synchronization and threshold calibration is proposed to leverage high-density fine-pitch interconnects. Utilizing frequency-division multiplexing, the transceiver transfers four-bit data through one RF band to reduce intersymbol interferences. A forwarded clock is also transmitted through the same interconnect with the data simultaneously to enable low-power PVT-insensitive symbol clock recovery. A carrier synchronization algorithm is proposed to overcome nontrivial current and phase mismatches by including DC offset calibration and dedicated I/Q phase adjustments. Along with this carrier synchronization, a threshold calibration process is used for the transceiver to tolerate channel and circuit variations. The transceiver implemented in 16-nm FinFET occupies only 0.006-mm2 and achieves 10 Gb/s with 0.75-pJ/bit efficiency and <2.5-ns latency.","A 7.5-mW 10-Gb/s 16-QAM Wireline Transceiver with   Carrier Synchronization and Threshold Calibration for Mobile  Inter-chip Communications in 16-nm FinFET   Jieqiong Du†   University of California, Los  Angeles, USA  du.jieqiong@ucla.edu  Wei-Han Cho   University of California, Los  Angeles, USA  weihan.cho@ucla.edu  Po-Tsang Huang  National Chiao Tung University,  Taiwan            bug.ee91g@nctu.edu.tw  Special Session Paper  Chien-Heng Wong   University of California, Los  Angeles, USA  kenonearth@ucla.edu  Yilei Li   University of California, Los  Angeles, USA      ylli1986@ucla.edu  Sheau-Jiung Lee  TSVLink Corp, Santa Clara, USA  sjlee@tsvlink.com  Yo-Hao Tu   National Central University,  Taoyuan, Taiwan  100581002@cc.ncu.edu.tw  Yuan Du   University of California, Los  Angeles, USA       yuandu@ucla.edu  Mau-Chung Frank Chang   University of California, Los  Angeles, USA  mfchang@ee.ucla.edu  ABSTRACT  A compact energy-efficient 16-QAM wireline transceiver  with carrier synchronization and threshold calibration is  proposed to leverage high-density fine-pitch interconnects.  Utilizing frequency-division multiplexing, the transceiver  transfers four-bit data through one RF band to reduce intersymbol interferences. A forwarded clock is also transmitted  through the same interconnect with the data simultaneously  to enable  low-power PVT-insensitive symbol clock  recovery. A carrier synchronization algorithm is proposed to  overcome nontrivial current and phase mismatches by  including DC offset calibration and dedicated I/Q phase  adjustments. Along with this carrier synchronization, a  threshold calibration process is used for the transceiver to  tolerate channel and circuit variations. The transceiver  implemented in 16-nm FinFET occupies only 0.006-mm2  and achieves 10 Gb/s with 0.75-pJ/bit efficiency and <2.5ns latency.  Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  bear this notice and the full citation on the first page. Copyrights for  not made or distributed for profit or commercial advantage and that copies  components of this work owned by others than ACM must be honored.  Abstracting with credit is permitted. To copy otherwise, or republish, to  post on servers or to redistribute to lists, requires prior specific permission  and/or a  fee. Request permissions  NOCS '19, October 17–18, 2019, New York, NY, USA   © 2019 Association for Computing Machinery. ACM ISBN 978-1-45036700-4/19/10$15.00   from Permissions@acm.org.  https://doi.org/10.1145/3313231.3352381  CCS CONCEPTS  • Hardware  Interconnect   KEYWORDS  Frequency-division Multiplexing  (FDM), Quadratureamplitude Modulation (QAM), carrier synchronization,  threshold calibration, wireline, forwarded clock.   ACM Reference format:  Jieqiong Du, Chien-Heng Wong, Yo-Hao Tu, Wei-Han Cho, Yilei  Li, Yuan Du, Po-Tsang Huang, Sheau-Jiung Lee and Mau-Chung  Frank Chang. 2019. A 7.5-mW 10-Gb/s 16-QAM Wireline  Transceiver with Carrier Synchronization and Threshold  Calibration for Mobile Inter-chip Communications in 16-nm  FinFET. In International Symposium on Networks-on-Chip  (NOCS’19), October 17-18, 2019, New York, NY, USA. ACM,  NEW York, NY, USA, 8 pages. https://doi.org/10.1145/3313231.  3352381  1 Introduction  Energy-efficiency, bandwidth, area, and latency are key  design parameters for mobile inter-chip communications. To  support ever-demanding data communications, advanced  packaging technologies such as the integrated fan-out (InFO)  are introduced to provide high pin counts and high-density  integration [1]. To take full advantage of these packaging  technologies, compatible input-output (I/O) circuitries should  also be fast, compact, energy-efficient, and low-latency.       NOCS’19, October 17-18, 2019, New York, NY, USA  Jieqiong, et al.  data in time, FDM transceivers transfer multiple low-speed  data streams simultaneously through orthogonal frequencies,  avoiding high-speed power-hungry serializers  / deserializers. Double-sideband signaling used  in FDM  transceivers also self-equalizes the channel and further  enables high-speed communication without additional  equalization circuitries [2]. Requiring no high-speed  serializers/de-serializers or equalizers which are commonly  used in TDM transceivers and contribute to more energy  consumption and latency, FDM transceivers can be energy /  area-efficient with low latency. In [3], a tri-band 16-QAM  transceiver  achieved  0.95pJ/bit  energy-efficiency,  consuming only 0.01mm2 in 28nm CMOS area.   However, prior arts of FDM architecture faced  challenges of high-linearity requirement and high inter-band  interferences owing to the adoption of multiple frequency  bands [3,6]. In addition, two issues that were not handled in  previous works need to be resolved in coherent multi-level  modulated systems: 1) carrier synchronization and 2)  threshold calibration under channel and circuit variations.    To address the aforementioned issues, this paper  proposes a 16-QAM transceiver with carrier synchronization  and threshold calibration that consumes only a small area  and low energy. The FDM transceiver transfers data over  one RF band at 7.5 GHz with a symbol rate of 2.5 GHz. The  symbol clock is forwarded along with data through the same  channel and identical circuitry paths, which provides a  robust clock and data  tracking. Efficient carrier  synchronization and threshold calibration methods are also  introduced to improve the transceiver performance. With  fully differential current mode signaling, the transceiver  consumes 7.5mW at 10 Gb/s and occupies only 0.006 mm2  die area with a latency < 2.5ns.    2 System Architecture of the Transceiver  2.1 System Overview  As in Figure 1, the system architecture includes a transmitter  (TX) that performs 16-QAM modulation and clock-data  combining, a receiver (RX) that demodulates the data and  recovers the forwarded clock, and a carrier generation block  for TX/RX In-phase (I)/Quadrature-phase (Q) carrier  generation and distribution. A carrier-phase and comparatorthreshold controller adjusts receiver carrier phases and  comparator thresholds.   At the transmitter, data comes from either an PRBS  generator or a set of pre-defined symbol for calibration  purposes. The PRBS runs at 2.5 GHz at full speed. Two  current digital-to-analog converters map the data to 16QAM symbols. The baseband symbols are modulated to RF  Figure 1: Block diagram of the 16-QAM transceiver  DATA  CLK Forwarded  CLK SI/SQ ‘11’ ‘10’ ‘00’ ‘01’ Figure 2. Baseband symbols  (DAC  Output) Fowarded  clock @1.25GHz IH IZ IL y t i s n e D l a r t c e p S r e w o P DATA (DQ[3:0])  5GHz - 10GHz Frequency(GHz) Figure 3. TX output spectrum.      Frequency-division multiplexing wireline  transceivers  demonstrate great potential to satisfy the requirements,  according to recent publications [2,3]. Unlike conventional  time-division multiplexing (TDM) transceivers that serialize                        A 16-QAM Wireline Transceiver with Carrier Synchronization and Threshold Calibration  NOCS’19, Oct. 17-18, 2019, New York, NY, USA  band by two orthogonal I/Q carriers at 7.5 GHz. Along with  the modulated signals, a clock that toggles at one half the  symbol rate is transmitted through the same channel for  receiver clock recovery. Figure 2 shows the baseband  symbol timing. Figure 3. shows the transmitter output  spectrum.   At the receiving end, the combined signal is distributed to  three paths. At data path, transmitted data are demodulated  by I/Q mixers, low-pass filters, and comparators. An I/Q and  polarity swapping block is introduced so that the carrier  phase  interpolators only need 90º cover range for  synchronization. At clock path, the forwarded clock is  recovered by removing RF-band interferences using a lowpass filter.   This architecture offers several advantages. Data  transmission through RF frequency lowers the inter-symbol  interferences. By transmitting four-bit data simultaneously,  symbol rate can be reduced to ¼ of that of NRZ signaling,  which also relaxes the timing constraints for baseband signal  processing. Compared to prior arts of FDM transceiver,  having only one RF band also helps to reduce the peak-toaverage power ratio (PAPR) of the transmitted signal and  relax the requirement of linearity. Also, this compacts the  transceiver, reduces inter-band interferences level, and ease  the design of low-pass filters. Therefore, better area/energy  efficiency can be achieved than previous work. On the other  hand, by transmitting the clock along the data and using  identical circuitry for both the data and clock, the clock will  track the data without using a DLL.   2.2 Carrier Synchronization  For coherently modulated 16-QAM symbols, carrier phases  must be synchronized to recover the information. However,  carrier phase offsets change dramatically under channel and  circuit variations. A hardware-efficient single  tone  synchronization system using a comparator is designed in  Figure 4 [5]. During synchronization, only Q path is turned  on and a DC signal is applied to DAC. Optimal carrier phase  is found when either the output of I or Q path low-pass filter  becomes zero. Figure 5(a) shows an example for the optimal  phase sweep by constellation. By increasing the phase from  θ1 to θ2, I-amplitude crosses zero and by detecting the zerocrossing point through a comparator, the optimal phase can  be found. When reaching optimal phase, either II or Iq is 0  and the phase error reduces to k·90º(k=0,1,2,3). When k is  not 0, inverting and swapping the I/Q data outputs at the data  path can further compensates the phase error by shift data  phases by 180º and 90º/270º, respectively.   However, mismatches will degrade the effectiveness of  this algorithm by introducing significant DC offsets at RX  baseband and introducing I/Q carrier phase mismatch.  Figure 5(b) shows the effect in the system where zero  crossing shifts. To the first order, the phase error using the  algorithm when current offsets preset  is arcsine  (Ierr/Isig,max). When Ierr/Isig,max is larger than 0.1, this  phase error can be more than 5º. Mismatches in DACs,  mixers, low-pass filters, comparators, and clock feedthrough  all contribute to receiver output DC offsets. However, the  offset in the TX DAC is in/anti-phase with the transmitted  signal and the contribution will be zero at optimal phase.  Offsets from the receiver circuitry are more significant but  they are quasi-invariant. Therefore, receiver offsets can be  cancelled by a foreground calibration. Note that I/O buffer  mismatches do not contribute to output dc offset since they  are modulated to the RF frequency. On the other hand, the  carrier I/Q phase mismatch requires that the phase of I/Q  path must be calibrated separately.   During offset calibration shown in Figure 6, the  transmitter is turned off to remove transmitter mismatches  and the receiver carrier gen is turned on. In this way, I/O  buffer mismatches is removed, and offset resulting from  clock feedthrough are  included. The offset  is  then  compensated by tuning the comparator threshold until  sampled digital outputs are logic high for around 50% of the  time. The complete flow includes three steps: 1) receiver DC  offsets calibration; 2) Q path synchronization; and 3) I path  synchronization. I/Q carrier phases are calibrated separately  using the above zero-crossing detection method. To deal  with receiver I/Q phase mismatches, phase calibration is  performed to I and Q phases separately to minimizes I/Q  interferences in each path. The overall calibration flowchart  is shown in Figure 7.   The implementation of the algorithm employs two phase  interpolators (each covering 90º phase range), low-speed  DACs setting the comparator threshold, and a finite state  machine.   Figure. 4. Simplified system of single-tone carrier  synchronization.                  NOCS’19, October 17-18, 2019, New York, NY, USA  Jieqiong, et al.  2.3 Threshold Calibration  In addition to carrier synchronization, the comparator  thresholds must be calibrated to compensate circuit and  channel loss variations. A foreground calibration algorithm  using low-speed DACs is proposed here. Note that since  carrier phase offsets affect signal amplitude, it is essential  that carrier synchronization is performed before we calibrate  the comparator thresholds.   The algorithm is developed based on these factors: 1)  quasi-invariant signal amplitudes; 2) linear FDM system  operations; and 3) small inter-symbol interferences. The  threshold calibrations for I and Q paths are performed  separately.   When calibrating I path, Q path is turned off in TX to  remove possible I/Q interferences caused by residue carrier  phase errors after synchronization. On the other hand, four  signal levels (I3 > I1 > I-1 > I-3) are sent at TX I path each for  a period. During each phase, for example when I3 are sent,  the threshold IH of comparator for I3, RX and I1, RX level  detection is swept to find where sampled comparator output  is closest to be high for 50% of time. This threshold value is  recorded as IHH and expected to be equal to I3, RX. When I1 is  sent, the same threshold is tuned so that another value IHL  that matches I1, RX is found. The resulting threshold will be  the average of IHH + ILH.    Figure 5. Constellation of single-tone synchronization   Figure 6. Simplified offset cancellation schematics and  illustration.  Star t Turn Off TX;  turn on RX; turn on RX carr ier gen; set Comp_Re f_Zero = m in; record DIZ (DQZ) Comp_Re f_Zero ↑;  record DIZ (DQZ) N DIZ (DQZ)  number of ones  cross 50%?  Y Freeze Comp_Re f_Zero Q polarity  swap Y I polarity  swap Y N DQZ = 0 ?  DIZ = 0 ?  N IQ Swap Freeze Rx  Carrier Q  Phase N Freeze Rx  Carrier I  Phase Y DIZ  cross 50%?  Y N DIZ  / DQZ  number of ones  cross 50%?  Rx Carr ier  IQ Phase Delay ↑; record DIZ & DQZ   TX turn on Q path  and s end ‘11’; RX Carr ier IQ Phase  set to 0; record DIZ RX Current  DC Offsets Caliration Quadrature-phase path  Synchronization  TX turn off Q  path;  turn on I path  and s end ‘11’  TX turn off Q  path;  turn on I path  and s end ‘11’ Rx Carr ier  I Phase ↑; record DIZ  Rx Carr ier  Q Phase ↑; record DQZ  N DIZ  1s cross 50%?  DQZ 1s   cross 50%?  N Y Freeze Rx  Carrier I  Phase Y Freeze Rx  Carrier Q  Phase DQZ = 0 ?  N Y Q polarity  swap DIZ = 0 ?  N Y I polarity  swap Finish In-phase Path  Synchronization Figure 7. Carrier synchronization flow chart.                A 16-QAM Wireline Transceiver with Carrier Synchronization and Threshold Calibration  NOCS’19, Oct. 17-18, 2019, New York, NY, USA  3 Circuit Design of the Dual-band 16-QAM  Transceiver  The transceiver uses fully differential architecture to reduce  simultaneous switching noise, crosstalk, and even-order  nonlinearity. The transmitter and receiver are detailed as  follows.   3.1 Transmitter  Shown in Figure 8, the dual-band transmitter consists of  three parallel branches – two branches for the in-phase and  quadrature-phase data paths  that modulate baseband  symbols to RF band and one branch to forward the clock for  clock recovery at the receiver. Each branch comprises a 2bit thermometer-coded current-steering digital-to-analog  converter (DAC) to map 2-bit data into 4-level symbols and  a double-balanced mixer to upconvert baseband signal to RF  pass band. A summer follows the mixers, sums the current  signals from all three branches, and drives a pair of  differential channels. This transmitter transmits a total of  four bits data in addition to clock simultaneously through  one common electrical lane within one unit interval (400ps  for a total data rate of 10 Gb/s). In addition, to accommodate  different channel losses, the transmitter output power can be  adjusted by programming the reference current level of the  DAC.       To simplify the clock recovery at the receiver, the clock  path includes a dummy mixer to track the time delay of data  path although no frequency mixing is performed. The clock  runs at one half data rate and lags the data by half a data  period so that the transitions of clock will fall around the  optimum sampling timing for data recovery at receiver.  Because the clock propagates along with the data through the  same physical interconnect, clock signal tracks data signal in  spite of chip or interconnect variation. Therefore, this allows  self-tracking of the clock without requiring a delay-lock loop  or other de-skew circuit in most source synchronous  systems.  3.2 Receiver  At the receiving end, a current amplifier amplifies received  current signal and provides 100-Ω wideband differential  channel termination to reduce reflection. The amplifier then  distributes the received current signal to 3 parallel signal  paths. Each is comprised of a double-balanced mixer to  down-convert RF signal to baseband, a 3-order low-pass  filter to remove adjacent band interference, three parallel  continuous-time comparators to decode the received symbol,  and a decoder to recover the transmitted data.   The current amplifier employs gain-reused regulated  cascode structure with active inductor shunt peaking to  improve energy-efficiency, as Figure 9 shows. A first order  analysis of the circuit reveals that the input differential  impedance of the current amplifier is dictated by 2/gm12/gm2 – the difference between the NMOS and PMOS  transconductance. Therefore,  the differential  input  impedance can be reduced to 100-Ω without burning too  much current at the input stage. On the other hand, the active  inductor shunt peaking helps to improve the bandwidth of  the current amplifier. However, the input impedance is PVT  sensitive, which could result in reflections if the input  impedance deviates too much from 100-Ω and would  compromise signal integrity.  Since the impedance is inversely proportional to transistor  transconductance,  the  impedance  is also  inversely  proportional to the square root of the Ibias. To compensate the  PVT variation of the transistor, the bias current of the input  current amplifier can be programmed through a low-speed  current DAC. Thirty-percent variability of reference current  enables a tuning range of about fifteen percent for the input  impedance. Consuming around 1mA current, the current  amplifier provides less than -10 dB return loss from DC to  10 GHz.       A double-balanced mixer follows the current amplifier  and down-converts the RF signal to baseband. Doublebalanced structure is used at both the transmitter and receiver  to reduce LO leakage. Similar to the transmitter, a dummy  mixer also presents at the receiver clock path to match the  time delay of the RF data paths.        Following the double-balanced mixer, a third-order  Bessel Gm-C low-pass filter removes high-frequency  interferences resulting from adjacent band and frequency  mixing. Bessel type filters are adopted because they provide  relatively constant group delay within band-of-interest to  minimize signal distortion and inter-symbol interferences.  The third-order Bessel Gm-C low-pass filter has a cut-off  frequency at 1.5GHz. Figure 10 shows the simulated eye  diagrams at low-pass filter output.  The baseband symbol restored by the low-pass filter is  then converted to digital information through three parallel  continuous-time comparator. The  thresholds of  the  comparators are set by three different reference currents  which can be programmed to deal with different channel  attenuations. A decoder then converts the 3-bit signal into 2bit data, which is later sampled at the clock transition edges.  3.3 Carrier Generation  The carrier generation uses current mode logic and provides  TX/RX I/Q carriers by dividing an external RF source. Two  6-bits phase interpolators provide tunable phase delay for  RX I/Q synchronization with 0.8ps (~2º) resolution at 7.5  GHz.         NOCS’19, October 17-18, 2019, New York, NY, USA  Jieqiong, et al.  4 Measurement Results  A test chip comprising a carrier generation block, a digital  baseband controller, and four-lane transceiver front-ends is  fabricated in TSMC 16nm FinFET; the active area per lane  is only 0.006 µm2. Figure 11 shows the microphotograph of  fabricated chip and Figure 12 is the testing environment. The  fabricated chip is wire-bonded to PCB for characterization  and is tested with 231-1 PRBS data under two fine-pitch  channel conditions – 1-inch and 5-inch FR-4 PCB  differential traces (3-mil width and 3-mil spacing).  Figure 8. Schematics of transmitter.  Test Chip UART Mode PC Test Chip Bias UART Gen . DQ[3:0] CLK RX TX Scope Forwarded CLK PRBS DQ[3:0] TX RX 2.5GHz Figure 12. Experiment Platform.  15 GHz  Figure 9. Schematics of the current amplifier.  Figure 10. simulated eye diagram at RX LPF.  Figure 13. 10-Gb/s 231-1 PRBS eye diagram before (left)  and after (right) carrier synchronization and threshold  calibration  Figure 14. Left: 4-Gb/s 231-1 PRBS eye diagram of clock  and Q channel. Right: transmitted and received clock  (upper); PRBS data from TX generator (middle);  demodulated data at receiver (lower).  Figure 11. Micrograph of the test chip.                                                    A 16-QAM Wireline Transceiver with Carrier Synchronization and Threshold Calibration  NOCS’19, Oct. 17-18, 2019, New York, NY, USA  TABLE I  Metric  ISSCC’15  [6]  Comparison with Prior Art  CICC’15  [2]  ISSCC’1 6 [3]  Architecture  TDM  FDM  FDM  65nm  0.7 V  FR-4        1.5-inch  40nm  0.9V  FR-4         2-inch  28nm  1.2 V  FR-4       2-inch  ISSCC’17    [8]  N/A  14nm  N/A  EMIB  1.1mm  This Work  FDM  16nm  1 V  FR-4       1-inch  FR-4       5-inch  Tech  Supply  Channel Type  Data Rate Per   Lane  Power  Energy  Efficiency  Area  6 Gb/s  4 Gb/s  10 Gb/s  2 Gb/s  10 Gb/s  4 Gb/s  3.4 mW  5.4 mW  9.5 mW  2.4 mW  7.5 mW  5.3 mW  0.58 pJ/bit  1.35 pJ/bit  0.95 pJ/bit  1.2 pJ/bit  0.75 pJ/bit  1.3 pJ/bit  0.15 mm2  0.008 mm2  0.01 mm2  0.0013 mm2  0.006 mm2      The transceiver can operate up to 10 Gb/s with 1-inch FR4 traces and up to 4 Gb/s with the 5-inch FR-4 traces while  consuming 7.5 mW and 5.3 mW, respectively. Without  calibration, the demodulated eye diagram is closed as Figure  13 shows. After both carrier synchronization and threshold  calibration, the output eye diagram opens, and transceivers  achieved BER<10-12 at 4Gb/s and BER<10-8 at 10 Gb/s. The  test results also show that the data path latency is tracked by  the clock path; clock transitions still track the center of data  eye after demodulation, as is shown in Figure 14. In addition,  the measured latency of transceiver is less than 2.5 ns  (including the latency on the 5-inch FR-4 PCB trace). The  latency is measured by the time delay between outputs of the  PRBS generator and the receiver output data, as shown in  Figure 14.  CONCLUSION  In summary, this paper presents an area-compact and  energy-efficient 16-QAM FDM wireline transceiver. The  demonstrated  transceiver can  transfer multi-bit data  simultaneously over an RF band with minimum intersymbol interferences and concurrently forward data symbol  clocks over the baseband via the same physical channel for  effective  symbol  clock  recovery. Unique  carrier  synchronization and threshold calibration methods are also  developed and verified to mitigate channel and circuit loss  variations. The realized transceiver has achieved 0.75-pJ/bit  transmission efficiency at 10 Gb/s with < 2.5 ns latency over  1-inch differential FR-4 traces. It has also achieved 4-Gb/s  data rate over 5-inch differential FR-4 traces. It consumes  only 0.006 mm2 die area per lane.   ACKNOWLEDGMENTS  The authors would like to thank TSMC for chip fabrication.   "
UBERNoC - unified buffer power-efficient router for network-on-chip.,"Networks-on-Chip (NoCs) address many shortcomings of traditional interconnects. However, they consume a considerable portion of a chip's total power - particularly when the utilization is low. As transistor size continues to shrink, we expect NoCs to contribute even more, especially static power. A wide range of prior-art focuses on reducing the contribution of NoC power consumption. These can be categorized into two main groups: (1) power-gating, and (2) simplified router microarchitectures. Maintaining the performance and the flexibility of the network are key challenges that have not yet been addressed by these two groups of low-power architectures. In this paper, we propose UBERNoC, a simplified router microarchitecture, which reduces underutilized buffer space by leveraging an observation that for most switch traversals, only a single packet is present. We use a unified buffer with multiple virtual channels shared amongst the input ports to reduce both power and area. The empirical results demonstrate that compared to a conventional router, UBERNoC achieves 58% and 69% reduction in power and area respectively, with negligible latency overhead.","UBERNoC: Unified Buffer Power-Efficient Router for Network-on-Chip Hossein Farrokhbakht University of Toronto h.farrokhbakht@mail.utoronto.ca Henry Kao University of Toronto h.kao@mail.utoronto.ca Natalie Enright Jerger University of Toronto enright@ece.utoronto.ca ABSTRACT Networks-on-Chip (NoCs) address many shortcomings of traditional interconnects. However, they consume a considerable portion of a chip’s total power – particularly when the utilization is low. As transistor size continues to shrink, we expect NoCs to contribute even more, especially static power. A wide range of prior-art focuses on reducing the contribution of NoC power consumption. These can be categorized into two main groups: (1) power-gating, and (2) simplified router microarchitectures. Maintaining the performance and the flexibility of the network are key challenges that have not yet been addressed by these two groups of low-power architectures. In this paper, we propose UBERNoC, a simplified router microarchitecture, which reduces underutilized buffer space by leveraging an observation that for most switch traversals, only a single packet is present. We use a unified buffer with multiple virtual channels shared amongst the input ports to reduce both power and area. The empirical results demonstrate that compared to a conventional router, UBERNoC achieves 58% and 69% reduction in power and area respectively, with negligible latency overhead. KEYWORDS Network-on-Chip, Static Power, Concurrent Traversal ACM Reference Format: Hossein Farrokhbakht, Henry Kao, and Natalie Enright Jerger. 2019. UBERNoC: Unified Buffer Power-Efficient Router for Network-on-Chip. In International Symposium on Networks-on-Chip (NOCS ’19), October 17–18, 2019, New York, NY, USA. ACM, New York, NY, USA, 8 pages. https://doi.org/10. 1145/3313231.3352362 1 INTRODUCTION The Network-on-Chip (NoC) is a key component in chip multiprocessors (CMPs) as it supports communication between many nodes. As core counts increase, the need for high-performance and efficient NoCs becomes increasingly important. The NoC routers consume a considerable portion of the chip’s power [15]. For instance, SCORPIO’s 36-core NoC [5] contributes 19% to tile power and 11% to chip area. Large routers are provisioned to tolerate peak loads, but typically go underutilized for most traffic and consume power even when idle. Also, as transistor size continues to shrink, the Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’19, October 17–18, 2019, New York, NY, USA © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6700-4/19/10. . . $15.00 https://doi.org/10.1145/3313231.3352362 contribution of NoC’s static power is projected to increase [4, 30]. Power analysis for the 32nm technology node demonstrates that static power consumption increases by 3× compared to the same design in a 65nm technology [29]. Both the inefficient use of router resources and further transistor size scaling may push routers to have greater static power consumption in future designs. Existing techniques to reduce static power consumption of NoC routers leverage the fact that utilization of NoC resources is low in typical applications. These solutions can be categorized into two major techniques: (1) power-gating and (2) simplified microarchitectures. Power-gating is a well-known technique for reducing static power consumption in a wide range of domains such as processor cores [21], functional units [31, 32], and NoC [4, 10, 12]. Powergating saves static power by disconnecting an entire block from the power supply when idle, and powering it on when needed [17]. The powering-on and off processes impose non-negligible overheads. Increased frequency of these processes at high network loads results in additional power and latency overheads if not used efficiently. While recent research alleviates these issues [3, 4, 10–12], these methods only save power at low injection rates. On the other hand, simplified microarchitectures either minimize or eliminate router structures [8, 13, 19, 24, 25, 33]. Existing work targets buffers and virtual channels (VCs) as they contribute a considerable percentage to both NoC power and area [13, 19, 33]. These techniques are inherently throughput limited due to the reduction in buffer depth. Most power-gating and simplified microarchitectures sacrifice flexibility in routing algorithms, specifically the ability to support adaptive routing [1, 4, 10, 11, 13]. Additionally, those techniques that do support different routing algorithms impose additional overheads [19]. We make the key observation that the contribution of single packet traversals, i.e., only a single packet is in the switch traversal stage, is dominant. In other words, router resources which enable concurrent switch traversals, namely one buffer per input port and crossbar, are being underutilized. By reducing the total amount of buffer space, we can achieve sizeable power and area savings. To this end, we propose the Unified Buffer Power-Efficient Router for Network-on-Chip (UBERNoC), a novel router microarchitecture which reduces the complexity of both the input buffers and crossbar switch. We reduce the number of input buffers by unifying them into a single buffer with multiple virtual channels shared between ports. In turn, the crossbar complexity is also reduced. We also propose an alternative UBERNoC architecture, UBERNoC2, to provide greater throughput at high injection rates by trading off power and area over the single buffer design. This alternative design utilizes two buffers which are shared between two ports each. To summarize, our main contributions are: • We observe that the situation of a single packet in the switch traversal stage is dominant–even at high injection rates. NOCS ’19, October 17–18, 2019, New York, NY, USA Hossein Farrokhbakht, Henry Kao, and Natalie Enright Jerger (a) (b) (c) Figure 1: Percent of concurrent switch traversals for (a) Bit-Complement, (b) Transpose, and (c) Uniform traffic. • We observe that total buffer space equivalent to a single input buffer, which consists of multiple virtual channels, can handle most network traffic loads. • We propose UBERNoC, a power-efficient simplified microarchitecture that reduces both buffer space and crossbar complexity to save power and area. • We propose UBERNoC2, a high-throughput simplified microarchitecture with two shared buffers that also saves power and area. Compared to a conventional router, UBERNoC router achieves 58% and 69% reduction in power and area without degrading the average packet latency of SPLASH-2 benchmarks [34]. 2 MOTIVATION A conventional NoC router is comprised of input port buffers, a crossbar, switch and virtual channel allocators. Input buffers contain multiple VCs to increase throughput and prevent deadlock (network and protocol level). The structure of the input buffers and crossbar in a conventional router is optimized for packet latency and throughput, allowing for multiple packets to enter the switch traversal stage concurrently. However, we observe that the frequency of single switch traversals is more than multiple traversals (e.g. two concurrent switch traversals) and that existing router resources are frequently underutilized. Fig. 1 shows a breakdown for the number of concurrent packets in the switch traversal stage for synthetic traffic. These results are collected for an 8×8 mesh using XY routing with conventional VC routers consisting of three pipeline stages and three VCs per input buffer (each VC is four flits deep). One traversal means only a single packet is in the switch traversal stage whereas four traversals means that four packets are performing switch traversal simultaneously. The fraction of multiple traversals increases as injection rate increases; however at the saturation point, single packet traversals still make up a considerable percentage of the distribution: 50%, 85.2%, and 49.9% for Bit-Complement, Transpose, and Uniform. SPLASH-2 benchmarks show even fewer concurrent switch traversals, on average 98.8% and 1.2% for single and two packet traversals. The contribution of three and four switch traversals are much lower than that of single packet traversals. Take four concurrent Figure 2: Percent of total buffer space used excluding injection queue. switch traversals as an example. This situation is exceedingly rare, even at the saturation point, because not only do all these packets need to be straight (e.g. North-to-South traversal), or three straight and one ejecting packet, all of them must be queued for switch traversal at the same time. In addition, all of the downstream routers must have free buffer space otherwise the packets would stall. Note that these packets have to be straight for four concurrent switch traversals, otherwise there would be output port contention with a turning packet, limiting concurrent switch traversals to a maximum of three packets. Furthermore, these situations can only happen in central routers of the mesh. Boundary routers, which make up nearly half of all the routers in an 8×8 mesh, can only see a maximum of three concurrent traversals comprised of two in-flight and one ejecting packet, the occurrence of which is uncommon. We ignore injection packets in this analysis because our proposed design targets the main router microarchitecture. Modifications to injection queue may have upstream consequences (e.g., network interface) which is orthogonal to our work. At low injection rates, the switch contains only a single packet. Even in the worst case (i.e., the saturation point), the contributions of single switch traversals and two switch traversals are dominant. The conventional crossbar’s ability to transmit multiple packets concurrently is not being efficiently used. We analyze average buffer UBERNoC: Unified Buffer Power-Efficient Router for Network-on-Chip NOCS ’19, October 17–18, 2019, New York, NY, USA Figure 3: Impact of virtual channel depth (in flits) on saturation point and latency in a conventional NoC router. utilization for synthetic traffic in Fig. 2. Buffer utilization is calculated as the average percentage of total buffer space used for a conventional router where each input buffer contains a total 12 flit-sized slots (3 VCs with 4 slots per VC), or 48 flit-sized slots per router (excluding the injection port buffer). The average buffer space used is less than 25% (the size of a single input buffer) for most of the injection rates, up to 0.17 in Bit-Complement and 0.26 in Uniform. Within this region of operation, total space equivalent to a single input buffer is sufficient to handle all packets without incurring additional latency due to contention of buffer space. Single packet switch traversals and low buffer utilization inspire us to consider an alternative, cheaper router implementation. We consolidate this underutilized buffer space to save both power and area by reducing the number of buffers to one shared unified buffer.1 An alternative approach might be to reduce the buffer depth instead. However, reducing depth causes additional contention and throughput degradation as shown in Fig. 3. Smaller buffer depths result in lower saturation points. Although reducing both the number of buffers (unifying) and buffer depth effectively minimize total buffer space, our observations suggest that only a single input port’s buffer is being efficiently utilized. Reducing depth would increase contention for that buffer and hence decrease throughput. Consolidating buffer space into a unified single buffer has less throughput impact since only the underutilized ports are being penalized. Thus we keep the depth of the single unified buffer the same as a single buffer in the conventional router. 3 PROPOSED TECHNIQUE: UBERNoC Based on our observations, the probability that routers see multiple simultaneous switch traversals is low. Packets do not take full advantage of the multiple concurrent traversals allowed by a conventional crossbar; it is unnecessary to dedicate one buffer per input port. We modify the router microarchitecture to remove extraneous capabilities by serializing concurrent traversals to single 1 This buffer has the same number of virtual channels as one input buffer in the conventional NoC router. (a) (b) Figure 4: UBERNoC router microarchitecture (a) and its switch crossbar (b). traversals to reduce both static power consumption and area. In our microarchitecture, we reduce total buffer space by sharing a single unified buffer between all ports (excluding the injection port). Although our proposed design simplifies the router microarchitecture, it does not hinder other capabilities such as support for different routing algorithms, scalability, or throughput. UBERNoC requires an arbitration scheme at the input ports. This arbitration imposes latency when there are multiple routers trying to send packets. We tackle this issue by proposing an alternative microarchitecture, UBERNoC2, with two shared buffers, each shared between two ports, which supports two concurrent switch traversals. 3.1 UBERNoC Microarchitecture The UBERNoC microarchitecture (Fig. 4) simplifies the conventional router buffer (Fig. 4a) and crossbar switch (Fig. 4b) by unifying the input buffers for the four directions (North, East, South, and West) into a single buffer. We keep the injection buffer separate from the unified buffer to prevent injected packets from stalling due to contention. An additional multiplexer (Minput ) allocates packets at the input ports into the unified input buffer. Arbitration is needed at Minput since all ports share the same buffer; request signals are added to choose which port gets access (Sec. 3.2). The depth of the UBERNoC input buffer is the same as a single conventional input buffer to prevent additional contention of buffer space as NOCS ’19, October 17–18, 2019, New York, NY, USA Hossein Farrokhbakht, Henry Kao, and Natalie Enright Jerger adding complexity elsewhere. We address this issue by proposing an alternative design with two shared input buffers (Sec. 3.4). 3.3 Routing Examples Fig. 6 reflects possible traffic scenarios in a 3×3 mesh network using UBERNoC, assuming packets are a single flit for simplicity. Since the UBERNoC router uses just a single unified buffer, it serializes packets (excluding injection packets) before switch traversal; it does not support concurrent switch traversals for multiple in-flight packets as in the baseline router. Latency can be incurred in UBERNoC when multiple packets seek to traverse the same switch at the same time. However, as shown in Sec. 2, single packet switch traversals are the most frequently observed and serialization has negligible impact on the latency. A common case is shown in Fig. 6a, with no contention in the network. Since there are no concurrent switch traversals, the latency is the same as the conventional router. Fig. 6b demonstrates two straight packets traversing R 11 . This is not an issue for conventional routers since both packets reach the switch traversal stage at the same time and there is no downstream contention. On the other hand, UBERNoC orders the two incoming packets into the unified buffer and each packet goes through switch traversal sequentially, which incurs an extra latency to complete both traversals. The cases in Fig. 6c and 6d demonstrate two packets accessing the same downstream link. Both the conventional router and UBERNoC incur the same latency for these cases. However, the latency is incurred due to different reasons. The conventional router experiences this latency due to the contention of the same output port. Arbitration in switch allocation gives access to the output port to one packet at a time. UBERNoC imposes latency when allocating packets to the unified buffer using the same arbitration scheme; however, it effectively removes contention for the output ports. Fig. 6e illustrates two turn packets arriving at R 11 simultaneously. Similar to the case in Fig. 6b where there is no downstream contention, UBERNoC incurs a single cycle latency over the conventional router due to serializing both packets during switch traversal. When three packets arrive at switch traversal with no downstream resource contention (Fig. 6f), UBERNoC will need an extra two cycles over the conventional router to complete all switch traversals. The worst case for UBERNoC is shown in Fig. 6g where four packets arrive at R 11 ’s switch at the same time. The conventional router stores each packet in its respective input buffer (as shown in Fig. 7a) and packets execute all stages of the pipeline concurrently (Fig. 8a) in four cycles. However, UBERNoC orders the packets upon allocation into its unified input buffer as shown in Fig. 7b. Each packet enters the pipeline stages one at a time (Fig. 8b). In this case UBERNoC takes seven cycles to complete traversals for all packets – three extra cycles over the conventional router. As mentioned in Sec. 2, this scenario is a rare occurrence, hence these extra two to three cycles of latency are rarely incurred. Note that these examples illustrate routing in-flight packets within the network. UBERNoC still allows concurrent traversal of an injection packet and an in-flight packet as long as they do not contend for the same downstream resources. In these cases with injection packets, UBERNoC will perform same as the conventional router. Figure 5: Timeline of arbitration and flow control for routing case in Fig. 6b. explained in Sec. 2. We also do not change the number of VCs to prevent additional congestion, to maintain deadlock freedom, and to support adaptive routing algorithms. The unified input buffer simplifies the UBERNoC crossbar switch. The proposed crossbar in Fig. 4 takes a single input from the unified buffer (Bin ) which reduces multiplexer size from 5-to-1 (four directions and inject) down to 2-to-1 (Bin and inject). 3.2 Arbitration and Flow Control The input port multiplexer (Minput ) uses round-robin arbitration when multiple upstream routers request access to the unified buffers. Requests to send packets to downstream routers are generated at the route computation stage. Buffer backpressure uses a creditbased system at packet granularity. Fig. 5 provides an arbitration and virtual cut-through flow control example for the routing case shown in Fig. 6b. Both routers R10 and R21 request access to R11 ’s unified buffer at time t0 . Arbitration initially gives access to R21 . R11 transmits packet X to its downstream router, and at time t1 enough space is vacated for an entire packet, which results in credit being sent to R21 . After some processing time, R21 transmits its packet A to R11 . Router R11 can send a request to forward packet A to its downstream router once the head flit (Ah e ad ) arrives. It can forward packet A even though the tail flit has not been received. At time t2 the tail flit (At a i l ) arrives at R11 and access to the unified buffer is given to R10 . A credit is sent to R10 given that there is enough space for a new packet. Router R10 can then transmit packet B to R11 after receiving and processing the credit. UBERNoC currently does not support wormhole flow control as flits from R10 and R21 may become interleaved when arbitration and backpressure are at flit granularity. This arbitration scheme may underutilize VCs. In the example above, suppose R10 wants to send packet B to VC1 and R21 wants to send packet A to VC2 . Router R10 has to wait for R11 to receive At a i l from R21 in VC2 before it can transmit – even though VC1 may be empty. The serialization of buffer access incurs latency since VCs are used inefficiently. The ideal solution for this scenario is for R10 and R21 to send their packets simultaneous to fully utilize the VCs (as can be done in a conventional router). This issue can be addressed with more complex arbitration or an additional crossbar at the input stage; however, our goal is to simplify the router without UBERNoC: Unified Buffer Power-Efficient Router for Network-on-Chip NOCS ’19, October 17–18, 2019, New York, NY, USA (a) (b) (c) (d) (e) (f ) (g) Figure 6: Different routing scenarios in UBERNoC: (a) single packet traversal with no additional latency, (b) two straight packet traversals which imposes additional latency in UBERNoC, (c,d) two packet traversals which imposes latency in both the conventional NoC router and UBERNoC, (e) two turn packet traversals which imposes additional latency in UBERNoC (f ) three packet traversal, (g) four packet traversal – inducing worst case latency. (a) (b) Figure 7: Buffer allocation for packets in routing example Fig. 6g for the (a) conventional router and (b) UBERNoC. (a) (b) Figure 8: Four-stage pipeline diagrams for routing packets in example Fig. 6g for the (a) conventional router in four cycles and (b) UBERNoC in seven cycles. 3.4 Alternative Design: UBERNoC2 UBERNoC, which uses a single unified buffer, may incur additional latency in three ways: (1) reduction of total buffer space causing additional contention; (2) arbitration of multiple requests resulting in inefficient use of VCs; and (3) the serialization of switch traversals. To mitigate these issues, we propose an alternative UBERNoC microarchitecture, UBERNoC2, which contains two unified buffers, each shared between two ports (Fig. 9). Input ports North and East share one unified buffer, and ports West and South share the other. We choose this input port pairing because most packets are straight traversals in XY routing [12] and better utilize this configuration. Both unified buffers have the same depth and same number of VCs as a single buffer in the conventional router. The crossbar has minimal added complexity over single buffer UBERNoC, containing 3-to-1 (B1 , B2 and inject) instead of 2-to-1 (Bi n and inject) multiplexers. UBERNoC2 solves throughput issues in UBERNoC by: (1) Figure 9: UBERNoC2 router microarchitecture with two shared input buffers. Doubling total buffer size (excluding injection port) to reduce contention for buffer space. (2) Arbitration for buffer access serializes at most two requests compared to four. Two requesters for two different unified buffers (e.g. Ni n and Si n ) can access their respective input buffers simultaneously, improving both VC utilization and latency. (3) Two concurrent switch traversals are supported for certain packets (e.g., North-to-South and South-to-North). 4 EVALUATION We implement UBERNoC in Booksim [16] and run traces gathered from SPLASH-2 benchmarks [34]. The traces are generated using gem5 [2] in syscall emulation mode, each consisting of traffic over 10 million cycles after thread creation and initialization. We use an 8×8 2D mesh, with each router attached to a single processor core for 64 cores total. Each core consists of a private L1 cache and a slice of physically distributed, logically shared L2 cache. Cache coherence is maintained via a MESI directory protocol. We compare UBERNoC to the state-of-the-art simplified router microarchitecture and power-gating techniques: (1) ConvNoC: A conventional NoC router which contains multiple virtual channels per input port. (2) Low-Cost: A simplified router microarchitecture [19]. (3) SPONGE: Highly-efficient state-of-the-art power-gating technique [10]. To evaluate SPONGE, we set wakeup latency and break-even time2 to 8 cycles and 10 cycles based on prior work [22]. Low-Cost router has up to a 2-stage router pipeline since the intermediate 2 Break-Even Time (BET) is the minimum number of consecutive cycles that a power gated router must be in a powered-off state to compensate for wake-up overhead. NOCS ’19, October 17–18, 2019, New York, NY, USA Hossein Farrokhbakht, Henry Kao, and Natalie Enright Jerger Table 1: Simulation Parameters. Core L1 Cache L2 Cache Cache Coherence Memory Topology Link Bandwidth Flow Control Virtual Channel VC Depth Routing Alg. 64 cores, x86 ISA, 2GHz, out-of-order, 8-wide issue private, 16KB Ins. + 16KB Data, 4-way set, 3-cycle latency shared, distributed, 256KB/node, 16-way set, 15-cycle latency MESI, CMP directory, 64-byte block 1GB/controller, 1 controller at each mesh corner 8x8 mesh 128 bits/cycle Wormhole (ConvNoC, SPONGE, Low-Cost) VCT (UBERNoC) 3-VN, 1 VC/VN 4-flit/VC (ConvNoC, SPONGE, UBERNoC) 2-flit/VC and 4-flit for intermediate buffer (Low-Cost) XY and Adaptive (Duato) Figure 10: Average packet latency of SPLASH-2 benchmarks. buffer adds an extra cycle to turn packets and some of the injection and ejection packets. We implement Low-Cost with three VCs per buffer to maintain deadlock freedom even though the original work uses neither VCs nor multiple physical channels. The addition of VCs requires an extra virtual channel allocation stage; however, we assume this logic fits into the original pipeline. The rest of the methods have three pipeline stages and three virtual networks. We evaluate UBERNoC with both XY and adaptive routing [7]. We also evaluate UBERNoC across synthetic workloads with a bimodal distribution of short 1-flit and long 4-flit packets. The simulation has a 30,000 cycle warm-up phase, after which we collect network statistics for one million cycles. Table 1 lists all configuration parameters. We use DSENT [30] to model static power and area for a 45 nm process. 4.1 Performance, Power, and Area Fig. 10 shows average packet latency of UBERNoC and UBERNoC2 compared to the aforementioned techniques. The number of single packet switch traversals dominates for SPLASH-2, making up on average of 98.8% of the switch traversals. UBERNoC is designed to exploit this type of traffic. Both UBERNoC and UBERNoC2 add negligible packet latency over ConvNoC, on average 4% and 0.2%. Low-Cost reduces packet latency by 35% on average over ConvNoC due to the shorter one to two stage pipeline. We did not add an extra virtual channel allocation stage to their pipeline even though we added VCs to prevent protocol-level deadlock. SPONGE reduces average packet latency compared to ConvNoC by 18.6% due to a single-cycle bypass path through power-gated routers. Packets execute turns only on the powered-on columns of routers. The number of power-on router columns is determined by traffic load; in the case of SPLASH-2, it’s a single column. Fig. 11 shows static power and area normalized to ConvNoC. Compared to ConvNoC, UBERNoC improves power and area by 58% and 69%. UBERNoC2 implements an extra input buffer over Figure 11: Static power and area for different schemes. UBERNoC; however, it still improves power and area by 39% and 54%. Compared to Low-Cost, both UBERNoC routers offer more savings due to reduced total buffer space. SPONGE improves static power by 26% through efficient management of power-gating idle routers; however it incurs an additional 34% area overhead over ConvNoC due to extra hardware needed to support power-gating on top of the conventional architecture. 4.2 Synthetic Traffic Evaluation Fig. 12 shows throughput for various synthetic traffic patterns. At low injections rates, both Low-Cost and SPONGE offer lower packet latency compared to other techniques due to the minimal pipeline stages in Low-Cost and bypass paths in SPONGE. At these injection rates, the majority of switch traversals are single traversals. Both UBERNoC and UBERNoC2 achieve the same latency as ConvNoC. The fluctuations in SPONGE’s packet latency as load increases are due to its power-gating strategy. Packets are only able to turn in powered-on columns. Congestion builds in these columns as traffic load increases, at which point another column powers on to alleviate the congestion causing peaks in packet latency. As network load increases, traffic patterns Bit Complement, Tornado and Uniform see increased amounts of concurrent switch traversals (Sec. 2). Single buffer UBERNoC saturates earlier compared to the other techniques due to serialization from arbitration at the input port (Sec. 3.2) and serialization of switch traversals (Sec. 3.3). However in Bit-Reverse, Shuffle and Transpose, UBERNoC still outperforms Low-Cost in throughput since the number of single switch traversal is dominant, even at the saturation point. UBERNoC2 outperforms the other simplified microarchitectures for high injection rates. The two shared buffers allows for two concurrent input buffer writes and switch traversals, lessening latency overheads seen in single buffer UBERNoC. UBERNoC2 offers similar throughput as ConvNoC in Bit Reverse, Shuffle and Transpose. 4.3 Sensitivity to Routing Algorithms Fig. 13 shows average packet latency for different techniques. We exclude SPONGE from this analysis because it only supports deterministic routing. Adaptive routing can cause routing deadlock so we designate one of the three VCs as an Escape VC [7] to maintain deadlock freedom. The number of turn packets increases in adaptive routing and is reflected in the latency degradation experienced by Low-Cost due to the extra pipeline stage turn packets must traverse. The number of packets arriving at a router simultaneously is lower in adaptive routing. UBERNoC experiences a slight increase in saturation point due to the decrease in arbitration requests at the input stage. UBERNoC2 shows a minor improvement in packet latency at high injection rates. We also evaluate both UBERNoC routers on SPLASH-2 benchmarks using adaptive routing and observe no difference compared to XY routing. Average packet latency UBERNoC: Unified Buffer Power-Efficient Router for Network-on-Chip NOCS ’19, October 17–18, 2019, New York, NY, USA (a) Bit Complement (b) Bit Reverse (c) Shuff le (d) Tornado Figure 12: Latency and throughput for synthetic traffic patterns. (e) Transpose (f ) Uniform due to a bypass ring (NoRD) and wake-up overheads for turn packets (TooT). PowerPunch [4] reduces the delay overhead of the wakeup process through an early wake-up mechanism. SMART [11] makes power-gating more efficient by reducing wake-up overheads through an application-specific mapping and routing technique. To avoid latency penalties of wake-ups, SPONGE [10] keeps a small-set of routers in powered-on state, although, large networks see high packet latency since turn packets are detoured to the power-on routers. Muffin [9] bypasses all packet types when traffic is low. However, it adds additional buffers, imposing area and power. Most of the above methods cannot handle adaptive routing efficiently. Adaptive routing is critical to enable high throughput during periods of contention. 5.2 Simplified Router Microarchitectures Recent work to improve buffer utilization explores the use of shared buffers. The Centralized Buffer Router [13] consolidates buffers for each port into a buffer shared between all ports. Similar concepts such as RoShaQ [33] and DSB [28] also propose shared buffer architectures; however, these methods require small buffers for each port to reduce packet latency. An additional crossbar at the input stage is needed to allocate packets to the shared buffer slots. Although some of these techniques route through a fast bypass path at low loads, at high loads, packets traverse through two crossbars and are buffered twice. ViChaR [26] creates a unified buffer for VCs, dynamically adapting to a single deep VC or separate shallow VCs depending on traffic. The challenge with this design is power and area overheads from the complex control logic. Other techniques involves simplifying the switch. Both RoCo [20] and Low-Cost [19] segment the crossbar to handle X and Y traffic separately. Switch Folding [6] applies time-multiplexing to a router’s Figure 13: Latency and throughput for uniform traffic using adaptive routing. in Low-Cost using adaptive routing over XY routing increases by 15% for the SPLASH-2 benchmarks. 5 RELATED WORK We classify prior work into two different groups: (1) power-gating techniques which are aimed at reducing static power consumption by powering off the entire router or certain network submodules, (2) simplified router microarchitectures which reduce both power and area of the routers by minimizing or eliminating the number of buffers and simplifying the router’s crossbar. 5.1 Power-gating Techniques We focus on coarse-grained power gating techniques as these are most relevant to our proposed scheme. However, fine-grained power-gating techniques have been proposed to reduce the power consumption of sub-modules within a router [18, 22, 23, 27]. Router Parking [29] selectively power-gates routers connected to poweredoff cores; delay is mitigated through detouring. Centralized control and the need to detour packets limits scalability. NoRD [3] and TooT [12] bypass packets to reduce router wake-ups which improves energy-efficiency. However, both incur latency overheads NOCS ’19, October 17–18, 2019, New York, NY, USA Hossein Farrokhbakht, Henry Kao, and Natalie Enright Jerger output ports to reduce the complexity of the conventional crossbar and the arbitration. Completely eliminating buffers is another approach to save power and area [8, 14, 25]. However, during high loads bufferless routers significantly increase packet latency and network traffic since packets are deflected and rerouted in nonminimal paths; they are throughput-limited designs. In contrast, Routerless Networks-on-Chip [1] completely remove routers from the network by implementing multiple ring networks. Although they eliminate complex router structures, large buffer overheads are incurred. The aforementioned techniques address the underutilization of resources by simplifying or removing router structures. These simplifications, however, push complexity into other areas and limit NoC functionality. 6 CONCLUSION Conventional router resources allow for multiple packets to traverse the switch concurrently. This capability is not exploited by most workloads. Resources that support concurrent switch traversals, namely the input buffers and crossbar, often go underutilized. As a result we present a new energy-efficient router microarchitecture, UBERNoC, that minimizes underutilized resources to save both power and area. We unify the input buffers for each port into a single buffer shared between all ports (expect injection port), which in turn also simplifies the crossbar. We also give an alternative high-throughput UBERNoC design, UBERNoC2, with two unified buffers. Compared to a conventional router, UBERNoC provides 58% and 69% reduction in power and area with negligible increase in packet latency within the SPLASH-2 benchmarks. ACKNOWLEDGEMENTS We thank the anonymous reviewers and members of the NEJ Group for their helpful feedback on this work. This work was supported in part by the Natural Science and Engineering Research Council of Canada, the University of Toronto and the Percy Hart Early Career Professorship. "
NoC-enabled software/hardware co-design framework for accelerating k-mer counting.,"Counting k-mers (substrings of fixed length k) in DNA and protein sequences generate non-uniform and irregular memory access patterns. Processing-in-Memory (PIM) architectures have the potential to significantly reduce the overheads associated with such frequent and irregular memory accesses. However, existing k-mer counting algorithms are not designed to exploit the advantages of PIM architectures. Furthermore, owing to thermal constraints, the allowable power budget is limited in conventional PIM designs. Moreover, k-mer counting generates unbalanced and long-range traffic patterns that need to be handled by an efficient Network-on-Chip (NoC). In this paper, we present an NoC-enabled software/hardware co-design framework to implement high-performance k-mer counting. The proposed architecture enables more computational power, efficient communication between cores/memory - all without creating a thermal bottleneck; while the software component exposes more in-memory opportunities to exploit the PIM and aids in the NoC design. Experimental results show that the proposed architecture outperforms a state-of-the-art software implementation of k-mer counting utilizing Hybrid Memory Cube (HMC), by up to 7.14X, while allowing significantly higher power budgets.","NoC-enabled Software/Hardware Co-Design Framework for  Accelerating k-mer Counting  Biresh Kumar Joardar*, Priyanka Ghosh*, Partha Pratim Pande*, Ananth Kalyanaraman*, Sriram Krishnamoorthy†  *School of EECS, Washington State University    Pullman, WA 99164, U.S.A.   {biresh.joardar, priyanka.ghosh, ananth, pande}@wsu.edu   †HPC Group, Pacific Northwest National Laboratory  Richland, WA 99352, U.S.A.  sriram@pnnl.gov  ABSTRACT  Counting k-mers (substrings of fixed length k) in DNA and protein  sequences generate non-uniform and irregular memory access  patterns. Processing-in-Memory (PIM) architectures have the  potential to significantly reduce the overheads associated with  such frequent and irregular memory accesses. However, existing  k-mer counting algorithms are not designed to exploit the  advantages of PIM architectures. Furthermore, owing to thermal  constraints, the allowable power budget is limited in conventional  PIM designs. Moreover, k-mer counting generates unbalanced and  long-range traffic patterns that need to be handled by an efficient  Network-on-Chip (NoC). In this paper, we present an NoCenabled software/hardware co-design framework to implement  high-performance k-mer counting. The proposed architecture  enables more computational power, efficient communication  between cores/memory – all without creating a thermal  bottleneck; while the software component exposes more inmemory opportunities to exploit the PIM and aids in the NoC  design. Experimental results show that the proposed architecture  outperforms a state-of-the-art software implementation of k-mer  counting utilizing Hybrid Memory Cube (HMC), by up to 7.14X,  while allowing significantly higher power budgets.  KEYWORDS  Manycore, M3D, Thermal, PIM, k-mer counting, Co-design   1 INTRODUCTION  Analysis of biomolecular data such as DNA and proteins has been  one of the primary drivers of scientific discovery in biological  sciences. From a computational perspective, these biomolecules  can be represented as strings (equivalently, sequences). Hence,  sequence analysis occupies a significant portion of many  bioinformatics workflows. One such operation is k-mer counting,  where the goal is to determine the counts of all distinct fixed  length substrings of length k in a large collection of input  sequences. Computing k-mer abundance profiles  is often  necessary for several bioinformatics applications e.g., de novo  genome assembly, repeat identification, etc.       Challenges: From a software perspective, implementing k-mer  counting in a resource- and time-efficient manner is a challenging  © 2019 Association for Computing Machinery. ACM acknowledges that this  contribution was authored or co-authored by an employee, contractor or affiliate of  the United States government. As such, the United States Government retains a  nonexclusive, royalty-free right to publish or reproduce this article, or to allow others  to do so, for Government purposes only.   NOCS '19, October 17–18, 2019, New York, NY, USA   © 2019 Association for Computing Machinery.   ACM ISBN 978-1-4503-6700-4/19/10…$15.00  https://doi.org/10.1145/3313231.3352367  task. Existing software-only solutions for efficient k-mer counting  e.g. KMC2 [1], Gerbil [2], do not consider hardware limitations,  resulting in sub-optimal performance. The counting process  generates irregular memory accesses and sparse computations  that results in more frequent memory read/writes. Conventional  memory architectures provide limited bandwidth with higher  read/write latencies. This presents a performance bottleneck for  k-mer counting, which relies on repeated memory access. As a  result, computing units often remain idle, as a large portion of the  execution time is spent moving data to/from memory. Moreover,  k-mer counting generates significant communication between the  Processing Elements (PEs). For example, in Gerbil [2], k-mers are  repeatedly distributed among the threads responsible for  counting, introducing significant amount of on-chip traffic.  Without an efficient communication backbone, this can lead to  longer execution times as PEs would remain idle for a greater  number of cycles waiting for data. To overcome these  inefficiencies in conventional architectures, we posit that a  carefully designed software/hardware co-design framework can  be better equipped to derive the best out of both worlds. More  specifically, in this work, we argue that the emerging paradigm of  Processing-in-Memory (PIM), enabled by a Network-on-Chip  (NoC), presents a promising solution to these applications.   PIM takes advantage of emerging 3D-stacked memory + logic  devices (such as Micron’s Hybrid Memory Cube or HMC) to  enable high-bandwidth, low latency and low energy memory  access [3]. However, conventional PIM architectures are restricted  by thermal constraints as temperature impacts both memory  retention and overall performance [4, 5]. Conventional 2.5D PIM  and 3D PIM architectures often have limited power budget and  computational capability before  reaching  the allowable  temperature threshold [5, 6]. The role of NoC in PIM architectures  is also understudied. Due to a single layer of logic in existing PIM  architectures, planar NoC is used for efficient communication  among the PEs [7]. However, 2D NoCs such as mesh, are not  suited for long range communication that is inherent in k-mer  counting. Moreover, k-mer counting generates unbalanced traffic  that imposes an additional layer of design complexity.  To overcome the above-mentioned challenges, in this paper, we  propose an NoC enabled manycore architecture that exploits the  benefits of emerging Monolithic 3D (M3D) integration to integrate  multiple logic layers in PIM architectures with 3D-stacked  memory, for high-performance k-mer counting. Experimentally,  we show that even with multiple logic layers and higher power  budget, the proposed architecture does not violate thermal  constraints. The main contributions of this work are:      NOCS 19  B.K.Joardar et al.  • We profile k-mer counting to extract features relevant  to the design of a high-performance NoC and the  overall PIM architecture for k-mer counting.  • We  present  a  software/hardware  co-design  framework: the hardware consists of a PIM with  multiple logic layers enabled by M3D integration while  the software component enables high-performance kmer counting by utilizing the benefits of PIM and aids  in NoC design.   • We perform a thorough experimental evaluation of the  proposed co-design framework and show significant  improvements over an appropriate baseline.  2 RELATED WORKS  2.1 k-mer counting  The task of k-mer counting is memory-intensive and involves  creating a histogram of all k-length substrings in a DNA sequence.  KMC2 [1], DSK [8] and Gerbil [2] are some of the popular tools  for this purpose, with Gerbil representing the state-of-the-art in  software as it outperforms most of the other tools. However, it  requires the repetitive use of off-chip secondary memory and  therefore will fail to fully exploit the high-bandwidth, low latency  memory access facilitated by PIM. Manycore CPU- and GPU-  based platforms are the preferred choices for implementing k-mer  counting [1, 2]. However, these works do not address the memory  bottleneck issues. In [9], the authors designed a custom FPGAbased architecture connected to an HMC for approximate k-mer  counting. However, the memory (HMC) is connected to the PEs  using serial links in a 2.5D architecture , which is not as efficient  as completely on-chip solutions i.e. 3D PIM [3, 10].   2.2 Processing-in-memory  Processing-in-Memory (PIM) involves moving the computational  units closer to memory. This allows efficient data transfer from  memory enabled by 3D integration [3]. Prior works has mostly  focused on Through Silicon Via (TSV)-based PIM architectures,  which are prone to high temperatures [5, 11]. Heat from  processing elements can significantly affect the retention time of  DRAMs [4]. Beyond 85ᵒC, the overheads to counter lower DRAM  retention can significantly offset the benefits of PIM [5].   To reduce the effect of temperature, 2.5D architecture is a popular  choice to implement PIM where PEs are placed near the memory  and connected via interposers. However, lateral heat flow from  PEs can significantly affect memory temperature [6]. In 3D-PIM  architectures, memory is stacked directly on top of the PEs, which  enables better throughput and latency [3]. However, they are  more prone to high temperature as PEs are placed in the same  vertical stack. In [12], the authors propose to use a mix of 2.5D +  3D PIMs. They map the application on PEs based on its memory  and compute requirements for best performance. However, their  methodology assumes prior knowledge of an application, which  is often not feasible. Memory-centric NoC, that connects multiple  HMCs to facilitate efficient data transfer has been studied [7]. The  role of NoC connecting different vaults of an HMC is discussed in  [13]. However, these implementations are limited to 2D NoC  2  which is inefficient in addressing the challenges posed by k-mer  counting i.e. unbalanced traffic and long-range communication.  To overcome these limitations, we propose an NoC-enabled PIMbased architecture that amalgamates: (a) multiple logic layers in  conventional PIM, (b) M3D-based vertical integration, and (c)  efficient 3D-NoC design for high-performance k-mer counting,  while remaining within 85ᵒC temperature. To take advantage of  PIM’s more efficient memory access and aid NoC design, we also  propose an alternative software approach to count k-mers that  outperforms the Gerbil framework.  3   High-performance k-mer counting   Problem statement: Given a DNA sequence s of length l, a “kmer” is defined as a substring of length k in s (k being an integer,  k l). All k-mers in s can be generated by simply sliding a window  of length k over s. Given a set S of n such input sequences (aka.  “reads”), the problem of k-mer counting is one of determining the  total number of occurrences for each distinct k-mer that is present  in the reads of S. In this work, we mainly focus on Gerbil [2] as  our software baseline for k-mer counting. Gerbil is a recently  proposed methodology that outperforms several well-known  counters e.g. KMC2 [1] and DSK [8] for higher values of k, e.g.  k=32, making it an appropriate software baseline to consider.   3.1  Gerbil: Overview and Analysis  Gerbil is a two-phase algorithmic implementation for k-mer  counting: (a) 'Distribution' phase where the input reads are  partitioned into multiple intermediate files on the disk, and (b)  'Counting' phase, which reads each of these intermediate files (one  by one) for counting. The entire procedure is designed to make  optimal use of conventional manycore architectures. Each step in  Gerbil operates in a pipelined fashion for high throughput  counting. Popular techniques like load balancing and use of failure  buffers to handle hash conflicts, etc., have also been used for better  performance. Fig. 1 illustrates the overall workflow of Gerbil.   Next, we thoroughly analyze Gerbil using detailed full-system  simulations on Gem5 [14] to study relevant features that are  crucial in designing an efficient manycore architecture. Fig. 2(a)  shows the different categories of instructions (operations)  involved in Gerbil with real-world inputs.  We observe that nearly  two-thirds of Gerbil consists of integer operations. Memory  operations (including I/O) constitute the second largest category  (32.5%). The remaining is made up of NoOps while floating-point  instructions are negligible.    Traditional off-chip main-memory and secondary memory  accesses are slower [10], which can cause significant CPU stalls.  In Gerbil, memory and I/O operations contribute significantly to  Fig. 1: Illustration of workflow in Gerbil (F: Input files, B: Buckets,  C: CPU; ‘Buckets’ is synonymous to ‘intermediate files’ in [2])        NoC-enabled Software/Hardware Co-Design Framework for Accelerating k-mer Counting  NOCS 19  32.5% 66.5% NoOp Int Float Mem 16% 12% 8% 4% 0% CPU Utilization 2 4 6 1 8 2 1 2 1 5 n o i t a z i l i t u U P C e z i l d e Runtime 1 0.8 0.6 0.4 0.2 0 m a o N r m i t n u r No. of intermediate files (a)            (b)  Fig. 2: Gerbil: (a) Instruction types, and (b) CPU utilization and  runtime (normalized) for varying number of intermediate files  the runtime, the effect of which is captured in Fig. 2(b).  Fullsystem simulations on Gem5 with 64 Intel x86 cores executing  Gerbil shows that CPUs are utilized less than 15% of the time (Fig.  2(b)) for any number of intermediate files. The intermediate files  are generated after Distribution phase of Gerbil which are stored  and then eventually read back from the slow off-chip secondary  memory for further processing (as shown in Fig. 1). Moreover, the  counting process involves irregular memory accesses, which  makes caching ineffective. As a result, even though integer  instructions constitute the majority of Gerbil operations , Fig. 2(b)  clearly highlights that most of the execution time is spent in  fetching/storing data rather than actual computation. The CPU  utilization gets worse if more intermediate files are generated  (4.5% CPU utilization in the case of 512 files) as it involves more  off-chip memory access. This translates to a significant increase  in runtime as observed in Fig. 2(b). Overall, it is clear that Gerbil  does not efficiently utilize the computing resources resulting in  sub-optimal performance. Fig. 2 also proves that slow memory  access presents a more serious bottleneck to performance than  computation, for k-mer counting, making it an ideal case for PIM.  However, Gerbil’s dependence on secondary memory (Fig. 1)  makes it inappropriate for PIM architectures as it’ll fail to fully  exploit the high-bandwidth,  low  latency memory access  facilitated by PIM. Therefore, A PIM-friendly k-mer counting  software solution that complements the hardware is necessary.   Fig. 3 shows the communication between every CPU (C i) pair for  three different input datasets in the form of a heat map. Here, we  define amount of communication (traffic) as the number of flits  exchanged between a pair of cores during k-mer counting as  obtained using full-system Gem5 simulations considering a  manycore system with 64 cores. As shown in Fig. 3, k-mer  counting in Gerbil exhibits significant amount of data exchanges  between cores. Darker patches (a few have been highlighted in red  in Fig. 3(a) as examples) indicate heavier communication between  a pair of cores. Planar logic in conventional PIM offers only a  Fig. 4: Illustration of workflow in proposed k-mer counting  methodology: PIM-Counter (F: Input files, B: Buckets, C: CPU)   limited number of design and floor-planning choices. Hence,  frequently communicating cores may get potentially placed far  from each other, leading to long range communication. Also, we  observe several lighter patches indicating lower communication  in Fig. 3. This shows that the communication in Gerbil is highly  unbalanced. Few of the cores have heavy data traffic while the rest  have relatively negligible traffic. These heavily communicating  cores e.g. C1 in Fig. 3(a), can become traffic hotspot during  execution, which affects performance. Without a suitable NoC  backbone, this can result in higher latency that in turn will  increase execution time. It is well known that 2D NoCs (due to  single layer of logic in conventional PIM) are not scalable and not  suited to handle long range communication. Therefore, an  efficient NoC is crucial for high-performance k-mer counting.   3.2 PIM-Counter: PIM Friendly k-mer Counter   In this section we present PIM-Counter, a PIM-friendly multithreaded algorithm designed to overcome the I/O bottleneck of  Gerbil, exploit the PIM-based architecture and aid in the NoC  design. Fig. 4 shows the workflow of PIM-Counter. As discussed  earlier, Gerbil relies on secondary memory usage, which results in  inefficient CPU utilization (Fig. 2) and is not suited for PIM-based  architectures. In contrast, the proposed PIM-counter (Fig. 4), uses  an on-chip memory-friendly approach to utilize the benefits of  PIM. As illustrated in Fig. 4, PIM-Counter has three main steps:     Step-1: Input loading: Instead of reading the input files in  batches using multiple I/O passes as in Gerbil (Fig. 1), PIMCounter performs a single I/O pass. The inputs are then loaded  uniformly across the PIM cubes. Here, a cube (Fig. 5, which shows  the overall PIM architecture) is analogous to an HMC vault [13]  that consists of both logic and memory. However unlike  conventional HMC vaults, we also consider PEs e.g. CPUs, as part  of the logic layer (i.e. 3D PIM). We discuss the hardware  architecture in more details in next section.  Step-2: Bucketing: Once the strings are loaded onto the memory,  uniformly across the partitions (‘cubes’ in Fig. 5), the local  thread(s) in the corresponding cubes generate all k-mers from  each string by sliding a window of length k. To overcome the            (a)             (b)   (c)  Fig 3: CPU-to-CPU communication profile for Gerbil in the form of heat map for input datasets (a) E. Coli, (b) Prochlococcus sp., and (c)  Vibrio cholerae (Ci: CPUi; Red boxes highlight few of the patches of heavy communication)  3                                           NOCS 19  B.K.Joardar et al.  challenge imposed by the use of a large value of k, we use the  concept of minimizers, which was originally introduced in the  context of building de Bruijn graphs [15]. The idea is to hash each  k-mer using its least (or equivalently, most) frequent m-mer,  where m<k (e.g., m=7; k=32) and migrate that k-mer to the  minimizing m-mer’s bucket.  Here, the term bucket refers to the  collection of all k-mers that share the same minimizer and is  analogous to the ‘intermediate files’ used in Gerbil. However,  unlike Gerbil, these buckets are present in the on -chip memory.  Each cube is responsible for a different, non-overlapping set of  buckets. The mapping of bucket to cube id is achieved using a hash  function in linear congruential form (e.g. ((Ax+B) mod P), A, B and  P are constants), which distributes all possible buckets across the  different cubes. As a result, the responsible bucket for a k-mer  could either reside on the local cube (same cube as the computing  PE) or on a remote cube (any other cube except the local cube). For  example, in Fig. 5, Cube-16 is a local cube to CPU-16, while Cube1 is remote cube to CPU-16. Memory in local cube can be accessed  by the cores using vertical interconnects only. A remote cube,  however, must be reached via the use of one or more planar links.  Accessing remote cubes is costlier as data must traverse longer  physical distance that can result in higher execution time. The  NoC should support this data movement.   In PIM-Counter, the data movement (traffic pattern) between PEs  depends on the hash function which defines the mapping of k-mer  buckets to cube-ids. Therefore, it is important to choose suitable  values for A, B and P (and hence the hash function) such that the  resulting traffic is balanced. Overall, our aim here is to choose a  suitable mapping that distributes the traffic among the PEs evenly  to avoid hotspots in the NoC during execution. We use full-system  Gem5 simulations to determine the hash function that yield better  traffic distribution (shown in experimental results).  Step 3: Counting: In the final step, the thread(s) local to each  cube aggregate the counts for each distinct k-mer represented in  its local buckets; this is achieved using a parallel reduction. Here,  PIM-Counter fully exploits the locality benefits of PIM as data is  already available on each thread’s corresponding local cube (due  to the previous bucketing phase) and can be accessed using just  the vertical links. Due to the physical proximity of memory in  PIM, CPU stalls are greatly reduced as data can be fetched  relatively faster than in conventional architectures (where data is  fetched from physically distant/off-chip memory).   Overall, PIM-Counter presents a PIM-friendly k-mer counting  alternative that can outperform other counting tools as it benefits  Fig. 5: Proposed PIM-based architecture with multiple logic and  memory layers enabled via M3D integration  4  from high-bandwidth, low-latency and low-energy memory  access facilitated by PIM. It also enables efficient communication  between PEs by reducing traffic hotspots.   4 NoC-ENABLED 3D-PIM DESIGN  In this section, we introduce the features of the proposed 3D-PIM  enabled by M3D integration followed by the NoC design that  supports the communication generated by k-mer counting.  4.1  PIM-architecture for k-mer Counting  PIM allows high bandwidth, low-latency and low-energy memory  access by moving computation closer to memory [10]. The faster  memory access enabled by PIM is crucial for k-mer counting as a  large fraction of time (>85%) is spent in fetching/storing data  to/from memory in Gerbil (Fig. 2). However, temperature presents  an important limitation in conventional PIM architectures. DRAM  retention capability is lowered beyond 85ᵒC. After temperature  exceeds this threshold, refresh rate must be doubled for every  ∼10ᵒC increase in memory temperature. Higher refresh rates  consume more power and, results in lower memory performance  [5]. Also, traditional power management techniques are often not  tailored for memory. Therefore, placing memory directly on top  of (or nearby) the PEs in PIM, without addressing thermal issues,  can be detrimental to performance .    In [6], the authors found that 2.5D PIM architectures are prone to  lateral heat flow from PEs even when placed 10mm farther from  the HMC. Placing memory farther away to reduce temperature ,  also defeats the main purpose of PIM, which is to bring  computation closer to memory. 3D PIM architectures where PEs  are in same vertical stack as memory, are even more sensitive.  Therefore, conventional PIM architectures (both 2.5D and 3D)  typically use either (a) PEs with simpler architectures (as complex  cores e.g. Out-of-Order (OoO) CPUs tend to consume more power  [5]), (b) fewer number of cores e.g. [12], or (c) minimal computing  power [6], (or all of the above) to remain within the temperature  threshold. Due  to  these  restrictions, conventional PIM  architectures have lower computation capability that affects  performance and are not scalable with increasing system size.   Moreover, PIM architectures are restricted to single logic layer and  multiple memory layers, as logic (PEs) dissipates more heat than  memory [5]. It is well known that 2D logic provides limited floorplanning choices and require more die area than an equivalent 3D  counterpart. However, multiple logic layers stacked vertically in  3D ICs are prone to higher temperatures as PEs farther away from  the sink cannot dissipate heat easily, resulting in worse  temperature [16]. As PEs consume more power than memory, use  of multiple layers of logic in PIMs is typically avoided. As a result,  only a few cores can be integrated given a fixed area constraint.  Overall, our objective for a “suitable PIM architecture” is one that  should: (a) allow larger volume of computation (logic) to be  integrated without incurring extra area and thermal overheads;  and (b) enable efficient data exchange between cores and memory.   Taking advantage of the benefits of 3D ICs in this work, we  propose a PIM architecture that incorporates multiple logic layers  in conventional PIM for high-performance. Fig. 5 shows the  proposed architecture with multiple logic layers (similar to 3D ICs        NoC-enabled Software/Hardware Co-Design Framework for Accelerating k-mer Counting  NOCS 19  [16]) and multiple memory layers. Each logic layer consists of  multiple PEs, while the memory layers consists of conventional  DRAM. The cores are connected using a Network-on-Chip (NoC)  to support efficient on-chip communication between cores. We  discuss NoC design in the next sub-section. The use of multiple  logic layers enables a greater number of cores to be integrated  compared to traditional PIM (single logic layer) under an “isoarea” setting. All the layers are virtually (not physically) divided  into several equal cubes. Each cube consists of equal amount of  resources i.e. one core per logic layer (placed vertically on top of  each other) and the portion of memory directly above it. For  example, in Fig. 5 (assuming 2-logic layers and following similar  numbering convention of CPUs), Cube-16 consists of CPU-16,  CPU-32 and part of memory directly above it.  Conventional TSV-based 3D architectures are susceptible to  higher temperatures and hence cannot be used to design the  proposed architecture (Fig. 5) [11]. Consecutive layers in TSVbased designs are physically attached using a bonding material  e.g. Benzocyclobutene  (BCB),  that exhibits poor thermal  conductivity. This impedes the seamless flow of heat across the  layers resulting in considerable increase in temperature in the  layers away from the heat sink. Moreover, the relatively thicker  silicon substrate (several micrometers) in TSV-based designs  causes the heat to spread laterally within the substrate instead of  vertically towards the sink. This results in higher on-chip  temperatures, which is undesirable in PIM architectures.   On the other hand, emerging M3D integration allows faster  dissipation of heat than its TSV-based counterparts [16]. Absence  of a bonding material and relatively smaller dimensions  (nanometers as opposed to micrometers) leads to superior thermal  characteristics than TSV-based designs. Therefore, we argue that  we should design high-performance yet thermally viable PIM  architectures with multiple logic (and memory) layers as shown  in Fig. 5 using M3D integration. Experimentally, we show that  M3D-based PIM designs are superior  in terms of both  performance and temperature, enabling higher power budgets  compared to their TSV-based solutions. Moreover, M3D enables  design of area- and power-efficient multi-tier logic blocks [17].  The possibility of multi-tier logic blocks e.g. NoC routers, enable  design of high-performance and energy-efficient NoCs, which is  essential to support efficient k-mer counting.   4.2 NoC design for k-mer Counting  For achieving high performance, the choice of overall NoC  connectivity should be governed by the traffic pattern generated  by the application under consideration. As shown in Fig. 3, k-mer  counting introduces significant long-range and unbalanced traffic  pattern that should be handled by the NoC. The unbalanced traffic  in k-mer counting is addressed by choosing a suitable mapping to  cubes (hash-function) in PIM-Counter as discussed in Section 3.  PIM-Counter makes the traffic more uniform compared to Gerbil,  reducing chances of traffic hotspots (shown later in experimental  results section). To efficiently handle the long-range traffic  pattern, 3D small-world (SW) NoC architecture is a suitable  choice. The vertical links in 3D NoCs bring cores physically closer  Fig. 6: Illustration of proposed M3D-enabled SW-NoC with multitier routers [17] and small-world properties [19] (The color  contrast between Layer1 and Layer2 is for differentiation only)  and enable long-range communication shortcuts necessary for  designing high-performance SW NoC [19]. Moreover, the vertical  connectivity in M3D is facilitated by monolithic inter-tier vias  (MIVs), which are 100x smaller and more energy efficient than  conventional TSVs [18]. Overall, we utilize the benefits of M3D to  design high-performance, yet energy efficient SW NoCs.   To design a suitable 3D SW NoC, the placement of links and  routers need to be optimized based on the application (k-mer  counting in this case). By optimizing the placement of the  routers/links, it is possible to address the communication  challenges inherent in k-mer counting (Fig. 3) effectively. We  demonstrate that the designed NoC (executing PIM-Counter)  outperforms Gerbil running on an equivalent platform, in later  section. Next, we discuss the details of the NoC optimization.   Optimization Objective: For the NoC performance evaluation,  we consider two objectives: latency and energy. We estimate  network latency and energy using analytical models proposed in  [16] for optimization purpose. For an N core system, the average  network latency is modeled as:   𝐿𝑎𝑡 = 1 ∑ 𝑓𝑖𝑗 𝑁 𝑁 ∑ ∑(𝑟 ⋅ ℎ𝑖𝑗 + 𝑑𝑖𝑗 ) ⋅ 𝑓𝑖𝑗                         (1) 𝑖=1 𝑗=1 Here, 𝑓𝑖𝑗 represents the number of flits exchanged between core i  and core j (Fig. 3) obtained from full-system k-mer counting  simulations on Gem5. The parameter r represents the number of  router stages, ℎ𝑖𝑗 denotes the number of hops between the two  cores while 𝑑𝑖𝑗 incorporates the effect of physical distance that  messages must traverse based on the routing protocol.   The network energy is modeled using the following equations:  𝑁 𝑁 𝐸𝑟𝑜𝑢𝑡𝑒𝑟 = ∑ ∑ 𝑓𝑖𝑗 𝑖=1 𝑗=1 𝑅 ⋅ ∑ 𝑟𝑖𝑗𝑘 ⋅ (𝐸𝑟 ⋅ 𝑃𝑘 )               (2)  𝑘=1 𝑁 𝑁 𝐿 𝑉 𝐸𝑙𝑖𝑛𝑘 = ∑ ∑ 𝑓𝑖𝑗 ⋅ (∑ 𝑝𝑖𝑗𝑘 ⋅ 𝑑𝑘 ⋅ E𝑝𝑙𝑎𝑛𝑎𝑟 + ∑ 𝑞𝑖𝑗𝑘 ⋅ 𝐸𝑣𝑒𝑟𝑡𝑖𝑐𝑎𝑙 𝑖=1 𝑗=1 𝑘=1 𝑘=1 )  (3)  𝐸 = 𝐸𝑟𝑜𝑢𝑡𝑒𝑟 + 𝐸𝑙𝑖𝑛𝑘                           (4)   Here 𝐸𝑟 denotes the average router logic energy per port and 𝑃𝑘  denotes the number of ports available at router k. The total link  energy can be divided into two parts due to the different physical  characteristics of planar and vertical links. 𝑑𝑘 represents the  physical link length of link k. Here, 𝑞𝑖𝑗𝑘 and 𝑟𝑖𝑗𝑘 indicate if a  vertical link or router k is utilized to communicate between core i  and core j respectively. E𝑝𝑙𝑎𝑛𝑎𝑟 and E𝑣𝑒𝑟𝑡𝑖𝑐𝑎𝑙 denote the energy  consumed per flit by planar metal wires and vertical links (TSV or  MIV) respectively. All the required power numbers were obtained  5              Gerbil Pim-Counter 2.5X reduction U P C n o i t a 100% 75% 50% 25% 0% z i l i t u B.K.Joardar et al.  Gerbil Pim-Counter NOCS 19  11.7% 87.7% NoOp Float Int Mem (a)   y r o m e M f o . o N s n o i t c u r t s n i 3.0 2.0 1.0 0.0 Api Dcn (b)  Eco Fig 7: PIM-Counter: (a) Instruction types, and (b) Number of  memory operations compared to Gerbil (normalized)  using Synopsys Prime Power for 28nm nodes. The total network  energy E is the sum of router logic and link energy .  We optimize the two objectives, latency and energy, using a  machine learning-enabled Multi-Objective Optimization (MOO)  algorithm: MOO-STAGE [16]. By learning the search space, MOOSTAGE can find better solutions than several conventionally used  MOO algorithms in much less time. Hence, it is a suitable choice  of MOO solver for optimizing the NoC. Overall, MOO-STAGE  finds the best placement of links and routers in the SW NoC that  achieves  good trade-off between both objectives: latency and  energy, to enable high-performance k-mer counting To ensure  that the optimized architectures are realistic, we make sure that  there is always at-least one path for communication between any  pair of cores. M3D specific design aspects, e.g. possibility of multitier routers has also been incorporated in the optimization.  Following prior designs, e.g. [17], we restrict routers to span  across at-most two layers only. Fig, 6 shows an illustration of the  NoC for the proposed M3D-enabled PIM   5 EXPERIMENTAL RESULTS  In this section, we evaluate the performance of the proposed NoCenabled, software/hardware co-design framework for k-mer  counting. First, we analyze PIM-Counter (Sec. 3) in detail. Next,  we evaluate the performance and thermal profile of the proposed  PIM architecture and the designed NoC .      5.1 Experimental Setup  We use a detailed full system simulator, Gem5 [14] to characterize  the performance of the PIM-based manycore architecture  proposed in this work. We modify the memory and Garnet  network models within Gem5 to implement the various memory  and NoC architectures considered in this work. All experiments  are performed on a 64-core system where each core is based on  Intel x86 architecture. The memory system comprises of private  32KB L1 instruction and data caches, shared 16MB L2 cache  (256KB distributed L2 per core). The simulated system operates  with a clock frequency of 2.5GHz. The CPU power profiles are  extracted using McPAT [20] while the on-chip temperatures are  obtained using Hotspot [21] simulations. The TSV and M3D layers  are modeled in Hotspot based on parameters e.g. layer thickness,  thermal conductivity, etc. as listed in [11].  For all experiments, we  have considered DRAM-based memory. Following prior works,  the memory is modeled as a multi-layer stack in the proposed PIM  design. All layers in the same vertical stack have equal area i.e.  the memory die area is assumed to be same as the logic die area  in the proposed 3D-PIM architecture.   6  Fig 8: Average CPU utilization in Gerbil and PIM-Counter  Api Bac Dcn Dro Eco Pla Pro Rat Str Vib For experimental evaluations, we chose ten different genomic  sequences from across the species spectrum: six prokaryotic  genomes including, Prochlorococcus sp. (Pro), S. pneumoniae (Str),  V. cholerae (Vib), E. coli (Eco), B. circulans (Bac), P. vivax (Pla) and  four eukaryotic genomes namely, A. melifera  (Api), D.  melanogaster (Dro), D. labrax (Dcn) and R. norvegicus (Rat). This  collection represents a wide variety in genome input complexities  (including k-mer composition and abundance levels), intended to  help us test our framework under different input scenarios, as kmer counting is known to be an input-dependent problem [22].   5.2 Performance Evaluation of PIM-Counter  We first evaluate the performance of the proposed PIM-Counter  framework based on its traffic pattern, types of instructions and  CPU utilization to compare with Gerbil. We profile PIM-Counter  using full-system simulations on Gem5 similar to Gerbil (Sec. 3).  Fig. 7(a) shows the distribution of instructions for PIM-Counter  (similar to Fig. 2). Interestingly, we note that integer operations  constitute a far greater proportion i.e. 87.7% in PIM-Counter (as  opposed to 66.5% in Gerbil (Fig. 2)). Memory (including I/O)  instructions only contribute to 11.7% of overall instructions.  Floating point instructions and NoOps were negligible in both  Gerbil and PIM-Counter. Fig. 7(b) compares the actual number of  memory (including I/O) instructions for Gerbil and the PIMCounter. We note that the PIM-Counter reduces the number of  memory operations by ~2.5X compared to Gerbil. This is  important as memory operations contribute significantly to  execution time. Coupled with a more efficient memory access  provided by PIM, this results in a significant improvement in CPU  utilization – as shown in Fig. 8. On average, the CPUs are utilized  75% of the time as opposed to <15% in Gerbil.  This shows that by  facilitating easier memory access, we can improve hardware  utilization significantly. This is expected as PIM-Counter reduces  the amount of I/O operations (which are slow), while promoting  more on-chip memory usage to take advantage of PIM.   Next, we look at the corresponding traffic pattern generated when  PIM-Counter is executed on a 64-core architecture. Fig. 9 (a-c)  shows the traffic pattern between CPUs for PIM-Counter with  three real world datasets, namely: Eco, Pro and Vib, as examples.   Fig. 9(d) compares the standard deviation of Gerbil’s traffic  (normalized) with respect to that of PIM-Counter for these three  datasets. The standard deviation of traffic captures the variation  among the number of flits associated with each PE. Higher values  of standard deviation indicate a more unbalanced traffic which  can lead to traffic hotspots in the NoC resulting in h igher  execution times (discussed in Sec. 3). Contrasting these traffic  patterns with those of Gerbil shown in Fig. 3, it is clear that PIMCounter achieves a more balanced traffic. For instance, in the case  of Eco, PIM-Counter generates a 69% (Fig. 9(d)) better balanced                              NoC-enabled Software/Hardware Co-Design Framework for Accelerating k-mer Counting  NOCS 19      (a)             (b)      (c)            (d)  Fig 9: CPU-to-CPU communication profile for PIM-Counter as heat map (C i: Core i;) for input datasets (a) E. Coli (Eco), (b) Prochlococcus  sp. (Pro), (c) Vibrio cholerae (Vib), and (d) Standard Deviation of Gerbil’s traffic normalized with respect to that of PIM -Counter (PC)  traffic pattern. This can be attributed to the appropriate mapping  logic and memory power is varied simultaneously to study the  of k-mers to cube-id in PIM-Counter. Hence, traffic hotspots are  maximum on-chip temperature. From Fig. 10(b) we note that M3D  less likely, leading to better performance in PIM-Counter.    based PIM provides a much higher power budget (up to 8W more)  than their TSV-based counterpart under similar settings. The  higher power budget is achieved as M3D-based architectures do  not have layers with poor thermal conductivity and have  relatively smaller dimensions (discussed in Sec. 4) which aide in  quick dissipation of heat. As a result, the temperature increase is  significantly contained allowing more power budget (and multiple  layers of logic) in an M3D-enabled PIM without exceeding 85ᵒC.   5.3 Thermal Evaluation  For any new PIM architecture, thermal feasibility is a major  concern [5]. In Sec. 4, we argued that it is possible to integrate  multiple layers of logic (similar to conventional 3D ICs) in PIM  using M3D. Therefore, before performance analysis, in this  section, we first investigate and experimentally validate the  thermal feasibility of the proposed PIM architecture.   Fig. 10(a) shows the variation of maximum on-chip temperature  for k-mer counting as more logic layers are added. Each layer has  been modeled following [11] in HotSpot. Here, we assume that the  number of memory layers to be fixed while varying the number  of logic layers beneath it. For all experiments, an ambient  temperature of 45ᵒC and an inexpensive low-end cooling  (convention resistance = 2ᵒC/W [11]) is used. Fig. 10(a) indicates  that even with a simple cooling solution, up to four layers of logic  can be easily integrated in M3D-based 3D-PIM without reaching  temperature threshold of 85ᵒC. On the other hand, TSV-based PIM  only allows a maximum of 2 logic layers for k-mer counting.  Beyond two layers, TSV-based PIM architectures necessitate  higher refresh rates and more expensive cooling solutions to be  viable. Note that adding the second layer of logic results in  temperature close to the threshold (81ᵒC) which may still  necessitate precautions for safe operation. However, even with  four logic layers, M3D-based PIM architecture exhibits maximum  temperature of 74ᵒC only. Therefore, contrary to conventional  PIM architectures, it is possible to have multiple logic layers in an  M3D-enabled PIM without violating thermal constraints.    Also, since core and memory power depend on several factors e.g.,  voltage-frequency settings, technology node etc., it is important  to study the power budget available in both architectures (without  exceeding 85ᵒC) for a complete analysis. Fig. 10(b) shows the  amount of power budget available in both PIM architectures when  5.4 Performance Evaluation  Next, we present the NoC and overall full-system performance  evaluation. Fig. 11 shows the performance of the optimized M3Denabled SW-NoC for both Gerbil and PIM-Counter. The NoCs for  both Gerbil and PIM-Counter have been designed following the  same optimization methodology discussed in Sec. 4.2. Here, we  assume a 64-core architecture arranged in four layers (16 cores per  layer) with M3D-integration for both Gerbil and PIM-Counter.  From Fig. 11, we note that the optimized NoC for PIM-Counter  outperforms its Gerbil counterpart by 14% on average for all the  datasets. This happens as PIM-Counter has a more balanced  traffic, which reduces hotspots leading to faster communication.  On the other hand, Gerbil has a handful of cores contributing  significantly higher traffic than the rest (Fig. 3). Routers/Links  near them experience more congestion , affecting performance.   Moreover, it is well known that the performance of the k-mer  counting is input-dependent [22]. Hence, it is possible that an NoC  optimized with traffic pattern associated with one input may  perform sub-optimally when used for a different input. However,  it is not desirable for an NoC design to exhibit significant  performance variation across inputs. Fig. 12 shows the NoC  performance variation for different datasets. For this experiment,  we consider the same NoC as in Fig. 11, which was optimized  following the traffic pattern (fij in Eqns. (1)-(4)) of one input  dataset (seen) e.g. Bac. This NoC is then used for executing k-mer  e r u t a r e p m e T M3D 100 Threshold = 85ᵒC 80 ) C TSV ᵒ ( 60 40 One Two Three Four Number of Logic layers (a)  20 15 10 5 0 ) W ( r e w o P c i g o L M3D TSV 8W higher  power budget 0 5 10 15 Memory Power (W) 20 (b)  Fig 10: (a) Maximum on-chip temperatures with varying number of logic layers for k-mer  counting, and (b) Power budget study, in TSV and M3D-based PIM architectures  1 0.8 0.6 0.4 P D E k r o w t e N i p A Gerbil PIM-Counter c a B n c D o r D o c E l a P o r P t a R r t S i b V Fig. 11: Performance of optimized M3D-based  NoC with PIM-Counter, normalized with respect  to Gerbil running on an equivalent platform.   7                                                 NOCS 19  d e z i l a m r o P D E N 1.05 1 0.95 0.9 Api Bac Dcn Dro Eco Pla Pro Rat Str Vib Fig. 12: NoC performance variation for different inputs   counting with other remaining (unseen) nine datasets i.e.  excluding Bac, and so on. We observe from Fig. 12 that the  designed M3D-based SW-NoCs for PIM-Counter shows minimal  (<= 4.5%) performance variation when other datasets are tested  for performance. This happens as PIM-Counter uniformly  distributes traffic among the PEs for all datasets (Fig. 9), which  reduces the input-dependent behavior. Hence, from Fig. 11 and  Fig. 12, it is clear that PIM-Counter enables better NoC design that  outperforms Gerbil and deliver high-performance communication  support for all inputs tested. However, as Gerbil spends more than  85% of the time for memory (and I/O) accesses (Fig. 2), only  improvement  in NoC performance does not capture the  performance gain of the proposed architecture for a large portion  of time Therefore, full system experiments are necessary.   For full-system evaluation, we compare the execution time for the  64-core NoC-enabled 3D-PIM architecture (similar to Fig. 5)  running PIM-Counter (PC + 3D-PIM), with Gerbil executing on an  equivalent 3D architecture connected to a conventional HMC  (GHMC). The cores are equally distributed over four layers and  connected by optimized M3D-enabled SW-NoCs (same ones  considered in Fig. 11) for both the cases. Overall, GHMC includes  both (a) software baseline: Gerbil, and (b) hardware baseline: 2.5D  PIM architecture with HMC similar to [9]. Here, we do not use the  custom FPGA-based PEs proposed in [9] as they are specifically  designed to  implement probabilistic approximate counting  approaches, which is different than exact counting implemented  by both PIM-Counter and Gerbil. However, please note that the  proposed PIM architecture is generic and can incorporate any  type of PEs (e.g. the exact counting, FPGA equivalent of [9])  instead of the x86 cores used here . Fig. 13 shows the full system  runtime comparison between GHMC and PC + 3D-PIM. From Fig.  13, we note that PC + 3D-PIM outperforms GHMC by up to 7.14X  in runtime. The improvement is achieved as PIM-Counter avoids  external I/O and promotes the use of on-chip memory while  multiple layers of logic, optimized NoC and 3D-PIM enable faster  computation, communication and easier memory access.   6 CONCLUSION  Counting k-mers is a memory-intensive task essential in several  bio-informatics applications that process DNA and protein  sequences. Existing software frameworks significantly improve  the processing of k-mer counting. However, without proper  architectural support, software gains cannot be fully realized. In  this work, we proposed an NoC-enabled software/hardware codesign that enables high-performance k-mer counting on a 3DPIM architecture. We show that using M3D integration, it is  possible to design PIM with multiple logic layers and significantly  higher power budget without violating temperature constraints.  As a result, we upgrade the capabilities of traditional PIM  8  B.K.Joardar et al.  PC + 3D-PIM GHMC 8 4 0 p u d e e p S Api Bac Dcn Dro Eco Pla Pro Rat Str Vib Fig. 13: Speed-up in execution time using proposed co-design  framework over Gerbil + conventional HMC   architectures i.e. more computation capability with  lesser  footprint. Overall, the proposed architecture shows up to 7.14X  better execution times compared to a state-of-the-art software  framework executed on an equivalent 3D manycore system.   ACKNOWLEDGMENTS  This work was supported in part by the US National Science  Foundation (NSF) grants CCF-1815467, CNS-1564014, CCF1514269 and US Army Research Office grant W911NF-17-1-0485.  "
Direct-modulated optical networks for interposer systems.,"We present a new interposer-level optical network based on direct-modulated lasers such as vertical-cavity surface-emitting lasers (VCSELs) or transistor lasers (TLs). Our key observation is that, the physics of these lasers is such that they must transmit significantly more power (21x) than is needed by the receiver. We take advantage of this excess optical power to create a new network architecture called Rome, which splits optical signals using passive splitters to allow flexible bandwidth allocation among different transmitter and receiver pairs while imposing minimal power and design costs. Using multi-chip module GPUs (MCM-GPUs) as a case study, we thoroughly evaluate network power and performance, and show that (1) Rome is capable of efficiently scaling up MCM-GPUs with up to 1024 streaming multiprocessors, and (2) Rome outperforms various competing designs in terms of energy efficiency (by up to 4x) and performance (by up to 143%).","Direct-Modulated Optical Networks for Interposer Systems Mohammad Reza Jokar∗ University of Chicago jokar@uchicago.edu Lunkai Zhang*† University of Chicago lunkai.zhang.1984@gmail.com John M. Dallesasse University of Illinois at Urbana–Champaign jdallesa@illinois.edu Frederic T. Chong University of Chicago chong@cs.uchicago.edu Yanjing Li University of Chicago yanjingl@uchicago.edu ABSTRACT We present a new interposer-level optical network based on direct-modulated lasers such as vertical-cavity surfaceemitting lasers (VCSELs) or transistor lasers (TLs). Our key observation is that, the physics of these lasers is such that they must transmit significantly more power (21×) than is needed by the receiver. We take advantage of this excess optical power to create a new network architecture called Rome, which splits optical signals using passive splitters to allow flexible bandwidth allocation among different transmitter and receiver pairs while imposing minimal power and design costs. Using multi-chip module GPUs (MCM-GPUs) as a case study, we thoroughly evaluate network power and performance, and show that (1) Rome is capable of efficiently scaling up MCM-GPUs with up to 1024 streaming multiprocessors, and (2) Rome outperforms various competing designs in terms of energy efficiency (by up to 4×) and performance (by up to 143%). 1 INTRODUCTION Interconnects in all scales are increasingly critical in computer systems. In particular, in interposer systems, chip-tochip interconnects are essential for achieving seamless coupling of all processing and memory elements. ∗ These two authors contributed equally. † Lunkai Zhang is now at Intel. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’19, October 17–18, 2019, New York, NY, USA © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6700-4/19/10. . . $15.00 https://doi.org/10.1145/3313231.3352368 1 In this paper, we focus on interposer-level optical networks that are based on direct-modulated optical links. Directmodulated lasers, such as vertical-cavity surface-emitting lasers (VCSELs) [7, 13, 18] and transistor lasers (TLs) [8, 24], provide a promising path to efficient optical networks, thanks to their high energy efficiency, high data rate, and integration and manufacturing advantages (details in Sec. 2). However, existing optical network architectures have various limitations when they are applied to direct-modulated optical links. For example, the fully-connected point-to-point (FCP2P) network, while is easy to implement and consumes low power, lacks the flexibility to share and allocate bandwidth between different channels, which can lead to large application performance impact. As another example, architectures designed for silicon photonics [15, 16, 25] achieve high performance but impose large power overhead when they are used with direct-modulated optical links (details in Sec. 3). Therefore, investigation of architectures that are optimized for direct-modulated optical links is required. To this end, we present a new direct-modulated optical network, called Rome, which is tailored for direct-modulated optical links in order to meet the high bandwidth, performance, and energy efficiency requirements in interposer systems. This is achieved based on the following key observation: the receiver of a direct-modulated optical link requires substantially less optical power (e.g., 21×) than that generated by the transmitter. We salvage this otherwise-wasted excess optical power to achieve efficient bandwidth sharing by making the following changes to a FC-P2P design: (1) splitting each optical signal generated by a single transmitter into multiple signals that are then delivered to different photodetectors; and (2) allowing each receiver to dynamically switch between multiple photodetectors to receive signals from different transmitters. Rome works well in interposer systems because the number of modules (i.e., active chips, which are interposer network nodes) that can be integrated Table 1: High-Speed Link Technologies (measurement results from industry designs and research prototypes). Link Technology Distance Energy Efficiency Data Rate Applicable to interposer? Long-reach electrical (Intel [4, 9]) 50mm to 1m 1.7 pJ/bit to >3.8 pJ/bit >10Gbps No Short-reach electrical (NVIDIA [17]) 0.54 pJ/bit 20Gbps Yes for small scale SiPh with WDM (Oracle [26, 27]) >50cm 1.8 pJ/bit per λ 25Gbps Yes VCSEL (IBM [18]) >50cm 1 pJ/bit 25Gbps Yes TL (UIUC [24]) >50cm 1 pJ/bit 40Gbps Yes Note: Energy-per-bit is for the whole link except for SiPh, which does not include RX power. ⩽ 4 .5mm on an interposer may be limited (4 - 16; see Sec. 4), and the “free” excess optical power is sufficient to allow signals to be split and delivered to a significant portion (>50%) of all network nodes, which allows the network bandwidth to be flexibly allocated to different source/destination pairs based on traffic patterns to improve network performance. As a result, Rome retains the benefits of FC-P2P (low power and low design complexity) whiles overcomes its major limitation (low performance due to the lack of flexibility to allow bandwidth sharing). As a case study, we apply Rome to Multi-Chip Module GPU (MCM-GPU) systems [2] to evaluate network performance and power. Our results show that Rome can efficiently scale MCM-GPUs with up to 1024 streaming multiprocessors (SMs), which is >16× larger than state-of-the-art commercial GPUs (e.g., NVidia Pascal) and 4× larger than existing research proposals [2]. We also quantitatively compare Rome with different interconnect technologies and architectures. Our results show that, compared to state-of-the-art electrical 2D mesh, Rome reduces network power by up to 62% (and up to 4× with more advanced laser technologies) while improving application performance by up to 143%. Compared to architectures designed for silicon photonics, Rome reduces network power by >2.5× while achieving comparable application performance. The major contributions of this paper are: (1) Present a new direct-modulated optical network, Rome, that leverages the novel observation that excess optical power is available to support flexible bandwidth allocation while imposing minimal costs in interposer systems. (2) Evaluate an example of Rome as applied to a large-scale MCM-GPU system to demonstrate its efficiency and effectiveness, and quantitatively compare it to other interposer network designs. To the best of our knowledge, this is the first detailed study on optical interposer networks, with a focus on direct-modulated optical links. The rest of the paper is organized as follows. Section 2 provides background and related work. Section 3 discusses the details of Rome. Evaluation methodology and results are presented in Sec. 4, followed by conclusions in Sec. 5. 2 BACKGROUND AND RELATED WORK In this section, we discuss various link technologies, and present direct-modulated optical links in details. 2 2.1 High-Speed Link Technologies Table 1 summarizes key technology parameters and characteristics of various high-speed link technologies. Longreach electrical links are widely used in bridge and backplane scales, but are not practical at the interposer level due to large transceiver area [4]. Short-reach electrical links achieve high energy efficiency by limiting the distance of data transmission (e.g., to a few mm’s) [17]. Therefore, a multi-hop topology (e.g., mesh) is the only viable option, making it inefficient to achieve high-bandwidth and lowlatency global communications for large-scale systems. There are several advantages of optical links over electrical ones, including higher energy efficiency, longer transmission distance, and better bandwidth scalability. Within optical link technologies, silicon photonics (SiPh) which supports wavelength-division multiplexing (WDM) has been shown to be efficient in chip-to-chip or even on-chip networks [15, 16, 25]. A unique advantage of SiPh links is that multiple wavelengths (e.g., up to 64 in dense WDM) can be carried by a single waveguide. This allows a SiPh network to achieve high bandwidth without the need for dense waveguides. A disadvantage of SiPh is the low energy efficiency of the external lasers. For example, in Table 1, the off-chip laser alone consumes a major portion of the transmitter energy (1.5pJ/bit out of the total 1.8pJ/bit). Another challenge is that SiPh requires separate modulators (ring resonators) to modulate the always-on signals. The optical properties of ring resonators are highly sensitive to waveguide width, refractive index, modal index, sidewall roughness, and temperature [11]. In this paper, we focus on another promising optical technology: direct-modulated optical links based on VCSELs or TLs. VCSEL-based optical networks are already widely adopted in datacenters. Their efficiency for chip-to-chip or on-chip communications has also been demonstrated [13, 18]. TL [8, 24] is an InGaP/GaAs heterojunction bipolar transistor with the addition of quantum wells for photon generation and optical cavity for coherent light output. Measurement data in actual research prototypes demonstrate that TLs are on par with or superior to state-of-the-art VCSELs (Table 1), but TLs are expected to be more scalable than VCSELs as both technologies scale: the power consumption of TLs and VCSELs is similar (due to similar threshold current and voltage which are tied to material properties), but TLs can potentially reach much higher speed and energy efficiency. Table 2: Excess Optical Power in VCSEL/TL Links. OMA state-of-the-art [18] projection [7] 0.63mW (avg) 5mW receiver sensitivity 0.03mW [20] 0.12mW TX/RX power ratio 21× 41 .5× Figure 1: A Direct-Modulated Optical Link. This is because the modulation bandwidth of TLs is limited by tunneling absorption latency (10fs), which corresponds to a data rate of up to 840Gbps and energy efficiency of up to 20f J/bit [8, 24]. In contrast, the modulation bandwidth of VCSELs is fundamentally limited by the carrier recombination lifetime (200ps). Therefore, the maximum projected VCSEL data rate and energy efficiency are limited to 160Gbps and 100f J/bit, respectively [7]. In general, the advantages of direct-modulated optical networks include: (1) Direct-modulated lasers are typically more energy efficient than always-on lasers. (2) Although VCSELs and TLs can be subject to thermal variations, they can still operate reliably because laser drivers can dynamically adjust the input power strength based on thermal sensor information [18]. (3) Separate modulators (ring resonators) are not needed, which reduces network power and design complexity. (4) VCSEL and TL links are ultra high-speed and highly energy-efficient as shown in Table 1. (5) Both VCSELs and TLs can be integrated with silicon [18, 24]. A disadvantage of VCSEL/TL links is that it is not clear how they can efficiently support WDM. However, we will show in Sec. 4 that there is sufficient space in the interposer to fit all waveguides even without WDM. Therefore, we deem VCSEL/TL links a good fit for interposer-level networks. 2.2 Direct-modulated Optical Links Figure 1 depicts a direct-modulated optical link. On the transmitter side, after serialization, the laser (VCSEL or TL) is modulated by a driver, and generates an optical signal that travels through a waveguide. On the receiver side, the photodetector (PD) converts the optical signal back to electrical, which then passes through the receiver (consisting of TIA, LA, etc.) and deserializer. Complex clock recovery circuits are not likely to be needed at the interposer level [14]. To perform optical switching (so that one source node can reach multiple destination nodes using one output channel), either passive power splitters (which can be used to split one optical signal into multiple ones, either evenly or with different weights) [21] or ring resonators may be used. We opt to perform optical switching using splitters because: (1) they do not impose additional power cost and design complexity; (2) they are easy to manufacture; (3) they function reliably and incur low optical loss (e.g., 0.35dB loss per splitter based on actual measurement data [21]); and (4) given excess optical power (Sec. 2.3), using splitters alone already enables high 3 flexibility and performance in interposer networks. Ring resonators may improve performance (see Sec. 4), but the power and design costs are expected to be higher. 2.3 Excess Optical Power in Direct-Modulated Optical Links The most important and unique observation, which serves as the basis and motivation of Rome, is that the optical power generated by a VCSEL or TL (as measured by optical modulation amplitude, or OMA) is much higher than what is required by the receiver. The reason is fundamental to optical device physics: it is well known that the data rate increases as the OMA increases, with a square root dependence. Therefore, to support high-speed laser operations, a certain high OMA is required (e.g., at least 0.5mW even with aggressive device optimizations [12]). State-of-the-art low-power and high-speed VCSEL and TL prototypes achieve an average OMA of >0.63mW [18, 24]. In the future, a combination of lower threshold current together with improved conversion efficiency will lead to higher OMA (5mW) even at high temperatures of >200◦C (based on simulation results for future VCSELs [7]; similar for future TLs). On the receiver side, low receiver sensitivity (i.e., the optical power required for PDs to sense the signals reliably) of 0.03mW has already been demonstrated [20]. As summarized in Table 2, the ratio between OMA and receiver sensitivity is high: > 21×. Note that, the 0.63mW OMA reported in [18] is the average OMA, which is much lower than the max OMA corresponding to the high-state signal. Thus, the 21× ratio is pessimistic. In the future, with significant improvement on OMA, even if we assume that receiver sensitivity increases linearly as data rate, the ratio would still be very high. In essence, due to fundamental physics, it is not possible to reduce laser OMA to match receiver sensitivity, so excess optical power is available and abundant in direct-modulated optical links. Such “free” optical power enables the Rome architecture, which provides flexible bandwidth sharing and allocation with minimal overhead. 3 THE ROME NETWORK 3.1 Limitations of Existing Optical Network Architectures Optical networks favor single-hop architectures because optical links are known to be efficient for long-distance communications. Also, single-hop architectures scale better to large systems, which requires large bandwidth between any pair of nodes. Thus, we focus our study on single-hop architectures. FC-P2P is a simple single-hop architecture that achieves full connectivity between any node pairs. Figure 2(a) shows a two-node example of FC-P2P. A major limitation of FC-P2P is the lack of flexibility to share and allocate bandwidth. For example, suppose there is a large amount of traffic between SRC #2 and DES #1, creating a hot spot in the link between the two nodes. In this case, even if the link between SRC #2 and DES #2 is idle, it cannot be utilized to alleviate the hot spot. Such inflexibility can considerably limit network performance (results in Sec. 4). We also consider existing single-hop architectures designed for SiPh links, where ring or serpentine-based links are commonly used to connect to all nodes. In such a network topology, global communications can be achieved using various schemes: single-writer, multiple-reader (SWMR), single-reader, multiple-writer (SRMW), and multiple-reader, multiple-writer (MRMW) [15, 16, 25]. If we apply the SWMR scheme [15] to direct-modulated optical links, then each VCSEL/TL link would connect to all network nodes, and the node containing the laser is the writer while all other nodes are readers. There are several limitations of this approach: (1) Ring resonators may be needed to direct optical signals to all destination nodes. If passive splitters are used instead, it would mean that each link must support broadcast operation all the time. Broadcasting requires each VCSEL/TL to output high optical power, which substantially increases power consumption. (2) Arbitration among the readers imposes high control overhead [15]. (3) To achieve the same peak bandwidth as FC-P2P, SWMR require a larger number of receivers: in FC-P2P, each link is connected to one transmitter and one receiver; In SWMR, n − 1 receivers (where n is the number of network nodes) are needed instead, because each link has a dedicated receiver in each destination node. The SRMW scheme [25] suffers from a similar set of limitations as SWMR (the only difference is that, instead of requiring a larger number of receivers as in SWMR, it requires a larger number of transmitters compared to FC-P2P, under the same peak bandwidth requirement). MRMW [16] essentially “combines” SWMR and SRMW, and it follows that the combined limitations of SWMR and SRMW apply. 3.2 The Key Idea of Rome To overcome the limitations of existing architectures, we create the Rome network1 . Rome is based on FC-P2P, but it takes advantage of the key observation that excess optical power is available (Sec. 2.3) to overcome FC-P2P’s inflexibility limitation. In Rome, splitters are used to split each optical signal in FC-P2P into multiple signals connecting to multiple 1 The name comes from “all roads lead to Rome”, because the network enables multiple routes to reach the same destination. Figure 2: FC-P2P vs. Rome. destination nodes, while ensuring that each split signal is still strong enough to be reliably received. Moreover, Rome allows each receiver to dynamically switch among multiple PDs. As shown in Fig. 2(b), each receiver in a destination node is connected to multiple PDs, and each PD is connected to a fast BJT (bipolar junction transistor) switch that is turned on/off dynamically based on information from Rome’s control unit. At any given time, only one switch is connected, forming a link from the corresponding source node to the receiver’s home node. This way, unlike the SWMR design, Rome does not require a large number of receivers. To see how Rome enables flexible bandwidth sharing and allocation, let’s revisit the hot spot example in FC-P2P. As shown in Fig. 2(b), Rome allows DES #1 to dynamically reconfigure the PD switches in both CH#1 & CH#2 to receive data from SRC #2, effectively alleviating the hot spot, because now a single channel may be shared by multiple receivers in different destination nodes. Moreover, different from the SWMR design where the receiver of a channel can be any network node, in Rome there is a limit on the splitting degree, i.e., the number of destination nodes sharing the same data channel, which is constrained by laser OMA, receiver sensitivity, and various sources of optical losses throughout the whole link. Since there is no need to increase laser OMA or use ring resonators to perform broadcast, power overhead and design complexity is minimal. Rome therefore can be thought of a “sweet-spot” between FC-P2P and SWMR, which is capable of achieving balanced network cost and flexibility tradeoffs by varying the splitting degree. 3.3 Physical Implementation of Rome We describe the physical implementation of Rome in the context of MCM-GPU on a passive optical interposer (containing waveguides, splitters, and couplers); but the general idea is applicable to other systems. As an example, Fig. 3 depicts a 9-module MCM-GPU laid out as a 2D matrix, with a splitting degree of 2. Each GPU module contains a local memory partition. All memory partitions as well as I/O interfaces are accessible by all GPU modules through the Rome network, so the entire interposer system behaves as a large monolithic GPU (similar to [2]). The network is partitioned into two 4 is generally considered to be less scalable, it is adequate in the interposer scale, which leads to lower design complexity. The centralized control unit in Rome, called the packet dispatcher, is connected to all network nodes using VCSEL/TL links. It is responsible for packet scheduling, PD switch control, and credit-based flow control. Under the coordination of the packet dispatcher and routers in network nodes, a network packet’s life cycle in Rome consists of four stages: 1. Packet Combining. To amortize scheduling overhead and make efficient use of the high data rate and bandwidth in the optical links, each source router tries to pack multiple network requests with the same destination into one large network packet whenever possible. 2. Packet Registration. The source router registers a network packet by sending a “Registration Request” message to the packet dispatcher (a description of this message is shown in Table 3), when either of the following condition is met: (1) The size of the packet exceeds a size threshold ; (2) the network packet has been waiting for a time period longer than a time threshold. If multiple packets fulfill one of these conditions at the same time, the one with the longest wait time takes priority. These criteria allow Rome to adapt to different traffic patterns while preventing starvation and minimizing scheduling overhead and performance impact. A different policy may be adopted if the network is imposed with other fairness or data criticality requirements. The control information of each registered packet is stored in a FIFO in the packet dispatcher. The number of registered packets for one (source, destination) pair cannot exceed Pair_Token_Limit, which is constrained by the FIFO size. 3. Packet Scheduling. In every clock cycle, for every (source, destination) pair, the packet dispatcher first examines the oldest network packet for which at least one channel that can transfer the packet is unoccupied. Among these “transferable” packets, some of them may share the same source or destination node, so the packet dispatcher avoids any conflict by selecting at most one packet for each (source, destination) pair according to the first-come, first-serve policy. Finally, for each selected packet, the packet dispatcher issues a “Scheduling Request” control message (format specified in Table 3), which contain all the information needed to guarantee correct transmission, to the corresponding source and destination nodes. 4. Packet Transfer. Once a (source, destination) pair receives “Scheduling Request”, the destination router decodes the message and generates appropriate control signals to the BJT switches. The source router transmits the packet at the time slot given by the packet dispatcher. The packet is guaranteed to be transmitted successfully at the scheduled time slot, since the packet dispatcher takes into account the latency for control/data communication and BJT/PD switching (plus some guardbands to eliminate synchronization issues). Figure 3: Rome Implementation on a 9-module MCM-GPU (memory and I/O interfaces are not shown). subnets: one for traffic from SMs to remote memory parti. tions, and the other in the reverse direction. TL and PD array chips are connected to the transceivers in each GPU module using hybrid integration techniques through TSVs (similar to [13]). Light signals produced by VCSELs/TLs are vertically emitted through the bottom of the chip and coupled into the waveguides. Input light signals are coupled into the PDs that are connected to BJT switches. BJT switches receive control signals from the router, and their outputs are OR’ed together to be used as the input to the optical receivers. Waveguides are routed in a long, serpentine-like manner to minimize optical losses due to waveguide crossings (similar to [10, 25]). The width between two neighboring waveguides is 2× the minimal waveguide pitch to allow space for splitting. The power ratio of the two split signals is variable, such that the power of the signal that is fed to the local GPU module is just sufficient to meet the receiver’s sensitivity requirement, which can be estimated statically during the design phase. 3.4 Routing, Arbitration, and Flow Control Routing is straightforward in Rome, because there is only one route for every packet (even though the route is split to reach multiple destinations). However, we need to correctly control the BJT switches in each destination node to avoid conflicts from multiple source nodes. There are two options to implement this arbitration: centralized and distributed. In the distributed approach, the router in each destination node is responsible for making arbitration decisions, and it requires at least one bi-directional control link between every pair of nodes. The number of control links is larger than that for a centralized approach in most cases, because the number of nodes in an interposer system is limited: a 16-node interposer system is already a very aggressive target (details in Sec. 4). Therefore, although a centralized approach 5 Table 3: Control Messages in Rome. Message Type Fields Registration Request valid, DST_ID, packet_id, packet_size Scheduling Request to SRC valid, packet_id, delay, channel_id Scheduling Request to DST valid, packet_size, delay, channel_id Almost-full valid, 1-bit almost-full signal Table 4: MCM-GPU System Configurations (parameters are derived based on [2]). configuration interposer area 4x64 16x16 9x64 16x64 1500mm2 1500mm2 1500mm2 3000mm2 GPU module area 256mm2 64mm2 144mm2 144mm2 network peak bandwidth requirement 2.25TB 2.82TB 6TB 11.25TB projection 7nm-node 7nm-node near-future far-future To support credit-based flow control, a node will send an “Almost-Full” message to the packet dispatcher when its input buffer size drops below a threshold. To ensure that all types of control messages (as summarized in Table 3) can be sent simultaneously, 4 control links are dedicated to each node in each subnet. The latency to transfer one control message is <= 1 cycle for a 1GHz clock and 25Gbps optical links, if all control messages are <= 25 bits. 4 EVALUATION We perform thorough evaluation for four nxm MCM-GPU system configurations (where n is the number of GPU modules, and m is the number of SMs per module) as summarized in Table 4 to demonstrate the efficiency of Rome. 4.1 Power, Area, and Splitting Degree Based on the area constraints and peak bandwidth requirements in each interposer configuration in Table 4, as well as optical link technology parameters summarized in Table 5, we calculate (1) the area required to hold all optical network resources (including waveguides, TLs, and transceivers), (2) network power, and (3) the max splitting degree that can be achieved. The results are shown in Table 6. All required network resources fit under the interposer/GPU module area constraints for the near-term configurations. For the aggressive 16x64 configuration, we expect that with more advanced waveguide techniques [23], the width of the waveguide bundle would be reduced by >2×, which fits under the interposer area constraint. The power of Rome presented in Table 6 is calculated by summing up the power consumption of the following network components: transmitters, receivers, serializer/deserializer (SerDes), BJT switches, and the packet dispatcher. Rome’s network power is already small with today’s technology, and it is expected to improve further as VCSELs and TLs scale. In all configurations, the max splitting degree is 8 for OMA=0.63W and receiver sensitivity=0.03mW (after accounting for optical losses including coupling, propagation, bending, and splitter losses). The splitting degree is the same for 6 Figure 4: Power Comparison of Various Networks. all configurations because propagation loss of S i 3N 4 waveguides is very small 2 . The projected max splitting degree will be even higher for future VCSELs/TLs based on Table 2. For Rome, we compare the power consumption of a centralized control approach (using the packet dispatcher as discussed in Sec. 3) with a distributed approach (assuming minimally that 1 control link is allocated per data channel). The centralized approach consumes less power in the 16x16, 9x64, and 16x64 configurations, and is comparable (consumes 0.2W more power) in the 4x64 configuration. This result confirms that a centralized approach is more efficient in Rome. Figure 4 compares the power consumption of various interposer network technologies and architectures, under the same peak bandwidth constraint. Rome consumes very little additional power (due to package dispatcher overhead) compared to FC-P2P. TL or VCSEL-based SRMW or SWMR schemes consume much higher power than both Rome and FC-P2P because the number of control links and receivers (for SWMR) or transmitters (SRMW) are much higher. Rome outperforms SiPh SWMR by >2.5× for the same reason, and also because Rome’s link-level energy efficiency is higher (1pJ/bit vs. 1.8pJ/bit). Electrical 2D mesh consumes the least power in the 4x64 configuration given today’s technology because the energy efficiency of the short-reach electrical links is very high (0.54pJ/bit). However, Rome achieves lower (up to 62%) power consumption in all other configurations. Moreover, as TLs/VCSELs scale, which improves their energy efficiency and data rate, the power of Rome reduces further, leading to 1.4× - 3.9× power reduction vs. electrical 2D mesh across all configurations. Note that, in contrast to the scalable TL/VCSEL links, further energy efficiency scaling for electrical links is generally considered difficult, if not impossible, at the interposer scale [14, 17]. 2We use S i 3 N4 waveguides instead of Si waveguides commonly used in SiPh architectures because they support a wider range of wavelengths, including 980nm or 850nm which are common VCSEL/TL wavelengths that are transparent to Si. Their propagation loss is much lower, but they must sustain a wide pitch than silicon waveguides due to crosstalk issues. Table 5: Direct-Modulated Link Technology Parameters (based on industry/research prototype results). data rate link energy (no SerDes) SerDes power link power Average VCSEL OMA Receiver sensitivity TX area & power RX area & power 26.3mW∗ 25Gbps [18] 1pJ/b[18] 1.3mW [17] 0.63mW [18] 0.03mW [20] 2128um 2 , 13.4mW [18] 1800um 2 , 5.3mW per PD [20] S i 3 N4 waveguide pitch waveguide layers waveguide propagation loss waveguide coupling loss splitter loss waveguide bending loss projected data rate & link power 100Gbps, 35.2mW∗∗ [7] 5um minimum [19, 21] 2 [19] 0.001dB/cm [5] 3dB [22] 0.35dB [21] 0.009dB [1] ∗ link power = data rate × link energy (no SerDes) + SerDes power. ∗∗ Assume power of TX, RX, and SerDes scales at the same rate as laser. configuration number of links (data + control) 744 + 32 960 + 128 2016 + 72 3840 + 128 4x64 16x16 9x64 16x64 total power: network + packet dispatcher 20.4W + 0.03W 28.6W + 0.46W 54.9W + 0.14W 104.4W + 0.46W Table 6: Rome Network Results. waveguide-bundle width (must be <module height h ) 3.88mm, <h = 16mm 5.44mm, <h = 8mm 10.44mm, <h = 12mm 10.1mm (projected), <h = 12mm total TX+RX area mm2 / % of interposer 7.1mm2 / 0.48% 16.4mm2 / 1.1% 33.6mm2 / 2.3% 64.0mm2 / 2.2% max splitting degree 8 8 8 8 projected total power [7] 6.8W + 0.03W 16.9W + 0.46W 20.3W + 0.14W 33.8W + 0.46W projected max splitting degree 11 11 11 11 Notes: 1. Results are calculated based on parameters in Tables 4 and 5. 2. Packet dispatcher power is obtained from RTL synthesis using Synopsys Design Compiler with a 40nm library, for Pair_Token_Limit=8. Table 7: Simulation Parameters. GPGPU Parameters: frequency 1GHz L1 cache (per SM) / L2 cache 64KB / 32MB Max #warps per SM 64 DRAM bandwidth per SM 16GB/s Rome Parameters (based on physical requirements and sensitivity analysis): Pair_Token_Limit 8 BJT switch time 1 cycle TX/RX latency 4ns link latency 1ns size threshold 64B lifetime threshold 4ns 4.2 Performance Evaluation We conduct detailed architectural simulation using GPGPUsim [3] to evaluate application performance (GPGPU-sim is adopted for interposer simulation by grouping all SMs that belong to the same GPU module into a single “cluster”). We simulate Rome with different splitting degrees (sd). When sd=1, Rome is equivalent to FC-P2P. An Ideal network is Rome with no control overhead and with sd=the total number of nodes. We also simulate the SiPh SWMR and electrical mesh networks for comparison purposes. Table 7 summarizes the simulation parameters. Similar to previous MCM-GPU studies [2], we use memoryintensive applications (i.e., applications whose memory access per kilo instructions is >40 bytes) from Rodinia with the native input set [6]. We are only able to report results obtained from kmeans and pathfinder for the 16x64 configuration because all other applications exhibit limited parallelism. However, the characteristics of these two applications are representative of various parallel applications, and our results still provide meaningful insights. Figure 5 shows the instruction per cycle (IPC) results normalized to FC-P2P for each interposer configuration. In all cases, Rome outperforms the electrical mesh (by 20%-143% on average across different interposer configurations) as expected due to the multi-hop nature in the mesh, and the benefit of Rome increases as the number of network nodes (i.e., GPU modules) increases. The performance of SiPh is similar to Rome with sd=the number of nodes in the network (within 5% of each other), which is also expected because Figure 5: Normalized IPC vs. FC-P2P of Various Interposer Networks. both networks achieve the same bandwidth and the same degree of flexibility in bandwidth allocation. Comparing Rome with different sd’s, for the 4x64 configuration, sd=2 yields the best performance, improving IPC by 13% on average and up to 49% in pathfinder vs. FCP2P. sd=4 actually results in lower IPC than sd=2 because of higher control overhead. For the 16x16 and 9x64 configurations, application performance increases as sd increases. Even though the network can benefit from a higher sd, sd=8 (the max sd achievable today) already fills 77% and 83% of the gap between FC-P2P and the Ideal network for the 16x16 and 9x64 configurations, respectively (in 16x16, sd=16 fills 82% of this gap, which is only slightly higher than sd=8). The benefit of Rome can be similarly observed for the aggressive 16x64 configuration. sd=8 for kmeans achieves very similar performance to sd=16, while pathfinder can benefit further from sd=16. However, sd=8 still improves pathfinder’s IPC by 60% compared to FC-P2P, which fills 68% of the gap between FC-P2P and the Ideal network. If ring resonators are used in 7 the place of splitters to perform signal switching, Rome is expected to achieve the performance of that provided by setting sd=number of network nodes, at the price of additional power/design overhead to incorporate ring resonators, as well as additional scheduling and control overhead. 5 CONCLUSIONS High-performance and energy-efficient networks are essential for enabling future large-scale interposer systems. In this paper, we present Rome, a new network tailored for direct-modulated optical links in the interposer scale. Rome utilizes the key observation that the optical power generated by a laser (TL or VCSEL) is substantially higher than what is needed by the receiver to allow flexible bandwidth allocation by splitting the optical signal (without increasing power consumption or design complexity), which leads to high flexibility and performance with minimal costs. Our detailed comparison of various architectures and link technologies reveals broad perspectives. While electrical networks can be efficient in small-scale systems, they do not scale beyond a few nodes at the interposer level. We quantitatively demonstrate the energy efficiency benefit of optical networks over electrical ones, and this benefit is expected to increase as optical technologies improve, in contrast to electrical links for which further scaling is difficult. 6 ACKNOWLEDGEMENTS We thank Prof. Milton Feng, Prof. Lynford Goddard, and Patrick Su of UIUC for their comments. This work is sponsored in part by NSF award number 1640192 and E2CDA-NRI, a funded center of NRI, a Semiconductor Research Corporation (SRC) program sponsored by NERC and NIST. "
Reinforcement learning based interconnection routing for adaptive traffic optimization.,"Applying Machine Learning (ML) techniques to design and optimize computer architectures is a promising research direction. Optimizing the runtime performance of a Network-on-Chip (NoC) necessitates a continuous learning framework. In this work, we demonstrate the promise of applying reinforcement learning (RL) to optimize NoC runtime performance. We present three RL-based methods for learning optimal routing algorithms. The experimental results show the algorithms can successfully learn a near-optimal solution across different environment states.","Reinforcement Learning based Interconnection Routing for Adaptive Traffic Optimization Sheng-Chun Kao*1 , Chao-Han Huck Yang*1 , Pin-Yu Chen2 , Xiaoli Ma1 , Tushar Krishna1 1Georgia Institute of Technology 2 IBM Watson AI Foundation Group {skao6,huckiyang}@gatech.edu,pin-yu.chen@ibm.com, {xiaoli, tushar}@ece.gatech.edu Abstract Applying Machine Learning (ML) techniques to design and optimize computer architectures is a promising research direction. Optimizing the runtime performance of a Networkon-Chip (NoC) necessitates a continuous learning framework. In this work, we demonstrate the promise of applying reinforcement learning (RL) to optimize NoC runtime performance. We present three RL-based methods for learning optimal routing algorithms. The experimental results show the algorithms can successfully learn a near-optimal solution across different environment states. Keywords Network-on-Chips, Intelligent Physical Systems, Reinforcement Learning, Congestion Control, Scalable Modeling. ACM Reference Format: Sheng-Chun Kao*1 , Chao-Han Huck Yang*1 , Pin-Yu Chen2 , Xiaoli Ma1 , Tushar Krishna1 . 2019. Reinforcement Learning based Interconnection Routing for Adaptive Traffic Optimization. In International Symposium on Networks-on-Chip (NOCS ’19), October 17–18, 2019, New York, NY, USA. ACM, New York, NY, USA, Article 111, 2 pages. https://doi.org/10.1145/3313231.3352369 Figure 1: Proposed NoC reinforcement learning scheme 1 Introduction Researchers have started applying machine learning (ML) algorithms for optimizing the runtime performance of computer systems [1]. Networks-on-chip (NoCs) form the communication backbone of many-core systems; learning traffic behavior * Equal Contribution. Code: github.com/huckiyang/interconnect- routing- gym. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner /author(s). NOCS ’19, October 17–18, 2019, New York, NY, USA © 2019 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-6700-4/19/10. https://doi.org/10.1145/3313231.3352369 and optimizing the latency and bandwidth characteristics of the NoC in response to runtime changes is a promising candidate for applying ML. This work explores opportunities that Reinforcement learning (RL) techniques [2] provide for learning optimal routing algorithms for varying traffic within a NoC. RL techniques work via continuous interactions with an environment to learn the optimal policy. They have demonstrated promising results in robotics [3], playing Atari games, and computer network traffic control [4]. In this work, we study how classical RL algorithms work for NoC routing and develop a framework for applying these RL algorithms to NoCs. We further present an extended OpenAI Gym package for studying RL-based routing control in NoC simulations based on gem5 [5]. Our results show the RL agents were able to learn and pick the optimal routing algorithm for a traffic pattern to maximize a customized network objective such as the routing throughput. 2 RL-based Routing Optimization Overview. We develop a framework to use RL to optimize NoC routing decision. As shown in Fig. 1, in NoC environment, our RL agent keeps records of the current network state and its corresponding reward (throughput), and then suggests an action (a choice of routing algorithms) with the highest expected reward, based on the learned information. Target Task. The goal of our RL agent is to learn an optimal routing algorithm that maximizes throughput for the current application. Defining Utility Function for NoCs. RL works by optimizing actions for a utility or reward function. It treats the problem as a Markov process, which means if we have current state with the learned information, we can decide future action to optimize reward. In our use case for NoCs, we define the utility objective (U1 ) by calculating the throughput: U1 ≡ T hr ouдhput = N umber o f P acke t sr e c e iv e d N umber o f Cycl ese x e c u t i on . (1) Proposed RL Framework. As a central motivation in RL, value function approaches attempt to find a policy that maximizes the return by maintaining a set of estimates of expected reward. We use the designed Utility functions in Equation 1. The RL agent’s (Fig 1) action selection is modeled as a policy (π ): (2) (3) where the return R could be calculated: π : S × A → [0, 1] π (a |s ) = Pr (at = a |st = s ) . ∞ R = γ t Ut , 1 t =0 Figure 2: Reward over training episodes on (a) the Case 1 and (b) the Case 2 with standard deviation; throughput of fixed routing algorithms and RL-selected routing on (c) the Case 1 and (d) the Case 2. Ut is the temporal utility measured at the time t , and γ is the discount factor in the Markov process. The action-value function of such an optimal policy Q π is called the optimal action-value function to attain maximum expectation of R as: Q π (s , a ) = E [R |s , a , π ]. (4) RL Algorithms. We consider three temporal differential approaches: Q-learning, SARSA, and Expected-SARSA. We do not use deep reinforcement learning (DRL) methods owing to a high real-time memory consumption of DRL from previous studies [2] which make them prohibitive. 3 Experimental Methodology Extending OpenAI Gym for Interconnection Routing. OpenAI Gym [3] is a benchmark suite for developing RL algorithms. Consequently, we provide a first scalable environment for fast prototyping new RL-integrated NoCs, called int er conne t -r out inд-дym (icr -дym). Our proposed icr -дym environment includes: • S t at e – gem5 statistics with the injected flits, received flits, • Ac t ion – a set of standard routing algorithms (e.g., xy, oblivand average latency ious north-last, adaptive-north-last, random-adaptive) for • Rewar d – a customized network objective(s) (e.g., latency RL agents to choose from and throughput) of a selected NoC topology • I n f o – Boolean format for thresholding at desired reward Case Study 1: NoCs with Incremental Injection Rate. In this scenario, we use Garnet2.0 [6], an NoC simulator network model inside gem5 [5]. We provide a target topology as an 8-by-8 mesh. We start packet injection at a low rate and then increase the rate as time goes on. Our goal is to optimize the performance by choosing optimal routing algorithms from the action space at each transition of environment state. We set the action space of icr -дym in both two case studies as four choices: random routing, xy routing, oblivious North-last, and adaptive North-last (which uses the number of free virtual channels at the next router as a proxy for choosing the output port). Case Study 2: NoCs with Dynamic Traffic Patterns. In this scenario, we simulate the workload of a data center network. For example, in a Google data center, the primary application could change from mail service to video traffic in the different time frame of the day. Therefore, we simulate this scenario by switching from one network traffic pattern to another. We use seven different synthetic traffic patterns provided by Garnet2.0 in the experiments, e.g., random, transpose, and bit reversed traffic, as shown in Fig. 2 (d). Then our environment is defined as the continuously changing network traffic NoC. We apply RL to optimize the routing algorithm decision at each state transition. 4 Evaluation In Case 1, the reward is defined as throughput. The reward feedback in each episode is shown in Fig. 2 (a). We can observe that the rewards of the three RL algorithms converge and are stable. We examine our learned models by testing their throughput through one episode of an entire state transition which is 0.1 to 0.9, as described in Case 1. We compare the throughput of NoCs guided by our RL agents in different injection rates with the throughput of fixed baseline routing algorithms (e.g., random routing, xy routing), as shown in Fig. 2 (c). For example, the throughput of random routing saturates to near 0 after rate 0.3 because of deadlock. Oblivious North-last avoids deadlock and saturates at higher throughput. As for our RL method, the Sarsa chooses Adaptive North-last from rate 0.1 to 0.4 and oblivious North-last at rate 0.5. However, the QL always makes the optimal choice out of four routing algorithms. In Case 2, we have different traffics patterns (e.g., random, tornado traffics) as our states. we show the result under the injection rate of 0.5 in Fig. 2 (b), and the results under different injection rates can all converge and follow the same trend. The throughput of an entire state transition is in Fig. 2 (d). We could observe all three RL methods deliver near optimal choices across all states. Through theses experiments, we show that our method could serve as a decision agent for the data center facing various workloads. 5 Conclusion We develop and demonstrate a framework to apply RL to act as a continually learning agent that configures the routing algorithms decision in a NoC. We concretely show the effectiveness of policy-based RLs on NoC problems. We hope this work will inspire future extensions to bring more RL algorithms to a wide range of NoC problems for the computer networks community. "
Channel mapping strategies for effective protection switching in fail-operational hard real-time NoCs.,"With Multi Processor System-on-Chips (MPSoC) scaling up to thousands of processing elements, bus-based solutions have been dropped in favor of Network-on-Chips (NoC) as proposed in [2]. However, MPSoCs are yet hesitantly adopted in safety-critical fields, mainly due to the difficulty of ensuring strict isolation between different applications running on a single MPSoC as well as providing communication with Guaranteed Service (GS) to critical applications. This is particularly difficult in the NoC as it constitutes a network of shared resources. Moreover, safety-critical applications require some degree of Fault-Tolerance (FT) to guarantee safe operation at all times.","Channel Mapping Strategies for Effective Protection Switching in Fail-Operational Hard Real-Time NoCs Max Koenen Technical University of Munich Chair of Integrated Systems Munich, Germany max.koenen@tum.de Thomas Wild Technical University of Munich Chair of Integrated Systems Munich, Germany thomas.wild@tum.de Nguyen Anh Vu Doan Technical University of Munich Chair of Integrated Systems Munich, Germany anhvu.doan@tum.de Andreas Herkersdorf Technical University of Munich Chair of Integrated Systems Munich, Germany herkersdorf@tum.de ACM Reference Format: Max Koenen, Nguyen Anh Vu Doan, Thomas Wild, and Andreas Herkersdorf. 2019. Channel Mapping Strategies for Effective Protection Switching in Fail-Operational Hard Real-Time NoCs. In International Symposium on Networks-on-Chip (NOCS ’19), October 17–18, 2019, New York, NY, USA. ACM, New York, NY, USA, 2 pages. https://doi.org/10.1145/3313231.3352372 1 INTRODUCTION With Multi Processor System-on-Chips (MPSoC) scaling up to thousands of processing elements, bus-based solutions have been dropped in favor of Network-on-Chips (NoC) as proposed in [2]. However, MPSoCs are yet hesitantly adopted in safety-critical fields, mainly due to the difficulty of ensuring strict isolation between different applications running on a single MPSoC as well as providing communication with Guaranteed Service (GS) to critical applications. This is particularly difficult in the NoC as it constitutes a network of shared resources. Moreover, safety-critical applications require some degree of Fault-Tolerance (FT) to guarantee safe operation at all times. To overcome these issues, we proposed in [4] a hybrid NoC combining Time-Division-Multiplexing (TDM) circuit-switching, as proposed in [3], and packet-switching. The TDM part is used to provide GS and FT to safety critical applications while packetswitching is used for Best-Effort (BE) traffic. FT is achieved by providing each connection with a backup path and introducing protection switching to the NoC—as known from SONET/SDH [1]. We examined three different versions: 1:n, 1:1, and 1+1 protection. 1:n protection means that several connections share the same backup path. When using 1:1 protection, each connection has its own backup path that is not overlapping with any other path in the NoC (primary or backup). With 1+1 protection, flits are sent over Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’19, October 17–18, 2019, New York, NY, USA © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6700-4/19/10. . . $15.00 https://doi.org/10.1145/3313231.3352372 both paths. We concluded that 1+1 protection is the most suitable candidate to implement FT in a NoC. However, the performance of the NoC, i.e. the achievable resource utilization, is highly dependent on the application scenario and the mapping of the critical channels and applications. In this work, we evaluate different types of mapping for different application scenarios, to develop mapping strategies that maximize the achievable resource utilization while still fulfilling all requirements of all critical applications. 2 EXPERIMENTAL SETUP 2.1 Mapping Strategies In order to evaluate the impact of the TDM tasks mapping on the BE traffic, we defined the following strategies (all to be minimized): • S 1 - total number of reserved slots: minimizing the amount of reserved resources for the critical traffic, that is, minimizing the paths latency for the critical applications. • S 2 - standard deviation (SD) of reserved slots per link: spreading out the critical traffic as much as possible, higher amount of reserved resources but trying to use all links equally. • S 3 - SD of path lengths: keeping all critical paths equally long. • S 4 - SD of number of TDM tasks per row and column: similar amount of critical tasks in each row and column of the system, intuitively beneficial for the X-Y-routed BE traffic. 2.2 System Definition The hard real-time guarantees and FT are given by design of the TDM part of the hybrid NoC. To measure the performance of the hybrid NoC we evaluate the effect the TDM traffic and 1+1 protection have on the BE traffic in the network, i.e. the saturation point of the BE traffic. We did all our simulations on an 8x8 mesh. The BE traffic has a fixed size of 15 flits per packet, uses static X-Y-routing, and the input buffers in the routers for the BE traffic have a size of 16 flits. We simulated mappings with a slot table size of 4 and 8 to evaluate the effect of different slot table sizes. 2.3 Application Scenarios We chose four different application scenarios and compared different mappings with each other. For each scenario we chose a NOCS ’19, October 17–18, 2019, New York, NY, USA Koenen et al. different set of task graphs that we mapped onto the system. The sets differ in number and size of the task graphs as well as their connectivity. All task graphs are directed, acyclic, and have vertices with a degree of up to 5. Each node of the system runs either one task from a critical task graph or a BE process. The critical tasks send data to their subsequent task(s) according to their task graph. The BE tasks send data to each other in a uniform random fashion (i.e. uniform random amongst all nodes running BE processes). In each scenario, the critical applications have a combined flit injection rate of 5%/node (the average injection rate for TDM traffic is 0.05 · 64 = 3.2 flits/cycle) which we considered to be a reasonable amount for a mixed critical system. This injection rate is evenly divided between all critical channels and it refers to the usable data, so the actual injection rate is twice as high (i.e. 10%) because of the 1+1 protection. For the BE traffic we simulated injection rates between 10% and 27.5% in 2.5% steps. Again, the injection rate refers to the equivalent of all nodes injecting that amount of traffic. The different simulated scenarios are defined as follows: • Scenario 1: 4 task graphs, 32 nodes, 33 channels • Scenario 2: 6 task graphs, 27 nodes, 21 channels • Scenario 3: 4 task graphs, 22 nodes, 22 channels • Scenario 4: 2 task graphs, 19 nodes, 18 channels We chose these scenarios representative for different real-world application scenarios to determine whether the mapping objectives yield application specific or application independent results. For each scenario, we created four mappings, denoted M1 , M2 , M3 , M4 , minimizing respectively, each of the objectives S 1 , S 2 , S 3 , S 4 . For scenario 1 (in which exactly half the nodes run critical tasks) we also created a mapping, denoted M5 , in which the whole NoC is virtually split in half (two sections of 8 by 4) and TDM and BE traffic do not overlap at all. This was chosen to evaluate the naive approach of simply dividing the two application domains (critical and non-critical). The goal is to not only evaluate the performance of 1+1 protection under different circumstances but also to identify metrics that determine good mappings. 3 EVALUATION Table 1 shows the results of the simulations with a slot table size of 8. For each mapping, we simulated a reference where there is no TDM traffic and the normal mapping in which each TDM channel has as many reserved slots as are necessary to fulfill the bandwidth requirements. We consider the NoC to have reached saturation when the average latency of BE packets exceeds 250 cycles (from generation until delivery), which is a pessimistic assumption (in general, only some nodes experience a sudden increase in latency). The results show that M2 yields the most consistent results that are independent from the application scenario. Mapping objectives that do not consider the distribution of critical traffic on the links are not suited to produce application-independent mappings. Spreading out the traffic is also beneficial since a single fault will affect fewer channels. Moreover, it is easier to find new alternative paths once a fault has occurred which would allow to return to a fault-tolerant state. Dividing the NoC in different areas only makes sense for very specific application scenarios and is generally not a good idea, especially in case of static X-Y-routing for the packet-switched BE traffic. As an illustration of the evolution of the average BE latency, Table 1: Achievable BE flit injection rates, slot table size 8 Mapping Scenario 1, M1 Scenario 1, M2 Scenario 1, M3 Scenario 1, M4 Scenario 1, M5 Scenario 2, M1 Scenario 2, M2 Scenario 2, M3 Scenario 2, M4 Reference TDM 17.5% 15.0% 20.0% 17.5% 17.5% 12.5% 20.0% <10.0% 17.5% 17.5% 15.0% 15.0% 20.0% 17.5% 22.5% 17.5% 20.0% 15.0% Mapping Scenario 3, M1 Scenario 3, M2 Scenario 3, M3 Scenario 3, M4 Scenario 4, M1 Scenario 4, M2 Scenario 4, M3 Scenario 4, M4 Reference TDM 20.0% 17.5% 22.5% 20.0% 20.0% 17.5% 22.5% 15.0% 22.5% 17.5% 22.5% 20.0% 20.0% 20.0% 22.5% 12.5% Figure 1: Average BE latency - Scenario 1 Fig. 1 shows a comparison between M1 and M2 for Scenario 1. The simulation results of the mappings for a slot table size of 4 show a similar trend as the ones for a slot table size of 8. 4 CONCLUSION AND FUTURE WORK In this work, we presented different channel mapping strategies for 1+1 protection switching in a hybrid fail-operational hard real-time NoC. The results showed that, contrary to an intuitive approach, a good strategy (for the best BE traffic performance and highest total traffic load) is not to minimize the amount of reserved resources but instead to evenly spread the TDM traffic across the entire NoC. As the most important mapping objective, we identified the minimization of the standard deviation of used slots per link. In future work, we plan to automatically generate and optimize the mapping process, and to evaluate strategies for other topologies than a mesh and for BE routing algorithms other than X-Y routing. ACKNOWLEDGMENTS This research is supported by the German BMBF project ARAMiS II with funding ID 01 IS 16025 (https://www.aramis2.com/). "
Approximate nanophotonic interconnects.,"The energy consumption of manycore is dominated by data movement, which calls for energy-efficient and high-bandwidth interconnects. Integrated optics is promising technology to overcome the bandwidth limitations of electrical interconnects. However, it suffers from high power overhead related to low efficiency lasers, which calls for the use of approximate communications for error tolerant applications. In this context, this paper investigates the design of an Optical NoC supporting the transmission of approximate data. For this purpose, the least significant bits of floating point numbers are transmitted with low power optical signals. A transmission model allows estimating the laser power according to the targeted BER and a micro-architecture allows configuring, at run-time, the number of approximated bits and the laser output powers. Simulations results show that, compared to an interconnect involving only robust communications, approximations in the optical transmission lead to up to 42% laser power reduction for image processing application with a limited degradation at the application level.","Approximate Nanophotonic Interconnects Jaechul LEE jaechul.lee@irisa.fr Univ Rennes, Inria, CNRS, IRISA F-22305 Lannion, France Sébastien LE BEUX slebeux@encs.concordia.ca Dept. of Electrical and Computer Engineering Concordia University Montreal, Quebec, Canada ABSTRACT The energy consumption of manycore is dominated by data movement, which calls for energy-efficient and high-bandwidth interconnects. Integrated optics is promising technology to overcome the bandwidth limitations of electrical interconnects. However, it suffers from high power overhead related to low efficiency lasers, which calls for the use of approximate communications for error tolerant applications. In this context, this paper investigates the design of an Optical NoC supporting the transmission of approximate data. For this purpose, the least significant bits of floating point numbers are transmitted with low power optical signals. A transmission model allows estimating the laser power according to the targeted BER and a micro-architecture allows configuring, at run-time, the number of approximated bits and the laser output powers. Simulations results show that, compared to an interconnect involving only robust communications, approximations in the optical transmission lead to up to 42% laser power reduction for image processing application with a limited degradation at the application level. KEYWORDS Optical Network-on-Chip, nanophotonic interconnects, approximate communications, energy efficiency ACM Reference Format: Jaechul LEE, Cédric KILLIAN, Sébastien LE BEUX, and Daniel CHILLET. 2019. Approximate Nanophotonic Interconnects. In International Symposium on Networks-on-Chip (NOCS ’19), October 17–18, 2019, New York, NY, USA. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3313231.3352365 1 INTRODUCTION Multiprocessor System-on-Chip (MPSoC) are evolving towards the integration of hundreds of cores on a single chip. This evolution Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’19, October 17–18, 2019, New York, NY, USA © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6700-4/19/10. . . $15.00 https://doi.org/10.1145/3313231.3352365 Cédric KILLIAN cedric.killian@irisa.fr Univ Rennes, Inria, CNRS, IRISA F-22305 Lannion, France Daniel CHILLET daniel.chillet@irisa.fr Univ Rennes, Inria, CNRS, IRISA F-22305 Lannion, France leads to significant challenges for the design of efficient interconnect due to the ever growing data exchange between processors. Electrical Network-on-Chips (NoCs) are now facing challenges related to the end of the CMOS scaling, as it already costs more energy to move data than to compute them [2]. Nanophotonic interconnects, also called Optical NoC (ONoC), are promising solutions to overcome bandwidth and power limitation. However, their implementation remains challenging due to the low efficiency of the lasers, which are key devices in such interconnects. The laser power consumption is mainly driven by the high signal power required to transmit data at the targeted Bit Error Rate (BER). Approximate computing is emerging as an efficient method to improve energy efficiency and execution speed of embedded computing systems [18]. It relies on a accuracy reduction of the data representations, which allows lowering design constraints and improve performances at the cost of result degradation at the application level. The paradigm has been deployed in numerous systems at both operator and memory levels. Recently, the design of a lower power Electrical NoC [1] has been investigated using approximate computing techniques. For this purpose, data are injected using low voltage supplied buffers. While the results are promising, the approach cannot be directly applied to nanophotonics interconnects, which requires control of the signal power emission directly on the laser side. In this paper, we propose an approximate nanophotonic interconnect. It relies on the transmission of the least significant bits using low power optical signals. The resulting higher error rate allows drastic reduction in the laser power consumption. The most significant bits remain transmitted using high power signals, which allows exploring robustness and energy efficiency trade-offs for on-chip optical interconnects. The method we propose is applied on the transmission of Float Point (FP) numbers in Single Writer Multiple Reader (SWMR) optical channels. SNIPER [5] simulations are carried out for AxBench [19] benchmark applications running on shared-memory architectures. Results show that the proposed method allows reaching up to 42% laser power saving. To the best of our knowledge, this work is the first attempting to implement approximate computing concepts into nanophotonic interconnects. The paper is organized as follow. Section 2 introduces related works and Section 3 presents the method allowing to apply approximate computing concepts in nanophotonic interconnects. Section 4 describes the considered hardware implementation and Section 5 NOCS ’19, October 17–18, 2019, New York, NY, USA Jaechul LEE, Cédric KILLIAN, Sébastien LE BEUX, and Daniel CHILLET details the simulation setup. Results are provided in Section 6 and the last concludes and gives perspective to this work. 2 RELATED WORKS On-chip optical interconnects rely on the propagation of optical signals to transmit the information. The optical signals are emitted by laser sources, which account for a large part of the interconnect power consumption [15]. The laser power consumption depends on the optical power to be emitted, which is estimated according to numerous device level characteristics (e.g. photodetector sensitivity, laser efficiency, waveguide loss, and microring resonator insertion loss) and system level parameters (e.g. number of wavelengths, propagation distance). In order to adapt the laser power to communication requirements, prediction on network traffic has been investigated in [7, 21]. In [16], the availability of the destination is validated before starting the communication. The Bit Error Rate (BER) is a key metric to evaluate the required laser output power, taking into account the device characteristics. Most ONoCs rely on Wavelength Division Multiplexing (WDM), which allows propagating multiple signals simultaneously on a same link. Since the targeted BER is the same for all transmitted signals, all the lasers emit at a same power level. Hence, while such methods lead to homogeneous energy/bit figures, which simplify the design task, they do not allow taking into account the significant of the transmitted information, which we explore in this work. Approximate computing takes advantage of the intrinsic robustness of applications to improve their design energy efficiency, memory footprint and throughput. For instance, approximations allow reducing the hardware complexity of arithmetic circuits such as adder and multiplier [8]. It also contributes to improve power consumption of memories [12] and sensors for IoT applications [14]. In the context of electrical NoCs, approximation techniques have been deployed to reduce the number of transmitted bits [3]. In [1], voltage lowering techniques are applied to lower the robustness of links transmitting the least significant bits of FP numbers. In this work, we investigate the design of energy propositional optical links allowing to transmit the Most and Least Significant Bits at low BER and high BER respectively. To the best of out knowledge, this work is the first addressing the deployment of approximate computing for the design of a nanophotonic interconnects. 3 PROPOSED APPROXIMATE ON-CHIP OPTICAL COMMUNICATION In this work, we aim at lowering the transmission robustness of the signals transporting the least significant bits. By tolerating approximation in the communications, constraints on the laser power are lowered, which contributes to improve the interconnect energy efficiency. In the following, the proposed generic approximate communication scheme for ONoC is first detailed. We then illustrate use case scenarios and we discuss design trade-off exploration opportunities. 3.1 Method Overview The approach aims at maximizing the ONoC energy efficiency while minimizing the impact of the approximation at the application level. For this purpose, we discriminate bits according to their significant: Axmax  Not Ax  (variable)  BPL  Ax  (variable)  Not Ax  (fixed)  BSL  (cid:1)  (cid:1)  (cid:1)  N data-1  (cid:2)(cid:5)(cid:16)(cid:6)(cid:15)(cid:1) (cid:14)(cid:13)(cid:17)(cid:6)(cid:15)(cid:1) Phigh  Plow  0  (cid:18)(cid:3)(cid:1)(cid:1)(cid:1)(cid:1)(cid:19)(cid:23)(cid:1) (cid:4)(cid:1) A X m ax  Ax m ax-B PL  0  Bit   positions  (cid:1)  (cid:18)(cid:9)(cid:24)(cid:23)(cid:1) (cid:18)(cid:9)(cid:19)(cid:23)(cid:1) (cid:1)  (cid:18)(cid:8)(cid:24)(cid:23)(cid:1) (cid:18)(cid:8)(cid:19)(cid:23)(cid:1) (cid:1)  (cid:18)(cid:22)(cid:1) BERnot-ax (low)  BERax  (high)  (cid:18)(cid:16)(cid:8)(cid:7)(cid:12)(cid:5)(cid:10)(cid:1)(cid:1) (cid:20)(cid:12)(cid:11)(cid:21)(cid:1) with:    NW =  Ndata  BSL  Axmax  j =   BSL  Axmax-BPL  i =   BSL  Figure 1: Overview of the proposed method to transmit approximated data in nanophotonic interconnects. most and least significant bits are transmitted under low and high BER respectively. The signals for which high error rate is tolerated will be emitted at a lower power level, thus reducing the laser power consumption. For this purpose, we assume Nd a t a bit-width data to be transmitted in parallel using Nw optical wavelengths emitted by Nw lasers, as illustrated in Figure 1. The number of consecutive bits to be sequentially transmitted at a given wavelength is defined by Bit Stream Length (BSL). The discrimination between the bits that cannot be approximated (i.e. N otAx ) from the bits than can be approximated (i.e. Ax ) is defined by Axma x . In order to introduce flexibility in the communication system, we define Bit Protection Level (BPL). It allows adapting, for a given application, the actual number of bits to be approximated. BPL value ranges from 0 to Axma x . We thus obtain the following three groups of bits: • bits ranging from Nd a t a−1 to Axma x correspond to the Most Significant Bits (MSB). They cannot be approximated (i.e. Not Ax ) and they always require a robust communication. The bits are thus transmitted using high power signal level Ph i дh , which is defined to allow reaching a nearly error free communication at low BER (i.e. BER N o t −a x , typically 10 The transmission of the rest of the bits depends on BPL. • bits ranging from Axma x to Axma x − BP L correspond the flexible part of the bits that are transmitted without error (i.e. at BER N o t −a x ) using Ph i дh laser power. • bits ranging from Axma x − BP L to 0 are the Least significant Bits (LSB), which are approximated (Ax ). For a given application, they are considered as not too sensitive to tolerate errors during their transmission. The laser power level of the corresponding signals is lowered to Pl ow , which leads to BERa x (e.g. 10 −5 ). −12 ). Table 1 summarizes the parameters used in our formulations.       Approximate Nanophotonic Interconnects NOCS ’19, October 17–18, 2019, New York, NY, USA Symbol Nd a t a Axma x Definition Data size Max. number of approximated bits Bit Protection Level 0 ≤ BPL ≤ Axma x Number of Lasers Bit Stream Length BER for non approximated bits BERa x BER for approximated bits Table 1: Parameters for approximated data BERno t −a x BPL NW BSL 3.2 Quality of Result/Energy Efficiency Trade-offs Our approach aims at improving the interconnect energy efficiency by adapting the transmission robustness to the application requirements. The exploration of the robustness/energy trade-off requires to take account of the data type and their communication rate while executing an application. We apply our approximation technique on FP numbers, which are intrinsically resilient to errors [10]. Indeed, the impact of errors introduced in the LSBs of the Mantissa is limited thanks to the normalization. To do so, we first define Axma x at design-time according to the maximum approximation tolerated in the system (e.g. all the benchmark applications). Second, BPL is defined at run-time depending on the requirements specific of a given application; this allows transmitting none/part/all of the remaining bits with approximation, i.e. using signals at Pl ow instead of Ph i дh . The power saving Ps av i nд is formulated as follow: PS av i nд = (Nd a t a − Axm a x + B P L ) × Ph i дh + (Axm a x − B P L ) × Pl ow Nd a t a × Ph i дh (1) Figures 2-a and b illustrate IEEE 754-2008 double-precision and IEEE 754-2008 single-precision FP format examples, for which we assume Axma x = 48 and Axma x = 16 respectively. Regarding the double-precision example, BPL is set to 4: bits 44 to 47 are transmitted at Ph i дh (in addition to bits 48 to 63 that are permanently protected thanks to Axma x ). Since we assume BS L = 4, the 44 unprotected bits are transmitted in parallel using 11 signals (i.e. λ0 to λ10 ) at power Pl ow within 4 clock cycles. For the single-precision example, only the signal at λ0 is emitted at Pl ow since we assume BP L = 8 and BS L = 8 (i.e. each wavelength is responsible for transmitting 8 bits). In addition to the ratio of laser emitting at Pl ow , the actual energy saving depends on i) the BERs targeted for approximated and protected bits (i.e. BERa x and BER N o t −a x ) and ii) the ratio of FP numbers transmitted on the interconnect. Indeed, not all data can be approximated. For instance, instructions and integer numbers are highly sensitive to errors and should be transferred with all signals at Ph i дh , which can be achieved by setting BPL to the maximum value. The ratio of FP numbers are obtained using simulation, as detailed in Section 5. (a)  Not Ax  Axmax=48  BPL=4  Sign Exponent  Ax  Mantissa  63  52  48  47  44  43  31  0  (cid:18)(cid:23)(cid:27)(cid:1) (cid:18)(cid:23)(cid:26)(cid:1) (cid:18)(cid:23)(cid:25)(cid:1) (cid:18)(cid:23)(cid:24)(cid:1) (cid:18)(cid:23)(cid:23)(cid:1) (cid:18)(cid:23)(cid:22)(cid:1) (cid:18)(cid:31)(cid:1) (cid:18)(cid:30)(cid:1) (cid:18)(cid:29)(cid:1) (cid:18)(cid:28)(cid:1) (cid:18)(cid:27)(cid:1) (cid:18)(cid:26)(cid:1) (cid:18)(cid:25)(cid:1) (cid:18)(cid:24)(cid:1) (cid:18)(cid:23)(cid:1) (cid:18)(cid:22)(cid:1) BERnot-ax   BERax  (cid:4)(cid:2)(cid:5)(cid:1) (cid:28)(cid:26)(cid:1) (cid:26)(cid:30)(cid:1) (cid:26)(cid:1) (cid:23)(cid:28)(cid:1) (cid:26)(cid:1) (cid:4)(cid:3)(cid:5)(cid:1) (cid:25)(cid:24)(cid:1) (cid:23)(cid:28)(cid:1) (cid:30)(cid:1) (cid:26)(cid:1) (cid:30)(cid:1) (cid:6)(cid:12)(cid:11)(cid:16)(cid:11)(cid:1) (cid:2)(cid:17)(cid:13)(cid:11)(cid:17)(cid:1) (cid:3)(cid:7)(cid:5)(cid:1) (cid:6)(cid:10)(cid:1) (cid:3)(cid:9)(cid:5)(cid:1) (cid:3)(cid:4)(cid:8)(cid:14)(cid:15)(cid:16)(cid:19)(cid:11)(cid:17)(cid:1) (cid:23)(cid:22)(cid:19)(cid:23)(cid:24)(cid:1) (cid:23)(cid:22)(cid:19)(cid:23)(cid:24)(cid:1) (cid:3)(cid:4)(cid:8)(cid:11)(cid:17)(cid:1) (cid:23)(cid:22)(cid:19)(cid:27)(cid:1) (cid:23)(cid:22)(cid:19)(cid:27)(cid:1) (b)  Axmax=16  Not Ax  BPL=8  Ax  Sign Exponent  Mantissa  31  24  16  15  8 7  0  (cid:18)(cid:25)(cid:1) (cid:18)(cid:24)(cid:1) (cid:18)(cid:23)(cid:1) BERnot-ax   (cid:18)(cid:22)(cid:1) BERax  Figure 2: Example of floating point data approximation: a) IEEE 754-2008 double-precision (DP) format, b) IEEE 7542008 single-precision (SP) format. 4 HARDWARE IMPLEMENTATION AND COMMUNICATION MODEL In the following subsections, we present the hardware architecture allowing to implement the proposed approximate communication scheme. The transmissions of data with and without approximation are detailed. Then, we introduce the communication model allowing to estimate the required laser power according to the targeted BER. 4.1 SWMR Channel Design Without lack of generality, we illustrate in Figure 3 the proposed hardware implementation on a Single Writer Single Reader (SWSR) channel. The implementation on SWMR channels relies on the replication of the hardware blocks related to the reader. The writer (source) is connected to the reader (destination) using waveguides (one in the figure). Each waveguide allows transmitting Nw signals using WDM (λ0 ..λ Nw −1 ). The signals are emitted by on-chip lasers and are combined into waveguides using, for instance, MultiMode Interference (MMI) couplers [6]. In case no communication occurs, the lasers are turned OFF by setting their bias current to zero. This is achieved by controlling the laser drivers through the Optical Link Manager. The design of centralized manager to configure the communication channels is out of the scope of the paper and has been already investigated in [17]. When a communication is initiated, the manager activates the lasers (the lasing effects occur after few ns typically). Then, the data to transmit are serialized and OOK modulation is carried out by MRs on the corresponding optical signals. As defined in the previous Section, the degree of serialization depends on the bit stream length (BS L), which corresponds to ratio between the data bit width (Nd a t a ) and the number of wavelengths (Nw ). We assume that data transmission involve the use of all the signals. The design of the serializers, which has been investigated in [9, 20], is out of the scope of the paper. The modulated signals NOCS ’19, October 17–18, 2019, New York, NY, USA Jaechul LEE, Cédric KILLIAN, Sébastien LE BEUX, and Daniel CHILLET (cid:5)(cid:14)(cid:16)(cid:10)(cid:7)(cid:6)(cid:12)(cid:1)(cid:2)(cid:10)(cid:13)(cid:11)(cid:1)(cid:3)(cid:6)(cid:13)(cid:6)(cid:9)(cid:8)(cid:15)(cid:1) Microring Resonator (MR) at λx  Modulator  Filter  Photo  detector  OFF for data ‘1’  ON for data ‘0’  OFF  ON  Modulator  driver  On-chip   laser at λx  (cid:19)(cid:18)(cid:1) λx(cid:1) (cid:19)(cid:18)(cid:1) Laser  driver  Signal power level wrt. transfer type  BER  P o w e r Error   tolerant data  Sensitive  data  App. requirements /  power budget  Floating Point data  Integers or instructions  Power  ilow  Ilaser   ihigh  Laser Modulation Current  ioff  (cid:19)(cid:22)(cid:1) (cid:19)(cid:23)(cid:1) (cid:19)(cid:4)(cid:21)(cid:23)(cid:1) (cid:1)  Phigh  Plow  Poff  Phigh  Plow  Poff  (cid:19)(cid:22)(cid:1) (cid:19)(cid:23)(cid:1) (cid:19)(cid:4)(cid:21)(cid:23)(cid:1) (cid:1)  No data  (cid:19)(cid:22)(cid:1) (cid:19)(cid:4)(cid:21)(cid:23)(cid:1)(cid:1)  (cid:19)(cid:23)(cid:1) Laser currents are  defined at design time  Source   (cid:1)  Tx  S e r i a i l e z r 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 S e r i a i l e z r S e r i a i l e z r Data to send  (cid:20)(cid:1) 101101  000110 000110  (cid:19)(cid:23)(cid:1) λNw-1(cid:1) (cid:19)(cid:22)(cid:1) (cid:1)  Rx  Destination  Received data  (cid:20)(cid:1) 101101  000110 001100  D e s e r i a i l e z r D e s e r i a i l e z r D e s e r i a i l e z r 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 (cid:19)(cid:4)(cid:17)(cid:21)(cid:23)(cid:1) (cid:19)(cid:23)(cid:1) (cid:19)(cid:22)(cid:1) (cid:1)  Laser Configuration  Driver current depends on the  transmission type (Not Ax or Ax)  Figure 3: Optical link channel allowing the transmission of approximated data. propagate along the waveguide until reaching the destination. At this stage, the signals are ejected from the waveguide using MRs in ON state. They are redirected to photodetectors from which opto-electronic conversions are carried out. The serial streams are then de-serialized back into the original data format. The power at which the lasers emit signals depends on the approximation level of the data to transmit. We distinguish two scenario involving no approximation and approximation, as detailed below: • Robust transmission is used for sensitive data such as instructions and integer numbers. It involves low BER, with BERno t −a x (e.g. 10 −12 ), which is guaranteed by injecting high optical power (Ph i дh ) from the lasers. This results in robust but power hungry communications for all bits. • Approximate transmission is dedicated to data that can be approximated using the technique defined in the previous Section. It involves heterogeneous power level emission for the lasers: MSB and LSB are transmitted under high power Ph i дh and low power Pl ow levels respectively. The manager individually configures the laser drivers according to the selected bit protection level. The two possible power levels (Ph i дh and Pl ow ) are defined by evaluating the SNR on the photodetectors, as detailed in the following. 4.2 Laser Power Model Ph i дh and Pl ow depend on the targeted BER for robust and approximate communications, i.e. BERno t −a x and BERa x respectively. To evaluate the laser power, we use the following transmission models proposed in [9]: S N R = [erfc −1 (1 − 2.BER )]2 (2) S N R = (cid:3) × (O Ps i дna l − O Pc r o s s t a l k ) in (3) where SNR is the Signal to Noise Ratio calculated for an OOK modulation, O Ps i дna l is the optical signal power received by a photodetector and O Pc r o s s t a l k is the worst case crosstalk in the channel. (cid:3) is the photodetector responsitivity (1 A/W in this work), and in is the dark current (4μA). The model allows accurate evaluation of the losses experienced by the signals propagating on SWMR channels. Indeed, the losses depend on the signal wavelength and the MRs transmission. It also allows estimating the crosstalk by considering the distance between signal and MR resonant wavelengths. In our implementation, we assume 6.9dB extinction ratio for the MRs and 1dB/cm waveguide loss. Regarding the lasers, we assume CMOS compatible PCM-VCSELs [4]. 5 EXPERIMENTAL SETUPS 5.1 3D ONoC architecture We assume a 3D integrated circuit composed of an electrical layer and an optical layer implementing computing cores and the nanophotonic interconnect respectively. Figure 4-a illustrates a shared memory architecture example with 4 clusters and 4 cores per cluster. Each cluster K includes a last cache level (L2), shared among the cores in a same cluster, and each core has its own private L1 data (L1d) and instructions (L1i) caches. In this shared memory architecture, the communications on the nanophotonic interconnect are initiated by the L2 and the main memory (DRAM) through the Optical Network Interface (ONI). Each ONI is composed of a single Writer and K Readers to interface with the K + 1 SWMR channels. The transmitted data on the channels are either instructions, cache coherency messages, integer numbers and FP numbers. Approximate communications are only carried out for FP numbers. The bits transmitted at BERa x depend on the selected BPL. When                             Approximate Nanophotonic Interconnects NOCS ’19, October 17–18, 2019, New York, NY, USA (cid:3)(cid:31)(cid:27)(cid:29)(cid:1) (cid:6)(cid:30)(cid:17)(cid:1) (cid:6)(cid:30)(cid:14)(cid:1) (cid:3)(cid:31)(cid:27)(cid:30)(cid:1) (cid:6)(cid:30)(cid:17)(cid:1) (cid:6)(cid:30)(cid:14)(cid:1) (cid:3)(cid:31)(cid:27)(cid:32)(cid:1) (cid:6)(cid:30)(cid:17)(cid:1) (cid:6)(cid:30)(cid:14)(cid:1) (cid:3)(cid:31)(cid:27)(cid:31)(cid:1) (cid:6)(cid:30)(cid:17)(cid:1) (cid:6)(cid:30)(cid:14)(cid:1) (cid:6)(cid:31)(cid:1) (cid:1)  (cid:1)  Cluster4  Cluster 3  Cluster2  (cid:9)(cid:8)(cid:4)(cid:31)(cid:1) DRAM  ONI1  ONI2  ONI3  ONI4  ONI5  Electrical layer  Optical layer  Cluster2  On-chip lasers  Tx  Rx  V S T s (a)  BPL (from OS)  Address Data type  @135135  instruction  @136547  instruction  integer  @135487     (cid:1)     (cid:1)  @216447  float  @267874  float     (cid:1)     (cid:1)  Local L2 Memory   @216447  (cid:7)(cid:11)(cid:24)(cid:1)(cid:12)(cid:19)(cid:14)(cid:1) (cid:21)(cid:16)(cid:20)(cid:25)(cid:20)(cid:14)(cid:15)(cid:25)(cid:15)(cid:13)(cid:25)(cid:20)(cid:23)(cid:24)(cid:1) (cid:1)  (cid:1)  (cid:1)  to ONIs  Modulators  Deserializers  from ONI  (cid:11)(cid:15)(cid:22)(cid:26)(cid:15)(cid:24)(cid:25)(cid:1)(cid:10)(cid:23)(cid:20)(cid:13)(cid:15)(cid:24)(cid:24)(cid:1)(cid:1) (cid:28)(cid:1)(cid:2)(cid:14)(cid:14)(cid:23)(cid:15)(cid:24)(cid:24)(cid:1)(cid:14)(cid:15)(cid:13)(cid:20)(cid:14)(cid:15)(cid:23)(cid:1) Value  ..00110..  ..11011..  ..01011..     (cid:1)  ..00110..  ..01101..     (cid:1)  Search @ in FP  addresses map  (cid:1)  Serializers  (cid:18)(cid:12)(cid:24)(cid:15)(cid:23)(cid:24)(cid:1) (cid:1)  Driver  (cid:1)  (cid:1)  ..00110..  (cid:1)  Laser  configurations  Phigh  Plow  (b)  (cid:3)(cid:5)(cid:27)(cid:6)(cid:1) Core L in Cluster K  Figure 4: Architecture overview: (a) 3D ONoC and (b) SWMR channel management for approximate communications. transmitting others data, BPL is set to the maximum value by the manager. 5.2 Approximation under Cache Coherence Traffic Cache coherency involves traffic between L2 and DRAM that is mainly initiated by cache miss or write-back access [13]. However, the traffic initiated by cache coherence protocols also involves addresses of the data to be transmitted and is thus data type agnostic. To define whether a data can be approximated or not, we use local tables to associate ranges of memory addresses to data types [10]. Table 2: Hardware parameters of the simulated architecture Parameters Cache protocol L1 I / Dcache L2 cache L2 cache line size Value MSI 32 KB 2048 KB 64 Bytes The Figure 4-b illustrates the principle with a cache miss scenario. The L2 on cluster 2 receives a request containing the address of a data to be returned. The address (@216447 in the example) is compared to the address ranges of data to be approximated (FP numbers). In case of matching, the power level of lasers corresponding to the approximated bits is lowered to Pl ow . The approximation level (BPL) is application specific and can be defined at run-time by an Operating System. The manager then grants the SWMR access to the L2 that initiated the request and the data is transmitted to the serializers for signal modulation. In order to reduce the latency and hardware overhead, the whole cache line is transferred: the size of response packets (which can be approximated) is larger than packet request size (which can never be approximated). The considered memory characteristics are given in Table 2. 0 0.5 1 1.5 2 2.5 3 3.5 4 1E-12 1E-11 1E-10 1E-9 1E-8 1E-7 1E-6 1E-5 1E-4 1E-3 1E-2 1E-1 a L s e r o p w e c r n o s u m p t i n o ( m W ) 17 9 5 10-12  SWMR channel size   (k clusters + 1 DRAM)   10-11 10-10 10-9  10-8  10-7  10-6  10-5  10-4  10-3 10-2  10-1  a)  BER  0 0.5 1 1.5 2 1 2 3 4 5 e s a L r o p w e r c n o s u m p i t n o b)  16 24 32 48 64  Number of wavelengths  Figure 5: Laser power requirements according to a) the target BER for 16-wavelengths channels and b) the number of wavelengths for SWMR channels with 4 readers. 5.3 Simulation platform We evaluate the proposed approximation technique on benchmark applications from Approxbench suite [11]. It provides standard quality metric to measure the approximation out of quantifying precision for blackscholes, canneal, sobel, and streamcluster applications. The benchmarks are simulated using SNIPER [5] to extract inter-core and core-to-memory communications traces. We simulate architectures with 16, 32, and 64 cores. We consider 4 cores per cluster (i.e. L = 4), hence the number of clusters are 4, 8, and 16. Each architecture has a shared DRAM, hence resulting in 5, 9, and 17 SWMR channels and ONIs. From the simulation results, we extract the communication traces between L2 caches and the DRAM. This allows evaluating, for each application, the number and the type of data transfer on the optical interconnect. The amount of transmitted FP data gives the number of network packets to be sent using our approximation technique. The power saving is obtained by applying our model of equation 1. 6 RESULTS 6.1 laser Power Estimation We first estimate the laser power consumption according to BER ranging from 10 −1 to 10 −12 . For this purpose, we model SWMR links with 4, 8, and 16 readers and the device characteristics defined in Section 4.2. We assume a 50nm Free Spectral Range (FSR) and 16 wavelengths per channel. As illustrated in Figure 5-a, the required laser power is quasi-proportional to the BER, which is explained by the relatively low crosstalk (approximately 3nm spacing between the MRs resonant wavelengths). Figure 5-b shows the impact of the crosstalk on a 4 readers link by considering 16, 32, and 64 wavelenghts. We obtain quadratic growth of the power with the BER, which would thus lead to significant power saving when using high BER for transmission of approximate data.                 NOCS ’19, October 17–18, 2019, New York, NY, USA Jaechul LEE, Cédric KILLIAN, Sébastien LE BEUX, and Daniel CHILLET 6.2 Benchmark Characterization Figure 6 presents, for each application, the distribution of the transmitted packet type (Instruction, Integer Data, and FP Data) according to the architecture size. (cid:27) (cid:42) (cid:26) (cid:1) (cid:19) (cid:20) (cid:15) (cid:24) (cid:8) (cid:10) (cid:15) (cid:19) (cid:25) (cid:18) (cid:18) (cid:20) (cid:10) (cid:1) (cid:13) (cid:20) (cid:1) (cid:19) (cid:20) (cid:15) (cid:24) (cid:22) (cid:20) (cid:21) (cid:20) (cid:22) (cid:6) (cid:31)(cid:30)(cid:30) (cid:39)(cid:30) (cid:38)(cid:30) (cid:37)(cid:30) (cid:36)(cid:30) (cid:35)(cid:30) (cid:34)(cid:30) (cid:33)(cid:30) (cid:32)(cid:30) (cid:31)(cid:30) (cid:30) (cid:8)(cid:18)(cid:23)(cid:20)(cid:10)(cid:12)(cid:1)(cid:14)(cid:16)(cid:9)(cid:13)(cid:12)(cid:1) (cid:2)(cid:9)(cid:21)(cid:12)(cid:15)(cid:14)(cid:17)(cid:12)(cid:1)(cid:20)(cid:12)(cid:21)(cid:23)(cid:15)(cid:22)(cid:1) (cid:5)(cid:8)(cid:3)(cid:1)(cid:45)(cid:1)(cid:32)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:2)(cid:3)(cid:7)(cid:9)(cid:25)(cid:1)(cid:45)(cid:1)(cid:33)(cid:32)(cid:27)(cid:33)(cid:34)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:2)(cid:6)(cid:4)(cid:1)(cid:45)(cid:1)(cid:35)(cid:34)(cid:1) (cid:37)(cid:32)(cid:1) (cid:36)(cid:32)(cid:1) (cid:35)(cid:32)(cid:1) (cid:34)(cid:32)(cid:1) (cid:33)(cid:32)(cid:1) (cid:1) (cid:29) (cid:44) (cid:28) (cid:1) (cid:17) (cid:18) (cid:14) (cid:22) (cid:10) (cid:23) (cid:11) (cid:12) (cid:20) (cid:1) (cid:20) (cid:12) (cid:24) (cid:18) (cid:19) (cid:1) (cid:20) (cid:12) (cid:21) (cid:9) (cid:4) (cid:31)(cid:36) (cid:33)(cid:32) (cid:3)(cid:8)(cid:19)(cid:19) (cid:12)(cid:8)(cid:17) (cid:36)(cid:34) (cid:31)(cid:36) (cid:36)(cid:34) (cid:33)(cid:32) (cid:2) (cid:17)(cid:8)(cid:10)(cid:16) (cid:23)(cid:10)(cid:14) (cid:20)(cid:17)(cid:12) (cid:4)(cid:6)(cid:1)(cid:11) (cid:8)(cid:24) (cid:8) (cid:5)(cid:19)(cid:24)(cid:1)(cid:11)(cid:8)(cid:24)(cid:8) (cid:31)(cid:36) (cid:33)(cid:32) (cid:7) (cid:24)(cid:22)(cid:12)(cid:8)(cid:18)(cid:10)(cid:17)(cid:25)(cid:23)(cid:24)(cid:12)(cid:22) (cid:5)(cid:19)(cid:23)(cid:24)(cid:22)(cid:25) (cid:10)(cid:24)(cid:15)(cid:20) (cid:19) (cid:36)(cid:34) (cid:31)(cid:36) (cid:36)(cid:34) (cid:33)(cid:32) (cid:7)(cid:20)(cid:9)(cid:12)(cid:17) (cid:32)(cid:1) (cid:33) (cid:32)(cid:27)(cid:33)(cid:1) (cid:33) (cid:32)(cid:27)(cid:34)(cid:1) (cid:33) (cid:32)(cid:27)(cid:35)(cid:1) (cid:33) (cid:32)(cid:27)(cid:36)(cid:1) (cid:33) (cid:32)(cid:27)(cid:37)(cid:1) (cid:33) (cid:32)(cid:27)(cid:38)(cid:1) (cid:2)(cid:3)(cid:7)(cid:1) (cid:33) (cid:32)(cid:27)(cid:39)(cid:1) (cid:33) (cid:32)(cid:27)(cid:40)(cid:1) (cid:33) (cid:32)(cid:27)(cid:41)(cid:1) (cid:33) (cid:32)(cid:27)(cid:33) (cid:32)(cid:1) (cid:33) (cid:32)(cid:27)(cid:33) (cid:33)(cid:1) (cid:34)(cid:32)(cid:1) (cid:34)(cid:36)(cid:1) (cid:34)(cid:40)(cid:1) (cid:33)(cid:38)(cid:1) (cid:33)(cid:34)(cid:1) (cid:2)(cid:6)(cid:4)(cid:1) (cid:32)(cid:1) (cid:36)(cid:1) (cid:40)(cid:1) Figure 6: Benchmarks characterizations. We first observe that ratio of FP numbers strongly depends on the application. For instance, Canneal and Sobel involve 4% and 80% of transmitted FP numbers respectively, which can be explain by the data type each application uses. Interestingly, the results show that the ratio of transmitted packets that can be approximated increases with the number of cores. This is due to the increasing number of shared data induced by parallel execution of the kernel. This characterization allow the designer to target on which type of data approximation will have more impact on power saving. We provide energy saving in the following section for Sobel application by using the proposed approach on the FP numbers which are mostly represented on the interconnect. 6.3 Power Breakdown and Quality of Result In the following, we apply the proposed approach to approximate FP numbers in Sobel benchmark. We consider a 64 cores architecture with 16 SWMR links containing 16 wavelengths each. We use Nd a t a = 64 and we assume Axma x = 32. Power reduction are estimated for BERa x ranging from 10 −1 to 10 −11 and for BPL ranging from 0 to 28. Figure 6 illustrates the power reduction with respect to a transmission of all bits at BERno t −a x = 10 −12 and with BPL=32. We observe that increasing BERa x and decreasing BPL can bring more power gain respectively. Reducing both bit protection level and communication robustness allow reaching up to 42% power reduction. These results demonstrate the effectiveness of the proposed approach to drastically reduce the power consumption. We also compare images processed with and without approximation by computing their Mean Square Error (MSE). The baseline solution leads to M S E = 0 and implies using only robust communication at BERno t −a x and maximum protection level BP L = 32. As it can be seen on the figure, using approximate communication with BERa x = 10 −2 and BP L = 16 leads to acceptable degradation of the processed image (M S E = 4.8E −4 ) as edge detection is still achieved. The resulting technique thus leads to 20% laser power saving without any loss in the detection accuracy. The highest power reduction (42%) is achieved for BERa x = 10 −1 and BP L = 0. However the resulting image degradation is severe (M S E = 4.4E −2 ) and could (cid:5)(cid:8)(cid:3)(cid:1)(cid:45)(cid:1)(cid:36)(cid:26)(cid:40)(cid:3)(cid:1)(cid:36)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:2)(cid:3)(cid:7)(cid:9)(cid:25)(cid:1)(cid:45)(cid:1)(cid:33)(cid:32)(cid:27)(cid:34)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:2)(cid:6)(cid:4)(cid:1)(cid:45)(cid:33)(cid:38)(cid:1) (cid:5)(cid:8)(cid:3)(cid:1)(cid:45)(cid:1)(cid:36)(cid:26)(cid:36)(cid:3)(cid:1)(cid:34)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:2)(cid:3)(cid:7)(cid:9)(cid:25)(cid:1)(cid:45)(cid:1)(cid:33)(cid:32)(cid:27)(cid:33)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:2)(cid:6)(cid:4)(cid:1)(cid:45)(cid:1)(cid:32)(cid:1) Figure 7: Laser power saving and image degradation for Sobel benchmark application. Results are given for BERa x −1 to 10 −11 and for BPL ranging from 0 to 28. ranging from 10 We assume a 64 cores architecture with 17 clusters, Nd a t a = 64, Axma x = 32 and BERno t −a x = 10 −12 . lead to wrong detection, which might not be acceptable in some applications. Interestingly, numerous intermediate solutions exist and are reachable by adapting either the laser modulation current or the protection level. The latter can be done at run-time using our approach, thus leading to true energy proportional nanophotonic interconnects. 7 CONCLUSIONS In this work, we have investigated the design of an Optical NoC supporting the transmission of approximate data. For this purpose, the least significant bits of floating point numbers are transmitted with low power signals. A transmission model allows estimating the laser power according to the targeted BER and a micro-architecture allows configuring, at run-time, the bit protection level and the laser output power. Simulations results show that, compared to an interconnect involving only robust communication, approximations in the optical transmission lead to up to 42% laser power reduction for image processing application and shown the associated application degradation. 8 ACKNOWLEDGEMENT This work benefited from the support of the project SHNoC ANR18-CE25-0006 of the French National Research Agency (ANR). Approximate Nanophotonic Interconnects NOCS ’19, October 17–18, 2019, New York, NY, USA "
Flow mapping and data distribution on mesh-based deep learning accelerator.,"Convolutional neural networks have been proposed as an approach for classifying data corresponding to labeled and unlabeled datasets. The fast-growing data empowers deep learning algorithms to achieve higher accuracy. Numerous trained models have been proposed, which involve complex algorithms and increasing network depth. The main challenges of implementing deep convolutional neural networks are high energy consumption, high on-chip and off-chip bandwidth requirements, and large memory footprint. Different types of on-chip communication networks and traffic distribution methods have been proposed to reduce memory access latency and energy consumption of data movement. This paper proposes a new traffic distribution mechanism on a mesh topology using distributer nodes by considering memory access mechanism in the AlexNet, VggNet, and GoogleNet trained models. We also propose a flow mapping method (FMM) based on dataflow stationary which reduces energy consumption by 8%.","Flow Mapping and Data Distribution on Mesh-based Deep  Learning Accelerator   Special Session Paper  Seyedeh Yasaman Hosseini Mirmahaleh †,   Midia Reshadi   Department of Computer Engineering    Science and Research Branch,   Islamic Azad University    Tehran, Iran     yasaman.hosseini@srbiau.ac.ir,  reshadi@srbiau.ac.ir  Hesam Shabani ,  Xiaochen Guo   Lehigh University     Bethlehem, PA, USA    hes318@lehigh.edu,  xig515@lehigh.edu  Nader Bagherzadeh    Department of   Electrical Engineering   and Computer Science    University of California Irvine     Irvine, CA, USA    nader@uci.edu  ABSTRACT  Convolutional neural networks have been proposed as an approach  for classifying data corresponding to labeled and unlabeled datasets.  The fast-growing data empowers deep learning algorithms to achieve  higher accuracy. Numerous trained models have been proposed,  which involve complex algorithms and increasing network depth.  The main challenges of implementing deep convolutional neural  networks are high energy consumption, high on-chip and off-chip  bandwidth requirements, and large memory footprint. Different  types of on-chip communication networks and traffic distribution  methods have been proposed to reduce memory access latency and  energy consumption of data movement. This paper proposes a new  traffic distribution mechanism on a mesh topology using distributer  nodes by considering memory access mechanism in the AlexNet,  VggNet, and GoogleNet trained models. We also propose a flow  mapping method (FMM) based on dataflow stationary which reduces  energy consumption by 8%.    KEYWORDS  Convolutional Neural Network (CNN), Deep Neural Network (DNN),  Network on Chip (NoC), Flow mapping; Traffic distribution.  ACM Reference format:  Seyedeh Yasaman Hosseini Mirmahaleh, Midia Reshadi, Hesam Shabani,  Xiaochen Guo, and Nader Bagherzadeh. 2019. Flow Mapping and Data  Distribution on Mesh-based Deep Learning Accelerator. In Proceedings  of  International Symposium on Networks-on-Chip, New York, NY, USA, October  17–18, 2019 (NOCS ’19), 8 pages.  https://doi.org/10.1145/3313231.3352378  Permission to make digital or hard copies of all or part of this work  for personal or classroom use is granted without fee provided that  copies are not made or distributed for profit or commercial advantage  and that copies bear this notice and the full citation on the first  page. Copyrights for components of this work owned by others than ACM  must be honored. Abstracting with credit  is permitted. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee. Request permissions  from permissions@acm.org.  NOCS '19, October 17–18, 2019, New York, NY,USA   © 2019 Association for Computing Machinery.   ACM  ISBN  978-1-4503-6700-4/19/10...$15.00  https://doi.org/10.1145/3313231.3352378  1  INTRODUCTION  The development of information technology and various  data types for datasets have rendered data classification  challenging. CNN is able to categorize the data using different  methods, such as labeling, semantic segmentation, and  classification based on input data features of the dataset [4],  [5], [8], [9]. Moreover, CNN is a powerful approach for yield  enhancement in hierarchical labeling [8], [10].  However, the  complexity of deep learning algorithms and increasing  network depth have also been challenging system designs in  the aspect of energy consumption, on-chip network  bandwidth, memory capacity, and off-chip memory traffic [2],  [3], [4].  Different deep  learning accelerators (DLA) have been  proposed to overcome the challenges of CNN and deep neural   networks, which are tailored to the unique features of  different neural networks.   To date, several solutions such as on-chip communication  networks, fast memory access mechanism, parallel and  pipelined units for computational operations, and memories  with different emerging technologies have been proposed to  meet the performance and throughput requirements of CNN  and DNN applications [3], [4].  Nevertheless, the high memory  traffic and energy consumption of DNNs are still problems  that limit the scalability of the applications. Mesh topology  supports good bisection bandwidth as a communication  network which has a positive impact on the performance of a  network on chip (NoC). Mesh is scalable and can support  many processing elements (PEs) without affordable overhead  as compared to a shared bus. A lot of different deadlock-free  routing algorithms have been proposed for a mesh network.  Mesh has a planar structure; thus, traffic could be distributed  to all four directions of up, down, left, and right, while in some  topologies, traffic distribution is restricted to only one or two  directions (e.g., ring)  [26].              In this paper, mesh topology is used for the proposed deep  learning accelerator and the traffic pattern of trained models  are  extracted  based  on  the  processing  element  communications. We propose a flow mapping method and  traffic distribution of trained models on a mesh-based  accelerator with and without distributer nodes based on  memory access mechanism. Multiply-accumulate operations  are performed by PEs and PEs are connected by network-onchip interconnection. This paper presents a technique to  reduce the traffic using distributer nodes for transferring data  among the PEs according to weight reuse. The key idea is  alleviating the traffic based on the distributer nodes for  transferring data between PEs according to input data weight.  Because the flow between PEs plays an important rule in  energy consumption, we propose a FMM based on the impact  of different traffic patterns on energy consumption.   Analyzing the mesh architecture and relationship between  shared bus and the source or destination nodes of various  patterns shows energy consumption improvement. FMM has  a positive impact on the reduction of energy consumption and  total delay based on distributer nodes of a mesh owing to  reducing total hop count.  The remaining of this paper is organized as follows. Section 2  reviews previous studies on deep learning accelerators and  DNNs. Section 3 focuses on the system architecture while  Section 4 analyzes data flow with and without the distributer  nodes. The experimental results are evaluated in Section 5, and  the Section 6 concludes the paper.  2 RELATED WORK  DNN accelerators have been proposed as domain specific  hardware to accelerate the inference phase of DNNs. In the  training phase of the neural networks, GPUs are typically used  due to the mass parallelism and the large amount of data to be  processed. Weights in the filters are updated to optimize for  inference accuracy and the result of this phase is trained  models which will be used by inference accelerators. GPUs  consume higher energy as compared with hardware  accelerators. Inference is typically performance critical and  often processed in mobile and low power devices [38].  Therefore, hardware accelerators are typically used for  inference instead of GPUs.   Weight and neuron pruning, semantic segmentation, and  dropout methods have been proposed for accelerating the  inference phase of CNN and DNN trained models with high  accuracy [6], [7], [11], [34], [36]. Numerous hardware  acceleration methods have been proposed in order to improve  memory access, delay, energy consumption, and bandwidth  requirement for processing CNN and DNN trained models.  Some DLA utilized on-chip memory such as eDRAM and  SRAM for global buffer and local data storage in order to  reduce the number of memory access and the amount of offchip bandwidth requirement [1], [2], [34], [35]. TETRIS [32]  employed 3D-stacked memory technology to achieve high  bandwidth and low energy consumption.  The type of on-chip communication network and dataflow  mapping have significant impact on delay, bandwidth,  memory requirement, and energy consumption of a NoC  based accelerators. Chen et al. [13] proposed an architecture  named Eyeriss that reduced energy consumption, delay, and  bandwidth requirement using a hybrid topology consisting  shared bus and local NoC interconnecting PEs, and row  stationary method used for dataflow mapping. But, Eyeriss  may suffer scalability issue due to utilizing middle shared bus.  Eyeriss.v2 proposed a  flexible architecture based on  bandwidth requirement that increased the complexity of  multiply-accumulate (MAC) units [33]. Maeri provided an  ASIC based flexible design based on binary tree as a  communication platform for CNN, DNN, recurrent neural  network (RNN), and sparse CNN (SCNN) [17].  Tensor processing unit (TPU) [14] is an ASIC based accelerator  for computing tensorflow applications. The ASIC based  accelerators such as TPU and NVDLA [30] are proposed for  CNN and DNN acceleration in order to improve latency and  throughput.  Jouppi et al. achieved 15X-30X  faster  computational speed than contemporary GPU or CPU using 8bit multiplications instead of 16-bit in the inference phase [14].  Searching space between computation graphs of tensorflow  created a chance of finding a mapping solution in order to  speed up processing CNN and DNN models [31].                3 SYSTEM ARCHITECTURE  Mesh topology is employed as a communication network  for distributing traffic of CNN and DNN trained models. Mesh  network connects all PEs and a shared bus is used to connect  the mesh network to a global buffer (GB). Figure 1 shows  proposed DLA architecture which includes a global buffer,  mesh, and shared bus.   The input feature map (ifmap), filter, and partial sum (Psum)  of trained models are stored in the global buffer. The traffic of  a DNN trained model is distributed onto the mesh topology  based on memory access mechanism in either unicast or  multicast mode with the distributer nodes. We analyze energy  consumption, total hop count, and latency of the trained  model traffic distribution based on our flow mapping method.  Mesh topology includes 168 PEs which is determined by  Maestro tool [24]. Maestro calculates the proper number of  PEs based on kernel and ifmap sizes for the computations of a  trained model [18], [24].  We employ two methods including multicast and multicast  with distributer nodes for transferring DNN trained model  traffics.  Multicast method:  Flits are transferred on the mesh from a  source node to multiple destination nodes in a parallel manner  at any moment.  Multicast with distributer nodes method: Flits are first  transferred from the source node to distributor nodes and then  the distributor nodes transfer data to the desired destination  nodes in a multicast manner concurrently.        This section explains AlexNet traffic distribution per layer on  the mesh and we describe the architecture of proposed DLA,  switch, and switch selector structures.    3.1 Overview  We describe our proposed DLA as a set of integrated  components for transferring and distributing data. Figure 1  shows the architecture of DLA while the traffic of AlexNet  convolutional operations is distributed on 12×14 partition of  12×15 2D mesh. The nodes on the left most column are  connected to a shared bus to access the global buffer. The node  structure includes a PE connected to a router with a tiny  switch.   switch multicasts flits on three links based on address lines  (S0, S1 , S2, S3, and S4) and the enable port (EN) that is  demonstrated in Figure 2 (b).  We design a switch selector that consists of two active-low  decoders: R-decoder and C-decoder for selecting switches on  rows and columns of the mesh, as shown in Figure 2 (c). The  switch selector enables row and column switches based on the  address lines (S0, S1, S2, S3, and S4) for transferring flits to  destination nodes. Indeed, all nodes can operate as a  distributer node and location of distributer nodes on mesh is  flexible based on filter, channel, and kernel sizes of CNN and  DNN trained models and distributer nodes are selected by the  switch selector based on filter, channel, and kernel size  parameters.  (a)  (b)  Figure 1: Architecture of the proposed DLA  3.2 Router  We design the router with a simpler switch structure instead  of a crossbar switch because of simple routing algorithm, as  shown in Figure 2 (a). Flits are distributed on the distributer  nodes after transferring from the shared bus by the switch  selector. The routing algorithm is based on multicast XY with  an on/off buffer backpressure flow control mechanism [19],  [20]. The router includes four in/out full-duplex ports and one  injection and ejection port between the PE and router, and  each port has a buffer. A multicast buffer is designed to copy  and send data for multicast routing and it is connected to other  ports via crossbar switch. First, each input flit is transferred on  the multicast buffer and is sent on output links for data  multicasting. The router function is based on two-stage  pipeline and two-stage pipeline includes the buffer write (BW)  and switch allocation (SA). A time elapsed for the route  computation (RC) and switch transmission (ST) is not  significant because the structure of the switches and switch  selector are very simple. Thus, RC and ST stages can work  simultaneously with BW and SA.   We design the switch supports multicast XY routing and flow  mapping of DNN trained models on the mesh. Accordingly,  the switch has a simpler structure and mechanism without  arbiter circuit as compared to the traditional mesh router. The  (c)  Figure 2: A schematic of a router: (a) A router, (b) A switch, and (c)  A switch selector  3.3 Traffic Flow of AlexNet Convolution on Mesh  Topology  This section investigates the traffic flow of AlexNet as an  example of CNN and DNN models. Figures 3(a)–(e) show  destination nodes on a 12×14 2D mesh for CONV1, CONV2,  CONV3, CONV4, and CONV5 for the AlexNet model. The  destination nodes are dedicated to convolution operations and  are denoted as gray-colored nodes on the mesh topology. The  shared-bus transfers data from the GB to destination nodes  based on destination address. The traffic is distributed per  convolution owing to the stride size. For CONV1 traffic  distribution is based on the stride size of 4, as shown in Figure  3(a). The stride size is 2 for CONV2–CONV5. However, we                considered stride size 1 for CONV2–CONV5 to reach the  maximum utilization of the mesh node for AlexNet traffic  distribution; stride size 1 has no negative impact on rectified  linear unit computation [27]. Figure 3 illustrates the location  of destination nodes in order to describe and analyze the  traffic distribution mechanism.  Figure 3: AlexNet traffic distribution on 12×15 2D mesh (a) CONV1  (b) CONV2 (c) CONV3 (d) CONV4 (e) CONV5  4 FLOW MAPPING BASED ON DISTRIBUTER NODES  ON A MESH TOPOLOGY  Each trained model traffic can be distributed over a  topology for row, weight, and output stationary dataflow  execution models. Each will affect energy consumption in  different manners. This section investigates different dataflow  approaches based on data placement-stationary in previous  papers and the proposed dataflow approach. To describe  traffic distribution of CNN and DNN trained models, we use  AlexNet as an example of traffic distribution on the mesh  based on row-node stationary and memory access mechanism.  AlexNet is a state-of-the-art eight-layer trained CNN model  for visual recognition. The eight layers of the AlexNet model  consist five convolutional layers and three fully-connected  layers [11].   4.1 Influence of Dataflow on energy consumption   In CNN and DNN algorithms synaptic weights are multiplied  with different subset of ifmap values [16]. There is a tradeoff  between reusing the weights to minimize data movement or  duplicating the weights to maximize parallelism. To minimize  data movement, the weights are locally reused (LR). To  maximize parallelism, the weights are duplicated and hence  not locally reused (NLR).  NLR synaptic weights: Lu et al. [1] and Jouppi et al. [16]  employed the NLR approach for transferring data in order to  increase processing speed of their proposed architecture.   Dataflow based on LR approaches are categorized as follows:   Weight stationary (WS): Weight elements are received from  the GB and broadcasted to PEs and after fixing in each PE,  convolution calculation is performed between fixed weight in  each PE and ifmap elements broadcasted from GB onto PEs  [5], [12].  Output stationary (OS): In output-stationary DLA, outputs or  both weights and input activations are mapped to PEs from  GB. The Psum results are sent to the GB after finishing local  computation [2], [12], [15].   Row stationary (RS): The ifmap and filter are transferred from  the GB to PE units horizontally, whereas Psums are  accumulated vertically by a multiply-accumulate (MAC)  operation of PEs, and accumulated Psums are transferred to  the GB [13].   Eyeriss proposed a row stationary approach for computing  and transferring data between GB and PE units, and the  energy consumption was analyzed based on dataflow.  Analyzing dataflow in row, weights, NLR, and output  stationaries illustrates the maximum improvement of row  stationary compared with other dataflows [13].  Row-node stationary (RNS): We propose row-node stationary  (RNS) dataflow as a state-of-the-art approach for traffic  distribution of DNN trained models based on flow mapping  and memory access mechanism. An accelerator can transfer  data on sets of nodes based on RNS dataflow in the vertical  and horizontal directions using distributer nodes in parallel.  Figure 4 shows the RNS method for transferring ifmap, filter,  and Psum data on a set of nodes.  Figure 4:  Dataflow on a set of nodes based on row-node stationary: (a) A  row of filter weights is reused and distributed in vertical and horizontal  directions based on the location of distributer node, (b) A row of ifmap  values is reused and distributed in vertical and horizontal directions based  on the location of distributer node, (c) A row of Psums is accumulated  vertically.              4.2 Flow Mapping based on Distributer Node   This paper proposes a RNS-based flow mapping algorithm,  which uses distributer nodes for DNN trained models. This  section demonstrates traffic distribution using distributer  nodes considering memory access mechanism and the location  of the distributer nodes. The flow mapping method is  investigated due to energy consumption and it is based on  multicast distribution.  64-bit values of ifmap and filter matrices are transferred to PEs  for convolutional operations due  to memory access  mechanism and interconnection between the shared bus and  a source or a destination node. Traffic distribution is based on  the distributer nodes location on the mesh. We consider the  minimum number of hop count between destination nodes for  reducing energy consumption induced by accumulating  Psums. Figures 5(a) and (b) show traffic distribution based on  memory access mechanism and distributer nodes for CONV1  and CONV2 of AlexNet model in 12×15 2D mesh, respectively.   The distributer nodes are dedicated to transfer data to the  adjacent nodes and are denoted as red-color nodes on the  mesh. Yellow-color nodes are distributer nodes and  destination nodes on 12×14 partition; but, red-color nodes are  only destination nodes for distributing AlexNet traffic on  12×14 partition. The connected pointers to distributer nodes  show adjacent destination nodes for receiving data from  distributer nodes.   Figures 5(c) and (d) show traffic distribution without  distributer nodes based on memory access mechanism for  CONV1 and CONV2 on 12×15 2D mesh, respectively. We  demonstrate the reduced hop count of CONV1 and CONV2 as  an example of five convolution layers of AlexNet trained  model using distributer nodes, as shown in Figures 5 (a) and  (b). The location of destination nodes is based on stride and  ifmap sizes of each convolutional layer of AlexNet. The  elements of ifmap and filter matrices are distributed on 12×14  partition using connected nodes to a shared bus, as shown in  Figure 5. Obtained Psum is transferred to GB using nodes  which are connected to the shared bus. Partitioning on 12×14  mesh is based on the kernel and stride sizes [11]. Kernel and  stride sizes are 11×55 and 4 for CONV1, respectively, in the  trained AlexNet model and kernel and stride sizes are 5×27  and 1 for CONV2, respectively. Finally, kernel and stride sizes  are 3×13 and 1 for CONV3-CONV5, respectively [11].  We employ partitioning on the mesh based on kernel size of  convolutional due to the relationship between ifmap and filter  sizes which multiply operations determine the kernel size in  each layer. AlexNet traffic is distributed on mesh as an  example of CNN model whereas the kernel size of CONV1 and  CONV2 is more than 12×14 mesh. Nevertheless, we map the  flow of CONV1 and CONV2 on two 11×7, one 5×13, and one  5×14 partitions, respectively. The traffic of CONV1 is mapped  four times on two 11×7 partitions while we map traffic of  CONV2 one time on 5×13 and 5×14 partitions based on kernel  sizes of  CONV1 and CONV2. Figure 5: AlexNet traffic distribution on 12×15 2D mesh: (a) and (b)  with distributer node; (c) and (d) without distributer node  5 EXPERIMENTAL RESULT  The traffic of the DNN trained models is distributed on  12×15 2D mesh based on RNS flow mapping method. We  evaluate our proposed mapping based on AlexNet, VGG-16,  and GoogleNet traffics with and without distributer nodes.  This section analyzes the result of traffic distribution of the  proposed method compared to other methods. Experiments  include an estimation of energy consumption and total delay  of the AlexNet traffic distribution on the mesh and Maeri  topologies. Area consumption is obtained for the mesh, Maeri,  and Eyeriss  topologies. We develop a cycle-accurate  simulation tool based on SystemC inspired by the Noxim tool  [25], [21], [29] and we obtain the total delay and energy  consumption of  mapping trained model flows onto the mesh  and Maeri. Energy consumption is measured based on Noxim  core energy model and NoC interconnection is synthesized by  the VC707 of Xilinx family [23], [27]. Area consumption of the  mesh, Maeri, and Eyeriss switches are obtained by the VC707  device in Xilinx Vivado tool.    We analyze AlexNet and VGG-16 traffics by getting an  activity  log from caffe [22]. The experimental result  demonstrates total runtime, and average utilization for  AlexNet, VGG-16, and first nine convolutions of GoogleNet.  We also achieve GoogleNet traffic by getting an activity log  from Tensorflow library [28].  5.1 Delay, Area and Energy Consumption   Maeri achieved impressive effect on improving performance  of CNN, DNN, and RNN compared to other DLAs using a  binary tree topology and high bandwidth external links. Maeri        employed 64-bit buffers in distributer tree and multipliers  switches for store and forward values of ifmap and weight  matrices nevertheless MAERI improved area consumption  compared with Eyeriss and micro switch arrays [12], [17].  AlexNet traffic is distributed on Maeri topology for comparing  the simulation results of Maeri and 12×15 2D mesh with and  without distributer nodes [17]. The simulation results show  decreasing energy consumption and total delay for 12×15 2D  mesh with distributer node by approximately 43.66% and  0.59% compared with Maeri, respectively, as shown in Figure  6.  Our DLA achieves significant reduction in energy and area  consumption compared with Maeri due to simple router  structure with less buffers nevertheless the experimental  results only demonstrate area consumption of Eyeriss  including multicast controllers. Although, area consumption  of Eyeriss will be higher than Maeri while we involve scratch  pad buffers of Eyeriss in synthesis process [17]. We propose a  simple structure mesh topology whereas FMM method has no  dependency to the number of buffers so it causes using less  number of buffers in this design. Experimental result  demonstrates reducing area consumption comparing with  Maeri in Figure 7.  (a)   (b)  Figure 6: Comparing total energy and total delay of 12×15 2D mesh with  distributer nodes, 12×15 2D mesh without distributer nodes and Maeri: (a)  Comparing total energy of 12×15 2D mesh with distributer nodes, 12×15 2D  mesh without distributer nodes and Maeri, (b) Comparing total delay of  12×15 2D mesh with distributer nodes, 12×15 2D mesh without distributer  nodes and Maeri  The traffic of trained models is distributed on mesh nodes in  the left, right, up, and down directions using distributer nodes  that multi-side traffic distribution reduces total delay and  energy consumption compared to Maeri. Indeed, Maeri  employed more buffers than our proposed DLA for supporting  weights reusability and RNN in order to retrain data.  Figure 7 shows that area consumption is decreased based on  LUT for 12×15 2D mesh with distributer nodes by  approximately 93.56% as compared to Maeri. AlexNet flows  were mapped on Maeri topology for five convolutional layers  of AlexNet and Maeri had better  latency and area  consumption than Eyeriss, systolic array, and micro-switches  array accelerators [12], [13], [16], [17].   FPGA LUT Figure 7: Comparing switch area consumption of 12×15 2D mesh with  distributer nodes, 168 switches of Eyeriss and 64 multiplier switches of  Maeri  We improve memory access and experiments demonstrate  reducing memory access approximately 62.5% compared to  using no distributer nodes in AlexNet traffic on 12×15 mesh  shown in Figure 8.  Figure 8: Comparing memory access of 12×15 2D mesh with distributer  nodes and without using distributer nodes for AlexNet traffic distribution   based on cycles for writing and read memory  5.2 Total Runtime   We propose row-node stationary (RN) for flow mapping on  mesh that distributes the traffic of CNN and DNN trained  models based on RN stationary. This subsection analyzes and  compares RN with other dataflows of DLAs. We obtain total  runtime of CONV1 and CONV11 traffic distribution of VGG16 for RN using our SystemC-based simulator. Although, the  total runtime of various dataflows is evaluated using Maestro  tool [18], [24].  Table 1 demonstrates total runtime for CONV1 and CONV11  of VGG-16, respectively. The simulation results illustrate  reducing total runtime for RN stationary by approximately  99% compared with weight stationary (WS) dataflow in  CONV1 and CONV11 [18].   0.00E+00 5.00E-06 1.00E-05 1.50E-05 2.00E-05 2.50E-05 3.00E-05 12×15 mesh without distributer node  12×15 mesh with distributer node Maeri T o t a l e n y g r e ( J ) Total Energy  12×15 mesh without distributer node  12×15 mesh with distributer node Maeri 4600 4620 4640 4660 4680 4700 4720 12×15 mesh without distributer node  12×15 mesh with distributer node Maeri T o t a l d e l y a ( C c y l e ) Total Delay 12×15 mesh without distributer node  12×15 mesh with distributer node Maeri 0.00E+00 1.00E+03 2.00E+03 3.00E+03 4.00E+03 5.00E+03 6.00E+03 7.00E+03 8.00E+03 9.00E+03 Eyeriss Maeri Mesh N u m b r e o f T U L s Eyeriss Maeri Mesh 0 50 100 150 200 250 300 350 12×15 mesh without distributer node  12×15 mesh with distributer node M e m y r o s s e c c a ( C c y l ) s e Memory access                                    Table 1. Total run time comparing between various dataflows with 168 PEs  for CONV1 and CONV11 of VGG-16  CONV  Dataflow  Total runtime  (Cycle)  1  1  1  1  1  1  11  11  11  11  11  11  RN  NLR  Ws  Shi  DLA  RS  RN  NLR  Ws  Shi  DLA  RS  17034  501258240  25961600  249446400  1157409792  164204544  17722  360316928  217317376  2020081664  673876224  830472192  5.3 Computing Runtime and Average Utilization  To investigate the performance of CNN and DNN trained  models, we evaluate compute runtime and average utilization  of systolic array, TPU, Eyeriss, and our proposed DLA for  AlexNet and first nine convolutions of GoogleNet. Compute  runtime of Eyeriss, systolic array, and TPU is investigated  using Scale-sim as a Python-based cycle-accurate tool [37],  [39]. Although, we obtain compute runtime of mesh by our  simulator. This subsection demonstrates the simulation results  of systolic array, TPU, Eyeriss, and our DLA for AlexNet and  GoogleNet trained models. Indeed, we analyze CNN and DNN  traffic distributions on various topologies in order to estimate  compute runtime and average utilization.  Table 2 demonstrates the compute runtime and average  utilization for AlexNet and GoogleNet trained models. We  illustrate reduction of the compute runtime and average  utilization of mesh by approximately 95.4% and 10.64%  compared with the systolic array for AlexNet trained model.  The experiments demonstrate improvement of compute  runtime and average utilization of our proposed DLA by  approximately 30.65 % and 18.75% compared with TPU for first  nine-convolutions of GoogleNet, respectively.   We achieve an improvement of compute runtime and average  utilization by employing distributer node, multicast traffic  distribution in left, right, up, and down directions of mesh  nodes using distributer nodes.  Table2. Average utilization and run time comparison between  various topologies for AlexNet and GoogleNet traffic distribution  Array  size  Compute  runtime  (Cycle)  Average  utilization  (%)  GoogleNet  Systolic array  256×256  297163  Trained  model  AlexNet  AlexNet  Topology  Proposed mesh  based DLA  TPU  AlexNet  Systolic array  AlexNet  GoogleNet  GoogleNet  Eyeriss  Proposed mesh  based DLA   TPU  12×14  113352  256×256  10026200  32×32  12×14  12×14  2504183  16377164  180182  256×256  259827  88.57  96.25  99.12  98.05  84.52  68.67  68.67  To investigate the performance of our DLA, we estimate total  runtime that is analyzed the delay of AlexNet, VGG-16, and  GoogleNet traffic distributions on the mesh, as shown in  Figure 9. We consider the delay of the first-nine convolutions  of GoogleNet trained model.  Total Runtime ) e l c y C ( e m i t n u r l a t o T 3.50E+05 3.00E+05 2.50E+05 2.00E+05 1.50E+05 1.00E+05 5.00E+04 0.00E+00 AlexNet  VGG-16  GoogleNet Trained models Figure 9: Total runtime of traffic distribution of AlexNet, VGG-16,  and GoogleNet on the mesh  Bandwidth requirement and memory access are challenging  faced by increasing complexity and depth of CNN and DNN  trained models. Thus, we obtain maximum bandwidth  requirement by Scale-sim tool for our proposed DLA  compared with other topologies which is shown in Table 3  [37], [39].  Table 3 demonstrates bandwidth requirement for various  topologies based on AlexNet, VGG-16, and GoogleNet traffic  distributions. We improve bandwidth requirement for our  proposed DLA by approximately 97.7% and 92.1% compared  with TPU and Eyeriss, respectively. Experiments also  demonstrate that bandwidth requirement is improved for  mesh by approximately 98.17 and 91.1% compared with TPU  and Eyeriss for VGG-16 traffic distribution, respectively. Our  DLA requires less bandwidth compared with other topologies  owing to high bisection bandwidth , flow mapping method,  and RNS-based traffic distribution.    Table3. Bandwidth requirement comparing between various  topologies for AlexNet, GoogleNet and VGG-16 traffic distributions  Trained  model  GoogleNet  GoogleNet  GoogleNet  AlexNet  AlexNet  AlexNet  AlexNet  VGG-16  VGG-16  VGG-16  VGG-16  Topology  Proposed mesh  based DLA  TPU  Systolic array  Proposed mesh  based DLA  TPU  Systolic array  Eyeriss  Proposed mesh  based DLA  TPU  Systolic array  Eyeriss  Array  size  Bandwidth requirement  (Byte/Cycle)  12×14  256×256  256×256  12×14  256×256  256×256  12×14  12×14  256×256  256×256  12×14  0.08  3.62  49.71  0.08  3.14  3.14  1.02  0.08  4.38  12.108  0.9                [15] Bert Moons, and Marian Verhelst, A 0.3-2.6 UPS/W Precision-Scalable  Processor for Real-Time Large-Scale ConvNets.  Symposium (VLSI), 2016.  [16] N P. Jouppi et al., In-Datacenter Performance Analysis of a Tensor  Processing Unit.  Conference (ISCA), 2017.  [17] Hyoukjun Kwon, Joel S. Emer, and Tushar Krishna, MAERI: Enabling  Flexible Dataflow Mapping over DNN Accelerators via Reconfigurable  Interconnects.  Conference (ASPLOS’18), 2018.  [18] Hyoukjun Kwon, Michael Pellauer, and Tushar Krishna, MAESTRO: An  Open-source Infrastructure for Modeling Dataflows within Deep Learning  Accelerators.  Conference (ArXiv), 2018.  [19] Masoumeh Ebrahimi, Masoud Daneshtalab, Pasi Liljeberg, and Hannu  Tenhunen, HAMUM-A novel routing protocol for unicast and multicast traffic  in MPSoCs.  Conference (PDP), 2010.   [20] Masoud Daneshtalab, Masoumeh Ebrahimi, Siamak Mohammadi, and Ali  Afzali-Kusha, Low-distance path-based multicast routing algorithm for  network-on-chips.  Journal (IET), 2009.  [21] https://github.com/davidepatti/noxim  [22] http://caffe.berkeleyvision.org/  [23] https://www.xilinx.com/products/design-tools/vivado.html  [24] http://synergy.ece.gatech.edu/tools/maestro/  [25] Vincenzo Catania, Andrea Mineo, Maurizio Palesi, Davide Patti, and  Salvatore Monteleone, Cycle-Accurate Network on Chip Simulation with  Noxim.  Journal (TOMACS), 2016.  [26] Natalie Enright Jerger, Tushar Kishna, and Li-Shuan Peh, On-Chip  Networks: Second Edition 2nd: Morgan & Claypool Publishers.  Book, 2017.  DOI: https://doi.org/10.2200/S00772ED1V01Y201704CAC040  [27] Hyoukjun Kwon, and Tushar Krishna, OpenSMART: Single-Cycle Multihop NoC Generator in BSV and Chisel.  Conference (ISPASS), 2017.  [28] https://www.tensorflow.org/  [29] Kun-Chih, Jimmy Chen, and Ting-Yi Wang, NN-Noxim: High-Level  Cycle-Accurate NoC-based Neural Networks Simulator.   Conference  (NOCARC), 2018.  [30] “Nvdla deep learning accelerator.” http://nvdla.org, 2017.  [31] Ahmad Albaqsami, Maryam S. Hosseini, and Nader Bagherzadeh, HTFMPR: A Heterogeneous TensorFlow Mapper Targeting Performance using  Genetic Algorithms and Gradient Boosting Regressors.  Conference (DATE),  2018.  [32] Mingyu Gao, Jing Pu, Xuan Yang, Mark Horowitz, and Christos  Kozyrakis, TETRIS: Scalable and Efficient Neural Network Acceleration with  3D Memory.  Conference (ASPLOS), 2017.  [33] Yu-Hsin Chen, Joel S. Emer, and Vivienne Sze, Eyeriss v2: A Flexible and  High-Performance Accelerator for Emerging Deep Neural Networks.  Journal  (ArXiv), 2018.  [34] Renzo Andri, Lukas Cavigelli, Davide Rossiy, and Luca Benini,  Hyperdrive: A Systolically Scalable Binary-Weight CNN Inference Engine for  mW IoT End-Nodes.  Conference (ISVLSI), 2018.  [35] Fabian Schuiki, Michael Schaffner, Frank K. Gürkaynak, and Luca Benini,  A Scalable Near-Memory Architecture for Training Deep Neural Networks on  Large In-Memory Datasets.  Journal (ITC), 2019.  [36] Sina Shahhosseini, Ahmad Albaqsami, Masoomeh Jasemi, and Nader  Bagherzadeh, Partition Pruning: Parallelization-Aware Pruning for Deep  Neural Networks.  Conference (ArXiv), 2019.  [37] Ananda Samajdar, Yuhao Zhu, Paul Whatmough, Matthew Mattina, and  Tushar Krishna, SCALE-Sim: Systolic CNN Accelerator Simulator.   Conference (ASPLOS’18), 2018.  [38] Maohua Zhu, Liu Liu, Chao Wang, and Yuan Xie, CNNLab: a novel  parallel framework for neural networks using gpu and fpga-a practical study  with trade-off analysis, arXiv preprint arXiv: 1606.06234, 2016.  [39] https://github.com/ARM-software/SCALE-Sim  6 CONCLUSION  In this paper, we propose deep learning accelerator for  improving memory access, energy consumption, and memory  requirement. Memory access and energy consumption of CNN  trained models, such as AlexNet remain a challenge. This  paper proposes an FMM based on the trained model traffic  distribution on a mesh topology to solve problems with  reducing memory access and energy consumption. We  provide traffic distribution approach with distributer nodes  based on memory access mechanism in order to reduce the  energy consumption of mesh topology. We evaluate the total  energy and delay of AlexNet, VGG-16, and GoogleNet traffic  distributions on a 12×15 mesh. The experimental results  demonstrate a reduction of total energy and delay by  approximately 8% and 1.4%, respectively, by decreasing the  total hop count. FMM reduced the total energy and delay with  the distributer nodes compared with the pattern without the  distributer nodes. Multicast traffic distribution in multi-side  with the distributer nodes decreases total energy and flow on  the mesh.  ACKNOWLEDGMENT  We thank the Synergy lab team from Georgia Institute of  Technology for responding our questions and providing more  information about the Maeri project and their kind help in  compiling and using Maestro and Scale-sim simulators.  "
3D NoCs with active interposer for multi-die systems.,"Advances in interconnect technologies for system-in-package manufacturing have re-introduced multi-chip module (MCM) architectures as an alternative to the current monolithic approach. MCMs or multi-die systems implement multiple smaller chiplets in a single package. These MCMs are connected through various package interconnect technologies, such as current industry solutions in AMD's Infinity Fabric, Intel's Foveros active interposer, and Marvell's Mochi Interconnect. Although MCMs improve manufacturing yields and are cost-effective, additional challenges on the Network-on-Chip (NoC) within a single chiplet and across multiple chiplets need to be addressed. These challenges include routing, scalability performance, and resource allocation. This work introduces a scalable MCM 3D interconnect infrastructure called ""MCM-3D-NoC"" with multiple 3D chiplets connected through an active interposer. System-level simulations of MCM-3D-NoC are performed to validate the proposed architecture and provide performance evaluation of network latency, throughput, and EDP.","3D NoCs with Active Interposer for Multi-Die Systems Vasil Pano Drexel University 3141 Chestnut St Philadelphia, PA, USA 19104 vasilpano@gmail.com Special Session Paper Ragh Ku(cid:139)appa Drexel University 3141 Chestnut St Philadelphia, PA, USA 19104 fr67@drexel.edu Baris Taskin Drexel University 3141 Chestnut St Philadelphia, PA, USA 19104 taskin@coe.drexel.edu ABSTRACT Advances in interconnect technologies for system-in-package manufacturing have re-introduced multi-chip module (MCM) architectures as an alternative to the current monolithic approach. MCMs or multi-die systems implement multiple smaller chiplets in a single package. (cid:145)ese MCMs are connected through various package interconnect technologies, such as current industry solutions in AMD’s In(cid:128)nity Fabric, Intel’s Foveros active interposer, and Marvell’s Mochi Interconnect. Although MCMs improve manufacturing yields and are cost-e(cid:130)ective, additional challenges on the Network-onChip (NoC) within a single chiplet and across multiple chiplets need to be addressed. (cid:145)ese challenges include routing, scalability performance, and resource allocation. (cid:145)is work introduces a scalable MCM 3D interconnect infrastructure called ‘‘MCM-3D-NoC’’ with multiple 3D chiplets connected through an active interposer. System-level simulations of MCM-3D-NoC are performed to validate the proposed architecture and provide performance evaluation of network latency, throughput, and EDP. CCS CONCEPTS •Computer systems organization → Integrated Circuits; VLSI (very large scale integration); KEYWORDS 3D NoCs, Interconnect Network, Multi-Die Systems, MultiChip Modules Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro(cid:128)t or commercial advantage and that copies bear this notice and the full citation on the (cid:128)rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi(cid:139)ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci(cid:128)c permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’19, New York, NY, USA © 2019 ACM. 978-1-4503-6700-4/19/10. . . $15.00 DOI: 10.1145/3313231.3352380 Figure 1: MCM with active interposer and 3D chiplets. 1 INTRODUCTION Monolithic fabrication is the predominantly preferred method for the manufacturing of contemporary computing systems. All major components of the system, including I/O, memory, CPU cores, and GPU cores, are all integrated within a single die on a single silicon technology. (cid:145)e major detriments of these monolithic designs include increased costs, reduced fabrication yields, and limited scalability [1]. (cid:145)erefore, multi-chip modules (MCMs), which break the monolithic structure into multiple smaller, higher yielding die, are actively researched in industry and academia [2–5]. Although multi-chip modules have been explored in the past, they did not gain traction in industry due to the limited package interconnect technology for inter-die communication. Current MCMs implement di(cid:130)erent package interconnect approaches which include AMD’s server-grade EPYC and (cid:145)readripper processors [3], Intel’s Foveros and Embedded Multi-die Interconnect Bridge (EMIB) [4], active interposer research [5], and NVIDIA’s research in multi-chiplet with MCM-GPU [2]. Although higher yield is achieved with the introduction of multi-die systems, some performance degradation is expected due to the decrease in connectivity of the processing elements (PEs) across multiple die. (cid:145)e decrease in connectivity of distant PEs especially evident in the case of PEs distributed over multiple 3D die (i.e 3D chiplets with a 3D NoC). (cid:145)is exemplary topology is depicted in Figure 1, with six 3D chiplets connected through the active silicon interposer over the package. (cid:145)e 3D chiplet topology with a 3D NoC is the topology of choice for the recently introduced industrial solution for Intel’s Foveros [6]. In order for a packet to reach a destination in a di(cid:130)erent 3D chiplet, it has to (cid:128)rst traverse the layers in the local 3D chiplet, traverse the active silicon interposer layer, and then reach the PE at a layer of the destination 3D chiplet. Similar performance degradation (although to a lesser extent) can been seen in 2D ICs with 2D NoCs across multiple die [1, 5]. In this work, a MCM-3D-NoC is proposed to provide a scalable solution for future generation processors. MCM-3D-NoC connects multiple 3D chiplets on a single package through an active interposer, which acts as an interconnect layer for all the chiplets. An illustrative package with six 3D chiplets connected with an active interposer is shown in Figure 1. Each chiplet has three layer and the active silicon acts as an interconnect layer with dedicated logic to connect the chiplets in the package. Although the illustration shows an homogeneous architecture, MCMs are capable of heterogeneous designs at a lower cost and higher yield than monolithic fabrication. (cid:145)e bene(cid:128)ts of MCMs vs. monolithic, which include scalability, modular design, and higher yield, are largely unquanti(cid:128)able, although system-level evaluation provides useful insights on the performance of future MCM-enabled systems. Proposed MCM-3D-NoC encapsulates a custom inter-chiplet routing algorithm for improved latency and throughput. Additionally, resource allocation and scalability analysis are performed to evaluate the impact of various chiplet con(cid:128)gurations. MCM-3D-NoC system performance is evaluated with a custom SystemC cycle-accurate network simulator. A background on current package interconnect technologies and related works are presented in Section 2. (cid:145)e proposed MCM-3D-NoC is detailed in Section 3. (cid:145)e custom inter-chiplet routing algorithm is discussed in Section 4. Simulation setup and performance evaluation of MCM-3D-NoC are presented in Section 6. Finally, the conclusions of this work are presented in Section 7. 2 BACKGROUND AND RELATED WORKS Multi-chip modules are implemented in consumer-level products [3] to tackle scalability challenges that arise from manufacturing a monolithic single-chip multi-IP package. (cid:145)ese multi-chiplet packages improve manufacturing yields considerably [1] and improve processing scalability for futuregeneration workloads targeting exascale computation [7]. Current die-to-die connectivity includes Intel’s Foveros and EMIB [4], AMD’s In(cid:128)nity Band [3], Marvell’s Mochi interconnect [8], and passive/active silicon interposers [1, 5]. Each of these interconnects has their respective detriments and bene(cid:128)ts, measured primarily in 1) bandwidth, 2) power/energy pro(cid:128)le, and, 3) ease of scalability. Figure 2: MCM-3D-NoC architecture scaled down for clarity. Each 3D chiplet can represent IP blocks, memory, or compute units. Jerger et al. [1, 5] and Vivet et al. [9] investigate the feasibility of active silicon interposers for 3D ICs integration, including heterogeneous systems. Stow et al. [10] and Coskun et al. [11] perform a cost and performance comparison between traditional monolithic 2D SoCs, 2.5D passive interposers, and 2.5D/3D active interposers to demonstrate the trade-o(cid:130)s between the interposer types for current and future high-performance systems. Yin et al. [12] introduce a modular deadlock-free routing algorithm for 2D NoCs on chiplet-based systems with active interposers. Ku(cid:139)appa et al. [13] propose a novel clock generation and distribution network for multi-die architectures connected through an active silicon interposer. (cid:145)e proposed clock network generates and distributes a resonant clock through the active silicon interposer between dies, with each die served through resonant local clock trees. One major thrust in relevant research is towards, photonic NoCs, which have gained interest to address bandwidth challenges associated with many core systems [14, 15]. Abellan et al. [14] study the bandwidth challenges associated with silicon-photon links with di(cid:130)erent NoC architectures for 2D-mesh based designs. Narayan et al. [15] propose a wavelength selection technique along with a framework to model system performance and power for 2.5D systems. Research on monolithic (as opposed to MCM) 3D NoCs, include Pavlidis et al. [16] and Feero et al [17], which provide performance evaluation of NoCs implemented on 3D ICs to demonstrate the superior functionality in terms of throughput and latency compared to traditional 2D NoCs. Vivet et al. [18] have fabricated a 4×4×2 3D NoC prototype on an active interposer. (cid:145)e versatility of the 3D NoC topologies with arbitrary 2D topologies in each of the individual layers and partially connected layers is shown in [19]. (cid:145)e elevator placement is optimized in [20] using a heuristic, signi(cid:128)cantly improving the 3D NoC performance. More et 2 (a) DOR Routing (b) West First / North Last Routing (c) Chiplet-First DOR Routing (d) Chiplet-First West First Routing Figure 3: (cid:135)e di(cid:130)erent routing algorithms evaluated for MCM-3D-NoCs. al. [21] investigate an arbitration-free design for the shared vertical channels and compared with other 3-D NoC architectures using traditional synthetic tra(cid:129)c pa(cid:139)erns and Rentian tra(cid:129)c emulating applications for CMPs. To the best of the author’s knowledge, this work is the (cid:128)rst to introduce and evaluate a non-monolithic, multi-chiplet, 3D NoC. 3 MULTI-CHIP MODULE 3D NOC A smaller representative model of the proposed MCM-3D-NoC with active interposer is depicted in Figure 2. (cid:145)e base layer of the topology is located on the shared active silicon interposer, which is capable of additional logic, as opposed to a passive silicon interposer. (cid:145)e active interposer does not have any complex processing elements, memory, or additional IPs; the base layer acts only as a routing infrastructure for the 3D chiplets. (cid:145)e upper layers represent the 3D chiplets, which are not connected to each-other except for the base layer on the active interposer. (cid:145)ese 3D chiplets are modular, and additional chiplets of various dimensions and purposes can be added to MCM-3D-NoC easily. For the purpose of clarity, the evaluated MCM-3D-NoC topology in this work is homogeneous and arbitrarily de(cid:128)ned as 9 chiplets, with 3 layers each, and 4×4 routers per layer. (cid:145)e 3D chiplets are positioned in a 3×3 grid. Although, scalability analysis performed in Section 6.3, evaluates a separate larger topology for thoroughness. Concerted e(cid:130)ort is made to keep the router architecture relatively simplistic to avoid overly complicated logic on the active silicon interposer. Nonetheless, resource allocation analysis performed in Section 6.2 evaluates the impact of additional bu(cid:130)er con(cid:128)gurations on the MCM-3D-NoC. (cid:145)e router is equipped with 4 bu(cid:130)ers per input port and 2 virtual channels to guarantee deadlock and livelock avoidance. (cid:145)ere are a maximum of 7 ports in the 3D chiplet routers, 4 for the cardinal directions, 2 for the up and down directions, and an additional for local tra(cid:129)c (i.e. PEs, IPs, memory). (cid:145)e routers on the base layer do not have additional ports for local tra(cid:129)c and for the down direction, therefore conserving energy and decreasing area overhead. (cid:145)e base layer has 3 a one-to-one router pairing with each of the 3D chiplets, therefore the base layer has a total of 12×12 routers. Any transaction between 3D chiplets needs to traverse the base layer on the active silicon interposer. 4 ROUTING ALGORITHM (cid:145)e routing algorithm used for MCM-3D-NoC considers the source chiplet, destination chiplet, and active interposer layer. (cid:145)e interposer layer does not generate or receive traf(cid:128)c (as shown in Figure 2), therefore all the tra(cid:129)c generated in the network has a source and destination pair located in any of the 3D chiplets present in system. If both the source and destination chiplets are the same then the packet is routed using deterministic XYZ routing algorithm. If the destination chiplet is di(cid:130)erent from the source then the packet is (cid:128)rst routed to the base layer for intra-die communication. On the base layer, the packet is routed to the destination XY coordinates and then to the Z layer of the destination chiplet. Further details regarding the di(cid:130)erent routing algorithms evaluated, including partially-adaptive West First and North Last, are included in Sections 4.1–4.3. 4.1 Baseline Algorithm In order to route a packet with a destination to a di(cid:130)erent 3D chiplet and currently located on the base layer, the baseline algorithm utilizes a deterministic dimension order routing (DOR) XY algorithm to reach the XY destination coordinates and then route to the destination Z layer. Two partially-adaptive algorithms for the base layer, West First and North Last, are implemented to quantify the performance bene(cid:128)t of the increase network utilization that partially-adaptive algorithms provide. A fully-adaptive routing algorithm is not implemented due to the hardware constrain imposed by the fabrication of logic on silicon interposer. (cid:145)e baseline routing algorithms are shown in Figure 3(a)– 3(b). Both partially-adaptive algorithms can pick between di(cid:130)erent shortest path routes based on the bu(cid:130)er availability at each intermediate node, as shown in Figure 3(b). 4.2 Chiplet-First Algorithm (cid:145)e “Chiplet-First” routing algorithm is shown in Figure 3(c). (cid:145)e purpose of the “Chiplet-First” algorithm is to improve network utilization on the 3D chiplet and decrease network congestion on the base layer by bringing the packet closer to the destination before entering the active silicon interposer. If a packet destination is located on a di(cid:130)erent 3D chiplet, and the source node is located on the top-most layer of the 3D chiplet then the packet is routed to the edge of the 3D chiplet (cid:128)rst then routed the base layer. (cid:145)e packet is routed towards a speci(cid:128)c edge based on the XY distance of the destination node. If the destination node is further in the X direction then the packet is routed to the le(cid:137) or right edge of the 3D chiplet. If the destination is equidistant from the source node then the edge is selected based on the bu(cid:130)er availability at the intermediate node. 4.3 Chiplet-First West First Algorithm (cid:145)e Chiplet-First West First algorithm combines both the previously detailed Chiplet-First algorithm and the partiallyadaptive West First algorithm on the base layer. (cid:145)e ChipletFirst West First routing algorithm is shown in Figure 3(d). (cid:145)e deterministic XYZ routing algorithm used for sourcedestination pairs within a 3D chiplet remains the same across all di(cid:130)erent routing algorithms. (cid:145)is decision is made to accurately quantify the performance improvement to the base layer only, either directly by implementing partiallyadaptive routing algorithms, or indirectly by implementing Chiplet-First routing algorithm which decreases network congestion on the base layer. 5 DECONSTRUCTING 3D NOCS FOR MULTI-DIE SYSTEMS Monolithic 3D NoCs have the bene(cid:128)t of added connectivity between the layers as opposed to transferring packets to the base layer for inter-die communication. (cid:145)is addition in connectivity and performance improvement in monolithic 3D NoCs are more evident with each added IC layer. (cid:145)e primary challenge that next generation systems have to solve is the communication bo(cid:139)leneck between vertical and horizontal transmissions. For each added layer in a 3D IC, the base transmission layer between multiple die is one hop further. Bus-based communication techniques like the ElevatorFirst [19] or arbitration-free vertical channels [21] have partially solved long-distance vertical communication but no studies have implemented these algorithms and architectures for multi-die systems. In addition, the limited resources on the base layer increase network congestion and delay across the entire system as opposed to only a particular die with heavy load. In order to fully evaluate the impact of chiplet-based systems in 3D IC, performance comparison between a monolithic 3D NoC and MCM 3D NoC is detailed in Figures 4-5. A cycle-accurate SystemC network simulator is used to perform MCM-3D-NoC performance evaluation. (cid:145)e base layer mesh size arbitrarily chosen for this experimental evaluation is 12×12 (total 144 routers). On the active interposer, are nine 3D chiplets on a 3×3 grid with dimensions of 4×4×3. (cid:145)e monolithic 3D NoC is in turn sized to a 12×12×4 to preserve the same structure and number of routers in the network. Additionally, networks with 2 and 3 number of layers are evaluated to quantify the performance change. (cid:145)e nonadaptive XYZ routing algorithm is used to route packets for the monolithic 3D NoC and the baseline algorithm discussed in Section 4.1 is used to route packets for MCM 3D NoC. Figure 4: Latency comparison of monolithic 3D NoC with MCM-3D-NoC under uniform random tra(cid:129)c. Latency: Average network latency across multiple (cid:131)it injection rates is shown in Figure 4. With the increase of (cid:131)it injection rate, latency of MCM 3D NoC increases considerably faster compared to monolithic 3D NoC. (cid:145)ere is a clear divide between latency in monolith 3D NoC and MCM-3DNoC, particularly with the increase of the number of layers of IC. (cid:145)is increase in latency is a(cid:139)ributed to increase in network tra(cid:129)c to the base layer from the additional layers of the 3D IC, essentially creating a bo(cid:139)leneck in the inter-die communication. When the 3D NoC is provisioned with 2 layers only, the di(cid:130)erence in latency is negligible as seen also in [12]. (cid:145)e increase in latency is not seen at all with the monolithic 3D NoC due to the highly connected structure of the topology which doesn’t require traversal to the base layer. 4 Figure 5: (cid:135)roughput comparison of monolithic 3D NoC with MCM-3D-NoC under uniform random traf(cid:128)c. (cid:135)roughput: Network throughput across multiple (cid:131)it injection rates is shown in Figure 5. Similar behavior to latency, network throughput saturates faster and lower when separating the monolithic 3D NoC into smaller 3D chiplets in the MCM-3D-NoC architecture. Even though the 2 layer topology has particularly good throughput (as compared to the monolithic 3D NoC), the increase in number of layers quickly decreases throughput and saturates the network at lower (cid:131)it injection rates. (cid:145)e monolithic 3D NoC is not phased by the increase of layers as packet traversal does not require sidetracking to the base layer, therefore increasing overall throughput of the network. Although the performance bene(cid:128)ts of monolithic 3D NoC cannot be ignored, successful fabrication of a large monolithic 3D NoC would require drastic changes to the manufacturing process. Primarily to improve yield and decrease fabrication costs, industry has moved to a chiplet-based manufacturing process which would in turn permit robust fabrication of large scale computing packages capable of hundreds of PEs. 6 MCM-3D-NOC PERFORMANCE EVALUATION Power estimations are performed utilizing the latest version of DSENT [22]. (cid:145)e packet size is 8 (cid:131)its and the (cid:131)it size is 32 bits. Five tra(cid:129)c pa(cid:139)erns are used to analyze MCM-3D-NoC: 1) uniform random, 2) 25% localized (to 3D chiplet) tra(cid:129)c, 3) 50% localized tra(cid:129)c, 4) 75% localized tra(cid:129)c, and 5) hotspot. (cid:145)e performance metrics evaluated are: 1) throughput, 2) average latency, and 3) EDP per (cid:131)it. Figure 6: MCM-3D-NoC average latency under uniform random tra(cid:129)c with various routing algorithms. Initially, the average network latency is swept across multiple packet injection rates to evaluate both low-load latency and saturation throughput. Later, performance is evaluated at network saturation. (cid:145)e average network latencies of MCM-3D-NoC with various routing algorithms under uniform random tra(cid:129)c, are shown in Figure 6. (cid:145)e results indicate that for uniform random tra(cid:129)c, the non-adaptive XY routing algorithm outperforms all other partially-adaptive algorithms at high tra(cid:129)c load. (cid:145)e XY routing algorithm has more global, long-term information about the characteristics of uniform tra(cid:129)c, which leads to even distribution of tra(cid:129)c across the MCM-3D-NoC. (cid:145)e Chiplet-First routing algorithm performs poorly under uniform random tra(cid:129)c, due to also being partially-adaptive and routing to the edges of the 3D chiplet may cause congestion at the top layer, therefore increasing latency. (cid:145)is latency evaluation has similar results compared to the 2D NoC with active silicon interposer latency evaluation performed in [12]. 6.1 Simulation Results Simulations results of MCM-3D-NoC with various routing algorithms under di(cid:130)erent tra(cid:129)c are shown in Figure 7. (cid:135)roughput: (cid:145)roughput results are shown in Figure 7(a). (cid:145)e maximum throughput achieved under uniform random tra(cid:129)c is with Chiplet-First routing algorithm, at 0.086 (cid:131)its/cycle/IP. Partially-adaptive algorithm, including Chiplet-First West First, show a decrease in network throughput with uniform random tra(cid:129)c of ≈20% compared to XY routing. Deterministic routing algorithms perform particularly well under uniformly distributed tra(cid:129)c. MCM-3D-NoC under localized tra(cid:129)c pa(cid:139)ern performs similarly to the uniform random tra(cid:129)c. In the hotspot tra(cid:129)c pa(cid:139)ern simulated, 4 close nodes 5 (a) Saturation throughput. (b) Average latency before saturation. (c) Normalized energy-delay/(cid:131)it at saturation. Figure 7: Simulation results of MCM-3D-NoC for varying tra(cid:129)c patterns and routing algorithms. are designated as the hot spot nodes, which receive tra(cid:129)c in addition to the regular uniform tra(cid:129)c. (cid:145)e nodes are located at the center 3D chiplet on the top-most layer. (cid:145)e network under hotspot tra(cid:129)c pa(cid:139)ern saturates sooner and throughput is not as high as the rest of the tra(cid:129)c pa(cid:139)ern. It can be seen that partially-adaptive routing algorithms perform be(cid:139)er than the XY algorithm, due to the non-uniformity of the tra(cid:129)c pa(cid:139)ern. Latency: Network latency evaluation is shown in Figure 7(b). As shown in Figure 6, XY routing algorithm is the preferred choice for MCM-3D-NoC running uniform random tra(cid:129)c. Not only the network saturates sooner under partiallyadaptive algorithms but also the latency is higher when not saturated. But when MCM-3D-NoC runs non-uniform tra(cid:129)c like the hotspot tra(cid:129)c pa(cid:139)ern, the partially-adaptive algorithms perform signi(cid:128)cantly be(cid:139)er than XY routing, up to ≈12% with Chiplet-First West First routing algorithm. Energy-delay product: Energy-delay product evaluation is shown in Figure 7(c). Energy estimations are performed using the latest version of DSENT [22] using the 22nm technology node. EDP is useful because network latency and energy are both taken into account, therefore characterizing the improvement in performance considering the added energy consumption. MCM-3D-NoC running hotspot tra(cid:129)c pa(cid:139)ern displays an improvement in EDP up to ≈12%. In the rest of the tra(cid:129)c pa(cid:139)erns analyzed, EDP is within ≈5% of the XY routing algorithm. 6.2 Resource Allocation Analysis Simulations results of MCM-3D-NoC with additional resources under di(cid:130)erent tra(cid:129)c pa(cid:139)erns are shown in Figure 8. (cid:145)e MCM-3D-NoC is evaluated with increased bu(cid:130)er depth across the entire system and only the routers on the base layer (denoted by “Extra” ). (cid:145)e standard deterministic XY routing algorithm is used to route packets across the network. (cid:145)e (cid:128)rst two test cases demonstrate the impact of 6 random and localized tra(cid:129)c pa(cid:139)erns. MCM-3D-NoC under additional bu(cid:130)er space on the base layer compared to the previously evaluated MCM-3D-NoC. (cid:145)e second set of cases, demonstrate the impact of distributing resources equally across the 3D chiplets and the base layer on the active silicon interposer, or dedicating the extra resources only for the base layer. (cid:145)e same number of bu(cid:130)ers are used on MCM-3D-NoC, only their distribution di(cid:130)ers (6 bu(cid:130)ers across vs. 4 bu(cid:130)ers across and 8 extra on the routers of the base layer). (cid:135)roughput: (cid:145)roughput results are shown in Figure 8(a). With the addition of the 4 extra bu(cid:130)ers on each router on the base layer, throughput is improved up to ≈10% in uniform hotspot tra(cid:129)c pa(cid:139)ern shows minimal change with the addition of bu(cid:130)ers on the base layer. Allocating the additional resources to the routers on the base layer as opposed as throughout the 3D chiplets, improves throughput up to ≈10%. (cid:145)e base layer acts as an interconnect for the many 3D chiplets and any additional resource helps improve the performance of the network. (cid:145)erefore, at the same cost of area and bu(cid:130)er energy consumption, it is advised to dedicate additional resource to the base layer as opposed to all system components. Latency: Network latency evaluation is shown in Figure 8(b). (cid:145)e addition of bu(cid:130)er space has a minimal impact on latency across all tra(cid:129)c pa(cid:139)erns and con(cid:128)gurations. Although, latency improves ≈8% on MCM-3D-NoC under hotspot tra(cid:129)c pa(cid:139)ern if additional resources are dedicated on the base layer. Energy-delay product: Energy-delay product evaluation is shown in Figure 8(c). With the addition of bu(cid:130)er space, energy consumption is consequently increased. (cid:145)erefore EDP is increased across all tra(cid:129)c pa(cid:139)erns and con(cid:128)gurations. It is worth noting that under hotspot tra(cid:129)c pa(cid:139)ern, EDP is increased only ≈20% if resources are dedicated to the base layer, as opposed to ≈31% if shared across the system. (a) Saturation throughput. (b) Average latency before saturation. (c) Normalized energy-delay/(cid:131)it at saturation. Figure 8: Simulation results with increased resources for varying tra(cid:129)c patterns and XY routing algorithm. (a) Saturation throughput. (b) Average latency before saturation. (c) Normalized energy-delay/(cid:131)it at saturation. Figure 9: Simulation results of larger MCM-3D-NoC for varying tra(cid:129)c patterns and routing algorithms. 6.3 Scalability Analysis A larger 4×16×16 MCM-3D-NoC with 4 3D chiplets on a 2×2 grid is simulated for scalability analysis. Simulations results of the larger MCM-3D-NoC with various routing algorithms under di(cid:130)erent tra(cid:129)c pa(cid:139)erns are shown in Figure 9. (cid:145)e evaluation investigates the performance impact of having fewer and bigger 3D chiplets on the MCM as opposed to the previous evaluation of 9 3D chiplets. (cid:135)roughput: (cid:145)roughput results are shown in Figure 9(a). Similar to the smaller MCM-3D-NoC, under uniform random tra(cid:129)c, throughput is higher with XY routing algorithm. MCM-3D-NoC under 25% localized tra(cid:129)c pa(cid:139)ern, shows an increase in throughput when utilizing partially-adaptive routing algorithms, achieving up to ≈27% throughput improvement with the Chiplet-First West First routing algorithm. (cid:145)is improvement is a(cid:139)ributed to the larger 3D chiplet size and fewer 3D chiplets on the network, therefore lowering congestion and improving throughput. (cid:145)e MCM under other tra(cid:129)c has similar throughput across routing algorithms. (cid:145)e 4 nodes that comprise the hotspot which resides on 7 the center of the network is split across one corner of each 3D chiplets, therefore MCM-3D-NoC using the Chiplet-First routing algorithm has improved throughput of ≈15%. Latency: Network latency evaluation is shown in Figure 9(b). Under uniform random tra(cid:129)c, latency remains within ≈3%, across all routing algorithms. An improvement in latency of ≈4% can be seen on MCM-3D-NoC under 50% Energy-delay product: Energy-delay product evaluation is shown in Figure 9(c). Due to the similarity in latency, EDP variance is within ≈4% across all tra(cid:129)c pa(cid:139)erns and routing algorithms. MCM-3D-NoC under hotspot tra(cid:129)c pattern performs best with the Chiplet-First West First routing algorithm. localized and hotspot tra(cid:129)c pa(cid:139)ern. 7 CONCLUSIONS (cid:145)is work proposes a network interconnect for multi-die systems capable of linking multiple 3D chiplets with an active T. Mourier, and S. Ch ´eramy, (cid:3)D Advanced Integration Technology for Heterogeneous Systems,” in Proceedings of the International 3D Systems Integration Conference (3DIC), August 2015. [10] D. Stow, Y. Xie, T. Siddiqua, and G. H. Loh, “Cost-e(cid:130)ective Design of Scalable High-performance Systems using Active and Passive Interposers,” in Proceedings of the IEEE/ACM International Conference on Computer-Aided Design (ICCAD), pp. 728–735, November 2017. [11] silicon interposer. (cid:145)e active interposer acts as an interconnect layer that allows for modular plug-and-play heterogeneous exascale architectures. To verify functionality and performance, a SystemC MCM-3D-NoC simulator is evaluated under various tra(cid:129)c pa(cid:139)erns and routing algorithms. A custom “Chiplet-First” routing algorithm is implemented which improves network congestion on the base interconnect layer. Multiple routing algorithms are evaluated and it is found that non-uniform tra(cid:129)c pa(cid:139)erns have an improvement in throughput up to ≈15%. In addition, resource allocation analysis has concluded that over-provisioning the base layer, improves throughput, latency and EDP up to ≈10%. Scalability analysis indicates that MCM-3D-NoC can be adapted to multiple network con(cid:128)gurations and 3D chiplet sizes with minimal impact on performance. (cid:145)e bene(cid:128)ts of MCM design, which include enhanced scalability, modularity, and higher fabrication yields, go beyond system-level performance metrics. In that regard, MCM-3D-NoC is developed to provide a thorough analysis and evaluation of challenges and solutions in the MCM interconnect network. "
BINDU - deadlock-freedom with one bubble in the network.,"Every interconnection network must ensure, for its functional correctness, that it is deadlock free. A routing deadlock occurs when there is a cyclic dependency of packets when acquiring the buffers of the routers. Prior solutions have provisioned an extra set of escape buffers to resolve deadlocks, or restrict the path that a packet can take in the network by disallowing certain turns. This either pays higher power/area overhead or impacts performance. In this work, we demonstrate that (i) keeping one virtual-channel in the entire network (called 'Bindu') empty, and (ii) forcing it to move through all input ports of every router in the network via a pre-defined path, can guarantee deadlock-freedom. We show that our scheme (a) is topology agnostic (we evaluate it on multiple topologies, both regular and irregular), (b) does not impose any turn restrictions on packets, (c) does not require an extra set of escape buffers, and (d) is free from the complex circuitry for detecting and recovering from deadlocks. We report 15% average improvement in throughput for synthetic traffic and 7% average reduction in runtime for real applications over state-of-the-art deadlock freedom schemes.","BINDU: Deadlock-Freedom with One Bubble in the Network Mayank Parasar Tushar Krishna School of ECE, Georgia Institute of Technology, Atlanta, GA, USA mparasar3@gatech.edu tushar@ece.gatech.edu ABSTRACT Every interconnection network must ensure, for its functional correctness, that it is deadlock free. A routing deadlock occurs when there is a cyclic dependency of packets when acquiring the buffers of the routers. Prior solutions have provisioned an extra set of escape buffers to resolve deadlocks, or restrict the path that a packet can take in the network by disallowing certain turns. This either pays higher power/area overhead or impacts performance. In this work, we demonstrate that (i) keeping one virtual-channel in the entire network (called ‘Bindu’) empty, and (ii) forcing it to move through all input ports of every router in the network via a pre-defined path, can guarantee deadlock-freedom. We show that our scheme (a) is topology agnostic (we evaluate it on multiple topologies, both regular and irregular), (b) does not impose any turn restrictions on packets, (c) does not require an extra set of escape buffers, and (d) is free from the complex circuitry for detecting and recovering from deadlocks. We report 15% average improvement in throughput for synthetic traffic and 7% average reduction in runtime for real applications over state-of-the-art deadlock freedom schemes. KEYWORDS Computer architecture, Network-on-chip, Interconnection network, Deadlock ACM Reference Format: Mayank Parasar Tushar Krishna. 2019. BINDU: Deadlock-Freedom with One Bubble in the Network . In International Symposium on Networkson-Chip (NOCS ’19), October 17–18, 2019, New York, NY, USA. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3313231.3352359 1 INTRODUCTION Deadlock-freedom is necessary for designing any functionally correct interconnection network. Routing-level deadlocks occur when there is a cyclic dependency between the packets in the network as they try to acquire the buffers. Prior solutions on routing deadlocks can mainly be categorized into deadlock avoidance and deadlock recovery. Deadlock avoidance [9, 12, 19] designs the routing algorithm such that a cyclic dependence between packets can never get created at runtime to begin with by disabling certain turns. These turn models are extremely popular in regular network topologies such as Mesh classic examples being XY, West-first and their variants [12]. These turn models can be implemented for all packets or only those within Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’19, October 17–18, 2019, New York, NY, USA © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6700-4/19/10. . . $15.00 https://doi.org/10.1145/3313231.3352359 certain escape virtual channels (VCs) [11]. Deadlock avoidancebased solutions become challenging to adopt for irregular topologies (which may occur due to heterogeneous SoCs, network faults, or power-gating) [16] as they require complex analysis of the channel dependence graph (CDG) to disable turns, and also lead to performance losses due to non-minimal routes [16]. In deadlock-recovery [3, 16, 17], deadlocks are dynamically detected and resolved. These solutions are amenable to arbitrary topologies [16, 17]; however, they are extremely complicated to implement due to global deadlock-detection and resolution. There is another class of solutions that achieves deadlock freedom using bubbles (i.e. empty VCs or buffers) in the network. These techniques allow for cycles in the CDG but control injection of packets such that packets never get into a cyclic buffer dependency at runtime [6, 7, 15]. The underlying theory, Bubble Flow Control [15], states that one bubble within a cyclic dependence can ensure forward progress. Multiple implementations of this theory exist today. Critical Bubble Scheme (CBS) [7] and its variants [6] are the stateof-the-art for rings and Torii; here one bubble in every ring is marked as a critical bubble and circulates through the ring. Static Bubble (SB) [16] embeds extra buffers at various routers at design time, and turns them ON upon deadlock-detection to introduce a bubble. Brownian Bubble Router (BBR) [14] reserves one bubble in every router in the network, and circulates it across the router’s ports. CBS, SB and BBR require 4k, O(k) [16], and k2 bubbles respectively in a k × k mesh/torus1 . More bubbles naturally lead to reduced throughput and low buffer utilization. In this work, called BINDU (Bubble in Irregular Network for Deadlock pUrging), we demonstrate, for the first time, that it is possible to provide deadlock freedom with just a single bubble (referred to as a ‘Bindu’2 and defined formally in §3.1) in the entire network. The Bindu moves through the network in a fixed path, covering all the routers and their input ports in the network. During its course the Bindu shuffles the packets present in the network, naturally resolving any deadlock that comes in its path. The following are the primary contributions of this work: (1) A novel technique to guarantee deadlock freedom in arbitrary irregular topologies by having one or more Bindus (empty VC) in the entire network, and to force these Bindus to move through-out the network at a periodic rate. (2) BINDU frees the designer from any consideration of deadlock when designing their routing algorithm, allowing high performance with minimal overhead. (3) And evaluation of how BINDU performance compares with previously proposed deadlock freedom techniques with both 1 In practice, the number of bubbles needed during operation in CBS becomes higher since transitioning from one ring to the other requires at least two bubbles in that ring. 2 Bindu is a Hindi word, meaning ‘point’. With a collection of points, we can draw any geometrical figure; similarly with BINDU we can make any topology deadlock free. NOCS ’19, October 17–18, 2019, New York, NY, USA Mayank Parasar Tushar Krishna synthetic traffic and real applications on both regular and irregular topologies3 . The rest of the paper is organized as follows: §2 provides background information and related work on deadlock-freedom in NoCs; §3 deep dives into the BINDU technique, discussing the concept, proof of deadlock-freedom, and implementation; §4 then delves into experimental results, and §5 presents conclusion. 2 BACKGROUND AND RELATED WORK Deadlocks refer to a condition in which a set of agents (i.e. packets) wait indefinitely trying to acquire a set of resources (input buffers). Deadlocks primarily occur because of the cycles present in the Channel Dependency Graph (CDG) of the topology. The CDG is a directed graph, in which each node is a link in the topology and each edge in CDG defines the order in which packets want to traverse the topology to reach its destination. Since there could be multiple paths from a given source to destination, there could be many edges in CDG connecting two different nodes. These edges can form a cycle in themselves leading to a deadlock. The choices of the path that a packet can take are determined by the routing algorithm, therefore CDG is the function of both routing algorithm and topology. We classify deadlock resolution solutions in NoCs into the following categories: deadlock-avoidance, deadlock-recovery, and bubble-based4 . A High-level qualitative comparison of BINDU with state-of-the-art schemes have been paraphrased in Table 1. 2.0.1 Deadlock Avoidance. In deadlock avoidance [8, 9, 19] schemes, the CDG is made acyclic via turn-restrictions in the routing algorithm (e..g, XY, West-first, and so on in a Mesh, Up-Down [18] routing in irregular topologies), ensuring that a cyclic dependence is never created during runtime. However, the loss in path-diversity leads to loss in network throughput. To address this, escape VC [11] based schemes are often used, where the turn restrictions are present in only a subset of VCs, while the rest of the VCs can use all paths. The CDG can therefore have cycles, but there is at least one acyclic escape path to which all packets have access. Escape VC based schemes however lead to an increase in the total number of VCs, adding area and power at every router. Another key challenge with the aforementioned deadlock avoidance schemes, beyond the performance or area/power implications, is that they are highly topology-dependent, since every topology has a unique CDG. As topologies become irregular (due to heterogeneity, or waning silicon reliability or due to power gating of routers/links), coming up with unique turn restrictions and encoding them in the packets is an expensive operation. 2.0.2 Deadlock Recovery. Another school of thought for resolving deadlocks stems from the observation that deadlocks are a rare phenomenon, therefore instead of adding turn restrictions or extra VCs in the routers to avoid deadlocks, one should detect, locate, and resolve deadlocks. Deadlock recovery algorithms allow packets to take all paths provided by the topology to reach to their destination. However, in doing so, the packets can get deadlocked due to cyclic CDGs. To recover from it, these techniques require detection via 3We would like to note that the focus of this work is not on the important problem of dynamic fault-tolerance; rather it guarantees deadlock freedom in static irregular topologies, which may be created due to faults or at design time. 4 In the literature, bubble-based schemes are often classified as flow-control based schemes within deadlock avoidance [17]. We classify them separately to allow us to take an in-depth view and compare them against BINDU. Table 1: Qualitative Comparison of Deadlock Freedom Mechanisms Full Path Diversity High Throughput No Extra VCs No D’lock Detect Topology Agnostic Mechanism ✓/✗** ✓/✗* CDG [9] Escape VC [10] BFC [7, 15] Deflection [13] Recovery [16, 17] BBR [14] BINDU *At low-loads, full path diversity is available. But at high loads, packets cannot control the directions or paths along with they are deflected. ** Within esc VC: limited path diversity + requires topology info to drain. ✗ ✓ ✗ ✗ ✓ ✗ ✓ ✓ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✗ ✓ ✓ ✗ ✗ ✓ ✓ ✓ ✓ ✗ ✓ ✓ ✓ ✓ ✓/✗** Figure 1: Examples of Bindu-paths. Each Bindu must go through all input ports of all routers of the network, at least once. (a) Bindu moving through all ports of a router before jumping to the next router, (b) Bindu jumping between input ports of different routers throughout its path, (c) A tree-based Bindu-path for an irregular topology timeouts, location via probes, and resolution via synchronization messages [2, 3, 16, 17]. Implementing deadlock recovery is therefore quite complex. Moreover, this technique is not scalable; as network sizes increase, the probability, shape, and length of deadlock-ring also increases, making it even more difficult to locate them dynamically [16]. There have also been proposals to drop flits in the network [23], if they fail to win the switch for a threshold number of tries, thereby breaking any deadlocks. However, this approach comes with the overhead of tracking and re-transmitting the dropped flit as NoCs do not tolerate packet/flit losses. 2.0.3 Bubble based approaches. Bubble flow control (BFC) [15] based approaches fall in between the avoidance and recovery based solutions presented above. They allow cyclic CDGs - but guarantee that cycles will actually not get created at runtime by ensuring that at least one bubble (i.e., empty VC [7, 16] or flit-buffer [6]) will be present within any cycle, to provide forward progress. Thus the most popular implementations of BFCs is in rings and torii [6, 7, 15]. Challenge with BFC and its variants is that each ring in the topology BINDU: Deadlock-Freedom with One Bubble in the Network NOCS ’19, October 17–18, 2019, New York, NY, USA Figure 2: Walkthrough figure showing the BINDU in action. Here deadlock involving router-0,1,3 and 4 is resolved by intra-router Bindu movement of Bindu-1 and deadlock involving router-4, 5, 7 and 8 is resolved by inter-router Bindu movement of Bindu-2. Network state corresponding to each type of Bindu movement is shown in sub-figure (b) and (c) respectively. needs at least one bubble (and two for injection), leading to a loss in throughput. BFC has been implemented in arbitrary topologies [22], two such example of prior works are as follows. Static Bubble [16] is a recovery based scheme where a subset of routers have extra buffers inserted at design time that are ’off ’. Upon location of the deadlocked ring, the buffer in one of the routers is turned on to introduce a bubble into the ring. However, this scheme requires expensive circuitry for deadlock detection and turning the bubble on and off. BBR [14] keeps one bubble in every router, and keeps it circulating through all the input ports of the router to introduce bubbles in deadlocks going through that router. However, because of the significant number of bubbles in the network (one per router) this scheme suffers from low throughput. In this paper, we present a deadlock-freedom solution using a single bubble (called Bindu) in the entire network. Blocked Packet: A packet which is indefinitely stuck because it is part of a deadlock ring. Unblocked Packet: A packet which might be temporarily stalled, because of congestion in the network or unavailability of credits at the downstream router. However, it is guaranteed to eventually leave the router. Empty slot: An empty VC in a router. 3 BINDU NETWORK 3.1 Definitions We formally define terms here that we will use throughout the paper. Bindu: In BINDU, Bindu refers to a reserved packet-sized empty VC, that is instantiated at the starting of the network run. It makes pro-active movement throughout the network covering all the input ports of every router in the network, as shown in Fig. 1. Unlike ‘Bubble’ used in previous works [7, 15], no packet can sit inside the Bindu. As a Bindu procatively moves through the network, packets get displaced, as shown in the walk-through Fig. 2. k-Bindu: BINDU networks can incorporate multiple Bindus within the network, each following its own path as shown in Fig. 1(c) and Fig. 2. We refer to this configuration as ‘k-Bindu’, where ‘k’ is the number of Bindus instantiated at the starting of network run. Bindu movement: Movement of Bindu from one VC to another within the router or across routers. Moving a Bindu from <Router_i, Port_m, VC_k> to <Router_i+1 Port_n, VC_o>, effectively means reading the packet from <..., VC_o> and writing it to <..., VC_k>5 . 5We assume Virtual Cut-Through, i.e., all VCs that the Bindu traverses are sized to hold the largest packet. Wormhole designs will be discussed in §3.6 3.2 Basic Idea and Walk-through Example Deadlocks are characterized by cyclic dependency of packets, which renders the forward movement impossible. BINDU tries to resolve this cyclic dependency using one or more Bindus in the network. A Bindu can be randomly initialized at one of input VCs of any router, barring the injection input VCs. Multiple Bindus can co-exist within a router - but cannot reserve the same VC at the same time. Each Bindu pro-actively moves in a predefined path which cycles back from where it started (Fig. 1). Bindu’s movement results in the partial shuffling of packets in the network. This results in naturally resolving any deadlock cycle which comes in Bindu’s path. To understand the BINDU technique in more detail, let us walk-through the scheme with an example. Fig. 2(a) shows two deadlocks in a 3x3 Mesh; the link between router-2 and router-5 is faulty, resulting in an irregular topology. The number on the packet refers to its destination router-id. Even though one Bindu is enough to resolve the deadlock, we show two Bindus in the walk-through figure to underline the generality of the scheme. In Fig. 2(b), we focus on the deadlock between packets in Routers0, 1, 3 and 4. Bindu-1 moves within Router-4 from the North to the South to the West port. This results in an empty slot in the South port, which can now be used by the packet stuck in Router-1 to make forward progress, resolving the deadlock. In Fig. 2(c), we focus on the deadlock between packets in Routers4, 5, 7 and 8. Bindu-2 jumps from Router-8 to Router-7. An important point to observe is that Bindu-2 first traverses within Router-8 from South to West (similar to Bindu-1), and then jumps to the connected Router-7 to its East port. The deadlock is resolved as Bindu NOCS ’19, October 17–18, 2019, New York, NY, USA Mayank Parasar Tushar Krishna Figure 3: The figure shows: (a) The way Bindu resolves the deadlock when it brings an empty slot to the deadlock ring. (b) How Bindu resolves the deadlock when it brings a unblocked packet to the deadlock ring. (c) Bindu resolves the deadlock by shuffling the packets present within the deadlock ring. The number inside the packet refers to its destination. replaces an earlier blocked (i.e., deadlocked) packet at West input port of Router-8 with an empty VC. All Bindus need to traverse through all ports of all routers, as we discuss in §3.3. The implications of Bindu’s intra and inter-router movements on the router micro-architecture are discussed in §3.6. In this section, we explain the proof of BINDU technique for deadlock freedom using Fig. 3 as reference. We refer to the terms defined in §3.1 and provide the following arguments. 3.5 Proof of Deadlock freedom 3.3 Bindu Path All Bindus move through all the input ports of every router in the topology (both regular and irregular) at a periodic rate indefinitely. There can be multiple possible paths. Fig. 1 shows three possible paths for Bindus in a 3×3 Mesh. In Fig. 1(a), each Bindu snakes through the network routers, moving through all input ports at each router; in Fig. 1(b), Bindus jump between input ports of different routers in each step; Fig. 1(c), the Bindu uses a tree to loop through the entire network as it is an irregular topology where the original snake does not work. As mentioned earlier, there can be more than one Bindu in the network, and each Bindu can choose different paths. The Bindu-path is encoded within each router; i.e., each router has a preset order of input ports through which the Bindu should move, and a preset neighbor to which the Bindu should jump. The next inport-id (intra-router) or router-id (inter-router) where Bindu needs to move is provided by the current router. A router can have multiple paths encoded, indexed by the Bindu id. We discuss static vs. dynamic configurability of Bindu paths in §3.6. 3.4 Bindu Movement Moving a Bindu from VC-A to VC-B requires explicitly moving the packet from VC-B to VC-A. VC-B could be a VC within the router (during intra-router Bindu movement), or at a neighboring router (during inter-router Bindu movement). Moving a Bindu across routers can lead to a temporary misroute of the packet. In our design, we constrain Bindu to move only across VC-0 within the input ports of all routers in the network during both intraand inter-router Bindu movement. This decision simplifies the router micro-architecture for BINDU (§3.6). VC-0 is Virtual-Cut Through. A Bindu movement takes f cycles, where f is the number of flits in the packet. Naturally, the Bindu movement period p needs to be greater than the size of the largest packet that can sit in VC-0. (1) By the virtue of the Bindu’s looped path, it is guaranteed to visit every deadlock ring in the network. (2) As Bindu moves into the deadlock ring and then moves out of deadlock ring it either brings a fresh packet or an empty slot into the deadlock ring. (3) If Bindu brings in the empty slot at its place (Fig. 3-(a)), then, by default, it resolves the deadlock as the earlier deadlocked packet can take up that empty slot breaking the deadlock dependency. (4) If Bindu brings in a fresh packet to the deadlock ring, then there could be two possibilities. This fresh packet can either be ‘unblocked’ or ‘blocked’, as defined in §3.1. (5) If the fresh packet is unblocked (Fig. 3-(b)), it will naturally leave the deadlock ring; this will create an empty slot in the deadlock ring, and deadlock will naturally get resolved as described in point 3. (6) However, if the fresh packet is blocked, then deadlock would persist until the next Bindu movement into the deadlock ring. If the next Bindu movement creates the empty slot or brings in unblocked fresh packet to deadlock ring, then deadlock will get resolved as mentioned in point 3 and point 5. (7) There could be a very rare, corner case in which Bindu movement will keep bringing a blocked packet to the deadlock ring (Fig. 3-(c)). This, in fact, means that the packets being brought into the deadlock ring by Bindu are part of the same deadlock ring. Here, Bindu movement effectively shuffles the packets within the deadlock ring. This shuffling of packets within the deadlock ring ensures that eventually, at least one packet would reach to its destination because of shuffling and eject-out of the network as shown in Fig. 3-(c). This would finally lead to deadlock resolution. BINDU: Deadlock-Freedom with One Bubble in the Network NOCS ’19, October 17–18, 2019, New York, NY, USA (8) A final pathological corner case could be when all packets of the network are in one big deadlock loop (e.g., this could occur in a ring topology). In such a scenario, the movement of packets due to Bindu will continue to remain in a cyclic loop. However, once a Bindu completes a full looped path, all packets would have effectively been spun around, as described in [17]. Eventually after ‘k’ spins one of the packets would reach its destination and eject out from the network. This would again lead to deadlock resolution. It is worth noting that we did not actually observe either (7) or (8) in our extensive experiments. Livelocks: Livelock is the condition where a packet keeps moving indefinitely but never reaches its destination. Recall that packets get misrouted by one-hop due to a Bindu movement. As long as the packet makes two forward hops before a Bindu misroutes it again, it will not livelock. Since Bindu paths are fixed, for the Bindu to arrive at the same router again, it will take N × r × p cycles, where ‘N ’ is the number of nodes, ‘r’ is the router radix and ‘ p’ is the Bindu movement period. During this time, if the network is not congested, a packet will definitely move forward two hops (even if N and r are small, p can be set to a large enough value to ensure two hops). If the end points are congested, it is theoretically possible, though extremely unlikely, for the same packet to be stuck at a router till the Bindu traverses the entire network and returns, and get misrouted again. However, as long as the network is deadlock-free (proven above), it cannot be congested indefinitely, thereby ensuring that eventually the packet will move forward two hops, and thus not livelock. Figure 4: Router micro-architecture of BINDU. Additional components over baseline router are highlighted 3.6 Router micro-architecture The router micro-architecture is shown in Fig. 4. We discuss the key modules incorporated to facilitate Bindu movement: Bindu-bus: This is a bus connecting all the input ports, and is used to facilitate movement of the Bindu between VC-0 of the input ports (by moving the packet into the VC occupied by Bindu previously). A Bus suffices since only one Bindu can move per cycle; if multiple Bindus are concurrently present at a router, their period is skewed such that they do not contend for the bus in the same cycle. Credit management unit: The upstream router is agnostic to the fact that there is a Bindu or an actual packet sitting at the downstream router. This implies that whenever Bindu replaces a packet in the Figure 5: The figure shows irregular topologies, created out of a regular mesh. Faults in the network are shown as link failures at a random location, distributed randomly throughout the topology router, there is no need to send updated credit signals to upstream routers involved. However, when Bindu replaces an empty slot, within the router, then both upstream routers, the one connected to the input port where Bindu was originally present and the other connected to the input port which originally had an empty slot, needs to be updated with credits accordingly. Bindu movement unit: It is responsible for the overall movement of Bindu within the router until it leaves the router at a specific period. It comprises of two registers - one holding the Bindu movement period, and the other encoding its path within the router, including the output port connecting to the next router to route the Bindu to. Multi-flit packets with Virtual Cut-Through (VCT) Routers. If the head-flit is present in VC-0 and it is the turn of this VC to turn into a Bindu, the VC is locked till the entire packet arrives and is not allowed to take part in switch arbitration. Once the entire packet arrives, transfer of this packet into the VC previously occupied by the Bindu is performed. The intra router Bindu period is chosen appropriately at design time such that entire packet can arrive and move before the next movement. If the head flit has already left, then the packet is allowed to naturally drain into its downstream VC without moving into the Bindu VC. Multi-flit packets with Wormhole Routers: BINDU, as defined so far, works if VC-0 in each router is VCT, i.e., sized to hold complete packets (while other VCs can be smaller). To implement BINDU in wormhole routers, we would need to support packet truncation within VC-0, like prior works in deflection routing [13]. Bindu Movement Example. We explain the Bindu movement within and across the router, with the example shown in Fig. 4. (1) A Bindu enters the router from the North input port - this effectively means that a packet sitting at the North input port leaves the router to go sit in the South input port of the upstream router, in place of the Bindu, as shown in Fig. 4(a). (2) The Bubble Movement Unit encodes the period and the route of the Bindu within this router. In the example in Fig. 4(b), it moves the Bindu from the North to the East input port, via the Bindu bus. This step is similar to BBR [14]. (3) The Bindu will traverse all input ports sequentially. The last stop of the Bindu will be the input port, whose corresponding output port is connected to the next router in Bindu’s path. NOCS ’19, October 17–18, 2019, New York, NY, USA Mayank Parasar Tushar Krishna Figure 6: Performance of BINDU compared against Deadlock avoidance, Deadlock recovery and BBR for synthetic traffic: Uniform-Random, Transpose and Shuffle. Evaluated for vc=2, 64 node irregular topology derived from 8x8 Mesh. Table 2: Qualitative Comparisons of CBS, BBR and BINDU Figure 7: The graph compares the performance of BINDU with num Bindu=1, 32, 64 respectively with Critical Bubble Scheme and BBR. Graphs are for regular 8x8 Torus topology. (4) The Bindu will use the crossbar to traverse to the neighbor by moving a packet (or an empty slot) at VC-0 of the corresponding input port from the neighbor to the current location occupied by Bindu, as shown in Fig. 4(a). (5) This whole process is now repeated at the neighboring router. Implementation Cost. The hardware overhead of BINDU comes because of addition of Bindu-bus, and Bindu Movement Unit. We used DSENT [21] to estimate the area and power overhead. Area overhead comes around 6% and static power overhead is 5% over baseline router with VC=4. Implementation choice for Bindu: In this work, we proposed to embed Bindu-path inside the router, and Bindu moves through the network along that specified path. Another implementation choice could be to think of Bindu as a dummy packet which moves throughout the network in a cyclic manner and never gets consumed. We then could have the path for Bindu embedded inside the Bindu itself. This would further simplify the router micro-architecture of BINDU, as each router would then only need to read the content of Bindu to know where to route it next. It would also be easier to reconfigure Bindu’s path dynamically by updating the route within Bindu based on some metric. However, the flexibility and reconfigurability comes at the cost of scalability; encoding the Bindu-path will need l ogN × l ogr × r-bits (where N is the number of nodes, and r is the router radix), which can exceed typical flit sizes for large networks. Implementation of Bindu-Path: There could also be multiple ways in which Bindu path can be implemented as shown in Fig. 1. These paths would have different network-performance sensitivity for different irregular topologies. All these design choices are interesting to explore in future work, we, however, do not present these studies in this paper in the interest of space. We assume a snake-like structure for regular topologies, and a tree for irregular topologies. BBR [14] Bubble is empty VC Random proactive bubble movement within router one bubble per router. 8x8 torus requires 64 bubbles BINDU Bindu is reserved VC or dummy packet Proactive Bindu movement as per Bindu-path. one Bindu in the entire network. 8x8 torus requires one Bindu Works for any arbitrary topology Works for any arbitrary topology uses minimal random adaptive routing in-frequent misroute during bubble exchange BINDU uses minimal random adaptive routing At most one packet per Bindu movement flexible in terms of number of Bindus and their path dynamically reconfigurable CBS [7] Bubble Implementation Bubble empty VC is an Movement Minimum Empty Buffers Topologies Routing restriction Misrouting Bubble moves naturally as the packet moves one bubble per ring dimension. 8x8 torus network requires 32 bubbles closed loop/ring topologies, for example Torus uses dimensional order routing in the VC that contains the bubble No packet gets mis-routed Flexibility not flexible not flexible Reconfigurable not urable reconfignot urable reconfig3.7 Comparison with CBS and BBR Here we delineate BINDU from two notable works which use bubble to provide deadlock-freedom to the network. We paraphrase the main points in Table 2. BBR can be viewed as Bindu-64. 4 EVALUATIONS 4.1 Methodology BINDU is evaluated using gem5 [5] with the Garnet2.0 [1] network model and the Ruby memory model. We use DSENT [21] to model power and area for a 11 nm process. Table 3 lists all key configuration parameters for our evaluation. Baseline Networks. We select state-of-the-art baseline deadlockfree networks to compare against BINDU. From deadlock-avoidance, we use escape VCs (which are known to perform better than turnrestriction schemes [16, 17]). From deadlock-resolution, we choose SPIN [17], which has been shown to performs better than Static Bubble [16]. From bubble-based, we choose BBR [14] and CBS [7]. BBR works for arbitrary topologies while CBS only for Torii. BINDU: Deadlock-Freedom with One Bubble in the Network NOCS ’19, October 17–18, 2019, New York, NY, USA Figure 8: Graphs are for Transpose traffic pattern as number of Bindus increase from 1 to 64 in 8x8 irregular Mesh topologies with given fault. Graph-(a) shows the effect of low-load latency. We observe that the affect of number of bubbles on performance, is more for the router with fewer VCs compared to the router with more VCs per input port. All Bindus in BINDU are confined to VC-0 of each input port. Graph-(b) shows the effect of saturation throughput. We observe that with increase in number of Bindus, saturation throughput decreases. Bindu-64 is similar to BBR Table 3: Key Simulation Parameters. Real application simulation parameters 64 cores and RISCV ISA (Ligra), 1GHz 16 cores and x86 ISA (Parsec3.0), 1GHz Private, 16KB Ins. + 16KB Data, 4-way set assoc. Shared, distributed, 64KB, 8-way set assoc. 64B MESI Directory (Ligra) Vnets=5 MOESI hammer (Parsec3.0) Vnets=6 Target Networks irregular 8x8 Mesh (Ligra and Synthetic workloads) irregular 4x4 Mesh (Parsec3.0) 1-cycle 1, 2 and 4 Virtual Cut Through. Single packet per VC 128 bits/cycle Escape VC [11] with Up-Down [18] within Esc-VC SPIN [17] CBS [7]; BBR [14]; BINDU-k (k: num of Bindus) Core L1 Cache Last Level Cache Cache Block Size Cache Coherence Topology Router latency Num VCs Buffer Organization Link Bandwidth Deadlock Avoidance Deadlock Recovery Bubble based Benchmarks. Both real applications and synthetic traffic are used to evaluate BINDU. Applications are drawn from the Ligra benchmark suites [20] and from Parsec3.0 [4] . For synthetic traffic, we focus on uniform random, transpose and shuffle traffic with the mix of 1-flit and 5-flit packet size; results for other traffic patterns are qualitatively similar. The simulator is warmed-up for 1000 cycles, thereafter network statistics are collected by injected fixed number of tagged packets by each node in the system. Simulation completes when all the tagged packets are received. We use an 8×8 irregular network for Ligra and synthetic traffic. Ligra applications have been simulated in syscall-emulation (SE) mode of gem5 while Parsec3.0 applications are simulated on irregular 4x4 network using full system simulation mode in gem5. Topologies. BINDU performance is evaluated on fault-free and faulty 2D mesh networks as well as 2D Torus network topology.To create irregular topologies from 2D mesh, faults are injected randomly into the network while network connectivity is maintained as shown in Fig. 5. For the 8×8 network, we consider a range of faulty links up to 12 in 2D mesh. 4.2 Performance Irregular topologies. Fig. 6 shows the performance comparison of BINDU with other state of the art deadlock-freedom schemes. for irregular 2D 8x8 Mesh. Key-takeaway from this performance graph is that except regular 2D Mesh, BINDU performs comparably to the state of the art solutions. In fact, under certain traffic pattern for example uniform random and shuffle, BINDU performs better Figure 9: (a)Sensitivity of saturation throughput with increase in interrouter Bindu movement period of one Bindu for uniform random traffic. These results are for irregular 8x8 Mesh with VC=2. (b)Uniform-random traffic, VC=2, with Fault-1. The graph shows the extra link traversal over the baseline using minimal deadlock-free routing. Here B-1_P-X means Bindu-1 with ‘X’ as Bindu Movement Period than state-of-the-art. On average, we see 15% improvement over saturation throughput in synthetic traffic pattern using BINDU. Bubble-based Schemes. Fig. 7 compares the performance of state-of-the-art bubble based deadlock-freedom techniques with BINDU for regular 8x8 Torus topology. Since CBS has 32, and BBR has 64 bubbles, we also contrast against iso-Bindu configurations. BINDU provides up to 2.2× higher throughput. Also, the performance of BINDU decreases as the number of Bindus increases in the topology. BBR can be approximately considered as Bindu-64 as now each router has a Bindu/bubble. Therefore, BBR’s performance can be approximated with Bindu-64, and Bindu-32 lies between Bindu-1 and BBR. We observe 35% throughput improvement for VC=2 and 15% higher throughput for VC=4 with BINDU-1 compared to BBR. 4.3 Sensitivity studies 4.3.1 Number of bubbles. Fig. 8(a) shows the sweep of low load latency as the number of Bindus increases in the topology from one to 64. In general, we see low load latency increases with an increase in the number of Bindus, both for regular as well as irregular 8x8 Mesh topology. Affect on low load latency is more prominent for VC=2 than VC=4. Also, sensitivity reduces for higher fragmented topology (for example fault-12) compared to lower fragmented topology (for example fault-1). Similar trends are shown by saturation throughput as the number of Bindus increases, saturation throughput decreases, for both regular and irregular Mesh topology in Fig. 8(b). This happens mainly because of two reasons. Firstly, as the number of Bindus increases, more packets will be misrouted in the network. Secondly, Bindus cannot be consumed therefore they put indirect restrictions on the number of packets that can be injected into the network. Since Bindu only stays in VC-0, we see less sensitivity in saturation throughput when there are many VCs (for example VC=4) compared to when there are fewer VCs (for example VC=2). NOCS ’19, October 17–18, 2019, New York, NY, USA Mayank Parasar Tushar Krishna Figure 10: Packet latency from real workloads and their normalized runtime improvement with BINDU when compared to other state of the art schemes. (a) and (b) are the packet latency and normalized runtime for Parsec while (c) and (d) are the same metrics for Ligra 4.3.2 Inter-router Bindu movement period sweep. Fig. 9-(a) shows the sensitivity of saturation throughput with increase in inter-router Bindu period. The experiment is performed for uniform random traffic pattern with VC=2, with one Bindu in the whole network. We observe decrease in saturation throughput with increase in interrouter Bindu-period. This happens because it takes longer for the Bindu to reach to the packets stuck in deadlock and free them from deadlock. 4.3.3 Energy Overhead. Fig. 9-(b) shows the energy overhead for various schemes, in the form of extra link-traversal per packet, over the baseline assuming ideal minimal routing without any overhead for an irregular mesh with one fault. For escape VC the extra link traversal comes because of the non-minimal Up-Down [18] path a packet takes to reach its destination within the Escape VC. We observe higher overhead at lower injection rate because the fewer the packets in the network, the more sensitive they are to non-minimal path of escape VC. SPIN’s [17] overhead over ideal minimal routing is because of probes used for detecting deadlock, especially at higher loads due to increased forking at intermediate routers. BBR’s [14] link traversal overhead is because of increased bubble-exchange at high injection rate. BINDU’s extra link traversal over ideal minimal routing is because at higher injection rate, there are more packets in the network, hence the likelihood of Bindu replacing a packet, as it moves along its path, increases. In summary, we can see that the additional energy expended by the misroutes due to Bindu movement is negligible at low-loads, and less than 10% post saturation (even with an aggressive Bindu movement period of 1), which is either equal to or much lower than other state-of-the-art solutions. 4.4 Real application results Fig. 10-(a) and (b) show the packet latency and normalized runtime improvement for Parsec3.0 benchmarks respectively. This culminates into 6% average improvement in runtime for fault-0 and 7% average improvement in the runtime for fault-8 in 4x4 mesh respectively. Fig. 10-(c) and (d) show the packet latency and normalized runtime improvement for Ligra applications respectively. There is overall around 5% improvement of BINDU over other schemes, in both average total packet latency and runtime of the application. 5 CONCLUSION BINDU is the first work, to the best of our knowledge, to demonstrate deadlock freedom by reserving a single bubble (empty VC) in the entire network, and pro-actively moving it through all routers and all input ports. BINDU requires no deadlock-detection (unlike deadlock recovery schemes) and requires no turn-restrictions or escape VCs (unlike deadlock avoidance schemes). BINDU is topology-agnostic and provides around 15% average throughput improvement over state-of-the-art techniques in synthetic traffic, and around 7% improvement on an average runtime of real applications. This makes BINDU an effective solution to implement in irregular network topologies to guarantee deadlock-freedom. "
Energy-efficient and high-performance NoC architecture and mapping solution for deep neural networks.,"With the advancement and miniaturization of transistor technology, hundreds of cores can be integrated on a single chip. Network-on-Chips (NoCs) are the de facto on-chip communication fabrics for multi/many core systems because of their benefits over the traditional bus in terms of scalability, parallelism, and power efficiency [20]. Because of these properties of NoC, communication architecture for different layers of a deep neural network can be developed using NoC. However, traditional NoC architectures and strategies may not be suitable for running deep neural networks because of the different types of communication patterns (e.g. one-to-many and many-to-one communication between layers and zero communication within a single layer) in neural networks. Furthermore, because of the different communication patterns, computations of the different layers of a neural network need to be mapped in a way that reduces communication bottleneck in NoC. Therefore, we explore different NoC architectures and mapping solutions for deep neural networks, and then propose an efficient concentrated mesh NoC architecture and a load-balanced mapping solution (including mathematical model) for accelerating deep neural networks. We also present preliminary results to show the effectiveness of our proposed approaches to accelerate deep neural networks while achieving energy-efficient and high-performance NoC.","Energy-Efficient and High-Performance NoC Architecture and Mapping Solution for Deep Neural Networks Special Session Paper Md Farhadur Reza Department of Electrical & Computer Engineering Virginia Polytechnic Institute and State University farhadurreza@vt.edu Paul Ampadu Department of Electrical & Computer Engineering Virginia Polytechnic Institute and State University ampadu@vt.edu ABSTRACT With the advancement and miniaturization of transistor technology, hundreds of cores can be integrated on a single chip. Networkon-Chips (NoCs) are the de facto on-chip communication fabrics for multi/many core systems because of their benefits over the traditional bus in terms of scalability, parallelism, and power efficiency [20]. Because of these properties of NoC, communication architecture for different layers of a deep neural network can be developed using NoC. However, traditional NoC architectures and strategies may not be suitable for running deep neural networks because of the different types of communication patterns (e.g. oneto-many and many-to-one communication between layers and zero communication within a single layer) in neural networks. Furthermore, because of the different communication patterns, computations of the different layers of a neural network need to be mapped in a way that reduces communication bottleneck in NoC. Therefore, we explore different NoC architectures and mapping solutions for deep neural networks, and then propose an efficient concentrated mesh NoC architecture and a load-balanced mapping solution (including mathematical model) for accelerating deep neural networks. We also present preliminary results to show the effectiveness of our proposed approaches to accelerate deep neural networks while achieving energy-efficient and high-performance NoC. CCS CONCEPTS • Computer systems organization → Multicore architectures; • Networks → Network on chip; • Hardware → Network on chip; • Computing methodologies → Neural networks. KEYWORDS Deep Neural Network (DNN); Mapping; Network-on-Chip (NoC) ACM Reference Format: Md Farhadur Reza and Paul Ampadu. 2019. Energy-Efficient and HighPerformance NoC Architecture and Mapping Solution for Deep Neural Networks Special Session Paper. In International Symposium on Networkson-Chip (NOCS ’19), October 17–18, 2019,New York, NY, USA. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3313231.3352377 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’19, October 17–18,2019, New York, NY, USA © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6700-4/19/10. . . $15.00 https://doi.org/10.1145/3313231.3352377 1 INTRODUCTION Neural Networks (NNs) have been widely utilized for solving complex real world problems, such as pattern classification, forecasting, optimization, control and so on, because of its (NNs) remarkable success in many applications, e.g., medicine, finance, and weather. NNs are composed of a large number of simple processing units, called neurons, which are interconnected to form a network. A NN contains input, output and hidden layers of neurons. Deep neural network (DNN) is a NN with more complexity, and it (DNN) contains at least two hidden layers, besides input and output layers. DNN is very popular to solve complex problems as it (DNN) is capable of handling very large, high dimensional data sets, can model complex non-linear relationships, and can discover latent structures in unlabeled, unstructured data. For example, DNN can take thousands of images, and can cluster them according to their similarities, such as human, cat and dog images. Because of the large number of layers and neurons, neurons of the DNN need to be distributed at the different processing nodes (cores) in multi/manycore architecture for high parallelism. As computation (at neuron) is dependent on communication and DNN has high inter-neuron connectivity, DNN needs efficient communication architecture to achieve the required parallelism. Several hardware implementations have been widely used to meet the high speed and real-time response requirements of DNNs, such as systolic array for processing DNN in Google’s tensor processing unit [15]. However, they do not provide generalized, scalable and energy-efficient communication method for processing any kind of DNNs in large-scale multi/many-core architecture. Networkon-Chips (NoCs) have been adopted as a viable solution to manage complex interconnection and provide energy efficient performance in multi/many core systems. NoC can provide high-performance and low-latency communication for neural networks applications because of its (NoC) scalability and parallelism properties [20]. NoC supports parallelism as multiple cores can communicate with each other (for computation) if there are no interference between two communications. NoC provides scalability as resources can be added (capacity increases) in the network without changing the existing components, e.g., routers, timing buffers, etc. A pair of nodes has multiple paths (high path diversity) to reach each other in the NoC. Therefore, NoC is fault-tolerant as nodes (neurons) can communicate even if a component fails because of the path diversity property of NoC. Because of the modular design of NoC, regions or a component of the NoC (not doing any computation or communication) can be switched off (power-gating) without affecting other components. This power-gating capability is a very important feature for large-scale systems, as un-utilized resources NOCS ’19, October 17–18,2019, New York, NY, USA Md Farhadur Reza et al. can be power-gated to reduce static power consumption. Short point-to-point links (wires) in NoC have lower dynamic power consumption than the long wires in bus system. These NoC properties are very much suitable for running large-scale DNN. However, NNs have different types of communication patterns (e.g., one-to-many and many-to-one communication between layers). This makes the task of designing NoC architecture challenging for running DNN. Furthermore, because of the different communication patterns, computations of the different layers of the neural networks need to be mapped in a way that reduces communication bottleneck in NoC. example, when the difference between output error of NNs and actual output (actual output can be known by other methods) is within an accepted error threshold. Figure 1: Multi-Core NoC Architecture A multi-core architecture on 3X 3 NoC is illustrated in Figure 1 where heterogeneous processor tiles are placed in a 2D grid. Each tile contains a processor (with distinct computation power), its local cache, a slice of the shared last-level cache, and a router (configured with different communication capacity) for data and control transmission between the tiles via varying bandwidth links. Data flows from L1 and L2-caches of one core to another core, from core to memory controller for memory traffic, etc. In this work, our main goal is to achieve energy-efficient and high-performance (improves the parallelism) NoC architecture and mapping solutions for accelerating DNN. We discuss communication patterns of DNN in Sec. 2, and NoC design challenges in Sec. 3. We explore different NoC toplogies and then propose the NoC topology for DNN in Sec. 4. Mathematical model of mapping DNN onto NoC architecture is formulated using linear programming in Sec. 5. Then we explore different mapping heuristics and then propose fast heuristic for mapping DNN onto NoC in Sec. 6. Simulation results are demonstrated in Sec. 7 to show the effectiveness of the proposed NoC topology and mapping solutions. Related work is presented in Sec. 8. 2 COMMUNICATION PATTERNS OF NN NNs perform two major functions: learning and generalization. Generalization is the process of accepting a new input vector (at the input layer) and producing an output response. Inter-neuron connection weights is used in order to compute the output. Learning is the process of adapting inter-neuron connection weights in order to minimize a loss or cost function given an input vector. In general, the learning process is stopped according to a given criterion, for Figure 2: A typical Deep Neural Networks A typical DNN is composed of neurons connected to each other, as shown in Fig. 2. The input information is processed from the input layer to the output layer. The network inputs are the inputs of the first layer. The outputs of the neurons in one layer form the inputs to the next layer: the inputs are multiplied by the connection weights and then summed together to produce the outputs, which are scaled by an activation function to produce ’1’ or ’0’. The network outputs are the outputs of the output layer. DNN consists of convolution layers (including pooling layers) and fully connected layers. In convolution layers, each hidden/output neuron is connected to a small region of the input neurons. In fully connected layers, each hidden/output neuron uses all input neurons for computation. Therefore, communication can be very high and diverse between layers of DNN. Communication can consume 30% of the overall energy in a deep learning accelerator [8]. Therefore, DNN presents not only the basic computation patterns of NNs, i.e., matrix multiplication followed by nonlinear activation functions (e.g., sigmoid, Relu etc.) at each layer, but also the intensive communications between the layers. 3 NOC DESIGN CHALLENGES FOR NN Computation has become more energy-efficient with the continued scaling of transistors. Billions of transistors can now be integrated on a single chip. However, interconnection energy efficiency has not been scaled in the same way as that of transistor. For example, from 90nm to 7nm transistor technology, interconnection energy efficiency has been scaled down by only 1.6X compared to 6X scaling down of transistors (computation) [6]. Because of this disparity, energy consumption in the on-chip systems is increasingly being dominated by the interconnection delay [3]. NoC power consumption has been steadily increasing with the increase in the number of cores that are integrated on the chip. Research has shown that NoC power consumption can reach up to 30% of the overall chip power [14]. This high NoC power consumption challenge has led to many development in NoC architecture, for Energy-Efficient and High-Performance NoC Architecture and Mapping Solution for Deep Neural Networks Special Session Paper NOCS ’19, October 17–18,2019, New York, NY, USA example, router architecture enhancement, adaptive routing algorithms, power/thermal-aware designs, etc. Regarding performance, computational parallelism is hampered in multi-core systems as computation can be halted at a core by the data-dependent communication from the remote core or memory. Interconnection between cores imposes a limit on performance gain from multi-core systems [4, 6]. Therefore, energy-efficiency and high-performance are the major design challenges of NoC. On a NN, data are stored in a distributed manner (i.e., the weights of synapses) and participates in the computation through local neurons. Although these architectures greatly mitigate the requirement of memory bandwidth, communication across cores (adjacent and non-adjacent) emerges as a new challenge for energy-efficiency and high-performance. For example, in IBM TrueNorth system, the local neuronal execution within a neurosynaptic core is extremely efficient, while the energy consumption of core-to-core communication increases rapidly with the distance between source and destination [2]: an inter-core data transmission could consume 224x energy of an intra-core one (i.e., 894p J vs. 4p J per spike per hop). As the scale and density of the NN increase, more inter-core interconnections are required, and the effect of inter-core communication quickly becomes a severe design challenge for NoC. 4 EXPLORE NOC TOPOLOGIES FOR DNN We explore the NoC topolgies to check their suitability in terms of energy-efficiency and high-performance for running DNN. Topology is the static arrangement of channels and nodes in NoC. It is difficult to optimize a NoC for performance (latency and throughput) and cost (hardware overhead). Several design parameters are used to evaluate the NoC topology performance, such as hop count, path diversity, router complexity (ports), and bisection bandwidth. Bisection (channel) bandwidth of a NoC is the minimum channel count over all bisections, where bisection is a cut that partitions the entire network nearly in half. Topology should be easy to layout on chip substrate. Topology should meet the wire budgets of the NoC. NoC complexity (arbitration, crossbar, etc.) and area increase with the number of ports at the router. Also router has a physical limitation with the number of ports (at a router) because of the limited number of input/output (I/O) pins at a node. Therefore, topology needs to be efficient in terms of NoC cost (router complexity and area). Because of the complexity of communication patterns in DNN, a NoC with less complexity and cost is needed for running and accelerating DNN. We discuss several promising NoC topologies and then present our proposed efficient topology for DNN. 4.1 2D-Mesh Mesh NoC is easy to layout on-chip because of its (mesh) regular and equal-length links. Mesh network also have high path diversity (multiple shortest paths between source/destination pair), which helps to reduce contention in NoC. Furthermore, path diversity improves the fault-tolerance and load balancing properties of NoC. However, performance in mesh NoC depends on the placement of neurons in different parts of NoC because of the different degrees of the routers in center or edge of the mesh. 4.2 Concentrated-Mesh Concentrated mesh (CMesh) [9, 12] includes more than one core per router. CMesh reduces the number of routers and links to connect the cores (compared to same number of cores in 2D-mesh). This property makes it (CMesh) feasible for integrating large number of neurons with small NoC (less routers and links). CMesh, however, reduces the bisection bandwidth because of the less number of routers and links in the NoC. Furthermore, a router in CMesh needs extra ports for connecting concentrated local cores (attached to that router). Additional port incurs buffer queues, allocators, etc. in NoC. Therefore, only a limited number of cores should be concentrated in CMesh. 4.3 Crossbar Crossbar is efficient for small-scale groups because of its many-tomany communication nature. With crossbar, each neuron can reach each other in one hop. This property makes crossbar effective for communication patterns in NNs. For a small-scale NNs, adjacent layers can be connected with crossbar for many-to-many communication among neurons. However, crossbar incurs high hardware overhead, and it(crossbar) is not scalable for large-scale NNs mainly because of the quadratic increase in the number of crosspoints and increase in link length. 4.4 Ring Router in ring network only requires 3 ports (local port, left and right ports), which is less than the port numbers in 2D-mesh NoC (3 to 5 ports). So, overhead in a ring router decreases, for example, arbitration becomes simpler. Ring network provides higher priority to in-flight packet than injected packet, which reduces contention in NoC. However, ring network has disadvantages too. Hop count (number of hops a message takes from source to destination) and latency (time for packet to traverse network) of ring network increases drastically with the increased number of cores/routers in the NoC. Ring network is difficult to scale as its bisection bandwidth remains constant. Also ring network doesn’t have path diversity. Considering the advantages and disadvantage of ring network, ring network can be used for local neurons in a single layer of NN. As local neurons of a layer do not communicate with each other, ring network can be used for sending data to remote neurons in another network (ring or different kinds). 4.5 Torus Torus NoC solves the router port asymmetry problem in 2D-Mesh NoC. Torus NoC provides high path diversity among neurons. Because of the router port symmetry and high path diversity, torus balances the traffic throughout the NoC. This load-balanced distribution can reduce the latency and improves the throughput of NoC. However, torus has a scalability problem because the length of the long link increases with the NoC size, where wire delay increases with the increase in link length. Torus is also harder to layout because of the presence unequal link lengths. Folded torus [28] is an alternative form of torus NoC. Folded torus removes the long wrap-around links, but it (folded torus) increases the length of all the links (for uniform channel length). NOCS ’19, October 17–18,2019, New York, NY, USA Md Farhadur Reza et al. 4.6 Flattened Butterfly Flattened butterfly [16] exploits high-radix routers to reduce hop count. It (flattened butterfly) also provides high path diversity. It flattens the routers in each row of the conventional butterfly network into a single router. However, it still needs long cable as in conventional butterfly. Also flattened butterfly requires channel (link) counts that are quadratic in the number of interconnected nodes. So, flattened butterfly is not suitable for large-scale DNN because of the complexity of routers and higher number of channels. 4.7 Fat Tree A fat tree is a binary tree where the resources are located at the leafs, and the intermediate nodes act as routers. The fat tree has the advantage that it is recursively scalable and easily partitionable network. In fat tree [27], packets are adaptively routed up to the common ancestor and then routed down to the destination. However, upper levels of the fat tree can create bottleneck, where upper levels are used for routing between neurons. This problem impact can be slightly reduced by allocating a higher channel bandwidth to channels located close to the root node. Fat tree also has path diversity problem as there are no alternative paths between any pair of nodes (other than upper levels). Because of the mentioned problems, fat tree is not suitable for large-scale systems to run DNN. 4.8 Star Star network has a very large node radix, and it suffers from single point of failure problem. Furthermore, higher radix requires more links and port counts at each router. Increasing ports requires additional buffer queues, requestors to allocators, ports to crossbar. Therefore, star network is not suitable for running DNN. However, star topology can be combined with other kind of topologies for use in NoC, like star-ring topology in [17]. 4.9 Express Channel Express channel [13] enabled NoC topology can reduce multi-hop count highly, and so it reduces delay and power dissipation in the NoC. Express channel is implemented by inserting long wire links between remote routers (more than one hop away routers). The number of long links depends on wire budget, and the placement of long links is an NP-Hard optimization problem. Furthermore, wire delay increases for longer link, and this makes long wire link not feasible. To resolve the long link delay problem, different kinds of express channel interconnect technologies, such as wireless [10] and optical interconnect [18], have been studied to improve the NoC performance. However, those technological advancement also introduces huge overhead because of their cost and implementation complexity, for example, wireless interconnect needs on-chip antenna. 4.10 3D-Mesh 3D integration stacks multiple dies in NoC, which decreases network hop count, wire delay, and power consumption compared to conventional 2D integration. 3D NoC [22] has better scalability than 2D NoC for many-core integration in a single chip. However, thermal hotspots (due to heat dissipation) and manufacturing difficulty are the major challenges of 3D integrated circuit (IC). 4.11 Proposed NoC Architecture for DNN Through topology exploration, we derive the NoC topology for low latency and energy-efficient data communications between the layers of the NNs. Topology selection depends on the communication patterns in DNN. We propose CMesh NoC for DNN. CMesh reduces the hop count for communication between two layers, and reduces the number of routers and links (and so area/cost) in NoC for connecting cores compared to regular 2D-Mesh. For a example, in a NoC with 16-cores, total hop count for ring, 2D-mesh, and CMesh are 54, 34, and 16, respectively. Based on the size of NNs, we can map part or full of the two layers at the cores concentrated at a router. This localizes the communication within a tile, and this reduces the data movement through NoC. Because of this reduction in data movement, this architecture can significantly reduce the latency and energy consumption of DNN. Our architecture provides robustness against faulty cores and routers due to manufacturing and ageing defects. The fault-tolerant property originates mainly due to high path diversity of the CMesh architecture. Furthermore, we can disable the faulty cores, and can route the inter-neuron traffic by avoiding faulty cores. A 36-core architecture connected through concentrated 9-routers is illustrated in Figure 3, where cores are placed in a 3 × 3 2D-Mesh topology. Figure 3: 36-core Architecture connected through 3X 3 CMesh NoC 5 MATHEMATICAL MODEL FOR MAPPING DNN ONTO NOC The mapping optimization target is to reduce the congestion on the routers and links of the NoC and also to reduce the energy consumption on the NoC. The proposed mapping algorithm considers power and thermal budgets of the NoC, computational capacity of the cores, communication capacity of the NoC links and computational demand of the neurons. Our mapping algorithm considers both NoC architecture and neurons and mapping is done in a way to reduce the distance of the data communication to improve the energy-efficiency and performance of the running NNs. We initially formulate mapping problem using linear programming model. In our problem, a DNN is presented in a graph with both computation and communication demands of the neurons. Based on the Energy-Efficient and High-Performance NoC Architecture and Mapping Solution for Deep Neural Networks Special Session Paper NOCS ’19, October 17–18,2019, New York, NY, USA neurons demands, a neuron is classified as latency-intensive (communication) or throughput-intensive (computation) neuron, and then mapped to processing (heterogeneous) cores in NoC. Mapping is done with the help of a architecture-neuron-aware model and heuristic presented in this section and Sec. 6, respectively. In the 2D-Mesh NoC, adjacent layers of a DNN are mapped to the adjacent rows and columns in the NoC to reduce the communication distance of the neuron computations (activation functions and weights). Communication with connection weight of ""zero"" is eliminated from the communication. As NNs have a lot of connections among neurons, this elimination of zero weights reduces the communication significantly in the NoC. We propose architecture-neuron-aware mapping for high parallelism and hotspots minimization in NoC systems while configuring NoC (voltage-levels of the cores and routers) at run-time. State-of-the-art contiguous mapping solutions try to map the neurons as close as possible to minimize the overall power consumption in a chip. These minimum-path solutions lead to resource over-utilization and under-utilization problems in multicore system. Another mapping solution is to separate the layers of NNs and map to different regions of NoC. However, isolation of different layers of neurons introduces traffic imbalance in the network because all the neuron traffic have to travel outside the region to communicate with neuron in another layer. Furthermore, neuron isolation solution may not be possible because of resource capacity limitations in the system, e.g., NoC link bandwidth limitation. In our proposed mapping solution, different layers of NNs share the resources and neurons are distributed uniformly to solve resource utilization and hotspots problem and improves the computational parallelism. Moreover, our solution takes the heterogeneous resource capacity (e.g., varying link bandwidth) into consideration during mapping. We assume neurons are mapped to different cores so that several neurons can be executed in parallel to increase the overall parallelism (reducing processor idle time). Let’s assume, a set of K neural layers for mapping. A NN Ak have k layers, each k layer has M variable neurons, and mt h neuron of Ak is represented by T m k . The neuron computation demands and the communication dependency among the tasks are represented by a NN as shown in Figure 4. Each vertex denotes a neuron computation T m k with computation demand (cmp )m into a (cid:112)(n/4) × (cid:112)(n/4) CMesh NoC as presented in Figure 3. A tile k . Each directed edge represents communication k with bandwidth demand of (com)mn from neuron T m k to neuron T n . We assume a many-core chip plane with n processor tiles connected contains multiple cores (in CMesh), the associated router with the cores, and all the associated links of a router, in the NoC. Let (αm be a binary variable to indicate if a neuron T m k is mapped to node i , ρ i be the computation demand at core i , r i be the communication demand at router i , and bw i j be the communication demand on link li j . We assume there are C number of cores integrated with a router. Our objective is to map the neurons of various layers onto multicores in order to minimize the peak energy dissipation (both from computation and communication) at a tile, to avoid energy hotspots chip wide. The load-balanced optimization objective to minimize energy dissipation at a tile i (and to improve the parallelism ) is k )i k Figure 4: An example of DNN communication graph G : max (λ1 · ecmp · ( presented in the following equation, Eq. 1. min c ) + λ2 · ec om · (r i +  bw i j )) ∀ j ∈l i j ρ i 1 ≤c ≤C 1 ≤i ≤n (1) where, ecmp and ec om are the computation and communication energy co-efficients, and λ1 and λ2 ( 0 < λ < 1 ) are the scaling parameters for computation and communication demands. During neuron mapping, the mapping solution also needs to satisfy the power and thermal budget constraints of a chip. Furthermore, the solution needs to meet the maximum computation and communication capacity constraint of a core and a link, which means maximum core and link capacity have to be finite values designed for the chip. Next, we outline the thermal, power, communication, and computation constraints during DNN-NoC resource co-allocation. 5.1 Tile-Level Thermal Constraint Load-balanced mapping algorithm needs to ensure that thermal dissipation of a tile i does not exceed maximum temperature budget thbu d дe t of a chip component. Temperature of a node i is measured using ambient temperature (ta ), power consumption (pw i ), surface area (sa ), and thermal resistance (t r ): thi = ta + (pw i · t r i )/sai . Thermal dissipation is also affected by the temperature effect of the neighbor nodes and their alignment. So, mapping of neurons to cores has a significant impact on temperature budget. For simplicity, thermal heat dissipation thi (in watts) at a tile is calculated by summing up the temperature dissipation of the concentrated cores, the router and associated links attached to that router. + thr ou t e r i 1 ≤c ≤C thc o r e c ) + th l e ak aдe ) i , i j i ≤ thbu d дe t , ∀1 ≤ i ≤ n . (2) thi =  (thl i nk 1 ≤i , j ≤n i j + ( 5.2 Chip-Level Power Budget Constraint Besides tile-level thermal budget constraint, the load-balanced mapping needs to give a solution for DNN without exceeding overall designed power budget pw bu d дe t of the chip. Power consumption of a component is calculated by multiplying energy dissipation with NOCS ’19, October 17–18,2019, New York, NY, USA Md Farhadur Reza et al. execution time. Overall power consumption is calculated by summing up the power consumption of the cores and NoC components (routers and links). Besides dynamic power consumption, leakage power also contributes to the overall chip power consumption. ( pw l i nk i j 1 ≤i , j ≤n +  1 ≤i ≤n (pw r ou t e r i + pw c o r e i ) +  l e ak aдe ) i , i j pw 1 ≤i , j ≤n ≤ pw bu d дe t . (3) 5.3 Communication Constraint NoC links have heterogeneous traffic demands because of the presence of heterogeneous components and traffic patterns of NNs. Hence there is a need for heterogeneous NoC link bandwidth configuration depending on the traffic demands of NNs. A communication from node p to node q go through a set of links depending on the underlying routing algorithm, e.g., XY-routing. Several communication flows can share a link li j . P athp q contains all the links needed for communication between a source and a destination. Total communication traffic on a link li j can not exceed the maximum pre-defined communication capacity bwma x of a link. · (αm k )p · (α n k )q ) ≤ bwma x , bw i j = ( (com)mn 1 ≤p, q ≤n k 1 ≤m, n ≤M ∃l i j ∈P a t hp q 1 ≤k ≤K (4) ∀1 ≤ i , j ≤ n . Depending on communication traffic, voltage-level of the router (and link) can be scaled to reduce the energy consumption. 5.4 Computation Constraint A single chip can have heterogeneous cores with different computational abilities. Let’s assume, each core i has its maximum computation capacity of P . Multiple (independent) neurons can be mapped to a single core. The mapping needs to ensure that the sum of the computation demand of all the neurons assigned to core i does not exceed computational capacity P of core i , i.e., , ∀1 ≤ i ≤ n . ρ i =  (cmp )m k · (αm k )i ≤ P c ap i c ap i c ap i (5) 1 ≤k ≤K 1 ≤m ≤M Based on the computation demand at a core, voltage-level of the core can be scaled to reduce the energy consumption. The objective function along with the power/thermal budgets, communication and computation capacity constraints form a mixed integer linear programming (MILP) model. However, the linear programming solver cannot produce optimal solution for large-scale DNN and NoC within a finite time (to be applicable in real-time systems). Therefore, based on the mathematical model in this section, we explore mapping algorithms and then present the heuristic to produce the mapping solutions for large-scale DNN and NoC within a quick time. 6 EXPLORE MAPPING ALGORITHMS FOR DNN In this section, we explore the mapping algorithms (heuristics) to check their suitability in terms of energy-efficiency and highperformance for running DNN. 6.1 Shortest Path Algorithm Shortest path, as in [21], is the best algorithm for minimizing communication distance between the neurons of the NNs in NoC. Because of the nearest placement of communicating neurons, shortest path algorithm improves communication performance by reducing delay. This reduction in communication distance also minimizes the energy consumption in NoC. However, neuron mapping based only on communication distance can produce unbalanced load distribution: some parts of the NoC are highly utilized, where others parts are under-utilized. Over-utilization of a resource increases contention among packets. So, shortest path solution can create hotspots. Hotpsots can severely hamper the chip performance (e.g., congestion increase, throughput degradation) and reliability (e.g., burn the chip, fault and/or deadlock in the chip). Packets passing through the hotspot area have the higher chance of facing problems, e.g., delay, contention. These problems in a hotspot region can propagate throughout the network as incoming packets will experience problems while trying to use the hotspot resource. That results in blocked resource problem for other incoming traffic as the resources are already occupied by the packets in the hotspot region. The blocked resource problem results in degraded parallelism, where parallelism is very much important for DNN to finish the computation as quickly as possible. 6.2 Load-Balanced Algorithm Another mapping solution is to distribute the loads throughput the NoC, as in [23–25]. The load-balanced mapping solution minimizes the hostspots problem (in shortest path solution) and improves the parallelism in the system. This load-balanced distribution reduces thermal-effects of active nodes (to each other) and increases the thermal design power of the chip. Reduction of the hotspots reduces maximum load at a node and/or a link and balances the load throughout the NoC. Lower hotspots minimizes congestion and queuing delay in the NoC, which results in improved performance. 6.3 Proposed Architecture and Neuron-Aware Mapping Heuristic The proposed mapping algorithm considers the capacity and utilization of the resources in the NoC, and considers the number of neurons and communication patterns in the DNN. During mapping, first node with the most no. of available neighbor nodes is selected for first neuron mapping. Then next neuron that has communication with the already mapped neuron(s) is selected. Then heuristic consider all the available nodes for next neuron mapping, and select the node (depending on constraints in Sec. 5) that yield the minimum peak load (energy) at a node. Total load at a node is calculated by the communication loads (from other neurons) through that node. It can happen that resource capacity (core or link or router) is not available at the concentrated cores for mapping of neurons of the adjacent layer. Then, heuristic considers the closest lowestutilized node for next neuron mapping. In the proposed mapping, neurons of the adjacent layers is mapped closer to each other, where neurons of the same layer can be mapped to non-adjacent location. Refinement phase may be needed to move a neuron from a node to another node (of the adjacent router) if that moving reduces overall energy consumption. Number of neurons to be mapped per node is Energy-Efficient and High-Performance NoC Architecture and Mapping Solution for Deep Neural Networks Special Session Paper NOCS ’19, October 17–18,2019, New York, NY, USA decided based on the number of nodes in the NoC and number of neurons in the DNN. IBM TrueNorth architecture has around 256 neurons per core with 4096 neurosynaptic cores and total 1 million neurons [2]. For a single chip, number of neurons per core depends on both the size of the DNN and the capacity of the cores connected through NoC. Our target is to map the least amount of neurons per node as higher number of neurons (per node) can create congestion in the NoC, and congestion can propagate throughout the NoC and can hamper the progress of other packets (inter-neuron communication) and degrade the parallelism of a DNN. Figure 5: Neuron Mapping onto Concentrated Mesh NoC A sample of neuron mapping in CMesh NoC is presented in Fig. 5. The simplified pseudocode of architecture-neuron-aware Mapping mapping algorithm to avoid hotspots is shown in Algorithm 1. The algorithm initially checks the computation and communication demands of the neuron. The algorithm then maps the neuron to a node (based on the classification), which produces the lowest peak load in the system for power/thermal hotspots minimization. The algorithm also sets the router and core voltage-levels based on the communication demands at the router and computation demands at the core, respectively. During both mapping and NoC configuration assignment, the solution also needs to satisfy computation and communication capacity and power and thermal budgets constraints. The proposed heuristic has a time complexity of O (KMn), where K, M, n are number of layers in a NN, average no. of neurons per neural layer, and no. of cores in the NoC. 7 SIMULATIONS We have generated datasets to simulate DNN on NoC. Generated DNN contains five layers of neurons, where each layer contains 10 neurons. We mapped the NNs onto 256-core, 64-core, and 16-core NoC architectures. Simulated NoC architectures are 2D-mesh and CMesh NoCs. Though we simulated smaller number of neurons and NoC for simplicity, our proposed approach can handle both largescale NNs and NoC. NNs are mapped and run onto different NoC architectures. The proposed architecture-neuron-aware mapping algorithm is implemented using inhouse-developed C++ simulator to deliver mapping solution for large-scale DNN and NoC. The results of the mapping solution are fed to the Garnet [1] on-chip network in gem5 platform [5] for communication performance evaluation in real systems. Garnet/gem5 simulation configurations include: instruction set architecture = ALPHA , interconnect Frequency = 1GHz, no. of virtual networks (VN) = 3, no. of virtual channels (VC) per VN = 4, no. of buffers per VC = 4 and XY-routing [11]. Algorithm 1: Neuron-to-node assignment and NoC configuration heuristic input : Deep Neural Network, NoC Topology Graph output : Neuron Mapping, NoC Configuration for all layers in the deep neural network do for all unmapped neurons in the layer do Select first neuron F T ; select a node F N ← ma x (f i |i ∈ N ); map(F T )=F N ; update neuron list excluding F T ; update topology with F N node load; for all unmapped neurons in the n e x t _l ay e r do find next neuron N T ; generate candidate node list LC ← n i |i ∈ N with link bandwidth configuration along routing paths that satisfy communication and computation capacity, power and thermal budgets select next node N N ←m i n (ma x (Load i |i ∈ N )) in map(N T )=N N ; update neuron list by removing N T ; update topology with N N node load and links weight; configure node voltage-level; if N e x t _l ay e r (cid:44) O u t pu t _l ay e r then Go to the next layer of the deep neural network; LC ; end break; end end else end end return map; Figure 6: Energy comparison of CMesh and Base Mesh Figure 7: Latency comparison of CMesh and Base Mesh Data injection rate of the input neurons of the first layer is defined as the number of packets are input in each node in every cycle. The injection rate between the NN layers is triggered after the neuron collects all the inputs from the previous layer. Maximum 4 neurons are mapped to a single core to reflect the same kind of concentration in CMesh. DSENT [26] tool is integrated with gem5 to evaluate the energy consumption. Simulation results show the NoC performance in terms of energy, latency, and throughput. As shown in Figure 6, the proposed CMesh (64-router) solution reduces energy consumption by 100% compared to that of baseline mesh because of the less data movement through the NoC as communication (between neural layers) is more concentrated on the same router and/or closest routers. Furthermore, by reducing number of routers (while keeping the same number of cores), CMesh (16-router) reduces energy consumption by 250% compared to that of baseline mesh (64-router). The additional improvement comes from the reduction in the routers and links in the CMesh. As NOCS ’19, October 17–18,2019, New York, NY, USA Md Farhadur Reza et al. evenly throughout the NoC. The mathematical model to map the deep neural networks onto NoC is also presented. Simulation results demonstrate that the proposed NoC architecture and mapping solution provides energy-efficient and high-performance solution for accelerating deep neural networks. "
Distributed SDN architecture for NoC-based many-core SoCs.,"In the Software-Defined Networking (SDN) paradigm, routers are generic and programmable forwarding units that transmit packets according to a given policy defined by a software controller. Recent research has shown the potential of such a communication concept for NoC management, resulting in hardware complexity reduction, management flexibility, real-time guarantees, and self-adaptation. However, a centralized SDN controller is a bottleneck for large-scale systems. Assuming an NoC with multiple physical subnets, this work proposes a distributed SDN architecture (D-SDN), with each controller managing one cluster of routers. Controllers work in parallel for local (intra-cluster) paths. For global (inter-cluster) paths, the controllers execute a synchronization protocol inspired by VLSI routing, with global and detailed routing phases. This work also proposes a short path establishment heuristic for global paths that explores the controllers"" parallelism. D-SDN outperforms a centralized approach (C-SDN) for larger networks without loss of success rate. Evaluations up to 2,304 cores and 6 subnets shows that: (i) D-SDN outperforms C-SDN in path establishment latency up to 69.7% for 1 subnet above 32 cores, and 51% for 6 subnets above 1,024 cores; (ii) D-SDN achieves a smaller latency then C-SDN (on average 54%) for scenarios with more than 70% of local paths; (iii) the path success rate, for all scenarios, is similar in both approaches, with an average difference of 1.7%; (iv) the data storage for the C-SDN controller increases with the system size, while it remains constant for D-SDN.","Distributed SDN Architecture for NoC-based Many-core SoCs Marcelo Ruaro marcelo.ruaro@acad.pucrs.br PUCRS, Porto Alegre Brazil Nedison Velloso nedisonnfv@gmail.com PUCRS, Porto Alegre Brazil Axel Jantsch axel.jantsch@tuwien.ac.at TU Wien, Vienna Austria Fernando G. Moraes fernando.moraes@pucrs.br PUCRS, Porto Alegre Brazil ABSTRACT 1 INTRODUCTION In the Software-Defined Networking (SDN) paradigm, routers are generic and programmable forwarding units that transmit packets according to a given policy defined by a software controller. Recent research has shown the potential of such a communication concept for NoC management, resulting in hardware complexity reduction, management flexibility, real-time guarantees, and self-adaptation. However, a centralized SDN controller is a bottleneck for large-scale systems. Assuming an NoC with multiple physical subnets, this work proposes a distributed SDN architecture (D-SDN), with each controller managing one cluster of routers. Controllers work in parallel for local (intra-cluster) paths. For global (inter-cluster) paths, the controllers execute a synchronization protocol inspired by VLSI routing, with global and detailed routing phases. This work also proposes a short path establishment heuristic for global paths that explores the controllers’ parallelism. D-SDN outperforms a centralized approach (C-SDN) for larger networks without loss of success rate. Evaluations up to 2,304 cores and 6 subnets shows that: (i) D-SDN outperforms C-SDN in path establishment latency up to 69.7% for 1 subnet above 32 cores, and 51% for 6 subnets above 1,024 cores; (ii) D-SDN achieves a smaller latency then C-SDN (on average 54%) for scenarios with more than 70% of local paths; (iii) the path success rate, for all scenarios, is similar in both approaches, with an average difference of 1.7%; (iv ) the data storage for the C-SDN controller increases with the system size, while it remains constant for D-SDN. CCS CONCEPTS · Computer systems organization → Multicore architectures; System on a chip; · Hardware → Networking hardware ; KEYWORDS Network-on-Chip (NoC), Software-Defined Networking (SDN), Distributed Management, System-on-Chip (SoC), Many-core. ACM Reference format: Marcelo Ruaro, Nedison Velloso, Axel Jantsch, and Fernando G. Moraes. 2019. Distributed SDN Architecture for NoC-based Many-core SoCs . In Proceedings of International Symposium on Networks-on-Chip, New York, NY, USA, October 17–18, 2019 (NOCS ’19), 8 pages. https://doi.org/10.1145/3313231.3352361 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’19, October 17–18, 2019, New York, NY, USA © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6700-4/19/10. . . $15.00 https://doi.org/10.1145/3313231.3352361 A state-of-the-art Many-Core SoC (MCSoC) contains up to hundreds of homogeneous or heterogeneous processing elements (PEs), leading to significant design complexity. While researchers have proposed solutions to separately provide QoS, fault-tolerance, aging, security, energy, power and temperature management, the next SoC generation must integrate and combine available techniques to solve all these issues simultaneously in a scalable, multi-objective, and self-adaptive management. The need for such comprehensive design drives the research for self-adaptation properties [5]. Networks-on-Chip (NoCs) implement parallel and scalable communication in MCSoCs [6]. To obtain self-adaptation in the communication infrastructure, the MCSoC needs to be able to know and control the NoC resources at runtime. Past works for NoC management focus on hardware-centric approaches, with specific router designs [1, 6]. These approaches have the advantage of highperformance. However, they do not allow effectively to manage the network due to their lack of a holistic view of the system. Software-based management also has been explored to meet specific purposes, as to manage dynamic Time Division Multiplexing (TDM) [20] or Spatial Division Multiplexing (SDM) allocation for real-time guarantees [11]. The Software-Defined Networking (SDN) paradigm [14] is a concept in the computer network field and stands as a promising proposal to be applied for NoC management in MCSoCs. SDN means to remove the communication control logic from the hardware level bringing it to the software level. This approach simplifies the on-chip communication architecture since it promotes a generic and simple communication design paradigm, and also leveraging to a self-adaptive communication infrastructure due to its global view of the system and flexible management. In SDN, routers that conventionally have its design customized for a specific purpose, become programmable hardware units, with the ability to change its connections according to applications’ needs. The routers are configured by a software SDN Controller, which establishes and releases connections between PEs at runtime following a single or multi-objective policy. Recent research highlight the benefits of the SDN paradigm for NoC management in MCSoCs [7, 10, 17], which can be summarized as: (i) hardware complexity reduction; (ii) management flexibility; (iii) real-time guarantees; (iv ) security; (v ) self-adaptation. SDN presents two challenges: (i ) higher path setup latency compared to hardware-based approaches (typically two orders of magnitude higher); and (i i ) lack of scalability due to the adoption of a centralized controller. Challenge (i ) is inherent to software-based approaches, such overhead is not prohibitive when the connections do not need to be made for every packet (which is costly even for hardware approaches), and the priority is flexible and multiobjective management. Recent research [4, 16, 17] show that the NOCS ’19, October 17–18, 2019, New York, NY, USA Ruaro, et al. setup latency can be mitigated establishing connections at the application startup (in parallel o to the object code loading) and releasing at the end of application execution. Thus, the applications use the connections previously established, without the need to wait for new connections during their execution. Regarding challenge (i i ), related work on SDN for NoC assume only centralized controllers, without evaluations for large-scale systems (more than 256 PEs). To achieve scalability, computer networks already adopt distributed SDN management [14]. Thus, motivated by challenge (i i ), this work has as main goal the proposition and evaluation of a distributed SDN architecture for large scale MCSoCs. Routers are grouped into clusters, with one controller per cluster. For local paths (i.e., intra-cluster paths) each controller defines the paths independently and in parallel to other controllers. For a global path, (i.e., extra-cluster paths), the controllers cooperate in a synchronized way to manage the path setup and release. The global path establishment is inspired by VLSI routing [3], with global and detailed routing phases and the adoption of the Hadlock shortest path search algorithm[3]. The contributions of this work can be summarized as follows: (1) a cluster-based SDN architecture; (2) a synchronized path establishment and release protocol among clusters; (3) a heuristic to define short paths among clusters. The rest of this paper is organized as follows. Section 2 discusses related work, positioning our work w.r.t the state-of-the-art. Section 3 presents the cluster-based SDN architecture (contribution 1). Section 4 details the distributed SDN protocol (contribution 2). Section 5 presents the heuristic to define short paths (contribution 3). Section 6 evaluates the SDN proposal, and Section 7 concludes this paper. 2 RELATED WORK Table 1 classifies the related work on SDN for NoCs, detailing the similarities and differences to the work herein proposed. Table 1, 2nd column details the SDN proposal goal. Works [7, 10, 18] use SDN for specific purposes as QoS, security and power saving. Works [2, 4, 8] propose a generic SDN architecture as proposed in this work. The rules implemented inside the controller can be designed according to different policies, for fault-tolerance, security and QoS. Table 1, 3rd column details the adopted NoC architecture that are majority based on a 2D-mesh topology [2, 4, 7, 8, 18], due to its easy implementation and reusability. Berestizshevsky et al. [2] use two physical networks, one for control and another one for the application’s data. We adopt a Multiple Physical Network (MPN) architecture. MPNs enable communication isolation and a higher path diversity by implementing several simple and replicated networks. Previous works show the benefits to adopt MPNs. Authors in [6, 12] compare MPNs to SDM. With an equivalent bandwidth, MPN increases the area following an O (n) complexity, while SDM increases O (n2 ). The path delay increases with the number of subchannels (SDM) with a complexity of O (n), while it is constant for MPN. Yoon et al. [21] present a comparison of MPN and Virtual Channels (VC) and conclude that MPN presents better area scalability. Also, MPNs scale better regarding power dissipation and are better suited for new technology nodes. Table 1, 4th column details the evaluated system sizes (i.e., maximum PE count). An important observation can be drawn from this column: as most works adopt a centralized controller, evaluations are limited to a small number of PE count (≤ 256). The work of [18] addresses up to 1,024 PEs; however, the SDN concept adopted in this work follows a different paradigm, without an SDN controller able to achieve global and self-aware communication management. The system size of 16x16 found in the related work points to a possible limit of the centralized control in SDNs, as empirically observed in [2, 4]. This work aims to address such limitation by proposing a distributed SDN management. Table 1, 5th column qualitatively compares the router complexity. Works [4, 7, 8] embed additional hardware features in the router design increasing its complexity. We argue that in SDN, routers must be as simple as possible, having only the role to forward packets and to accept configuration from a controller. Such design philosophy drives the SDN router design of computer networks [14], which this work also follows, as detailed in Section 3.1. Table 1, 6th column presents works that validate the proposal using RTL simulations. Our work uses a SystemC-RTL model for simulations. We argue this level is necessary since it enables to measure the real overheads of the proposed methods. Virtual simulators as Mininet [7], and Omnet++ [2] can hide communication bottlenecks and deadlocks in an MCSoC context. The last column of Table 1 presents the method to design the SDN controller. Most works adopt a centralized controller [2, 7, 8, 10]. Consequently, the study of distributed SDNs is the main motivation and goal of the work herein presented. 3 SDN ARCHITECTURE This Section presents two SDN architectures, the baseline centralized SDN (C-SDN) architecture (based on [17]), and the proposed Table 1: Related work in SDN for NoCs. Work Goal NoC Architecture Cong et al.[4] 2014 Sand. et al.[8] 2016 Bere. et al.[2] 2017 Scion. et al.[18] 2018 Kostrzewa et al. [10] 2018 Ellinidou et al. [7] 2019 This work Generic SDN Generic SDN Generic SDN SDN for power saving Mesh, 2D, One physical network Mesh, 2D, One physical network Mesh, 2D, Two physical networks System Size Router Complex. 256 70 256 High High Minimal RTL SDN Controller No Yes No Yes Inside each router Centralized, Intra-chip Centralized, Intra-chip Does not exist, made by each OS Mesh, 2D, Two hierarch. networks 1,024 Medium SDN for QoS Torus, 2D, One physical network 10 N.A. Yes Centralized, Intra-chip SDN for security Mesh, 2D, Multiple many-core chips Generic SDN Mesh, 2D, Multiple physical networks 30 High 2,304 Minimal No Centralized, Off-chip Yes Distributed, Intra-chip Distributed SDN Architecture for NoC-based Many-core SoCs NOCS ’19, October 17–18, 2019, New York, NY, USA distributed SDN architecture (D-SDN). Figure 1(a) overviews the assumed baseline MCSoC architecture. It contains a set of PEs interconnected by an NoC. Each PE has a CPU, a local scratchpad memory, a Network Interface (NI), and a Packet-Switching (PS) router. The presence of a shared-memory is also frequent, with the local memory assuming the role of an L1 cache. In Figure1(a), the PEs are homogeneous, but heterogeneous PEs can also be assumed. Shared-memory and heterogeneous systems are orthogonal features to our proposal. Figure 1: (a) Baseline MCSoC, (b) C-SDN, (c) proposed D-SDN with a cluster-based SDN management. Figure 1(b) depicts the C-SDN. This architecture contains an MPN, with one PS subnet and a set of Circuit-Switching (CS) subnets with SDN routers (SR). The subnets are not connected to each other. The PS subnet is shared between best-effort applications’ data and SDN control data. Each PS router has input-buffers (usually 8-flit depth), credit-based control flow, and wormhole packet switching (PS). Figure 1(c) presents the proposed D-SDN. The hardware architecture is the same as the C-SDN. The proposed software architecture comprises of several controllers, and each one is in charge to manage a cluster of SRs. The definition of the cluster size occurs at design-time. 3.1 SDN Router (SR) The SR, Figure 1(b) and (c), is a forwarding unit that connects a given input port to an output port. Instead of using input buffers, SRs use Elastic Buffers (EB) [13]. A crossbar connects the input port with its respective output port. The SDN controller configures the crossbar through a configuration packet. The SR has a configuration interface connected to the NI. When the NI detects the configuration packet in the PS subnet, it drops the packet, avoiding it to be handled by the CPU, extracts its contents, and configures one of the SRs. It has been shown that the 32-bit SR router area (1,177 gates) amounts to 25% of an 8-buffer depth 32-bit PS router, without virtual channels [17]. 3.2 SDN Controllers The C-SDN controller is a software task with exclusive access to OS services and assigned to some PE. The C-SDN controller implements the Hadlock shortest-path algorithm [9] that find paths in a given CS subnet. The CS subnet is selected following a utilization-based heuristic among all CS subnets [17]. The D-SDN management is implemented as a high priority distributed application. Each task of the application is a controller. The controllers work in parallel for local paths. When a global path is requested, the controllers execute the distributed SDN protocol. 4 DISTRIBUTED SDN PROTOCOL This Section details the distributed protocol to establish and release a path. When a given path requester (e.g., application, OS, hardware module, among others) needs a path establishment or release for communicating PEs it sends a PATH_REQ message to the controller of the cluster where the source PE address is located. Such a controller becomes a temporary coordinator, which is responsible for coordinating a global path establishment or release. 4.1 Global Path Establishment Global path establishment requires a consistent global state of the network, achieved through the communication between controllers. A token manager provides a token to one controller of the system at each time. Figure 2(a) presents the global path establishment protocol. The requester asks a path to the controller. The receiving controller becomes the coordinator of the path, storing requests into a FIFO. The FIFO size is equal to the number of system PEs. The coordinator starts to handle a new global path request by removing it from the FIFO, sending a TOKEN_REQUEST message to the token manager, and continues in an I DLE state working on local paths only. When the coordinator receives a TOKEN_GRANT message it begins to execute the path establishment protocol, divided into three phases. Phase 1 - Consistency . Consistency is the phase where the coordinator achieves the global view of network. It sends a BORDER_STATUS _REQ message to all controllers of the system. These controllers stop to handle new requests and start to work in cooperation with the coordinator to define a global path. All controllers execute in parallel the updat e _bor der () function, which creates the status of the cluster border (Bs t a t ). The border status of each cluster is gathered at the coordinator after it receives all BORDER_STATUS_ACK messages. Phase 2 - Path search. The coordinator starts to search for the path. Figure 2(b) overviews the process. The process is hierarchically divided in global and detailed routing. Global routing is a coarse-grain search heuristic that selects the clusters where the path will traverse. Detailed routing is a fine-grain heuristic that defines the intra-cluster routers of the path. After selecting the cluster in global routing, the coordinator sends a DETAILED_ROUTING_REQ to the involved controllers. Both the coordinator and controllers execute the detailed routing. When a given controller finishes the detailed routing, it sends a DETAILED_ROUTING_RESPONSE to the coordinator, informing about path success or failure. If one of the controllers fails in the detailed routing, the coordinator either can choose to re-execute the global routing with different parameters or it concludes that such path cannot be created. If successful, the protocol advances to the last phase. Phase 3 - Path Configuration. With a GLOBAL_PATH_END message the coordinator notifies all controllers that the previous phase ended. Controllers not involved in the path just send a GLOBAL_ PATH_END_ACK message to the coordinator, and are released to handle new path requests. The controllers belonging to the path define their intra-cluster paths by sending configuration packets to each SR router. Note, that the configuration process is also executed in parallel since each controller keeps the list of its path’s NOCS ’19, October 17–18, 2019, New York, NY, USA Ruaro, et al. Figure 2: (a) Global path protocol. (b) Global and detailed routing. (c) Representation of the cluster’s border status (Bs t a t ). routers. After the end of the configuration, the controller sends a GLOBAL_PATH_END_ACK message to the coordinator. When the coordinator receives all messages from the controllers, it concludes the path establishment protocol by sending a PATH_REQ_ACK message to the path requester and releases the token at the token manager. The information related to the border status is mandatory since it represents the free crossing points between clusters. The first phase of the protocol obtains this information. Figure 2(c) illustrates how the updat e _bor der () function computes the border status. The goal is to return the status of the crossing points. The simplest solution would be to gather the status of the input port of the input router (blue router) at current cluster. However, we observed that this approach leads to a poor path success rate during detailed routing, since the router could be blocked due to the usage of all its neighbors’ inputs (as is the case of the blue router of Figure 2(b)). The same applies to the output router (red router) of the previous cluster. In a worst-case scenario, a path can not reach it since all its input ports could be already blocked (this is not the case for the red router because only its south input is blocked). To compute the border status accurately, we evaluate all inputs, as illustrated in Figure 2(c). A path can cross from a cluster to another if the output router can be reached for, at least, one input port, and if the input router has at least one free output port. The communication complexity of this protocol in terms of number of messages is O (4.N + 2.N .GRa + 2.M ), where N is the number of controllers in the system, M is the path’s length (hops), and GRa is the number of global routing re-executions. The computational complexity is related to the updat e _bor der () and global and detailed routing heuristics. The updat e _bor der () complexity is O (4.CSn ), where CSn is the number of CS subnets, and 4 is the border number. Section 5 presents the complexity of both heuristics. 4.2 Global Path Release The requester starts the path release protocol, by notifying the coordinator. The path release is entirely done is software, using one structure (sr _input ) that stores the status of each SR’s input port. Thus, it is not necessary to release physically the SR. The controller releases a path by tracking sr _input from the source router until reaching a cluster border or the target router. When the path release reaches a cluster border, the controller sends a LOCAL_RELEASE_ REQ message to the controller of the next cluster. If the release process reaches the target router, the controller checks if it is the coordinator of the path. If the controller is not the coordinator, it sends a LOCAL_RELEASE_ACK message to the coordinator. The coordinator finishes the path release protocol by sending the response to the path requester. This protocol does not require a token to start its execution because the release process does not require a global view of the system, i.e., it is local to each cluster. The communication complexity in terms of number of messages is O (N ), where N is the number of clusters that the path crosses. The computation complexity is O (N R ) where R is the path’ length in hops. 5 SPESH HEURISTIC This Section details the path establishment heuristic, SPESH ś Short SDN Path Establishment Heuristic. Paths found by the heuristic may not be the shortest ones, because the global routing reduces the path search space due to its coarse grain search method [3]. 5.1 Global Routing Algorithm 1 presents the GLOBAL_ROUTING(). The global routing goal is to find clusters for a given path. The global routing only selects the clusters, and not the entry and exit point of each cluster. Hence, after global routing, the coordinator sends the border information (cluster side and status of each crossing point) to each controller, and each one computes the crossing point in parallel, as part of the detailed routing. Such approach reduces the global routing overhead. The GLOBAL_ROUTING() algorithm assume as inputs: C Sn : number of Circuits Switched (CS) subnets; Sc , Tc : addresses of the source and target clusters, respectively; G Su : global subnet utilization, stores the utilization of each system’s cluster subnet; Distributed SDN Architecture for NoC-based Many-core SoCs NOCS ’19, October 17–18, 2019, New York, NY, USA AVc : available controllers, stores the controllers that failed during detailed routing. Initialized with all controllers as available; G Ra : global routing attempts, stores the number of GLOBAL_ROUTING() calls made by coordinator. Initialized with 1. Incremented when global routing is re-executed; C B : cluster border status, stores the status of all input/output ports that are in the 4 borders of each system’s cluster. The first phase of the protocol, consistency, fills the GSu and C B variables. ALGORITHM 1: GLOBAL_ROUTING The default operation mode of the algorithm is to find minimal global paths, min_path = T RU E (line 4). If the number of attempts to execute the global routing is higher than one (GRa > 1), the algorithm switches to shortest paths, min_path = F ALS E (line 6). The internal loop, lines 9-21, seeks a path for all subnets. Line 10 executes the Hadlock algorithm at cluster level to find a cluster path, returning the path length (P Lc u r r ). If the Hadlock algorithm succeeded (line 11), the retrace procedure is executed, returning the set Ct emp , with the clusters’ addresses in the path (line 12). The next step computes the impact of the path in the subnet utilization (line 13). If the cluster subnet utilization reduces, or if it is the same but with a shortest path length (line 14), the path is set as a candidate path (lines 15-18). The algorithm distributes the communication load because it selects the subnet that achieved lower utilization. This approach also helps to increase the success rate during the detailed routing since the path uses the clusters with a smaller load. Next, the path search mode switches to shortest paths (line 23). If the previous search mode was set to find only minimal paths and the path was not found (line 24), the internal loop executes one more time. Thus, the algorithm explores all subnets, with minimal and non-minimal paths lengths, trying to reduce the subnets utilization. The GLOBAL_ROUTING() algorithm finishes returning the selected subnet, and a set C = {c 1x , y , c 2x , y , . . . } with the addresses of the clusters that the path will cross. With C = ∅ the coordinator sends a GLOBAL_PATH_END message to all controllers finishing the global path search. With C , ∅, the coordinator sends the DETAILED_ROUTING_REQ for the clusters in C , starting the detailed routing at each controller. The coordinator verifies if the detailed routing failed in some cluster at a given subnet after receiving all DETAILED_ROUTING_ RESPONSE messages. The AVc array set the index related to all clusters/subnet where the detailed routing failed as not available for the next global routing execution. The GRa increments if the detailed routed failed in some cluster, which leverages the global routing re-execution. The number of runs that the algorithms make is a trade-off between runtime and success rate to find the paths. Our experiments adopt empirically the number of re-execution to be less or equal to CSn (GRa ≤ CSn ), as we observed that the success rates slightly improve increasing the number of iteration. The global routing complexity is O (CSn · CH ), where CH is Hadlock’s complexity CH = O (X · Y ), with X and Y the numbers of clusters along the X and Y axes. 5.2 Detailed Routing The detailed routing has two steps: (i) from the entry and exit border information sent by the coordinator, select the entry and the exit routers of the cluster; (ii) find the shortest path between these two routers using the Hadlock’s algorithm at router’s level. A controller receiving a DETAILED_ROUTING_REQ message should select two routers addresses to establish the path. If the path starts or ends in the cluster, one of the routers’s address is the source or target routers’ addresses of the path (Sr and Tr ). Otherwise, the path crosses the cluster’s borders. The algorithm tries to align the crossing point to Sr or Tr . This approach maximizes the chances to find a minimal path since it tries to find a router inside a rectangle defined by Sr and Tr addresses. Note that the crossing point selection is synchronized with neighbor clusters without the need for communication between them since they have the same Bs t a t (border status) sent by the coordinator. Figure 3 shows an example of crossing point selection at the borders of cluster 1x1. The entry border is located at the west side, and exit border in the south side. The green dots represent free crossing points, and the black ones represent blocked crossing points. The example adopts a 16x16 system, with an 8x8 cluster. Figure 3: Example of crossing point selection at cluster 1x1. For the entry border, west, the free crossing points indexes are 2, 3, 6. In this case, the crossing point selection algorithm chooses index 6 because it is at same axis than Sr . For the exit border, south, the free crossing points indexes are 2, 3, 4, 5. In this case, the selected index is 5 since it is the nearest index to Tr . The detailed routing executes the Hadlock’s algorithm locally (router’s level) after selecting the routers’ addresses. The Hadlock in this level uses the subnet selected in global routing, and the entry and exit routers addresses selected in the first step. After the Hadlock execution, the controller sends the DETAILED_ROUTING_ NOCS ’19, October 17–18, 2019, New York, NY, USA Ruaro, et al. RESPONSE message to the coordinator informing if the path was found or not. The detailed routing complexity is O (2N + CH ), where N is the cluster border size in number of PEs, 2 is due the double execution of step 1. CH is Hadlock’s complexity CH = O (X · Y ), with X and Y as the cluster dimensions. 6 EXPERIMENTAL RESULTS This Section presents the experimental results. Work [16] evaluates the centralized SDN (C-SDN) against a distributed hardware-based search path algorithm, with an average difference on the path search success equal to 0.23%. Such result allows defining the C-SDN approach as the baseline reference. 6.1 Experimental Setup A SystemC-RTL description models the hardware. The software is modeled in C code (mips-gcc cross-compiler, version 4.1.1, optimization O 2). The CPU running the controller is a Plasma processor [15] (MIPS processor) at 100 MHz. Each scenario has a set of t ot al _path requisitions with the sourcetarget pairs randomically generated according to a locality rate L. The size of t ot al _path is equal to #P E · CSn , corresponding to the maximum number (worst-case) of simultaneous paths that can be established into the system. L is the local path rate, corresponding to the number of local paths in t ot al _path . A synthetic task (requester ) serves as path requester. The number of requester tasks is the same as the number of controllers. The number of path requests per requester is Results evaluate the path establishment, using the following performance figures: Total Latency: Time that each approach needs to establish total_path. It comprises the time from the first PATH_REQ to the last PATH_REQ_ACK. Includes FIFO time and token waiting time. The parallelism between controllers favors this metric when establishing local paths. Setup Latency: Individual path setup latency. Time measured at the coordinator, from the moment that it starts to define a path up to its configuration. Excludes FIFO time and the token waiting time in case of global paths. . t o t a l _p a t h numb e r o f c l u s t e r s . Success rate: s u c c e s s _p a t h s t o t a l _p a t h s 6.2 Scalability Evaluation This section evaluates the scalability of the approach, by varying the system size, CSn , and with L = 0.8. Five system sizes are evaluated: 36 PEs ś 6x6(3x3) (6x6 system, 3x3 clusters), 64 PEs ś 8x8(4x4), 256 PEs ś 16x16(8x8), 1024 PEs ś32x32(16x16), 2304 PEs ś 48x48(16x16). The reason to adopt L = 0.8 comes from the fact that task mapping heuristics try to place communicating tasks near to each other [19]. The goal of the following experiment is to determine when DSDN is faster than C-SDN. Figure 4(a) presents the total latency with D-SDN values normalized in relation to C-SDN. For CSn = 1, the D-SDN is always faster than the C-SDN, reaching an improvement of 69.7% for 2,304 PEs. For CSn = 6, D-SDN outperformed C-SDN for all system’s sizes starting from 1,024 PEs and reaching an improvement up to 51% for 2,304 PEs. The total latency increases with the number of subnets (CSn ) because the space exploration of global routing increase, leveraging the algorithm to perform more re-executions. In summary, the C-SDN presents a better total latency for smaller systems, on average, lower than 256 PEs, while D-SDN outperform C-SDN for the larger system sizes. C-SDN CSn=1, D-SDN CSn=2, D-SDN CSn=4, D-SDN CSn=6, D-SDN 2.4 2.2 2 1.8 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2 ) s e l c y c K ( y c n e t a l p u t e s . g v A CSn= 1,D-SDN CSn= 1,C-SDN CSn= 2,D-SDN CSn= 2,C-SDN CSn= 4,D-SDN CSn= 4,C-SDN CSn= 6,D-SDN CSn= 6,C-SDN 180 160 140 120 100 80 60 40 20 0 36 603 1170 1737 2304 36 603 1170 1737 2304 # Processing Elements (b) # Processing Elements ) . M R O N ( y c n e t a l l a t o T (a) Figure 4: Total and average setup latency for different system sizes and subnets (CSn ), for L = 0.8. While total latency reflects parallelism since the time is measured from the first path setup request until the last path setup ack, the graph of Figure 4(b) presents the average setup latency (AS L) per path. For system sizes below 1024 PEs on average, it is possible to observe that the D-SDN presents a higher AS L. The main reason explaining this result is that in D-SDN, even a short path can traverse two different clusters, requiring the execution of the distributed protocol. Note that the C-SDN AS L increases with the system size, due to the larger path search space. In the D-SDN the cluster size for 1,024 and 2,304 is the same, 16x16. Thus, the search space for local paths (80%) is the same, which explain the small difference between both results. This result unveils the improvements achieved due to the parallel execution of detailed routing and the routers’ configuration phase. Figure 5 evaluates the success rate, setup latency and path length, considering the 1024 PEs system, CSn = 4, and L = 0.8 (other scenarios follows the same behavior). Figure 5(a)(b) presents the success rate for D-SDN and C-SDN, respectively. The success rate is similar for both approaches. Both curves sustain a success rate equal to 100% up to 10% of total_paths for D-SDN and 11.3% for C-SDN. After such path saturation point, the number of established path reduces, reaching 23.7% for D-SDN and 22.76% for C-SDN for total_paths equal to 100%. As mentioned in the experimental setup, the total_paths is a worst-case scenario, with all possible paths in the NoC being requested. Figure 5 (c)(d) depicts the setup latency, each dot corresponding to a path setup. Such graphs should be analyzed with the success rate graphs. In the beginning, with a success rate of 100% and few paths established, the setup latency is lower than 200K clock cycles. Near to the saturation point, the worst setup latency is achieved, with higher latencies observed in C-SDN (again, due to the larger path search space). After this point, both approaches fail to find some paths, which reduces the setup latency. The fact that it is not necessary to configure the routers when a path fails also contributes to reduce the setup latency. Figure 5(c) presents an interesting behavior, the division between global paths (top cluster of points) local path (bottom cluster of points). Such division occurs because the global paths have a setup latency overhead on average 7.3 times over local paths for this scenario (explained bellow). Figure 5(e) presents the path length distribution. The x-axis corresponds to the path length, in hops, and the y-axis to the number of established paths (973 for D-SDN and 932 for C-SDN). The C-SDN applies a centralized shortest path algorithm (Hadlock), enabling             Distributed SDN Architecture for NoC-based Many-core SoCs NOCS ’19, October 17–18, 2019, New York, NY, USA 2000 1800 1600 1400 1200 1000 800 600 400 200 0 0 20 40 60 Total paths (%) 80 100 e S t p u l a t e n c y ( K c y c l s e ) D-SDN Local paths Global paths 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 0 20 40 60 Total paths (%) 80 100 u S s s e c c a r t e D-SDN 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 0 20 40 60 Total paths (%) C-SDN 80 100 u S s s e c c a r t e C-SDN 2000 1800 1600 1400 1200 1000 800 600 400 200 0 0 20 40 60 Total paths (%) 80 100 e S t p u l a t e n c y ( K c y c l s e ) 0 10 20 30 40 50 60 70 80 0 10 20 30 40 50 60 a P # t h t s e b a i l s h d e Path lenght (hops) D-SDN C-SDN 4.2% 4.7% 0.6% -1.0% -0.7% 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 36 64 256 1024 2304 u S s s e c c a r t e # Processing Elements D-SDN C-SDN (a) (b) (c) (d) (e) (f) Figure 5: (a,b) Success rate evaluation; (c,d) setup latency evaluation; (e) path length distribution; (f ) success rate for different system’s sizes, CSn = 4, L = 0.8. this approach to find paths not reached by the distributed approach. According to the graph, C-SDN longest path has 64 hops, while D-SDN 57 hops. Note that, while in C-SDN the saturation point is reached later (13% better - Figure 5(a,b)), the C-SDN also has longer paths, creating more congestion and explaining its lowest final success rate. Thus, D-SDN has a higher number (10%) of established paths below 20 hops (843 against 767 for the C-SDN). Figure 5(f ) details the success rate for different systems sizes. The rate is slightly higher in C-SDN for system smaller than 256 PEs but lower in larger systems. The threshold of 256 PEs can also be observed for the other subnets. Below 256 PEs, the C-SDN outperforms D-SDN on average by 4.9%, and from 256 PEs, the D-SDN outperform C-SDN on average by 0.4%. The overall success rate difference between both approaches is 1.7%. Such difference allows us to conclude that the smaller path search space exploration does not negatively impact the path quality provided by D-SDN. The experiment depicted in Figure 6 evaluates the impact of the cluster size. This experiment adopts the 48x48 system, CSn = 4 and L = 0.8, with four different cluster sizes All four D-SDN scenarios achieve better performance than C-SDN, and the 16x16 case enjoys the lowest latency. The C-SDN latency is constant due to the absence of clusters. In general, we conclude that cluster size affects performance significantly. Hence, it is mandatory to identify the optimal configuration for a given system. 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 8x8 (64) 12x12 (144) 16x16 (256) 24x24 (576) u S s s e c c R a t e Cluster size (#PEs) D-SDN C-SDN 0 20 40 60 80 100 120 64 160 256 352 448 544 A g v . e s t p u l a t n e y c ( K c y c l ) s e Cluster size (#PEs) D-SDN C-SDN (a) (b) Figure 6: Cluster size: (a) Success rate, (b) Avg. setup latency. 6.3 Locality This Section evaluates the locality effect in the C-SDN and D-SDN approaches. Figure 7 presents experimental results with L varying from 0 (all paths being global) to 1 (all paths being local). This experiment adopts the 32x32(16x16) system, and CSn = 4 (other CSn presents similar behaviour). Figure 7(a) addresses the total latency. As expected the D-SDN total latency reduces as the number of local paths increases (higher parallelism). D-SDN achieves the worst latency (on average 30%) for scenarios with L<0.7. This result is due to the higher number of global routing runs. For scenarios with local paths with L ≥ 0.7, the D-SDN outperforms C-SDN on average by 54%. We argue that scenarios presenting L ≥ 0.7 are those that correspond to real workloads due to the tasks mapping heuristics that favors local paths [19]. Figure 7(b) shows the success rate for this experiment. It is possible to observe that both approaches present a similar success rate, with a difference not higher than 1%. Also, the success rate is a function of the locality, because global paths lead to congestion, which reduces the number of available paths. 0.1% 0.9%-0.6% -0.5% 0.6% -0.1% 0% -0.7% -1% 0.1% -0.2% 0 0.05 0.1 0.15 0.2 0.25 0.3 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 u S s s e c c a r t e L D-SDN C-SDN 0.E+00 1.E+08 2.E+08 3.E+08 4.E+08 5.E+08 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 o T t a l l a t n e y c ( c y c l ) s e L D-SDN C-SDN (a) (b) Figure 7: Locality: (a) Total latency, (b) Success rate. 6.4 D-SDN Protocol and Memory Overheads Figure 8(a) presents the relationship between the setup latency to establish global and local paths (y-axis), with the same Manhattan distance, for different D-SDN system sizes (x-axis). According to the results, the distributed protocol imposes a significant overhead in small system sizes, e.g., the global path setup latency is 21.9 times higher for the smallest system and CSn = 4. This graph represents the trade-off between the time to execute local and global paths. For small cluster sizes, the local path search is much faster than the search of global paths. The relationship reduces to 5ś10 times by increasing the cluster size. This result follows the scalability evaluation related to the cluster size, where 16x16 clusters achieved the best results. The second evaluated overhead is the memory footprint. The algorithm size (text segment) corresponds to ≈ 17 Kbytes for D-SDN and ≈ 7 Kbytes for C-SDN. Figure 8(b) presents the data memory requirement per controller. C-SDN presents a linear growth, while                                 NOCS ’19, October 17–18, 2019, New York, NY, USA Ruaro, et al. 0 5 10 15 20 25 30 35 40 36 603 1170 1737 2304 G l a b o l a p t h r e v o r e v o d a e h l a c o l a p t h s e s ( t p u l a t e n y c a r t e ) # Processing Elements CSn = 1 CSn = 2 CSn = 4 CSn = 6 0 10 20 30 40 50 60 70 80 90 36 603 1170 1737 2304 D a t a r e p n o c r t o l l r e ( K y b t ) s e # Processing Elements D-SDN, CSn = 1 C-SDN, CSn = 1 D-SDN, CSn = 2 C-SDN, CSn = 2 D-SDN, CSn = 4 C-SDN, CSn = 4 D-SDN, CSn = 6 C-SDN, CSn = 6 (a) (b) Figure 8: (a) Overhead of global vs. local paths with same Manhattan distance. (b) Data requirements per controller. in D-SDN the memory footprint is a function of the cluster size, and not to the system size. Thus, the D-SDN memory footprint scales with the system size, while the C-SDN not. 6.5 Individual Setup Latency This last experiment adopts a different data set from previous ones. The experiment consists in requesting only one path with its source and target addresses located at the opposite corners of the system, thus exploring the maximum Manhattan distance for a given path. The goal of this experiment is twofold: (i) evaluate the performance for a scenario without congestion; (ii) evaluate how the setup latency behaves increasing the number of clusters. The cluster size is equal to 8x8 for all scenarios, and only the system size changes. Figure 9 compares the setup latency for D-SDN and C-SDN for this single path request. It is possible to observe that D-SDN always outperforms C-SDN. The improvement increases with larger systems regardless of the number of clusters, reaching up to 85.5% for 36 clusters (48x48(8x8)). The lower setup latency of D-SDN is due to the parallel phases of detailed routing and path configuration. 0 50 100 150 200 250 300 4 8 12 16 20 24 28 32 36 e S t p u l a t e n c y ( K c y c l s e ) # Clusters D-SDN C-SDN Figure 9: Setup latency w.r.t cluster number. ACKNOWLEDGMENTS We acknowledge financial support by ÖAW Austria and CAPES/FAPERGS Brazil ś Fernando Gehm Moraes supported by FAPERGS (17/2551-0001196-1 and 18/2551-0000501-0) and CNPq (302531/2016-5) and Marcelo Ruaro by CAPES/FAPERGS (88887.196173/2018-00). 7 CONCLUSION AND FUTURE WORKS This work proposed and evaluated a distributed SDN architecture for the management of MCSoCs. The D-SDN has an inherent challenge that is the synchronization protocol. However, the cost of the protocol becomes relatively lower as the system grows. On average, D-SDN outperformed C-SDN in total latency in systems larger than 256 cores without loses in success rate. The goal of our proposal is to adopt SDN in systems with hundreds of PEs. Using larger systems, the results showed that the amount of memory for the controllers and the total latency for the establishment of paths scale with the size of the system. Also, the path quality, considering the path length and the success path establishment rate, does not suffer from the distributed approach, being slightly better than the C-SDN approach. Therefore, the proposed D-SDN approach advances the state-of-the-art related to a self-adaptive management of large NoC-based MCSoCs. Future works are directed to develop a fault-tolerance protocol among controllers and providing QoS and security services for user’s applications based on the distributed SDN architecture. "
ClusCross - a new topology for silicon interposer-based network-on-chip.,"The increasing number of cores challenges the scalability of chip multiprocessors. Recent studies proposed the idea of disintegration by partitioning a large chip into multiple smaller chips and using silicon interposer-based integration (2.5D) to connect these smaller chips. This method can improve yield, but as the number of small chips increases, the chip-to-chip communication becomes a performance bottleneck. This paper proposes a new network topology, ClusCross, to improve network performance for multicore interconnection networks on silicon interposer-based systems. The key idea is to treat each small chip as a cluster and use cross-cluster long links to increase bisection width and decrease average hop count without increasing the number of ports in the routers. Synthetic traffic patterns and real applications are simulated on a cycle-accurate simulator. Network latency reduction and saturation throughput improvement are demonstrated as compared to previously proposed topologies. Two versions of the ClusCross topology are evaluated. One version of ClusCross has a 10% average latency reduction for coherence traffic as compared to the state-of-the-art network-on-interposer topology, the misaligned ButterDonut. The other version of ClusCross has a 7% and a 10% reduction in power consumption as compared to the FoldedTorus and the ButterDonut topologies, respectively.","ClusCross: A New Topology for Silicon Interposer-Based Network-on-Chip Hesam Shabani Lehigh University Bethlehem, PA, USA hes318@lehigh.edu Xiaochen Guo Lehigh University Bethlehem, PA, USA xig515@lehigh.edu ABSTRACT The increasing number of cores challenges the scalability of chip multiprocessors. Recent studies proposed the idea of disintegration by partitioning a large chip into multiple smaller chips and using silicon interposer-based integration (2.5D) to connect these smaller chips. This method can improve yield, but as the number of small chips increases, the chip-to-chip communication becomes a performance bottleneck. This paper proposes a new network topology, ClusCross, to improve network performance for multicore interconnection networks on silicon interposer-based systems. The key idea is to treat each small chip as a cluster and use cross-cluster long links to increase bisection width and decrease average hop count without increasing the number of ports in the routers. Synthetic traffic patterns and real applications are simulated on a cycle-accurate simulator. Network latency reduction and saturation throughput improvement are demonstrated as compared to previously proposed topologies. Two versions of the ClusCross topology are evaluated. One version of ClusCross has a 10% average latency reduction for coherence traffic as compared to the state-of-the-art network-oninterposer topology, the misaligned ButterDonut. The other version of ClusCross has a 7% and a 10% reduction in power consumption as compared to the FoldedTorus and the ButterDonut topologies, respectively. CCS CONCEPTS • Hardware → Communication hardware, interfaces and storage: Networking hardware. KEYWORDS Silicon interposer, die stacking, network-on-chip, topology ACM Reference Format: Hesam Shabani and Xiaochen Guo. 2019. ClusCross: A New Topology for Silicon Interposer-Based Network-on-Chip. In International Symposium on Networks-on-Chip (NOCS ’19), October 17–18, 2019, New York, NY, USA. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3313231.3352363 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’19, October 17–18, 2019, New York, NY, USA © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6700-4/19/10. . . $15.00 https://doi.org/10.1145/3313231.3352363 1 INTRODUCTION As the number of transistors increases, more processor cores can be integrated into a Chip Multi-Processor (CMP) to boost the computation throughput. With the invention of High Bandwidth Memories (HBMs) [20], memory bandwidth can also be significantly improved by connecting multiple 3D-stacked DRAMs to processor chips through silicon interposers to satisfy the overall demands from the processors cores. Each processor core, however, might need to access multiple memory locations and the increased number of cores also escalate coherence traffic among the cores. The on-chip networks are facing fundamental challenges to enable the scalability of the CMPs and to satisfy both the coherence and memory traffic demands. In the meanwhile, with the increasing number of cores, on-chip power consumption is about to exceed the total power budget due to the limitation of the power delivery network and thermal dissipation capability. On-chip network designs have to be power efficient to meet the system power constraint. Inspired by silicon interposer-based memory integration (e.g., HBM) which is also referred to as 2.5D integration, recent studies [16] proposed the idea of disintegration by taking apart a large system into smaller parts by using the interposer-based integration to improve overall yield. This is because a smaller chip has fewer components and hence is less likely to catch defects. Having multiple smaller chips instead of a big chip also provides modularity and a defective small chip can be replaced at a lower cost when re-integrated through interposers. Nevertheless, as multiple smaller chips are integrated through the interposers, the amount of chipto-chip communications are increased. Heavy traffic through the interposers can become a performance bottleneck [16]. Moreover, any processor core can also access different parts of the on-chip memories. Hence, the memory traffic also needs to pass across different chips through the interposers. Even though disintegration can improve the yield and reduce the fabrication cost, the interconnection network can become a performance bottleneck if it is not carefully designed to overcome the challenges posed by the interposer-based multi-chip systems. Topology is one of the most important elements in interconnection network design, which has a direct influence on network performance. In interposer-based systems, memory traffic can compete with coherence traffic for bandwidth [16]. The network topology should be designed to reduce such contention by increasing the number of links and bandwidth on segments that are critical to both memory and coherence traffic. In this work, a new interconnection network topology, ClusCross, is proposed for silicon interposer-based multi-chip systems. This topology is based on the idea of clustering. In order to decrease the network diameter and increase the cross-chip bandwidth, ClusCross NOCS ’19, October 17–18, 2019, New York, NY, USA Hesam Shabani and Xiaochen Guo maps a cluster of routers onto each small chip and increases the number of cross-cluster long links. As a result, the proposed topology can increase path diversity and bisection bandwidth, which can help to reduce contentions between memory and coherence traffic. In addition, the use of cross-cluster long links can effectively reduce the number of hop counts for long-distance communication in both memory and coherence traffic. The main contributions of this paper include: • Two versions of ClusCross on-chip network topology are proposed with the aim of improving network performance in NoC-on-interposer systems through decreasing average hop count and increasing cross-chip bandwidth by leveraging long links. • Performance and cost of the proposed ClusCross topologies are evaluated and compared against other existing topologies using synthetic memory and coherence traffic. • System performance of ClusCross topologies is evaluated using the PARSEC suite traces that are appropriate for CMP assessment. The rest of the paper is organized as follows: In Section 2, a brief overview of the interconnection networks based on silicon interposers is provided and related work for both conventional NoC topologies and topologies for silicon interposer systems are discussed. Section 3 presents the structure of the ClusCross and two versions of this topology. In Section 4, evaluation results are shown using both synthetic traffic patterns and real applications. The proposed topologies are compared against other topologies designed for interposer-based systems. Section 5 concludes the paper. 2 BACKGROUND AND RELATED WORK 2.1 Interposer-Based Interconnection Networks Technology scaling does not benefit wires as much as it does to transistors [6]. On-chip communication becomes a bottleneck for both power consumption and performance. Three dimensional (3D) integration promises to bring processing elements and memory components physically close to each other to reduce wire distance and hence overcome the communication bottleneck. True 3D integration, however, requires through-silicon vias (TSVs), which is complicated to implement on processor dies and might introduce severe thermal issues and die yield reduction [16], [8]. As an alternative, individual chips can be connected to silicon interposer layer through micro-bumps. Hence, memory and processor chips can be connected through a layer of silicon interposers on a substrate die to increase memory bandwidth. Since interposer integration does not need TSVs in the silicon interposer layer, higher die yield and additional routing capabilities are provided for the system [22]. In addition, interposer-based systems have lower manufacturing and R&D cost as compared to the true 3D integration [22]. Although the physical design of the interposer integration also has technology-related challenges such as thermal management and pin assignment [24], these challenges are solvable in near term [22]. Consequently, interposer-based systems are the most promising near-term solution for die-stacking integration. Several commercial products of interposer-based ICs are already on the market [18], [19]. For example, the HBM uses TSVs to integrate stacks of DRAM dies and connects the DRAM stacks to processor die using silicon interposers. Multiple processor chips can be connected through silicon interposers as well, which is used by [16] as a design method to improve yield. This previous work [16] shows that there is a trade-off between yield and performance when changing the chip size. Based on this study, partitioning a 64-core system into four smaller chips achieves the best performance and yield trade-off. Figure 1 shows an example of an interposer-based system, which consists of four 16-core processor chips, an interconnection network, as well as four HBM DRAM stacks placed on the left and right side of the processor dies. Figure 1: An illustration of a 64-core system composed of four 16-core processor chips and four HBM DRAMs. The HBMs and processor dies are connected through a interposer layer. There are two types of the interposer layer: active and passive. An active interposer layer includes both interconnection links and routers, which requires transistors to be built on the interposer layer and hence can increase cost. A passive interposer layer [2], [7] has no transistors and has only interconnection links. A passive interposer layer tends to have a lower cost and higher yield as compared to the an active interposer layer due to the absence of transistors. When designing interconnection networks for interposer-based systems, it is important to use minimum number of transistors. Therefore, minimizing the number of routers and links on the interposer layer should be one of the design goals. 2.2 Conventional NoC Topologies Network topology has a significant impact on communication latency because the number of the required hops for transmitting a message from a source to a destination node through routers can vary on different topologies. Moreover, the hop count influences network bandwidth and the power dissipation on interconnection networks. Typically, the smaller the average hop count is, the better the network performs. In addition, the bisection width has a determinant role in network throughput. Another factor to consider is whether the network can have deadlocks. It is better to have a deadlock-free routing algorithm because detecting and resolving deadlocks could introduce significant latency overheads [21]. Mesh and Torus topologies are commonly used in conventional network-on-chips because of their simplicity and regularity [23]. The main difference between these topologies is that there are additional long links in Torus networks for connecting the edge nodes. Leveraging these long links can improve load balance on the links and increase path diversity, which helps the communication paths to be quickly reconfigured to use alternative paths and can lead to have more efficient data transfer and link utilization [5]. Moreover, ClusCross: A New Topology for Silicon Interposer-Based Network-on-Chip NOCS ’19, October 17–18, 2019, New York, NY, USA adding long links can reduce network diameter, which is defined as the maximum distance between any two nodes. Nevertheless, implementing long links typically requires repeaters to be inserted to optimize energy and delay. Long links can consume more dynamic power in comparison with short links. Therefore, adding long links has a trade-off between performance and power. Network topology also influence the complexity of router designs. The number of incoming and out-going links at each node determines the sizes of the input buffers and crossbar switches. Therefore, topologies with lower node degree is simpler to implement. In addition to the router complexity, the total number of links also directly influence the implementation cost and total power consumption. Mesh is cost-efficient in comparison with Torus because it has fewer links. FoldedTorus was proposed [4] to reduce the length of the long links in Torus, which improves performance. Increased the number of long links in FoldedTorus, however, leads to an increased area. The design principle of the proposed ClusCross topology can be applied to both general-purpose on-chip networks and interposerbased systems. Section 3.2 discusses an example of general-purpose ClusCross, in which the maximum number of ports in the routers is kept the same as it is in Mesh, Torus, and FoldedTorus. 2.3 Silicon Interposer-Based Topologies In a interposer-based system, the connection between processor die to the interposer layer is through micro-bumps, which has a 50µm pitch width. To accommodate this relatively large pitch width, the number of connections through micro-bumps is limited. Therefore, the network topology for interposer-based system typically uses concentrated nodes [1] to reduce routing nodes on the interposer layer [13]. Moreover, concentrated topologies reduce the average hop count to reach destinations for memory-bound requests. Prior work [13], [16] used a 4-to-1 concentration to design network topology for interposer-based system, which requires 8-port routers in the network. Aligned and misaligned topologies are two types of concentrated topologies which have been proposed for interposer-based system with a minimally active area on the interposer layer. The key difference between aligned and misaligned topologies is that the misaligned topology places routers in between chips on the interposer layer, whereas the aligned topology only places links in between chips on the interposer layer. It is important to consider the cross-chip traffic in the interposer network because the cross-chip traffic includes both the coherence traffic and memory traffic. Unlike the coherence traffic within the chip, which can be transferred on chip or on the interposer, the cross-chip traffic has to be transferred on the interposer layer. Hence, the utilization of the middle links, which are also the bisection-crossing links, are important in interposer-based systems. In aligned topologies, the links that cross chips are shared between coherence and memory traffic. When both core-to-core coherence messages and memory messages want to pass through the cross-chip links in the interposer layer, messages queue up and then serialize behind each other to pass through the link. Whereas, in misaligned topologies, routers are the shared resources across chip, which allows both coherence and memory traffic to traverse through the routers at the same time. Therefore, the misaligned topologies are better at reducing queuing delays for massages to pass across chips [16]. In this work, we focus on the misaligned topologies because they tend to have better performance in comparison with aligned topologies [16]. The misalignment can be applied in X dimension (x) or X and Y dimensions (x+y) and it depends on the topology structure. FoldedTorus can be misaligned in both X- and Y- dimensions, whereas butterfly structures only can apply misalignment in the x dimension because adding one misaligned row to the butterfly topologies can change their structures. When misalignment is applied in one dimension or two dimensions, the total number of nodes in network can change as well. Accordingly, misalignment in both dimensions increases the total number of nodes and links in topology structure which is not cost and power efficient [16]. On the other hand, since minimizing active area in the interposer layer is preferred for NoC-on-interposer, topologies that have fewer number of nodes and links are preferred. In the meanwhile, it is desired to have topologies with smaller network diameter and lower average hop count. Therefore, topologies that misaligned to one dimension (x) are typically better for NoC-on-interposer systems. Misaligned ButterDonut x and FoldedTorus x [16] have been proposed for NoC-on-interposer systems, which are demonstrated in Figure 2. ButterDonut x topology is designed based on the idea of increasing bisection width without the need to add more ports to routers and this topology adds more long links. The ButterDonut x topology has better performance as compared to the butterfly topologies evaluated in previous work [16] because it has more bisection width and fewer links. For example, ButterDonut x has twelve East-West bisection links, and eight North-South bisection links. These topologies are subject to network deadlock because there are rings in their structure. Virtual channels [5] and bubble flow control [3] are two approaches which are widely used for avoiding deadlock. ButterDonut x topology uses flit-level bubble flow control with extra virtual channels in X-dimension to prevent deadlock from happening because the ButterDounut x topology only has rings in X-dimension [16]. Figure 2: Illustrations of existing misaligned interposerbased topologies. (a) FoldedTorus x and (b) ButterDonut x. Both ButterDonut x and FoldedTorus x use relatively short long links and many of these long links do not cross multiple chips. To use long links more efficiently, the proposed ClusCross topologies use fewer but longer long links to increase the bisection width. 3 CLUSCROSS TOPOLOGIES This section presents the proposed ClusCross topology for both the interposer-based systems and as a general-purpose NoC network topology. The key idea of ClusCross is to map network clusters NOCS ’19, October 17–18, 2019, New York, NY, USA Hesam Shabani and Xiaochen Guo Figure 3: Illustrations of two versions of ClusCross topology. (a) ClusCross x-v1 and (b) ClusCross x-v2. Table 1: A comparison of interposer-based topologies. Topology (20 nodes) Number of Links/Long links Diameter CMesh x ButterDonut x FoldedTorus x ClusCross x-v1 ClusCross x-v2 31/0 36/28 40/22 38/14 40/16 7 4 4 4 4 N-S/E-W Avg. Hop Bisection Links 4/5 12/8 8/10 10/12 12/14 3.63 3.18 3.18 3.03 2.98 onto disintegrated multi-core chips and use cross-cluster long links to increase cross-chip bandwidth. This topology can be generalized to other multi-core systems without interposers as well. Section 3.2 presents network characteristic comparison between the generalpurpose ClusCross with other general-purpose topologies. 3.1 ClusCross Topologies Designed for Interposer-Based Systems Two versions of misaligned ClusCross x are proposed based on the architecture that disintegrates 64-core to four 16-core chips and re-integrates them through silicon interposers. Both versions use long links to connect nodes across the chips and clusters, which can reduce network diameter and add more bisection bandwidth without increasing the number of ports in the routers. The structure of the two versions of ClusCross with 16 memory nodes are illustrated in Figure 3. The Figure shows long links without regular layout for easier illustration, but standard Manhattan routing is used for all wires in the evaluation. The design principle of these two versions of ClusCross is to start from a Mesh topology and replace some of the cross-cluster short links with cross-cluster long links. These topologies are designed to use only a few long links to increase bisection bandwidth and path diversity. Smaller average hop counts in these topologies can result in reduced network latency, which will be evaluated in Section 4. The ClusCross x-v1 topology has fewer links, which has lower cost and power consumption as compared to the ClusCross x-v2 topology. ClusCross x-v2, however, increases the number of bisection width both vertically and horizontally, which leads to increased throughput saturation for memory traffic and coherence traffic. The number of East-West bisection links has influence on network throughput for memory traffic because the four HBMs are located on the left and right of the system. Adding East-West links to connect the chips on and left and right half of the system can reduce contention caused by memory traffic [16]. In addition, the total number of bisection links, East-West and North-South, Table 2: A comparison of general-purpose topologies. Topology (64 cores) Mesh Torus FoldedTorus ClusCross Number of Links/Long Links 112/0 128/16 128/32 128/28 Diameter 14 7 7 6 Bisection Links 8 16 16 22 Node Degree 2-4 4 4 4 have an impact on the network throughput for passing core-to-core coherence messages. Table 1 presents important network parameters for the two versions of ClusCross x and three existing concentrated misaligned topologies. All five topologies have 20 concentrated router nodes on the interposer layer, four 16-core chips, and four HBMs with a total of 16 memory channels. Each node underneath the chips is connected to four cores, each node on the edge of the chip is connected to two cores and two memory channels, and each node between two chips is connected to four cores (two cores on each chip). CMesh x is a concentrated misaligned mesh topology, in which the 20 concentrated nodes are connected in a mesh topology. All four topologies with long links have smaller diameter as compared to CMesh x. The two versions of ClusCross have fewer long links as compared to ButterDonut x and FoldedTorus x, but the long links in ClusCross are longer as compared the the long links in ButterDonut x and FoldedTorus x. Hence, the average hop counts are reduced. The design of ClusCross intentionally uses cross-cluster long links. As a result, the N-S and E-W bisection links are increased. ClusCross topologies are susceptible to network deadlock because of existing rings in their structure. Extra virtual channels [5] are applied for maintaining deadlock freedom in these topologies. Virtual channels are created through sharing a physical channel and using dedicated buffers for each source and destination pair. They are useful for improving network performance and saturation throughput because virtual channels allow alternative paths for transferring packets in the network and sharing network links [10]. Increasing the number of virtual channels, however, is not free. More virtual channels adds more hardware overhead. 3.2 General-Purpose ClusCross Topology The design principle of ClusCross can be applied to general-purpose networks as well. This section describes the general-purpose version of ClusCross and compares the network parameters with other general-purpose topologies. However, the evaluation of the generalpurpose ClusCross is not the focus of this work. The ClusCross topology is useful for general-purpose network because it combines short links with long links and have important features from Mesh and Torus, which are widely adopted generalpurpose on-chip interconnection network topologies. The generalpurpose ClusCross uses similar idea of clustering and increases long link connections among clusters to reduce network diameter. Hence, it increases path diversity and bisection width for highperformance interconnection network design. Figure 4 shows an example of the general-purpose ClusCross topology. This topology is a symmetric design and all nodes have a degree of four, which is the same as Torus. ClusCross: A New Topology for Silicon Interposer-Based Network-on-Chip NOCS ’19, October 17–18, 2019, New York, NY, USA interconnection networks. In the injection mode, a specific injection rate of packets injected into the simulated network is used to measure the average latency and throughput. The performance measurement in the reply-and-request mode is typically based on the total time to finish the work and therefore this measurement is determined by the worst case. The reply-and-request mode is suitable to evaluate behavior of the memory traffic and there are limited number of Miss Status Handling Registers (MSHRs) for each node, which is defined as the maximum outstanding requests in the simulation. When all of MSHRs for a node are occupied, the next memory request will be stalled until one of the outstanding requests is replied [14], [17]. In this mode, the batch size defines a predetermined amount of work that each node needs to send before the simulation ends. Synthetic traffic patterns in the simulator are designed for homogeneous systems. Misaligned topologies, however, have two different types of nodes: processing nodes and memory nodes which have different behaviors. Memory nodes only reply to requests and do not initiate any communication by themselves, whereas processing nodes can both reply to request or initiate communication by themselves. We modified BookSim to differentiate different behaviors of these two types of nodes and modeled memory traffic (core requests, memory replies) and coherence traffic (core to core). Network configuration parameters are listed in Table 3. The lengths of the long links are estimated to be the Manhattan distance between two nodes and are faithfully modeled for each topology in anynet configuration in BookSim to have a fair comparison. The latency of the link is proportional to the length of the link. We used uniform random traffic pattern to emulate coherence traffic to evaluate ClusCross in comparison with other topologies, in which each source node is equal likely to send to any other destination nodes. Uniform random traffic distributes the packages uniformly, which is commonly used for network evaluation and it creates balanced loads [5]. The sample period in Table 3 is only applicable in injection rate mode. Latency and throughput of the network is determined after each sample period. After the warm up phase (three sample periods), measurement phase begins. After each sample period, statistics are reported. Before reporting final latency and throughput values, all of the measurement packets are drained from the network [15]. In Table 3, max samples define the maximum number of sample periods used in a simulation. Figure 5 depicts how many virtual channels would be needed in each port for different topologies to maintain deadlock freedom and improve network performance. As shown in this Figure, saturation throughput will be constant after applying more than eight virtual channels. Hence, we chose eight virtual channels to minimize hardware overhead while providing a high saturation throughput. In addition to synthetic traffic patterns, we also evaluated the proposed topology using realistic parallel applications. The traces are from simulations of PARSEC V2.1 on multiprocessor systems on Netrace 1.0 [12]. Netrace is a trace-based framework which generated packet traces by simulating a 64-core system through running the PARSEC V2.1 benchmark on M5 simulator [11]. Netrace considers dependencies between packets in the simulations. The architecture parameters used in Netrace are summarized in Table 4. Figure 4: An illustration of a 64-node general-purpose ClusCross topology. ClusCross tries to limit the use of the long links that are relatively longer. There are only four long links that are longer than the wrapped-around links in Torus and all the other long links are shorter than or the same as the wrapped-around links in Torus. The proposed general-purpose ClusCross also has fewer long links in comparison with the FoldedTorus. Like the Torus, the ClusCross topology is regular, all nodes have four links and also this topology is edge-symmetric, which can improve load balancing across the channels. Virtual channels can be applied to ClusCross to maintain deadlock freedom [9]. It is important to mention that both Torus and FoldedTorus require virtual channels to avoid deadlocks because using wrappedaround long links can create rings in the topology. A comparison on network characteristics between general-purpose ClusCross and three other general-purpose topologies is summarized in Table 2. 4 EVALUATION RESULTS This section will focus on evaluating the proposed ClusCross topology for interposer-based system. This section will first present experimental setup, and then presents evaluation results on system performance, power, and area. 4.1 Experimental Setup The proposed ClusCross topologies are evaluated and compared against two other topologies, ButterDonut x and FoldedTorus x, by using the BookSim 2.0 simulator [14] for system performance, area, and power consumption. It is feasible to use BookSim to simulate interposer-based systems because the back-end-of-line process in manufacturing metal layers of the interposer integration is the same as the process for metal interconnects in regular 2D chips. Metal density, resistance and capacitance of interposer’s metal layers are considered the same as conventional on-chip wires [13]. BookSim 2.0 is a cycle-accurate interconnection network simulator that can provide system performance evaluation such as bandwidth and latency using synthetic traffic patterns and traces from real applications. BookSim 2.0 can also report power consumption and area. The models in BookSim 2.0 has been validated against the RTL implementations of the actual NoC router circuitry. Injection mode and reply-and-request mode are two types of synthetic traffic patterns that can be used in BookSim to evaluate NOCS ’19, October 17–18, 2019, New York, NY, USA Hesam Shabani and Xiaochen Guo Table 3: Network Parameters. Table 5: PARSEC V2.1 applications. Common Parameters 8, 8 Flit router buffer size Virtual Channels Channel Width 128 Router Pipeline 4 Stages 10,000 Sample Period Max Samples 10 Technology Node 32 nm 1 GHz Clock Frequency Batch Size 100,000 Outstanding Requests 4 Routing Algorithm CMesh x, FoldedTorus x Dimension Order Routing Shortest-Path ButterDonut x, ClusCross x Figure 5: Saturation throughput of topologies for different numbers of VCs with the shortest-path routing algorithm under coherence traffic. Table 4: Architecture Parameters. Core L1 Cache L2 Cache Memory Coherence Protocol 64 Cores, 2GHz, Alpha ISA, In-Order 32KB Instruction/32KB Data, 4-Way Associative, 64B Lines, 3 Cycle Access Time 64 Bank Fully Shared S-NUCA, 16MB, 64B Lines, 8-Way Associative, 8 Cycle Bank Access Time 150 Cycle Access Time, 8 On-Chip Memory Controllers MESI PARSEC V2.1 includes a set of parallel applications, which are used in the evaluation. The simulated cycles, number of packets, average injection rate, and size of the header of each application are listed in Table 5. Benchmark Blackscholes -Large Bodytrack -Large Canneal -Medium Dedup -Medium Ferret -Medium Fluidanimate -Large Swaptions -Large Vips -Medium X264 -Medium Simulated Cycles 5833784581 Simulated Avg Injection Packets Rate 113795888 0.019506 Size of Header (B) 247 4577920449 385863891 0.084288 23109826318 372046797 0.016099 5450782575 431833996 0.079224 8263033596 287425404 0.034784 10172100050 187830527 0.018465 1754464418 310331287 0.176881 5431630907 334870995 0.061652 42969045899 584828673 0.013610 244 243 241 242 247 244 240 240 4.2 System Performance Evaluation Using Injected Traffic Patterns Coherence and memory traffic are emulated using different injected traffic patterns. Figure 6 presents the network latency of different topologies under different coherence traffic injection rates. As shown in this Figure, ClusCross x-v2 has the lowest average latency in comparison to the other topologies because of a smaller number of average hop counts in the network. For instance, ClusCross x-v2 has a 10.2% and a 12.9% average latency reduction at the 0.04 injection rate in comparison with ButterDonut x and FoldedTorus x, respectively. Additionally, this Figure shows that ClusCross x-v2 has the best saturation bandwidth as compared to the other topologies because it has the highest number of total bisection links, North-South and East-West, in the network, which has a significant influence on the saturation throughput for coherence traffic. In order to analyze the amount of average packet latency for memory traffic, we evaluate performance by using both the injection mode and the reply-and-request mode in the simulator. Figure 7 shows the average packet latency and saturation throughput for memory nodes in injection mode. ClusCross x-v2 has a small reduction in average latency at the 0.07 injection rate in comparison to ButterDonut x and ClusCross x-v1, while has a 8.3% reduction as compared to FoldedTorus x. ClusCross x-v2 has a better saturation throughput as compared with the others because of having highest number of East-West bisection links, which is important for memory traffic. This means that the cross-cluster long links are particular useful for memory intensive workloads. We also evaluated packet latency of all topologies using the replyand-request mode to emulate memory traffic on the simulator. As we mentioned before, this mode simulates a specific batch size till all of the requests are replied, and the batch size is set in the simulator. The results are shown in Table 6. The two versions of ClusCross have lower average latency than the other two topologies do. ClusCross: A New Topology for Silicon Interposer-Based Network-on-Chip NOCS ’19, October 17–18, 2019, New York, NY, USA Figure 6: Average packet latency and saturation throughput of different network topologies for coherence traffic. Figure 8: Total simulation runtime normalized to CMesh x. Figure 7: Average packet latency and saturation throughput of different network topologies for memory traffic. Table 6: Average packet latency of memory traffic in replyand-request batch mode. ClusCross x-v2 18.27 ClusCross x-v1 18.33 ButterDonut x 18.87 FoldedTorus x 19.36 4.3 System Performance Evaluation Using PARSEC In this section, the PARSEC benchmark suite is used to evaluate system performance of ClusCross in comparison with other topologies. Figure 8 shows total simulation runtime required to complete each PARSEC applications. These results are consistent with results of average packet latency in Figure 6. For application workloads with limited network pressure, topologies exhibit similar performance. Figure 9 presents average packet latency for different topologies normalized to CMesh x. Results have shown that ClusCross x-v2 has the lowest latency in comparison to the other topologies. ClusCross x-v1 also has lower latency as compared to FoldedTorus x and ButterDonut x. 4.4 Power and Area Evaluation The power consumption in BookSim is calculated for 32 nm technology node. The simulator uses an analytical model to assess the area and power consumption. The size of the crossbar switch, number of connections, and number of buffers determine the area and static power consumption. Recorded activities in different components during simulation are applied to calculate the dynamic power consumption. Channel and switch components are the dominant factors Figure 9: Average packet latency normalized to CMesh x. which contribute the most to the total power consumption of the on-chip network. Figure 10 shows power consumption breakdown of different topologies. The main difference in power consumption in these topologies is the channel component. ButterDonut x consumes more power in comparison with the other topologies because it has more long links. Both versions of ClusCross consume less power on channels as compared to ButterDonut x and FoldedTorus x. This is because ClusCross uses fewer long links even though some of the long links are longer than the ones used in ButterDonut x and FoldedTorus x. The main reason for consuming more power in longer links is that longer wires are more resistive and have greater parasitic capacitance. After repeater insertion optimization, longer links still tend to consume more power than shorter links. Figure 11 shows the area breakdown of on-chip network components for different topologies. The number of total links and length of the links determine the channel area, which is similar among different topologies. The main differences came from the switch component because all routers in ClusCross x-v2 and FoldedTorus x have eight ports, but some of routers in the two other topologies have fewer ports. Therefore, it is expected that the topologies which have routers with fewer ports have a smaller area. 5 CONCLUSION This paper proposes a new class of on-chip network topology, ClusCross, designed to improve network performance parameters for NoC-on-interposer systems. The key idea is to map network NOCS ’19, October 17–18, 2019, New York, NY, USA Hesam Shabani and Xiaochen Guo "
Ghost routers - energy-efficient asymmetric multicore processors with symmetric NoCs.,"Asymmetric multicore architectures have been proposed to exploit the benefits of heterogeneous cores. However, asymmetric cores present challenge to network-on-chip (NoC) designers since the floorplan is not necessarily regular with ""nodes"" being different size. In contrast, most of the previously proposed NoC topologies commonly assume a regular or symmetric floorplan with equal size nodes. In this work, we first describe how asymmetric floorplan leads to asymmetric topology and can limit overall performance. To overcome the asymmetric floorplan, we present Ghost Routers - extra ""dummy"" routers that are added to the NoC to create a symmetric NoC architecture for asymmetric multicore architectures. Ghost router provides higher network path diversity and provides higher network performance that leads to higher system performance. Ghost routers also enable simpler routing algorithms because of the symmetric NoC architecture. While ghost routers is a simplistic modification to the NoC architecture, it does increase NoC cost. However, ghost routers exploit the observations that in realistic systems, the cost of NoC is not a significant fraction of overall system cost. Our evaluations show that ghost routers can improve performance by up to 21% while improving overall energy-efficiency of the system by up to 26%.","Ghost Routers: Energy-Efficient Asymmetric Multicore Processors with Symmetric NoCs Hyojun Son KAIST Daejeon, Republic of Korea processor@kaist.ac.kr Hanjoon Kim Furiosa A.I. Seoul, Republic of Korea hanjoon@furiosa.ai Hao Wang University of Wisconsin-Madison Wisconsin, United States pkuwangh@gmail.com Nam Sung Kim Samsung Electronics Suwon, Republic of Korea nam.sung.kim@gmail.com John Kim KAIST Daejeon, Republic of Korea jjk12@kaist.edu ABSTRACT Asymmetric multicore architectures have been proposed to exploit the benefits of heterogeneous cores. However, asymmetric cores present challenge to network-on-chip (NoC) designers since the floorplan is not necessarily regular with “nodes” being different size. In contrast, most of the previously proposed NoC topologies commonly assume a regular or symmetric floorplan with equal size nodes. In this work, we first describe how asymmetric floorplan leads to asymmetric topology and can limit overall performance. To overcome the asymmetric floorplan, we present Ghost Routers – extra “dummy” routers that are added to the NoC to create a symmetric NoC architecture for asymmetric multicore architectures. Ghost router provides higher network path diversity and provides higher network performance that leads to higher system performance. Ghost routers also enable simpler routing algorithms because of the symmetric NoC architecture. While ghost routers is a simplistic modification to the NoC architecture, it does increase NoC cost. However, ghost routers exploit the observations that in realistic systems, the cost of NoC is not a significant fraction of overall system cost. Our evaluations show that ghost routers can improve performance by up to 21% while improving overall energy-efficiency of the system by up to 26%. CCS CONCEPTS • Hardware → Network on chip. KEYWORDS Ghost router, network-on-chip, asymmetric NoC ACM Reference Format: Hyojun Son, Hanjoon Kim, Hao Wang, Nam Sung Kim, and John Kim. 2019. Ghost Routers: Energy-Efficient Asymmetric Multicore Processors with Symmetric NoCs. In International Symposium on Networks-on-Chip Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’19, October 17–18,2019, New York, NY, USA © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6700-4/19/10. . . $15.00 https://doi.org/10.1145/3313231.3352360 (NOCS ’19), October 17–18, 2019, New York, NY, USA. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3313231.3352360 1 INTRODUCTION Network-on-chip (NoC) can have a significant impact on the overall performance and scalability of a multicore system. There has been significant amount of research done on various aspects of NoC to improve the performance and energy-efficiency, including alternative topologies, routing algorithms, flow control, and router microarchitecture. However, prior work have often assumed a “symmetric” NoC where the topology and in particular, the floorplan is symmetric. However, with asymmetric or heterogeneous multicores, this assumption is not necessarily valid. Different heterogeneous multi-core systems have been proposed, such as big.LITTLE [2], Cell processor [16], and heterogeneous ISA architectures [19] often require both a big core and a small core. There has been significant amount of work done on heterogeneous NoC [5] – e.g., HeteroNoC [14] evaluates with asymmetric cores but they assume the topology and the floorplan is symmetric and matches well to a conventional 2D mesh topology. Different asymmetric NoCs have also been proposed, with each optimizing the NoC for different type of communication – e.g., NoC for GPGPU system [20], interconnect in interposers [9], and asymmetricity in local and global on-chip communication [1]. While prior approaches to asymmetric NoC propose optimization of NoC architecture for both performance and energy-efficiency, we take a different approach by modifying asymmetric NoC resulting from asymmetric multicores to create a symmetric NoC. There has been significant amount of work done to reduce the cost and energy of NoC, in order to improve the energy-efficiency of the overall system. To justify the impact of NoC on the overall system, some commonly cited work argue that NoC can consume up to 40% of the total system power [8]. However, this number is not necessarily realistic since Intel TeraFlops processor does not represent general-purpose multicore architectures as each node is a simplified architecture consisting of small amount of computation units. In comparison, the Intel Single-Chip Cloud (SCC) [17] multicore processors that consisted of general-purpose multicores demonstrated that the NoC can consume approximately 10% of the total chip power. However, even the 10% ratio was determined based on a power “virus” – i.e., a synthetic set of inputs that maximized the power consumption of the NoC. Thus, a realistic workload will NOCS ’19, October 17–18,2019, New York, NY, USA Hyojun Son, Hanjoon Kim, Hao Wang, Nam Sung Kim, and John Kim multicore processors to improve performance and overall system energy-efficiency. 2.1 Motivation: Asymmetric NoC A baseline architecture that we assume is shown in Figure 1 that consists of 4 big cores (C0 − C3) and 16 little cores (C4 − C19), in addition to 4 memory controllers – two on top and two on the bottom which is consistent with placement of memory controllers for modern multicore processors.We assume a single router for each component, including both the core and the memory controllers, but ghost routers can be extended to a topology with concentration where multiple nodes share a router. A conventional 2D mesh topology is shown in Figure 1(a) for the baseline asymmetric multicore architecture but the connectivity results in asymmetry in the topology. Because of the differences in physical size of the big cores compared to the little cores, the number of cores in each row differ – for example, there are 4 routers in each row for the little cores but only two routers for the big cores. Thus, the little cores in the middle are connected with a symmetric 2D mesh topology but the connectivity to the big cores and the memory controllers is limited and results in an asymmetric topology. The asymmetry in the topology leads to challenges both in terms of performance and design of the on-chip network. The asymmetry in the connectivity limits the path diversity and the routing paths available. As a result, even simplified routing, such as dimensionordered (DOR) routing including XY or YX routing cannot be used because of the limited connectivity. For example, routing from core connected with R4, R8, R12, R16 shown in Figure 1) to memory controller 0, YX routing cannot be used as the only possible route is using XY routing. As a result, traffic from most of the small cores must be routed through R0 to access the memory controller (R20). Adaptive routing is commonly used to improve performance in NoC but because of the limited connectivity, the benefits of adaptive routing is also limited as well. There are other challenges with asymmetric topology in the router microarchitecture design. For example, for a symmetric 2D mesh topology, the wire length between neighboring routers are nominally the same length. However, the asymmetry results in longer wires (e.g., between R0 and R1 in Figure 1(a)) and can require additional repeaters. The longer wires also increase the channel latency and impacts the credit round-trip latency – resulting in deeper input buffers for the longer channels. In the following section, we describe how the ghost router enables the benefits of a symmetric NoC architecture to result in higher overall performance and improved energy-efficiency through higher path diversity. 2.2 Symmetric NoC with Ghost Routers To overcome the asymmetric topology caused by the asymmetric multicores, we propose the placement of ghost routers to create a symmetric NoC (Figure 1(b)). In the asymmetric floorplan, the number of routers in the first and the third column of the network were smaller than the second and the fourth column since the number of components differed for each row of the mesh topology. However, by adding ghost routers to the odd columns, the floorplan (topology) is symmetric and increase the path diversity. Even though the ghost routers are placed over existing components (e.g., big cores), they (a) (b) Figure 1: High-level block diagram (floorplan) of an asymmetric multicore processor consisting of both Big and Little cores and (a) an asymmetric NoC caused by the asymmetric multicores and (b) a symmetric NoC with ghost routers. likely result in the NoC consuming much less than 10% of the total chip power. As a result, reducing NoC power likely does not have a significant impact on overall system energy-efficiency, especially if the NoC power reduction techniques degrades system performance. In this work, since the cost of NoC represents a small fraction of the overall system cost, we argue that by increasing the cost of the NoC to improve overall performance, the NoC energy consumed can increase but overall system energy-efficiency can be significantly improved. In particular, we explore the NoC when the floorplan is not necessarily symmetric. We show how asymmetric floorplan leads to asymmetric NoC, even if the topology itself is symmetric, and creates performance bottleneck because of the limited connectivity from the asymmetric NoC. To overcome the asymmetric organization, we propose Ghost Routers – extra or dummy routers added to the network to enable a symmetric NoC. The ghost routers are not connected to any endpoint nodes (i.e., cores or memory controllers) and thus, does not inject or eject any traffic – thus, from the endpoints perspective, appear as ”ghost.” However, ghost routers enable higher path diversity and increase overall performance. Although the ghost router increases the cost of the overall NoC (and increases the power consumption of the NoC) with the addition of more routers, overall system energy-efficiency is improved because of the improvement in overall execution time. Our evaluations show that ghost routers can improve overall system performance by up to 21% while improving overall energy-efficiency by up to 26%. 2 GHOST ROUTER ARCHITECTURE In this section, we first describe why symmetric NoC topologies can become asymmetric from asymmetric multicore processors and limitations of asymmetric NoC. We then describe how the proposed ghost router enables symmetric NoC organization for asymmetric Ghost Routers: Energy-Efficient Asymmetric Multicore Processors with Symmetric NoCs NOCS ’19, October 17–18,2019, New York, NY, USA Figure 2: A block diagram of a simplified conventional router microarchitecture. The shaded region corresponds to microarchitecture that is not utilized in a ghost router. are not connected to the components but only used to connect to neighboring routers. In our example shown in Figure 1(b), we introduced 8 additional routers to ensure a symmetric topology. The ghost routers are “dummy” routers that are not connected to any endpoint nodes – thus, it does not generate or inject any traffic and it also does not accept or eject any traffic. The ghost router enables higher path diversity (identical to a conventional 2D mesh topology) and introduces paths that were not available with the asymmetric NoC and enables a symmetric NoC. As a result, simplified routing algorithm such as DOR can be easily implemented. In addition, load-balanced routing algorithms such as randomized DOR (XY-YX), which randomly selects whether to traverse X-axis first or Y-axis first for each packet [18], can also be easily implemented with the additional path diversity. High-level block diagram of a ghost router is shown in Figure 2. The microarchitecture of the ghost router is mostly identical to any conventional NoC router; the only difference is that the injection/ejection ports of the router are not used. It is possible that the ghost router microarchitecture can be optimized to remove the injection buffers but we do not consider such optimizations in this work. The other ports of the ghost routers are used to connect to neighboring routers in the network. 2.3 Cost & Extensions The obvious cost introduced from ghost router NoC is the additional (ghost) routers which increases the NoC cost but as we later show in Section 4, overall system energy-efficiency is improved despite the increase in NoC energy cost. The ghost routers do not increase the overall bisection bandwidth of the NoC as the ghost routers are effectively added to a region of the system which has no other connectivity (GR0) and improves path diversity. Thus, the cost of ghost router is proportional to the number of ghost router added – in our example, 8 of the 32 routers are ghost routers, resulting in approximately 25% overhead. However, as we show in the following sections, overall system performance benefits outweigh the cost. Another benefit of ghost routers is that simple routing algorithm can be enabled with ghost routers. For example, no additional virtual channels (VCs) are necessary if a simple DOR routing is used to avoid routing deadlock; however, the minimum number of VCs needed without ghost routers is two. (a) (b) Figure 3: (a) Ghost routers added to a different floorplan organized as “clusters.” and (b) reducing the cost of ghost routers by simplifying some of the ghost routers. Ghost routers can also be used for alternative organizations of asymmetric multicores. For example, another possible floorplan is shown in Figure 3(a) where a clustering approach is used – a single Big core next to several Little cores form a “cluster.” This floorplan also leads to significant asymmetry in the topology and Figure 3(a) shows how ghost routers can be added to create a symmetric NoC. In general, the number of ghost routers added is proportional to the number of asymmetric rows multiplied by the number of asymmetric columns. In our example earlier (Figure 1(a)), there were 4 asymmetric rows (top 2 rows and the bottom 2 row) and 2 asymmetric columns (column 0 and 2) and resulted in 8 ghost routers. This creates a “full” symmetric topology but the number of ghost routers added can also be modified to further improve overall energy-efficiency. In our architecture and floorplan with ghost routers (Figure 1), we assume “full” symmetry by providing a 4×8 mesh with 32 routers. However, if the improvements from XY-YX routing is sufficient, then some of the ghost routers can be removed with no impact on overall performance as shown in Figure 3(b). This approach does introduce longer channels but 4 of the 8 ghost routers can be removed. 3 EVALUATION METHODOLOGY We evaluated the benefits of ghost routers using both synthetic, network-only simulations and full-system simulations. For networkonly synthetic traffic evaluations, we use cycle accurate Booksim2.0 [15] simulator and the different configuration are summarized below. ∙ nGH_XY _Y X : Conventional 2D mesh with XY_YX routing (no ghost routers) ∙ nGH_Ada pt: Conventional 2D mesh with minimal adaptive routing (no ghost routers) ∙ GH_XY _Y X : 2D mesh with ghost routers with XY_YX routing NOCS ’19, October 17–18,2019, New York, NY, USA Hyojun Son, Hanjoon Kim, Hao Wang, Nam Sung Kim, and John Kim Table 1: Big/little core and NoC configurations for full system simulation. (a) (b) Figure 4: (a) NEAR and (b) FAR core-to-MC mapping used in the evaluation of full system simulations. ∙ GH_Ada pt: 2D mesh with ghost routers with adaptive routing For nGH_XY _Y X , the routing decision between XY or Y X is not necessarily possible since some routes are not possible – thus, the routing defaults to deterministic routing. For Ada pt routing, the routing decision between XY and Y X is made adaptively at the source router, based on the network congestion (or downstream buffer credit count). We evaluated the 4 × 8 mesh topology (Figure 1) using uniform random (UR) traffic pattern and UR-asymmetric where some of the communication patterns are removed (i.e., no packets from memory controller to another memory controller). The big cores are assumed to have an injection rate that is twice that of the little cores. We assume packet sizes are bimodal in size and consist of single-flit packets as well as 5-flit packets. We assumed each router is 4 cycles with 1 cycles for the channel delay. We assume 20 virtual channels (VCs) and used to avoid both protocol and routing deadlock. 1 Both adaptive (ada pt) and randomized DOR (XY _Y X ) routing is deadlock-free since separate virtual channels (VCs) are used for XY routing and YX routing. For full-system evaluation, we use cycle accurate Gem5 [6] full system simulator, based on Figure 1 organization with big and little core configurations shown in Table 1. We evaluate two alternative memory controller mapping (NEAR and FAR) since the traffic to memory controllers has significant impact on overall on-chip traffic pattern (Figure 4). For NEAR, this mapping generates the smallest amount of on-chip network congestion and provide smallest benefit from ghost routers. 2 In comparison, FAR results in a memory controller mapping that is farthest away and generates for NoC congestion. We use SPEC CPU2006 [7] and our initial results use memory intensive application (i.e., mcf ) since this workload heavily utilizes the on-chip network for memory traffic. We also evaluated other SPEC CPU2006 benchmarks and different workload mix, including compute-intensive workloads, to evaluate the trade-off of ghost routers. The workloads used for full-system evaluation are shown in Table 2. For energy analysis, we estimated power consumption of ghost routers and the multicore system using McPAT [13], 1 Protocol deadlock is only needed for full-system simulations and not synthetic, networkonly simulations. 2 Even in this “localized” mapping, there is still traffic between each cluster of cores because of coherence traffic. Frequency Issue width ROBs ALUs Physical Regs Cache Network big core little core 3.6GHz 2.4GHz 8 2 192 48 6 2 Integer 256 Integer 64 Float 256 Float 64 32KB L1I, 64KB L1D Unified 1MB L2 64GB RAM 4 cycle router delay 1 cycle channel delay 20 VCs 16 flit buffers per VC 1.2GHz frequency Table 2: Description of different mix of workloads used in ghost router evaluations. Workload Memory Memory intensive mcf libquantum Compute – Mixed 1 mcf,libquantum Mixed 2 mcf,libquantum Compute intensive – gcc xlancbmk hmmer gobmk,sjeng, astar,hmmer bzip2,omnetpp,hmmer, xalancbmk assuming 22nm technology, 1.2V Vdd, and 1.2 GHz for the NoC with the 128-bit wide channels. 4 EVALUATIONS 4.1 Synthetic Results The results from network-only evaluations are shown in Figure 5. Ghost router do not impact the zero-load latency but improves network throughput. While ghost router architecture does increase the network cost with additional routers, the effective bisection bandwidth does not change – thus, the improved throughput is not from additional bisection bandwidth but with better channel utilization. By introducing ghost routers and providing higher path diversity in the network, higher throughput is achieved. With UR traffic, both Ada pt improves performance over XY _Y X routing algorithms for both nGH and GH architectures. However, GH_XY _Y X without adaptive routing nearly matches the throughput of nGH_Ada pt. In addition, GH_Ada pt provides approximately 24% more throughput compared with nGH_Ada pt. For some traffic pattern such as UR-asymmetric (Figure 5(b)), Ada pt routing on the baseline (nGH ) does not provide any significant benefits. However, using GH with random routing (XY-YX) provides 16% higher throughput and with adaptive routing (GH_Ada pt), the benefit increases to 28%, compared to nGH_Ada pt. The throughput of the NoC decreases with an adversarial memory mapping (Figure 5(c)) – the however, the Ghost Routers: Energy-Efficient Asymmetric Multicore Processors with Symmetric NoCs NOCS ’19, October 17–18,2019, New York, NY, USA (a) (c) (b) (d) Figure 5: Synthetic evaluations of ghost routers with (a) uniform random (UR) traffic pattern, (b) UR-asymmetric traffic pattern, (c) UR-asymmetric traffic pattern with FAR mapping, and (d) core-memory traffic. (a) (b) Figure 6: Performance comparison of ghost routers for memory intensive application (mcf ) with (a) FAR and (b) NEAR memory controller mapping. Performance is 1/execution_time and higher is better. benefits from ghost router is approximately the same as GH_Ada pt provides 27% improvement compared with nGH_Ada pt The benefit of ghost router is lower with just core-memory traffic (Figure 5(d)) since the amount of on-chip network traffic is lower. Figure 7: Energy consumption of each configuration with fullsystem simulation from memory-intensive workloads. 4.2 Full System Results The results from full system simulations with memory intensive application (i.e., mc f ) are shown in Figure 6 and he results are also normalized to nGH_XY _Y X . The results for FAR mapping shows similar trend as the synthetic workloads – GH improves performance over nGH and Ada pt further improves performance. GH_Ada pt improved performance by up to 40%, while improving performance by approximately 21% on average. Even with random routing, GH_XY _Y X improves performance by up to 26% (on average, approximately 14%). The cores that are located in different columns as the memory controllers (i.e., C4, C8, etc.) achieved higher performance improvement with ghost routers because of the additional path diversity enabled by the ghost routers. In comparison, the cores that are located on the same column as the memory controllers have smaller performance improvements. For these cores, adaptive routing does not necessarily provide any significant benefits – e.g., if the source and destination are in the same column, there is only one minimal path and no path diversity. However, these cores still benefited from ghost routers since the traffic from others cores (such as C4, C8, etc.) do not send traffic through these cores and by load-balancing the traffic from these cores (i.e., C4, C8, etc.), the cores without path diversity (i.e., C5, C9, etc.) also benefited from ghost routers. With NEAR memory controller mapping, performance benefits of ghost router topology is lower than FAR since the amount of on-chip traffic decreases as the traffic between core and MC traverse only a quarter of the entire network (or a small 2 × 4 mesh topology) and limit the opportunity for performance improvement from ghost routers. However, ghost router still increased performance by 15% on average (up to 38%) when using adaptive routing with ghost router, while random routing improved performance by 7% on average (up to 29%). For this particular traffic pattern, the single NoC channel between the cores and the MC is the bottleneck but with ghost router, additional NoC channel to the NoC is provided and results in performance improvement. In both FAR and NEAR MC mapping without ghost routers, adaptive XY-YX routing does not provide better performance than random XY-YX routing since core-to-MC routing path is limited and thus, adaptive routing algorithm cannot exploit path diversity in the on-chip network. It is also interesting to note that with NEAR, the performance slightly degrades for all of the different configurations, compared to nGH_XY _Y X . With nGH_XY _Y X , the big cores near the memory controllers obtain higher (unfair) bandwidth towards the memory controller because of its physical location near the MC; however, with the alternative architectures, more bandwidth NOCS ’19, October 17–18,2019, New York, NY, USA Hyojun Son, Hanjoon Kim, Hao Wang, Nam Sung Kim, and John Kim Figure 9: Performance improvements of GH_Ada pt over nGH_Ada pt. (a) (b) Figure 8: Normalized avg memory access latency comparison of memory-intensive workload with (a) FAR and (b) NEAR mapping. is sent through the big cores and degrades the performance of big cores (C0 − C3). However, with ghost routers, adaptive routing still provides significant benefits for the little cores. Figure 8 shows normalized average memory access latency when local cache miss occurs for each core using the FAR and NEAR mapping. The reduction in memory access latency correlates directly to the improvement in overall performance. With FAR mapping, adaptive routing with ghost router decreased latency by 21% on average (up to 35%), while random routing reduced latency by approximately 15% on average (up to 25%). With NEAR mapping, latency variance of routing algorithms was smaller than FAR mapping because of lower on-chip network congestion. 4.3 Energy-Efficiency Comparison The energy consumption of each configuration normalized to nGH_XY _Y X of FAR mapping is shown in Figure 7. Although ghost router topology adds additional routers to the NoC floorplan, because of significant execution time decrease, GH_Ada pt improves overall system energy efficiency by 26% compared to nGH_Ada pt and 18% with XY_YX routing, with FAR mapping. It is worth noting that the power consumed in the NoC for GH_Ada pt does increase by approximately 31% compared with nGH_Ada pt (and 30% for XY_YX) but the decreased overall execution time reduces the effect of increased NoC power and results in only 0.2% total NoC energy consumption overhead for adaptive routing (GH_Ada pt) and 7.5% for random routing(GH_XY _Y X ). We also evaluated the impact of ghost routers with computeintensive workloads and different workloads mix as described in Table 2. The performance comparison of nGH_Ada pt and GH_Ada pt for various applications is shown in Figure 9. Since multiple cores are executing the same workloads, we show min and max performance improvements (in addition to average) as the location of the cores have an impact on performance change. For memory-intensive Figure 10: Energy consumption comparison with ghost routers. applications (e.g., mcf, libquantum), GH_Ada pt provide significant performance improvement but for some compute-intensive applications (e.g., hmmer) the performance improvement is negligible since the NoC usage (or the memory accesses) is very small. For mixed workloads, GH_Ada pt provided high performance improvement for the cores that are executing memory-intensive workloads. The energy comparison is shown in Figure 10. Memory-intensive applications that has significant performance benefit also results in high energy improvements. For M i xed1 and M i xed2, GH_Ada pt still improves energy efficiency by approximately 12%, 8% respectively. With compute-intensive only workloads and ghost routers, GH_Ada pt results in approximately 31% more energy than nGH_Ada pt for the NoC for hmmer. However, since the NoC represents a small fraction of the overall system energy, the cost for hmmer is only an increase of approximately 1% overall system energy. 4.4 Related Work Different heterogeneous multi-core systems have been proposed. For example, big.LITTLE [2] is a heterogeneous core system proposed for embedded systems that consist of big, powerful cores and little, energy-efficient cores. The Cell processor [16] is another example of a heterogeneous system that consists of an IBM Power PC and multiple ”stream” processors. Heterogeneous ISA architectures [19] often require both a big core and a small core. There have been many different heterogeneous or asymmetric NoC that have been proposed. The heterogeneous NoC [5] often provide different optimization to exploit the communication pattern and/or the core architecture to improve performance and/or energy-efficiency. However, this often results in the need to create different type of routers within the NoC. For example, HeteroNoC [14] is an NoC that targets asymmetric cores, similar to this work; however, they assume the topology and the floorplan is symmetric and matches a conventional 2D mesh Ghost Routers: Energy-Efficient Asymmetric Multicore Processors with Symmetric NoCs NOCS ’19, October 17–18,2019, New York, NY, USA topology while exploring aymmetric router microarchitecture. Different asymmetric NoCs have also been proposed, with each optimizing the NoC for different type of communication – e.g., NoC for GPU system with different network for L1-to-L2 vs L2-to-L1 communication [20], NoC for many-to-few-to-many GPGPU communication and throughput-effective router microarchitecture [3], interconnect in interposers [9], and asymmetricity in local and global on-chip communication [1]. While prior approaches to asymmetric NoC propose optimization of NoC architecture for both performance and energy-efficiency, we take a different approach by modifying asymmetric NoC resulting from asymmetric multicores to create a symmetric NoC. In comparison, this work proposed to create homogeneous routers, even if the multicore architecture is heterogeneous. As a result, it simplifies overall NoC architecture design. There have been different NoC architecture proposed, including router microarchitecture [10] and topology [4, 11]. The router microarchitecture used in an NoC with ghost routers can adopt alternative router microarchitecture without losing any benefit of ghost routers. In addition, concentration or sharing of routers [12] can also be applied with ghost routers. Thus, previously proposed techniques to improve energy-efficiency or reduce NoC cost is applicable to ghost routers. In addition, it remains to be seen if ghost router can also be applied to alternative topologies (e.g., flattened butterfly [11], concentrated mesh [4]) in addition to 2D mesh assumed in this work. 5 SUMMARY In this work, we proposed ghost routers – extra or “dummy” routers added to the NoC to create a symmetric NoC for a multicore system with asymmetric cores. Ghost routers not only enable regular routing functions for conventional symmetric topology but also increases routing path diversity and improves NoC performance. Our evaluations show that with memory-intensive workloads, ghost routers can increase the power consumption of the network-on-chip (NoC) by up to 31%; however, overall system performance is improved by 21% (on average) and the overall system energy-efficiency is improves by approximately 26%. While this work introduced additional cost to the network itself, this work is not necessarily orthogonal to many different power-reduction techniques that have been proposed for NoC routers. Thus, the NoC power reduction techniques can be adopted on top of ghost router architectures (i.e., power-gating buffers, channels, etc.) to further improve overall energy-efficiency of the system. 6 ACKNOWLEDGEMENT This work was supported in part by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MEST) (No.NRF-2017R1A2B4011457) and in part by BK21 Plus project of the National Research Foundation of Korea Grant. "
Multi-carrier spread-spectrum transceiver for WiNoC.,"In this paper, we propose a low-power, high-speed, multi-carrier reconfigurable transceiver based on Frequency Division Multiplexing to ensure data transfer in future Wireless NoCs. The proposed transceiver supports a medium access control method to sustain unicast, broadcast and multicast communication patterns, providing dynamic data exchange among wireless nodes. The proposed transceiver designed using a 28-nm FDSOI technology consumes only 2.37 mW and 4.82 mW in unicast/broadcast and multicast modes, respectively, with an area footprint of 0.0138 mm2.","Multi-Carrier Spread-Spectrum Transceiver for WiNoC Joel Ortiz, Olivier Sentieys {joel.ortiz- sosa,olivier.sentieys}@ inria.fr Univ Rennes 1, Inria, CNRS/IRISA Lannion, France Christian Roland christian.roland@univ-ubs.fr UBS - Lab-STICC Lorient,France Cedric Killian cedric.killian@irisa.fr Univ. Rennes 1, Inria, CNRS/IRISA Lannion,France ABSTRACT In this paper, we propose a low-power, high-speed, multi-carrier reconfigurable transceiver based on Frequency Division Multiplexing to ensure data transfer in future Wireless NoCs. The proposed transceiver supports a medium access control method to sustain unicast, broadcast and multicast communication patterns, providing dynamic data exchange among wireless nodes. The proposed transceiver designed using a 28-nm FDSOI technology consumes only 2.37 mW and 4.82 mW in unicast/broadcast and multicast modes, respectively, with an area footprint of 0.0138 mm2 . CCS CONCEPTS • Networks → Network on chip; • Hardware → Radio frequency and wireless interconnect. KEYWORDS Wireless Network-on-Chip, WiNoC Channel, Communication Reliability, Digital Transceiver Architecture 1 INTRODUCTION Massive parallelism in manycore architectures for emerging highperformance computing applications requires the use of an efficient interconnection system. However, current electrical interconnections are not efficient enough to support an increasing number of cores while ensuring high performance and energy efficiency. For this reason, on-chip wireless interconnection technologies can provide a promising solution for a massive number of cores requiring a large Network-on-Chip (NoC), especially when considering broadcast/multicast system requirements. Nevertheless, the bandwidth needed to reach very high-speed data rate for each wireless link, highly increases the power consumption in all the Wireless NoC (WiNoC) transceivers. In order to keep a reasonable trade-off between power consumption and data rate, WiNoC designers have decreased the minimum required bandwidth to support a given data rate (e.g. 16 Gbps). However, this reduction produces significant communication errors that have to be compensated by increasing the transmission signal power and the receiver sensitivity. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’19, October 17–18, 2019, New York, NY, USA © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6700-4/19/10. . . $15.00 https://doi.org/10.1145/3313231.3352373 Current Single-carrier Non-Coherent On-Off Keying (SNC-OOK) transceiver architectures are designed to reach a communication link of 20 mm with a data transfer rate of 16 Gbps. The SNC-OOK transmitter output power Pt required to establish a wireless link into a noisy wireless channel is −14 dBm. However, as a large receiver bandwidth (BW) can reach prohibited levels of power consumption [3], designers have reduced the 3 dB receiver BW required to achieve 16 Gbps, from 16 GHz to 9.2 GHz [6]. This reduction intensifies the bit error rate (BER) at the receiver side and necessitate to increase Pt = −0.5 dBm to reach the same BER = 10−15 . This problem was modeled in MATLAB to verify the effects of the receiver BW reduction, using a second-order band-pass Butterworth filter configured with the parameters described above. As shown in Fig. 1, the system SNR is degraded by 14 dB at 10−7 , which is traduced in a high Pt to reach the same BER. In order to overcome this performance degradation, this work proposes a Multi-carrier NC-OOK (MNC-OOK) transceiver architecture, which allows to avoid BW reduction. Figure 1: Single-carrier non-Coherent OOK system model 2 PROPOSED MULTI-CARRIER WIRELESS INTERFACE ARCHITECTURE The MNC-OOK transceiver is based on Frequency Division Multiplexing (FDM) and Direct Sequence Spread Spectrum (DSSS) to overcome bandwidth reduction problems. The FDM technique is used as a mean to divide the total bandwidth into four different carrier frequencies with shorter bandwidth, e.g., 10 GHz/4 = 2.5 GHz. On the other hand, the DSSS technique enables parallel channel access, reusing existing frequencies. Digital Transmitter Architecture. A transmitter assigned to a carrier frequency fi is composed of three blocks: a serializer SER(32:1/2/8), a DSSS encoder, and an SER(8:1), as illustrated in Fig. 2. The DSSS encoder is designed using 8-bit and 4-bit registers initialized by the considered Hadamard codes (Ci ). The output of this sub-block is a specific code (Ci ) or its complement (Ci ). The SER (32:1/2/8) 2-bit output is associated with a 4-bit code, and the 1-bit output is related with an 8-bit code. In case of a simple point-to-point communication, any code is necessary, therefore, the system configures the SER 8-bit output. Hadamard codes are only used for multicast (all/many-to-one, many-to-many, and multiple-unicast) communication. Otherwise, in unicast/broadcast mode, any code is required. different to the theoretical BER for SNC-OOK. Consequently, the Pt does not require to be highly incremented, as previously explained. Moreover, in case of multicast mode, a 8-bit code size will provide better signal resilience considering the PG effects, as well as it can allocate more parallel channels [5]. Figure 2: MNC-OOK transceiver architecture. Digital Receiver Architecture. A receiver sub-block tuned for a certain carrier frequency fi , receives an analog signal and converts it into eight 4-bit words using its respective ADC, as depicted in Fig. 2. Afterwards, this digitized signal is forwarded into a block integrated by a DSSS decoder and hard decision sub-blocks. The DSSS decoder architecture was retrieved from [5] but without considering the channel compensation techniques. Each DSSS decoder and hard decision sub-block outputs are forwarded into their respective deserializer DESER(1:32), DESER(2:32), DESER(8:32) and buffer blocks. The access control sub-block deactivates unused sub-blocks during the communication patterns configuration phase, to save energy. 3 RESULTS In order to evaluate the BER, the end-to-end system is modeled in MATLAB using the channel model provided by [4]. This channel model consists of 4 antennas tuned at 200 GHz with a frequency range of 62 GHz, which is used by the FDM scheme. The nonoverlapping frequency bands are distributed using different carrier frequencies: f1 = 195 GHz, f2 = 200 GHz, f3 = 205 GHz, and f4 = 210 GHz. Simulation results are reported in Fig. 3 for unicast/broadcast and multicast communications. The modeled communication system comprises four wireless nodes, which requires three codes (i = 1 . . . 3) by each code size group j . The first group j = 1 provides a Processing Gain PG = 6 dB using a 4-bit code size, and the second group j = 2 brings a PG = 9 dB for 8-bit code size. The PG has two applications [2]. The first one is to improve the signal resilience providing better BER with the same Pt , and the second one aims at reducing Pt keeping the same BER. However, in order to make a fair comparison between both code sizes in terms of BER, the PG effects was suppressed during simulation. The different communication patters based on the MNC-OOK scheme are compared with the theoretical BER for SNC-OOK. As shown in figure 3, the average BER for each communication pattern is slightly 2 (a) (b) (c) Figure 3: performance BER for multicast with (a) PG = 6 dB, (b) PG = 9 dB, and (c) unicast/broadcast. The digital transceiver architecture is modeled in C/C++, synthesized from C to RTL by Catapult HLS and to the gate level by Synopsys Design Compiler. A 28-nm FDSOI technology library is used during hardware synthesis as a target with a supply voltage of 1 Volt. Data interfaces with the router/switch have 32-bit width. Synthesis results of the digital transceiver are detailed in Table 1, which reports both static and dynamic power consumption of each component. Table 1: Area and power consumption of digital transceiver architecture designed using 28-nm FDSOI. WI Block DSSS encoder DSSS decoder and Hard Decision Serializer 32:1:2:8-bit (312.5 Mbps) Serializer 8:1-bit (2.5 Gbps) Deserializer 8:32-bit (312.5 Mbps) Deserializer 2:32-bit (312.5 Mbps) Deserializer 1:32-bit (312.5 Mbps) Access Control Block Area (𝜇m2 ) Power (mW) 121 0.03 150 0.04 391 0.16 94 0.27 190 0.082 200 0.089 221 0.093 500 0.7 clock (GHz) 0.3125 2.5 0.3125 The area overhead of the MNC-OOK digital transceiver for a four-cluster architecture is around 0.0138 mm2 , which is 17 times smaller than the digital transceiver proposed in [1] for multicast communications. Unfortunately, the authors do not provide the power consumption, however, the large area overhead suggests a high power consumption compared to our proposition, which consumes only 2.37 mW in unicast/broadcast mode and 4.82 mW for the full multicast mode. "
Global and semi-global communication on Si-IF.,"On-chip scaling continues to pose significant technological and design challenges. Nonetheless, the key obstacle in on-chip scaling is the high fabrication cost of the state-of-the-art technology nodes. An opportunity exists however, to continue scaling at the system level. Silicon interconnect fabric (Si-IF) is a platform that aims to replace both the package and printed circuit board to enable heterogeneous integration and high inter-chip performance. Bare dies are attached directly to the Si-IF at fine vertical interconnect pitch (2 to 10 μm) and small inter-die spacing (≤ 100 μm). The Si-IF is a single-hierarchy integration construct that supports dies of any process, technology, and dimensions. In addition to development of the fabrication and integration processes, system-level challenges need to be addressed to enable integration of heterogeneous systems on the Si-IF. Communication is a fundamental challenge on large Si-IF platforms (up to 300 mm diameter wafers). Different technological and design approaches for global and semi-global communication are discussed in this paper. The area overhead associated with global communication on the Si-IF is determined.","Global and Semi-Global Communication on Si-IF Special Session Paper Boris Vaisband∗ Department of Elecetrical and Computer Engineering University of California, Los Angeles Los Angeles, CA, USA vaisband@ucla.edu Subramanian S. Iyer Department of Elecetrical and Computer Engineering University of California, Los Angeles Los Angeles, CA, USA s.s.iyer@ucla.edu ABSTRACT On-chip scaling continues to pose significant technological and design challenges. Nonetheless, the key obstacle in on-chip scaling is the high fabrication cost of the state-of-the-art technology nodes. An opportunity exists however, to continue scaling at the system level. Silicon interconnect fabric (Si-IF) is a platform that aims to replace both the package and printed circuit board to enable heterogeneous integration and high inter-chip performance. Bare dies are attached directly to the Si-IF at fine vertical interconnect pitch (2 to 10 µm) and small inter-die spacing (≤ 100 µm). The Si-IF is a singlehierarchy integration construct that supports dies of any process, technology, and dimensions. In addition to development of the fabrication and integration processes, system-level challenges need to be addressed to enable integration of heterogeneous systems on the Si-IF. Communication is a fundamental challenge on large Si-IF platforms (up to 300 mm diameter wafers). Different technological and design approaches for global and semi-global communication are discussed in this paper. The area overhead associated with global communication on the Si-IF is determined. CCS CONCEPTS • Hardware → Buses and high-speed links; Metallic interconnect; Die and wafer stacking; Package-level interconnect; • Networks → Routers . KEYWORDS silicon interconnect fabric, communication, heterogeneous integration, global interconnect ACM Reference Format: Boris Vaisband and Subramanian S. Iyer. 2019. Global and Semi-Global Communication on Si-IF Special Session Paper . In International Symposium on Networks-on-Chip (NOCS ’19), October 17–18, 2019, New York, NY, USA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3313231.3352379 ∗ The author is currently with McGill University, 3480 Rue University, 547 McConnell Engineering Building, Montreal, QC, Canada H3A 0E9. boris.vaisband@mcgill.ca Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’19, October 17–18, 2019, New York, NY, USA © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6700-4/19/10. . . $15.00 https://doi.org/10.1145/3313231.3352379 1 INTRODUCTION On-chip features have scaled approximately 1,000X in the past fifty years, whereas the package features have only scaled approximately 4X within the same period of time, as depicted in Figure 1. Nonetheless, further on-chip scaling is associated with significant challenges, most importantly, high cost per transistor. The high cost of on-chip scaling, combined with the need for heterogeneous integration and increased system performance (higher bandwidth and lower energy) [8, 18] are the main drivers for the ""More than Moore"" approach. Figure 1: Scaling trends of on-chip and package features. The silicon interconnect fabric (Si-IF) is a platform that enables scaling off-chip features and is effectively a system-on-wafer (SoW) exhibiting the performance of a system-on-chip (SoC). In the Si-IF approach, bare dies are integrated using thermal compression bonding (TCB) [7] on a Si wafer, eliminating the need for both package and printed circuit board (PCB). The dies are attached using fine pitch (2 to 10 µm) vertical pillars at high proximity (< 100 µm) [2]. Since no solder is used with TCB, no intermetallics are present at the interface between the Si-IF pillars and the die pads, significantly reducing the interface resistance [10]. An example of over a hundred indium phosphide dies integrated on Si-IF is shown in Figure 2 [16]. Similarly, any type of die can be integrated on the Si-IF, enabling a true heterogeneous system [8, 18]. The Si-IF fabrication process borrows heavily from the highly mature Si technology at relatively large features (minimum of 2 µm), leading to low cost of fabrication. In addition, the Si-IF is a passive platform (no active devices), allowing to maintain a low fabrication cost. The Si-IF alleviates bottleneck challenges in modern integrated circuits [13]. Specifically, the number of inputs/outputs (I/Os) on a chip, governed by Rent’s rule [12], can be increased by up to NOCS ’19, October 17–18, 2019, New York, NY, USA Boris Vaisband and Subramanian S. Iyer 2 SI-IF TECHNOLOGY The Si-IF technology is relatively mature [17]. A comparison is drawn in this section between the Si-IF platform and classical packaging constructs (package and PCB). The advantages and compatibility of the Si-IF as a platform for a large space of applications are discussed. The Si-IF is a single hierarchy integration scheme, where dies are attached directly to a Si wafer that serves as both the package and the PCB. The Si-IF includes wiring levels that are similar to on-chip top-level metals (fat wires) that support seamless on-chiplike communication with neighboring dies. Unlike the Si-IF, the classical package-PCB structures are meant mostly to fan out the pitch of the vertical interconnect from the fine on-chip pitch (2 to 10 µm) to the large PCB-level pitch (500 to 1,000 µm). Signals then traverse on the PCB towards the neighboring packaged chip and follow a reverse, fan-in, path to reduce the vertical interconnect pitch to the on-chip pitch of the second chip. Packages serve as an intermediate stage (between the chip and the PCB) with a vertical interconnect pitch of (100 to 200 µm). The pitch on the PCB cannot be significantly reduced due to material limitation (FR4 is relatively flexible and therefore prohibits fabrication of fine features). The difference between the Si-IF and a package-PCB structure is depicted in Figure 3. Interposers have been used in recent years to bring dies closer together, however, interposers add an additional layer of hierarchy to classical packaging, making the structure further complex and significantly increasing the cost. In addition, interposers can only accommodate a small number of dies and the interconnect pitch is still about an order of magnitude larger than the pitch on the Si-IF. The Si-IF addresses additional key challenges of classical packaging, for example, heat dissipation. In the package-PCB structure, the thermal path from the die towards the PCB is blocked (high thermal resistance) and a heat sink is typically placed on top of the die. Alternatively the Si-IF is a Si wafer that exhibits low thermal resistance and acts as a heat spreader. Moreover, heat sinking can be applied to both sides of the structure, on top of the dies and on the back of the Si-IF [1, 15]. A key advantage of the Si-IF, as mentioned in Section 1, is the large number of I/Os the platform supports. Rent’s rule dictates that the number of signal terminals T is related to the number of internal chip components g (e.g., gates, blocks) as follows T = t · дp , (1) where t and p are constants that represent the technology and circuit complexity. Typically, the exponent p is bounded 0 < p < 1 and approaches unity as the complexity of the chip decreases. In microprocessors, for example, the values of t and p are, respectively, 0.8 and 0.45 [4]. Current packaging technologies cannot support the number of terminals that Rent’s rule dictates, therefore the utilization of serializer/deserializer (SerDes) circuits – a ""bypass"" of Rent’s rule. To eliminate the use of power hungry SerDes, a fine I/O pitch is required similar to the pitch that the Si-IF supports. From (1), across various technology nodes (from 45 to 14 nm), the required vertical I/O pitch to enable elimination of SerDes, is approximately 3 to 7 µm. This range corresponds to the Si-IF vertical interconnect pitch that supports a massive number of parallel short interconnects to communicate among neighboring dies. Figure 2: Over a hundred indium phosphide dies integrated on the Si-IF [16]. The dies are of various size and aspect ratio. two orders of magnitude if integrated at fine pitch on the Si-IF. For example, the aggregate bandwidth of the IBM POWER9 chip is 1,206 GB/s [5] with 2,359 C4 pads dedicated to differential signaling [6], resulting in a data rate per pad of 8.2 Gb/s. Whereas, if integrated using a fine pitch of 3.5 µm, the number of pads dedicated to signaling could be increased by a factor of ~35 (assuming a C4 pad pitch on the package of 125 µm). Moreover, integration of an IBM POWER9 on Si-IF will result in significant reduction in communication energy. Since the Si-IF is agnostic to the type of dies that are integrated on the wafer, diverse technologies, materials, and functionalities within a single system are supported by the platform. The Si-IF is, therefore, compatible with a wide range of applications. For example, intelligent IoT edge devices that are expected to support both sensing and actuation, energy harvesting and storage, processing, memory, and communication [20]. Moreover, traditionally homogeneous applications, such as CPUs, are also shifting towards heterogeneity to alleviate the bottlenecks of classical von-Neumann architectures. Various emerging technologies, e.g., memristors, that require integration of disparate materials are explored for those applications. CPU-memory or GPU-memory systems are also fundamentally heterogeneous. Ideally each of the components (processor and memory) is fabricated using an optimal technology node and process for the function that the component is expected to perform. All of these applications can benefit from integration on the Si-IF platform. A wide variety of system-level challenges need to be addressed to enable the full potential of the Si-IF platform. Specifically, power delivery and thermal management [1, 15], testing, communication [19], etc. In this paper, global and semi-global communication on the Si-IF is discussed. The rest of the paper is composed of the following sections. The Si-IF technology is reviewed in Section 2. Global communication aspects including a comparison to classical package-PCB constructs are provided in Section 3. Semi-global communication and the differentiation between global and semi-global communication is discussed in Section 4. A scheme for external, i.e., off-Si-IF, communication is presented in Section 5, followed by some conclusions in Section 6. Global and Semi-Global Communication on Si-IF Special Session Paper NOCS ’19, October 17–18, 2019, New York, NY, USA Figure 3: A comparison between the Si-IF and a classical package-PCB construct [3, 8, 11]. 3 GLOBAL COMMUNICATION ON SI-IF Communicating to neighboring dies on the Si-IF (up to 500 µm) is a solved problem based on the approach described in Section 2 (also known as the SuperCHIPS protocol [10, 11]). Global communication however, requires additional attention. Since the Si-IF is an SoC-like SoW, the global communication approach of SoCs, i.e., networks-on-chip (NoCs) can be adapted for the wafer-scale integration. The network on interconnect fabric (NoIF) [19] borrows conceptually from NoCs. The NoIF however, performs additional services to support the system that is integrated on the Si-IF, including power management, synchronization, testing, and more [19]. Similarly to NoCs, the NoIF includes routing nodes, however, the nodes within the NoIF are dies, similar to the functional dies (FDs) that are integrated on the wafer. The node dies are called utility dies (UDs). UDs include various circuits to support the services that the NoIF provides to the system. A schematic of the NoIF is depicted in figure 4. The Si-IF facilitates IP reuse since the integration on the platform is backwards compatible to larger pitches and also solderbased integration. IP reuse is a major advantage of the Si-IF since a large portion of modern integrated circuits (ICs) is unnecessarily redesigned at advanced technology nodes. For example, SerDes circuits can provide high performance at older technology nodes (65 and 45 nm), however, since they are located within modern chips that do not support heterogeneous fabrication or integration, the SerDes circuitry must be redesigned. The Si-IF supports integrating off-the-shelf IPs such as SerDes components without the need to redesign them. This advantage can be utilized for the NoIF to support global communication on the wafer. µ-SerDes IPs can be integrated as a sub-component of a hierarchical UD (HUD), next to a UD control die (also a sub-component of the HUD). Additional examples of sub-component dies within the HUD are antenna die for wireless communication between Si-IFs and clock distribution across the Si-IF platform, and linear dropout (LDO) IP for power regulation. The HUD concept is illustrated in Figure 5. Communication among the sub-components of the HUD is realized using SuperCHIPS. In addition to µ-SerDes circuits, global communication across the Si-IF can be realized using optical interconnect. This approach however, requires development of photonic waveguides within the Si-IF platform as well as conversion circuits (electrical-photonic-electrical). Existing communication protocols and all NoC-related routing algorithms can be utilized for the NoIF to ensure high performance of global communication. Similarly to NoCs, NoIF is a necessary overhead for the system integrated on the Si-IF. The area overhead that the NoIF imposes is evaluated based on the parameters listed in Table 1 and plotted as a function of the number of UDs and the area of each UD in Figure 6. A typical minimal overhead of NoCs for SoCs at 32 nm is ~12% [14], marked for reference in Figure 6. Note that the columns for 4,096 UDs at 16 and 25 mm2 are missing since the values are above 100%. The NoIF exhibits a wide range of area overhead depending on the number and size of UDs. Nonetheless, majority of the UD number-area combinations exhibit an area overhead that is smaller than typical NoCs. Figure 4: A NoIF based on UDs integrated alongside FDs on the Si-IF. Figure 5: Hierarchical utility die consisting of subcomponent dies integrated alongside functional dies. NOCS ’19, October 17–18, 2019, New York, NY, USA Boris Vaisband and Subramanian S. Iyer Table 1: Si-IF and UD parameters used to determine the area overhead of the NoIF. Parameter Si-IF diameter (mm) Si-IF area (mm2 ) Effective area percentage (%) Effective area (mm2 ) Number of UDs Area of each UD (mm2 ) Value 300 ~70,686 90 ~63,617 {4, 16, 64, 128, 256, 1,024, 4,096} {1, 4, 9, 16, 25} Figure 7: Distance between neighboring UDs as a function of the number of UDs and the area of each UD. Note, large UDs refer to HUDs. Figure 6: Area overhead of NoIF as a function of the number of UDs and the area of each UD. Note, large UDs refer to HUDs. 4 SEMI-GLOBAL COMMUNICATION ON SI-IF Utilizing the NoIF incurs a certain cost. Data needs to be packetized and routed within the network until it reaches the destination FD. For communicating with distant FDs, it is the only reasonable approach. However, at shorter distance, the cost of propagating data within the NoIF may be higher than direct wire communication. Semi-global communication targets a range of destination FDs that are located outside of the range of the SuperCHIPS approach (effective up to 500 µm), but also placed too close to justify the cost of utilizing the NoIF. Semi-global communication does not load the NoIF. The communication range that is the cutoff point between global and semi-global communication in terms of energy and delay, is found to be approximately 10 mm [21]. Since an optimal die area for integration on the Si-IF is from 1 to 100 mm2 [9], this 10 mm distance may span one to ten dies. Repeaters, placed on both FDs and UDs, are required to propagate signals across 10 mm. The design space of the NoIF shown in Figure 6 can be narrowed down by addressing only the relevant number and size of UDs. The distance between neighboring UDs as a function of the number of UDs and the area of each UD is shown in Figure 7 (evaluation is based on the parameters from Table 1. Note that the columns for 4,096 UDs at 16 and 25 mm2 are missing since the values are smaller than zero, i.e., overlapping UDs. A zoom-in on the number and size of UDs that result in a distance of 10 mm between UDs is shown in Figure 8. For clarity, the UD area axis in Figure 8 is reversed (as compared to Figures 6 and 7). Figure 8 defines the right number and size of UDs (around the 10 mm cutoff Figure 8: Zoom-in on the distance between neighboring UDs (10 mm) that is the cutoff point between global and semiglobal communication, as a function of the number of UDs and the area of each UD. For clarity, the UD area axis in Figure 8 is reversed (as compared to Figures 6 and 7). point) that are required to ensure efficient communication on the Si-IF. It can be concluded from Figure 8 that, depending on the size of each UD, the number of UDs on the Si-IF should be between 300 and 500. According to Figure 6, the area overhead associated with this number of UDs, depending on the size of each UD, is approximately between 0.5 and 19.5%. 5 EXTERNAL COMMUNICATION In addition to communication on the Si-IF, a scheme is required to communicate between the system on the IF and external components, such as various interface I/Os, panels, additional Si-IFs, etc. To support external communication for the Si-IF, an FR4 ring connector along the periphery of the Si wafer is developed, as demonstrated in Figure 9. Pads are fabricated on the ring connector and compatible pads are fabricated on the periphery of the Si-IF. The ring connector attaches to the pads on the Si-IF using solder paste and enables communication to/from the integrated system. Any circuitry required for the realization of various communication protocols can be placed on the ring connector board and signals Global and Semi-Global Communication on Si-IF Special Session Paper NOCS ’19, October 17–18, 2019, New York, NY, USA "
APEC - improved acknowledgement prioritization through erasure coding in bufferless NoCs.,"Bufferless NoCs have been proposed as they come with a decreased silicon area footprint and a reduced power consumption, when compared to buffered NoCs. However, while known for their inherent simplicity, they suffer from early saturation and depend on additional measures to ensure reliable packet delivery, such as control protocols based on ACKs or NACKs. In this paper, we propose APEC, a novel concept for bufferless NoCs that allows to prioritize ACKs and NACKs over single payload flits of colliding packets by discarding the latter. Lightweight heuristic erasure codes are used to compensate for discarded payload flits. By trading off the erasure code overhead for packet retransmissions, a more efficient network operation is achieved. For ACK-based networks, APEC saturates at 2.1x and 2.875x higher generation rates than a conventional ACK-based bufferless NoC for packets between 5 and 17 flits. For NACK-based networks, APEC does not require concepts such as deflection routing or circuit-switched overlay NACK-networks, as prior work does. Therefore, it can simplify the network implementation compared to prior work while achieving similar performance.","APEC: Improved Acknowledgement Prioritization through Erasure Coding in Bufferless NoCs Michael Vonbun Chair of Integrated Systems Technical University of Munich Munich, Germany michael.vonbun@tum.de Adrian Schiechel Chair of Integrated Systems Technical University of Munich Munich, Germany adrian.schiechel@tum.de Nguyen Anh Vu Doan Chair of Integrated Systems Technical University of Munich Munich, Germany anhvu.doan@tum.de Thomas Wild Chair of Integrated Systems Technical University of Munich Munich, Germany thomas.wild@tum.de Andreas Herkersdorf Chair of Integrated Systems Technical University of Munich Munich, Germany herkersdorf@tum.de ABSTRACT Bufferless NoCs have been proposed as they come with a decreased silicon area footprint and a reduced power consumption, when compared to buffered NoCs. However, while known for their inherent simplicity, they suffer from early saturation and depend on additional measures to ensure reliable packet delivery, such as control protocols based on ACKs or NACKs. In this paper, we propose APEC, a novel concept for bufferless NoCs that allows to prioritize ACKs and NACKs over single payload flits of colliding packets by discarding the latter. Lightweight heuristic erasure codes are used to compensate for discarded payload flits. By trading off the erasure code overhead for packet retransmissions, a more efficient network operation is achieved. For ACK-based networks, APEC saturates at 2.1x and 2.875x higher generation rates than a conventional ACK-based bufferless NoC for packets between 5 and 17 flits. For NACK-based networks, APEC does not require concepts such as deflection routing or circuit-switched overlay NACK-networks, as prior work does. Therefore, it can simplify the network implementation compared to prior work while achieving similar performance. KEYWORDS network-on-chip, bufferless, erasure codes ACM Reference Format: Michael Vonbun, Adrian Schiechel, Nguyen Anh Vu Doan, Thomas Wild, and Andreas Herkersdorf. 2019. APEC: Improved Acknowledgement Prioritization through Erasure Coding in Bufferless NoCs. In International Symposium on Networks-on-Chip (NOCS ’19), October 17–18, 2019, New York, NY, USA. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3313231. 3352366 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’19, October 17–18, 2019, New York, NY, USA © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6700-4/19/10. . . $15.00 https://doi.org/10.1145/3313231.3352366 1 INTRODUCTION Overcoming both the wiring bottleneck of any-to-any connections and the bandwidth bottleneck of bus-based interconnects, Networks-on-Chips (NoCs) have developed into a mature interconnect technology due to their scalability for modern tiled multiprocessor System-on-Chips (MPSoCs) [1]. NoCs, in general, can be classified into either circuit- or packetswitched networks. Packet-switched NoCs are further divided into two categories: buffered and bufferless. Thanks to their high throughput and versatility, buffered NoCs are widely used in MPSoCs. Compared to buffered designs, bufferless NoCs have a reduced router complexity and a smaller silicon area footprint [14]. In modern MPSoCs, the power consumption of the interconnect accounts for a considerable amount of the overall power [9]. Therefore, bufferless NoCs are suitable for energy efficient designs, having less combinatorial logic to drive and sparing the buffers’ dynamic switching and static leakage power [10]. The lack of buffers requires bufferless NoCs to resolve conflicts originating from packets competing for the same link by dropping one of them. On the one hand, bufferless NoCs are deadlock-free by design thanks to the packet drop mechanism. On the other hand, a protocol is needed to ensure reliable communication and the network will saturate earlier because part of its bandwidth is occupied by retransmissions. Protocol-wise, ACK- or NACK-based signaling can be used. ACKbased networks are easier to implement, since loss protection for ACKs is not needed. However, the penalty of losing an ACK will cause the retransmission of a redundant packet that has already been received successfully by the receiver. For NACK-based networks, the loss of a NACK must be prevented, since the information of an undelivered packet will be lost alongside the loss of the NACK. To solve both problems, we propose APEC (Acknowledgment Prioritization through Erasure Coding), a novel bufferless NoC architecture that prioritizes ACKs and NACKs even if a link is already arbitrated by a regular packet. As a consequence, the flits within a packet are subject to erasures as well and the packet integrity is perturbed. To compensate this effect on the flit level, we propose to use lightweight heuristic erasure codes to protect the packets against interleaved ACK or NACK flits. Our contribution is two-fold: NOCS ’19, October 17–18, 2019, New York, NY, USA Vonbun et al. First, using APEC leverages the performance of ACK-based NoCs to a level where they become practical for on-Chip communication while reducing the router complexity compared to a NACK-based NoC at the same time. Second, using APEC in a NACK-based NoC can reduce the router resources compared to prior work while achieving the same performance. The paper is organized as follows. In Section 2 related work is presented. In Section 3, we introduce the concept of our improved acknowledgment prioritization and introduce an heuristic erasure code targeted at low encoder/decoder complexity. Section 3.3 covers the adaptations needed to build a NACK-based NoC using the proposed concept. In Section 4, we discuss simulation results for both ACK- and NACK-based NoCs, before concluding the paper in Section 5. 2 RELATED WORK Aiming at a reduction in router complexity and resources, bufferless NoCs have been studied in various previous works. In [6], Gómez et al. have shown that by eliminating the input and output buffers, the router frequency can be increased and the power consumption of the interconnect network can be reduced by introducing their blind packet switching (BPS) NoC. The proposed NoC uses a NACK-based protocol. It is based on a fully adaptive routing mechanism using deflection routing and multiple parallel links in the same direction to reduce the number of collisions and achieve a high network utilization. The differences to our work are threefold. First, as we focus on the novel concept of using erasure codes, we do not use deflection routing to isolate the influence of the new approach. As a side effect, link utilization is kept at a minimum since only minimum distance paths are used. Second, BPS uses 4 to 8 parallel links in the same direction by replicating the routing resources. This increases the network’s capacity and required resources by a considerable amount. In contrast, we use simple bidirectional links for all connections. Last, to ensure NACK delivery, BPS uses NACK queues that are unique per switch and hold up to 4 NACKs. In our approach, we have no additional buffers in addition to the router’s input buffers. Different routing algorithms and a comparison of wormhole and single-flit packets have been studied by Moscibroda and Mutlu in [14]. The routing algorithms relies on deflection routing and portprioritization. The flit priority is used in combination with a worm truncation mechanism, where new header flits can be generated on the go to avoid livelocks and to ensure packet delivery. As stated before, we avoid deflection routing and use a protocol based on acknowledgement flits to ensure packet delivery. In [8] Hayenga et al. used a hybrid approach of a circuit switched control network for the NACKs and a packet-switched bufferless NoC for data transmission. This hybrid approach allows fast source notifications in the case of packet collisions. In contrast, we use a single packet-switched NoC for both data and control messages, thus allowing us to keep the interconnect wires both short and localized instead of using global wires. Fallin et al. showed an improved version of a bufferless NoC using deflection routing in [3]. Replacing the arbitration logic and crossbar of conventional deflection routers by a permutation network, the router complexity could be reduced. To avoid livelocks, the concept (a) (b) (c) H H H B0 B1 B2 B3 A B1 B2 B3 B0 B1 B2 B3 T T C T HEAD H BODY Bn TAIL T ACK A CODE C Figure 1: Concept of discarding a payload flit. Packet integrity is (a) maintained, (b) perturbed by an ACK/NACK and (c) protected using an erasure code. of a golden packet was introduced, a priority status every packet might reach at some point and that is rare in a limited time window that suffices to deliver the packet. The absence of deflection routing and the focus on the packet organization instead of the router architecture are the main differences to our work. In [4], Fallin et al. have shown that by combining deflection routing with small side buffers to reduce the effects of deflections can reduce the energy consumption of an NoC. In [2], Daya et al. use a distance based priorization to reduce the number of deflections. In addition, look-ahead wires are used to arbitrate the path of a packet over multiple routers. Li et al. propose to use a bufferless NoC in combination with a second reliable buffered network in [11]. The proposed bufferless plane is operated without explicit ARQ messages that handles retransmissions; rather, retransmissions are handled by the second, reliable and buffered, NoC plane. This allows to use the bufferless as accelerator in high-load scenarios or as a power saver in low-load scenarios. Feng et al. focused on network multicast in bufferless NoCs in [5] by proposing a configurable switch based on a Banyan network to efficiently support multicasting. Packet collisions are handled using deflection routing. In contrast to the works described above, our work does not rely on deflection routing, intermediate NACK buffering, or adding a second network plane. Rather, it is a generic concept suited to realize both ACK- and NACK-based NoCs. 3 IMPROVED ACKNOWLEDGMENT PRIORITIZATION As the communication in bufferless NoCs is inherently unreliable, it has to be secured. To this end, Automatic repeat requests (ARQs) can be implemented, using control flits that signal the source if the packet could be delivered. It is clear that control flits should have a high priority such that they can be used for efficient signaling. Therefore, if a control flit hits a packet header, the packet header, and therefore the entire packet, is discarded in favor of the acknowledgment (cf. Figure 3 (a)). Additionally, to reduce the routing overhead, NoCs are often operated using wormhole switching, where a header switches the network and body flits follow along the arbitrated path from source to destination. Thus, if a header reaches its destination, the entire packet reaches the same destination. These two requirements are conflicting and lead to a design where body and control flits compete for the highest priority. To APEC: Acknowledgement Prioritization through Erasure Coding NOCS ’19, October 17–18, 2019, New York, NY, USA s p o r d K C A 80% 60% 40% 20% 0% 0 .00 5 flits 9 flits 0 .02 0 .04 Generation Rate [flit/tile/cycle] 0 .06 Figure 2: Percentage of dropped ACK flits in an 8-by-8 2Dmesh NoC with 5 and 9 flit per packet. maintain packet integrity, body flits are favored (cf. Figure 3 (b)). As this design decision makes the control flits vulnerable to an erasure themselves, a NACK-based bufferless NoC is therefore not feasible without taking additional measures anymore. For ACK-based bufferless NoCs, the consequence of this decision is depicted in Figure 2. It shows the development of the percentage of dropped ACKs over the network generation rate due to congested links. In this work, we propose to rethink the body-flit prioritization and change it in favor of control flits (cf. Figure 3 (f )). Violating the packet integrity yields an additional degree of freedom which can then be exploited to increase the network performance. 3.1 ACK-based Networks-on-Chips We assume that the routing information is stored in headers and ACKs as a source-destination pair (cf. Figure 4). Packet identification is based on this pair and a sequence number, which is copied to the ACKs upon successful packet transmission. For the release mechanism of arbitrated links, there are two options for ACK-based NoCs. The first and most commonly-used one is using a tail-flag in the last flit of the packet. Since they release the path arbitration of the header, they must not be deleted, which contradicts our design goal to some extent. Nevertheless, the NoC tolerates the loss of an ACK by design, so using tail flits remains a valid option. The second option is to encode the packet length into the header, which is then used to release the path arbitration. By using this scheme, all flits of the same packet following the header are bodyflits. In contrast to tail-flag based packets, this yields two advantages. First, less control bits per packet are required since the packet length can be encoded with logarithmic cost, whereas the tail-flag mechanism needs one bit per flit. Second, all flits can be erased without exception. With these preliminaries, ACK prioritization can be implemented as follows: whenever an ACK is observed at an input port of a router the ACK is forwarded even if the output link is arbitrated by another packet (cf. Figure 3 (f )). If the output link was arbitrated by a packet earlier, one of the packet’s flit gets erased in favor of an ACK. However, colliding ACKs are handled in the same manner as in a conventional ACK-based network by discarding one of the ACKs (cf. Figure 3 (c)). In order to ensure reliable communication, the sink’s network interface needs to check for the data integrity of incoming packets. It is sufficient in this case to just count the number of incoming flits using the flit’s data-valid-flag, as shown in Figure 4, to detect a corrupted packet. So far, the scheme does not differ from the conventional ACKbased operation: because of a collision with an ACK, an entire packet needs to be retransmitted from the source. In the conventional ACK-based case, this retransmission is due to a lost ACK, while in the ACK-prioritized case it is due to a corrupt packet. A proper counter measure that renders the ACK-prioritization superior, however, is the introduction of heuristic erasure codes at the flit level. This allows to compensate for discarded flits, which we will discuss in the next section. 3.2 Heuristic Erasure Codes Erasure codes for computer networks have been studied for more than a decade [12, 13, 15, 17]. Since they are targeted at compensating packet erasures, these codes cannot be applied to NoCs easily. The relationship between computer networks and NoCs is that the prior considers entire packets as the entities to be encoded (i.e. the source symbols) whereas in the bufferless NoC case, the payload flits are considered as source symbols, as we apply an erasure code on the flit level. This renders the proposed erasure codes infeasible for two reasons. First, for a good performance, most of these codes require a block length (k ) of more than 10000 packets. When translated to flits per packet in an NoC this becomes infeasible as the packets are designed to be short. Second, as the codes were designed for computer networks, the decoder complexity and decoding delay are too high for NoCs. For decoding, list and syndrome decoders have been proposed, which become efficient only for “reasonably sized packets (say above 256 bytes)” [15], which translates to the size of a single flit in an NoC. Instead, we propose a heuristic erasure code targeted at NoCs with both efficient encoding and decoding in mind. In theory, if a packet is extended by C code flits, it is also protected against C erasures. This relation holds only for two cases: (a) if C → ∞, or (b) if C = 1. Since the packet length is rather short, case (a) is not of interest. Case (b) can be achieved if the code flit is an XOR of all prior payload flits. Therefore, by also using a successive XOR in the decoder, a missing flit can be recovered. Our heuristic erasure code extends this concept by slicing the packet payload into blocks of B flits. For each block, one code flit, which is the XOR over all flits in one block, is appended at the end of the block. By adjusting the block size, we gain fine grained control over the code rate of the erasure code. The encoding process is described in detail in Algorithm 1 in Appendix A and depicted for a block size of B = 4 and a packet with 8 payload flits in Figure 5. Likewise, the decoding process is described in Algorithm 2 in Appendix A and illustrated in Figure 6. Similarly to the encoder, the decoder successively performs a bit-wise XOR on incoming flits with high data_valid flag. To this end, at the beginning of each block it stores the first flit in an XOR-register and resets its decoding counter, which is then increased in each clock cycle for B cycles. If data_valid is low, it stores the current value of the decoding counter. After B cycles, it is decided if at least B − 1 flits with data_valid flag arrived, and in case only B − 1 flits arrived, NOCS ’19, October 17–18, 2019, New York, NY, USA Vonbun et al. H B A/N (a) A/N (f ) B A B N A/N B A A A B N N N (b) cN (g) A/N N (c) (d) (e) HEAD ACK H A BODY NACK B N cNACK C N flit forwarded flit dropped desired path Figure 3: Flit prioritization in bufferless NoC routers. SRC DEST SEQ# PLEN . . . Table 1: Number of Flits per Packet using Different Schemes. Description Conventional ACK-based (without APEC) Uncoded 1 Codeflit per packet 4-Block XOR 8-Block XOR 16-Block XOR Packet length 9 17 33 65 Id A 9 10 11 10 10 17 18 21 19 18 33 34 41 37 35 65 U 66 C 81 C4 73 C8 69 C16 5 5 6 6 6 6 involves setting the NACK control-flag and switching the header and source fields of the NACKed packet’s header (cf. Figure 4), the silicon overhead of the NACK generation module is marginal. Second, the network interface needs a decoder module – just as in the ACK-based case – and the capability to generate NACKs after the decoder failed to decode the blocks of a packet. In comparison with a NACK-based NoC without erasure code protection, this introduces both an increased timeout at the source node and an increase in the average latency between packet injection and NACK generation. 3.3.1 NACK Combining. Special attention has to be paid to the case when two NACKs compete for the same link. To resolve this conflict, we merge the NACKs into combined NACKs (cNACKs), as illustrated in Figure 3 (g), instead of implementing a deflection routing mechanism for NACKs (cf. Figure 3 (e)). The structure of such a cNACK is depicted in Figure 4 (c): in addition to the number of NACKs the cNACK carries the source, destination, and sequence number of every NACK that got merged into the cNACK. Clearly, due to the limitation of the data width of a single flit, only a limited number of NACKs can be combined into a single cNACK. While the amount of bits required for the source and destination addresses is predetermined by the network size (4 bits for a 2-by-2 mesh, 8 bits for meshes up to 4-by-4, and 12 bits for meshes up to 8-by-8), the amount of bits needed for the NACKed packet’s sequence number correlates to the number of packets that have to be kept in the source’s network-interface until they are successfully received, i.e. until the timeout to wait for a NACK expires. (a) (b) (c) (d) A A A H H H H V V V V SRC DEST SEQ# . . . #NACK SRC0 DEST0 SEQ#0 SRC1 . . . . . . Figure 4: Structure of flit types of packet-length NoC: (a) Header, (b) ACK, (c) NACK/cNACK, and (d) Body. H, V, and A are single bit flags for header, data_valid, and acknowledgment, respectively. SRC, DEST, SEQ#, PLEN, and #NACK are multibit fields for source, destination, packet sequence number, packet length, and number of NACKs, respectively. the missing flit is reconstructed using the XOR-register. Otherwise, the decoding fails. Because of its simplicity, this scheme, although sub-optimal, is well suited for a low-cost implementation in bufferless NoCs. The overhead in terms of data redundancy is summarized in Table 1. 3.3 NACK-based Networks-on-Chips Choosing the packet-length resource release mechanism over the tail-flag mechanism as described in Section 3.1 allows to implement a NACK-based bufferless NoC, as control flit delivery is guaranteed even without using deflection routing (cf. Figure 3 (d)) but using the same prioritization as in the ACK-based case (cf. Figure 3 (f )). In contrast to an ACK-based NoC a NACK-based one has several advantages. Indeed, if a packet is received successfully, no control message is generated, thus resulting in a reduction of control message count. Also, if a packet gets already erased near the source, the corresponding router can immediately issue a NACK, causing an earlier release of the occupied resources and a reduced delay between retransmissions at the source. Despite the network operation with prioritized ACK delivery, as described in Section 3.1, the NACK-operated NoC needs further functionality. First, the NoC routers require additional circuitry to be able to generate NACKs themselves. Since the NACK generation only APEC: Acknowledgement Prioritization through Erasure Coding NOCS ’19, October 17–18, 2019, New York, NY, USA H B0 B1 B2 B3 B4 B5 B6 T H B0 B1 B2 B3 C0 B4 B5 B6 C1 T (a) Tail-flag H B0 B1 B2 B3 B4 B5 B6 B7 H B0 B1 B2 B3 C0 B4 B5 B6 B7 C1 (b) Packet length H Bn T Cn HEAD BODY TAIL CODE flit XOR Figure 5: Heuristic encoder example for a packet with 8 payload flits and an erasure code with a blocksize of 4. The encoding can be performed successively using one payload flit per cycle. H B0 B1 X B3 C0 B4 B5 B6 B7 C1 B2 X H B0 B1 B2 B3 B4 B5 B6 B7 H Bn Cn HEAD BODY CODE XOR Figure 6: Heuristic decoder example for a packet with 8 payload flits and an erasure code with a blocksize of 4. The decoding can be performed successively using one flit per cycle. The number of pending packets in the source can be modeled using an M/D/1 queuing system, which describes the number of packets in a single-server waiting queue. For such a system, the average number of packets in the queue is given by [7]: Lq = 1 2 (cid:16) λ µ (cid:17) 2 1 − λ where λ represents the arrival rate and µ the service rate. While λ equals the packet generation rate in an NoC, we use the inverse of the network-interface’s timeout t timeout as value for µ : µ , (1) µ = 1 t timeout t timeout = tnw + tdec + thol = 2 (N rows + N columns + Nmax. packet length ) tnw = 2 (N rows + N columns ) tdec = Nmax. packet length thol = Nmax. packet length , (2) where t timeout is the network-interface’s timeout, tnw the network’s round-trip time, tdec the decoding time, thol the head-of-line-blocking time in the network-interface, and N rows and N columns the number of rows and columns of the mesh, respectively. Since the decoding progresses after the reception of each new flit, it is concluded after the entire packet has been received. For instance, for an 8x8 mesh with 9-flit packets and 2 flits of erasure code overhead, t timeout = 2 (8 + 8 + 11) = 54, so µ = 1/54. Assuming a flit generation rate of 0.16flit/cycle serving a Source Encoder NI Router . . . Sink Decoder NI Router Figure 7: Block diagram of simulation model. single destination exclusively, we have λ = 0.16/9 packet per cycle. In this case, Lq = 11.52 packets, meaning that 4-bit sequence numbers are enough to support that flit generation rate on average. In the case of random uniform traffic, this rate reduces to (0.16 flit/cycle)/63 destinations = 2.54 · 10−3 flit/cycle/destination. Using a more conservative value of 5-bit sequence numbers, a flit width of 128 bits is sufficient to combine up to 7 NACKs in one cNACK for an 8x8 mesh. However, as our simulations in Section 4.3 showed, no more than 4 NACKs need to be combined in practice. 4 SIMULATION RESULTS For our simulations we developed a bufferless NoC model using the event-based network simulator Omnet++ [16]. Our simulation model is composed of 6 different blocks as depicted in Figure 7. Raw packets are generated at the source. In a coded simulation configuration, the encoder generates and inserts the code flits in the raw packets before sending them to the network interface. The network interface handles the transmission over the network which consists of interconnected routers. At the receiver side, the packet is passed to the decoder which tries to reconstruct possibly corrupted packets. After a successful decoding of a packet, it is sent to the sink. The model source files as well as the configuration files used to generate the results are freely available at https://github.com/ mvonbun/omnet- apec- bless- noc. 4.1 Simulation Setup In our simulations, we focused on uniform random synthetic traffic for the sake of comparability. The packet generation follows an independent and identically distributed Poisson process in each tile. The mean is chosen to meet the required generation rates between 0.01 flit/cycle/tile and 0.145 flit/cycle/tile with a step of 0.015. All packets within a single simulation run have a fixed size of 5, 9, 17, 33, 65, 127, or 257 flits. Routers have a single cycle latency. NOCS ’19, October 17–18, 2019, New York, NY, USA Vonbun et al. 250 200 150 100 50 ] e l c y c [ y c n e t a L . g v A A5 U5 C5 A9 U9 C9 A17 U17 C17 500 400 300 200 100 ] e l c y c [ y c n e t a L . g v A C5 C17 C65 C257 C8 17 C8 65 C8 257 C32 65 C32 257 C128 257 0 0 .00 0 .05 0 .10 Generation Rate [flit/tile/cycle] 0 .15 0 0 .00 0 .05 0 .10 Generation Rate [flit/tile/cycle] 0 .15 Figure 8: Latency comparison of conventional ACK-based (A) and ACK-based APEC without (U) and with (C) erasure code in an 8-by-8 2D-mesh NoC for different packet sizes. Figure 9: Latency comparison of different erasure codes of ACK-based APEC 8-by-8 2D-mesh NoC for different packet sizes. For our comparison, we favor the generation rate over the injection rate for two reasons. First, using the raw data generation rate allows for a comparison of both conventional packets and APEC NoC packets. Having redundant data, the packets in the APEC NoC cause an increase in net traffic. Using the injection rate, the redundant flits would be accounted like regular flits, thus complicating both the interpretation and comparison of the results. Second, the generation rate is closely related to the application layer of communication. Therefore, our latency is also measured between packet generation at the source and successful packet reception at the sink. The different configurations of the simulation are labeled using the same naming convention as in Table 1: A, U, C, and Cn (n = 4, 8, . . . ) are for conventional ACK-based, uncoded APEC (without erasure code), APEC with 1 code-flit per packet, and APEC with 1 code-flit per block of n payload flits, respectively. If not stated otherwise, all results were obtained using the packet length to release arbitrated links. The simulated time was 106 cycles with a warm-up period of 10000 cycles. 4.2 ACK-based Operation Conventional NoC Comparison. As can be seen from Figure 8, the latency of a conventional ACK-based NoC (solid green lines) saturates between 0.04 flit/cycle/tile and 0.055 flit/cycle/tile for long 17-flit-packets (half-diamond markers) and short 5-flit-packets (circle markers). Higher generation rates are infeasible using this scheme. Using APEC, without applying an erasure code (dashed brown lines), there is a latency penalty for small generation rates, as the decoder sends ACKs only after the entire packet has been received, as opposed to an instant ACK generation upon header reception in conventional ACK-based networks. Despite this latency penalty, using APEC, the NoC remains operational over a wider range of generation rates with a nearly linear increase in average packet latency and saturates at 0.085 flit/cycle/tile and 0.10 flit/cycle/tile for 17-flit-packets (crossed-circle markers) and short 5-flit-packets (square markers). The performance can be improved further by using a singleflit erasure code (dotted purple lines). This time, there is only a small latency penalty, mainly attributed to the increased packet length due to the erasure code. In contrast to the uncoded case before, the decoding delay is compensated by a reduction of the average number of retransmissions. As the generation rate can be increased to 0.115 flit/cycle/tile before saturation, APEC yields a 2.1x to 2.875x higher performance as compared to a conventional ACK-based NoC. Erasure Code Evaluation. A comparison of the performance of different erasure codes for packets of different sizes is provided in Figure 9. In this figure, packets with more flits have a higher base latency, since we used the application layer as the reference for the latency measurement, as described in Section 4.1. Also, longer packets block the available network resources for a longer time. On the other hand, longer packets are more efficient when the routing and acknowledgment overhead is taken into account. In Figure 9 we can see that adding only a single erasure code flit yields the best performance for almost all packet sizes (solid red lines). For 17-flit packets (square markers), adding 1 or 2 code flits, i.e. having a block length of 8 flits, yields almost equal results. If the packet size is increased to 65 flits (triangle markers), the trade-off between code strength and latency can be observed. Using a code with a small block length of 8 flits (dashed blue line) is inferior than using only a single code flit (solid red line), but adding another code flit by using a block length of 32 flit (dotted green lines) is superior. The same trend can be observed for even bigger packets with 257 flits (half square markers). A code with a block length of 8 flits adds too much redundancy but increasing the block length to 128 flits yields the best performance here. Tail Flag Based Link Releases. In comparison to a packet length based link release, tail flag based releases allow for simplified router designs. However, as the results in Figure 10 indicate, this comes also with a small performance degradation. Especially for small packet sizes of 5 and 9 flits, the packet length mechanism benefits from the additional degree of freedom of being able to delete the tail flit as well. As the packet size increases, the body-flit to tail-flit ratio decreases and the performance of both schemes equalize. APEC: Acknowledgement Prioritization through Erasure Coding NOCS ’19, October 17–18, 2019, New York, NY, USA 500 400 300 200 100 ] e l c y c [ y c n e t a L . g v A FC5 C5 FC9 C9 FC17 C17 FC33 C33 FC65 C65 500 400 300 200 100 ] e l c y c [ y c n e t a L . g v A C5 C17 C65 C129 C8 17 C8 65 C8 129 C32 65 C32 129 C64 129 0 0 .00 0 .05 0 .10 Generation Rate [flit/tile/cycle] 0 .15 0 0 .00 0 .05 0 .10 Generation Rate [flit/tile/cycle] 0 .15 Figure 10: Latency comparison of packet length based (C) and tail flag based (FC) link release of ACK-based APEC 8by-8 2D-mesh NoC for different packet sizes. Figure 12: Latency comparison of NACK-based APEC 8-by8 2D-mesh NoCs with different packet sizes and different erasure codes. 250 200 150 100 50 ] e l c y c [ y c n e t a L . g v A AC5 NU5 NC5 AC9 NU9 NC9 AC17 NU17 NC17 0 0 .00 0 .05 0 .10 Generation Rate [flit/tile/cycle] 0 .15 Figure 11: Latency comparison of ACK-based (A) and NACKbased (N) APEC 8-by-8 2D-mesh NoC for different packet sizes, both uncoded (U) and coded (C). 4.3 NACK-based Operation ACK-based NoC Comparison. The difference between NACKbased and ACK-based APEC networks as well a comparison of NACK-based NoCs with and without erasure coded packets can be seen in Figure 11. Just as in the ACK-based NoCs, APEC with an erasure code (green lines) outperforms its uncoded counterpart (blue lines). However, the performance gain when using erasure codes in NACK-based networks is more pronounced: for 5-flit-packets (solid lines), the saturation increases from 0.055 flit/cycle/tile to 0.085 flit/cycle/tile, and increases from 0.04 flit/cycle/tile to 0.1 flit/cycle/tile for 17-flit-packets (dotted lines). On the one hand, when compared to ACK-based networks, NACKbased ones show a very steep saturation point. The reason for this is mainly due to shorter retransmission times, as the NACKs are generated mostly within the NoC itself. If the NoC is temporarily under heavy load, the short turnaround time of NACKs increase the effect. In ACK-based networks, this effect is not as pronounced, as the retransmission timeouts are bigger. On the other hand, NACK-based networks yield roughly 50% lower latencies and show almost no increase in the latency over their feasible generation rates. Especially for bigger packets, ACKbased networks show a big latency increase. Erasure Code Evaluation. An analysis of the influence of the code block length is presented in Figure 12. Unlike in ACK-based APEC NoCs, the number of code flits per packet have a higher influence on the network. Already for medium sized packets of 17 flits (blue lines with square markers), using 2 code flits outperforms the single code flit code (solid and dashed lines). For bigger packets of 65 flits (green lines with triangle markers), both codes with block lengths of 8 (dashed line) and 32 (dotted line) flit saturate at higher generation rates than the single flit code (solid line), while the weaker of the two codes yields a lower latency. For big packets of 129 flits (purple lines with half-square markers), the same behavior can be observed. For these packets, both codes with 32 (dotted lines) and 64 (dash-dotted lines) flit blocks yield the best performance, while the single flit code (solid line) saturates early and the 8-flit-block code (dashed line) has an increased latency. Therefore, finding the right trade-off between code strength and latency is of importance for big packets using the NACK-based version of APEC. 5 CONCLUSION We presented APEC as a novel concept to solve the priority conflict between acknowledgments and body flits in bufferless NoCs using erasure codes on the packet level. Our new concept uses a lightweight heuristic erasure code and can be applied to both NoCs using ACK- and NACK-based retransmission protocols. Although ACK-based NoCs allow for a simple router design, they behave rather poorly in higher-load traffic scenarios. APEC however raises the performance of the NoC to a level that is feasible for a wide range of applications while keeping the simple router design. Employed in NACK-based NoCs, APEC allows for an efficient network implementation and avoids the use of deflection routing, NOCS ’19, October 17–18, 2019, New York, NY, USA Vonbun et al. A APPENDIX Algorithm 1 Heuristic Encoder with Blocksize B and n Body Flits function encode ( bodyflit b [n], blocksize B ) n ← len (b ) , c ← 0, o ← 0 for k = 0 : n − 1 do c ← c ⊕ b [k ] if k + 1 mod B = 0 then bout [k + o] ← b [k ] ▷ end-of-block ▷ init 5: o ← o + 1 bout [k + o] ← c 10: c ← 0 end if end for end function Algorithm 2 Heuristic Decoder with Blocksize B function decode ( bodyflit b [B ], bool dat a_val id [B ] ) B ← len (b ) , bl k _val id ← true, c ← 0, i ← −1 for k = 0 : B − 1 do c ← c ⊕ b [k ] else if i > −1 then if dat a_val id [k ] then 5: bl k _val id ← false ▷ init 10: break else i ← k 15: end if end if end for if bl k _val id & i > −1 then b [i ] ← c else b ← null end if 20: end function ▷ store erased flit position ▷ reconstruct ▷ decoding failure single flit NACK buffers or a circuit-switched NACK network as described in related work without a degradation in network performance. We have shown that the amount of redundancy added has almost no influence in ACK-based networks but becomes more important in NACK-based NoCs with long packets. As an extension to APEC, the code strength of the erasure code could be adaptive and encoded into the packet header, which could be used to secure high-priority packets in a mixed-critically traffic scenario. The concepts of APEC could also be applied to prioritize small high-priority packets in addition to control flits. However, the size of high-priority packets is likely to have a considerable impact on the network operation efficiency. Therefore, further investigations are necessary to determine both an adequate code strength to protect low-priority packets against multi-flit erasures and suitable sizes for both high- and low-priority packets. "
