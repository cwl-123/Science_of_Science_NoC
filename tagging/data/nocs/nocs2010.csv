title,abstract,full_text
Photonic Chip-Scale Interconnection Networks for Performance-Energy Optimized Computing.,"As chip multiprocessors (CMPs) scale to increasing numbers of cores and greater on-chip computational power, the gap between the available off-chip bandwidth and that which is required to appropriately feed the processors continues to widen under current memory access architectures. For many high-performance computing applications, the bandwidth available for both on- and off-chip communications can play a vital role in efficient execution due to the use of data-parallel or data-centric algorithms. Electronic interconnected systems are increasingly bound by their communications infrastructure and the associated power dissipation of high-bandwidth data movement. Recent advances in chip-scale silicon photonic technologies have created the potential for developing optical interconnection networks that can offer highly energy efficient communications and significantly improve computing performance-per-Watt. This talk will examine the design and performance of photonic networks-on-chip architectures that support both on-chip communication and off-chip memory access in an energy efficient manner.","Photonic Chip-Scale Interconnection Networks   for Performance-Energy Optimized Computing  Keren Bergman  Columbia University, USA  Abstract     As chip multiprocessors (CMPs) scale to increasing numbers of cores and greater on-chip  computational power, the gap between the available off-chip bandwidth and that which is required to  appropriately feed the processors continues to widen under current memory access architectures. For  many high-performance computing applications, the bandwidth available for both on- and off-chip  communications can play a vital role in efficient execution due to the use of data-parallel or data-centric  algorithms. Electronic  interconnected systems are  increasingly bound by  their communications  infrastructure and the associated power dissipation of high-bandwidth data movement. Recent advances  in chip-scale silicon photonic  technologies have created  the potential  for developing optical  interconnection networks that can offer highly energy efficient communications and significantly improve  computing performance-per-Watt. This talk will examine the design and performance of photonic  networks-on-chip architectures that support both on-chip communication and off-chip memory access in  an energy efficient manner.  Biography     Keren Bergman is a Professor of Electrical Engineering at Columbia University where she also directs  the Lightwave Research Laboratory (http://lightwave.ee.columbia.edu). She leads multiple research  programs on optical interconnection networks for advanced computing systems, data centers, optical  packet switched routers, and chip multiprocessor nanophotonic networks-on-chip. Dr. Bergman holds a  Ph.D. from M.I.T. and is a Fellow of the IEEE and of the OSA. She currently serves as the co-Editor-inChief of the IEEE/OSA Journal of Optical Communications and Networking.  4                 "
QuaLe - A Quantum-Leap Inspired Model for Non-stationary Analysis of NoC Traffic in Chip Multi-processors.,"This paper identifies non-stationary effects in grid like Network-on-Chip (NoC) traffic and proposes QuaLe, a novel statistical physics-inspired model, that can account for non-stationarity observed in packet arrival processes. Using a wide set of real application traces, we demonstrate the need for a multi-fractal approach and analyze various packet arrival properties accordingly. As a case study, we show the benefits of our multifractal approach in estimating the probability of missing deadlines in packet scheduling for chip multiprocessors (CMPs).","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip QuaLe: A Quantum-Leap Inspired Model for                                    Non-Stationary Analysis of NoC Traffic in Chip Multi-Processors Paul Bogdan, Miray Kas, Radu Marculescu and Onur Mutlu Department of Electrical and Computer Engineering Carnegie Mellon University Pittsburgh, PA 15213-3890, USA {pbogdan, mkas, radum, omutlu}@ece.cmu.edu fluctuations in the number of applications executing on the NoC at any given time. Based on the statistical features of the fitness distribution, we demonstrate that the NoC traffic can exhibit either a monofractal or multifractal behavior. Also, in contrast to traditional network designers that frequently use synthetic traffic patterns (e.g., uniform-random, immediate neighbor, tornado, or hotspot) for testing their designs, in this paper, we analyze network behavior using traces from a wide set of real applications and eliminate the inaccuracies observed in statistical network traffic models due to the use of artificial traffic patterns. In summary, our main contributions are as follows: Abstract This paper identifies non-stationary effects in grid like Network-on-Chip (NoC) traffic and proposes QuaLe, a novel statistical physics-inspired model, that can account for non-stationarity observed in packet arrival processes. Using a wide set of real application traces, we demonstrate the need for a multi-fractal approach and analyze various packet arrival properties accordingly. As a case study, we show the benefits of our multifractal approach in estimating the probability of missing deadlines in packet scheduling for chip multiprocessors (CMPs). Keywords: Chip Multi-Processors, Networks-on-Chip, Self-Similar Stochastic Processes, Multi-fractal Analysis. I. INTRODUCTION Traditionally, on-chip communication used a bus-based or point-to-point communication infrastructure. Given the lack of scalability in these approaches, Networks-on-Chip (NoC) emerged as a promising solution to on-chip communication [7]. General purpose CMPs with NoC-based communication are typically implemented in a tile-based structure where each tile consists of a processing element (PE), private/shared cache banks and a router [8]. On-chip networks resemble traditional data networks as the switches, routers and the packet-based communication constitute the basic elements of both types of networks. However, on-chip networks differ from general computer networks in many aspects, most notably in terms of optimizations needed to satisfy various performance, power, and area constraints. Nevertheless, the need for an in-depth understanding of the network traffic is unavoidably common to all networks as it is the key for optimized network design. Previous research includes several attempts to analyze and model the traffic behavior observed in different network types such as local area networks (LAN), wide area networks (WAN) [32] and the Internet (WWW) [6]. These papers focus on identifying self-similarity in network traces, leaving the issue of a more general multi-fractal model open. Since NoC design is a relatively new research area, the need for a multifractal traffic approach is yet unaddressed.  In this paper, we propose a statistical physics-inspired model which is able to capture the statistical characteristics of NoC traffic patterns accurately and explain the transition of the NoC traffic from mono to multifractal behavior. More precisely, in our approach, we consider each buffer in the NoC architecture as being characterized by a fitness distribution (with or without time dependency) based on whether or not the changes in the NoC traffic occur as a function of the intrinsic variability exhibited by the target applications or due to the 978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.34 241  • We propose QuaLe, a novel quantum-leap-inspired model for characterizing packet arrival processes which can capture the multifractal characteristics of NoC traffic in CMP platforms. Similar to Quale, defined as the universal property of an object which is independent from the object itself, our quantum-leap model introduces the fitness concept as a tool to encompass the universal multifractal behavior that is observed in NoC traffic due to various interactions among various traffic flows.  (cid:129) We illustrate the existence of non-stationary effects in NoC traffic and quantify the degree of multifractality when running various real world applications. We also estimate the probability of missing real-time deadlines and thereby discuss the implications of the multifractal approach in packet scheduling for CMPs. The rest of the paper is organized as follows: Section II provides a brief background on the theoretical concepts we use throughout  the paper. Section III reviews  the scientific approaches proposed for self-similar traffic. In Section IV, we present QuaLe, a novel statistical physics-inspired model which is able to account for the multifractal nature of NoC traffic. In Section V, we present our experimental methodology. In Section VI, we present an in-depth analysis of the main theoretical findings and in Section VII, we discuss their implications in CMP design, pointing out some future research directions. We conclude the paper by summarizing our main contributions in Section VIII. II. BACKGROUND ON SELF-SIMILARITY Self-similarity is a real-life concept that has been observed in many natural phenomena [33]. An object or data is called self-similar if it is still similar to the shape of the entire object/ data when we zoom in at different scales.  To better understand the self-similar (or fractal) nature of some objects, we show two examples in Figure 1: Figure 1.a shows that as we zoom in over a 3D Euclidean object, the final object starts to deviate significantly from the initial one. In contrast, Figure 1.b shows a 3D perspective of a mountainous area displaying self-similarity as we zoom in across its surface. a) b) Figure 1. a) An iterated zoom in process over a 3D sphere showing that Euclidean objects do not display self-similar characteristics. b) An iterated zoom in process over a mountainous area which displays a selfsimilar behavior in valley branching.  Apart from self-similar characteristics observed in various geometrical objects, self-similarity can be also conceived in a temporal manner. For instance, some data are considered to be self-similar in time, if the time series preserves its temporal properties with respect to scaling in time. Along the same lines, the self-similarity property of a stochastic process X(t) implies that its distribution over two nonoverlapping time intervals (i.e., X(t) and X(bt), for any real ) remains  the same up  to a scaling factor (i.e, , where   is called the Hurst parameter). In addition, a self-similar stochastic process is called long-range dependent (LRD), if its autocorrelation function decays as a power law   [21]. Moreover, if the H exponent varies in time, the stochastic process is called multifractal [20][35].  b 0! P X b t 0 H R! 0.5 H 1 k 2 H 2– P bHX t  x R k  a x = , b III. RELATED WORK AND NOVEL CONTRIBUTION The concept of self-similarity dates back to the early efforts of A. N. Kolmogorov to explain the chaotic nature of turbulence [17]. More precisely, in that paper, Kolmogorov proposes a mathematical formalism relating the self-similarity and small scale statistics of turbulent flows to the energy dissipation and universal scaling laws. Later on, several attempts were made to bridge more closely the theory with real measurements [13]; this includes the non-stationary aspects of turbulence via random cascade models [26][19][11][2][30]. We should also note that Mandelbrot constructed a mathematical formalism of roughness, and introduced the concept of fractal to denote the geometric scale-inference [20]. Over the years, self-similarity and multifractal formalisms have found application in many other fields such as diffusion-limited aggregation [36], dielectric breakdown [1], biological systems [37]. More recently, self-similarity and fractal approaches have been employed to study the structure of complex networks [9][25] or elucidate the departure of experimental measurements in various information networks from the standard Poisson assumption of packet arrival  time distribution [28][16][10]. Other experimental studies identify the existence 242 of self similar behavior in World Wide Web, local and wide area networks [6][32].  Informally, self-similarity in network traffic can be perceived as statistical similarity observed in bursty patterns of network traffic over a wide range of time-scales. This corresponds to scale-invariant burstiness [29][31][34] or mono-fractal behavior in network traffic, where the level of burstiness is typically captured via a single (Hurst) parameter. The most significant impact of self-similar behavior is the existence of long-range dependence (LRD), or long-range memory effects. In the NoC context, applications are mapped to the available network resources. This leads to interactions and contention at various network resources. Analyzing the actual characteristics of the network traffic is therefore a primary concern for optimization purposes. Indeed, one of the major challenges in constructing an accurate performance model for network analysis is the presence of non-stationary effects in traffic behavior. This is because operating the network close to (or at) criticality can only make the non-stationarity become more pronounced and the analysis more difficult. To date, there exist many experimental studies that demonstrate the existence of LRD, self-similarity, and even multifractality to some extent in various traffic traces [6][32]. However, proposing a model that is able to explain many features of the network traffic while being intrinsically related to the dynamics of NoCs remains an open problem.  Towards this end, we present a statistical physics model for NoC traffic characterization based on fitness distributions which is able to retrieve the mono-fractal and multi-fractal behaviors as particular cases. We test the validity of the theoretical conclusions by investigating the multifractal features of several application configurations running on a CMP platform where communication happens via the NoC approach. IV. QUALE: A QUANTUM LEAP INSPIRED TRAFFIC MODEL FOR  NOCS To investigate the temporal characteristics of NoC traffic, we adopt the quantum approach proposed in [4][5] and assume that, at any point in time, each buffer is characterized by a fitness function E. More precisely, we establish an analogy between thermodynamic systems and communication networks such that the number of packets stored in a buffer corresponds to the number of particles on a certain energy level. Consequently, the routing of packets (which naturally affect the number of arrivals at a certain buffer in the network) becomes similar to the migration of particles (i.e., quantum leap) among different energy levels. Within this quantum approach, the number of particles is determined by a fitness function E. Similarly, we assume that this fitness function determines the number of arrivals at a certain buffer. To study the temporal behavior of packet arrivals, we denote by ai(t) and   the cumulative number of arrivals at buffer i by time t and the probability that buffer i, characterized by fitness E received a packets by time t, respectively. As a first step in analyzing the multifractality of the NoC traffic, we investigate the statistical properties of the  distribution of packet arrivals at any buffer i (i.e.,  , where NB is the number of buffers in the network) by using a master equation as follows:  P i a t E = 1 NBy P i a t E i          t2 t2 Application 1 Application 1 v1 v1 t3 t3 g(y,t) g(y,t) time time g(y,t) g(y,t) y y L2  L2  cache cache PE PE L1 cache L1 cache Router Router Router v2 v2 v3 v3 v4 v4 (0,5) (0,5) (0,4) (0,4) (0,3) (0,3) (0,2) (0,2) (0,1) (0,1) (0,0) (0,0) (1,5) (1,5) (1,4) (1,4) (1,3) (1,3) (1,2) (1,2) (1,1) (1,1) (1,0) (1,0) (2,5) (2,5) (2,4) (2,4) (2,3) (2,3) (2,2) (2,2) (2,1) (2,1) (2,0) (2,0) (3,5) (3,5) (3,4) (3,4) (3,3) (3,3) (3,2) (3,2) (3,1) (3,1) (3,0) (3,0) b) y y Router Router Router L2  L2  cache cache PE PE L1 cache L1 cache u1 u1 Application 2 Application 2 u2 u2 u3 u3 t1 t1 Application 1 Application 1 v2 v2 v1 v1 v4 v4 v3 v3 (0,5) (0,5) (0,4) (0,4) (0,3) (0,3) (0,2) (0,2) (0,1) (0,1) (0,0) (0,0) (1,5) (1,5) (1,4) (1,4) (1,3) (1,3) (1,2) (1,2) (1,1) (1,1) (1,0) (1,0) (2,5) (2,5) (2,4) (2,4) (2,3) (2,3) (2,2) (2,2) (2,1) (2,1) (2,0) (2,0) (3,5) (3,5) (3,4) (3,4) (3,3) (3,3) (3,2) (3,2) (3,1) (3,1) (3,0) (3,0) a) u4 u4 Figure 2. Schematic representation showing: a) An application task graph mapped at time t1 onto a 4u6 NoC architecture and the distribution g(y,t) of scaling exponents associated with the input buffer of the PE located at (2,4). b) At time t2, a new application task graph is mapped onto the same architecture. Due to the changes in the overall traffic patterns and the increase in the communication load, the distribution g(y,t) associated with the input buffer of the local PE located at (2,4) can become very different compared to case a). > @ Based on the nonlinear exponents W(k), the multifractal spectrum can be expressed as follows: (1) w tP i a t E ---------------------------------w t + w ------ aP i y a t E ------------------------------ g y t w a A t  y ³  yd 0= P i a t E In Equation 1, A(t) represents the total number of packets in the network up to time t, y is the scaling exponent which depends on the fitness E as described in [5] and g(y, t) denotes the distribution of scaling exponents y in packet arrivals. Note that g(y, t) changes as a function of traffic patterns at time t. Equation 1 involves two main components. The first term states that the probability distribution   is proportional with time. Therefore, this term captures the long-term memory effects of the arrival process. The second term encompasses the statistical properties of the arrival process ai(t) at buffer i as a function of changes in the traffic patterns. For instance, as shown in Figure 2.a, one can assume that between t1 and t2 only Application 1 is running in the system and thus the distribution g(y, t) is skewed around a single or few fractal exponents. In contrast, when a new application enters into the system, depending on the network region where it is mapped to, the traffic pattern and the communication load may vary drastically. This situation has also a significant effect on the g(y, t) distribution associated with each buffer, causing it to become either more or less skewed around some particular values (see the change in the g(y, t) distribution of the input buffer at location (2,4) in Figure 2.b due to Application 2 entering the system). Simply speaking, the second term of Equation 1 states that the distribution of a new self-similar (i.e., re-scaled) stochastic process can be obtained from an initial distribution via a scaling relationship [21]. By substituting the k-th order moment of the number of arrivals at node i   into Equation 1, we obtain its time dependence as follows: a kP i a t E M k t  ³=  ad M k t  | t W k      W k  = ³ --- g y t k --------------- yd A y k 1+ 1– (2) A  where   denotes the maximum number of packets injected by time t and the nonlinear exponents W(k) represent a multifractal signature. It should be noted that when there are no changes in the network traffic pattern and the fitness distribution   associated with buffer i obeys the relation , Equation 2 characterizes a mono-fractal g i y t y 0 y 0– g i y t k G y | stochastic process. f D = m i n k D k > – W k  @ (3) where D represents the fractal dimension. Different from mono-fractal processes, the multifractal spectrum defined in Equation 3 states that a stochastic process can be characterized by multiple fractal dimensions Dand their normalized weights f(D).  Generally speaking, if the multifractal spectrum f(D) is large around a certain fractal dimension D, then there are many points characterized by this fractal dimension. In other words, the multifractal spectrum plays the role of a probability distribution function for the scaling exponents characterizing a stochastic process. Moreover, if the support of fractal dimensions Dis wide, then we can state that the stochastic process is characterized by many D exponents. As such, a multifractal spectrum is more likely to characterize the NoC traffic of CMP platforms as the complexity of the application and the distribution g(y, t) increases. V. EXPERIMENTAL METHODOLOGY The experimental results are obtained using an in-house cycle-accurate x86 NoC based CMP simulator. The front end of the simulator depends on Pin [18] and iDNA [3]. The highlevel view of the simulated architecture is presented in Figure 2. In this architecture, the NoC routers are virtual channel (VC) buffered 2D-mesh routers with 5 physical ports: one for each {North, South, East, West} direction and extra one for the local core the router is attached to.  The NoC traffic is dominated by the communication between PEs and shared L2 cache banks. Each PE has a private L1 cache. When an application cannot find the data in its private L1 cache, an address packet is created and sent to the L2 cache bank the requested data resides in. When the requested data becomes available in the L2 cache, a data packet is injected into the network as a reply to the received request. We model a static non-uniform cache architecture (SNUCA) where the L2 cache bank the cache line resides in is determined via the lower order bits in the address of the cache line. Therefore, depending on the requested data address, an 243          address packet might be destined to any of the shared cache banks in the network. In addition, we model our PEs to be selfthrottling, thus preventing a PE from injecting new packets into the network when its injection buffers are full. Table 1 lists the major system parameters. Table 1: Baseline processor, cache, memory and network configurations used in the experimental setup. Processing Element pipeline Fetch / Exec / Commit width L1 caches L2 caches 2 GHz Processing Element, 128-entry instruction window,  12-stage pipeline 3 instructions per cycle in each core; only 1 can be a memory operation Private, per-PE, 4-way set associative, 128B block size Shared 1MB bank per PE, 16-way set associative, 128B  block size, XOR based address-to-bank mapping Network router Buffered, wormhole switched, XY routing, Virtual channel (VC) flow control, 4 VCs per port, Round-Robin  packet scheduling, 4 flit buffer depth, 1 flit per Address  Packet, 4 or 8 flits per data packet 10x10 2D-mesh, each tile has a router, PE, private L1  cache, shared L2 cache bank Network topology  We model the PEs and the caches as in real systems.Thus, we do not have a direct control over the packet injection rate of real applications. L1 misses result in the creation of new address packets, which are then sent to the shared L2 caches to request data. Therefore, a smaller L1 cache size results in a higher number of misses and implicitly a higher packet injection rate, whereas a larger cache results in a lower injection rate. To discuss the impact of changing the injection rate over the distributions of inter-arrival times, we report in Section VI statistical information using different L1 cache sizes. Table 2: Classification of SPEC 2006 applications. Set Application Description SET-I SET-II SET-III 400.perlbench Perl scripting language 401.bzip2 File compressor 403.gcc C Language optimizing compiler 445.gobmk Go playing program 450.soplex Simplex Linear Program solver 454.calculix 3D Finite Element code  482.sphinx Speech recognition 429.mcf Single-depot vehicle scheduler 462.libquantum Quantum computer simulation 464.h264ref Video compression program 433.milc Quantum Chromodynamics 437.leslie3d Computational fluid dynamics 447.dealII Adaptive finite elements  456.hmmer A gene sequence database search 458.sjeng Chess & variants playing games 471.omnetpp Ethernet network simulator 473.astar 2D path finding library 435.gromacs Simulator for Lysosome protein 444.namd Biomolecular systems simulator Inj. Rate  (Packets/ Cycle) 0.001147 0.039072 0.052649 0.025125 0.077675 0.005126 0.049308 0.196724 0.096904 0.012494 0.047728 0.096399 0.007556 0.044859 0.015726 0.043674 0.016980 0.015665 0.065465 For our simulation experiments, we use a subset of SPEC 2006 applications [15]. Each benchmark is compiled using gcc 4.1.2 with -O3 optimizations and a representative execution 244 phase is chosen using PinPoints [27]. Our experiments involve two different scenarios simulated on a 10x10 mesh NoC:   (cid:129) Single-application scenarios: In this set of simulations, out of 100 PEs, only one PE executes an application. Depending on the requested memory address, address (request) packets may hit on any L2 bank in the system. We also vary the data packet size (i.e., 1 flit, 4 flits and 8 flits) and L1 cache size (i.e., 8K, 16K, 32K, 64K).  (cid:129) Dynamic multiple-application scenarios: In this set of simulations, all 100 PEs are utilized. We randomly divide the applications into three groups (see Table 2). The applications in the first group are executed during the entire run. The applications in the second group are dynamically replaced with the applications in the third group. We perform experiments with dynamic multiple-applications scenario on a 10u10 mesh NoC using two different L1 cache sizes (i.e., 64K and 8K), and two different routing algorithms (XY wormhole routing and deflection wormhole routing [23]) with 8-flit long data packets. VI. EXPERIMENTAL RESULTS AND ANALYSIS E f  0 E 2 @ > f 1 f 2 f E–a A. Impact of Workload on Traffic Statistics One way to elucidate the presence of temporal scaling in the NoC traffic involves computing the power spectrum of the inter-arrival times of data packets because the power spectrum of a time series measures the magnitude of variability and the degree of scaling as a function of frequency. To be more precise, a time series exhibits a scaling behavior over a certain frequency interval   if the power spectrum obeys a power law relation  , where   is the scaling exponent. In other words, the slopes observed in the power spectrum show that the fluctuations in variance are scale-invariant to certain frequency bands. Moreover, stochastic processes exhibiting a nonlinear power spectrum are called non-stationary [21][37]. Apart from mathematical intricacies, the power spectrum of inter-arrival times can show if the NoC traffic is nonstationary, i.e., whether or not the properties of the applications traffic patterns change as time progresses. Figure 3.a, Figure 3.b, and Figure 3.c show the power spectrum of benchmark 400.perlbench (on the log-log scale) while running on a 10u10 mesh NoC with various L1 cache sizes (e.g., 8K, 16K, 32K, 64K) and for several data packet sizes (1 flit (a), 4 flits (b) and 8 flits (c)). Figure 3.a shows that by reducing the L1 cache size from 64K to 8K, the non-stationary effects become more pronounced. For instance, for 1-flit long data packets, the slope of the power spectrum in Figure 3.a increases from 0.09257 to 1.07. This can be regarded as a nonstationarity signature since a stationary stochastic process would have a slope close to zero (i.e., it would look like an horizontal line). Similar trends can be observed in Figure 3.b and Figure 3.c for data packet sizes of 4 and 8 flits where the power spectrum slopes change in the range of [0.084, 1.072] and [0.09232, 1.075], respectively. This shows that the impact of packet size on the network traffic statistics is minimal, especially when compared against the influence of packet injection rate. The impact of the increased packet injection rate over nonstationarity is also confirmed by power spectrum of data from several other applications: 403.gcc (Figure 3.d), 444.namd   a) d) b) e) c) f) Figure 3. Power spectrum of inter-arrival times of data packets for different applications running on a 10u10 NoC with various L1 cache sizes: 400.perlbench for three different packet sizes: 1 flit (a), 4 flits (b) and 8 flits (c). Power spectrum of the inter-arrival times of data packets for three applications: 403.gcc (d), 444.namd (e) and482.sphinx3 (f) running on a 10u10 NoC with various L1 cache sizes and 4-flit long data packets. (Figure 3.e), and 482.sphinx3 (Figure 3.f). In contrast to previously discussed applications, these plots show a more complex behavior which require nonlinear fits (see the dashed lines) and display multiple scale breaks. This kind of more complex nonlinear behavior observed in the power spectra of these applications shows not only the existence of non-stationarity, but also a more complex behavior than a monofractal one. Considering the data presented in Figure 3, it can be stated that the power spectrum exhibits a wide range of scaling in time. These observations should not only raise awareness to non-stationarity effects, but also suggest that fractional Brownian motion [21] and other similar approaches are not necessarily adequate for modeling real NoC traces as they rely on a single scaling exponent characterized by a single fractal dimension. Instead, the richness of scaling displayed by various traces supports the existence of multiple fractal dimensions. We discuss this issue later in the paper. B. Multiscale Analysis of NoC Traffic Rather than complicating the traffic characterization problem, the multifractal approach basically reduces the statistics of a long times series to a distribution of scaling exponents (i.e., Equation 3) which encompasses the non-stationary aspects as well [14][35]. Therefore, we investigate next the presence of multifractality in NoC traffic. Single application scenarios: Figure 4 reports the multifractal spectrum (see Equation 2) of the inter-arrival times of 245 the data packets for three applications from single application scenarios (400.perlbench (a), 403.gcc(b), and 401.bzip2 (c)), running on a 10u10 mesh NoC with various L1 cache sizes (i.e., 8K, 16K, 32K, 64K). For all these applications and data packet sizes, the support of the multifractal spectrum shrinks with the increasing packet injection rate. However, this does not imply, the existence of a monofractal behavior because for a mono-fractal process, the spectrum would appear as a delta function (i.e., as a very narrow spike) centered around a certain value on the x-axis. In other words, for a mono-fractal stochastic process, if the time series of inter-arrival times between data packets were segmented into disjoint sets, each newly created time segment would be characterized by the same fractal dimension. On the other hand, for a multi-fractal stochastic process, each of the newly created time segment can have its own fractal dimension based on the particular characteristics of the time dependent generated network traffic. From this discussion, it can be concluded that the network evolves toward a congested state as the multifractal spectrum shrinks. We should also note that the 403.gcc and 437.leslie3d applications exhibit opposite behaviors with increasing packet injection rate, especially in terms of persistence tendency. The persistence tendency means that the stochastic process is characterized by some kind of periodicity reflected via higher correlation moments; this implies that the process exhibits higher a) d) b) c) e) f) Figure 4. Multifractal spectrum of the inter-arrival times of the data packets for three applications running on a 10u10 NoC with varying L1 cache sizes: 400.perlbench with 1 flit (a), 403.gcc with 4 flit (b), 401.bzip2 with 4 flit (c) data packets. The broad range of fractal dimensions exhibited in these graphs confirm the existence of multifractality in NoC traffic. Multifractal spectrum of inter-arrival times of data packets for six applications from dynamic multiple-applications scenarios, running on a 10u10 mesh NoC with various L1 cache sizes (i.e., 8K, 64K) and 8 flits data packets: 401.bzip2 (d) mapped on the PE located at (0,0) and the 433.milc mapped on PE located at (9,9), 401.bzip2 (e) mapped on the PE located at (0,0) and the 437.leslie3d mapped on PE located at (9,9) in Figure 4.e, and 450.soplex mapped on the PE located at (0,0) and the 464.h264ref (f) mapped on PE located at (9,9) in Figure 4.f. order memory effects [22]. On the other hand, the anti-persistent tendency shows that the stochastic process deviates from time periodicity and has memory effects of lesser degree. While the multifractal spectrum for 403.gcc shrinks towards the anti-persistent region (lower support of fractal dimensions - left), the multifractal spectrum of 401.bzip2 application shrinks towards the persistent region (higher support of fractal dimensions - right). Nevertheless, both graphs display a broad range of fractal dimensions concentrated around 1 which confirms the existence of multiscale, as well as a high degree of memory. Dynamic multiple-application scenarios: To investigate the impact of running multiple applications on the NoC traffic characteristics, we report the multifractal spectrum of the interarrival times of the data packets for six applications from dynamic multiple-applications scenarios, running on a 10u10 mesh NoC with two L1 cache sizes (i.e., 8K, 64K) and 8-flit data packets:  (cid:129) 401.bzip2 runs on the PE at (0,0) and 433.milc runs on the PE at (9,9) in Figure 4.d;  (cid:129) 401.bzip2 runs on the PE at (0,0) and 437.leslie3d runs on the PE at (9,9) in Figure 4.e;  (cid:129) 450.soplex runs on the PE at (0,0) and 464.h264ref runs on the PE at (9,9) in Figure 4.f. All these plots exhibit a wide range of fractal dimensions which correspond to a highly nonlinear exponent W(k) in Equation 2. It should also be noted that, with the increased communication load, the multifractal spectrum shrinks around 1 which corresponds to a high degree of memory effects. We also notice that the multifractal spectrum for 401.bzip2 and 464.h264ref exhibit a short discontinuity which can be attributed to the artifacts of fitting simulation data. However, this does not affect the conclusion about the existence of multifractal behavior. Also, the asymmetry displayed by all these multifractal spectra can be interpreted as the heterogeneity observed in both single and multiple application workloads traffic patterns. As discussed in the next section, the existence of multifractality in NoC traffic has direct implications in the design and optimization of NoC architectures.  VII. CAUSES AND IMPLICATIONS OF MULTI-FRACTAL BEHAVIOR CASE STUDY ON ESTIMATING DEADLINES In this section, we discuss possible causes of multifractality of the data inter-arrival times. We especially consider the compute versus stall times of applications. Finally, we briefly discuss a few implications of multi-fractality in CMP scheduling.  Compute vs. Stall times, Request-Reply Latencies, and MLP: Generally speaking, during its execution an application alternates between useful compute periods (i.e. periods when 246 a) b) c) Figure 5. a) The probability of the latency encountered by a data request to exceed a certain threshold for the PE located at (2,4) when running 450.soplex on a 10u10 NoC with 64K and 8K L1 cache sizes , XY wormhole routing and 8 flits per packet. b) The plot for the latencies encountered for each data requested by node (2,4) from other nodes on the 10u10 NoC. c) The probability of the latency encountered by a data request to exceed a certain threshold for the PE at (4,4), running 433.milc on a 10u10 NoC with two L1 cache sizes (64K and 8K), wormhole deflection routing and 8 flits per packet. there is forward progress on the application execution) and useless stall periods [24]. While a PE is in the stall state, it makes no progress on the application execution.  The stall time experienced by a PE is tightly coupled with the experienced request-reply latencies. We define the requestreply latency (RRL) as the time elapsed between the creation of an address request (address packet) and the receiving of its associated reply (data packet).  If we consider the use of strict in-order PEs that cannot overlap the latency of multiple packets, then the stall time experienced by a PE is dependent only on request-reply latencies, and can be defined as the sum of request-reply latencies of all packets. However, in our experimental setup, we evaluate state-of-the-art out-of-order execution PEs that can overlap the latency of multiple packets. Therefore, the stall time experienced by the core is not a simple sum of all packet latencies. The degree of memory level parallelism an application has is one of the most significant reason why the experienced stall time deviates from the sum of the request-reply latencies. Memory Level Parallelism (MLP) is defined as issuing and servicing of multiple requests in parallel [12]. During the execution of an application by an out-of-order PE, an application might have multiple outstanding requests. The latencies for some of the requests will overlap with the latencies of older requests. The degree of overlap between request latencies relates to the application MLP; higher MLP applications have more overlap among their requests. Implications: With increasing amount of traffic in the NoC, request-reply latencies start to increase. As a result, stall periods start dominating the compute periods, leaving the PE idle for most of its execution time and thus reducing its performance. In our experiments with multiple-application scenarios, we observe that the stall times dominate the processing times for several applications1. This is especially true for applications with high injection rates (e.g., 450.soplex - See Table 2).  Case Study: In Figure 5.a, we report the probability of request-reply latencies exceeding a certain threshold for 450.soplex. The presented data is collected in a dynamic multi1.If a core spends more than 60% of its execution in stall state, we say that its stall times dominate its processing times. 247 > –a e x / c @b P R R L x! application scenario where 450.soplex runs on the PE located at (2,4) of a 10u10 mesh NoC. The routing algorithm is XY routing and the simulation runs for 10M clock cycles. We should note that the PE located at (2,4) running 450.soplex spends 82.7% of the execution time in stall state. The dotted line represents the standard Markovian behavior which corresponds to an exponential probability of the request-reply latencies to exceed a given threshold. More precisely, the probability of request reply latency to exceed a certain threshold is given by:  , where b and c are the shape and scale parameters, respectively. As it can be observed from Figure 5.a, the Markovian curve underestimates the probability of exceeding a certain latency (i.e., the probability of missing a deadline). For instance, according to the Markovian curve the probability of waiting for a certain data packet approximately 600 clock cycles is 0.0001, while it becomes 0.007 for the multifractal curves. Thus, Markovian curves are quite optimistic in their estimations. This implies that when the multifractal effects are ignored, the likelihood of missing a deadline and failing to predict the actual request-reply latency is at least an order of magnitude higher. For completeness purposes, in Figure 5.b we report the magnitude and burstiness of request-reply latencies encountered at node (2,4) over the entire simulation. In Figure 5.c we present the probability of exceeding a certain threshold in request-reply latency for another application, 433.milc. The presented data is again collected in a dynamic multi-application scenario, at the node located at (4,4) of a 10u10 mesh NoC using two different L1 cache sizes (i.e., 64K and 8K). In this scenario, the PE located at (4,4) running 433.milc is stalled 60% of the time. The Markovian curve again underestimates the probability of exceeding a certain threshold in data latency. However, the difference between the estimations of multifractal curves and the Markovian curve is higher, especially when the packet injection rate is higher. For instance, the Markovian case predicts a probability of 0.00006 to exceed a threshold of 530 clock cycles, while the multifractal approach predicts a probability of 0.0045.  Effect of the Routing Algorithm: Different from the results presented in Figure 5.a, in the experiment presented in Figure 5.c, wormhole deflection is used as the routing algo  rithm. Comparing the results presented in Figure 5.a and Figure 5.c, it can be observed that the two multifractal probability curves corresponding to 64K and 8K L1 cache sizes are almost the same in Figure 5.a while they drastically deviate from one another in Figure 5.c. This is mostly due to the change in the routing algorithm. Therefore, it can be stated that the fixed XY routing does not introduce secondary effects as the deflection algorithm does. In addition, the higher degree of freedom in packet routing introduces i) higher latencies as L1 cache size decreases and ii) a more pronounced nonlinear behavior in the distribution of stall times. Using the QuaLe model in Section IV, it can be shown that the distribution of stall times has multifractal features that are captured via the fitness distribution.  Future Work: There are two major directions for future research. First, understanding the reasons for multi-fractality in NoCs is important. We hypothesize that the discrepancy between the Markovian curves and multi-fractal curves are due to effects like memory-level parallelism and phase behavior in the memory intensity of applications. Further research is needed to pinpoint the causes of multi-fractality. Second, developing better NoC policies based on the understanding of multi-fractality can prove fruitful. In our experiments, we have used application/distribution oblivious routing and packet arbitration policies (XY and deflection routing as routing algorithms and round-robin for packet arbitration). However, as discussed in this section, stall times, which are a function of the application network intensity and memory level parallelism, tend to dominate compute times. Therefore, new research aimed at designing application/stall time aware routing and scheduling policies, that can account for the multifractal features of various applications and thus prioritize the applications in the network accordingly, can be very promising. VIII. CONCLUSION This paper provides evidence that in NoC-based CMP systems with a large number of components, the NoC traffic needs to be characterized using a multifractal approach rather than standard Markovian approach. Using a new theoretical model, we have investigated the effect of packet injection rate and the data packet sizes on the multifractal spectrum of NoC traffic. For several applications, we have shown how the existence of multifractality can be identified and used in estimating the probability of missing a deadline for applications with packet deadline requirements. We have further shown that the stall times experienced by the applications start dominating the compute times, especially in loaded traffic scenarios, reducing the utilization of cores drastically. Therefore, our future work will focus on developing application/distribution aware routing and scheduling policies for CMP platforms based on multifractal features of the NoC traffic model proposed in this paper. Acknowledgements: The authors (P.B.,R.M.) acknowledge support from Global Research Collaboration (GRC) and National Science Foundation (NSF) via grants 2008-HJ-1823, CCF-0702420, and CCF-0916752, respectively. The authors (M.K.,O.M.) acknowledge the support from CMU SEI, CyLab, and equipment donated by Intel Corporation.  IX. "
Traffic- and Thermal-Aware Run-Time Thermal Management Scheme for 3D NoC Systems.,"Three-dimensional network-on-chip (3D NoC), the combination of NoC and die-stacking 3D IC technology, is motivated to achieve lower latency, lower power consumption, and higher network bandwidth. However, the length of heat conduction path and power density per unit area increase as more dies stack vertically. Routers of NoC have comparable thermal impact as processors and contributes significant to overall chip temperature. High temperature increases the vulnerability of the system in performance, power, reliability, and cost. To ensure both thermal safety and less performance impact from temperature regulation, we propose a traffic- and thermal-aware run-time thermal management (RTM) scheme. The scheme is composed of a proactive downward routing and a reactive vertical throttling. Based on a validated traffic-thermal mutual-coupling co-simulator, our experiments show the proposed scheme is effective. The proposed RTM can be combined with thermal-aware mapping techniques to have potential for higher run-time thermal safety.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip Traffic- and Thermal-Aware Run-Time Thermal  Management Scheme for 3D NoC Systems   Chih-Hao Chao, Kai-Yuan Jheng, Hao-Yu Wang, Jia-Cheng Wu, and An-Yeu Wu   Graduate Institute of Electronics Engineering, National Taiwan University   Taipei 10617, Taiwan ROC   Abstract—Three-dimensional network-on-chip (3D NoC), the  combination of NoC and die-stacking 3D IC technology, is  motivated to achieve lower latency, lower power consumption,  and higher network bandwidth. However, the length of heat  conduction path and power density per unit area increase as  more dies stack vertically. Routers of NoC have comparable  thermal impact as processors and contributes significant to  overall chip temperature. High temperature  increases the  vulnerability of the system in performance, power, reliability,  and cost. To ensure both thermal safety and less performance  impact from temperature regulation, we propose a traffic- and  thermal-aware run-time thermal management (RTM) scheme.  The scheme is composed of a proactive downward routing and a  reactive vertical throttling. Based on a validated traffic-thermal  mutual-coupling co-simulator, our experiments show  the  proposed scheme is effective. The proposed RTM can be  combined with thermal-aware mapping techniques to have  potential for higher run-time thermal safety.  Keywords - traffic-aware; thermal-aware; run-time thermal  management; routing; throttling; 3D NoC; 3D IC  I.   INTRODUCTION  As the complexity of the System-on-Chip (SoC) grows, on-chip  interconnections gradually dominate  the system performance.  Network-on-Chip (NoC) has been proposed as a novel, practical and  efficient communication infrastructure [1]. Recently, die-stacking  three-dimensional (3D) IC technology is emerging for its capability to  reduce wire delays by connecting with shorter vertical connections -  Through Silicon Via (TSV) [2][3]. The combination of NoC and TSV,  3D NoC, is motivated to achieve lower transmission latency, lower  network power consumption, higher device density, and higher  platform bandwidth [4][5].  Thermal issues are significant challenges for developing 3D IC  and also 3D NoC. As more dies stacked vertically, power density (in  W/m2) increases, and the length of heat conduction path increases.  High temperature results in longer propagation delay and increases the  leakage power. A chip operating above its thermal limit may generate  incorrect output data and suffer from reliability degradation. Besides,  heat also makes the cooling and packing cost increase. To solve the  heat problem, various thermal management schemes have been  proposed for 2D and 3D ICs, especially for high performance designs  like chip multi-processor (CMP) [6] and NoC [7].   Keeping high performance under a certain thermal limit is the  major goal of most thermal management schemes and thermal-aware  design techniques. By characterizing thermal profile of the MIT Raw  chip, [7] shows that NoC has comparable thermal impact as processors  and contributes significant to overall chip temperature. Due to the high  switching activity and the relative small area, [8] shows the average  power density of a NoC router is even higher than the floating-point  MAC and memo r y on In t e l ’ s 80 - co r e p ro ce sso r . Th e th e rma l  management scheme proposed in [7] , ThermalHerd, regulates the  This work is supported in part by the National Science Council, Taiwan, ROC under Grant NSC98-2220-E-002-034.  978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.32 223 Figure 1.  The proposed traffic- and thermal-aware run-time thermal  management (RTM) for 3D NoC  network temperature and eliminates thermal emergencies of 2D NoC.  With the distributed temperature-aware traffic throttling and the  proactive/reactive thermal-correlation based routing, the performance  degradation from regulation can be controlled. However, the vertical  stacking makes the thermal design more difficult, and the traditional  2D techniques for NoC have their limitation. For the 3D case, herding  the majority of the switching activities close to the heat sink is a  specialized technique. For the 3D-integrated microprocessors, [9]  proposes a micro-architecture design technique, Thermal Herding, to  control hotspots. By partitioning the processor into multiple layers,  Thermal Herding steers the majority of switching activity to the die  that is closest to the heat sink. For 3D CMP that place cores on  multiple layers, [6] identifies the critical concept for optimal thermal  management, and derives guidelines for their near-optimal policy.  To ensure the thermal safety and little performance degradation  for 3D NoC, we propose a new run-time thermal management (RTM)  scheme in this paper, as Fig. 1. We assume temperature distribution is  available through the distributed thermal sensors in 3D NoC, and the  scheme is composed of two techniques:   •  Traffic-aware downward routing: an adaptive proactive  technique to prevent heat accumulating inside 3D NoC and  maximize achievable performance under network constraints.  With monitoring the network status to prevent congestion,  the workloads are migrated toward heat sink adaptively.   Thermal-aware vertical throttling: an adaptive reactive  technique  to decrease  temperature  in emergency. We  improve the traditional distributed traffic throttling with  consideration of 3D characteristics, and provide a thermalaware adaptation for network availability.   We develop and validate a traffic-thermal mutual-coupling cosimulation platform [15] for 3D NoC. The experimental result shows  the proposed RTM scheme has better temperature controllability and  less performance degradation on regulation. For temperature-limiting  cases, the achievable throughput under the 80 ℃ thermal limit is  improved around 7%. The average throttling time is reduced around  70%, and the average throttling ratio is reduced around 9-15%.  •      The rest of paper is organized as the following. In Section II, we  describe the problem and goal of RTM for 3D NoC. In Section III, the  proposed traffic-aware and thermal-aware RTM scheme is described.  In Section IV, the experiments are shown and discussed. The related  work is introduced in Section V, and this paper is concluded in  Section VI.  II. PROBLEM DEFINITION  A. Motivation  The major goal of the RTM for 3D NoC is jointly optimizing  performance and temperature. The problem is complex due to the goal  and the optimization constraint change as temperature changes. If  temperature is lower than a given temperature limitation (thermal  limit), the goal is to maximize performance (achievable throughput). If  temperature is higher than thermal limit, the goal is to decrease the  temperature of overheat routers with minimum performance impact.   The temperature distribution of a 3D NoC is correlated to power  distribution, which depends on both application mapping and packet  routing. The mapping determines the computation power, and the  routing determines the communication power. For computationintensive tasks, such as processors with iterative arithmetic operations,  the power may be dominant by the local processor and memory. For  communication-intensive tasks, such as a router that neighbor routers  transmit many packets to it, the power consumption may be dominant  by the traffic-hotspot router.  We assume that mapping is fixed for  RTM of NoC, and we focus on routing-based approach.   B. Problem Description  For 3D NoC, balanced traffic distribution does not result in  balanced thermal distribution. The optimization is simultaneously  constrained by network bandwidth and thermal limitation due to the  mutual coupling effects. The major difference between 2D NoC and  3D NoC is the enlarged difference of thermal characteristic among  routers. Especially the vertical aligned routers. Assume the same  ambient temperature, fixed-sized heat sink, and constant air flow  velocity i.e. the same cooling environment for cooling for simplicity.  According to Fourier’s law, the router which is on the layer closer to  the heat sink has higher cooling efficiency. Inversely, the router that is  farther to heat sink is has lower cooling efficiency and is thermal  dominant.  The thermal coupling of vertical aligned tiles is much  larger than the coupling of horizontal aligned tiles [6]. As Fig. 2, we  from Fourier’s law. (cid:1986)(cid:1843) (cid:1986)(cid:1872)⁄ is the amount of heat transfer per unit time  simplify the modeling of the heat flow from a 3D temperature profile  in Watt. (cid:1863) is the heat conductivity of the material in W·m−1·K−1, and  (cid:1827)(cid:3030)(cid:3045)(cid:3042)(cid:3046)(cid:3046) is the cross surface area. (cid:1986)(cid:1846) is the temperature difference  to a vertical 1D profile for the discussion of the following two cases.  between the ends, and (cid:1986)(cid:1876) is the distance between the ends. Equation  Equation (1) shows the heat flow formula of 1D geometry derived  (1) states that the rate of heat flow through a homogeneous solid is  directly proportional to the area of the cross section of the direction of  heat flow, the temperature difference of two terminals, and the  conductivity.   (cid:1986)(cid:1843)(cid:1986)(cid:1872) (cid:3404) (cid:3398)(cid:1863)(cid:1827)(cid:3030)(cid:3045)(cid:3042)(cid:3046)(cid:3046) (cid:1986)(cid:1846)(cid:1986)(cid:1876)  (1)  We use the two following extreme cases to show the first two  problems of joint optimization of performance and temperature. The  third problem is caused by the non-ideality of proactive workload  migration. For simplicity, we denote bottom layer (lowest) as the layer  closest to the heat sink, and top layer (highest) as the farthest. Besides,  we assume distributed constant power trace for the following  discussion.  224 Figure 2.  Thermal coupling and problem simplification  1) Temperature-limiting performance optimization  As mentioned above, maximize throughput needs to balance  traffic to prevent channel loading above the bisection bandwidth. E.g.,  for uniformly distributed traffic on mesh or torus network, dimensionordered routing such as XYZ routing has best performance, and is  vertically balanced. The power consumption of each router is  correlated to the traffic loading. For each layer, the injected heat from  power equals to the conducted heat through the interface between  layers. For the case that heat sink is only at one terminal of the 1D  geometry, the temperature distribution forms a ladder. The heat flow is  toward the terminal where heat sink is, i.e. heat flows gather from top  to bottom and the difference of the temperature on each interface is  getting larger. In this case, the top layer has highest temperature. If the  temperature is low and no router is overheated, this scheme achieves  best performance. When the power density is high, it is prone to  overheat. This case is shown by Fig. 3(a).  2) Bandwidth-limiting thermal optimization  Assume we want to maximize heat conduction for a constant  offered traffic. From (1) and Fig. 2, to maximize the heat transfer from  bottom layer to heat sink, we assume the temperature of bottom layer  is the highest. This assumption makes no heat conduction from top to  bottom; only from bottom to top and sink. Since there is no other heat  sink for inside layers, the steady state temperatures of all layers will be  equal. Heat flow only exists between the interface of bottom layer and  heat sink. Any workload migration from bottom layer to other layers  makes heat generated in bottom layer decreases, and also decreases  the heat flow through the interface between bottom layer and heat sink.  In this case, all traffic is on the layer closest to heat sink. If the bottom  layer is not saturated, this scheme has best heat conduction. Otherwise  the network will suffer from congestion, which is shown by Fig. 3(b).  3) Non-ideality and assumption relaxation  The above two cases show the results of optimization for the  steady state in the simplified ideal cases. For the transient state,  several issues have to be considered. When the temperature is lower  than the thermal limit, the optimization goal is to maximize  performance with thermal consideration. Ideally this goal can be  achieved by controlling the proactive work load migration. However,  there are several assumptions too strong and have to be relaxed.   •  The heat conduction between routers within a layer is not  zero. Therefore the flow is not as simple as the 1D case. The  router in higher layer may have lower temperature than the  vertically aligned lower router in the transient state.  The power trace over time is not constant and is not zero  even without traffic. This relaxation makes the distribution of  the temperature varies all the time.  The granularity of control is not infinitely small: for  implementation consideration, both the granularity of the  amount of traffic migrated toward heat sink and the adjusting  step of throttling ratio are quantized into several levels.  •  •        Figure 5.  Power profile of routing paths in 3D NoC:  (a) ZXY routing; (b) XYZ routing  A. Traffic-Aware Downward Routing  1) Downward power migration  The two goals of the proposed traffic-aware downward routing are  proactively migrating the power distribution from top to bottom and  adaptively adjusting the amount of migration to prevent network  saturation. Power distribution of a NoC is correlated to traffic  prevent network congestion, the migration has to consider network  distribution, and need to be characterized for workload migration. To  status. We present the proposed technique in the following subsections.  The power (cid:1842) is composed of five parts, which is described by (2). (cid:1842)(cid:3046)(cid:3045)(cid:3030)  Since the mapping of task is predefined for RTM, the computation  including the buffering power inside source router. (cid:1842)(cid:3027) (cid:1861) s the power  power cannot be migrated, and only the communication power can.  Consider the power profile of the two routing paths shown in Fig. 5.  consumed on the path of z direction, and similarly (cid:1842)(cid:3025) and (cid:1842)(cid:3026) are the  power consumed on the x and y directions. (cid:1842)(cid:3031)(cid:3046)(cid:3047) is the power  is the sum of the power consumed from traffic source to source router,  change the distribution of vertical routing power (cid:1842)(cid:3027) and horizontal  routing power (cid:1842)(cid:3025) and (cid:1842)(cid:3026) . For minimal path routing such as changing  consumed from destination router to traffic sink. Routing can only  from XYZ to ZXY, the power migrates, and the overhead of migration  concentrated on bottom layer for maximizing heat conduction.  is negligible. As we shown in Section II.B.2, the power should be  Minimal path routing cannot migrate power to bottom layer if none of  source and destination routers are on the bottom layer.  (cid:1842) (cid:3404) (cid:1842)(cid:3046)(cid:3045)(cid:3030) (cid:3397) (cid:1842)(cid:3027) (cid:3397) (cid:1842)(cid:3025) (cid:3397) (cid:1842)(cid:3026) (cid:3397) (cid:1842)(cid:3031)(cid:3046)(cid:3047)  (2) 2) Downward routing  Downward routing is a non-minimal path routing that changes the  lying layer of horizontal routing path toward heat sink. In each pillar,  the number of downward level is determined according to the network  status. Given a 3D NoC with N layers, the maximum downward level  is N-1. Downward routing can adopt arbitrary traditional 2D routing  algorithm for horizontal routing on XY plane.   Fig. 6 shows a four-layer example of downward routing where  denotes downward by K layers. The horizontal routing algorithm is  XY-routing. When downward level is set to zero (DW_level=0), the  routing behavior is identical to XYZ routing. Fig. 6(b) shows the  routing paths when downward level is set to one (DW_level=1).  Because the destination routers are exactly one level below the source  router, the routing behavior is identical to ZXY in this example. Fig.  6(c) and Fig. 6(d) show that the routing paths when downward level is  set to two (DW_level=2) and three (DW_level=3). The routing  behavior shows the non-minimal path property, and it is in the order of  vertical-horizontal-vertical. If the destination is at the layer exactly  lower by the number of downward level, the routing is reduced to  vertical-horizontal e.g. ZXY routing. Fig. 6(c) and Fig. 6(d) also show  the example that the number of assigned downward level is larger than  the level distance between source and bottom. The horizontal routing  path is lying on the bottom layer.  Figure 3.  Two extreme optimization cases in the discussion of Section II.B.1)  and Section II.B.2). For transient case, a framework and a policy are required.  Figure 4.  Temperature regulation in emergency  Therefore, the temperature varies over time. If the accumulated heat  results in hotspot, overheat may occur. Hence a temperature  monitoring infrastructure and a reactive mechanism are required for  detection and cooling in emergency, which is shown in Fig.4. We  define the thermal limit is a temperature should not be touched, and a  threshold 1℃ below the limit is used for triggering the mechanism.  C. Design goal  [10] shows that the change of temperature is much  slower and smaller than the change of power. Therefore the required  frequency of sensing the temperature and redistribute the information  over the network is relatively small and negligible. With this  assumption, the design goal of our RTM is as follows:   Given thermal limit, traffic distribution, network topology, router  architecture, power model, and thermal model.  Find a framework and a policy for RTM.  Such that the achievable throughput is optimized with the constraint  that the temperature never goes above the thermal limit, and the  network has maximal availability.   III. RUN-T IME THERMAL MANAGEMENT FOR 3D NOC  For RTM of 3D NoC, we start from the framework developed for  2D NoC [7], and extend it to the third dimension. Because the third  dimension makes thermal management more difficult, the concepts  and policies for thermal optimization of 3D CMP [6][9] are referred.  Besides, for performance optimization of 3D NoC, benefits from the  advantage of vertical links is necessary. Therefore we assume the  crossbar-based network architecture is adopted [13][14]. Because the  optimization goal varies on different temperature, an adaptive solution  is preferred rather than a deterministic solution for achievable  maximum performance and controlling the temperature under thermal  limit.  225           Figure 8.  Traffic-aware level selection. All the routers inside the pillar adopt  the same downward level DW_level. To prevent network congestion, the  aggregated traffic load should  be smaller than a given amount. cnti is the  counter used for traffic load estimation and prediction.   B. Thermal-Aware Vertical Throttling  The goal of throttling is to effectively regulate the network  temperature with minimal performance effects. The idea of throttling  is creating a low power density region, and the heat generation rate of  the region decreases. If a path for heat conduction exists, i.e. the  neighbor’s temperature is lower than the region, temperature decreases  faster.  [7] shows the power impact of localized throttling.  As throttling ratio increases, less traffic is allowed to pass through a  router, and power consumed is reduced. Traffic throttling within a  router also affects the traffic of the neighboring router that exchanges  flits with the throttled router. This phenomenon helps cooling but  affects performance. Throttling can be implemented with many  different approaches, such as clock gating, dynamic voltage-frequency  scaling, and can be combined with data isolation techniques. For  simplicity and maximal cooling speed, global throttling (GT) is a  commonly adopted approach. However, if there are only few routers  overheated, GT inevitably throttles the remaining routers those are not  overheated, and the system performance drops drastically. On the  contrary, distributed traffic throttling (DTT) proposed in [7] only  throttles the input traffic of the overheated router to the reduce  workload. DTT has much less performance impact in comparison with  GT and is suitable for 2D NoC.   1) Vertical throttling  Although the collaborative scheme adopted by DTT also works  for 3D NoC, it is slow to form an effective heat conduction path. The  throttling region gradually grows outward from the overheat router in  DTT. Vertical throttling (VT) is a reactive mechanism that throttles  routers in the direction of maximum temperature decreasing. For 3D  NoC, VT throttles vertically aligned routers in the pillar. For an Nlayer 3D NoC, VT throttles the routers on upper N-1 layers  simultaneously and leaves the router in bottom layer. The routers in  bottom layer are never throttled because they always have large heat  conduction to the heat sink. Besides, downward routing aggregates  traffic load on bottom layer. If the router in bottom layer is throttled,  the performance impact will be large. For emergency cooling on  overheat, VT has higher cooling speed than DTT and less performance  impact than GT. However, if the temperature of the overheat router is  not very high and the heat generation of the router is relatively slow,  the cooling speed of DTT is enough and DTT has less performance  impact. VT can be viewed as a specialized variation of collaborated  DTT. The performance impact is larger due to the number of  simultaneous throttled router is larger. To reduce the impact, we  propose the thermal-aware vertical throttling (TAVT).   2) Thermal-aware selection of throttling level  Fig. 9 shows an example of TAVT in a 4-layer 3D NoC. There are  two design parameters for TAVT: ratio set and level set. The ratio set  determines how much a router is throttled. The level set determines all  the combinations of the throttling ratio of vertically aligned routers.  The number of throttling ratio can be determined as arbitrarily number.  With consideration of the implementation cost of throttling, we choose  a two throttling ratio for each router: full off (0%) and half off (50%).  The number of throttling level does not required to be equal to the  Figure 6.  Examples of downward routing in a 4-layer 3D NoC :   (a) without downward (downward level = 0); (b) downward level = 1;   (c) downward level = 2; (d) downward level = 3.  Figure 7.  An example of traffic-aware downward routing  TABLE I.   AGGREGATION OF TRAFFIC LOAD  USING FIXED DOWNWARD  LEVELS IN A 4-LAYER 3D NOC  DW_level  Layer 0 load  Layer 1 load  Layer 2 load  Layer 3 load  0  1  2  3  TL0  0  0  0  TL1  TL0  0  0  TL2  TL1  TL0  0  TL3 TL2+TL3 TL1+TL2+TL3 TL0+TL1+TL2+TL3 TLk denotes original horizontal traffic load in layer k  Non-minimal path routing naturally increases the zero load  latency and also has power overhead. However, due to the relatively  short distance among layers, the latency and driving power of a  vertical transfer is small. As [13] and [14], the crossbar switch-based  3D architecture is preferred due to its superior performance over  symmetrical 3D structure, and can be implemented with cost-effective  dimensionally-decomposed (DimDe) router. The latency overhead of  non-minimal path routing on z direction is constrained to one cycle, no  matter where the source and destination routers are.  3) Traffic-aware level selection  Downward routing provides a downward power migration  mechanism by transporting horizontal traffic from upper layer to  bottom layer. Fig.7 shows the traffic-aware downward routing. The  downward level for each pillar is different. If the traffic load is small,  more traffic can be migrated by a selecting a larger downward level.  However, if the aggregated traffic load on bottom layer is larger than  the bandwidth, the bottom layer will be congested. TABLE I. shows  the aggregated traffic load of each layer with different fixed  downward level. As the number of downward level increase, more  traffic load is aggregated on bottom layer (layer 3). Fig. 8 shows the  pseudo code of the proposed traffic-aware level selection. The  selection of downward level depends on the traffic load estimation and  prediction of each layer. To prevent network congestion, the selected  downward level should not make aggregated traffic load over the  limitation. The actual implementation of load estimation relies on  counters, which is updated individually inside each router, and the  summation of counter is taken only once on each interval.  226       Theorem The routing designed with the following procedures  guarantees deadlock-freedom.  Proof The routing guarantees deadlock-freedom, because no  dependency occurs as the following restrictions all true:  1. No cyclic dependency is formed in each tier, because a  packet must follow the restrictions for deadlock-freedom,  as long as the packet is transferred on the single tier.   Downward routing adopts deadlock-free routing (e.g. XY  routing in our experiment) for routing in a layer.  2. No cyclic dependency is formed across tiers, because a  packet is passed between tiers only in the descending  order.   Downward routing never routes a packet above the source  router. The horizontal route is always below or within the  same layer of the source router.  3. No cyclic dependency is formed within a pillar, because a  pillar router is a crossbar switch.   The vertical transfer is through the crossbar switch in  downward routing. The downward transfer can be  arbitrary level in each pillar, which is controlled by the  level selection mechanism. The upward transfer only  occurs at the bottom of destination router and is directly  to the destination.  Figure 11.  Theorem and proof for deadlock-free routing in 3D NoC [13]  channel to allow preemption or adopting turn-model to prevent  circular waiting are both effective approaches. However, the cost of  virtual channel is high for NoC, therefore many turn-model based  routing algorithms are proposed. Besides, virtual channel requires  more buffer, which makes extra power consumed and heat generated.  [13] derives a theorem to guarantees a routing algorithm to  be deadlock-free in 3D NoC. The theorem and proof is shown in Fig.  11. We follow the restrictions of this theorem to prove the deadlockfreedom of the proposed routing algorithm in both proactive and  reactive states  IV. EXPERIMENT AND DISCUSSION  A. Simulation environment  The simulation environment for RTM of 3D NoC couples the  network model, power model and thermal model. We integrate Noxim  [11] and Hotspot [10] as our simulator, and validate with CFD-RC [12]  for the accuracy of the vertical temperature distribution. We adopt the  tile geometry and power model of Intel’s 80-core processor [8]. We  add the model of basic 3D router and the DimDe router, and we  extend NoC simulator to generate a 4x4x4 3D architectures of NoC.  During network traffic simulation, the power trace is generated based  on the power model. The power trace and the physical floorplan are  used as inputs of the thermal simulation. For each router, the depth of  the buffering channel is 4 flits and the no virtual channel is used.  B. Evaluation of proposed traffic-aware downward routing  The first experimen t shows the effect iveness o f temperature  con trol and performance opt imization of the proposed proactive  traffic-aware downward rou t ing. F irst we show the steady state  maximum temperature of each layer in Fig. 12. The two extreme cases  are Fig. 12(a) and Fig. 12(d). With uniform traffic offered, XYZ is the  optimal routing algorithm that achieves maximum throughput and  distributes traffic evenly. This is the case of optimizing performance  without thermal consideration. Although the network is not saturated  for all packet inject ion rate (PIR) between 0.001 and 0.029, the  maximum temperature of the 3D NoC is over 140 ℃, which is above   (a)  (b)  (c)  (d)  (e)  Figure 9.  An example of thermal-aware vertical throttling with different  throttling levels and different throttling ratios on each layer:   (a) no throttling (normal mode); (b) throttling level =0 ;  (c) throttling level =1; (d) throttling level =2; (e) throttling level =3  Figure 10.  Thermal-aware selection of throttling level  number of downward level or the fully combination of the ratio set of  all routers in a pillar (e.g. 34=81 combinations for a 4-level 3D NoC  with 3 ratios in each router: full throttled, half throttled, and no  throttled in normal mode). For simplification of control, we set four  levels of throttling in emergency as shown in Fig. 9. When the  temperature is lower than the trigger threshold Ttrigger, the 3D NoC  runs normal mode without throttling, which is shown as Fig. 9(a). If  some router is detected overheated i.e. temperature over trigger  threshold, it enters emergency mode. Since top layer is usually hotter  than the layer below it, TAVT throttles the top router first. As Fig. 9(b)  shows, the input bandwidth is limited to half of the full bandwidth. If  the temperature decreases slow or does not decrease, the overheat  router needs higher cooling speed. , The degree of emergency can be  detected by continuously comparing the temperature with the trigger  threshold. If the router keeps in emergency mode, it needs higher  throttling level. The vertical aligned routers gradually throttle from top  toward bottom to provide a faster heat conduction channel. The flow  chart of selection of throttling level is shown in Fig. 10.  3) Reactive routing on throttling  If a router is throttled, the latency of packets routed through the  throttled router increases drastically. If the NoC adopts deterministic  routing, the packets will be blocked by the throttled router until the  temperature is lower than trigger threshold. Reactive routing is an  adaptive routing algorithm that prevents packets route through  throttled routers. The bypassing path is not limited to the path through  the neighboring routers of the overheated router. We use the never  throttled bottom router and downward routing for reactive routing.  C. Proof of Deadlock Freedom for the Proposed RTM  Any deadlock from traffic migrat ion will cause system stal l.  Therefore the routing has to be deadlock-free. Deadlock occurs when  the four following necessary conditions are all true: mutual exclusion,  partial allocation, no preemption, and circular waiting. Using virtual  227         (a)  (b)  (c)  (d)  Figure 12.  Maximum temperature of each layer with different fixed downward level, 1packet = 6 flits: (a) fixed downward level = 0 (XYZ routing);   (b) fixed downward level = 1; (c) fixed downward level = 2; (d) fixed downward level = 3  Figure 13.  The latency verses injection rate with uniform traffic offered,   1 packet = 6 flits, (without temperature limitation)  Figure 14.  The latency verses injection rate with transpose traffic offered,   1 packet = 6 flits, (without temperature limitation)  TABLE II.   ACHIEVABLE THROUGHPUT WITH UNIFORM TRAFFIC   TABLE III.   ACHIEVABLE THROUGHPUT WITH TRANSPOSE TRAFFIC  Thermal limit  60℃  XYZ (DW_level = 0) 0.0020  Fixed DW_level = 1  0.0023  Fixed DW_level = 2  0.0024  Fixed DW_level = 3  0.0025  Traffic-aware  0.0023  Improvement  15.00%  80℃  0.0059   0.0064   0.0065   0.0067   0.0063   6.78%  100℃  0.0096   0.0104   0.0108   0.0083   0.0101   5.21%  120℃  Infinite  0.0133   0.0230   0.0137   0.0160   0.0112   0.0112   0.0083   0.0083   0.0140   0.0195   5.26% -15.22% Thermal limit  XYZ (DW_level = 0) Fixed DW_level =1  Fixed DW_level =2  Fixed DW_level =3  Traffic-aware  Improvement  60℃ 0.0021 0.0024 0.0024 0.0025 0.0023 9.52%  80℃  0.0058   0.0064   0.0066   0.0050   0.0062   6.90%  100℃  0.0094   0.0103   0.0067   0.0050   0.0096   2.13%  120℃  Infinite  0.0131   0.0186   0.0104   0.0104   0.0067   0.0067   0.0050   0.0050   0.0121   0.0121   -7.63% -34.95% normal thermal limit. Therefore Fig. 12(a) is a thermal-limited case for  performance optimization. In our experiments, all the fixed downward  level cases except level = 0 are network limited. Therefore the  maximum temperatures of all fixed downward cases are just plotted  slightly over the PIR that reaches saturation throughput. Fig. 12(d) is  the case that maximizing heat conduction by concentrating all  horizontal routing power on the bottom layer. As we expect, it has the  lowest steady state temperature. Fig. 12 also shows that maximum  temperature changes as workload migration, and for uniform traffic,  the difference of temperature between layers is smaller when more  traffic is migrated downward. Fig. 13 and Fig. 14 show the latency  versus PIR of the proposed traffic-aware downward routing without  thermal consideration. Because  the behavior of  traffic-aware  downward routing is a combination of all the others, the latency is  among the two extreme cases. However, with thermal limit, the  achievable throughput is not necessarily the saturation throughput  defined by the PIR that corresponding to the twice zero-load latency.  We use TABLE II. and TABLE III. to show the thermal-limited  achievable throughput of the downward routing algorithms. The  rightmost column shows the saturation throughput calculated from the  PIR where the latency is double of zero-load latency, which can be  viewed as the thermal limit is at positive infinite. As thermal limit  becomes lower, the network is more prone to suffer from the problem  of temperature-limiting performance. Proactive workload migration  reduces the maximum temperature, and therefore eases the problem of  temperature-limiting. However, too much workload migration makes  the packets on bottom layer congested. The proposed traffic-aware  level selection adjusts the downward level according to the congestion  degree, so it prevents saturation on bottom layer. TABLE II. and  TABLE III. show that the proposed proactive technique improves the  achievable throughput for most temperature-limited cases in uniform  and transpose traffic. When thermal limit is 80℃, the improvement is  around 7%.  228                         (a)  (b)  (a)  (b)  Figure 15.  Number of the throuttled router over time for:   (a) global throttling (GT); (b) distributed traffic throttling (DTT)  Figure 16.  Number of the throttoled router over time for the proposed:   (a) vertical throttling (VT); (d) thermal-aware vertical throttling (TAVT).   C. Evaluation of proposed thermal-aware vertical throttling  The second experiment shows the availability impact of throttling.  GT, DTT, VT, and TAVT are all implemented and simulated  individually with the same PIR. The throttling ratio will keep the  setting of the selected throttling level. In this experiment, GT, DTT,  and VT have only one level of throttling, and TAVT has four as Fig. 9.  The throttling ratio of GT and DTT is 1.0, and the throttling ratio of  VT is combined by three 1.0 and one 0.5. Once the temperature of a  router is above the trigger threshold, GT throttles all 64 routers. DTT  only  throttles  the overheated  router, and VT  throttles  the  corresponding pillar of 4 routers. TAVT throttles according the  selected level.  Fig. 15 and Fig. 16 show the effective number of throttled router  in simulation. The number is calculated by accumulating the throttling  ratio of all 64 routers on each updating of throttling mechanism. In  this experiment, the interval of updating is 10-3 second. From Fig. 15  (a) and Fig. 15 (b), the two extremes of throttling are shown. GT has  highest cooling speed for 3D NoC, therefore it usually throttle only 1  ms. DTT throttles only the overheated routers. Therefore overheat  region grows if the heat conduction is not enough. The throttling  region reforms horizontally first and then vertically toward heat sink  When the cooling speed increases, the number of required throttled  router may decrease. Fig. 16 shows the proposed VT and TAVT.  Since VT directly throttles the vertical aligned routers to create a most  effective heat conduction path, the number of throttled router  decreases much faster than DTT. The throttling behavior of TAVT is a  combination of DTT and VT. TAVT adaptive changes the level, and  trades off between cooling speed and performance impact.   To analyze the cooling speed of different throttling approaches,  we record the throttling time of each throttled router. TABLE IV.  shows the statistics of the time. Because GT creates the largest low  power density region over the NoC, the cooling of GT is the fastest.  DTT has longer throttling time due to the reforming region is not  guided. Since the temperature of the router beneath the overheated  router is usually lower, the region tends to grows horizontally, not  vertically. The region reforms toward bottom layer unless the  accumulated heat makes the router in the beneath layer overheated.  However, the router close to heat sink has higher cooling efficiency.  Therefore DTT takes longer time to form a effective cooling region.  229 TABLE IV.   STATISTICS OF THE THROTTLING TIME (UNIT: 1MS)  GT  1.147 0.125 DTT  VT  14.333 2.481  2.222 0.324  TAVT Reduction  4.353  82.69% 0.581  85.42% (VT vs. DTT) Mean  Var.  Reduction  (TAVT vs. DTT) 69.63% 73.85% TABLE V.   Injection  Rate  GT  (flits/node/cycle) AVERAGE NETWORK THROTTLING RATIO  DTT  VT  TAVT  Reduction   (TAVT vs. DTT)  0.016  0.022  0.030  0.0160 0.0256 0.0320 0.0085 0.0126 0.0169 0.0105 0.0155 0.0175 0.0077 0.0108 0.0143 9.12% 14.68% 15.28% TABLE VI.  AVERAGE NETWORK AVAILABILITY  Injection  Rate  (flits/node/cycle) 0.016  0.022  0.030  GT  0.9840 0.9744 0.9680 DTT  0.9915  0.9874  0.9832  VT  TAVT  0.9895  0.9846  0.9825  0.9923 0.9893 0.9857 Improvement  (TAVT vs. DTT)  1.0008× 1.0019× 1.0026× When the router in the lower layer is cooled and not overheated, its  temperature may be still close to the thermal limit. The conceptual  difference between DTT and VT/TAVT is similar to the conceptual  difference between breadth-first search and depth-first search. The  variation and standard deviation also shows that overheat region  reforms but not directly toward heat sink. VT has much less average  throttling time than DTT. In this experiment, the reduction is 82.7%.  Because TAVT is adaptive and starts from the lower level, the average  throttling time is longer than VT. From TABLE IV. , the reduction of  TAVT in comparison with DTT is 69.6%. Both VT and TAVT have  smaller variation on throttling time.  Finally we show and discuss performance impact from the view of  the average network throttling ratio and the average network  availability. TABLE V. shows the average network throttling ratio and  TABLE VI. shows the  average network availability. Although the  throttling time of GT is shortest, too many routers are throttled;  therefore the average throttling ratio of GT is the highest, and the                              "
Physical vs. Virtual Express Topologies with Low-Swing Links for Future Many-Core NoCs.,"The number of cores present on-chip is increasing rapidly. The on-chip network that connects these cores needs to scale efficiently. The topology of on-chip networks is an important design choice that affects how these networks scale. Most current on-chip networks use 2-D mesh topologies which do not scale due to their large diameter and energy inefficiency. To tackle the scalability problem of 2-D meshes, various physical express topologies and virtual express topologies have been proposed. In addition, recently proposed link designs like capacitively driven low-swing interconnects can help lower link power and latency, and can favor these bypass designs. In this work, we compare these two kinds of express topologies under realistic system constraints using synthetic network traffic. We observe that both express topologies help reduce low-load latencies. Virtual topologies help improve throughput whereas the physical express topologies give better performance-per-watt.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip Physical vs. Virtual Express Topologies with Low-Swing Links for Future Many-core NoCs Chia-Hsin Owen Chen1 , Niket Agarwal2 , Tushar Krishna1 , Kyung-Hoae Koo3 , Li-Shiuan Peh1 , and Krishna C. Saraswat3 1Dept. of Electrical Engineering and Computer Science, Massachusettes Institute of Technology, Cambridge, MA 02139 2Dept. of Electrical Engineering, Princeton University, Princeton, NJ 08544 3Electrical Engineering Department, Center of Integrated Systems, Stanford University, Stanford, CA 94305 1 {owenhsin, tushar, peh}@csail.mit.edu, 2 {niketa}@princeton.edu, 3 {koo1028, saraswat}@stanford.edu, Abstract—The number of cores present on-chip is increasing rapidly. The on-chip network that connects these cores needs to scale efﬁciently. The topology of on-chip networks is an important design choice that affects how these networks scale. Most current on-chip networks use 2-D mesh topologies which do not scale due to their large diameter and energy inefﬁciency. To tackle the scalability problem of 2-D meshes, various physical express topologies and virtual express topologies have been proposed. In addition, recently proposed link designs like capacitively driven low-swing interconnects can help lower link power and latency, and can favor these bypass designs. In this work, we compare these two kinds of express topologies under realistic system constraints using synthetic network trafﬁc. We observe that both express topologies help reduce low-load latencies. Virtual topologies help improve throughput whereas the physical express topologies give better performance-per-watt. I . IN TRODUC T ION With Moore’s law providing more and more transistors onchip, architects have embraced many-core architectures to deal with increasing design complexity and power consumption of conventional single-processor chips. There are already fabricated designs with 10s of cores on a single chip [9], [17]. Going forward, we could have 100s or even 1000s of cores on a single die. These many-core architectures employ many simpler cores and interconnect the cores using a scalable onchip network fabric. To meet the bandwidth demands of these cores, designers use packet-switched on-chip interconnection networks and have moved beyond conventional shared buses. Selecting an appropriate topology is one of the most critical decisions in the design of on-chip networks; it impacts the zero-load latency and sustainable bandwidth, and also inﬂuences the power consumption of the network. Most existing on-chip networks utilize a 2-D mesh topology [6], [9], [17], as meshes have lower design complexity and map well to the 2-D chip substrate. However, they are energy inefﬁcient because the high network diameter leads to extra router hops to reach the destination, and router energy is high relative to link energy [9]. This poses serious scalability concerns for such topologies as node count increases in the future. To tackle the scalability issues of 2-D mesh, various express topologies have been proposed in the literature. One set of proposals employ long physical links between non-local routers to reduce the This research was supported in part by the National Science Foundation under grant CCF-811796 and the Marco Interconnect Focus Center. effective network diameter [4], [7], [12]. We will refer to these topologies as physical express topologies. Another set of proposals use various techniques to opportunistically bypass router pipelines at intermediate network hops and thus save on network latency and power [14]–[16]. We will refer to these techniques as virtual express topologies. Both physical and virtual express topologies have their advantages and disadvantages. Physical express topologies reduce the network diameter, thereby saving the latency and power due to bypassing of intermediate router hops. To accomplish the physical bypass paths, however, extra router ports, larger crossbars and extra physical channels are required. Extra router ports and larger crossbars lead to higher router area and power. While on-chip wires are relatively abundant, use of large number of dedicated point-to-point links leads to a large area footprint and low channel utilization. Virtual express topologies have the advantage that they do not use extra long physical links. However, the bypass of routers in such designs is opportunistic and not guaranteed. The virtual router bypass also differs from physical bypass in that the bypassing ﬂits still have to multiplex through the router crossbar and output link, while in a physical bypass, all intermediate routers and links are bypassed completely. The long links in the physical express topologies can also leverage some of the recent innovative low-swing interconnect proposals. Speciﬁcally, capacitively driven low-swing interconnects (CDLSI) [8] have the potential to save signiﬁcant energy while also providing modest latency reductions. CDLSI links can be used as single-cycle multi-hop express links that provide further latency savings since long hops can be covered in a single cycle and the power consumption would also be less, owing to the low-swing nature of these links. The low power of the CDLSI links can also be exploited by all the topologies for connecting neighboring routers. Although there are clear trade-offs between various physical and virtual express topologies, to the best of our knowledge, there has been no work comparing these various topology alternatives for many-core network on-chips (NoCs). In this paper, we compare a particular physical express topology (express physical channels (EPC) based on express cubes [4]), to a virtual express topology proposal (express virtual channels (EVC) [14]) for large node count on-chip networks. We present energy, area, latency and throughput results for a 256 978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.26 173 node conﬁguration with synthetic network trafﬁc. We observe that both EPC and EVC help in lowering low-load latency. However, while the EVC network is more robust across trafﬁc patterns, and offers a higher throughput than baseline for most cases, the EPC network is more trafﬁc dependent and bisection bandwidth dependent. If we compare the performance-perwatt, however, EPC with CDLSI links turns out to be the best. The main contributions of this work are as follows. (cid:129) We present a detailed characterization of how CDLSI links could be leveraged for long and short hop links to be used in various express topologies. (cid:129) We present a detailed comparison of physical and virtual express topologies for large node-count systems under different design constraints (bisection bandwidth, router area, and power). The rest of the paper is organized as follows. Section II provides circuit level details of CDLSI links and its latency, area, and energy properties. Section III discusses various physical and virtual express topologies proposed in the literature and motivates the need for comparison amongst them. Section IV discusses the evaluation methodology and presents quantitative results, and Section V concludes. I I . CA PAC I T IV E LY DR IV EN LOW-SW ING IN T ERCONN EC T S Conventional low-swing interconnects are attractive to NoC designs as they reduce the power consumption by reducing the voltage swing on the wires [18]. But this advantage is usually accompanied by a penalty in latency. Moreover, they usually require a second voltage power supply. Capacitively Driven Low-Swing Interconnects (CDLSI) were proposed by Ho. et al. in [8]. The design involves driving wires through a series capacitor, and these interconnects have been shown to have excellent energy savings without the degradation of performance. The use of the coupling capacitor results in a low-swing wire without the need for a second power supply. It also extends the wire bandwidth by introducing pre-emphasizing effects and signiﬁcantly reduces the driver energy consumption. The differential and twisted wires enable the signals to be sent at a low voltage swing, and eliminate the coupling noise. But this comes at the cost of 2X wiring area. The link area model we used in our experiments can be formulated as AC u = (2wf lit − 1)lw for normal non-differential Cu wires and ACDLS I = (4wf lit − 1)lw for CDLSI wires, where AC u stands for the area of a conventional Cu wire, ACDLS I stands for the area of a CDLSI wire, wf lit is the ﬂit-width, l is the link length, and w is both the width of the link and the spacing between two wires. The spacing between the links is assumed to be the same as the link-width. We applied a delay-power optimization, which is similar to the approach in [11], to both normal Cu wires and CDLSI wires. Detailed analysis of this approach can be found in [13]. For normal Cu wires, more than half of the total power is consumed by the wire. On the other hand, for CDLSI wires, transceivers consume most of the power. As a result, for a 1mm 2mm 4mm length BW 1Gbps 2Gbps 3Gbps 4Gbps 5Gbps Normal 1-10Gbps CDLSI D(ps) 108 108 108 101 94 108 E(fJ) 22.4 22.4 22.4 27.3 35.2 133 D(ps) 215 215 194 175 162 215 E(fJ) 27 27 30.4 35.8 43.6 265 D(ps) 430 430 430 368 330 430 E(fJ) 61 61 61 71.5 87.2 531 D E LAY (D ) AND EN ERGY (E ) CON SUM P T ION O F CDLS I W IR E AND NORMA L CU W IR E FOR D I FF ER EN T L ENG TH S AND BANDW ID TH S TABLE I given delay penalty, the impact on power saving in CDLSI wires is greater than that in normal Cu wires. Table I shows the latency and energy consumption of a CDLSI wire as a function of different wire lengths and bandwidths with 192 nm wire width at 32 nm technology node. The wire width and the wire spacing are chosen to be four times the minimum wire width deﬁned in ITRS [1] to meet the bandwidth requirements in our experiments. The table also shows the numbers for normal Cu wires. The energy consumption of a CDLSI wire is approximately 10x less than that of a normal Cu wire with the same wire length and bandwidth below 3 Gbps. It should be noted that CDLSI wires consume ultra-low energy even as the length of the wire increases. This property enables the use of CDLSI wires as express links for multi-hop communication. For example, the delay for a 4mm long CDLSI wire is 330 ps, that is, it is within two cycles for a 5GHz clock frequency, whereas the it takes three cycles to traverse a normal Cu wire of the same length. I I I . EX PR E S S TO PO LOG I E S An ideal interconnection network between processing cores in CMPs would be point-to-point, where the latency of the messages is equal to just the link latency, and there are no additional penalties due to contention and corresponding N − 1 links per node in a N node network, which will blow arbitration. However, such a network is not scalable, requiring up the area footprint. This has led to researchers proposing packet-switched networks with intermediate routers to multiplex the available bandwidth [3]. The router microarchitecture for such a network is described next. A. Baseline Router A NoC router typically has a 5-stage pipeline [5], with four stages in the router, and one in the link. In the ﬁrst stage, the incoming ﬂit gets buffered (BW-Buffer Write), and performs routing for determining the output port to go out from. In the second stage (VA-VC Allocation), the ﬂits at all input ports arbitrate for virtual channels (VCs) at the next router. In the third stage (SA-Switch Allocation), the ﬂits at all input ports, which have been assigned output VCs, now arbitrate for using the crossbar that will connect this input port to the required output port. In the fourth stage (ST-Switch Traversal), the ﬂits that won the switch traverse the crossbar. Finally, in the ﬁfth stage (LT-Link Traversal), ﬂits coming out of the switch traverse the link till the next router. A packet can go through VA and SA multiple times depending on the contention, until it 174 Router pipeline (BW, VA, SA, ST, LT)  Physical-bypass pipeline (LT) Virtual-bypass pipeline (ST, LT) 0 1 2 3 4 5 (i) Baseline 0 1 2 3 4 5 (ii) Physical Express Topology Route  computation EVC latch VC 1 VC 2 VC n Input buffers EVC latch VC 1 VC 2 VC n Input buffers EVC allocator Switch allocator Output 1 5x5  Crossbar switch Output 5 Route  computation VC allocator Switch allocator Input buffers Input 1 Input 9 VC 1 VC 2 VC n VC 1 VC 2 VC n Output 1 Input 1 9x9  Crossbar switch Output 9 Input 5 0 1 2 3 4 5 Input buffers (iii) Virtual Express Topology Buffer Write Routing VC  Allocation Switch  Allocation Switch  Traversal Link  Traversal Buffer Write Routing EVC  Allocation Switch  Allocation Switch  Traversal Link  Traversal (a) Comparison of the path taken by a ﬂit going from router 0 to router 5 in the Baseline, Physical Express Topologies, and Virtual Express Topologies (b) EPC Router Microarchitecture (c) EVC Router Microarchitecture Fig. 1. Comparison of Express Topologies Moreover, this eliminates the power that these packets would consume at the intermediate routers. The trade-off however is that each router now needs to have multiple ports, which in turn requires a bigger crossbar. The area and power of a crossbar increase as a square of the number of ports times the link width, and thus more ports is a potential concern. Moreover, the available metal area on-chip could add a limitation on the number of express links that can be added. Reducing the link width can address some of these concerns, but that would add serialization latency to the packets. Express Cubes [4], Flattened Butterﬂy [12] and MECS [7] are examples of physical express topologies. Express Cubes was a proposal for the multi-chassis shared-memory multiprocessor (SMP) domain for connecting nodes that are multiple hops away to reduce latency. Flattened butterﬂy is an on-chip network topology that uses routers with high port counts to map a richly connected butterﬂy network onto a two-dimensional substrate using a two-level hierarchy. It reduces the network diameter to two, thus minimizing the impact of multiple router hops. However, the total number of links required in each dimension grows quadratically with the number of nodes in the dimension. Multi-drop express channels (MECS) [7] uses a one-to-many communication model enabling a high degree of connectivity in a bandwidth efﬁcient manner. It solves ﬂattened butterﬂy’s problem of requiring too many links by using an express multi-drop bus originating from each router to connect other routers along the dimension. However, the crossbar has only one port per direction like the baseline, and so all the express links entering a router from a particular direction need to multiplex on the same crossbar port, thereby sacriﬁcing throughput. C. Virtual Express Topologies Virtual Express Topologies are a class of ﬂow-control and router microarchitecture designs that reduce router energy/delay overheads by creating dynamic fast paths in a network. The basic idea is that ﬂits try to bypass buffering and arbitrations at the router, and proceed straight to the switch (ST) and link traversals (LT), thereby reducing the latency, (a) 8x8 Baseline and EVC network (b) 8x8 EPC network Fig. 2. Network topology succeeds in obtaining a VC and switch. Thus each ﬂit entering a router takes multiple cycles before it can make a hop to the next router. This design will be referred to as the baseline design in the rest of the paper. This solution of having hop-by-hop traversal through multiple intermediate routers is simple and effective. However, spending multiple arbitration cycles in intermediate routers would add unnecessary delay to packets traveling longer distances, especially in future many core chips. Moreover, the buffering and arbitrations at the routers add to the power consumed. Thus, there have been many proposals that advocate for packets traveling longer distances to bypass intermediate routers. We characterize these into two categories. (cid:129) physical express topologies: adding physical channels to skip intermediate routers. (cid:129) virtual express topologies: having means to bypass arbitration and buffering at the intermediate routers. The basic ideas behind these techniques are shown in Figure 1(a), and described next. B. Physical Express Topologies Physical express topologies argue for physical express links between far away routers. Non-local packets can traverse these express links (i.e. LT) to avoid getting delayed at intermediate routers, and also not add to contention at the ports there. 175 and power (as buffer reads/writes and VC/Switch arbitrations are skipped). The ﬂits that fail to bypass get buffered and proceed through the conventional baseline pipeline described in Section III-A. No extra physical links are required, and the crossbar is same as the baseline crossbar. However, this means that the incoming ﬂits from various ports (which wish to bypass the router), and the locally buffered ﬂits, all need to multiplex on the same output links. Virtual express topology proposals essentially describe efﬁcient ways of performing this multiplexing, such that most ﬂits are able to bypass the routers along their path. Express Virtual Channels (EVC) [14], Token Flow Control (TFC) [15] and Prediction Router [16] are examples of virtual express topology proposals. Express Virtual Channels statically partitions all the VCs into 1-hop, 2-hop, . . . lmax -hop express VCs (EVCs). Flits arbitrate for appropriate EVCs during VA depending on their route. A ﬂit that gets a k-hop EVC can bypass the k-1 intermediate routers along that dimension, by sending a lookahead one cycle in advance to preallocate the crossbar at the next router. EVCs use deterministic XY routing, and do not allow bypassing at turns, to avoid contention among different lookaheads. Token Flow Control also enables router bypassing by obtaining and chaining together tokens (hints about buffers and VCs at neighboring routers) to form arbitrarily long bypass paths. It supports contention among lookaheads, thereby allowing adaptive routing, and bypassing along turns. Prediction Router [16] enables bypassing by predicting the output port and setting up the switch a-priori. D. Chosen Topologies We choose a 16x16 mesh as our baseline topology, Due to limited space, a smaller 8x8 mesh example is shown in Figure 2(a), with each router having 5-ports, and implementing the 5-stage pipeline described in Section III-A. The physical express topology we use for our evaluation studies is based on express cubes and uses 9-port routers, with express links connecting each router to the router that is 4-hops away, in each direction, as shown in Figure 2(b). We refer to this topology as Express Physical Channels (EPC). The virtual express topology we use for our experiments is EVC, since it maps well to the EPC design by not allowing turning paths, and restricting the bypassing to a maximum of lmax hops (which we choose to be four in our design)1 at a time. The EVC topology uses a 16x16 mesh like the baseline. EPC and EVC might not be the best possible physical and virtual express topologies for all scenarios in their individual domains, but they capture the essential properties of these design spaces that we wish to compare, namely the performance, power and area implications of long physical links and high crossbar ports versus opportunistic bypass and multiplexing on the crossbar ports and links. Figure 1 shows micro architecture of the EPC and EVC routers. 1Higher values of lmax complicate ﬂow-control signaling and result in diminishing returns [14] Technology Vdd Vthreshold Frequency Topology Routing Synthetic Trafﬁc 32 nm 1.0 V 0.7 V 5 GHz 16-ary 2-mesh (The physical express topologies have additional express links) Dimension ordered X-Y routing Uniform Random, Tornado, Bit Complement TABLE II S IMU LAT ION PARAM E T ER S 128 bit 24 bit Fig. 3. Equalized bisection bandwidth. IV. EVA LUAT ION In this section, we present a detailed comparison of a physical express topology, Express Physical Channels (EPC), and a virtual express topology, Express Virtual Channels (EVC), using both conventional full-swing copper interconnects and CDLSI low-swing links. We study the conﬁgurations along three dimensions under which future NoCs might be constrained, namely, bisection bandwidth, chip area and power. The simulation infrastructure and the various experiments are described next. A. Simulation Infrastructure To compare the different conﬁgurations, we used a cyclelevel on-chip network simulator, GARNET [2]. GARNET models a detailed 5-stage router pipeline as described in Section III. We used ORION 2.0 [10], an architecture-level network energy and area model, to evaluate the power consumption and area footprint of routers. For the links, we use the numbers discussed in Section II. All our experiments were done at 32nm technology node. In our results, we report only dynamic power and not leakage power. This is because at 32 nm technology node, leakage power is expected to be pretty high and aggressive leakage reduction techniques would be employed to reduce overall power consumption. Such techniques are not modeled in ORION, and thus we choose not to report leakage power. Evaluation with synthetic trafﬁc uses packets which uniformly consist of either 8-byte control packets or 72-byte data packets. We assume three message classes out of which two always carry control packets and one is dedicated to data packets. This was done to closely match modern cache coherence protocol requirements, as was done in [14]. Table II lists various network parameters that are common to all the conﬁgurations we evaluated for synthetic trafﬁc. We ran experiments for uniform random, tornado and bit complement trafﬁc. We do not report results for tornado trafﬁc since the trends were very similar to uniform random trafﬁc. Next, we describe various topology conﬁgurations that we evaluated. B. Conﬁgurations We evaluate two sets of conﬁgurations across all topologies. First we assume normal Cu wires for the links. We then perform experiments using CDLSI links for both local and express links in all topologies. 176 1) Conﬁgurations with normal Cu wires: For the EPC topology, we assume 1mm links between routers, and 4mm express links. It takes three cycles to traverse these express 4mm paths, as discussed in Section II. For the EVC network, it is assumed that the EVC ﬂits spend one cycle traversing the router (ST), and one cycle traversing the links (LT). The reverse signaling for buffer and VC ﬂow-control is assumed to take one cycle for both EPC and EVC. The baseline design is the 5-stage router design described in Section III. Future NoC designs could be constrained along different dimensions depending on system requirements. Three constraints which we feel NoC designs might face are bisection bandwidth, chip area and power consumption. With this in mind, we perform experiments to compare the various topologies under the above constraints. After ﬁxing a constraint, we perform design-space explorations to ﬁnd out the best performing conﬁguration for the particular topology. For the baseline design, we assume 128-bit links (ﬂit-width). Varying the ﬂit-width of the baseline will change the parameters of other topologies when we normalize designs. To study it’s effect, we performed all our experiments with varying baseline ﬂit-width and found that the conclusions of our experiments do not change. Hence we report results with 128-bit ﬂit-width as baseline. We will refer to this conﬁguration as Baseline. With 128-bit ﬂit-width, a 72-byte data packet has ﬁve ﬂits, while an 8-byte control packet forms one ﬂit. As mentioned before, we perform three sets of experiments: one in which bisection bandwidth across designs is equalized, another in which the router area is equalized and ﬁnally we compare designs given the same power budget. Conﬁguration with equalized bisection bandwidth: In the ﬁrst set of experiments, we keep the bisection bandwidth across all topologies constant. The EVC topology has the same number of ports at the Baseline topology and hence the ﬂit-width remains the same as 128 bits. As with every conﬁguration we choose the number of buffers and VCs that leads to the best performance for the EVC topology. We call this conﬁguration EVC-bw. The EPC topology has ﬁve times as many channels at the bisection cut of the Baseline design. To equalize the bisection bandwidth, the ﬂit-width of the EPC design thus becomes one-ﬁfth of that of the baseline, which is 24 bits. This is shown in Figure 3. A 72-byte data packet thus consists of 24 ﬂits, whereas an 8-byte control packet is composed of three ﬂits. Now we vary the number of VCs and buffers and ﬁnd out the best performing conﬁguration. This will be called EPC-bw in the rest of the paper. Conﬁguration with equalized router area: In the second experiment, we equalize the router area across all conﬁgurations. The crossbar and buffers are the major area consuming components in a router and hence we equalize them. The parameters of the router that impact crossbar and buffer area are ﬂit-width, number of VCs and number of buffers. We sweep the design space for the EPC and EVC topologies, varying the above three parameters so that the total area is same as that of Baseline. We report results for the best performing conﬁgurations and call them EVC-area and EPCarea. Baseline Baseline-CDLSI EVC-bw EVC-area EPC-bw EPC-area EVC-bw-CDLSI EVC-area-CDLSI EPC-bw-CDLSI EPC-area-CDLSI Flit width (bit) 128 128 128 128 24 64 128 128 24 64 #VC/msg class 6 6 12 6 4 6 12 6 4 6 TABLE III TO PO LOGY PARAM E T ER S #Buffer/port 36 36 84 36 108 66 84 36 108 66 Conﬁguration with same power budget: We plot the power-performance curve for all the designs and conﬁgurations that we studied and compare the performance of the designs under the same power budget. We did not have to run additional conﬁgurations for this study. 2) Conﬁgurations with CDLSI wires: We also evaluate all the topologies using CDLSI wires instead of conventional wires, and all other parameters remaining the same as before. We again assume 1mm links between routers, and 4mm express links. It takes 2 cycles to traverse these express paths using CDLSI wires. The conﬁgurations are called BaselineCDLSI, EVC-bw-CDLSI, EVC-area-CDLSI, EPC-bw-CDLSI, and EPC-area-CDLSI. Table III shows all the conﬁgurations. C. Results We evaluate the above mentioned conﬁgurations with a 16x16 mesh topology. We ﬁrst present the performance results for the various studies. 1) Normal Cu wires: The results with normal Cu wires are presented next. Same bisection bandwidth: Figure 4 shows the relative performance (packet latency vs. injection throughput) of various topologies for different synthetic trafﬁc, given the same bisection bandwidth. Note, we do not show tornado trafﬁc as it had results similar to uniform random trafﬁc. For both uniform random and bit complement trafﬁc, we observe that Baseline has the highest latency at low loads. The EPC-bw has the second highest latency and EVC-bw has the lowest low-load latency. The high hop-count of the baseline network leads to extra router and link hops resulting in higher low-load latencies. Both EVC-bw and EPC-bw bypass intermediate router pipelines and thus have lower low load latencies. The maximum difference in low-load latencies is for the bit complement trafﬁc. This is because in bit complement trafﬁc, the source and destinations are farthest away, leading to the best utilization of express paths (both virtual and physical). Although, EPC-bw completely bypasses the routers and EVC-bw bypasses only certain pipeline stages, the low-load latency for EVC-bw is lower. This is because of very low ﬂitwidths in the EPC-bw networks. The EVC-bw network has a ﬂit-width of 128 bits and equalizing the bisection bandwidth results in the EPC-bw network having a ﬂit-width of 24 bits. A data packet (72 bytes) thus has 24 ﬂits for EPC-bw, and 6 ﬂits for EVC-bw. The higher serialization latency of the physical express topology far outweighs the beneﬁts of 177 30 50 70 90 110 130 0 0.02 0.04 0.06 0.08 Packet injection rate (#packet/cycle/node) 0.1 A e v r e k c a p e g a t l a t n e y c ( e c y c l ) Baseline EVC-bw EPC-bw (a) Uniform Random Trafﬁc 30 50 70 90 110 130 0 0.01 0.02 0.03 0.04 Packet injection rate (#packet/cycle/node) 0.05 0.06 A e v r e k c a p e g a t l a t n e y c ( e c y c l ) Baseline EVC-bw EPC-bw (b) Bit Complement Trafﬁc Fig. 4. Network performance of various topologies with normal Cu wires under same bisection bandwidth 30 50 70 90 110 130 0 0.02 0.04 0.06 0.08 Packet injection rate (#packet/cycle/node) 0.1 A e v r e k c a p e g a t l a t n e y c ( e c y c l ) Baseline EVC-area EPC-area (a) Uniform Random Trafﬁc 30 50 70 90 110 130 0 0.01 0.02 0.03 0.04 Packet injection rate (#packet/cycle/node) 0.05 0.06 A e v r e k c a p e g a t l a t n e y c ( e c y c l ) Baseline EVC-area EPC-area (b) Bit Complement Trafﬁc Fig. 5. Network performance of various topologies with normal Cu wires under same router area complete physical bypasses, leading to overall higher latency than EVC-bw. In terms of throughput, the Baseline and the EVC-bw have much better saturation throughput than EPC-bw for all trafﬁc patterns. This is because physical express topologies do not optimally utilize the available link bandwidth. EVC-bw outperforms Baseline due to router bypasses and pushes the saturation throughput towards the ideal. In summary, given the same bisection bandwidth, EPC-bw performs worse than EVC-bw due to serialization effects and poor bandwidth utilization. Similar router area: Figure 5 shows the relative performance (packet latency vs. injection throughput) of various topologies for different synthetic trafﬁc patterns, given the same router area budget. When comparing topologies, with the router area normalized, we observe that the Baseline has the highest latency at low loads. The EPC-area network has the lowest low-load latency for both trafﬁc patterns, although it is only marginally better than EVC-area for uniform random trafﬁc. Unlike the equalized bisection bandwidth case, (where EPC-bw loses out to EVC-bw due to higher serialization latency), the 64-bit EPC-area topology has lower serialization latencies leading to better low-load latencies. In terms of throughput, EVC-area and EPC-area have better throughput than Baseline for uniform random trafﬁc due to better utilization of resources and lesser contention. EPC-area, however loses out on throughput to EVC-area because the partitioning of links leads to poorer utilization of physical links. For bit complement trafﬁc, the throughput of EPC-area 0 20 40 60 80 100 120 140 160 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 Packet injection rate (#packet/cycle/node) 0.08 0.09 0.1 o P w e r ( W ) Baseline EVC-bw EVC-area EPC-bw EPC-area 150W Power Budget 40W Power Budget Fig. 6. Power-Performance Curve with Normal Cu Wires is even worse than that of the Baseline. Our implementation of EPC networks assumes static routing and if a packet wishes to use an express link, it keeps on trying for it even if the local links are idle. In bit complement trafﬁc, most of the trafﬁc goes across the chip and thus requires express links. The static nature of our routing therefore leads to poorer utilization of local links and hence the inferior throughput. Smarter adaptive routing algorithms are required to choose between the local and express links for such express topologies and that is beyond the scope of this study. In summary, given the same router area budget, the performance of EPC-area is highly trafﬁc dependent, while EVCarea is more robust. Similar power budget: Many-core CMPs are already facing the power wall and if we go by current trends, the primary 178                           design constraint that future NoCs will face might be power. An NoC architect might need to choose between a set of network topologies, given a network power constraint. The performance evaluations from the equal bisection bandwidth and equal router area concluded in favor of EVC networks due to higher throughput than EPC and comparable low-load latencies. However, the complete picture can be seen in Figure 6 which shows the network power consumption of all topologies with varying performance for uniform random trafﬁc. Given this plot and a network power budget, one can choose the best performing network topology. For e.g., if the total onchip network power budget is 40W, the EPC-bw network is the best performing one, even though in terms of absolute latency and throughput, it is the worst as seen before. EPC-bw has the lowest ﬂit-width amongst all conﬁgurations and that leads to lower buffer, crossbar and link power consumption. However, if the desired throughput is higher than the saturation throughput of EPC-bw, the network power budget would have to be increased and EPC-area becomes a winner now since it is lower power than the EVC networks for the same throughput. But once EPC networks saturate, EVC networks perform the best at higher network budgets like 150W. The Baseline network always performs worse than EVC and EPC networks under a given power budget. Note that the observed power consumption of these topologies is much higher than what actual designs would allow in future, but there is a huge body of work that tackles the power consumption of routers and how to minimize that. The discussion of such works are beyond the scope of this work. In summary, EPC topologies are more attractive than the EVC networks from a performance/watt perspective as long as they meet the desired throughput. 2) CDLSI wires: The results with CDLSI wires are presented next. Same bisection bandwidth: Figure 7 shows the relative performance (packet latency vs injection throughput) of different topologies with various synthetic trafﬁc and given the same bisection bandwidth. The addition of CDLSI links does not affect the relative trends that were observed in the comparisons without CDLSI links. We thus, do not discuss these results and refer the readers to Section IV-C1. Similar router area: Figure 8 shows the relative performance (packets injected per cycle) of various topologies for synthetic trafﬁc and given the same router area. Adding CDLSI links to the EPC topology leads to lower long-hop latencies and the 4-hop links now take only two cycles to traverse, whereas it took three cycles with normal Cu wires. Lower long-hop latency speeds up packet delivery and eases contention and thus leads to better overall throughput. The effect of this is seen in the uniform random trafﬁc case, where we see that EPC-area-CDLSI has a better throughput than EVC-area-CDLSI. The trend was opposite with normal Cu wires. Thus, CDLSI wires help EPC topologies outperform EVC topologies, making them an attractive design under certain scenarios. With bit complement trafﬁc, the relative performance of various topologies is similar to what was seen with normal Cu wires in Section IV-C1. Similar power budget: CDLSI links do not affect the relative power consumption trends that we observed in Figure 6 with normal Cu links. We thus, do not discuss the details of the observation again. 3) Power breakdown: In order to understand the power consumption of various topologies, we study the power breakdown of various on-chip network components. Figure 9 shows the detailed network power breakdown normalized to the total power consumed by Baseline. As expected, the topologies with CDLSI links have lower power as compared to the same topology with Cu wires. This is due to the low-swing nature of CDLSI links. The link power of the CDLSI topologies goes down dramatically, thus lowering the overall power consumption. Another observation is that the buffers, crossbar and clock are the major power consuming components in the router. This is as expected. The EPC networks have the lowest overall power, primarily due to the reduced ﬂit-widths that these networks have. This reduces buffer and crossbar power. Interestingly, lower ﬂit-width also eases the load on the clock distribution network thus lowering clock power in the router. The power consumption of the allocators is negligible. D. Discussion We summarize the ﬁndings of our experiments next. (cid:129) Virtual express topologies provide the best throughput conﬁguration, given the same resources, and are more robust across all trafﬁc patterns. (cid:129) The latency and throughput of physical express topologies degrades a lot with thinner links due to the serialization latency. (cid:129) Given sufﬁcient link width (bisection bandwidth), the performance of physical express topologies is highly trafﬁc dependent. While uniform random trafﬁc is able to extract the beneﬁts of the express links and local links, trafﬁc patterns like nearest neighbor would result in complete non-utilization of express links, while bitcomplement needs to rely on an adaptive routing scheme to remove contention on the express links. (cid:129) When CDLSI links are used, the physical express topologies leverage the low latency physical express links to achieve the best low load latencies and also improve throughput. (cid:129) When performance/watt is considered, the physical express topologies are the better choice, until they saturate. After that express virtual topologies seem to perform best for a given power budget. V. CONC LU S ION In this paper, we compared the effectiveness of physical and virtual express topologies for meshes with large node counts, under constraints of bisection bandwidth, router area, and power. We also evaluated the impact of the capacitivelydriven low-swing interconnect (CDLSI) links on all designs. We observed that while both designs have similar low-load latencies, virtual express topologies give higher throughputs and are more robust across trafﬁc patterns. Physical express topologies, however, deliver a better throughput/watt, and can leverage CDLSI to lower the latency, and increase throughput. 179 30 50 70 90 110 130 0 0.02 0.04 0.06 0.08 Packet injection rate (#packet/cycle/node) 0.1 A e v r e k c a p e g a t l a t n e y c ( e c y c l ) Baseline-CDLSI EVC-bw-CDLSI EPC-bw-CDLSI (a) Uniform Random Trafﬁc 30 50 70 90 110 130 0 0.01 0.02 0.03 0.04 Packet injection rate (#packet/cycle/node) 0.05 0.06 A e v r e k c a p e g a t l a t n e y c ( e c y c l ) Baseline-CDLSI EVC-bw-CDLSI EPC-bw-CDLSI (b) Bit Complement Trafﬁc Fig. 7. Network performance of various topologies with CDLSI wires under same bisection bandwidth 30 50 70 90 110 130 0 0.02 0.04 0.06 0.08 Packet injection rate (#packet/cycle/node) 0.1 0.12 A e v r e k c a p e g a t l a t n e y c ( e c y c l ) Baseline-CDLSI EVC-area-CDLSI EPC-area-CDLSI (a) Uniform Random Trafﬁc 30 50 70 90 110 130 0 0.01 0.02 0.03 0.04 Packet injection rate (#packet/cycle/node) 0.05 0.06 A e v r e k c a p e g a t l a t n e y c ( e c y c l ) Baseline-CDLSI EVC-area-CDLSI EPC-area-CDLSI (b) Bit Complement Trafﬁc Fig. 8. Network performance of various topologies with CDLSI wires under same router area 0 0.2 0.4 0.6 0.8 1 1.2 o P w e r ( % ) Various configurations  Clock VC Allocator Buffer SW Allocator Crossbar Link Fig. 9. Network power breakdown for various topologies "
"Semiconductor Industry - Perspective, Evolution and Challenges.","In this talk the major challenges that the semiconductor industry will have to face in the second decade of the new millennium will be addressed. The industry ecosystem is moving toward a new equilibrium and, in this context, the semiconductor Industry will continue to play a dominant role to fuel the growth in particular in the electronic field. New applications have a common denominator of growing complexity with more and more limited power and not only in the mobile space, this will push the industry to keep particular emphasis on the power budget of the new designs both at silicon and at system level. From the architecture stand point, multiprocessing is already a reality and the industry will have to find new paradigms to handle the increased complexity both at system, design & silicon implementation level. The interconnect, in particular, will assume a dominant role in the new SOC design, including 3D, becoming a more and more strategic resource to properly achieve the future design objectives.","Semiconductor Industry: Perspective, Evolution   and Challenges  Alessandro Cremonesi  STMicroelectronics, Italy  Abstract     In this talk the major challenges that the semiconductor industry will have to face in the second decade  of the new millennium will be addressed. The industry ecosystem is moving toward a new equilibrium  and, in this context, the semiconductor Industry will continue to play a dominant role to fuel the growth in  particular in the electronic field. New applications have a common denominator of growing complexity  with more and more limited power and not only in the mobile space, this will push the industry to keep  particular emphasis on the power budget of the new designs both at silicon and at system level. From the  architecture stand point, multiprocessing is already a reality and the industry will have to find new  paradigms to handle the increased complexity both at system, design & silicon implementation level. The  interconnect, in particular, will assume a dominant role in the new SOC design, including 3D, becoming a  more and more strategic resource to properly achieve the future design objectives.  Biography     Alessandro Cremonesi received a Doctorate in Electronics Engineering from the University of Pavia,  Italy, in 1984. After a period of research activity in the opto-electronics field at the University of Pavia, he  joined STMicroelectronics working in different fields, including telecommunications, audio/video digital  signal processing and multimedia applications.     At present, Dr. Cremonesi is Vice President of the Strategy and System Technology Group and General  Manager of the Advanced System Technology (AST) group at STMicroelectronics, with the responsibility  of corporate system research and development and corporate strategic marketing activities across 14  different STMicroelectronics Labs worldwide.  5                 "
A Low-Latency and Memory-Efficient On-chip Network.,"Using multiple SDRAMs in MPSoCs and NoCs to increase memory parallelism is very common nowadays. In-order delivery, resource utilization, and latency are the most critical issues in such architectures. In this paper, we present a novel network interface architecture to cope with these issues efficiently. The proposed network interface exploits a resourceful reordering mechanism to handle the in-order delivery and to increase the resource utilization. A brilliant memory controller is efficiently integrated into this network interface to improve the memory utilization and reduce both memory and network latencies. In addition, to bring compatibility with existing IP cores the proposed network interface utilizes AXI transaction based protocol. Experimental results with synthetic test cases demonstrate that the proposed architecture gives significant improvements in average network latency (12%), average memory access latency (19%), and average memory utilization (22%).","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip A Low-Latency and Memory-Efficient On-chip Network  Masoud Daneshtalab, Masoumeh Ebrahimi, Pasi Liljeberg, Juha Plosila, Hannu Tenhunen  Department of Information Technology, University of Turku, Finland  { masdan, masebr, pakrli, juplos, hanten }@utu.fi Abstract- Using multiple SDRAMs in MPSoCs and NoCs to  increase memory parallelism is very common nowadays. In-order  delivery, resource utilization, and latency are the most critical issues  in such architectures. In this paper, we present a novel network  interface architecture to cope with these issues efficiently. The  proposed network  interface exploits a resourceful reordering  mechanism to handle the in-order delivery and to increase the  resource utilization. A brilliant memory controller is efficiently  integrated into this network interface to improve the memory  utilization and reduce both memory and network latencies. In  addition, to bring compatibility with existing IP cores the proposed  network  interface utilizes AXI  transaction based protocol.  Experimental results with synthetic test cases demonstrate that the  proposed architecture gives significant improvements in average  network latency (12%), average memory access latency (19%), and  average memory utilization (22%).  1. INTRODUCTION  Integrating a large number of functional and storage modules onto a  single die in the deep sub-micron regime and beyond is becoming a  major performance issue in System-on-Chip (SoC) architectures   [1] [2] [3] [4]. Network-on-Chip (NoC) has emerged as a solution to  address the communication demands of many/multi core architectures  due to its reusability, scalability, and parallelism in communication  infrastructure  [3] [4]. The fundamental function of network interfaces  (NI) is to provide communication between Processing Elements (PE)  and the network infrastructure  [5] [6] [7]. That is, one of the practical  approaches of network interfaces is to translate the language between  the PE and router based on a standard communication protocol such as  AXI  [6] and OCP  [7]. On top of that, in-order delivery is another  practical approach of network interfaces  [8] [9] [10]. In-order delivery  should be handled when exploiting an adaptive routing algorithm for  distributing packets through the network  [8], when obtaining memory  access parallelization by sending requests from a master IP core to  multiple slave memories  [9] [10], or when exploiting a modern memory  access scheduling in memory controller to reorder memory requests   [11]. In this paper, we introduce an efficient network interface  architecture where the key ideas are threefold. The first idea is to deal  with out-of-order handling in such a way that when a master sends  requests to different memories, the responses might be required to  return in the same order in which the master issued the addresses, and  therefore a reordering mechanism in NoC should be provided by the  network interface. The second idea is to improve resource utilization  because in on-chip networks we have limitation of hardware resources  (e.g. buffers), power consumption, and network latency. According to  our observation, utilization of reorder buffers in network interfaces is  significantly inefficient, due to the fact that the traditional buffer  management is not efficient enough for network interfaces. Therefore,  an advantageous reordering mechanism via resourceful management of  buffers in the network interface is presented. The last idea is to present  a brilliant memory controller that is efficiently integrated into the  proposed network interface, which helps to improve memory utilization  and reduce both memory and network latencies. Also, the proposed  network interface architecture exploits the AMBA AXI protocol, to  978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.19 99 allow backward compatibility with existing IP cores  [6]. We also  present micro-architectures of the proposed ideas, particularly the  memory access reordering mechanism. The paper is organized as  follows. In Section 2, the preliminaries are discussed. In Section 3, a  brief review of related works is presented while the proposed network  interface architectures and the brilliant memory controller are presented  in Section 4 and 5, respectively. The experimental results are discussed  in Section 6 with the summary and conclusion given in the last section.  PE PE PE NI R NI R NI R PE PE NI R NI R NI R PE PE NI R NI R NI R PE PE Network Interface (NI) Input  PU PE RU DU Input  Router (R) Arbiter Crossbar Routing Unit Output Output Output Output Output Input Input Input Input Input R o u t e r Fig. 1. Tile-based 2D-Mesh topology.  2. PRELIMINARIES  2.1. NETWORK-ON-CHIP ARCHITECTURE  A 2D-mesh NoC based system is shown in Fig. 1. NoC consists of  Routers (R), Processing Elements (PE), and Network Interfaces (NI).  PEs may be intellectual property (IP) blocks or embedded memories.  Each core is connected to the corresponding router port using the  network interface. To be compatible with existing transaction-based IPcores, we use the AMBA AXI protocol. AMBA AXI is an interfacing  protocol, having advanced functions such as a multiple outstanding  address function and data interleaving function  [6]. AXI, providing  such advanced functions, can be implemented on NoCs as an interface  protocol between each PE and router to avoid the structural limitations  in SoCs due to the bus architecture. The protocol can achieve very high  speed of data transmission between PEs  [6]. In the AXI transactionbased model  [6] [9], IP cores can be classified as master (active) and  slave (passive) IP cores  [10] [18]. Master IP cores initiate transactions  by issuing read and write requests and one or more slaves (memories)  receive and execute each request. Subsequently, a response issued by a  slave can be either an acknowledgment (corresponding to the write  request) or data (corresponding to the read request)  [10]. The AXI  protocol provides a “transaction ID” field assigned to each transaction.  Transactions from the same master IP core, but with different IDs have  no ordering restriction while transactions with the same ID must be  completed in order. Thus, a reordering mechanism in the network  interface is needed to afford this ordering requirement  [6][7] [14]. The  network interface lies between a PE and the corresponding attached  router. This unit forms the foundation of the generic nature of the  architecture as it prevents the PEs from directly interacting with the rest  of the network components in the NoC. A generic network interface  architecture is shown in Fig. 1.          Memory Address  Fig. 2. High-level structure of an SDRAM.  The network interface consists of input buffers (forward and  required row should to be opened and accessed using the activation and  reverse directions), a Packetizer Unit (PU), a Depacketizer Unit (DU),  read/write commands. The row conflict has the highest bank access  and a Reorder Unit (RU). A data burst coming from a PE is latched into  latency (tRP +tRCD +tCL). If the bank is closed (precharged) or there is no  the input buffer of the corresponding network interface. PU is  row in the row buffer then a row empty occurs. An activation command  configured to packetize the burst data stored in the input buffer and  should be issued to open the row followed by read or write  transfer the packet to the router. Similarly, data packets coming from  command(s). The bank access latency of this case is tRCD +tCL.   the router are latched into the input buffer located in the reverse path.  DU is configured to restore original data format, required for the PE,  from the packet provided by the router. The RU performs a packet  reordering to meet the in-order requirement of each PE.   2.2.1. MEMORY ACCESS SCHEDULING   The memory controller lies between processors and the SDRAM to  generate the required commands for each request and schedules them  on the SDRAM buses. The memory controller consists of a request  table, request buffers, and a memory access scheduler. A request table  is used to store the state of each memory request, e.g. valid, address,  read/write, header pointer to the data buffer and any additional state  necessary for memory scheduling. The data of outstanding requests are  stored in read and write buffers. The read and write buffers (request  buffers) are implemented as linked list  [12] [13]. Each memory request  (read and write) allocates an entry in its respective buffer until the  request is completely serviced. Among all pending memory requests,  based on the state of the DRAM banks and the timing constraints of the  DRAM, the memory scheduler decides which DRAM command should  be issued. The average memory access latency and memory bandwidth  utilization can be reduced and improved, respectively if an efficient  memory scheduler is employed  [11] [19] [20]. Fig. 3 reveals how the  memory access scheduling affects the performance. As shown in the  figure, the sequence of four memory requests is considered. Request 1  and 3 are row empties, and request 2 and 4 are row conflicts. Timing  constraints of a DDR2-512MB used as example throughout this paper  is 2-2-2 (tRP-tRCD-tCL)  [21]. As depicted in Fig. 3(a), if the controller  schedules the memory requests in order, it will take 22 memory cycles  to complete them. In Fig. 3(b) the same four requests are scheduled out  of order. As can be seen, request 4 is scheduled before request 2 and 3  to turn request 4 from a row conflict to a row hit. In addition, request 3  is pipelined after request 1, called bank interleaving, since it has the  different bank address from the bank address of request 1. As a result,  only 14 memory cycles are needed to complete the four requests. In  sum, how the memory scheduler can improve the memory performance  has been shown by this example where the memory utilization of the in  order scheduler and the out of order are 4(data)/22(cycle) = 18% and  4/14= 29%, respectively. In this work, we presented a brilliant memory  controller that is efficiently integrated into the proposed network  interface to improve the memory utilization and reduce both memory  and network latencies. The idea and the implementation details of the  proposed architecture are described in Section 5.  2.2. SDRAM STRUCTURE  SDRAM is designed to provide high memory depth and bandwidth.  Fig. 2 shows a simplified three dimensional architecture of an SDRAM  memory chip with the dimensions of bank, row, and column   [11] [19] [20]. An SDRAM chip is composed of multiple independent  memory banks such that memory requests to different banks can be  serviced in parallel. That is, a benefit of a multibank architecture is that  commands to different banks can be pipelined. Each bank is formed as  a two dimensional array of DRAM cells that are accessed an entire row  at a time. Thus, a location in the DRAM is identified by an address  consisting of bank, row, and column fields. A complete SDRAM access  may require three commands (transactions) in addition to the data  transfer: bank precharge,  row activation, and column access  (read/write). A bank precharge charges and prepares the bank, while a  row-activation command (with the bank and row address) is used to  copy all data in the selected row into the row buffer, i.e. sense  amplifier. The row buffer serves as a cache to reduce the latency of  subsequent accesses to that row. Once a row is in the row buffer, then  column commands (read/write) can be issued to read/write data  from/into the memory addresses (columns) contained in the row. To  prepare the bank for a next row activation after completing the column  accesses, the cached row must be written back to the bank memory  array by the precharge command  [11]. Also, the timing constraints  associated with bank precharge, row activation, and column access are  tRP, tRCD, and tCL respectively  [11] [19] [21]. Since the latency of a  memory request depends on whether the requested row is in the row  buffer of the bank or not, a memory request could be a row hit, row  conflict or row empty with different latencies  [22]. A row hit occurs  when a request is accessing the row currently in the row buffer and only  a read or a write command is needed. It has the lowest bank access  latency (tCL) as only a column access is required. A row conflict occurs  when the access is to a row different from the one currently in the row  buffer. The contents of the row buffer first need to be written back into  the memory array using the precharge command. Afterward, the  100   (a) (b) Request 1 Request 2 Request 3 Request 4 Data Request 1 Request 2 Request 3 Request 4 Data 2 cycles R1 C1 P2 R2 C2 R3 C3 D1 D2 22 cycles R4 C4 P4 D3 D4 R1 C1 P2 R2 C2 R3 C3 C4 D1 D4 D3 D2 14 cycles Request 1: Bank 0, Row 0 Request 2: Bank 0, Row 1 Request 3: Bank 1, Row 0 Request 4: Bank 0, Row 0 P R C D Bank Precharge Row Activation Column Access Data 3. RELATED WORK  Fig. 3. Memory access scheduling of four memory requests with (a) in order and (b) with out of order access scheduling.  formed as a group to be issued sequentially, i.e. as a burst. Increasing  the row hit rate and maximizing the memory data bus utilization are the  major design goals of burst scheduling. The core-aware memory  scheduler revealed that it is reasonable to schedule the requests by  taking into consideration the source of the requests because the requests  from the same source exhibit better locality  [20]. In  [19] the authors  introduced an SDRAM-aware router to send one of the competing  packets toward an SDRAM using a priority-based arbitration. An  adaptive history-based memory scheduler which tracks the access  patterns of recently scheduled accesses and selects memory accesses  matching the pattern of requests is proposed in  [24] and  [25]. As  network-on-chips are strongly emerging as a communication platform  for chip-multiprocessors, the major limitation of presented memory  scheduling mechanisms is that none of them did take the order of the  memory requests into consideration. As discussed earlier, requests with  the same transaction ID from the same master must be completed (turn  back) in order. While the requests would be issued out-of-order in  memories (slave-sides), the average network latency might be increased  significantly due to the out-of-order mechanism in master sides.  Therefore, it is necessary to consider the order of memory requests for  making an optimal memory scheduling.   The major contribution of this paper is to propose a novel network  interface architecture within a dynamic buffer allocation mechanism for  the reorder buffer to increase the utilization and overall performance.  That is, using the dynamic buffer allocation to get more free slots in the  reorder buffer may lead more messages to be entered to the network.  On top of that, an efficient memory scheduler mechanism based on the  order of requests is introduced and integrated in our network interface  to diminish both the memory and network latencies.   Due to the fact that most of the recent published researches have  focused on the design and description of NoC architectures, there has  been relatively little attention to network interface designs particularly  when supporting out-of-order mechanism. The authors in  [9] present  ideas of transaction ID renaming and distributed soft arbitration in the  context of distributed shared memories. In such a system, because of  using a global synchronization in the on-chip network, the performance  might be degraded and the cost of hardware overhead for the on-chip  network is too high. In addition, the implementation of ID renaming  and reorder buffer can suffer from low resource utilization. This idea  has been improved in  [14] by moving reorder buffer resources from the  network interface into network routers. In spite of increasing the  resource utilization, the delay of release packets recalling data from  distributed reordering buffer can significantly degrade the performance  when the size of the network increases  [14]. Moreover, the proposed  architecture is restricted to deterministic routing algorithms and thus, it  is not a suitable method for an adaptive routing. However, neither  [9]  nor  [14] has presented a micro-architecture of the network interface. An  efficient on-chip network  interface supporting shared memory  abstraction and flexible network configuration  is presented by  Radulescu et al  [10]. The proposed architecture has the advantage of  improving reuse of IP cores, and offers ordering messages via channel  implementation. Nevertheless, the performance is penalized because of  increasing latency, and besides, the packets are routed on the same path  in NoC, which forces the routers to use the deterministic routing. Yang  et al proposed NISAR  [8], a network interface architecture using the  AXI protocol capable of packet reordering based on a look up table; but  NISAR uses a statically partitioned reorder buffer, thereby it has a  simple control logic but suffers from low buffer utilization in different  traffic patterns. In addition, NISAR does not support burst transactions,  whereas burst type should be handled by the network interface.   Regarding the memory scheduler, several memory scheduling  mechanisms were presented to improve the memory utilization and to  reduce the memory latency. The key idea of these mechanisms is on the  scheduler for reordering memory accesses. The memory access  scheduler proposed in  [11] reorders memory accesses to achieve high  bandwidth and low average latency. In this scheme, called bank-first  scheduling, memory accesses to different banks are issued before those  to the same bank. Shao et al.  [23] proposed the burst scheduling  mechanism based on the row-first scheduling scheme. In this scheme,  memory requests that might access the same row within a bank are  4. PROPOSED NETWORK INTERFACE ARCHITECTURE  Since IP cores are classified into masters and slaves, the network  interface is also divided into the master network interface (Fig. 4) and  slave network interface (Fig. 5). Both network interfaces are partitioned  into two paths: forward and reverse. The forward path transmits the  AXI transactions received from an IP core to a router; and the reverse  path receives the packets from the router and converts them back to  AXI transactions. The proposed network interfaces for both master and  slave sides are described in detail as follows.  4.1. MASTER-SIDE NETWORK INTERFACE  As shown in Fig. 4, the forward path of the master network  interface transferring requests to the network is composed of an AXI101   Fig. 4. Master-side network interface architecture. Queue, a Packetizer unit, and a Reorder unit, while the reverse path,  receiving the responses from the network, is composed by a PacketQueue, a Depacketizer unit, and the Reorder unit. The Reorder unit is a  shared module between the forward and reverse paths.  AXI-Queue: the AXI master transmits write address, write data, or read  address to the network interface through channels. The AXI-Queue unit  performs the arbitration between write and read transaction channels  and stores requests in either write or read request buffer. The request  messages will be sent to the packetizer unit if admitted by the reorder  unit, and on top of that a sequence number for each request should be  prepared by the reorder unit after the admittance.  Packetizer: it is configured to convert incoming messages from the  AXI-Queue unit into header and data flits, and delivers the produced  flits to the router. Since a message is composed of several parts, the  data is stored in the data buffer and the rest of the message is loaded in  corresponding registers of the header builder unit. After the mapping  unit converts the AXI address into a network address by using an  address decoder, based on the request information loaded on relative  registers and the sequence number provided by the reorder buffer, the  header of the packet can be assembled. Afterward, the flit controller  wraps up the packet for convenient transmission.  Packet-Queue: this unit receives packets from the router; and  according to the decision of the reorder unit a packet is delivered to the  depacketizer unit or reorder buffer. In fact, when a new packet arrives,  the sequence number and transaction ID of the packet will be sent to the  reorder unit. Based on the decision of the reorder unit, if the packet is  out of order, it is transmitted to the reorder buffer, and otherwise it will  be delivered to the depacketizer unit directly.   Depacketizer: the main functionality of the Depacketizer unit is to  restore packets coming from either the packet queue unit or reorder  buffer into the original data format of the AXI master core.  Reorder Unit: it is the most influential part of the network interface  including a Status-Register, a Status-Table, a Reorder Buffer, and a  Reorder-Table. In the forward path, preparing the sequence number for  corresponding transaction ID, and avoiding overflow of the reorder  buffer by the admittance mechanism are provided by this unit. On the  other side, in the reverse path, this unit determines where the  outstanding packets from the packet queue should be transmitted  (reorder buffer or depacketizer), and when the packets in the reorder  buffer could be released to the depacketizer unit.   Status-Register and Status-Table: Status-Register (S_Reg) is an n-bit  register where each bit corresponds to one of the AXI transaction IDs.  As depicted in Fig. 6, this register records whether there are one or  more messages with the same transaction ID being issued or not. To  record the state of the outstanding messages, Status-Table (S_Table) is  adopted. Each entry of this table is considered for messages with the  Fig. 5. Slave-side network interface architecture. same transaction ID, and includes valid tag (v), Transaction ID (T-ID),  Number of outstanding Message (N-M) as well as the Expecting  Sequence number (E-S). The register and table might be updated in  both forward and reverse paths described as follows. In the forward  path, when the first message of each transaction ID requests for an  admittance from  the reorder unit  to enter  the network,  the  corresponding bit in the status register goes high (Procedure A: line 1,  Fig. 6(a)). The sequence number (Seq-Num) is produced by the reorder  unit, if the admittance is given. This value, indicating the order of the  messages within the transaction ID, is equal to zero for the first  message of each transaction ID (Procedure A: line 2). “ReservedSize”  keeps the required space of all outstanding transactions in the network.  Indeed, this register reserves the number of buffer slots required by  outstanding messages of different transaction IDs. In order to prevent  overflow of the reorder buffer, the reorder unit compares the new  message size with the free space of the reorder buffer. If the required  space is available, the message will be admitted and the required space  in the reorder buffer must be reserved (Procedure A, line 3). An  available (free) row in the status table will be initiated by procedure B,  when the second request of a transaction ID is admitted. For the rest of  the admitted requests of the transaction ID, the procedure C should be  executed as the sequence number is obtained by adding N-M and E-S  values. Also, the number of outstanding message (N-M) is increased by  +1, and the required space in the reorder buffer must is reserved by  procedure C. Note that E-S indicates the next response sequence  number of the corresponding transaction ID that should be delivered to  the depacketizer unit.  Procedure A:  1  S_Reg(T_ID)  2  SeqNum  3  ReservedSize  <= ‘1’;  <= (others =>’0’);  <= ReservedSize + NewMsgSize;  Procedure B:  1  S_Table(FreeRow)(v)   <= ’1’;  2  S_Table(FreeRow)(T_ID) <= Tran_ID;  3  S_Table(FreeRow)(N_M)  <= “0010”;  4  S_Table(FreeRow)(E_S)  <= (others =>’0’);  5  SeqNum     <= “001”;  6  ReservedSize    <= ReservedSize +         NewMsgSize;  Procedure C:  1  SeqNum   <= S_Table(FindRow)(N_M) +       S_Table(FindRow)(E_S);  2  S_Table(FindRow)(N_M) <= S_Table(FindRow)(N_M) + 1;   3  ReservedSize   <= ReservedSize + NewMsgSize;  102         Fig. 6. Status-Register and StatusTable of Reorder Unit. Fig. 7. Dynamic buffer allocation In the reverse path, the transaction ID and sequence number of the  FreeRow in the reorder table; and the last operation in E updates the  arriving response packet (message) are sent to the reorder unit to find  pointer to point to the available slot in the reorder buffer.  the related row in the status table according to the transaction ID (TID). Ifthe sequence number of incoming packet is equal to E-S value,  the packet is an expected packet (in-order) and should be delivered to  the depacketizer unit which releases the occupied buffer space;  thereafter, E-S and N-T values will be increased by +1 and -1,  respectively (Procedure D). If N-M value reaches zero, the transaction  will be terminated by resetting the valid bit for both status register and  status table. However, the packet is out-of-order and should be  delivered to the reorder buffer, if the sequence number of the packet is  not equal to E-S. Additionally, only one message with the given  transaction ID should have been sent to the network, if the given  transaction ID is not matched in the status table, thereby only the  corresponding bit in the status register will be reset.  Procedure D:  ReorderBuf(Current_Free_Slot)(V)    <= ‘1’;  ReorderBuf(Current_Free_Slot)(Data)  <= flit;  ReorderBuf(Current_Free_Slot)(P)    <= Next_Free_Slot;  Current_Free_Slot             <= Next_Free_Slot;  Procedure F:  Procedure E:   ReorderTable(FreeRow)(V)   <= ‘1’;  ReorderTable(FreeRow)(T-ID) <= HeaderFlit(TranID);  ReorderTable(FreeRow)(S-N) <= HeaderFlit(SeqNum);  ReorderTable(FreeRow)(P)   <= Current_Free_Slot;  1 S_Table(FindRow)(N_M) <= S_Table(FindRow)(N_M)-1;   2 S_Table(FindRow)(E_S) <= S_Table(FindRow)(E_S)+1;   3 ReservedSize          <= ReservedSize–                          ReceivedMsgSize;  The N-M value can be used as a ranking metric of packets. It  represents the number of packets with the same transaction ID as the  current packet which traveling inside the network once the packet is  injected. In fact, the N-M value indicates the order of packet to return  back. Therefore, a packet with a greater N-M value has lower priority to  be sent back to the corresponding master core and vice versa.  Reorder-Table and Reorder-Buffer: As shown in Fig. 7, each row of  the reorder table corresponds to an out-of-order packet stored in the  reorder buffer. This table includes the valid tag (v), the transaction ID  (T-ID), the sequence number (S-N) as well as the head pointer (P). In  the reorder buffer, the flits of each packet are maintained by a linked  list structure providing high resource efficiency with little hardware  overhead. On top of that, the goal of using the shared reorder buffer is  to support variable packet size and improve the buffer utilization which  can also increase the performance by feeding more packets into the  network. Fig. 7 exhibits a pointer field adopted to indicate the next flit  position in the reorder buffer. Using the proposed structure in Fig. 7,  each out-of-order packet updates the reorder table and reorder buffer  according to the procedure E, and F. The first three operations in the  procedure E, stores the transaction ID and sequence number from the  header flit of the out-of-order packet to the available slot indicated by  103 The procedure F is intended to store the incoming flits into the  reorder buffer. While Current_Free_Slot shows the current free location  in the reorder buffer in order to store the current flit, Next_Free_Slot  returns an available slot for the next flit. By repeating the operations in  the procedure F, whole of the payload flits will be stored in the reorder  buffer. The tail flit can be determined by extracting header flit  information. Whenever an in-order packet delivered to the depacketizer  unit, the depacketizer controller checks the reorder table for the validity  of any stored packet with the same transaction ID and next sequence  number. If so, the stored packet will be released from the reorder unit to  the depacketizer unit.  4.2. SLAVE-SIDE NETWORK INTERFACE  A slave IP core cannot operate independently. It receives requests  from master cores and responds to them. Hence, using reordering  mechanism in the slave network interface is completely meaningless.  But to avoid losing the order of header information (transaction ID,  sequence number, and etc) carried by arriving requests, a FIFO has  been considered. After processing a request in a slave core, the response  packet should be created by the packetizer. As can be seen from Fig. 5,  to generate the response packet, after the header content of the  corresponding request is invoked from the FIFO, and some parameters  of the header (destination address, and packet size, and etc) are  modified by the adapter, the response packet will be formed. However,  the components of the slave-side interface in both forward and reverse  paths are almost similar to the master-side interface components, except  the reorder unit.      Fig. 8. The proposed memory controller integrated in the slave-side network interface.  P(i) : priority of the i(th) request in the queue.  SN   : sequence number of a new request.  RA(i): row address of i(th) request.  CRA  : current row address issued prior.  ----------------  Process(input_queue)  Begin  For ‘i:1 to number of requests in input queue’ loop   If Req is a new packet then      P(i) <= SN;      Else      P(i) <= P(i)+1;        End if;   End loop;  End process;  ----------------  Process(arbiter)  Begin   MaxValue1 <=0; select1 <=0;       MaxValue2 <=0; select2 <=0;  For ‘i:1 to all requests in input queue’ loop       If RA(i)= CRA then      If p(i)>= MaxValue1 then        select1   <= i;        MaxValue1  <= p(i);      End if;    Else      If p(i)>= MaxValue2 then        select2   <= i;        MaxValue2  <= p(i);      End if;      End if;  End loop;  If select1 /= 0 then      select <= select1;  Else      select <= select2;  End process;  Fig. 9. Pseudo VHDL code of the arbiter in the memory controller.  5. ORDER SENSITIVE MEMORY SCHEDULER  The architecture of the proposed memory controller, dubbed OS  from Order Sensitive, is depicted in Fig. 8. As illustrated in the figure,  the proposed memory controller is efficiently integrated in the slaveside network interface. Requests after arriving to the network interface  on the edge of the network are stored in the respective queues based on  their target banks. The data associated with write requests are stored in  the write queue. The queues are implemented as the linked list structure  which has been described in subsection 4.1. Depending on the sequence  number, new received requests in each bank queue obtain a priority  value to access the memory. Once a new request enters a queue, the  process input_queue, shown in Fig. 9, updates the priority value of each  request in the queue. The packet’s sequence number of received request  is assigned as a priority value for this request. In addition, to prevent  starvation, the priority values of existing requests in the queue at every  input_queue event will be increased. As mentioned earlier, each bank  arbiter selects a request from the queue with the highest priority value  based on the bank timing constraints as the first level of scheduling  procedure. Since the row-first policy has better memory utilization in  comparison with the other bank arbitration policies, the bank arbiters of  the presented memory controller also takes advantage of the row-first  policy. The bank arbitration policy in our memory controller is shown  in Fig. 9. Whenever the arbiter process is activated, it tries to find a  request which is a row hit and has a higher priority value. If there are  not any row hits, the bank arbiter selects the highest priority request  which is a row conflict from the queue and issues the SDRAM  commands to service the selected request. In the second level of the  scheduling procedure, at each memory cycle the memory scheduler  decides which request from all bank arbiters should be issued. To  simplify  the hardware  implementation and provide  the bank  interleaving, round robin mechanism is utilized by the memory  scheduler.    6. EXPERIMENTAL RESULTS    In this section, we compare the proposed network interface  architecture with the baseline architecture by measuring the average  network latency under different traffic patterns. Hence, a 2D NoC  simulator is implemented with VHDL to assess the efficiency of the  proposed method. The simulator models all major components of the  NoC such as network interface, routers, and wires.  6.1. SYSTEM CONFIGURATION  In this work, we use a 25-node (5×5) 2D mesh on-chip network  configuration for  the entire architecture. In  this configuration,  illustrated in Fig. 10, out of 25 nodes, ten nodes are assumed to be  processors (master cores, connected by master NIs) and other fifteen  104                                             nodes are memories (slave cores, connected by slave NIs). For  convenience, we used MS (Master/Slave NI) and MS-OS (Master/Slave  NI where slave NIs uses Order Sensitive structure) to represent the  proposed architectures. The processors are 32b AXI and the memories,  specified in subsection 2.2.1, are DDR2-512MB (tRP-tRCD-tCL=2-2-2,  32b, 4 banks)  [21].  We adopt a commercial memory controller and  memory physical interface, DDR2SPA module from gaisler ip-cores   [26]. In addition, the on-chip network considered for experiment is  formed by a typical state-of-the-art router structure including input  buffers, a VC (Virtual Channel) allocator, a routing unit, a switch  allocator and a crossbar. Each router has 5 input/output ports, and each  input port of the router has 2 VCs. Packets of different message types  (request and response) are assigned to corresponding VCs to avoid  message dependency deadlock  [15]. The arbitration scheme of the  switch allocator in the typical router structure is round-robin.  Fig. 10. 5x5 NoC layout.  The array size, routing algorithm, link width, number of VCs,  buffer depth of each VC, and traffic type are the other parameters  which must be specified for the simulator. The routers adopt the XY   [16] [17] routing and utilize wormhole switching. For all routers, the  data width (flit) was set to 32 bits, and the buffer depth of each VC is 5  flits. The baseline architecture  [8] [9] (with fixed packet length) uses 1  flit for messages related to read requests and write responses, and 5 flits  for data messages, representative of read responses and write requests;  the size of read request messages typically depends on the network size  and memory capacity of the configured system. As discussed in the  Section 4, the message size of the proposed mechanism is variable and  depends on the request/response length produced by a master/slave  core. As the performance metric, we use latency defined as the number  of cycles between the initiation of a request operation issued by a  master (processor) and the time when the response is completely  delivered to the master from a slave (memory). The request rate is  defined as the ratio of the successful read/write request injections into  the network interface over the total number of injection attempts. All  the cores and routers are assumed to operate at 2GHz. For fair  comparison, we keep  the bisection bandwidth constant  in all  configurations. We also set the size of the reorder buffer to 48 words,  able to embed 6 outstanding requests with burst size of 8. All memories  (slave cores) can be accessed simultaneously by each master core with  continuously generating memory requests. Furthermore, the size of  each queue (and FIFO) is set to 8×32 bits.  6.2. PERFORMANCE EVALUATION  To evaluate the performance of the proposed schemes, the uniform  and non-uniform synthetic traffic patterns have been considered  separately for the specified configuration. These workloads provide  insight into the strengths and weakness of the different buffer  management mechanisms in the interconnection networks, and we  expect applications stand between these two synthetic traffic patterns.  The random traffic represents the most generic case, where each  processor sends in-order read/write requests to memories with the  uniform probability. Hence, the memories and request type (read or  write) are selected randomly. Eight burst sizes, among 1 to 8, are  stochastically chosen regarding the data length of the request. In the  non-uniform mode, the traffic consists of 70% local requests, where the  destination memory is one hop away from the master core, and the rest  30% traffic is uniformly distributed to the non-local memories. Fig. 11  and Fig. 12 show the simulation results under uniform and non-uniform  traffic models,  respectively. The proposed network  interface  architecture has been compared with the baseline. As demonstrated in  both figures, compared with the baseline architecture the presented  architecture reduces the average latency when the request rate increases  under uniform and non-uniform traffic models. One of the foremost  reasons of such an improvement is that because the size of packets is  not fixed and depends on the request and response lengths, the resource  utilization is high and thus, the latency is reduced. Another subtle  reason for improving the performance is that getting more free slots in  the reorder buffer allows more messages to enter the network.  Baseline  MS   MS - OS  ) e l c y c ( y c n e t a L e g a r e v A 350 300 250 200 150 100 50 0 0 0.2 0.4 0.6 0.8 1 Request Rate (fraction of capacity) Fig. 11. Performance evaluation of both configurations under uniform  traffic model. Baseline  MS   MS - OS  ) e l c y c ( y c n e t a L e g a r e v A 350 300 250 200 150 100 50 0 0 0.2 0.4 0.6 0.8 1 Request Rate (fraction of capacity) Fig. 12. Performance evaluation of both configurations under nonuniform traffic model.  105                 The average utilization of memories and average latency of the  network and memories have been computed near saturation point (0.6)  under the uniform traffic profile. As a result of using OS technique,  compared with the baseline architecture, the average utilization of  memories is improved by 22%. The average memory latency and  average network latency are reduced by 19%, and 12%, respectively.  6.3. HARDWARE OVERHEAD  The network interfaces were synthesized by Synopsys Design  Compiler using the UMC 0.09μm technology. In addition to the  aforementioned configuration of the network interface, the tran_id and  seq_id were set to 4-bit and 3-bit respectively. The layout areas and  power consumptions of the master-side, slave-side, and OS interfaces  are listed in Table 1. Since all queues (and FIFOs) are equal in the size,  it will not affect the comparison. Also, comparing the area cost of the  baseline model for each proposed network interface indicates that the  hardware overheads of implementing the proposed schemes are less  than 0.5%. Furthermore, for the slave-side interface within memory  controller, since each of the memories utilized in this work, has 4  banks, four bank queues have been implemented in the memory  controller.  Table 1. Hardware implementation details.  Area   Gates (#)  18218  32125  33218  (µm2)  42848  75559  80848  NI  Slave-side  Master-side  Slave-side with OS  Power (mW)  0.274  0.441  0.486  7. SUMMARY AND CONCLUSION  To increase the memory bandwidth, accessing multiple memories  simultaneously is necessitated, but it requires a reordering mechanism  to cope with the deadline. In this work, we presented a high  performance dynamic reordering mechanism integrated in the network  interface to improve the resource utilization, and overall on-chip  network performance. In addition to the resource utilization of the  network interface and on-chip network, the utilization of memories  considerably affects the network latency. Therefore, we have developed  a novel scheduling method for the modern SDRAM memories and  integrated in the network interface such that the network and memory  latencies reduced effectively  in comparison with  the baseline  architecture. The micro-architectures of  the proposed network  interfaces which are compatible with the AMBA AXI protocol have  been presented. A cycle-accurate simulator was used to evaluate the  efficiency of the proposed architecture. Under both uniform and nonuniform traffic models, in high traffic load, the proposed architecture  had lower average communication delay in comparison with the  baseline architecture.  "
Back Suction - Service Guarantees for Latency-Sensitive On-chip Networks.,"Networks-on-chip for future many-core processor platforms face an increasing diversity of traffic requirements, ranging from streaming traffic with real-time requirements to bursty latency-sensitive best-effort traffic from general-purpose processors with caches. In this paper, we propose Back Suction, a novel flow-control scheme to implement quality-of-service. Traffic with service guarantees is selectively prioritized upon low buffer occupancy of downstream routers. As a result, best-effort traffic is preferred for an improved latency as long as guaranteed service traffic makes sufficient progress. We present a formal analysis and an experimental evaluation of the Back Suction scheme showing improved latency of best effort traffic when compared to current approaches even under formal service guarantees for streaming traffic.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip Back Suction: Service Guarantees for Latency-Sensitive On-Chip Networks Jonas Diemer1 and Rolf Ernst Institute of Computer and Network Engineering Technische Universit ¨at Braunschweig, Germany Email: {diemer, ernst}@ida.ing.tu-bs.de 1 This work is supported by Intel Corporation. Abstract—Networks-on-chip for future many-core processor platforms face an increasing diversity of trafﬁc requirements, ranging from streaming trafﬁc with real-time requirements to bursty latency-sensitive best-effort trafﬁc from general-purpose processors with caches. In this paper, we propose Back Suction, a novel ﬂow-control scheme to implement quality-of-service. Trafﬁc with service guarantees is selectively prioritized upon low buffer occupancy of downstream routers. As a result, best-effort trafﬁc is preferred for an improved latency as long as guaranteed service trafﬁc makes sufﬁcient progress. We present a formal analysis and an experimental evaluation of the Back Suction scheme showing improved latency of best effort trafﬁc when compared to current approaches even under formal service guarantees for streaming trafﬁc. I . IN TRODUC T ION AND BACKGROUND Future multi-processor systems on chip (MPSoC) will rely on networks-on-chip as a scalable and modular interconnect, e.g. [1], [2], [3]. These MPSoCs will be used in multifunctional consumer and mobile devices, where they will be executing different application classes in parallel, such as streaming multimedia and general-purpose applications. Trafﬁc originating from streaming applications imposes a high load and often requires a guaranteed throughput (GT), but at the same time can tolerate high latencies due to its predictable access patterns. On the other hand, trafﬁc from general-purpose applications running on processors with caches only requires best-effort (BE) service, but is very sensitive to latency (as has been shown e.g. in [4]). Furthermore, the characteristics of trafﬁc from general-purpose applications are usually unknown, and accesses often occur in unpredictable bursts. An efﬁcient co-execution of such diverse application types is crucial to the success of these architectures. As an example, consider Fig.1 which shows a data-ﬂow graph of a complex real-time noise reduction application [5] consisting of 9 communicating tasks. This application may be executed on a many-core processor of a future mobile device to denoise recorded video in real-time. When mapped to multiple cores, its communication overlaps with that of best-effort applications. This requires predictable NoC management providing performance guarantees for realtime streaming applications with minimum latency impact on general-purpose applications. In this paper, we present Back Suction, a novel ﬂow control scheme to implement service guarantees. It uses available buffer space to allow progress of GT trafﬁc during idle times. In return, BE trafﬁc is prioritized over GT as long as the GT buffers are sufﬁciently ﬁlled. To maintain throughput guarantees, a low buffer occupancy is signaled to the upstream router resulting in prioritization of GT trafﬁc. This propagates Fig. 1. Example of a real-time noise reduction application to be mapped to an MPSoC along with best-effort applications upstream towards the source as long as GT trafﬁc does not receive enough idle slots. From this perspective, it works similar to the “back pressure” ﬂow control which avoids buffer overﬂows by preventing upstream routers from sending ﬂits when the local ﬂit buffers are (nearly) full. Hence, we name this ﬂow control “Back Suction” because routers can exert a suction against the direction of trafﬁc ﬂow when the local GT buffer will run out of ﬂits soon. Note that although back pressure and back suction use the same mechanism, i.e. signaling of buffer occupancy upstream, their purposes differ signiﬁcantly (end-to-end buffer management vs. quality-ofservice) so they should be used in conjunction. After detailing the workings of Back Suction in Sec.II, we conduct a formal analysis in Sec.III to prove that throughput guarantees are possible with only few local buffers. Additional buffer space is used to reduce the chance of low buffer occupancy during temporary overload and hence improves the BE latency, as we show in the experimental evaluation in Sec.IV. A. Baseline Architecture Although Back Suction is not speciﬁc to a certain network architecture, we assume a mesh network as a baseline, where every router is connected to four neighbors and one or two clients, such as processing elements and caches. Routers use wormhole ﬂow control, i.e. buffer management and arbitration are performed on equally sized ﬂits. Packets are comprised of a head ﬂit, multiple body ﬂits and a tail ﬂit. At every input, ﬂits of different packets are buffered in separate virtual channels (VC), which prevents head-of-line blocking. We use a distributed XY-routing scheme, although Back Suction can be used with any routing scheme that allows static routing of speciﬁc streams. Our baseline router has a four-stage pipeline and provides no special support for quality-of-service, treating all trafﬁc equal which provides hardly any timing guarantees. We do not consider speculation and lookahead mechanisms 978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.38 155 like [6], which just optimize the average latency. However, Back Suction can be used together with such techniques. In fact, as it reduces the contention latency of BE trafﬁc, the relative effect of our approach will be even larger when combined with low-latency routers. B. Related Work A plethora of quality-of-service (QoS) mechanisms exists for packet-switched networks-on-chip, which can be categorized by how they enforce service guarantees. One group of techniques relies on pre-allocation of time-slots for trafﬁc streams with guaranteed service requirements, e.g. [7], [8]. These have a low implementation overhead in the routers, but the contention-free allocation of time slots can be complex. Also, the static scheduling results in low resource utilization and/or high service latencies, especially for BE trafﬁc which has to wait for all other trafﬁc and may only use idle time slots. This is not optimal for systems where latency-sensitive BE trafﬁc is important. Other QoS mechanisms work by reservation of virtual channels and dynamic scheduling, e.g. [9], [10], [11], [12], [13]. Here, priorities are used along with scheduling analysis or simulation to provide quality-ofservice. In the latter case, no 100% guarantees can be given. These schemes can react more ﬂexibly to trafﬁc requirements, but usually require more complex routers. Best-effort trafﬁc is usually assigned the lowest priority. In summary, most of the existing QoS mechanisms favor trafﬁc with service requirements over BE trafﬁc, allowing progress of BE trafﬁc only during idle times, which leads to a noticeable increase of the average BE latency. On the other hand, the reduced latency implied by prioritizing GT trafﬁc is of no immediate value, as it usually has large latency margins. [14] presents an approach to this problem which prioritizes BE trafﬁc for improved latency and relies on distributed token-bucket trafﬁc shaping (DTS) of BE trafﬁc on every router port to keep the average BE rate low enough to fulﬁll throughput guarantees. As a result, BE latencies improve considerably under the presence of GT trafﬁc. However, buffer requirements are proportional to the shaper’s bucket size and hence the granularity at which guarantees can be given. As a consequence, DTS requires large buffers for the GT trafﬁc to maintain service guarantees in reasonable scenarios. Furthermore, DTS relies on injection rate limitation for overlapping streams and hence can not allow a GT stream more than its requested throughput. We will evaluate Back Suction against DTS in Sec.IV. C. Real-Time Scheduling Analysis We will analyze Back Suction in Sec.III using real-time scheduling analysis [15]. Scheduling analysis assumes a set of trafﬁc ﬂows (or tasks) transferred (or processed) by a communication (or processing) resource. For the trafﬁc ﬂow, formal and conservative characteristics are assumed, such as the worst-case transferal time (or core execution time) required per communication entity (or task activation) and an event model. The event model consists of so-called arrival functions η+ (∆t) and η− (∆t), which describe the maximum and minimum number of events (communication entities or task activations) that can arrive in any given time interval ∆t. From these, distance functions δ− (n) and δ+ (n) can be derived, which describe the minimum and maximum time interval containing n events. Hence δ− (n) is equal to the smallest ∆t for which η+ (∆t) ≥ n. For worst-case analysis, we will use η+ (∆t) and δ− (n). The other two functions are used for best-case analysis which we do not cover. Using these task properties and knowledge about the scheduling mechanism, one can derive real-time properties such as the worst-case response time or the worst-case backlog, similarly to the Network Calculus [16] approach. Real-time analysis allows the computation of tighter worst-case bounds compared to Network Calculus by considering both the worst-case behavior of competing trafﬁc ﬂows and worst-case service governed by the scheduler simultaneously. I I . BACK SUCT ION Trafﬁc streams compete for two resources in the routers: virtual channels and link bandwidth. To offer guaranteed service in a NoC with wormhole ﬂow control, virtual channels must be reserved in advance because they may be blocked by other trafﬁc for a long time due to downstream blocking. Hence, a GT connection must be established by reserving a set of virtual channels from the source to the sink along a ﬁxed path. This is facilitated using packet concatenation, cf. Sec.II-B. With the virtual channels being exclusively reserved, the GT ﬂow only contends for the link bandwidth at the output port of each router. Here, the arbiter prioritizes GT trafﬁc whenever the buffer occupancy in the downstream neighbor is low, which is communicated upstream via back suction signals (one per VC). A. Operational Example The workings of Back Suction are illustrated in Fig.2, which shows two simpliﬁed routers R1 and R2 and a sink network interface (NI). For each router, one input and one output port are shown, as well as the crossbar which is controlled by an arbiter. The routers in the example have 2 virtual channels, of which the upper ones are used for BE trafﬁc, while the lower ones are allocated to a GT stream. The network interface can exert back suction to its upstream router which would lead to a prioritization of GT trafﬁc. A rate limiter regulates the back suction signal at the sink based on the guaranteed rate of the corresponding GT stream to prevent that GT trafﬁc is prioritized more often than allowed. This is implemented as a leaky bucket shaper. The sink is the only place where back suction can be created, routers only forward back suction upstream as necessary. For this, a Threshold (“Thr.”) module in every router monitors the buffer occupancy of the GT VC asserting a back suction signals to upstream router when the buffer occupancy is below the suction threshold (e.g. 2). This module also prevents GT ﬂits to be sent if no back suction is signaled from downstream and the local VC is not sufﬁciently ﬁlled (i.e. less than 3 ﬂits). Hence, GT ﬂits are allowed to make progress during idle times without seeing a suction signal only if this does not create “spurious” suction signals. In the ﬁgure, inactive signals are drawn as dotted arrows while active ones are drawn solid. Buffer slots containing a ﬂit are ﬁlled blue, the transfer of ﬂits is shown as bold red arrows. All ﬂits in the example are body ﬂits and we assume that the routers have a unit delay and react to a back suction immediately (no signaling delay). Every row in Fig.2 represents one step in time. At t = 0, all GT buffers are ﬁlled to or above their thresholds, so no back suction is exerted. R1 receives a BE ﬂit, but can not send a GT ﬂit at this time, because the local Threshold module signals that this would create spurious back suction, as the buffer occupancy is just 156 from R2. R2 can not send any GT ﬂits now because the buffer occupancy is at the threshold and the NI does not exert back suction and hence remains idle. At t = 2, R1, R2 and the NI receive BE ﬂits. At t = 3, R1 receives a GT ﬂit, which was sent in an idle slot (i.e. there were no BE ﬂits available upstream). At the same time, the sink asserts the back suction signal so R2 sends a GT ﬂit from its local buffer which will drop below the threshold in the next cycle. As a consequence, at t = 4 R2 signals back suction to R1 which in turn sends a GT ﬂit. The NI continues to assert the back suction signal and hence, R2 also sends a GT ﬂit. At t = 5, the rate limit at the NI becomes active again, which causes the back suction signal to be deasserted so R2 sends a BE ﬂit. However, the GT VC at R2 has not been ﬁlled up to the threshold yet, so R1 continues to send GT ﬂits. Note that R1 does not assert back suction yet because it served the previous suction request with ﬂits from above the threshold. Consequently, R1 may receive a BE ﬂit at this time. At t = 6, however, the GT VC in R1 has dropped below the threshold, so R1 asserts the back suction signal upstream and receives a GT ﬂit in return. The GT buffer in R2 has been ﬁlled again, so both R1 and R2 now send BE ﬂits. At t = 7, all GT buffers are at their thresholds, so no back suction signals are active anymore and only BE ﬂits are sent. The example has illustrated the key properties of back suction. BE trafﬁc is prioritized over GT when back suction is not asserted from the downstream router. GT trafﬁc can make progress during idle times (e.g. at t = 0) if the source produces trafﬁc fast enough. By exerting a back suction signal, the sink requests prioritization of GT trafﬁc and is hence guaranteed to be able to receive GT ﬂits from its neighboring router. Back suction requests are handled from the local VC buffer. Routers can themselves assert suction signals to upstream neighbors (R2 at t = 4) when their local buffer occupancy drops below a threshold. This threshold must be high enough so ﬂits arrive from upstream before the local buffers deplete. Back suction requests propagate towards the source and may be absorbed along the way if local buffers contain enough ﬂits from earlier GT progress during idle times (R1 at t = 4). The sink (and hence the GT stream) is guaranteed to be served GT ﬂits at a rate controlled by its rate limiter, e.g. c ﬂits every T cycles (leaky bucket). If source produces ﬂits fast enough and BE load is low, GT trafﬁc will make enough progress during idle times and is hence never prioritized. As a result, latency for BE trafﬁc improves, as it is less likely to be delayed by GT trafﬁc. GT trafﬁc may even receive more than its requested rate using idle bandwidth, providing an in-order “SuperGT” service like in [12]. Note that if the source produces ﬂits slower than the guaranteed rate, back suction remains active and GT trafﬁc will always be prioritized. This is equivalent to a classical prioritization scheme and can be used to facilitate trafﬁc with guaranteed latency (GL) requirements, which, will not be discussed in the remainder of the paper. So far, we have considered a single GT stream only. However, multiple GT streams may overlap, i.e. share an output link of a router. They will have their exclusive VCs and hence also use separate Back Suction signals, so they only compete for bandwidth when multiple suction signals are active simultaneously. We rely on symmetric round-robin scheduling between multiple GT streams. This way, all streams receive an equal share of the link bandwidth. To allow nonFig. 2. Back Suction operational example at the threshold and the downstream router R2 is not asserting back suction. In R2, there are 3 GT ﬂits available, so a GT ﬂit can be sent even though the downstream sink does not assert back suction due to its Rate Limit being active (which means it has received enough trafﬁc before). Hence, this GT ﬂit makes progress beyond its guaranteed service by using the otherwise idle link bandwidth. At t = 1, a BE ﬂit is available in R1 and is hence sent, as there is no back suction coming 157 symmetric guarantees, we perform a real-time scheduling analysis exploiting knowledge about the rate limitation of back suction (see Sec. III). Note that there is a “ramp-up phase” for new streams during which the corresponding VC buffers are still ﬁlling up and hence the rate limitation at the ejection node is not effective yet. In the scope of this paper, we limit the injection rate to prevent interference of different streams during this phase and leave a more efﬁcient solutions for future work. In the following subsections, we describe the architectural implementation of the back suction ﬂow control. B. VC Management A standard router implementation only allows ﬂits of one packet at a time in a VC because there is only one set of status registers per VC. Hence, to avoid inter-packet dependencies, a VC can only be arbitrated for a new packet after the tail ﬂit of the previous packet has been sent [17]. This creates “bubbles” between packets during which a given VC is unavailable. This is complex to handle in a formal analysis required for real-time guarantees, as it introduces circular dependencies between routers based on the distribution of tail ﬂits. We solve this problem by “packet concatenation” introduced in [14]. With this technique, we do not generate head or tail ﬂits for intermediate GT packets in the network interface. Instead, a new “delimiter” ﬂit type is introduced that marks the end of a packet so individual packets can be reconstructed at the receiving end. Delimiter ﬂits are treated as regular body ﬂits in the routers, i.e. they do not induce routing or VC allocation (like head ﬂits) nor do they free a VC (like tail ﬂits). Hence, packets of a GT connection are concatenated into a single huge worm. As a result, a VC remains allocated to the GT stream throughout its lifetime avoiding unnecessary routing and VC allocation stages. This allows dynamic allocation of VCs to GT streams without additional logic in the routers. As stated above, we reserve a VC for every GT stream throughout its lifetime which can become costly if many low-bandwidth streams overlap at a single router. This can be avoided by rerouting or merging of streams at the application level. Also, sharing of VCs among different streams can be implemented by cutting streams temporally into shorter-lived sub-streams of higher average bandwidth. However, real-time analysis must then be performed on sub-streams considering the individual ramp-up phases, which is beyond the scope of this paper. C. Arbitration The main part of Back Suction is implemented in the router’s output arbitration stage. Here, requests ri from VCs of all other inputs are processed along with their corresponding trafﬁc class pi to generate the corresponding grant signals gi which control the crossbar switch. The arbitration also considers the suction requests si from the corresponding downstream VCs. Figure 3 shows a block diagram of the arbitration logic for one output port. Requests are ﬁrst demultiplexed based on their trafﬁc class to be fed to the corresponding arbiter. For GT trafﬁc, we use a round-robin arbiter, which assigns single cycles (for one ﬂit transferal) to the requesters in circular order. For BE trafﬁc, a slightly different winner-takes-all arbiter is used, which maintains a grant until a request is no longer active, i.e. until all ﬂits of a packet have been sent. This improves average latency over round-robin, as packets are sent in one piece if possible [17]. The GT requests are qualiﬁed based on the state of the back suction signals in the Request Fig. 3. Router switch arbitration logic for Back Suction (r=request, p=trafﬁc class, s=suction request, g=grant) Qualiﬁer block, detailed in the lower part of Fig.3. GT requests whose corresponding suction signal is not asserted are masked out in case of suction signals from other VCs (any s). If no suction is asserted, all GT signals compete for idle slots. The grant multiplexer selects which arbiter will generate the grants based on the presence of BE requests and suction requests (grant BE). The internal priority state of the roundrobin arbiter is not updated when the BE arbiter is selected (grant BE). D. Implementation Overhead of Back Suction The area overhead of the mechanism presented in this paper is relatively low. Compared to a standard round-robin arbiter with no QoS, back suction requires an additional arbiter, corresponding de-/multiplexers, a request qualiﬁer and some OR-gates, cf. Fig.3. Hence, we estimate the arbiter to be about 3-4 times larger than a standard round-robin arbiter without QoS. Compared to a simple prioritized arbiter, the overhead is about 1.5 times, as only the request qualiﬁer is added. The minimum buffer requirements for Back Suction are also very low, see Sec.III. Additional buffer space can be used to efﬁciently improve the latency of BE trafﬁc, but is not required for the guarantees. A threshold comparator at each VC buffers and one additional wire are needed for the back suction signaling between adjacent routers. The rate regulation at the ejection network interfaces can be efﬁciently implemented in hardware, as it only requires a small adder and a few registers. Compared to [14], we require one regulator per network sink as opposed to one per router port, which reduces the amount of regulators by 67% for mesh-like networks. With respect to power, Back Suction has also little overhead. In fact, all added arbitration logic only needs to be active on output ports through which a GT connection travels. Also, buffering is only done in the event of contention, where it is absolutely necessary even without QoS. We just optimize the selection of 158 which stream to buffer when. As a result, BE trafﬁc spends less time in the network on average and hence processors executing general-purpose workloads waste less time and power waiting for this trafﬁc. I I I . ANA LY S I S O F BACK SUC T ION In order to provide throughput guarantees to a stream, we must prove that while the back suction signal is active, a router can send ﬂits of a stream at least as fast as they are being forwarded (or ultimately consumed) in the downstream neighbor (or sink). For this to hold, a back suction signal must be propagated upstream early enough so that the local VC buffer does not deplete. This can be asserted by setting the back suction threshold appropriately, which we derive using real-time scheduling analysis introduced in section I-C. For this analysis, we consider every output port a resource under round-robin scheduling and every virtual channel allocated to a GT stream a task. Task execution constitutes the transferal of a ﬂit, which typically takes 1 cycle and hence we do not need to distinguish between preemptive and non-preemptive scheduling as all task activations are processed atomically. For the upcoming analysis, we model the binary per-VC back suction signal by discrete events which activate tasks. We deﬁne such a suction event as the removal of a ﬂit from an input VC whose current occupancy is at or below the suction threshold. In other words, whenever the back suction signal will be active after a ﬂit is removed from an input VC, a suction event occurs. We describe the maximum occurrence of back-suction events at the sink using an event model (arrival function), taking the implemented rate limitation into account. Note that this description does not need to be exact, a conservative approximation sufﬁces. The leaky bucket rate limitation that allows c suction event every T cycles can be described exactly by this maximum arrival curve: (cid:24) ∆t − k c−1(cid:88) (cid:25) η+ (∆t) = (1) T k=0 Note that the minimum distance between any two events is always at least 1 cycle, because the sink can only remove one ﬂit per cycle, hence the sum of displaced periodic step functions. Figure 4 shows the functions η+ and δ− for c = 3 and T = 5. Fig. 4. Maximum arrival (η+ , left) and minimum distance (δ− , right) functions for c = 3 events every T = 5 cycles A. Event Model Propagation Using the suction event model of the sink, an analysis can be performed at the router closest to the sink. From this, a new suction event model is obtained and used for the analysis of the next upstream router and so forth. These new event models account for some events being backlogged and forwarded upstream later at a faster rate than they initially appeared from downstream. The back suction event model of stream i at a router (k + 1) can be bounded by (cf. [15]) , η+ (cid:22) tsuction i,k (∆t + Rmax (cid:23)(cid:111) tc i,k − tc )+ (2) η+ i,k+1 (∆t) = min (cid:110) (cid:24) ∆t (cid:25) 2tc i,k where η+ i,k is the event model of (k + 1)’s downstream neighbor of stream i, Rmax is the worst-case response time of stream i at router k , tsuction is the delay between the servicing of a suction request by router k and the deassertion of the suction signal at router k , and tc is the cycle time. The ﬁrst term in the min-function represents the maximum rate at which ﬂits can be sent and hence at which suction events can occur (1 event per cycle). The second term is composed of two sub terms. The former is the worst-case arrival curve from router k , shifted to the left to take into account that events may be delayed by up to Rmax i,k . Events arriving during that period minus one execution time tc may all be forwarded to the upstream router at once, hence the right shift. The latter sub term takes the delay of the deassertion of the suction signal into account. After the last outstanding suction event for stream i has been served, the suction signal at router k will remain asserted for tsuction /tc cycles. This outdated signal may cause router k to prioritize stream i again, which only needs to be considered when there is at least one other GT stream j exerting back suction. Then, stream i will at most be served and interfere with the other stream(s) every other clock cycle. Hence, a conservative upper bound for the additional suction events can be given by (cid:98)tsuction/(2tc )(cid:99). Note that model propagation can be avoided if suction signaling delays are less than two clock cycles (tsuction < 2tc ) and suction events are propagated upstream with a constant delay (Rmax i,k = tc , which we leave for future work. B. Schedulability Requirements In order for a task to be executed (i.e. a GT ﬂit to be transferred), three requirements must be met: (1) The task must be activated, i.e. the downstream link must have asserted back suction; (2) the task must be selected by the scheduler, i.e. win output arbitration; (3) there must be ﬂits to send in the local VC buffer (i.e. the upstream task must have executed more often). The last requirement is difﬁcult to verify using real-time analysis, as it creates cyclic dependencies between resources. To break these, we assume that there are always enough ﬂits in the local VC buffer. This assumption is true if incoming suction events are propagated upstream early enough, so that ﬂits from upstream arrive before new suction events from downstream appear. Let β− i (∆t) denote the minimum service guaranteed to stream i, which describes how many ﬂits a stream will at least be allowed to send within a time window of ∆t. This can be used to compute stream i’s 159 bi (∆t) = η+ worst-case backlog bi (∆t), i.e. the number of activations that have arrived but not been served within any ∆t: i (∆t) − β− i (∆t − d) (3) The parameter d describes an additional delay between the requester of a service and the resource, i.e. the delay between a router asserting back suction and the upstream neighbor receiving this event. This may be different from the delay tsuction discussed in Sec.III-A. The maximum worst-case i = max{bi (∆t)} is equal to the number activation backlog b+ of suction events a router must serve directly from its local buffer. Hence, if the suction threshold is set to the activation backlog or higher it is guaranteed that the local buffer never depletes. If this is done for all routers of stream i, the source is guaranteed to be able to consume ﬂits at a rate governed by its rate limiter (η+ i ) and the throughput guarantees are met. Thus, to guarantee a speciﬁc throughput, we need to ﬁnd the maximum backlog b+ for every overlapping stream on every router using the propagated event model of the downstream neighbor and set the back suction thresholds accordingly. This implies that the local VC buffers must be larger than the threshold b+ i . Note that the actual buffer size of the corresponding VC should be larger than the suction threshold to avoid back suction being active all the time and allowing prioritization of BE trafﬁc under low utilizations, as we will show in Sec.IV. Furthermore, we assume that the source never runs out of data, which implies that the source must provide data at least as fast as it is consumed by the sink (plus an initial amount of data to ﬁll the VC buffers in the network). C. Activation Backlog and Latency Bound To compute the worst-case activation backlogs and hence the suction thresholds required for throughput guarantees, the event models of all streams must be considered at every router. We deﬁne B+ i (n) as be the maximum busy time of n activations of stream i, i.e. the maximum time required to transfer n ﬂits. In case of round-robin scheduling, we can i (n) = n · Ci + (cid:88) compute the busy time by (cf. [18]): This equation is composed of the time required to process n activations of stream i, each of which require a (worstcase) core execution time of Ci (ﬂit transferral time), plus the interference Ij from all other streams j , each of which is computed by i (n)) = ts · min (cid:40)(cid:24) n · Ci (cid:38) η+ j (B+ ts i (n)) · Cj (cid:39)(cid:41) i (n)) Ij (B+ Ij (B+ (cid:25) B+ j (cid:54)=i (4) ts (5) where ts is the round-robin slot time, i.e. the number of ﬂits transferred at once. The ﬁrst term in the min function represents the number of rounds required to process n activations of stream i. The second term computes the maximum number of slots stream j requests in any time window B+ i (n) from the maximum arrival curve η+ j of the other stream. This introduces an integer recursion, making the equation a ﬁxed-point problem. It can be solved by iterating over i (n). A ﬁxed-point is reached and the iteration terminates if the resource is not oversubscribed. In our network, the core execution time of any activation (ﬂit transferal time) as B+ , i i i (6) j (cid:54)=i B+ Rmax i j (B+ i (n) = i (n)|B+ i (n) < δ− i (n) < ∆t ∧ B+ i (n) < δ− n + (cid:88) well as the round-robin slot duration is one clock cycle tc : Ci = ts = tc . This allows the following simpliﬁcation: i (n))(cid:9) · tc min (cid:8)n, η+ From a task’s busy time B+ i (n) we can now derive its worstcase service curve β− i (∆t): i (∆t) = max{n|B+ i (n + 1)} (7) β− In other words, the minimum service guaranteed to task i is described by the maximum number of activations which are β− completed within ∆t under worst-case conditions. Note that it is only deﬁned over the busy window of stream i, i.e. when there are still outstanding activations (second condition in the max-function). The busy window also allows us to compute the worst-case response time of any activation by ﬁnding the maximum delay between the arrival of the nth event and the completion of n activations for all events arriving while the previous have not yet ﬁnished: = max{B+ i (n) − δ− i (n + 1)} (8) From this, we can compute the worst-case transfer latency l+ for any single packet of s ﬂits from source to sink at any given time. Up to dBuf ﬂits may be waiting in the VC buffers from source to sink when a new packet is injected. If we do not regard the history of past packets on the same stream, dBuf may be equal to the accumulated buffer depths from source to sink if all VC buffers are full because the source was injecting packets faster than the requested throughput. To transfer a packet of length s, s + dBuf ﬂits must be consumed at the sink. The sink is guaranteed to receive packets at a rate constrained by the event model, so it will require at most a time of δ− i (s+ dBuf ) to receive these ﬂits. To obtain the worst-case end-to-end transfer latency l+ i , the worst-case response time and the signaling delay d must be added to this time: i (s) = δ− D. Analysis Implementation and Evaluation Using the analysis methodology presented above, the feasibility and required suction thresholds of GT streams can be determined prior to the establishment GT streams as an admission control mechanism. We propose that this is performed by a central coordination instance called Resource Manager (RM), located in the operating system or run-time environment. We assume that GT streams are long-lived, being established during the start of real-time applications, so a central RM is no bottleneck. When applications request GT connections between a pair of network nodes, the RM performs a schedulability analysis for all existing overlapping GT connections before the new connections are established via packet concatenation. The analysis from Eq.3 and 7 must be done for every router with a GT stream starting from the sink of each stream and using event model propagation towards the source. Deadlock-free routing of streams avoids cyclic dependencies between propagated event models during this sequence, so each router only needs to be analyzed once. To evaluate the resulting suction thresholds, we perform an exploration for up to three overlapping GT streams (1 VC is i (s + dBuf ) + Rmax + d (9) l+ 160 generate packets of 32 bytes in size with uniform-randomly distributed delay between them (at most 20 cycles between min. and max. delay). For comparison, we have also modeled a common QoS scheme which gives priority to GT trafﬁc, and the Distributed Trafﬁc Shaping (DTS) mechanism of [14], which is capable of improving BE latencies like Back Suction. First, we consider a single synthetic stream of GT trafﬁc going from node (0,2) to node (5,4) requesting a guaranteed throughput of 2 bytes/cycle (50% of the link capacity). Hence, we have conﬁgured the rate regulation to T = 8 and c = 4. The same conﬁguration is used for the trafﬁc shapers along the route of the GT stream for DTS. We have determined a suction threshold of 1 using the detailed analysis from Sec.III-C. All other nodes generate synthetic BE trafﬁc at random times with varying average loads. For BE trafﬁc, three types of trafﬁc patterns are used: uniform random, and tornado. For uniform random, the destinations of every packet are randomly distributed over the whole network with equal probability. For transpose trafﬁc, the destination for trafﬁc from node (i, j ) is always node (j, i) (i.e. mirrored across the diagonal), For tornado trafﬁc it is ((i + 3) mod 8, j ) (i.e. halfway across on the x-dimension). The size of the VC buffers is varied from 4 to 16 to show the effect on BE latency. Figure 6(a) shows the throughput of the GT stream over varying BE loads for the different QoS mechanisms and different VC buffer sizes for uniform random trafﬁc. It can be observed that DTS is not capable of meeting the requested throughput under high BE load for a low buffer size of 4 as mentioned in Sec.I-B. In all other cases, the guarantees are met. For Back suction, the GT stream is even granted more trafﬁc under low load by using otherwise idle link bandwidth. Figures 6(b,c) show the average latency of BE packets originating at node (1,2), which is right in the path of the GT stream. It can be seen that Back Suction and DTS perform very similarly and are always better than the commonly used prioritization of GT trafﬁc, especially under low and medium network loads. The improvement of BE latency for Back Suction is 14% for random, 21% for transpose and 32% for tornado. The difference is most dominant for the tornado as its trafﬁc shows maximum overlap with the GT stream. The latency of BE trafﬁc increases slightly at high loads for reduced VC buffer sizes as this limits the ability of GT trafﬁc to make progress during idle times. Back Suction with just 8 ﬂits of VC buffer space still performs about as well as DTS with 16 ﬂits for random and tornado trafﬁc. Under low BE load, even 4 ﬂits of VC buffer size sufﬁce are sufﬁcient for Back Suction to match the performance of DTS. Note that DTS requires a buffer size of 16 to give guarantees (cf. Fig.6(a)) in all cases. Next, we evaluate Back Suction in a more complex scenario, for which we have modeled the application from the introductory example in Fig.1 using trafﬁc generators. Figure 7 shows a feasible (yet sparse) manual mapping spanning a larger area of the network. No link contains more than two overlapping GT streams. We set the source data rate r to 30% of the link bandwidth, which means that the maximum GT-utilization will be 2 · r = 60% of the link bandwidth. Figure 8 shows the requested and achieved total network throughput of every task of the application over the BE background load. It can be seen that the requirements are met in all cases. Back Suction usually achieves slightly higher throughputs (esp. under low loads) due to its SuperGT service. (a) (b) Fig. 5. Maximum worst-case backlog over the GT-utilization and hop count (a) and histogram (b) over different scenarios left for BE trafﬁc). For each stream i, the suction rate is limited explored. For each scenario, the GT-utilization U + = (cid:80) ci /Ti to ci suction events every Ti cycles, and all possible combinations of ci and Ti , with Ti = [1..16] and ci = [0..T − 1] are is calculated, i.e. the accumulated requested rate, and scenarios that are oversubscribed are dropped. For the remaining scenarios, the worst-case backlog b+ i = max{bi (∆t)} is computed, assuming a delay of d = 1. The analysis is ﬁrst done at the sink and continued for upstream routers hop by hop until either the streams are no longer feasible or a hop count of 16 has been reached. Figure 5(a) shows a scatter-plot of the worstcase backlog obtained from the analysis over the GT-utilization U + for different numbers of active streams. It can be seen that b+ i peaks at medium utilizations for high hop counts as the temporal overload increases due to suction backlog. For higher scenarios, a reduction of feasible scenarios with high hop count can be observed. Also note that scenarios with larger than the available VC buffer size are not feasible. However, as Fig.5(b) shows, most scenarios have low backlogs and hence low threshold and buffer requirements. The complex analysis performed for these results required about 70ms on average per scenario (including oversubscribed ones) using a non-optimized Python implementation on a PC, which is still sufﬁciently fast for an on-line implementation. b+ i IV. EX PER IM EN TA L EVALUAT ION To evaluate Back Suction experimentally, we use a cycleaccurate SystemC behavioral model of our proposed router architecture on an 8x8 mesh network conﬁguration with 4 virtual channels per port. Routers have a four-stage pipeline and a ﬂit traverses the switch/link in one cycle. We use synthetic trafﬁc generators to model BE and GT trafﬁc, which 161 (a) GT throughput for uniform random trafﬁc (b) BE latency for uniform random trafﬁc (c) BE latency for tornado trafﬁc Fig. 6. GT throughput and BE latencies for a single GT stream from node (0,2) to node (5,4) streaming applications. Back Suction implies little additional hardware costs and especially requires far less VC buffers and rate regulators than other approaches with similar BE latency improvement. Our experimental evaluation has demonstrated an improvement of the BE latency by up to 32% over a standard prioritization scheme. "
Comparing Energy and Latency of Asynchronous and Synchronous NoCs for Embedded SoCs.,"Power consumption of on-chip interconnects is a primary concern for many embedded system-on-chip (SoC) applications. In this paper, we compare energy and performance characteristics of asynchronous (clockless) and synchronous network-on-chip implementations, optimized for a number of SoC designs. We adapted the COSI-2.0 framework with ORION 2.0 router and wire models for synchronous network generation. Our own tool, ANetGen, specifies the asynchronous network by determining the topology with simulated-annealing and router locations with force-directed placement. It uses energy and delay models from our 65 nm bundled-data router design. SystemC simulations varied traffic burstiness using the self-similar b-model. Results show that the asynchronous network provided lower median and maximum message latency, especially under bursty traffic, and used far less router energy with a slight overhead for the inter-router wires.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip Comparing Energy and Latency of Asynchronous and Synchronous NoCs for Embedded SoCs Daniel Gebhardt School of Computing University of Utah gebhardt@cs.utah.edu Junbok You Kenneth S. Stevens Dept. of Electrical and Computer Engineering University of Utah {jyou , kstevens}@ece.utah.edu Abstract—Power consumption of on-chip interconnects is a primary concern for many embedded system-on-chip (SoC) applications. In this paper, we compare energy and performance characteristics of asynchronous (clockless) and synchronous networkon-chip implementations, optimized for a number of SoC designs. We adapted the COSI-2.0 framework with ORION 2.0 router and wire models for synchronous network generation. Our own tool, ANetGen, speciﬁes the asynchronous network by determining the topology with simulated-annealing and router locations with force-directed placement. It uses energy and delay models from our 65 nm bundled-data router design. SystemC simulations varied trafﬁc burstiness using the self-similar b-model. Results show that the asynchronous network provided lower median and maximum message latency, especially under bursty trafﬁc, and used far less router energy with a slight overhead for the interrouter wires. I . INTRODUCT ION Embedded, energy-constrained SoC designs can be roughly separated into two classes: platform-based and ﬁxed-function (also called application-speciﬁc). The former is concerned with being able to perform a wide variety of tasks, many of which cannot be foreseen at design time. The latter is targeted towards a particular function, or a few functions, that have known properties. A ﬁxed-function design might consist of a number of highly specialized cores and memories, and fewer generalpurpose processors. The network-on-chip (NoC) of both these classes should be optimized for minimal energy usage while meeting the predicted performance requirements; however, the application-speciﬁc NoC may be more specialized as it has a priori knowledge of the communication patterns between cores. This is in contrast to general-purpose interconnects that are often evaluated with trafﬁc patterns such as spatiallyuniform, bit-transpose, etc.. The domain of this work is the ﬁxed-function, rather than the platform-based SoC. Some globally-asynchronous locally-synchronous (GALS) interconnect solutions rely on a clock, either with standard synchronous clock distribution, or a mesochronous method. However, an asynchronous (also called clockless) network has a number of potential advantages over a clocked network in a GALS environment. Standard arguments for asynchronous circuit design include robustness to process/voltage/temperature variation, average-case instead of worst-case performance, and other such points. However, there are also many NoC-speciﬁc arguments. In a synchronous NoC, the clock tree for all routers and pipeline buffers can consume signiﬁcant power as shown in a heterogeneous network [1], and in a large CMP (chip multiprocessor) 33% of router power [2]. Many SoC designs have quite bursty and “reactive” trafﬁc. In this case, asynchronous methods are beneﬁcial in that they consume little dynamic power during periods of low trafﬁc without relying on clock gating techniques. Available bandwidth on each asynchronous link can be independently set, to some extent, by wirelength between routers, link pipeline depth, or by varying the physical wire properties (metal layer, width, and spacing). This is potentially useful when bandwidth requirements on core-to-core paths vary considerably. This is in contrast to clocked networks which commonly use a single frequency for all routers and is wasteful to those paths not requiring high bandwidth. A clocked NoC can use discrete “islands” of differing clock speeds to achieve a similar effect, but in a much coarser-grained fashion. Design automation techniques are commonly used to generate a NoC for a speciﬁc SoC design. These methods can decrease time of development in commercial products or allow a researcher to explore a larger design space. The NoC solution is chosen based on some metric, usually a function of energy and performance. In the optimization process, potential solutions must be evaluated for quality, and this often requires an abstracted model of the SoC characteristics. This abstraction can be done at a variety of levels depending upon completeness or availability of the SoC design and NoC components. Ideally, one could simulate the exact functionality of the various cores composing the design, and the NoC would be fully implemented to model the communication. Unfortunately, this method is labor and simulation-time intensive, and not a good choice for early-exploration of the NoC design space. As usual, tradeoffs must be made as function becomes more abstracted. A commonly used abstraction used in the literature has been titled a communication trace graph [3] (CTG) or a core graph. A path describes pairs of source and destination cores, and the particular links and routers a packet traverses. The CTG has a n-tuple of values per path, but often includes average expected trafﬁc rate per path and sometimes a latency requirement of a packet. An example CTG is shown in Figure 1 that we use in our evaluation. To our knowledge, there does not exist published methods to aid in automating high-level asynchronous NoC optimization 978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.21 115 VU 64 1 AU 20 MED CPU 14 200 SDRAM 3 304 DSP 11 SRAM 1 UP SAMP RAST IDCT 40 84 SRAM 2 224 58 167 BAB RISC Figure 1: Example CTG graph. Edge weights are in MBytes/s. for ﬁxed-function SoCs. This is in contrast to techniques that utilize synchronous tools for implementing a speciﬁc network [4]. In this work, we give an overview of our circuits and design automation techniques, and compare the resulting asynchronous NoC to a synchronous one generated by an existing tool. We also show that adding a measure of bursty trafﬁc to a CTG design abstraction leads to a more conclusive NoC evaluation. Also unique is our SystemC simulator that models asynchronous routers, and importantly, the link delay as a function of wire length between routers. This paper is organized as follows. Section II gives an overview of related work. The synchronous network generation framework is discussed in Section III. Our asynchronous router is discussed in Section IV at a circuit level. Section V describes our methodology for asynchronous NoC generation and simulation. Our evaluation methodology and setup is given in Section VI, with the results discussed in Section VII. We conclude in Section VIII. I I . RE LAT ED WORK The COSI framework [5] generates an application-speciﬁc NoC and ﬂoorplan, taking as input such constraints as core areas and average bandwidth between cores. While it is extensible with new algorithms and components, it does not consider asynchronous network components and, as future work, cites the need for integrating trafﬁc burstiness. For the Xpipes library, a heuristic search determines the topology and router conﬁguration [6]. It uses ﬂoorplan information, router energy models, and core communication requirements. The results indicate a signiﬁcantly reduced power and hop-count versus the best mesh topologies for a variety of SoC designs. It is part of a complete workﬂow to automatically synthesize a NoC down to chip layout [7]. A linear programming based method is presented in [3]. For the QNoC routers, application-speciﬁc optimization is discussed in [8], but it focuses on mapping logical resources of a mesh-style topology rather than physical concerns. Previous research on asynchronous interconnects is rich, but these designs are either hand-designed for a particular application, or general in design but possibly having overprovisioned resources for a power-constrained SoC. All but one of these existing routers use quasi delay-insensitive protocols between routers, rather than bundled-data. Fulcrum Microsystems created a large asynchronous crossbar to interconnect cores of a SoC [9]. The commercial startup Silistix, based on earlier academic research [10], sells EDA software and circuits that provide an customized asynchronous NoC, but has no published methods for the optimization process. The MANGO router [11] provides both best-effort and guaranteed-service trafﬁc. FAUST [12] is a platform and fabricated chip used in 4G telephony development, and uses an asynchronous mesh-based NoC [13]. The QNoC group has developed an asynchronous router that provides multiple service levels and dynamically allocated virtual channels per level [14]. A mesh-of-trees network was constructed from simple, bundled-data routers for a CMP [15]. A comparison between the asynchronous network ANOC, and the mesochronous clocked network DSPIN, was performed in [1]. For both designs, a physical layout and functional trafﬁc simulation was done for analysis. While DSPIN had 33% less area and 33% higher bandwidth than ANOC, ANOC had shorter packet latency and at least 37% lower power consumption. DSPIN was also compared against its asynchronous analog, ASPIN [16]. Average power, latency, and saturation threshold are superior in ASPIN with similar die area. Trafﬁc modeling for NoCs is one of the major outstanding problems in the ﬁeld [17]. The b-model [18] provides a simple method to produce and analyze the burstiness of self-similar trafﬁc with a single value. The b-model has been adapted to study burstiness effects in the Nostrum NoC [19]. Evidence of trafﬁc self-similarity and burstiness in MPEG-2 video applications has been shown [20]. Several analytic models of network performance have been developed for NoC design. A model has been developed to capture spatial and temporal characteristics of trafﬁc for regular, homogeneous NoCs [21]. A generalized analytic router model was developed in [22]. It provides detailed statistics during expected trafﬁc, and is applicable to heterogeneous, irregular networks, but relies on the Poisson arrival process and a synchronously-clocked router. I I I . SYNCHRONOU S N E TWORK G ENERAT ION The baseline network used for comparison purposes is generated by a research tool called COSI 2.0, a source-code release that incorporates much of the functionality of COSINoC (v.1.2) [5]. COSI takes as input a SoC design abstraction consisting of core dimensions or area, and a set of communication constraints between those cores, which are called ﬂows. This is a more generalized concept than the CTG mentioned in Section I, and COSI can consider temporal properties between ﬂows, such as mutual exclusion. Given these ﬂows, its optimization algorithms try to ﬁnd the network and ﬂoorplan that meets the constraints while minimizing power based on router and wire models. As output, COSI produces a ﬂoorplan, topology, and a SystemC-based simulator. Included with the software release are algorithms for generating a mesh and a min-cut partitioning method (hierarchical star) similar to that of [6]. We modiﬁed COSI to incorporate the Orion 2.0 router and wire models [23], and also made a number of other changes to COSI to improve its operation and result reporting. In order to explore the performance characteristics of the network, we moved away from the Poisson trafﬁc models commonly used for evaluations and instead use a model more representative of application trafﬁc. We implemented the b116 model trafﬁc generator [18], suggested as a key feature in future NoC benchmark sets [24]. The SystemC simulator produced by COSI was modiﬁed to use this bursty trafﬁc generator. Our model is parameterizable with the following inputs: • Source and destination cores. • A b-value in the range [0.5, 1.0) indicating burstiness. • Simulation duration. • Average bandwidth, i.e. desired total trafﬁc volume. • The smallest time-resolution of the burstiness. • Number of packets per message. Self-similar trafﬁc, down to the time resolution, is generated recursively with an algorithm closely following the original [18]. However, there are a number of interesting details to note. The b-model determines the total volume of data to send in each window determined by the speciﬁed time resolution. Within a window, a message is probabilistically sent each cycle such that over the time window the proper amount of data is sent. An entire message consisting of multiple packets is sent at once to emulate application-level data needs. It may be the case that the desired volume of trafﬁc per window exceeds the capacity of the link or output buffer, or the previous window has not ﬁnished sending its data yet. In these cases the packets are queued up in an “inﬁnite” buffer. Therefore, the model’s output is the ideal, desired data transmissions, but the actual achieved data is subject to network limitations as expected. This design uses a SystemC transaction level model (TLM) for its interface, and thus it is portable and relatively easy to connect to other tools’ outputs, as we did here with COSI. IV. A SYNCHRONOU S ROUT ER D E S IGN A. Overview This asynchronous router is designed for efﬁciency and simplicity. Each switch directs a ﬂit to one of two output ports. With bi-directional channels, this results in a threeported “T” router. The packet format consists of a single ﬂit containing source-routing bits in parallel, on separate wires, with the data bits. The packet is switched through a simple demultiplexer controlled by the most-signiﬁcant routing bit. The bits are simply rotated, or swizzled, for the output packet. The number of required routing bits is determined by the maximum hop count of a network generated for a speciﬁc SoC design. The width of each ﬂit must be determined based on required throughput or power and area constraints. This format has the overhead of requiring routing bits with every ﬂit. P o r t A t s d a t a b i r o u t e b i t s P o r t C Port B     (a) switch (b) merge Figure 3: Schematics of Switch and Merge modules. asynchronous controllers by hand or using Petrify, synthesized the full asynchronous structural router design including data path with Synopsys Design Compiler, and physically placed and routed with SOC Encounter. Functionality and performance were validated in the design with ModelSim using back annotated pre- and post-layout delays. Asynchronous circuits were veriﬁed by Analyze [29] and using static timing analysis. C. Evaluation We have constructed a number of routers with varying ﬂit widths, but for this paper use one with 32-bits of data and 8-bits for routing. The resulting area is 2740 µm2 , dynamic energy/ﬂit is 1.56 pJ, and leakage power is 0.009 mW. The area is dominated by data storage latches and the data MUXes used in the merge modules. The controllers (linear controllers in switch modules and merge controllers in merge modules) make a very small contribution to the total area. Dynamic energy is consumed when one data word passes a router from an input port to an output port. Energy is measured using HSPICE simulations with the spice netlist generated from the design using parasitic extraction from Mentor Graphics Calibre PEX. The same simulation was used in both HSPICE and ModelSim. The HSPICE control ﬁle was generated by converting a vcd ﬁle generated from the ModelSim simulation. This allowed us to more easily validate switching activity on the data and control paths. A 50% data switching activity factor was applied to the data bits for our power simulations. The maximum throughput of the router is 2.38 Gﬂits/s. This was measured by inserting data into the input ports at maximum rate and allowing the output port to communicate with another router with no wire delay. We deﬁne the backward latency of our routers as the delay from a request on an incoming channel to the acknowledgment on that channel, completing the handshake of the two-phase protocol. Fast backward latency is desirable because it frees the previous router’s output port for another transaction. We deﬁne forward latency as the delay from a request on an incoming channel of a router to the associated request on an output channel assuming no contention or stalling in the arbitration circuit. This is determined by the delay to buffer the data, arbitrate control, and switch to the outbound channel. Our router design has 250 ps backward latency and 460 ps forward latency. Our router’s low power and area are due to its simple architecture and the use of latches, rather than ﬂip-ﬂops, for the storage elements. Latches are about half the size and use less power than ﬂip-ﬂops. Since much of the area and power of many router architectures derives from memory elements, this advantage makes a signiﬁcant difference. Furthermore, the simplicity of the control circuits also contributes to high throughput. This router employs a bundled data protocol rather than delay insensitive codes which results in fewer wires per channel and efﬁcient use of standard cell libraries. However, the cost to this is that the circuit timing must be carefully speciﬁed and controlled, similar to clocked design, to ensure correct operation. V. A SYNCHRONOU S N E TWORK G ENERAT ION We built a tool, ANetGen, that has goals similar to COSI’s, but operates with our router model and its asynchronous considerations. ANetGen takes an input format that deﬁnes the CTG edges and expected trafﬁc bandwidth, as well as the core dimensions. The core ﬂoorplan is speciﬁed prior to ANetGen, which then determines physical placement of the routers and their logical topology. The objective function is to minimize wirelength and hop counts for high trafﬁc paths. It does this with a combination of simulated annealing (SA) and forcedirected movement techniques. A. Topology and Placement Asynchronous circuits have unique properties that can be leveraged to optimize the network. Speciﬁcally, the physical path length between endpoints directly affects packet latency, not just the number of routers and pipeline buffers a packet must travel through, assuming an uncongested path. This is in contrast to a synchronous system, where each network element constitutes at least one required clock cycle. Also, link energy 118 usage can be signiﬁcant [30] and will grow, relatively, with shrinking process technology. With this in mind, the physical placement of routers needs to be determined such that wirelength is minimized, especially on highly trafﬁcked paths. For these experiments, we assume soft IP (intellectual property) blocks which have cells placed and routed by the SoC developer, rather than a single hard macro block. This enables us to consider more options for router locations. In an actual design ﬂow, the router placement our tool generates will provide input to the hierarchical placer, or ﬂoorplacer [31] that will legalize the placement of cells and macros composing each core. The problem of ﬁnding the optimal tree topology is similar to the NP-hard quadratic assignment problem of mapping cores to a mesh topology [32]. For this, we utilized a simulated annealing method. The ﬁtness to be minimized is based on a topology’s router hop-count and wirelength, each weighted by the volume of trafﬁc expected over the path. In the current tool implementation we limit the topology to a tree, which has a minimal number of three-port routers. We save a detailed analysis and comparison with other topologies to future work, but this method produces good results, as seen in Section VII. Within the SA process, topology choices are explored by perturbing the topology and re-placing routers. We used a method extended from [33] that uses force-directed movement to provide router locations and link lengths to the SA process. Force vectors are applied to routers that are proportional to: (a) bandwidth requirements and (b) physical distances between the router and its attached core or router. The process is iterative, where a router moves a distance proportional to the sum of its force vectors. This movement will reduce wire lengths of paths that carry high trafﬁc. B. Simulator We chose to build an asynchronous network simulator using the SystemC library. The following modules were developed: an arbiter, an inport to the router, an outport from the router, and input and output port FIFOs. The SystemC Transaction Level Modeling (TLM) library is used for inter-router links and trafﬁc generation. We chose this method to allow easier extensibility of the channels if needed, and TLM provides a convenient way to model link and protocol delays. The trafﬁc generator and router ports use a simple socket to receive a generic payload transaction object that contains packet and routing information. When a TLM object is received by the inport’s socket, a wait is performed to model the wire delay. This delay is calculated from an interpolation of HSPICE simulations of various wirelengths in IBM’s 65nm technology. The wire energy per transfer is calculated using the Orion 2.0 model. The router waits an additional time period to model forward logic delay. The ﬂit is written to the FIFO, which triggers the arbiter. Another wait models the acknowledgment delay to the sender. Within the arbiter, a doSwitching SC METHOD is called whenever a packet is received by an input FIFO or acknowledged by an output FIFO. The arbitration mechanism is that described in Section IV. At each switching operation, the appropriate energy is logged. This energy was measured from transistor-level router simulations. Each outport operates in its own thread, waiting for a packet to be passed to it by the arbiter, or for a TLM response indicating that the channel is free. When there is data in the FIFO and the channel is free, it sends a new TLM generic payload. The outport also records wire energy of the transmitting link. V I . EVALUAT ION M E THODOLOGY The evaluation of all network solutions was done with the SystemC simulators generated by the tools. In this section, we present the benchmarks and simulation parameters. A. SoC Designs and NoC Generation We used two SoC design abstractions of the CTG format described in Section I for our evaluations. One is titled ADSTB and is from the public COSI 2.0 distribution. The other is an MPEG4 decoder originally described by [34] and used in several other NoC research projects. Bandwidth requirements were modiﬁed from those originally provided, and are shown in Figure 1 for MPEG4 and Table I for ADSTB. The die areas after router placement for the ADSTB and MPEG4 designs were 35.7 mm2 and 78.7 mm2 , respectively. These ﬂoorplans were from the COSI tool’s output. TABLE I: Average bandwidths for the ADSTB design. Sender CPU CPU DDR DDR Dem2 Demux MPEG2 Receiver AudioDec Demux CPU MPEG2 Demux MPEG2 DDR MBytes/s 1 1 3 593 31 7 424 Sender CPU CPU DDR Dem1 Demux HDTVEnc Receiver DDR MPEG2 HDTVEnc Demux AudioDec DDR MBytes/s 3 1 314 31 5 148 We generated a network for each design using the COSI and ANetGen tools. We also manually created an asynchronous network for the ADSTB design that is based on the topology of the COSI solution. For each radix-4 and radix-5 router, we manually replaced it with a construction of our radix-3 asynchronous routers, shown in Figure 4. The paths which carry the most trafﬁc were mapped to ports with the least number of routers between them, such as ports A and B . This construction is not a true radix-N switch, as it can have internal contention (e.g. A → C contends with B → D). We conﬁgured COSI to generate a hierarchical star network with N/3 + 1 partitions (N is number of cores), chosen based on empirical experimentation for low energy. The ﬂoorplanner A B C D A B E C D (a) Radix-4 (b) Radix-5 Figure 4: Asynchronous router constructions replacing those of radix > 3. External ports are labeled with letters. 119 was constrained to a square aspect ratio outline. The input to ANetGen was the same ﬂoorplan and communication properties as COSI. B. Simulation Parameters We instrumented the SystemC router and wire models from COSI and ANetGen to record energy usage, packet latency, and message latency over the course of a simulation. Orion 2.0 is used for the wire energy model in both frameworks, and also for the synchronous router leakage power and switching energy models. Energy for the asynchronous routers comes from circuit simulation described in Section IV. The link model assumes 50% of the wires switch per ﬂit transfer. This is a worst case model because real data will have a lower fraction of changing bits. Additionally, the asynchronous router’s sourceroute wires will change less than this as subsequent ﬂits often carry similar routing paths. Thus, the overhead of these additional routing wires is likely less than what is represented in the results. We chose parameters for the Orion router model to be near as possible to our asynchronous conﬁguration in both energy and performance. These are shown in Table II. Clock tree power estimation was excluded from these models. TABLE II: Orion 2.0 Model Parameters. Router Freq. Tech. Library Voltage 2 GHz 65 nm NVT 1.0 v Router I/O buff ’s Crossbar Flit width 2 / 1 ﬂit Multitree 32 bits V I I . RE SULT S In this section we present results that show the asynchronous networks provided lower message latency and used less power than the synchronous networks. Recall that a message is composed of a number of packets, and is typically managed at the transport layer. Message latency is deﬁned as the time the ﬁrst packet of the message leaves the sending core’s output buffer and enters the network to the time the tail packet leaves the network and enters the destination core. The following results were generated with a message size of 256 bytes, not counting ﬂits carrying address information. Simulations were run at three burstiness b-values {0.5, 0.65, 0.8}. We assume that packets are not dropped, and that the destination cores do not stall, blocking its input port. A. Message Latency Distribution Histograms of message latency are shown in Figure 5 for the ADSTB design, and a summary of both is presented in Table III. An increase in latency as trafﬁc burstiness rises shows that trafﬁc paths contend for switch and link resources for longer periods of time. At 0.5 burstiness, all networks operate with low latency of 150-190 ns for nearly all trafﬁc. At 0.8 burstiness, the asynchronous networks have more messages arriving in under 200ns, and a lower “re-peak” on the right side of the chart. TABLE III: Observed message latencies (ns); absolute maximum and latency bound of 99%. 99% less than ADSTB MPEG4 Maximum ADSTB MPEG4 Network sync. manual async ANetGen sync. ANetGen sync. manual async ANetGen sync. ANetGen Burstiness 0.65 231 262 291 1395 431 0.8 531 274 304 1903 697 51077 580 762 56041 2520 126480 914 912 158264 5288 0.5 158 188 192 838 275 1130 510 510 11722 704 B. Per-path Message Latency An understanding of latency and congestion within the network cannot be fully understood by the overall delay alone. Due to the heterogeneity and diverse path properties in an application-speciﬁc SoC, there is beneﬁt to analyzing each path through the network separately. For each path, or pair of communicating cores, Figure 6 shows the maximum latency for the ADSTB design and the median latency for the MPEG4 design. With few exceptions, the asynchronous network provides lower latency, at both low and high burstiness values. This is a combination of a number of factors. COSI’s synchronous routers operate with wormhole switching in which a blocked header ﬂit stalls up to two trailing ﬂits that in turn block other packets in other routers. Second, the asynchronous network can take advantage of short wires between routers and not delay a packet an entire cycle. Lastly, the COSI-based router design has the overhead of an extra ﬂit carrying address information. Despite the large variation in some paths, other paths show little difference between change in burstiness or between median and max delay. This is due to the network topology, where some paths have fewer hops and a lower chance of congestion, and others must traverse more routers. Both COSI and ANetGen map paths carrying more trafﬁc such that they have fewer router hops. C. Output Buffer Delay Another metric of measuring the network performance is the output buffer delay, which is from the time the trafﬁc generator places a packet in the output buffer to the time the packet enters the network. The buffer entry time is set for each packet by the trafﬁc generator when it pushes an entire message to the buffer at once. Therefore, the last packet of a 64-packet message would have a minimum delay of 64 sender-cycles. The trafﬁc generator operates detached from the network ﬂow control so an inﬁnite buffer is needed to accept its trafﬁc at any time. The network then empties that buffer as quickly as possible. As burstiness increases, the additional delay comes not only from contention within the network, but also from the local trafﬁc generator’s attempt to send more data in a shorter time period. This grows the buffer more rapidly, increasing delay, even if the network was uncongested. From the results in 120  0  20  40  60  80  100  0  100  200  300  400  500  600 D y a e l O u c c r s e c n a ( % ) Message Delay (ns) Message Delay Histogram - Various Burstiness b=0.50 b=0.65 b=0.80 (a) COSI-generated synchronous  0  20  40  60  80  100  0  50  100  150  200  250  300  350 D y a e l O u c c r s e c n a ( % ) Message Delay (ns) Message Delay Histogram - Various Burstiness b=0.50 b=0.65 b=0.80 (b) Manual asynchronous  0  20  40  60  80  100  0  50  100  150  200  250  300  350 D y a e l O u c c r s e c n a ( % ) Message Delay (ns) Message Delay Histogram - Various Burstiness b=0.50 b=0.65 b=0.80 (c) ANetGen asynchronous Figure 5: Histograms of message latency for the ADSTB design.  0  500  1000  1500  2000 M e g a s s e D y a e l ( s n ) Path ID Message Delay Per Path b=0.50 b=0.80 Asynchronous Synchronous (a) Maximum latency for ADSTB  0  500  1000  1500  2000 M e g a s s e D y a e l ( s n ) Path ID Message Delay Per Path b=0.50 b=0.80 Asynchronous Synchronous (b) Median latency for MPEG4 Figure 6: Observed latencies per path for 256-byte messages. Results are shown for the synchronous and asynchronous network, and trafﬁc of two burstiness amounts. Table IV, we see that the asynchronous networks consistently have a lower delay for both median and maximum values. D. Instantaneous Bandwidth A measure of network performance related to message latency is the instantaneous bandwidth (IB) available to a path at any given time. This is in contrast to the average bandwidth that an application produces over a long period. We deﬁne an IB requirement for a source-destination path by a pair of TABLE IV: Source output buffer packet delay (ns). Network Burstiness 0.65 730 508 500 170336 250 Median ADSTB 0.5 96 90 90 274 84 0.8 390065 320018 319818 1.1e6 171496 sync. manual async. ANetGen sync. ANetGen MPEG4 Maximum ADSTB sync. manual async. ANetGen sync. ANetGen 1274 952 912 11683 1036 261112 215908 231242 1.2e6 171358 1.3e6 1.2e6 1.2e6 2.99e6 1.1e6 MPEG4 values: {V , T }, where V is in bytes and T is in seconds. This might be used in validating an interconnect of, say, a real-time speech recognition SoC, where 18 MB must be processed in 0.1 s [35]. For example, the maximum synchronous network latency seen in simulation between the upsamp and sdram cores of the MPEG4 was 2525 ns. Suppose this path had an IB requirement of {256 by tes, 1000 ns} (equating to 256 MBytes/s). This network would be a poor choice because the application would occasionally not receive proper communication throughput, despite the fact that the network did support its average bandwidth. E. Power Consumption and Area We present the power consumption in Table V, broken down into the following areas: dynamic power of routers, leakage power of routers, dynamic power of wires, and leakage power of wires. These measurements do not include the power of clock distribution, and assume clock gating at the router. The wire power includes that from drivers and repeaters (large inverters). TABLE V: Power consumption (mW) of routers and wires. rtr dyn 5.5 0.95 0.95 12.3 2.4 rtr leak 5 0.072 0.054 15.7 0.09 wire dyn 7.86 12 6.3 28 20.5 wire leak 4.6 8 4.5 15 16.7 TOTAL 23 21 11 71 40 ADSTB sync manual async ANetGen MPEG4 sync ANetGen In both cases, the dynamic power of the asynchronous routers is reduced to about one-ﬁfth the power of the synchronous routers. The leakage power of the asynchronous routers is negligible. The manual asynchronous network for the ADSTB design has a noticeable increase in wire power. One reason is the additional links needed to form the cluster of 3-port routers in place of a higher-radix router. Second, the routing bits are on separate wires, rather than an address on the head ﬂit. Third, our routers and tools use bi-directional ports, with links instantiated in both directions. COSI, meanwhile, considers uni-directional router ports, and may produce a solution with fewer links. Overall, the asynchronous networks use less power than the synchronous networks. The majority of savings comes from signiﬁcantly lower router power, both dynamic and leakage. These results also point to the need for wire resources to be careful utilized, especially with energy-efﬁcient routers. 121                     The ANetGen networks have an area advantage over the synchronous ones as well. Router areas are 15630 µm2 (ANetGen) vs. 99704 µm2 (COSI) for ADSTB, and 26050 µm2 vs. 138822 µm2 for MPEG4. V I I I . CONCLU S ION In this paper we provided an examination of performance and energy of synchronous and asynchronous NoCs that are customized for a number of SoC designs. The asynchronous network formed by our tool ANetGen and our energy-efﬁcient routers only consumed half as much power as the synchronous case. Wires consumed the largest fraction due to repeater energy. Our asynchronous network also had lower latency, signiﬁcantly so for bursty trafﬁc, for 256-byte messages. For the ADSTB design, ANetGen yielded a lower-energy solution and slightly lower latency than a manually-designed network based on the synchronous topology. The evaluation suggests that the common abstraction of SoC requirements using only average bandwidth may not be sufﬁcient. The addition of a single-valued burstiness to the tuple representing a network ﬂow’s properties should be considered in other NoC evaluations. In future work, we will reﬁne ANetGen to consider a wider range of topologies and more closely integrate it with the ﬂoorplanning tool. ACKNOW L EDGMENT S This research is supported by the National Science Foundation under grant CCF-0702539 and Semiconductor Research Corporation under task 1817.001. We would like to thank ARM and IBM for providing the 65nm library cells and process technology. "
Hierarchical Network-on-Chip for Embedded Many-Core Architectures.,"The need for computing power drastically increases and one good solution is to use many-core architectures. Besides, complex embedded applications become data-dependent and their execution time depends on their input data. For this reason, on-line task and data allocation is needed to optimize the architecture efficiency. Moreover, communications are a complex problem in many-core architectures. Because of dynamic allocation, communication paths and network loads become unpredictable, which must be handled by the network. This paper proposes an evaluation of different network topologies in terms of performance and area for many-core architectures. It concludes that hierarchical networks are the best trade-off. In particular, the MultiCross topology is 10 times more efficient than the mesh topology.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip Hierarchical Network-on-Chip for Embedded Many-Core Architectures Alexandre GUERRE, Nicolas VENTROUX, Rapha ¨el DAVID Alain MERIGOT CEA, LIST, Embedded Computing Laboratory Gif-sur-Yvette, F-91191, France; Email: ﬁrstname.name@cea.fr Institut d’Electronique Fondamentale Universit ´e Paris Sud Orsay, F-91405, France; Email: alain.merigot@u-psud.fr Abstract—The need for computing power drastically increases and one good solution is to use many-core architectures. Besides, complex embedded applications become data-dependent and their execution time depends on their input data. For this reason, on-line task and data allocation is needed to optimize the architecture efﬁciency. Moreover, communications are a complex problem in many-core architectures. Because of dynamic allocation, communication paths and network loads become unpredictable, which must be handled by the network. This paper proposes an evaluation of different network topologies in terms of performance and area for many-core architectures. It concludes that hierarchical networks are the best trade-off. In particular, the MultiCross topology is 10 times more efﬁcient than the mesh topology. I . IN TRODUC T ION In response to an ever increasing demand for computing power, the performance of embedded system architectures have improved constantly over the years. Today, multi-core processors have become the mainstream in embedded systems [1]. In this context, according to ITRS [2], a 32% yearly increase in the number of cores will be necessary to keep up with the applications’ needs. ITRS assesses that in 2012, the number of cores in a chip, will exceed 100 cores. In order to guarantee a sufﬁcient energy and area efﬁciency, each part of the architecture must be optimized. In this paper, we will focus on communication structures adapted to shared-memory manycore architectures, also named Network-on-chips (NoCs) [3]. Processors are interconnected with local and shared memory banks. In addition, these many-core architectures must support the execution of dynamic computation-intensive applications. Algorithms have become highly data-dependent and their execution times depend on their input data. Therefore, to optimize the overall execution time, a dynamic data and task allocation is necessary. This dynamic optimization and the increase in the number of processors in a chip generate important communication constraints. Indeed, an off-line task partitioning imposes controlled and known communications, whereas an on-line dynamic task allocation generates unpredictable communications, i.e. network loads, message destinations and sources, or message sizes. This can have an impact on contentions and the efﬁcient bandwidth of the network. For these reasons, this dynamic context must be taken into account in the interconnection design. In the industrial world, multi-domain many-core architectures already exist and use different kinds of NoCs. HL-256 from Plurality [4] uses a butterﬂy multistage network, TILE64 from Tilera [5] prefers multiple meshes, whereas Larabee from Intel [6] implements multiple rings to interconnect their processors. Actually, a NoC is characterized by many parameters and its design space is very large. However, all these parameters have to be accurately chosen to optimize the architecture design. Virtual channels [7] or buffer sizes [8] are some examples of dimensioning parameters. In this paper, we will focus on the network topology, which has a very important impact on performances, especially with a dynamic and unpredictable context. To correctly size the architecture with a dynamic task allocation, it is essential to use a regular network to homogenize performances. In addition, the topology has to support and limit contentions whatever are communications. Some other network topology comparisons have been published. In [9], Salminen et al. list many existing networks and detail their whole parameters (the topology, the size), as well as evaluation criteria. However, no performance or area comparisons are done and their study cannot be entirely used to design new many-core architectures. In [10], Tutsch and Malek make an accurate comparison but only between a mesh and a butterﬂy multistage topology. An other approach, proposed by Murali and De Micheli [11], uses a tool to deﬁne a topology according to application needs. From an application graph, processors are organized around different topologies and tasks are statically allocated on them. The average latency, the area and the power consumption are used as exploration parameters. Unfortunately, this tool cannot deﬁne a multidomain architecture, as well as NoCs in a dynamic execution context. Finally, no existing study offers a large comparison between many NoC topologies for multi-domain, dynamic applications and shared-memory many-core architectures. In this paper, 8 different NoC topologies are compared. This includes 5 well-known basic interconnection topologies named multibus, ring, multistage, mesh and torus. Hierarchical networks are also studied and 3 of them are proposed: CrossTorus, CrossRing and Multicross. Performance comparisons will evaluate their abilities to support network loads, whereas area comparisons with ASIC synthesis will measure 978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.28 189 their area efﬁciency. These comparisons will explore NoC topologies for a many-core architecture in a multi-domain context. The remainder of this article is organized as follows: section II presents the simulation environment for the performance comparison and the ASIC synthesis tool for the area comparison. Section III describes all studied networks. Then, section IV presents an area and performance comparison. An area efﬁciency analysis of these 8 basic and hierarchical networks completes this section. Section V explores the different conﬁgurations of hierarchical networks in order to ﬁnd the best ones. Finally, section VI concludes on the best network topology for many-core shared-memory architectures and dynamic applications. I I . S IMU LAT ION AND SYNTH E S I S ENV IRONM EN T The comparison between our network topologies is done with two main criteria in order to study their area efﬁciency: the performance and the area. This section presents the environment that is used for these evaluations. A. Simulation Environment To evaluate NoC performances, a simulation environment is used to implement these 8 different topologies. This environment performs approximate-timed TLM SystemC communications to provide accurate and fast simulations [12]. This framework implements each network, trafﬁc generator and memory. The conﬁguration of each network is described in the next section. We consider only shared-memory architectures. Thus, each communication is a request to a memory. Access times and access conﬂict timings are computed into memories. Trafﬁc generators send only blocking read request with a ﬁxed data size. 1) Trafﬁc Modeling: We propose to implement two kinds of trafﬁc into our trafﬁc generators, in order to represent two different applications. The ﬁrst one is a uniform trafﬁc that matches with an unpredictable load in the network. A “locality rate” parameter introduces a locality notion for hierarchical networks. This corresponds to the intra-cluster communication rate. The second one is a normal distribution trafﬁc that mainly represents local communications. It stands for a “smart” application mapping on processors through the network. The normal distribution variance gives the main distance that can be reached by messages, whereas the mean represents the closest memory from the trafﬁc generator. Both trafﬁcs are representative of a dynamic multi-domain application. The message sending frequency can be dynamically modiﬁed. The frequency controls the network load during the simulation. When the message creation frequency is higher than the sending rate, messages are buffered in an inﬁnite FIFO memory. 2) Network Modeling: The main part of this simulator is the network modeling, because it gives the principal latency information. All basic networks use the same framework to estimate the time spent in it. This framework is described in a single SystemC module. The network behavior can be Fig. 1. Comparison between Noxim and our approach. (tg: trafﬁc generator) described as follows. First of all, a request is sent to the network and is stored in a list of pending requests. If it is the ﬁrst one in the list, it sets off an event and wakes up the main thread of the network. This thread processes pending requests, calculates the path taken by the request in the network and computes a penalty in case of contention. Then, the request is sent to the destination. When the response comes back, a wait function is launched with the computed communication time as argument. Finally, the response is sent back to the initiator. In the response, the initiator has some information like the number of crossed routers or the time spent in the different modules of the MPSoC platform. The difference between each NoC remains on the path decision and the contention calculation, which provides information on packet collisions. When a request is sent, an entry point ID is given to the request to know the position of the initiator. When the request enters in the network, this one begins with the path calculation. Depending on the routing type and this ID, the network builds a list of virtual routers and links. The time used by a request to enter and exit the router is associated with each element in the list. The network also calculates all contentions with other requests already in the network. The contention calculation is based on the comparison between the path and the timing of the new request, and requests already in the network. A proportional penalty is calculated with the latency of the network, according to the request size and the topology characteristics. For example, if two requests are routed on two different buses in a ring network, they will cross the same router without an added contention. To validate the accuracy of our approach, some comparisons have been done with the cycle-accurate Noxim simulator [13]. For the experiment, random trafﬁcs on a 4x4 mesh, with XY routing using a wormhole technique, are used. Figure 1 shows that our approach obtains an error inferior to 9% until the network overload. After this saturation, our approach has more pessimistic results than the Noxim simulator but this has no impact on this study. This framework has the particularity to support combinations of basic networks. As a result it allows the simulation 190 of a wide range of hierarchical network. In order to build a hierarchical network, a bridge must be placed between the different basic network parts to perform a correct interface. This bridge updates the requests and manages the number of incoming requests at the same time. Updating a message corresponds to changing the entry point ID of the request when it crosses the interface between the NoCs. When a request enters in another network, its entry point ID becomes the bridge ID. A semaphore is needed in the bridge because a TLM channel cannot block multiple requests. This case occurs when two trafﬁc generators send requests that need to transit to another network through the same bridge. If these two requests reach the bridge at the same time, they will access concurrently to the same channel. So the bridge has to manage these multiple requests to have a correct estimation of the time spent waiting to cross this interface. This management is done by a semaphore. An approximate-timed TLM communication can transit through the channel only if it can take a semaphore token. This technique allows to block the initiator thread of the second request and to continue the simulation. In addition it allows a correct timing simulation when exploring hierarchical networks. Indeed, timing is incremented by the SystemC kernel during the waiting phase and no estimation is done. With this bridge all networks can be combined in a hierarchical way to evaluate their beneﬁts and their drawbacks. B. Synthesis Environment All basic networks are described in RTL and synthesized using design compiler [14] from SYNOPSYS. For the mesh and the torus network, the VHDL codes come from the NoCem project [15]. The ring and multistage network are based on modiﬁed NoCem router. All synthesis are made with the TSMC 40nm library [16]. For a hierarchical network, its estimated area is obtained by adding areas of the basic networks composing it. As this estimation technique is based on real synthesis results, it allows realistic comparisons. Combining these two environments offers to ﬁnd the best trade off between performance and area by calculating the accurate area efﬁciency of each network. The next section describes in detail the conﬁguration and parameters of compared networks. I I I . N ETWORK D E SCR I P T ION S For the evaluation, 8 different NoC topologies are studied. This includes 5 well-known basic interconnection topologies named multibus, ring, multistage, mesh and torus. These networks have been chosen because they are representative of the network design space [9]. Hierarchical networks are also evaluated and 3 of them are proposed: CrossTorus, CrossRing and Multicross. We chose to compare hierarchic networks because they present a regular topology and potentially limit the number of contention like in hierarchical bus. First of all, we are going to present the basic implemented networks, then we will describe hierarchical ones. The multibus is implemented as shown in ﬁgure 2-a. In this network, an initiator drives a unique bus. The network Fig. 2. (a) Multibus description. (b) Single part of a mesh or torus network. (c) Multistage description. (d) Part of a ring network. owns several buses which can be shared or not. So changing the number of buses modiﬁes the bandwidth. It is possible to consider only one initiator per bus. In that case, the multibus is considered as a “fully connected interconnect”. For the rest of the paper, a multibus will refer to a fully connected network, i.e. initiators are not connected between them. The mesh and the torus have the same conﬁguration. One initiator and one target are linked together with a router as shown in ﬁgure 2-b. The number of columns and rows of the router matrix can be changed. An XY routing is implemented. These networks use a wormhole technique to transfer packets without virtual channels. The multistage is an indirect fully connected network (Fig.2c). It is divided into different stages which are composed of 4-input-output routers. These routers are linked with a butterﬂy topology. It also uses a wormhole technique to transfer packets without virtual channels. The multistage is used in a speciﬁc way, where all initiators are on the same side and all targets are on the other side. This arrangement simpliﬁes the use of this network and it corresponds exactly to shared-memory architectures. Figure 2-d presents a part of a ring network. In it, one target and one initiator are bounded to a router. A message has to cross every router when it transits through a ring. Each initiator can connect to only one ring. The number of rings will inﬂuence the bandwidth. Each ring is bi-directional. As for the mesh and torus, it uses a wormhole technique to transfer packets without virtual channels. In the case of clustered architectures, each cluster receives a multibus as an inside-cluster network. To allow extra cluster communications, initiator ports and slave ports can be used and reserved on the inside-cluster multibus. We consider a link between the two network levels like the combining of an input and an output of the cluster. For each link, a inter-cluster network is implemented and bridges are placed between networks. And so, if we consider two links between two hierarchical levels, this corresponds to two inter-cluster 191 Fig. 3. Cluster view in a 4 clusters 1 link CrossRing. Fig. 4. CrossTorus with 9 clusters and 2 links. networks. This duplication allows to increase the bandwith and limits contentions between cluster requests. The difference between hierarchical networks presented in the following paragraphs is about the inter-cluster network. The CrossRing is a hierarchical network with a ring as inter-cluster network. The inter-cluster ring parameters can be deﬁned as they could be in the non hierarchical case. Figure 3 presents a cluster view of a CrossRing network with one link between the two network levels. The CrossTorus links the clusters with a torus inter-cluster network. Figure 4 shows a 9-cluster CrossTorus network with two links between the two network levels. As for the CrossRing, the inter-cluster network parameters can be selected. The CrossRing and the CrossTorus are two-level hierarchical networks. The MultiCross implements a particular conﬁguration where 4 clusters are connected around a ring router like on ﬁgure 5. This network offers fast links between a group of 4 clusters without going through the inter-cluster network. It limits the number of nodes in the inter-cluster network and therefore reduces the area. The MultiCross is a 3-level network. The ﬁrst one corresponds to a multibus in each cluster. The second one connects 4 clusters in a group. The last level interconnects cluster groups. Nonetheless, the MultiCross is implemented by only two networks. Indeed, the second level network lies in each ring router. The next section details results from the area and performance evaluation. IV. N ETWORK EVALUAT ION The network evaluation consists of a performance and an area comparison. In addition, an area efﬁciency analysis completes our comparative study. The evaluation platform is Fig. 5. MultiCross with 16 clusters and 1 link. composed of 256 processors, represented by trafﬁc generators, and 256 memories. Even if their number is ﬁxed for each architecture, the processor clusterization and the number of links between hierarchical levels can be modiﬁed. To simplify the exploration space of hierarchical NoCs, only one conﬁguration for each topology is explored. We consider 16 clusters of 16 processors for the MultiRing and the MultiTorus topologies, whereas we focus on 32 clusters of 8 processors for the MultiCross. Each cluster owns 2 links with the upper hierarchical level. These conﬁgurations are explored in the next section. A. Performance Comparison In this paragraph, we will determine the best performance network under different trafﬁcs. We consider the global latency of a message as the time between the message creation and the answer reception in the trafﬁc generator. All considered latencies on next ﬁgures are the mean of 20 simulations with a minimun of 5000 requests by trafﬁc generators. 1000 warmup requests are generated at the beginning of each simulation. The simulation time depends on the packet injection rate. All routing algorithms are Deadlock free. All unﬁnished requests are not recorded and not taken into account in the calculation. The mean is computed from all trafﬁc generator results. In our study, we only consider the MPSoC working area, which is before the network saturation. A network has a better performance if it can handle more network loads before overloading. For this performance comparison, we use 3 different trafﬁcs: a uniform trafﬁc, a uniform trafﬁc with a percentage of requests that stay in each cluster, and a normal distribution trafﬁc. As already explained, the use of a uniform trafﬁc allows to simulate an unpredictable network load. Figure 6(a) shows the NoC average latency as a function of the network load. Because the multibus offers a direct and uniform link to each memory, it reaches the best performance. Actually, the more the distance with all memories is heterogeneous and important, the less the topology is high-performance with a uniform distribution. Thus, for instance, the torus is better than the mesh. The multistage owns a uniform 7-hop memory access, but its links between routers are too long to make this topology interesting. Besides, as depicted in this ﬁgure, the ring topology cannot reach good results with unpredictable network loads. Hierarchical networks are not better than a 192 torus. Their performance is explained by the fact that intercluster communications are penalized by the number of limited inter-cluster external links. Within hierarchical networks, the MultiTorus obtains the best performance, since the torus is well adapted to uniform communications. If we now consider a uniform trafﬁc with a percentage of requests that stay in each cluster, the performance of hierarchical networks increases as shown in ﬁgures 6(b) and 6(c). This parameter changes the repartition of the network load between local network and intra-cluster network. Non-hierarchical networks are not affected by this parameter because they are not clustered. Hierarchical networks become better than the mesh with 50% of intra-cluster communications and better than the torus with 75%. With a rate of 75% of intra-cluster communications, the MultiTorus becomes even better than the multibus. This trafﬁc only shows how the locality affects the hierarchical networks and improves their performance. If this rate increases enough, they all become better than the multibus, which is however high performance oriented. This behavior can be explained by the fact that the multibus inside the cluster has better performance than the ﬂat multibus. Indeed, the performance of the multibus is constrained by its size. Moreover, if communications stay inside a cluster, there is a smaller chance that contentions occur between communications. To correctly estimate the impact of the locality, the comparison has to be done with a different trafﬁc. We choose a trafﬁc under a normal law to represent this phenomenon. We choose the deviance of normal law at 20. This represents the number of memories around the transmitter that receive the majority of the requests. It makes around 80% of memory access locality for a 256 trafﬁc generator and 256 memory architecture. Figure 7 shows the average latency as a function of the network load with a normal distribution and a locality of 80%. As expected, the multibus is still not affected by the trafﬁc type and keeps its high performance. Indeed, all memories are at the same distance for all trafﬁc generators. On the contrary, a topology, with a non-uniform memory access, like the mesh is positively impacted, but this increase is limited by the topology borders. With this trafﬁc, the MultiCross and the MultiTorus have better performance than the torus. It conﬁrms that hierarchical networks are better with local trafﬁcs. Hierarchical networks remain less performant than multibus because the deviance is bigger than the cluster size and many communications go out of the clusters. The local trafﬁc could be representative of a real application behavior executed with a “smart” task allocation. In this case, hierarchical networks offer good performances and remain good candidates for a many-core architecture. Nonetheless, other criteria have to be veriﬁed. The next subsection continues the comparison in term of silicon area. B. Area Comparison This subsection focuses on the area, because it is an important parameter for the design cost of embedded systems. As already mentioned, the platform is composed of 256 (a) (b) (c) Fig. 6. (a) Average latency of all networks as a function of network load with a uniform distribution. (b) Average latency of all networks as a function of network load with a uniform distribution and 50% of inside-cluster requests for clustered networks. (c) Average latency of all networks as a function of network load with a uniform distribution with 75% of inside-cluster requests for clustered networks. trafﬁc generators and 256 memories. All area results are obtained, as explained in the second section, by the synthesis of ﬂat networks and by adding the area of different parts of hierarchical networks. The Table I shows the area of the different basic networks depending on the number of inputs/outputs of the entire 193 Fig. 7. Average latency of all networks as a function of network load with a normal distribution and 80% of locality. Fig. 9. 40nm. Network area in mm2 for 256 inputs/outputs in TSMC AR EA E ST IMAT ION TABL E IN mm2 BA SED ON TSMC 40NM L IBRARY. (X M EAN S THAT NO R E SULT S CAN BE PRODUC ED , B ECAU S E O F TH E MU LT I STAG E CON FIGURAT ION ) TABLE I ↓ Networks/Sizes → Multibus Mesh Torus Ring Multistage 8 0,0193 0,11 0,139 0,106 X 16 0,0706 0,235 0,258 0,196 0,3 32 0,247 0,46 0,524 0,38 X 64 0,905 0,913 1,017 0,698 1,698 256 17,19 4,24 4,337 3,030 9,595 Fig. 8. Evolution of network area in mm2 depending on the number of input. network. These results are also used to build area estimation for hierarchical networks. The symbol “X” means that the multistage network conﬁguration was not achievable. Indeed, with a 4-input-output router, the number of inputs has to be a power of 4. Figure 8 presents the area result table in a graphical view for a better comprehension. It is important to mention that the mesh, the torus and the ring have a linear rising that depends on the network size N. On the contrary, the multibus has an evolution in N 2 and the multistage in N ∗ log(N ). Figure 9 references the area of each basic and hierarchical network for a ﬁxed size of 256 inputs/outputs. The multibus is the biggest network, followed by the 4-parallel ring and the multistage. The size of a network depends on the number of routers and the size of each router. The multibus can be seen as a single router with 256 inputs and 256 outputs, which explains its big size. The ring and the multistage also have a big silicon area because of too many routers. Hierarchical networks in these conﬁgurations are smaller than basic networks. Indeed, they use a multibus inside the cluster, which is the smallest network for reduced number of inputs/outputs. In addition, they limit the number of routers on the inter-cluster network compared to a non-hierarchical network. For all these conﬁgurations, the MultiCross is the smallest network, this is because it uses the crossbar inside routers as a second hierarchical level, and takes advantage of the router natural function. This decreases the area by slightly augmenting the size of each router instead of multiplying them. The biggest difference is between the multibus and the MultiCross topologies. Indeed, the multibus is 17 times bigger than the MultiCross. To be able to combine these two comparisons, the next subsection presents the area efﬁciency as a criterion to conclude this evaluation. C. Area efﬁciency comparison The last two subsections show an area and performance comparison. But for embedded systems, it is important to consider these two criteria together and the area efﬁciency. This subsection presents the area efﬁciency of all presented networks depending on the trafﬁc. The area efﬁciency is not an easy criterion to represent for network. The performance is compared on the curve latency/network load. So, to introduce the area mesurement, we propose to normalize the network load by the size of the network. The new curves allow to compare normalized performance and so the area efﬁciency. Figure 10(a) shows the area normalized network load with a uniform trafﬁc. In general, hierarchical networks are more area efﬁcient than non-hierarchical networks, since they can reach a 194 (a) (b) Fig. 10. (a) Average latency of all networks as a function of area normalized network load with a uniform distribution. (b) Average latency of all networks as a function of area normalized network load with a normal distribution and 80% of locality. high performance with a smaller area. The multibus topology is the most performant, but with its important generated area, this topology becomes one of the less area efﬁcient. This explains why the mesh topology is more used in embedded architectures than the multibus topology. The MultiTorus is 5 times more efﬁcient than the mesh. Moreover, with an unpredictable load, it is also the best hierarchical network, with 33% more area efﬁcient than the MultiRing and 25% than the MultiCross. The difference of area efﬁciency between hierarchical networks with a uniform trafﬁc is explained by their inter-cluster networks. For example, the torus has a better area efﬁciency than the ring. Thus, the MultiTorus is more area efﬁcient than the MultiRing. As depicted in ﬁgure 10(b), the MultiCross remains the most area efﬁcient with a normal distribution and 80% of locality. It is 50% more efﬁcient than the MultiTorus, 10 times more efﬁcient than the mesh and 7 times more than a torus topology. Its high area efﬁciency is due to its structure, which is designed for intra-cluster and neighboring-cluster communications. This evaluation shows that hierarchical networks are more area efﬁcient than non-hierarchical networks for the three kinds of trafﬁcs. The MultiCross remains the best tradeoff if Fig. 11. Network area in mm2 for 256 inputs/outputs depending on different hierarchical network conﬁgurations in TSMC 40nm depending on the number of links (2 and 4), and the number of clusters (16,32,64). a “smart” dynamic allocation is considered on a many-core architecture. V. H I ERARCH ICA L NE TWORK EX PLORAT ION In the previous section, we made an arbitrary choice on the conﬁguration of hierarchical networks. This section explores different conﬁgurations in order to determine the best one. The problem is to correctly size the number of processors into the cluster and the number of links between them. Links limit the number of simultaneous messages. In addition, the more the number of inside cluster processors increases, the more the number of simultaneous inter-cluster communications are. We choose to duplicate the network for each link, although this choice has a direct impact on the ﬁnal area. We consider 2 or 4 links, and 16, 32 or 64 clusters. This means that the number of processors into a cluster is respectively equal to 16, 8 or 4. Area results are depicted in ﬁgure 11. We still consider the area efﬁciency as the main comparison criterion. Figures 12(a), 12(b), 12(c) present the performance normalized by the area with a uniform distribution and 50% of inside-cluster communications. The MultiRing has the best area efﬁciency for the 16-cluster conﬁguration. This is due to the high performance of the ring topology for small network sizes. However, duplicating the inter-cluster network does not offer enough performance compared to the area increase. The same conclusion can be derived with the MultiTorus. On the contrary, the MultiCross divides by 4 the number of routers. Therefore, it is possible to have more clusters without having a big area penalty. Like the MultiRing and the MultiTorus, the performance beneﬁt from duplicating the inter-cluster network is not enough in comparison with the area increase. The chosen conﬁgurations for the previous comparisons are the most efﬁcient and this exploration shows that it is necessary to correctly choose the number of clusters and links to obtain a good efﬁciency. If the architecture size changes, these conﬁgurations will not necessary be the best. 195 and data allocation is needed to optimize the architecture efﬁciency. This article deals with the problem of interconnecting hundreds of processors and memories in order to obtain an architecture for a multi-domain context. It focuses more speciﬁcally on the topology of the network-on-chip. The evaluation was done in terms of silicon area, performance and area efﬁciency. The ﬁrst conclusion is that hierarchical networks present an area efﬁcient solution. Moreover, the mesh, which is today a popular topology, has a bad area efﬁciency compared to hierarchical networks. The evaluation shows that the MultiCross has the smallest area and the MultiTorus is the most performant in hierarchical networks. The MultiCross presents the best area efﬁciency with local trafﬁcs, since it allows inexpensive communications between 4 clusters without disturbing other communications in the intercluster ring. The MultiCross is the best topology for a manycore architecture with a “smart” task and data allocation. The next step is to evaluate the power consumption which is also a criterion for embedded systems. “International Technology "
Improving the Performance of GALS-Based NoCs in the Presence of Process Variation.,"Current integration scales allow designing chip multiprocessors (CMP) where cores are interconnected by means of a network-on-chip (NoC). Unfortunately, the small feature size of current integration scales cause some unpredictability in manufactured devices because of process variation. In NoCs,variability may affect links and routers causing that they do not match the parameters established at design time. In this paper we first analyze the way that manufacturing deviations affect the components of a NoC by applying a comprehensive and detailed variability model to 200 instances of an 8×8 mesh NoC synthesized using 45 nm technology. A second contribution of this paper is showing that GALS-based NoCs present communication bottlenecks under process variation. To overcome this performance reduction we draft a novel approach, called performance domains, intended to reduce the negative impact of variability on application execution time. This mechanism is suitable when several applications are simultaneously running in the CMP chip.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip Improving the Performance of GALS-based NoCs in the Presence of Process Variation C. Hern ´andez, A. Roca, F. Silla, J. Flich, and J. Duato {carherlu, anrope2}@gap.upv.es, {fsilla, jﬂich, jduato}@disca.upv.es Universidad Polit ´ecnica de Valencia Abstract—Current integration scales allow designing chip multiprocessors (CMP) where cores are interconnected by means of a network-on-chip (NoC). Unfortunately, the small feature size of current integration scales cause some unpredictability in manufactured devices because of process variation. In NoCs, variability may affect links and routers causing that they do not match the parameters established at design time. In this paper we ﬁrst analyze the way that manufacturing deviations affect the components of a NoC by applying a comprehensive and detailed variability model to 200 instances of an 8x8 mesh NoC synthesized using 45nm technology. A second contribution of this paper is showing that GALS-based NoCs present communication bottlenecks under process variation. To overcome this performance reduction we draft a novel approach, called performance domains, intended to reduce the negative impact of variability on application execution time. This mechanism is suitable when several applications are simultaneously running in the CMP chip. I . INTRODUC T ION Several years ago chip multiprocessors (CMP) became the common choice to increase computing power while keeping energy consumption inside the power budget. As the number of cores increases, it is not feasible to interconnect them by using a bus or a crossbar due to scalability concerns, and thus a network-on-chip (NoC) is used. For example, a ring is used in the recent Nehalem-EX processor. This is also the case for the Tile-Gx100 chip [28], which includes ﬁve parallel 10x10 bidimensional meshes. In general, both academia and industry agree that NoCs are the best option for interconnecting a high number of cores [10], which will be the common case as VLSI technologies continue leveraging larger integration scales. CMP chips are feasible because of the large integration scales currently used. Unfortunately, the small feature size of current integration scales also cause some unpredictability in manufactured devices because of process variability. The consequence of process variation is that delay characteristics of manufactured devices do not exactly match the parameters established at the design phase. Actually, variability arises as one of the most important challenges to tackle in the development of new on-chip system architectures as technology scales down from 65nm to 16nm [18]. Several are the sources of variability, like variations in Lef f (effective transistor channel length) and also variations in the threshold voltage. These variations are basically a consequence of deviations introduced in the photolitographic process and due to random dopant ﬂuctuations, respectively. Additionally, capacitance and resistance variations introduced as a consequence of varying wire dimensions because of defects in the chemical metal planarization process are another source of uncertainty in current and future chips. Recently, a comprehensive and detailed model for characterizing variability in NoC links has been proposed [6], clearly showing that links are affected by this phenomenon. This model can be extended to NoC routers in order to analyze how process variation simultaneously affects both components of the network. This analysis would be extremely useful for NoC designers. In this paper we take this challenge and thoroughly analyze the behavior of links and routers in the presence of variability, showing that not all of them can operate at the same frequency. Having a network where not all of its components are able to switch at the same frequency requires that NoC designers take a decision on how to face this challenge. The most basic approach is to lower the operating frequency for the entire NoC to the frequency of the slowest component. However, when process variation causes large differences in frequencies, this solution may lead to a great performance penalty. For example, in [6], it has been shown that differences in link frequency for a given chip may range from 1.3x in a 45nm technology to 1.8x in a 16nm one. Thus, setting NoC frequency to the one of the slowest component may cause considerable drops in aggregated bandwidth. A more sophisticated approach to face the fact that not all NoC components are able to switch at the same frequency is allowing that each of them works at its maximum achievable frequency. Such a system is known as GALS system (Globally Asynchronous Locally Synchronous) [12]. The concept behind GALS is quite simple, although implementing such a NoC is not that easy because current design tools are not ready yet for the design of GALS circuits. Nevertheless, GALS systems do not avoid the performance reduction introduced as a consequence of different routers and links switching at different frequencies. As we will show in this paper, slower network components tend to become bottlenecks that create congestion, which slows down network trafﬁc, thus reducing network performance. The conclusion that can be learnt from the two approaches just presented is that technological solutions to face the different frequencies present in a NoC are not enough, by themselves, to mitigate the impact of process variation in NoCs. On the opposite, some kind of architectural solution is required in addition to the underlying technological approaches. Thus, combining both solutions, the technological and the architectural ones, the impact of process variation on network performance may be drastically reduced. We end this paper by drafting such an architectural approach. This approach, called Performance Domains, is useful in the case that the chip is simultaneously executing several applications, assigning some of its cores to each of them. Note that this will be the common case as CMPs increase the number of cores. Virtualization techniques (deﬁning parts of the chip as virtual machines) will also deploy such partitioned scenario. The rest of the paper is organized as follows. Section II 978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.13 35 presents previous work in this area and assesses how this paper advances the state of the art in the process variation arena. This section also introduces which are the contributions of this work. Section III presents the NoC that will be used as test bench to apply the new variability model for NoCs introduced in Section IV. Later, in Section V, that model is applied to the NoC designed in Section III in order to assess how process variation affects the components of the network. The results from this section are used in Section VI to analyze the performance of NoCs in the presence of variability. Next, in Section VII, we draft a new approach intended to reduce the impact of process variation in application execution time. Finally, in Section VIII, the main conclusions of this work are presented. I I . R ELATED WORK AND CONTR IBUT ION S Many recent works have already partially analyzed variability in NoCs. For example, in [13] the authors identify the main sources of process variation in NoC links and provide an analytical expression of timing variability from the variation of parameters involved in the interconnect delay. However, this study lacks the inﬂuence of spatial features of variability in NoC links. Other studies focus on the impact of withindie variation in devices without considering variations in the links [22][20]. However, links in the NoC are also affected by variability as shown in [13] and [6]. The impact of process variation is also analyzed in [14]. However, this study presents two important drawbacks. First, although the consequences of variability in the router architecture are deeply analyzed, this study does not consider delay variations between routers. Additionally, random variability is inaccurately analyzed because it is simplistically modeled as a percentage of the nominal delay, thus not considering other studies that show that random variations strongly depend on the critical path depth and the size of devices [7] [6]. The second drawback of [14] is that it states that variability will not considerably affect NoC links because it is possible to take advantage of the huge slack present in links in order to compensate delay variations. However, as shown in [6], this is only true when links are designed to present minimal delay at the expense of a tremendous power consumption [4]. As can be seen, there is no study that simultaneously analyzes the impact of variability in routers and links. Nevertheless, the point where most studies agree is that process variation causes that routers in a NoC present different delays, despite that they were initially designed to be identical. The same is true for links. In order to deal with this variety of frequencies in the NoC and thus avoid reducing the frequency of the entire NoC to that of the slowest component, the GALS mechanism could be leveraged. GALS was initially proposed to support mesochronous systems where there is not a single clock signal that synchronously clocks all parts of the system, but they are clocked by independent signals having the same frequency but, probably, different phase. Thus, GALS perfectly supports a NoC where its components operate at different frequencies. Additionally, [16] analyzed how to efﬁciently build voltage and frequency islands from a GALS-based NoC. Nevertheless, although the GALS technique enables addressing a system having components working at different frequencies, it is not able to minimize the impact of process Technode (nm) Link length (mm) Core area (mm2 ) Vdd (V) M4-M5 Width (nm) M4-M5 Spacing (nm) M4-M5 Thickness (nm) M4-M5 Height (nm) Dielectric TABLE I 45 0.83 0.48 0.9 140 140 280 290 2.5 PHY S ICAL AND ELECTR ICAL DATA FOR THE 45NM NOC variation in NoCs. Actually, this claim is one of the three main contributions of this paper. We will show that GALSbased NoCs present communication bottlenecks caused by the slower routers and links. These slower components cause congestion that spans to the rest of the network, thus reducing performance. In order to carry out this performance analysis, accurate data about the operating frequency of routers and links in the presence of process variation must be gathered. We have collected this frequency data by synthesizing a entire NoC with a 45nm technology and applying an extension of the variability model in [6] to it. This process variation analysis is another of the contributions of this paper. Moreover, it is noteworthy to mention that, up to our knowledge, this is the ﬁrst time that such a comprehensive and detailed variability model has been simultaneously applied to all components of a NoC, that is, routers and links. Actually, the variability model presented in this paper is more accurate than previously presented ones, as will be shown later. The last contribution of this paper is the draft of an architectural proposal for reducing the impact of process variation on the average execution time of applications running in the CMP chip. This proposal, called performance domains, is based on assigning regions of the CMP chip to applications considering not only the availability of idle cores but also tacking into account process variation data. I I I . D E S IGN ING A 1 . 33GHZ N ETWORK -ON -CH I P In order to analyze the inﬂuence of process variation on NoC performance, we need a network to be used as a test bench so that the variability model presented in Section IV can be applied to it. Therefore, we have synthesized an 8x8 2D-mesh network using 45nm technology. To do so, we have designed a wormhole router from scratch. The ﬁrst concern to address when designing such a network is which will be the CMP conﬁguration the network will be embedded into. To answer this question, a 65nm real implementation NoC layout [15] has been considered. According to this implementation, all cores are identical, and their size is 1mm2 . Additionally, the gap between cores is 0.2mm. Therefore, links connecting NoC switches are 1.2mm long. In this study, we are going to synthesize an 8x8 CMP using 45nm instead of 65nm. Therefore, core size and link length must be appropriately reduced according to the feature size of the 45nm technology. Table I shows the dimensions for cores and links once scaled down to that technology, as well as other physical and electrical parameters of the 45nm technology, as stated in [25]. In the following sections, the router and links designed to be used in such a CMP are described. 36 A. Router Architecture In this section we describe the router design used throughout this paper. The router is a pipelined input buffered wormhole router with ﬁve stages: input buffer (IB), routing (RT), switch allocator (SW), crossbar (XB), and link traversal (LT). We have designed a simple router with no virtual channels and ﬁve input and output ports. Thus, four ports are intended to provide connectivity with the neighboring routers in the 2D mesh and the ﬁfth port connects to the local computing core. Link width is set to 4 bytes. Flit size is also set to 4 bytes. Input buffers can store four ﬂits. A Stop&Go ﬂow control protocol has been deployed in order to control the advance of ﬂits between adjacent routers. Additionally, the routing stage has been implemented to support the XY routing algorithm. Moreover, there is a RT module for each input port. Similarly, one SW module has been implemented for each output port, although all the ﬁve SW modules work in a coordinated way, obviously. Finally, each SW module has been designed using a round-robin arbiter according to [21]. The router has been implemented using the 45nm technology open source Nangate [25] with Synopsys DC. We have used M1-M3 metalization layers to perform the Place&Route with Cadence Encounter. Table II summarizes the frequency and area results of the router implementation. area / Freq area (um2 ) freq (GHz) Prelayout 17651 1.75 Postlayout 19779 1.33 TABLE II ROUTER AREA AND FREQUENCY Table III summarizes the delay, area, and number of gates for each of the modules of the router. Note that the area numbers in that table are for a single instance of each module, but some of them are replicated in the designed router. This has to be taken into account if area numbers in Tables II and III are compared. Additionally, the area numbers in Table III have been obtained by independently synthetizing each module to work at its maximum frequency. When, on the opposite, the whole router is synthesized at once, those numbers slightly change. On the other hand, it is noteworthy to mention that, although the number of gates in the critical path in the XB stage is not the highest one, the gates present in it are slower than the gates present in other stages due to the large load the gates in the XB stage support, thus causing that the XB stage becomes the bottleneck in our router. module IB RT SW XB area (um2 ) 3113.45 124.26 337.88 1975.6 critical path (ns) 0.55 0.32 0.52 0.75 gates 177 72 35 519 critical path depth 6 8 12 8 AREA , DELAY, AND NUMBER O F GATE S FOR THE ROUTER MODULE S TABLE III B. Link Design When designing a link several concerns must be taken under consideration for the sake of efﬁciency. More speciﬁcally, power and area must be optimized for a target link delay. For example, as links are usually long interconnects, they will present a considerable capacitance and resistance. To deal with them, repeaters are used. Actually, repeater insertion is an efﬁcient method to reduce interconnect delay and signal transition times. In our case, in order to minimize power consumption [4] we have chosen the proper number of minimum sized repeaters that satisﬁes the delay constraint imposed by the frequency of the router presented in the previous section. As shown before, the post-layout router delay is equal to 0.75ns. Therefore, our link has been designed to present a delay similar to the delay of the router in order to save power. To satisfy those premises we choose a supply voltage equal to 0.9V and a link consisting of 5 repeaters of size 2. Additionally, links are placed in metalization layers M4 and M5, whose physical dimensions are shown in Table I. With this conﬁguration we obtained a nominal link delay of 0.67ns by using SPICE and the PTM model for 45nm [26] for the link simulations. IV. CHARAC TER IZ ING VAR IAB IL ITY IN NOC S Variability sources can be divided into front-end and backend ones. The front-end phase of the IC fabrication process is related with the steps involved in the creation of devices whereas the back-end stage comprises steps involved in the wiring deﬁnition. Front-end process variation can be further decomposed into systematic and random components. It is commonly accepted that random and systematic variations are uncorrelated [20]. Additionally, spatial correlation depends on some ρ parameter dependent on the exact manufacturing process. The ρ parameter represents the fraction of the chip that is correlated. Values for the ρ parameter used in this study range form 0.5 (half of the chip size) to 1 (the entire chip) according to [20]. On the other hand, back-end variation sources are capacitance and resistance variations due to defects introduced by the chemical metal planarization process in metalizations, for example. The way back-end variation affects a NoC depends on the exact dimensions of the metalizations considered. However, as shown in [6], when NoC links are designed for minimizing power consumption, delay variation due to back-end variation is less than 0.1% because in that scenario link delay is dominated by repeater delay and therefore variations in wire resistance have a negligible contribution to the resulting link delay. For these reasons, we are not going to consider in this study the inﬂuence of back-end variability. A. Modeling Front-end Systematic Variation The systematic component of variation is strongly related with the photolitographic process. Lens aberrations may lead to an important systematic spatial non-uniformity of Lef f over the reticle ﬁeld. According to [24], 3σ Lef f can be as high as 12% for 45nm processes. Additionally, in order to model the spatial non-uniformity of Lef f as well as its correlation we have used Gaussian Random Fields (GRF). When using GRF with stationary and isotropic ﬁelds, the variance (σ2 i ) of the random ﬁeld L(x, y), representing transistor gate length (Lef f ) in the (x,y) die position depends only on the euclidean distance between two given locations [20]. Then, the gate length distribution (L) only depends on a correlation function. The correlation model we have used is the spherical model proposed by [20], which is derived from the measurements of [5]. In this model we have used two different values for the ρ parameter: ρ = 0.5 and ρ = 1. 37 In order to be able to catch the spatial features of variability, the layout of the NoC implementation presented in the previous section has been considered to compute Lef f maps on it. To do so, the chip surface has been discretized using a 1000x1000 square matrix and R [27] has been used to implement the Gaussian Random Fields with the spherical model mentioned before. Note that the matrix size used allows that each router in the NoC is assigned 467 points of that matrix and each link is represented by 127 points. Once the Lef f values are computed for all the chip surface, the delay for every gate in routers, or repeaters in links, is computed according to the explanation in Section IV-C B. Modeling Front-end Random Variation The main source of random variation in NoCs is threshold voltage variation due to Gaussian Random Dopant Fluctuations (RDF). According to [8], RDF will increasingly affect deep submicron technologies scaling below 45nm. We set 3σVth variations to 40% according to [24] for a 45nm technology. To compute σVth for a given device, according to [1], we can use Equation 1, and then relate the σV th0 value of the minimum size device with the σVth of a device of size h, as shown in Equation 2, which clearly shows that σVth can be minimized by increasing the width of devices, represented by h. However, this would increase the area required to implement the circuit as well as the power consumption, which is not a good option. σVth ∝ 1 pWef f Lef f (1) σV th = σvth0√h (2) C. Introducing Process Variation into Routers and Links Once we know the behavior of random and systematic variations we have to introduce both variability sources in our target design. To do so, we will separately simulate the presence of variability in links and routers. In order to introduce process variations in routers, they will ﬁrst be synthetized using Synopsys DC and Placed&Routed using Cadence Encounter. Then, after post-layout synthesis, we will obtain an accurate .sdf ﬁle representing the delay of each cell of the router and a .def ﬁle representing the location of a given cell. We will use the .def ﬁle in order to map the Lef f values previously computed with the R tool to individual logic gates at each of the routers in the network. Then, we will calculate the delay of each router cell according to the value of Lef f assigned to it and will modify the .sdf , which will be used by the PrimeTime tool in order to compute the delay of the router. This methodology was already used in [22]. Note that in our study, delays for different routers are individually computed, thus taking into account their position in the die. In order to compute gate delay we know from [17] that delay of devices can be represented by Equation 3. L1.5 D ∝ ef f ∗ V dd (V dd − Vth )α Vthef f = Vth0 − V dd ∗ exp(−αdiblLef f ) (3) (4) For applying that equation, we need to know Vth which will simultaneously depend on systematic and random variations. In order to compute the systematic component of Vth , we have to catch the dependence of the threshold voltage with gate 38 length. This dependence is satisﬁed with the model presented in [3] and shown in Equation 4. In that equation Vth0 is the threshold voltage for long channel transistors and αdibl is the DIBL coefﬁcient. Then, applying to those formula parameters for the 45nm technology taken from [7] we get the systematic component of Vth . Regarding the random component of Vth , it is obtained by using Equations 1 and 2. Note that we know the exact size for each cell in our design and therefore we will compute individualized random variations for each of them. Actually, this is an improvement over previous studies that incorporated random variability over design netlists because they assume that all gates in the design were equally affected by random variability independently on their size. The last step is adding the effect of random variation and systematic variation and compute the ﬁnal Vth value for a given device. To do so, the resulting value of the random σVth is added to the systematic Vthef f and the result is then used in Equation 3 for asserting the ﬁnal delay value of the device. In order to introduce process variations in links, they are simulated with SPICE using the PTM model of 45nm [26] to be able to introduce variations. The methodology to characterize the presence of variations in NoC links is presented in [6]. The location of links is known based on the real NoC implementation presented in [15]. With the location of links and the number of repeaters of the designed link, we know exactly the Lef f value of each repeater of each link in the network. As PTM does not directly uses the effective channel length we translate Lef f variations into Lgate variations. Note that from [23] we know that Lef f = Lgate − 2 ∗ Lint + XL . Where Lint and XL represent the channel length offset due to mask effect. Random variations, as in the case of the routers, are introduced varying the threshold voltage taking into account the size of repeaters. V. ANALYZ ING VAR IAB IL ITY IN NOC S A. Variability in the Router In order to analyze the effects of variability in the router presented in the previous section, we applied the variability model to 200 instances of our 8x8 mesh NoC synthesized using 45nm technology and studied how variability modiﬁes the operating frequency of each of the routers of the network and also each of their modules. From the 200 NoC instances analyzed, 100 of them were produced using a value equal to 1 for the ρ correlation value while the other 100 were produced with ρ = 0.5. It is important to remind the reader that the nominal operating frequency without variability is 1.333 GHz. We analyze the inﬂuence in the router of random and systematic variations. Figures 1(a) and 1(b) show the probability distribution function (pdf ) of the operating frequency of the router and each of its stages in two scenarios: when only systematic variation with correlation 1 is considered (1(a)) and when only random variation is taken into account (1(b)). Figures 1(a) and 1(b) show that systematic variation has a larger inﬂuence in the operating frequency of the router than random variability. Additionally, as is shown in ﬁgure 1(a) frequency variations in a router when only systematic variability exists present similar pdf for all the modules. This is due to the fact that systematic variability is highly correlated. Thus, as the router 1.5 1 0.5 0 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 router 1.5 1 0.5 0 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 ib module 1.5 1 0.5 0 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 rt module P e r e g a n e c t 1.5 1 0.5 0 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 sw module 1.5 1 0.5 0 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 xb module Nominal Frequency = 1.333GHz (a) Only systematic Lef f variations considered(ρ = 1). 0 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 5 router 1.5 1 0.5 0 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 ib module 3 2 1 0 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 rt module P e r e g a n e c t 0 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 5 sw module 1.5 1 0.5 0 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 xb module Nominal Frequency = 1.333GHz (b) Only random Vth variations considered. Fig. 1. Frequency variation in router pipeline is small, all the components of the router are affected by variability in a similar way. This means that if the frequency of one of the stages is reduced because of systematic variability, then the frequency of the other stages will probably be also reduced. Therefore, it can be seen as a biased variability that causes that the critical path does not change among the routers, thus making that the XB stage remains being the bottleneck of almost all instances of the routers. On the other hand, frequency variation in a router when only random variability is considered does not present similar pdf for all the stages. This is due to the nature of random variability which differently affects two adjacent components. Thus, random variability may even be reduced or canceled as the number of gates in a chain of logic increases [6]. Therefore, the critical path of a stage may change depending on the variability map of each router. Thus, the peaks of the pdf in ﬁgure 1(b) represent a different critical path inside the corresponding stage. This non-biased variability makes that the bottleneck of the router will not always be the XB stage but other stages can constraint the maximum operating frequency of the router. This can be seen in Table IV which shows the correlation between the operating frequency of the stages and the operating frequency of their router. Note that the correlation when only systematic (sys) variability is considered is higher than when only random (rnd) variability is taken into account. Additionally the correlation of the XB stage is the highest one in all cases, as explained before. Figure 2 shows the probability distribution function (pdf ) of the operating frequency of the router and each of its stages when systematic and random sources of variation are simultaneously considered and correlation is ρ = 1. Table IV shows that there exist small differences in frequency, between high and low correlation. Table V shows the main parameters of each conﬁguration described above. It shows the nominal, maximum, mean, and minimum frequencies and the frequency variation of each pdf. 1.5 1 0.5 0 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 router 1.5 1 0.5 0 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 ib module 1.5 1 0.5 0 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 rt module P e r e g a n e c t 1.5 1 0.5 0 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 sw module 1.5 1 0.5 0 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 xb module Nominal Frequency = 1.333GHz Fig. 2. Frequency variation in router pipeline as a consequence of both systematic and random variations. Frequency variation is computed as (σ/µ) where σ is the standard deviation and µ is the mean of the pdf. Data in Table V conﬁrm that the exact value of th ρ parameter introduces very small differences. Moreover, random variability moves the mean frequency more than the systematic one. As mentioned before, this is due to the fact that random variability makes that the critical path changes from one instance of the router to another more often than systematic variability. B. Variability in Links Figures 3, 4, and 5 show link operating frequency variation as a consequence of systematic Lef f variation, random Vth 39 stage / variability IB RT SW XB sys(ρ = 1) 0.9935 0.9968 0.9945 1.0000 sys(ρ = 0.5) 0.8941 0.7617 0.9857 0.9997 rnd 0.1493 0.2867 0.0938 0.9998 sys(ρ = 1)+rnd 0.9659 0.9685 0.8896 0.9754 sys(ρ = 0.5)+rnd 0.8318 0.7651 0.8833 0.9876 TABLE IV CORRELAT ION BETWEEN STAGE DELAY AND ROUTER DELAY Parameters Nom. Freq. Max. Freq. Mean Freq. Min. Freq. sys(ρ = 1) 1.3333 1.7153 1.3339 1.0707 0.0672 sys(ρ = 0.5) 1.3333 1.6639 1.3344 1.0811 0.0684 rnd 1.3333 1.3661 1.3101 1.2422 0.0124 sys(ρ = 1)+rnd 1.3333 1.6835 1.3099 1.0672 0.0681 sys(ρ = 0.5)+rnd 1.3333 1.6420 1.3112 1.0672 0.0692 σ/µ TABLE V NOM INAL , MAX IMUM , MEAN AND M IN IMUM FREQUENC IE S AND FREQUENCY VAR IAT ION O F A ROUTER 0 1.3 1.35 1.4 1.45 1.5 1.55 1.6 1.65 Frequency. Original Frequency = 1.4843GHz 1.7 1.75 0.5 1 1.5 2  Link frequency variation (correlation = 1) P e r n e c t e g a 0 1.3 1.35 1.4 1.45 1.5 1.55 1.6 1.65 Frequency. Original Frequency = 1.4843GHz 1.7 1.75 0.5 1 1.5 2  Link frequency variation (correlation = 0.5) P e r n e c t e g a Fig. 3. variations. Link frequency variation as a consequence of systematic Lef f variations, and both random and systematic variations, respectively. Figure 3 shows that when considering only systematic variations, link operating frequency varies between 1.35GHz and 1.7GHz, for both values of ρ, despite that the nominal frequency was 1.48GHz. The exact value of correlation does not introduce signiﬁcant differences, as shown by the σ parameter (σ = 7.2 and σ = 7.0 for ρ = 1 and ρ = 0.5, respectively). When only random variations are analyzed, variations in wires move practically in the same range than systematic variations. The bottom plot of Figure 4 shows the maximum achievable operating frequency of all wires of each link in the network. However, as all wires of a given link have to work at the same frequency, the slowest wire will cause a considerable operating frequency slowdown. This is shown in the top plot of Figure 4. Where the mean frequency is reduced to 1.34GHz and the frequency variation is reduced to 5.58%. This effect is similar to the behavior analyzed by Bowman et. al in [2]. In this work it is shown that when the number of critical paths increase the mean delay increases and the standard deviation decreases respectively. In the case of links, a higher number of wires per link will cause a higher frequency slowdown but also a reduction in the σ of the link operating frequency. Finally, Figure 5 shows that when random and systematic variations are simultaneously considered the average link operating frequency is reduced as a consequence of random variations. The mean values of link operating frequency are 1.31GHz and 1.32GHz (ρ = 1 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 Frequency. Original Frequency = 1.4843GHz 1.8 1.9 0 1 2 3  Link frequency slowdown due to random variation P e r n e c t e g a 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 Frequency. Original Frequency = 1.4843GHz 1.8 1.9 0 0.5 1 1.5 2 Link frequency random variation P e r n e c t e g a Fig. 4. Link frequency variation as a consequence of random Vth variations. 0 1.1 1.15 1.2 1.25 1.3 1.35 1.4 Frequency. Original Frequency = 1.4843GHz 1.45 1.5 0.5 1 1.5 2 Link frequency rnd+sys variation (correlation=1) P e r n e c t e g a Fig. 5. Link frequency variation for both systematic and random variations. and ρ = 0.5). However, frequency variation of links is almost the same than when considering only systematic variations, σ = 7.2 and σ = 7.5 for ρ = 1 and ρ = 0.5, respectively. Note that, due to space limitations, the plot for ρ = 0.5 is not shown. V I . CON S EQUENC E S O F VAR IAB IL ITY IN GALS -BA S ED NOC S In this section we analyze the consequences of variability in GALS-based NoCs. For that purpose, we have simulated the 8x8 CMP test bench network and collected performance metrics, mainly link utilization, message latency and network throughput. The trafﬁc pattern used is intended to simplistically emulate the coherent trafﬁc present in a CMP chip, and is the composition of two different types of trafﬁc. The ﬁrst one, which accounts for the 60% of the overall network trafﬁc, follows a uniform destination distribution and tries to emulate cache-coherent trafﬁc between cores. The second 40 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 (a) ρ = 0.5 (b) ρ = 1 Fig. 6. Network performance in the presence of process variation. (a) ρ = 0.5 (b) ρ = 1 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.1 1.1 1.1 1.1 1.1 1.1 1.1 1.1 1.1 1.1 1.1 1.1 1.1 1.1 1.1 1.1 1.1 1.1 1.1 1.1 Fig. 8. Operating frequency distribution in a NoC in the presence of PV. Applications Execution time Speed-Up (%) Applications Execution time Speed-Up (%) (a) Nominal network (b) Network with PV  14  12  10  8  6  4  2  0  14  12  10  8  6  4  2  0 # # # # # # # # # # # # # # # # # # # # # # # # # 0 1 2 3 4 8 9 1 1 1 1 1 1 1 2 2 2 2 2 2 3 3 3 3 3 0 1 2 6 7 8 9 0 4 5 6 7 8 2 3 4 5 6 # # # # # # # # # # # # # # # # # # # # # # # # # 0 1 2 3 4 8 9 1 1 1 1 1 1 1 2 2 2 2 2 2 3 3 3 3 3 0 1 2 6 7 8 9 0 4 5 6 7 8 2 3 4 5 6 Fig. 7. Link utilization in the presence of process variation. (a) ρ = 0.5 (b) ρ = 1 trafﬁc is intended to emulate memory accesses targeted to on-chip memory controllers. We have simulated a CMP chip having four memory controllers located at both sides of the chip: two of them on the right side and the other two on the left side. Additionally, the two memory controllers on the right are directly connected to the 4 upper or 4 lower routers in the right edge of the network. The memory controllers on the left side follow the same interconnection pattern. In this conﬁguration, every core sends 30% of the messages it generates to the memory controller closer to it and the remaining 10% to other memory controllers, with a uniform distribution. Figure 6 shows the average message latency versus received trafﬁc for several of the chips analyzed. Low and high correlation values are considered (ρ = 0.5 and ρ = 1.0). As can be seen, in the presence of variability (curves labeled “Chip #n) the network is able to manage almost 20% less trafﬁc than in the absence of process variation. Moreover, average message latency is increased by 23% even for low trafﬁc loads. On the other hand, it is interesting to notice that the overall performance of the network in the presence of variability is almost independent of the exact characteristics of that variability. This is shown by all the ”Chip #n curves” being almost overlapped. Nevertheless, when the correlation of the manufacturing process is lower, we can see more differences in network performance. One of the reasons for the differences in performance shown in Figure 6 is the lower average network bandwidth caused by link random variation (note that systematic variation does not cause a reduction in the aggregated network bandwidth). However, this reduction in network bandwidth does not completely explain the plots in Figure 6. An important contribution to that performance reduction is shown in Figure 7, that displays utilization for all links in the NoC when it is close to saturation both with and without process variation. As can be seen, process variation causes larger differences in link utilization, as shown in Figure 7(b), where a few links present a much larger utilization than others (on the right end) or a much Fig. 9. Speed up when executing freqmine on 9-cores. lower utilization (left end). This uneven distribution of link utilization helps explaining the performance loss in Figure 6. V I I . PER FORMANC E DOMA IN S According to the results in previous sections, the traditional synchronous design technique is not feasible anymore because NoC clock frequency should be lowered to match the frequency of the slowest component in the NoC, noticeably reducing network performance. This fact is widely known by NoC architects. As a solution, GALS systems have been proposed [16]. In these systems, each of the components of the network is clocked at the maximum frequency it allows. However, this technological patch has several architectural consequences. The immediate one is that slower components quickly become bottlenecks, as shown in the previous section. Therefore, in order to avoid those bottlenecks, we should devise an architectural improvement to current NoCs. This improvement, which we have named Performance Domains, is based on two different observations. The ﬁrst one is that frontend systematic variations cause different delays for different components in the network, but those differences are quite small for adjacent routers and links, due to correlation, as shown in Figure 8. The second observation is that many-core chips are usually devoted to several applications simultaneously, by means of virtualization mechanisms. This virtualization layer assigns cores to applications on demand and usually isolates trafﬁc among the applications so that trafﬁc from one application does not traverse a NoC region assigned to another application. When assigning cores to applications, if no variability data is taken into account, we may assign to an application some cores belonging to a fast area of the chip and some other cores belonging to a slow area. In this case, communication among cores using such an assignment would end up running at the frequency of the slowest router or link because of the bottlenecks mentioned before, causing that the fastest 41 Applications Execution time Speed-Up (%) Applications Execution time Speed-Up (%)  10  8  6  4  2  0  10  8  6  4  2  0 # # # # # # # # # # # # # # # # 1 2 3 4 8 9 1 1 1 1 1 1 2 2 2 2 0 1 6 7 8 9 4 5 6 7 # # # # # # # # # # # # # # # # 0 1 2 3 8 9 1 1 1 1 1 1 2 2 2 2 0 1 6 7 8 9 4 5 6 7 (a) ρ = 0.5 (b) ρ = 1 Fig. 10. Speed up when executing swaptionts on 16-cores. resources are underutilized. On the opposite, we propose to assign cores to applications taking into account variability information, thus assigning to a given application cores that are interconnected by links and routers working at similar frequencies. This assignment would not waste faster resources because those resources would not be mixed with slower ones. As an example of the impact on application time of a variability-aware core assignment, we have simulated the execution of two different applications from the PARSEC benchmark suite on our 8x8 CMP test bench. The ﬁrst one is the freqmine benchmark running on 9 cores. The second one is the swaptions benchmark running on 16 cores. In the ﬁrst case we have run the 9-core freqmine program in all the square 9-regions available in the 8x8 CMP shown in Figure 8. In the case for the 16-core swaptions application, we have performed the same test but with square 16-core regions. Obviously, cache coherency between cores is enabled. Figure 9 shows the speed up for the different executions of freqmine on 9 cores. Labels in the X axis denote the identiﬁer in the 8x8 CMP of the top-left core of the 9-core subCMP. As can be seen, depending on the application placement, execution time could be reduced down to 14%. Figure 10 shows that for the swaptions application execution time may be reduced by 10%. Nevertheless, an efﬁcient variability-aware assignment may be complex to achieve. First, at initialization time, the chip must run a self test to get link and frequency information and use that information in such a way that building a given performance domain later is a straight-forward task that does not consume too many resources. Additionally, when creating a performance domain not only variability data has to be taken into account, but also the performance of the underlying routing algorithm for a given performance domain shape. In this way, performance domains should present a good bisection bandwidth in order to allow for high network throughput. Moreover, as we may end up having non-rectangular domains, the XY routing algorithm is not a valid option anymore. Efﬁcient routing algorithms and routing implementations should be deployed, like the LBDR implementation [19], which additionally allows that routers and links belong to more than a routing region simultaneously. This will be required for accessing the on-chip memory controllers, which will probably be located outside the performance domain. V I I I . CONC LU S ION S This paper presents a variability model that simultaneously considers process variations in routers and links. We have applied that model to an 8x8 mesh NoC implemented in 45nm 42 technology, collecting data on how variability affects routers and links and also the individual stages of routers. Additionally, we have analyzed how the diversity of frequencies present in the network due to process variation affects network performance. The conclusion of this analysis is clear. Average application execution time may be improved if the cores that applications use are carefully selected taking into account variability data and not only considering the availability of idle cores. However, additional work has still to be done to make our proposal, called performance domains, a reality. ACKNOWLEDGMENT This work was supported by the Spanish MEC and MICINN, as well as European Comission FEDER funds, under Grants CSD2006-00046 and TIN2009-14475-C04. It was also partly supported by the project NaNoC (project label 248972) which is funded by the European Commission within the Research Programme FP7. "
A Low-Overhead Asynchronous Interconnection Network for GALS Chip Multiprocessors.,"A new asynchronous interconnection network is introduced for globally-asynchronous locally-synchronous (GALS)chip multiprocessors. The network eliminates the need for global clock distribution, and can interface multiple synchronous timing domains operating at unrelated clock rates. In particular, two new highly-concurrent asynchronous components are introduced which provide simple routing and arbitration/merge functions. Post-layout simulations in identical commercial 90 nm technology indicate that comparable recent synchronous router nodes have 5.6-10.7x more energy per packet and 2.8-6.4x greater area than the new asynchronous nodes. Under random traffic, the network provides significantly lower latency and competitive throughput over the entire operating range of the 800 MHz network and through mid-range traffic rates for the 1.36 GHz network, but with degradation at higher traffic rates. Preliminary evaluations are also presented for a mixed-timing (GALS) network in a shared-memory parallel architecture, running both random traffic and parallel benchmark kernels, as well as directions for further improvement.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip A Low-Overhead Asynchronous Interconnection Network for GALS Chip Multiprocessors Michael N. Horak ECE Department Univ. of Maryland mnhorak@umd.edu Steven M. Nowick Dept. of Computer Science Columbia University nowick@cs.columbia.edu Matthew Carlberg EECS Department UC Berkeley carlberg@eecs.berkeley.edu Uzi Vishkin ECE Department Univ. of Maryland vishkin@umd.edu Abstract A new asynchronous interconnection network is introduced for globally-asynchronous locally-synchronous (GALS) chip multiprocessors. The network eliminates the need for global clock distribution, and can interface multiple synchronous timing domains operating at unrelated clock rates. In particular, two new highly-concurrent asynchronous components are introduced which provide simple routing and arbitration/merge functions. Post-layout simulations in identical commercial 90nm technology indicate that comparable recent synchronous router nodes have 5.6-10.7x more energy per packet and 2.86.4x greater area than the new asynchronous nodes. Under random trafﬁc, the network provides signiﬁcantly lower latency and competitive throughput over the entire operating range of the 800 MHz network and through mid-range trafﬁc rates for the 1.36 GHz network, but with degradation at higher trafﬁc rates. Preliminary evaluations are also presented for a mixedtiming (GALS) network in a shared-memory parallel architecture, running both random trafﬁc and parallel benchmark kernels, as well as directions for further improvement. 1 Introduction A recent NSF-sponsored workshop on networks-on-chip (NoCs) focused on the research challenge of maintaining the scalability of interconnection networks [24]. The consensus is that current techniques, when extrapolated to future technologies, will face signiﬁcant shortcomings in several key areas. First, power consumption is expected to exceed the budgets for commercial chip multiprocessors (CMPs) by a factor of 10x by 2015 following the projected technology roadmap. In addition, latency and throughput are predicted to become signiﬁcant bottlenecks for system performance. Finally, there are less quantiﬁable, but signiﬁcant, issues of increased design time and support for scalability, reliability and ease-of-integration of complex heterogeneous systems. These latter issues are expected to be important requirements for implementating future systems, speciﬁcally handling synchronous domains with arbitrary unrelated clock frequencies and allowing dynamic voltage scaling. The goal of this paper is to address some of these bottlenecks, with the design and evaluation of a low-overhead and ﬂexible asynchronous interconnection network. This work is part of the C-MAIN Project (Columbia University/University of Maryland Asynchronous Interconnection Network), an ongoing effort to develop low-cost and ﬂexible NOCs for highperformance shared memory architectures. The target design is a mixed-timing network, shown in Figure 1, consisting of the core asynchronous network surrounded by mixed-timing interfaces. In particular, the network provides ﬁne-grained pipelined integration of synchronous components in a globallyasynchronous locally-synchronous (i.e. GALS) style architecture [6, 31, 28]. The synchronous components may be processing cores, function units or memory modules. To support scalability, synchronous components may have arbitrary unrelated clock rates, i.e. the goal is to integrate heterochronous Mixed−Timing Network Sync S A A S Sync Sync S A A S Sync Async Network Sync Sync S A S A A S A S Sync Sync Figure 1. Mixed-timing network systems [31].1 The ﬁrst contribution is two new highly-concurrent asynchronous network primitives, to support the routing and arbitration functions of the network. Each primitive is carefully designed for high performance and low area and power overheads, using a transition-signalling, i.e. two-phase, communication protocol [29], which has only one roundtrip communication per channel per transaction. In principle, transition-signaling is a preferred match for high-performance asynchronous systems, yet it presents major practical design challenges: most existing two-phase asynchronous pipeline components are complex, with large latency, area and power overheads. Mixed-timing interfaces are then designed, based on the approach of [9], with new customized protocol converters. An important overall target of this work is to use standard cell design whereever possible, with static gates and simple one-sided (i.e. “bundled”) timing constraints. The second contribution is the detailed evaluation of the interconnection network at both the circuit and system level. Layouts of the routing and arbitration network primitives are implemented in a commercial 90nm technology following a standard cell methodology, and each primitive is compared in detail with recently-published comparable synchronous primitives implemented in the same technology [2], which use a latencyinsensitive style of synchronous communication. The primitives are then assembled into a variant Mesh-of-trees (MoT) topology (see Section 2.1), a network that has proven to be effective in a high-performance, single-chip synchronous parallel processing architecture based on a shared-memory model [2]. This network uses deterministic wormhole routing [4, 23, 5] and extremely simple binary router nodes with low functionality. Reported results on a synchronous shared-memory processor using this topology and node structure have demonstrated its viability for a number of high-performance CMP applications [2], as opposed to the more complex 5-ported routing nodes typical in many NOCs for distributed embedded processors [11, 23, 5]. Hence, a direct comparison was targeted, from asynchronous node implementations to system-level interconnection network. A detailed evaluation was conducted at many levels of integration. Initial simulations of the new asynchronous routing and arbitration circuits are promising, showing signiﬁcant benThis work was supported in part by NSF awards CCF-0811504 and CCF0325393, and by an Initiatives in Science and Engineering (ISE) grant from Columbia University. M.N. Horak is currently afﬁliated with Advanced Simulation Technology, Inc.; M. Carlberg contributed to this work while at Columbia University. 1 In contrast, some recent solutions have been proposed for specialized systems with narrower bounds on operation, such as mesochronous (all components operate at the same clock rate, with stable but unknown phase differences) and plesiochronous (all components operate at nominally identical clock rates, but with slight frequency mismatch) systems [31]. 978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.14 43 eﬁts in power and area, and roughly comparable performance, when compared to synchronous components in identical technology. Detailed simulations were also conducted on an asynchronous network and a mixed-timing version of the network, which were compared to synchronous networks from 400MHz to 1.36 GHz. Finally, the mixed-timing network was embedded and co-simulated with an XMT shared-memory processor [20] on several parallel kernels. The new GALS XMT processor provides comparable performance to the existing synchronous XMT except in the most challenging cases of high clock rate or high trafﬁc rate. Future directions for performance optimization are also outlined. Related Work. GALS and Asynchronous NOCs. There has been a surge of interest in recent years in GALS design [8, 31], especially for low- and moderate-performance distributed embedded systems. More recently, several GALS NoC solutions have been proposed to enable structured system design. The CHAIN chip area interconnect [1] provides robust self-timed communication for system-on-chip (SoC) designs, including for a specialized multiprocessor for neural simulations [25]. The Nexus asynchronous crossbar [18] provides system-level communication and has been used in recent Ethernet routing chips. The NoC solution in [4] presents a low-latency service mesh network with an innovative system-level modeling framework. MANGO [6] supports quality-of-service gurantees and adaptive routing. The prototype GALS NoC of [26] supports SoC system debugging. The RasP network [15] uses pulse-based asynchronous pipelines to achieve high performance and small wire-area footprint. Earlier work provided an asynchronous node architecture and implementation for coarsegrain complex-functionality routing nodes [11]. Several of these approaches have been highly effective, especially for low- and moderate-performance distributed embedded systems [1, 4], thus targeting a different point in the design space than the proposed work. Some have lower throughput (e.g., 200 to 250 MHz) [25, 4], while those with moderate throughput (e.g. near 500 MHz) [6, 28]) often have signiﬁcant overheads in router node latency. Most use highfunctionality coarse-grained routing nodes (with 4 routing ports and 1 entrance/exit port, routing tables, crossbar, and extra buffering) [6, 28] based on a standard 5-ported node architecture [11]. Almost all use four-phase return-to-zero protocols, involving two entire roundtrip channel communications channel per transaction (rather than the single roundtrip communication targeted in the proposed work), as well as delay-insensitive data encoding, i.e. dual-rail, 1-of-4, m-of-n (which results in lower coding efﬁciency than the single-rail bundled encoding used in our work) [1, 6, 11, 25, 18, 28, 4]. Finally, several approaches use specialized circuits with dynamic logic [18] or pulse-mode [15] operation. Closer to our work is a promising recent approach targeting a two-phase protocol using a commercial CAD ﬂow [26]. However, it has overheads due to a delay-insensitive (LEDR) data encoding and ﬂipﬂop-based registers, and is not currently suitable as a general GALS NOC: it does not provide any routing nodes, only channel multiplexers to support silicon debugging. The GALS neural network system of [25] also includes two-phase channels between chips, with four-phase channels on chip; the former use m-of-n delayinsensitive codes with large encoding and decoding overheads. Asynchronous Transition-Signaling Pipelines. The proposed NoC is based on Mousetrap pipelines [29], which use a lowoverhead latch-based architecture (see Section 2.2). In this paper, these pipeline components are enhanced to support routing and arbitration. Several previous transition-signaling linear asynchronous pipelines have been proposed, but most have signiﬁcant overheads. Some use complex and expensive latch structures, including specialized capture-pass latches [30] and double-edge-triggered ﬂipﬂops [7]. Others uses lightweight transparent latches, but require double latch registers per stage (whereas Mousetrap only requires single registers) [27]. Closest to our work are the recent non-linear routing and arbitration nodes by Gill and Singh [13], and extended by Mans r e t s u l C g n i s s e c o r P 0 1 2 3 0 1 2 3 (a) 0 1 2 3 s e l u d o M y r o m e M 0 1 2 3 1 2 3 4 0 1 2 3 (b) 0 1 2 3 0 1 2 3 (c) (d) Figure 2. Mesh-of-trees network (N=4) nakkara and Yoneda [19]. This work makes useful advances in two-phase asynchronous pipeline design, but designs are relatively unoptimized. For the routing primitives, all prior designs stall when one output channel is congested, while the proposed design allows pass-through trafﬁc to the uncongested output channel. In addition, the prior FF-based designs are expected to have higher energy per transaction than the proposed latchbased design. For the arbitration primitives, the prior designs use 2 FF-based data registers vs. only 1 latch-based data register in the proposed design, which should result in signiﬁcantly worse area, energy, latency and throughput than the proposed design, though ours may have higher glitch power when the latches are transparent. In addition, our design supports wormhole routing, while these do not. 2 Background 2.1 Mesh-of-trees network The Mesh-of-trees (MoT) network [2] used in this paper and in recent publications is a variant the traditional mesh-of-trees network [17], designed to provide the needed bandwidth for a high-performance, ﬁne-grained parallel processor using global shared memory. It has been proven effective in recent detailed evaluations on a range of trafﬁc for on-chip parallel processing. Recent extensions have been proposed to reduce area overhead through a hybrid MoT/butterﬂy topology, which maintains the throughput and latency beneﬁts of MoT with the area advantages of butterﬂy [3]. The MoT network consists of two main structures: a set of fan-out trees and a set of fan-in trees. Figure 2(b) shows the binary fan-out trees, where each source is a root and connects to two children, and each child has two children of their own. The 16 leaf nodes also represent the leaf nodes in the binary fanin trees that have destinations as their roots (Figure 2(c)). An MoT network that connects N sources and N destinations has logN levels of fan-out and logN levels of fan-in trees. There is a unique path between each source-destination pair. A memory request packet travels from the root to one of the leaves of the corresponding fan-out tree. It passes to the leaf of a corresponding fan-in tree, and travels to the root of that fan-in tree to reach its destination (Figure 2(d)). In general, contention can occur when two packets from different sources to different destinations compete for a shared resource. In the MoT network, fan-out trees eliminate competition between packets from different sources, and fan-in trees eliminate competition between packets to different destinations. This separation guarantees that, unless the memory access trafﬁc is extremely unbalanced, packets between different sources and destinations will not interfere. Therefore, the MoT network provides high average throughput that is very close to its peak throughput. There are three switching primitives in a MoT network: (a) routing, (b) arbitration, and (c) linear pipeline primitives (the latter is optional for performance improvement as a microarchitectural “repeater” to divide long wires into multiple short segments). 2.2 Mousetrap pipelines The new asynchronous network primitives for fan-out and fan-in nodes, introduced in the following section, are based on 44     Latch Control R E G Latch Control R E G Req0 Ack0 Data0 Req1 Ack1 Data1 Req B Data Ack Figure 3. Structure of routing primitive an existing linear asynchronous pipeline called Mousetrap [29]. Mousetrap is a low-overhead asynchronous pipeline that provides high-throughput operation. Each Mousetrap stage uses a single register based on level-sensitive latches (rather than edge-triggered ﬂipﬂops) to store data, and simple stage control consisting of only a single combinational gate. These designs use single-rail bundled data encoding, where a synchronousstyle data channel is augmented with an extra req wire, and a single transition on the req accompanying the data “bundle” indicates the data is valid. The req wire has a simple one-sided timing constraint that its delay is always slightly greater than the data channel. (For further details, see [29].) 3 Asynchronous Primitives This section introduces the two new asynchronous components: the routing and arbitration network primitives. These components are the fundamental building blocks of the asynchronous Mesh-of-trees network, and can also be used to construct alternative network topologies. A basic overview is provided, further details can be found in [16, 22]. 3.1 Routing primitive The routing primitive performs a fan-out (i.e. demultiplexing) operation, with one input port and two output ports, shown in Figure 3. During the operation, packets arriving at the input port are directed to exactly one of the two output ports. Basic operation. Figure 3 shows the structure of the routing primitive. Adjacent primitives communicate using request (req) and acknowledgment (ack) signals following a transitionsignaling protocol. The basic operation, assuming an empty primitive, begins with new data arriving along with a routing signal B. An important feature of the routing primitive is that, unlike Mousetrap pipeline stages, the registers are normally opaque (i.e. disabled), preventing data from propogating to subsequent stages before the routing decision is made. After the data inputs are stable and valid, a request transition on Req occurs at the input. The latch controller selected by the routing signal, B, enables its latches (i.e. makes them transparent) and data advances to the selected output channel. The toggle element generates a request transition on Req0/1 to the following stage. Then, in parallel, the latches are quickly closed, safely storing data, and an acknowledgment transition on Ack is sent to the previous stage. Architecture of the routing primitive. Each primitive consists of two registers and latch controllers, one pair per output port. Each register is a standard level-sensitive D-type transparent latch register that is normally opaque, preventing data from passing through. Each latch controller, shown in Figure 4, is responsible for controlling three signals, which enable data storage and inter-stage communication: (i) the register enable signal (En); (ii) the corresponding request output (Req0/1) to the next stage; and (iii) the acknowledgment (Ack) to the previous stage. The toggle element converts an input Req transition to an output Req0/1 transition on the appropriate port. The toggle output for a speciﬁc port will transition once for every data item, when both toggle input (y0/1) and enable (En) inputs are high. The acknowledgment (Ack) signal to the left environment is generated by the XOR gate shown in Figure 3. The XOR gate Ack Req Ack0 w x0 y0 Toggle 0 Req0 B Figure 4. Latch controller of routing primitive En Flow Control Unit Latch Controller Ack1 Ack0 L4 Q D EN L3 Q D EN Req0 Req1 Data0 Data1 Ack Req Mutex E D Q L1 E D Q L2 E D Q L5 E D Q L6 E D Q L7 mux_select E 0 1 Datapath Data D Q GER Figure 5. Structure of arbitration primitive merges two transition signals, Req0 and Req1.3 Enhanced concurrency feature. The routing primitive includes a powerful capability to decouple processing between the two output routing channels. In particular, if one of the output channels is stalled, awaiting acknowledgment, the other output channel can successively process multiple full transactions. This concurrency feature has the potential for signiﬁcant system-level performance beneﬁts, since it entirely avoids stalling input packets heading to an unblocked output channel. 3.2 Arbitration primitive The arbitration primitive accepts data from exactly one of two input ports and forwards it to a single output port, thus providing complementary functionality to the routing primitive. Basic operation. Figure 5 shows the design of the basic arbitration primitive. An operation begins with new data appearing at the input of an empty primitive followed by a request transition from the previous stage to the ﬂow control unit. The ﬂow control unit will arbitrate the request through a mutex component and perform two actions: setting the correct multiplexer select signal (mux select) and forwarding the winning request to the latch controller by enabling either L1 or L2. The latch controller will then store the new data and concurrently generate a request to the next stage while acknowledging to the ﬂow control unit that data has been safely stored. At this point, the ﬂow control unit will reset the mutex and then acknowledge to the previous stage that it may accept new data on that channel, thereby completing the transaction. Architecture of the basic arbitration primitive. Figure 5 shows the structure of the arbitration primitive. The arbitration functionality is performed by the mutual exclusion element (mutex), an analog arbiter circuit. The design features seven standard level-sensitive D-type transparent latches (numbered L1 through L7). Latches L3 through L7 are all normally transparent (enabled). Latches L1 and L2 are normally opaque (disabled). XOR gates are used at the inputs of the mutex as 3 For simplicity, initialization circuitry is omitted, but it is included for all reported experiments. 45 Ack1 Ack0 L4 Q D EN L3 Q D EN Req0 Req1 L9 Q D E L8 Q D E glue1 glue0 Mutex E D Q L1 E D Q L2 S R Q mux_select Figure 6. Enhanced ﬂow control unit “inequality” testers, generating a request transition to the mutex when new data has arrived and then resetting the mutex after that data has been stored in the register. Another XOR gate at the input of latch L5 functions as a “merge” element, joining two transition-signaling signals, Req0 and Req1, into a single signal, Req. The XNOR gate is used as a Mousetrap latch enable with feedback path from the output of L5. Finally, there is one multiplexer and register (transparent latch) per data bit.3 Power optimization. The basic design of Figure 5 allows unnecessary glitch power consumption to occur on the datapath. The optimization in Figure 6 eliminates this glitching. Specifically, the mux select signal may transition more than once per cycle for transactions on the Req1 port. The optimization adds an SR latch to store the most recent mutex decision at the end of each transaction. The result of this optimization is that the mux select is limited to at most one transition per transaction. The resulting power savings can be signiﬁcant, since the majority of the power is consumed in the datapath. Wormhole routing capability. The ﬁnal enhancement in Figure 6 is support for wormhole routing of multi-ﬂit packets [4]. A ﬂow-control unit, or ﬂit, is the smallest granularity of message sent through the network. Wide packets are split into multiple ﬂits that travel contiguously through the network. In wormhole routing, once arbitration is won by a packet head ﬂit in an arbitration node, each remaining ﬂit in the packet must be guaranteed unarbitrated access through the node until the last ﬂit of the packet exits. In the design, to bias the selection of the mutex so that the next ﬂit of a multi-ﬂit packet is guaranteed to advance without new arbitration, a “kill your rival” protocol is implemented. When the ﬁrst ﬂit of a multi-ﬂit packet wins the mutex, the opposing request input to the mutex is forced to zero, or “killed”. This operation either prevents future requests at the other mutex input from occurring, or in the case where a request was already pending, kills the opposing request until the entire multi-ﬂit packet has advanced. The kill function is achieved using a NOR gate located at the input of the mutex. 4 Mixed-Timing Interfaces Mixed-timing interfaces are now introduced to allow for integration into a GALS system with robust communication between synchronous terminals through the asynchronous network, as shown in Figure 1. The mixed-timing interfaces are designed using existing mixed-timing FIFOs [9] and new custom asynchronous protocol converters. Each mixed-timing FIFO is a token ring of identical storage cells that have data enqueued from one timing domain and dequeued from another. The synchronous portions of the FIFOs have full or empty detectors. Detection circuits generate signals to stall synchronous terminals in order to to prevent overrun or underrun conditions in the FIFO. The asynchronous portions do not require explicit full or empty detection, as they will simply withhold acknowledgment until an operation can be performed. The mixed-timing interfaces provide communication between synchronous terminals and the asynchronous network. Figure 1 shows the interfaces instantiated in the mixed-timing network (marked as “S→A” and “A→S”). Details of the mixed-timing interfaces are shown in Figures 7 and 8. Each interface contains a mixed-timing FIFO and custom protocol converter. The protocol converter translates clk req_put full Data Sync Terminal get_req pipe_req get_ack Protocol Converter pipe_ack Sync−Async FIFO Data M O U S E T R A P Req Ack Data Routing Primitive (Root) Figure 7. Sync/Async interface block diagram clk Req Ack put_req put_ack Protocol Converter Arbitration Primitive (Root) Data Async−Sync FIFO Sync Terminal empty req_get Data Figure 8. Async/Sync interface block diagram handshaking signals between the two-phase transition signaling of the asynchronous network and the four-phase return-to-zero signaling of the existing mixed-timing FIFO. Each converter is designed as a low-latency burst-mode asynchronous controller using the MINIMALIST CAD tool [12, 21]. To improve throughput, a Mousetrap pipeline stage [29] is added to the synchronous-asynchronous interface (Figure 7) between the protocol converter and the routing primitive at the root of the fan-out tree. The Mousetrap stage, when empty, will store new data and acknowledge the mixed-timing FIFO faster than the routing primitive. 5 Experimental Evaluation Detailed evaluations of the new interconnect network are now presented, as well as comparisons to an existing fabricated synchronous version [2], at several distinct levels. These range from detailed post-layout simulation of network primitives to pre-layout system-level evaluation of fully-assembled networks, both asynchronous and mixed-timing (using interconnected post-layout components interconnected with delays extrapolated from a comparable synchronous chip ﬂoorplan [2]). Finally, several parallel kernels are run on the mixed-timing network in shared-memory CMP simulation environment. 5.1 Asynchronous primitives The asynchronous primitives are evaluated using four metrics – area, power, latency and maximum throughput – and are compared in detail with the synchronous primitives recently proposed in [2]. In particular, Balkan et al. [2] provide detailed physical layouts in a commercial 90 nm technology library. The asynchronous primitives were designed in the same technology for the purpose of direct comparison. Results are presented for both primitives with 8-bit wide datapath, simulated at 1.2 V and 25◦C in the nominal process using Cadence NC-Sim. These experiments do no currently assess the potential impact of clock gating, since the synchronous chip in [2] did not include this optimization. High-level clock gating (e.g. of an entire tree) is unlikely to provide signiﬁcant beneﬁts because of the path diversity and rapidly-changing memory access patterns in this topology and architectural domain. While low-level clock gating is certainly possible (e.g. per routing primitive cell), it is expected to add signiﬁcant area and power overheads, due to the ﬁne granularity of the network nodes. In addition, clock gating still requires global clock distribution, and does not ﬁt well with the heterochronous system goals of this work. Area and power. As indicated in Table 1, the asynchronous primitives achieve signiﬁcant power and area savings compared to existing synchronous designs, using 36% and 16% of the cell area respectively and 18% and 9% of the energy per packet of the existing synchronous designs. Several metrics are used to assess area and power consumption. Area is measured as the total cell area occupied by a single primitive. Energy per packet is reported as the average energy consumed by a primitive during simulation with a sequence of 100 packets containing random data either routed to, or arriving at, random destinations. Leakage power is reported as the subthreshold leakage power consumed by the regular-Vt 46 cells. Idle power is measured when no requests arrive to either primitive at a clock rate of 1 GHz with no clock-gating optimizations. Even with efﬁcient clock-gating schemes, dynamic power is consumed due to the local clock network, while the asynchronous does not consume dynamic power when idle. Table 1. Network primitives: area and power Component Type Area Routing Async Routing Sync Arbitration Async Arbitration Sync (μm2 ) 358.4 988.6 349.3 2240.3 Energy/ Packet (pJ) Leakage Power (μW) 0.37 2.06 0.33 3.53 0.56 1.82 0.50 4.13 Idle Power (μW) 0.6 225.6 0.5 388.6 The above results indicate a substantial reduction in area and dynamic and static power for both asynchronous primitives over their synchronous counterparts. The synchronous designs use two registers at input ports of routing and arbitration primitives [2] in order to provide better throughput in congested scenarios, and to support a latency-insensitive style dynamic ﬂow control, for a total of six datapath registers between the two primitives. In contrast, the asynchronous primitives have a total of only three datapath registers between the two nodes. Furthermore, due to the Mousetrap-based design style, each asynchronous register is a single bank of transparent D-latches, while each synchronous register is a single bank of more expensive edgetriggered ﬂipﬂops. Finally, in spite of the single-latch-based asynchronous methodology, each asynchronous latch register, in congested scenarios, can still hold a distinct data item, and therefore the network provides 100% storage capacity. As a result, the asynchronous primitives provide signiﬁcantly lower area and power overheads, as well as a ﬂexible “data-driven” operation (i.e. processing data items only on demand). Latency and throughput. The asynchronous primitives exhibit competitive performance in terms of latency and maximum throughput, while using less area and power than the synchronous designs. Latency is measured as the delay from a request transition on an input interface to its appearance at an output of an empty primitive. Maximum throughput, given in Giga-ﬂits per second (GFPS), is evaluated under different steady-state trafﬁc patterns. Throughput is measured at the root primitive of a 3-level fan-out or fan-in tree which which captures the interactions between neighboring primitives. Table 2 shows results of latency and throughput experiments for the routing primitive. For this primitive, throughput is evaluated for three steady-state trafﬁc patterns where consecutive packets are routed to a single, random or alternating destination port. The initial performance results for the routing primitive are encouraging, as the latency and highest throughput approach the results measured from the synchronous design. The latency of the asynchronous primitive is only 6% higher than the synchronous, indicating that in lightly-loaded scenarios, the asynchronous fan-out tree is expected to have similar total latency as a synchronous fan-out tree operating near 2 GHz. With sufﬁciently distributed routing destinations, the expected performance is the random case, which can take advantage of the concurrent operation between ports to achieve 69% of the synchronous maximum throughput. Future work on system-level optimization is expected to increase the maximum throughput in the random case. In particular, initial analysis indicates that the addition of linear pipeline primitives (i.e. Mousetrap stages) near the root of the fan-out tree will further improve performance. Table 2. Routing primitive: performance Component Type Async Sync Latency (ps) 546 516 Max. Throughput (GFPS) Single Random Alternating 1.07 1.34 1.70 1.93 1.93 1.93 Table 3 shows results of latency and throughput experiments for the arbitration primitive. For this primitive, maximum throughput is measured for two steady-state trafﬁc scenarios Table 3. Arbitration primitive: performance Component Type Async Sync Latency (ps) 489 474 Max. Throughput (GFPS) Single All 1.08 2.09 2.04 2.09 690.8 µm 277.2 µm 554.4 µm Figure 9. Projected 8-terminal asynchronous network ﬂoorplan 138.16 µm where successive packets arrive at a single port or simultaneously at all ports. The latency and highest throughput of the two arbitration primitives are nearly identical. This result indicates the high performance of the arbitration primitive. In a lightlyloaded network, latency is the dominant factor for performance, and the asynchronous primitives should operate comparable to synchronous primitives with clock frequency near 2 GHz. In a heavily-loaded network, the throughput of the root arbitration primitives determine the total output trafﬁc. With well-balanced trafﬁc, the throughput of the asynchronous fan-in tree should approach the all scenario. Mutual exclusion element. A ﬁnal component of this evaluation is on the performance of the the mutual exclusion (“mutex”) element in the arbitration primitive of Figure 5. Unlike the ﬁxed-latency synchronous arbitration of the previous MoT network [2] and other synchronous NOCs. asynchronous arbitration is implemented by an analog component with variable-time resolution. Theoretically, this element may exhibit arbitrary ﬁnite resolution time depending on input scenarios. However, detailed simulations show that this component exhibits nearly ﬁxed delay except for extreme and rare cases: a baseline latency of 150ps when receiving single requests (i.e. no contention), and only noticeable degradation beginning when two competing inputs arrive within the same 2 picosecond interval. 5.2 Mixed-timing interfaces The performance of the mixed-timing interfaces is now evaluated. Two metrics have been simulated: latency and maximum throughput. Latency is the delay from a request at the input channel to its appearance at the output channel, beginning with an empty FIFO. Maximum throughput is deﬁned as the highest operating rate where the FIFO does not report full or empty with a highly-active input environment. Table 4 shows simulation results for mixed-timing interfaces with 3-place FIFOs. The latency of the asynchronous-synchronous FIFO varies depending on the exact moment when data items are enqueued during a clock cycle, hence the Min and Max columns. Table 4. Mixed-timing interfaces: performance Interface Type Sync-Async Async-Sync Latency (ns) Max. Throughput Min Max (MHz) 0.97 932.8 3.56 843.2 2.95 5.3 Asynchronous network This section presents the preliminary system-level performance evaluation of an 8-terminal pre-layout asynchronous network. Projected network ﬂoorplan. The projected asynchronous ﬂoorplan is shown in Figure 9. It is based on the ﬂoorplan for the comparable fabricated synchronous MoT network [2]. As in [2], the MoT network is partitioned into four physical slices, each interfacing to two source terminals and two destination terminals, indicated by the white horizontal stripes. The gray horizontal stripes contain inter-partition routing channels. In the synchronous network, the primitives are placed, routed 47 Simulation results. System-level performance evaluation for the asynchronous network is shown in Figure 10, where it is also compared with synchronous results for the three different clock frequencies (400 MHz, 800 MHz and 1.36 GHz). Experiments are conducted for a wide range of input trafﬁc rates up to the maximum input trafﬁc rate of the 1.36 GHz synchronous network. The 1.36 GHz, 800 MHz and 400 MHz networks have maximum input trafﬁc rates of 348.2 Gbps, 204.8 Gbps and 102.4 Gbps, respectively (for 8 terminals and 32-bit datapath), so no further data exists beyond this point on the respective graphs in Figure 10. For throughput, of the asynchronous network tracks the 1.36 GHz synchronous network up to an input trafﬁc rate near 200 Gbps, where the asynchronous network reaches saturation. The throughputs of the 400 MHz and 800 MHz synchronous networks likewise track up to their individual maximum input trafﬁc rates (102.4 and 204.8 Gbps, respectively), at which point they are cut off. The asynchrononous network, in contrast, is capable of accepting higher trafﬁc rates, since it not inherently limited by a clock rate, hence its graph continues but with no further change in throughput. Note that one of the driving goals in using the MoT topology is that it sustain high throughput and low latency at the highest trafﬁc rates, to satisfy the high trafﬁc demands of the XMT parallel processor. The ﬁgure validates the use of this topology, since the synchronous saturation throughput is 91% of maximum rate [2]. For latency, the asynchronous network always has signiﬁcantly lower latency than both the 400 MHz and 800 MHz synchronous networks, over their entire operating ranges. It also has lower latency than the 1.36 GHz synchronous network at all input trafﬁc rates up to 150 Gbps, which is at 43.1% of the maximum input rate for this synchronous network; beyond this point, these latency diverge rapidly as the asynchronous network reaches saturation. Bottleneck Analysis and Directions for Further Improvement. These results on throughput and latency for the asynchronous network are promising, and competitive over a range of scenarios, but also indicate some bottlenecks. In particular, while the asynchronous network dominates both the 400 and 800 MHz synchronous networks, its saturation point and achievable throughput still fall short compared to the 1.36 GHz synchronous network for higher input trafﬁc rates. To identify the bottleneck sources, it is useful to analyze performance results in Tables 2 and 3. In that section, the synchronous primitive components had throughputs of 1.93 and 2.09 GHz, respectively, without wiring delays [2]. After layout, the synchronous primitives could operate only up to 1.36 GHz. Hence, a synchronous derating factor of approximately 70% occurs on the achievable throughput between pre- and post-layout. By extrapolation, a similar derating would be expected for the asychronous primitives. Interestingly, in an asynchronous MoT network — unlike synchronous — the performance demand is greatest at two nodes, which are critical bottlenecks: the root of the routing (i.e. fan-out) network and the root of the arbitration (i.e. fan-in) network. This node streams packets from the source, and receive converging streams at the sink, hence must be activated with highest frequency. Given that in Figure 10, random inputs are simulated, then the relevant entry in Table 2 for the routing primitive root is the “random” throughput result: 1.34 GFPS. The trafﬁc pattern on the arbitration primitive root is unknown, but lies between random (“all”, 2.04 GFPS) and single-channel (“single”, 1.08 GFPS) in Table 3. Therefore, it is likely that the single bottleneck in the entire asynchronous MoT network is the root of the routing network. The derating factor that appears between Table 2 (1.34 GFPS) and the network simulations of Figure 10 (800 GFPS) is 59.7%. Given the approximations involved, this derating can be regarded as roughly tracking the synchronous factor of 70%. In summary, an important future direction for performance improvement in the asynchronous network is to signiﬁcantly optimize the root routing nodes. Three optimizations are Figure 10. Performance comparison: asynchronous and synchronous networks and optimized by CAD tools. For the asynchronous network, the tools could not be directly applied while preserving asynchronous timing constraints, hence the routing and arbitration primitives are treated as hard macros, and assigned placement at regular intervals within the partitions (indicated by white circles). Then, derived wire delays based on this placement are assigned to inter-primitive connections for simulation. The path shown in the ﬁgure illustrates one fan-out tree of the network. Experimental setup. Performance is evaluated based on two metrics: throughput and latency. The evaluation for these metrics is performed following the approach proposed by Dally and Towles [10], adapted to accommodate the analysis of an asynchronous network. In particular, throughput, given in Gigabits per second (Gbps), is the output data rate of the network during the “measurement phase” of the simulation. Latency, given in nanoseconds (ns), is measured as the time from creation of a packet until it reaches its destination. Speciﬁcally, packets are ﬁrst placed into source queues at input ports of the network before they can be inserted. This method ensures that stalling effects at the inputs are captured in latency results. Three clock rates (400 MHz, 800 MHz and 1.36 GHz) are chosen for a detailed performance comparison with the new asynchronous network. The synchronous network at 800 MHz was selected since it provides fairly high performance, but not peak rate operation, and 400 MHz was selected to show moderate performance. The 1.36 GHz clock rate was selected to show an extreme comparison, where the synchronous design is operating at its maximum possible rate, as determined by static timing analysis (see [2]). Experiments are conducted under uniformly random trafﬁc with packet source queues installed at network input ports for injecting trafﬁc to accurately measure network performance [10]. In the synchronous case, packets are generated at each port at random clock edges and inserted into queues (implemented in hardware), depending on the desired average rate [2]. For the asynchronous, packets are generated at random intervals, following an exponential distribution with the mean corresponding to the desired input trafﬁc rate, and inserted into queues (implemented in behavioral Verilog). Note that there is a subtle difference in evaluating the synchronous network versus the asynchronous. The performance of the asynchronous network only depends on input trafﬁc rate; since it operates in a data-driven style, the input trafﬁc drives the simulated behavior. In contrast, the performance of the synchronous interconnection network depends on two distinct parameters: input trafﬁc rate and clock frequency. Here, clock frequency determines the range of valid input trafﬁc rates, and the input trafﬁc rate (relative to that frequency) determines the performance. In addition, the asynchronous network in this simulation framework can accomodate a wide range of input trafﬁc rates (even though saturation will eventually occur), while the maximum input rate of the synchronous network is inherently limited by the clock frequency. 48 Figure 11. Performance comparison: mixed-timing and synchronous-only networks (normalized to clock cycles) promising: (i) at the circuit level, to apply gate resizing and better repeater insertion at this node, to improve latency and throughput; (ii) at the micro-architecture level, the asynchronous design was not optimized for congestion; initial results suggest that insertion of linear Mousetrap FIFO stages at its ports can improve overall system throughput; and (iii) at the node design level, to explore higher-throughput variants. 5.4 Mixed-timing network After evaluating the performance of the asynchronous network, mixed-timing FIFOs are added to form the ﬁnal mixedtiming network. The mixed-timing network, unlike the synchronous, provides the capability of easily integrating into heterogeneous timing systems with multiple synchronous domains operating at arbitrary or dynamically-variable clock frequencies. Latency and throughput are again evaluated following the methodology from Dally and Towles [10]. In this comparison of networks having synchronous terminals, including the mixed-timing network, the input trafﬁc rate and throughput are given in ﬂits per cycle per port, the average rate at which ﬂits enter and exit network ports. Latency is normalized to the number of cycles. Using the normalized values, the synchronous will have the same performance at all valid clock rates, relative to clock cycles. The performance of the mixed-timing network, however, depends on the selftimed asynchronous network, and will change with the clock frequency. Figure 11 shows performance results for the mixed-timing network, a ﬂexible asynchronous network capable of interfacing with multiple synchronous domains operating at arbitrary clock frequencies. Since Table 4 of Section 5.2 indicates that the mixed-timing interfaces have somewhat lower performance than the asynchronous primitives, the mixed-timing network is evaluated at three clock rates that operate below their maximum operating rates: 400, 600 and 800MHz. The normalized results of Figure 11 indicate a wide range of behavior, from comparable throughput and signiﬁcantly lower latency for the 400 MHz mixed-timing network, to a mix of advantages and performance degradation in the 600 and 800 MHz mixed-timing networks. At higher clock frequencies, the performance of the mixed-timing network is affected by bottlenecks introduced by mixed-timing interfaces. The 600 MHz and 800 MHz mixed-timing networks achieve maximum throughputs that are 67% and 55% of a synchronous network operating at those frequencies, respectively. Performance bottlenecks occur due to synchronization overheads in the mixedtiming interfaces. In particular, the mixed-timing interfaces must stall when FIFOs approach near-full or near-empty to prevent FIFO errors. As the clock rate increases and approaches the maximum throughput of the interfaces, these penalties occur more often, resulting in lower throughput of the mixedtiming network. On the positive side, however, it is noteworthy that the 600 MHz and 800 MHz mixed-timing networks provide identical 49 Figure 12. Speedup comparison: mixed-timing vs. synchronous network on 4 parallel XMT kernels (normalized to synchronous XMT performance) throughput to synchronous at input trafﬁc rates up to 65% and 52%, respectively, and lower latency than synchronous at input trafﬁc rates up to 60% and 29%, respectively. Further design improvements to the mixed-timing interfaces of [9], especially for the async-sync interface (when the sync interface is stalled and empty, waiting for a new item, resulting the large latency overhead listed in Table 4) are expected to have a large impact on further improving system performance. 5.5 Parallel kernels in a GALS CMP architecture This section presents simulation results on several parallel kernels on an existing shared-memory parallel processor, XMT [20, 14], incorporating the proposed mixed-timing network. These simulations provide realistic trafﬁc loads for the network, as opposed to uniformly random trafﬁc used in previous experiments. XMT overview. The XMT shared-memory architecture targets ﬁne-grain thread parallelism, and has shown signiﬁcant beneﬁts in handling complex and dynamically-varying computation (e.g. ray tracing) as well as regular computations (e.g. indexed-tree database searches). Centered around a simple abstraction, the XMT processor also provides a useful framework to support the productivity of parallel programmer [32]. An existing synchronous CMP implementation incorporates a clocked MoT network between multiple cores and multiple memories. For our experiments, the synchronous MoT network is replaced by the new mixed-timing MoT network. The resulting GALS CMP architecture is capable of supporting multiple synchronous cores operating at unrelated clock rates. Details of XMT parallel interconnect network. The XMT simulation assumes an 8-terminal mixed-timing MoT network, containing 8 source terminals and 8 destination terminals. In particular, for this CMP platform, the mesh serves as a high-speed parallel interconnect between cores and partitioned shared L1 data cache [20, 14]. Each of the 8 cores, or processing clusters, itself contains 16 separate processing units, or thread control units (TCUs), for a total of 128 TCUs. In XMT, no data is cached locally in a TCU. The shared L1 cache is partitioned into 8 smaller cache modules, and each TCU may access any cache module. Since the interconnect is designed to support loads and stores, two distinct new mixed-timing networks are modelled: one from processing clusters to caches and one from caches to processing clusters. Each load operation uses a 1-ﬂit packet, and each store operation uses a 2-ﬂit packet [20, 14]. Experimental setup. Three different clock frequencies were selected: 200, 400 and 700MHz. Speedup results for the mixedtiming network are normalized relative to performance of the synchronous network. The synchronous network has speedup of 1.0 in all cases. In addition, four different XMT parallel kernels were simulated: array summation (add): each parallel thread computes the sum of a sub array serially, and the resulting sums are added to compute the total for the entire array (size 3M); matrix multiplication (mmul): the product of two 64 × 64 matrices is computed, where each parallel thread computes one row of the result matrix; breadth-ﬁrst search (bfs): a parallel BFS algorithm is executed on a graph with 100K vertices and 1M edges; and array increment (a inc): each parallel thread increments 8 elements of the array (size 32K). The average network ultilization exhibited by these XMT kernels are: add: 0.492; mmul: 0.395; bfs: 0.091; a inc: 0.927. Overview of results. Simulation results are shown in Figure 12. The add kernel provides steady trafﬁc throughout the execution of the program that remains below the saturation throughput of the network with mixed-timing, hence GALS XMT performance remains comparable to an XMT with the synchronous network. The mmul kernel has a lower average network utilization than add ; however, this does not reﬂect the true pattern of memory access. The trafﬁc appears in bursts, causing very high trafﬁc, followed by periods of low trafﬁc as the processing clusters compute the results. In the 700 MHz case, the burst trafﬁc exceeds the saturation throughput, thereby degrading the performance of the application. The bfs kernel has the lowest trafﬁc rate of all benchmarks; since it operates below the saturation throughput of the network, performance is comparable to synchronous for all simulations. The a inc kernel has extremely high trafﬁc. The average input trafﬁc at 700 MHz will be 164.9 Gb/s, which exceeds the saturation throughput of 112 Gb/s. The network latency increases exponentially under that level of trafﬁc, and the performance of the GALS XMT processor therefore decreases. 5.6 Evaluation summary Overall, the above results are promising, while still indicating areas for further improvement. For the XMT experiments, on both add and bfs kernels (where the former had an average network utilization of nearly 50%), the GALS XMT had comparable performance to the synchronous XMT at 700MHz. Performance of kernel mmul degraded only modestly (by about 14%) at 700MHz. Only kernel a inc, which had high average network utilization (0.927), showed signiﬁcant degradation (by about 37%). In each case, there was almost no performance degradation, and sometimes an improvement, at the lower clock rates (200MHz, 400MHz). In addition, extrapolating from Figure 11, the mixed-timing network provides lower latency over much of this range. These results conﬁrm two observations from Section 5.4: the mixed-timing network performs well at (i) relatively high clock rates with low to moderate input trafﬁc, and (ii) low to moderate clock rates with high input trafﬁc. However, further design improvements are still needed, to be explored along the guidelines outlined in Sections 5.3 and 5.4, to make the mixed-timing network more competitive at high clock rates. 6 Conclusions and Future Work A new low-overhead asynchronous network was introduced for chip multiprocessors, that provides scalable,high-bandwidth on-chip communication, beginning with the detailed design of network primitives and extending to initial system-level performance evaluation. Unlike a synchronous network, it provides support for heterochronous systems consisting of synchronous nodes with unrelated arbitrary clock rates. The synchronous implementations of comparable network primitives use 5.6-10.7x the energy per packet and 2.8-6.4x the area when compared with the new asynchronous designs. Mixed-timing interfaces with new custom protocol converters were then proposed to provide robust communication between synchronous and asynchronous timing domains. Then, network primitives were assembled into a Mesh-of-trees [2] topology for preliminary system-level performance evaluation against a synchronous MoT network, ﬁrst in isolation and then with accompanying mixed-timing interfaces. Finally, the mixed-timing network is embedded and co-simulated with the XMT processor and the performance is evaluated by running several parallel kernels. The new GALS XMT processor provides comparable performance to the existing synchronous XMT except in the most challenging case of high clock rate and high trafﬁc rate. As future work, further architectural and circuit-level optimizations discussed in previous sections are expected to improve overall system-level performance, as well as to develop a CAD ﬂow for automated synthesis, possibly building on [26]. In addition, we aim to redesign the asynchronous MoT network as an MoT/butterﬂy hybrid topology, as recently proposed for a synchronous network [3], to further reduce area overheads. "
Asynchronous Bypass Channels - Improving Performance for Multi-synchronous NoCs.,"Networks-on-Chip (NoC) have emerged as a replacement for traditional shared-bus designs for on-chip communications. As with all current VLSI design, however, reducing power consumption in NoCs is a critical challenge. One approach to reduce power is to dynamically scale the voltage and frequency of each network node or groups of nodes (DVFS). Another approach to reduce power consumption is to replace the balanced clock tree with a globally-asynchronous, locally-synchronous (GALS) clocking scheme. NoCs implemented with either of these schemes, however, tend to have high latencies as packets must be synchronized at intermediate nodes between source and destination. In this paper, we propose a novel router microarchitecture which offers superior performance versus typical synchronizing router designs. Our approach features Asynchronous Bypass Channels (ABCs) at intermediate nodes thus avoiding synchronization delay. We also propose several new network topology and routing algorithm that leverage the advantages of the bypass channel offered by our router design. Our experiments show that our design improves the performance of a conventional synchronizing design with similar resources by up to 26% at low loads and increases saturation throughput by up to 50%.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip Asynchronous Bypass Channels: Improving Performance for Multi-Synchronous NoCs Tushar N. K. Jain, Paul V. Gratz, Alex Sprintson and Gwan Choi Department of Electrical and Computer Engineering, Texas A&M University {tnj07,pgratz,spalex,gchoi}@tamu.edu Abstract—Networks-on-Chip (NoC) have emerged as a replacement for traditional shared-bus designs for on-chip communications. As with all current VLSI designs, however, reducing power consumption in NoCs is a critical challenge. One approach to reduce power is to dynamically scale the voltage and frequency of each network node or groups of nodes (DVFS). Another approach to reduce power consumption is to replace the balanced clock tree with a globally-asynchronous, locally-synchronous (GALS) clocking scheme. NoCs implemented with either of these schemes, however, tend to have high latencies as packets must be synchronized at intermediate nodes between source and destination. In this paper, we propose a novel router microarchitecture which offers superior performance versus typical synchronizing router designs. Our approach features Asynchronous Bypass Channels (ABCs) at intermediate nodes thus avoiding synchronization delay. We also propose a new network topology and routing algorithm that leverage the advantages of the bypass channel offered by our router design. Our experiments show that our design improves the performance of a conventional synchronizing design with similar resources by up to 26% at low loads and increases saturation throughput by up to 50%. Index Terms—NoC, GALS, asynchronous interconnect, on-chip networks. I . IN TRODUC T ION With the continued advance of Moore’s Law, ever increasing transistor densities are yielding ever greater numbers of processing elements (PEs) on chip, resulting in the current proliferation of chip-multiprocessor (CMP) and system-onchip (SoC) designs. As the number of PEs increases, the communication between those components is becoming ever more critical. Networks-on-Chip (NoCs) have recently emerged as a scalable alternative to the bus-based and ad-hoc interconnect seen in past designs. NoCs leverage the design techniques of multi-hop interconnection networks developed for the large scale multiprocessor computers of the 1990s, to provide lowlatency, scalable communication between PEs on chip. Although Moore’s law continues to provide greater transistor densities, current generation VLSI presents several challenges going forward. In particular, balancing performance against the energy and power consumption in a given design is a challenge. A popular method to achieve this balance is dynamic voltage and frequency scaling (DVFS) [1]. In CMP and SoC designs which support DVFS, the voltage and frequency of individual PE nodes, or sets of PE nodes are dynamically managed to reduce overall power and energy consumption while maintaining enough performance to meet application demands [2]. Another common approach to reduce the power and energy consumption in CMPs and SoCs is the use of a globally-asynchronous, locally synchronous (GALS) clocking 978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.15 51 Fig. 1: An ABC network topology showing red and blue chains. Also depicts path traversed between (0,6) and (6,4). scheme [3]. In traditional VLSI design, a single synchronized clock is maintained throughout the entire chip. These synchronized architectures require fully balanced clock distribution trees to ensure minimal clock skew between communicating components. Fully balanced clock distribution trees, however, consume a signiﬁcant portion of the total chip power which can be as high as 30% of the total power consumed by the chip [3]. On-chip power consumption can be reduced by replacing the balanced clock tree with a GALS clocking scheme which only guarantees minimal clock skew within the local processing element. PEs in both DVFS and GALS systems can be implemented as multi-synchronous systems with each PE operating in their respective clock domains which may have different phase and frequency relations. Communication between such nodes typically requires synchronization into the receiver’s clock domain in order to avoid metastability problems caused by violations of setup or hold time of the ﬂip-ﬂops when latching data from a different clock domain [4]. A well established approach to avoiding metastability involves adding multiple stages of latching on incoming signals to reduce the likelihood of a metastable event. While this approach allows reliable communication across clock domains, it comes at the cost of an added two or three cycles of latency for every clock domain traversal. As the number of CMP and SoC PE nodes increases, the number of clock domains which data must traverse between source and destination will increase, thereby increasing the effective latency of that communication. This latency has been shown to directly impact the system performance and the overall cost [5]. With low communication latency as our objective, we propose a new router microarchitecture for multi-synchronous NoCs which avoids synchronization delay at the intermediate nodes via an asynchronous by-pass channel (ABC) around these nodes. We also propose a novel 2-D grid based topology and routing algorithm to complement our router design. The main contributions of this paper are: 1) We present the ABC router design, a novel router design which offers a highly skew tolerant solution to DVFS and GALS NoCs; 2) ABC routers offer a low latency, asynchronous bypass path through the intermediate routers, avoiding synchronization delay and latching at intermediate nodes; 3) We also present an optimized network topology and routing algorithm to leverage the advantages of ABC routers. I I . MOT IVAT ION The intuition behind the ABC design is that a packet should not be required to latch at the intermediate nodes within an NoC. At a minimum, intermediate node latching results in one cycle of overhead for synchronous designs and at least two cycles of overhead for multi-synchronous designs which require synchronization. If intermediate node latching can be avoided, a signiﬁcant fraction of the packet latency overhead can be removed. In the absence of network congestion, a packet should not, in-fact, require latching at intermediate nodes. While at high congestion, when multiple packets are vying for the same buffer FIFO and output port resources, the packet must be latched at the intermediate nodes as it waits for the contention to clear. At low congestion levels, however there is no such constraint. As we will show, the ABC design offers an asynchronous bypass path which bypasses the buffer FIFOs as well as synchronization at the intermediate nodes. Thus the packets can virtually ﬂy through the network without getting latched at the intermediate nodes, reaching there destination with a delay approaching the wire delay of the connecting links. I I I . N E TWORK O PERAT ION On its path through the network a packet traverses three node classes, the source node, intermediate nodes and the destination node. In a typical GALS or DVFS NoC each of these nodes may operate in its own clock domain. The goal of the ABC router design is to reduce or remove latching and synchronization at the intermediate nodes. Furthermore, the routers should independently and dynamically determine the availability of the bypass channels without the overheads of allocation or setup and tear-down. In this section we discuss the design and operation of the ABC router to show how these goals were met. A. ABC Network Architecture As illustrated in the packet propagation path shown Figure 1, we observe that packets traversing a given intermediate node router are most likely to travel straight, that is from input port on one side of the router to output port on the opposite Fig. 2: ABC router microarchitecture design. side. In a typical 2D Mesh network, a minimum length path between any two nodes may be found which has at most one turn. We propose to leverage this observation by biasing our ABC routers towards the straight path over the turn path. ABC routers are pre-disposed to allow the asynchronous propagation of packets along the straight path, as long as conditions allow. In 2D Mesh networks, only a limited set of sourcedestination pairs require no turn during packet traversal, those pairs which are in the same column or row. We observe that it should be possible to gain a further beneﬁt from the ABC router’s straight path bias if the network topology provides a greater number source-destination pairs which do not require a turn. To this end, we propose a new class of network topologies, the double-chain topology, one of which is shown in Figure 1. A double-chain topology is comprised of two disjoint but overlapping chains, each of which connects all network nodes. Double-chain topologies provide an advantage over 2D Mesh networks for ABC routers by providing two paths comprised solely of “straight-path” links between all source-destination pairs. As the traditional 2D mesh terminology of North, South, East and West is less useful in a double-chain topology, we will label these chains the red chain and a blue chain respectively. As shown in Figure 1, nodes within each chain are perceived to be ordered such that traversing a given chain in one direction is considered to be increasing (e.g. the red+ direction) and traversing that chain in the opposite direction is considered to be decreasing (e.g. the red- direction). Jumping from one chain to other is referred to as a turn. To avoid deadlock in the network, the chains themselves are ordered, and a jump is allowed only from the blue chain to the red chain and not the vice-versa. B. Router Microarchitecture Figure 2 shows a high-level block diagram of the proposed ABC router. Similar to a typical 2D Mesh NoC router, the 52 ABC router contains ﬁve input and output ports corresponding to the four neighboring directions and the local processing element (PE). All communications are carried out in the form of packets, subdivided into ﬂits, which are sent through the NoC using wormhole ﬂow-control. The ABC router design is source-synchronous, hence each port’s link is comprised of a clock bit along with data bits and an on/off ﬂow control bit. Both the data and clock signals travel same link length and router logic, therefore they face nearly the same delay and their skew does not increase much for each hop traversed. Nevertheless, after place and route, if an unexpected skew is observed between the clock signal and the ﬂits, transparent latches or buffers may be inserted to reduce it, at the cost of some added latency. 1) Packet Structure: ABC packets are deterministically source routed, i.e. they have a full set of routing directions encoded into the header. Source routing is used to ensure a minimal delay and skew induced at each hop. The three most signiﬁcant bits of each ﬂit contain ﬂit type (Header, Body or Tail) and a valid bit. The fourth and ﬁfth most signiﬁcant bits of the header ﬂit encode the routing directions for the packet at that particular node. After each hop, the current routing bits are shifted away until only a ﬂag is left which marks arrival at the destination. Since the static shift operation can be achieved without any logic, we are able to retrieve the next hop information at the node without impacting the skew between the ﬂits and its clock signal. 2) Router Datapath: As shown in Figure 2, the ABC router contains ﬁve output unit blocks corresponding to each direction. These output unit blocks are broken into three types. Output block “A” connects the blue (+ or -) and local inputs to the opposite blue chain output, while output block “B” connects the all the possible inputs to the corresponding red output. Block “C” connects all possible inputs to the router’s local PE. Block “A” has a smaller set of inputs because it is illegal to route from a red input to a blue output as discussed in Section III-A. At the intermediate nodes, if a packet travels straight through the router, it is desirable that the packet be forwarded without latching. To this end, output blocks “A” and “B” both contain an “ABC Path” which the router is pre-disposed to use in the absence of congestion. When an incoming ﬂit arrives, it is simultaneously latched into the synchronous FIFO (syncFIFO) and propagated along the ABC path towards the output mux. In the absence of congestion, indicated by a lack of packets currently waiting for the output port and the presence of favorable downstream control ﬂow, the ﬂit is propagated through the output port mux via the ABC and the contents of the sync-FIFO are ﬂushed away. Alternately, if the buffer FIFOs are not empty, indicating presence of older packets, the contents of the sync-FIFO are forwarded to the straight path bi-synchronous FIFOs [6] (bi-FIFOs) and the output port is fed by one of the bi-FIFOs. Thus, the sync-FIFO acts as a backup in case the ﬂit was not able to successfully utilize ABC. Alternately, if the packet needs to turn at that particular node, the ﬂits are latched into one of the turn path bi-FIFOs. Although there are no buffer FIFOs along the ABC path, we employ bi-synchronous FIFOs (bi-FIFOs)[6] to store packets in the event of congestion. Bi-FIFOs allow writes and reads to be done in different clock domains, although they require an overhead of two to three clock cycles to avoid metastability. In the ABC routers, bi-FIFO writes are done in the incoming clock domain and reads are done in the local clock domain. Overall the complete router has 10 bi-FIFOs, two of which are shared between block “C” and the two block “B”s. The path joining a given input port with the same color output port (e.g. red- to red+), is referred to as a “straightpath”. All other paths are referred to as turn paths as shown in Figure 2. Each turn path is made up of a single bi-FIFO. The sync-FIFOs are standard FIFOs which are clocked in the incoming clock domain. In the current ABC router design, the ABC is only a bypass around the straight path FIFOs, as shown in Fig. 2. We considered providing ABC paths for the turns as well, but the approach was not followed, due to exponential increase in FIFOs required to implement it. ABC routers employ on/off ﬂow control. This signal which is generated at the downstream node, needs to be synchronized with respect to the outgoing clock of the upstream node. Thus, summing up the round trip latency, which is nearly 1.5 times the clock cycle as discussed in Section IV and two cycles for synchronization, to ensure no ﬂit gets dropped in the process, the depth of the FIFO must be overcommitted by at least four more than the maximum packet length. 3) Router Arbiter: As shown in Fig. 2, the output channel is shared between ABC, the straight path bi-FIFO and turn path bi-FIFOs. To avoid livelock and ensure routing fairness the arbiter selects the ABC path in the ABC mode, and arbitrates among the bi-FIFOs according to the Round Robin algorithm. However, on transition from ABC mode to FIFO mode, the straight path bi-FIFO is given a priority. As shown in Fig. 2, the router arbiter is divided into two types of control modules for each output unit block, the clock control module (Ccontrol) and the data control module (Dcontrol). The Ccontrol module arbitrates the output clock while the Dcontrol module controls the output data. As the Ccontrol and Dcontrol modules receive inputs from two different clock domains, metastability must be addressed. We avoid all metastable conditions through control logic synchronization as explained in the next section. Further detail on metastability avoidance is given in Section III-B4. On arrival of a head ﬂit at the output, the router becomes active and remains in this state until a tail ﬂit arrives, when it becomes idle. Arbitration occurs in the idle state, except if an off signal is received from the downstream node while the ABC is in use in which case the chosen path is switched to the straight path bi-FIFO. 4) Control Logic: The control logic in an ABC router can be represented as a Moore ﬁnite state machine (FSM). The various states of this FSM are displayed in Figure 3. Each output port in the ABC router operates in one of two modes: ABC mode where ﬂits are forwarded along with the upstream or the incoming clock, and FIFO mode where incoming packets are synchronized into the local clock domain and forwarded along with the local clock. This ensures that the 53 Fig. 3: FSM depicting the working of the router. output data ﬂits are synchronous with the output clock thus avoiding potential metastability conditions downstream. The ABC state and the FIFO state are two stable states for this FSM. The state machine can stay in any other state for only one cycle of the local clock. The trigger signal for transition from one mode to another is generated in the outgoing clock domain. This signal is then synchronized into the desired clock domain by standard, two-cycle synchronization. Once the trigger signal has been synchronized the transition to the new clock begins and only after a successful transition of the clock is the datapath switched, thus avoiding potential metastability conditions in the control logic. ABC Mode to FIFO Mode: Transition from ABC mode to FIFO mode for a given output port occurs on the arrival of a packet in one of the bi-FIFOs or an off signal from the downstream node. As presented in the Fig. 3, in both the cases, ﬁrst the output clock is switched to the local clock (State 1). The datapath is then switched to one of the bi-FIFOs depending on which has a packet in it (State 2). The straight path bi-FIFO is given preference just after the transition to ensure that all the ﬂits of a packet on ABC pass through the router before a new packet is transmitted thus avoiding interleaving. In State FIFO the router begins transmission of the packets present in the bi-FIFO. During the normal operation in FIFO mode, the bi-FIFOs are selected by Round Robin algorithm. The ﬂits are then transmitted depending on the off signal from the downstream node. Altogether the transition from ABC mode to FIFO model takes three cycles to complete. This added latency is not seen by ﬂits traversing the straight path because they are already latched into the sync-FIFO. FIFO Mode to ABC Mode: This transition occurs when the FIFO mode is currently selected and there are no ﬂits occupying any FIFO associated with the port. These transitions can be divided into 4 stages corresponding to States 3 to States 9 in Figure 3: (i) In the ﬁrst stage, the read signals of all the bi-FIFOs are deactivated. This makes sure that no packet is read out during the clock transition (State 3). (ii) If all the turn buffer FIFOs are empty, the straight path is then selected as the output datapath, (State 4). (iii) In the third stage if the straight path bi-FIFO is empty and the downstream node is not transmitting an off signal, the output clock is switched to the incoming clock;(State 5 to State 8). It takes at least two cycles for the arrival of a packet at the input of the bi-FIFOs to translate in the reading clock domain, so to ensure that no packet was latched into the bi-FIFO while the clock transition was taking place, this stage takes four cycles to complete. (iv) In the ﬁnal stage, the datapath is switched to ABC State 9. However, if a ﬂit arrives on the straight-path in any of the above mentioned stages the transition is aborted and the datapath is switched back to the straight path bi-FIFO at a cost of three additional cycles. IV. TO PO LOGY AND ROU T ING A LGOR I THM As discussed in Section III-A, we propose a new class of topologies, double-chain topologies, to leverage the advantages of our ABC router design. In particular, double-chain topologies take advantage of the lower latency of the straightpath over the turn-path in an ABC router. These topologies are comprised of two disjoint but overlapping chains, each of 54 Note that for the purposes of this discussion we will examine chains rather than rings, to simplify deadlock avoidance. The intent of double-chain topologies is to increase the number of source-destination pairs which are connected by straight-paths, thereby reducing the average packet latency. In order to evaluate the relative merits of individual topologies we must understand the relative latencies of the straight-path versus the turn-path. B. Router Traversal Latency When not experiencing congestion, packets traversing an ABC router may take either a straight-path ABC channel or be latched into a turn-path bi-FIFO. The latency of bi-FIFO is two cycles. Thus, combined with the latency of the control logic, there is a penalty of three cycles for a packet to make a turn. Alternately, packets which traverse the router on a straightpath, utilizing the ABCs, will only face the link delays. According to the International Technology Road map for Semiconductors [7], assuming 22 nm technology, the global clock frequency would be around 3 GH z while the link delay should be between 700-1300 ps/mm. Thus, we will assume the link delay, combined with the small amount of combinatorial logic delay in each router to be approximately 75% of a clock cycle. C. Routing Algorithm To complement the new, double-chain topologies we use with ABC routers, we developed a modiﬁed version of standard XY dimension order routing (DOR). Our routing algorithm leverages the observation that in any double chain network there are always three different routes between any two nodes: 1) following the straight-path on the blue chain, 2) following the straight-path on the red chain, or 3) following a DOR path starting on the blue chain and turning to the red chain. Our algorithm attempts to optimize packet route based upon assumptions of link and turn delays under no load. Suppose that each link between two adjacent nodes causes a delay of x clock cycles. A turn costs 3 cycles in addition to the link delay. If a ﬂit turns it faces an additional delay of (3/x + 1) times the link delay. Thus, if the number of hops is less than or equal to (3/x + 1), traveling on ABC is a better choice than making a turn. Traveling along the straight path requires more hops than this value, turning leads to less delay in terms of clock cycles. Our routing algorithm selects from among the three legal paths between a source-destination pair based upon this estimation of latency under no-load. Using the estimated router latencies discussed in Section IV-B, yields a turn cost of (3/.75 + 1) = 5. Thus, for the given topologies, we remain on the same chain if the number of hops is less than or equal to ﬁve else we make a turn. As can be seen from the Fig. 1, a packet generated at (0,6) will travel only through the blue chain to reach (6,4). D. Analysis In this subsection we discuss our selection of doublechain topologies for a 7x7 network. Initially we ran a brute force search to ﬁnd the optimum topology. We exhaustively (a) Serpentine (b) Hook (c) Sickle (d) ’H’ Shape Fig. 4: 2-D topologies for ABC router network. which connects all network nodes. As the possible space of all possible double-chain topologies is large, in this section we discuss the selection of a double-chain topology and routing algorithm suitable for the 2D planar silicon substrate of current NoCs. A. Topology Characteristics To limit the state space of to those networks that might be easily implemented in 2D planar silicon, we began with the following restrictions: • The topology should not require more router ports than a typical 2D Mesh would require (ﬁve ports in total). Adhering to this restriction helps ensure that the design complexity and area of the ABC router remains similar to that of a 2D Mesh router. • Each topology must consist of only two chains, each of which must pass through every network node. Simple analysis shows that it is not possible to connect all nodes with more than two chains without increasing the number of router ports or subdividing the network into nonoverlapping chains. Subdividing the network into more than two non-overlapping chains always performs worse than connecting the non-overlapping chains together. • Each chain may only connect near neighbor nodes together in one hop. This restriction ensures that there are no “ﬂy-over” links, links which pass through but do not connect to a given node. Fly-over links reduce the maximal wire density without adding to a node’s I/O bandwidth. • The overall maximum bisection bandwidth of the topology must not be signiﬁcantly more than that of a standard mesh topology. Strictly speaking, connecting all network nodes together in two chains will always require at least one more bisection link than a comparable 2D Mesh topology. We will only examine topologies which increase bisection wire density minimally and in our evaluation we will ensure packet ﬂit counts are adjusted to reﬂect the slightly reduced available wire density. 55 in Fig. 4(a) and implemented using the routing algorithm described in Section IV-C. To capture ABC router network performance results, a fully synthesizable, Verilog implementation of the ABC network was designed and simulated. The baseline design consists of an 7x7, 2-D mesh network with XY DOR routing. In the baseline design packets are synchronized at every hop, incurring a delay of three clock cycles per hop. The baseline router has two, eight-ﬂit-deep, virtual channels (VCs) per port, yielding approximately the same area overhead as the ABC router. We evaluated three types of synthetic workloads: uniform random, bit complement, and transpose. In all cases packets were injected according to a uniform random process. Five experimental runs were completed for each workload and the mean and standard deviation of the results are shown. Packet length was varied randomly between two to ﬁve ﬂits and the simulator was run for 1000 cycles of warm-up followed by 5000 monitored packets. To determine the performance under a realistic workload the ABC network was also evaluated under network traces taken from the SPLASH-2 suite of benchmarks [8]. The traces were obtained from a forty-nine node, shared memory CMP system simulator, arranged in a 7x7 2-D mesh topology [9]. Due to the simulator performance limitations associated with full Verilog simulation of all 49 cores, we were forced to run only a small portion of the actual trace. To have an un-biased evaluation, we chose 500,000 cycles from the middle of each trace with a warm-up period of 10,000 cycles. Finally, the Serpentine topology, shown in Fig. 4(a), has one extra bisectional link than a standard 2D Mesh topology. Assuming equal bitwidths between the ABC and baseline topologies would give the ABC network 14.28% more bisection bandwidth. To approximate a uniform wire density across all bisections between the ABC and baseline designs, we decreased the number of ﬂits in each baseline network packet by one relative the ABC design. B. Synthetic Trafﬁc Fig. 6 compares the latency performance of ABC router with the baseline for uniform random, transpose, and bitcomplement workloads. In uniform random trafﬁc, each source is equally likely to send a packet to each destination. Among the three trafﬁc patterns, random trafﬁc uniformly balances load even for topologies and routing algorithms that normally have poor load balance. The other two patterns concentrate on individual source-destination pairs, thus stress the load balance of a topology and routing algorithm [10]. For uniform random trafﬁc, shown in Figure 6(a), ABCs offer 20% improvement in no-load latency and saturation throughput is improved by 50% over the baseline design. The improvement in the saturation throughput occurs due to our topology’s increased number of bisectional links compared to a standard mesh network. Transpose trafﬁc, in Figure 6(b), shows little improvement in the no-load latency. This is because, for transpose trafﬁc, packets must always take a turn due to the arrangement of source-destination pairing. Saturation throughput, however, is Fig. 5: Average and worst case hop count for the four topologies. computed all the possible topologies that met the topology restrictions described in Section IV-A and evaluated them based upon average, zero-load latency for random trafﬁc given the routing algorithm described in Section IV-C. Using this technique we were able to ﬁnd the optimum topologies for 4x4 and 5x5 network, however due to the exponential expansion of the solution set we were unable to compute the optimal topology for larger networks. Instead, we extrapolated the top four results obtained from the evaluation of 4x4 and 5x5 networks to up to a 7x7 by network and then compared the four resultant topologies to determine the best topology. Figure 4 depicts the four topologies evaluated for an 7x7 2-D network. The evaluation results for these topologies are shown in Figure 5. The ﬁgure shows both the average, as well as the worst case no-load, packet latency for every possible source-destination pair. As shown, the Serpentine topology performs the best with respect to both the average as well as the worst case latency. This is because it is the only topology among the four, in which each node is connected to all its physical neighbors through both the chains resulting in lower cost paths in comparison to the other topologies for all source and destination pairs. The Hook topology is also a close contender. We found, however, that as the turn-path penalty decreases with respect to the straight path, the Serpentine topology performs better than the Hook topology. Therefore, we use the Serpentine topology for our implementation. These four topologies do not represent all possible topologies, and we plan to investigate an optimal ABC topology further in future work. V. EX PER IM EN T S AND EVALUAT ION In this section, we evaluate ABC routers experimentally to gain an understanding of their performance under different types of synthetic and realistic workloads. We compare ABC’s performance against a baseline, synchronizing router design. A. Methodology In these experiments, we evaluate a network of ABC routers connected in the 7x7 2-D Serpentine topology depicted 56 (a) uniform random (b) transpose (c) bit-complement Fig. 6: Average latency vs injection rate greatly improved as compared to the baseline design. For bit complement trafﬁc, shown in Figure 6(c), ABCs offer an improvement of 26% in no-load latency as well as 26% percent improvement in saturation throughput. In bit complement trafﬁc, packets travel further than the other two synthetic patterns; therefore, ABC’s beneﬁts compound resulting in much better performance than baseline. Generally, ABCs outperform baseline for all the workloads. An important feature common for all the three loads is the shape of the curve. In all the three graphs the shape of the curve for the ABC design is uneven, unlike the baseline. For example, for random trafﬁc there is a distinct bump in latency at 15% trafﬁc. At around 8% trafﬁc, congestion causes the routers to switch back and forth between FIFO and ABC modes leading to an increase in the acceleration of latency until the load becomes large enough that the routers get locked in FIFO mode, thus causing a fall in the rate and thus a bump is formed. C. Realistic Trafﬁc Figure. 7 presents the relative performance of the ABC network with respect to the baseline design for the SPLASH2 traces. From the ﬁgure it can be observed that ABC outperforms the baseline design in all the traces except one, (LU), where baseline is marginally better than ABC. ABC shows an improvement of nearly 30-35% over the baseline design for FFT and Water-Spatial. The Geometric Mean for the relative latency is 84.5% less than baseline thus, on the average, the ABC network performed 15.5% better than the baseline design for the fraction of realistic trafﬁc observed. V I . R E LATED WORK In this section we describe the connections between this work and other works which address performance improvements in asynchronous interconnection networks and new topologies to reduce hop count. A. Asynchronous Interconnect A critical aspect of multi-synchronous interconnect design is avoidance of metastable state due to a violation of setup or hold time of the ﬂip-ﬂops and registers present in the system. In the metastable state the output of the system is unpredictable. A well established approach for synchronization Fig. 7: Relative Performance against baseline for realistic workload (SPLASH-2) in NoCs was proposed by Panades and Greiner, in the form of an optimized bi-synchronous FIFO featuring low-latency and small footprint [6]. This FIFO adds a latency of two and three clock cycles to gain robustness against metastability. Another solution, presented by Dally and Poulton, consists of delay-line synchronizers, using a variable delay on the data lines [11]. This delay avoids switching in the metastable window of the receiving registers. This solution requires careful, post-manufacturing tuning to ensure metastability is avoided. Variable delay lines also may make this solution undesirable as they are not always available in standard cell libraries. Mangano et. al. proposed the skew insensitive link (SKIL) solution to address metastability in mesochronous interfaces [12]. SKIL supports arbitrarily skewed clock signals by relying on a two stage buffer FIFO structure by writing and reading ﬂits into and from the buffer FIFO in a ping-pong fashion. It should be noted that some of the above synchronization solutions could be used to augment our ABC approach, we plan to explore this in future work. In each of the above techniques, data must be synchronized at each intermediate node. Synchronization delay at every hop forms a major portion of the total latency and can be reduced by either 57 lowering the hop-count or reducing the per-hop delay. Another facet of GALS architecture is a completely clockless implementation where information exchange occurs using a handshake to explicitly indicate the validity and acceptance of data. This is in contrast to the multi-synchronous and synchronous design styles, which use a globally distributed clock signal to indicate moments of stability of the data. Some popular implementations are CHAIN [13], MANGO [14], ANoC [15], and QNoC [16]. While, these techniques offer high performance in terms of latency and reduced idle power, they involve asynchronous or non-standard cell modules in the router design, whereas, ABC routers can be implemented using standard cells. B. Low Hop Count Topologies New topologies to lower the hop count have also been explored. Dally proposed express cubes which help in lowering the per-hop latency rather than the hop count [17]. An express cube is a k-ary n-cube, augmented by one or more physical links or express channels that allow nonlocal messages to bypass the intermediate nodes. Kim et al. and Grot et al. proposed topologies with higher radix routers, leading to lower hop counts but resulting in more complex router designs [18, 19]. Express virtual channels (EVCs) have also been proposed to reduce per-hop delay. EVCs virtualize the physical links in an express-cube, limiting wasted physical link bandwidth [20]. EVC-based ﬂow control allows virtual bypassing of the packet at the intermediate nodes thus reducing the per-hop delay. This method is efﬁcient for multi-hop packets but does not fare well for communications between neighboring routers. In contrast to the prior work, the ABC router design targets both the per-hop latency and the hop count to reduce network latency. ABC routers, thus, perform well for both long-haul and short-haul packet communication. Also in contrast to EVCs and express cubes, ABCs are physical links present in the router offering an asynchronous bypass from the buffer FIFOs. Since ABCs are inherent to the router, they can be dynamically assigned at every hop, unlike EVCs which must be allocated upstream at speciﬁc nodes only. A packet takes an ABC whenever possible thereby avoiding FIFOs and synchronization delay, resulting in a lower per-hop latency. V I I . CONC LU S ION S AND FU TUR E WORK In this paper we present the design, implementation, and evaluation of asynchronous bypass channel (ABC) routers. ABC routers aim to achieve lower latencies by eschewing synchronization and latching at the intermediate nodes between source and destination with a network. We present a detailed microarchitecture for an ABC router. We also present a new class of network topologies and associated routing algorithm to complement the router design. This new class of topologies, double-chain topologies, leverages the ABC router’s bias towards the straight path over the turn path. Our experiments show that our ABC router, interconnected via a double-chain topology, produces signiﬁcantly lower latencies compared to baseline. This paper introduces a framework for ABC router based networks. Our future work will focus on improving the per58 formance of the network through lower latency synchronizing techniques in the turn path. We also plan to further investigate optimal topologies for ABC enhanced routers. As mentioned in Section III-B, skew reducing latches might be required if the links are physically long. Further study needs to be done in this direction. The presented network offers the beneﬁt of path diversity which could not be exploited by the static routing algorithm thus necessitates employing a dynamic algorithm. Also, dynamic congestion information may be used to inform the mode switch between the FIFO and ABC modes to reduce the thrashing we see in lightly congested network and improve performance. "
Fault-Tolerant Flow Control in On-chip Networks.,"Scaling of interconnects exacerbates the already challenging reliability of on-chip networks. Although many researchers have provided various fault handling techniques in chip multi-processors (CMPs), the fault-tolerance of the interconnection network is yet to adequately evolve. As an end-to-end recovery approach delays fault detection and complicates recovery to a consistent global state in such a system, a link-level retransmission is endorsed for recovery, making a higher-level protocol simple. In this paper, we introduce a fault-tolerant flow control scheme for soft error handling in on-chip networks. The fault-tolerant flow control recovers errors at a link-level by requesting retransmission and ensures an error-free transmission on a flit-basis with incorporation of dynamic packet fragmentation. Dynamic packet fragmentation is adopted as a part of fault-tolerant flow control to disengage flits from the fault-containment and recover the faulty flit transmission. Thus, the proposed router provides a high level of dependability at the link-level for both datapath and control planes. In simulation with injected faults, the proposed router is observed to perform well, gracefully degrading while exhibiting 97% error coverage in datapath elements. The proposed router has been implemented using a TSMC 45 nm standard cell library. As compared to a router which employs triple modular redundancy (TMR) in datapath elements, the proposed router takes 58% less area and consumes 40% less energy per packet on average.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip Fault-Tolerant Flow Control in On-Chip Networks  Young Hoon Kang, Taek-Jun Kwon, Jeffrey Draper  University of Southern California / Information Sciences Institute  {youngkan, tjkwon, draper}@ISI.EDU Abstract— Scaling of interconnects exacerbates the already  challenging reliability of on-chip networks. Although many  researchers have provided various fault handling techniques in  chip multi-processors  (CMPs),  the  fault-tolerance of  the  interconnection network is yet to adequately evolve. As an endto-end recovery approach delays fault detection and complicates  recovery to a consistent global state in such a system, a link-level  retransmission is endorsed for recovery, making a higher-level  protocol simple. In this paper, we introduce a fault-tolerant flow  control scheme for soft error handling in on-chip networks. The  fault-tolerant flow control recovers errors at a link-level by  requesting retransmission and ensures an error-free transmission  on a  flit-basis with  incorporation of dynamic packet  fragmentation. Dynamic packet fragmentation is adopted as a  part of fault-tolerant flow control to disengage flits from the  fault-containment and recover the faulty flit transmission. Thus,  the proposed router provides a high level of dependability at the  link-level for both datapath and control planes. In simulation  with injected faults, the proposed router is observed to perform  well, gracefully degrading while exhibiting 97% error coverage in  datapath elements. The proposed router has been implemented  using a TSMC 45nm standard cell library. As compared to a  router which employs triple modular redundancy (TMR) in  datapath elements, the proposed router takes 58% less area and  consumes 40% less energy per packet on average.   Keywords-component;  handling, networks-on-chip   fault-tolerant  router,  soft-error   INTRODUCTION  I.  As process technology scales, the integration of billions of  transistors comes with an increased likelihood of failures.  Smaller dimension circuits are getting more sensitive to a  particle strike, increasing the probability that the charge due to  a high-energy particle flips the value in a cell [19]. With  technology trends in device scaling, high clock frequencies,  and supply voltage decreases, fault rates are increasing, which  makes a reliable design a real challenge. Previous literature [1,  18] has suggested various fault handling mechanisms in  complicated processor cores, but fault  tolerance of the  underlying interconnection networks is yet to adequately  evolve. Especially, soft error handling in on-chip networks has  not been significantly addressed, with prior work assuming that  an end-to-end recovery approach works well.   Previous chip multi-processor (CMP) research [18, 16] has  assumed that the CMP network is unreliable. As the packet can  be misrouted or corrupted during transmission, end-to-end  recovery is enforced by examining an error detection code at  the destination. However, end-to-end recovery  involves  additional recovery messages and delays fault detection,  complicating recovery to a consistent global state in a CMP.  For example, if a packet is lost, the system triggers recovery  after a given amount of time to avoid deadlock. This increases  the fault detection latency of the packet, while requiring all  nodes to coordinate their validations for checkpoints.   Unlike end-to-end recovery, an approach of link-level  recovery between routers has many benefits as presented in [4,  5, 15]. Link-level retransmission does not require large  retransmission buffers to account for timeout latency. Note that  the timeout latency can be defined as the round-trip time  between source and destination plus some slack for contention  delay [18]; the required storage overhead is not trivial.  Moreover, as link-level retransmission eventually guarantees  end-to-end delivery, no explicit acknowledgment is necessary,  enabling a simple higher-level protocol and reducing the  network traffic.  This paper addresses a fault-tolerant flow control scheme  for soft error handling in on-chip interconnection networks.  The importance of this study is that it presents and evaluates a  method for protecting packets at a link-level for both datapath  and control planes with little hardware overhead, rather than  relying on end-to-end recovery. The proposed fault-tolerant  scheme ensures an error-free transmission on a flit-basis, while  using dynamic packet fragmentation at error detection. Thus,  the proposed router disengages flits from the faulty flit and  safeguards the flits following a faulty flit.    Dynamic packet fragmentation in networks-on-chip (NoCs)  was first introduced in [9, 10] for enhanced virtual channel  (VC) utilization and deadlock avoidance in multicast routing.  The proposed router exploits fragmentation as a response to  error detection and renews state information by reallocating a  new VC. Upon fragmentation, trailing flits can then avoid a  faulty VC which has corrupted states.  The proposed router is evaluated in erroneous environments  by fault injection mechanisms. Statistical fault injection (SFI)  [13] is applied at each bit of a network link with independent  probability. At the various fault rate levels, the proposed router  is observed to perform well, gracefully degrading while it  shows resilience to link errors without missing any flits. In the  intra-router error analysis, the router is observed to have 97%  error coverage for datapath elements.    The proposed  router and comparable designs are  implemented using a TSMC 45nm standard cell library. As  compared to a router [3] using triple modular redundancy  (TMR) for datapath elements, the proposed router occupies  978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.18 79       58% less area and consumes 40% less energy per packet on  average.   This paper is organized as follows. Section 2 describes soft  error handling in the proposed router, and Section 3 evaluates  the scheme in various simulation environments. The related  work is discussed in Section 4, and Section 5 concludes the  paper.  II. SOFT ERROR HANDLING  In on-chip interconnection networks, soft errors could be  categorized  into  two  types based on  the  location and  probability that errors occur: link errors and intra-router errors  [15]. Links are purely wires and affected by cross-talk, whereas  the individual router components are logic circuits and errors  occur in the form of transient faults, which may also cause  erroneous states. The transient faults may not affect the results  of a computation since masking effects (logical masking,  electrical masking, and latching-window masking [19]) reduce  the manifested error probability. Although the errors in links  and intra-routers are distinct in terms of the probability of the  occurrence, they can be tackled together in the proposed  scheme, as described in the following discussion.   Intra-router errors can further be divided into errors in  datapath components and control planes. As datapath  components handle storage and movement of flits in input  pipelines, MUXes, buffers, a crossbar, and output pipelines,  any errors in datapath components are manifested at the output;  thus, they can be handled together with link errors. If an error  occurs in any of the datapath elements, the erroneous flit is  detected by the error detection code at the next hop, just as if a  link error has occurred. So, the location of an error occurrence  in the datapath is not important (except control parts of the  datapath elements such as selection bits of a crossbar). The  significance of errors in the datapath is that errors are reflected  to the output. Therefore, we divide soft errors in the network as  errors in datapath components and control plane. In the rest of  this section, we focus on how the errors in the datapath and  links are detected.  As packet flows are controlled by handshaking between  routers  in on-chip networks, faulty flits can be easily  retransmitted at a link-level. The proposed router implements  retransmission of faulty flits at a link-level and reconstructs a  discontinued packet due to a fault while preserving the flit  order. Based on the fragmentation scheme defined in [9], we  develop a new fault tolerant router, exploiting dynamic packet  fragmentation for handling a discontinued packet due to a  detected error. Fragmentation renews the state information in  control planes through a VC reallocation, preventing corrupted  states from affecting the rest of the flits.  A. Proposed Router Architecture  This section describes architectural details of the proposed  router. Figure 1 features the proposed router pipeline stages and  microarchitecture with added overhead for fault handling in  gray. A combinational Cyclic Redundancy Check (CRC) unit  using an 8-bit CRC polynomial is attached at the input pipeline  (for fault detection) and output of the input unit (for new CRC  generation). Although a CRC is used in the proposed router for   pipeline C R C Input unit preRC ST SAVA Router pipeline stages VC ctrl head VC C R C VC ctrl pipeline Output  unit Crossbar Figure 1.  Proposed router microarchitecture and pipeline stages (preRC: pre  routing computation, SAVA: switch & VC allocation, ST: switch traversal)   error checking, Error Correcting Codes (ECC) [2] can be an  alternative and also applicable for correction as well as  detection. There are trade-offs in design choices between  correction and retransmission, but the comparison of the two  schemes is beyond the scope of this paper.  The CRC unit at the input port checks errors when a flit is  received at the input pipeline, and at the same time the flit is  stored in the corresponding VC buffer. So the CRC is  performed in parallel with flit processing, and it does not  impact  the critical path. From  timing analysis of  the  synthesized design, the critical path is measured to be the VC  and switch (SW) allocation stage (SAVA) in a two-stage router  pipeline. Thus, the CRC unit does not affect the clock period  because the CRC update is performed in the SW traverse (ST)  stage. Note that the flits are updated for VC ID and route field  at the input unit. The new CRC for the modified flits can be  attached just before the flits leave the input unit. The modified  flit with new CRC is transferred intact throughout the router,  traversing the MUX, crossbar and output pipeline. Therefore,  the data-path from the output of the input unit to the input  pipeline at the next router is covered with the flit-level CRC.   In order to provide full coverage of the data-path, we  assume the input buffer is protected with ECC since input  buffers are primarily memory elements comprised of either  SRAM or dense latches. As a result, the flits are protected  while they are transferred in the router along the solid lines as  shown in Figure 1.  In addition to the CRC, a header buffer is included at every  VC for re-establishing a fragmented packet. Upon error  detection, the proposed router fragments the packet to contain  the error and renews the VC states. After fragmentation, body  and tail flits do not have routing information to reconstruct the  trailing part of the packet. Thus a copy of the most recent  header flit should be maintained at every router which the  packet passes through until the tail flit exits the buffer. Using  the buffered header  information,  the fragmented packet  requests a new VC and SW allocation [9]. If both requests are  granted, packet forwarding starts by sending the buffered  header (virtual header) and the rest of the flits can follow the  virtual header. As the header is used for reallocating a VC, the  critical information of the header must remain error-free.  Therefore, we assume the header buffer is also protected with  ECC like the input buffers.     80   B. Fault-Tolerant Flow Control  Now that a preliminary router infrastructure has been  established for fault detection and packet reconstruction, a new  flow control scheme for fault handling is presented. A  conventional VC router, which adopts credit-based flow  control, returns a credit when a flit is forwarded to the next  router and the corresponding buffer entry is free. The upstream  router which receives the credit then sends a next flit. This  enhances buffer recycling by signaling to the upstream router  as soon as the buffer entry is empty. However, if an error is  detected at the downstream router and the buffer entry which  held the faulty flit in the current router is replaced by the next  flit, the faulty flit cannot be recovered unless retransmission is  requested from the source node. So a new flow control scheme,  which keeps the sent flit until the safety of the flit is ensured at  the downstream router, is necessary.  Figure 2 illustrates cycles for credit return to the upstream  router when the router frees the associated buffer. In a  conventional router (Figure 2(a)), the credit is sent at the same  time the flit is forwarded to the next router as described above.  On the other hand, the proposed router (Figure 2(b)) waits a  turnaround  time (5 cycles  in  the proposed router, but  turnaround time can differ based on the router pipeline stages)  until the sent flit is implicitly ACKed from the downstream  router. The downstream router checks errors as soon as a flit is  received and sends a NACK signal in the case that an error is  detected. If no error is detected, the CRC unit just waits until  the next flit is received. In the current router, however, if the  NACK signal is not received within the turnaround time (5  cycles), the current router knows the sent flit is safely delivered  to the next router and can return the credit at the next cycle  (cycle 6). If a NACK signal is received, the router starts the  recovery process from the faulty flit using the proposed  fragmentation technique.  The presented fault-tolerant flow control ensures error-free  flit delivery, but it limits throughput by holding the flits longer  than the conventional case. At a low traffic load, however, the  proposed router does not impact performance for an error-free  scenario with short packets. If the number of buffer entries  covers the credit round trip time, packets are transferred  without delay because pipeline stages are not increased for flit  forwarding and the CRC is performed in parallel with flit  processing.  The flow control described above is for the input buffers.  As the header buffer is maintained separately and cannot be  occupied by regular flits, the flow control of the header buffer  should be controlled independently from the regular input  buffers. For example, if a header flit is forwarded to the next  router, a corresponding credit of the header flit is returned after  the implicit ACK. The upstream router then may send a next  body flit assuming there is an available buffer entry at the  downstream router, but the input buffers at the downstream  router may be full since the forwarded flit was the header flit.  Thus, a flit buffer might be overwritten due to overflow if not  handled properly.  Overflow of the flit buffers can be prevented if an entire  credit count is communicated between routers, but such a  scheme incurs wiring overhead. Dedicating an extra credit line  credit pipe line flit (a) Credit-based flow control in conventional VC router  credit 6 5 flit 1 C R C 3 nack 4 2 (b) Fault-tolerant flow control in proposed router  Figure 2.  Flow control schemes  for a header flit in each VC is a better solution. By returning a  header credit upon header flit forwarding, the upstream router  can discriminate which type of flit is forwarded and determine  whether it can send a next body flit at the appropriate time.  However, in the case of virtual header forwarding, the header  credit should not be returned since a virtual header is not a flit  received from the upstream router, but created internally in a  router. The NACK signal is not necessarily dedicated for the  header flit because the upstream router can differentiate which  flit is diagnosed as faulty among the sent flits by counting the  turnaround time.   Another technique which prevents overflowing of the flit  buffers is using a state machine for the credit counts. When the  upstream router receives the first credit after the header flit  forwarding, the upstream router knows that the header is safely  forwarded at the downstream router and waits until the next  credit is received to send a next body flit. Since using the state  machine does not cost extra wires between routers, we adopt  this technique for the flow control of the header flit.  1) Containment: Having established a mechanism for  initiating flow control which detects errors at the downstream  router, containment and recovery is now considered for the  fault-tolerant scheme. Dynamic packet fragmentation  is  adopted as a part of the fault-tolerant flow control to disengage  flits from the fault-containment VC and recover the faulty flit  transmission. If a VC is diagnosed as faulty during a packet  forwarding, the fragmentation technique severs the packet and  steers clear of the erroneous VC by requesting a new VC. So  the rest of the following flits can avoid the corrupted states of  the VC and be transferred safely to the destination.   Errors in the control plane, such as states of the VC, can be  detected via consistency checks: protocol consistency  (checking the sequence of the head, body, and tail flits), credit  consistency (checking the credits between the receiving end of  the channel and the sending end of the channel), and state  consistency (checking the inputs received are appropriate for  the state) [5]. Sometimes the errors in the control plane are not  observable in the flits themselves and cannot be corrected  unless the corrupted VC is restarted and synchronized with an  adjacent module. By signaling a fault detection upon a  consistency mismatch, dynamic packet fragmentation can  81     release the hold of an output VC and renew state information  by reallocating a VC. So the fault-containment VC can be  avoided, and spreading of state corruption to adjacent modules  can be prevented.   Without the fragmentation and reallocation capability,  errors in the control planes could be recovered by using triple  modular redundancy (TMR) or checkpointing every state in the  VC. Note that TMR comes with a costly area overhead and  checkpointing every state requires complicated control logic.  The proposed fault-tolerant scheme with dynamic packet  fragmentation efficiently handles erroneous states with little  hardware overhead. Moreover, dynamic packet fragmentation  can be combined with various fault detection schemes  suggested in [11, 15, 7, 12] to build more reliable routers.   2) Recovery: The recovery process using fragmentation is  as follows. Once an error is detected at the downstream router,  the faulty flit is used as a virtual tail flit, and the in-flight flits  which are already transmitted following the faulty flit are  squashed at the downstream router. In the upstream router, a  VC  is reallocated for retransmission, and  the packet  forwarding starts from the virtual header using the buffered  header information. The faulty flit is retransmitted as a first  body flit and the rest of the body and tail flits follow the  reallocated VC.  In the case of a faulty header, however, there is an  exception. The body and tail flits have no problem as they are  used as virtual tail flits at the error detection, but the header  cannot be used as a virtual tail since it is a first flit of the packet  and it should be dropped. Therefore, the faulty header is not  transferred to the output unit, and its allocated VC is released  immediately at the next cycle.  A fragmented packet can be refragmented if another error  occurs for the same packet. Note that the virtual header and  virtual tail flits are created during error recovery; intermediate  routers treat them identically as normal header and tail flits,  respectively. By receiving the fragmented packets at the  destination node, the destination node knows that the packet  encountered an error during routing, and a reassembly process  can start. The detailed operation of reassembly is not addressed  in this paper, and we leave it as a future work.     End-to-end recovery causes the possibility of latent errors  which are undetected during transmission. The undetected  errors may offset errors, making error detection at the end node  difficult. However, link-level retransmission in the proposed  router can detect errors quickly before another occurs, greatly  increasing the error containment capability. Therefore, the  proposed router not only provides dependability in payloads of  the packet, but safeguards critical information of the header.  III. EVALUATION  This section describes fault-tolerance analyses through a  performance, area and power evaluation. A baseline and the  proposed fault-tolerant router were developed in structural  VHDL code and synthesized using Synopsys Design Compiler  targeting a TSMC 45nm standard cell library. Table I  summarizes    the    common    router    features    and   network  parameters for synthesis and simulation.  TABLE I.   DESIGN EVALUATION PARAMETERS  Topology  # of ports  # of VCs  Flit size  Buffer per port  Mesh 4x4  5  4  128 bits  24 (6-entry depth  per VC)  Routing  Router uArch  Link latency  Packet length  Operating  conditions  Dimension-order  (XY)  Two-stage  1 cycle  0.9V, 25℃  5 & 8 flits  Figure 3.  Performance  A. Performance  Figure 3 shows the average latency for 5- & 8-flit packet  simulations under uniform random traffic in an error-free  scenario. Baseline indicates a generic two-stage router without  a fault-tolerant scheme, whereas FTFC models the proposed  router which implies fault-tolerant flow-control scheme using  dynamic packet fragmentation. The numbers in parentheses of  each scheme indicate a number of buffer entries per VC. As  Baseline has 6-entry buffers per VC, the FTFC router is  assumed to have 5-flit entries for the regular buffers in addition  to an extra header buffer for fairness of the comparison.  In the 5-flit packet simulation, the FTFC router performs  well in an error-free scenario without performance degradation  at low traffic loads. The less flexible buffer utilization in the  FTFC by dedicating a single entry as a header copy buffer did  not affect performance. At the saturation point, however, the  FTFC router suffers from an increased latency by 11.4%. This  is because the fault-tolerant flow control delays buffer  recycling and increases the congestion at high traffic loads.  In  the 8-flit packet simulation,  the FTFC  router  underperforms in terms of latency and throughput by 16% and  8%, respectively. The given buffer entries do not cover the  increased credit round trip time or cannot buffer the entire  packet; therefore, credit stalls dominate the latency. If the  number of flit buffers is over the credit round trip time or a  unified buffer structure is used for flexible buffer utilization,  the FTFC router would appear to have close latency to the  Baseline at low traffic loads as shown in the 5-flit packet  simulation case. For the rest of the evaluation, we assume 5-flit  packets to rule out the effect of performance degradation due to  credit stalls.  82     Some amount of throughput is indeed sacrificed to achieve  the  link-level retransmission capability. How  this cost  compares to an end-to-end retransmission scheme and a  comparison of error coverage between the two approaches is  left as future work.  B.   Link error analysis  This section describes a link error analysis with statistical  fault injection into the RTL model. The analysis provides an  expectation of  the performance  impact  in erroneous  environments and verifies the behavior of the proposed router  when  link errors are present. Figure 4(a)  illustrates  performance levels of the proposed router versus different error  rates. Errors are injected at each network link bit with  statistically independent probability. Although the crosstalk  effect cannot be assumed to be accurately modeled by  statistical independence of errors on adjacent wires [20], the  independent error model simplifies the error modeling and is  appropriate to reflect different types of errors.   No error describes the case without errors in the FTFC  router, whereas error rate indicates soft errors get inserted in  the same router with corresponding random distribution. We  modeled  error  insertions  ranging  from  1.0E-06  (errors/wire/cycle) to 1.0E-03 (errors/wire/cycle), which is an  extreme case where transient errors occur at high probability to  significantly stress the network. Since there are around 10,000  links in the network, the range of the error rate from 1.0E-06 to  1.0E-03 approximates 1 error in every 100 cycles to 10 errors  in every cycle on average in the network. Simulation is  performed under the above conditions until every inserted  packet is delivered to its destination. Among the network links  in which errors are inserted, we assume that critical control  signals of the links (credit and NACK lines) and critical fields  of the flit (e.g. valid and VC ID of each flit) are protected in  redundancy such as TMR. In such a condition, routers are  assumed to never miss returned credits, NACK, and valid flits.  From the Figure 4(a), the proposed router gracefully  degrades in performance without missing any packet in  erroneous environments. At low error rates, the proposed  scheme shows almost the same performance with negligible  difference in latency to the error-free scenario, while it suffers  more in latency and throughput at high error rates. Especially at  the 1.0E-03 error rate, almost every flit encounters at least one  error while it traverses the network. So the network saturates at  a low injection rate of 0.25.    Figure 4(b) demonstrates the average packet latency  according to the error rates for specific traffic injection rates.  As error rates are increased, the average packet latency  gradually increases until around the 1.0E-04 error rate and  starts to saturate above that. A higher error rate increases the  probability that more flits are affected, causing more packets to  be fragmented. This increases the network congestion since the  fragmented packets generate virtual header flits. Figure 4(c)  shows a three-dimensional space of average latency.  Figure 5 shows the statistics of the fragmented packets,  using a logarithmic scale on the Y-axis. The number of  fragmented packets increases with increasing error rate and is  independent of the traffic rate. Since the number of fragmented   (a) Performance in various error rates  (b) Pefromance according to error rates for specific traffic injection rates  (c) 3D space of performance based on traffic injection and error rates  Figure 4.  Performance in link errors  packets corresponds to the number of packets generated, the  network saturates early as more packets are fragmented due to  errors.  In this link error analysis, although we quantified the  performance impact of erroneous scenarios, a key qualitative  outcome of these experiments was that the proposed router  provided full reliability in the presence of transient link errors  with little hardware overhead.  83       TABLE II.   NET PERCENTAGES OF DATAPATH COMPONENTS  Nets  Nets  Input pipeline  2.93%  Output Pipeline  2.84%  CRC  9.12%  Flit buffers  44.05%  Input MUXes  8.5%  Datapath total  78.29%  Crossbar  10.85%  Router  47470  TABLE III.   TOGGLE COVERAGES  Toggle coverage  Baseline  97.5%  FTFC  95.4%  Figure 5.  Fragmented packets  C. Intra-router error analysis  This section analyzes error coverage of the intra-router  components and quantifies the resilience of the overall design.  In order to model all router states and logic accurately, we  generated a netlist of the proposed router. The netlist enables  the investigation of error propagation and provides guidance  for the detectable errors to the output. So the error coverage is  performed by checking that erroneous net candidates are  reflected to the output. Note that errors in a flit are detected at  the downstream router and recovered during retransmission. A  soft error reflected to the output flit implies a recoverable error  in the proposed router.  Among nets which connect the synthesized cells in the  netlist, we listed every possible net in which an error can occur.  The net list is then analyzed to determine whether each error on  a net is reflected to the flit output of the router. From the list of  the nets, the portions in datapath elements are extracted in  Table II. Although the CRC units in the proposed router are  extra logic added for error detection and not present in the  Baseline router, the errors in the CRC fields are also  correctable using retransmission. Hence, we added the CRC  units in the list of correctable datapath elements. As a result,  78.29% net errors in the router can occur in the datapath  elements and considered correctable with the proposed scheme.   Before we measure the error coverage of the intra-router  logic, we evaluated the test vectors in terms of the toggle  coverage. Table III shows the toggle coverage to both Baseline  and FTFC router netlists. From the table, over 95% of the  netlists are toggled in both routers. This indicates that the input  vectors reach almost every area of the router during on-line  operation. The FTFC router, however, gets slightly less  coverage than the Baseline router. This is because the test  vectors do not in themselves generate erroneous scenarios, so  the error handling parts of the proposed router are not  triggered. Since the toggle coverage implies the effectiveness  of the test vectors, a 95% toggle coverage is a good basis for  drawing conclusions about the following error evaluation.  To quantify the error coverage of the proposed faulttolerant scheme, we performed the following procedures using  84 Figure 6.  Error coverage in datapath components  Synopsys TetraMAX. At the beginning of the simulation, every  net is marked as an X state. As the simulation progresses, the X  marked errors are moved to either a detected or undetected  state, depending on whether it is observed at the output. So, a  detected state is considered when the input vector reaches the  error and the value is propagated to the flit output for  observation, whereas an undetected state implies the error is  not reflected to the outputs.   Figure 6 shows the error coverage profiling of each  datapath element in the entire router. Assuming the flit buffers  are protected with ECC, 97% of the errors in the nets are  observed to be reflected in the given datapath elements. 3% of  undetected errors on average may be regarded as masked or  latent under the given test vectors, but cannot be confirmed as  correctable errors. Upon analysis of the undetected errors, we  concluded that they were mostly parts connected in control  units or not reached with the given test vectors. Even if we  exclude the ECC protected flit buffers from the experimented  datapath elements, 94% of datapath errors are observed to be  covered.   Combining both results of link and intra-router error  analysis, the proposed router provides a good coverage of  errors with little hardware overhead as analyzed in the  following.   D. Place & Route  This section provides accurate timing, area, power, and  energy analyses based on the implemented designs. In addition  to  the  two designs, Baseline and FTFC routers, we  implemented a comparable alternative fault-tolerant scheme, a                bit-level TMR router [3]. TMR is just applied to the datapath  elements in the Baseline router to cover the same functionality  as the proposed router. Therefore, the area overhead of the  TMR router is three times of input pipeline, flit buffers, input  MUXes, crossbar, and output pipelines of the Baseline router  plus voting MUXes. The voting MUXes are placed right after  the input pipeline, which is the same place the proposed router  performs CRC. So the errors of the intra-router and links are  detected at the input port and corrected with forward progress  in the TMR router.    The Baseline, FTFC, and TMR routers were  then  synthesized and placed & routed from standard cell netlists. To  minimize dynamic power, clock gating is applied to the flit  buffers in the three routers. A general back-end flow was  followed using Cadence SOC Encounter for the generated  layouts. Figure 7 shows proportionally sized layout pictures of  the three implemented routers.  The FTFC router is measured to have the same clock period  as the Baseline router (0.7 ns) while the TMR router takes more  time (0.8 ns) in critical path delay due to the voting MUXes.  With Forward error recovery (FER), the TMR router makes  forward progress without performance degradation in packet  latency, but it negatively affects critical path delay and  increases area overhead significantly.   The extra logic in the proposed router, however, takes only  13.7% more area overhead than the Baseline router, but it does  not affect the critical path. Rather, the area overhead which is  mostly dedicated to control logic and CRC units is much less  than that of the TMR router. In this area overhead computation,  the link areas which require two times more wires in the TMR  router are not even counted. Moreover, if error resiliency is  considered, the FTFC router is even more superior to the TMR  router. Since the 8-bit CRC checksum detects packets with any  odd number of bits in error and an error burst up to checksum  width [17, 12], the FTFC router provides a high level of  resiliency compared to the TMR. The TMR router can be  resilient up to 1/3 errors in the links and datapath components  depending on the granularity of the TMR, but the increase in  area raises the probability that the error can occur. On the other  hand, the proposed router with 8-bit CRC requires a negligible  hardware overhead compared  to TMR, providing  the  probability to miss any random error less than 0.004 [12].  Combining the error coverage of the datapath elements with  the area analysis, the error coverage of the overall router area  can be generated. Assuming that the error coverage of the nets  can be applied to the area, 97% error coverage of the datapath  implies 75.9% error coverage of the overall router area. Note  that according to the techniques analyzed in [3], the faulttolerant router designs with various levels of protection  typically result in 2-3x more area overhead. Even though the  fault-tolerant capability of the proposed router as designed is  limited to the links and datapath elements, the level of overall  protection achieved is still remarkable, especially given that it  is accomplished with little hardware overhead.  Figure 8 depicts a network power analysis of the presented  routers. While the same workload used in the performance  evaluation is applied to the routers, a value change dump  (VCD) file which captures every switching activity is generated  Figure 7.  Router layouts  from routers in the network. The VCD files are then applied to  Synopsys PrimeTime PX, which analyzes power dissipation  and supports propagation of switching activity for accurate  power analysis. The acquired power values of the routers are  aggregated together for the network power. As can be seen in  the figure, the FTFC router takes 35% more network power  than the Baseline router, but it consumes 41% less power than  the TMR router on average. This is mainly a consequence of  less area overhead in the FTFC router versus the TMR router.  The  injected error rate has  little effect on  the power  consumption of the FTFC router.  Figure 9 compares the energy efficiency of the three routers  throughout the traffic loads. The energy value indicates energy  consumption per packet at each traffic load. The proposed  router shows 40% lower energy compared to the TMR router.  Although the TMR router provides FER which is advantageous  to latency, it suffers from high energy dissipation. From the  observed analyses presented in this section, the proposed router  is outstanding compared to the TMR router and provides  remarkable dependability with less hardware overhead.  IV. RELATED WORK  This section reviews prior schemes of handling errors in  NoC routers. The MIT reliable router [4] implements a linklevel fault-tolerant protocol, the Unique Token Protocol. This  flow control keeps at least two copies of a packet in the  network at all times. If the network is broken between two  copies of the packet, each copy follows multiple paths to the  destination, and the token is changed to replica for all copies of  the packet. The destination node then knows a packet is  duplicated by receiving a packet with a replica token and  deletes duplicates.  The reliable router is somewhat close to the proposed router  in that it reconstructs a fragmented packet, but the flow control  scheme is different because the proposed router maintains a  single copy of the packet in the network. If the packet is  fragmented, the severed packets, but originally a single packet,  are delivered to the destination as separate. On the other hand,  in the reliable router, if the packet is split, it resends the packet  from the header, not from the flit that the error is detected.  Since the reliable router keeps two complete copies of the  packet in the network, packet level recovery is maintained by  sacrificing network  throughput more  than  the proposed  scheme.  85   "
Design of High-Radix Clos Network-on-Chip.,"Many high-radix Network-on-Chip (NOC) topologies have been proposed to improve network performance with an ever-growing number of processing elements (PEs) on a chip. We believe Clos Network-on-Chip (CNOC) is the most promising with its low average hop counts and good load-balancing characteristics. In this paper, we propose (1) a high-radix router architecture with Virtual Output Queue (VOQ) buffer structure and Packet Mode Dual Round-Robin Matching (PDRRM) scheduling algorithm to achieve high speed and high throughput in CNOC, (2) a heuristic floor-planning algorithm to minimize the power consumption caused by the long wires. Experimental results show that the throughput of a 64-node 3-stage CNOC under uniform traffic increases from 62% to 78% by replacing the baseline routers with PDRRM VOQ routers. We also compared CNOC with other NOC topologies, and found that using the new design techniques, CNOC has the highest throughput, lowest zero-load latency, and best power efficiency.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip Design of High-Radix Clos Network-on-Chip  Yu-Hsiang Kao, Najla Alfaraj, Ming Yang, and H. Jonathan Chao  Department of Electrical and Computer Engineering  Polytechnic Institute of New York University  Brooklyn, NY, USA 11201  Abstract—Many high-radix Network-on-Chip (NOC) topologies  have been proposed to improve network performance with an  ever-growing number of processing elements (PEs) on a chip. We  believe Clos Network-on-Chip (CNOC) is the most promising  with its low average hop counts and good load-balancing  characteristics. In this paper, we propose (1) a high-radix router  architecture with Virtual Output Queue (VOQ) buffer structure  and Packet Mode Dual Round-Robin Matching (PDRRM)  scheduling algorithm to achieve high speed and high throughput  in CNOC, (2) a heuristic floor-planning algorithm to minimize  the power consumption caused by the long wires. Experimental  results show that the throughput of a 64-node 3-stage CNOC  under uniform traffic increases from 62% to 78% by replacing  the baseline routers with PDRRM VOQ routers. We also  compared CNOC with other NOC topologies, and found that  using the new design techniques, CNOC has the highest  throughput, lowest zero-load latency, and best power efficiency.  Keywords - Network on Chip; Chip Multiprocessor; Clos  network; High radix NOC  I.   INTRODUCTION  Chip multiprocessors (CMP) have become favored over  traditional superscalar processors for efficiently exploiting  single-chip computational potential. One major  factor  motivating CMP development is that computation speed can be  increased with only a modest increase in power. CMP systems  consist of regular processing elements (PE) and memory  modules. In a CMP system, the chip area is normally divided  into a number of tiles, each containing a PE or a memory  module. Tiles are interconnected through a Network-on-Chip  (NOC), which has a great influence on CMP performance.  The performance of an NOC is determined by many  factors, including topologies, routing algorithms, and flow  control mechanisms. Topology determines the capacity of an  NOC. The capacity is defined as the best possible throughput,  assuming perfect routing and flow control, that could be  achieved by the network under the given traffic pattern [20].  After the topology is decided, the routing algorithm and flow  control are developed to arrange the management of physical  resources in an efficient way to approach the performance  bound. Many NOC topologies proposed to date consist of  regular interconnection structures and low-radix routers, such  as 2-D Mesh and 2-D Torus for ease of implementation.  However, as more tiles or network nodes are put on the same  chip, latency is increased due to the rapidly growing queueing  delay in each router along with a large network diameter. Also,  due to the nature of the network configuration, the capacity of a  low-radix network is usually low, causing low throughput and  low power efficiency.  A high-radix Clos network [1] provides much better  scalability than a low-radix network in terms of zero-load  latency and throughput. A 3- or 5-stage Clos network can  easily accommodate several hundred nodes with a reasonable  router radix. The number of hops a packet traverses in the Clos  network is limited to three or five. Thus, a high-radix Clos  Network-on-Chip (CNOC) provides smaller zero-load latency  as compared to a low-radix NOC. Another advantage of CNOC  is its good load-balancing nature, from the multiple paths  available between any pair of PEs.  One major concern for the Clos network is its large number  of long interconnects, which may lead to an increased routing  area and power dissipation. With “Routing over logic” layout  style [2], the area overhead caused by the long wires can be  eliminated. One method to overcome the energy problem  caused by the long interconnects is to minimize the average  power consumption under uniform  traffic by carefully  designing the floor plan of an NOC. In other words, given an  arbitrary NOC topology, by placing the routers in a proper way  the average power consumption under uniform traffic on the  long wires can be significantly reduced.   In this paper, we propose: (1) the design of the high-radix  CNOC with Virtual Output Queue (VOQ) buffer structure and  Packet Mode Dual Round-Robin Matching  (PDRRM)  scheduling algorithm in the routers to achieve high speed and  high throughput, (2) a heuristic floor-planning algorithm to  minimize the power consumption caused by the long wires.  Based on the design, we describe a study comparing a 64-node  CNOC composed of radix-8 routers to other topologies with  the same router radix upper bound, including 2-D Mesh, Fat  Tree [3], and Concentrated MeshX2 [4]. We show that with the  PDRRM VOQ router, CNOC has the smallest zero-load  latency, highest  throughput, and best power efficiency  compared to other high-radix topologies and the 2D Mesh  network.   II. RELATED WORK  Low-radix NOCs, such as 2-D mesh or torus network, have  the advantage of modest design complexity with a regular  interconnection structure and short wires. However, these  networks suffer several disadvantages, including large network  diameter and energy inefficiencies due to higher hop counts. To  overcome this issue, several high-radix NOC topologies have  been proposed during the past few years. Balfour and Dally [4]  proposed Concentrated Mesh (CMesh) with express channels,  which adapts  the 2-D Mesh by allowing several  tiles  connecting to the same router, and adding express channels on  the edge of the Mesh. Kim et al. [5] proposed Flattened  978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.27 181     Butterfly, which has a regular floor plan as CMesh, with an  even higher router radix to achieve lower zero-load latency and  better throughput. Some studies have been done to evaluate Fat  Tree [6], which could be configured as a high-radix network as  well.   Clos network topology has been considered for NOC  design in the SPIN [7] and Reduced Unidirectional Fat Tree  (RUFT) [8] networks. In the SPIN network, the local traffic  between PEs connected to the same router does not need to  travel more than one hop. While in the 3-stage CNOC, any  packet needs to travel exactly three hops to reach its destination  port. [8] showed that by replacing Fat Tree with RUFT, which  does not allow local traffic, hardware complexity and power  consumption are significantly reduced. However, [8] did not  address  the high-radix Clos network configuration and  implementation issues. Both [7] and [8] did not provide latency  and power comparisons between Clos network and other NOC  topologies.  III. CNOC ARCHITECTURE  Figure 1. shows the configuration of 64-node, 3-stage  CNOC that we adopted in this paper. The switch modules  (SMs) in the first, second, and third stages are denoted as input  modules (IMs), center modules (CMs), and output modules  (OMs). In this configuration, there are 24 routers, 8 for each  stage, with the network capacity at 100% under uniform traffic.  One thing to notice is that all the links shown in the figure are  uni-directional, rather than bi-directional. Hence, in CNOC  every flit has to travel exactly three hops from source to  destination. In the Fat Tree network, the localized traffic does  not need to travel all the way to the root router, because the  links between routers and PEs are bi-directional. Figure 2.  shows the configuration of a 64-node Fat Tree, in which there  are 32 radix-8 routers and 16 radix-4 routers.  The two  localized traffic streams are illustrated by red lines. Compared  to the 3-stage Clos network configuration, Fat Tree network  requires more routers and has a higher wiring complexity. As a  result, Fat Tree has much lower power efficiency compared to  CNOC, which will be shown in section VI.  Input ports  of the PEs OMs CMs c1 c2 c3 c4 c5 c6 c7 c8 b1 b2 b3 b4 b5 b6 b7 b8 IMs a1 a2 a3 a4 a5 a6 a7 a8 Output ports  of the PEs Figure 1.  Configuration of a 64-node 3-stage CNOC. All links are unidirectional, and packets can only travel upward. All routers are of radix-8.  Figure 2.  Configuration of a 64-node Fat Tree. All links are bi-directional.  Localized traffics are shown as the red lines.  182 In CNOC, each router is implemented as an input-queued  crossbar switch. CNOC applies wormhole switching, in which  packets are divided into a number of fixed-length flits. Also a  credit-based flow control mechanism is applied in which on  two sides of a link, the upstream entity has to keep track of the  available buffer space in the downstream entity to prevent  buffer overflow. The architecture of the SM can be the same as  the baseline Virtual Channel (VC) router described in [14],  while in this paper we propose a VOQ-based NOC router to  increase throughput and reduce average latency.  To route a packet in CNOC with the destination address  decided, there are multiple choices of different paths, each  corresponding to a CM. In this paper, we forward the packets  in a round-robin manner for the ease of implementation and its  good load-balancing feature.  IV. PDRRM VOQ ROUTER DESIGN  The input buffer structure and switching allocation scheme  are two main factors that affect NOC router performance.   To resolve the well-known Head of Line (HOL) blocking  problem caused by single FIFO structure in the input-queued  routers, [13] first proposed the concept of VC, which improves  performance by decoupling the physical channels from the  buffer resource. Today, the VC structure has become the  dominant input buffer structure in NOC for its simplicity and  efficiency in improving the throughput and avoiding deadlock.   The choice of Switch Allocation (SA) schemes in a VC  router is the other factor that greatly affect the router  throughput. Different allocation schemes, such as inputfirst/output-first separable allocator, wavefront allocator, are  proposed to increase the matching quality in a VC router.  However, without any speedup in the switch or multiple  iterations in SA, none of these methods can give satisfying  throughput with the conventional single crossbar, input-queued  VC router design. According to our simulation results, the  throughput of an 8 ൈ 8 router with 8 VCs can only achieve  65% throughput, with input-first separable allocator and roundrobin arbiters.   There are sophisticated designs proposed in the past few  years that aim at high throughput router design [15][14][16].  These methods require a  lot more hardware resources  compared to the canonical VC based, input queued, single  crossbar router design with separable allocators. In a high-radix  router the critical path usually resides in the allocator, and the  input-first/output-first separable allocators provide shorter  critical path compared to other allocation schemes.  This is why  [15] chose input-first separable as their allocation scheme. In  this paper, we try to find a solution for the high-radix highthroughput router design that maintains the simplicity of the  input-first separable allocation scheme for the configuration of  a Clos network.  A. VOQ Buffer Structure  The concept of VOQ has been applied to lossless networks  to alleviate HOL blocking problem. In the NOC environment  with wormhole switching, the VOQ structure can be regarded  as a special case of the VC structure. In a VC router, an input  VC can store packets that target on any output port. In a VOQ      router, each VC has an associated output port, and can only  store packets destined to that output port. So ݊ VOQs are  required by each input port in an ݊ ൈ ݊ switch.  A phenomenon, which we termed tail of line (TOL)  blocking effect, hinders the performance of VOQ routers if we  connect the VOQ routers to form a multi-stage network. This  problem appears with packet lengths larger than one flit.   NOC routers apply wormhole routing, which forces flits of  the same packet concatenated as a worm in any logical queue  of the network. In a VOQ router, from an output port’s  perspective, there is one VOQ in each input port that targets at  this output port. If we have an ݊ ൈ ݊ switch, for each output  port there are ݊ corresponding VOQs, each in an input port,  that could potentially make a request to send a flit to this output  port. In other words, there could be at most ݊ packets targeting  the same output port at the same time. If there are two packets  destined for the same output port, and both of them aim at the  same VOQ in the downstream input port, the flits of the two  packets cannot interleave on the output physical channel of the  current router. Therefore, one of the packets has to wait until  the other packet finishes transmitting before it can send out its  first flit. We termed this phenomenon TOL blocking because  the winning packet’s tail flit blocks other packets with the same  next hop VOQ.  TOL blocking reduces the throughput of a multi-stage  network for two reasons. First, the packets which are blocked  by the tail flits of other packets will cause HOL blocking to the  successive packets in their logical queues. Second, if a packet  is broken for some reason, like back pressure caused by credit  based flow control, and spreads in several routers, those  packets which are blocked by the broken packet’s tail flit  cannot utilize the unused physical channels.  Figure 3. shows an example of TOL blocking. There are  two input/output ports in each router, and two VOQs in each  input port. If there are two packets in the upstream router, both  in VOQ2 but different input ports, targeting VOQ1 in the  downstream router2, one of them will block the other until the  tail flit of the winning packet enters the downstream router. In  this example packet2 has to wait until the tail flit of packet1  enters the downstream router. If the routers employ roundrobin arbiters to form separable allocators, packet3 and packet1  will receive grants alternately. When packet3 receive grants,  flits of packet2 cannot take advantage of the idle output  physical link since it is blocked by the tail flit of packet1.  Downstream Router1 IA OA B3B3 IA OA Downstream Router2 B1 B1 B1 IA OA IA OA Upstream Router B3B3 T1 B1 IA OA B2B2 H2 IA OA VOQ1 VOQ2 VOQ1 VOQ2 IA: input arbiter OA: output arbiter Figure 3.  TOL blocking example.  B. PDRRM Algorithm  To alleviate the TOL blocking involved in the multi-stage  network where VOQ routers are used, we propose a scheduling  algorithm, called PDRRM. Besides the property of alleviating  TOL blocking, we hope PDRRM also has the properties of  sustaining high speed operation and  providing good matching  quality with one iteration of arbitration in each clock cycle.   To meet the above requirements, we design the algorithm  based on Dual Round-Robin Matching (DRRM) [17], which is  similar to the input-first separable allocation scheme in VC  routers for its shorter critical path. Also we take the concept of  SA+VA from [14] for reducing the pipeline stages of the router  and illuminating the need for Virtual channel Allocation (VA).  In the SA+VA scheme, VA and SA are done in the same clock  cycle, but VCs are assigned non-speculatively after SA. VA  simply involves finding a free VC for each output port from a  pool of free VCs every clock cycle and assigning it to the flits  that win SA. To alleviate the TOL blocking problem, we  change the original DRRM algorithm into packet mode,  meaning that the pointers of the input arbiters (IAs) and output  arbiters (OAs) in a separable allocator are updated after a  packet has been transferred to the next hop, unless there is no  buffer space in the input port of downstream router or the flits  in the winning VOQ drain out.  In the description of the algorithm, the flit types are  abbreviated as follows: Single flit (S), Head flit (H), Body flit  (B), and Tail flit (T). A VOQ is called an eligible if it contains  a packet that is destined for an available downstream VOQ  with free buffer space.  Step 1: Request. Each  IA sends an output  request  corresponding to the first eligible nonempty VOQ in a fixed  round-robin order, starting from the current position of the  pointer in Step 2. The pointer of the IA is advanced by one  location beyond its current position if (1) its request is not  granted in Step 2, or (2) the request is granted but after one flit  is served, this VOQ becomes empty, or (3) the granted flit is a  T or S. Otherwise, the pointer remains unchanged.  Step 2: Grant. If an OA receives one or more requests, it  chooses the one that appears next in a fixed round-robin  schedule starting from the current position of the pointer. The  output notifies each requesting input whether or not its request  was granted. The pointer of the OA is incremented to one  location beyond the granted input if an S or T is granted. The  pointer of the OA remains at the granted input if an H or B is  granted. The pointer of the OA remains unchanged if there is  no request.   VOQ PDRRM routers achieve high performance  in  multistage network for three reasons. First, VOQ eradicates the  HOL blocking problem in an input-queued switch. On the other  hand, VC can only reduce the frequency of the occurrence of  HOL blocking. Second, by applying the packet mode switching  concept the TOL blocking problem is effectively alleviated,  since the scheduler tends not to interleave flits of different  packets on the output physical links. Third, packet mode  scheduling allows the existing matching between input/output  pairs to last for a longer time compared to flit mode scheduling,  and the unmatched input ports can gradually augment the  183   matching pattern by sending requests to the available output  ports. This is somewhat similar to the concept of multiple  iterations employed in DRRM and iSLIP [18].   C. PDRRM VOQ Router Pipeline  Figure 4. shows our proposed VOQ router pipeline, which  consists of 3 stages: (1) Switch+VOQ Allocation (SA+VA),  Buffer Write (BW), (2) Buffer Read (BR), (3) Switch Traversal  (ST). In CNOC Routing computation (RC) is done in the PEs  and the RC information for each packet is attached in the head  flits. Hence there is no RC or next hop routing computation  (NRC) in the pipeline stages.   Figure 5.  Two different NOC layout styles.  B. Wiring density  In evaluating an NOC floor plan, despite its power  characteristics, we also need to consider its feasibility in terms  of wiring density. According to [9], wiring density is the  maximum number of tile-to-tile wires routable across a tile  edge. In the scenario of routing over logic, this means that there  are some parts of the tiles that are not routable for global  interconnects, like the regions occupied by core logics and first  level caches of the PEs. In [9], it is assumed that 40-60% of the  length of a tile edge is routable for global interconnects. In this  paper, we assume the dimensions of the tiles are 1.5mm by  1.5mm. Hence, the total wire width on a tile edge cannot  exceed 0.6-0.9 mm. In this paper, we assume the worst case, so  only 0.6mm of a tile edge is routable for CNOC. Also, we  assume the wire width to be 400nm for the global interconnect.  Hence, each tile edge can accommodate 1500 wires. In other  words, each tile edge can accommodate 1500/W links for the  NOC, where W is the bus width.   184   C. Problem Definition  We are given a set of ܰ ൈ ܰ identical tiles ܶ , a set of  routers ܴ ൌ ሼܴ௜ , ݅ ൌ 1,2,3 … ݊ሽ, and a set of links ܮ ൌ ሼܮ௜ , ݅ ൌ 1,2,3 … ݉ሽ that connect ܶ and ܴ to form a 3-stage Clos  network. The positions of the tiles are fixed, while the positions  of the routers are variable. A router can be located in any  intersection of the tile edges. Once an intersection has been  taken, it is not available to other routers. Before describing the  objective function, we first define ܯሺܮ௜ ሻ as the Manhattan  distance between the two nodes that ܮ௜ connects, and ܧ௞ is the  energy that a flit consumes when it travels through a link with  Manhattan distance ݇ . The Manhattan distance between a  router and a tile is defined as the Manhattan distance between  the two closest points of the tile and router. The floor planning  problem is to determine the location of each router, so that the  objective function Φሺܴ , ܮሻ ൌ ∑ ܧெሺ௅೔ሻ  is minimized, the  maximum wire length is within the boundary to achieve the  operating frequency without pipelining, and at the same time  the wiring density constraint is met. The physical meaning of  Φሺܴ , ܮሻ is the sum of energy consumption for all links to  transfer one flit, given a placement of the routers. In CNOC,  under uniform traffic load, every link has the same probability  to transfer a flit if round-robin or random-routing algorithms  are applied. Hence Φሺܴ , ܮሻ can be regarded as an indicator of  total power consumption of CNOC under uniform traffic.  Figure 6. shows an example for placing two routers on a chip  with 3 ൈ 2 tiles. The Manhattan distance between router “A”  and “B” is 2. The Manhattan distance between router “A” and  tile “k” is 3, calculating from “A” to the upper left corner of tile  “k”.  ௠ ௜ୀଵ plan. One thing to notice is that there could be multiple  solutions for this problem. For example in Figure 7. router o1,  o3, o5, o7 may be moved to the central points of the their own  quadrants without affecting the power performance and still  under the constraint of wiring density.  o7 7 7 7 7 6 6 5 5 7 7 7 7 6 6 5 5 i7 i6 o6 8 8 8 8 6 6 5 5 o8 c7 c6 c5 i5 o5 8 8 8 8 6 6 5 5 i8 c8 c4 i4 1 1 2 2 4 4 4 4 o1 i1 c1 c2 c3 o4 1 1 2 2 4 4 4 4 o2 i2 i3 1 1 2 2 3 3 3 3 1 1 2 2 3 3 3 3 o3 Figure 7.  Floor plan of the 64-node 3-stage CNOC.  VI. EXPERIMENTAL RESULTS  In this section, we compare the performance of the VOQ  PDRRM router with the VC router, and compare the CNOC  with other high-radix NOC topologies.   A. VOQ PDRRM Single Router Performance  Methodology: We used our  in-house cycle-accurate  simulator to evaluate delay performance for the single router  with different schemes. In the baseline VC router, there are  eight VCs in an input port. The memory in each input port is  shared by all of its eight VCs to increase the buffer utilization  and implemented as linked lists. The VC router follows the  design in [14], by performing VA and SA in the same clock  cycle. VCs are assigned non-speculatively after SA, which is  an input-first allocator, so that VA simply involves finding a  free VC for each output port from a pool of free VCs every  clock cycle and assigning it to the flits that win SA. For the  VOQ router, we apply two different scheduling schemes –  DRRM and PDRRM. In the VOQ routers each input port has  eight VOQs and the input port memory is shared by the eight  VOQs.  Figure 8. shows the delay performance results of the VOQ  and VC routers with packet lengths equal to eight and input  port memory size equal to 256 flits under uniform traffic.  For  the VC routers, we implemented two kinds of scheduling  schemes – Parallel Iterative Matching (PIM) and DRRM. Both  PIM and DRRM are input-first, which means that an input port  first chooses an input VC, either randomly (PIM) or according  to round-robin order (DRRM), and then an output port chooses  an input port to be granted, either randomly (PIM) or according  to round-robin order (DRRM). An 8 ൈ 8 single VC router with  1-iteration PIM can only achieve about 65% throughput. The  DRRM has a similar throughput as the PIM scheme.   In the VOQ schemes, we evaluated three kinds of  scheduling algorithms: DRRM, PDRRM I, and PDRRM II.  PDRRM I follows the algorithm described in section IV, and  PDRRM II has some differences. In PDRRM II, the input  pointers is “stubborn”, meaning that when a local winner in an  input port loses arbitration in the OAs, the input pointer does  not update until it finally gets granted. In this way the property  of fairness in DRRM is well preserved. As illustrated in Figure  8. , both DRRM and PDRRM II can achieve 100% throughput,  while PDRRM I can achieve 81%. DRRM can achieve 100%  throughput under uniform  traffic because of  the desynchronization effect  [19]. PDRRM  II preserves  the  stubbornness of DRRM in the IAs to maintain the fairness  between different VOQs in the same input port, so it can also  achieve 100% throughput under uniform traffic. However,  from the simulation results we learned that PDRRM II has a  much higher average latency in the high-load compared to  PDRRM I, which is not tolerable in on-chip environment.  Hence throughout this paper we adopt PDRRM I as the  scheduling algorithm in CNOC. Though the throughput of  PDRRM I cannot achieve 100%, we show that it can still  maintain 78% throughput under multi-stage configuration,  unlike DRRM dropping to 71%, which is prone to suffer from  the TOL blocking problem.   Figure 8.  Latency vs. injection rate for different scheduling schemes in VC  and VOQ routers, with packet length equal to eight.  Figure 9.  Latency vs. injection rate in CNOC with different router designs.  B. VOQ CNOC VS. VC CNOC   Figure 9. shows the delay performance for a 64-node 3stage CNOC with different router designs, each with an input  port memory size of 256 flits. With the VC router and DRRM  scheduling scheme, the throughput is 62%. As we expected, the  throughput of VOQ with the DRRM scheduling scheme drops  186       to 71% because of the TOL blocking issue. The PDRRM VOQ  scheme can achieve 78% throughput, which gives the best  performance.  C. Latency Performance Comparison between CNOC and  Other NOC Topologies  In this sub-section, we compare the performance of CNOC  with other NOC topologies, including the conventional 2 Mesh and  two other high-radix networks:  (Generalized Fat Tree) [3], and CMeshX2 [4]. The reason why  we chose GFT(2,4,4) and CMeshX2 is that both topologies  consist of routers with at most 8 input/output ports Generalized Fat Tree: The GFT configuration is described  in [3]. GFT(p,q,r) means that there are p stages of connections  between different stages of the routers. Each router has  upward connections and r downward connections.  shows the configuration of a GFT(2, 4, 4), where there are 32  radix-8 routers and 16 radix-4 routers. All the links in the Fat  Tree are bi-directional, which means that a packet only needs  to travel upward to the lowest common ancestor router of its  source and destination PE before travelling downward. routing algorithm that we use in Fat Tree is oblivious routing,  in which a packet travels to one of the random lowest common  ancestor routers, and then travels down to the destination along  the only route.  Concentrated Mesh: In CMesh, every router is connected  to four PEs, and routers form a Mesh network as the  conventional 2D Mesh network. CMeshX2, where there are  two independent CMesh networks (32 radix-8 routers in a 64 node system), has better power-efficiency and twice the bi section bandwidth compared to CMesh. So we selected  CMeshX2 with XY routing as one of the high to evaluate. We actually tried CMeshX2 with express channels,  but the throughput does not change too much under uniform  traffic with express-channel-prioritized XY routing.  in this paper we only evaluate CMeshX2.  Figure 10.  Latency vs. injection rate for different topologies with uniform  traffic, packet length = 8.  Figure 10. illustrates the delay performance re CNOC with PDRRM, and other three topologies traffic. The packet size equals eight. For the Fat Tree applied two different router designs, VC DRRM and VOQ  PDRRM. For CNOC, we set the input port memory size to be  end-to-end latency below a certain level, which is cycles in our simulation scheme. The power efficiency is  higher if a network spends less time to finish the traffic load,  and at the same time spends less energy during th other words, the more packets a network can transmit in  spending a joule, the more power efficient it is.  Figure 11. shows the normalized power efficiency five evaluated schemes. Clearly CNOC has  the best performance, followed by the other two high topologies. The 2D Mesh has the worst power efficiency  because of its large average hop counts and low throughput. The reason why CNOC outperforms Fat Tree with the same  high-throughput router is because CNOC has  Thus, the average cost in CNOC to transfer a packet is less Fat Tree, since fewer routers are involved in the operation. Figure 11.  Normalized network power efficiency for four topologies VII. CONCLUSION  Low-radix NOC architectures do not scale well with the  increasing number of PEs in the CMP. The diameter of the  network rises rapidly with the growing number of tiles, making  the end-to-end communication latency intolerable.  radix NOC topologies have been proposed to improve performance. Among these we believe the Clos Network Chip (CNOC) is the most promising with its low average hop  counts and good load-balancing characteristic.  propose (1) PDRRM VOQ router design to achieve  and high throughput in CNOC, (2) a heuristic  algorithm to minimize the power consumption caused by the  long wires. Experimental results show that the throughput of a  64-node 3-stage CNOC under uniform traffic 62% to 78% by replacing the baseline Virtual Cha routers with PDRRM VOQ routers. We also compared CNOC  with other high-radix NOC topologies under the same router  radix upper bound, and found that under the new design  techniques CNOC has the highest throughput, lowest zero latency, and best power efficiency.  ACKNOWLEDGMENT  This work was supported by a Polytechnic Institute of NYU  Angel Fund grant and by U.S. Army CERDEC. would also like to thank Dr. N. Sertac Artan and Dr. Yang Xu  for their precious comments, Chao Zeng and  their help in simulations.    "
A Network Congestion-Aware Memory Controller.,"Network-on-chip and memory controller become correlated with each other in case of high network congestion since the network port of memory controller can be blocked due to the (back-propagated) network congestion. We call such a problem network congestion-induced memory blocking. In order to resolve the problem, we present a novel idea of network congestion-aware memory controller. Based on the global information of network congestion, the memory controller performs (1) congestion-aware memory access scheduling and (2) congestion-aware network entry control of read data. The experimental results obtained from a 5×5 tile architecture show that the proposed memory controller presents up to 18.9% improvement in memory utilization.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip A Network Congestion-Aware Memory Controller   Dongki Kim, Sungjoo Yoo, Sunggu Lee  Embedded System Architecture Laboratory  Department of Electronic and Electrical Engineering  Pohang University of Science and Technology (POSTECH)  {dongki.kim, sungjoo.yoo, slee}@postech.ac.kr  ABSTRACT  Network-on-chip and memory controller become correlated  with each other in case of high network congestion since the  network port of memory controller can be blocked due to the  (back-propagated) network congestion. We call such a  problem network congestion-induced memory blocking. In  order to resolve the problem, we present a novel idea of  network congestion-aware memory controller. Based on the  global information of network congestion, the memory  controller performs (1) congestion-aware memory access  scheduling and (2) congestion-aware network entry control  of read data. The experimental results obtained from a 5x5  tile architecture show that the proposed memory controller  presents up to 18.9% improvement in memory utilization.  1.  Introduction  In many-cores, the performance of memory and network can  often determine the system performance. In order to increase  memory performance, multiple memory modules (in short,  multiple memories) start to be used [1][2][3][4][5]. In  addition, 3D die stacking is expected to make their usage  popular [6][7][8][9]. For the performance improvement of  individual memory, there have been presented several studies  on memory  access  scheduling  [10][11][12][13][14].  Network-on-chip (NoC) becomes more and more popular in  many-cores [2][3][8]. As the NoC size increases, network  congestion becomes one of the most important problems  limiting NoC performance thereby system performance.  Several methods, e.g., adaptive routing [15][16][17] have  been presented in order to address the network congestion  problem.     In most of previous work, memory and network have been  assumed to be independent of each other. However, in case  of heavy network congestion, both memory and network  become correlated with each other, in a negative way, to  yield system performance degradation. In this paper, we  tackle a problem due to such a correlation which is called  network congestion-induced memory blocking.  In  the  problem, the network congestion propagates from hot spots  up to the memory controller. It prevents the memory  controller from servicing productive requests 1 and, finally,  degrades memory performance thereby system performance.   In order to resolve the problem, in this paper, we propose a  novel  idea called network congestion-aware memory  controller. In the proposed method, first, network congestion  information is propagated to the memory controller. Then,  the memory controller takes two measures of network  congestion-aware memory access scheduling and network  entry control in order to prioritize productive requests  (requests from the uncongested area) over unproductive ones  (requests from the congested area which will only consume  resource in the memory controller and/or NoC without  contributing to system performance improvement). We  evaluate the proposed idea with a tile-based many-core  architecture.  This paper is organized as follows. Section 2 reviews  related work. Section 3 gives preliminaries. Section 4  explains our motivation. Section 5 presents the proposed  method of network congestion-aware memory controller.  Section 6 reports experimental results. Section 7 concludes  the paper.  2.  Related Work  Rixner, et al. present memory access scheduling policies  including first ready-first come first serve (FR-FCFS) and  open/close page schemes [10]. Heithecker and Ernst utilize a  traffic shaping in assigning memory resource to high priority  requests thereby reducing performance degradation in other  low priority requests [11]. Ahn, et al. present a memory  controller with per-thread request buffers in order to avoid  inter-thread interference while exploiting the intra-thread  locality, e.g., high row buffer hit rate [18]. Akesson, et al.  present a memory controller called Predator where memory  accesses are grouped to enable parallel bank accesses thereby  improving memory utilization and easing QoS support [12].  Mutlu, et al. present a fair memory access scheduling which  balances the slow-down (=the ratio of memory access latency  between shared and alone cases) in memory access latency  among cores when a shared DRAM is accessed by multiple  1 Productive requests are the ones that will improve system  performance if they are served by the memory controller.  978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.36 257                                                                              cores [13]. They also present another fair memory access  scheduling called PAR-BS, which  tries  to avoid  the  starvation of low priority requests by applying a batch  scheduling [14].  There have been presented several solutions to the NoC  congestion problem. Most of existing solutions utilize, as the  congestion metric, the resource utilization of self and  neighbor routers, e.g., buffer availability [15] and queue  length in output port [16][17].  Singh, et al. present a method  called GOAL where the routing decision is based on the  queue length of each output port [16]. They also presents a  method called GAL which utilizes the information of output  queue length for both quadrant selection and routing in the  tori architecture [17]. Gratz, et al. present a notion of  regional congestion awareness (RCA) where a router  determines, on each incoming packet, the output port based  on the global congestion information on each quadrant of  mesh architecture [19]. Our method presented in this paper is  based on the global congestion information used in the RCA  method. In our case, we exploit the global congestion  information for memory access scheduling and network entry  control. Our method can be combined with the RCA method,  which is beyond the scope of this paper.  There has been little work on the integrated and cooperative  solution which includes both memory controller and network  (congestion avoidance). Recently, Jang and Pan present a  memory-aware NoC router [20]. In this work, the router  performs switch allocation, i.e., output port arbitration  considering  the memory access  latency of packets  contending for the same output port. The router performs  memory access scheduling which was previously executed  by the memory controller. However, network congestion  information is not utilized in this work.  In our work, the contribution is to identify the problem of  network congestion-induced memory blocking and  to  integrate network congestion awareness into the memory  controller in order to resolve the problem.  3.  Preliminaries: DRAM, Memory Access  Scheduling and Memory Controller  Figure 1 illustrates an internal structure of DRAM. As shown  in the figure, the memory array is divided into banks  (typically 4 or 8 banks per DRAM). Each bank, which can be  accessed independently of the other banks, is a two  dimensional array of memory cells consisting of rows and  columns. Figure 1 illustrates a four-bank DRAM. The  DRAM access typically takes three steps. In step 1, the bank  and row are selected by a bank/row address and a set of  control signals including RAS (row address strobe) as the  arrow denoted with ‘(1)’ shows in Figure 1. The selected row  (typically, 2KB data) is fetched to the row buffer (consisting  of sense amplifiers and latches), which incurs a latency of  tRCD. This operation is often called row activation and it is  said that the row is open when it is fetched to the row buffer.  In step 2, the desired data (a dark rectangle in Figure 1) is  258 accessed (read or written) by a column address and a set of  control signals including CAS (column address strobe) as the  arrow denoted with ‘(2)’ shows in the figure. The accessed  data flows over external IO signals, DQs after the latency of  column access called CAS latency (CL) in case of read  operation. In step 3, after finishing accesses to the open row  in the row buffer, the row is closed by the memory controller  issuing a memory command called precharge (PRE) to  DRAM as the arrow denoted with ‘(3)’ shows in the figure.  The precharge command incurs a latency of tRP. Another row  from the same bank can be activated only after the previously  open row is precharged.  Accessed data (1) Row Decoder Row address RAS Bank0 Memory Array tRCD (3) tRP Row Buffer DQ (2) CL Column Decoder Column add ress CAS Figure 1 DRAM structure and access flow  Memory access scheduling  tries  to  improve memory  performance considering the fact that multiple banks can be  accessed independently (as far as there is no conflict in the  address/control/data buses) and an access to the open row  requires a minimum access latency. Figure 2 illustrates a  memory access scheduling policy called FR-FCFS (first  ready-first come first serve) [10] where the oldest ready  request is served by the memory controller. We assume a  DDR SDRAM whose CL, tRP, and tRCD are all 3 cycles and  BL (burst length) is 8.   Bank 0 Bank 2 tRCD CL tRCD CL tRP tRP Data Out B0R0C0 tRCD CL tRP Data Out B2R2C0 Data Out B0R1C1 Cyc le 0 2 4 5 10 15 20 B0R0C0 B2R2C8 B0R1C8 Figure 2 FR-FCFS policy in memory access scheduling              Assume that three read requests arrive at the memory  controller at time 0, 2, and 4 as shown by the three dashed  arrows at the bottom of Figure 2. Each request is represented  by a tuple, “bank ID, row address, column address”. The first  request accesses bank 0 (1st bank), row 0 (1s t row in bank 0)  and column 0 (1s t data in row 0), the second one bank 0, row  1, and column 8, and the third one bank 2, row 2, and column  8. At time 0, the row activation command for the first request,  B0R0C0 is sent to DRAM, which takes 3 cycles (tRCD) to  open the row.2 At time 3, the column access command is sent  to access the open row. After the delay of CL, i.e., 3 cycles,  at time 6, the data of the first request come out of the DRAM.  At time 7, a precharge command is sent to close the open  row in order to prepare the access to another row, row 1 in  the same bank. Note that the precharge command can overlap  with the read data operation.3  When serving the requests in a FCFS manner, the third  request can start to be served only after the service of the  second request is finished, which will take a large delay in  our example. FR-FCFS can give a reduced latency as Figure  2 shows. Even though the third request arrives at the memory  controller later than the second request, it is served earlier  than the second one since it is ready, i.e., the memory  command of row activation for bank 2 can be issued at the  time when the third request arrives. As Figure 2 shows, the  row activation of the third request is issued at time 4 to open  the row 2 at bank 2. The data of the third request start to  come out of the DRAM at time 10. Thus, the service of all  the three requests finishes at time 20 in the FR-FCFS policy.  If the FCFS is applied, the total cycle will be 30 since all the  requests are served in a sequential manner.  Memory access scheduling is usually performed by the  memory controller. Figure 3 shows a simplified block  diagram of memory controller. It is connected with network  interface (NI) and DRAM as shown in the figure. It takes  memory access requests from NI, e.g., via read/write address  channels in the case of AXI protocol [21]. The requests are  stored in the request buffer (RB). The memory access  scheduler checks the status of each bank with bank FSMs  and the requests in the RB. The scheduler applies a  scheduling algorithm, e.g., FR-FCFS [10], PAR-BS [14], etc.,  in order to select a request from the RB. Then, it generates  corresponding DRAM commands for row activation, column  access for read or write, precharge, etc. The generated  DRAM commands are usually stored in the command buffer  and issued to DRAM at their scheduled cycles.  2 In this example, for simplicity, we assume that the memory  controller does not consume internal delay cycles. In our  experiments, the memory controller takes internal latency cycles  from the reception of request from the network to the issue of  DRAM commands.   3 The overlap period between read and precharge operations  depends on BL, tRTP (read-to-precharge latency) and AL (additive  latency). For more details, refer to the DRAM specification, e.g.,  [24].  259 In case of write request, the data buffer (DB) receives write  data from NI, e.g., via write data channel in AXI protocol,  and sends them to DRAM when the corresponding column  access commands are issued to DRAM. In case of read  request, the DB receives data from DRAM and then sends  them to NI, e.g., via read data channel in AXI protocol.  Bank FSM Open ACT PRE Close Request Buffer B0R0C0 B0R1C1 B2R2C3 Memory  Access Scheduler Command Buffer ACT B0R0 Network Interface Address channels DRAM Address RAS, CAS DQ Data channels Data Buffer Figure 3 A simplified block diagram of memory  controller  4.  Motivation  Figure 4 illustrates the problem of network congestioninduced memory blocking. Figure 4 (a) shows a 3x3 tile  architecture where the memory controller (MC) is located at  the center tile. We assume that there is a network congestion.  The shaded routers (small shaded rectangles denoted with  ‘R’) and thick links between them represent the congested  area.  In Figure 4 (a), the congestion is mostly on the third  column of the mesh topology and affects up to the MC  through the router connected to the MC.  PE 0 PE 3 PE 6 PE 1 MC PE 7 R R R R R R PE 2 PE 5 PE 8 R R R MC RB R_PE0 R_PE3 DB D_PE8 D_PE2 NI R (a) 3x3 tile architecture (b) Blocking in MC and NI  Figure 4 Network congestion-induced memory blocking  Figure 4 (b) shows a detailed view of MC and connected  router corresponding to the dashed area in Figure 4 (a). In                                                                                   Figure 4 (b), the MC has two read requests from PE0 and  PE3 (R_PE0 and R_PE3 in RB). The MC has also two data  bound for PE8 and PE2 (D_PE8 and D_PE2 in DB). In this  case, since the paths from the MC to PE8 and PE2 are  congested, and the output port of MC is blocked by the  congestion, the two data cannot enter the NoC. Thus, they  just wait, until the congestion is removed, while occupying  the precious resource, DB. Since the DB is full and there is  no available space to store data coming from the DRAM, the  two read requests in RB cannot issue their memory  commands for read operations. Thus, no memory command  is issued to the DRAM and the DRAM remains idle until the  network congestion is removed in this case. We call such a  case network congestion-induced memory blocking. The  problem is resolved only when the congestion is removed (or  mitigated). As will be reported in Section 6, such a problem  can cause a significant degradation in memory utilization  thereby system performance degradation.  From the viewpoint of memory controller, the above problem  is caused by the fact that conventional memory access  scheduling does not take network status into account. In the  case of Figure 4, the memory access scheduling produces  data D_PE8 and D_PE2 in order to maximize only the  memory performance,  i.e., memory utilization without  considering system performance. The lack of knowledge on  network status in memory access scheduling causes the DB  to be occupied only by the blocked data bound for the  congested area.    The above problem can be resolved by a network congestionaware memory controller. To be specific, the MC takes two  measures in case of network congestion: (1) prioritization of  requests from the uncongested area and (2) congestion-aware  network-entry control of read data. The rationale behind the  two measures is as follows. At high congestion, as shown in  Figure 4 (b), the MC resource (especially, DB and the output  port) tends to be occupied by the requests from the congested  area. Thus, in order to prevent those requests from occupying  the MC resource, requests from the uncongested area need to  be prioritized. Regarding the network entry control, read data  bound for the congested area, when they enter the NoC, will  aggravate the NoC congestion and can cause the blocking  problem. Thus, in this case, read data bound for the  uncongested area are prioritized in entering the NoC. By  doing that, the memory controller can reclaim data buffer  space more quickly and support more requests with the  reclaimed resource.  5.  Network Congestion Awareness in  Memory Controller  In order to realize the idea of network congestion-aware  memory controller, the congestion information needs to be  transferred from the NoC to the MC (Section 5.1). Then,  both memory access scheduling and network entry control  need to exploit the congestion information (Section 5.2).   R R R R R R R R R NW (Quadrant 2) R SW (Quadrant 3) R R R R R R R R R R R NE (Quadrant 1) R SE (Quadrant 4) R R R R R R R R R R R R R R Figure 5 Regions (quadrants) to manage congestion information  5.1 Congestion Information Management  Congestion information is collected within the NoC and  transferred to the MC. As the congestion information, we  utilize the global congestion information called regional  congestion in [19]. The information of regional congestion  represents how much congested each quadrant of a router is.  Figure 5 illustrates the regional congestion information. The  router (shaded rectangle) at (3,3) has the congestion  information for each of the four quadrants (north-west/east,  and south-west/east) seen from its location.   12 12 4 15 15 17 17 18.25 R R R R R R 6 7 7.5 6 7 R R R 19.5 24.25 4 4 4 Figure 6 Propagation of regional congestion information  Figure 6 illustrates how each router manages the regional  congestion information. In order to quantify the congestion  level, we use the number of occupied VCs (vc) and crossbar  demand (xb, the number of contending packets bound for the  same output port) per router.4 The higher the number of vc  and xb is, the higher congestion the network suffers from in  the corresponding quadrant. As dashed arrows represent in  Figure 6, each router collects the congestion information  4 In [19], three possibilities of congestion information, i.e., the  number of occupied VCs (vc) / buffers (bf) and crossbar demand  (xb) are presented. The combination of vc and xb proves to give  the best results.  260                                                                                    from neighbor routers via a set of sideband signals. 5 It  calculates its congestion level by a weighted sum of its local  congestion information (vc and xb) and those of its neighbors  collected via the sideband signals, and propagates the new  congestion information to its neighbor routers. Note that the  congestion information propagates from a router to all its  neighbors, i.e., in four directions in the mesh architecture.  For simplicity, Figure 6 shows only the south-west-bound  flow of congestion information.6   The congestion level of each quadrant around the MC is  propagated to the MC as the dashed line between the router  and the MC shows in Figure 7.   MC RB DB NI Congestion info R R R Figure 7 Propagating congestion information to memory  controller  5.2 Network Congestion-Aware Memory Access  Scheduling and Network Entry Control  We integrate congestion awareness into two functions of  memory controller: memory access scheduling and network  entry control of read data.   Figure 8 shows the pseudo code of network congestionaware memory access scheduling. We use FR-FCFS as the  baseline policy of memory access scheduling. Note that there  is no limitation to memory access scheduling policies in  integrating congestion awareness.   5 The congestion information is carried on 9 bits of sideband signals  in our work. However, the overhead of sideband signals can  become negligible since they can be reduced down to a serial link  without losing the benefit of adaptive routing as reported in [19].  A detailed analysis on the effect of reducing the number of  sideband signals will be our future work.  6 In adaptive routing which utilizes the regional congestion  information, each router selects a link with the lowest congestion  level. For instance, in the case of the (shaded) router at (1,1) in  Figure 6, when a north-east bound packet arrives at the router, it  selects the east-bound output port as the target output port since  the congestion level of north-bound output port, 24.25 is higher  than that of east-bound output port, 18.25. For more details of  regional congestion awareness, refer to [19].  261 1   // CQ and UCQ: congested and uncongested quadrants 2 // R_CQ and R_UCQ: requests from CQ and UCQ 3 if (high congestion in any quadrant) 4 R_UCQ = All_Requests – R_CQ 5      selected = FR_FCFS(R_UCQ) 6      if selected == nul l 7         selected = FR_FCFS(R_CQ) 8 else // no heavy congestion 9      selected = FR_FCFS(All_Requests) Figure 8 Network congestion-aware memory access scheduling  If any of four quadrants around (the router connected to) the  MC has a high congestion level (line 3), then the requests  from the congested quadrant(s) (CQ) are excluded from the  set of candidate requests in the request buffer (line 4). The  FR-FCFS policy is applied to the new candidate set (R_UCQ  in Figure 8). If there is a selected request, it is served by the  MC (line 5). If there is no selected one, then a candidate is  selected, if any, from the requests from the congested  quadrant (R_CQ) (lines 6-7). If there is no quadrant with  high congestion, then the original FR-FCFS is applied to all  requests in the request buffer.  The network entry control is applied to read data stored in  the data buffer of memory controller. The procedure to select  a data (to be injected to the NoC) from the data buffer is  similar to that in Figure 8. First, if there is any quadrant with  high congestion, then the read data bound for the quadrant  are excluded from the set of ready read data in the data buffer.  If there is any data bound for the uncongested area, then the  oldest of such data is selected and sent to its destination. If  there is no data bound for the uncongested area, then the  oldest ready data bound for the congested area is selected.  Note that the granularity of selection is a burst of data. In our  experiments, a burst of data corresponds to 8 words of 64b  data for L1 cache miss.   6.  Experiments  6.1 Experimental Setup  Figure 9 shows a 5x5 tile architecture used in our  experiments. The 5x5 architecture has one MC at the center  tile. The MC runs at 400MHz and is connected to a  conventional DRAM memory channel of DDR2-800 (CLtRP-tRCD=6-6-6). The MC has a request buffer and a data  buffer whose sizes are varied in our experiments. The MC  supports AXI protocol [21] at its bus interface. Thus, we use  a network interface between the router and the MC. We use a  Tensilica LX2 core (64b, 16K/16K, cache line size of 64B,  400MHz) as the PE.   The NoC router supports four-stage pipeline (input buffering,  virtual channel allocation & lookahead routing computation,  switch allocation and switch traversal), 4 virtual channels  (VCs) per input port, wormhole switching, speculative  allocation [22], XY routing and 64b flits. There are two types  of packets: request and response packets. The read request                                                                                       packet consists of one header flit while the write request  packet 9 flits (one header flit and 8 body flits for write data).  The read response packet has 9 flits (one header flit and 8  flits for read data) while the write response has one header  flit. PEs, NoC, and MC have the same operating frequency.7  vortex ammp mcf mcf art art mcf art ammp vortex art mcf MC ammp vortex ammp vortex vortex art mcf mcf art ammp vortex ammp Figure 9 A 5x5 mesh architecture  We  run  the cycle-accurate simulation of  the entire  architecture consisting of PEs, NoC, and MC (including  DRAM). We developed SystemC models of NoC and MC  and use the cycle-accurate simulation model of LX2 core.8  We utilize programs from SPEC2000 benchmarks and map  them on the PEs. We use two sets of program mapping on  PEs: single program and multi-program mappings. The  single program mapping runs different phases [23] of one  memory-intensive program, mcf on the 24 PEs while the  multi-program mapping utilizes four memory-intensive  programs (vortex, ammp, mcf, and art). Figure 9 shows the  mapping of multiple programs onto PEs. We run different  phases of four programs on PEs.   The in-order cores, i.e., Tensilica LX2 cores used in our  experiments do not generate enough memory accesses to  cause high network congestion. However, in reality, there  can be heavy traffic, especially, between cores (for cache line  copies and cache coherence traffics) at hot spots [19]. Thus,  we modified the example in two ways. First, we emulate the  7 Since a four-stage pipeline is used in our NoC router, a higher  frequency setting, e.g., 2GHz NoC, can reduce the impact of NoC  latency, and increase the relative portion of memory access  latency in total cache miss latency. However, the proposed  method still gives performance improvements even with the  relatively low frequency setting on NoC in our experiments.  8 For the reduction in simulation runtime, we used cycle-accurate  L1 miss traces on behalf of cycle-accurate PE models in the  simulation of entire architecture. The usage of cycle-accurate  trace does not hurt simulation accuracy since the LX2 core is an  in-order core.  262 network congestion by overlaying a synthetic traffic pattern  called Tornado [16] over the architecture as shown in Figure  9. The arrows in the figure represent the overlaid Tornado  traffics. In addition, we adjust the flit acceptance rate at each  PE in the congested area (shaded rectangles in the figure) in  order to scale the level of network congestion. Three levels  (low/medium/high) of congestion are controlled by setting  the rate to 1/6, 1/8 and 1/10.9   6.2 Experimental Results  Figure 10 shows the performance comparison between the  baseline and proposed methods for the single program  mapping in the 5x5 architecture. We use, as the baseline  method, the conventional memory controller (explained in  Section 3) which applies FR-FCFS and closed page policy to  memory access scheduling and FIFO operation to the  network entry control of read data without considering  network congestion. As the performance metric, we use  memory utilization represented by the number of requests  (served at the MC) every 100 clock cycles. Thus, the higher  the number is, the better memory performance is achieved. In  order to select the threshold of congestion level for high  congestion (for line 3 in Figure 8), we swept the two  threshold values (one for scheduling and the other for  network entry control) and set both thresholds to 170.10 The  results in Figure 10 are obtained with 18 simulation runs  each of which simulates 4M clock cycles and takes 4 hours  per simulation run on an i7 machine (6GB).  Figure 10 (a) and (b) each show three sets of results for three  different levels of network congestion (low/medium/high).  We also vary the data buffer (DB) size while setting the  request buffer size to 24. The figure shows that as the DB  size increases, the memory utilization of baseline method  increases, which confirms a well-known principle that the  larger buffer, the higher resource utilization. Given the same  DB size, the proposed method offers better memory  utilizations by up to 17.4% (when DB size = 64). Comparing  the cases having a similar performance level, the proposed  method enables reducing the area overhead of data buffer by  more than 67%. For instance, a DB size of 32 in the proposed  method gives a better performance than a DB size of 96 in  the baseline method.  Comparing Figure 10 (a) and (b), the proposed method gives  performance improvement over all the three cases. Especially,  the performance gain is significant in the case of high  network congestion. Figure 11 shows the performance gain  obtained from the proposed method with respect to the  baseline (another view of the same data in Figure 10). As the  figure shows, at a high congestion level, the performance  gain is significant (12.6% ~ 17.4%). However, at the low and  9 The acceptance ratio of 1/6 represents the case that the destination  accepts a flit every six cycles.   10 We use the same parameter settings, e.g., a normalization of 5  bits, as in [19].                                                                                                                                                    medium levels of network congestion, the performance gain  is modest (4.4% ~ 14.1%).  Figure 10 Performance comparison  Figure 11 Performance gains  Figure 12 gives another explanation of high performance  gain of the proposed method. The figure shows the fraction  of time that the data buffer is full. The figure shows that as  the congestion level becomes higher, the data buffer becomes  full more frequently in the baseline method.  It is because, at  high congestion, the read data bound for the congested area  occupy the output port of memory controller as well as the  majority of data buffer resource. That is, network congestioninduced memory blocking occurs as exemplified in Section 4.  Thus, they prevent the other data bound for the uncongested  area from being served. The proposed method reduces the  period of full data buffer by up to 12.1% (high congestion,  DB size = 64), which translates into the performance gain in  Figure 11.  Figure 12 Fraction of time when data buffer is full   (single program case)  Network congestion-aware memory access scheduling and  network entry control can be applied independently of each  other. Figure 13 shows the effect of each method obtained  when each of them is applied independently to the case of  high congestion level. The figure shows that the network  congestion-aware memory access scheduling (‘Schedule’ in  the figure) alone offers up to 16.0% improvement in memory  utilization while the network congestion-aware entry control  (‘Entry’) alone offers a performance improvement by up to  8.1%.   Figure 13 Decomposition of performance impact  Comparing the results in Figures 11 and 13, the effects of  both congestion-aware memory access scheduling and  network entry control are not always additive. In case of  small DB sizes (32 and 64), the effect of network congestionaware memory access scheduling dominates. The effect of  network congestion-aware network entry control becomes  more significant with large DB sizes. It is because large DB  (a) Base line (b) Proposed method # requests/100 cycles # requests/100 cycles High High Medium Medium Low Low DB size DB size 4 .98  5 .23  5 .32  4 .06  4 .18  4 .27  3 .22  3 .35  3 .48  3.00  3.50  4.00  4.50  5.00  5.50  6.00  32 64 96 5 .43  5 .86  5 .92  4 .49  4 .77  4 .86  3 .68  3 .93  3 .92  3.00  3.50  4.00  4.50  5.00  5.50  6.00  32 64 96 Gain (%) DB size Low Medium H igh 0.00  5.00  10.00  15.00  20.00  32 64 96 Fraction of t ime (%) DB size 0.00  10.00  20.00  30.00  40.00  50.00  60.00  70.00  32 64 96 Low-base Low-prop Med-base Med-prop High-base High-prop Low-base High-prop DB size Performance gain 0.00% 2.00% 4.00% 6.00% 8.00% 10.00% 12.00% 14.00% 16.00% 18.00% 32 64 96 Schedule Entry 263                        sizes allow for more space to keep data bound for the  congested area in case of high congestion. Thus, there are  more opportunities that data bound for the uncongested area  bypass data bound for the congested area thereby reducing  memory blocking period and finally improving memory  utilization.  utilization), especially when the network suffers from severe  congestion. As future work, we will work on an adaptive  solution  to cope with dynamically changing network  congestion. We will also work on fairness issues between  requests from congested and uncongested areas.  Medium High Low Gain (%) 20.00  18.00  16.00  14.00  12.00  10.00  8.00  6.00  4.00  2.00  0.00  32 64 DB size 96 Figure 14 Performance gains in the multi-program case  Figure 14 shows the performance gain of the proposed  method for  the multi-program mapping on  the 5x5  architecture as shown in Figure 9. The proposed method  gives a significant performance improvement by up to 18.9%  (high congestion, DB size = 64). Figure 15 confirms that the  reduction in memory blocking period (enabled by the  proposed memory controller) offers such a performance  improvement shown in Figure 14.   Fraction of t ime (%) 70.00  60.00  50.00  40.00  30.00  20.00  10.00  0.00  Low-base Low-prop Med-base Med-prop High-base High-prop 32 64 DB size 96 Figure 15 Fraction of time when data buffer is full   (multi-program case)  7.  Conclusion  In this paper, we explained a performance problem called  network congestion-induced memory blocking and presented,  as a solution to the problem, a memory controller which  performs memory access scheduling and network entry  control of read data in a network congestion-aware manner.  The proposed memory  controller gives  significant  performance improvements (by up to 18.9% in memory  264 8.  "
Comparison of Deadlock Recovery and Avoidance Mechanisms to Approach Message Dependent Deadlocks in On-chip Networks.,"With the transition from buses to on-chip networks in SoCs the problem of deadlocks in on-chip interconnects arises. Deadlocks can be caused by routing cycles in the network, or by message dependencies, even if the network itself is actually free of routing cycles. Two basic approaches to counter message dependent deadlocks exist: deadlock avoidance, which is most popular in NoCs, and deadlock recovery, which has until now only been used in parallel computer networks. Deadlock recovery promises low buffer space requirements and does not impose restrictions on connections between individual communication partners. For this study, we have adapted a deadlock recovery scheme for the use in NoCs and compared it to strict ordering as a representative of deadlock avoidance in terms of throughput and buffer space. The results show significant buffer space savings for deadlock recovery, however, at the cost of reduced data throughput.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip 		  		  	 	 		  +"".#/+(#/&,*/')""+"".#/#.(#./"",.$ ,#.#+,++0%#)*10#'+'% .;9>D?I9>;/D?L;HI?J7;J(K;D9>;D ""DIJ?JKJ;<EH""DJ;=H7J;:-OIJ;CI (KD?9> ;HC7DO Q7D:H;7I B7DA;IJ>EC7I M?B:>;HA;HI:EH<RJKC :; %'7DJ?G;KJI9>B7D: C8! &""D<?D;ED.;9>DEBE=?;I ""DJ;BB;9JK7B+HEF;HJO,;KI; %&(KD?9> ;HC7DO IE;H;D IEDDJ7=B7DJ?G 9EC>;BCKJ H;?D?=?D<?D;ED 9EC    )  )  )'#()$#  '$"" *((  )$  $#% #),$' ( #  $ (  )  %'$!""  $  !$ (  #  $#%  #)'$##)( '((  !$ (  #    *(  .  '$*)#  .!(  #  ) #),$' $'.""((%##(+#)#),$' )(! (  )*!!.  '  $  '$*)#  .!(  ,$  (  %%'$(  )$ $*#)'  ""((  %##)  !$ (  -()  !$  +$#  ,  (  ""$()  %$%*!'  #  $ (  #  !$  '$+'.  ,  (  *#)!  #$,  $#!.  #  *(  #  %'!!! $""%*)'  #),$' (  !$  '$+'.  %'$""((  !$,  *' (%  '&*'""#)(  #  $(  #$)  ""%$(  '()')$#(  $# $##)$#(  ),#  #+*!  $""""*#)$#  %')#'(  $' )(()*.,+%)!$ '$+'.(""$') *(  #  $ (  #  $""%'  )  )$  ()')  $''#  (   '%'(#))+  $  !$  +$#  #  )'""(  $  )'$*%*) #  *'  (%    '(*!)(  ($,  (##)  *'  (% (+#(  $' !$ '$+'. $,+' )  ) $() $ '* ))'$*%*)        	  	   	 	  	 	   	 		 	  ""  "").,*/.""*) .>;IJ;7:?BO?D9H;7I?D=-OIJ;C	ED	>?F-E9ECFB;N	 ?JO  B;7:I  JE  7D  7BIE  H?I?D=  87D:M?:J>  H;GK?H;C;DJI  E<  J>; -EI  9ECCKD?97J?ED  F7HJD;HI   .>;H;<EH;  FEM;H <KB  ?DJ;H 	 9EDD;9JI 7H; H;GK?H;: <EH  J>;I; IOIJ;CI  !?=>  J>HEK=>FKJ  ?I D;;:;:  <EH CKBJ?C;:?7  EH  D;JMEHA  FHE9;II?D=  BEM  B7J;D9O <EH  H;7B	J?C; 7FFB?97J?EDI   ""D 7::?J?ED  JE J>;I; H;GK?H;C;DJI J>;H;7H;7BIEDED	<KD9J?ED7B 9EDIJH7?DJIIK9> 7IBEMFEM;H 9EDIKCFJ?ED7D:IC7BB:?;7H;7?DEH:;HJEH;:K9;9EIJI  );JMEHA  ED  >?F  )E  56 56 56  ?I  7D  ED	9>?F 9ECCKD?97J?ED  F7H7:?=C  IK?J78B;  <EH  B7H=;  7D:  9ECFB;N -EI  )EII97B;L;HOM;BB ?DJ;HCIE<J>HEK=>FKJ 7D:J>KI ;D78B;-EIM?J>7>?=>DKC8;HE<9ECCKD?97J?EDF7HJD;HI  .OF?97BBO  J>;I; F7HJD;HI 7H; I;F7H7J;:  ?DJEC7IJ;HI J>7J 79	 J?L;BO  I;D:  H;GK;IJI  7D:  IB7L;I  J>7J  FHE9;II  ?D9EC?D=  H;	 GK;IJI7D: H;IFED: JEJ>;IF;9?<?9 C7IJ;H  H8?JH7HOC7IJ;HI 7D: IB7L;I 97D 8; 9EDD;9J;:  JE7)E 7I BED= 7I7D 7FFHE 	 FH?7J; );JMEHA ""DJ;H <79; )"" ?I7L7?B78B; J>7J JH7DIB7J;I  J>; C7IJ;HIB7L;FHEJE9EBIJEJ>;)EFHEJE9EB  C7@EH :H7M879AI E<)EI  ?I  J>7J  J>;O7H; IKI9;FJ?8B; JE:;7:BE9AI M>?9> ?I7BIEADEMD  <HEC E<<	9>?F HEKJ;H D;J	 MEHAI   ;7:BE9AI  C7O  H;IKBJ  <HEC  J>;  HEKJ?D=  ?D  J>;  D;J	 MEHA  EH  <HEC  C;II7=;  :;F;D:;D9?;I   ""D  J>;  <EHC;H  97I; 978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.11 17 ,358? 58)0++,4 :;849 8,96549, 8,7;,9 :  225=,+ :;849 ,99(.,+,6,4+,4*? ),:=,,48,7;,9 :(4+ 8,96549,6(*1,:9 * 8,(:,9-58)0++,4:;84 ?=KH;(;II7=;:;F;D:;D9?;I?DD;JMEHAK I?D=23	HEKJ?D= HEKJ?D=9O9B;I7H;?DJHE:K9;:8O?D7FFHEFH?7J;HEKJ?D=9ED	 IJH7?DJIEH7B=EH?J>CI .>;O97D 8;;7I?BO7LE?:;:8O;DIKH 	 ?D=79O9B?9D;JMEHAF7J>I56  !EM;L;HC;II7=;:;F;D:;D9?;I8;JM;;D:?<<;H;DJJOF;I E< C;II7=;I IK9>  7I  IK9> 7I  H;GK;IJ	H;IFEDI; M>?9>  ?I  J>; JOF?97B  97I;  ?D  C;CEHO	9;DJH?9  -EI  97D  FHE:K9;  HEKJ?D= 9O9B;I  ;L;D  ?<  J>;  D;JMEHA  ?JI;B<  ?I  <H;;  E<  9O9B;I  KH?D= EF;H7J?EDE<J>;D;JMEHAJ>;I;9O9B;I97D J>;DB;7:JE:;7:	 BE9AI  ?=KH;I>EMI7C;I>J>7J 79JK7BBOI>EKB:8;<H;;E< HEKJ?D=9O9B;I7I ?J KI;I23HEKJ?D=  .>?IHEKJ?D=7B=EH?J>C 7BBEMI  EDBO 7  IK8I;J  E<  7BB  FEII?8B;  JKHDI  E<  F79A;JI  ?D  J>; HEKJ;HI  ;F;D:;D9?;I  8;JM;;D C;II7=;I >EM;L;H  B;7:  JE J>;9H;7J?EDE<JKHDI J>7J 7H;79JK7BBO<EH8?::;D 8O23HEKJ	 ?D=   .>?I  C7O  B;7:  JE  HEKJ?D=  9O9B;I  M>?9>  97D  H;IKBJ  ?D :;7:BE9AI:KH?D=EF;H7J?ED  ;7:BE9AI 7H;  7 I;L;H ;  ?IIK;  ?D  H;7B	MEHB: 7FFB?97J?EDI >EM;L;H  EDBO  H;9;DJ  FK8B?97J?EDI  7::H;II C;II7=;  :;F;D	 :;D9?;I .>;H;7H;CKBJ?FB;7FFHE79>;IJE9EKDJ;H:;7:BE9AI ?D)EI7D:F7H7BB;B9ECFKJ;HD;JMEHAIM>?9>97D8;97J;	 =EH?P;:  ?DJE  JME  9B7II;I  :;7:BE9A  7LE?:7D9;  7D:  :;7:BE9A H;9EL;HO   ;7:BE9A  7LE?:7D9;  =K7H7DJ;;I  J>7J  7  :;7:BE9A 97DDEJ  E99KH  :KH?D=  EF;H7J?ED   ""D  :;7:BE9A  H;9EL;HO I9>;C;I  7  :;7:BE9A C7O E99KH  :KH?D=  EF;H7J?ED  >EM;L;H J>;IOIJ;C?I78B;JEH;9EL;H7D:H;JKHDJEDEHC7BEF;H7J?ED  ;7:BE9A 7LE?:7D9; 9ED9;FJI 56  B?A; IJH?9J EH:;H?D=EH ;D:	JE	;D:<BEM9EDJHEB J>7J 7H;FEFKB7H?D)EI 9EC;7J7 FH?9;   ""D IJH?9J EH:;H?D=:?<<;H;DJC;II7=;  JOF;I 7H;I;F7H7J	 ;:?DJE:?<<;H;DJF>OI?97B56EHBE=?97BD;JMEHAI56B;7:?D= JE7I?=D?<?97DJ?D9H;7I;E< J>;D;JMEHAI9>?F7H;7 L;DJ>; IEBKJ?ED  JE  H;7B?P;  BE=?97BBO  I;F7H7J;  D;JMEHAI  M>?9>  ?I 9>;7F;H?D9ECF7H?IEDJECKBJ?FB;F>OI?97BD;JMEHAIED ED; 9>?F  ?I  7BH;7:O  L;HO  ;NF;DI?L;  ?D  J;HCI  E<  9>?F  7H;7  .>; H;7IED ?I J>7J J>;BE=?97BI;F7H7J?ED E< J>;:?<<;H;DJC;II7=; JOF;I  ?I 79>?;L;:  8O  L?HJK7B  9>7DD;BI  ? ;   7::?J?ED7B  8K<<;H GK;K;I !EM;L;H J>;?DJ;HD7B8K<<;HIE<7)EHEKJ;H:EC?	 D7J;?JI9>?F7H;7H;GK?H;C;DJI  BEM  9EDJHEB  C;9>7D?ICI 56 56 7IIKH;  J>7J  ;DEK=> 8K<<;H  IF79;  ?I  7L7?B78B;  7J  J>;  H;9;?L;H  I?:;  .>KI  8K<<;H	 IF79;  ?D<EHC7J?ED  CKIJ  8;  C7:;  7L7?B78B;  7J  J>;  I;D:;H  .OF?97BBO  9H;:?J	87I;:  C;9>7D?ICI  7H;  KI;:  M>?9>  >7L; J>H;;C7@EH:H7M879AI56  NJH7 8K<<;H?D= ?I D;;:;:  JE>?:;  J>;  HEKD:	JH?F J?C; E<J>;9H;:?JI   ::?J?ED7B87D:M?:J>?IH;GK?H;:<EH9H;:?J9ECCKD?	 97J?ED   ;:?97J;:  8K<<;HI  7H;  D;;:;:  F;H  9EDD;9J?ED  ?D  J>; D;JMEHA?DJ;H <79;  DEJ>;H  :;7:BE9A  7LE?:7D9;  I9>;C;  ?I  8K<<;H  IF79?D=  ""D8K<<;HIF79?D=J>;I?P;E<J>;H;9;?L;8K<<;HICKIJ8;9>E 	 I;D  ?D  EH:;H  JE  =K7H7DJ;;  J>7J  7BB  F79A;JI  ?D@;9J;:  ?DJE  J>; D;JMEHA8O J>;I;D:;HI97D8; 799;FJ;:7JJ>; H;9;?L;H I?:;  BJ>EK=>  ?J  ?I M?:;BO KI;:  ?D  F7H7BB;B  9ECFKJ;HI  56  ?J  ?I KIK7BBO  DEJ  KI;:  ?D  )EI  :K;  JE  ?JI  ;DEHCEKI  8K<<;H  7D: J>KI9>?F7H;7 H;GK?H;C;DJI  .>;EDBOADEMD )E7H9>?J;9	 JKH;  KI?D=  8K<<;H  I?P?D=  ?I  J>;  C;CEHO  D;JMEHA  E< .?B;H7I .?B;FHE9;IIEH56  I  :;I9H?8;:  78EL;  :;7:BE9A	7LE?:7D9;  IEBKJ?EDI  H;	 GK?H;  7::?J?ED7B 7H;7  B7J;D9O 7D:EH  87D:M?:J>  !;D9; M; M7DJ  JE  ?DL;IJ?=7J;  M> ;J>;H  :;7:BE9A  H;9EL;HO  7  9ED9;FJ ADEMD  <HEC  F7H7BB;B  9ECFKJ;H  D;JMEHAI  >7I  7:L7DJ7=;I 9ECF7H;: JE:;7:BE9A7LE?:7D9;M>;D 7FFB?;: JE)EI I H;FH;I;DJ7J?L;  C;J>E:  <EH  :;7:BE9A  7LE?:7D9;  M;  9>EI; IJH?9J  EH:;H?D=   ;7:BE9A  H;9EL;HO A;;FI  J>;  H;GK?H;:  9>?F 7H;7BEMI?D9;?JD;;:IEDBOED;F>OI?97B9>7DD;B7D:8K<<;H F;H  HEKJ;H  FEHJ  ? ;   DE L?HJK7B  9>7DD;BI  .>;H;  ?I  DE B?C?J7	 J?ED  ED  J>;  I;D:?D=  GKEJ7  7I M?J> ;D:	JE	;D:  <BEM  9EDJHEB I9>;C;I  I?D9;  7BB  9ECCKD?97J?ED  F7HJD;HI  7H;  7BBEM;:  JE I;D:7IBED=7IJ>;9EHH;IFED:?D=D;JMEHA?DJ;H<79;>7IIK<	 <?9?;DJ8K<<;H IF79;7L7?B78B;  *KH7FFHE79>  ?I87I;: ED  J>; :;7:BE9A	H;9EL;HOI9>;C;I""-! 567D:C""-!56 M>?9>  M;H;  8EJ>  J7H=;J;:  7J  CKBJ?	9ECFKJ;H  ;DL?HEDC;DJI ED  H79A  B;L;B   .>?I  F7F;H  :;I9H?8;I  J>;  7:EFJ?EDI  JE  J>;I; :;7:BE9A  H;9EL;HO I9> ;C;I J>7J M;H;  D;9;II7HO <EH 7D ;<<?	 9?;DJ  KI;  ?D MEHC>EB;  87I;:  ED	9>?F  D;JMEHAI   ""D  9EDJH7IJ JE  ""-!  7D:  C""-!  EKH  )E  :;7:BE9A  H;9EL;HO I9>;C;  ?I  9ECFB;J;BO  7M7H;  E<  C;II7=;  EH:;H?D=  <HEC IEKH9;  JE  :;IJ?D7J?ED   ""D  J>;  ;L7BK7J?ED  M;  9ECF7H;  D;J 	 MEHAIM?J>EKH:;7:BE9AH;9EL;HOI9>;C;JED;JMEHAIKI?D= IJH?9J EH:;H?D=H;7B?P;:8OL?HJK7B 9>7DD;BI?DJ;HCIE<F;H	 <EHC7D9;7D:8K<<;H 9EIJI  !;H ;8O M; 7BIE?DL;IJ?=7J;JH7<	 <?9  9>7H79J;H?IJ?9I  B?A;  >EJIFEJ  JH7<<?9  JE  I>7H;:  C;CEH?;I 7D:J>;;<<;9JIE<BE97B?P7J?EDE<IK9>A?D:E<JH7<<?9  .>;  H;IJ  E<  J>;  F7F;H  ?I  IJHK9JKH;:  7I  <EBBEMI  .>;  D;NJ I;9J?ED FHEL?:;I7DEL;HL?;ME< H;B7J;:MEHA ?D J>; <?;B:E< :;7:BE9A H;9EL;HO ?DF7H7BB;B 9ECFKJ;H D;JMEHAI  -;9J?ED  :;I9H?8;IJ>;9ED9;FJE<EKH)E:;7:BE9A	H;9EL;HOC;9>7	 D?IC   -;9J?ED    I>EMI  J>;  H;IKBJI  <HEC  EKH  9ECF7H?IED  E< IJH?9J  EH:;H?D=  7D:  :;7:BE9A  H;9EL;HO  ?D  ED	9>?F  D;JMEHAI  -;9J?ED9ED9BK:;IJ>?IF7F;H  """"  ,'.1*,$ .>;87I?I E<EKH :;7:BE9A  H;9EL;HOI9>;C; 9EC;I <HEC ""-!56M>?9> ?DJHE:K9;:J> ;;II;DJ?7B8K?B:?D=8BE9AI E<  :;7:BE9A  H;9EL;HOC;9>7D?ICI ;7:BE9A :;J;9J?ED  7D: 7D  7::?J?ED7B  9ECCKD?97J?ED  9>7DD;B  H;I;HL;:  <EH  :;7:	 BE9A;:F79A;JI .>;:;7:BE9A:;J;9J?ED7B=EH?J>C?D""-! ?I7 I?CFB;  J?C;H87I;: IEBKJ?ED   ""J  ?I H;7B?P;: 8O7  J?C;H  ?D ;79>  8K<<;H  GK;K;  J>7J  ?I  IJ7HJ;:  M>;D  7  F79A;J  ;DJ;HI  J>; GK;K;  7D:  H;I;J M>;D  J>;  F79A;J  B;7L;I  ?J  7=7?D  !EM;L;H M>;DJ>;J?C;HH;79>;I79;HJ7?DJ>H;I>EB:?J?I7IIKC;:J>7J 7  :;7:BE9A  >7I  E99KHH;:   .>?I  :;7:BE9A  :;J;9J?ED  C;J>E: :;J;9JI 7BB :;7:BE9AI  I?D9;  7  :;7:BE9A  ?I 7D  ?D<?D?J;BO BED= 8BE9A7=; !EM;L;H?J97D7BIE:;J;9J 7:;7:BE9A<EHF79A;JI J>7J  7H;  J;CFEH7H?BO  8BE9A;:  EDBO  8KJ  DEJ  F7HJ  E<  7  H;7B :;7:BE9A  .>;799;IIE<:;7:BE9A;:F79A;JI ? ;  F79A;JI<EH M>?9>  J>;  :;7:BE9A  9ED:?J?ED  ?I  <KB<?BB;:  JE  J>;  H;I;HL;: 9>7DD;B  ?I ;N9BKI?L;  ? ;  EDBOED;  F79A;J  ?I 7BBEM;:  ?D  J>?I 9>7DD;B 7J7DO J?C;  ""D""-!J>;;N9BKI?L;D;II?I=K7H7D	 J;;:8O79?H9KB7J?D=JEA;D  7I;:  ED  ""-!  7D  ;NJ;DI?ED  97BB;:  C""-! 56 M7I  FK8B?I>;:  ?D  J>;  <?;B:  E<  F7H7BB;B  9ECFKJ;H  D;JMEHAI J>7J  IKFFEHJI  J>; H;9EL;HOE< :;7:BE9AI 97KI;:  8OC;II7=; :;F;D:;D9?;I   ""D  C""-!  7::?J?ED7B  EKJFKJ  8K<<;HI  M;H; 7::;:JEJ>;D;JMEHA?DJ;H<79;IE<J>;J?B;I .>;I;7::?J?ED7B EKJFKJ 8K<<;HI7H; H;I;HL;: <EH:;7:BE9A;:F79A;JI   ""D7::?	 J?ED  J>;  D;JMEHA  ?DJ;H<79;I  M;H;  ;NJ;D:;:  8O  FH;;CFJ?ED 97F78?B?J?;I J>7J7BBEMJ>;FH?EH?J?P;:FHE9;II?D=E<7F79A;J 7HH?L?D=EDJ>;H;I;HL;:9>7DD;B .>;H;IKBJ?D=F79A;JE<J>?I FHE9;II?D= M>?9>  ?I  J>;D;NJ F79A;J  ?D  J>; C;II7=; :;F;D 	 :;D9O  9>7?D  M?BB  8;  ?CC;:?7J;BO  I;DJ  EKJ  ?D  J>;  H;I;HL;: 9>7DD;B .>?I C;7DI 7 :;7:BE9A;: F79A;J 7D: 7BB  ?JI  IK8I;	 GK;DJ F79A;JI J>7J 7H;9H;7J;:<HEC  J>?IF79A;J :K; JEC;I	 I7=;  :;F;D:;D9?;I  7H;  JH7L;B?D=  ?D  J>;  H;I;HL;:  9>7DD;B  .>;H;<EH; CKBJ?FB;BE=?97B9>7DD;BI?D J>; H;I;HL;:9>7DD;B <EH  J>;  :?<<;H;DJ C;II7=;  JOF;I  7H;  DEJ  H;GK?H;:   !EM;L;H I?D9; J>;H; ?IEDBO;N9BKI?L;799;II JE J>;H;I;HL;:9>7DD;B J>?I  C;J>E:  E<  >7D:B?D=  C;II7=;  :;F;D:;D9?;I  8BE9AI  J>; H;I;HL;:9>7DD;BH;B7J?L;BOBED=  ""-!7IM;BB 7IC""-!M;H;:;L;BEF;:<EH7FFB?97	 J?ED ?D F7H7BB;B 9ECFKJ;HD;JMEHAIED H79A B;L;B 7D:DEJ <EH KI; ?DED	9>?FD;JMEHAI  .>KI J>;O:EDEJA;;FJ>;EH:;HE< F79A;JI  8;JM;;D  IEKH9;  7D:  :;IJ?D7J?EDI  .>;  D;JMEHA  7H	 9>?J;9JKH;I  ?D 56 M;H;  EFJ?C?P;:  M?J>  J>;  ?DJ;DJ?ED  E< H;79>?D= >?=>  J>HEK=>FKJ  ? ;   J>;  J7H=;J  ?I  J>;  ;<<?9?;DJ  KI; E<7 >?=>DKC8;HE<L?HJK7B 9>7DD;BI  .>;D;JMEHA7H9>?J;9	 JKH;I  I?CKB7J;:  ?D 56 >7L;  KF  KF  JE    L?HJK7B  9>7DD;BI  .>?I  ?I  ?D 9EDJH7IJ  JE)EI M>;H; 8EKD:7HO9ED:?J?EDI 7D: J>KI J> ;:;I?=D  J7H=;JI 7H; :?<<;H;DJ  /IK7BBOED; E<  J>; :;	 I?=D J7H=;JI <EH 7 )E ?IJEA;;FJ>; 9>?F7H;7 H;GK?H;C;DJI 7I  IC7BB  7I  FEII?8B;   EDI;GK;DJBO M; M?BB  DEJ  ?CFB;C;DJ CKBJ?FB;L?HJK7B 9>7DD;BI ?DJ>;DEHC7B:7J7 F7J>E<EKH D;J	 MEHA  7I  :ED;  ?D 56   K;  JE  7BB  J>;I;  :?<<;H ;D9;I  C7@EH 7:7FJ7J?EDIM;H ;H;GK?H;:?D J>?I9ECF7H?IED8;JM;;D :;7:	 BE9AH;9EL;HO7D:IJH?9JEH:;H?D=  """"""  '*$,*0,3*,)*.>;  C7?D  :?<<;H;D9;  E<  J>;  :;7:BE9A  H;9EL;HO  I9>;C; <EH  MEHC>EB;  87I;:  )EI  J>7J  ?I  FH;I;DJ;:  ?D  J>?I  F7F;H 18 9ECF7H;:JEC""-!?IJ>;>7D:B?D=E<C;II7=;:;F;D:;D 	 9?;I  ?D  J>;  H;I;HL;:  :;7:BE9A  9>7DD;B   *KH  )E  :;7:BE9A H;9EL;HO9ED9;FJKI;ICKBJ?FB;D;IJ;:L?HJK7B9>7DD;BIM?J>	 ?D J>;:;7:BE9A9>7DD;B JEI;F7H7J;:?<<;H;DJC;II7=;JOF;I  .>;:;7:BE9A :;J;9J?ED 7B=EH?J>C 7D: J>;799;II  H;=KB7J?ED C;9>7D?IC JEJ>;:;7:BE9A9>7DD;B 7H;I?C?B7H ?DFH?D9?FB;  !EM;L;HJ>;799;II H;=KB7J?ED >7I8;;D 7:7FJ;:7IM;BB 7I J>;  <BEM  9EDJHEB  7D:  IM?J9>?D=  <KD9J?ED7B?JO  E<  7  9EDL;D	 J?ED7B )E  HEKJ;H   ""D  7::?J?ED  EKH )E :;7:BE9A  H;9EL;HO I9>;C;  D;;:;:  7  879A	E<< C;9>7D?IC  <EH  J>;  J?B;I M>;D  7 :;7:BE9AE99KHI  ?DEH:;H JE9B;7H  J>;9ED=;IJ?ED E< F79A;JI ?D  J>;D;JMEHA  DEJ>;H  ;NJ;DI?ED E< EKH )E:;7:BE9A H;	 9EL;HOI9>;C;?IJ>;<KD9J?ED7B?JOJEH;J7?DJ>;F79A;JEH:;H ?D97I;E<H;:?H;9J?EDE<F79A;JIJEJ>;:;7:BE9A9>7DD;B   +"")'+%,$#//%##-#+""#+!'#/ *KH :;7:BE9A H;9EL;HOI9>;C;KI;ID;IJ;:L?HJK7B 9>7D	 D;BI  M?J>?D  J>;  9>7DD;B  H;I;HL;:  <EH  :;7:BE9A;:  F79A;JI :;7:BE9A  9>7DD;B  ?CFB;C;DJ;:  8O J>; :;7:BE9A  8K<<;H  JE I;F7H7J;  J>;  :?<<;H;DJ  C;II7=;  JOF;I  ; =   H;GK;IJI  EH  H;	 IFEDI;I  .>?I  C;7DI  J>;  9ED9;FJ  E<  IJH?9J  EH:;H?D=  ?I  7F	 FB?;:M?J>?DJ>;:;7:BE9A9>7DD;B .>;HEKJ;HI7IM;BB7IJ>; D;JMEHA ?DJ;H <79;I >7L; IK9>  7  :;7:BE9A  8K<<;H   ""D  J>; D;J	 MEHA  ?DJ;H<79;I J>; :;7:BE9A 8K<<;H  ?I 9EDD;9J;: ?D F7H7BB;B JE  J>;  ?DFKJ  8K<<;H  ?D  EH:;H  JE H;9;?L;  F79A;JI  JH7L;B?D=  ED J>;:;7:BE9A9>7DD;B  ?=KH; I>EMI  J>; 9EDD;9J?ED E< J>; :;7:BE9A  8K<<;H  ?D  J>;  HEKJ;H   ""J  97D  8; I;;D  J>7J  J>;  :;7:	 BE9A 8K<<;H  ?I 9;DJH7B  JE  J>;  HEKJ;H  ? ;   ?J  ?I  DEJ  ?DIJ7DJ?7J;: F;HFEHJ7I?IJ>;97I; <EH  ?DFKJ 7D:EKJFKJ 8K<<;HI  ""<7D7F	 FB?97J?ED KI;I9ECFB;NFHEJE9EBIM?J> 7>?=> DKC8;H E<:;	 F;D:;DJ C;II7=;I  ?J  H;GK?H;I CK9>  B;II 8K<<;H IF79;  JE ?D	 9H;7I;  J>;  DKC8;H  E<  L?HJK7B  9>7DD;BI  E<  J>;  9;DJH7B  :;7: 	 BE9A 8K<<;H  J>7D JE?CFB;C;DJ 7::?J?ED7BL?HJK7B 9>7DD;BI ?D J>;  ?DFKJ	  7D:  EKJFKJ  8K<<;HI  .>;  D;IJ;:  L?HJK7B  9>7DD;BI M?J>?D  J>;  :;7:BE9A  9>7DD;B  :;7:BE9A  L?HJK7B  9>7DD;B 07H;7IIE9?7J;:JEJ>;C;II7=;JOF;IFH;I;DJ?DJ>;D;J 	 !5;:,8 46;:);--,8   ;:6;:);--,8   				   	  /,(+,8-20:+(:(- 20: ?=KH; ;7:BE9A;:C;II7=;IJH7L;B EDJ>;DEHC7B:7J7B?DA IJ>;JEA ;D >7I?JIEMDF7J>EDBO:7 J7F7 J>II>EMD  	 # 04 (:(6(:/ -58:51,4  ,(+25*1 8,*5<,8? *54:852 ;40: (:(6(:/-58 8,+08,*:,+ 6(*1,:9  # 5;: ,(+25*1);--,8 +,(+25*1*/(44,2 =0:/<08:;(2 */(44,29%9 MEHA   .>?I  C;7DI  ED;  0  ?I  ?CFB;C;DJ;:  F;H  C;II7=; JOF;  .>;  799;II  E<  F79A;JI  JE  J>;  :;7:BE9A  9>7DD;BI  ?I  EDBO 7BBEM;:M>;D 7  JEA;D M>?9> ?I 9?H9KB7J?D= J>HEK=> 7  JEA;D :?IJH?8KJ?ED  H?D=  D;JMEHA  ?I  97K=>J  .>;  JEA;D  :?IJH?8KJ?ED D;JMEHA  9EDD;9JI  7BB  HEKJ;HI  7D:  D;JMEHA  ?DJ;H<79;I  !EM	 ;L;H?D9EDJH7IJJE""-!CKBJ?FB;JEA;DI9?H9KB7J;J>HEK=> J>;JEA;D:?IJH?8KJ?EDH?D=D;JMEHAE<EKH:;7:BE9AH;9EL;HO IOIJ;C  .>;  DKC8;H  E<  JEA;DI  ?I  ;GK7B  JE  J>;  DKC8;H  E< 0I  7D:  ;79>  JEA;D  ?I  7IIE9?7J;:  JE  7  9;HJ7?D  0   HEKJ;H  EH  D;JMEHA  ?DJ;H<79;  J>7J M7DJI  JE  H;:?H;9J  7  F79A;J ?DJEJ>;:;7:BE9A9>7DD;B8;97KI; J>;9ED:?J?ED<EH 7:;7:	 BE9A?IC;J>7IJEM7?J<EH7JEA;DJ>7J<?JIM?J>J>;C;II7=; JOF; E< J>?IF79A;J .>; JEA;D J>;DJH7L;BIM?J>J>;H;:?H;9J	 ;:F79A;J?DJ>;:;7:BE9A9>7DD;BJEJ>;:;IJ?D7J?EDCE:KB; M>;H;J>; JEA;D ?IH;B;7I;: 7=7?D ?DJEJ>; JEA;D:?IJH?8KJ?ED D;JMEHA7<J;H J>;F79A;J >7I8;;D 9ECFB;J;BO H;9;?L;:  .E	 =;J>;H M?J> J>; H;:?H;9J?ED E< J>;F79A;J 7 9ECC7D: ?II;DJ L?7  J>;  JEA;D :?IJH?8KJ?ED  D;JMEHA  JE 7BB D;JMEHA  ?DJ;H<79;I ?DEH:;HJEIJEFI;D:?D=<B?JI<EH7FH;:;<?D;:879A	E<<J?C;  .>;:;9?I?ED<EH?CFB;C;DJ?D=ED;L?HJK7B9>7DD;B?DJ>; :;7:BE9A  8K<<;H  <EH  ;79>  C;II7=;  JOF;  >7I  J>;  <EBBEM?D= 9ED I;GK;D9;I  ""D9EDJH7IJJEC""-!DEFH;;CFJ?ED <KD 9J?ED7B?JO?I H;GK?H;:?DJ>;D;JMEHA?DJ;H<79;IE<J>;J?B;I .>;H;7	 IED  ?I J>7J ?DEKH :;7:BE9A H;9EL;HOI9>;C;IK99;;:	 ?D=F79A;JI; =  7  H;IFEDI;F79A;JE<7F79A;J 7HH?L	 ?D=L?7 J>;:;7:BE9A9>7DD;B7H;DEJ7KJEC7J?97BBOFKJ ?DJE J>;:;7:BE9A9>7DD;B 7=7?D '?A;7DOEJ>;H F79A	 ;JJ>;O7H;FKJ?DJEJ>;DEHC7BEKJFKJ8K<<;H   ""D9H;7I;:  J>HEK=>FKJ  8;97KI;  <?HIJBO  7BB  7L7?B78B; :;7:BE9A  L?HJK7B  9>7DD;BI  97D  8;  KI;:  7J  I7C;  J?C; 7BIE  :K;  JE CKBJ?FB;  JEA;DI   -;ED:BO  J>;  E99KF7J?ED J?C;E< 7 08O7 H;:?H;9J;: F79A;J  ?II?=D?<?97DJBO H;:K9;:?D9ECF7H?IEDJEC""-! ""<7F79A;J ?IF7HJ E<  7  C;II7=;  :;F;D:;D9O  9>7?D  ?D  C""-!  J>; :;7:BE9A 9>7DD;B  ?I E99KF?;:  8O J>?I F79A;J KDJ?B  J>; B7IJ  F79A;J  ?D  J>;  C;II7=;  :;F;D:;D9O  9>7?D  E<  J>?I F79A;J >7IH;79>;:?JI:;IJ?D7J?ED .>?IJ?C;:E;IDEJ EDBO  ?D9BK:;  J>;  D;JMEHA  JH7DI<;H  B7J;D9?;I  8KJ  7BIE IB7L;  FHE9;II?D=  J?C;I   ""D  9EDJH7IJ  EKH  :;7:BE9A  H;	 9EL;HOI9>;C;H;B;7I;I J>;JEA;D <EH 7 07<J;H  J>; F79A;J  >7I  8;;D  H;9;?L;:  8O  ?JI  :;IJ?D7J?ED  D;JMEHA ?DJ;H<79;?D:;F;D:;DJE<7DOC;II7=;:;F;D:;D9?;I   O7:7FJ?D= J>;  H;:?H;9J?ED  <KD9J?ED7B?JO ?D  J>; :;7:	 BE9A  H;9EL;HO  KD?J  ?J  ?I  FEII?8B;  JE  H;J7?D  J>;  F79A;J #02, !5;:,8 ,:=581 4:,8-(*, #51,4 +09:80);: 054 804. 4,:=581 ?=KH;.EA;D:?IJH?8KJ?EDH?D=D;JMEHA 19  	 	  	   EH:;H  E<  H;:?H;9J;:  7D:  :;7:BE9A;:  F79A;JI  M?J>EKJ J>;H;GK?H;C;DJE<7::?J?ED7BH;EH:;H?D=8K<<;HI  ""DEH:;HJE;D78B;79EHH;9J :7J7 JH7DI<;HED J>;:;7:BE9A 9>7DD;B  8;JM;;D  J>;  HEKJ;HI  J>;  HEKJ;H  ?DJ;H<79;  H;GK?H;I 7:7FJ7J?ED   ""J  CKIJ  8;  78B;  JE  I?=D7B  J>;  H;9;?L?D=  HEKJ;H M>;J>;H  J>;  DEHC7B  9>7DD;B  EH  ED;  E<  J>;  :;7:BE9A  9>7D	 D;BI ?I79J?L;  ""< J>;DKC8;H E<:;7:BE9AL?HJK7B 9>7DD;BI ?D J>;D;JMEHA ?I +  J>; B?DACKIJ 8; 78B;  JEI?=D7B +IJ7J;I  !EM;L;H?<M;9ECF7H;J>?IJE7D;JMEHAJ>7JKI;IIJH?9JEH 	 :;H?D= <EH :;7:BE9A  7LE?:7D9;  J>; M?H?D=EL;H>;7:  ?I L;HO IC7BB  I  J>;  HEKJ;H M?J>  ?CFB;C;DJ;: IJH?9J  EH:;H?D= CKIJ 7BIE  I?=D7B  M>?9>  E<  J>; + L?HJK7B  9>7DD;BI  ?I  KI;: J>; M?H?D=EL;H> ;7:F;HB?DA<EH:;7:BE9AH;9EL;HO?ID;=B?=?8B;  *KH  :;7:BE9A  H;9EL;HO  IOIJ;C  =K7H7DJ;;I  JE  IEBL;  7BB :;7:BE9AI:K;JEJ>;IJH?9JEH:;H?D=?DJ>;:;7:BE9A9>7DD;B  .>; IJH?9J EH:;H?D=  ?D  J>; :;7:BE9A 9>7DD;B 7IIKH;I  J>7J  J>; J;HC?D7J?D=C;II7=; ?I78B;JE7HH?L;7J ?JI:;IJ?D7J?ED CE:	 KB;  7J M>?9>  ?J  M?BB  8;  9EDIKC;:   J;HC?D7J?D=  C;II7=; M?BBDEJ JH?==;H J>;9H;7J?ED E<EJ>;HF79A;JI 7I?J ?I J>; B7IJ C;II7=;?D79>7?DE<:;F;D:;DJC;II7=;I -?D9;J>;7HH?L7B E< J>; J;HC?D7J?D=C;II7=;; =  H;IFEDI;F79A;J?I=K7H7D	 J;;:7BBIK8EH:?D7J;C;II7=;I; = H;GK;IJ F79A;J97D7BIE 7HH?L;7JJ>;?H:;IJ?D7J?EDI   #""),!(#0#!0',+ .>;  :;7:BE9A  :;J;9J?ED  ?D  EKH  HEKJ;HI  ?I  7  >;KH?IJ?9 J?C;H87I;:IEBKJ?ED7I?D""-!  .>?IC;7DI7BB?DFKJ7D: EKJFKJ  8K<<;HI  7H;  ;GK?FF;:  M?J>  J?C;HI  7D:  7  :;7:BE9A  ?I :;J;9J;: M>;D IK9> 7  J?C;H  H;79>;I 7 9;HJ7?D  J>H;I>EB: I;; I;9J?ED ""0 <EH  7:;GK7J;  L7BK;I  E<  J>;  J>H;I>EB:   /DB?A; C""-!  M;  :E  DEJ  9EDI?:;H  8K<<;H  <?BB  B;L;BI  I?D9;  J>?I 9H?J;H?ED  ?I  7BM7OI  JHK;  ?D  97I; E< :;7:BE9AI 7D: 8BE9A7=;I <EH8K<<;HIE<EH<B?JII?P;  .>;  :;7:BE9A  :;J;9J?ED  E<  J>;  :;7:BE9A  H;9EL;HO  KD?J ,/ M?BB C7HA  J>; >;7:;H  <B?J  E< 7  F79A;J  <EH  H;:?H;9J?ED M>;D  J>;  :;7:BE9A  :;J;9J?ED  9ED:?J?ED  >7I  8;;D  C;J  .>; H;:?H;9J?ED M?BB  J>;D  8;  ;N;9KJ;: M>;D  J>; ,/  >7I  97F	 JKH;:  J>;  799EH:?D=  JEA;D   ""< J>;  >;7:;H  <B?J  ?I  78B;  JE  FHE	 9;;:  7BED=  J>;  DEHC7B  :7J7  F7J>  8;<EH;  J>;  H;GK?H;:  JEA;D 97D  8;  97K=>J  J>;D  J>;  H;:?H;9J?ED  ?I  I?CFBO  78EHJ;:  .>; H;:?H;9J?EDE<F79A;JIJ>7J 7H;EDBOF7HJ E<7J;CFEH7HO9ED 	 =;IJ?EDFEI;IDEFHE8B;CI  !EM;L;H  B?A; 7DO H;:?H;9J?ED  ?J M?BB 9>7D=;  J>; EH:;H  E< F79A;JI  ?< DE7::?J?ED7B C;7IKH;I 7H;J7A;DI;;I;9J?EDT,;J7?D?D=F79A;JEH:;HU8;BEM   #""'.#!0',+ ""D EH:;H  JE;D78B;  J>;  H;:?H;9J?ED E<  7  F79A;J  <EH M>?9> J>;:;7:BE9A:;J;9J?ED9ED:?J?ED>7I8;;DC;JI;L;H7BC;7	 IKH;I>7L;JE8;J7A;D ""D IEC;E<J>;7H9>?J;9JKH7B9>7D=;I JEJ>;HEKJ;H97D8;I;;D J>;:;7:BE9A8K<<;H >7IJE8;9ED 	 D;9J;:JEJ>;?DFKJFEHJIL?7:;	CKBJ?FB;N;HI7D:M?J> CKBJ?	 FB;N;HI JE J>;EKJFKJ FEHJI  ?D EH:;H  JE7BBEM J>; H;:?H;9J?ED E<:;7:BE9A;:F79A;JI <HECJ>;EKJFKJ8K<<;H7D:JEJH7DI<;H F79A;JIJEJ>;?H:;IJ?D7J?ED  ""D7::?J?ED  J>; :;7:BE9A8K<<;H >7IJE8;9EDD;9J;:JEJ>;IM?J9>JE;D78B;J>;H;:?H;9J?EDE< :;7:BE9A;:F79A;JI?DJ>;?DFKJ8K<<;H  !EM;L;HJ>;I;9>7D=;I?DJ>;HEKJ;H7H9>?J;9JKH;7H;DEJ IK<<?9?;DJ <EH 7)EHEKJ;HJ>7J ?CFB;C;DJIMEHC>EB;HEKJ	 ?D= 56  ""D 9EDJH7IJ  JE""-!7D:C""-!  ?D 7 )E J>; 8K<<;H  GK;K;I  7H;  H;7B?P;:  8O  ""*  GK;K;I  ?D M>?9>  EDBO J>; <?HIJ <B?J  ?D 7GK;K;  ?I799;II?8B;  -E  ?D EH:;H JE8; 78B; JE  H;:?H;9J  7DO  F79A;J  <EH  M>?9>  7  :;7:BE9A  M7I  :;J;9J;: J>;  >;7:;H  <B?J  E<  7  F79A;J  CKIJ  7BM7OI  8;  J>;  <?HIJ  ?D  7 8K<<;H GK;K;  .>;H;<EH;J>;>;7:;H <B?J E<+79A;J?D ?=KH; ?IDEJ7BBEM;:JE;DJ;HJ>;EKJFKJ8K<<;HE<FEHJ7BJ>EK=> J>?I8K<<;H>7I<H;;IF79; .>;<B?JIE<+79A;J C7O;DJ;HJ>; EKJFKJ8K<<;HEDBOM>;DJ>;B7IJ<B?JE<+79A;J>7IB;<J   K7H7DJ;;?D=J>7JJ> ;>;7:;H<B?J?I7BM7OI7JJ>;<?HIJFE	 I?J?ED?D78K<<;H GK;K; 97D8; 79>?;L;:8O7:7FJ?D=J>;7H	 8?JH7J?ED 7D:IM?J9>?D= <KD9J?ED E<  J>; HEKJ;H 7IM;BB 7I J>; <BEM  9EDJHEB  C;9>7D?IC   .>;  7H8?JH7J?ED  7D:  IM?J9>?D= <KD9J?ED  ?I 7:7FJ;:  ?D IK9> 7 M7O  J>7J  ?J  JH7DI<;HI7 >;7:;H <B?J  <HEC  7D  ?DFKJ  8K<<;H  JE  7D  EKJFKJ  8K<<;H  EDBO 7<J;H  J>; EKJFKJ 8K<<;H >7I8;;D 9ECFB;J;BO <H ;;  ? ;  7<J;H  J>; J7?B  <B?J E<  7  FH;L?EKI  F79A;J  >7I  B;<J  J>?I  EKJFKJ  8K<<;H  .>;  <BEM 9EDJHEB <KD9J?ED ?I7BIE9>7D=;:?DEH:;HJEB;J 7D;MF79A;J <HEC 7D KFIJH;7C  HEKJ;H  ?DJE J>;  ?DFKJ 8K<<;H EDBO7<J;H  J>; ?DFKJ8K<<;H>7I8;;D9ECFB;J;BO;CFJO  .>;I; C;7IKH;I  H;IKBJ  ?D 7  BEM;H;: ;<<;9J?L; GK;K; I?P; E<J>;8K<<;HIJ>KI:;9H;7I?D=J>;J>HEK=>FKJE<J>;D;JMEHA I;;?=KH; 1>?B;J>;F79A;JB7J;D9O7JBEMD;JMEHABE7: ?IEDBOIB?=>JBO:;9H ;7I;:J>;B7J;D9O?D9H;7I;IDEJ?9;78BO7J >?=>;HBE7:IE<J>;D;JMEHA7D:?JIJ>HEK=>FKJ?IH;:K9;:   !(,$$1+!0',+)'04+"",(#+'/0.' 10',+#03,.( .>;  JEA;D  :?IJH?8KJ?ED  D;JMEHA  ?I  DEJ EDBO KI;:  <EH  J>; :?IJH?8KJ?ED  E<  J>;  JEA;DI  J>7J  H;=KB7J;  J> ; 799;II E<  :;7:	 BE9A;:  F79A;JI  JE  J>;  :;7:BE9A  9>7DD;B   ""J  7BIE  :?IJH?8KJ;I J>;  879A	E<<  9ECC7D:  <EH  J>;  J?B;I  M>;D  7  :;7:BE9A  >7I 8;;D :;J;9J;:7D:7F79A;J  ?I H;:?H;9J;: <J;H H;9;?L?D=J>; 879A	E<<  9ECC7D:  J>;  J?B;I  7H;  DEJ  7BBEM;:  JE  ?D?J?7J;  <KH	 J>;H  JH7DI<;HI 7J  J>;?H  9ECCKD?97J?ED  ?DJ;H<79;  <EH 7  9;HJ7?D 879A	E<<  F;H?E:  .>?I  879A	E<<  F;H?E:  ?I  I;J  7J :;I?=D  J?C; IE?JIL7BK;:E;IDEJ>7L;JE8;:?IJH?8KJ;:J>HEK=>J>;JEA;D :?IJH?8KJ?ED  D;JMEHA   .>;  879A	E<<  9ECC7D:  EDBO  <EH8?:I J>;  ?D?J?7J?ED  E<  D;M  JH7DI<;HI  <EH  ;N7CFB;  H;GK;IJ  F79A;JI <HEC FHE9;IIEHI  (;II7=;I E< >?=>;H C;II7=;  JOF;  ; =   H;	 IFEDI;F79A;JIE<C;II7=;JOF;97DIJ?BB8;I;DJEKJ 8OJ>; C;CEH?;I?DEH:;HJE:H7?DJ>;D;JMEHAE<FEII?8BO9ED=;IJ	 ;:H;GK;IJF79A;JI  .>?I879A	E<<<KD9J?ED7B?JO?IH;GK?H;:?DEH:;HJE;D78B; J>;H;9EL;HOE<J> ;D;JMEHA JE7 <KBBOEF;H7J?ED7BIJ7J;7<J;H 7  :;7:BE9A  EH  J;CFEH7HO  9ED=;IJ?ED  <J;H  7  :;7:BE9A  E9	 9KHIJ>;799EH:?D=HEKJ;HM?BB:;J;9J J>;:;7:BE9AEDBO7<J;H J>;  :;<?D;:  :;7:BE9A  :;J;9J?ED  J>H;I>EB:  J?C;  KH?D=  J>?I J?C;  J>;  J?B;I  7H;  IJ?BB  I;D:?D=  <B?JI  ?DJE  D;JMEHA  7D:  J>; 8K<<;HI  E< J>; D;JMEHA M?BB 7BB 8;  9ECFB;J;BO <?BB;:  KF  I	 IKC?D=J>7JJ>;H;?IDE879A	E<<<EHJ>;J?B;IM> ;D7H;:?H;9	 J?ED  E99KHI  J>;  D;JMEHA  MEKB:  ?CC;:?7J;BO  8BE9A  7=7?D :K; JEE99KHH?D=:;7:BE9AI  .>7JC;7DI J>;D;JMEHAMEKB: DEJH;9EL;H<HECJ>;:;7:BE9A8KJMEKB:;DJ;H7IJ7J;M>;H; J>;  J>HEK=>FKJ  MEKB:  8;  7FFHEN?C7J;BO ;GK7B  JE J>;  87D:	 M?:J>E<J>;7L7?B78B;:;7:BE9A9>7DD;BI  .>; :7J7 M?:J> E< J>;  JEA;D :?IJH?8KJ?ED D;JMEHA;GK7BI B:+M?J>D8;?D=J>;DKC8;HE<:;7:BE9AL?HJK7B9>7D	 D;BI  *D; M?H;  ?I  H;GK?H;:  JE  I?=D7B  J>;  7HH?L7B  E<  :7J7  7D: 7DEJ>;HM?H;?IH;GK?H;:?DEH:;HJEI?=D7BM> ;J>;H7JEA;DEH 7879A	E<<9ECC7D:>7I7HH?L;:  .>; H;IJE<J>;M?H;I?IH;	 20  	 # 04 (:(6(:/ -58:51,4 ,(+25*1 8,*5<,8? *54:852 ;40: (:(6(:/-58 8,+08,*:,+ 6(*1,:9 /,(+,8-20:+(:(- 20: 46;:);--,8   ;:6;:);--,8    # 5;: ,(+25*1);--,8 +,(+25*1*/(44,2 =0:/<08:;(2 */(44,29%9 ?=KH; ,;J7?D?D=J>;F79A;J EH:;H 8OH;:?H;9J?EDE<7F79A ;J ?D7HEKJ;H GK?H;:JEJH7DI<;HJ>;?D<EHC7J?EDJEM>?9>:;7:BE9AL?HJK7B 9>7DD;B  J>;  JEA;D  8;BED=I   EH  7  D;JMEHA M? J>  JME :;F;D	 :;DJ  L?HJK7B  9>7DD;BI  J>;  :7J7 M?:J>  E<  J>;  JEA;D  :?IJH?8K	 J?EDD;JMEHA?IJ>KI   #0'+'+%!(#0.""#. .>;:;7:BE9A H;9EL;HOI9>;C; 7I ?J >7I8;;D :;I9H?8;: 78EL;  :E;I  DEJ A;;F  J>; EH:;H E< F79A;JI  <EH 7D  ?D:?L?:K7B 9EDD;9J?ED  8;JM;;D  IEKH9;  7D:  :;IJ?D7J?ED   1>;D  :;7:	 BE9A;:F79A;JI7H;H;:?H;9J;:?DJEJ>;:;7:BE9A9>7DD;B J>;O C7OEL;HJ7A;  F79A;JI  ?D  J>; DEHC7B  :7J7 9>7DD;B  .>?I  97D 8;I;;D?D?=KH;M>;H;F79A;JM>?9>>7I8;;DH;:?H;9J	 ;:JEJ>;:;7:BE9A9>7DD;B?IEL;HJ7A?D=F79A;J  ""D EH:;H JE H;J7?D J>;F79A;J EH:;H ED;FEII?8B; IEBKJ?ED MEKB:8;JE?CFB;C;DJH;EH:;H?D=8K<<;HI?DJ>;D;JMEHA?D	 J;H<79;   !EM;L;H  J>?I  MEKB:  B;7:  JE  7::?J?ED7B  9>?F  7H;7 9EIJI7D:?IDEJD;9;II7HO<EHEKHH;7B?P7J?EDE<J>;:;7:BE9A H;9EL;HO9ED9;FJ  ""DIJ;7:E<7H;EH:;H?D=8K<<;HM; H;:?H;9J F79A;JI <HEC  J>; DEHC7B  :7J7 F7J>  J>7J MEKB: 8; EL;HJ7A;D 8O7D7BH;7:OH;:?H;9J;:F79A;JM?J>I7C;IEKH9;7D::;IJ?	 D7J?ED?DJEJ>;:;7:BE9A9>7DD;B  ;IF?J;J>;79JK7B799;II H;IJH?9J?EDI  E<  J>;  :;7:BE9A  9>7DD;B  J>?I  B;7:I  JE  CKBJ?FB; F79A;JI M?J>?D  ED;  :;7:BE9A L?HJK7B 9>7DD;B  !EM;L;H  J>?I FEI;I  DE  FHE8B;C  I?D9;  7BB  J>;I;  F79A;JI  >7L;  J>;  I7C; IEKH9;  7D:  :;IJ?D7J?ED  7D:  J>KI  97DDEJ  <EHC  7  :;7:BE9A  ?D J>;:;7:BE9A9>7DD;B  L;HO  J?C;  7  F79A;J  JH7L;B?D=  ?D  J>;  :;7:BE9A  9>7DD;B ;DJ;HI  7  :;7:BE9A  8K<<;H  GK;K;  L?7  ?DFKJ  FEHJ  '  J>;  HEKJ;H 9> ;9AIM>;J> ;H J>;H; ?I 7 >;7:;H  <B?J  M?J>;GK7B IEKH9;7D: :;IJ?D7J?ED  ?D  J>;  DEHC7B  ?DFKJ  8K<<;H  E<  J>;  FEHJ  ' M>?9> M?BB  8;  EL;HJ7A;D  D  ;N7CFB;  <EH  IK9>  7  I?JK7J?ED  97D  8; I;;D  ?D ?=KH;    ?D M>?9>  +79A;J  MEKB:  EL;HJ7A;  +79A	 ;J   .>;H;<EH;  +79A;J  ?I  H;:?H;9J;:  ?DJE  J>;  :;7:BE9A 9>7DD;B 7D:  ?J  8OF7II;I  J>;  :;7:BE9A 8K<<;H  ?D  EH:;H  JE 7H 	 H?L;7JJ>;:;IJ?D7J?ED8;<EH;+79A;J   ""D  EH:;H  DEJ  JE  EL;HJ7A; C;II7=;I  ?D  J>;  EKJFKJ  8K<<;H J>;:;7:BE9A8K<<;H C7OI;D:EKJ  ?JI F79A;J EDBO ?< J>;H; ?I DE>;7:;H <B?JM?J> J>;I7C;:;IJ?D7J?ED 7I J>;F79A;J  ?D J>; EKJFKJ 8K<<;H ""D97I;J>;H;?IIK9> 7>;7:;H<B?J?J?IC7HA;: <EH H;:?H;9J?ED ?DJE J>;:;7:BE9A9>7DD;B ?D J>;D;NJ :EMD	 IJH;7C  HEKJ;H  7D: I;DJ  EKJ  8;<EH;  J>;  F79A;J  ?D  J>;  :;7:	 BE9A8K<<;H  ,;J7?D?D=  J>;  EH:;H  8O  H;:?H;9J?D=  7::?J?ED7B  F79A;JI  ?I FEII?8B;8;97KI;J>;:;7:BE9A9>7DD;B?I?CFB;C;DJ;:B?A;7 L?HJK7B 9>7DD;B  ?D F7H7BB;B  JE J>; DEHC7B :7J7 F7J>  .>KI 7BB F79A;JIJ7A;J>;I7C;F7J>JEJ>;?H:;IJ?D7J?ED  ""0  2+, ""().""D  J>?I  I;9J?ED  EKH )E  :;7:BE9A  H;9EL;HOI9>;C; M?BB 8;9ECF7H;: JEIJH?9J EH:;H?D=?DJ;HCIE<9>?F7H;7H;GK?H;	 C;DJI 7D: F;H<EHC7D9;   ""D  J>;  <EBBEM?D= ?DL;IJ?=7J?EDI  J>; ?D<BK;D9;E<:?<<;H;DJJ?C?D=IE< J>;:;7:BE9A H;9EL;HO KD?	 <EHC  7D:  BE97B?P;:  JH7<<?9  :?IJH?8KJ?EDI  7D:  E<  :?<<;H;DJ 8K<<;HI?P;IEDJ>;D;JMEHAJ>HEK=>FKJM?BB8;I>EMD   #03,.(.!&'0#!01.#/ .>;87I?9 D;JMEHA7H9>?J;9JKH;?II>EMD ?D ?=KH;  BB D;JMEHA7H9>?J;9JKH;I?DL;IJ?=7J;:8;BEM7H;+N+C;I>;IKI	 ?D=MEHC>EB;<EHM7H:?D=7D:23	HEKJ?D=  ""<DEJ IJ7J;:EJ>	 ;HM?I;  J>;  ?DL;IJ?=7J;:  D;JMEHAI  7H;  N  C;I>;I   .>; 8K<<;H?D=IJH7J;=OE<J>;)EHEKJ;HI?I7D?DFKJ	7D:EKJFKJ 8K<<;H;:  7H9>?J;9JKH;  7I  I>EMD  ?D  J>;  <?=KH;I  78EL;   ""<  DEJ IJ7J;: EJ>;HM?I; 7BB 8K<<;H GK;K;I 7H;  <B?JI :;;F  7BIE J>; :;7:BE9A 8K<<;H GK;K;I  ?D  J>;:;7:BE9A9>7DD;B  .>; ?DFKJ 7D:EKJFKJ8K<<;HIE< J>;J?B;ID;JMEHA?DJ;H<79;I7H;<B?JI :;;F  ""D  J>; J?B; ?DFKJ 7D:EKJFKJ 8K<<;H E<J>;D;JMEHAIKI	 ?D=IJH?9J EH:;H?D= <EH :;7:BE9A 7LE?:7D9;  J>; CKBJ?FB; L?H	 JK7B  9>7DD;BI  M?BB  8;  ?CFB;C;DJ;:  EDBO  ?<  J>;  799EH:?D= C;II7=;JOF;I7HH?L;7JJ>;J?B;EH7H;I;DJEKJE<J>;J?B;  ""DEH:;HJE?DL;IJ?=7J;C;II7=;:;F;D:;DJ:;7:BE9AI J>; I?CKB7J;:-E>7IJEKI;7JB;7IJ 7JME	M7OFHEJE9EB ""DEKH I?CKB7J?EDI  J>?I  M?BB  8;  H;GK;IJ	H;IFEDI;  JH7<<?9  8;JM;;D FHE9;IIEH 7D:C;CEHO J?B;I  79> I?CKB7J;:-E  ?I 8K?BJ E<   C;CEHO  J?B;I  M>?B;  J>;  H;IJ  E<  J>;  J?B;I  7H;  FHE9;IIEHI  .>;C;CEHOJ?B;I7H;FB79;:?DJ>;C?::B;E<;79> 8EH:;HE< J>;  C;I>  D;JMEHA  I;; ?=KH;    9ECF7H78B;  JE  J>; .?B;H7 .?B;56M>?9>>7IC;CEHO?DJ;H<79;I  .>; <EBBEM?D=B?IJ M?BB :;I9H?8; J>;87I?9D;JMEHA7H9>?	 J;9JKH;IE<M>?9>I?CKB7J?EDH;IKBJIM?BB8;I>EMD8;BEM  ( .>;  D;JMEHA 7H9>?J;9JKH;  :;DEJ;:  7I (;I> ?D J>;J;NJ7D::?7=H7CI8;BEMKI;IIJH?9JEH:;H?D=? ; ?J  $  $   $  $  $  $  $  $  $   $  $  $   $  $  $  $  $  $  $   $  $ ?=KH;);JMEHA7H9>?J;9JKH;7D:C7FF?D=E<J?B;I 21  	 	 	   ;--,8""6(*,5-,:=5819 !5;:,8 9 (4+,:= 5819 --,*:95-+(6:,+""=0:*/04.(4+ 25=54:852;4*:0549 ' 9 : 0 2 &  , * ( 6 9  8 , ;  	  	       8  / 9 , / 9 , ;  : >   8 / 9 , ;  : >   8 ;  : / 9 , >  / 9 , ;  : >  / 9 , ,9/62 ,9/ 862 ,9/62	 ,9/ 862	   	 	 	 	  	    ' 9 4 &  ? * 4 , ( : ?=KH;K <<;H IF79;H;GK?H;C;DJ E<J>;D;JMEHA 7H9>?J;9JKH;I   	 	      ?I?CFB;C;DJ;:M?J>L?HJK7B9>7DD;BIJEI;F7H7J;J>; JMEC;II7=;JOF;I   (' .>?I  D;JMEHA  ?I  ?CFB;C;DJ;:  M?J>  EKH )E :;7:BE9A H;9EL;HOI9>;C;  .>;D7C;;NJ;DI?ED .?'   :;DEJ;I:?<<;H;DJ J?C?D=L7BK;I<EH J>; :;7:BE9AH;9EL;HOI9>;C;7II>EMD?D.78B;   (-)	* -7C; 7I (;I>  ;N9;FJ  J>7J  J>; I?P; E< J>;HEKJ;HI?DFKJ7D:EKJFKJ8K<<;HGK;K;I7H;;NJ;D: 	 ;:JE<B?JI?DI?P;   ('-)	* -7C;  7I  D;JMEHA  7H9>?J;9JKH; (;I>H.?EDBOJ>;HEKJ;HI8K<<;H GK;K;I7H;;NJ;D:	 ;:  JE    <B?JI   !EM;L;H  J>;  GK;K;I  E<  J>;  :;7:BE9A 8K<<;H7H;DEJ;NJ;D:;:   (-)	*-7C;7I (;I>NJK<;N9;FJ J>7JJ>; HEKJ;H8K<<;HGK;K;I7H;;NJ;D:;:JE<B?JI   ('-)	*,EKJ;H8K<<;HGK;K;I7H;;NJ;D: 	 ;:JE<B?JI  .>;7H;79EIJIE<J>;JME7DJ?	:;7:BE9AIEBKJ?EDIJ>7JM; M7DJ  JE  9ECF7H;  97D  7FFHEN?C7J;BO  8;  :;J;HC?D;:  8O 7::?D=KFJ>;H;GK?H;:8K<<;HIF79;?D<B?JI<EH J>;799EH:?D= D;JMEHAI  ?=KH;   :;F?9JI  IK9>  7  9ECF7H?IED  <EH  J>;  D;J	 MEHAI =?L;D  ?D J>;B?IJ 78EL;   ""J I>EMI  J>;C7@EH 7:L7DJ7=; E<  :;7:BE9A  H;9EL;HO  9ECF7H;:  JE  IJH?9J  EH:;H?D=  J>;  I7L	 ?D=I  ?D  H;GK?H;:  8K<<;H  IF79;  7H;  7BCEIJ      -?D9;  J>; DKC8;H E<L?HJK7B 9>7DD;BI?D7D;JMEHA?CFB;C;DJ?D=IJH?9J EH:;H?D=  =HEMI M?J>  J>;  DKC8;H  E<  :;F;D:;DJ C;II7=;I  ?D J>;9ECCKD?97J?ED FHEJE9EB  J>;:?<<;H;D9;  ?D H;GK?H;:9>?F 7H;7 M?BB  8;  ;L;D  >?=>;H  <EH  7FFB?97J?EDI  KI?D= CEH;  9EC	 FB;NFHEJE9EBI  !EM;L;H  J>;I; I7L?D=I ?D  J;HCI E< 9>?F 7H;7 >7L;  JE8; F7?:8O7  H;:K9J?ED E< J>; D;JMEHAI J>HEK=>FKJ M>?9> M?BB 8;I>EMD?DJ>;<EBBEM?D=?DL;IJ?=7J?EDI   )#'*'+%/''	 $,. 0&#,""#""),!(.#!,2#.4 "" $)""# ( !$ ))$# '($!. !( 	 $'$ . !( . . . .      20:,4,8(:054!(:,&- 20:9 *?*2,9 ' ?=KH;<<;9JIE<7:7FJ7J?EDE<IM?J9>?D=7D:<BEM9EDJHEB<KD9J?ED EDB7 J;D9O7D:J>HEK=>FK J<EH :?<<;H;DJ F79A ;J B;D=J>IFB  #""1!0',+,$#.$,.*+!# 4""-00',+,$3'0!&'+% +""),3,+0.,)1+!0',+ .>; <?HIJ ;NF;H?C;DJI E< M>?9> J> ; H;IKBJI 7H; I>EMD ?D ?=KH;   I>EKB: I>EM EDBO>EM  J>;  D;JMEHA  J>HEK=>FKJ  ?I 7<<;9J;:8OJ>;7:7FJ7J?EDIE<J>;IM?J9>?D=7D:<BEM9EDJHEB <KD9J?EDIJ>7J7H;D;9;II7HO<EHJ>;)E:;7:BE9AH;9EL;HO  .>;  79JK7B  :;7:BE9A  :;J;9J?ED 7D:  H;:?H;9J?ED  <KD9J?ED7B?JO ?I  DEJ  J;IJ;:  ?D  J>?I  I?CKB7J?ED  .>;H;<EH;  EDBO  JH7<<?9  8;	 JM;;D  J>; FHE9;IIEHI  ?IKI;:  J>7J  :E;I DEJ 9H ;7J; 7DO:;7:	 BE9AI:K;JEC;II7=;:;F;D:;D9?;I ?D J>;C;I> KI?D=:;7:	 BE9A  H;9EL;HO  .>;  J7H=;J  7::H;II;I  E<  J>?I  ?DJ;H  FHE9;IIEH JH7<<?97H;KD?<EHCBO:?IJH?8KJ;:  ?=KH;   I>EMI  J>;  H;IKBJI  E<  JME  I?CKB7J?EDI  <EH  ;79> D;JMEHA 7H9>?J;9JKH;  ?D M>?9>  J>; F79A;JI E<  J>;  ?DJ;H FHE	 9;IIEH  JH7<<?9  >7L;  :?<<;H;DJ  I?P;I   NFB  :;DEJ;I  7  F79A;J B;D=J>E<<B?JI<EH D;JMEHANNFB7F79A;JB;D=J>E< <B?JI  .>;  H;IKBJI  I>EM  7  BEM;H  J>HEK=>FKJ  <EH  J>;  D;JMEHA M?J>:;7:BE9AH;9EL;HO  .>?IM7I;NF;9J;: 7IJ>;;<<;9J?L;	 BO KI78B;  8K<<;H  I?P; M7I  H;:K9;:  8O  J>;  7:7FJ7J?ED  E<  J>; IM?J9>?D=7D: <BEM9EDJHEB <KD9J?EDI .>; H;B7J?L; H;:K9J?ED E<  J>HEK=>FKJ  ?I  B7H=;H  <EH  IC7BB  F79A;JI  I?D9;  EDBO  J>; 8K<<;H IF79;  BE97J;: 8;>?D:7  F79A;JI  J7?B  <B?J  ?D 7  GK;K;  ?I KDKI78B; EHB7H=;F79A;JI J>;H;:K9J?ED ?IH;B7J?L;BOIC7BB :K;  JE J>;  BEM;H  DKC8;H  E<  J7?B  <B?JI  J>7J  7H;  FH;I;DJ  ?D  J>; D;JMEHA EH  BEM J>HEK=>FKJ  J>; ?D9H;7I; E< B7J;D9O ?IEDBO IC7BB<EHJ>;D;JMEHA('   2)10',+,$""'$$#.#+0#""),!(#!,2#.4'*'+%/ .>;?DL;IJ?=7J?EDFH;I;DJ;:?D J>?II;9J?ED M? BB I>EM J>; ?D<BK;D9;  E<  :?<<;H;DJ  J?C?D=I  E<  J>;  :;7:BE9A  H;9EL;HO I9>;C; EH7BBJ>;<EBBEM?D=?DL;IJ?=7J?EDI7BIE?DJ>;I;9	 J?EDI8;BEMJME9B7II;IE<JH7<<?97H;KI;:  ""DJ;H FHE9;IIEH JH7<<?9 +79A;JI8;JM;;D J>;FHE9;IIEH J?B;I  .>;  :;IJ?D7J?ED  7::H;II;I  E<  J>;I;  F79A;JI  7H; KD?<EHCBO  :?IJH?8KJ;:   .>;  ?DJ;H  FHE9;IIEH  JH7<<?9  ?I I;J JE7H7J;E< <B?JI9O9B;M>?9> ;GK7BIJE E< J>; C7N?CKC  J>HEK=>FKJ  E< J>?I KD?<EHCBO :?IJH?8KJ	 ;:JH7<<?99B7II  22                (;CEHOJH7<<?9EDI?IJIE<J>;H;GK;IJF79A;JIE<J>; FHE9;IIEH  J?B;I  JE  J>;  C;CEH?;I  7D:  J>;?H  H;IFEDI;I 879AJEH;GK;IJ?D=FHE9;IIEH .>;H;GK;IJF79A;J 9ED	 I?IJI  E<    <B?JI  7D:  J>;  H;IFEDI;  F79A;J  ?I    <B?JI  ?D I?P;  ""<DEJ IJ7J;:EJ>;HM?I; ;79>FHE9;IIEH I;D:IJ>; I7C;  I>7H;  E<  H;GK;IJ  F79A;JI  JE  ;79>  E<  J>; C;CE	 H?;IKD?<EHCBO:?IJH?8KJ;::;IJ?D7J?EDI  ?=KH;I>EMIJ>;J>HEK=>FKJ7JJ>;C;CEHOJ?B;<EH J>; D;JMEHAI (;I>  7D: (;I>H.?  <EH :?<<;H;DJ  J?C?D=I  E<  J>; :;7:BE9A  H;9EL;HO  I9>;C;  .>;  C;CEHO  J>HEK=>FKJ  ?I  7I	 IKC;:7IJ>;I;D:H7J;E<H;IFEDI;<B?JIE<ED;E<J>;C;CE	 H?;I  ?D  J>;  IOIJ;C   ""D ?=KH;   J>?I  J>HEK=>FKJ  ?I  :;F?9J;: EL;H  J>; I;D: H7J; E<  H;GK;IJ  <B?JI  F;H FHE9;IIEH  J?B; F;H  9O	 9B; -?D9;FHE9;IIEHI7H;I;D:?D=H;GK;IJIJEC;CEH?;I J>;H;GK;IJI;D:H7J;E<ED ;FHE9;IIEH?IGK?J;BEM  ""J97D 8; I;;D  J>7J 7J <?HIJ J>;C;CEHO J>HEK=>FKJE<J>; ('  D;JMEHAI H?I;I B?A;  J>; J>HEK=>FKJ E<  J>; (  !EM;L;H  78EL;  7  9;HJ7?D  I;D:  H7J;  E<  H;GK;IJI  :;7:BE9AI IJ7HJ JEE99KH 7D: J>;C;CEHOJ>HEK=>FKJ :;9H;7I;I  .>?I?I :K;JEJ>;:;7:BE9AIJ>7J8BE9AJ>;7<<;9J;:F7HJIE< J>;D;J	 MEHA  KDJ?B  J>;O 7H;  :;J;9J;:  !EM;L;H  J>;  879A	E<<  F;H?E: ?D?J?7J;: 8O;79> :;7:BE9A  7BIE J7A;I  ?JI JEBB   ""D9H;7I?D=  J>; H;GK;IJ=;D;H7J?EDH7J;;L;DCEH;B;7:IJE7>?=>;H<H;GK;D 	 9OE<:;7:BE9AI7D:7I?DA?D=C;CEHOJ>HEK=>FKJ .>;H;7	 IED ?I J>7JM?J>>?=>;H JH7<<?9M>?9> C;7DICEH;F79A;JI ?D J>;D;JMEHAJ>;FHE878?B?JO<EH:;7:BE9AI?D9H;7I;I  .>;F;H<EHC7D9;E<J>;:;7:BE9AH;9EL;HOI9>;C;IM?J> :?<<;H;DJ  J?C?D=I 97D  7BIE 8;  I;;D  ?D  J>;  J>HEK=>FKJ =H7F>I ?D ?=KH;    .?C?D= .  >7I  7  L;HO BEM  :;7:BE9A  :;J;9J?ED J>H;I>EB:  7D:  879A	E<<  J?C;  .>?I  B;7:I  JE  87:  J>HEK=>FKJ <?=KH;I  :K;  JE J>;  >?=>  DKC8;H  E<  F79A;JI M>?9>  79JK7BBO 7H;DEJF7HJE<7:;7:BE9A 8KJ 7H;H;:?H;9J;:  .>KI8BE9A?D= J>;  :;7:BE9A  9>7DD;B  <EH  H;7BBO  :;7:BE9A;:  F79A;JI   .>; I;9ED: H;7IED ?IJ>;BEM879A	E<<J?C; J>7J ?IDEJIK<<?9?;DJ JE<H;;J>;D;JMEHAE<F79A;JI7<J;H7:;7:BE9A>7IE99KHH;:  .>;8;IJ 9ECFHEC?I;I;;C JE8; J?C?D=I.EH. M?J>. I>EM?D= 7 8;JJ;H C;CEHO J>HEK=>FKJ 7J 7  H;GK;IJ I;D:  H7J; E<  ""D9EDJH7IJ.I>EMI78;JJ;HF;H<EHC7D9;7J>?=>;H H;GK;IJ=;D;H7J?EDH7J;I:K;JE?JI>?=>;H879A	E<<J?C;  !EM;L;H  7I  ;NF;9J;:  J>;  C;CEHO  J>HEK=>FKJ  E<  D;J	 MEHAIKI?D=:;7:BE9AH;9EL;HO?I9B;7HBO8;BEMJ>;J>HEK=> 	 FKJ E<J>;C;I>KI?D=IJH?9JEH:;H?D=  ?HIJ J>?I ?I:K;JE J>; 7:7FJ;:  IM?J9>?D=  7D:  <BEM	9EDJHEB  <KD9J?EDI   -;9ED:  J>; D;JMEHA  J>HEK=>FKJ  E<  7  D;JMEHA  I?=D?<?97DJBO  ?D9H;7I;I M?J>7::?J?ED7BL?HJK7B9>7DD;BI .>?I=7?D?I <EH;N7CFB;KF JE<EHL?HJK7B9>7DD;BIJ>7J97D8;KI;:?D:;F;D:;DJ 	 BO E< C;II7=;  JOF;I  56  EH  L?HJK7B 9>7DD;BI  :;:?97J;:  JE C;II7=;JOF;IJ>?I=7?D?IIC7BB;H8KJIJ?BB;N?IJ;DJ  HEC  J>;I; I?CKB7J?EDIM; 97D 9ED9BK:;  J>7J )EI KI	 ?D=  :;7:BE9A  H;9EL;HO  I>EKB:  DEJ  8;  EF;H7J;:  F;HC7D;DJBO 78EL;J>;?HC7N?CKCJ>HEK=>FKJ7IJ>;>;H;8O97KI;:>?=> :;7:BE9AH7J;M?BB79JK7BBOH;:K9;J>;FEII?8B;J>HEK=>FKJ   ,!)'5#""#*,.4!!#//.$$'! .>; 7IIKCFJ?ED E< KD?<EHCBO:?IJH?8KJ;:  H;GK;IJI E<  J>; FHE9;IIEHI  JE  J>;  C;CEH?;I  ?I  DEJ  L;HO  H;7B?IJ?9   ""D  7  H;7B IOIJ;C  ?J  MEKB:  8;  :;I?H78B;  <EH  J>;  7FFB?97J?EDI  F;H<EH 	 C7D9;  7D:  ;D;H=O  9EIJ  J>7J  J>;  FHE9;IIEHI  KI;  J> ;  D;7H;H C;CEH?;ICEH;J>7DJ>;C;CEH?;I<7HJ>;H7M7O  !;H;8O J>; ,358?#/85;./6;: ,9/ ,9/8# 	 ,9/8#   ,9/8#  ,9/8#        	  ? 8 5 3 , 3  (   5  9 : 0 2  , 9 4 5 6 9 , 8  5  , : ( 8  + 4 , ""   	 	   !,7;,9 :- 20:.,4,8 (:0548 (:,5- 54,685* ,9 9 58 ?=KH; .>HEK=>FK JE<IJH?9J EH:;H?D=9ECF7H;:JE:;7:BE9AH;9EL;HO M?J>:?<<;H;DJJ?C?D=I:;7:BE9A:;J;9J?ED879A 	E<< /&.#,#*' +#03,.('/0+!#  ),!)'50',+!0, . :?IJ7D9;?IC;7IKH;:?D>EFIHEKJ;HI8;JM;;DJ>;FHE9;IIEH 7D:J>;C;CEHO  ""DJ>;<EBBEM?D=I?CKB7J?EDI($ ('$ J>; H;GK;IJ  JH7<<?9 M?BB 8;  BE97B?P;:  .>?IC;7DI FHE9;IIEHI I;D:CEH;H;GK;IJIJED;7H;HC;CEH?;I7D:B;IIJEC;CEH?;I BE97J;:  <7HJ>;H 7M7O M>?B; J>; JEJ7B  H;GK;IJ I;D: H7J;IJ7OI 9ED IJ7DJ .>;:;=H;;E<J>; BE97B?P7J?ED ? ;  J>;I>7H;E<J>; F79A;JI  J>7J  ?I I;DJ  JED;7H;H C;CEH?;I  ?I :;I9H?8;: 8O J>; BE97B?P7J?ED  <79JEH  >?=>;H  BE97B?P7J?ED  <79JEH C;7DICEH; F79A;JI  7H;  I;DJ  JE  J>;  9BEI;H  C;CEHO  .>;  :?IJH?8KJ?ED  E< J>; I>7H;I E< F79A;JI I;DJ  JE7 C;CEHO <HEC 7 FHE9;IIEH  ?I 97B9KB7J;::;F;D:?D=ED  J>;D;JMEHA:?IJ7D9;  JE J>;C;CE	 HO'799EH:?D=JEJ>;<EBBEM?D=<EHCKB7   -?CKB7J?ED  H;IKBJI 7H;I>EMD  <EH 7  BE97B?P7J?ED  <79JEH E<   ($  ('$  7D:    ($ ('$ .>;J7H=;J7::H;II;IE<J>;?DJ;HFHE9;IIEH JH7<<?9M?BBIJ?BB8;KD?<EHCBO:?IJH?8KJ;: .>;H;IKBJIE<J>;I; I?CKB7J?EDI7H;I>EMD?D?=KH;  1>?B;  J>;  D;JMEHAI  KI?D=  J>;  )E  :;7:BE9A  H;9EL;HO ('$FHE<?J <HEC 7D?D9H;7I?D=BE97B?P7J?EDE< J>; JH7<<?9:K;JE7H;:K9;::;7:BE9AE99KHH;D9;H7J;J>;C;CE	 HO J>HEK=>FKJ  E<  J>; C;I>  KI?D=  IJH?9J  EH:;H?D= ($ I?DAIM?J>?D9H;7I?D=BE97B?P7J?ED .>;H;7IED?IJ>7JJ>;KD?	 <EHCBO:?IJH?8KJ;:H;GK;IJJH7<<?9B;7:IJE7=EE::?IJH?8KJ?ED E<  J>;  JH7<<?9  ED  7BB  J>;  HEKJ;HI  7BED=  J>;  HEM  E<  HEKJ;HI M>;H; J>;C;CEH?;I ED  JEF 7D:8EJJEC 7H;FB79;:  !EM;L	 ;HJ>;BE97B?P7J?ED E<J>;C;CEHO JH7<<?9 7D:J>;IJ7J?9HEKJ	 ?D=  7B=EH?J>C  D7C;BO  23	HEKJ?D=  B;7:  JE  CEH;  JH7<<?9  ?D J>;  HEKJ;HI  :?H;9JBO 7HEKD:  J>; C;CEH?;I  7D:  B;II  JH7<<?9  ?D J>;  <7HJ>;H  7M7O  HEKJ;HI   .>;I;  HEKJ;HI  IK<<;H?D=  <HEC  ?D	 9H;7I;:BE7: J>;D8;9EC; J>;8EJJB;D;9A7D:J>;H;7IED <EH J>;H;:K9;:D;JMEHAJ>HEK=>FKJ   $$#!0,$1$$#.'5#/,+#*,.4&.,1%&-10 ""D  J>?I  I;9J?ED  J>;  ;<<;9J  E<  :?<<;H;DJ  ?DFKJ  7D:  EKJFKJ 8K<<;H I?P;IEDJ>;D;JMEHAJ>HEK=>FKJI>EKB:8;?DL;IJ?=7J	 ;: EHJ>?IFKHFEI;J>;D;JMEHAI($-)	*( 23 ,358?#/85;./6;: ? 8 5 3 ,9/ ,9/5* ,9/5*	 #	 #	5*  #	5* 	 , 3  (  5  9 : 0 2  , 9 4 5 6 9 , 8  5  , : ( 8  + 4 , "" ,358?#/85;./6;: ,9/5*	 ,9  /5*	>: ;,9  /5*	>: ;-   #	5*	 #	5*	> :;#	5*	> : ;-          	       	   	 	     	 	   ? 8 5 3 , 3  (  5  9 : 0 2  , 9 4 5 6 9 , 8  5  , : ( 8  + 4 , "" !,7;,9 :- 20:.,4,8 (: 0548(:,5- 54,68 5* ,9 9 58 &- 20:9 * ? * 2,' !,7;,9 :- 20:.,4,8 (: 0548(:,5- 54,685* ,9 9 58 &- 20:9 * ? * 2,' ?=KH;);JMEHAJ>HEK=>FKJM?J>KD?<EHC7D:BE97 B?P;:C;CEHOJH7 <<?9 ?=K H; );JMEHA7H9>?J;9JKH;IM?J>8K <<;HIE<7D:<B?JII?P; '$-)	*($-)	*7D:(' $-)	* M?J>J>;?H;NJ;D:;: HEKJ;H8K<<;HI7H;KI;:  .>;JH7<<?9KI;:?DJ>?I?DL;IJ?=7J?ED?IJ> ;I7C;?DJ>;FH;L?	 EKI I;9J?ED   ? ;   J>;  ?DJ;H FHE9;IIEH  JH7<<?9  ?I  IJ?BB  KD?<EHCBO :?IJH?8KJ;:M>?B;J>;C;CEHOJH7<<?9?I7=7?DBE97B?P;:  ?=KH;I>EMIJ>;I?CKB7J?EDH;IKBJI<EHJ>;I;D;JMEHA 7H9>?J;9JKH;I 8EJ> D;JMEHA7H9>?J;9JKH;IFHE<?J  <HEC  J>;  ?D	 9H;7I;  E<  8K<<;H  IF79;  <HEC    <B?JI  JE    <B?JI   !EM;L;H  J>; =7?D  E<  J>; ('$-)	* ?I  ;L;D  7  B?JJB;  >?=>;H J>7D  E< ($-)	*  I?D9;  7<J;H  J>;  ;NJ;DI?ED  E<  J>; 8K<<;HIJ>;OI>EM7FFHEN?C7J;BO;GK7BC;CEHOJ>HEK=>FKJ  1>;D  9ECF7H?D=  JME  D;JMEHA  7H9>?J;9JKH;I  J>7J  >7L; 7FFHEN?C7J;BO  J>;  I7C;  8K<<;H  IF79;  H;GK?H;C;DJI  D7C;BO ('$-)	* 7D: ($ I;; ?=KH;  M; 97D  I;;  J>7J  J>;  D;JMEHA  M? J>  :;7:BE9A  H;9EL;HO  FHEL?:;I >?=>;H  J>HEK=>FKJ  .>;  <KHJ>;H  ?D9H;7I;  E<  J>;  HEKJ;H  I?P;I <HEC    JE   <B?JI  F;H  8K<<;H  GK;K;  :E;I  EDBO B;7:  JE  >?=>;H J>HEK=>FKJ  <EH L;HO>?=> H;GK;IJ =;D;H7J?ED H7J;I M>?9>  B?; 78EL;J>;F;7AC;CEHOJ>HEK=>FKJ  0  *)'/- ""*) .>?I IJK:OI>EMI 7 9ECF7H?IED 8;JM;;D D;JMEHA 7H9>?	 J;9JKH;I  KI?D= IJH?9J EH:;H?D=  ? ;   :;7:BE9A  7LE?:7D9;  7D:  7 )E  :;7:BE9A  H;9EL;HO  I9> ;C;  ?D  EH:;H  JE  7FFHE79>  J>; FHE8B;C  E<  C;II7=;  :;F;D:;DJ  :;7:BE9AI  .>; )E  :;7:	 BE9A  H;9EL;HO  9ED 9;FJ  ?I  87I;:  ED  7  :;7:BE9A  H;9EL;HO I9>;C;  <HEC  J>;  <?;B:  E<  F7H7BB;B  9ECFKJ;H  D;JMEHAI  7D: M7I7:7FJ;:JE)EI<EHJ>?IIJK:O  .>;  ?DL;IJ?=7J?EDI  I>EM  7  I?=D?<?97DJ  I7L?D=  ?D  8K<<;H IF79;  <EH  J>; )E  :;7:BE9A  H;9EL;HOI9>;C;  J>7J  ?I  KF JE <EHD;JMEHAIM?J>:;F;D:;DJC;II7=;I EH D;JMEHAI M?J> CEH; :;F;D:;DJ C;II7=;I  J>; I7L?D=I ?D 8K<<;H IF79; 7D:J>KI7BIE?D9>?F7H;7 M?BB8;;L;D>?=>;H !EM;L;HJ>?I 9EIJ  H;:K9J?ED >7I  JE 8;  F7?: 8O H;:K9;:  D;JMEHA  J>HEK=>	 FKJ M>?9>  ?I  I?=D?<?97DJ  <EH  KD?<EHCBO :?IJH?8KJ;: C;CEHO H;GK;IJ  JH7<<?9  !EM;L;H <EH CEH; BE97B?P;:C;CEHO799;II JH7<<?9 J>;:?<<;H ;D9; ?DJ>HEK=>FKJ  ?I:;9H;7I?D=M?J>7D?D 	 9H;7I?D=BE97B?P7J?ED:;=H;;  $)*1' ()..>?IMEHA>7I8;;D <KD:;:8OJ>; ;HC7D;:;H7B(?D	 ?IJHOE<:K97J?ED 7D:,;I;7H9> (KD:;H =H7DJ  H;<;H	 ;D9;  (  ,7F?:(+-E  M?J>?D  J>;  ,;I;7H9>  +HE	 =H7CC;"".  ,,)56 .E8?7I @;HH;=77H: 7D: ->7DA7H (7>7:;L7D  IKHL;O  E<  H;I;7H9>  7D: FH79J?9;IE<);JMEHA 	ED	9>?F(ECFKJ -KHL 0EB(+H;II  56 N;B  #7DJI9>  7D:  !7DDK .;D>KD;D  ;J  7B   );JMEHAI  ED  9>?F  $BKM;H 97:;C?9+K8B?I>;HI!?D=>7C(/- 56'K97;D?D? 7D: ?EL7DD? ;(?9>;B? );JMEHA IED>?FI );M-E +7H7:?=CECFK J;H0EB""ECFK J;H -E9?;JO 56  1   #   7BBO  7D:     '   -;?JP  ;7:BE9A	<H;;  C;II7=;  HEK J?D=  ?D CK BJ?FHE9;IIEH ?DJ;H9EDD;9J?EDD;JMEHA I"" .H7DI ECFKJ 0EB   ""ECFKJ;H -E9?;JO 56  !7DDIIED D:H;7I  7D:  EEII;DI D:H;7 I  7D:  ,7:KB;I9K  D:H;7 I LE?:?D=(;II7=;	;F;D:;DJ ;7:BE9A?D);JMEHA 	7 I;:-OIJ;CIED >?F0'-"";I?=D! ?D:7M?+K8B?I>?D=EHFEH7J?ED 56 1;DJPB7 <<7L?: 7D: H?<<?D+7JH?9A  7D:!E<<C7DD!;DHO ;J  7B  *D	 >?F  ""DJ;H9EDD;9J?ED H9>?J;9JKH;  E<  J>; .?B; +HE9;IIEH  "" (?9HE 0EB -;FJ  	*9J  56 ;8H;C?9>7;B	.;I<7=?EH=?I  077D:H7=;H 1  4>7D=(  EEII;DI $  , ?@FA ;C7   ,K7:KB;I9K  ;7:BE9A +H;L;DJ?ED ?DI9SJ>;H;7B +HEJE9EB EHH;9J >7H:M7H;:;I?=D7D:L;H ?<?97J?EDC;J>E:I J>""""+ 1  7:L7D9;:H;I;7H9>MEHA ?D=9ED<;H;D9;!,(  56  $;;I  EEII;DI  7D:  #E>D  ?;B?II;D  7D: D:H;?  ,7:KB;I9K J>;H;7B );JMEHAED>?F ED9;FJIH9>?J;9JKH;I7D:""CFB;C;DJ7J?EDI"" ;I .;IJ0EB "" ECFKJ;H-E9?;JO+H;II 56  KH7D: 3   ;HD7H:     7D: '7 JJ7H:    /-. *D	9>?F  :?IJH?8KJ;: 7H9>?J;9JKH;  <EH  7    87I;87D: CE:;C -E   ""D  ;I?=D    ,;K I;  ""+	 -E ""ECFK J;H-E9?;JO+H;II0EB  56  3ED=  !E  -ED=  7D:  +?DA IJED  .   (       FHE=H;II?L;  7FFHE79>  JE >7D:B?D=  C;II7=;	:;F;D:;DJ  :;7:BE9A  ?D  F7H7BB;B  9ECFK J;H  IOIJ;CI "".H7DIED+7H7BB;B 7D:?IJH?8K J;:-OIJ;CI0EB 56D@7D$ 0 7D:+?DAIJED. ( D;<<?9?;DJ<K BBO7:7FJ?L;:;7:BE9A H;9EL;HO  I9>;C;  ""-!  +HE9   D:  DDK7 B  ""DJ;HD7J?ED7 B -OCFEI?KCEDECFKJ;H H9>?J;9JKH; 56'?ED;B ( )?7D:+>?B?F$ (9$?DB;O-K HL;OE<1EHC>EB;,EK J?D= .;9>D?GK ;I  ?D  ?H;9J  );JMEHA I  ECFKJ;H  0EB    ""  ECFK J;H -E9?;JO+H;II 56 +7 HJ>7 +H7 J?C +7D:;  7D:  H;9K  7D: #ED;I (  ;J  7B  +;H<EHC7D9; ;L7BK7J?ED  7D:  :;I?=D  JH7:;	E <<I  <EH  D;JMEHA 	ED	9>?F  ?DJ;H9EDD;9J 7H9>?J;9JKH;IECFKJ;HI"".H7DI79J?EDIED0EB 24 "
Ultra Fine-Grained Run-Time Power Gating of On-chip Routers for CMPs.,"This paper proposes an ultra fine-grained run-time power gating of on-chip router, in which power supply to each router component (e.g., VC queue, crossbar MUX, and output latch) can be individually controlled in response to the applied workload. As only the router components which are just transferring a packet are activated, the leakage power of the on-chip network can be reduced to the near-optimal level. However, a certain amount of wakeup latency is required to activate the sleeping components, and the application performance will be degraded. In this paper, we estimate the wakeup latency for each component based on circuit simulations using a 65 nm process. Then we propose four early wakeup methods to overcome the wakeup latency. The proposed router with the early wakeup methods is evaluated in terms of the application performance, area, and leakage power. As a result, it reduces the leakage power by 78.9%, at the expense of the 4.3% area and 4.0% performance when we assume a 1 GHz operation.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip Ultra Fine-Grained Run-Time Power Gating of On-Chip Routers for CMPs ∗ Hiroki Matsutani1 , Michihiro Koibuchi2 , Daisuke Ikebuchi3 , Kimiyoshi Usami4 , Hiroshi Nakamura1 , and Hideharu Amano3 1The University of Tokyo {matutani,nakamura}@hal.rcast.u-tokyo.ac.jp 4-6-1 Komaba, Meguro-ku, Tokyo, Japan 3Keio University 3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Japan {ikebuchi,hunga}@am.ics.keio.ac.jp 2National Institute of Informatics 2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan koibuchi@nii.ac.jp 4Shibaura Institute of Technology 3-7-5 Toyosu, Kohtoh-ku, Tokyo, Japan usami@shibaura-it.ac.jp Abstract This paper proposes an ultra ﬁne-grained run-time power gating of on-chip router, in which power supply to each router component (e.g., VC queue, crossbar MUX, and output latch) can be individually controlled in response to the applied workload. As only the router components which are just transferring a packet are activated, the leakage power of the on-chip network can be reduced to the near-optimal level. However, a certain amount of wakeup latency is required to activate the sleeping components, and the application performance will be degraded. In this paper, we estimate the wakeup latency for each component based on circuit simulations using a 65nm process. Then we propose four early wakeup methods to overcome the wakeup latency. The proposed router with the early wakeup methods is evaluated in terms of the application performance, area, and leakage power. As a result, it reduces the leakage power by 78.9%, at the expense of the 4.3% area and 4.0% performance when we assume a 1GHz operation. 1 Introduction Recently, Network-on-Chips (NoCs) have been used in chip multi-processors (CMPs) to connect a number of processors and cache memories on a single chip, instead of traditional bus-based on-chip interconnects that suffer the poor scalability. Figure 1 illustrates an example of CMP inspired by [2], in which eight processors (or CPUs) and 64 L2 cache banks are interconnected by sixteen on-chip routers. These cache banks are shared by all processors and thus a cache coherence protocol is running on the CMP. NoCs can be evaluated from various aspects, such as the ∗ This research was performed by the authors for STARC as part of the Japanese Ministry of Economy, Trade and Industry sponsored “NextGeneration Circuit Architecture Technical Development” program. The authors thank to VLSI Design and Education Center (VDEC) and Japan Science and Technology Agency (JST) CREST for their support. This work was also supported by Grant-in-Aid for JSPS Fellows. 978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.16 61 CPU0 CPU7 CPU1 CPU2 CPU6 CPU5 CPU3 CPU4 L1 D$ I$ L2$ bank On-chip router Figure 1. Example of 8-core CMP throughput, communication latency, hardware amount, and power consumption, but especially those used in CMPs are required to reduce the communication latency and power consumption. The communication latency is the primary performance factor, since it directly increases the cache access latency that affects the application performance on CMPs. As for the cost factor, the power consumption is becoming more and more important in almost all systems, since it affects the heat dissipation, packaging, and running costs of the system. The power consumption is classiﬁed into dynamic switching power and static leakage power. The switching power is consumed only when packets are transferred on a NoC, while the leakage power (or static power) is consumed without any packet transfers as long as the NoC is powered on. Since the NoC is the communication infrastructure of CMPs, it must be always ready for the packet transfers at any workload so as not to increase the communication latency; thus a run-time power management that dynamically stops the leakage current whenever possible is highly required. In this paper, we propose an ultra ﬁne-grained run-time power gating of on-chip router, in which power supply to each router component (e.g., VC queue, crossbar MUX, and output latch) can be individually controlled in response to the applied workload. As only the router components which are just transferring a packet are activated, the leakage power of the on-chip network can be reduced to the near-optimal level. However, such techniques inherently increase the communication latency and degrade the application performance, since a certain amount of wakeup latency is required to activate the sleeping components. To mitigate the wakeup latency, we propose four early wakeup methods that can preliminarily detect the next packet arrival and activate the corresponding components. The proposed on-chip router with the early wakeup methods is evaluated in terms of the application performance, area, and leakage power. The rest of this paper is organized as follows. Section 2 surveys coarse-grained and ﬁne-grained power gating techniques. Section 3 designs the ﬁne-grained power gating router and Section 4 proposes four early wakeup methods. Section 5 evaluates the power gating router with the early wakeup methods. Finally, Section 7 concludes this paper. 2 Power Gating Techniques Power gating is a representative leakage-power reduction technique, which shuts off the power supply of idle circuit blocks by turning off (or on) the power switches which are inserted between the GND line and the blocks or between the VDD line and the blocks. This concept has been applied to circuit blocks with various granularities. Depending on the granularity of target circuit blocks (i.e., power domains), the power gating is classiﬁed into coarse-grained and ﬁnegrained approaches. Coarse-Grained Power Gating Each target circuit block is surrounded by a power/ground ring. Power switches are inserted between the core ring and power/ground IO cells. The power supply to the circuit block can be controlled by the power switches. Since the power supply to all cells inside the core ring is controlled at one time, this approach is well suited to the IP- or module-level power management. The coarse-grained approach has been popularly used, since its IP- or module-level power management is straightforward and easy to control. However, it typically imposes a microsecond order wakeup latency. Fine-Grained Power Gating This approach has received a lot of attention in recent years because of its ﬂexibility and short wakeup latency [6][13]. Although various types of ﬁnegrained power gating techniques have been proposed, we focus on the method proposed in [13]. In this method, customized standard cells, each of which has a virtual ground (VGND) port by expanding the original cell, are used. These standard cells that share the same active signal form a single micro power domain, by connecting their VGND ports to a shared local VGND line, as shown in Figure 2. Power switches are inserted between the VGND line and GND line to control the power supply to the micro power domain. Figure 2 illustrates two micro power domains, each of which has its own local VGND line and power switch. In this paper, we focus on the ﬁne-grained approach because it is more suitable for on-chip routers than the coarseMicro power domain #2 Micro power domain #1 VDD GND VGND #2 GND VDD Std. cell Std. cell PS Std. cell Std. cell ISO cell VGND #1 Std. cell Std. cell Std. cell Std. cell ISO cell PS Figure 2. Concept of the ﬁne-grained power gating. PS and ISO refer to a power switch and an isolation cell, respectively. grained one. Each input physical channel works independently of each other unless packet contentions with the other physical channels occur. In addition, all virtual channels in the same physical channel are not always used. Actually zero or only a few virtual channels are occupied at the same time in most cases. This indicates that a ﬁner-grained partitioning can exploit the spatial and temporal communication locality and increase the power gating opportunities. 3 Ultra Fine-Grained Power Gating Router Here, we propose an ultra ﬁne-grained power gating router. We ﬁrst show how an on-chip router is divided into a number of micro power domains. Then we implement these power domains using a 65nm process and evaluate them in terms of the area overhead and wakeup latency. 3.1 Power Domain Partitioning Figure 3(a) illustrates a typical input-buffered wormhole a p × p crossbar switch, and a round-robin arbiter that allorouter. The router has p input and output physical channels, cates a pair of output virtual and physical channels for each incoming packet. Each input physical channel has a separated buffer queue for each virtual channel, while each output physical channel has a single 1-ﬂit buffer or latch to decouple the switch and link delays. Figure 3(b) shows an input physical channel that supports v virtual channels. It has a routing computation unit and a vto-1 multiplexer (VCMUX) that selects only a single output from v virtual channels. Each virtual channel has a control logic, status registers, and an n-ﬂit buffer queue. Figure 3(c) shows a p × p crossbar switch. It is composed of p p-to-1 multiplexers (CBMUXes), each of which is controlled by a select signal from the arbiter. Prior to partitioning the on-chip router into a number of micro power domains, we estimate the gate count of each router component, since the leakage power is proportional to the device area. An RTL model of the router is designed. Its parameters p, v , and n are set to 5, 4, and 4, respectively. The ﬂit width w is set to 128-bit. 62 Input channels Arbiter Output channels port 1 port 2 port 3 port p CB (pxp) Route comp vc 1 VC state VCMUX idata vcsel vc 2 VC state vc v VC state odata port vcsel n-flit size port 1 port 2 port 3 port p portsel idata 1 idata 2 idata p odata 1 odata 2 odata p CBMUX (a) Router overview (b) Input physical channel (c) Crossbar switch Figure 3. Micro power domains of on-chip router. Each gray area denotes a power domain. Table 1. Hardware amount of each router component (before PS insertion) [kilo gates] Module 4-ﬂit VC buffer 1-ﬂit output latch 5-to-1 CBMUX 4-to-1 VCMUX Others Total Count Total gate count 20 111.06 5 5.49 5 4.91 5 4.21 1 16.92 142.58 Table 2. Hardware amount of each router component (after PS insertion) [kilo gates] Module 4-ﬂit VC buffer 1-ﬂit output latch 5-to-1 CBMUX 4-to-1 VCMUX Others Total Count 20 5 5 5 1 ISO 2.07 0.51 0.52 0.54 0 3.64 PS 2.25 0.16 0.02 0.02 0 2.44 Overhead 3.9% (15.4%) 12.2% (24.6%) 10.9% (23.3%) 13.3% (25.9%) 0% (11.1%) 4.3% (15.9%) Table 1 shows the gate count of each router component. In this table, “Others” include the gate counts of routing computation units, an arbiter, VC status registers, and the other control logic, but each of these components is quite small compared to the other components (their total area is only 11.9% of the router). Thus, these miscellaneous logics are removed from the power domain list in order to simplify the power gating router design. Consequently, the router area is divided into 35 power domains including VC buffers, Output latches, VCMUXes, and CBMUXes, which can cover 88.1% of the total router area. 3.2 Power Domain Implementation Here, we design all power domain types (i.e., VC buffer, Output latch, CBMUX, and VCMUX) in order to estimate their area overhead and wakeup latency. The following design ﬂow is used for all power domain types: 1) An RTL model of a power domain with an active signal is designed. 2) The RTL model is synthesized by the Synopsys Design Compiler. 3) Isolation cells are inserted to all output ports of the synthesized netlist in order to hold the output values of the domain when the power sup(a) 4-ﬂit VC buffer domain (b) 5-to-1 CBMUX domain Figure 4. Waveforms of circuit simulations ply is stopped. 4) The netlist with isolation cells is placed by the Synopsys Astro. 5) The virtual ground (VGND) lines are formed, and the power switches are inserted between the VGND and GND lines by the Sequence Design CoolPower. 6) The netlist with power switches is routed by the Synopsys Astro. 7) The previous two steps are performed again in order to optimize the VGND, power switch sizing, and routing. Using this design ﬂow, we can obtain layout data (GDS ﬁles) of all power domain types. Notice this ﬂow is fully automated; so an additional design complexity for the ultra ﬁne-grained power gating is small. Table 2 shows the area overhead of the isolation cells and power switches for each router component. In this table, ISO and PS show the total gate counts of isolation cells and power switches used in the router, respectively. In the Overhead column, a value without parentheses shows the area overhead of the isolation cells and power switches. The total area overhead of the power gating router is only 4.3%. In this design ﬂow, we used the customized standard cells each of which has a VGND port. We selected 106 cells from a commercial 65nm standard cell library and modiﬁed them to have a VGND port by expanding their cell height. In the Overhead column, a value with parentheses considers the area overhead of the customized standard cells against the original ones. In this case, the total area overhead is increased to 15.9% but it is still reasonable, since leakage current of these cells can be cut off when they are not activated. 63 ) d e z i l a m r o n ( e m i t n o i t u c e x E  2  1.5  1  0.5  0 2-cycle wakeup 3-cycle wakeup 4-cycle wakeup a b c d e f g Benchmark programs h i j Ave. this graph, 1.0 indicates the execution time without power gating (i.e., 0-cycle wakeup). As shown, the average execution time of these applications is increased by 23.2%, 35.3%, and 46.3% when the wakeup latency is two, three, and four cycles, respectively. Deﬁnitely, such performance penalty is unacceptable even if the leakage power is signiﬁcantly reduced. To mitigate or remove the wakeup latency, an early wakeup method that can preliminarily detect the next packet arrival and activate the corresponding components is thus essential. Figure 5. Execution time of SPLASH-2 benchmark without early wakeup methods. 4 Wakeup Control Methods 3.3 Wakeup Latency Estimation To estimate the wakeup latency of each power domain, the following steps are performed: 1) A spice netlist of the target power domain is extracted from the GDS ﬁle by the Cadence QRC Extraction. 2) The wakeup latency is measured based on circuit simulations of the spice netlist by the Synopsys HSIM. Figure 4 shows the measured waveforms of the VC buffer and the CBMUX domains. The waveforms of the output latch and VCMUX domains are omitted, since they are quite similar to those of the VC buffer and CBMUX domains, respectively. In each ﬁgure, the ﬁrst (top) and second waveforms show the lower two bits of the output (OUT[1] and OUT[0]). The third waveform shows the 1GHz clock signal and the fourth one shows the active signal. In these simulations, the lower two bits of input (IN[1] and IN[0]) are set to 1 and 0, respectively. Then, the active signal is asserted at the second rising edge of the clock. As a result, the output values of the VC buffer reach to the expected values within 2.8nsec, while those of the CBMUX take approximately 1.3nsec. In this paper, we assume the wakeup latency of each power domain is two, three, and four cycles when the target NoC is operated at 667MHz, 1GHz, and 1.33GHz, respectively. This assumption is a little bit conservative, since the actual wakeup latencies are less than 3nsec as mentioned above. 3.4 Wakeup Latency Impact NoCs on CMPs are quite latency sensitive, since certain communications (e.g., request and reply) are always required to access remote cache blocks. Thus, the wakeup latency of router components imposed by the power gating would degrade the application performance signiﬁcantly. To clearly show the negative impact of the wakeup latency, here we preliminarily evaluate the application performance on the CMP illustrated in Figure 1 without any early wakeup methods. Figure 5 shows the execution cycles of SPLASH-2 benchmark that includes (a) radix, (b) lu, (c) fft, (d) barnes, (e) ocean, (f) raytrace, (g) volrend, (h) water-nsquared, (i) waterspatial, and (j) fmm. As wakeup latencies, two, three, and four cycles are assumed. Here, we omit the detailed simulation parameters (we will describe them in Section 5). In This section proposes four wakeup control methods that mitigate the wakeup latency and improve the performance. 4.1 Naive Method Naive method wakes up each micro power domain of a router as early as possible without any special modiﬁcations to the router. It assumes a conventional three-cycle router, in which a header ﬂit is transferred through three pipeline stages that consist of the routing computation (RC) stage, the virtual channel and switch allocation (VSA) stage, and the switch traversal (ST) stage. Assuming a packet is transferred from Router (i − 1) to Router i, each micro power domain of Router i is woken up as follows. • VC buffer: Activation of an input VC buffer in Router i is triggered when a packet header in Router (i − 1) completes the VA and SA operations. • VCMUX: Activation of a VCMUX in Router i is triggered when a packet header in Router (i − 1) completes the SA operation, since the VCMUX is shared by all virtual channels in the same input physical channel (see Figure 3(b)). • CBMUX: Activation of all CBMUXes in Router i is triggered when a packet header in Router (i − 1) completes the SA operation. • Output latch: Activation of all Output latches in Router i is triggered when a packet header in Router (i − 1) completes the SA operation. (i − 1) until the activation of the VC buffer in Router i has In Naive method, every packet header must wait at Router been completed. This directly increases the communication latency and degrades the application performance. Also, it seems to be inefﬁcient that all CBMUXes and Output latches in Router i are woken up at the same time, although only a single pair of CBMUX and Output latch will be really used. However, routing computation in Router i must be completed before ﬁnding out which CBMUX and Output latch in Router i are really used. Since the activation requires multiple cycles (e.g., two cycles for 667MHz), it is difﬁcult to wake up only the necessary CBMUX and Output latch without any additional latency penalty. To mitigate or remove the above issues, the following sections propose more efﬁcient wakeup methods. 64     Wakeup signal to Router i is asserted HEAD NRC VSA ST NRC VSA ST NRC VSA ST DATA_0 DATA_1 DATA_2 SA ST SA ST SA SA ST ST SA ST SA SA ST ST SA ST SA ST 1 2 3 4 5 6 7 8 9 10 11 12 ELAPSED TIME [CYCLE] ROUTER (i-2) ROUTER (i-1) ROUTER i Figure 6. Router pipeline (Look-ahead) 4.2 Look-Ahead Method Look-ahead method wakes up each micro power domain as early as possible by the look-ahead routing [5][12] that can detect which input channel of two hops away will be used. Since an activation of a micro power domain is triggered prior to several cycles before packets reach to the domain, it can mitigate or remove the negative impact of the wakeup latency. Figure 6 illustrates how the look-ahead routing detects which input channel of two hops away will be used. In this hop. Assuming a packet is transferred from Router (i − 2) ﬁgure, NRC denotes the routing computation for the next to Router i via Router (i − 1), the NRC unit at Router (i − 2) computes the output channel of the next router (i.e., Router (i − 1)), instead of its own output channel. Since the output channel of Router (i − 1) is directly connected to an input channel of Router i, the NRC unit at Router (i − 2) can detect which output channel of Router (i − 1) and which input channel of Router i will be used. As shown in Figure 6, Router (i − 2) can trigger the activation of Router i when it completes its NRC. There is a (i − 2) until the packet actually reaches to Router i. ﬁve cycle margin after a packet completes the NRC of Router Activation of each micro power domain using Look-ahead method is summarized as follows. • VC buffer: Activation of an input VC buffer in Router i is triggered when a packet header in Router (i − 2) completes the NRC operation. • VCMUX: Activation of a VCMUX in Router i is triggered when a packet header in Router (i − 2) completes • CBMUX: Activation of a CBMUX in Router i is trigthe NRC operation. gered when a packet header in Router (i − 1) completes • Output latch: Activation of an Output latch in Router the NRC operation. i is triggered when a packet header in Router (i − 1) completes the NRC operation. Notice that Naive method wakes up all CBMUXes and Output latches at once without considering which output channel will be used, so as not to increase wakeup latency. Compared to Naive method, Look-ahead method is more efﬁcient, since it wakes up only a single pair of CBMUX and Output latch which will be used in several cycles later. An input channel (or NRC unit) of Router (i − 2) has to deliver the wakeup signal to the corresponding VC buffer and VCMUX of Router i. Also, the NRC unit of Router (i − 1) has to deliver the wakeup signal to the corresponding CBMUX and Output latch of Router i. To deliver these signals, a wakeup control network is needed. The wakeup signal spans the twice longer distance than a wire between two neighboring routers; thus an additional cycle would be required to deliver the wakeup signal, depending on the distance between two routers. Another difﬁculty of Look-ahead method is the wakeup control of the ﬁrst hop. We assume that the source network interface (source NI) can trigger the activation of the ﬁrst and second hops during the packetization. However, assuming the source NI triggers the activation of the ﬁrst hop one cycle ahead, it compensates only one cycle of the ﬁrst-hop wakeup latency but suffers the remaining latency. The i-th hop router can mitigate T i recover cycles of the wakeup latency. T i recover is calculated as follows. (cid:2) 2n − Twire − 1 i ≥ 2 T i recover = 1 i = 1 , (1) where n is the router pipeline depth (e.g., three stages) and Twire is the wire delay of a wakeup signal. Assuming n = 3 and Twire = 1, the second or farther hop routers can mitigate up to four cycles of the wakeup latency, though the ﬁrst hop mitigates only a single cycle. 4.3 Look-Ahead with Ever-On Method Look-ahead with ever-on method is an extension of the original Look-ahead method to mitigate the wakeup latency of the ﬁrst hop. In the ever-on method, VC buffers which are activated frequently as the ﬁrst hop are selected as “ever-on” domains, and their power supplies are never stopped. The ever-on domains must be selected quite carefully, since they always consume leakage power. The other power domains are woken up in the same way as the original Look-ahead. To select the ever-on domains, we ﬁrst analyzed trafﬁc patterns of the target CMP illustrated in Figure 1. As a result, VC buffers which are directly connected from CPU cores are selected as ever-on domains in this paper. We will evaluate the impact of these ever-on domains in terms of the application performance and leakage power in Section 5. 4.4 Look-Ahead with Active Buﬀer Window (ABW) Method Look-ahead with active buffer window (ABW) method is another extension of the original Look-ahead method to completely remove the ﬁrst-hop wakeup latency at the expense of leakage power. This method is inspired by a leakage-aware buffer management proposed in [4]. In the ABW method, each VC buffer domain is divided into more ﬁner ﬂit-level power domains, each of which can be activated and deactivated independently. For example, a 4-ﬂit VC buffer is divided into four ﬂit-level power domains. To store incoming ﬂits without any ﬁrst-hop wakeup latency, a certain number of the ﬂit-level power domains in each VC buffer are always kept active, regardless of the workload. The activated part of the VC buffer is called “active buffer window”. The active buffer window size is always kept. That is, 65 Table 3. Simulation parameters (CMP) Processor L1 I-cache size L1 D-cache size # of processors L1 cache latency L2 cache size # of L2 cache banks L2 cache latency Memory size Memory latency UltraSPARC-III 16 KB (line:64B) 16 KB (line:64B) 8 1 cycle 256 KB (assoc:4) 64 6 cycle 4 GB 160 cycle ) d e z i l a m r o n ( e m i t n o i t u c e x E Table 4. Simulation parameters (NoC) Topology Routing Switching # of VCs Buffer size Router pipeline Flit size Control packet Data packet 4×4 mesh dimension-order wormhole 4 4 ﬂit [RC][VSA][ST] 128 bit 1 ﬂit 5 ﬂit ) d e z i l a m r o n ( e m i t n o i t u c e x E  2  1.5  1  0.5  0  2  1.5  1  0.5  0 Orig LA LA ever-on LA ABW without early wakeup methods   +35.3% → +4.0% a b c d e f g Benchmark programs h i j Ave. (a) Wakeup latency: 3-cycle Orig LA LA ever-on LA ABW without early wakeup methods   +46.3% → +6.7% a b c d e f g Benchmark programs h i j Ave. (b) Wakeup latency: 4-cycle the active buffer window in a VC buffer moves whenever a part of the active buffer is consumed, in order to prepare for the next ﬂit arrival. Assuming that the active buffer window size is two, two ﬂit-level power domains are activated to prepare for the next ﬂits. The other power domains are woken up in the same way as the original Look-ahead method. 5 Evaluations This section evaluates the ultra ﬁne-grained power-gating router with the early wakeup methods in terms of the application performance and leakage power. 5.1 Simulation Environment A NoC used in the 8-core CMP illustrated in Figure 1 is simulated. The cache architecture is SNUCA [7] and a cache coherence protocol is running on it. Table 3 lists the processors and memory system parameters. Table 4 shows the on-chip routers and network parameters. To simulate the above-mentioned CMP, we use a full-system multi-processor simulator: GEMS [10] and Virtutech Simics [8]. Network Model: We modiﬁed a detailed network model of GEMS, called Garnet [1], in order to accurately simulate the proposed ultra ﬁne-grained power gating of on-chip routers and the four wakeup methods. As shown in Table 4, typical 3-stage pipelined routers are used in the NoC. In the Look-ahead based wakeup methods, the wire delay of a wakeup signal Twire is set to one cycle. The size of a VC buffer in the router is set to four ﬂits. In the ABW method, the active buffer window size is set to two ﬂits. Cache Coherence Protocol: Token coherence protocol [9] is used. To avoid end-to-end protocol (i.e., requestreply) deadlocks, the on-chip network uses four virtual channels (VC0 to VC3) as follows. Figure 7. Execution time of SPLASH-2 benchmark with different wakeup methods. • VC0: Request from L1$ to L2$ bank; Request from L2$ bank to L1$ • VC1: Request from L2$ bank to directory controller; • VC2: Reply from L1$/directory to L2$ bank; Reply Request from directory controller to L2$ bank • VC3: Persistent request from L1$ from L2$ bank to L1$/directory The utilization ratio of each virtual channel is different. For example, the utilization of VC1 is low when the main memory is accessed sparsely due to frequent cache hits. VC3 is assigned to the persistent requests for avoiding the starvation, but its trafﬁc amount is quite small since such situation is not so frequent (e.g., 0.19% of all requests [9]). Our ultra ﬁnegrained power gating technique can exploit such imbalanced use of power domains inside a router. Benchmark Programs: To evaluate the application performance of the proposed ﬁne-grained power gating with different wakeup methods, we use ten parallel programs of SPLASH-2 benchmark [15]. Sun Solaris 9 operating system is running on the 8-core CMP. These benchmark programs are compiled by Sun Studio 12 and are executed on Solaris 9. The number of threads is set to eight in each program. 5.2 Application Performance Here, we count the execution cycles of the ten benchmark programs when the proposed ultra ﬁne-grained power gating technique is applied to on-chip routers in the CMP. We also compare the proposed early wakeup methods in terms of the application performance. As derived in Section 3.3, 66         Table 5. Leakage power of each router component [uW] Module 4-ﬂit VC buffer 1-ﬂit output latch 5-to-1 CBMUX 4-to-1 VCMUX Others Total Count Total leakage power 20 189.07 5 16.71 5 11.41 5 13.45 1 38.36 269 the wakeup latency of each power domain is less than 3nsec; thus we assume the wakeup latency is two, three, and four cycles in our simulations when the target NoC is operated at 667MHz, 1GHz, and 1.33GHz, respectively. Figure 7(a) shows the application performance of the original Look-ahead, Look-ahead with ever-on, and Look-ahead with ABW methods 1 , when the wakeup latency of every domain is set to three cycles. The benchmark set includes (a) radix, (b) lu, (c) fft, (d) barnes, (e) ocean, (f) raytrace, (g) volrend, (h) water-nsquared, (i) water-spatial, and (j) fmm. Their application performance is normalized so that the original application performance without power gating (i.e., 0cycle wakeup) is 1.0. As shown in the graph, all the programs indicate a similar tendency. Although the power gating with no early wakeup methods increases the execution time by 35.3% (see Section 3.4), those with the original Lookahead, Look-ahead ever-on, and Look-ahead ABW methods increase 10.5%, 4.0%, and 2.4% on average, respectively. Look-ahead ever-on and Look-ahead ABW methods can successfully mitigate the wakeup latency when the target NoC is running at 1GHz. Figure 7(b) shows the application performance when the wakeup latency is four cycles assuming an operating frequency of 1.33GHz. The power gating with no early wakeup methods increases the execution time by 46.3%, while those with Look-ahead ever-on and ABW methods increase only 6.7% and 4.9%, respectively. Thus, these early wakeup methods are still reasonable at such a high operating frequency. 5.3 Leakage Power Reduction In this section, we estimate the average leakage power of the proposed power gating router with different wakeup methods when the application workload is applied to it. Table 5 shows leakage power of each router component, based on the post-layout design of the power-gating router implemented in Section 3.2. The run-time power gating is applied to VC buffer, Output latch, VCMUX, and CBMUX, while the others are not. We used the 106 customized standard cells based on a low-power version of a commercial 65nm standard cell library. Temperature and core voltage were set to 25C and 1.20V, respectively. These leakage parameters were fed to the full system CMP simulator, in order to evaluate the run-time leakage power of the routers when the application programs are running on them. 1We omitted Naive method, since its performance is evidently inferior to the original Look-ahead method. To clearly show the leakage power reduction of each power domain type, the proposed ﬁne-grained power gating is gradually applied as the following three steps. • Level 1: VC buffers are power gated. • Level 2: VC buffers, VCMUXes, and CBMUXes are power gated. • Level 3: VC buffers, VCMUXes, CBMUXes, and Output latches are power gated. Figure 8(a) shows an average leakage power of the router when Level 1 power gating, which covers only VC buffers, is applied. The wakeup latency of all power domains is set to three cycles assuming an operating frequency of 1GHz. In this graph, 100% indicates the leakage power of the router without power gating (i.e., 269uW). The original Look-ahead method shows the smallest leakage power in these methods, while it cannot avoid the ﬁrsthop wakeup latency and degrades the application performance, as shown in Section 5.2. Look-ahead with ABW method consumes the largest leakage power since an active buffer window (i.e., two ﬂits) of each VC buffer is always activated, although it achieves the best performance. Lookahead with ever-on method consumes a little bit more leakage power compared to the original Look-ahead method, since it has some ever-on power domains to mitigate the ﬁrst-hop wakeup latency. Fortunately, the leakage power of these everon domains is not crucial, since they are limited to VC buffers of VC0 and VC2 in input physical channels directly connected from processor cores. As a result, Look-ahead with ever-on method is the best choice to balance the performance and leakage power. In Level 1 power gating, the ever-on method reduces the average router leakage power by 64.6%. Figure 8(b) shows an average leakage power of the router when our ultra ﬁne-grained power gating is fully applied. In this Level 3 power gating, Look-ahead with ever-on method reduces the average router leakage power by 78.9%. 6 Related Work As the standby power consumption is becoming more and more serious, various power gating techniques have been applied to on-chip routers to reduce the standby leakage power [11][12][14]. In [14], each router is partitioned into 10 smaller sleep regions with control of individual router ports. An input physical channel level power gating is studied in [12], while a virtual channel level power management is discussed in [11]. In [3], PMOS power switches controlled by an ultra-cut-off (UCO) technique are inserted on each NoC unit to maintain minimum leakage in standby mode. Compared to these approaches, we employ a more ﬁner approach in which each router is partitioned into 35 micro power domains in order to fully exploit the spatial and temporal communication locality inside and outside a router. Existing input buffer power gating [11][12] is corresponding to our Level 1 power gating, while our Level 3 approach covers more area and reduces more leakage power, as shown in Section 5.3. Moreover, in this paper, we designed the proposed ultra ﬁne-grained power gating router with a 65nm process in the same manner as in [6]. We also showed the actual area overhead and wakeup latency, based on this design. 67 ] % [  100 ) d e z i l a m r o n ( r e w o p e g a k a e l r t e u o R  80  60  40  20  0 ] % [  100 ) d e z i l a m r o n ( r e w o p e g a k a e l r t e u o R  80  60  40  20  0 Orig LA LA ever-on LA ABW -64.6% a b c d e f g Benchmark programs h i j Ave. (a) Power domains: VC buffer Orig LA LA ever-on LA ABW -78.9% a b c d e f g Benchmark programs h i j Ave. (b) Power domains: VC buffer, VCMUX, CBMUX, and Output latch Figure 8. Average leakage power of an on-chip router with different wakeup methods. When the power gating techniques are applied to on-chip networks, the wakeup control of power domains is one of the most important factors on the application performance. In [4], as a leakage-power aware buffer management method, a certain portion (i.e., window size) of the buffer is made active before it is accessed, in order to remove the performance penalty. Also, our original Look-ahead method is inspired by [12]. However, [12] considers only the wakeup control for input physical channels, while this paper proposes those for VC buffers, VCMUXes, CBMUXes, and Output latches, each of which is activated in different conditions. 7 Summary and Future Work On-chip communications inherently have a strong spatial and temporal locality inside and outside a router. To fully exploit the locality, we proposed an ultra ﬁne-grained power gating router, in which power supply to each router component (e.g., VC buffer, VCMUX, CBMUX, and Output latch) can be individually controlled in response to the workload. We implemented the power gating router that consists of 35 micro power domains using a 65nm process. The power gating router with the early wakeup methods was evaluated in terms of the area, application performance, and leakage power. The area overhead for the power switches and isolation cells is only 4.3%, while the customized cell height currently increases the overhead to 15.9%, although there is room to be optimized. The simulation results show that the router leakage power is reduced by 78.9%, even when application programs are running, at the expense of the 4.0% 68 performance overhead when we assume a 1GHz operation. • More accurate trade-off analysis: As future work, we will focus on the following issues. Inserting power switches negatively affects the critical path delay. Also, a small amount of energy is consumed by power switches and a single-bit wakeup signal to wake up a power domain. More sophisticated wakeup policies that take into account the overhead energy are required. • Routing protocols: Look-ahead wakeup methods are designed for deterministic routing. We will consider possibilities to apply them to oblivious and adaptive routings. • Power gating for the network interfaces: We are considering the use of various information from the coherence protocol or operating system to guide the run-time power gating decisions. "
Performance Evaluation of a Multicore System with Optically Connected Memory Modules.,"Over the past years, there have been impressive advances in bandwidth, latency, and scalability of on-chip networks. However, unless the off-chip network bandwidth and latency are also improved, we might have unbalanced systems which will limit the improvements to overall system performance. In this paper, we show how dense wavelength-division multiplexing (DWDM) -based optical interconnects could be used to emulate multiple buses in a fully-buffered DIMM (FB-DIMM) -like memory system to improve both bandwidth and latency. We evaluate an optically connected memory using full-system simulations of an 8-core system running memory-intensive multithreaded workloads. We show that for the FFT benchmark, optically connected memory can reduce the average memory request latency by 29% compared to a single-channel DDR3 SDRAM system and provide an overall performance speedup of 1.20. We also show that at least two DDR3 memory channels are needed to match the performance of a single optical bus, which demonstrates the advantage of optical interconnects in terms of savings in the number of pins required.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip Performance Evaluation of a Multicore System with Optically Connected Memory Modules Paul Vincent Mejia∗ , Rajeevan Amirtharajah∗ , Matthew K. Farrens† , and Venkatesh Akella∗ ∗ Department of Electrical and Computer Engineering † Department of Computer Science University of California, Davis Davis, CA 95616 Abstract—Over the past years, there have been impressive advances in bandwidth, latency, and scalability of on-chip networks. However, unless the off-chip network bandwidth and latency are also improved, we might have unbalanced systems which will limit the improvements to overall system performance. In this paper, we show how dense wavelengthdivision multiplexing (DWDM) -based optical interconnects could be used to emulate multiple buses in a fully-buffered DIMM (FB-DIMM) -like memory system to improve both bandwidth and latency. We evaluate an optically connected memory using full-system simulations of an 8-core system running memory-intensive multithreaded workloads. We show that for the FFT benchmark, optically connected memory can reduce the average memory request latency by 29% compared to a single-channel DDR3 SDRAM system and provide an overall performance speedup of 1.20. We also show that at least two DDR3 memory channels are needed to match the performance of a single optical bus, which demonstrates the advantage of optical interconnects in terms of savings in the number of pins required. Keywords-DRAM, OC-DIMM, Optics, DWDM, Performance Evaluation. I . IN TRODUC T ION There has been enormous interest in networks-on-chip and numerous papers about improving the bandwidth and latency of on-chip networks have been published [1]. However, there has been less attention paid to how to get data into and out of the chip, i.e. the challenges of off-chip interconnects. We believe that as the core count per chip increases to hundreds or thousands and networks-on-chip become more sophisticated and scalable, off-chip interconnects will carry a higher burden to sustaining high overall system performance. Therefore, it is important to study ways of improving the bandwidth and latency of off-chip interconnects, especially the interconnect between the CPU and DRAM. Memory latency is improving much more slowly than memory bandwidth [2]. So, improving on-chip network bandwidth and latency without a corresponding improvement in the off-chip memory bandwidth and latency will result in unbalanced systems [3] and limit the scalability of future multicore processors and high performance SoCs. To improve off-chip bandwidth, one has to provide more memory channels which, in turn, increases the number of pins. Unfortunately, the growth of total package pin count is projected at only 10% per year [4]. This will limit how much bandwidth one can provide to a chip. One solution to this problem is the use of 3D die stacking [5] which enables the use of optical interconnects with dense wavelength-division multiplexing (DWDM). With DWDM, multiple bits of data can be on the same waveguide simultaneously to realize an optical bus. A single-waveguide, 64-wavelength, 10 Gbps DWDM optical bus can provide a peak bandwidth of 80 GBps. As a comparison, a 64bit DDR3-1333 memory channel has a peak bandwidth of 10.6 GBps. Thus, one optical ﬁber can provide the bandwidth of almost 8 DDR3-1333 channels without an increase in pin count. Though the higher bandwidth potential of optical interconnects is well-known, it is not clear whether optical interconnects can reduce the latency of a memory system and, if so, by how much. The goal of this paper is to address this issue by leveraging DWDM-based optical buses in memory systems using offthe-shelf DRAM devices. The total memory access latency Tm is given by Tm ≈ Tq + Td + Ta (1) where Tq is the queueing delay, Td is the transport delay and Ta is the DRAM access time. With the increase in the number of cores per chip and the improvements to the on-chip networks, Tq will be the dominant factor in the overall memory latency. Single-channel memory systems suffer from high contention for the shared data bus, thus increasing the queuing delay of memory requests. Using electrical I/O would require doubling the pin count for memory interface to support two memory channels. The intuition behind our approach is as follows. We emulate multiple memory channels using multiple wavelengths on the optical bus, so that several memory transactions can be serviced simultaneously. This will reduce the queueing delay and, hence, reduce the overall memory latency. This introduces some interesting research questions — what is the best memory access protocol to use in WDM 978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.31 215 optical interconnects? How many buses should we emulate? What is the impact on memory latency? What is the impact on overall execution time of the program? This paper attempts to answer these questions. The rest of this paper is organized as follows. Section 2 describes recent studies related to this paper. Section 3 describes the proposed optically connected memory system. Section 4 explains the experimental setup used in our study. Section 5 provides an assessment of the performance of the proposed memory system design compared to conventional DDR3 SDRAM systems. Lastly, section 6 concludes this paper. I I . R E LAT ED WORK Tan et al. [6] demonstrated the feasibility of using an optical data bus for computer interconnections to avoid signal integrity issues inherent in using multi-drop electrical buses. Batten et al. [7] described a custom opto-electrical crossbar network that connects DRAM modules to processors. Corona [8] uses a pair of waveguides to connect the memory controllers to external 3D-stacked DRAMs. This paper describes a variation of the fully-buffered DIMM (FBDIMM) network using off-the-shelf DRAM devices which we believe is less ambitious and, hence, more practical as a near term solution. The concept of optically connected DIMMs, or OC-DIMMs, was evaluated on synthetic traces primarily to illustrate the bandwidth and power advantages of the proposed design [9], [10]. Our work describes the OC-DIMM architecture in detail and involves full-system simulations with emphasis on memory request latency and overall system performance. Hendry et al. [11] described a system-level analysis of on-chip photonic networks, but detailed evaluation of the beneﬁts of off-chip photonic networks was not considered. This paper can be viewed as a complementary to the work mentioned in the sense that it evaluates the system-level beneﬁts of off-chip photonic interconnects. I I I . O P T ICA L LY CONN EC T ED M EMORY MODU L E S A. Architecture The OC-DIMM system uses a serial packet-based protocol to communicate between the memory controller and the DIMMs. A similar protocol is used in FB-DIMM systems [12]. An FB-DIMM system uses a point-to-point interface between the memory controller and the nearest DIMM and between each other DIMM through the onmodule advanced memory buffers (AMB). On the other hand, an OC-DIMM system uses an optical broadcast bus that is interfaced to all the DIMMs through the on-module optical memory controller (OMC). Figure 1 shows the OC-DIMM memory system topology interfaced with a 3D-stacked multicore system which includes the optical components on a separate layer. Using DWDM, multiple wavelengths from a single waveguide Laser Source modulators / detectors Memory CPUs Northbound Bus Southbound Bus C M O 0 M M I D C M O 1 M M I D … C M O N M M I D Figure 1. OC-DIMM system topology. are grouped together to form the southbound (SB) and the northbound (NB) buses of the OC-DIMM system. The SB bus carries commands or WRITE data packets to the DIMMs. The NB bus carries READ data packets from the DIMMs to the memory controller. This architecture enables simultaneous READ and WRITE cycles on different DIMMs. The memory controller broadcasts commands and WRITE data packets on the SB bus. If a command is addressed to the DRAM devices in a particular DIMM, the OMC on that DIMM decodes the command and controls the DRAM devices. If WRITE data is sent on the SB bus, the OMC of the designated DIMM stores the data in its FIFO buffer and subsequently stores them to the DRAM devices. In most cases, multiple SB packets are needed to send an entire block of WRITE data. When a READ command is sent on the SB bus, READ data is returned through the NB bus. Similarly, multiple NB packets are needed to return a block of READ data. B. Access Protocol The proposed OC-DIMM access protocol is adopted from the protocol used in FB-DIMM systems. Modiﬁcations were made so that the OC-DIMM system can beneﬁt from the ﬂexibility provided by the DWDM optical interconnects. The main difference is the size of a packet. FB-DIMM systems use ﬁxed-sized packets: 168 bits for a NB packet and 120 bits for a SB packet. An OC-DIMM packet can be made as small or as large depending on the number of wavelengths available and the organization of these wavelengths. Consequently, the bus bandwidth of the SB and NB buses can vary. Data that is sent in one DRAM clock cycle constitutes a packet. Assuming an OC-DIMM system that is built based on PC3-10600 DIMMs (comprised of 666 MHz DDR3-1333 chips) and a per wavelength bandwidth of 10 Gbps, then a single wavelength can transmit 15 bits of data per packet. Each packet may contain commands, READ data, or WRITE data. The number of wavelengths needed in a SB bus is given by λSB = (cid:24) CRC + PacketID + (CMD × Slots ) 15 (cid:25) (2) where CRC is the number of checksum bits, PacketID is a 2-bit identiﬁer that speciﬁes the packet type, CMD is 216                                     Table I WAV E L ENG TH A S S IGNM EN T S . NB Bus SB Bus Misc. Total Wavelengths 2NB Notation 4NB 8NB 16NB 24 × 2 12 × 4 7 × 8 3 × 16 15 1 64 15 1 64 7 1 64 15 1 64 a 24-bit command, and Slots is the number of command and/or WRITE data slots per SB packet. A SB packet can be either a command or a command-with-data packet. Independent commands to separate DRAM ranks can be sent in one packet, making it possible to process multiple memory transactions to different ranks simultaneously. For instance, a SB packet that can carry 4 slots can initiate up to 4 memory requests concurrently. In this example, the SB bus needs to be 8 wavelengths wide, based on equation (2) and assuming a 22-bit CRC. Each slot in a SB packet can also be used to transmit a 16-bit WRITE data along with a 2-bit ECC and a 6-bit rank ID that identiﬁes the destination rank. For example, a SB bus that can carry 5 slots is capable of transmitting one command and a 64-bit WRITE data. Multiple such command-with-data packets may be needed to construct a block of WRITE data. In this example, 8 command-with-data packets are needed to write a 64-byte cache block into a given rank of DRAM devices. The command slots of a command-with-data packet may be the accompanying RAS or CAS command of the WRITE transaction, READ commands to a different rank, or simply NOPs. The number of wavelengths needed in a NB bus is given by λNB = (cid:24) CRC + READ + ECC 15 (cid:25) . (3) For example, to transmit a 128-bit READ data, a NB packet must carry the 128-bit data, a 16-bit ECC, and a 22-bit CRC. Therefore, a NB bus needs to be allocated 12 wavelengths to satisfy this bandwidth requirement. This wavelength assignment matches the DIMM bandwidth of a PC3-10600, which is 10.6 GBps. C. Wavelength Assignment 1) Multiple Data Channel Conﬁguration: In this study, we assumed a photonic technology that can multiplex up to 64 wavelengths per waveguide, which is the expected capability in future 22 or 16 nm process [7], [8]. Also, we used a single waveguide for performance comparison with a single electrical memory channel. Table I shows different ways of how wavelengths from a single waveguide can be assigned to form the SB and the NB buses of an OC-DIMM system. The notation given describes the number of NB buses in the system. A 4NB OC-DIMM system can process 4 READ requests concurrently, and each 12-wavelength wide NB bus DIMM Clock Southbound Bus CMD DIMM0 Command Bus ACT READ DIMM0 Data Bus Northbound0 Bus DIMM1 Command Bus ACT READ DIMM1 Data Bus Northbound1 Bus DIMM2 Command Bus ACT READ DIMM2 Data Bus Northbound2 Bus DIMM3 Command Bus ACT READ DIMM3 Data Bus Northbound3 Bus D0 D1 D2 D3 D4 D5 D6 D7 D0-D1 D2-D3 D4-D5 D6-D7 D0 D1 D2 D3 D4 D5 D6 D7 D0-D1 D2-D3 D4-D5 D6-D7 D0 D1 D2 D3 D4 D5 D6 D7 D0-D1 D2-D3 D4-D5 D6-D7 D0 D1 D2 D3 D4 D5 D6 D7 D0-D1 D2-D3 D4-D5 D6-D7 Figure 2. READ transaction in a 4NB OC-DIMM system. DIMM Clock Southbound Bus C-D0 C-D1 C-D2 C-D3 C-D4 C-D5 C-D6 C-D7 DIMM Command Bus ACT WRITE DIMM Data Bus D0 D1 D2 D3 D4 D5 D6 D7 Figure 3. WRITE transaction in a 4NB OC-DIMM system. is capable of carrying 128 bits of READ data per packet. This system’s 15-wavelength wide SB bus is capable of transferring 64 bits of WRITE data, half of what a single NB bus can deliver. Therefore, a 4NB system can provide a total peak bandwidth of (10.6 × 4) + 5.3 = 48 GBps. Figure 2 illustrates how READ requests are processed concurrently in a 4NB OC-DIMM system. Assuming that the transaction queue has at least 4 pending READ requests to different DIMMs, DRAM commands to each of these DIMMs are sent in a single SB packet (CMD). Commands are received and decoded by each of the OMCs. The OMCs relay DRAM commands (ACT for row activation and READ for column read) to their respective DRAM ranks to perform READ transactions concurrently. The DRAM devices burst data back to the OMCs where they are formatted into packets before they are sent to the memory controller through the NB buses. Standard DIMMs have a burst width of 64 bits, so it takes a burst length of 8 beats, or 4 DRAM clock cycles, to move an entire 64-byte cache block out of the DRAM devices. Figure 2 is not drawn to scale for easy illustration. Nevertheless, the memory controller must schedule commands accordingly so that DRAM timing constraints (tRAS , tCAS , etc.) are satisﬁed. Figure 3 illustrates how a WRITE transaction is executed in a 4NB OC-DIMM system. The memory controller sends command-with-data packets (C-Dx) to the designated DIMM through the SB bus. Since a 15-wavelength wide SB bus can transmit 64 bits of WRITE data per packet, the memory controller needs to send 8 command-with-data packets to write an entire 64-byte cache block. The ﬁrst SB packet (C-D0) consists of the row-activate command (ACT) and the ﬁrst sub-block of WRITE data. The last SB packet (C-D7) of a WRITE transaction consists of the column-write command (WRITE) and the last sub-block of the WRITE data. The SB packets in between carry the rest 217 DIMM Clock Southbound Bus CMD DIMM Command Bus ACT READ DIMM Data Bus Northbound Bus D0 D1 D2 D3 D4 D5 D6 D7 D0 D1 D2 D3 D4 D5 D6 D7 Figure 4. READ data transfer on a 7-wavelength NB bus. DIMM Clock Southbound Bus CMD DIMM Command Bus ACT READ DIMM Data Bus Northbound Bus D0-D1 D2-D3 D4-D5 D6-D7 D0-D3 D4-D7 Figure 5. READ data transfer on a 24-wavelength NB bus. of the WRITE data sub-blocks and may be accompanied by READ commands to other DIMMs, if available. Otherwise, NOPs are inserted. An OC-DIMM system can also be conﬁgured to have more than 4 NB buses, thereby increasing the number of concurrent READ requests that it can handle. However, due to a limited number of wavelengths per waveguide, this approach come at the expense of increasing the latency of individual READ transactions. An 8NB system has twice the number of NB buses but half the bus bandwidth. With 7 wavelengths, each NB bus is capable of transferring 64 bits of READ data per packet for a peak bandwidth of 5.3 GBps, which is only half the bandwidth of a PC3-10600 DIMM. Figure 4 illustrates how an OC-DIMM system with a 7-wavelength wide NB bus returns a READ transaction. Even if the burst length required to move a block of data out of the DRAM devices is 8 beats, transmission of data back to the memory controller will still take more cycles because of limited NB bus bandwidth. The ﬁgure shows that a 7-wavelength wide NB bus requires twice the number of NB packets to send a 64-byte block of READ data. Therefore, an 8NB OC-DIMM system increases the amount of concurrency in terms of the number of NB data buses at the expense of increased data transmission time. Using a 64-wavelength waveguide, an OC-DIMM system can have up to a maximum of 16 NB buses. For the same reasons explained previously, the increase in the number of NB buses limits the available bandwidth per NB bus. In a 16NB system, a 3-wavelength NB bus provides only one-eighth of a PC3-10600 DIMM’s bandwidth, which can further increase data transmission time. 2) High-Bandwidth Bus Conﬁguration: An OC-DIMM system can also have fewer, yet higher-bandwidth NB buses by allocating more wavelengths to them. In a 2NB OCDIMM system, each NB bus has a bandwidth of 21.33 GBps. To take advantage of this, a NB bus must be interfaced to a matching pair of DIMMs that operates in lockstep and sends 128 bits of data per DRAM burst. This setup is similar to the dual-channel conﬁguration supported by the Intel 875P chipsets [13]. The DRAM devices now require a burst length of 4 beats, instead of 8, to read a 64-byte cache block. Table II SY S T EM CON FIGURAT ION . Processors L1 I/D Cache L2 Cache Main Memory 8 out-of-order cores, 4 GHz clock, 32-entry Instruction Window, 64-entry Re-order Buffer, 1 KB YAGS branch predictor 128 KB, 4-way associative, 64-byte line 4 MB, 8 banks, 4-way associative, 64-byte line 8 GB DIMMs, 64-bit DIMM data channel, DDR3-1333 SDRAM devices, 10.6 GBps DIMM bandwidth, tRCD -tRP -tCL of 9-9-9 Figure 5 illustrates how an OC-DIMM system with a 24wavelength wide NB bus returns a READ request in less transmission time. Ideally, more wavelengths can be assigned to a NB bus and more DIMMs can be matched to operate in lockstep to further reduce the burst length to 2, or even 1. However, current DDR2 and DDR3 SDRAM devices support burst lengths of 4 and 8 only. IV. EVA LUAT ION A. Full-System Simulator For our experiments, we used the Simics full system simulation platform [14]. We used Simics as a functional simulator to emulate a Sun server running an unmodiﬁed Solaris operating system. We leveraged this platform by using the timing simulator modules, Opal and Ruby, from the GEMS toolset [15]. Opal models an out-of-order SPARCv9 processor while Ruby models a multiprocessor memory system. These tools are used to simulate an 8core CMP that implements a directory-based MOESI cache coherence protocol. While Ruby provides a detailed model of a multiprocessor memory hierarchy, it does not accurately model the DRAM behavior. In order to simulate a detailed DRAM system, we integrated the DRAMsim memory system simulator [16] with Ruby. Table II shows a summary of the simulated system parameters. We modiﬁed DRAMsim to model our proposed OCDIMM system. We also conﬁgured DRAMsim to model a conventional DDR3 memory module for performance comparisons. We used an 8 GB PC3-10600-like dual-rank DIMMs to model both OC-DIMMs and standard DDR3 DIMMs. The device parameters were based on Micron’s 4Gb TwinDie DDR3 SDRAM components [17]. Additonally, we conﬁgured DRAMsim to model a transaction queue size of 256. B. Workload Characteristics We used multithreaded benchmarks from the SPLASH2 [18] and PARSEC-2.1 [19] suites. We ran full-system 218 Table III Table V WORK LOAD S AND CON FIGURAT ION S . OC -D IMM SY S T EM ORGAN I ZAT ION AND P EAK BANDW ID TH . Program Problem Size # DRAM Requests FFT Radix Canneal 1 M data points 2 M integers 100 K elements 5.24 M 2.37 M 2.99 M Table IV OC -D IMM CHANN E L LAT ENCY BR EAKDOWN . Latency Component Value Controller-to-DIMM ﬂight SB DIMM-to-DIMM ﬂight Optical receiver delay Command check and decode DRAM access Data Serialization Optical transmitter delay NB DIMM-to-DIMM ﬂight DIMM-to-Controller ﬂight Frame-into-Controller 10.45 ps/mm 10.45 ps/mm 7.4 ps 3000 ps 27000 ps 3000 ps 36.3 ps 10.45 ps/mm 10.45 ps/mm 1500 ps simulations of programs from both benchmark suites using the setup described in section IV-A and a single-channel DDR3 memory controller populated with a single dual-rank 8 GB DDR3 memory module. From these experiments, we picked those programs that require high off-chip memory bandwidth demand and reasonable simulation time, namely FFT, Radix, and Canneal. Table III shows a summary of our selected workloads. C. Memory Request Latency Breakdown 1) OC-DIMM Channel Latency: Table IV summarizes the latency breakdown of an OC-DIMM system channel populated with DDR3-1333 components. The delay values of the optical components are predicted values for a 32 nm process technology [20], [21]. The OMC component delays were derived from the AMB delays of an FB-DIMM system based on DDR2-667 components [22]. These delays were adjusted for a DDR3-based memory module. We used these latency numbers in our OC-DIMM model for our performance studies. 2) Queuing Delay and Data Transmission Time: The previous subsection described the OMC-related overheads experienced by an OC-DIMM and the amount of ﬂight time required on an optical medium. This subsection describes two other components of the total latency of a memory request. Queuing delay is the amount of time that a memory request has to wait in queue before it is serviced. In systems running memory-intensive workloads, the number of pending memory requests in the memory controller’s transaction queue can be huge. In situations like this, the queuing delay can become a signiﬁcant part of the total memory request latency. Therefore, a good memory design must be able to minimize the queuing delay incurred by a memory request. Data transmission time, on the other hand, is the time for a packet to pass through the bus, not including Total DIMMs Conﬁg Notation BW Per NB Bus (GBps) BW Per SB Bus (GBps) Total BW (GBps) 1 2 4 8 16 32 1NB-1D 1NB-2D 2NB-1D 1NB-4D 2NB-2D 4NB-1D 1NB-8D 2NB-4D 4NB-2D 8NB-1D 1NB-16D 2NB-8D 4NB-4D 8NB-2D 16NB-1D 1NB-32D 2NB-16D 4NB-8D 8NB-4D 16NB-2D 10.67 21.33 10.67 21.33 21.33 10.67 21.33 21.33 10.67 5.33 21.33 21.33 10.67 5.33 1.33 21.33 21.33 10.67 5.33 1.33 21.33 21.33 21.33 21.33 5.33 5.33 21.33 5.33 5.33 2.67 21.33 5.33 5.33 2.67 5.33 21.33 5.33 5.33 2.67 5.33 32.00 42.67 42.67 42.67 48.00 48.00 42.67 48.00 48.00 45.33 42.67 48.00 48.00 45.33 26.67 42.67 48.00 48.00 45.33 26.67 time of ﬂight, and is equal to the size of the packet divided by the bus’ bandwidth [23]. We are particularly interested in addressing queuing delay and data transmission time by using the OC-DIMM memory system. Creating multiple data buses by organizing groups of wavelengths allows multiple requests to be serviced concurrently, thus addressing queuing delay. Furthermore, DWDM optical buses provide high bandwidth density which can help reduce data transmission time. D. OC-DIMM System Organization This study encompasses all the possible organizations of an OC-DIMM system with 1 to 32 dual-rank DIMMs, organized into 1 to 16 NB buses. Table V summarizes the different OC-DIMM organizations possible using a single 64-wavelength waveguide. The organizations are grouped according to the total number of DIMMs in the system. The notation used in the table describes the number of NB buses and the number of DIMMs that share a NB bus. For example, the notation 8NB-2D means an OC-DIMM organization of 8 NB buses with two DIMMs sharing a NB bus. Each DIMM provides a peak bandwidth of 10.6 GBps. A simple OC-DIMM organization have a matching NB bus bandwidth and 1 or more DIMMs that share a NB bus. OCDIMM systems with a NB bus bandwidth of 21.3 GBps and at least 2 DIMMs per NB bus are conﬁgured such that a matching pair of DIMMs operates in lockstep to match the NB bus bandwidth, as described in section III-C2. OCDIMM systems whose NB bus bandwidth is less than the DIMM’s bandwidth are those that stretch the number of NB buses to 8 or 16 at the expense of lower per-NB-bus bandwidth. 219                     !""""   #              !""""   #  #              !""""   #  #  #       $%&'%$                !""""   #  #  #       $%&'%$        !""""   #  #  #       $%&'%$        !""""   #  #  #  Figure 6. DDR3 SDRAM latency-bandwidth curves.                     !""""   #$              !""""   #$  #$              !""""   #$  #$  #$       $%&'%$                !""""   #$  #$  #$  #$       $%&'%$        !""""   #$  #$  #$  #$  #$       $%&'%$        !""""   #$  #$  #$  #$  #$  Figure 7. OC-DIMM latency-bandwidth curves. V. R E SU LT S A. Latency-Bandwidth Characteristics Figure 6 shows the latency-bandwidth characteristics of a DDR3 SDRAM system. These latency and bandwidth values were measured for a random address trace with varying memory request rate. We investigated a conﬁguration space of 1 to 32 dual-rank DIMMs organized into 1 to 4 channels. A similar notation is used to denote a DDR3 SDRAM system’s organization. The notation 2C-4D means an organization of 2 channels with 4 DIMMs sharing a channel. Each subplot in the ﬁgure is organized according to the total number of DIMMs in the system. The ﬁrst observation is that the sustained bandwidth is signiﬁcantly increased as the number of channels is increased. Having multiple independent channels reduces the amount of contention due to shared resources, like the address, command, and data buses, and allows for multiple memory transactions to be serviced concurrently. This greatly reduces the queuing delay for memory requests. Another observation is that an increase in the number of DIMMs (or ranks) per channel slightly increases the sustained bandwidth. Having multiple ranks in a channel provides more memory banks to distribute requests to and more opportunities for scheduling and pipelining requests. Overall, a 4-channel DDR3 SDRAM system can sustain a bandwidth of up to 28.4 GBps. Having 2 channels can sustain up to 14.2 GBps, while having a single channel can sustain up to 7.3 GBps.         !""#$%&""'  !""%$!% !  ""   # $ "" & % '  ( %       )         )         )         )         )         )         )        )          )        )        )     )* !""#$%&""'  !""%$!%         , +              , , + +                     , , , + + +                            , , , , + + + +                             , , , , , + + + + +                                   , , , , , + + + + +            -.$  %( ((!%  +,)./ 0!&((% 1,)./ %-"" -$          1  ""  !""%$!%     )         )          , +  Figure 8. FFT Memory Request Latency Breakdown. Average memory request latency is reduced by 29% in OC-DIMM compared to a singlechannel DDR3 SDRAM. Two channels of DDR3 memory is needed to yield a latency reduction of 7.66% over OC-DIMM.         !""#$%& ' '  (&%&       ! "" $ #  % ' &    )   *    + *  )   *  )   *    + *    + *  )   *  )   *    + *    + *    + *  )   *  )   *    + *    + *    + *    + *  )    *  )   *     + *    + *    + *    + *     + *  )  ,  *  )    *   ,  + *     + *    + *    + *     + * **-, )*.  /  0  1$  (&%&  )    *  )    *    + * Figure 9. FFT Execution Time. OC-DIMM provided a performance speedup of 1.20 compared to a single-channel DDR3 SDRAM. Two channels of DDR3 memory is needed to yield a speedup of 1.03 over OC-DIMM. Figure 7 shows the latency-bandwidth characteristics of an OC-DIMM system. The sustained bandwidth is signiﬁcantly increased as the number of NB buses is increased from 1 to 2. Ignoring 8NB OC-DIMM systems for a moment, 1NB systems have the lowest sustainable bandwidth because it suffers from high contentions for the shared NB bus. On the other hand, the poor bandwidth performance of 8NB systems is due to the very limited bandwidth of its SB bus. Since commands are transmitted through the SB bus, 8NB systems suffer highly from the inability to schedule as much commands as possible to take advantage of the multiple NB buses available. In fact, 8NB systems have the lowest SB bus bandwidth among all the OC-DIMM organizations. A 16NB system beneﬁts from having twice as much SB bandwidth and yields one of the highest sustained bandwidth, although it experiences very high latency due to long transport delays, as explained in section III-C1. The latency-bandwidth characteristics of 2NB and 4NB systems are very much alike. A 4NB system experiences lower queuing delays by being able to service more requests concurrently. On the other hand, a 2NB system has higherbandwidth NB buses that help reduce data transport delay by about half. As the plots show, these two OC-DIMM organizations’ beneﬁts equally outweigh each other. The only marked difference occurs when the OC-DIMM system has 32 DIMMs. In this case, a 2NB-16D system has a deeper NB channel than a 4NB-8D system. A deeper NB channel 220                                                                                                                                                                                                                                                                                       !""#$%&""'  !""%$!% !  ""   # $ "" & % '  ( %       )         )         )         )         )         )         )        )          )        )        )     )* !""#$%&""'  !""%$!%         , +              , , + +                     , , , + + +                            , , , , + + + +                             , , , , , + + + + +                                   , , , , , + + + + +            -./$0  0%( ((!%0  +,)/ 1!&((% ,)/ %."" .$           ""  !""%$!%     )        )           , +  Figure 10. Radix Memory Request Latency Breakdown. Average memory request latency is reduced by 24.48% in OC-DIMM compared to a singlechannel DDR3 SDRAM. Two channels of DDR3 memory is needed to yield a latency reduction of 19.03% over OC-DIMM.         !""#$%& ' ""  (&%&       ! "" $ #  % ' &    )   *    + *  )   *  )   *    + *    + *  )   *  )   *    + *    + *    + *  )   *  )   *    + *    + *    + *    + *  )    *  )   *     + *    + *    + *    + *     + *  )  ,  *  )    *   ,  + *     + *    + *    + *     + * **, )*-  .  /  0$  (&%&  )  ,  *  )    *    + * Figure 11. Radix Execution Time. OC-DIMM provided a performance speedup of 1.02 compared to a single-channel DDR3 SDRAM. Two channels of DDR3 memory is needed to yield a speedup of 1.01 over OC-DIMM. means more in-ﬂight memory transactions that try to contend for the NB bus, thus increasing the queuing delay. Overall, the maximum sustainable bandwidth of an OCDIMM system is about 23.3 GBps, which is attainable by using any of the several optimum OC-DIMM organizations — 2NB-1D, 2NB-2D, 2NB-4D, 2NB-8D, 4NB-1D, 4NB2D, 4NB-4D, and 4NB-8D. Although a 16NB system can sustain as much bandwidth, its high latency makes it less attractive compared to the others. Compared to a DDR3 SDRAM system, the OC-DIMM system’s sustained bandwidth is 3.2× the sustained bandwidth of a single-channel DDR3 SDRAM system. A DDR3 SDRAM system would need up to 4 channels of memory in order to attain this bandwidth. B. Latency and Execution Time This section compares the performance of the OC-DIMM memory system against conventional 1-channel and 2channel DDR3 SDRAM systems for multithreaded workloads. Figure 8 shows the average memory request latency breakdown of FFT for both DDR3 SDRAM and OC-DIMM systems. A 2-channel DDR3 SDRAM system yielded a lower average memory latency compared to a 1-channel DDR3 SDRAM. The signiﬁcant decrease in queuing delay is due to the fact that a multi-channel memory system can handle more requests concurrently. Adding more DIMMs to a channel resulted in modest decrease in latency.         !""#$%&""'%%  !""%$!% !  ""   # $ "" & % '  ( %                                                                                                    ) !""#$%&""'%%  !""%$!%         + *              + + * *                     + + + * * *                            + + + + * * * *                             + + + + + * * * * *                                   + + + + + * * * * *            ,-$.  .%( ((!%.  *+-/ 0!&((% 1+-/ %"" $        1  ""  !""%$!%                        + *  Figure 12. Canneal Memory Request Latency Breakdown. Average memory request latency is reduced by 15.99% in OC-DIMM compared to a single-channel DDR3 SDRAM. Two channels of DDR3 memory is needed to yield a latency reduction of 15.81% over OC-DIMM.            !""#$%& ' &&  (&%&       ! "" $ #  % ' &       )    * )     )     )    * )    * )     )     )    * )    * )    * )     )     )    * )    * )    * )    * )      )     )     * )    * )    * )    * )     * )    +  )      )   +  * )     * )    * )    * )     * ) )),+ )-  .  /  0$  (&%&      )     )    * ) Figure 13. Canneal Execution Time. OC-DIMM provided a performance speedup of 1.11 compared to a single-channel DDR3 SDRAM. Two channels of DDR3 memory is needed to yield a speedup of 1.07 over OC-DIMM. As expected, 2NB and 4NB OC-DIMM systems performed better than the other OC-DIMM organizations. 16NB systems experienced long transmission times which have become a signiﬁcant portion of the total memory request latency. 8NB systems’ signiﬁcant queuing delay is due to the fact that they are limited by the SB bus bandwidth. Memory requests are not being scheduled immediately because of contention for the SB bus, forcing them to wait longer in the queue. The summary plot of ﬁgure 8 compares the best-case average memory request latency of a 1- and 2-channel DDR3 SDRAM with OC-DIMM. OC-DIMM reduced the average memory request latency by 29% compared to a 1-channel DDR3 SDRAM system. A 2-channel DDR3 SDRAM system can reduce the average memory latency by 7.66% at the cost of doubling the pin count for the extra memory channel. Figure 9 shows the execution time of the FFT program across the different memory conﬁgurations. A performance speedup of 1.20 is gained by using OC-DIMM compared to a 1-channel DDR3 SDRAM system. A DDR3 SDRAM system has to utilize 2 memory channels in order to perform better than the OC-DIMM system. A 2-channel DDR3 SDRAM system can provide a program speedup of 1.03% over OC-DIMM. The next ﬁgures show the average memory request latencies and execution times for Radix (ﬁgures 10 and 11) and Canneal (ﬁgures 12 and 13). These ﬁgures also show over221                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               all performance speedup in using the OC-DIMM memory system compared to a standard 1-channel DDR3 memory. [11] G. Hendry et al., “Analysis of photonic networks for a chip multiprocessor using scientiﬁc applications,” in Int’l. Symp. on Networks-on-Chip, 2009, pp. 104–113. V I . CONC LU S ION Multiwavelength optical buses can provide high bandwidth density than electrical interconnects and can scale well with the increasing memory bandwidth demand of future multicore systems. Our OC-DIMM memory system leverages this technology for processor-DRAM communication. Organizing multiple wavelengths into groups to form multiple independent data channels helps reduce the queuing delay of memory requests because of increased concurrency. Full-system simulations show that, for memoryintensive multithreaded workloads on an 8-core CMP, a single-waveguide OC-DIMM system yields a memory request latency reduction of up to 29% and a performance speedup of up to 1.20 compared to conventional singlechannel DDR3 SDRAM systems. In the future, we plan to extend this work to include a detailed system-level power analysis of the memory subsystem. ACKNOW L EDGM EN T This work was supported in part by the CITRIS seed grant #3-34PICOB. "
Evaluating Bufferless Flow Control for On-chip Networks.,"With the emergence of on-chip networks, the power consumed by router buffers has become a primary concern. Bufferless flow control addresses this issue by removing router buffers, and handles contention by dropping or deflecting flits. This work compares virtual-channel (buffered) and deflection (packet-switched bufferless) flow control. Our evaluation includes optimizations for both schemes: buffered networks use custom SRAM-based buffers and empty buffer bypassing for energy efficiency, while bufferless networks feature a novel routing scheme that reduces average latency by 5%. Results show that unless process constraints lead to excessively costly buffers, the performance, cost and increased complexity of deflection flow control outweigh its potential gains: bufferless designs are only marginally (up to 1.5%) more energy efficient at very light loads, and buffered networks provide lower latency and higher throughput per unit power under most conditions.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip Evaluating Bufferless Flow Control for On-Chip Networks George Michelogiannakis, Daniel Sanchez, William J. Dally, Christos Kozyrakis Electrical Engineering Department, Stanford University E-mail: {mihelog, sanchezd, dally, kozyraki}@stanford.edu Abstract—With the emergence of on-chip networks, the power consumed by router buffers has become a primary concern. Bufferless ﬂow control addresses this issue by removing router buffers, and handles contention by dropping or deﬂecting ﬂits. This work compares virtual-channel (buffered) and deﬂection (packet-switched bufferless) ﬂow control. Our evaluation includes optimizations for both schemes: buffered networks use custom SRAM-based buffers and empty buffer bypassing for energy efﬁciency, while bufferless networks feature a novel routing scheme that reduces average latency by 5%. Results show that unless process constraints lead to excessively costly buffers, the performance, cost and increased complexity of deﬂection ﬂow control outweigh its potential gains: bufferless designs are only marginally (up to 1.5%) more energy efﬁcient at very light loads, and buffered networks provide lower latency and higher throughput per unit power under most conditions. I . IN TRODUC T ION Continued improvements in VLSI technology enable integration of an increasing number of logic blocks on a single chip. Scalable packet-switched networks-on-chip (NoCs) [7] have been developed to serve the communication needs of such large systems. As system size increases, these interconnects become a crucial factor in the performance and cost of the chip. Compared to off-chip networks, on-chip wires are cheaper and buffer cost is more signiﬁcant [7]. Router buffers are used to queue packets or ﬂits that cannot be routed immediately due to contention [6]. Several proposals eliminate router buffers to reduce NoC cost. In these bufferless schemes, contending packets or ﬂits are either dropped and retransmitted by their source [9] or deﬂected [2], [18] to a free output port. Frequent retransmissions or deﬂections degrade network performance. However, under light load, dropping or deﬂecting may occur infrequently enough to have a small impact on performance. Bufferless ﬂow control proposals often report large area and power savings compared to conventional buffered networks (e.g. 60% area and up to 39% energy savings in a conventional CMP network [18]). However, previous work has aimed to reduce the cost of router buffers. For example, by using custom SRAMbased implementations, buffers can consume as little as 6% of total network area and 15.5% of total network power in a ﬂattened butterﬂy (FBFly) network [14], [17]. Furthermore, ﬂits can bypass empty buffers in the absence of contention [24], reducing dynamic power consumption in lightly-loaded networks. These optimizations may reduce buffering overhead up to a point where the extra complexity and performance issues of bufferless ﬂow control outweigh potential cost savings. In this paper, we compare a state-of-the-art packet-switched bufferless network with deﬂecting ﬂow control, BLESS [18], and the currently-dominant virtual channel (VC) buffered ﬂow control [6]. To perform an equitable comparison, we optimize both networks. In particular, VC networks feature efﬁcient custom SRAM buffers and empty buffer bypassing. We also propose a novel routing scheme for BLESS, where ﬂits bid for all outputs that would reduce their distance to their destinations regardless of dimension order constraints. In an 8×8 2D mesh with 5×5 routers and uniform trafﬁc, this routing scheme reduces latency by 5% over dimension-ordered routing (DOR). We ﬁnd that bufferless ﬂow control provides a minimal advantage at best: in a lightly-loaded 8×8 2D mesh, bufferless ﬂow control reduces power consumption by only 1.5%, mostly due to buffer leakage power, when using a high-performance, high-leakage process. However, at medium or high loads the buffered network offers signiﬁcantly better performance and higher power efﬁciency with 21% more throughput per unit power, as well as a 17% lower average latency at a 20% ﬂit injection rate. The buffered network becomes more energy efﬁciency at ﬂit injection rates of 7% (11% with low-swing channels). Buffer optimizations play a crucial role: at a ﬂit injection rate of 20%, buffers without bypassing consume 8.5× the dynamic power with bypassing. Finally, the age-based allocator required to prevent livelocks in BLESS is 81% slower than an input-ﬁrst separable switch allocator used in VC ﬂow control. The rest of this paper is organized as follows: Section II provides the necessary background on bufferless interconnects. Section III discusses our evaluation methodology. Section IV presents our novel routing scheme for BLESS. Section V examines the implications for router microarchitecture. In Section VI we present our evaluation and results. Section VII discusses further design parameters that can affect our comparisons. Finally, Section VIII concludes this paper. I I . BACKGROUND Deﬂection ﬂow control was ﬁrst proposed as “hot-potato” routing in off-chip networks [2]. Recent work has found that network topology is the most important factor affecting performance, and that global or history-related deﬂection criteria are beneﬁcial [16]. Furthermore, dynamic routing can be used to provide an upper bound for delivery time [4]. In this paper, we consider BLESS, a state-of-the-art bufferless deﬂection ﬂow control proposal for NoCs [18]. In BLESS, ﬂits bid for their preferred output. If the allocator is unable to grant that output, the ﬂit is deﬂected to any free output. This requires that routers have at least as many outputs as inputs. Flits bid for a single output port, following deterministic DOR. To avoid livelock, older ﬂits are given priority. Finally, injecting ﬂits to a router requires a free output port to avoid deﬂecting ﬂits to ejection ports. Two BLESS variants were evaluated in [18], FLIT-BLESS and WORM-BLESS. In FLIT-BLESS, every ﬂit of a packet can be routed independently. Thus, all ﬂits need to contain routing information, imposing overhead compared to buffered networks, where only head ﬂits contain routing information. To reduce this overhead, WORM-BLESS tries to avoid splitting worms by providing subsequent ﬂits in a packet with higher priority for allocating the same output as the previous ﬂit. However, worms 978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.10 9 may still have to be split under congestion, and WORM-BLESS still needs to be able to route all ﬂits independently. Due to the lack of VCs, trafﬁc classes need to be separated with extra mechanisms or by duplicating physical channels. Additionally, BLESS requires extra logic and buffering at network destinations to reorder ﬂits of the same packet arriving out of order. The number of packets that can arrive interleaved is practically unbounded, unlike VC networks where destinations just need a FIFO buffer per VC. While most bufferless proposals either drop or deﬂect ﬂits, both mechanisms can be combined [8]. Other techniques also eliminate buffers: circuit-switching relies on establishing endto-end circuits in which ﬂits never contend [15]. Finally, elastic buffer ﬂow control [17] uses channels as distributed FIFOs in place of router buffers. I I I . M ETHODO LOGY We use a modiﬁed version of Booksim [5] for cycle-accurate microarchitecture-level network simulation. To estimate area and power we use ITRS predictions for a 32nm high-performance process [12], operating at 70◦C. Modeling buffer costs accurately is fundamental in our study. Orion [23] is the standard modeling tool in NoC studies, but a recent study shows that it can lead to large errors [13], and the update ﬁxing these issues was not available at the time of this work. Instead, we use the models from Balfour and Dally [1], which are derived from basic principles, and validate SRAM models using HSPICE. We assume a clock frequency of 2GHz and 512-bit packets. We model channel wires as being routed above other logic and include only repeater and ﬂip-ﬂop area in channel area. The number and size of repeaters per wire segment are chosen to minimize energy. Our conservative low-swing model has 30% of the full-swing repeated wire traversal power and twice the channel area [11]. Router area is estimated using detailed ﬂoorplans. VC buffers use efﬁcient custom SRAM-based buffers. We do not use area and power models for the allocators, but perform a detailed comparison by synthesizing them. Synthesis is performed using Synopsys Design Compiler and a low-power commercial 45nm library under worst-case conditions. Place and route is done using Cadence Silicon Encounter. Local clock gating is enabled. We choose FLIT-BLESS for our comparisons. FLIT-BLESS performs better than WORM-BLESS [18], but incurs extra overhead because all ﬂits contain routing information. However, in our evaluation we do not model this overhead, giving BLESS a small advantage over buffered ﬂow control. We use two topologies for a single physical network with 64 terminals. The ﬁrst is an 8×8 2D mesh with single-cycle channels. Routers are 5×5 and have one terminal connected to them. The second is a 2D FBFly [14] with four terminals connected to each router. Therefore, there are 16 10×10 routers laid out on a 4×4 grid. Short, medium and long channels are two, four and six clock cycles long, respectively. Injection and ejection channels are a single cycle long. For both topologies, one clock cycle corresponds to a physical length of 2 mm. These channel lengths are chosen so that both networks cover an area of about 200 mm2 , a typical die size in modern processes [22]. We assume a two-stage router design. The VC network features input-ﬁrst separable round-robin allocators, speculative switch allocation [21] and input buffer bypassing [24]. No communication protocol is assumed. The VC network uses DOR for the mesh and FBFly. The deﬂection network uses multidimensional routing, explained in Section IV. We do not assume adaptive routing for the VC network since such a comparison would require adaptive routing for the bufferless network as well. We choose the number of VCs and buffer slots to maximize throughput per unit power. While this penalizes the VC network in area efﬁciency, power is usually the primary constraint. We generate results for either uniform random trafﬁc or we average over a set of trafﬁc patterns: uniform random, random permutation, shufﬂe, bit complement, tornado and neighbor [5]. This set is extended for the FBFly to include transpose and a trafﬁc pattern that illustrates the effects of adversarial trafﬁc for networks with a concentration factor. Averaging among trafﬁc patterns makes our results less sensitive to effects caused by speciﬁc trafﬁc patterns. IV. ROUT ING IN BU FFERL E S S N ETWORK S BLESS networks use DOR [18]. In VC networks, DOR prevents cyclic network dependencies without extra VCs. However, in bufferless networks ﬂits never block waiting for buffers, so there can be no network deadlocks, making DOR unnecessary. We propose two oblivious routing algorithms that decrease deﬂection probability. We observe that a ﬂit often has several productive outputs (i.e. outputs that would get the ﬂit closer to its destination). For example, in a 2D mesh, those are the two outputs shown in Figure 1(a), unless the ﬂit is already at one of the axes of its ﬁnal destination. Our ﬁrst routing algorithm, multi-dimensional routing (MDR), exploits choice by having ﬂits request all of their productive outputs. If both outputs are available, the switch allocator assigns one pseudorandomly. With MDR, there is one productive output in each dimension with remaining hops. If a ﬂit exhausts all hops in a dimension, it will have one less productive output, increasing its deﬂection probability. We can improve MDR by prioritizing the dimension that has the most remaining hops, which increases the number of productive outputs at subsequent hops. We call this scheme prioritized MDR (PMDR). Figure 1(b) shows an example path with PMDR in a 2D mesh. Due to PMDR, all the hops except the last one have two productive outputs. In an FBFly with minimal routing, ﬂits only take one hop in each dimension, so PMDR is equivalent to MDR. However, PMDR increases allocator complexity: since a BLESS allocator already needs to prioritize ﬂits by age, PMDR requires either prioritizing output ports or two allocation iterations. Figure 2 compares DOR, MDR and PMDR in mesh and FBFly bufferless networks. In the mesh, MDR offers 5% lower average latency than DOR and equal maximum throughput. In the FBFly, MDR achieves 2% lower average latency and 3% higher maximum throughput. Under a sample 20% ﬂit injection rate, 13% more ﬂits were only able to choose a single output with DOR compared to MDR. Also, PMDR achieves only marginal improvements over MDR (0.5% lower average latency in the mesh). Given its higher allocator complexity, we use MDR for the rest of the evaluation. V. ROUT ER M ICROARCH I T EC TUR E This section explores router microarchitecture issues pertinent to our study of bufferless networks. 10 S D X axis Y axis (a) MDR requests all productive outputs at each hop. D S (b) PMDR prioritizes the output with the most hops remaining in that dimension. Figure 1. MDR and PMDR routing algorithms for deﬂection bufferless networks. 15 0 5 10 15 20 25 Injection rate (flits/cycle * 100) 30 35 20 25 30 35 40 45 50 55 A e v r e g a t i l f l a t y c n e ( k c o c l s e c y c l ) 2D mesh. Routing Comparison. Width 64 bits. Uniform traffic DOR MDR PMDR 5 0 5 10 15 20 25 Injection rate (flits/cycle * 100) 30 35 10 15 20 25 30 35 40 45 A e v r e g a t i l f l a t y c n e ( k c o c l s e c y c l ) 2D FBFly. Routing comparison. Width 64 bits. Uniform traffic DOR MDR = PMDR Figure 2. Comparison of DOR, MDR and PMDR routing algorithms. Local request (lowest prio) R s e u q e t s ( u o t u p t + i t m s e t a m p ) G r n a t a n g s i l s Figure 3. BLESS allocator implementation. A. Allocator Complexity We design and implement an age-based BLESS allocator, shown in Figure 3. Requests are partially ordered according to their age by the sorting blocks shown on the left, each of which sorts two requests. The output arbiters satisfy the oldest request ﬁrst, and forward conﬂicting requests to the next arbiter. The oldest incoming ﬂit is guaranteed to not be deﬂected, thus preventing livelocks. Although the logic of each arbiter is simple, all requests need to be granted [18] because all ﬂits need to be routed. This creates a long critical path that passes through each output. This critical path scales linearly with router radix. Note that this design would perform slightly worse than the idealized BLESS allocator that we use in our simulation-based evaluation, as it trades off accuracy for cycle time. Table I compares our BLESS allocator and an input-ﬁrst separable speculative switch allocator with round-robin arbiters for a VC network with 2 VCs [3]. Dynamic power is estimated by applying Synopsys Design Compiler’s default activity factor Figure 4. Empty buffer bypassing. Table I A LLOCATOR SYN TH E S I S COM PAR I SON . Aspect Sep. I.-F. Age-based Delta Number of nets Number of cells Area (µm2 ) Cycle time (ns) Dyn. power (mW) 1165 1100 2379 1.6 0.48 1129 1050 2001 2.9 0.27 0% 0% -16% +81% -44% to all inputs. The VC network switch allocator represents a reasonable design and has comparable cycle time to other separable and wavefront allocators [3], [5]. We compare against the switch allocator because a similar VC allocator has a shorter critical path and speculation parallelizes VC and switch allocation [3]. As Table I shows, the age-based allocator has an 81% larger delay. If the allocator is on the router’s critical path, this will either degrade network frequency or increase zero-load latency if the allocator is pipelined. The separable allocator has a larger area than the age-based allocator, and consumes clock tree and ﬂip-ﬂop power because it needs to maintain state for the roundrobin arbitration. However, the difference is a small fraction of the overall router power. 11                         10 0 10 20 30 Injection rate (flits/cycle * 100) 40 50 20 30 40 50 60 70 80 90 100 A e v r e g a l a t y c n e ( k c o c l s e c y c l ) 8x8 2D mesh. Width 64 bits. Uniform traffic VC−buffered Deflection 2 0 10 20 30 Injection rate (flits/cycle * 100) 40 50 3 4 5 6 7 8 9 10 P o w e r u s n o c m p i t n o ( W a t t s ) 8x8 2D mesh. Width 64 bits. Uniform traffic VC−buffered Deflection 0 0 10 20 30 40 50 Injection rate (flits/cycle * 100) 60 70 10 20 30 40 50 60 70 A e v r e g a l a t y c n e ( k c o c l s e c y c l ) 4x4 2D FBFly. Width 64 bits. Uniform traffic VC−buffered Deflection 1 0 10 20 30 40 50 Injection rate (flits/cycle * 100) 60 70 2 3 4 5 6 7 8 9 P o w e r u s n o c m p i t n o ( W a t t s ) 4x4 2D FBFly. Width 64 bits. Uniform traffic VC−buffered Deflection Figure 5. Latency and power comparison. B. Buffer Cost Our models assume efﬁcient custom SRAM blocks for buffers [1], which impose a smaller overhead compared to other implementation options. However, designers might be unable to use such designs, e.g. due to process library constraints. Using either standard-size SRAM blocks or ﬂip-ﬂop arrays will likely make buffers more costly, increasing the motivation for eliminating them or reducing their size. Additionally, we use empty buffer bypassing [24], shown in Figure 4. This allows ﬂits to bypass empty buffers in the absence of contention, reducing dynamic power under light load. We ﬁnd that these schemes sufﬁce to implement efﬁcient buffers, but additional techniques could be applied. For instance, leakage-aware buffers [20] reduce leakage by directing incoming ﬂits to the least leaky buffer slots and supply-gating unused slots. Also, to increase buffer utilization and reduce buffer size, researchers have proposed dynamic buffer allocation [19]. To check the accuracy of our models, we synthesized and then placed and routed a 5×5 mesh VC router with 2 VCs and 8 buffer slots per VC. Due to process library constraints, we were only able to use ﬂip-ﬂop arrays for buffers. With this implementation, the ﬂip-ﬂop arrays occupied 62% of the overall router area and consumed 18% of the router power at a 20% ﬂit injection rate with uniform random trafﬁc. A compiler-generated SRAM block from the same library would occupy 13% of the router area. The SRAM area results are in line with our models. V I . EVALUAT ION This section presents a quantitative comparison of BLESS and VC ﬂow control and discusses design tradeoffs. A. Latency and Throughput Figure 5 presents latency and power as a function of ﬂit injection rate. VC routers are optimized for throughput per unit power. They have 6 VCs with 9 buffer slots per VC in the mesh, and 10 slots per VC in the FBFly. The VC network has a 12% lower latency for the mesh on average across injection rates, and 8% lower for the FBFly. At a sample 20% ﬂit injection rate, the VC network has a 17% lower latency for the mesh, and 10% lower for the FBFly. Deﬂecting ﬂits increases the average channel activity factor for a given throughput. As shown, the VC network provides a 41% higher throughput for the mesh, and 96% higher for the FBFly. If we average over the set of trafﬁc patterns, the VC network provides a 24% and 15% higher throughput for the mesh and FBFly, respectively. Due to the larger activity factor, the deﬂection network consumes more power than the buffered network for ﬂit injection rates higher than 7% for the mesh and 5% for the FBFly. For lower injection rates, buffer power is higher than the power consumed by deﬂections. However, even then the deﬂection network never consumes less power than 98.7% of the VC network power for the mesh, and 98.9% for the FBFly. The higher power in VC networks is mainly due to buffer leakage. 12                     s e c n e r u c c o f o e g a t n e c r e P 60 50 40 30 20 10 0 0 8x8 2D mesh. Uniform traffic. 20% flit injection rate VC−buffered Deflection 10 20 30 40 50 Flit contention or deflection latency (clock cycles) 60 Figure 6. Blocking or deﬂection latency distribution. These small power gains are outweighed by the allocator complexity and the other issues discussed in this paper. Furthermore, if a network always operates at such low injection rates, it is likely overdesigned because the datapath width need not be this wide, making the network more expensive than necessary. Without empty buffer bypassing, the dynamic buffer power increases signiﬁcantly, as detailed in Section VI-B. In this case, the deﬂection network consumes less power for ﬂit injection rates lower than 17% for the mesh, and 12.5% for the FBFly. This emphasizes the importance of buffer bypassing. Figure 6 shows the distribution of cycles that ﬂits spend blocked in buffers or being deﬂected in the 2D mesh for a 20% injection rate. It was generated by subtracting the corresponding zero-load latency from each ﬂit’s measured latency. Since the latency imposed by an extra hop is 3 cycles (2 cycles to traverse a router and 1 to traverse a link), and each deﬂection adds an even number of hops, the deﬂection network histogram has spikes every 6 cycles. Thus, this graph also shows the distribution of the number of deﬂections per ﬂit. In contrast, the VC network has a smooth latency distribution. The average blocking latency for the VC network is 0.75 cycles with a standard deviation of 1.18, while the maximum is 13 cycles. For the deﬂection network, the average is 4.87 cycles with a standard deviation of 8.09, while the maximum is 108 cycles. Since the average zero-load latency is 19.5 cycles, the VC network has 17% lower latency. These higher latency variations may be crucial to performance: in timeout-based protocols, high latencies will cause spurious retransmissions, and many workloads use synchronization primitives that are constrained by worst-case latency (e.g. barriers). Figure 7 shows throughput versus area and power Paretooptimal curves for both networks. Each point of each curve represents the maximum packet throughput achievable by a design of a given area or power. Results are averaged over the set of trafﬁc patterns of each topology. These curves were generated by sweeping the datapath width so that a packet consists of 3 to 18 ﬂits. They illustrate that power or area savings of a network can be traded for a wider datapath, which increases maximum throughput. Thus, points of equal area or power do not indicate an equal datapath width. As illustrated, the mesh VC network provides 21% more throughput per unit power on average, and requires 19% less 13 power to achieve equal throughput compared to BLESS. The deﬂection network provides 5% more throughput per unit area due to the buffers occupying 30% of the area, as explained in Section VI-B. Consequently, the deﬂection network requires 6% less area to achieve equal throughput. If the VC network was optimized for area, the buffers would be signiﬁcantly smaller. The FBFly VC network provides 21% and 3% more throughput per unit power and area respectively. Achieving equal throughput requires 19% less power and 3% less area. Widening the datapath favors the buffered network. While buffer and channel costs scale linearly with datapath width, crossbar cost scales quadratically. Therefore, taking extra hops becomes more costly. Allocation cost becomes less signiﬁcant because it is amortized over the datapath width. However, that cost is relatively small, as shown in Section VI-B. The quadratic crossbar cost is also the reason the VC FBFly is more area efﬁcient than the deﬂection network, since widening the datapath to equalize throughput has a larger impact in the area of highradix routers. B. Power and Area Breakdowns Figure 8 shows the power and area breakdowns for the mesh with a 20% ﬂit injection rate. Each router has 6 VCs, with 9 buffer slots per VC. The buffer cost without bypassing is included. Output clock and FF refer to the pipeline ﬂip-ﬂops at output ports that drive the long channel wires. Crossbar control is the power for the control wires routed to crossbar crosspoints. Channel traversal refers to the power to traverse the repeated channel segments. Channel clock is the clock wire power to the channel pipeline ﬂip-ﬂops. Leakage power is included for buffers and channels. For a 20% ﬂit injection rate, the average channel activity factor is 24.7% on the VC network and 29.3% on the deﬂection network. The extra 4.6% is due to deﬂections. This extra power equals 5.5× the buffer access and leakage power. Buffer leakage power is only 0.6% of the overall network power. Removing the buffers saves 30% of the overall network area. The same SRAM buffers without bypassing consume 8.5× the dynamic power with bypassing. In general, there is no ﬁxed relationship between the number of buffering events in a VC network and the number of deﬂections in a BLESS network, but intuitively, both increase at roughly the same rate with network utilization, as they depend on contention events. Thus, it is insightful to compare the energy of a buffer read and write with the energy consumed in a deﬂection. Writing and then reading a single 64-bit ﬂit from and to an input buffer consumes 6.2pJ, while a channel and router traversal takes 20.9pJ (80% of this energy is consumed in the channel). A deﬂection induces at least 2 extra traversals, causing 42pJ of energy consumption, 6.7× the dynamic energy for buffering the contending ﬂit instead. Therefore, increasing router and channel traversals with deﬂections is not energyefﬁcient. C. Low-swing Channels Low-swing channels favor the deﬂection network because they reduce deﬂection energy. With our low-swing channel model, the VC mesh network offers 16% more throughput per unit power than the deﬂection network for the mesh and 18% more for the FBFly. Also, the VC network offers comparable     0 0 5 10 15 20 Power consumption (W) 25 30 2 4 6 8 10 12 14 A e v r e g a m x a i m u m t h r u p h g u o t ( e k c a p t s / e c y c l * 0 0 1 ) 8x8 2D mesh. Average of 6 traffic patterns VC−buffered Deflection 0 0 0.5 1 1.5 2 2.5 Area (square mm) 3 3.5 4 2 4 6 8 10 12 14 A e v r e g a m x a i m u m t h r u p h g u o t ( e k c a p t s / e c y c l * 0 0 1 ) 8x8 2D mesh. Average of the 6 traffic patterns  VC−buffered Deflection 1 0 2 4 6 8 10 Power consumption (W) 12 14 16 2 3 4 5 6 7 8 9 10 11 A e v r e g a m x a i m u m t h r u p h g u o t ( s e k c a p t / e c y c l * 0 0 1 ) 4x4 2D FBFly. Average of the 8 traffic patterns VC−buffered Deflection 1 0 0.5 1 1.5 Area (square mm) 2 2.5 3 2 3 4 5 6 7 8 9 10 11 A e v r e g a m x a i m u m t h r u p h g u o t ( s e k c a p t / e c y c l * 0 0 1 ) 4x4 2D FBFly. Average of the 8 traffic patterns VC−buffered Deflection Figure 7. Throughput versus power and area Pareto-optimal curves. (1% more) throughput per unit area for the mesh, and 6% more for the FBFly. This increase in area efﬁciency for the VC network is due to differential signaling, which doubles the channel area, thus reducing the percentage of the total area occupied by the buffers to 19%. Moreover, the deﬂection network consumes less power for ﬂit injection rates smaller than 11% for the mesh, and 8% for the FBFly. However, compared to the VC network, the power consumed by the deﬂection network is never less than 98.5% for the mesh and 99% for the FBFly. Figure 9 illustrates the results for the mesh. D. Deadlock and Endpoint Buffers In a network with a request-reply protocol, destinations might be waiting for replies to their own requests before being able to serve other requests [10]. Those replies might be sent from a distant source or might face heavy contention. Therefore, arriving requests might ﬁnd the destination’s ejection buffers to be full, without a mechanism to prevent or handle this scenario. Preventing this requires ejection buffers able to cover for all possible sources and their maximum outstanding requests. As an example, in a system with 64 processors where each node can have 4 outstanding requests to each of four cache banks (16 requests total), each processor and cache bank needs to buffer 256 requests. This requires a total buffer space of 128KB, whereas an 8×8 2D mesh with 2 VCs, each having 8 64bit buffer slots, needs 20KB. Note that 20KB is only a small fraction of the system-wide SRAM memory in many designs (e.g. CMPs with multi-megabyte caches). Alternatively, ﬂits that cannot be buffered can be dropped, deﬂected back to the router, or extra complexity needs to be added, such as feedback to the router so that ﬂits are sent to the ejection port only if there is buffer space for them. This issue becomes more severe with more complex protocols. E. Flit Injection Injection in deﬂecting ﬂow control requires feedback from the router because at least one output port must be free [18]. However, acquiring this information is problematic, specially if the round-trip distance between the router and the source is more than one clock cycle. Alternatively, ﬂits can be deﬂected back to the source if there is no free output. However, this causes contention with ejecting ﬂits and costs extra energy. In any case, the injection buffer size may need to be increased to prevent the logic block (e.g. the CPU) from blocking. F. Process Technology Our evaluation uses a 32nm high-performance ITRS-based process as a worst case due to its high leakage current. To illustrate the other extreme, we use the commercial 45nm lowpower library used for synthesis in Section V-A. Its leakage current is negligible. With empty buffer bypassing, the deﬂection network never consumes less energy than the VC network. Both 14                                         VC BLESS 0 1 2 3 Power (W) 4 5 6 Chanel power breakdown Traversal Clock Flip-flop Leakage VC BLESS 0.00 0.05 0.10 0.15 0.20 0.25 Power (W) 0.30 0.35 0.40 0.45 Router power breakdown (excluding buffers) Xbar traversal Xbar ctrl Xbar leakage Output FF Output leakage Bypass No bypass 0.00 0.05 0.10 0.15 0.20 Power (W) 0.25 0.30 0.35 0.40 Buffer power breakdown in VC networks Dynamic Leakage 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 A r a e ( m m 2 ) Area breakdown Channel Crossbar Output SRAM Buffers Figure 8. Power and area breakdowns for the 2D mesh under a 20% ﬂit injection rate with full-swing channels. 2.5 0 10 20 30 Injection rate (flits/cycle * 100) 40 50 3 3.5 4 4.5 5 5.5 6 P o w e r u s n o c m p i t n o ( W a t t s ) 8x8 2D mesh. Low−swing channels. Uniform traffic VC−buffered Deflection 0 0 2 4 6 8 10 Power consumption (W) 12 14 16 2 4 6 8 10 12 14 A e v r e g a m x a i m u m t h r u p h g u o t ( e k c a p t s / e c y c l * 0 0 1 ) 8x8 2D mesh. Low−swing. Average of the 6 patterns VC−buffered Deflection Figure 9. Power consumption with varying injection rate and throughput-power Pareto-optimal curves with low-swing channels. consume approximately the same amount of power even for very low injection rates. Furthermore, the VC mesh described in Section VI-A provides 21% more throughput per unit power and 10% more throughput per unit area. Therefore, there are no design points that would make the deﬂection network more efﬁcient in this process. Changing process technologies affects the buffer to overall network power cost ratio. Extremely costly buffer implementations would increase this ratio in favor of the bufferless network. In such processes, the bufferless network might be the most efﬁcient choice. However, even the 32nm high-leakage process we used does not fall in this category. In any case, design effort should ﬁrst be spent on implementing the buffers more efﬁciently before considering bufferless networks. V I I . D I SCU S S ION Our quantitative evaluation tries to cover the design parameters that are most likely to affect the tradeoffs between buffered and bufferless networks. However, it is infeasible to characterize the full design space quantitatively. In this section we qualitatively discuss the effect of varying additional parameters. Trafﬁc classes: Systems requiring a large number of trafﬁc classes or VCs may have allocators slower than the agebased allocator of Section V-A. However, more trafﬁc classes also increase the demand for endpoint buffering discussed in Section VI-D. For a single trafﬁc class, we have shown that at least a buffered network with 2 VCs is more efﬁcient than a deﬂection network. Network size: While network size affects the relevant tradeoffs, smaller networks provide fewer deﬂection paths. The deﬂection and buffering probabilities are similarly affected by size. Thus, none of the two networks is clearly favored by varying network size. Sub-networks: A deﬂection network design could be divided into sub-networks to make it more efﬁcient, but the same is true for the VC network. For each sub-network of the deﬂection network, we can apply our ﬁndings to design a similar and more efﬁcient buffered network. Dropping ﬂow-control: Dropping ﬂow control faces different challenges. For example, its allocators are not constrained to produce a complete matching. However, dropping ﬂow control requires buffering at the sources. Dropping, as deﬂecting, causes ﬂits to traverse extra hops, which translates to energy cost and increased latency. Therefore, the fundamental tradeoff between buffer and extra hop costs remains. However, the number of extra hops in dropping networks is affected by topology and routing more than in deﬂection networks. In general, dropping ﬂow control may be more or less efﬁcient than deﬂection ﬂow control, depending on a particular network design. 15                 Self-throttling sources: In our evaluation, trafﬁc sources do not block under any condition (e.g. if a maximum number of outstanding requests is reached). Self-throttling sources are more likely to be blocked when using a deﬂection network due to its latency distribution, as discussed in Section VI-A. Blocking the sources hides the performance inefﬁciencies of the network by controlling the network load. This favors networklevel metrics, but penalizes system performance. For example, in a CMP, blocking the CPUs increases execution time, which is the performance measurable by end users. Complete system implementations are likely to use self-throttling sources. Therefore, performing an equitable comparison requires taking the number of cycles that sources are blocked into account. V I I I . CONC LU S ION S We have compared state-of-the-art buffered (VC) and deﬂection (BLESS) ﬂow control schemes. We improve the bufferless network by proposing MDR to reduce deﬂections. This reduces average latency by 5% in an 8×8 2D mesh, compared to DOR. We also assume efﬁcient SRAM-based buffers that are bypassed if they are empty and there is no contention. The deﬂection network with MDR consumes less power up to a ﬂit injection rate of 7%. However, it never consumes less power than 98.7% of that of the VC network. Networks constantly operating at low injection rates are likely overdesigned as they don’t need such large datapaths. In the same 8×8 2D mesh, VC ﬂow control provides a 12% smaller average latency compared to deﬂection ﬂow control. At a ﬂit injection rate of 20%, the average VC network blocking ﬂit latency is 0.75 cycles with a standard deviation of 1.18, while for the deﬂection network the average deﬂection latency is 4.87 cycles with a standard deviation of 8.09. The VC network achieves a 21% higher throughput per unit power. Furthermore, the BLESS allocator has an 81% larger cycle time than a separable input-ﬁrst round-robin speculative switch allocator. Finally, bufferless ﬂow control needs large buffering or extra complexity at network destinations in the presence of a communication protocol. Our work extends previous research on deﬂection ﬂow control by performing a comprehensive comparison with buffered ﬂow control. Our main contribution is providing insight and improving the understanding of the issues faced by deﬂection ﬂow control. Our results show that unless process constraints lead to excessively costly buffers, the performance, cost and complexity penalties outweigh the potential gains from removing the router buffers. Even for the limited operation range where the bufferless network consumes less energy, that energy is negligible (up to 1.5%) and is accompanied by the shortcomings presented in this paper. Therefore, we believe that design effort should be spent on more efﬁcient buffers before considering bufferless ﬂow control. ACKNOW L EDG EM EN T S We sincerely thank Daniel Becker, Nathan Binkert, Jung Ho Ahn and the anonymous reviewers for their useful comments. This work was supported by the National Science Foundation under Grant CCF-0702341, the National Security Agency under Contract H98230-08-C-0272, the Stanford Pervasive Parallelism Lab, the Gigascale Systems Research Center (FCRP/GSRC). George Michelogiannakis was supported by a Robert Bosch Stanford Graduate Fellowship. Daniel Sanchez was supported by a Fundacion Caja Madrid Fellowship and a Hewlett-Packard Stanford School of Engineering Fellowship. "
Power-Efficient and High-Performance Multi-level Hybrid Nanophotonic Interconnect for Multicores.,"Network-on-Chips (NoCs) are becoming the defacto standard for interconnecting the increasing number of cores in chip multiprocessors (CMPs) by overcoming the scalability and wire delay problems of shared buses. However, recent research has shown that future NoCs will be limited by power dissipation and reduced performance forcing architects to explore other technologies that are complementary metal oxide semiconductor (CMOS) compatible. In this paper, we propose ET-PROPEL (Extended Token based Photonic Reconfigurable On-Chip Power and Area-Efficient Links) architecture to utilize the emerging nanophotonic technology to design a high-bandwidth, low latency and low power multi-level hybrid interconnect that balances cost and performance. We develop our interconnect at three levels: at the first level (x) we design a fully connected network for exploiting locality; at the second level(y), we design a shared channel using optical tokens to reduce power while providing full connectivity and at the third level (z), we propose a novel nanophotonic crossbar that provides scalable bisection bandwidth. The first two levels are combined into T-PROPEL(token-PROPEL, 64 cores) and four separate T-PROPELs are combined into ET-PROPEL (256 cores). We have simulated both T-PROPEL and ET-PROPEL using synthetic and SPLASH-2 traffic, where our results indicate that T-PROPEL and ET-PROPEL significantly reduce power(10-fold) and increase performance (3-fold) over other well known electrical and photonic networks.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip Power-Efﬁcient and High-Performance Multi-Level Hybrid Nanophotonic Interconnect for Multicores Randy W. Morris, Jr. and Avinash Karanth Kodi School of Electrical Engineering and Computer Science, Ohio University, Athens, OH 45701 E-mail:rmorris@prime.cs.ohiou.edu, kodi@ohio.edu Abstract—Network-on-Chips (NoCs) are becoming the defacto standard for interconnecting the increasing number of cores in chip multiprocessors (CMPs) by overcoming the scalability and wire delay problems of shared buses. However, recent research has shown that future NoCs will be limited by power dissipation and reduced performance forcing architects to explore other technologies that are complementary metal oxide semiconductor (CMOS) compatible. In this paper, we propose ET-PROPEL (Extended Token based Photonic Reconﬁgurable On-Chip Power and Area-Efﬁcient Links) architecture to utilize the emerging nanophotonic technology to design a high-bandwidth, low latency and low power multi-level hybrid interconnect that balances cost and performance. We develop our interconnect at three levels: at the ﬁrst level (x) we design a fully connected network for exploiting locality; at the second level (y), we design a shared channel using optical tokens to reduce power while providing full connectivity and at the third level (z), we propose a novel nanophotonic crossbar that provides scalable bisection bandwidth. The ﬁrst two levels are combined into T-PROPEL (token-PROPEL, 64 cores) and four separate T-PROPELs are combined into ET-PROPEL (256 cores). We have simulated both T-PROPEL and ET-PROPEL using synthetic and SPLASH-2 trafﬁc, where our results indicate that T-PROPEL and ETPROPEL signiﬁcantly reduce power (10-fold) and increase performance (3-fold) over other well known electrical and photonic networks. I . INTRODUC T ION As continuous transistor scaling provides chip designers with billions of transistors, system architects have embraced multicore architectures to provide the exponential growth in performance. To overcome the scalability and wire delay problems of shared buses [1], most recent designs have adopted a modular, regular and packet-switched communication paradigm called Network-on-Chips (NoCs) [2], [3]. However, with increasing number of cores, the design of NoCs connecting these cores has become extremely critical from both power and performance viewpoint [4]. For example, in the Intel TeraFLOPS processor architecture, the interconnect consumes more than 28% of the total power budget, when the expected power budget should be less than 10% [5]. Moreover, fundamental signalling limitations (reﬂections, crosstalk), electromagnetic interference (EMI), clock skew and other problems associated with metallic interconnects will only exacerbate the power dissipation and performance limitations of NoCs with further technology scaling [6], [7], [8]. Nanophotonic technology is an emerging solution for future on-chip interconnects and provides several signiﬁcant advantages over metallic interconnects such as: (1) bit rates independent of distance, (2) higher bandwidth due to multiplexing of wavelengths, (3) larger bandwidth density by multiplexing wavelengths on the same waveguide/ﬁber, (4) lower power by dissipating power only at the endpoints of the communication channel and many more [9]. In addition, nanophotonic technology used for on-chip applications enable higher bandwidth through dense wavelength division multiplexing (DWDM) and low latency. Recent developments in nanophotonics have demonstrated functional devices such as micro-ring resonators [10], waveguides, and photodetectors [11] with on-chip dimensions and compatible with complementary metal oxide semiconductor (CMOS) [12], [13], [14], [15], [16], [17]. These nanophotonic device level developments combined with future interconnect uncertainty have fuelled tremendous interest in nanophotonics for future on-chip interconnects. In this paper, we propose to utilize the emerging ﬁeld of nanophotonics to design ET-PROPEL (Extended-Token based Photonic Reconﬁgurable On-Chip Power and AreaEfﬁcient Links); a high bandwidth, low latency and low power nanophotonic architecture that combines wavelength division multiplexing (WDM), space division multiplexing (SDM), optical tokens and nanophotonic crossbars to develop a two-hop network for 256 cores. ET-PROPEL is developed as a multi-level interconnect that utilizes different interconnect designs at each level to balance power consumption, area overhead and overall network performance. The ﬁrst two levels (x and y) are combined into T-PROPEL (token-PROPEL). At the ﬁrst level (x), we design a fully connected network to exploit local communication among neighboring tiles. At the second level (y), we extend the ﬁrst level (x) to communicate with the second level by sharing optical tokens to provide complete connectivity without exponentially increasing the number of components - a balance between area and performance. At the third level (z), we combine four T-PROPELs using fatree topology to design ET-PROPEL by using arbitration free nanophotonic crossbars as this provides a high bisection bandwidth. The N (cid:2) N nanophotonic crossbar is designed using waveguides and double micro-ring resonators for optical switching. We designed ET-PROPEL with a hybrid of nanophotonic multi-level communication paradigms because we feel no one implementation (ﬂat topologies) is best for all levels of communication. For example, a fully connected interconnection network will require a large number of optical components (area overhead), a shared interconnection network with optical tokens will increase the level of contention (latency) and arbitration free optical crossbars are difﬁcult or almost impossible 978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.30 207 An optical receiver performs the optical-to-electrical conversion of data, and is comprised of a photodetector, a transimpedance ampliﬁer (TIA), and a voltage ampliﬁer. A photodetector is used to convert the incoming optical beam into an electrical current. Due to Si transparency to wavelengths of light found in optical communication (1.1 - 1.55 mm), pure Si is not the right material for on-chip photodetection. Recent research has shown Germanium-on-Silicon-on-Insulator (Geon-SOI) detectors are a viable solution for CMOS compatible on-chip applications [11]. A recent demonstrated Si-CMOSAmpliﬁer has power dissipation of 1.1 mW/Gbps with a data rate of 10 Gbps [11]. In addition, the Si-CMOS-Ampliﬁer has an area overhead of 175 mm x 150 mm, making it a valuable TIA and voltage ampliﬁer for on-chip applications. ET-PROPEL is designed using micro-ring resonators, silicon waveguides and Ge-on-SOI photodetectors. I I I . T-PROPEL : ARCH I TECTURE AND IM PL EM EN TAT ION A. Architecture Prior research has shown that optics is more advantageous compared to electronics in terms of power, delay, and area at core-to-core distances for 22nm and 17nm technology nodes [14], [15], [13], [12]. Due to this, we choose 22nm technology for our work. In T-PROPEL, we combine four cores together and connect them with a shared L2 cache, which we call a tile. This grouping reduces the cost of the interconnect as every core does not require lasers attached and more importantly, facilitates local communication through cheaper electronic switching [19]. Figure 1 shows the layout of T-PROPEL, which consists of 16-tiles in a grid fashion with 4-tiles in x (level one) and y-directions (level two). Communication in the x-direction (level one) takes place directly without optical arbitration as all tiles in each x-direction are fully connected. For communication in the y-direction, optical tokens are required as multiple tiles share the same waveguides. This design results in a one hop network and a detailed explanation of the operations of T-PROPEL are given in the next subsection. It should be noted, that we chose to use tokens for the y-direction only as this will result in a maximum of one hop with minimal increase in the number of nanophotonic components. B. T-PROPEL: Inter Tile Communication In T-PROPEL, we adopt two different levels of communication. For source and destination tiles that are in the same row (x direction, level one), we adopt an arbitration free fully connected optical communication. If the source and destination tiles are located in different rows (y direction, level two), we use optical tokens shared by x-direction tiles for communication. In the following paragraph, we describe in detail the operation and waveguide assignment for communication that takes place in the x-direction. Figure 2 shows tiles (3,0) to (3,3) arranged along the xdirection. Each waveguide is associated with a unique source and destination tile and all wavelengths that traverse the waveguide are used to communicate with the source and destination. This design requires three waveguides per tile or Fig. 1. Proposed layout of a T-PROPEL architecture for 64 cores and to implement (complexity). We provide a quantitative analysis by comparing the performance of T-PROPEL and ET-PROPEL to other leading electrical and nanophotonic NoC designs in terms of throughput, latency and power for both synthetic and SPLASH-2 benchmarks for 64 and 256 cores. Our results indicate that T-PROPEL and ET-PROPEL signiﬁcantly reduce power (10-fold) and increase performance (3-fold) over other well known electrical and photonic networks. In what follows, we brieﬂy describe the nanophotonic components, proposed architecture and results. I I . S I L ICON NANO PHOTON IC D EV IC E S AND COM PON ENT S In this section, we brieﬂy describe the silicon nanophotonic interconnects and components. Nanophotonic interconnect will require (i) lasers to generate the carrier signal, (ii) modulators and drivers to encode the data, (iii) medium (waveguides, ﬁbers, free space) for signal propagation, (iv) photodetectors to detect light and (v) back-end signal processing (transimpedance ampliﬁers (TIA), voltage ampliﬁers, clock and data recovery) to recover the transmitted bit. Indirect modulation with an external laser will need an on-chip modulator. Micro-ring resonators [10] is a common device used for indirect modulation of an optical signal and have been demonstrated with small footprints (10 mm) and lower power dissipation (0.1 mW) [10]. In addition, microring resonators have been demonstrated with extinction ratios greater than 9 dB, optical losses as low as -0.12 dB/cm, and modulator insertion loss of 1 dB, which are sufﬁcient for the receiver design used in ET-PROPEL [13], [11], [10], [18]. CMOS compatible Silicon (Si) waveguides allow for signal propagation. Recent research has shown that it is possible to multiplex 64 wavelengths of light onto a single waveguide with 60 GHz spacing between wavelengths [13]. In addition, waveguides with submicron-size cross-sections (0.5 mm) and low-loss (1.3 dB/cm) have been demonstrated [12]. 208 Fig. 2. The routing and waveguide assignment proposed for x-direction communication. a total of 12 waveguides for x-direction. In the ﬁgure, the 12 waveguides are implemented in a u-shape. This u-shape design allows for optical data to be modulated on the waveguides in the ﬁrst pass and for the optical data to be received on the second pass. In the ﬁrst pass, each tile modulates data on three different waveguides, which will be received by three other tiles. In Figure 2, two enlarged sections of the 12 waveguides show how 64 micro-ring resonators are placed along the waveguide for both modulation and detection. The routing in the y-direction when the source and destination tiles are in different rows requires optical tokens as all xdirection tiles share the same waveguide for communication to the same destination. These optical tokens allow for arbitration of the waveguides, so two or more tiles do not communicate on the same waveguide at the same time. In y-direction communication, dedicated waveguides are routed past a row of tiles where they will modulate the light and are then routed to a destination y-direction tile where the data will be received. Since there are four tiles located in any row, only these four tiles will arbitrate for a speciﬁc destination waveguide. This results in a total of 13 waveguides needed for each row of tiles, where one of 12 waveguides are used to communicate with 12 tiles on different rows and an addition waveguide for optical tokens. The optical token waveguide circulates optical tokens which are used for arbitration and only requires 12 wavelengths, one for each tile not located in the same row. Figure 3 shows the layout and functionality for communication in the y-direction. In Figure 3, we only show how the top x-direction tiles communicate with the far left y-direction tiles and how the bottom x-direction tiles communicate with the far right tiles for clarity purposes. To illustrate with an example, suppose Tile (3,3) needs to communicate with Tile (0,0). First Tile (3,3) will capture the token needed to communicate with Tile (0,0). This can be accomplished by turning on the micro-ring resonator that corresponds to the needed token (wavelength). After Tile (3,3) captures the corresponding token, it modulates the light on the waveguide that is Fig. 3. The layout and implementation for y-direction communication. Each modulation and detection point in the ﬁgure except for the optical token waveguide represents 64 micro-ring resonators. connected directly to Tile (0,0). When Tile (3,3) completes the communication with Tile (0,0) it will inject the token back into the optical waveguide for other tiles to use. To show the reverse communication, Tile(0,0) turns on the micro-ring resonator needed to capture the token for communication with Tile (3,3). After capturing this token, Tile (0,0) communicates directly with Tile (3,3). When communication is complete, the token is injected back into the optical token waveguide. The y-direction communication described between Tile (3,3)/Tile (0,0) and Tile (0,0)/Tile (3,3) is identical for any combination of source and destination tiles. In our design, we limit the sharing of the tokens between four tiles, which reduces the contention enabling a high-throughput, low latency interconnect design. Optical tokens are required for arbitration as one waveguide is shared between four tiles during y-direction communication. Each tile in T-PROPEL controls the injection of the token (in the x-direction) for communication in the y-direction. For example from Figure 3, Tile (3,0) controls the injection of optical tokens for Tile (3,0), Tile (3,1), Tile (3,2) and Tile (3,3) that are used for communication with Tile (0,0), Tile (1,0) and Tile (2,0). Tile (3,0) is called the token control tile for the above y-direction communication. To further emphasize this, Tile (0,3) controls the injection of the tokens for Tile (0,0), Tile (0,1), Tile (0,2) and Tile (0,3) that want to communicate with Tile (1,3), Tile (2,3) and Tile (3,3). IV. ET-PROPEL : ARCH I T EC TUR E AND IM P LEM EN TAT ION T-PROPEL can be scaled directly to 256 cores by connecting 64 tiles in a 2-D grid manner, but this would result in a large network design with each x-direction requiring 56 tokens and 112 waveguides. In order to reduce the number of optical components, reduce power consumption and provide scalable bisection bandwidth, we propose an alternate design, called (extended) ET-PROPEL, with which we can increase the communication bandwidth without signiﬁcantly increasing 209 Fig. 5. (a) Double micro-ring resonator switching two optical light beams. (b) Proposed 4-input 64-wavelength nanophotonic crossbar implementation using micro-ring resonators. l(0) Fig. 4. The proposed ET-PROPEL architecture. Each of the clusters is the original T-PROPEL architecture designed for 64 cores. Inter-cluster connectivity is established using 4-input 64-wavelength optical crossbar implemented using micro-ring resonators. All combination are not shown for clarity. Each nanophotonic crossbar in the ﬁgure represent four crossbar which are also not shown for clarity. the cost of the network. We utilize optical crossbars that can provide N (cid:2) N switching functionality. An optical crossbar allows incoming light to be switched from one waveguide to another depending on the wavelength and the input waveguide. Using double micro-ring resonators and unique waveguide routing, we design an optical crossbar device with dimensions suitable for on-chip applications. The proposed ET-PROPEL is shown in Figure 4. We combine four 64-core T-PROPELs using optical crossbars to design a 256-core ET-PROPEL. We adopt a fat tree topology with multiple roots to provide a scalable inter-cluster different T-PROPELs are connected together with the 4 (cid:2) 4 bandwidth. Every tile with similar coordinates Tiles (x,y) on optical crossbars. For example, Tiles (0,0) on T-PROPEL 0, 1, 2 and 3 are connected together with the optical crossbar. Similarly, Tiles (0,1) on T-PROPEL 0, 1, 2 and 3 are connected together with another optical crossbar and so on. 0 0 A. Nanophotonic Crossbar implementation This subsection discusses the construction of an nanophotonic crossbar using double micro-ring resonators. A double micro-ring resonator consists of two micro-ring resonators that are placed in between two waveguides to retain the same direction of light from input to output port. Figure 5(a) shows the operating principle of a double micro-ring resonator allowing light of the same wavelength to be switched between the two waveguides. l(1) is switched to the bottom waveguide and l(2) is switched to the top waveguide. Figure 5(b) shows the double micro-ring resonator implementation of a 64wavelength 4 (cid:2) 4 optical crossbar. It should be mentioned that each micro-ring resonator in the ﬁgure represents 16 microring resonators which are not shown for clarity and would be placed adjacent to each other allowing the 15 additional wavelengths to be switched between waveguides. In the double ring optical crossbar, wavelengths are switched at locations where two waveguides are running parallel to each other. At this point, light is switched from one waveguide to the other. This switching enables an input port to be connected to all output ports. In Figure 5(b), l(0(cid:0)15) is switched between 210 (48(cid:0)63) . (16(cid:0)31) that waveguide 0 and waveguide 1, waveguide 2 and waveguide 3 and also between waveguide 0 and waveguide 3, l(16(cid:0)31) is switched between of waveguide 1 and waveguide 2, l(32(cid:0)47) is switched between waveguide 0 and waveguide 3 and also between waveguide 1 and waveguide 3, and lastly, l(48(cid:0)63) is between of waveguide 0 and waveguide 2 and also between waveguide 0 and waveguide 3. For a further understanding of the double ring optical crossbar, we will show how light from input 0 is switched and arrives on the 4 outputs. As light travels from input 0, it ﬁrst becomes adjacent with waveguide connecting input 1. At this point, l(0) (0(cid:0)15) is placed on waveguide connecting input 1, allowing input 0 to communicate with output 0 using (0(cid:0)15) . Then light traveling down input 0 becomes adjacent with input 2. At this point, l(0) (48(cid:0)63) is placed on waveguide 2, allowing input 0 to communicate with output 3 using l(0) As the light continues traveling down waveguide 0, it will become parallel with waveguide 3. At this point l(0) is originally from waveguide 0 is placed on waveguide 3, allowing input 0 to communicate with output 1 using l(0) Lastly, l(0) (32(cid:0)48) arrives at output 2 as it was the only light not switched. This allows input 0 to communicated with output 2 using l(0) (48(cid:0)63) . This concept is expanded for other inputs which creates a 4 (cid:2) 4 64-wavelength optical crossbar. B. Design Cost The double rings optical crossbar consists of a total of 8 sets of double-ring resonators. Each set is comprised of 32 microring resonators or a total of 256 micro-ring resonators are used to construct the double rings optical crossbar. The optical double micro-ring resonator has dimensions of 15 mm (cid:2) 20 crossbar contains 96 double micro-ring resonators, where each mm. This includes a 5 mm spacing between each double micro-ring resonator which is used to prevent cross coupling between adjacent micro-ring resonators. This results in the optical crossbar having a height of 80 mm (3 double micro-ring resonator sets and 4 waveguides) and a width of 720 mm (3 double micro-ring sets). The double rings optical crossbar has two waveguide crossings with a maximum distance of about 1 mm. In calculating the optical loss, we used a waveguide loss of -1.3 dB/cm, a micro-ring traversal loss of -1 dB, waveguide cross-over loss of 0.05 -dB and a bending loss of -1 dB. The (16(cid:0)31) . maximum number of double micro-ring resonators an optical beam will need to traverse is two or four single micro-ring resonators. Also, the optical beam will encounter two crossover losses, two bending losses and have a waveguide traversal loss of 1mm. This results in a total optical power loss of 6.23 dB. The double micro-ring nanophotonic crossbar is both power loss efﬁcient (-6.23 dB) and area overhead efﬁcient (0.0576 mm2 ). V. P ER FORMANCE EVALUAT ION In this section, we compare ET-PROPEL and T-PROPEL to well known electrical and nanophotonic networks. For TPROPEL, we compare it to mesh [20], Cmesh [19], and Flattened-butterﬂy [21] in terms of throughput, power and latency. For ET-PROPEL, we compare it to mesh [20], Cmesh [19], Flattened-butterﬂy [21], Fireﬂy [22], and an optical crossbar in terms of throughput, power and latency. A. Energy Comparison and Estimates For electrical interconnects, we consider wires implemented in semi-global metal layers for inter-router links. The wire capacitances, resistances and device parameters were obtained from International Roadmap for Semiconductors and Berkeley Predictive Technology Models. The power per segment of a repeater-inserted wire is given by Psegment = Pd ynamic + Pl eakage +Pshort(cid:0)ckt where Pd ynamic is the switching power, Pl eakage is the power due to the subthreshold leakage current and Pshort(cid:0)ckt is the power due to the short-circuit current [19], [23], [24]. At 90nm technology node, we obtain a link power of 10.27 mW for 1 GHz clock and a Vd d of 1.2 V for a ﬂit width of 128 bits [23] by considering a power-optimal repeater insertion. At 22nm, ITRS projects the clock to be 9 Ghz [25]. For a ﬂit size of 128 bits, the power dissipation will be 198 mW. To reduce the power dissipation at future technology nodes, we reduce the network frequency to 5 Ghz and reduce the power consumption to 110 mW/ﬂit/hop. This number is comparable to power values from [22]. In [22], they use an energy of 19 pJ/ﬂit/hop or 95mW/ﬂit/hop given a 5 GHz clock. At 22 nm, we estimate the buffer power to be 20.15 mW and occupies an area of 185 mm2 , which is similar to the power value from [26]. In [26], they use a energy of 61.7 pJ for a 567 bit ﬂit at 45 nm technology. If a 128 bit ﬂit is scaled to 22 nm, the buffer power would be 17.1 mW. A 5 (cid:2) 5 matrix crossbar with tri-state buffer connectors [27] is considered for the regular NoC design. The area of the crossbar is estimated by the number of input/output signals that it should accommodate. At 22 nm, we estimate the power value for a 5 (cid:2) 5 crossbar to be 21.65 mW and this value is similar to [26]. We believe that the values obtained here for electrical components (buffers, links, crossbars)[22], [26] closely matches to other network designs, giving us conﬁdence in our calculations. For optical links, we assume a power dissipation of 1.1 mW/Gbps per optical receiver[11], and a power dissipation of 0.1 mW/Gbps per modulator [10]. We calculate the off-chip laser power to be 2.63 W if we assume COR E AND CACHE PARAM E TER S U S ED FOR S PLA SH -2 SU I T E S IMULAT ION . TABLE I Parameter L1/L2 coherence L2 cache size/accoc L2 cache line size L1 cache/accoc L1 cache line size Core Frequency(GHz) Threads(core) Issue policy Memory Size(GB) Memory Controllers Value MOESI 4MB/16-way 64 64KB/4-way 64 2.5 2 In-order 4 16 a -9.8 dBm sensitive receiver and a maximum optical loss of 44 dB. B. 64-core Synthetic and SPLASH-2 Trafﬁc Results We evaluated T-PROPEL by comparing it to mesh, Cmesh [19] and ﬂattened-butterﬂy [21] for synthetic trafﬁc. For synthetic trafﬁc, we subjected each network to uniform, bitreversal, perfect-shufﬂe, matrix-transpose, complement and neighboring trafﬁc traces [20]. This provides a method to stress each network with trafﬁc that is found in shared memory applications. We use a ﬂit size of 128 bits and a packet size of 512 bits (4 ﬂits). For each network, we kept the bandwidth the same at 640 Gpbs for each electrical and optical link. This results in a 5 GHz clock for electrical components if we assume a 128 bit ﬂit. In addition, for each router input has 4 VC and each VC can hold a total of 4 ﬂits or one packet. In simulating T-PROPEL, we assumed a one cycle delay to capture and process a token and a one cycle delay to inject the token back into optical waveguide. In addition, we compare T-PROPEL to mesh and ﬂattenedbutterﬂy using real Splash-2 application traces that were collected using Simics, a full system simulator [28]. We evaluated these networks using the following benchmarks: FFT (16 K), LU (512 (cid:2) 512), radiosity (largeroom), Ocean (258 (cid:2) 258), Raytrace (Teapot), Radix (1 M), Water (512), FMM (16K) and Barnes (16 K). Table 1 shows the tile parameters used to evaluate PROPEL. For each SPLASH-2 trace, we use the Multifacet General Execution-driven Multiprocessor Simulator (GEMS) [29] package to provide correct cache coherence requests. Once the traces were collected, each application trace and the synthetic trafﬁc were executed on the cycle accurate NoC simulator. The throughput for all trafﬁc traces for 64-core networks are shown in Figure 6(a). In the ﬁgure, the results are normalized relative to the mesh network, showing the increase in throughput of each network relative to the mesh network. From the ﬁgure, it is easily seen that T-PROPEL has the highest saturation throughput for uniform, bit-reversal and matrix-transpose. In these trafﬁc traces, the low hop count in T-PROPEL (one) coupled with low contention for tokens increase the performance (throughput and latency). In butterﬂy and perfect-shufﬂe trafﬁc traces, ﬂattened-butterﬂy slightly outperforms T-PROPEL. In these trafﬁc traces, multiple tiles 211 Fig. 6. Simulation results for 64-core which show (a) saturation throughput and (b) power dissipation for all trafﬁc traces relative to mesh. are contending for the same optical token which degrades the throughput of T-PROPEL. In complement trafﬁc, all cores within the tile contend for the same output port creating a low throughput network. This is seen for networks where cores are concentrated such as ﬂattened-butterﬂy and T-PROPEL. Figure 6(b) shows the normalized power dissipation. In the ﬁgure, the results are normalized relative to the mesh network. As shown, T-PROPEL reduces the power by about 10-fold when compared to mesh network and 5-fold compared to ﬂattened-butterﬂy network and is the most power efﬁcient network. The power savings in T-PROPEL is primarily due to the nanophotonic link design. Figure 7(a) shows the speed-up relative to mesh for the select SPLASH-2 benchmarks. In water and barnes benchmarks, T-PROPEL has the highest speed up factor of over 3. This is due to the fact that water and barnes applications have a higher percentage of non-local trafﬁc which requires multiple hops when compared to a mesh network. Since T-PROPEL has a maximum hop count of one, non-local trafﬁc would see a higher speedup than mesh. For FFT, LU, Ocean, Radiocity, Raytrace and Radix, T-PROPEL has a speedup of about 2.5fold. This is also a result of T-PROPEL having a maximum of hop count of one, which can more quickly send data than mesh and ﬂattened-butterﬂy. In FMM trafﬁc, T-PROPEL has the lowest speedup. As FMM trafﬁc creates the most contention for optical tokens, this results in a decrease in speedup. Figure 7(b) shows the power dissipation relative to mesh. From the ﬁgure, T-PROPEL dissipates about 12-fold less power than mesh and about 6-fold less power than ﬂattened-butterﬂy for each application. In T-PROPEL, the major contribution of power dissipation is at the crossbars and VCs, where as in mesh and ﬂattened-butterﬂy the power dissipation is in the crossbar, VCs and electrical interconnects. C. 256-core Synthetic Trafﬁc We evaluated ET-PROPEL by comparing it to mesh [20], ﬂattened-butterﬂy [21], Fireﬂy [22] and an optical crossbar using synthetic trafﬁc. In Fireﬂy, we did not implement the reservation assist method during simulation as this slightly degrades the performance of the network. In optical crossbar network, we implement a token-based communication with reduced bandwidth as compared to Corona [14]. For each network, we kept the same bandwidth at 640 Gpbs for each electrical and optical link. This results in a 5 GHz clock for electrical components for a 128 bit ﬂit. In simulating ETPROPEL and the optical crossbar, we assume a one cycle delay to capture and process a token and a one cycle delay to inject the token back into optical waveguide. For ETPROPEL, due to the limited number of wavelengths (64) that can traverse down a waveguide with limited switching capability (16 wavelengths) of our proposed nanophotonic crossbar, we extend the total number of crossbars to 64 to keep identical link bandwidth (640 Gbps). Saturation throughput for 256-core networks normalize to the mesh network is shown in Figure 8(a). Here we can see ET-PROPEL is the second highest performing network for all trafﬁc except for uniform trafﬁc. For bit-reversal trafﬁc, the optical crossbar has the highest performance, because tokens are evenly shared across the network and the optical crossbar is a one hop network. This is the same case for matrixtranspose and perfect-shufﬂe trafﬁc traces. In complement trafﬁc, ET-PROPEL and the optical crossbars are the best performing networks. This is a result of optical interconnects and the low maximum hop counts found in these two networks. For butterﬂy trafﬁc, Fireﬂy slightly outperforms ET-PROPEL because Fireﬂy design favors butterﬂy trafﬁc over all the other networks. Figure 8(b) shows the power dissipation. As can be easily seen, ET-PROPEL signiﬁcantly reduces the power dissipation over electrical networks. ET-PROPEL dissipates about 12-fold less power when compared to mesh and about 5-fold less power when compared to the ﬂattened-butterﬂy. ETPROPEL has a 12-fold reduction because the average power for ET-PROPEL is about 1.07 mW/bit and the average power power power for mesh is about 12.07 mW/bit. As for the other nanophotonic networks, ET-PROPEL dissipates about 3-fold 212 Fig. 7. Simulation results for 64-core show SPLASH-2 (a) speedup and (b) power dissipation. less power than Fireﬂy (2.9 mW/bit) and the optical crossbar (0.8 mW/bit) dissipates slightly less power. Fireﬂy dissipates more power because most trafﬁc will require electrical link traverses. The optical crossbar dissipates less power because it is a one hop network. V I . R ELATED WORK Although there have been considerable work in off-chip optical interconnects, there have been few signiﬁcant on-chip nanophotonic designs. In a nanophotonic network proposed by HP [14], a 3D stacked 64-cluster, 256-core optical crossbar that uses optical tokens for media access arbitration has been proposed. This design scales as O(N2 ), where N is the number of clusters, which increases the cost and complexity of the network. Optical crossbars can suffer for high contention for resources with is seen in uniform trafﬁc. ET-PROPEL overcomes this by limitation by limiting the number of tiles that are shared by a token. Another nanophotonic design proposed by [13] uses optical interconnects for direct access to dynamic random access memory (DRAM). This design tackles the high DRAM communication latency but requires cores that are not in the same group to communicate with each other through slower memory. In ET-PROPEL, we use nanophotonic interconnects to connects all cores together for communication. Shacham et.al. [15] have proposed circuitswitched photonic interconnects, where electronic set-up, photonic communication and tear-down are implemented. The disadvantage of this approach is the excess latency for path set-up, which is performed using electrical interconnects. In our work, we do not require an electrical set-up path, which result in a fast network. Fireﬂy [22] is an optoelectronic multistage NoC that is comprised of an electrical network and an optical network. When packets enter the network they ﬁrst traverse the electrical network and then traverse the optical network or vice versa. Since Fireﬂy requires packets to traverse an electrical network, this can result in higher power dissipation. As ET-PROPEL uses only nanophotonics for interrouter communication, we consume substantially lower power than Fireﬂy. A maximum of two hops found in ET-PROPEL allow for an increase in throughput over Fireﬂy, which is seen in this work. Recently, Phastlane [30], an optical mesh network, has been proposed that allows packets to move from router to router optically without requiring an initial path setup circuit. Phastlane buffers incoming packets electrically if the output channel they need is blocked resulting in higher packet latency and power dissipation. T-PROPEL does not buffer packets at intermediate router resulting in lower power consumption than Phastlane. V I I . CONCLU S ION In this paper, we proposed an on-chip scalable NoC called ET-PROPEL that uses emerging nanophotonic components to overcome the limited bandwidth and high power dissipation bottlenecks found metallic based NoCs. Our analysis clearly shows signiﬁcant saving (10-fold) in power and an increase in performance (3-fold), when T-PROPEL is compared to both mesh and Flattened-butterﬂy for Splash-2 suite. Moreover, this architecture has the desirable features identical to a mesh architecture which can be scaled in two dimensions, and provides fault-tolerance due to multi-path connectivity. We also analyzed the scalability of T-PROPEL architecture and developed an extended ET-PROPEL architecture. ETPROPEL combines the beneﬁts of three different optical communication method create the most cost effective (area overhead) and high performance network to date. Acknowledgements: This research was partially supported by NSF grants ECCS-0725765 and CCF-0953398. "
Physical-Aware Link Allocation and Route Assignment for Chip Multiprocessing.,"The architecture definition, design, and validation of the interconnect networks is a key step in the design of modern on-chip systems. This paper proposes a mathematical formulation of the problem of simultaneously defining the topology of the network and the message routes for the traffic among the processing elements of the system. The solution of the problem meets the physical and performance constraints defined by the designer. The method guarantees that the generated solution is deadlock free. It is also capable of automatically discovering topologies that have been previously used in industrial systems. The applicability of the method has been validated by solving realistic size interconnect networks modeling the typical multiprocessor systems.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip Physical-aware link allocation and route assignment for chip multiprocessing Nikita Nikitin Univ. Polit `ecnica de Catalunya Barcelona, Spain Satrajit Chatterjee Strategic CAD Lab, Intel Corp. Hillsboro, OR, USA Jordi Cortadella Univ. Polit `ecnica de Catalunya Barcelona, Spain Mike Kishinevsky Strategic CAD Lab, Intel Corp. Hillsboro, OR, USA Umit Ogras Strategic CAD Lab, Intel Corp. Hillsboro, OR, USA Abstract—The architecture deﬁnition, design, and validation of the interconnect networks is a key step in the design of modern on-chip systems. This paper proposes a mathematical formulation of the problem of simultaneously deﬁning the topology of the network and the message routes for the trafﬁc among the processing elements of the system. The solution of the problem meets the physical and performance constraints deﬁned by the designer. The method guarantees that the generated solution is deadlock free. It is also capable of automatically discovering topologies that have been previously used in industrial systems. The applicability of the method has been validated by solving realistic size interconnect networks modeling the typical multiprocessor systems. I . IN TRODUC T ION The constantly increasing complexity of chip multiprocessing (CMP) systems requires scalable and efﬁcient communication topologies. Network-on-Chip (NoC) [1], [2] has become the dominant interconnection paradigm for the design of CMPs. The modern CMPs and NoCs require a thorough elaboration process that involves a variety of design problems: topology selection and mapping, physical planning, routing and switching schemes, and other optimization tasks. The large number of options and constraints makes it impossible to fully explore the solution space. On the other hand, dividing the design problem into smaller subproblems and doing a myopic optimization for each one of them may result in largely suboptimal solutions. Let us consider a design example. A CMP system is speciﬁed by a set of processing elements (PE), routers and communication requirements between PEs. Let us assume that the underlying system topology has been selected and the assignment of the PEs to the routers has been performed. Some of the design problems we would like to solve are as follows: • Find a subset of links satisfying the communication requirements of the system and minimizing the design cost. • Deﬁne the routing paths for each pair of communicating PEs that satisfy the performance requirements. • Guarantee that the selected routes to communicate the PEs are deadlock-free. The ﬁrst problem can be referred to as the link allocation problem. The other two problems are related to the efﬁcient route assignment with deadlock avoidance. These problems have different optimization criteria. By solving one of them optimally and independently from the others, no acceptable solution might be found when solving the subsequent problems. It is therefore necessary to devise non-myopic strategies to explore the solution space in a way that all design constraints are met and the implementation cost is minimized. This paper presents a mathematical formulation that combines the three previous problems in one model. Various constraints and cost functions are combined to do link allocation and route assignment simultaneously while optimizing the cost and performance of the system. The model also guarantees that the derived solution is deadlock-free. The model can be deﬁned as a conjunction of linear inequalities and a linear (or quadratic) cost function with Boolean and integer variables, thus enabling the use of integer programming (IP) solvers. The paper is organized as follows. Next section summarizes the related work. Section III presents an overview of the paper and illustrates the basic contributions with an example. The problem description, formulation and solution are presented in Sect. IV. The results obtained from various experiments are discussed in Sect. V. Finally, Sect. VI concludes the paper. I I . R ELAT ED WORK The application of linear programming techniques to the design of on-chip networks has already been proposed in [3]. The authors presented mixed integer linear programming formulations for the problems of ﬂoorplanning, topology and route generation for NoCs. However, the model introduced in this paper differs by ensuring deadlock freedom of the routing solution. It also explicitly states the link allocation problem for mesh networks and discusses a number of speciﬁc cost functions and constraints. Many approaches have been suggested for the on-chip routing problem. Several schemes have been proposed to guarantee deadlock and livelock-free properties of the communication algorithm, such as odd-even routing [4] or turn prohibition [5]. Numerous works on the on-chip routing enhancements include combination of the deterministic and adaptive schemes for performance improvement [6], incorporation of the applicationspeciﬁc topology, trafﬁc and bandwidth information for congestion avoidance and performance increase [7], [8]. Though the link allocation problem for CMP has not been emphasized so far, irregular meshes [9] have been recently 978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.22 125 (a) Full mesh (b) Partial mesh (c) Minimal stronglyconnected mesh Fig. 1: Different link allocation solutions for a 3x3 mesh. found to provide a simple and ﬂexible extension of the regular mesh topologies in order to support PEs of different sizes. The interest for irregular mesh structures is sustained by the works that have appeared lately using this type of topology. An algorithm for the deadlock-free communication in irregular meshes, extending the deterministic XY-routing with hardcoded routing tables, was presented in [10]. A variety of adaptive strategies is also available in the literature. The work in [11] proposes an adaptive routing for irregular meshbased NoC topologies, supported by a ﬂoorplanning method to generate the layouts suitable for this algorithm. Applicationspeciﬁc information is used to perform deadlock-free adaptive routing for irregular meshes with regions in [12]. Another algorithm for trafﬁc balancing is presented in [13]. This paper proposes a novel approach to simultaneously deﬁne the network topology and the message routes for communicating processing elements. Various optimization targets and constraints are incorporated into the design space exploration algorithm. The deadlock freedom of the routing conﬁguration is guaranteed by the turn prohibition technique [5]. The quality of the solutions in terms of area and performance is demonstrated with the comparison with the XY and oddeven [4] routing implemented in a full-mesh topology. I I I . OV ERV I EW This section gives an overview of the contributions of the paper by using a simple example. In this work, we consider communication topologies that can be mapped onto a twodimensional grid. Every link can connect two adjacent routers and the transit time through each link is constant and known a priori. We also assume that the system has already been ﬂoorplanned and each PE is connected to a router. Figure 1 shows three different topologies based on an underlying 3x3 mesh. Figure 1a depicts a fully-connected mesh in which all possible links have been laid out. This topology can provide a very high performance. Every router can reach any other router in no more than four hops. However, this topology requires a costly implementation in wiring and router area. Every router implements a crossbar that has a quadratic cost on the number of links of the router. On the other hand, Fig. 1c shows a mesh with the minimum number of links to preserve strongly connectedness, which results in a very area-efﬁcient implementation. However, the diameter of the network doubles since some routes may require eight hops to communicate one PE with another. Fig. 2: (a) Communication graph; (b) and (c) two different link allocation and route assignment solutions. Additionally, some links may become over-congested if they have to be shared among different routes that carry dense trafﬁc, thus incurring in a signiﬁcant throughput penalty due to the contention in the network. The designer will probably want to ﬁnd an intermediate topology that satisﬁes certain throughput constraints with a reduced implementation cost. Figure 1b depicts one of these solutions. The cost has been reduced by removing some of the links of the full mesh. However the diameter has already been increased (some routes require ﬁve hops). This may also involve extra congestion in some speciﬁc links. The solution space of this type of problems is huge. It becomes even larger when we consider the route assignment problem. Figure 2a depicts the communication graph for six PEs that need to exchange information. In this particular example, every PE is assumed to be attached to a router. Figure 2b shows a solution in which the links have been allocated to provide a minimum hop-count for every communication edge. The routes are represented by the dotted lines. The longest routes are for the pairs (1, 4), with the route 1 → 2 → 4, and (3, 6), with the route 3 → 5 → 6. Let us assume that the trafﬁc through the links 1 → 2 and 2 → 4 is very congested due to the intensive communication requirements of the PEs attached to those routers. Let us also assume that edge 1 → 4 is not critical and has a low priority. Hence, the designer might consider to deviate the trafﬁc 1 → 4 through another route, as shown in Fig. 2c. This solution implies an extra link in the network, but may contribute to meet the throughput requirements of the system. Note that the route 1 → 3 → 5 → 6 → 4 is not using the shortest path. The model presented in this paper allows the exploration of non-optimal paths to alleviate the trafﬁc in congested links. Even though this work assumes a 2D grid as the underlying structure, the topology of the network is not limited to regular meshes. Irregular meshes with PEs exceeding the size of one tile can also be considered, thus providing an extra ﬂexibility in the exploration of solutions. Furthermore, the links are not constrained to have equal length. This makes the design ﬂow even more ﬂexible in terms of ﬂoorplanning and placement. 126 A. Sketch of the model To explore the space of solutions, the mathematical model presented in this paper introduces a set of constraints for link allocation, route assignment and deadlock avoidance. The set of constraints can be extended to cover other design criteria. One of the most practically important constraints is the limitation of the number of ports in every router. As it will be discussed further, port limitation highly reduces the complexity of the router, thus saving area and power resources of the system. The traditional link capacity constraint is also supported. These two types of constraints are associated to physical requirements of the design. To guarantee a sufﬁcient performance in the system, a set of delay constraints are deﬁned. These constraints are modeled as a maximum hop-count (structural latency) for each net1 . The communication demands are deﬁned by the bandwidth requirement between each pair of PEs. An essential property of route assignment is deadlock and livelock freedom. The incorporation of turn prohibition constraints guarantees the absence of any deadlock and livelock in the explored solutions. The mathematical model includes four optimization objectives in the cost function: the minimization of the number of links in the grid, the maximum hop-count over all nets, the total net delay (sum of all net delays) and the uniform trafﬁc distribution. The ﬁrst objective is related to the area resource optimization, whereas the other three objectives guide the search toward increasing the performance of the system. The constraints of the model can be represented as linear inequalities with integer variables. In this way, the problem can be speciﬁed with an integer linear programming (ILP) or integer quadratic programming (IQP) model, depending on the cost function. Even though these are NP-complete problems, the experimental results show that optimal solutions can often be found with moderate computational cost. Section V will present results obtained from different benchmarks. IV. TH E IN TEG ER PROGRAMM ING MODE L This section presents the integer programming (IP) model for link allocation and route assignment problem. Table I summarizes the input parameters for a quick reference. We introduce several types of variables and set notations to formulate the problem. The summary of the IP notations can be found in Table II. A. Parameters and variables of the problem The problem consists of deﬁning a set of routes in a 2-dimensional grid structure that satisﬁes a set of physical and performance constraints. The routes must support the communication among the PEs of the system. The grid structure with size (x, y) is represented as a directed graph G(R, L). The vertices of the grid deﬁne a set of routers R = {R0 , .., Rr−1}, where the total number 1We refer to a net as a logical connection between two PEs, represented as an edge in the communication graph. TABLE I: Input parameters of the problem. Input Description (x, y) n Bk Dk Cj Grid size Number of nets Required bandwidth for net Nk Maximum hop-count for net Nk Capacity of link Lj Maximum number of input ports for router Ri Pi,out Maximum number of output ports for router Ri Pi,in TABLE II: Notation for the IP problem. Notation Type Lj Lk Tp j Dmax I (Ri ) O(Ri ) Binary variable Real variable Set Description Link presence in the solution Link usage by net Nk Prohibition of turn Tp Maximum net delay Incoming links of router Ri Outgoing links of router Ri of routers is r = x · y . The edges of the grid deﬁne a set of uni-directional links L = {L0 , .., Ll−1}, where the total number of links for a grid with size (x, y) is calculated as l = 2 (x(y − 1) + (x − 1)y). A global assumption about the grid is that every pair of neighboring routers may have up to two uni-directional links to send data in both directions. Each link Lj has a maximum capacity parameter Cj (f lits/cycle). It limits the amount of data that can be transmitted over the link in one cycle. tion graph GC (PE, N ) that represents the logical connectivity Another input of the problem is the underlying communicaof the network. Every vertex represents a processing element and every edge represents a logical connection between a pair of processing elements. Every edge in the set N = {N0 , .., Nn−1 } is a net of the system. Each net Nk has two associated parameters: the required bandwidth Bk (ﬂits/cycle) and a maximum delay constraint Dk (hops) for the packet transmission from source to destination. The additional parameters Pi,in and Pi,out specify constraints on the number of input and output ports for router Ri , respectively. B. Path selection constraints We focus on the deterministic routing path selection, without considering path diversity mechanisms. The latter would allow multiple paths for a pair of communicating processors. On the contrary, we assume there is only one path to send data packets for each communicating pair. Path diversity is an option for the routing path selection task. Our assumption for considering only one path eliminates the need to perform packet ordering at the destination router. We start the constraint set description with introducing the basic mechanism for path selection and link representation in the model. Any conﬁguration for link allocation contains a subset of links from the full grid. The presence of each link 127 2 , in a conﬁguration is represented by the set of variables Lj i.e., Lj = 1 iff link Lj is present in the conﬁguration. The routing paths for every net are represented by another set of binary variables Lk j , specifying the fact net Nk uses link Lj in its routing path. As we show below, this provides high ﬂexibility for the solution space as every net may be routed through any arbitrary subset of links. j variables (n · l), To reduce the potential large number of Lk it is possible to introduce rules that bound a routing region for a net. For example, we may not be interested in having long routing paths for the short nets with source and destination routers located at the neighboring grid vertices. In this case we may limit search region to be within few hops. An example is presented in Fig. 3. Net Nk is connecting two neighboring nodes, located in the corner of the grid. By limiting the maximum path length to 5 hops, only 10 links are eligible for selection in the route (marked with dashed lines). The maxhop constraints contribute to signiﬁcantly reduce the number of link variables. The sets of variables Lj and Lk j are both related to the selection of link Lj . The Lk j variable deﬁnes the relationship between a link and a particular net Nk , while Lj deﬁnes whether there is at least one net Nk(cid:48) ∈ N such that Lk(cid:48) j = 1. In other words, ∀k : Lk j ⇒ Lj . Assuming both sets Lj and Lk consist of binary variables and n is the total number of nets, we can write the following relations for each Lj : j Lj ≤ (cid:88) N (cid:88) Lk j , j ≤ n · Lj . Lk (1) N These two constraints guarantee the consistency of the variables from both sets in the IP model. We now formulate the set of routing constraints that allow one and only one path selection for each net. Let us denote the set of incoming links to the router Ri as I (Ri ) and the set of outgoing links as O(Ri ). For each net Nk with source at router Rs , destination at router Rd and any intermediate router Ri ∈ R\{Rs , Rd }, the following constraints are deﬁned: for Rs : (cid:88) for Rd : (cid:88) for Ri j = 0, j = 1, j = (cid:88) The ﬁrst two equations in (2) represent the boundary conditions on the path for the source and destination routers, while the last one can be treated as a path maintenance constraint for the intermediate routers. Indeed, the source router Rs is the one that injects the Nk packets into the network, thus there (cid:88) (cid:88) : (cid:88) Lk j = 1 Lk j = 0 O(Rd ) O(Rs ) Lk Lk I (Rd ) I (Ri ) O(Ri ) I (Rs ) Lk Lk j . (2) 2 For the sake of notation simplicity we use Lj to denote both the variable and the link. Fig. 3: Path region limitation for Nk with 5 hops. (a) Closed path cycle. (b) Open path cycle. Fig. 4: Path cycles introducing redundant links. should be no input links to this router. The number of output links, carrying the Nk packets from Rs should be equal to one, as we allow only one path for each net. The inverse situation is observed at the destination router Rd , that consumes Nk packets: there is one input link that delivers the Nk packets to Rd , while the number of output links is zero. The last equation in (2) guarantees that if an intermediate router Ri has an input link for Nk , then it will have an output link for this net. This condition assures that the path will be constructed correctly from source to destination node. Note that the path constraints in (2) do not prevent the conﬁguration from having cycles like the ones depicted in Fig. 4. Generally, a closed path cycle (Fig. 4a) may occur without breaking any constraint in (2), but allocating extra links. An open path cycle (Fig. 4b) may also occur, replacing one of the path turns with the sequence of three complementary turns and occupying redundant links. In the IP model, these cycles can never appear due to the turn prohibition mechanism to avoid deadlocks and the cost function of the problem, that tends to minimize the number of links in the network. For efﬁciency reasons, we have found interesting to add explicit constraints on the number of input (output) path links for each router: for Ri j ≤ 1. Lk (3) : (cid:88) I (Ri ) Even though the overall number of constraints in the model increases, our experiments show that the problem is solved faster due to the limitation of the solution space. The constraints (3) avoid the exploration of solutions with redundant links that will never be optimal. C. Deadlock avoidance Deadlocks and livelocks may occur in the wormhole routing networks due to the limited capacity of the router input 128 (a) 8 possible turns in 2D grid. Two turns, TWN and TNW , depicted in dashed lines, are prohibited. (b) The cycle that still occurs after prohibiting turns TWN and TNW . Fig. 5: Turns in a 2D grid. buffers [14]. An important property of the routing algorithm is deadlock freedom. In deterministic routing, the propagation paths for every net are deﬁned statically. Thus, deadlock freedom can be guaranteed by incorporating certain restrictions into the path selection procedure. One of the approaches for deadlock and livelock avoidance is turn prohibition [5]. There are eight possible turns a packet may follow in a 2D grid (Fig. 5a). We refer to a turn according to the directions of the input and output links of the turn, namely: west-north (WN), north-east (NE), east-south (ES), south-west (SW) in the clockwise direction and west-south (WS), south-east (SE), east-north (EN), north-west (NW) in the counter-clockwise direction. In order to guarantee that deadlocks never occur, certain turns should be prohibited in both cycles (clockwise and counter-clockwise). Speciﬁcally, prohibition of one turn from each cycle is enough to assure that the cycles will not occur. However prohibition of some turn pairs will still allow deadlocks resulting from the complex cycles depicted in Fig. 5b. Luckily, these are just four pairs and they are easy to identify [5]. Thus, when prohibiting two turns, one from each cycle, we should check that they do not belong to the same pair. Finally, we want to apply the turn prohibition mechanism to guarantee deadlock freedom in the IP model. We introduce a set of binary turn variables to represent each one of the 8 possible turns: {TWN , TNE , TES , TSW , TWS , TSE , TEN , TNW }. For example, the west-north turn will be prohibited in the ﬁnal solution if and only if TWN = 1. We formulate three sets of the turn constraints, based on the considerations above. First, we have to guarantee that one turn is removed from each of the two potential cycles (Fig. 5a), that is TWN + TNE + TES + TSW = 1, TWS + TSE + TEN + TNW = 1. (4) Second, the excluded turns should not belong to the same pair that still allows complex cycles (Fig. 5b): {L2 129 j ∧ Lk (a) No turn in case of intersection. (b) Turns exist in case of touching. Fig. 6: Turn existence in dependence of nets positioning. TWN + TNW ≤ 1, TNE + TEN ≤ 1, TES + TSE ≤ 1, TSW + TWS ≤ 1. (5) To ensure that none of the selected paths incorporates a prohibited turn, we should guarantee that from each pair of links, that contribute to the turn, at most one link can be selected for the net path. We need to formulate these constraints with the net-related variables Lk j . Two neighboring links may exist in the solution independently, but the turn will occur only when there is a net that traverses these links in sequence. This idea is illustrated with the examples in Fig. 6. On the left example, two intersecting nets are depicted: N1 propagating from south to north and N2 from west to east. All four links exist in the routing solution: Lnorth = Least = Lsouth = Lwest = 1. However the solution does not contain any turn. This fact is reﬂected by the net-related variables that take the following values: L1 L2 north = 1, L1 east = 0, L1 south = 1, L1 west = 0, north = 0, L2 east = 1, L2 south = 0, L2 west = 1. north} or {Lk west , Lk south , Lk None of the pairs {Lk east } has both variables set to 1 (that would describe a turn condition). On the right example, two touching nets are shown: N1 propagating from west to north and N2 from south to east. All the four links are still present, but the net-related variables have different values now: L1 L2 north = 1, L1 east = 0, L1 south = 0, L1 west = 1, north = 0, L2 east = 1, L2 south = 1, L2 west = 0. south = 1, L2 In this case one turn is introduced by each net: an east-north turn TEN by N1 and a north-east turn TNE by N2 . In the IP model, this fact is observed by obtaining two pairs of non-zero variables: {L1 north = 1} and east = 1}. Therefore, in order to exclude a turn from the solution, we must prevent all contributing link pairs from having both variables set to 1. More formally, if turn Tp is prohibited, then for any net Nk and any pair of links Lj and L(cid:48) j that contribute to the turn, the following implication is required: Tp ⇒ ¬(Lk j (cid:48) ), that is equivalent west = 1, L1 E. Link capacity constraints Another set of constraints in our model refers to the link capacity. A link Lj can support a bandwidth up to Cj ﬂits per cycle. The bandwidth of each link is one of the input parameters of the problem. As one physical link may be used by several nets, the total trafﬁc in the link will be deﬁned by the sum of the bandwidths of all nets that are routed through the link. The net-related variables Lk j can be used to deﬁne whether the path of net Nk uses the link. The following constraint guarantees that the total trafﬁc in the link does not exceed the link capacity: (cid:88) j · Bk ≤ Cj . Lk (8) N F. Net delay constraints The proposed model offers a high ﬂexibility in the selection of the routing path for any net as there are no limitations on the path shape or length. However, designers might be interested in limiting the path hop-count. This may be especially important for time-critical nets or some short nets that we want to prevent from having very long paths. In other words, we want to introduce performance constraints that approximate the net delay by the hop-count metric. This simple metric enables us to use the IP formulation, and yet at the same time accurately captures latency for low trafﬁc loads. The hop-count of a net can be calculated as the sum over all net-related link variables, since only the links with Lk j = 1 contribute to the path. Given a limit Dk for the hop-count of net Nk , we obtain the following constraint for the delay of net Nk :(cid:88) L j ≤ Dk . Lk (9) G. Cost functions A variety of cost functions are introduced to ﬁnd solutions with different optimization criteria. These cost functions are further discussed in the experimental section. The ﬁrst three cost functions are linear, so the obtained problem is classiﬁed as an Integer Linear Programming (ILP) problem, while the last cost function is quadratic, resulting into an Integer Quadratic Programming (IQP) problem. The cost functions do not need to be used in isolation. Linear combinations with weighted coefﬁcients can be used for a multiple cost optimization. 1) Number of links: the total number of links is obtained by summing variables Lj over the set L: min (cid:88) L Solving the IP problem with the cost function in the form (10) tends to ﬁnd a feasible routing solution with minimized area and power consumption. (10) Lj . Fig. 7: NoC router with I/O ports in 5 directions. to ¬(Tp ∧ Lk j (cid:48) ). This represents the third set of the turn prohibition constraints for the problem, that we can formulate in the following manner. For every net Nk , turn Tp and all pairs of links Lj and L(cid:48) j , that form Tp : j ∧ Lk j (cid:48) ≤ 2. Tp + Lk j + Lk (6) The constraints (4), (5) and (6) are sufﬁcient for the IP model to ensure a deadlock and livelock-free solution. D. Port limitation constraints A new design option introduced in this paper is the constraint on port limitation. A typical router for a 2D grid network has input (I) and output (O) ports in 5 directions, i.e. 10 ports in total (Fig. 7). However the router complexity highly depends on the number of ports. For instance, the size of the internal crossbar grows quadratically with the number of ports, contributing to the overall area and power consumption of the network. Also by constraining the number of ports, the physical design of the router and the routing of the wide links become easier. Finally, few-ported routers can often be implemented with single cycle latencies, low area and short cycle time. Many-ported routers often require a trade-off between latency, cycle time and area. By considering the use of few-ported routers, it is possible to have a global view of the optimization problem since we are not a priori restricted to the larger areas and latencies inherent in many-ported routers. Thus, it is useful for the designer to have the capability of limiting the number of ports for each particular router. The IP model can be easily extended with port limitation constraints. These limitations can be reduced to limitations on the number of links each router is connected, since each link is connected to a port of the router. The constraints may also distinguish between input and output ports. Let the limits for the number of input and output ports of the router Ri be Pi,in and Pi,out , respectively. Let us also introduce an indicator function PE(Ri ), that is equal to one if router Ri has a processing element connected to it, or zero otherwise. If PE(Ri ) = 1, then a local port connection exists and the port limitation should be decreased by one. We have the following set of constraints for each network router Ri : Lj ≤ Pi,in − PE(Ri ), Lj ≤ Pi,out − PE(Ri ). (7) (cid:88) (cid:88) I (Ri ) O(Ri ) 130 2) Maximum net delay: we may want to minimize the maximum net delay over all nets in the network in order to ﬁnd a feasible routing solution with the highest performance (net delay constraints for particular nets may still be incorporated). The introduction of a new variable to represent the maximum delay, Dmax , is required:(cid:88) L and the cost function is simply: j ≤ Dmax . (11) Lk min Dmax . (12) 3) Total net delay: the minimization of the total delay over all nets (that is equivalent to minimizing the average net delay) can be regarded as another performance metric. The cost function in this case is obtained by summing all the netj for all nets from N : min (cid:88) related variables Lk N (cid:88) (13) Lk j . L 4) Uniform trafﬁc distribution: this cost function aims at assigning a uniformly distributed trafﬁc over all the links of the network. The distribution tends to decrease the contention delays in the network and, hence, increase the overall network performance by improving the throughput and the average packet delay. This cost function introduces quadratic terms, so the problem becomes an IQP problem. A more uniform distribution is obtained by minimizing the sum of the squares of the link trafﬁcs: min (cid:88) L H. Problem formulation Having discussed the set of the constraints and the cost functions, we are now ready to present the formulation of the IP problem for link allocation and route assignment: (cid:32)(cid:88) Bk · Lk (cid:33)2 (14) N . j Find the optimal value for a cost function obtained as a linear combination of (10), (12), (13) and (14) subject to physical constraints (7), application-speciﬁc communication constraints (8), performance constraints (9), deadlock-avoidance constraints (4), (5), (6) and additional model constraints (1), (2), (3), (11). Note that the user is free to select a subset of constraints if certain features are not required for the design. The cost function can also be extended with small effort due to the ﬂexibility of the model. 131 V. EX PER IM EN TA L RE SU LT S This section presents the experimental results to demonstrate the functionality and the quality of the proposed IP model. We deﬁne the trade-offs and discuss the results for several design problems with various constraint sets and optimization objectives. We use CPLEX [15] to solve the IP model and an accurate ﬂit-level C++ simulator with a variety of routing schemes to obtain the network parameters. Different testcases are used throughout the experiments: we start with artiﬁcial conﬁgurations and we next consider a testcase with typical server workloads from the SPEC2006 benchmarks. Three types of the experiments are introduced. Section V-A analyzes the area-performance trade-off. In Sect. V-B, the application of the model for performance optimization by means of the routing paths redistribution is analyzed. Finally, the use of port limitation constraints for design exploration and tuning is presented in Sect. V-C. A. Area-performance trade-off One of the optimization tasks in the design of multiprocessor interconnection network is the minimization of the number of links. Given a set of constraints, the goal is to ﬁnd the minimal number of links that satisfy the constraints, determine the link allocation and assign the trafﬁc routes. This optimization contributes to decrease area and leakage power. However, the average hop-count delay may increase as the number of the links decreases and the packets have to follow longer roundabout paths. For this reason, the dynamic power may also increase. Note, that the variation in power will be deﬁned by the relation between leakage and dynamic power. This set of experiments demonstrates the ability of the model to explore the area-performance trade-offs by link reallocation. Given the communication graph, we ﬁrst search for the minimal number of links to enable the connectedness of all routers and estimate the maximum net delay value. Additionally, we investigate minimal link solutions subject to the limitation on the maximum net delay. In order to ﬁnd the minimal link allocation, we solve the problem with the cost function in form (10). We apply the constraints (9) to limit the net delay and (4)-(6) to guarantee the deadlock freedom. We show the area-performance trade-off for a 4x3 network with a complete communication graph, i.e. with net between every pair of processing elements. The minimal number of links required to connect all routers is 12 and forms the unidirectional ring topology, that is depicted in Figure 8a. For this allocation the packet delivery will take up to 11 hops for some nets. However, the diameter of the network can be reduced to 5 hops. The solution obtained with this delay limitation is presented in Fig. 8b. It incorporates 20 links, but the delay for any net is now guaranteed not to exceed 5 hops. Obviously, there is a trade-off between these two cases. We explore it by varying the delay constraint value in the speciﬁed range. The set of solution points is displayed in Fig. 10 (“Minimal”). Based on this dependency, one can determine the minimal (a) Minimal link number (b) Minimal hop-count Fig. 8: Link allocation solutions for a 4x3 network. (a) Minimal link number (b) Minimal hop-count Fig. 9: Deadlock-free link allocation for a 4x3 network. number of links to guarantee a particular network diameter (for example, 14 links are required for the 8-hop network). Another important property is deadlock freedom. We provide a similar function after incorporating the turn prohibition constraints into the problem. Due to the extra limitations in routing paths, deadlock-free solutions tend to include more links for the same hop-count limit. Thus, the minimal number of links to provide full connectivity is 22 (Fig. 9a) and the solution with minimal delay of 5 hops has 26 links (Fig. 9b). This trade-off is also depicted in Fig. 10 (“Deadlock-free”). Finally, we note that even the “Minimal” solutions can be designed to be deadlock-free by choosing a suitable architecture. For example, the solution shown in Fig. 8a can be realized in practice by using a token-ring architecture [16] or virtual channels and dateline scheme [14]. Our model is also capable of discovering well-known structures, such as the bidirectional ring, that is seen in the variety of cell processors [17]. This proves that the class of the generated solutions is actually used in practice. The area-performance trade-offs discussed in this section demonstrate the suitability of the model for design exploration and optimization. By incorporating additional application constraints, the user is allowed to perform more accurate, application-speciﬁc optimizations. B. Performance optimization by route reassignment Another application of the model is related to the optimization of the network delay by routing path redistribution. In this experiment we assume the communication requirements of the network, including the nets and their bandwidths, are speciﬁed. The objective is to minimize the average packet delay of the network. The average optimal hop-count delay can be obtained with the cost function (13), but the contention affects the delay value signiﬁcantly once the network enters the saturation region. However, the contention delays can be alleviated by distributing the trafﬁc uniformly over the network. For this purpose, we are using the cost function (14) to select routing paths that distribute the trafﬁc more uniformly. The quality Fig. 10: Area-performance trade-off points in terms of the link number and net delay for a 4x3 network. of the solution is estimated by simulation and compared to that of the XY and odd-even routing algorithms. Using the example of a typical server workload, we demonstrate that the obtained solutions improve the network delay as compared to the classical routing algorithms for a wide range of injection rates. Furthermore, the throughput increases as the saturation occurs at higher injection rates. In this experiment we are considering the typical server workload trafﬁc pattern collected using the SPEC2006 benchmarks. A 16-core application is assumed to be mapped onto a 4x4 full mesh, with all links present (Fig. 11). The cores connected to the routers 1 and 2 are the memory controllers that receive high trafﬁc from the other cores. The trafﬁc injected by each core is assumed to have Poisson distribution. In Fig. 12, the comparison for the average delay estimation at different trafﬁc rates is shown. We draw the packet delay as a function of the total injection rate to the network (in packets/cycle), for each of the three mentioned routing algorithms: XY, odd-even and the one using routing tables, based on the IQP solution. The timeout for the IQP solution was set to 1000 seconds. Simulation shows that the average packet delay, obtained with the IQP routing, is better than the delays of XY or OE schemes in the large range of injection rates. The XY-routing is only winning slightly the IQP conﬁguration when the injection rates are small, as contention is low and the XY scheme results into the most uniform solution. However, as soon as contention effects start to contribute signiﬁcantly to the delay value (injection rates ≥ 0.3 pkt/cycle), the IQP routing improves the average packet delay, as compared to Fig. 11: Underlying mesh for a typical server workload testcase. 132 Fig. 12: Average delay depending on the injection rate. both XY and odd-even schemes. It can be also seen from the graph that the saturation occurs at higher rates, hence, the network throughput is increased. C. Design optimization by port limitation Port limitation is another feature introduced by the model in order to extend the user design ﬂexibility. The ability to limit the number of ports provides the means to account for the router design complexity at the network planning stage. Typical routers with 5-in and 5-out ports (Fig. 7) have complex designs and are not capable of running the full bandwidth. Hence, a mismatch between the network ﬂoorplanning stage and the router functionality appears, resulting in a potential loss of performance. By limiting the maximum number of ports of the routers, the use of complex routers during the network planning is avoided. This limitation also allows the optimization of area and leakage power. We use a simple intuitive model to measure the area variation of the components in the network. We assume that the major network components are the links and the routers. The total link area is proportional to the number of links that is obtained from the IP solution. The area of the router can be approximated by the complexity of the crossbar and buffer area [18]. The crossbar area has a quadratic dependency on the number of ports, while the buffer area dependency is linear. We assume that the leakage power is proportional to the network area. In order to estimate the variation of the delay and the dynamic power of the solution, we use a simulator with the incorporated Orion power model [19]. The same typical server workload example of the system, mapped to the 4x4 network, will be used to demonstrate the port limitation functionality. We perform a set of experiments, aimed at ﬁnding the optimal route assignment and link allocation, subject to additional limitations on the maximum number of input and output ports of the routers. Further we estimate the network parameters and make comparison to the results obtained for the full mesh solution with XY-routing. In these experiments, the number of ports includes the local connections to PEs (see discussion of (7)). In the full mesh solution, there are no limitations on the number of router ports. The largest routers with 5 input and 5 output ports appear in locations 5, 6, 9 and 10 (Fig. 11). Table III shows the results of Fig. 13: Network layout with 4 input and 4 output port limitation. Links connecting routers with the co-located PEs are not shown. solving the route assignment and link allocation problem with the number of input, output or both types of ports limited to 4. Each row is related to a different experiment with a particular port limitation. The ﬁrst two columns of the table represent the maximum number of ports that a router may have. The values in the following columns are normalized to those obtained for the full-mesh solution with XY-routing. Thus, the ratio of the total link, crossbar and buffer area is reported in columns from third to ﬁfth. The average packet delay and dynamic power are reported in the last two columns. An example of the network layout for the experiment with 4input and 4-output port limitation (4th experiment in Table III) is depicted in Fig. 13. This layout contains 42 links instead of the 48 links in the full-mesh solution. The total area of links, crossbars and buffers in the presented solution has been decreased by 12.5%, 13.7% and 6.4%, respectively. One can observe the average packet delay increase by 9.9% as well as the dynamic power increase by 6.4%. The increase of average delay, unless the contention is high, is caused by the removal of links, since the minimal path for the neighboring routers rises to 3 hops. However, the increasing delay may be an acceptable solution if certain nets are not critical (see discussion of the example in Fig. 2). Otherwise, a designer is allowed to put a limiting hop-count constraint for the critical nets. The variation in power should be calculated together with the leakage power decrease, that is correlated with the network area. The total power estimation is technology dependent, but due to the growing importance of the leakage power resulting from the technology downscale [20], the variation in power may be negligible as compared to the area savings. This example demonstrates the potential introduced by the port limitation mechanism. Its applicability can be combined with other design constraints. This provides a designer with a vast spectrum of possibilities for exploration and tuning. TABLE III: Port limitation results for server workload testcase. Port limit in out 5 5 5 4 4 5 4 4 Area xbar 1.000 0.928 0.928 0.863 link 1.000 0.916 0.916 0.875 Average delay 1.003 1.037 1.040 1.099 Dynamic power 1.001 1.027 1.037 1.064 buffer 1.000 0.969 0.969 0.936 133 TABLE IV: CPU time for link minimization. Number of cores 8 9 10 12 16 Network Minimal link size count (sec) 4x2 0.21 3x3 0.81 5x2 0.55 4x3 7.86 4x4 147.96 Deadlock-free (sec) Feasible Optimal 0.70 1.98 5.31 31.74 4.40 23.60 49.90 4031.25 2117.89 Timeout D. Computational time The computational complexity of the IP model depends signiﬁcantly on the number of binary variables of the model that determines the span of the branch-and-bound search. For a square mesh of P processing elements, the number of variables is about 4P 3 . Still, efﬁcient ILP solvers can handle this model for problems with moderate size. Table IV shows the CPU time for solving the model with the link minimization cost function (10), that is the most time consuming linear cost function. The third column shows the time to ﬁnd the minimal number of links that guarantee network connectedness. The last two columns report the CPU times for ﬁnding deadlock-free solutions, which are larger due to the introduction of the turn prohibition constraints. When an optimal solution is hard to ﬁnd, a feasible solution close to the optimal might be also sufﬁcient. The column “Feasible” reports the time required by the solver to ﬁnd the optimal solution, while the rest of the time was spent to prove the non-existence of a better solution. The solution for the 4x4 network was obtained by deﬁning a CPU timeout of three hours. The reported solution is the last one obtained within the timeout, without knowing whether it was optimal or not. Given the behavior for the other cases, we conjecture that this solution is very close to the optimal. The results show that optimal solutions can be obtained for moderate size networks. The model is also useful to partially explore the search space with CPU time limits, still obtaining high-quality solutions. V I . CONC LU S ION S In this paper we have presented a mathematical formulation of the problem for the simultaneous route assignment and topology selection in multiprocessor networks. It provides a solution for a large variety of routing and optimization problems in the design of on-chip networks with already ﬂoorplanned processing elements. The proposed IP model enables the designer to perform network topology exploration and tuning, subject to a large set of user-deﬁned constraints. The port limitation constraint prevents the incorporation of complex routers and enables a large ﬂexibility for exploration and optimization. The model was validated with a set of testcases, including typical server workloads, demonstrating the capability to explore solutions with different area-performance trade-offs and performing independent optimizations in various domains. ACKNOW LEDGM ENT This research has been funded by a grant from Intel Corp., research project CICYT TIN2007-66523 and FPI grant BES2008-004612. "
Impact of Half-Duplex and Full-Duplex DMA Implementations on NoC Performance.,"NoCs performance are usually explored stand-alone, overlooking the impact of the higher communication levels in the ISO OSI micro network stack. Nevertheless, since CPUs have to be relieved of communication management, higher communication levels such as DMA engines necessarily influence the communication performance. In this paper, we investigate how two different DMA implementations, full-duplex and half-duplex, can bias the behavior of a NoC designed for MPP architectures. From our studies, it turned out that a full-duplex DMA is more effective in preventing possible deadlock situations. Moreover, a deep performance analysis of a state-of-the-art NoC, in terms of transactions completion time, queuing time and injection delay, confirms the impact of the DMA in NoC-based MPP platforms, showing the advantages of a full-duplex approach.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip Impact of half-duplex and full-duplex DMA Implementations on NoC performance Francesca Palumbo, Danilo Pani, Alessandro Pilia, Luigi Raffo DIEE - Dept. of Electrical and Electronic Engineering University of Cagliari Email: {francesca.palumbo, danilo.pani, luigi}@diee.unica.it Abstract—NoCs performance are usually explored stand-alone, overlooking the impact of the higher communication levels in the ISO OSI micronetwork stack. Nevertheless, since CPUs have to be relieved of communication management, higher communication levels such as DMA engines necessarily inﬂuence the communication performance. In this paper, we investigate how two different DMA implementations, full-duplex and half-duplex, can bias the behaviour of a NoC designed for MPP architectures. From our studies, it turned out that a full-duplex DMA is more effective in preventing possible deadlock situations. Moreover, a deep performance analysis of a state-of-the-art NoC, in terms of transactions completion time, queuing time and injection delay, conﬁrms the impact of the DMA in NoC-based MPP platforms, showing the advantages of a full-duplex approach. I . INTRODUCT ION It is fairly well known that in the era of Ultra Large Scale Integrated systems, complexity has become one of the fundamental design issues to be handled. Integration fosters the realization of very large architectures able to achieve performance improvements leveraging on both frequency increase and simultaneous multithreading, through the integration of several processing cores on the same die. Multi-Processor Systems on Chip (MPSoCs) and Massively Parallel Processors (MPPs) are typical examples of this trend. Such complex architectures are the answer to the needs of today’s computationally hungry applications based on explicit thread level parallelism. Considering this scenario, it is clear the need for communication infrastructures able to fulﬁl stringent requirements. Communication backbone has to be modular, ﬂexible and able to support both parallelism and a certain Quality of Service (QoS). Networks on Chip (NoCs) are able to provide natively these features, becoming the most adopted solution in MPPs (e.g. the teraﬂop research chip by Intel [1] and Tile64 of Tilera [2]). Academic and industrial researches in this ﬁeld have been quite active [3], [4]. It is universally accepted that communication infrastructure performance impacts on the overall system behaviour. Targeting multithreaded applications, it is necessary to relieve processing elements and CPUs of communication management, to fully exploit the available time for computations. For instance, waiting either for a bus grant (in bus-based networks) or for the acknowledge of a circuits (in some NoCs) could lead to time hungry busy conditions. Highlevel communication modules, such as Direct Memory Access (DMA) engines, can be adopted to handle memory accesses and communications over the interconnection infrastructure, without any CPU intervention. In this paper we demonstrate that not only the NoC can inﬂuence the overall system behaviour, but it is also true that higher level communication modules, such as DMA engines, must be carefully taken into account since stand-alone NoC testing overlooks their actual impact on the system performance. To this aim, instead of designing the communication infrastructure and analyzing its impact on the computation, we assume a communication-centric platform model and we evaluate the bias of the higher communication layers in an ISO OSI micronetwork stack description [5] on the NoC performance. Our reference scenario is a tile-based architecture, composed or not of homogeneous tiles, where heterogeneous trafﬁc patterns are supported. The proposed investigations are focused at the tile level, evaluating the impact of two different DMA models, respectively half-duplex and full-duplex, over the overall system and the NoC performance. In particular, we demonstrate how full-duplex DMAs help in preventing possible deadlock situations. Through the analysis of the outcomes of several simulations, it will be discussed also the impact on traditional NoC metrics (e.g. injection delay, queuing time, latency and transactions completion time) of the two DMA models. The rest of this paper is organized as follow. Sect. II gives an idea of the reference scenario, Sect. III summarizes some interesting works in this ﬁeld, Sect. IV describes the two DMA implementations developed and adopted, Sect. V deeply analyzes the impact those implementations have on the overall system and NoC performance and , ﬁnally, Section VI concludes with some remarks. I I . CENAR IO Throughout the paper we will refer to a generic 8 × 8 2D tile-based MPSoC architecture composed of homogeneous tiles, as depicted in Fig 1. The tile components are: • the processing core; • the local memory, implemented as a Harvard model; • the DMA module, which is in charge of relieving the processor of low-level communication management; • the NoC modular elements, Network Interface (NI) and router. Typically, MPP architectures have to support heterogeneous trafﬁc patterns composed of small data transactions (e.g. 978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.35 249 T_00 T_01 T_02 T_07 T_10 T_11 T_12 T_17 T_70 T_71 T_72 T_77 P R O C. C O R E DMEM NoC node DMA NI ROUTER IMEM NoC Fig. 1: 8 × 8 tile-based 2D mesh of homogeneous tiles. end-to-end control packets and scalar data) and very long and heavy communications (e.g. long streams due to load balancing). Therefore hybrid switching networks ([6], [7]) can be adopted in order to provide the appropriate level of QoS to the different transaction types. To this aim, in this work we exploit the architecture presented in [8], which is dual mode and able to support packet and circuit switching in parallel. This reference scenario is conceived to efﬁciently implement both Task and Thread Level Parallelism. Each tile of the MPP has to execute one or more threads being able to communicate with other tiles of the same cluster assigned to a task, in order to exchange shared data. We suppose a distributed memory hierarchy, so that shared data are packetized and sent to solve interdependencies. If the size of such data is too large, they are sent exploiting the circuit switching support. I I I . RE LAT ED WORK S As far as we know not many works, already presented to the scientiﬁc community, have investigated the impact of highlevel communication components on the NoC performance. An interesting research in this ﬁeld has been proposed by Marescaux et al. in [9]. They analyzed, through a case study, the inﬂuence of inter-processor communications on the NoC trafﬁc. In particular, they discussed the comparison of cachebased versus scratch-pad managed interprocessor communication and demonstrated that considerable beneﬁts on the NoC performance can be achieved exploiting the presence of distributed DMAs to help performing scratch-pad management. Unfortunately, in their work the implementation of the DMA is not described but in any case the emphasis is on the importance of evaluating how higher communication layers can bias the NoC performance, as in the work we are proposing. In their works, Marescaux et al. use the DMAs to relieve the processing cores of communication management, which is the same approach followed in our work, but this is not always the case. In the Tile64 of Tilera [2], communication is not mediated through DMA engines, even if in each tile an autonomous 2-D DMA able to perform block copy functions (cache-to-memory, memory-to-cache and cache-to-cache) is embedded. Tile64 uses those DMAs to optimize packets processing: (a) to copy the next packet while the main core processes the previous one, (b) to copy shared data from the external memory into the caches and vice versa (memory consistency management). In the teraﬂop research chip by Intel [1], no DMAs are mentioned and each core is directly interfaced with the Router Interface Block, which acts as a NI being able of performing protocol conversion. Another work focused on the interaction between the NoC and higher level communication modules has been proposed by Meloni et al. [10]. They discussed how the chosen settings for the communication infrastructure can inﬂuence the overall behaviour of the designed system. In particular the relationship between NoC performance and its conﬁguration parameters, in the case of trafﬁc generated by cache operations, is examined. They conclude that, relatively to the execution time and energy consumption, it is very important to understand how the interconnect architectural parameters impact on the system performances, since a strong relationship can be highlighted between the cache serialization factor and the NoC packets dimension. As well as the Marescaux et al. research, this work proves that stand-alone testing is not sufﬁcient to completely determine the behaviour of a NoC in real scenarios. The DMA is a key component for several application oriented architectures, such as Digital Signal Processors (DSPs). In this case, the DMA is used to handle data transfers between memories and on-chip peripherals, e.g. (a) cache services, (b) programmed peripheral services and (c) user-programmed data transfers between different memory locations. To give an example of a commercial DMA, we can look at the Enhanced Direct Memory Access Controller (EDMA) of the C6000 DSP family by Texas Instruments [11]. The architecture of this memory mapped peripheral is divided in two parts: a Channel Controller (EDMACC) and a Transfer Controller (EDMATC). The EDMACC is the user-programmable part and consists of the register the user can set in order to properly instruct the device for periodic/one-shot transfers. It includes both the possibility of reloading a given parameters set (for periodic operations) and post-completion linking of parameters (to automatically perform ping-pong buffering, circular buffering and other typical DSP transfer). Since the EDMA is a multichannel device, every channel being associated to speciﬁc peripheral/CPU events, the user can conﬁgure the device to serve multiple peripherals. The EDMATC is the part of the device in charge to actually perform the transfers. The EDMACC operates on the basis of a parameter set speciﬁed by the user for every transfer. Such parameters are stored into an internal EDMA memory, called PaRAM, divided into Channel Parameters and Linking Parameters, respectively if they are peripheral/CPU triggered or simply linked at the completion of a data transfer (linking copies them on the Channel Parameter set relative to the channel to be served). Both these parameter sets share the same structure of the entry, depicted in Fig. 2. Due to the complexity of the DSP algorithms several transfer modes are provided: 1D-2D, element-frame synchronized, input and output sorting, and so on. The programming of the device in order to serve data transfers is available through a Quick DMA (QDMA) service, comparable to the normal service but user-triggered at the application level and without link/reload possibility. When the user wishes to execute a speciﬁc transfer between memory locations, it ﬁlls in the QDMA registers with a register/pseudo-register approach. The 250 EDMA channel Options Parameters (OPT) EDMA channel Source Address (SRC) Array/frame Count (FRMCNT) Element Count (ELECNT) EDMA channel Destination Address (DST) Array/frame Index (FRMIDX) Element Index (ELEIDX) Element Count Reload (ELERLD) Link Address (LNK) Fig. 2: EDMA channel parameters entries on C671x DSPs [11]. [Bold font = parameter names; Dashed registers = not present in the QDMA]. QDMA registers and the QDMA pseudo-registers are mapped at different addresses. Writing to the QDMA registers conﬁgures, but does not submit, a QDMA transfer request; whereas, writing to any of the pseudo-registers submits it [11]. C P U [8], would not be penalized at all by the choices done at the higher communication layers. MEMORY SL ADDR GEN_S MMR_S D_CNT_S MMR_R FSM S FSM R ADDR GEN_R S L N O C MEMORY ADDR GEN MMR FSM C P U N O C IV. HAL F -DU PL EX AND FUL L -DU PL EX DMA Benini and De Micheli in [5] demonstrated that it is possible to abstract electrical, logic, and functional properties of the interconnection scheme considering it as a micronetwork stack. Fig. 3 depicts the complete data ﬂow from source to destination in our reference MPP environment (Sect. II), highlighting the different ISO OSI layers. It is straightforward then to notice that the performance of the modules in the Network layer can be inﬂuenced by those of the components in the System/Transport layer. Even though this is well known from the scientiﬁc literature for the NI, it is less obvious for the DMA, as discussed in Sect. III. ISO OSI  LAYER IP core IP core IP core Application DMA END-to-END PROTOCOL DMA Network Interface Network Interface NETWORK PROTOCOL Router Router DMA Network Interface System / Transport Router Network END-to-END PROTOCOL NETWORK PROTOCOL Router NETWORK PROTOCOL NETWORK PROTOCOL NETWORK PROTOCOL Link Wiring / Data link Fig. 3: Communication dataﬂow and ISO OSI layers. In our reference scenario, such as in [9], the DMA acts as a communication manager. To deﬁne the impact it can have on the NoC behaviour, we have developed and integrated in the reference architecture two different DMA engines, halfduplex and full-duplex. Half-duplex refers to the possibility of transmitting data in just one direction at a time, whereas a full-duplex DMA engine is able to deal with communications in both directions in parallel. Since having a DMA inserted in the communication ﬂow has a cost, we are going to evaluate which implementation is able to better guarantee that the dual mode switching policy, implemented by NoCs such as [6], [7], D_CNT D_CNT_R (a) Half-duplex DMA Block Diagram  (b) Full-duplex DMA Block Diagram  Fig. 4: Half-duplex DMA Block Diagram (a) and full-duplex DMA Block Diagram (b). [Grey = SEND; Black = RECEIVE; SL = shared logic]. A. Half-duplex DMA Fig. 4a and Fig. 4b depict a very simple block diagram of the two DMA models developed and compared. The half-duplex one (Fig. 4a), is composed of a state machine (FSM ) handling all the control and dispatching/receiving data, extracted/written from/to a unique Memory Mapped Register bank (MMR), to/from the NoC. Data are read/written from/to the Memory, according to the address generated by an Address Generator (ADDR GEN ) controlled by the FSM. A down counter (D CNT ), programmed as soon as a transaction occurs, is used to access the proper location of the MMR bank. Differently from the EDMA architecture (Sect. III), all the registers must be sequentially written in order to trigger any data transfer. Both DMA engines are treated as memory mapped peripherals characterized by a bank of registers, shown in Fig. 6a for the half-duplex DMA. Compared to the one of the EDMA architecture, this MMR bank, composed of 32-bit registers, is different, presenting only a subset of parameters, several end-to-end reserved registers and being explicitly designed for a Harvard memory model (replicated ﬁelds for data and instructions). Together with the reserved registers there are some registers similar to those presented in Fig. 2. IOFFSET and DOFFSET are programmed by the CPU with the ﬁrst memory address occupied by a long stream of data to be transmitted. These two ﬁelds, not used in the receiving phase, are equivalent to the EDMA SRC parameter. ILENGHT and DLENGTH are used both in transmission and reception and represent the size of the data stream to be transmitted/received. In the EDMA this information is provided by the ELECNT ﬁeld. Since we assume that all the elements to be transferred have the same size, it is not necessary to specify the value of the element size. Finally, the STATUS REGISTER conveys mainly control information, used by the NI (both transmitting 251 31  30  29 28 RSV RSV RSV C  26 27 T  21 20 15 14 9 8 7 0 (X , Y) COORDINATE  TASK ID THREAD ID RSV  RSV  Fig. 5: Half-duplex and full-duplex DMA Status Register.[RSV = Reserved end to end control ﬁeld]. and receiving) to properly packetize the data. A detailed representation of this register is provided in Fig. 5. The C ﬁeld is used to deﬁne the priority level required to accomplish the demanded QoS. • C = 0 stands for packet switching transactions and is used to: – send scalar data to solve an interdependency among tasks in a multithreading environment (T = 0), – notify to the other interested tiles a task is migrated, when load balancing is implemented (T = 1). • C = 1 stands for circuit switching transactions establishment. In this case a higher priority packet is sent to reserve the path to destination when: – a vectorial data has to be transmitted to solve an interdependency among tasks (T = 0), – a task has to be moved from a tile to another, when load balancing techniques are implemented (T = 1). Fig. 6b summarizes all the four possible transactions. It is important to notice that for each combination of C and T during the transmitting/receiving phase, different information from/to the MMR bank are transferred to/from the NI. In the EDMA, the OPT register is in charge to deﬁne the transfer conﬁguration options, obviously more complex than our ones that require only these C and T bits. Moreover in the STATUS REGISTER: • (X,Y) COORDINATE are the destination tile coordinates, • TASK ID identiﬁes a speciﬁc task, • THREAD ID identiﬁes a speciﬁc thread of a task. ILENGTH DLENGTH RESERVED_4 RESERVED_3 STATUS REGISTER[28:9 ] (a) C=1; T=1; DCv =5  DLENGTH RESERVED_2 STATUS REGISTER[28:9 ] (c) C=1; T=0; DCv =3  RESERVED_2 DATA [15:0] DATA [31:16] STATUS REGISTER[28:9 ] (b) C=0; T=0; DCv =4  Xnew  - Ynew  STATUS REGISTER[28:9 ] (d) C=0; T=1; DCv =2  (b) IOFFSET ILENGTH DOFFSET DLENGTH STATUS REGISTER RESERVED_0 RESERVED_1 RESERVED_2 RESERVED_3 RESERVED_4 (a) Fig. 6: (a) Half-duplex Memory Mapped Registers (MRR) bank. (b) Information of the MMR bank exchanged between the DMA and the NI to establish a transaction. [DCv = default value assumed by DMA down counters] B. Full-duplex DMA The full-duplex DMA engine is similar to the half-duplex one, thus in this section just the implementation details differing from Sect. IV-A are described. Two channels are required here to manage the sending phase and the receiving one. Consequently two different MMR banks will be exploited to implement a non-blocking full-duplex DMA. These MMR banks are depicted in Fig. 7, clearly the Send one (Fig. 7a) is identical to the half-duplex MMR bank, whereas the Receive one (Fig. 7b) implements only a subset of registers. The nonblocking feature achieved with the full-duplex DMA guarantees the possibility of serving the demanded QoS. It is possible in fact to receive packets while a request for a circuit switching communication is pending. On the contrary, in the half-duplex engine a request for circuit establishment is blocking. In fact, even if the NI would start a communication with the DMA to send a lower priority packet, it will not be possible to serve it without losing the information stored in its MMR bank. IOFFSET ILENGTH DOFFSET DLENGTH STATUS REGISTER RESERVED_0 RESERVED_1 RESERVED_2 ILENGTH DLENGTH STATUS REGISTER RESERVED_2 RESERVED_3 RESERVED_4 RESERVED_3 RESERVED_4 (a) Send (b) Receive Fig. 7: Full-duplex Memory Mapped Registers (MRR) banks: (a) Send channel, (b) Receive channel. Since having two separated channels operating in parallel makes the control more complicated, two different FSMs exchanging control signals will be necessary to support the full-duplex behaviour. Fig. 4b depicts a very simple block diagram of a full-duplex DMA and, as it can be noticed, comparing it with Fig. 4a the logic is somehow duplicated. V. IM PACT EVALUAT ION This Section demonstrates how DMAs can bias the NoCs behaviour. It is organized into two parts: the ﬁrst one is more theoretical and discusses the possibility of preventing possible deadlocks adopting the full-duplex model, the second one presents a deep investigation of the impact on the NoC performance comparing the results achieved exploiting the full-duplex and the half-duplex engines. It should be noted that a DMA-free implementation, less efﬁcient from every point of view, cannot be full-duplex. 252 A. Deadlock Prevention As already said in Sect. II, our MMP environment is supported by a dual mode NoC. This interconnection is non-exclusive [8], meaning that circuit switched and packet switched communications are served in parallel, in order to allow a more efﬁcient support of heterogeneous trafﬁc characteristics. Nevertheless, depending on the DMA adopted, the combination of best effort and guaranteed throughput features can cause the onset of deadlocks. As discussed by Hansson et al. in [12], since NoCs are essential components of SoCs and MPSoCs, lots of researches have been dedicated to deadlock avoidance at the interconnection level. Indeed their focus has mainly been on the routers, not considering the interactions between the NoC and its connected modules. Hansson et al. demonstrated that these interactions can introduce message dependencies that possibly would cause deadlocks for the MPSoCs as a whole. Dally and Towles [13] demonstrated that deadlocks may occur in presence of evident cycles in the dependency graph of the system. In his work, Hansson pointed out that, even when NoC and IP dependency graphs are cycle-free in isolation, together they may still create cycles. The main principle at the base of deadlock-free NoCs is the consumption assumption [14]: all the messages forwarded through the routers can be accepted and delivered if the destination node is able to consume them. Nevertheless, deadlock could still arise due to protocol interactions between the NI and the higher level communication layers. Generally this is always the case when there is a shared resource [13]. Song et al. [14] demonstrated also that having separated NI buffers per message type, as in our reference architecture, is a necessary but not sufﬁcient condition to avoid deadlocks. Keeping all these considerations in mind, during our investigations we discovered that decoupling the send and receive channels also at the DMA level in combination with all the other techniques of deadlock avoidance at the NoC level [12], helps in breaking possible resource dependencies providing a more efﬁcient support to the consumption assumption. In fact as mentioned in Sect. IV-B, assuming to support both circuit and packet switching together, half-duplex DMAs are blocking with respect to lower level priority packets consumption. That is because a request for circuit establishment would freeze the half-duplex DMA until a positive response from the destination tile is received in order to preserve the content of the STATUS REGISTER (assuming that requests cannot be dropped end-to-end in case of negative answers). This situation would saturate the NoC and since packets, both lower and higher priority ones, ﬂow according to a wormhole switching technique (which is commonly adopted in NoCs) the situation presented in Fig. 8 can occur. During Phase 1 (Fig. 8a), T 12 is busy in a circuit switched communication with T 02 and then, even if T 00 sends iteratively its request (REQ) to T 02, the answer will always be negative (NACK). Meanwhile, other tiles (e.g. T 01, T 10 and T 11) can send lower priority packets to T 00 and, being the half-duplex DMA blocking T_00 T_10 T_00 REQ B D M A NI C NI D O D E C D M A NI C NI D O D E C D M A NI C NI D O D E C 1 0 H D M A NI C NI D O D E C D M A NI C NI D O D E C D M A NI C NI D O D E C T_01 REQ NACK D M A NI C NI D O D E C D M A NI C NI D O D E C T_11 (a) Phase 1 D M A NI C NI D O D E C T_01 D M A NI C NI D O D E C T_02 C I R C U I T T_12 T_02 0 1 BT D M A NI C NI D O D E C ACK D M A NI C NI D O D E C T_10 T_11 (b) Phase 2 T_12 Fig. 8: Deadlock example caused by the half-duplex DMA engine: (a) Phase 1 - Saturation; (b) Phase 2 - Deadlock. [H=Header, B=Body, T=Tail] with respect to them, those packets will not be consumed by T 00, causing the saturation of the decode channel (NI DEC) of the T 00 NI and possibly also of the lower priority FIFO (F1) in the connected router output channel. We found out that Phase 1 can lead to the occurrence of Phase 2 (Fig. 8b), where T 02 has been released and the REQ of T 00 is accepted, but the acknowledge packet (ACK) sent by T 02 to T 00 never reaches it. The reason is that if NI DEC and F1 of T 00 have previously been saturated, any packet ﬂowing according to a wormhole strategy (in this case a lower priority one from T 11) can prevent the ACK to arrive to its destination. In fact, the REQ is still pending, therefore the saturation of NI DEC and F1 in T 00 will never be resolved and any wormhole switching lover priority packet can be blocked between two routers (T 11 and T 01 in Fig. 8b), not allowing the ACK to proceed, despite its higher priority. Clearly, adopting a full-duplex DMA engine, non-blocking by deﬁnition, lower priority packets will not be blocked. Fullduplex DMA is able to consume any packet directly during Phase 1, when the REQ for T 02 is pending, thus a situation like the one presented in Fig. 8b is completely avoided. Obviously this is not the only possible way of dealing with this problem. Having implemented a half-duplex DMA: • one could deﬁne strategies of deadlock prevention or recovery at the operating system level; • the NI can be instrumented with watchdog timers and end-to-end control logic, in order to ﬂush a request and to recover correct system behaviour. Nevertheless, considering the beneﬁts that will be described in the next section and the fact that the area overhead is not 253 so much (11 32-bits registers instead of 7 in the MMR bank and a slightly more complex datapath), in our reference MPP environment we have favoured the full-duplex engine. B. Impact on the NoC Performance This Section explores the impact of the DMA model on the performance of the reference architecture (Fig. 1). The simulations have been performed using a parallel framework conceived for NoCs [15] that allowed us to deeply analyze the impact of the integrated DMA engines. Hereafter the two Designs Under Test (DUTs) are going to be referred as halfduplex DUT (hdDUT ) and full-duplex DUT (fdDUT ). The adopted framework extracts some typical metrics of NoCs and is able to stimulate the DUTs using different: injection time characteristics (for both lower level priority packets and circuit request packets), mapping on the 8×8 2-D reference tile-based architecture, and architectural parameters. 2 x 10 5 HD med. HD max. FD med. FD max. 1.5 1 0.5 ] s k c i t k c o c # l [ e m i T 0 50000 69354 93548 117741 141935 Number of injected packets 166129 190322 Fig. 9: Injection Delay sweeping on the number of injected low priority packets. [Number of Injected Circuits = 5000]. Fig. 9 shows the results in terms of Injection Delay (ID) for both the hdDUT and the fdDUT. The framework to perform this test conﬁgures automatically 32 different simulations sweeping on the number of injected lower priority packets (linearly incremented from 50000 to 200000) and collects the values of the ID (time that lasts between the theoretical scheduled time for a packet to be injected and its actual injection time during execution). Considering both the hdDUT case and the fdDUT case the increment of the medium and maximum ID is quite linear (more in the fdDUT case), but the values of the latter are considerably smaller. According to our analysis, the reason is that having a full-duplex engine allows to experience less stalls thanks to its non-blocking behaviour. This characteristic impacts also on the shape of the curves depicted in Fig. 9, less regular for the hdDUT. These observations are conﬁrmed by looking at the Average Packets Queuing Time (APQT), shown in Fig. 10. The APQT is representative of the incremental average over all the tiles (64 in our case) of the clock ticks necessary to each packet to leave the encountered queues in the path to destination. In particular for all the injected packets, the differences between the packets header arrival time and the relative tail departure time are added, queue by queue. Fig. 10 depicts the curves obtained for the hdDUT (a) and the hdDUT (b). These curves are far from being similar: the hdDUT case showing various “steps”, which are determined by the FIFOs saturation effect explained in Sect. V-A relatively to Fig. 8a. When a circuit is established, it can happen that, sooner or later depending on the duration of the circuit switched communication, the FIFOs along the circuit path or related to the sending DMA saturate in the hdDUT. When ﬁnally the circuit is released, all the stacked packets will proceed to their destination quite immediately, therefore their contributions to the APQT will be added quite contemporarily. Since circuits lengths, in our simulations, vary between 170-700 clock ticks and a DMA is considered busy even if still waiting for the receiver availability, then the value of the APQT could rapidly increase, as the steps in Fig. 10a prove. This behaviour does not affect the fdDUT since it is non-blocking. The zoom in Fig. 10a details one of the steps, showing that the increment is not instantaneous, but in 20 clock cycles the APQT experiences an increment of 4971 clock cycles. Fig. 10b demonstrates also that the nonblocking feature of the full-duplex engine makes the overall architecture more robust. In fact, even if the number of packets is linearly incremented from 50000 to 200000, the value of the APQT saturates always within 800 clock cycles and 1000 clock cycles. ] s k c i t k c o c # l [ e m i T 20 clk tks  s k t k c l 1 7 9 4 10000 8000 6000 4000 2000 0 0 0.2 0.4 0.6 0.8 1 1.2 Simulation Time [#clock ticks] 1.4 1.6 1.8 (a) Half Duplex DMA Implementation 2 x 10 5 ] s k c i t k c o c # l [ e m i T 1000 800 600 400 200 0 0 2.5 5 7.5 Simulation Time [#clock ticks] 10 12.5 (b) Full Duplex DMA Implementation 15 x 10 4 Fig. 10: Average Packets Queuing Time sweeping on the number of injected low priority packets: (a) hdDUT (b) fdDUT . [Number of Injected Circuits = 5000]. Going on with the lower priority packets injection time sweep test, Fig. 11 provides an overview of the performance, obtained for the hdDUT (a) and the fdDUT (b), in terms of Intrinsic NoC Latency (INL). The INL is the difference between the injection and the arrival time at the Network level (meaning that just the time spent to cross the routers is considered) and is retrieved by the simulation framework as a summary of statistics comprehensive of the mean, the standard deviation, the minimum, the lower quartile (25th percentile), 254             the median (50th percentile) and the upper quartile (75th percentile) over all the injected packets. In the hdDUT case the resulting INL values are better. In fact, considering the ﬁrst and the last simulations (5000 circuits each and respectively 50000 and 200000 lower priority packets), the INL goes from 60 to 2607 and from 60 to 3777 clock cycles. Moreover Fig. 11a highlights that the median of the INL is more stable, being always localized around 61 clock cycles, meaning the hdDUT is more robust to the variation of the lower level packets injection. (a) Half Duplex DMA Engine (b) Full Duplex DMA Engine 50000 69354 93548 117741 141935 Number of injected packets 166129 190322 50 100 150 200 T i m e [ k c o c # l i t s k c ] 50000 69354 93548 117741 141935 Number of injected packets 166129 190322 8000 7000 6000 5000 4000 3000 2000 1000 T i m e [ k c o c # l i t s k c ] Fig. 11: Intrinsic NoC Latency sweeping on the number of injected low priority packets: (a) hdDUT (b) fdDUT . [Number of Injected Circuits = 5000]. Nevertheless, this result can be deceptive. In fact, the INL represents only the time necessary to cross the routers, not the end-to-end transaction completion time. To this aim, the framework provides also the Transactions Completion Time (TCT) observable in Fig. 12. The TCT sums the ID and the INL contributions and is retrieved by the framework as a summary of statistics. The TCT of the fdDUT is better and it seems also to follow a more regular linear increment, being not subjected to stalls due to the blocking behaviour of the half-duplex DMA. Therefore an interesting conclusion in this sense is that, even if adopting a full-duplex DMA implies more time for the packets to cross the network, they can be scheduled in advance, thus the overall completion time beneﬁts from this behaviour. Lower level packets carrying interdependencies are more quickly delivered, allowing for a shorter thread completion time. For the sake of brevity it is not possible in this paper to present other complete examples of the performed tests. Nevertheless, the results were generally quite similar, at least in the global behaviours. Fig 13 depicts the ID obtained with two different allocation algorithms, namely a Custom [16] and a Buddy one [17], running eight simulations with a constant (a) Half Duplex DMA Engine (b) Full Duplex DMA Engine 50000 69354 93548 117741 141935 Number of injected packets 166129 190322 0.5 1 1.5 2 x 105 T i m e [ k c o c # l i t s k c ] 50000 69354 93548 117741 141935 Number of injected packets 166129 190322 0.5 1 1.5 2 x 105 T i m e [ k c o c # l i t s k c ] Fig. 12: Transaction Completion Time sweeping on the number of injected low priority packets: (a) hdDUT (b) fdDUT . [Number of Injected Circuits = 5000]. (a) Buddy Allocation Algorithm 300 400 500 600 700 Standard deviation [#clock ticks] 800 900 1000 0.5 1 1.5 2 2.5 3 x 10 5 T i m e [ k c o c # l i t s k c ] HD med. HD max. FD med. FD max. 300 400 500 600 700 Standard deviation [#clock ticks] 800 900 1000 0.5 1 1.5 2 2.5 3 x 10 5 T i m e [ k c o c # l i t s k c ] (a) Custom Allocation Algorithm Fig. 13: Injection Delay sweeping on the standard deviations of the Gaussian distributions of the packets injection time: (a) Buddy allocation algorithm, (b) Custom allocation algorithm. [Number of Injected Circuits = 5000, Number of Injected Packets = 200000]. 255                         number of circuits and packets and constant circuits injection time distribution (uniform), varying only the packets injection time distribution (Gaussian with the same mean sweeping on the standard deviation). The Buddy allocation algorithm, which allocates regular rectangular clusters, is able to provide more robustness to the variation of the injected trafﬁc. In fact, all the curves in Fig. 13a are not subjected to the ﬂuctuations characterizing the curves in Fig. 13b, which refers to the Custom algorithm. Nevertheless, it is also true that allocating irregular areas, as the Custom allocation scheme does, helps in achieving smaller ID values. It is important to notice that the fdDUT is still able to provide better results than the hdDUT, but all their curves are quite linear since in this simulation set the injected population size is constant. V I . CONCLU S ION S In this paper we presented the effect of two different DMA models, half-duplex and full-duplex, on the performance of a hybrid switching NoC. The main goal was to highlight that optimal NoC performance cannot be achieved overlooking the effects that, in an ISO OSI stack representation of the end-toend communication process, the higher level communication layers can have. In other words, stand-alone NoC testing could lead to draw conclusions about the interconnection backbone that could not hold if some key aspects of the higher level layers are not taken into account. To this aim, we investigated the resulting performance of a NoC interfaced with a half-duplex DMA engine and with a full-duplex one, introduced to relieve the CPU of memory accesses and communication management. First of all, it turned out that a full-duplex implementation helps in preventing possible deadlock situations. In fact, it does not limit the consumption assumption and then does not create any stall of higher priority packets due to the saturation of lower priority FIFOs. Moreover, simulation results demonstrate that the performance of the NoC in terms of injection delay and queuing time beneﬁt from the possibility of avoiding long stalls due to the native blocking behaviour of the half-duplex DMA. Full-duplex DMAs do not generate FIFOs saturation and consequently avoide long delays in packets injection. Finally, we have also shown that, despite NoC latency is better adopting the half-duplex DMA, the overall completion time is worst. Summarizing, beyond the optimization and tuning effort at the NoC level, some effort should also be put in the deﬁnition of the best high-level communication modules in the system. As a matter of fact, we demonstrated that not only the communication backbone can inﬂuence the overall system performance, but also such higher level communication modules can limit the performance of the interconnect. In particular, for hybrid switching architectures such as [6], [7], [8], the adoption of a full-duplex DMA could lead to better performance. Taking into account the application level performance, the exploitation of a DMA engine is necessary to reduce the overhead of the CPU, due to the direct management of the communication, thus the DMA model should be carefully considered to avoid losing the advantages of a particular NoC. Acknowledgments The research leading to these results has received funding from the European Community’s Seventh Framework Programme (FP72007-2013) under grant agreement n. 248424, MADNESS Project.  [1] S. Vangal, J. Howard, G. Ruhl, S. Dighe, H. Wilson, J. Tschanz, D. Finan, P. Iyer, A. Singh, A. Singh, T. Jacob, S. Jain, S. Venkataraman, Y. Hoskote, and N. Borkar, “An 80-tile 1.28tﬂops network-on-chip in 65nm cmos,” in Solid-State Circuits Conference, 2007. ISSCC 2007. Digest of Technical Papers. IEEE International, 2007, pp. 98–589. [2] S. Bell, B. Edwards, J. Amann, R. Conlin, K. Joyce, V. Leung, J. MacKay, M. Reif, L. Bao, J. Brown, and M. Mattina, “Tile64tm processor: A 64-core soc with mesh interconnect,” in International SolidState Circuits Conference, 2008 (ISSCC 2008). [3] T. Bjerregaard and S. Mahadevan, “A survey of research and practices of network-on-chip,” ACM Comput. Surv., vol. 38, no. 1, p. 1, 2006. [4] E. Salminen, A. Kulmala, and T. D. H ¨am ¨al ¨ainen, “Survey of networkon-chip proposals,” OCP-IP white paper, 2008. [5] L. Benini and G. De Micheli, “Powering networks on chips: energyefﬁcient and reliable interconnect design for socs,” in ISSS01: Proc. of the 14th international symposium on Systems synthesis. New York, NY, USA: ACM, 2001, pp. 33–38. [6] K. Goossens, J. Dielissen, and A. Radulescu, “The Æthereal network on chip: Concepts, architectures, and implementations,” IEEE Design and Test of Computers, vol. 22, no. 5, pp. 21–31, 2005. [7] N. D. E. Jerger, L.-S. Peh, and M. H. Lipasti, “Circuit-switched coherence,” in NOCS ’08: Proc. of the Second International Symposium on Networks-on-Chip, 2008, pp. 193–202. [8] F. Palumbo, S. Secchi, D. Pani, and L. Raffo, “A novel non-exclusive dual-mode architecture for mpsocs-oriented network on chip designs,” in SAMOS08: Proc. of the 8th international workshop on Embedded Computer Systems. Berlin, Heidelberg: Springer-Verlag, 2008, pp. 96– 105. [9] T. Marescaux, E. Brockmeyer, and H. Corporaal, “The impact of higher communication layers on noc supported mpsocs,” in NOCS ’07: Proc. of the First International Symposium on Networks-on-Chip, 2007, pp. 107–116. [10] P. Meloni, G. Busonera, S. Carta, and L. Raffo, “On the impact of serialization on the cache performances in network-on-chip based mpsocs,” in Proc. of the Euromicro Symposium on Digital Systems Design, 2007, pp. 556–562. [11] TI, TMS320C6000 DSP Enhanced Direct Memory Access (EDMA) Controller "
A 128 x 128 x 24Gb/s Crossbar Interconnecting 128 Tiles in a Single Hop and Occupying 6% of Their Area.,"We describe the implementation of a 128×128 crossbar switch in 90 nm CMOS standard-cell ASIC technology. The crossbar operates at 750 MHz and is 32-bits for a port capacity above 20Gb/s, while fitting in a silicon area as small as 6.6 mm 2 by filling it at the 90% level (control not included). Next, we arrange 128 1 mm 2 ""user tiles"" around the crossbar, forming a 150 mm 2 die, and we connect all tiles to the crossbar via global links that run on top of SRAM blocks that we assume to occupy three fourths of each user tile. Including the overhead of repeaters and pipeline registers on the global links, the area cost of the crossbar is 6% of the total tile area. Thus, we prove that crossbars are dense enough and can be connected ""for free"" for valencies exceeding by far the few tens of ports, that were believed to be the practical limit up to now, and reaching above one hundred ports. Applications include Combined Input-Qutput Queued switch chips for Internet routers and data-center interconnects and the replacement of mesh-type NoC for many-core chips.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip A 128 × 128 × 24Gb/s Crossbar Interconnecting 128 tiles in a single hop and Occupying 6% of their area Giorgos Passas, Manolis Katevenis, and Dionisis Pnevmatikatos Inst. of Computer Science (ICS) Foundation for Research and Technology (FORTH) Heraklion, Crete, Greece Email: {passas, kateveni, pnevmati}@ics.forth.gr Abstract—We describe the implementation of a 128×128 crossbar switch in 90nm CMOS standard-cell ASIC technology. The crossbar operates at 750MHz and is 32-bits for a port capacity above 20Gb/s, while ﬁtting in a silicon area as small as 6.6mm2 by ﬁlling it at the 90% level (control not included). Next, we arrange 128 1mm2 “user tiles” around the crossbar, forming a 150mm2 die, and we connect all tiles to the crossbar via global links that run on top of SRAM blocks that we assume to occupy three fourths of each user tile. Including the overhead of repeaters and pipeline registers on the global links, the area cost of the crossbar is 6% of the total tile area. Thus, we prove that crossbars are dense enough and can be connected “for free” for valencies exceeding by far the few tens of ports, that were believed to be the practical limit up to now, and reaching above one hundred ports. Applications include Combined Input-Qutput Queued switch chips for Internet routers and data-center interconnects and the replacement of mesh-type NoC for many-core chips. I . INTRODUCT ION Crossbar switches are basic building blocks for interconnection networks. Because their cost grows with the square of their port count, it is commonly believed that they become overly expensive for valencies above 32 or 64. In particular, designers generally believe that many-core chips need a multi-hop Network-on-Chip (NoC) to interconnect a few tens of processing tiles because a crossbar would be prohibitively expensive in terms of wires and crosspoints. Moreover, in the domain of router systems for multiprocessors, clusters, or the Internet, several designers consider that internal speedup is expensive for high-valency crossbars, and that crosspoint queueing is an effective method to eliminate it, thus replacing combined-input-output queueing. To our surprise, when we set out to quantitatively measure the above costs by doing real VLSI layouts, we discovered gross misconceptions in the above common beliefs. Using a conservative 90nm CMOS standard-cell ASIC technology, we layout a 128×128 crossbar switch with a port capacity of 24Gbps in an area of just 6.6mm2 (control is not contained in that area). Gate count and wiring is so large that the standard EDA tools were unable to automatically place and route the crossbar components. However, owing to the regularity of the circuit, we wrote a script that algorithmically places the gates in an orderly layout. Although the standard cells are packed together at an area utilization of 90%, regularity and wiring resources are such that the EDA tools are then able to successfully route all the required wires above the standard cells. To our knowledge, this is denser than any previously published high-valency crossbar layout. We assume that our crossbar is surrounded by 128 IP tiles, of size 1mm×1mm each, arranged in a 12×12 = 144 square array, where the crossbar and its control replace the 16 = 4×4 centermost tiles. Assuming that three fourths of each tile contain SRAM blocks (e.g. cache memories next to a processor, or queues in a switch chip), we ﬁnd out that all 128 input and 128 output links of the crossbar can be easily routed over the SRAM blocks, thus incurring virtually no area overhead to the IP tiles for wiring –actually, the area overhead for repeaters and pipeline registers on these wires is 0.5% of the total tile area. Including this overhead, the area cost of the whole crossbar network is 6% of the total tile area. Following the above discussion, the rest of this paper is organized as follows. We start by discussing related work in the next section II. In section III, we examine various site plans of the crossbar in its context of IP tiles and we explain why we opted for a centralized crossbar. In section IV, we describe the crossbar circuit and its layout. Section V presents area cost numbers of a baseline crossbar scheduler to demonstrate the feasibility of the control circuit. Finally, section VI is a conclusion. I I . RE LAT ED WORK Kim e.a. [1] have recently shown that high-valency switch chips reduce the diameter of the interconnection network, and with it, its latency and power consumption, thus being a good tradeoff. Switch chips usually employ a crossbar to interconnect their ports because the crossbar is the simplest non-blocking topology. However, as the valency of the switch increases, the scheduling of the crossbar becomes harder. To simplify and improve the performance of crossbar scheduling, Kim e.a. adopted crosspoint queueing (CQ) as an alternative to the traditional input queueing (IQ). However, 978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.37 87 CQ was found expensive due to the high partitioning of the switch memory; since there is one memory per crosspoint, the total number of memories grows as O(N 2 ), which is costly for ﬂow and congestion control algorithms –see references [2][3] for a further discussion on this cost. Thus, Kim e.a. proposed the hierarchically-queued crossbar (HQ) as an organization that lowers memory partitioning. In this organization, an N × N crossbar is partitioned in (N/k)2 k × k sub-crossbars and memories are placed only at the inputs and outputs of the sub-crossbars. Hence, the total number of memories is reduced from O(N 2 ) to O(N 2 /k). Unfortunately, this organization has a major disadvantage: Although partitioning is lowered, it remains unacceptably high, especially when N is large. The reason is that each sub-crossbar has to be relatively small in order to be efﬁciently scheduled, which, in turn, implies a small k and a quick growth rate of the total number of memories. High memory partitioning also increases the switch implementation cost. Switch memories are usually implemented with SRAM blocks to increase memory density. Given that technology bounds the maximum on-chip memory capacity, the higher the partitioning, the smaller the SRAM blocks. For small SRAM blocks, the area overhead of the peripheral control circuitry becomes comparable to the area of the memory cell array [4]. Scott e.a. [5] showed an implementation of HQ for a 64×64 switch with 8×8 sub-crossbars. Due to the high memory partitioning, they implemented the memories with registers to avoid the small and costly SRAM blocks. Thus, they lowered memory density at least by an order of magnitude. We are studying the combined input and output queued (CIOQ) switch organization. CIOQ places memories only at the inputs and outputs of the crossbar and compensates for scheduling inefﬁciencies by over-provisioning the throughput of both the crossbar and the memories –this overprovisioning is usually referred to as internal speedup. Thus, in CIOQ, memory partitioning grows as O(N ), i.e. by a factor of N/k slower than in HQ. We focus on the cost of speeding up the crossbar. We consider a reference 128×128×10Gb/s CIOQ switch and we study the implementation of a 128×128×20Gb/s crossbar, which gives a speedup of two. On the other hand, the cost of speeding up the memories is obvious and low, as their throughput easily expands by arranging some SRAM blocks in parallel1 . Our work is also related to [6]. Pullini e.a. showed that using ﬂat EDA ﬂows, it is impossible to place and route a 32×32 32-bit-wide crossbar with area utilization above 50%. They considered this utilization unacceptable, thus they suggested avoiding excessively large crossbars in NoC. 1 32-bit-wide, 4K-word-tall, single-port SRAM blocks have a worst-case latency of 2.9ns and an area of 0.3mm2 and are optimal both latency and area wise [4]. For a throughput of 30Gbps, it sufﬁces to arrange just three such blocks in parallel. By contrast, in the same technology, we demonstrate an hierarchical ﬂow which gives an area utilization of 90% for the same crossbar width and four times higher valency. Last but not least, a high-valency crossbar was also used in the many-core chips of the IBM Cyclops64 supercomputer. The only publicly available information we could ﬁnd on this crossbar, though, is in reference [7]. According to this reference, this is a 96×96 96-bit-wide crossbar implemented in a 90nm technology, running at 533MHz, and occupying 27mm2, including the circuits for queuing, arbitration, and ﬂow control. However, reference [7] provides insufﬁcient information on the structure of the crossbar circuit and layout, hence a direct comparison is difﬁcult. Judging from this information, the crossbar organization we propose here is radically different. In particular, the IBM Cyclops64 processor chips appear using a crossbar with an one-dimensional, port-sliced layout, while we describe a more scalable twodimensionallayout, studying in detail the organization of the crossbar links over the SRAM memories, and we show that bit slicing signiﬁcantly reduces area. Furthermore, we show why and how the metal tracks over both the standard cells and the SRAM blocks sufﬁce to route the whole wiring of the crossbar. I I I . CRO S SBAR S IT E PLAN S We describe and compare several alternative locations of the crossbar on the chip in its context of user tiles. Typical designs use cores consisting of some processing element, P , and one or more memory blocks, denoted $ in the ﬁgures below. We call these user tiles, and we do not care whether their memory is used as cache or local memory, and whether it is physically built as a couple of large blocks or a collection of several smaller ones. Similarly, in CIOQ switch chips, each user tile contains the input and output queues associated with one of the switch ports, and thus consists mostly of SRAM blocks, plus a small area for port control. We denote by N the number of user tiles; N will also be the crossbar valency. We are interested in N = 128, but, in this section, we will be treating N as a parameter for the sake of presentation. We consider dies with an aspect ratio of 1 (i.e. square), as this is the best in terms of balancing the distances in the two axes; however, our discussion applies equally well to dies with any other reasonable aspect ratio. We denote by α the die edge size, so the total die area is α2 , excluding chip I/O (pad and associated) circuitry. We consider a = 12mm. Finally, we assume that the memory of each user tile comprises a signiﬁcant portion of the tile area –up to three fourths thereof– and we use that area for routing wires on top of SRAM. This assumption is in accordance with most general purpose processing cores where the actual processing part is rather small compared to the memories and caches needed to support this processing. On the other hand, processing elements use substantial logic and complex 88 P P P P P P P P P S S S S S S S S S S P S P S P P P P P S P S P S P S P S S P S P S P S S S S S P S P S S P S S S xbar tile (a) (b) (c) (d) Figure 1. Crossbar site plans: (a) column of user tiles using side crossbar, (b) matrix of user tiles using distributed crossbar, (c) matrix of user tiles using centralized crossbar, (d) balanced link density of (c) in a “12×12 minus 4×4” matrix of user tiles. interconnections and beneﬁt from the use of all metal layers. In contrast, SRAMs are much simpler in their wiring needs; in our reference process, they obstruct only the four lower of nine metal layers, thus leaving the ﬁve upper metal layers available for routing in both directions. A. Column (1-D) of User Tiles using Side Crossbar Figure 1(a) shows the ﬁrst site plan considered. The user tiles are arranged in a single column, with the crosspoints to each of them lying on its side. While being simple, this arrangement is non-scalable to large N . It can be easily shown that when the area of the crossbar is much smaller than the total area of the die, which is the usual case for non-trivial user tiles, the aspect ratio of each user tile converges to 1/N . User tiles with such aspect ratios are clearly problematic for large N . One reason is that memory is often built using a few, relatively large SRAM blocks, in order to optimize memory density; another disadvantage is that very long tiles incur longer wire delays. A straightforward improvement is to place the crossbar in the middle of the die area, and arrange half of the user tiles on each side thereof; a similar arrangement was proposed in [8][7]. In this way, the aspect ratio is improved by a factor of two, and this is probably the best arrangement for small N . However, it remains non-scalable to large N . B. 2-D Matrix of User Tiles using Distributed Crossbar Figure 1(b) depicts a more scalable plan. The user tiles are Γ-shaped with the processor at the corner and the memory placed in a √N × √N matrix where each tile owns a occupying at least two thirds of their area. The tiles are crossbar input line spanning its row and a crossbar output line spanning its column. Thus, each tile also embraces the √N × √N crosspoints between the √N (horizontal) crossbar input lines that are owned by the tiles in its row and the √N (vertical) crossbar output lines that are owned by the tiles in its column. Providing space for the crossbar lines can be achieved in two ways. The traditional way is to leave wiring channels in both directions between the user tiles. This approach has considerable overhead, and it has been used for interconnection topologies that do not require many wires, such as 2-D meshes. However, in ﬁgure 1(b), we show an alternative approach, where the crossbar lines are routed above the SRAM blocks. The size of the over-the-tile routing channel is 0.5 × α/√N . This channel can be utilized according to the routing pitch on each metal layer. Using an average mid and higher metal layers routing pitch of 500nm and assuming N = 144 and a = 12mm, we ﬁnd that there are on the order of one thousand available wires per direction on top of each user tile. Since √N crossbar lines are routed over each tile in each direction, a line width up to 80 wires is feasible. Given the availability of many wires, this topology comes as a natural extension of the 2-D mesh used frequently in many-core designs [9] [10] [11] [12]. From another point of view, it can be considered as an enriched mesh, where each tile uses a router with more crosspoints and links. C. 2-D Matrix of User Tiles using Centralized Crossbar Figure 1(c) shows an alternative two-dimensional plan. The user tiles are now square and the memory occupies at least three fourths of their area. The crosspoints are concentrated in the central region of the matrix replacing some of the user tiles. Hence, the crossbar lines are shorter, as all the crosspoint logic is kept in close proximity, while the wires connecting the user tiles with the crossbar are extended to reach the center. Essentially, instead of routing the crossbar lines over the tiles, we route links connecting the user tiles to the crossbar. The over-the-tile link density can be balanced as in ﬁgure 1(d). The tiles are divided in four isometric groups and the tiles of each group are routed to a distinct edge of the crossbar region2. The density varies from its lowest at the corners of the die, to its highest at the periphery of the crossbar. The highest density over a tile is N 2∗γ uni2Care must be taken to avoid unwanted swastika-like conﬁgurations. 89 directional links, where γ the ratio of the edge length of the crossbar region to the edge length of a user tile. In ﬁgure 1(d), N = 128 and γ = 4, thus at most 16 links are routed on top of a user tile. On the other hand, assuming the same die edge and routing pitch, the wiring resources over the user tiles remain the same as in the example of the previous subsection. Thus, we can permit a link width up to 60 wires. The examples and analysis in the last two subsections clearly demonstrate that even for a conservative technology, there are enough wiring resources to route either the distributed or the centralized crossbar. D. Distributed versus Centralized Crossbar Comparison While both crossbars are feasible, each has advantages and disadvantages. In particular, communication distance scales better and over-the-tile wiring is better balanced when the crossbar is distributed, but, then, the cost of the global links also increases. First, the total length of the global links is higher. In the distributed crossbar plan, the total length is 2×N ×α. On the other hand, in the centralized version, it can be approximated by the sum of the distances of all tiles to and from the center √N √N X X i=1 j=1 2 × (|i × α √N − α 2 | + |j × α √N − α 2 |), which converges to N × α. Hence, the distributed crossbar approximately doubles the total length of the global links. Assuming the total length of the global links is much higher than the distance between successive repeaters or pipeline registers, by doubling it, the distributed crossbar also doubles the number of repeaters and pipeline registers. As we will show in section IV, the overhead of repeaters and pipeline registers nears 40% of the whole crossbar area in our implementation. A second disadvantage of the distributed crossbar has to do with the speed of the global links. In the distributed crossbar, half of the global links, the vertical ones, are actually multiplexor circuits, while in the centralized version, all global links are “logicless”, driving a single value to the crossbar region. Because repeaters are faster than multiplexor gates, by distributing the crossbar, we also lower the bandwidth and increase the latency of the global links. IV. THE CRO S SBAR C IRCU IT AND LAYOUT We implemented the centralized crossbar in a 90nm CMOS standard-cell standard-performance ASIC process [4]; standard-cell height is 2.8µm, supply voltage ranges from 0.9 to 1.1V, junction temperature varies from -40 to 125◦C, and there are 9 layers of interconnect (M1-M9). In order to synthesize, place, and route the circuit, we used some popular industrial tools. Initially, we were experimenting with ﬂattened designs, but, due to the large 0 1 2 3 4 5 6 7 8 9 10 11 12 13 0 1 2 3 4 5 6 7 8 9 10 11 12 13 127 127 0 1 2 3 4 5 6 7 15 31 63 127 mux13−18 out mux32 out 0 1 2 3 4 5 6 7 8 9 10 21 42 mux11 out mux23 out mux47 out mux95 out (a) (b) (c) (d) Figure 2. Four circuits for the implementation of a crossbar output line size of both the circuit and the die, the result we got was long tool running times and low performance, in terms of both area efﬁciency and timing closure; this is in accordance with the results presented in reference [6]. Thus, given the regularity of crossbars, we decided to specify the structure of the circuit ourselves, using hierarchical prototyping. Below, we describe the process we followed. A. Circuits for Crosspoint Implementation Figure 2 depicts four alternatives for crosspoint implementation; 128 crossbar input lines and a single crossbar output line are shown, assuming each crossbar input line fans-out to 128 circuits same as the above. In (a), each input line crosses the output line through a tristate driver. The advantages of this circuit are that: (i) it is simple; and (ii) it requires a single metal track to implement the output line. The disadvantages are that: (i) each driver is loaded with the large parasitic capacitance of the whole output line; and (ii) EDA ﬂows encounter problems in designs that use tristate drivers3 . (b-d) describe approaches using multiplexor gates. In our reference process, such gates can be compiled to either transmission or logic gates. In (b), the crossbar output line is implemented with a chain of two-to-one multiplexors. As in (a), a single metal track sufﬁces, but a prohibitively long delay of 128 multiplexors in series is needed. In (c), multiplexing is parallelized by connecting the multiplexors in a binary tree. The levels of multiplexors are reduced to 7 (=log2 N ), at the cost of 7 metal tracks. 3 For example, optimization through buffer insertion is impossible, unless the bus is explicitly segmented. Furthermore, Automatic Test Pattern Generation (ATPG) is complicated by the requirement that a single tristate driver drives the bus at any point in time, so that bus contention is avoided. 90 However, as we will show in section IV-C, these tracks introduce zero area overhead, as they are already available over the multiplexor standard cells. Thus, (c) is preferable to (a) and (b). Finally, in (d), a shorter and wider circuit is shown, as an alternative to (c). The multiplexors have a valency of four and are connected in a quad tree. B. Bit Slicing versus Port Slicing Comparison The bit width, W , of the multiplexors is derived from the ratio of the desired port throughput to the achieved clock frequency. Two groupings of gates are then possible. One grouping, called port slicing [13], places all bits of each multiplexor close to each other; for example, they can be arranged as a √W × √W matrix. This grouping minimizes the span of each control wire, since all gates that it controls are placed in close proximity. The other grouping, called bit slicing [13], places together all multiplexor gates for a given bit position, and replicates this structure W times, for all bit positions. This grouping minimizes the distance among gates that contribute to the generation of each crossbar output signal. Intermediate solutions are also possible (see e.g. reference [14] for “byte slicing”), but they are not studied here. In order to compare bit and port slicing, we have to contrast: (i) the wiring for the connection of the data and control ports of the crossbar with the slices; and (ii) the sizing of the multiplexor gates. As we described in section III, the data ports of the crossbar have to be uniformly distributed at its periphery in order to balance the over-the-tile link density. Hence, no matter how the crossbar is sliced, analogous wiring is needed for the connection of the crossbar data ports with the slices. Besides, for the control ports, the critical problem is their fan-out capacitance, which is the same in both port and bit slicing, rather than the length of their path to the slices. Thus, we focused on gate sizing. We ﬁrst emulated one output line of a bit-sliced crossbar by implementing the binary-tree circuit of ﬁgure 2(c); we considered each row corresponds to a standard cell row and we empirically set its width to 80µm to provide sufﬁcient space for in-place optimization. We placed the circuit, performed trial routing to extract its parasitic capacitances, inplace optimized it under a range of timing constraints, and ﬁnally routed it. For each timing constraint that was satisﬁed after ﬁnal routing, we measured the actual delay and the total standard-cell area of the circuit. The resulting delayarea curve is plotted in ﬁgure 3(a), labeled bit width = 1. Notice that the inverse plot is infeasible, as EDA algorithms prioritize speed over area. By increasing the distance between the rows of multiplexor gates, we emulated port-sliced conﬁgurations of various bit widths. A bit width W corresponds to √W −1 empty standard cell rows between successive rows of multiplexor ) 0 0 0 1 x m u q s ( a e r A l l e C d r a d n a t S 4 bit width = 64 36 1 64 36 1 3 2 1 0.6 0.8 1.0 1.2 1.4 0.6 0.8 1.0 1.2 1.4 Delay (ns) Delay (ns) (a) (b) Figure 3. Delay-area curves of a port-sliced (a) binary- and (b) quad-tree crossbar output line as a function of its bit width. Bit-slicing coincides with port slicing when bit width = 1, and stays unaffected by bit width. gates, or (√W − 1) × 2.8µm. Figure 3(a) plots curves for W = 1, 36, and 64. We observed that for distances up to 200µm (W <5000) the EDA algorithms optimize the circuit by increasing the size of the multiplexor gates, while they switch to repeater insertion for longer distances. We repeated the same experiment for the quad tree. Figure 3(b) shows the resulting performance curves. We conclude that, for both trees, the closer the multiplexor gates are clustered together, the faster and the more area efﬁcient the circuit that results. Then, the output load of the gates is reduced, thus their performance increases, and smaller-size gates sufﬁce, thus reducing area; reduced area further improves performance. Moreover, we observe that the quad tree is more area efﬁcient being three times shorter4 . Last, for a circuit delay of 0.8ns, a 36-bit wide port-sliced implementation would increase the standard-cell area of a crossbar output line by at least 40% compared to a bit-sliced one. Hence, bit slicing is preferable to port slicing. C. Organization of the Bit Slice We implemented a crossbar bit slice by: (i) structuring a crossbar output line as one of the circuit instances that generate the curve “bit width = 1” of ﬁgure 3(b); (ii) replicating the line for each crossbar output; (ii) placing the standard cells of all lines with a custom script5 ; (iii) deﬁning IO circuits and similarly placing them; and ﬁnally (iv) routing the whole circuit using the standard EDA tools. Our approach was to implement an as-fast-as-possible bit slice, while pipelining its IO links. The ﬂoor plan and the 4An additional, but minor, factor increasing the area efﬁciency of the quad over the binary tree, is the better area efﬁciency of the four- over the two-to-one multiplexor gates. 5 The script generates the coordinates of standard cells and writes a placement deﬁnition ﬁle in a standard format 91         control flip flops (F) and drivers (C) F127 F0 5.6 duplicated in. drivers I129 I131 I132 I134 I135 I255 I254 I125 I0 I2 I5 I1 I4 I7 I127 I126 0 2 1 . 4 F56 F61 C62 C63 F120 2 2 . 4 1040 F0 F5 F8 F16 F13 C14 signals to same out C15 C23 C6 C7 F64 5.6 C975 C967 C983 C1022 C1023 C966 15.7 2.8 I128 I130 I133 I253 8 2 1 c r a b s s o r i u p n t s a n g s i l 128 crossbar output signals d u p l i c a t e d c r o s s b a r i n p u t s g n a s i l 1 2 8 . 8 7.6 73.1 4 5 1 u m output flip flops (F) 1114 um 967.7 967.7 2.8 2.8 24.4 2.8 in. drivers (I) U O 0 T U O 1 T U O 7 2 1 T mux matrix 1024 output control signals from arbiter Figure 4. Floor plan of the crossbar bit slice elements of the bit slice are shown in ﬁgures 4 and 5 respectively. We implemented a crossbar output line with the instance (bit-width=1, delay=0.8ns) of ﬁgure 3(b). This instance uses identical four-to-one multiplexors for all levels of the multiplexor tree except for the root. The non-root multiplexors are built with And-Or-Invert standard cells as in ﬁgure 5(a). We placed them in a wide area, as in (b), to provide for the quad tree wiring. The root multiplexor is bigger, built with NAND gates, and takes up four standard-cell rows; however, it is not further described here for brevity. We stacked the multiplexors as in ﬁgure 5(c), forming a crossbar output line. By replication, we placed all crossbar output lines in a 967.68µm×128.8µm area, which is labeled “multiplexor matrix” in ﬁgure 4. Each crossbar input drives the ﬂip ﬂop of ﬁgure 5(d). In turn, the ﬂip ﬂop drives a 1-mm-long-wire, which spans the width of the multiplexor matrix and fans out to each crossbar output line, plus one multiplexor at each crossbar output line. We halved this load by duplicating the crossbar inputs –half on the left and half on the right sides of the multiplexor matrix. By approximating the resulting capacitance, we synthesized the driver of ﬁgure 5(e) and by replication, we generated drivers for all crossbar inputs. We placed the drivers as in ﬁgure 4. Except for data, inputs to the bit slice are control signals that conﬁgure the crossbar output lines, i.e. control their multiplexors. We assigned the control signals at the top of the multiplexor matrix, as in ﬁgure 4. We used two bits for each level of each multiplexor tree, for a total of 1024 signals; each bit fans out to each multiplexor at that level. The signals to leaf multiplexors have the largest loading capacitance; they fan out to 32 multiplexors each, while spanning the height of the multiplexor matrix. For these signals, we speciﬁed and placed drivers, similarly to the 4 6 0 m e t a l t r a c k s i n M 3 1 2 8 . 8 0 u m FF FF INV INV 7.56 um 1.96 0.84 1.96 1.96 0.84 24.36 um 7.00 6.16 11.20 INV INV FF 5.60 2.52 7.56 (a) 4:1 mux circuit INV AOI  AOI  OAI INV r f o m a r b t i e r INV INV FF r f o m n x i o p t r B B A B A A B D CK A in[0] in[1] sel[1] in[3] sel[0] in[2] to 32 4:1 muxes to 32 4:1 muxes to 32 4:1 muxes INV INV (d) Flip flop  standard cell OAI INV AOI AOI INV 4:1 mux1 4:1 mux0 4:1 mux42 7.56 um 27 metal tracks in M2 + 27 in M4  4:1 mux2 15.68 um out QB Q (f) Control signal driver (b) 4:1 mux floor plan (c) Xbar out. line floor plan (e) Xbar input driver Figure 5. Elements of the crossbar bit slice crossbar inputs; ﬁgure 5(f) shows their circuit and ﬂoor plan. For the rest of the control signals, we placed just ﬂip ﬂops. We also placed a row of ﬂip ﬂops at the bottom of the multiplexor matrix to register the outputs of the bit slice. Finally, we routed the circuit using the standard EDA routers. We budgeted M2 and M4 for the wiring of the multiplexor trees and their control signals; the crossbar inputs were routed in M3. Thus, the bit slice obstructs M2-M4 from the higher levels of our hierarchy. Simple calculations sufﬁce for one to verify the routability of the layout6 . The whole bit slice takes up a 1114µm×154µm area, while running at up to 750 MHz under worst-case operating conditions of 0.9 Volts and 125◦C. At this clock frequency, the maximum length of a repeated and pipelined link is – nota bene– 6mm at the M5-M8 layers. D. Organization of the Crossbar Tile To give a port throughput of 20Gbps, a crossbar clocked at 750MHz sufﬁces to be 27-bits wide; we made it 32-bits wide for orthodox data alignment, thus also increasing the port capacity to 24Gbps. 6 The layout should have remained routable with a single metal layer in the vertical direction, but due to artifacts of the routing tool –few DRC violations and slower critical paths were reported, we provided both M2 and M4. 92                                   I O Lin ks 1 3 7 2 }1 6 9 } 0 } 0  31 bits reg. ring 31 } 0 1 31 m u 8 m 2 m 9 2 . m u 4 5 1 1 0 1 3 1 0 1 3 slice 2 r e p e a e t r s 30 31 1.1mm 2.3mm 31 0 1 31 0 1 } 95 } 64 }3 2 } 6 3 0 1 3 1 0 1 3 1 1.5  mm  6 mm  1 cycle m 8.5 m 2 cycle s Figure 6. Floor plan of the crossbar tile To compose the crossbar, we placed 32 bit slices in two stacks as in ﬁgure 6. The data ports of the crossbar are uniformly distributed at its periphery, while its control ports are all assigned at the center of the eastern side (not shown in the ﬁgure). Each bit of each data port needs to be connected with the corresponding pin of the homonym bit slice, while each control bit to all bit slices. Thus, the longest route equals the half perimeter of the crossbar, which is 15% shorter than the maximum length of a pipelined and repeated link. Hence, we surrounded the bit-slice stacks with a ring of ﬂip ﬂops to register their IO. We left an empty space of 28µm between the slices for repeater and buffer insertion; we came up with it by empirically specifying a value to get timing closure and then repeatedly lowering it. Thus, the whole crossbar area is 2.3mm×2.9mm. A simple solution that veriﬁes the routability of the layout is the following. Each bit is routed in M7 and M8 to the proximity of its bit slice, and from there, in M5 and M6, to the corresponding pin of the slice. Using an average routing pitch of 500nm, there are above 4500 metal tracks in each dimension of the crossbar, thus each bit can be allocated its own metal track in M7 and M8; routing in M5 and M6 is trivial. The wires are routed on top of the bit slices, which obstruct the M1-M4 layers. Thus, the whole crossbar obstructs all metal layers from M1 to M8. The control signals can be routed similarly and are also buffered to lower their fan-out. E. Organization of the Global Links We emulated a user tile with a small circuit which serves as a trafﬁc source and sink, while obstructing area and metal layers in accordance with the user-tile conﬁguration of ﬁgure 1(c). Nevertheless, the obstructed area is 985.6µm×985.6µm, leaving 14.4µm in each direction for repeaters and ﬂip ﬂops on global links. The global links are routed in M5 and M6, over the user tiles, as in ﬁgure 1(d). The die can be conceivably divided by three homocentric circles as in ﬁgure 7. The inner circle is the circumcircle Figure 7. Division of the die into three conceivable circles: (inner) crossbar tile, (median) user tiles that reach the periphery of the inner circle in at most one clock cycle, (outer) user tiles that reach the periphery of the inner circle in at most two cycles. of the crossbar tile; the median and outer circle contain all user tiles that reach the periphery of the inner circle in at most one and two clock cycles respectively. The corner-to-corner latency is 8 clock cycles, composed by: (i) a two-cycle trip from the corner of the die to the crossbar tile; (ii) a single-cycle trip from the edge of the crossbar tile to the corresponding bit slice; (iii) a singlecycle trip inside the bit slice; and (v) the symmetrical trip from the output of the bit slice to the destination corner. The careful reader should have observed that the control signals are also pipelined; they propagate in a ﬂip-ﬂop tree. In the ﬁrst cycle, a control signal arrives at the crossbar tile from the crossbar control circuit; in the second cycle, the value of the signal is loaded to all 32 slices; and in the third cycle, it is loaded to the multiplexors inside the bit slice. F. Quantitative Data We proﬁled the crossbar circuit with respect to: (i) gate, buffer/repeater, and ﬂip-ﬂop count and standard-cell area – see table I; (ii) power consumption –see table II; and (iii) metal-track utilization –see table III. Area Cost: The area numbers for the bit slice come up directly from the description of its ﬂoor plan in subsection IV-C. We observe that the 75% of the total standard-cell area within a bit slice is allocated to multiplexors, while the remaining is equally shared by IO circuits, i.e. buffers and ﬂip ﬂops; not shown in the table is that the data wires account for 65% of the buffer area and 15% of the ﬂip-ﬂop area, while the control wires account for the rest. The total standard-cell area of the bit slice is 0.16mm2. Thus, given that the bit slice is ﬂoor-planned in a 1.1mm×0.154mm box, its area utilization is above 95%. Moving one level higher in the hierarchy, the crossbar tile utilizes the standard cells of 32 bit slices plus the ﬂip ﬂops, buffers, and repeaters on the wires that connect the ports of the bit slices with the ports of the crossbar tile. The repeaters and buffers take up 0.56mm2 and the ﬂip ﬂops 93         Count (×103 ) Standard-Cell Area (mm 2 ) Logic Gates Buffer/Repeater Standard Cells Flip Flops Logic Gates Buffers/Repeaters Flip Flops Total Bit Slice Bit Slice X 32 Crossbar Tile Whole 63.0 2000 2000 2000 1.0 33 60 92 1.4 45 54 56 0.121 3.872 3.872 3.872 0.020 0.640 1.196 1.855 0.020 0.640 0.824 0.862 0.161 5.152 5.892 6.589 Table I CUMU LAT I V E A R EA CO S T another 0.18mm2; not shown in the table is that the data and the control ports account for 65% and 35% of this overhead area respectively. Hence, the total standard-cell area of the crossbar tile is 5.9 mm2 . Given that the crossbar tile is ﬂoorplanned in a 2.3mm×2.9mm box, its area utilization nears 90%. In the top level of the hierarchy, the actual area cost of the global links is 3% of the tile area, corresponding to the empty space that we leave between the user tiles for insertion of ﬂip ﬂops and repeaters. However, this cost could be lowered to the actual standard-cell area by pushing the repeaters and ﬂip ﬂops inside the user tiles. Hence, we consider that their actual area overhead coincides with their standard-cell area, which is 0.66mm2 for repeaters and 0.04mm2 for ﬂip ﬂops. Thus, the global links add a 10% overhead to the area of the crossbar tile. Overall, the whole crossbar circuit utilizes a standard-cell area of 6.6mm2 and a silicon area of 7.5mm2, or 6% of the total area of the tiles. Furthermore, 60% of the total standard-cell area is occupied by multiplexor gates, while the remaining 40% by repeaters, buffers, and ﬂip ﬂops on multiplexor wires –15% control and 25% data. Power Cost: Table II shows power consumption; we break it down into power consumption due to wires, standard cells, and leakage; leakage was always negligible. For our measurements, we simulated a permutation communication pattern updated circularly in every clock cycle; we also assumed a toggle rate of one and a power supply of 0.9 Volts. First, we observe that power consumption within a bit slice is almost equally due to the capacitance of the interconnect and the standard cells. Second, the wires within the crossbar tile add an overhead of 40% to the power consumption of all slices; this is mostly due to the capacitance of the wires. Likewise, the global links add a further 30% to the power consumption of the crossbar tile. Overall, the whole crossbar power is 9 Watts; 30% is consumed on the multiplexor gates and 70% on the multiplexor wires and their repeaters, buffers, and ﬂip ﬂops. Metal Track Utilization: We deﬁne the metal utilization on a metal layer as track L × R A × 100%, Wires Standard Cells Total Bit Slice Bit Slice X 32 Crossbar Tile Whole 0.073 2.336 3.842 5.462 0.085 2.720 3.100 3.590 0.158 5.056 6.942 9.052 Table II CUMU LAT I V E P OW ER CO S T (WAT T S ) Bit Slice Crossbar Tile Global Links M2 M3 M4 M5 M6 M7 M8 M9 23 60 20 0 0 0 0 0 < 5 < 5 < 5 38 47 46 30 0 < 5 < 5 < 5 < 5 < 5 0 0 0 Table III M E TA L T RACK U T I L I ZAT I ON (% ) where L and R the total wire length and routing pitch on that layer, and A the area of the reference region. Table III shows numbers for each level of the crossbar hierarchy. We observe that the lower two levels utilize approximately half of the available wiring resources, while the top level utilizes a negligible amount. As a consequence, the area of the crossbar is a linear, rather than quadratic [15], function of its width. V. FEA S IB IL ITY O F CONTROL The control of the crossbar datapath mainly comprises a scheduler circuit, which updates the crossbar conﬁguration based on the trafﬁc at the crossbar network interfaces. A thorough study of the queueing stradegy at the network interfaces and the scheduler circuit will appear in a follow-up paper. Here, we give brief area cost estimates of a baseline conﬁguration. We placed and routed a centralized iSLIP scheduler. The implementation is similar to [16], with the difference that, instead of pipelining successive iterations, we pipeline the phases of a single iteration. Our implementation consists of 256 128:1 identical round-robin arbiters, implementing the 128 grant and the 128 accept arbiters of iSLIP. Each arbiter 94 [4] FARADAY Technology Corporation, “UMC’s 90nm Logic SP-RVT Standard Cell Process,” www.faraday-tech.com. [5] S. Scott, D. Abts, J. Kim, and W. J. Dally, “The BlackWidow high-radix Clos network,” SIGARCH Computer Architecture News, vol. 34, no. 2, pp. 16–28, 2006. [6] A. Pullini, F. Angiolini, S. Murali, D. Atienza, G. De Micheli, and L. Benini, “Bringing NoCs to 65 nm,” IEEE Micro, vol. 27, no. 5, pp. 75–85, 2007. [7] Y. Zhang, T. Jeong, F. Chen, H. Wu, R. Nitzsche, and G. R. Gao, “A study of the on-chip interconnection network for the IBM Cyclops64 multi-core architecture,” in Proceedings of the IEEE International Parallel and Distributed Processing Symposium (IPDPS), 2006. [8] R. Kumar, V. Zyuban, and D. M. Tullsen, “Interconnections in multi-core architectures: Understanding mechanisms, overheads and scaling,” SIGARCH Comput. Archit. News, vol. 33, no. 2, pp. 408–419, 2005. [9] W. J. Dally and B. Towles, “Route packets, not wires: Onchip inteconnection networks,” in DAC ’01: Proceedings of the 38th annual Design Automation Conference, 2001. [10] M. B. Taylor et al., “The RAW microprocessor: A computational fabric for software circuits and general-purpose programs,” IEEE Micro, vol. 22, no. 2, pp. 25–35, 2002. [11] Tilera, “TILE-Gx processors http://www.tilera.com/products/TILE-Gx.php, 2009. family,” [12] D. N. Truong et al., “A 167-processor computational platform in 65nm CMOS,” IEEE Journal of Solid-State Circuits (JSSC), vol. 44, no. 4, pp. 1130–1144, 2009. [13] W. Dally and B. Towles, Principles and Practices of Interconnection Networks. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 2003. [14] N. McKeown, M. Izzard, A. Mekkittikul, W. Ellersick, and M. Horowitz, “Tiny-Tera: A packet switch core,” IEEE Micro, vol. 17, no. 1, pp. 26–33, 1997. [15] H. Wang, L.-S. Peh, and S. Malik, “Power-driven design of router microarchitectures in on-chip networks,” in Proceedings of the IEEE/ACM International Symposium on Microarchitecture (MICRO), 2003. [16] P. Gupta and N. McKeown, “Designing and implementing a fast crossbar scheduler,” IEEE Micro, vol. 19, no. 1, pp. 20–28, 1999. is placed in an 154µm×154µm area and routed in M2-M4. The area of each arbiter is dominated by the parallel preﬁx trees needed to implement priority encoding. The arbiters are placed in a 3mm×4mm grid and interconnected with point to point links in M5-M9. The whole scheduler circuit is wire limited, featuring an area utilization as low as 50%. Hence, the whole scheduler is by 30% larger than what could ﬁt in the sub-area of the 4mm×4mm region. Despite that, we conclude that a baseline conﬁguration is very close to ﬁt in our conﬁguration. Wiring, and thus area, could be lowered e.g. by compromising some scheduling performance. V I . CONCLU S ION Though designers generally believe that high-valency crossbars are expensive due to their quadratic area cost, we demonstrated that they are implementable with a minimal area budget even in a conservative 90nm technology. We showed a crossbar implementation interconnecting 128 1mm2 tiles in a single hop, while providing a bidirectional channel capacity of 48Gbps and occupying 6% of the total tile area. Our implementation featured the following points: • The centralized organization allows area-efﬁcient crosspoints and global links; • the custom placement of the crosspoints guides the EDA tools to regular and compact routing solutions; • the plentiful of wiring resources allows all crossbar lines to be routed over the crosspoint standard cells, thus taking up virtually zero area; • the plentiful of wiring resources on top of SRAM blocks allows all global links to be routed over the user tiles, thus connecting the user tiles to the crossbar “for free”. Hence, we also proved that the area of the crossbar scales linearly with its width. Applications of our implementation include CIOQ switch and many-core chips. D EBT We thank professor Christos Sotiriou for useful discussions on EDA ﬂows and algorithms. "
Design of a High-Throughput Distributed Shared-Buffer NoC Router.,"Router microarchitecture plays a central role in the performance of an on-chip network (NoC). Buffers are needed in routers to house incoming flits which cannot be immediately forwarded due to contention. This buffering can be done at the inputs or the outputs of a router, corresponding to an input-buffered router (IBR) or an output-buffered router (OBR). OBRs are attractive because they can sustain higher throughputs and have lower queuing delays under high loads than IBRs. However, a direct implementation of an OBR requires a router speedup equal to the number of ports, making such a design prohibitive under aggressive clocking needs and limited power budgets of most NoC applications. In this paper, we propose a new router design that aims to emulate an OBR practically, based on a distributed shared-buffer (DSB) router architecture. We introduce innovations to address the unique constraints of NoCs, including efficient pipelining and novel flow-control. We also present practical DSB configurations that can reduce the power overhead with negligible degradation in performance. The proposed DSB router achieves up to 19% higher throughput on synthetic traffic and reduces packet latency by 60% on average for SPLASH-2 benchmarks with high contention, compared to a state-of-art pipelined IBR. On average, the saturation throughput of DSB routers is within 10% of the theoretically ideal saturation throughput under the synthetic workloads evaluated.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip Design of a High-Throughput Distributed Shared-Buffer NoC Router Rohit Sunkam Ramanujam∗ , Vassos Soteriou† , Bill Lin∗ and Li-Shiuan Peh‡ ∗Department of Electrical and Computer Engineering, University of California, San Diego †Department of Electrical Engineering and Information Technology, Cyprus University of Technology ‡Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology Abstract—Router microarchitecture plays a central role in the performance of an on-chip network (NoC). Buffers are needed in routers to house incoming ﬂits which cannot be immediately forwarded due to contention. This buffering can be done at the inputs or the outputs of a router, corresponding to an input-buffered router (IBR) or an output-buffered router (OBR). OBRs are attractive because they can sustain higher throughputs and have lower queuing delays under high loads than IBRs. However, a direct implementation of an OBR requires a router speedup equal to the number of ports, making such a design prohibitive under aggressive clocking needs and limited power budgets of most NoC applications. In this paper, we propose a new router design that aims to emulate an OBR practically, based on a distributed shared-buffer (DSB) router architecture. We introduce innovations to address the unique constraints of NoCs, including efﬁcient pipelining and novel ﬂow-control. We also present practical DSB conﬁgurations that can reduce the power overhead with negligible degradation in performance. The proposed DSB router achieves upto 19% higher throughput on synthetic trafﬁc and reduces packet latency by 60% on average for SPLASH-2 benchmarks with high contention, compared to a state-of-art pipelined IBR. On average, the saturation throughput of DSB routers is within 10% of the theoretically ideal saturation throughput under the synthetic workloads evaluated. Keywords-On-chip interconnection networks, Router microarchitecture. I . IN TRODUC T ION Network-on-Chip (NoC) architectures are becoming the de facto fabric for both general-purpose chip multi-processors (CMPs) and application-speciﬁc systems-on-chips (SoCs). In the design of NoCs, high throughput and low latency are both important design parameters and the router microarchitecture plays a vital role in achieving these performance goals. High throughput routers allow an NoC to satisfy the communication needs of multi- and many-core applications, or the higher achievable throughput can be traded off for power savings with fewer resources being used to attain a target bandwidth. Further, achieving high throughput is also critical from a delay perspective for applications with heavy communication workloads because queueing delays grow rapidly as the network approaches saturation. A router’s role lies in efﬁciently multiplexing packets onto the network links. Router buffering is used to house arriving ﬂits1 that cannot be immediately forwarded to the output A short version of this paper appeared in the IEEE Computer Architecture Letters [16]. 1A ﬂit is a ﬁxed-size portion of a packetized message. links due to contention. This buffering can be done either at the inputs or the outputs of a router, corresponding to an input-buffered router (IBR) or an output-buffered router (OBR). OBRs are attractive for NoCs because they can sustain higher throughputs and have lower queueing delays under high loads than IBRs. However, a direct implementation of an OBR requires each router to operate at P times speedup, where P is the number of router ports. This can either be realized with the router being clocked at P times the link clock frequency, or the router having P times more internal buffer and crossbar ports. Both of these approaches are prohibitive given the aggressive design goals of most NoC applications, such as high-performance CMPs. This is a key reason behind the broad adoption of IBR microarchitectures as the preferred design choice and the extensive prior effort in the computer architecture community on aggressively pipelined IBR designs. In this paper, we propose a new router microarchitecture that aims to emulate an OBR without the need for any router speedup. It is based on a distributed shared-buffer (DSB) router architecture that has been successfully used in highperformance Internet packet routers [4], [15]. Rather than buffering data at the output ports, a DSB router uses two crossbar stages with buffering sandwiched in between. These buffers are referred to as middle memories. To emulate the ﬁrst-come, ﬁrst-served (FCFS) order of an output-buffered router, incoming packets are timestamped with the same departure times as they would depart in an OBR. Packets are then assigned to one of the middle memory buffers with two constraints. First, packets that are arriving at the same time must be assigned to different middle memories. Second, an incoming packet cannot be assigned to a middle memory that already holds a packet with the same departure time2 . It has been shown in [4], [15] that for a P port router, N ≥ (2P − 1) middle memories are necessary and sufﬁcient to ensure that memory assignments are always possible. This scheme emulates a FCFS output-buffered router if unlimited buffering is available. However, just as the design objectives and constraints for an on-chip IBR are quite different from those for an Internet packet router, the architecture tradeoffs and design constraints for an on-chip DSB router are also quite different. First, limited power and area budgets restrict practical implementations to small amounts of buffering. This makes it imperative to 2 This is necessary to avoid switch contention. 978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.17 69 explore power and area efﬁcient DSB conﬁgurations suitable for being implemented on a chip. Next, a ﬂow-control protocol which can work with few buffers is necessary since NoC applications such as cache coherence protocols cannot tolerate dropping of packets. A novel ﬂow-control scheme is also needed for ultra-low latency communication in NoCs for supporting a wide range of delay-sensitive applications. Unlike Internet routers that typically use store-and-forwarding of packets, ﬂit-level ﬂow-control is widely used in on-chip routers where bandwidth and storage are allocated at the level of ﬂits. Finally, another key difference is the need for on-chip routers to operate at aggressive clock frequencies. This makes it important to have an efﬁcient router pipeline where delay and complexity are balanced across all pipeline stages. Our proposed router microarchitecture tackles all these challenges with novel designs. The remainder of this paper is organized as follows. Section II provides background information on throughput analysis and on existing router architectures. Section III describes our proposed distributed shared-buffer router microarchitecture for NoCs. Next, Section IV provides extensive throughput and latency evaluations of our proposed DSB architecture using a detailed cycle-accurate simulator on a range of synthetic network traces and trafﬁc traces gathered from real system simulations, while Section V evaluates the power and area overhead of DSB routers. Section VI reviews related work. Finally, Section VII concludes the paper. I I . BACKGROUND We ﬁrst provide a brief background on throughput analysis. We then present a short description of OBR and IBR microarchitectures, focusing on their deﬁciencies in practically attaining ideal throughput, before discussing distributed-sharedbuffer Internet routers and how they mimic output buffering. A. Throughput Analysis In this section, we provide a brief overview of the analysis techniques used to evaluate ideal network throughput. In particular, we elaborate on the concepts of network capacity, channel load, and saturation throughput. These concepts are intended to capture what could be ideally achieved for a routing algorithm R on a given trafﬁc pattern Λ. To decouple the effects of the router microarchitecture, including buffer sizing and the ﬂow-control mechanism being utilized, ideal throughput analysis is based on channel load analysis. We ﬁrst review the concept of network capacity. Network capacity: Network capacity is deﬁned by the maximum channel load γ ∗ that a channel at the bisection of the network needs to sustain under uniformly distributed trafﬁc. As shown in [2], for any k × k mesh, for even k for odd k The network capacity, N , in ﬂits per node per cycle is then deﬁned as the inverse of γ ∗ : = ￿ 4 N = 1 γ ∗ = ￿ k for even k for odd k 4 k2−1 4k γ ∗ k 4k k2−1 TRA FFIC PAT TERN S AND THE IR CORR E S POND ING IDEA L SATURAT ION THROUGH PU T UNDER D IM EN S ION -ORD ERED XY ROU T ING . TABLE I Trafﬁc Uniform Tornado Complement Description Destination chosen at random, uniformly (x, y) to ((x + ￿ k 2 ￿ − 1)%k, (y + ￿ k 2 ￿ − 1)%k) (x, y) to (k − x − 1, k − y − 1) Saturation throughput 1.0 0.66 0.5 For example, for a k × k mesh, with k = 8, N = 4/8 = 0.5 ﬂits/node/cycle. Next, we review the concept of saturation throughput. Saturation throughput: For a routing algorithm R and a given trafﬁc pattern Λ, the expected channel load on a channel c is denoted as γc (R, Λ). The normalized worst-case channel load, γwc (R, Λ), is then deﬁned as the expected number of ﬂits crossing the most heavily loaded channel, normalized to γwc (R, Λ) = maxc∈C γc (R, Λ) where C is the set of all channels in the network. Given this deﬁnition of normalized worst-case channel load, the saturation throughput corresponds to the average number of ﬂits that can be injected by all the nodes in the network per cycle so as to saturate the most heavily loaded channel to its unit capacity. This is given as: γ ∗ . γ ∗ Θ(R, Λ) = 1 γwc (R, Λ) Saturation throughput is deﬁned speciﬁcally for a given routing algorithm R and trafﬁc pattern Λ. Table I shows a few commonly used trafﬁc patterns and their corresponding saturation throughput under dimension-ordered XY routing (DOR-XY). Note that 100% capacity cannot always be achieved with DOR-XY routing even under an ideal router design, deﬁned as the one that can handle injection loads up to the saturation throughput. For example, for an adversarial trafﬁc pattern like bit-complement trafﬁc, it is well-known that DOR-XY routing saturates at 50% of network capacity. To decouple the effects of the chosen routing algorithm on network performance, we assume DOR-XY routing throughout the remainder of this paper. The goal of our router design is to reach the ideal router performance and thus approach the achievable saturation throughput. B. Output-buffered routers Fact 1: An OBR with unlimited buffering can achieve the theoretical saturation throughput. Fact 2: OBRs with unlimited buffering have predictable and bounded packet delays when the network is below saturation. Emulating the ﬁrst-come, ﬁrst-served (FCFS) behavior of OBR architectures is important for exploiting their attractive high-throughput and low-delay properties. Throughput guarantees offered by all oblivious routing algorithms [2], which are often used in NoCs because of their simplicity, assume ideal output-buffered routing with inﬁnite buffers in their throughput analysis. When the network topology and the trafﬁc matrix are 70 both known, the saturation throughput for oblivious routing algorithms can be computed based on worst-case channel load analysis (as described in Section II-A). Even when no information about the spatial characteristics of the trafﬁc is available, which is often the case, worst-case throughput guarantees can be provided for oblivious routing functions by solving bipartite maximum-weight matching problems for each channel [20]. These throughput guarantees do not hold if the routers used do not emulate an OBR. Generally, using IBRs, the worst-case saturation throughput of an oblivious routing algorithm can be quite far off from the value predicted by worst-case throughput analysis. So one key advantage of OBR emulation is to provide and retain such guarantees with the limited hardware resources available in on-chip routers. OBRs also have lower and more predictable queueing delays than IBRs because of their FCFS servicing scheme. Flits are not delayed in OBRs unless the delay is unavoidable due to multiple ﬂits arriving at the same time at different inputs destined for the same output. On the other hand, the switch arbitration schemes used in IBRs for multiplexing packets onto links are sub-optimal and result in unpredictable packet delays. The predictability of packet delays is an important concern for delay-sensitive NoC applications and OBR emulation is a step forward in this direction. Figure 1 depicts the OBR architecture. In this architecture, incoming ﬂits are directly written into the output buffers through a concentrator. Since up to P ﬂits may arrive together for a particular output in the same cycle, a direct implementation of an OBR would require a router speedup of P, where P is the number of router ports (i.e., P = 5 in Figure 1). Router speedup can be realized in two ways. First, by clocking the router at P times the link frequency, which is highly impractical with today’s aggressive clock rates. Even if realizable, this will lead to exorbitant power consumption. Second, it can be realized with higher internal port counts at the buffers and the crossbar: each output buffer needs P write ports, with input ports connected to the output buffers through a P × P2 crossbar. This scenario leads to huge CMOS area penalties. High power and area requirements for OBR implementation are the key reasons behind the broad adoption of IBR microarchitectures as the principal design choice and the extensive prior effort in the computer architecture community on aggressively pipelined IBR designs (see Section VI), despite the very attractive property that an OBR can theoretically reach the ideal saturation throughput. Subsequently, in Section II-D we review a distributed shared-buffer (DSB) router architecture that emulates an OBR, inheriting its elegant theoretical properties, without the need for P times router speedup. We ﬁrst review the IBR microarchitecture that is widely used in on-chip interconnection networks. C. Input-buffered router microarchitecture Figure 2-A sketches a typical input-buffered router (IBR) microarchitecture [2] that is governed by virtual channel ﬂowcontrol [1]. We adopt this as the baseline input-buffered router for our evaluations. The router has P ports, where P depends on the dimension of the topology. In a 2-dimensional mesh, North South East West Concentrator North South Eas t Wes t Injection port Legend: Input port Ejec tion port Output port Output buffer Fig. 1. Output-buffered router (OBR) architecture model. Fig. 2. (A) A typical input-buffered router (IBR) microarchitecture with virtual channel ﬂow-control and 2 virtual channels (VCs) and (B) its 3-stage pipeline (1) route computation (RC) + virtual channel allocation (VA) + switch arbitration (SA), (2) switch traversal (ST) and (3) link traversal (LT). P = 5 as it includes the 4 NSEW ports as well as the injection/ejection port from/to the processor core. At each input port, buffers are organized as separate FIFO queues, one for each virtual channel (VC). Flits entering the router are placed in one of these queues depending on their virtual channel ID. Queues of an input port typically share a single crossbar input port, as shown in Figure 2-A, with crossbar output ports directly connected to output links that interconnect the current (upstream) router with its neighboring (downstream) router. Figure 2-B shows the corresponding IBR router pipeline. The router uses look-ahead routing and speculative switch allocation resulting in a short three-stage router pipeline. Route computation (RC) determines the output port based on the packet destination and is done one hop in advance. Speculative switch allocation (SA) is done in parallel with VC allocation (VA) and priority is given to non-speculative switch requests to ensure that performance is not hurt by speculation. Once SA and VA are completed, ﬂits traverse the crossbar (ST) before ﬁnally traversing the output link (LT) towards the downstream router. Head ﬂits proceed through all the stages while the body and tail ﬂits skip the RC and VA stages and inherit the VC allocated to the head ﬂit. The tail ﬂit releases the reserved VC after departing the upstream router. To attain high throughput, an IBR relies on several key microarchitectural components to effectively multiplex ﬂits onto the output links. First, additional buffering allows a greater number of ﬂits to be housed at a router during high contention. This increases the number of competing ﬂit candidates to eventually traverse a link towards a downstream router, while also alleviating congestion at upstream routers. Second, a higher number of VCs allows a greater number of individual packet ﬂows to be accommodated, increasing buffer 71 Input  Port 1  Input  Port 2  Input  Port P  P x N  XB1  Middle Memory 1  N x P  XB2  Middle Memory 2  .  .  .  Middle Memory N  Output  Port 1  Output  Port 2  Output  Port P  Legend:  Input port  Output port  Crossbar  Middle memory  Fig. 3. Distributed shared-buffer router architecture model. utilization and ultimately link utilization. Finally, the VC and switch allocators need to have good matching capability, i.e. they should ensure that VCs and crossbar switch ports never go idle when there are ﬂits waiting for them. However, with matching algorithms that can be practically implemented in hardware, the throughput of IBRs can be quite far off from the ideal saturation throughput. D. Distributed shared-buffer routers DSB routers have been successfully used in highperformance Internet packet routers [4], [15] to emulate OBRs without any internal router speedup. Rather than buffering data directly at the output ports, a DSB router uses two crossbar stages with buffering sandwiched in between. Figure 3 depicts the DSB microarchitecture used in Internet routers. The input ports are connected via a P × N crossbar to N middle memories. These N middle memories are then connected to the output ports through a second N × P crossbar. Every cycle, one packet can be read from and written to each middle memory. It should be noted here that when contention occurs in Internet routers, packets can be dropped. This is not allowed in our proposed on-chip DSB architecture (see Section III). To emulate the ﬁrst-come, ﬁrst-served (FCFS) packet servicing in OBRs, a DSB router has to satisfy two conditions: (a) a packet is dropped by the DSB router if and only if it will also be dropped by an OBR, and (b) if a packet is not dropped, then the packet must depart the DSB router at the same cycle as the cycle in which it would have departed an OBR. To achieve this emulation, each packet arriving at a DSB router is timestamped with the cycle in which it would have departed from an OBR (i.e., in FCFS order). When a packet arrives, a scheduler is needed to choose a middle memory to which to write this incoming packet and to conﬁgure the corresponding ﬁrst crossbar. Also, at each cycle, packets whose timestamp is equal to the current cycle are read from the middle memories and transferred to the outputs through the second crossbar. In [4], [15], it was shown that a middle memory assignment can always be found and a DSB router can exactly emulate an OBR if the following condition is satisﬁed: Fact 3: A distributed shared-buffer router with N ≥ (2P − 1) middle memories and unlimited buffering in each can exactly emulate a FCFS OBR with unlimited buffering. At least (2P − 1) middle memories are needed to ensure that two types of conﬂicts can always be resolved: The ﬁrst type of conﬂict is an arrival conﬂict. A packet has an arrival conﬂict with all packets that arrive simultaneously at other input ports since packets arriving at the same time cannot be written to the same middle memory. With P input ports, the maximum number of arrival conﬂicts a packet can have is (P − 1). The second type of conﬂict is a departure conﬂict. A packet has a departure conﬂict with all other packets that have the same timestamp and need to depart simultaneously through different output ports. With P output ports, a packet can have departure conﬂicts with at most (P − 1) other packets. Therefore, by the pigeonhole principle, N ≥ (2P − 1) middle memories are necessary and sufﬁcient to ﬁnd a conﬂict-free middle memory assignment for all incoming packets. I I I . PRO PO SED D I STR IBU T ED SHARED BU FFER (DSB ) ROU TER FOR NOC S A. Key architectural contributions The proposed DSB NoC router architecture addresses the bottlenecks that exist in the data path of IBRs which lead to lower than theoretical ideal throughput. At the same time, it tackles the inherent speedup limitations and area penalties of OBRs while harnessing their increased throughput capabilities. The middle memories decouple input virtual channel queueing from output channel bandwidth, as any ﬂit can acquire any middle memory provided that there are no timing conﬂicts with other ﬂits already stored in the same middle memory. Essentially, middle memories provide path diversity between the input and output ports within a router. Although based on the DSB architecture used in Internet routers, the proposed NoC router architecture faces a number of challenges speciﬁc to the on-chip domain. First and foremost, NoC applications such as cache coherence protocols cannot tolerate dropping of packets unlike Internet protocols. As a result, the DSB architecture used in Internet routers cannot be directly applied to the onchip domain. To guarantee packet delivery, a ﬂow control mechanism needs to be in place. The proposed DSB router uses credit-based ﬂit-level ﬂow control. To implement creditbased ﬂow control, we introduce input buffers with virtual channels and distribute the available router buffers between the input ports and the middle memories. Flow-control is applied on a ﬂit-by-ﬂit basis, advancing each ﬂit from an input queue towards any time-compatible middle memory and ultimately to the output link. Flits are timestamped and placed into a middle memory only when the next-hop router has buffers available at its corresponding input port. Further, since the middle memory buffering is limited due to power and area constraints, ﬂits are held back in the input buffers when they fail to ﬁnd a compatible middle memory. Next, since power is of utmost importance in the NoC domain, the power-performance tradeoff of different DSB conﬁgurations need to be explored. Although, theoretically, 2P − 1 middle memories are needed in a P-port router to avoid all possible arrival and departure conﬂicts, having a large number of middle memories increases power and area overheads by increasing the crossbar size. Therefore, we evaluate DSB conﬁgurations with fewer than 2P − 1 middle memories and estimate its impact on performance. Finally, on-chip routers need to operate at aggressive clock frequencies, pointing to the need for careful design of router pipelines with low complexity logic at each stage. Our design assumes a delay and complexity-balanced 5-stage pipeline. 72 time through port p, the timestamper ﬁrst computes the time the ﬂit would leave the middle memory assuming there are no ﬂits already stored in the middle memories that need to depart through port p. Let us denote this time as T [ p], T [ p] = Current Time + 3 since two pipeline stages, namely, CR+VA (Conﬂict resolution + VC allocation) and XB1+MM WR (Crossbar 1 and Middle Memory Write) lie between the TS and MM RD + XB2 (Middle Memory Read and Crossbar 2) stages in the DSB pipeline (see Figure 4). Next, we consider the case when there are ﬂits in the middle memories destined for output port p. To handle this case, the timestamper remembers the value of the last timestamp assigned for each output port till the previous cycle. The last assigned timestamp for output port p is denoted as LAT [ p]. As timestamps are assigned in a strictly increasing order, the assigned timestamp for output port p in the current cycle must be greater than LAT [ p]. In other words, a ﬂit that is currently being timestamped can depart the middle memory through output port p only after all ﬂits that are destined for the same output port and were timestamped at an earlier cycle, depart the middle memory. This emulates the FCFS servicing scheme of OBRs. Hence, the earliest timestamp that can be assigned to a ﬂit for output port p is given as: Timestamp = max(LAT [ p] + 1, T [ p]) (1) If ﬂits at more than one input port request a timestamp for output port p in the same cycle, the timestamper serves the inputs in an order of decreasing priority (either ﬁxed or rotating). The timestamp of a ﬂit at the highest priority input is computed as above and the remaining ﬂits at other inputs are assigned sequentially increasing timestamps in the order of decreasing priority. Conﬂict resolution (CR) and virtual channel allocation (VA) comprise the second pipeline stage of the DSB router. The CR and VA operations are carried out in parallel. The task of the CR stage is to ﬁnd a conﬂict-free middle memory assignment for ﬂits that were assigned timestamps in the TS stage. As mentioned earlier, there are two kinds of conﬂicts in shared-buffer routers – arrival conﬂicts and departure conﬂicts. Arrival conﬂicts are handled by assigning a different middle memory to every input port with timestamped ﬂits. Departure conﬂicts are avoided by ensuring that the ﬂits stored in the same middle memory have unique timestamps. These restrictions are enforced due to the fact that middle memories are uni-ported3 and only one ﬂit can be written into (via XB1) and read from (via XB2) a middle memory in a given cycle. The implementation details of the TS and CR stages are explained in the next section. The virtual channel allocator arbitrates for free virtual channels at the input port of the next-hop router in parallel with conﬂict resolution. VC allocation is done only for the head ﬂit of a packet. The VC allocator maintains two lists of VCs – a reserved VC pool and a free VC pool. VC allocation is done by picking the next free output VC from the free VC list of the given output port, similar to the technique used in [7]. Additionally, when output VCs are freed, their VC number 3 Single-ported memories are power and area-efﬁcient. Fig. 4. Distributed shared-buffer (A) router microarchitecture with N middle memories and (B) its 5-stage pipeline: (1) Route computation (RC) + timestamping (TS), (2) conﬂict resolution (CR) and virtual channel allocation (VA), (3) ﬁrst crossbar traversal (XB1) + middle memory write (MM WR), (4) middle memory read (MM RD) and second crossbar traversal (XB2), and (5) link traversal (LT). The proposed DSB architecture can achieve much higher performance than virtual-channel IBRs with comparable buffering while adding reasonable power and area overheads in managing middle memories and assigning timestamps to ﬂits. B. DSB microarchitecture and pipeline Figure 4 shows the router microarchitecture of the proposed DSB router and its corresponding 5-stage pipeline. Incoming ﬂits are ﬁrst buffered in the input buffers which are segmented into several atomic virtual channels (VCs). The route computation stage (RC) employs a look-ahead routing scheme like the baseline IBR architecture, where the output port of a packet is computed based on the destination coordinates, one hop in advance. This is done only for the head ﬂit of a packet. The remaining pipeline stages of a DSB router are substantially different from those of IBRs. Instead of arbitrating for free virtual channels (buffering) and passage through the crossbar switch (link), ﬂits in a DSB router compete for two resources: middle memory buffers (buffering) and a unique time at which to depart from the middle memory to the output port (link). The timestamping stage (TS) deals with the timestamp resource allocation. A timestamp refers to the future cycle in which a ﬂit will be read from a middle memory, through the second crossbar, XB2, onto the output port. Timestamping is carried out in conjunction with lookahead routing. A ﬂit (head, body or tail) enters the TS stage and issues a request to the timestamper if it is at the head of a VC or if the ﬂit ahead of it in the same VC has moved to the second stage of the pipeline. A ﬂit can also re-enter the TS stage if it fails to ﬁnd either a conﬂict-free middle memory or a free VC at the input of the next-hop router, as will be explained later. If multiple VCs from the same input port send simultaneous requests to the timestamper, it picks a winning VC and assigns the earliest possible departure time for the output port requested by the ﬂit in the selected VC. Let us assume that the output port requested is p. In order to ﬁnd the earliest possible departure 73 ReqVCi0 ReqVCi1  ReqVCi2  ReqVCi3  ReqVCi4  OPi0  OPi1  OPi2  OPi3  OPi4  Arbiter for selecting a winning VC   at each input port   Input  priorities  Relative offset  VCwini0 VCwini1  VCwini2 VCwini3  VCwini4  offesti0 offseti1 offseti2 offseti3 offseti4  (b) Offset generation = = = = + + + >B  0 1 >B  0 1 (LAT[OPi]+1)%B - B  (curr_time+3)%B - B  Input Priorities   PTS0    offseti  Input Priorities     (curr_time+3)%B  1 0  TSi  (a) Arbitration offseti0  LAT[OPi0]  (cur_time)%B  Compute  timestamp  input 0  offseti1  LAT[OPi1]  (cur_time)%B  Compute  timestamp   input 1  … offseti4  LAT[OPi4]  (cur_time)%B  Compute  timestamp  input 4  TSi0  TSi1  TSi4  TSi0 TSi1  TSi2 TSi3  TSi4  TSi0 TSi1  TSi2 TSi3  TSi4  TSi0 TSi1  TSi2 TSi3  TSi4  PTS1    offseti  + … Input Priorities      (LAT[OPi]+1)%B     (curr_time+2)%B     (LAT[OPi]+1)%B     (curr_time+1)%B  PTS4  (c) Timestamp computation (d) Compute timestamp block Fig. 5. Block Diagrams of the TS stage Middle Memory Compatibility Bitmap  Middle Memory Grant Bitmap  PTS0  PTS1  Read MMRT[PTS0]  C[0][0]  C[0][1]  C[0][N-1]  Read MMRT[PTS1]  C[1][0]  C[1][1]  C[1][N-1]  .  .  .  .  .  .  Priority-  based  middle  memory  assignment  G[0][0]  G[0][1]  G[0][N-1]  G[1][0]  G[1][1]  G[1][N-1]  .  .  .  PTS4  Read MMRT[PTS4]  C[4][0]  C[4][1]  C[4][N-1]  G[4][0]  G[4][1]  G[4][N-1]  Fig. 6. Block Diagram of the CR stage is moved from the reserved VC list to the end of the free VC list. If a free VC exists and the ﬂit is granted a middle memory, it subsequently proceeds to the third pipeline stage, where it traverses the ﬁrst crossbar (XB1) and is written to its assigned middle memory (MEM WR). If no free VC exists (all VCs belong to the reserved VC list), or if CR fails to ﬁnd a conﬂict-free middle memory, the ﬂit has to be re-assigned a new timestamp and it therefore re-enters the TS stage. When the timestamp of a ﬂit matches the current router time, the ﬂit is read from the middle memory (MM RD) and passes through the second crossbar (XB2) in the fourth pipeline stage. We assume that the output port information is added to every ﬂit and stored along with it in the middle memory. Finally, in the link traversal (LT) stage, ﬂits traverse the output links to reach the downstream router. C. Practical implementation In this section, we describe the implementation details of the timestamping and conﬂict resolution stages, which are unique to the proposed DSB architecture. It must be noted here that the proposed implementation is only one among a range of possible design implementation choices that span a spectrum of area/delay tradeoffs. We speciﬁcally focus on the implementation of a 5-ported 2D mesh router. However, our design can be extended to higher or lower radix routers. The high level block diagrams of the logic used in the TS stage are shown in Figures 5(a)- 5(c). The ﬁve input ports are labeled from i0 to i4 . First, as shown in Figure 5(a), when multiple VCs from the same input port send simultaneous requests to the timestamper, a winning VC is selected using a matrix arbiter. In addition to the timestamping requests, 74 the arbiter also takes as input the size of the f ree VC lists for the output ports requested by each of the ﬂits in the TS stage and gives priority to body and tail ﬂits that have already acquired a VC over head ﬂits, if they are likely to fail in the VC allocation stage. This avoids wasted cycles resulting from re-timestamping of ﬂits. After choosing a winning VC at each input port, the output ports requested by the ﬂits in the selected VCs, OPi , are used to generate timestamp offsets for each of these ﬂits, as shown in Figure 5(b). Timestamp offsets are required to assign sequentially increasing timestamps to ﬂits based on input priority when ﬂits from more than one input port simultaneously request a timestamp for the same output port, as discussed in Section III-B. Internally, the offset generator block is comprised of separate sub-blocks, one for each output port. The offsets are generated on a per output port basis and its value for a ﬂit is equal to the number of higher priority ﬂits requesting a timestamp for the same output port. The ﬁnal timestamp assigned to a ﬂit at input i is the sum of the timestamp assigned to the highest priority ﬂit requesting the same output port as the current ﬂit (given by Equation 1) and the computed offset. T Si = max(LAT [OPi ] + 1, current time + 3) + offseti (2) In the DSB architecture, ﬂits are stored in middle memories only after they have reserved buffering at the next-hop router. Let the total buffering at each input port be B ﬂits. If the current time is denoted by curr t ime, we restrict the maximum timestamp assigned for an output port to curr t ime + B − 1. This is because, assigning a timestamp equal to curr t ime + B means that there are B ﬂits before the current ﬂit (with timestamps curr t ime to curr t ime + B − 1) that have been timestamped for the same output port and have not yet departed the router. If all timestamped ﬂits succeed in acquiring an output VC and a conﬂict-free middle memory, these B ﬂits would reserve all available buffering at the input of the next-hop router and any ﬂit with a timestamp greater than curr t ime + B − 1 would fail to reserve an output VC. Hence, assigning timestamps greater than curr t ime + B − 1 is not necessary. This fact is used to simplify the hardware for detecting departure conﬂicts. From this discussion, at most B unique timestamps are assigned for an output port, which can be represented using ￿log2 B￿ bits. We ensure that each middle memory has exactly B ﬂits of buffering so that a ﬂit with timestamp T is always stored at the T t h location within the middle memory. In this way, a ﬂit with timestamp T can only have departure conﬂicts with ﬂits stored at the T t h location of any one of the N middle memories. With timestamps represented using ￿log2 B￿ bits, the timestamp assignment has to be carried out using mod ul o-B arithmetic. Under this scheme, the current time rolls over every B clock cycles, implemented using a mod -B counter. The timestamps are interpreted to be relative to the current time and also roll over beyond B. Hence, if curr t ime%B has a value t , ﬂits stored in the middle memories can have B unique timestamps between t and (t − 1)%B, representing times from curr t ime to curr t ime + B − 1. If the last assigned timestamp for an output port, OP, falls behind curr t ime%B (i.e. LAT [OP] = curr t ime%B), it is advanced along with         the current time to ensure that the last assigned timestamp is either equal to or ahead of curr t ime%B. This prevents old values of LAT [OP] from appearing as future timestamps after rollover. Figure 5(d) presents the logic diagram for the timestamp computation block. When assigning a timestamp for output port OPi , (LAT [OPi ] + 1)%B is simultaneously compared to (curr t ime + 1)%B and (curr t ime + 2)%B, and the results are ORed together. A 1 at the output of the OR gate signiﬁes that (LAT [OPi ] + 1)%B is behind (curr t ime + 3)%B and vice-versa. The greater of the two times is chosen and the corresponding ﬂit offset is added to obtain the ﬁnal timestamp according to Equation 2. If the timestamp computed using Equation 2 is greater than B, it is rolled over by subtracting B from the result, as shown. In the last block of Figure 5(c), the timestamps are shufﬂed according to input priority, which is assumed to be a rotating priority over all inputs. In this respect, PT S0 is the timestamp of the input with priority 0, PT S1 is the timestamp of input with priority 1, and so on. This helps with the priority-based middle memory assignment during the CR stage. If an input does not hold a ﬂit that needs to be timestamped, an invalid timestamp value is stored instead. The task of the conﬂict resolution stage (CR) is to detect arrival and departure conﬂicts. To keep track of the occupancy of the middle memory buffers, we use an auxiliary data structure called the middle memory reservation table (MMRT). For N middle memories, with B ﬂits of buffering per middle memory, the MMRT is an array of B registers, each N bits wide. The registers are indexed from 0 to B − 1. If bit MMRT[i][j] is set, it implies that memory bank j holds a ﬂit with timestamp i and vice versa. Departure conﬂicts are resolved using the middle memory reservation table. For each timestamp that needs to be assigned a middle memory (PT S0 ... PT S4 ), the MMRT register indexed by the timestamp represents the middle memory compatibility bitmap for the timestamp. In Figure 6, the bits C[i][0] to C[i][N − 1] represent the individual bits of the N-bit register, MMRT [PT Si ]. If bit C[i][ j] is 1, it means that middle memory j already has a ﬂit with timestamp PT Si and hence, has a departure conﬂict with any ﬂit with this timestamp. On the other hand, if C[i][ j] is 0, the ﬂit with timestamp PT Si is compatible with middle memory j. If an input does not have a ﬂit that needs to be timestamped, the compatibility bits for all middle memories are set to 1 (meaning incompatible). Next, arrival conﬂicts are resolved in the middle memory assignment stage. The middle memories are assigned ﬁxed priorities with memory N-1 given the highest priority and memory 0 the lowest priority. In the middle memory assignment stage, the inputs are granted the highest priority compatible middle memory in the order of decreasing input priority while ensuring that more than one input is not granted the same middle memory. Bit G[i][j] denotes the grant bit and it is set to 1 if the input with priority i has been granted middle memory j. This memory assignment scheme was speciﬁcally designed to have low middle memory miss rates when the number of middle memories is fewer than 2P − 1 (P being the number of ports) for 5-ported mesh routers. Having less than 2P − 1 middle memories is necessary to reduce the power and area of DSB routers as shown in Section V. When the number of middle memories is at least 2P − 1, memory assignment schemes with less delay can be implemented as it is much easier to ﬁnd conﬂict-free middle memories. The above logic distribution between the TS and CR stages was architected to even out the Fan-Out-of-4 (FO4) delays across the four stages (excluding LT) of the DSB pipeline. The FO4 calculations were carried out using the method of Logical Effort [19], and was applied to each logic block. For a 5-ported DSB router with 5 VCs per input port, 4 ﬂits per VC (B=20 ﬂits) and 5 middle memories with 20 ﬂits per middle memory, the critical path delays of the TS and CR pipeline stages were estimated at 19 FO4s and 18 FO4s, respectively. A delay of less than 20 FO4 for each stage in the proposed architecture enables an aggressively-clocked high-performance implementation. In particular, assuming a FO4 delay of 15 ps for Intel’s 65 nm process technology, our proposed design can be clocked at a frequency of more than 3GHz. IV. THROUGH PU T AND LAT ENCY EVA LUAT ION A. Simulation setup To evaluate the effectiveness of our proposed DSB router against a baseline input-buffered router (IBR) architecture with virtual channel (VC) ﬂow-control, we implemented two corresponding cycle-accurate ﬂit-level simulators. The baseline IBR simulator has a three-stage pipeline as described in Section II-C. The DSB simulator models the ﬁve-stage router pipeline described in Section III-B. Both simulators support k-ary 2-mesh topologies with their corresponding 5ported routers. DOR-XY routing is used for all our simulations where packets are ﬁrst routed in the X-dimension followed by the Y-dimension. We use DOR-XY because our main focus is on highlighting the improvement in performance due to the DSB router architecture, rather than the routing algorithm. We present results for both synthetic and real trafﬁc traces. The three synthetic trafﬁc traces used are uniform, complement and tornado trafﬁc, shown in Table I. These three traces represent a mixture of benign and adversarial trafﬁc patterns. The ideal saturation throughputs that can be achieved for these three trafﬁc patterns using DOR-XY (based on channel load analysis) are also shown in Table I. All throughput results presented subsequently are normalized to the ideal saturation throughput for the given trafﬁc pattern. An 8 × 8 mesh topology is used for our simulations with synthetic trafﬁc. Multi-ﬂit packets composed of four 32-bit ﬂits are injected into the network and the performance metric considered is the average packet latency under different trafﬁc loads (packet injection rates). The latency of a packet is measured as the difference between the time the header ﬂit is injected into the network and the time the tail ﬂit is ejected at the destination router. The simulations are carried out for a duration of 1 million cycles and a warm-up period of ten thousand cycles is used to stabilize average queue lengths before performance metrics are monitored. In addition to the synthetic traces, we also compare the performance of the two router architectures on eight trafﬁc traces from the SPLASH-2 benchmark suite [17]. The traces 75 NOM ENC LATUR E O F TH E ROU T ER M ICROARCH I TEC TUR E S COM PAR ED TABLE II Conﬁg. IBR200 DSB200 DSB300 #VCs #Flits/VC Input buffers (per port) Middle memory buffers #Middle Memories (MM) 5 10 20 20 8 5 5 5 4 4 Total Flits/MM buffers 200 200 300 used are for a 49-node shared memory CMP [8] arranged as a 7 × 7 mesh. The SPLASH-2 traces were gathered by running the corresponding benchmarks with 49 threads4 on Bochs [9], a multiprocessor simulator with an embedded Linux 2.4 kernel. The memory trace was captured and fed to a memory system simulator that models the classic MSI (Modiﬁed, Shared, Invalid) directory-based cache coherence protocol, with the home directory nodes statically assigned based on the least signiﬁcant bits of the tag, distributed across all processors in the chip. Each processor node has a two-level cache (2MB L2 cache per node) that interfaces with a network router and 4GB off-chip main memory. Access latency to the L2 cache is derived from CACTI to be six cycles, whereas off-chip main memory access delay is assumed to be 200 cycles. Simulations with SPLASH-2 traces are run for the entire duration of the trace, typically in the range of tens of millions of cycles, which is different for each trace. B. Performance of the DSB router on synthetic traces The nomenclature of IBR and DSB conﬁgurations referred to in this section is presented in Table II, along with details of the buffer distribution in each conﬁguration. For the DSB architecture, the number of ﬂits per middle memory is always equal to the number of input buffers to simplify departure conﬂict resolution, as discussed in Section III-C. Theoretically, a 5-ported DSB router needs 9 middle memories to avoid all conﬂicts in the worst case, i.e., when all possible arrival and departure conﬂicts occur simultaneously. However, keeping the power overhead in mind, we evaluate a DSB conﬁguration with only 5 middle memories (DSB200) and compare its performance to a conﬁguration with 10 middle memories (DSB300). In addition to the conﬁgurations shown in Table II, we also implemented an OBR simulator with a very large number of buffers (10,000 ﬂit buffers) at each output port, emulating inﬁnite output buffer capacities. Redundant pipeline stages are introduced in the OBR simulator totaling a pipeline depth of ﬁve, to ensure the same pipeline length as our DSB router for fair comparisons. This helped us to compare the performance of DSB conﬁgurations with an OBR with same number of pipeline stages, but which behaves ideally and incurs no delay due to switch arbitration and buffer capacity limitations. We refer to this OBR conﬁguration as OBR-5stage. We ﬁrst compare the performance of IBR200, DSB200, DSB300 and OBR-5stage. IBR200 and DSB200 have the same number of buffers (200 ﬂits). DSB300 has the same number of input buffers but double the number of middle 4 Two of the eight traces, fft and radix could not be run with 49 threads. They were run with 64 threads instead. The memory traces of the ﬁrst 49 threads were captured and the addresses were mapped onto a 49 node system. ) l s e c y c ( y c n e t a l e g a r e v A 250 200 150 100 50 0 0 IBR200 DSB200 DSB300 OBR−5stage 0.2 0.4 Normalized throughput 0.6 0.8 ) l s e c y c ( y c n e t a l e g a r e v A 1 250 200 150 100 50 0 0 IBR200 DSB200 DSB300 OBR−5stage 0.2 Normalized throughput 0.6 0.8 0.4 ) l s e c y c ( y c n e t a l e g a r e v A 250 200 150 100 50 0 0 1 IBR200 DSB200 DSB300 OBR−5stage 0.2 0.4 Normalized throughput 0.6 0.8 1 (a) Uniform (b) Complement (c) Tornado Fig. 7. Performance comparison of different router architectures. NORMAL I ZED SATURAT ION THROUGH PU T COM PAR I SON FOR D I FFER EN T NOC ROU T ER M ICROARCH I T EC TURA L CON FIGURAT ION S TABLE III Trafﬁc pattern Uniform Complement Tornado IBR200 80% 85% 75% DSB200 89% 93% 89% DSB300 89% 94% 89% OBR-5stage 98% 97% 97% memories compared to DSB200. The average packet latency curves for different injected loads are shown in Figure 7 for the three synthetic trafﬁc patterns and the saturation throughput values are presented in Table III. The saturation throughput is assumed to be the injection rate at which the average packet latency is three times the zero-load latency. Table III shows that with an aggregate buffering of 200 ﬂits, DSB200 outperforms IBR200 by 11.25%, 9.5% and 18.5% on uniform, complement and tornado trafﬁc, respectively, in terms of saturation throughput. Although IBR200 has a slightly lower latency than DSB200 under low loads due to the shorter router pipeline, the higher saturation throughput of DSB200 gives it a deﬁnite edge under moderate to high loads. It must be noted here that during the course of an application running on a CMP, there may be transient periods of high trafﬁc or localized trafﬁc hotspots during which parts of the network are driven close to (or past) saturation. A router with higher saturation throughput can minimize the occurrence of these transient hotspots and provide tremendous latency savings during these periods, which can far outweigh the slightly higher latency under low loads. This is more clearly depicted in the SPLASH2 results presented in Section IV-C. The performance of DSB200 is nearly identical to DSB300, with negligible difference in saturation throughputs for all three trafﬁc patterns. This is because the probability of more than ﬁve arrival and departure conﬂicts occurring simultaneously is very low. We observe in our experiments that even under very high injection loads, less than 0.3% of the ﬂits failed to ﬁnd a conﬂict-free middle memory in the worst case over all trafﬁc patterns. Hence, it can be concluded that although theoretically nine middle memories are needed to resolve all conﬂicts, in practice, ﬁve middle memories result in very little degradation in throughput. The reason fewer middle memories are attractive is because of the signiﬁcant power and area savings that can be achieved by using smaller crossbars and fewer buffers. The saturation throughput of DSB200 is also quite close to that of OBR-5stage. For uniform, tornado and complement trafﬁc, the saturation throughput of the DSB200 architecture is within 9%, 4% and 8%, respectively, of the throughput of OBR-5stage. The slightly lower saturation throughput of DSB routers is a result of having far fewer buffers compared to OBR-5stage’s inﬁnite buffering. 76                         which has no extra delay introduced as a result of switch contention, has higher average packet latency than IBR200. For the SPLASH-2 benchmarks with high output port contention (fft, water-nsquared, water-spatial and radix), on an average, DSB200 has 60% lower latency than IBR200. Hence, for applications that demand high throughput and drive the network towards saturation or close to saturation, DSB routers are clearly superior to IBRs. Lastly, there is negligible difference in performance between DSB200 with 5 middle memories and DSB300 with 10 middle memories. This further proves that even with real application traces, more than ﬁve simultaneous arrival and departure conﬂicts occur very rarely. Hence, 5 middle memories are sufﬁcient to achieve comparable performance to a 10 middle memory conﬁguration, but at a signiﬁcantly lower power and area overhead, as will be discussed in the next section. V. POW ER AND AR EA EVA LUAT ION Table IV compares the power and area of IBR and DSB router microarchitectures with the same aggregate buffering. We use the power and area models in Orion 2.0 [5], [21] for our analysis. The models use parameters from TSMC 65nm process libraries and include both dynamic and leakage power components. The operational frequency used is 3GHz at 1.2V. A typical ﬂit arrival probability of 0.3 is assumed at each input port. SRAMs are used as input and middle memory buffers and low threshold voltage transistors are used to ensure low delays for 3GHz operation. The VC allocator is conﬁgured to simply select a free VC from a free VC list as described in Section III-B in both DSB and IBR architectures while a matrix arbiter is used for switch arbitration in IBRs. The power and area of the arbitration logic of the DSB router were extrapolated from the power and area numbers of the arbitration logic of an IBR using the number of 2-input gates required to implement the logic in the two architectures. As shown in Table IV, DSB200 consumes 35% more power and occupies 58% more area than the corresponding IBR200 router. The higher power and area of the DSB router is due to the presence of an extra crossbar and a more complex arbitration scheme (involving timestamping and conﬂict resolution) compared to the switch arbiter in an IBR. Although the increased power cost per router for a DSB router is substantial when compared to an IBR, the overall power increase for a NoC application is often less. In Table IV, the power penalty per tile (processor + router) of using a DSB router is presented for three different scenarios where the NoC consumes 10%, 15% and 20% of the total tile power. Even for applications where the router consumes as high as 20% of the tile power, the power per tile with a DSB200 router is only 7% higher than the tile power with IBR200. On the other hand, if the router consumes only 10% of the tile power, the power per tile with a DSB200 router is just 3.5% higher than tile power with IBR200. We believe that the increased power cost is justiﬁed for applications that demand high bandwidth and exhibit high contention since latency reductions of more than 60% can be achieved using DSB routers. As with power, even in the case of area, the increase in area of the entire tile as a result of using a DSB router in place of an IBR is again 77 Fig. 8. Network latency for SPLASH-2 benchmarks. C. Performance of DSB on real trafﬁc traces In this section, we present simulation results using the eight benchmark traces from the SPLASH-2 suite [17]. For uniform, tornado, and complement trafﬁc, the trafﬁc matrix Λ is assumed to be ﬁxed and stationary. As discussed in Section II-A, using idealized load analysis, the ideal saturation throughput can be computed based on just Λ in these cases. However, in the case of real trafﬁc traces, like the SPLASH-2 traces, the trafﬁc pattern is space- and time-varying. Therefore, the notion of ideal saturation throughput can neither be clearly deﬁned nor easily determined. Instead, we compare the average packet latencies over the entire trace duration using our cycle-accurate ﬂit-level simulators. Figure 8 shows the latency results for IBR200, DSB200, DSB300 and OBR-5stage conﬁgurations normalized to the average packet latency of IBR200. The ﬁrst observation to be made here is that the average packet latency of the DSB conﬁgurations is comparable to OBR-5stage for seven of the eight SPLASH-2 benchmarks evaluated. For the water-nsquared trace, OBR-5stage has a lower latency than DSB200 (and DSB300) because this trace has trafﬁc hot spots where packets are injected at a rate where both OBR and DSB routers get saturated. In such a situation, the large number of output buffers available in OBR-5stage help in attaining a lower average latency. In most traces, the ability of a DSB router to closely match an OBR in terms of latency illustrates its capability to emulate OBRs, even with limited buffering. Next, we observe that DSB200 outperforms IBR200 on fft, water-nsquared, water-spatial and radix traces by 72%, 64.5%, 97.5% and 8%, respectively. For the three traces where the performance improvement is over 50%, i.e., fft, water-nsquared and water-spatial, IBR200 gets saturated during portions of all three traces while DSB200 saturates only in the case of the water-nsquared trace. So, it can be inferred from these results that a relatively small increase in saturation throughput translates into tremendous reductions in packet latency for applications that demand high bandwidth. However, IBR200 has 32%, 18% and 24% lower latency for raytrace, barnes and ocean benchmarks. This is because these traces have negligible output port contention and the higher delay of DSB routers is due to their longer pipeline depth of 5 as compared to 3 for IBR200. This is proved by the fact that even OBR-5stage, TABLE IV ROU TER POW ER AND AREA COM PAR I SON Router Conﬁg. Power (mW) Area (mm2 ) Router Conﬁg. Power (mW) Area (mm2 ) IBR200 240 0.19 DSB200 324 0.3 Per router 1.35 Power penalty Per tile NOC power = 10% NOC power = 15% NOC power = 20% 1.035 1.052 1.07 COM PAR I SON O F DSB ROU T ER S W I TH 5 AND 10 M IDD LE M EMOR I E S TABLE V Router Conﬁguration DSB200 DSB300 Power (mW) 324 496 Area (mm2 ) 0.3 0.5 very low since the router area is only a small portion of the total tile area. As discussed earlier, the DSB200 conﬁguration with 5 middle memories and DSB300 conﬁguration with 10 middle memories have similar performance. However, DSB200 consumes 35% less power and occupies 40% less area compared to DSB300 (Table V). The power and area savings are achieved by using fewer buffers and two 5×5 crossbars in DSB200 instead of 5 × 10 and 10 × 5 crossbars used in DSB300. V I . R ELAT ED WORK Sophisticated extensions to input-buffered router microarchitectures have been proposed for improving throughput, latency and power. For throughput, techniques like ﬂitreservation ﬂow-control [13], variable allocation of virtual channels [12] and express virtual channels [8] have been proposed. As these designs are input-buffered, they are only able to multiplex arriving packets from their input ports across the crossbar switch, unlike our proposed DSB architecture which can shufﬂe incoming packet ﬂows from all input ports onto the middle memories and then onto the crossbar switch. The proposed DSB architecture offers better opportunities for packet multiplexing and improved packet ﬂow, which helps in mimicking the high throughput and predictable delay characteristics of output-buffered routers. There have been several input-buffered router proposals that target network latency, making single-cycle routers feasible, such as speculative allocation [10], [11], [14] and route lookaheads [3], [7]. For power savings, techniques such as row-column separation [6] and segmentation [22] of crossbars and straightthrough buffers [22] have been proposed. These latency and power optimization techniques are orthogonal to our proposal as they do not target throughput. Some of these techniques can be applied to the DSB router as well to reduce latency and energy consumption. As already mentioned, DSB routers [4], [15], which can emulate an output-buffered router without router speedup, have been successfully used in Internet routing. Stunkel et. al [18] proposed the IBM colony router which is a customized architecture for off-chip interconnection networks with large central buffers and three crossbars. Although this architecture is similar to that of DSB routers, it does not use timestamping of ﬂits for OBR emulation. Instead, packets potentially incur large de-serialization and serialization latencies to support wide SRAM accesses. V I I . CONC LU S ION S In this paper, we proposed a distributed-shared-buffer (DSB) router for on-chip networks. DSB routers have been successfully used in Internet routers to emulate the ideal throughput of output-buffered routers, but porting them to on-chip networks with more stringent constraints presents tough challenges. The proposed DSB router achieves up to 19% higher saturation throughput than input-buffered routers (IBRs) and up to 94% of the ideal saturation throughput for synthetic trafﬁc patterns. The higher saturation throughput translates to large reductions in network latency with SPLASH-2 benchmarks. For SPLASH-2 applications which exhibit high contention and demand high communication bandwidth, DSB routers on an average have 60% lower network latencies than IBRs. ACKNOW LEDGM ENT This work was supported in part by NSF (grant no. CCF 0702341). "
Addressing Manufacturing Challenges with Cost-Efficient Fault Tolerant Routing.,"The high-performance computing domain is enriching with the inclusion of Networks-on-chip (NoCs) as a key component of many-core (CMPs or MPSoCs) architectures. NoCs face the communication scalability challenge while meeting tight power, area and latency constraints. Designers must address new challenges that were not present before. Defective components, the enhancement of application-level parallelism or power-aware techniques may break topology regularity, thus, efficient routing becomes a challenge.In this paper, uLBDR (Universal Logic-Based Distributed Routing) is proposed as an efficient logic-based mechanism that adapts to any irregular topology derived from 2D meshes, being an alternative to the use of routing tables (either at routers or at end-nodes). uLBDR requires a small set of configuration bits, thus being more practical than large routing tables implemented in memories. Several implementations of uLBDR are presented highlighting the trade-off between routing cost and coverage. The alternatives span from the previously proposed LBDR approach (with 30\% of coverage) to the uLBDR mechanism achieving full coverage. This comes with a small performance cost, thus exhibiting the trade-off between fault tolerance and performance.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip Addressing Manufacturing Challenges with Cost-Efﬁcient Fault Tolerant Routing S. Rodrigo1 , J. Flich1 , A. Roca1 , S. Medardoni2 , D. Bertozzi2 , J. Camacho1 , F. Silla1 and J. Duato1 1Parallel Architectures Group 2ENDIF, Department of Engineering Technical University of Valencia University of Ferrara Valencia, Spain Ferrara, Italy Abstract—The high-performance computing domain is enriching with the inclusion of Networks-on-chip (NoCs) as a key component of many-core (CMPs or MPSoCs) architectures. NoCs face the communication scalability challenge while meeting tight power, area and latency constraints. Designers must address new challenges that were not present before. Defective components, the enhancement of application-level parallelism or power-aware techniques may break topology regularity, thus, efﬁcient routing becomes a challenge. In this paper, uLBDR (Universal Logic-Based Distributed Routing) is proposed as an efﬁcient logic-based mechanism that adapts to any irregular topology derived from 2D meshes, being an alternative to the use of routing tables (either at routers or at end-nodes). uLBDR requires a small set of conﬁguration bits, thus being more practical than large routing tables implemented in memories. Several implementations of uLBDR are presented highlighting the trade-off between routing cost and coverage. The alternatives span from the previously proposed LBDR approach (with 30% of coverage) to the uLBDR mechanism achieving full coverage. This comes with a small performance cost, thus exhibiting the trade-off between fault tolerance and performance. Index Terms—Networks-on-chip; Fault-tolerance; routing I . IN TRODUC T ION Main microprocessor manufacturers have shifted to Chip Multi-Processors (CMPs) for their latest products. In CMPs many cores are put together in the same chip, and as technology advances more cores are included. Recently, Intel has announced a research chip with 48 cores, each being x86 compatible, under the Tera-scale Computing Research Program [1]. Previously, Intel also developed a chip prototype [2] that included 80 cores (known as TeraFlops Research chip). Embedded systems are also shifting to multi-core solutions (MultiProcessor System-on-Chips; MPSoCs). A clear example of high-end SoCs are the products offered by Tilera [3] where multi-core chips provide support to a wide range of computing applications, including advanced networking, high-end digital multimedia, wireless infrastructure and cloud computing. CMPs and high-end MPSoCs rely on an on-chip (NoC) network able to handle all the communication trafﬁc between cores. The most straightforward topology for NoCs is the 2D mesh structure as it offers regularity and simplicity for routing. All the links have the same length thus, exhibiting the same latency. Also, local trafﬁc is well supported since latency is low. One of the inconveniences of the 2D mesh, however, is the relative higher hop count for messages travelling distant nodes. Fortunately, this impact is minimized with the use of wormhole and virtual cut-through switching (where hop count is additive to latency). On the other hand, in CMP systems and high-end MPSoCs, tile design is gaining momentum. The tile is designed in isolation and, once ﬁnished, the chip is built by replicating tiles. By doing this, the design effort to build a chip is drastically reduced. Tiled designs also advocate for regular network structures like 2D meshes. Due to the previous reasons, we advocate for 2D meshes in CMPs and high-end MPSoCs. However, even if the design of a CMP chip with a 2D mesh network is correct, the ﬁnal on-chip network may face new raising challenges, leading to a non-regular network structure. Several challenges are being identiﬁed in the following years to come: manufacturing defects, effective chip utilization, voltage/frequency islands, and effective power saving techniques. As technology advances, correct manufacturing becomes challenging and defective components are more frequent in ﬁnal chips. A clear example is the allocation of a defective tile that, if not addressed, will ruin the 2D mesh structure of the network, leading to an irregular network not handed by the routing algorithm and thus making the chip useless. Another challenge is getting enough parallelism to efﬁciently use tens and hundreds of cores. This will render to a low utilization of the cores, or alternatively, to the need to partition the chip into multiple domains, each one running a different application. In this scenario, and to ﬁt as many applications as possible, the partitions of the chip resources will become irregular. The way applications are mapped onto the chip belongs to the concept known as virtualization, where a real chip is divided in several smaller virtual chips. Voltage/frequency islands are being identiﬁed as a need, where different regions of the chip will have different operating conditions. This will require the isolation of such regions so to avoid conﬂicting mismatches that may lead to bottlenecks, e.g. a message crossing different domains. Finally, another major challenge is the need of efﬁcient power saving methods. As the number of cores increases, probably most of the cores will remain unpowered (in sleep mode) most of the time, so enabling large savings in power consumption. The same should be applied to the on-chip network, which has been reported to consume 30% of the total chip power consumption. Powering off and on different routers will lead to temporal irregular patterns. All these previous challenges require some effort at the onchip network level, speciﬁcally supporting irregular network topologies. But rather than addressing completely irregular topologies (more suitable for low-end MPSoC systems) the effort must be made to address irregular topologies derived from an original 2D mesh with the following properties: (1) a router is connected to at most four routers each one in a different direction and dimension, and (2) a hop along a valid direction and dimension will not cross more than one row and column. In this paper we provide a step further and provide a mechanism able to cover all the possible cases derived from a 2D mesh, that is, with full support for any fail978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.12 25 ure/virtualization/domain/region conﬁguration. The mechanism, referred to as uLBDR (universal Logic-Based Distributed Routing) is an evolution of LBDR [4]. The remaining of the paper is organized as follows. In Section II we describe the related work. In Section III we describe the uLBDR mechanism. In Section IV we describe two router implementations to support the uLBDR mechanism. Then, in Section V we evaluate uLBDR. Finally, in Section VI we conclude the paper and provide future directions. I I . R E LAT ED WORK The ﬁrst straightforward way to deal with irregular topologies corresponds to source-based routing. The node that sends a message computes a path, from a routing table stored at the end-node, and stores it in the message header. One example is Street-Sign Routing [5]. It compresses the message header to minimize the impact on network bandwidth. The router id and the direction for the next turn is included in the message header. However, a table is still needed at every end node. In distributed routing, however, the message header only contains the destination ID and each router computes the next link to forward the message. One of the most used techniques to deal with non-regular topologies is to implement forwarding tables at routers, so each one stores the output port that must be used based on the destination. The main advantage of table-based routing is that any topology and any routing algorithm can be implemented, including fault-tolerant routing algorithms. However, routing tables do not scale in terms of latency, power consumption, and area, thus being impractical for NoCs [6]. Using solutions from the off-chip network domain for NoCs must be a step reviewed thoroughly. Message dropping proposals like TCP/IP protocols [7] would affect severely network performance, so they are not suitable. Techniques used in large parallel systems that switch off healthy nodes (lamb nodes) like the Blue Gene/L system [8] to keep topology and routing algorithm untouched also impacts performance. Other mechanisms, focused on routing optimization [9] require the use of virtual channels (up to ﬁve in some cases) but they do not achieve 100% coverage practically. Also, these mechanisms rely on adaptive routing, and the network must deal with out-of-order issues, a feature that could be difﬁcult to implement in NoCs. Region-Based Routing (RBR) [10] (and a similar proposal [11]) has been proposed as an attempt to achieve faulttolerance in on-chip networks while requiring few resources. RBR groups at each router different destinations (region) that can be reached by an output port. The main drawback [12] of such mechanism is that, even with 16 regions deﬁned, it still does not achieve 100% coverage. Also, it signiﬁcantly increases the router delay path [12]. Default-Backup Path (DBP) [13] deals with the issue of maintaining healthy processing elements when the attached router fails. It adds redundant wiring and buffers connecting output and input ports directly. It, however, does not address routing in irregular topologies. In [14] authors propose an architecture based on deﬂection routing that attempts to detect fault errors adding CRC modules at input and output ports for crossbar faults and SEC for each type of fault. The link fault matrix is n×n being n the codes for link faults with the support of routing matrices, one dimension. It uses a variant of deﬂection routing called delta X Y with weighted priority. Deﬂection (or reﬂection) can lead to potential starvation that is solved with message dropping, thus impacting performance. Also, routing matrices exhibit poor scalability as the number of destinations augments. There are several proposals relying on the use of tables, thus suffering from the same scalability problems and routing costs. Adaptive stochastic routing (ASR) [15] relies on a selflearning method to handle failures by assigning conﬁdence ﬁelds to output ports for different tasks running in the system. Thus, it requires a table on each router having n entries for n tasks. In [16] the objective is to minimize routing tables, either at end-nodes or at routers. Three techniques are proposed: Turn-table (TT), XY Deviation Table (XYDT) and Source Routing Deviation Points (SRDP). Mainly, all these techniques rely on a routing table to handle irregular cases in combination with routing strategies like X Y (combined with Y X ), source routing or the don’t turn technique (meaning, a message does not change direction when traversing the router unless is noted). In [17], authors propose a compendium of state-of-theart LUTs implementations for routing purposes. These designs shift from being fully hardwired to being partially or fully conﬁgurable depending on the degree of ﬂexibility. As a summary, all the proposals tend to use routing tables (either at sources or at routers) and/or rely on an excessive number of resources (virtual channels) to solve the deadlock problem. Also, none of the solutions (except when using tables) provides full coverage (all the possible failure cases) for a 2D mesh. I I I . ULBDR D E SCR I P T ION The description of uLBDR will be guided as an evolution from the basic mechanism, previously proposed, and with low coverage, to the most enhanced version, with full coverage. In each, example cases will be described so to motivate the need for the next version. As we focus on 2D meshes we will refer to each router port by its direction, being N , E , W , and S for north, east, west, and south, respectively. A. First mechanism: LBDR The uLBDR evolves from the basic LBDR mechanism. LBDR requires eight routing bits (Rne , Rnw , Ren , Res , Rwn , Rws , Rse , Rsw ) and four connectivity bits (Cn , Ce , Cw , Cs ) per router to deﬁne the routing and connectivity pattern. Those bits are used by a combinational logic to decide the output port to use for every message. The LBDR mechanism relies on the use of minimal paths for every source-destination pair. Indeed, LBDR uses two comparators to decide the directions to use based on the relative position of the current router and the destination router. The directions (labelled as N (cid:2) , E (cid:2) , W (cid:2) , and S (cid:2) in Figure 1(a)) are then checked against the routing/connectivity bits. Therefore, only minimal paths are allowed. This leads, as we will see in the evaluation, to a low percentage of topologies being supported. Indeed, in a 2D mesh is easy to imagine sets of failed links/routers that require non-minimal paths for some source-destination pairs. A single hole in the center of the network is a clear example. Figure 1(b) shows a topology not supported by LBDR. The path from A to B is non-minimal. At router A, the possible directions to reach B are N and E , however, both links are missing, and therefore no possible way out. 26 (a) ULBDR core routing logic: LBDR (b) Topology not supported by LBDR Fig. 1. uLBDR core mechanism: LBDR. B. Second Mechanism: LBDRdr We need to provide non-minimal support in an efﬁcient way, that is, with as minimum logic as possible. Figure 2(a) shows the additional logic proposed. In particular, for every input port of the router a deroute option is provided. A set of two bits encode the deroute option that can be N , E , W , or S . Whenever the previous LBDR logic is unable to provide a valid output port (NOR gate with four inputs) the deroute option is taken into account. The logic is replicated for every input port, therefore the deroute option used is the one associated with the input port the message arrived from. Alternatively, the deroute option can be designed for the entire router, instead of having a deroute option per input port. However, this reduces ﬂexibility and leads to non-supported topologies (this alternative will be analyzed later). Figure 2(b) shows an example, where two different deroute options are required for two different input ports at router A. If going N , and the message comes from input port S , then a deroute is set to W . On the other hand, if the message is coming from W , and the intention is to go E , then a deroute is set to S . It is worth mentioning that the deroute option needs to be computed in accordance to the routing algorithm, as it must not introduce cycles that could lead to deadlocks. In Figure 2(b) a deroute option at input port W at router B can not be set to S as it would let messages crossing a routing restriction. The algorithm computes ﬁrst the set of routing bits. This is done by taking into account the topology (including the failed/powered down routers and links) and the routing algorithm. The selected algorithm in this paper is SR [18] as it can be applied to any topology and does not require virtual channels1 . LBDR bits are computed by taking into account the location of the routing restrictions. Thus, computation is straightforward and the complexity is low (linear with the number of routing restrictions). Once LBDR bits are computed the deroute options are searched. To do this, the algorithm looks for a valid path for every source-destination pair (the algorithm is computed ofﬂine before any normal operation of the chip, thus computation complexity is not a major issue). As LBDR may allow multiple paths for a given source-destination, the algorithm deeply searches all the paths in a recursive way. Two end nodes are connected by LBDR if all the possible paths reach the destination. In the search of all paths, whenever it fails to provide a valid path, then, a misrouting action is needed. 1Alternatively, for a healthy chip the X Y routing algorithm can be used. 27 Figure 2(c) shows the case at router B for messages going from router A to router C . In this situation, the algorithm tries all the possible deroute options available, one per output port but avoiding U turns (so, west port is not considered). Options leading to crossing routing restrictions are also evicted. The algorithm starts with the ﬁrst deroute option and keeps following the path, thus taking the deroute, checking if the path (and all their possible alternative paths) will reach the destination. In case of success, the deroute option is set. In case of failure (destination is not reached) another deroute option is tried. In case all deroute options fail the topology is not covered by LBDRdr . Notice that several deroute options may be required for a single path. Figure 2(c) shows the route along the path using the deroute bits at router B . This mechanism will enhance greatly the coverage of uLBDR. However, there are subtle cases that are still not covered by LBDRdr . Figure 2(d) shows an example. The problem comes by the fact that for some destinations located at the same quadrant, at router B the routing engine should provide one port (N ) for some destinations (router C ) and another port (W ) for other destinations (router A). As LBDR (or LBDRdr ) works in quadrants, there is no way to indicate the router which option should be given to the message. C. Third Mechanism: LBDRf ork+dr To solve the previous problem, LBDRdr is enhanced with an additional feature. The message is simply forked through two output ports. This leads, however, to important changes in the router design. We describe them next. The router arbiter needs to be changed to allow one message to compete for more than one output port at the same time. There are two alternatives. In the ﬁrst one, the arbiter may consider a request from a message to two output ports as an indivisible request, therefore, granting or denying access to both outputs at the same time. This leads to a simpler design of the buffering at the input port, as one read pointer is needed. In the second one, the arbiter may grant or deny access to one port regardless of the action performed for the other port. This leads to a complex input buffering, as message forwarding may be shifted for both output ports, thus each requiring a read pointer. We assume the ﬁrst option because of its simplicity and the fact the fork operations will be used in few cases. Deadlock may occur in wormhole switching, since two fork messages may compete for the same set of resources. Although the routing algorithm used is deadlock-free, performing fork actions (like collective communication) leads to deadlock. (a) Deroute logic (b) Two different deroute options (c) Deroute solution (d) Topology not supported by LBDRdr Fig. 2. uLBDR: LBDRdr mechanism. (a) Fork logic (b) Complete uLBDR mechanism Fig. 3. uLBDR: LBDRf ork+dr mechanism. Imagine that a message m1 gets the output port N at router X and requests output port E at router Y . However, message m2 gets output port E at router Y and requests output port N at router X . None of the messages will advance since the input buffers will ﬁll and the output ports will never be released. The easiest solution (from the problem’s point of view) is the use of virtual cut-through (VCT) switching, thus ensuring a packet2 will ﬁt always in a buffer. Thus, the output ports in the previous example will be released (the packet has been forwarded entirely) and the requests for the output port will be granted. Other options rely on performing ﬂitlevel circuit switching [19] and wormhole switching between routers. Basically, those solutions label each ﬂit with identiﬁers so ﬂits from different messages can be mixed in the same buffer. The problem with this kind of solutions is that internal tables are required so to keep ﬂit identiﬁers. We opt for the ﬁrst solution, thus enforcing VCT switching. Although VCT is seen as demanding much buffer space at routers, a careful design of the router can minimize this impact. In this paper we show key design options that enable its use in NoCs (Section IV). Asuming VCT switching, one of the packets being forked will reach the ﬁnal destination. However, the other replica will not reach the destination. In this situation, the packet needs to be removed from the network. This will be easily achieved by silently destroying the packet at a router. In Section V we 2 In VCT we use the term packet since a message may be packetized. will show area and latency results for two routers designed for LBDRf ork+dr , thus showing upfront the real impact of such router changes. The LBDRf ork+dr mechanism is shown in Figure 3(b). Additionally to the original mechanism more routing bits are added: Rnn , Ree , Rww , Rss . Similarly to Rxy bits, these bits are used to indicate where vertical or horizontal routing restrictions (similar arrows in Figure 2(c)) exist at the next hop. Two additional comparators are used (comparators block in Figure 3(b)) to know if a packet is one hop away from its destination through any direction (signals N1 , E1 , W1 , and S1 ). Also, new four additional fork bits (Fn , Fe , Fw and Fs ) are used per input port. These bits are set to reﬂect the ports that must be used to fork a packet. Whenever a packet’s destination is in the same quadrant deﬁned by the fork bits, then the packet is forked. Fork bits are checked in (cid:2) , parallel with the LBDR logic, just after computing the N (cid:2) , W (cid:2) , and S (cid:2) signals. Keep in mind those bits are computed ofﬂine. Once LBDR and deroute bits are computed and do not provide a valid path to reach a destination, then fork bits are computed. The algorithm checks for every destination in the same quadrant (N E , ES , SW , W N , N S and W E ) if the path (and all their possible alternative paths) will reach the destination with forks included. In case of failure (destination is not reached), the topology is not covered. E IV. ROU T ER S D E SCR I P T ION In this section we provide a brief description of the routers used for the evaluation: a non-pipelined MPSoC router and 28 a pipelined CMP router. In both cases an initial wormhole router design has been evolved with minimum changes so to allow virtual cut-through switching. Changes required are: (1) in ﬂow control, (2) in the arbiter logic (support fork operations), and (3) in the crossbar to remove stale copies of forked packets. A. MPSoC Router Design Typically, NoC building blocks for use in MPSoCs target lower operating speeds with respect to CMPs and are generally unpipelined [20]. The reference component that we consider to assess the feasibility of uLBDR is an input buffered router implementing wormhole switching (Figure 4). Size of the input buffer is tunable and set to 4 slots here. In 1 clock cycle, a ﬂit covers the distance between two consecutive input buffers of connected routers through the inter-router link. The switch traversal inside the router is controlled by a modular arbiter (one round-robin arbiter for each output port). A lightweight stall/go ﬂow control policy is implemented. It requires two control wires: one going forward and ﬂagging data availability (”valid”) and one going backward and signalling either a condition of buffer ﬁlled (”stall”) or of buffer free (”go”). This latter signal is indicated as f low control in Fig.4. (cid:42)(cid:85)(cid:68)(cid:81)(cid:87) (cid:19) (cid:17)(cid:17) (cid:42)(cid:85)(cid:68)(cid:81)(cid:87) (cid:49) (cid:42)(cid:85)(cid:68)(cid:81)(cid:87) (cid:47)(cid:50)(cid:42)(cid:44)(cid:38) (cid:53)(cid:40)(cid:52) (cid:19)(cid:171)(cid:3)(cid:53)(cid:40)(cid:52)(cid:3)(cid:49) (cid:42)(cid:85)(cid:68)(cid:81)(cid:87) (cid:19) (cid:56)(cid:47)(cid:37)(cid:39)(cid:53) (cid:49)(cid:50)(cid:53)(cid:55)(cid:43) (cid:36)(cid:85)(cid:69)(cid:76)(cid:87)(cid:72)(cid:85) (cid:50)(cid:88)(cid:87)(cid:83)(cid:88)(cid:87) (cid:49)(cid:82)(cid:85)(cid:87)(cid:75) (cid:53)(cid:40)(cid:52) (cid:19) (cid:53)(cid:40)(cid:52) (cid:49) (cid:41)(cid:79)(cid:82)(cid:90) (cid:38)(cid:82)(cid:81)(cid:87)(cid:85)(cid:82)(cid:79) (cid:44)(cid:81)(cid:83)(cid:88)(cid:87) (cid:37)(cid:88)(cid:73)(cid:73)(cid:72)(cid:85) (cid:49)(cid:82)(cid:85)(cid:87)(cid:75) (cid:39)(cid:68)(cid:87)(cid:68) (cid:18) (cid:57)(cid:68)(cid:79)(cid:76)(cid:71) (cid:39) (cid:52)(cid:54)(cid:40)(cid:55) (cid:52)(cid:54)(cid:40)(cid:55) (cid:52)(cid:54)(cid:40)(cid:55) (cid:39) (cid:38)(cid:47)(cid:53) (cid:39) (cid:52) (cid:38)(cid:47)(cid:53) (cid:39) (cid:52) (cid:38)(cid:47)(cid:53) (cid:52)(cid:54)(cid:40)(cid:55) (cid:52) (cid:38)(cid:47)(cid:53) (cid:52) (cid:41)(cid:79)(cid:82)(cid:90) (cid:38)(cid:82)(cid:81)(cid:87)(cid:85)(cid:82)(cid:79) (cid:41)(cid:54)(cid:48) (cid:44)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)(cid:37)(cid:88)(cid:73)(cid:73)(cid:72)(cid:85)(cid:3)(cid:171)(cid:17)(cid:17) (cid:56)(cid:47)(cid:37)(cid:39)(cid:53) (cid:171)(cid:171)(cid:17) (cid:39)(cid:68)(cid:87)(cid:68) (cid:18) (cid:57)(cid:68)(cid:79)(cid:76)(cid:71) (cid:41)(cid:79)(cid:82)(cid:90) (cid:38)(cid:82)(cid:81)(cid:87)(cid:85)(cid:82)(cid:79) (cid:42)(cid:85)(cid:68)(cid:81)(cid:87) (cid:49) (cid:53)(cid:40)(cid:52) (cid:19) (cid:53)(cid:40)(cid:52) (cid:49) (cid:36)(cid:85)(cid:69)(cid:76)(cid:87)(cid:72)(cid:85) (cid:50)(cid:88)(cid:87)(cid:83)(cid:88)(cid:87) (cid:171)(cid:17)(cid:17) (cid:41)(cid:79)(cid:82)(cid:90) (cid:38)(cid:82)(cid:81)(cid:87)(cid:85)(cid:82)(cid:79) Fig. 4. MPSoC router schematic. LBDR and LBDRdr mechanisms are implemented in a similar way, from an architecture viewpoint. The head ﬂit contains destination coordinates which are read, after storage in the input buffer, by the routing logic. The output signals elaborated by the LBDR/ LBDRdr module represent match signals sent to the arbiters. A match signal indicates that the packet from a given input port requires a speciﬁc output port. It is interesting to note that the LBDR/ LBDRdr routing logic enables to preserve the modular design style of the router architecture (one routing module per input port). This router was evolved to VCT switching to support LBDRf ork+dr . The signals used and the architecture schematic are the same of Figure 4, just the meaning of ﬂow control signals and the arbiter behaviour change. First of all, we had to evolve the basic stall/go ﬂow control protocol to credit-based ﬂow control. In fact, stall/go would have been acceptable only in case all packets were of the same length. If packets exhibit variable length (e.g., reads vs writes, variable 29 number of write/read burst beats, etc.), then the router arbiter needs to know the number of available slots in the downstream buffer before granting a new packet head. Therefore, we now use the f lowcontrol signals in Figure 4 as credits. An input buffer asserts a credit high when it has a grant from the arbiter AND it has valid ﬂits to send. The arbiter behaviour had to be modiﬁed as well. A port arbiter (say for the N output port) performs round-robin arbitration among all inputs with valid asserted and presenting a headf lit. Say that input N is the winner. Then, the arbiter compares its counter value (denoting the number of free slots in the downstream input buffer) with the packet length from the N inputport. If it is larger, then grant is asserted enabling switch traversal to all the winning packet ﬂits. If there is no space downstream for the entire packet, the grant is kept low. In LBDRf ork+dr , packets can be forked through two output ports. When this happens, the LBDR logic asserts two match signals heading to two different port arbiters. When BOTH of them assert their grant signals, a unique grant is sent to the requesting input buffer, as illustrated in Figure 4. One of the packets will reach destination. The other one will reach a router where the LBDR logic will not provide a valid match signal. In that situation, the grant signal is set by default to asserted, thus the packet will be forwarded to the crossbar which is not conﬁgured for the input port, thus the packet will be ﬁltered. The input buffer is not aware that no arbitration has been performed for the forked packet, and the grant signal is kept asserted, thus will also correctly generate a credit to the upstream router, since buffer slots are cleared. This is the way the misrouted forked packet is silently discarded. B. CMP Router Design The CMP router is a pipelined input-buffered wormhole router with ﬁve stages: input buffer (IB), routing (RT), switch allocator (SW), crossbar (XB) and link traversal (LT). We used a simple router with no virtual channels and ﬁve input/output ports. Flit size is set to 4 bytes. The input buffer size is set to four ﬂits. The RT module has been implemented to support X Y , LBDR, LBDRdr , and LBDRf ork+dr routing algorithms and the Stall/Go ﬂow control. Finally, the SW module has been designed with a round-robin arbiter as in [21]. The router has been implemented using the 45nm technology open source Nangate [22]. In order to adapt the basic CMP router to VCT we have performed the following changes. First, buffers at routers have been set to maximum packet size, in our case to four ﬂits. In addition, packetization is performed at the interface nodes when required (notice that this is also needed for the MPSoC router, probably with a different packet size). Message sizes in CMPs (using a coherence protocol) are known beforehand. Usually a short message contains a memory address and a coherence command and a long message also includes the cache line. In our case (in the evaluation), short messages are set to 8 bytes and long messages to 72 bytes (cache line size is 64 bytes). Assuming 8-byte ﬂits, short messages are not packetized and long messages are packetized in 11 packets (taking into account packet header is replicated). To efﬁciently forward packets in VCT we need to change the ﬂow control mechanism (as in MPSoC router). In the CMP case where packets sizes are known, we opted for the Stall/&Go ﬂow control at the packet level. That is, a stall or go signal is asserted per packet. Notice that we assume links with one cycle delay, thus round trip time is set to three cycles. Buffers of four ﬂits are thus enough to avoid introducing bubbles. However, for messages with sizes lower than packet size (and round-trip time; e.g. one-ﬂit packets) bubbles between packets are generated. To avoid bubbles we decided to pad short packets to four-ﬂit packets. Obviously, this may affect performance. In the next Section we analyze the impact of padding and packetization. SW is the most critical stage in our design. Thus, the arbiter modiﬁcations applied in the MPSoC arbiter are not affordable for the CMP router. To solve this we have implemented the arbiter shown in Figure 5. This arbiter is the same used in the WH design but it adds a new module performed in parallel. This module arbitrates between fork requests. The grant signals of this module enable (or disable) the non-fork grants. Higher priority is given to fork requests. By doing this minimum impact on the SW latency is expected. Stale packets (generated by fork operations) are silently discarded in the same way as in the MPSoC router.         L LBDR ...         SLBDR         L LBDR−DR ARBITER OUTPUT PORT         L ... ... OUTPUT PORT         SLBDR         S         S FORK         S         NFORK         SFORK ... FORK ARBITER FORK? Fig. 5. New arbiter for the CMP router with fork requests. V. EVALUAT ION In this Section we provide several evaluation results to assess the impact of the different LBDR versions. First, we provide a coverage analysis for each solution. Then, we analyze the overhead of the mechanisms in the router designs, including also the overhead incurred by the VCT router counterparts. Finally, we provide performance results by running real applications in a full system simulator environment. We test in particular if the packetization requirement by LBDRf ork+dr affects ﬁnal application’s execution time. Also, performance results when using routing tables are obtained. A. Coverage Analysis In this Section we evaluate the coverage provided by different versions of the mechanism, from the original LBDR to the full uLBDR mechanism, LBDRf ork+dr . Coverage is measured as the percentage of topologies supported from a pool of topologies. A topology is considered supported if every node in the network reaches all possible destinations. A set of topologies derived from the link variability analysis provided in [12] has been used. In particular, different NoC operating frequency thresholds were set and links not reaching those thresholds (due to variability effects) were labelled as faulty. Chips were modelled on a real 65nm implementation NoC layout where all cores are identical, and their size is 1mm2 . Two different conﬁgurations were used, 4 × 4 and 8 × 8 NoCs with different values of spatial correlation (λ 0.4 and λ 1.2) and variance (σ 0.05 and σ 0.18). In total, 1423 topologies have been evaluated. The evaluation comprehends four different scenarios, LBDR, LBDR with 1 global deroute (LBDR1dr ), LBDRdr as explained in Section III-B and LBDRdr+f ork (or uLBDR), the full mechanism. In Figure 6(a) results show how the addition of the two enhancements, deroutes and forks, affects signiﬁcantly the coverage. Although having one global deroute per router helps to increase coverage by 50%, further beneﬁts are obtained for deroutes per each input port (5 per router), as coverage further increases to 80%. Finally, the fork mechanism is the one that guarantees full coverage. Figure 6(b) shows the average percentage of deroutes and forks required per chip. As can be seen, an small set of deroutes are required. As irregularity and network size increases the use of deroutes also increases and the fork mechanism is even less utilized. B. MPSoC Router Overhead We synthesized the MPSoC router with a 65nm STMicroelectronics technology library and Synopsys Physical Compiler. Routers with all routing mechanisms (LBDR, LBDRdr and LBDRf ork+dr ) were synthesized both for max. performance and for the same target speed (that of the slowest architecture, i.e., LBDRf ork+dr ). All routers implement the same amount of buffering (4 slots). The choice of a speciﬁc routing mechanism affects the maximum achievable speed by each router: 1GHz for LBDR, 950 MHz for LBDRdr , and 750 MHz for LBDRf ork+dr . Post-synthesis area results for the routers are illustrated in Figure 6(c). By looking at the maximum performance ﬁgures, LBDRf ork+dr is about 10% larger than LBDR, clearly due to the more complex port arbiters and to their need to handle true credit based ﬂow control. To make this relatively more complex circuit faster, the synthesis tool tried to speed up the crossbar at the cost of further increased area. We also observe that LBDR and LBDRdr feature approximately the same maximum area, except for hardly controllable speciﬁc optimizations that the synthesis tool applies to the two netlists. The take-away message here is that the logic complexity of these two routing mechanisms is pretty much equivalent. When the three routers under test were re-synthesized to meet the performance of the slowest one (the LBDRf ork+dr ), then of course the relaxation of the delay constraint for LBDR and LBDRdr allowed the synthesis tool to infer a more area efﬁcient gate level netlist for them. As a consequence, the area efﬁciency gap with LBDRf ork+dr became as large as 44,7%, the absolute worst-case. We conclude that whenever the three routing schemes are employed at their maximum performance, the area gap is not signiﬁcant (around 10%) while tremendously gaining in fault coverage. When the target speed is affordable for each of them and close to that of the slowest scheme, then the choice between the routers becomes a true area-coverage trade-off decision. When the target frequency is very low (a few hundred MHz, not showed in Figure 6(c)), then the gate level netlists of the three schemes can be almost equally optimized, resulting in almost the same area while keeping the coverage differences. C. CMP Router Overhead Table I summarizes (upper part) frequency and area results of the CMP router for different switching techniques and routing mechanisms. The ﬁrst thing to highlight is the 30 (a) Coverage (b) Utilization of deroutes and forks per router (c) Normalized area results for the MPSoC router VCT-LBDRf ork+dr VCT-LBDRf ork+dr area / Freq area (μm2) frequency (GHz) Fig. 6. Performance results. Coverage, utilization of deroutes and forks, and MPSoC router. All the router WH-XY WH-LBDR WH-LBDRdr 19860 20335 20418 1.33 1.33 1.33 Only the routing modules WH-XY WH-LBDR WH-LBDRdr 137.592 143.304 141.479 3.45 2.78 2.56 TABLE I VCT-xy 13644 1.53 VCT-xy 86.3 4.55 area / Freq area (μm2) frequency (GHz) 14944 1.47 137.91 2.56 AR EA AND FR EQUENCY FOR TH E CMP ROU T ER AND TH E ROU T ING MODU L E S , BOTH IN WH AND VCT V ER S ION S , D I FF ER EN T ROU T ING A LGOR I THM IM P L EM EN TAT ION S . (a) 2-cycle delay router (RT) vs one-cycle delay (b) 2-cycle RT module (RT) vs one-cycle RT (c) Irregular topology 2-cycle RT module (RT) router (LBDR) module (LBDR) vs one-cycle RT module (LBDR) Fig. 7. Execution time of applications in a 4 × 8 CMP system. Normalized execution time of applications when routers with two cycle delays (RT) and one-cycle delay (LBDR) are used. improvement of both area and frequency of the VCT router. The reason for this improvement is due to the use of buffers with the same size of packets. This has simpliﬁed the IB stage because in VCT the ﬂit header of every packet is mapped always into the same buffer slot, thus simplifying read logic. Also, the logic to keep track the number of mapped ﬂits in the buffer has also been simpliﬁed. Due to the per-packet ﬂow control only a control signal is required. Although such simpliﬁcations can also be made in WH, bubbles will be introduced (known as atomic buffer allocation). There is no difference in the operating frequency when using either XY, LBDR or LBDRdr , and only a marginal increase in area (differences fall within the uncertainties of the synthesis optimization process). This is due to the RT stage not setting the maximum frequency of the router. LBDRf ork+dr experiences, however, a small impact in performance and area. This performance degradation is due to the overhead added to the SW stage. Table I also shows (bottom part) the area overhead and frequencies of the different routing modules, thus not considering the entire router. There are signiﬁcative differences between the XY module for WH and for VCT. These differences are due to the different ﬂow control mechanism used in both versions. Also, the different input buffer design affects the routing module. On the other hand, LBDR and LBDRdr mechanisms have an small impact on area but a large one in frequency. Also, the complexity of LBDRf ork+dr has a large impact on both area and frequency. However, remember that this module is not the one setting the router frequency. D. Performance Analysis We have run several analysis for performance using the GEMS/SIMICS platform [23] upgraded with an event driven cycle-accurate network simulator. Several SPLASH-2 [24] applications (Barnes, FFT, LU, Radix, and Raytrace) and Apache application have been run in the platform. Two CMP chip conﬁgurations have been used, the ﬁrst one with 16 cores spreaded in a 4 × 4 mesh, and the second one with 32 cores spread in a 4 × 8 mesh. In both cases a directory-based cache coherency protocol is used to keep coherency between private L1 caches and a shared (but distributed) L2 cache. Figure 7(a) shows the normalized execution time when using a router with distributed routing tables (two cycles delay 31 (a) Flit size is set to 2 bytes (b) Flit size is set to 8 bytes Fig. 8. Execution time of applications in a 4 × 4 CMP system where packetization is performed. router) and a router using LBDRdr engine (one cycle delay router). Wormhole switching is assumed and ﬂit size is set to 4 bytes. As can be seen, a slow router (routing tables) affects greatly the execution time of applications, in some cases by a factor of 2. In all applications the slower router behaves worse. Therefore, the designer needs to avoid the large latency of routing tables. Figure 7(b) shows the case when two routers are compared, one with the RT stage experiencing two cycles (only for header ﬂits) and one with the RT stage having a single cycle. Notice that in this situation the overhead of routing tables is smaller. Finally, Figure 7(c) shows performance achieved in an 4 × 8 topology where two links are missing, so deroutes and forks are necessary. In that situation, notice that applications could be run as the routing mechanism is able to support paths (coverage of the topology). The execution time of applications did not varied much. However, the interesting point is the success in running the application in a failed mesh network without using routing tables. To conclude the evaluation, Figure 8 shows performance results when packetization is used in a VCT router. Also, results assuming WH switching are included for comparison purposes. Figure 8(a) shows the worst case for packetization when ﬂit size is narrow (2 bytes). In that situation packetizing messages usually doubles execution time of applications. However, in Figure 8(b) where ﬂit size is widened (8 bytes) the situation changes and now the impact is much lower. Notice that packetization overhead in execution time is lower than 20% in all the applications, being on average 5%. Anyway, there is an overhead in packetization. V I . CONC LU S ION S In this paper we have presented uLBDR, a logic-based routing layer for on-chip networks to support any irregular topology derived from a 2D mesh without using routing tables. The objective of the full mechanism, LBDRdr+f ork is to offer full coverage on this set of topologies, result of several challenges to be taken into account: fault-tolerance, chip virtualization, and power-aware techniques. This is achieved with a trade-off between router design and coverage. The mechanism proposed spans from low coverage (30%) with no router overhead and no performance impact, to full coverage with a marginal impact on router design. In particular LBDRf ork+dr version requires a VCT router design and its impact on router frequency is 30% on an MPSoC router and no impact on a CMP pipelined router design. To sum up, a clear trade-off lies between coverage of irregular 2D-Mesh derived topologies and performance of applications. Future research will target an in-deep tuning of router architectures with uLBDR, specially reducing the paid overhead. ACKNOW L EDG EM EN T S This work was supported by the Spanish MEC and MICINN, as well as European Comission FEDER funds, under Grants CSD2006-00046 and TIN2009-14475-C04. It was also partly supported by the project NaNoC (project label 248972) which is funded by the European Commission within the Research Programme FP7. "
Low-Power Bioelectronics for Massively Parallel Neuromonitoring.,"Currently emerging intracortical biosensing devices are promising alternative to allow studying the neural activity underlying cognitive functions and pathologies, understanding neurons interactions, locating onset seizures, detecting mind driven decisions, etc. This talk covers low-power analog circuits and packaging techniques used for the design and integration of biosensing Microsystems. Such devices are interconnected to intracortical neural tissues, and include high-reliability wireless links used to power up such implanted devices and bidirectionally exchange data with external base station. Global view of typical devices altogether with corresponding multidimensional challenges will be described. Special attention will be paid to report two case studies: 1) Automatic detection of action potentials from massively parallel channels, and 2) Epilepsy seizures monitoring and onset treatment.","Low-Power Bioelectronics for Massively   Parallel Neuromonitoring  Mohamad Sawan  University of Montreal, Canada  Abstract     Currently emerging intracortical biosensing devices are promising alternative to allow studying the  neural activity underlying cognitive functions and pathologies, understanding neurons interactions,  locating onset seizures, detecting mind driven decisions, etc. This talk covers low-power analog circuits  and packaging techniques used for the design and integration of biosensing Microsystems. Such devices  are interconnected to intracortical neural tissues, and include high-reliability wireless links used to power  up such implanted devices and bidirectionally exchange data with external base station. Global view of  typical devices altogether with corresponding multidimensional challenges will be described. Special  attention will be paid to report two case studies: 1) Automatic detection of action potentials from  massively parallel channels, and 2) Epilepsy seizures monitoring and onset treatment.  Biography     Mohamad Sawan was born in Lebanon, received the B.Sc. degree in 1984 from Laval University, and  the Ph.D. degree in 1990 in electrical engineering, from Sherbrooke University, Canada. He joined  Polytechnique Montréal in 1991, where he is currently a Professor of Microelectronics and Biomedical  Engineering. His scientific interests are the design and test of mixed-signal (analog, digital, RF, MEMS  and optic) circuits and Microsystems: design, integration, assembly and validations. These topics are  oriented toward the biomedical and telecommunications applications. Dr. Sawan is a holder of a Canada  Research Chair in Smart Medical Devices. He is leading the Microsystems Strategic Alliance of Quebec  (ReSMiQ) receiving membership support from 11 Universities.     Dr. Sawan is Deputy Editor-in Chief of the IEEE Trans. on circuits and systems II (TCAS-II), Editor/  Associate Editor of several International Journals such as the IEEE Transactions on Biomedical Circuits  and Systems (TBioCAS) and the Springer Mixed-signal Letters. He is founder / co-founder of several  IEEE International conferences such as NEWCAS, BiOCAS, and ICECS, he is the founder of the  Polystim Neurotechnologies Laboratory at Polytechnique Montréal. Dr. Sawan published more than 500  papers in peer reviewed journals and conference proceedings, and he was awarded 9 patents pertaining  to the field of biomedical sensors and actuators.     Dr. Sawan received several prestigious awards; the most important of them are the Medal of Honor  from the President of Lebanon, the Bombardier Award for technology transfer, the Barbara Turnbull  Award for medical research in Canada, and the achievement Award from the American University of  Science and Technology. Dr. Sawan is Fellow of the IEEE, Fellow of the Canadian Academy of  Engineering, Fellow of the Engineering Institute of Canada, and Officer of the Quebec’s National Order.  3                   "
Distributed Sequencing for Resource Sharing in Multi-applicative Heterogeneous NoC Platforms.,"In the context of heterogeneous NoC architectures for embedded systems, it is today mandatory to support multiple applications given the plurality of standards and usages. While static reconfiguration between applications has already been extensively studied, we propose a potential increase in hardware resource usage by enabling concurrent or overlapping applications on the top of a heterogeneous NoC platform. In this paper, we describe a distributed sequencing protocol allowing hardware resource sharing between several applications. This protocol ensures correct synchronization of the processing between hardware resources without the need of a global fine-grain scheduler on the system, thus alleviating the pressure on the run-time system. The proposed protocol has been integrated and validated in a NoC-based digital baseband for 4G SDR telecom applications, and was integrated on a manufactured chip on a STMicroelectronics CMOS 65 nm LP technology.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip Distributed Sequencing for Resource Sharing in Multi-Applicative Heterogeneous  NoC Platforms  Yvain Thonnart, Romain Lemaire, Fabien Clermidy  CEA – LETI – Minatec, Grenoble, France  {firstname.lastname}@cea.fr  Abstract— In the context of heterogeneous NoC architectures  for embedded systems, it is today mandatory to support  multiple applications given the plurality of standards and  usages. While static reconfiguration between applications has  already been extensively studied, we propose a potential  increase in hardware resource usage by enabling concurrent or  overlapping applications on the top of a heterogeneous NoC  platform. In this paper, we describe a distributed sequencing  protocol allowing hardware resource sharing between several  applications. This protocol ensures correct synchronization of  the processing between hardware resources without the need of  a global fine-grain scheduler on the system, thus alleviating the  pressure on the run-time system. The proposed protocol has  been integrated and validated in a NoC-based digital baseband  for 4G SDR telecom applications, and was integrated on a  manufactured chip on a STMicroelectronics CMOS 65nm LP  technology.  INTRODUCTION  I.  While last decade’s Application Specific Integrated  Circuits (ASICs) had been designed to fulfill a single welldefined and immutable application, the present decade has  been, for consumer electronics, the one of Systems-on-Chip  (SoC). There has been a race for the integration of more and  more functionality, and the limiting factor has soon become  the interconnect architecture. Nevertheless, the development  of Networks-on-Chip (NoC) has broken this wall by  allowing for more complex interconnect topologies on which  information is carried within data packets [1]. Using the NoC  technology, it is now not so rare to find more than a hundred  cores in the bigger SoCs, as well in the domains of massively  parallel homogeneous single-chip multiprocessors and of  heterogeneous manycores SoCs. These latter ones, however,  often suffer from an important hardware underutilization.  Indeed, they aim at integrating optimized functions for all  possible user needs, which are clearly not all exploited at the  same time. For that reason, there has been a trend to develop  reconfigurable hardware in the most recent SoCs, which are  still heterogeneous, but integrate blocks that may perform a  whole set of functions.  Hence, in order to maximize utilization of these flexible  resources, it is possible to map several tasks on a same  reconfigurable hardware. If dynamic reconfiguration has  already been well studied for SoC, the idea to have  simultaneous multiple applications running on the NoC has  currently not been investigated much. While [2] presents the  application mapping and scheduling problems as number 2  and 3 in the list of open NoC problems, simultaneous  multiple applications is only evoked as rather long term  perspective. The problem has been known for a long time in  the context of macroscopic networks and particularly grid  computing [3][4][5]. However, multiple parallel applications  are indeed becoming a reality in the context of NoC and  consumer electronics in general, with the user demand of  compound usages, which are quite well introduced by  [6][7][8][9]. Most works, nevertheless, focus on sharing the  interconnect infrastructure, and not so much the processing  elements  (PEs).  [6] and  [7]  rather  investigate  the  interconnect sizing for compound usages, while [8] and [9]  refine the scheduling of communications on the NoC for  multi-application. [10] and [11] work on centralized resource  management for fast platform reconfiguration, which is the  enabler for multi-application, with different strategies to  synchronize the elements. [10] introduces the notion of  migration points in the code of the applications, while [11]  introduces in-band signaling within the data-flows to define  stable points.  In this paper, we propose a distributed sequencing  protocol for resource sharing on the NoC platform, with “assoon-as-possible” asynchronous reconfiguration of the PEs  requiring no centralized resource management.   II. MOTIVATIONS & PROBLEM FORMULATION  A. User need for multi-application  With the increasing integration density of new CMOS  technologies,  there has been a natural demand  for  functionality integration, which led to the explosion of  functionalities in handheld and multimedia devices, ranging  from wireless basebands to multimedia codecs. As these  usages are not exclusive, they can be combined in different  use-cases that should be supported by the system. While  these multiple use-cases are defined by user actions, with  non-critical latencies for reconfiguration in tenths of second,  there are also cases of multi-application needs related to the  environment, which need seamless switching in order not to  disrupt the provided service. The wireless telecom domain is  a good example of such a need, on several aspects. On one  hand, it presents soft-handover between base stations, where  a second identical application is turned on before the first  one is terminated, when the mobile phone is changing zones.  On the other hand, it may present a vertical handover  between modes or standards, when the environment becomes  too noisy, or when an improved service becomes available.  B. Resource sharing  Mapping several simultaneous independent applications  on a network-on-chip implies on one hand to share the NoC  978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.33 233   links between applications [6]. On the other hand, it would  also be interesting to share the PEs between several  applications, insofar as they do not reach their maximal  utilization in a single application. For instance, signal  processing occurring in wireless telecom and multimedia  applications can rely on finite impulse response (FIR) filters,  fast Fourier transforms (FFT) and so on, which are processed  by similar hardware blocks in both domains. When these  atomic tasks are short enough to allow for idle phases while  meeting the real-time constraints, sharing the PEs for another  application requiring a lower computation load than the  inactivity of the first application may be considered.  Besides, given the increasing complexity of present-day  systems-on-chip, a centralized resource management for  scheduling the different applications and loading the new  configurations becomes more and more difficult  to  implement. Fine-grain reconfiguration needed for application  overlapping on the PEs is not accessible in this framework.  In order to enable co-existence of independent applications  in a scalable way, it would be necessary to develop  distributed multi-tasking capabilities on the PEs, including  synchronization protocols between PEs so that agents agree  on the application that should be processed during data  exchanges.  enough to confine the stalling of the upstream traffic given  the load allocation on the downstream shared resources.   D. Application model  Data-flow applications may be represented as task graphs  where each vertex, or “node”, is an elementary task, which  takes its inputs from the tasks which are related to it with an  incoming arc and sends its outputs to the tasks related to it  with an outgoing arc. In this work, we consider a one to one  mapping of the tasks on the hardware resources (a single task  is mapped on a single resource). Besides, since we  distinguish PEs from MBs, nodes are labeled accordingly. In  the following graphical representations of task graphs, square  nodes will be MBs, round nodes will be PEs.  This work focuses on the interleaving of independent  task graphs, not on the actual fine grain programming and  execution model of each task graph: each one could rely on a  different execution semantics (Kahn process networks,  Synchronous Dataflow, Dynamic Dataflow…) as long as this  semantics allows to define consistent phases, i.e. higher-level  coarse scheduling that would segment the application so that  it can be interleaved with other applications.  III. APPLICATION SEGMENTING & COMPOSITION  C. Application and architecture definition  A. Spatial segmenting of an application  The present study focuses on data-flow applications  mapped on heterogeneous NoC platforms. This class of  applications presents task parallelism, where, in the context  of systems-on-chip, a single task is mapped on a single PE.  In most cases these PEs are working in tight flow, i.e.  processing small amounts of data which do not require large  buffering in the inputs. For instance, image processing often  rely on pixel-based convolutions, even though execution is  triggered at a higher level (line or frame). This is quite  different from older results on concurrent scheduling for grid  computing which considered infinite buffering capacity [3].  In various cases nevertheless, there may be a need for a  larger buffering capacity, in the case of data-dependent  processing, where the previous data decoded determines a  parameterization of the current processing, or also in the case  of large data reordering, e.g. transposition of large matrices.  For that reason, the targeted data-flow applications present  specific PEs, which essentially have a large buffering  function, that we will call memory buffers (MBs).  Data of an application should not interfere with the data  used by another application. Hence, with simultaneously  running applications it is not possible to share a MB,  contrary to data processing, where the tight flow data of one  application could be stalled for a moment while data of  another application is being processed. Nevertheless, stalling  an application to let a resource process data from another  application will lead to stall all upstream resources running  the first application in sequence. This leads to a second usage  of the MBs, which should also absorb the upstream traffic of  an application during the downstream processing of a  concurrent application. They should consequently be large  The coexistence of multiple applications on the networkon-chip is possible only if one can stall the other applications  in MBs, while access to the shared resources is granted only  to the elected application. Hence, applications should be  segmented using MBs, so that portions of applications that  are exclusive can be limited in space.  In order to do so, the task graph should be enriched by  inserting MBs on well-chosen edges in order to create  separating sets of MBs, i.e. sets of MBs that would cut the  graph into disconnected components when removed. Each  disconnected component of the task graph, including the  MBs at the boundary will be called a “segment” in the  following. Experience has shown that the functionally  required MBs were often segmenting naturally  the  application. For instance, telecom applications may be split  in time domain, frequency domain, and bit domain.  B. Compound task graph  Given the mapping of the tasks of each application on the  hardware resources of the NoC platform, it is possible to  derive a composition of several task graphs. The result is a  directed graph, whose nodes are labeled with the set of  applications using the corresponding hardware resource, and  whose edges are labeled with an application, with potentially  as many edges between two nodes as there are concurrent  applications in the system. This differs from a wide range of  previous works on task graph composition, where the  scheduling would require ensuring a partial order on the  compound graph [5].  As the segments defined on each application are  considered together, they may intersect and form “multisegments”, defined as the union of the application segments  sharing at least a common node, as shown in Figure 1.   234 Memory buffer a1a1 specific resource a1a1 a1a1 a1+ a1+ a2 a2 shared resource a1+ a1+ a2 a2 a1a1 a1a1 a2a2 segment 2’ (a1)  a1+ a1+ a2 a2 a1+ a1+ a2 a2 a1+ a1+ a2 a2 a2a2 a2a2 a2a2 multi-segment 2 (a1+a2)  multi-segment 1 (a1+a2)  segment 2’’ (a2)  a2a2 a1a1 a1a1 a2a2 a2a2 a1a1 a2a2 a1+ a1+ a2 a2 a1+ a1+ a2 a2 multi-segment 3 (a1+a2)  Figure 1.  Labeled compound task graph for composition of two (similar) applications a1 and a2, with associated segementing  C. Inter-segment spatial task parallelization  For a well segmented system containing several  concurrent applications, considering that data of each  application may be stalled in the MBs at the boundaries of  each segment or multi-segment, the scheduling of the  application on each multi-segment is independent from the  others. Indeed, provided that sufficient data is present in the  input MBs of a multi-segment, and that sufficient free space  is available in the output MBs of the multi-segment, there is  no dependency outside of the multi-segment. As an example  multi-segments 1 and 3 could be running application a1  while multi-segment 2 would run application a2 (Figure 1).  Spatial task parallelization might sometimes benefit from  additional MBs to increase decoupling of the segments.  Nevertheless, this has a cost in area and might also increase  the end-to-end latency more than the real-time constraint.  D. Intra-segment temporal task parallelization  The co-existence of several concurrent applications on  shared resources is only possible if all applications are wellbehaving, and do not keep the shared resources forever.  Since the PEs within a segment are working in tight flow, it  is possible to starve them until they have processed all the  data that was already sent to them, and once they are free, to  reconfigure them on another task for another application.  This cannot be done at any time, however, insofar as the data  granularity may be different along the data flow, and a given  PE may be treating many small data items that would be  interpreted as a bigger chunk of data by its direct successor.   We propose, within the processing of the tasks on all PEs  of a segment, to identify “sessions” as global barriers within  the application, i.e. consistent subdivisions of the tasks  allowing stopping all PEs of the segment in a state where  they will drain all the pending data to the output MBs. In  other words, a session is a part of the application processing  on the segment that does not need to record a data history to  switch context and resume on another application.  When the underlying execution semantics of the task  graph is a variant of synchronous dataflow, then sessions are  naturally defined by a repetition vector for the SDF. For  Kahn process networks, a session could be a process firing  sequence that would return to the initial marking of the Petri  nets. A coarsening of the global barriers can be performed on  the segment, in order to find a trade-off between the amount  of reconfiguration between applications and the size of the  boundary MBs. Indeed, on one hand, given the real-time  constraints, reconfiguration of the PEs between applications  should be kept as low as possible, i.e. with the longest  possible sessions. On the other hand, the size of the MBs  should guarantee that data for the current session (and even  possibly several sessions) is present on the input MBs for  upstream isolation, and that results of the current session will  have enough space in the output MBs for downstream  isolation with other segments.  IV.  INTRA-SEGMENT TASK SYNCHRONIZATION  PROTOCOL  A. Scheduling issues for resource sharing  The basic idea for concurrent scheduling of several  applications is to rely on the data availability on the inputs,  so that the hardware resources switch to the corresponding  application as data are being pushed on the pipeline. This  idea of in-band signaling has already been used in [10], but  rather for deactivation of an application after data has been  processed, to suspend execution and notify a centralized  controller. In our proposal, the signaling would be used for  activation of an application along the task graph.  However, several issues may induce deadlocks, when  multiple applications are competing for several shared  resources that may be acquired in a different order by the  applications.  Two examples of deadlocks are introduced in  the following paragraphs.  1) Cyclic dependencies  Potential cycles within a task graph can be discarded for  session signaling by propagating messages only along a  spanning tree (acyclic) over the task graph. Nevertheless,  cycles may appear when composing task graphs: An  example of cycle in the compound task graph although both  applications are linear pipelines is presented in Figure 2.   On this example, if switching to an application is done  following the data arrival at each PE, there may be a  scheduling where the above shared resource is switched to  application a1, while the below shared resource is switched  to application a2, forming a deadly embrace that prevents  both applications to acquire the second needed shared  resource.  235   Memory buffer a1a1 shared resource a1+ a1+ a2 a2 specific resource a2a2 a1a1 a2a2 a1+ a1+ a2 a2 a2a2 a1a1 Figure 2.  Example combination of task graphs  leading to a cyclic dependency  2) Flow divergence  It would not be sufficient, in order to guarantee a possible  scheduling of the compound task graph, to forbid sharing a  resource that would induce a cyclic dependency in the  composition. There are indeed cases of composition where  the resulting compound task graph is acyclic, but may not be  freely  scheduled without  synchronization  between  applications. This actually can happen when there are several  unrelated shared nodes in the segment, i.e. nodes that are  neither predecessor nor successor of one another. One case is  when there are several input MBs on each application, which  could trigger two different seeds for the propagation of the  switching on an application, or when there is a flow  divergence (split or fork) in the graphs of the applications. It  is this latter case that is presented in Figure 3.   a1a1 a1a1 a1+ a1+ a2 a2 a2a2 a2a2 a2a2 a2a2 a1+ a1+ a2 a2 a1a1 a1a1 Figure 3.  Example nonlinear task graphs leading to a race condition  In this example, there is a potential race condition in the  propagation of the applications a1 and a2, and one can find a  particular scheduling where the fork in a1 would be  propagated first to the above shared node, and try to access  the below shared node after that, while this node would  already have switched to application a2. Each application  having acquired only one of the shared resources, neither one  would be able to propagate on the whole segment. Actually,  to guarantee a correct scheduling in this case, the in-band  information asking to switch to an application should be  propagated at the very same instant to both shared resources.   Although rare, this “isochronic-fork” property cannot be  easily enforced by quality of service on the network-on-chip,  due to the intrinsic communication latencies of the packets in  the NoC that depend on the path length and on the network  congestion.  3) Limitations of centralized synchronous scheduling  To enforce isochronicity of application switching, a  synchronous scheduling approach is most often used in  distributed data-flow systems. In this case, synchronous  scheduling is performed by a centralized controller, as in  [10] for instance. This solution, however, has a non236 negligible impact on the maximum sustainable load on the  platform and the real-time constraints.  First, the reconfiguration can only be performed once all  the PEs have finished processing their tasks (or sessions).  And because of the processing latency of all resources on the  segments, the time needed to flush the pipeline may be much  higher than the time needed to flush only the upstream nodes  of the pipeline. Meanwhile, these latter ones are idle, even  though another application could have been scheduled on  them. Concurrent dynamic scheduling of both applications,  on  the contrary, would enable “as-soon-as possible”  arbitration and reconfiguration between applications.  Then, the decision on the next scheduling by the  centralized controller requires knowledge of the states of all  PEs at each time, implying the emission of status signals by  each PE to the centralized controller, which needs to be  reactive to all these signals, in order not to lose too much  time in context saving and interruption handling before  taking a decision. This leads to a necessary use of either a  specific micro-controller, or reactive software architecture on  a more general purpose processor, which limits the reuse of  this processor for other functions.  Finally, depending on the session granularity on each  application,  the computation  load on  this centralized  controller may be so important that it cannot perform the  application scheduling on all the segments of the system.  Hence, in the case of complex systems-on-chip for a  multiplicity of demanding applications, there might be a  need for replication of the controllers to control sub-circuits,  and software synchronization of these controllers.  B. Compound task graph enrichment for concurrent  scheduling  Our proposal  is  to enforce  the naïve concurrent  scheduling of applications along their respective datapaths  by a few added virtual dependencies that would define a  consistent switching of the PEs to the same application. The  key element is that the decision to switch to an application  should be taken in a single point, and then propagated  downstream to all shared nodes in the segment.  Formally, it consists in defining a separating vertex in the  compound task graph, so that if it were removed, the graph  would be split into single application components upstream.  This node would be in charge of arbitration between  applications  transmitting  requests  from  the upstream  components, and defining the next application to schedule on  the downstream component. Clearly this requires virtual  dependencies that have no physical data transfers associated  to them, which will correspond to additional signaling  packets exchanged on the NoC between hardware resources.  C. Shared subset within a multi-segment  In order to create this separating vertex in the graph, that  we will call the “sequencing node”, the first step is to define  the single application components of the graph. They  correspond to the single application fanout cone of all the  input MBs of the application, i.e. all the single application  nodes for which a path exists from the input MBs without  crossing a shared node.      The shared subset of the graph on the multi-segment is  defined as the complementary component when all single  application components have been considered. It may also be  defined as the union of the fanout cones of all the shared  nodes of the multi-segment. More precisely, as there is a  finite number of shared nodes on each segment, it is possible  to find a minimal set of shared nodes which are predecessors  of all the nodes in the shared subset. This set of shared nodes  is a separating set of the compound task graph, which will be  designated as the “multi-applicative border” of the multisegment. An example of decomposition is shown in Figure 4.   a1component shared subset a1a1 a1a1 a1+ a1+ a2 a2 a2a2 a2 component a1+ a1+ a2 a2 a2a2 a2a2 a1+ a1+ a2 a2 a1a1 a1a1 a2a2 Border Figure 4.  Shared subset and border identification  D. Sequencing node selection  Once the shared subset and the multi-applicative border  have been identified, what is left to do for single point  synchronization is to modify the graph so that the new multiapplicative border is limited to a single node. Then, this  “sequencing node” will present a fanout cone that covers all  the shared subset: the decision to switch to either application  will propagate to all the shared subset. There are two options  to modify the graph so that the multi-applicative border is a  single sequencing node.   • Either  to  insert a vertex between  the single  application components and the shared subset, with  arcs going out of the single application components  redirected to it before entering the shared subset.  • Or to re-route some of the arcs pointing to the multiapplicative border, and add arcs within the border, to  remove the pointed shared nodes from the border  until a single shared node remains on the border.   The first solution actually corresponds to the insertion of  a new hardware resource that would behave as a PE, except  that it would not perform any real processing on the data, but  only propagate the switching decision as an in-band  signaling message. Clearly this option to have all data transit  through a control element is sub-optimal, but if one considers  a credit-based pulling of the data it is easy to propagate only  the control through the new hardware resource while the  credits sent by the successors on reception of the switching  decision would directly pull the data from the real producers.  Hence, the new hardware resource is only a small local  scheduler for the multi-segment. If relevant, the local  schedulers of all the multi-segments in the system could be  merged in a single entity on the chip responsible for  distributed scheduling of the applications on the platform.  This resembles the synchronous centralized scheduling  presented before, but the present solution is not synchronous,  since it only has to make the critical decision step for each  segment, based on a few local information, and let this  decision propagate asynchronously through NoC packets all  along the task graph of the selected application. In other  words, this centralized solution presents the advantage of  being much less constrained in terms of computation load on  the controller, and also allowing for “as-soon-as-possible”  asynchronous scheduling along the pipeline.  The second solution is even less costly in hardware, since  no new node is inserted in the graph: re-routing arcs only  consists in modifying the destinations of the signaling  packets in the NoC. Hence, the hardware cost is exactly the  same as the one of a naïve approach. In more detail, the  method consists in electing one of the shared nodes of the  multi-applicative border as the future sequencing node. This  elected node should be made a predecessor of all the other  nodes on  the border, so  that  its own fanout cone  encompasses all the shared subset. Besides, arcs should be  added from the maximal elements of all single application  components, so that the elected node has a visibility of the  data availability of all the possible applications that can be  scheduled on the shared subset of the multi-segment. Once  these arcs are added, the node becomes a sequencing node on  the graph, and will indeed arbitrate between applications that  are ready upstream and initiate their processing on the shared  subset of  the multi-segment, by  letting  this decision  propagate downstream following the data-flow.  E. End of propagation on the shared subset  Since there is not necessarily a total order on the shared  nodes of a multi-segment, the way the decision propagates  on the shared subset is not identical between applications.  There might be corner cases where the decision to schedule a  second application may follow so closely the decision to  schedule the first application at the sequencing node level,  that the first decision does not have time to propagate all  along the shared subset. Hence, the shared nodes of the end  of the first application data-flow might occur sooner in the  pipeline of the second application data-flow, and be switched  to the second application before the decision to switch to the  first application reaches them, leading to a deadlock.  It is very unlikely to face this fast propagation issue when  functional data need to be processed by the sequencing node.  Nevertheless, if there is no way to ensure the propagation of  the decision to the whole shared subset, additional arcs  should be added to notify the sequencing node at the end of  the propagation. These arcs do not bear exactly the same  meaning as the previously defined arcs, insofar as they create  a cycle in the propagation of session signaling. Therefore,  the added arcs are not interpreted as participating to the  required inputs to switch to an application at the beginning of  a session, but rather as required before the end of the session.  Only upon reception of these signals the sequencing node  will be able to decide to schedule a new session on an  application. These arcs will be called “secondary session  requests”, while the arcs needed for session initialization will  be called “primary session requests”.  237   F. Flow-control from MBs  The previous paragraphs were rather focusing on the PE  scheduling considering the applications for which a session  could be scheduled on the segment. Nevertheless, the actual  opportunity to schedule a session depends on two things: the  availability of input data in the input MBs, and the  availability of free storage space in the output MBs. As the  application is pushed with the data, input MBs should initiate  communications when sufficient data is present in the MB to  handle a full session of the segment. However, flow-control  from downstream is not described in the initial task graph.  Therefore, arcs should be added from the output MBs to the  first nodes in the segment, on which signaling messages  should be sent when sufficient storage space is available in  the MB to absorb all data produced during the session. It  would also seem that these newly added arcs are creating  cycles in the initially acyclic task graphs, but actually, only  the newly added arcs going out from the output MBs are  bearing information. The incoming arcs on the output MBs  are obviously not intended to ask the MBs to switch to the  application, since MBs are dedicated to a single application.  Actually, the cycle is initiated on the outgoing edge signaling  the availability of free space, and the final arc describing the  real data transfer is, in the dependency sequence, a successor  of this initial arc, which, so to say, “acknowledges” the  availability of the output MB with the data transfer.  G. Distributed sequencing protocol  Considering all the graph transformations that were  described in the previous paragraphs, it is possible to derive  a final compound task graph for the multi-segment, where a  sequencing node has been elected among the shared nodes,  and which includes additional arcs to enforce the desired  scheduling on the multi-segment, as shown in Figure 5.   sequencing  node a1a1 a1a1 a1+ a1+ a2 a2 a2a2 a2a2 a2a2 a1+ a1+ a2 a2 a1a1 a1+ a1+ a2 a2 a1a1 a2a2 Figure 5.  Sequencing node and added dependencies  In this example, the dashed arrows are new dependencies  that were added to create a sequencing node, and to allow for  downstream flow control. The dotted arrows are the  (optional) arcs that ensure the end of the propagation of the  sequencing node’s decision on the entire shared sub-graph.  As stated before, they are not used for session initialization  but rather for session termination.  As regards effective messages conveyed through the  NoC, the solid arrows correspond to a three-step exchange:  238 • First, a session request with a source identifier and an  application identifier, which notifies that the sender is  ready to send data from the given application to the  receiver.  • Then, the receiver acknowledges that it is also ready for  this application with the emission of the first credits to  start pulling the data from the sender.  • Finally, the tight-flow credit-data exchange runs between  sender and receiver until end of session is reached by both.  The dashed and dotted arrows are actually the same  message as the first step of the above exchange, with no  effective data transfers after that. The interpretation of a  dependency as a “primary session request” required for  session initialization (solid or dashed arrow ) or as a  “secondary session request” required for session termination  (dotted arrow) is not encoded in the NoC message itself, but  programmed in the receiver as a function of the source  identifier.  Once a PE has received all the required primary session  requests on a given application, this application may be  scheduled, and arbitration is performed between applications  that are ready to be scheduled. Actually, because of the graph  transformations that were performed, it is only in the  sequencing node that effective arbitration will be performed.  Indeed, all other shared nodes will miss at least one session  request for all applications before they receive the session  request that is propagated from the sequencing node that will  trigger their switching on the scheduled application.  An important point is that the operation to trigger a  session on the task associated to an application is the same  for every PE, i.e. all PEs include the same hardware.  Therefore, the sequencing node selection is purely defined  by software. As a consequence, the mechanism can be used  even to allow for concurrent applications that were not  known at design time.  V. NETWORK INTERFACE MICROARCHITECTURE  A. Insertion within the CCC of PEs  At the PE level, the proposed protocol is implemented  within a Communication and Configuration Controller  (CCC) [12]. This CCC allows for distributed dynamic  reconfiguration of the PE without needing the intervention of  an external controller. When requested  to execute a  configuration that is not loaded, the CCC is able to fetch the  needed configuration and resume execution once it has been  received. The configuration sequence is described in a small  microprogram describing the task to perform for a given  application, which is here enriched with a specific barrier  instruction, “WAIT_SESSION”, which stalls the processing  until a new session is scheduled for this task.  As several applications may be run concurrently on the  PE, there are actually as many microprograms as there are  concurrent tasks that should be executed. These microprograms all include a number of “WAIT_SESSION”  barriers, and  the  local  runtime execution of  these    microprograms switches between all of them only on these  barriers, knowing the current task that should be processed.  A Multi-Task-Manager block (MTM) is added on top of  the microprograms, to handle the session request messages  that are exchanged within the NoC. When enough incoming  session requests have been received for an application, the  outgoing session requests are forwarded downstream, a  session on the corresponding task is started, and the local  runtime notifies  the end of  the session when  the  corresponding  microprogram  has  reached  a  WAIT_SESSION barrier. The resulting network interface  architecture for PEs is described in Figure 6.   SR_in SR_out MTM MTM Task0 SR tables Task1 SR tables current_task_id session_begin session_end Config request Config CFM Task0 µP Task1 µP mux / mux / demux demux Config Exec/End Config slotid Exec/End Local runtime execution Credit Data Credit Data NI flow flow control control Data Data Core Slots CoreCore Figure 6.  Multi-Task Manager (MTM) integration in the CCC of a PE  For each concurrent task that may run on the PE, the  MTM contains session  request descriptor  tables. A  configuration part describes the input and output session  requests  that  should be  received and  sent. The  SR_MASK_begin table encodes the source identifiers of the  required incoming primary session requests, while the  SR_MASK_end table encodes the source identifiers of the  required incoming secondary session requests. A SR_OUT  table describes a list of data pairs, with the path to the  receiver of the session request, and the source identifier  (from the receiver’s point of view) of the session request.  The status part of the session request descriptors contains  a two-place FIFO queue that describes the session request  messages  that were already  received  for  this  task,  respectively for the pending session (or the session to start if  no session is running) and the session to come after. This  resembles older methods for grid computing, which used  arrived queues for predecessors of the same task, and ready  queues to arbitrate between tasks [4].  The session is started when the queue output contains  SR_MASK_begin, and may end when all data has been  processed and the queue output is equal to SR_MASK_end.  As the session starts, the MTM iterates over the list of  SR_OUT and sends the corresponding outgoing session  request messages. As the session ends, the queue is shifted.  B. Insertion within the memory controllers  As regards the memory controllers handling the MBs  [13], they should send the session requests corresponding to  the availability of free space for the incoming flows, and the  availability of enough data for the outgoing flows. In the  case of simple rotating buffers with read and write pointers,  the used and empty spaces are easily computed by a pointer  difference, and the comparison with thresholds programmed  by the system host triggers the emission of the corresponding  session requests. For more complex reshuffling patterns, the  number of available data is computed within the firmware  that describes the reshuffling patterns. Contrary to what was  described for the PEs, there is no replication of the tables and  micro-programs for each task, since each MB is dedicated to  a unique application.  The behavior of the FSM controlling the incoming and  outgoing session requests is a bit different in the sense that  the emission of the outgoing session requests is not  conditioned by the reception of incoming session requests,  but rather by software thresholds on the used and empty  spaces. Hence, the incoming session requests are never  primary, so SR_MASK_begin would be equal to 0. For the  incoming side of a MB, SR_MASK_end would describe all  the arcs corresponding to a functional data transfer in the  graph. For the outgoing side, SR_MASK_end is also equal to  0 since no arc can point to the outgoing side of a MB.  C. Programming from the host controller  In order to set-up a new application, the host should  configure first the MBs to size them for a session, then the  micro-programs describing the sequence of configurations  for the task to be processed by each PE, then the session  request descriptors of the application for all PEs and MBs to  describe the enriched task graph, then the activation  thresholds of the MBs for the application, and finally should  enable the flow arrival at the input MBs. The PEs will then  automatically fetch their configurations and concurrently  schedule sessions for the application until they all reach the  end of their own tasks.  VI. RESULTS  The proposed concurrent multi-applicative framework,  protocol and architecture were integrated for two concurrent  applications within a 4G SDR NoC based platform, made of  two chips containing respectively 6 NoC units (in a Xilinx  Virtex 5 FPGA) and 22 NoC units, of which 6 4-buffer  memory controllers, 15 PEs and 1 host processor. This latter  chip  [14] was  fabricated  and  tested  in  the  STMicroelectronics CMOS65 LP technology.  The area cost for the multi-application architecture is of  about 3.5 kGate for the MTM block, and an overhead of  about 5 kGate for the duplication of the microprograms in  each CCC.  Nevertheless, these relatively small area figures  are to be mitigated by the fact that the most efficient  interleaving is only attainable if configurations for both  applications may be stored within the core slots, in order to  have a minimal context switch time without the need to fetch  the configurations at each session barrier. As regards  239   performance of the distributed scheduling algorithm on the  multi-segments, the establishment of a session on all PEs of  a segment is in the order of ten cycles per PE to propagate,  leading to a total of about 200 ns for a ten-PE segment,  including  the NoC communications. This should be  compared to the latencies of a centralized controller handling  one or more interrupts from the PEs that is more in the range  of 500ns.  The proposed mechanism was used within the platform  to operate on the 3GPP Long-Term Evolution (LTE) 4G  telecom standard, to increase the utilization of the FFT/iFFT  PEs for MIMO OFDM modulation. The real-time constraints  for this are that 14 OFDM symbols should be transmitted  from and to the mobile terminal within one time slot, i.e.  fourteen 2048-point iFFT for Tx and fourteen 1024-point  FFT for Rx every millisecond on each antenna. Using the  FFT engines on the chip, the computation time of a single  2048-point iFFT is 31 µs, and the computation time of a  single 1024-point FFT is 22 µs.  As the MIMO version uses two antennas, there were  initially two iFFT engines for Tx modulation, and two more  FFT engines for Rx demodulation of the telecom protocol.  On this 4-FFT-engine mapping, this leads to a utilization of  43% on each Tx FFT-engine, and 31% on each Rx FFT  engine. Of the two options to increase this utilization, it is  not possible to use a single FFT core for Tx and another one  for Rx, since the flows on both antennas are related to each  other, On the contrary, as Tx and Rx streams are decoupled,  it is possible to have a single FFT per antenna, handling both  Tx and Rx streams.  Hence we defined two separate applications for Tx and  Rx, which compete for the FFT/iFFT engines of each  antenna. Using the proposed protocol, it is possible to have  the 14 iFFTs and 14 FFTs running on a single FFT/iFFT  engine, for a total utilization (including communication and  reconfiguration overhead) of 80%, still meeting the real-time  constraints. Considering a circuit where only two FFT/iFFT  engines would be instantiated, this leads to a power saving of  12 mW (i.e. the idle power of two FFT/iFFT engines), and a  total area reduction of 1.5 mm² in CMOS 65nm.  VII. CONCLUSION  As the demand for simultaneous support of multiple  applications running concurrently on a single chip is  continuously growing, and in the context of the emergence  of heterogeneous NoC-based manycores SoCs, we proposed  in this paper a methodology and a network interface  architecture adaptation allowing to share efficiently between  several applications the PEs that would not have a high  utilization in the context of a single application. This  methodology relies on a distributed concurrent scheduling of  the tasks of multiple applications, which does not require the  intervention of a centralized scheduler. In order to overcome  potential deadlocks  in  the concurrent scheduling, an  algorithm to introduce virtual dependencies between PEs  was proposed. This algorithm prevents the system to perform  conflicting scheduling choices in different places by a  minimal increase in synchronization between PEs. The  proposed network interface architecture adaptation to support  this concurrent scheduling protocol has an acceptable area  cost and as it is independent of the considered applications  and fully programmable by software, it allows combining  any applications together as long as all real-time constraints  can be met. This multi-applicative framework for resource  sharing was integrated into a 4G SDR wireless telecom  baseband processing chip [14], allows for better utilization of  PEs for current most advanced telecom protocols as LTE,  and is expected in the long run to enable a seamless vertical  handover between telecom standards, according to the  available services in the air interface.  "
Network-on-Chip Architectures for Neural Networks.,"Providing highly flexible connectivity is a major architectural challenge for hardware implementation of reconfigurable neural networks. We perform an analytical evaluation and comparison of different configurable interconnect architectures (mesh NoC, tree, shared bus and point-to-point) emulating variants of two neural network topologies (having full and random exponential configurable connectivity). We derive analytical expressions and asymptotic limits for performance (in terms of bandwidth) and cost (in terms of area and power) of the interconnect architectures considering three communication methods (unicast, multicast and broadcast). It is shown that multicast mesh NoC provides the highest performance/cost ratio and consequently it is the most suitable interconnect architecture for configurable neural network implementation. Simulation results successfully validate the analytical models and the asymptotic behavior of the network as a function of its size.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip Network-on-Chip Architectures for Neural Networks   Dmitri Vainbrand and Ran Ginosar  Technion—Israel Institute of Technology, Haifa, Israel  Abstract  Providing highly flexible connectivity is a major  architectural challenge for hardware implementation  of reconfigurable neural networks. We perform an  analytical evaluation and comparison of different  configurable interconnect architectures (mesh NoC,  tree, shared bus and point-to-point) emulating  variants of two neural network topologies (having full  and random exponential configurable connectivity).  We derive analytical expressions and asymptotic  limits for performance (in terms of bandwidth) and  cost (in terms of area and power) of the interconnect  architectures  considering  three  communication  methods (unicast, multicast and broadcast). It is  shown that multicast mesh NoC provides the highest  performance/cost ratio and consequently it is the most  suitable interconnect architecture for configurable  neural network implementation. Simulation results  successfully validate the analytical models and the  asymptotic behavior of the network as a function of its  size.  I. INTRODUCTION  The inherent parallelism of multi-processor VLSI  systems on chip (SoC) enables the efficient emulation  of biological neural networks and the construction of  artificial neural networks for complex tasks such as  pattern  recognition. When  the  structure  and  connectivity are implemented rigidly in hardware, the  emulated neural networks suffer of limited flexibility  and functionality  [6],  [7], requiring redesign if any  connectivity or function needs to be changed. To  overcome  these  limitations, we  seek a SoC  architecture  that  enables  programmable  and  reconfigurable neural networks. Such architecture  could serve as a generic medium for neuroscience and  machine learning research, enabling emulation of  arbitrary neural network  topology and support  dynamic connectivity changes as a result of training.  A key issue in emulating reconfigurable neural  network is the complexity of dynamic and flexible  neural connectivity. We investigate and compare  several interconnect architectures (mesh NoC, tree,  bus,  point-to-point)  and  three  packet-based  communication methods  (unicast, multicast and  broadcast) and study how they support configurable  communications for spiking neural networks.  We  show that mesh NoC using multicast is the most  suitable architecture for a wide range of neural  network topologies.   Background and related work are presented in Sect.   II. Theoretical cost and performance analysis is  provided in Sect.  III, followed by simulations in Sect.   IV   II. BACKGROUND AND RELATED WORK  A. Spiking Neural Networks  Artificial neural networks  [5] have evolved from  McCulloch-Pitts threshold on/off neurons, through the  more biologically realistic continuous activation  networks representing firing rates, to more accurate  spiking neural networks, which also reflect inter-spike  spatio-temporal relations, and which are the subject of  this paper.   B. Neural Network Implementation  Parallel processors and special purpose hardware  are most  suitable  for  fast  emulation  of  computationally intensive neural networks, for both  real time applications and for very large tasks.  Previous  highly  flexible  and  configurable  implementations were limited to emulating a small  number of neurons  [8], while systems of many  neurons offered limited flexibility   [6],   [7], and  flexible systems of many neurons required very large  implementations  [9], due to the quadratic complexity  of a fully connected design.   Hierarchical  and  simplified  communication  methods have been used to mitigate the complexity of  full connectivity. Shared bus neural network  implementations typically provide efficient event  driven communications, whereby only the address of  the spiking neuron is broadcast to all bus elements.  Non-arbitrated shared bus, which detects and ignores  collisions, was described  in   [10]. Other works  employed arbitrated buses using the Address Event  Representation (AER) asynchronous protocol  [11]–  [13]. That representation is useful for either point-topoint connections  [11],  [12] or broadcasting over a  shared bus  [13]. The former case requires expensive  communication network while parallelism is quite  limited in the latter case.  SoC implementations of neural networks using  NoC have been proposed by  [14]– [16]. NoC are  particularly attractive for spiking neural networks, as  they  facilitate  parallelism,  reconfigurability,  independence of the network topology, and network  expandability. A small 2D torus network with four  processing elements (neurons) per routing node is  described in  [17]. The architecture supports deadlock  free X-Y routing, and uses wormhole packet switched  communication. The packet combines the outputs of  four logical neurons and is sent to the router of the  next layer of the layered neural network router, thus  978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.23 135       enabling only layered structure and not allowing  arbitrary communications. That NoC architecture is  less applicable to spiking neural networks. Scaling is  also limited.  An FPGA-based mesh NoC using XY unicast  routing for clustered neural network has been  proposed in  [19]. As shown in this paper, unicast  communications may be a limiting factor in neural  network implementations.  A large scale multi-chip spiking neural-network  system was reported in  [18]. A hierarchy of seven  chips, 20 processors per chip and 1000 spiking  neurons per processor is contemplated. Small AER  packets are exchanged using multicast NoC. As the  ability  to  emulate  arbitrary neural network  connectivity depends strongly on the size of routing  tables, the paper addresses optimization techniques to  reduce table sizes.  While  [18] employs a mesh,  [17] uses 2D torus and   [19] describes a more complex hierarchy. While  [17]   and  [19] rely on unicast,  [18] chooses multicast.  While  [17] uses wormhole routing,  [18] and  [19]  employ short AER packets. Packets in  [18] consist of  source address and  [19] uses destination address. This  variety of approaches raises the question of which  architectural choices are preferred over others, and  whether indeed NoC is the appropriate solution for the  emulation of large scale neural networks. This paper  attempts to address these questions, by an analytical  evaluation and comparison of different interconnect  architectures emulating different neural network  topologies.  Previous  cost  /  performance  characterizations employed either analytical  [22],  [23]  or empirical  [24],  [25] approaches. We adopt the  method of  [20], for its generality and applicability to  NoC and non-NoC architectures, as explained in the  following section.   III. NETWORK ARCHITECTURE  This work investigates the emulation of neural  networks on many processors inside a single SoC.  Several neurons can be allocated to each processor.  Spikes transmitted between neurons that are assigned  to the same processor are transferred internally within  the processor. Spikes between neurons assigned to  different processors must be transferred over the  interconnect. In the following, Section  III.A defines  cost and performance metrics. In Sections  II.B–E we  consider four alternative implementations for how the  processors are interconnected, and evaluate how the  interconnect architecture  impacts  the cost and  performance of configurable neural networks. In  Section  II.F we summarize the results. We compare  the following interconnect architectures:  NoC_Mesh: a mesh network-on-chip.  NoC_Tree: a tree network-on-chip.  136 AER_BUS: a broadcasting AER protocol on a  shared bus.  P2p: a point to point reconfigurable connection  matrix.  We consider  three different communication  methods (""casts""): unicast (UC), multicast (MC) and  broadcast (BC). Two types of neural network models  are investigated: Hopfield network  [4], which requires  full connectivity of the neurons (each neuron sends  each spike to all other neurons), and Randomly  Connected NN  (RNDC), defined by  random  exponential local connectivity (each neurons sends its  spikes to a small set of neighbors). Both NN models  incur uniform traffic. The former represents maximal  connectivity bounds, whereas the latter, adopted from   [1], represents more realistic scenarios.  A. Cost and Performance Metrics  We use a metric similar to  [20]. Cost is associated  with interconnection area and power dissipation  (another cost item, not discussed in this paper, relates  to the size of memory needed for routing tables).  Performance is evaluated by effective bandwidth,  throughput and maximal spiking frequency of the  neurons. The maximal theoretical bandwidth is:  BW nn arch cast ( , , ) = ( ) w i f ( ) i ∑ ∈ i { links } TotalDist nn arch cast ( , , )  (1)  were w(i) is the width of link i,  f(i) is its switching  frequency, and TotalDist(nn,arch,cast) is the average  total distance traversed by each neural spike (going to  all its destinations), measured in the number of hops;  arch∈{ NoC_Mesh, NoC_Tree, AER_BUS, p2p},  cast∈{UC, MC, BC}, and nn∈{Hopfield, RNDC}.  While the maximum BW indicates the possible level  of parallelism for a given architecture, communication  method and neural network, it does not take into  account inter-packet interactions and variable latency.  If the network is allowed to operate at maximum BW,  it stalls to a halt due to congestion. It is shown  empirically in Sect  IV that if the network operates  below  the congestion  threshold,  there are no  congestion effects at all, and we can assume fixed  small router delays. This is modeled by an empirical  architecture-specific utilization factor Uarch, defining  the effective bandwidth:  BW nn arch cast ( , , BW U (2)  = × ) arch eff For topologies with a constant number of wires per  link w and constant frequency, (2) becomes:  w TL ⋅ (3)  BW eff = arch f U arch arch TotalDist  is the total number of links in the  where  TL arch architecture. The area cost of the architecture is:  A W w i l i ( ) ( ) = arch p ∑ ∈ i { Arch links } (4)                  where l(i) is the length of link i and Wp is the wire  pitch for a given technology. We disregard router  delays, since they do not scale with network size and  thus the link delay is also the link cycle time:  R C l 2 0 0 cycleT = (5)  where R0 and C0 are  the wire resistance and  capacitance per unit length, respectively, and l is the  average  link  length. Thus  the maximum  link  frequency is:  2 0 0 1 1 arch cycle f T R C l = = (6)  Power dissipation is estimated as dynamic power  dissipated on the link and gate capacitances:   P C i f i V U ( ) ( ) { } 2 DD links arch arch arch i ∈ = ∑ (7)  The maximum spiking frequency is determined by  the biologically inspired neuron refractory period  Trefractory, a ""blanking"" time following a spike during  which the neuron cannot fire again:  1 ,max spike refractory f T = (8)  In biological cortical neural networks, the refractory  period and the average synaptic (axonal) delay are  typically in the same range (2-10ms), which in our  case equals the packet end-to-end delay:   refractory Ax cycle T T T Dist ≅ ≅ ⋅ (9)  where Dist is the average distance (in number of  hops) between two connected neurons.  In the special case of unicast NoC implementations,  each spike results in a succession of packets, one per  destination. The delay between issuing the first and  the last packets of the same spike is  avg cycle N T where avgN  is  the average number of  spike  destinations for a given neural network. We arbitrarily  choose a minimal time between successive spikes  fired by the same neuron of 10 avg cycle N T spike interval should be larger than the time of  sending all packets of one spike) and the maximal  firing frequency for unicast NoCs combines both  issue and refractory delays:   (the inter( ) ,max 1 min 10 , 1 10 UC spike cycle avg cycle avg cycle f T N T Dist N T = ⋅ ≈ (10)  For multicast and broadcast, only one packet is  issued per each spike regardless of its number of  destinations. The maximal firing frequency for  multicast and broadcast NoC is thus implied by (9):  1 MC BC | spike ,max cycle f T Dist = (11)  The above definition of maximal firing frequency  reflects the basic property of refractory time in neural  networks in the presence of the geometrical and  electrical delays of the implementation. Note that the  maximal firing frequency does not necessarily match  the maximal NoC bandwidth. Moreover, as shown  below,  some of  the  studied  implementations  (especially broadcasting NoC and BUS) are jammed  when all neurons fire constantly at their maximal  frequency.  BWeff of (3) is the average rate at which an entire  network absorbs new massages. The average rate at  which a single processor can feed spikes into the  network is obtained by dividing into the number of  processors  pn :  p out , eff p BW f n = (12)  The fact that the maximal firing rate fspike,max may be  different than fp,out is expressed by a factor K,  indicating the degree to which a given implementation  enables neurons to operate at their maximal rate and  providing a  figure of merit  for comparing  implementations: p out , ,max spike f K f = (13)  If K>1, multiple logical neurons can fit efficiently  into a single physical processor. If K<1, fspike,max is  unachievable. Finally, the cost-performance ratio R is:  ,eff arch arch arch BW R A P = ⋅ (14)  B. Mesh NoC  A mesh NoC comprises n processors and n routers  arranged in a n n× mesh (Figure 1).                                                  1) Emulating Hopfield NN on a Mesh NoC  Consider a Hopfield (fully connected) NN emulated  on a unicast mesh NoC. For simplicity of the analysis,  assume that each processor emulates a single neuron.  For each spike, a neuron sends a packet to all the  other neurons (n-1 packets). The total number of hops  traversed by one spike is the sum of distances  between this neuron and all other neurons:  2 Hopfield UC Mesh , ( 1) ( 1) 3 Substituting (15) and (16) into (3) yields:  3 w Mesh TotalDist n Dist n n = − ⋅ = − ⋅ (16)  Hopfield eff Mesh UC , , n 1 and the average frequency of feeding new spikes  from a single processor is:  NoC NoC BW f U = + (17)  ( ) Hopfield eff Mesh UC , , Hopfield p out Mesh UC , , , n 3 w 1 NoC NoC BW f f U n n = = + (18)  Following (10) we conclude:  f ,max 10 n Comparing the results of (17) and (19) we can  derive:  UC spike NoC f ≅ (19)  ( ) Hopfield p out UC , , , max 30 1 NoC UC spike f wU K O n f n = ≅ = (20)  This result implies that the UC mesh NoC does not  offer sufficient bandwidth to emulate a Hopfield NN  at  the maximal  firing  frequency. Only about  ( ) nK O n = neurons may fire close to their maximal  rate and the remaining other neurons will fire at a  negligibly low rate. Alternatively, all neurons could  fire at a 1 n fraction of their maximal rate.   Turning now to multicast and broadcast NoCs, they  are essentially the same for Hopfield NN, as each  spike is transmitted to n-1 other neurons. The number  of hops traversed per spike are the number of edges in  the mesh spanning tree,    TotalDist Hopfield MC BC / n≅  (21)  Thus:  Hopfield eff MC BC , | NoC MC BC | 1 2 1 (1) BW wf U O n       = − =  (22)  Intuitively, one packet, sent to all neurons, utilizes  the entire network. Thus, the network can handle only  one spike at a time. The average NoC input frequency  is:  MC BC / Hopfield eff MC BC , / p out , , 2 Hopfield NoC NoC BW wf U f n n = ≅ (23)  The fact that the source neuron issues one packet  per spike enables a much tighter bound for spiking  frequency, following (11):   1 ,max 3 2 NoC spike refractory f f T n = ≅ (24)  Comparing (24) and (23) yields:  4 wU ( ) Hopfeild Mesh MC BC , / 1 3 n The remaining parameters depend only on  the  interconnect topology, and are the same for all  communication methods:  NoC K O n ≅ = (25)  ( ( ) ) ( ) 0 2 0 0 0 2 0 0 2 W lw n 1 C lw n 2 1 1 P wU 2 1 mesh p mesh mesh mesh NoC dd A n C n f R C l P n n P V (cid:1) R l = − = − = = − (26)  The asymptotic metrics for Hopfield NN emulated on  a mesh NoC are summarized in Table 1  Table 1: Hopfield network emulated on a mesh NoC  Metric  UC  MC  BC  BW  ( ) 1O n  O(1)  O(1)  Area  Power  O(n)  O(n)  Spiking Frequency  O(1/n)  ( ( ) ) 1O n  ( ) 1O n  K  1O n  2) Emulating RNDC NN on a Mesh NoC  For RNDC model, the probability of having a  connection from neuron a to neuron b is defined  similarly to  [1]:  ( ), 2 p a b ( , ) 2 where D(a, b) is the Euclidean distance between a  and b, λ is a spatial connectivity constant, and  C=Nlinks= ||p(·)|| is  the average number of connections  per neuron. The mean distance between  two  connected neurons is:  1 D a b C e λ πλ − = (27)  2 2 2 2 2 x y , 2 2 x y Dist x y e dxdy λ λ πλ − + = + = ∫∫ (28)  When emulating RNDC neural network on a  unicast mesh NoC, a neuron sends individual packets  to all of its C destinations. The average total number  of hops for each spike packet is:  TotalDist Dist Nlinks 2 (29)  Thus, the bandwidth (3) of RNDC emulated on  unicast mesh NoC is:  Cλ = ⋅ = 138                                                     ( ) RNDC eff UC , 1 NoC NoC w n n BW f U Cλ − = (30)  Simulations in  [1] provide an example of small  neural microcircuits (~1000 neurons)  that reach  optimal performance with:   3 2 n C n λ≅ ≅ (31)  Substituting these values for unicast mesh:   6 RNDC eff UC , NoC NoC BW w n f U ≅ ⋅ ⋅ (32)  Using (28) and (31) in (10) we obtain:  1 f ,max 10 C T 10 UC spike NoC cycle f n = = ⋅ (33)  Also,   RNDC eff UC , UC p out , 5 6 1 1 NoC NoC BW w f f U n C n O n λ −  −     = =  =       (34)  Similarly, the K ratio is:  UC p out , ,max 3 1 10 C 1 1 1 UC NoC UC spike f K wU C f n O O n For RNDC NN emulated on a multicast NoC, the  total number of hops is approximated by the length of  the linear path, traversing a distance of 2λ to the first  destination and then adding one hop per destination:  TotalDist 2 (36)  λ   λ   −     = =  =         = (35)  RNN Mesh MC , = + C λ Thus, the bandwidth (3) of RNDC implemented on  a multicast mesh NoC is:  ( ) ( ) ( ) ( ) ( ) RNDC eff mesh MC , , 3 2 1 2 2 1 2 NoC NoC NoC NoC w n n BW f U C w n n f U O n n n λ − ≅ + − = = + (37)  Using (28) and (31) in (11) we obtain:  f f ,min 3 2 2 MC spike NoC NoC f n λ = = (38)  Following (37):  ( ) ( ) MC p out , 2 1 1 2 NoC NoC w n f f U O n C n λ −  =      = + (39)  Finally the K ratio is:  ( ) ( ) MC p out , , max 6 2 1 2 2 1 MC NoC MC spike w n f K U f n C O O C n λ λ  λ   + λ  − = = +  =      = (40)  Another practical case of random connectivity is the  Locally Connected NN (LCNN) where:  ≅ (cid:2)  C n { } LCNN RNDC λ = (41)  Thus, since the connectivity is practically bounded,  ( ) ( ) ( ) MC p out , , 2 1 1 2 NoC NoC C n w n f f U O n C λ λ − = → + (cid:2) (42)  ( ) ( ) 1 4 4 (1) 3 2 MC NoC NoC w n K U wU O n C λ λ − = ≅ = + (43) Thus, the multicast mesh NoC offers sufficient  bandwidth for emulating any size of locally connected  NN. Both maximal firing frequency and the average  NoC input frequency do not decrease when the  network size grows. For instance, a NoC with one  logical neuron in each processor is only 75% utilized  even if all neurons fire constantly at their highest  frequency.  With broadcasting NoC the spike is sent to all  destinations regardless of connectivity pattern and the  performance is independent of the neural network  topology. Thus, all BC performance parameters are  similar to those calculated for the Hopfield NN model  (equations (22)–(25)). The remaining cost factors  depend on neither  the NN  topology nor  the  communication method. They are the same as in (26).   Setting  NoCU constant for all communication  methods (UC, MC, BC) we achieve same operating  frequency and power consumption but different levels  of throughput: MC provides the highest throughput  using  the same power. Table 2 summarizes  asymptotic results for the RNDC model implemented  on a mesh NoC using different communication  methods (UC,MC,BC).  Table 2: RNDC with  3 2 , n C n λ≅ ≅  on a mesh NoC   Parameter  UC  MC  BC  BW  ( )1 6O n ( ) O n  O(1)  Area  Power  O(n)  O(n)  Spiking  Frequency  ( ( ) ) 1O n  ( ) 31O n  ( ( ) ) 1O n  K  31O n  ( )1 6O n − 1O n  C. Tree NoC   Consider NoC with a binary tree topology, having n  physical neurons at the leaves and n-1 routers as in  Figure 2. The results can also be generalized to trees  of higher degrees. The diameter of the binary tree is  2log2n. The total number of links is:  TL 2 n 1 w ( ) tree NOC , = − (44)  139                                                                  In a Hopfield (full connectivity) neural network  with unicast communications, half of all traffic passes  through the root, resulting in a serious congestion.  This issue has been addressed by fat-trees (FT)  [2], in  which link bandwidth increases when going upward  to the root, maintaining more uniform traffic. We  employ the approximation of FT of  [3] (Figure 3),  enabling nodes with small constant degree at the cost  of more routers (nlog(n)/2) and more complex  connectivity (nlog(n) links). Such a FT introduces  multiple alternative paths, providing additional  bandwidth for short massages.  Figure 2: Regular binary tree with 8 leaves  Based on the mesh NoC analysis of Sect.  III( B), we  conclude that Hopfield NN is better emulated using  either broadcast or multicast rather than unicast  communications. Broadcast and multicast perform  identically emulating Hopfield NN, and result in  uniform traffic on the regular tree. For RNDC NN  topology,  recalling  the exponential connection  probability of (27), we investigate the question  whether a FT is needed. The spikes sent from a  neuron which is connected to other neurons at  distance D traverse only a sub-tree of k=log(D) levels,  or k+1 levels when connected to other neurons at  distance 2D (a less likely case). The ratio of  bandwidth required in levels k+1 and k is reflected by  the ratio of the probabilities:  BW k ( + 1) BW k ( ) = p D (2 ) p D ( ) = − 2 D λ e − D λ e − D λ = e = e K − 2 λ  (45)  In a regular binary tree, the link capacity reduces by  half at every level,  e k λ− 2 0.5 , leading to:   = λ ) ( − = k log 0.52 (46)  Thus, for connectivity with average distance 2λ, a  sub-fat tree of height  k ∼ log  provides the  λ 2 ( 2 ) required communications, and the bandwidth of a  normal binary tree suffices at the higher levels of the  tree. In conclusion, the desired architecture combines  a fat-tree at the bottom with a normal binary tree at  the higher levels, depending on λ. To satisfy any  value of λ, a full fat tree is required. Next we compare  FT NoC performance to Mesh NoC. The number of  links in the FT:                The area of FT is given by  FT NoC _ FT NoC , FT NoC , 2 log 4 p p A TL l w W n w W l n n = ⋅ ⋅ ⋅       = ⋅ ⋅ + (55)  And the power dissipation (using P0 as in (26)):  P C f V U ( ) 2 T FT NoC dd , FT NoC , 2 0 4 log NoC NoC n P U = ≅ ⋅ (56)   In summary, a fat tree is required for implementing  RNDC  on  tree NoC,  resulting  in  longer  communication paths and consequently in lower  frequency compared to mesh NoC.   D. AER Shared Bus  The AER shared bus naturally employs only  broadcast communications. A neuron  transmits  address events (namely, a packet containing only its  address) on the bus once it gains bus control. Each  receiving neuron compares the source address with  the addresses of the neurons to which it is connected.  Following   [20]  the  total  length of a bus  is  l n ( )4 2 Each spike occupies the entire bus regardless of the  topology of the emulated NN. Following (3), the bus  effective bandwidth is:  BW BUS L − =  (Figure 4).   NN AER BUS _ BUS BUS w f U = ⋅ ⋅ (57)  The bus operating frequency can be related to the  mesh NoC frequency following (6) and (26):  1 4 ( )2 2 0 0 4 BUS NoC BUS f f R C L n = = − (58)  Likewise, the bus BW can also be expressed in terms  of the BW of the MC mesh NoC (the preferred mesh  NoC communication method):  ( ) Hopfield AER BUS _ Hopfield Mesh MC , 2 4 4 BUS NoC U BW BW U n ≅ ⋅ − (59)  Evidently, AER bus utilization is lower than NoC  utilization, and is decreasing as the network size  grows. Even when disregarding this and assuming  U ,  it  is evident  that NoC effective  bandwidth is about n2 times higher than the AER bus  bandwidth. Moreover, the latter is n2.5 lower than the  bandwidth of mesh NoC used for RNDC emulation in  (37):  BUS NoC U≈ ( ) RNDC AER BUS _ RNDC Mesh MC , 2 4 4 BUS NoC U BW BW U n n ≅ ⋅ − (60)  On the other hand, the area consumed by the AER  bus is about twice smaller than the area for for NoC  Mesh.  n n l l Figure 4: AER shared bus topology  The power dissipated by AER bus follows (7):  2 ( ) 2 T BUS DD BUS 0 4 BUS BUS P C f V U P U n = = ⋅ − (61)  where  2 0 0 dd P V (cid:1) R d . Comparing with (26), observe  that both the AER bus and the mesh NoC dissipate  roughly similar power, O(1/n). In summary, the mesh  NoC offers higher performance than the AER bus at  the same cost.  E. Point to Point Architecture:  We consider n neurons arranged in a regular mesh  and  fully connected with point-to-point  (PTP)  unidirectional XY routed links. The total length of all  PTP connections can be calculated by multiplying the  total number of links  TL n n 1 / 2  by the  ( ) p p 2 = − average length of the link,  p p 2 2 l n / 3 l = ( ) P PL 2 1 1 3 The average frequency for P2P is:  1 9 l n n n = ⋅ −  (62)  P P 2 2 p p 2 2 0 0 0 0 9 4 4 NoC f f R C l n n R C l = = = (63)  With Hopfield NN (full connectivity) every spike  traverses n-1 links. The effective bandwidth is:  w TL f U P P 2 P P 2 P P 2 Hopfield eff P P , 2 P P 2 P P 2 1 U 2 It is intuitively evident that the P2P architecture can  carry out n/2 simultaneous transactions with average  frequency of  f . Similarly to the AER BUS  BW n w n f ⋅ ⋅ ⋅ = = − ⋅ ⋅ ⋅ = (64)  2P P architecture, P2P bandwidth is compared with the  multicast mesh NoC implementation (22):   w n 9 f U P P 2 Hopfield eff P P , 2 P P 2 Hopfield Mesh NoC , 4 9 4 NoC NoC BW n U BW U ⋅ ⋅ ⋅ = = = (65)  Observe  that  the P2P advantage of higher  parallelism is balanced by the higher frequency of the  NoC, so that they yield similar bandwidth.   141                                                   Considering RNDC (random local connectivity)  implementation on P2P, every spike traverses on  average C links. Applying average connection length  of 2λl to (5) yields the average frequency of  1 1 R C l λ λ 4 The effective bandwidth for RNDC implementation  on P2P is  RNDC P P 2 2 2 2 0 0 4 NoC f f = = (66)  P P 2 RNN P P 2 P P 2 RNDC eff P P , 2 RNN P P 2 P P 2 P P 2 NoC 2 w n ( 1) 2 C n f w n ( 1) 8 Comparing this to MC Mesh NoC architecture (37):  w TL f U BW C n f U U ⋅ Cλ ⋅ ⋅ ⋅ = = ⋅ − ⋅ ⋅ = = ⋅ − ⋅ = (67)  ( ) ( ) RNDC eff P P , 2 P P 2 RNN eff mesh MC , , 2 1 2 16 C NoC BW n n C U BW U λ λ = + + = (68)  Using the assumption in (31),  ( ) ( ) RNDC eff P P , 2 3 P P 2 RNN eff mesh MC , , 2 3 3 (cid:4) 1 P P 2 RNN eff mesh MC , , 1 2 16 16 NoC n NoC BW n n n U U BW n Un BW U = + + = ⋅ → ⋅  (69)  Thus, while the effective bandwidth of Hopfield  implementation is about the same for P2P and mesh  NoC, RNDC bandwidth of P2P can be higher than the  mesh NoC.  The PTP area is:  lw ( ) P P 2 P P 2 1 3 And P2P power dissipation is:  3 A L w n n n = = ⋅ −  (70)  ( ) 2 P P 2 P P 2 P P 2 0 P P 2 1 4 Comparison P2P and mesh NoC costs:   P A T dd P f C V U P n n U ≅ ⋅ ⋅ = −  (71)  ( ) 2P P NoC O n P ≅   and   ( ) 2P P NoC O n n A ≅ (72)  These results imply that although point-to-point  architecture can provide better performance for  certain Neural Network topologies than mesh NoC., it  comes at a price of higher power dissipation and  significantly higher area. The combined cost AP of  P2P is n2 times higher than the mesh NoC.  F. Summary of Cost and Performance   In the previous sections we have analyzed cost and  performance of different interconnect architectures  implementing neural networks. In Section  B it is  shown that multicast is preferred for NN emulation on  mesh NoC, as summarized in Table 1 and Table 2. In  Sections  C— E  we have analyzed the cost and  performance of NoC Tree, AER shared bus and Pointto-Point connectivity and compared performance and  cost to MC Mesh NoC, as summarized in Table 3.  It is evident from Table 3 that mesh NoC with MC  communications  is  preferred  for  large-scale  configurable VLSI  implementation of neural  networks. It offers the highest performance/cost ratio,  provides a high bandwidth which, thanks to high level  of parallelism, grows with the size of the network.  Only the maximally parallel P2P provides a higher  bandwidth than the mesh NoC, but at an extremely  high  cost. The  shared bus  and  fat  tree  implementations are less favorable.   Another relevant cost item relates to the size of  memory required for  the routing  tables. While  detailed analysis is outside the scope of this paper, we  have found that memory requirements are roughly  similar in all architectures.  IV. PERFORMANCE SIMULATIONS  The mesh NoC architecture was simulated using  UC, MC and BC in order to validate the analytical  model and to gain insight into network behavior. We  employed  the NS2 Network Simulator   [21]  to  investigate our neural networks. The link frequency  fNOC=1GHz and the packet header is 10bit. The  network size n (n processors emulating one neuron  each) was varied from 25 to 196 and higher in some  cases. For each n, a MC and a UC mesh NoCs are  simulated with both Hopfield and RNDC connectivity  patterns, using Poisson firing rate. The realistic  connectivity parameters of RNDC  (31) were  employed. For each simulated network, the firing rate  was varied in search of a “knee point” (Figure 5), in  which contentions became significant and the average  network delay started to grow exponentially. For lack  of analytical identification of that knee point, we  place it at the point where the average delay doubles  relative to its initial value. This metric is selected as  the maximal firing frequency. We then examine the  dependence of the maximal firing frequency on  network properties, validating our analytical model.   Comparing Hopfield NN (top row of Figure 5) with  RNDC (bottom row), we observe that the full  connectivity of Hopfield networks comes at the price  of higher average delay and lower achievable firing  frequencies. Considering unicast (left hand side of  Figure 5) versus multicast (right hand column), it is  evident that multicast enables higher firing rates.  Notice that while above the knee point the delays are  intolerable, below it the delays seem constant. This  validates the model assumption that the network  effectively operates with no congestion. Notice  further that if each processor emulates k neurons, then  the maximum  firing  rate would  decrease  proportionately by k.  142                         Table 3: Cost and performance comparison  NoC Mesh  MC  NoC Fat Tree  AER  Bus  P2P  Hopfield BW  O(1)  2 log n O n n       2 1 O n       O(1)  RNDC BW  ( ) n O C λ +         ( ) 2 log n n C λ O         + 2 1 O n       2 ( 1) n n O − Cλ       Practical  RNDC BW  ( ) O n  2 log n O n )2O n       ( )2O n − 5 6O n       Area  O(n)  ( O(n)  )1O n − )2O n − )2O n − ( ( ) ) 2O n n  Power  O(n)  )2O n − ( ) ( )2 )1 log O n ( ( ( O n n  )4O n − perf/ cost  Hopfield  perf/ cost  RNDC   ( ( 3 2 O n )3O n − − ( ( )1 1 2 O n − ( ( )5 2 6 O n − The maximum firing rates of a Hopfield NN  achieved for the simulated values on n is shown in  Figure 6 (a). The firing rate achieved using UC is  lower than MC and BC, and it decreases faster with n.  The simulations indeed validate that the MC firing  rate behaves approximately as O(1/n) and UC  frequency behaves as  ( ) 1O n n .  Similar results for RNDC NN are shown in Figure  6 (b). MC achieves higher spiking rate than both UC  and BC, and the rate scales better with network size.  The simulations indicate that MC spiking rate ranges  between O(n-0.5) and O(n-0.8) depending on  the  specific connectivity pattern. BC spiking rate is  O(1/n), the same as for Hopfield NN, and UC scales  slightly better than BC.  V. CONCLUSION  This paper presents  theoretical  analysis  for  determining a preferred interconnect architecture for  general purpose configurable emulation of spiking  neural networks. We show that a mesh NoC is  preferred over other analyzed architectures (fat tree,  point to point and shared bus). Further, it is shown  that multicast communications outperform unicast and  broadcast. Simulations successfully validate  the  analytical models and the asymptotic behavior of the  network as a function of its size. The results may be  extended to the more general case of address event  transactions.  Future research may address the following open  questions. Certain neural networks may be  implemented efficiently on partly-reconfigurable  NoCs, in which not all connectivity patterns are  allowed. The architecture, mapping algorithms and  memory size of such implementations should be  studied. Another open issue relates to the architecture  of the processors, and how it affects the NoC. Third,  the architecture of multicast NoC routers may be  further optimized  for emulating spiking neural  networks. Multi-level hierarchical modular NoC  architectures may be applied to enhance scalability of  neural network emulations. Last, the NoC may be  enhanced to emulate more precisely axonal delays and  exact event timings.   VI. "
Improved Utilization of NoC Channel Bandwidth by Switch Replication for Cost-Effective Multi-processor Systems-on-Chip.,"Virtual channels are an appealing flow control technique for on-chip interconnection networks (NoCs), in that they can potentially avoid deadlock and improve link utilization and network throughput. However, their use in the resource constrained multi-processor system-on-chip (MPSoC) domain is still controversial, due to their significant overhead in terms of area, power and cycle time degradation. This paper proposes a simple yet efficient approach to VC implementation, which results in more area- and power-saving solutions than conventional design techniques. While these latter replicate only buffering resources for each physical link, we replicate the entire switch and prove that our solution is counter intuitively more area/power efficient while potentially operating at higher speeds. This result builds on a well-known principle of logic synthesis for combinational circuits (the area-performance trade-off when inferring a logic function into a gate-level netlist), and proves that when a designer is aware of this, novel architecture design techniques can be conceived.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip Improved Utilization of NoC Channel Bandwidth by Switch Replication for Cost-Effective Multi-Processor Systems-on-Chip F. Gilabert and M. E. G ´omez Universidad Polit ´ecnica de Valencia Valencia, Spain S. Medardoni and D. Bertozzi University of Ferrara Ferrara, Italy Abstract—Virtual channels are an appealing ﬂow control technique for on-chip interconnection networks (NoCs), in that they can potentially avoid deadlock and improve link utilization and network throughput. However, their use in the resourceconstrained multi-processor system-on-chip (MPSoC) domain is still controversial, due to their signiﬁcant overhead in terms of area, power and cycle time degradation. This paper proposes a simple yet efﬁcient approach to VC implementation, which results in more area- and power-saving solutions than conventional design techniques. While these latter replicate only buffering resources for each physical link, we replicate the entire switch and prove that our solution is counterintuitively more area/power efﬁcient while potentially operating at higher speeds. This result builds on a well-known principle of logic synthesis for combinational circuits (the area-performance trade-off when inferring a logic function into a gate-level netlist), and proves that when a designer is aware of this, novel architecture design techniques can be conceived. I . IN TRODUC T ION Head-of-Line (HoL) blocking is a signiﬁcant performance limiting factor in NoCs, due to the use of FIFO queues. Virtual channel (VC) ﬂow control [13] has been conceived as a workaround for this problem and relies on the decoupling of network buffers from physical communication links. Typically, a single buffer is associated with each physical channel. VCs provide multiple buffers for each channel, so that when a certain VC is congested, the packets in the other VCs can still progress through the same physical channel and the network throughput can be signiﬁcantly improved. Even considering that a VC switch can implement the same amount of buffering resources of its VC-less counterpart by simply re-structuring them into multiple smaller VCs instead of a single queue [11], the incremental complexity when augmenting a baseline switching fabric with virtual channels is still severe [3], [5]. Most research efforts documented in the open literature to mitigate such an overhead and optimize the architecture of VC switches fall in the domain of chip multi-processors (CMPs) and fundamentally aim at delay-optimized implementations [6], [7], [15]. This is justiﬁed by the requirements of that domain, implying pipelined routers operating in the multi-GHz speed range and a shared memory trafﬁc exerting stringent latency-throughput demands. Full-custom design techniques are typically adopted, thus making even single cycle routers feasible while meeting the above speed requirements. In contrast, Multi-Processor Systems-on-Chip (MPSoCs) represent a much more resource-constrained domain. Areaand power-efﬁcient implementations are of paramount importance to fulﬁll the requirements of embedded system platforms like those for multimedia, broadband and networking applications [19], [8]. Here operating frequencies are typically lower, routers are unpipelined and the design ﬂow is mainly synthesis-based [24]. In this domain, resource overprovisioning to meet predeﬁned performance constraints is clearly not affordable, therefore VCs are an attractive solution to maximize link utilization. Unfortunately, MPSoCs are most sensitive to their area and power overhead, making their use controversial in this domain. An intensive effort is underway to minimize their implementation cost [9], [20]. This paper proposes a simple yet efﬁcient approach to VC implementation for NoCs, well suited for the tight resource budgets of embedded computing platforms and proving more area/power-aware than conventional architectures for VC switches. The key idea behind our work is that by replicating not just the buffering resources for each physical channel but rather the entire VC-less switch as many times as the intended number of VCs, the resulting design features a counterintuitively lower area. This result builds on a well known principle of logic synthesis, namely the area-performance trade-off for inferring the gate-level netlist of combinational logic. In practice, a design will be synthesized as a lowarea netlist when delay constraints are loose, while highperformance designs can be materialized at the cost of area. While we have previously used this principle in [26] to develop power-optimal selection strategies of RTL arithmetic soft-macros, this paper moves a signiﬁcant step forward and takes proﬁt of it for the design of NoC switches. Based on common practice, adding VCs to a VC-less switch adds up new control tasks to the critical path. In principle, the same link-buffer decoupling can be achieved by replicating the entire switch multiple times. Even assuming the aggregate buffering is kept the same, this solution requires more resources in parallel than the conventional one but leaves the logic functions on the critical path unaltered. This property is very useful, since when the two designs are aligned to the frequency of the slowest one, the architecture with replicated switches can be inferred with relaxed performance constraints and areaefﬁcient netlists can be derived. Overall, this area efﬁciency compensates for the replicated crossbars. We prove this with a commercial synthesis toolﬂow (although the underlying principle can be implemented by any synthesis tool) and on a 65nm industrial technology library for a link width of 32 and 64 bits (a typical upper bound for the MPSoC domain). Our approach can be pushed to the limit by replicating not just the switch but even the entire network multiple times, thus materializing the solution ﬁrst proposed by [10] and commercially exploited by [8]. However, in line with [21], we feel that replicated networks are a fair alternative to our multi-switch approach only when keeping the aggregate switch buffering and ﬂit width the same. The original proposal in [10] in contrast envisions a replication of sub-networks that results in an increase of channel bandwidth: this solution addresses a different problem than the one investigated in 978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.25 165 this paper, which instead aims at the switch architecture that improves utilization of the available channel bandwidth with the minimum area/power overhead. Under these assumptions, this paper compares VC implementation through switch replication with a conventional VC switch and with full network replication, thus proving the overall better area-performance ﬁgures of the proposed approach. Since this work provides a thorough investigation of three VC architecture variants across a range of design parameters (link width, target speed), and combines quality metrics of their physical designs with their system-level performance ﬁgures, we restrict ourselves to statically allocated VCs and leave extension of our design approach to dynamically allocated ones for future work. I I . PR EV IOU S WORK Virtual channel ﬂow control was ﬁrstly proposed by [13] as an effective workaround for head-of-line blocking. Since its implementation requires extra buffers, the router power consumption will increase with the number of VCs [3]. State-of-the-art of router pipeline, microarchitecture and circuit are illustrated in [6], which develops canonical pipelines for wormhole routers, virtual-channel routers and speculative virtual-channel routers. The basic techniques to improve router energy-delay are: speculation [7], [6], bypassing [4], lookahead pipelines [14], [15], simpliﬁed VC allocation [15] and lookahead routing [16]. While these techniques are effective under low-load, express virtual channels (EVCs) can be used to skip the router pipeline [12] under high-load. A number of works has evolved the VC switch microarchitecture to optimize latency in the CMP domain. The work in [7] presents the design of a low-latency router where control overheads (routing and arbitration logic) are removed from the critical path, thus achieving a cycle time of around 12 FO4 delays plus clock overhead. While [7] proposes a speculative single-cycle router clocked at 250 MHz, the work in [15] goes a step forward and proposes a non-speculative single-cycle router pipeline (at low load) clocked at 3.6 GHz. In [2], a ﬁve stage router design at 4 GHz is presented with a peak injection/ejection data rate of 256Gbits/s per node. Prior work on VC switch architectures for SoC platforms have very different design concerns from those above. Typically, low-frequency unpipelined routers are used in synthesized NoCs, like in [24]. The work in [5] presents a detailed implementation of a NoC router with a parameterizable number of VCs. In this work, a signiﬁcant area increase associated with VC implementation is pointed out as an open research concern. Two low-cost virtual channel allocators for NoCs are proposed in [9], reducing the number of both input and output VC arbiters and the size of the output VC arbiters respectively. The work in [20] introduces the use of slow-silent VCs to reduce the switching power of NoCs while keeping leakage power small. This paper shares an underlying theme with many other recent papers focusing on the embedded computing domain that excessive complexity must be revisited in the context of MPSoCs. In this direction, our approach to cost-effective VC switch design does not imply microarchitecture modiﬁcations of baseline VC-less switches but rather re-uses them as basic building blocks for the fast construction of virtual channel networks. The closest paper to our work is [22], where a multiplane virtual channel router which has multiple crossbar switches and modiﬁed switch allocator is illustrated. This is proposed as a way to increase the ﬂit transfer rate between input and output queues. Unfortunately, their interesting ﬁnding ultimately implies an increased switch complexity and a critical path degradation. I I I . CONV EN T IONA L VC SW I TCH ARCH I T EC TUR E In this section, the architecture of a VC switch with conventional design techniques is presented. It will serve as the reference architecture for comparison. The xpipesLite library of soft macros for NoC assembly is used as the baseline NoC experimental platform [1]. A. Baseline VC-less switch The baseline xpipesLite architecture leverages an output buffered switch implementing wormhole switching and source-based routing [1], and is illustrated in Fig. 1. While the tunable-size buffer for performance optimization is located at the switch output ports (6 slots throughout this work), input ports perform just retiming and ﬂow control by means of a ﬁxed 2-slot buffer (minimum required to cover the roundtrip latency). Therefore, the crossing latency is 1 cycle in the switch and 1 cycle in the downstream/upstream link. The choice for retiming at input AND output ports stems from the need to break the timing path across switch-toswitch-links. Our previous implementation effort in 65nm technology of NoC topology layouts indicated that when wiring is considered, the delay of inter-switch links causes a signiﬁcant performance drop for most regular NoC topologies depending on their connectivity pattern [18]. The xpipesLite switch relies on a stall/go ﬂow control protocol [25]. It requires two control wires: one going forward and ﬂagging data availability (”valid”) and one going backward and signaling either a condition of buffer ﬁlled (”stall”) or of buffer free (”go”). With this scheme, power is minimized as any congestion issue simply results in no unneeded transitions over the data wires , and recovery from congestion is instantaneous. Arbitration is not centralized, i.e., there is a round-robin (RR) arbiter for each output port. The critical path of the switch starts from the input buffer, goes through the arbiter, the crossbar selection signals, some combinational logic to shift some bits in packet headers and ﬁnally includes a library setup time for correct sampling at the output port (check Fig.1). B. Conventional Multi-Stage VC Switch The above baseline switch was augmented with virtual channels by following conventional design techniques [6], [7]. As previously mentioned, we restrict the focus of this paper to statically allocated VCs and to deterministic routing algorithms and come up with a reference VC switch architecture deeply optimized for this case, unlike [6], [7]. A switch input port receives the VC ID together with the ﬂit from the upstream switch (Fig.2(a)). This ID is used to select the VC where arriving ﬂits must be stored. Also, a stall signal is generated by each VC and propagated upstream to the attached output port to notify availability of buffer space on a per-VC basis. Each VC implements its own buffering space and a very simple decoding logic that reads the target output port for each packet from the routing ﬁeld in the header. In turn, each VC sends head ﬂits, that must be arbitrated, to the internal arbitration logic and all ﬂits to the datapath crossbar going through a multiplexer. Flit space in the VC buffer remains reserved until a grant signal from the arbitration logic is received. When this occurs, the arbiter also drives the multiplexer control signals (see Fig. 3-left). Similarly to [15], switch allocation is performed immediately after the ﬂit arrives, and the routing information is used to 166 ARB0 ARB1 ARB2 ARB3 ARB4 C R O S S B A R FLOW CONTROL MANAGER PATH_SHIFT PATH_SHIFT PATH_SHIFT PATH_SHIFT PATH_SHIFT Fig. 1. Architecture schematic. . . . Grant 0 Grantl N Data Valid VC_grant Data Valid VC_ID IN0 IN1 IN2 IN3 IN4 STALL0 STALL1 STALL2 STALL3 STALL4 IN_BUF IN_BUF IN_BUF IN_BUF IN_BUF . . . Stall 0 Stall N Data Valid VC_ID Data Valid VC 0 Data Valid . . . Data Valid VC N Data Valid to first stage arbiter OUT_BUFFER OUT_BUFFER OUT_BUFFER OUT_BUFFER OUT_BUFFER STALL0 OUT0 STALL1 OUT1 STALL2 OUT2 STALL3 OUT3 STALL4 OUT4 Data Valid VC 0 . . . Data Valid VC N Data Request Grant Data Request Grant Ouput Port Arbiter Data Valid VC_ID . . . Stall 0 Stall N (a) Input port schematic. Fig. 2. (b) Outport port schematic. Input and output ports of the multi-stage VC switch. identify the intended switch output port. VCs are assigned nonspeculatively after switch allocation: the winning VC that is granted access to a given output port automatically reserves the VC with the same ID at that output port. This is because VCs are statically allocated. As we will clarify shortly hereafter, it can never occur that a VC is granted access to an output port and the intended VC at that port is occupied. Switch allocation implies a 2 stage arbitration, like in [6], [7]. The detailed architecture is illustrated in Fig.3. We denote the resulting VC switch as the multi-stage architecture. First, each input port arbitrates among its VCs through a V : 1 VC arbiter, selecting a single VC out of V that is considered for arbitration in the next phase. Second, each of the P output ports makes use of a P : 1 port arbiter to discriminate among all the input VCs winning the ﬁrststage arbitration and requiring that speciﬁc output port. All arbiters implement a RR policy and, differently than the VCless switch, arbitration is now performed at ﬂit-level instead of packet-level in order to avoid starvation. One rule that we enforce during switch allocation is that a ﬂit can only win the arbitration in the VC (ﬁrst stage) arbiter if it is a payload ﬂit or it is a header that requires an output VC that has free buffer space to allocate the header and it is not in use by another input VC. The port arbiters update at each cycle the status of each output VC (see signal ”VC Status” in Fig. 3), and the ﬁrst stage arbiters use that information to validate incoming requests from the input VCs. Thus, it is not possible to waste a cycle by selecting a winner in switch allocation that will ﬁnd its target VC reserved or with no space. To provide fairness among all the input VCs, if the winner of the VC arbiter does not win the port (second-stage) arbitration, it receives the highest priority in the VC arbiter. Guaranteeing that the last winner will be proposed again as soon as possible. This architecture limits the amount of VCs of each output port that can be allocated in a single cycle to 1. But, as the crossbar is only P xP (with P the number of switch I/O ports), only a single ﬂit would be able to reach a particular output port at each cycle. So there is no performance penalty. Figure 2(b) shows the schematic of the switch output port. Toward the switch side, together with the new ﬂit to be stored, the port arbiter indicates the VC that must store it. The output port implements a buffer for each VC. At the interface to the downstream switch, there is a further arbiter that decides which VC will send a ﬂit based on a RR policy. A VC only activates its request signal if the corresponding downstream stall signal is not asserted. When an output port sends a ﬂit out, it has to send also the ID of the VC that is actually sending the ﬂit. The dashed line in Fig.3 illustrates the critical path of the multi-stage VC switch. Like the VC-less switch, it includes part of the control and of the datapath (essentially, arbitration and crossbar selection). However, it is actually longer due to the more complex 2-stage arbitration. C. Motivation By inferring 5x5 VC-less switch and corresponding multistage switch architectures in the same 65nm technology library with Synopsys Physical Compiler, we obtained the critical path delay and area results when 2 VCs were considered for the multi-stage architecture. The conventional multi-stage realization of a VC switch incurs a delay overhead of 20%, associated with the more complex arbitration. For the sake of fair area comparison, the VC-less switch was re-synthesized to match the same delay of the multi-stage one. This relaxation of delay constraints enabled the logic synthesis tool to infer the same logic functions with a more compact gate-level netlist, in practice moving the design point along the performance-area optimization curve. This is a well known principle of logic synthesis [17] and therefore holds in general, while the amount of achieved area savings depends on the speciﬁc design, on the set of cells available in the technology library and on the optimization techniques implemented by the synthesis tool at hand, resulting in different combinational logic implementations. When focusing on the crossbar, we noticed that Synopsys Physical Compiler tends to implement it under loose delay constraints by means of a tree of smaller multiplexers that combine more than two inputs at each level of the tree. As the constraint is made tighter, two relevant optimizations are used. On one hand, an AND-OR tree of gates is used, leveraging speciﬁc compound cells. On the other hand, the driving strength is exploited as much as possible to meet the predeﬁned delay target. To the limit, the compound cells 167 Flit Buffer (VC 0) . . . Flit Buffer (VC V) Input Port 1 Request + VC ID Decode + Control 1−of−V P VC ID 1 Output Port 1 VC ID 1−of−P Wants to tranmist a flit? V:1 Arbiter Request P:1 Arbiter To other Output Port Arbiters 1−of−V VC Status Succes P*V V Decode + Control Flits  Crossbar 1−of−P Crossbar control signal . . . Datapath . . . V V . . . From other Output Port Arbiters Fig. 3. Detailed control path of the multi-stage VC switch. INPUT LINK_0 M U X VC_ID VC−LESS SWITCH OUT 0 A R B I T E R S VC−LESS SWITCH OUT N Fig. 4. Multi-switch implementation of a VC switch. VC SWITCH are broken down into individual logic cells, with selective optimization of the driving strength. A further analysis of compiler behavior is outside the scope of this paper. By looking at area numbers, despite the fact that in the VC switch buffering is doubled, it can be observed that twice the area of a VC-less switch accounts for only 86% of the area of the multi-stage switch and that the relaxation resulted in almost 10% area savings with respect to the netlist synthesized for maximum performance. This result suggested the novel VC switch implementation which we hereafter illustrate. IV. PRO PO S ED MU LT I -SW I TCH IM P L EM EN TAT ION VCs provide multiple buffers for each channel, so that when a certain VC is congested, the packets in the other VCs can still progress through the same physical channel, increasing switch control logic complexity and critical path delay, as proved by the multi-stage architecture. As an alternative, the VC switch architecture that we propose consists of replicating not just buffers per channel, but rather the entire baseline VC-less switch as many times as the intended number of VCs. Replicated switches then share the same physical input and output links, similar to what conventional VCs do, but with the main difference that in the new implementation VCs have their own access to a replicated crossbar and the ﬁrst stage of arbitration can be ﬁnally removed, as illustrated in Fig.4. We call this the multi-switch VC implementation. The underlying principle is simple: instead of replicating buffering resources inside a switch, we rather propose to replicate the baseline VC-less switch without impacting its internal critical path. Similar to the multi-stage architecture, also this solution requires an additional stage of link arbitration in order to multiplex the outputs of the baseline VC-less switches into the same physical output links connecting to downstream switches. As Fig.4 indicates, this stage is cascaded to the replicated VC-less switches and it is exactly the same which is used at the output of the multi-stage implementation. Like this latter, it arbitrates on a ﬂit-by-ﬂit basis while the arbiters of the replicated switches keep arbitrating at the packet level. Interestingly, delay of this arbitration stage does not add up to that of the VC-less switches to determine the critical path, since they are separated by a retiming stage (the switch output buffers). In practice, the critical path of the multi-switch architecture is the same of a VC-less switch . However, one might argue that this comes at the cost of replicating more physical resources (e.g., the crossbars). At this point, a basic principle of logic synthesis comes into play and leads to opposite conclusions. When comparing the multi-stage with the multi-switch VC implementations, this latter has less functions on the critical path hence potentially resulting in a more area/power-efﬁcient gate-level netlist after logic synthesis. In fact, the multi-switch architecture certainly provides a higher maximum speed than the multi-stage one. However, if we require the two architectures to be aligned to the speed of the slowest one (the multi-stage), then combinational logic of the multi-switch design can be inferred with relaxed delay constraints and therefore thoroughly optimized for area and power. The speciﬁc case study of the switch crossbar has been already discussed in section III-C. Now, the issue is to determine whether the area savings achieved by logic synthesis are enough to compensate for the larger amount of hardware resources that are instantiated in the multi-switch architecture, especially the replicated crossbars. Please observe that the multi-stage and the multiswitch architectures can be designed to instantiate the same overall amount of buffering resources: n VC queues in the multi-stage switch are equivalent to a single queue in n replicated switches. Experimental results will prove that this approach is worthwhile in terms of area and power efﬁciency. Above all, our design technique allows designers of VCless switches to have VC functionality readily available with minimum effort and to achieve (if needed) a higher maximum operating speed than conventional multi-stage VC switches. No modiﬁcations of the baseline switch are required and the low-latency optimizations proposed in [7] can still be applied to our architecture in case an even higher performance is required. 168 Architecture MStage MSwitch MNetwork MStage MSwitch MNetwork number of VCs 2 2 2 4 4 4 Sub-modules 2 switches 2 networks 4 switches 4 networks Sub-module Aggregate ﬂit width link width 32 bits 32 bits 32 bits 32 bits 16 bits 32 bits 32 bits 32 bits 32 bits 32 bits 8 bits 32 bits Sub-module no of queues 2 input and 2 output 1 input and 1 output 1 input and 1 output 4 input and 4 output 1 input and 1 output 1 input and 1 output Input queue Output queue Aggregate switch slots slots buffering 2 ﬂits 6 ﬂits 1280 bits 2 ﬂits 6 ﬂits 1280 bits 4 ﬂits 12 ﬂits 1280 bits 2 ﬂits 6 ﬂits 1280 bits 2 ﬂits 6 ﬂits 1280 bits 8 ﬂits 24 ﬂits 1280 bits CON FIGURAT ION S FOR TH E COM POUND SW I TCH ARCH I T EC TUR E S UND ER T E S T. F L I T W ID TH I S 32 B I T S TABLE I VC SWITCH SWITCH network 1 SWITCH network 2 Fig. 5. Multi-stage VC switch vs multi-network compound switch: aggregate link width and switch buffering are kept constant. A. Full network replication If we push our approach to the limit, we should replicate the entire physical network multiple times and not just the switches. This solution has already been adopted, e.g., in [23], [8]. In [23] four separate and independent NoCs are used, while [8] uses 5 mesh networks and leverages the onchip wiring resources to provide massive on-chip communication bandwidth. In this paper, we move from a different perspective: the available link bandwidth is satisfactory and we search for cost-effective design techniques for its optimal exploitation. As a consequence, for the sake of comparison we consider replicating multiple physical networks while keeping the aggregate link width and aggregate VC switch storage the same. In practice, in order to construct the multi-network architecture, we equally partition the same number of wires of the multi-stage/switch NoC across the multiple physical networks (see Fig. 5). Aggregate switch buffering in all architectures is the same. In the multi-stage solution, n virtual queues are instantiated in the same switch for each input and output port. In the multiswitch solution, n replicated switches have a single queue per port. In the multi-network architecture, switches in each network have a ﬂit width n times lower and single queues with a size (in slots) n times larger. This way, the overall amount of ﬂip ﬂops for buffering purposes stays the same throughout all the architectures under test. One clear disadvantage of the multi-network architecture lies in the fact that due to the smaller ﬂit width of its switching sub-modules, packet latency increases. In particular, with n VCs in the multi-stage architecture, in each of the n replicated networks packets suffer from a latency increase which is more than n times. This is due to the control bits in the network protocol. In particular, in the xpipesLite NoC architecture 3 bits in each ﬂit are devoted to the ﬂit type, which in the simplest case can be a head, a payload or a tail. Such constant size ﬂit type consumes a percentage of the ﬂit width which grows as the ﬂit width becomes smaller. V. EX P ER IM EN TA L R E SU LT S : PHY S ICA L SYN TH E S I S We fed the multi-switch, the multi-stage and the multinetwork compound switch designs to a commercial backend synthesis ﬂow for a 65nm STMicroelectronics technology. Physical synthesis was performed with Synopsys Physical 2.5 2 1.5 1 0.5 a e r A d e z i l a m r o N 0 0 Multi-Switch 64 Multi-Stage 64 Multi-Switch 32 Multi-Network 64 Multi-Stage 32 Multi-Network 32 1 2 3 Critical Path [ns] 4 5 6 Fig. 6. Area comparison. 2 Virtual channels Compiler and place&route with Cadence SoC Encounter. The switch radix for all experiments was 5x5. 2 and 4 VCs were considered, while link width was varied from 32 to 64 bits. Aggregate input/output link width is the same for all compound switches. Aggregate buffering at switch input and output ports was also the same, and buffer conﬁgurations of the compound switches are illustrated in Table I for an aggregate link width of 32 bits. Similar considerations apply for an aggregate link width of 64 bits. The designs were synthesized at their maximum performance ﬁrst, then the delay constraint was gradually relaxed, thus getting area/critical path curves. For 2 VCs, such curves are illustrated in Fig.6. We observe the following: (i) The multi-switch architecture can achieve a higher speed than the multi-stage one since it implements less control functions on the critical path. Therefore, the physical synthesis tool can reduce area of this design while relaxing its performance constraint. It is then possible to match the same maximum speed of the multi-stage architecture, while incurring a lower area, since the area scalability process for the internal combinational logic (e.g., the crossbar) is very effective. Area savings in almost all cases amount to 10%. (ii) When operating at a lower speed, both gate-level netlists can be optimized for the relaxed timing constraint . This optimization process saturates around a cycle time of 3ns for the multi-switch, while the multi-stage can still be optimized until 4.5ns. As a consequence, there is a target cycle time (3ns) beyond which the multi-stage architecture actually saves area. Apart from the unrealistically low operating speed at the break-even point, the area savings from there on are marginal (maximum of 5% at 200 MHz for 4 VCs). (iii) While it is true that replicating a 64-bit mux-based crossbar is more expensive, it has to be considered that the multi-stage architecture employs additional mux-based logic: one to select one VC per input port and one demux to send the crossbar output to one output VC. In practice, the multi-stage has a larger overall mux-based logic. So, by increasing the width of the crossbar data path, the multi-stage architecture is 169   2 1.8 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2 0 Clock Out.Buff. Crossbar Arbiter In.Buff. Link Mux Link Demux Link Arbiter Fig. 7. Area breakdown of 32-bit multi-switch (max. and relaxed perf.) and multi-stage VC implementations. more impacted. The overall balance is in favour of the multiswitch solution (area-wise) even at 64 bits. (iv) The multi-network architecture scales in the same way as the multi-switch one. However, area is always consistently better. This is due to the absence of de-mux logic at switch inputs, of the mux logic at switch output and of the link arbiter controlling this latter. Moreover, this architecture makes use of multiple smaller crossbars with respect to the multiswitch solution. On average, area savings amount to around 5%. Even at very low speeds, area is lower and this solution is competitive, in that region, with the multi-stage architecture. Experiments with 4 VCs show a similar trend than as for the 2 VCs case. However, those results are not shown for lack of space. Let us now better detail how the area optimization process operates in the multi-switch architecture (2nd and 3rd bars in Fig.7). When we relax the timing constraint, area of the noncombinational circuits remains almost unchanged. In contrast, we observe that gate-level netlist transformations during logic synthesis enable a signiﬁcant reduction of combinational logic area (crossbar, arbiters, multiplexers, buffer control logic). For the crossbar, what actually happens when a lower delay is required is that driving strength of gates is largely increased, and complex logic cells (like 4-input multiplexers) are decomposed into simpler and individually tunable logic gates. When comparing 1st and 2nd bar in Fig.7, it is clear that the capability to relax performance and optimize area of the multi-switch gate-level netlist enables a higher area efﬁciency. We also noticed that for multi-switch implementations, the maximum performance is always the same regardless the number of VCs, while for the multi-stage case the addition of one VC incurs an 8% degradation of the maximum speed (around 16% for 2VCs and so on). The multi-switch architecture thus avoids the well-known degradation of max. speed with number of VCs since it keeps adding resources in parallel without impacting the critical path. This holds until the critical path moves from the switch internal logic to the switch-to-switch link. At that point, multi-stage and multi-switch architectures would feature the same degradation since they have the same link architecture (including the same link arbitration logic). We also performed power-analysis of all switch implementations after place&route. All designs have 2VCs, 32-bit ﬂits and were synthesized and operated at the speed of the multistage one (700 MHz). Results are illustrated in Fig.8. Let us consider idle power ﬁrst. This latter is typically determined by registers and by the clock tree. Since all architectures have been designed with the same amount of aggregate buffering, their idle power is almost the same. Consider that we are using a low-power, especially lowleakage industrial technology library at 25 degrees, so leakage Multi  Switch Multi  Network Multi Stage Multi  Switch Idle Multi  Network  Idle Multi Stage  Idle Fig. 8. Power analysis with 50% switching activity and with idleness. contribution was not signiﬁcant in our results. By experimenting with an average switching activity at switch inputs of 50% (target output ports and VC IDs randomly chosen), it is possible to notice a large power consumption increment in the combinational logic blocks (crossbar, arbiter). One thing to notice is that the multi-switch total crossbar power is lower than the multi-stage. This is due to two reasons. First, the multi-stage switch is synthesized at maximum performance while the multi-switch crossbar with relaxed delay constraints. This enables area and power optimizations by the synthesis tool. Second, the link injection rate typically constrains the crossbar transmission rate. Despite having replicated crossbars, we don’t have replicated links. Usually there is only one ﬂit than can cross the crossbars per input port at a given cycle, due to the link injection rate of one ﬂit per cycle. Even though, there are some cases in which it is possible to have several ﬂits that can cross both crossbars at the same cycle. For example, an input port with 2 VCs may have one VC recovering from a stall condition, while the other will be transmitting a packet. In this case, both crossbars will be in use until the buffers are depleted, after that the link injection rate will be constraining the crossbar transmission rate to 1 ﬂit per cycle. If we also consider that input and output buffers in multi-stage are more complex than the sum of the input and output buffers in the two VC-less switches, and that some arbiters in the multi-switch architecture arbitrate on a packetrather than ﬂit-basis, then we understand why multi-switch saves about 18% power with respect to multi-stage. Multi-network power consumption is 21% higher than multi-switch. Packets in multi-network are in fact longer than in multi-switch. So for a given set of transactions, multinetwork will always send a higher number of ﬂits, which translates into a higher switching activity. Thus, it features a higher power consumption. Packet length Multi-Switch/ Multi-Network Multi-Network Multi-Stage 2 VCs 4 VCs 23 ﬂits 36 ﬂits 95 ﬂits PACK E T L ENG TH FOR A BUR S T O F 10 32 -B I T WORD S . AGGR EGAT E L INK W ID TH IN TH E NOC : 3 2 B I T S . TABLE II V I . EX P ER IM EN TA L R E SU LT S : SY S T EM -L EV E L P ER FORMANC E By using a transaction level simulator for the xpipesLite NoC architecture, we compare performance of three NoCs providing VC functionality with the Multi-Stage, the MultiSwitch and the Multi-Network approach, assuming that all NoCs operate at the speed of the slowest one (Multi-Stage). As mentioned, the aggregate buffering and the aggregate link bandwidth of all three solutions remain constant. This decision affects only the conﬁguration of the Multi-Network 170 2VC-Multi-Stage 2VC-Multi-Switch 2VC-Multi-Net 4VC-Multi-Stage 4VC-Multi-Switch 4VC-Multi-Net Vanilla ) s e l c y c ( y c n e t a L e g a s s e M e g a r e v A  250  200  150  100  50  1  2  3  4 Traffic (bits/cycle/node)  5  6 Fig. 9. Average latency vs Throughput for uniform trafﬁc in a 32 bit 4x4 mesh. solution, in which the buffering of every switch and network interface is increased as many times as the number of VCs to account for the lower ﬂit width. Therefore, the packet length for a single processor transaction increases with the number of VCs. For this reason, all performed simulations do not keep constant the packet length but rather the length of the processor transactions, which range from 4 to 16-beat bursts. Each data word of the burst consists of the actual 32-bits of data plus 4 byte enable control bits. Table II shows, for each switch architecture and for a NoC aggregate link width of 32 bits, the packet lengths corresponding to a processor burst transaction of 10 words. As can be observed, the packet length in the case of Multi-Switch (or Multi-Stage) is 3 ﬂits for the header and 20 for the payload. Notice that each processor data word is translated into two ﬂits. In the xpipesLite NI [1] the ﬂit generation of consecutive words of data are decoupled from each other by guaranteeing that a ﬂit contains only bits of a single data word. This way, latency is minimized. This however implies the use of padding when necessary, since the second ﬂit decoding the data word might be partially empty. In contrast, by reducing the ﬂit width, the removal of padding bits allows the packet length to increase less than linearly. For example, in Table II the Multi-Network solution with 4VCs requires 95 ﬂits. This is less than 4 times more than the equivalent Multi-Switch solution, despite having a ﬂit width four times smaller (the ﬂit width is 8 bits). For multi-stage and multi-switch, the ideal link width to accommodate 32 bit processor data words into the lower number of ﬂits and limit padding would be 39 bits: 36 bits for data plus byte enable and 3 bits for ﬂit type encoding. Anyway, we decided to keep a standard 32 bit NoC link conﬁguration, which enables to somehow contain packet length in the multinetwork architecture and therefore represents a best case for it. Similar considerations hold for 64 bit links. Finally, for each test, we have considered two network sizes: a 4x4 mesh with 16 cores and a 16x16 mesh with 256 cores. Both topologies were considered with 2 and 4 VCs, and for 32 bit and 64 bit NoC link architectures. A. Uniform trafﬁc Figure 9 shows the throughput versus average latency of a 4x4 mesh when considering an uniform trafﬁc pattern with a link width of 32 bits. It shows the results for all three switch architectures with 2 and 4 VCs. Regarding the comparison between Multi-Stage and Multi-Switch, the use of two VCs increases the maximum throughput and decreased the average latency over the vanilla VC-less architecture, but as we 171 Fig. 10. Multi-Switch throughput normalized to Multi-Network. increase the number of VCs, the improvement in performance becomes smaller. The reason for this lies in the small size of the topology: the HOL effect is almost removed when two VCs are used. Notice that the difference in performance between Multi-Stage and Multi-Switch is really small. This is due to the fact than 16 cores can not inject trafﬁc enough to saturate the network before saturating the destinations, in other words, the ejection links of each switch become congested sooner than the switch-to-switch links, thus making the impact of the switch architecture over performance marginal. On the other hand, the Multi-Network achieves a lower maximum throughput than the vanilla architecture, while obtaining a higher latency. This behavior is caused by the increased packet length. Even in the absence of network congestion, as the ﬂit width is decreased, each packet requires a higher amount of cycles to reach the destination, so the base latency increases with the number of VCs. Also, as the ﬂit width is reduced, the number of bits that can be injected into the network in a single cycle decreases, so the throughput is also reduced as the number of VCs is increased. This produces an undesired effect: as we increase the number of sub-networks in Multi-Network, we are reducing the performance of the system. This effect is so critical than even the Vanilla solution outperforms the Multi-Network architecture with two VCs. On the other hand, the results for different mesh sizes and link widths show that, as network size increase, MultiNetwork performance become worse, even worse than the Vanilla solution for a 16x16 mesh. Regarding the MultiStage and Multi-Switch solutions, as network size increases, the use of two VCs is not enough to completely alleviate the effect of HOL over performance. So, when four VCs are considered, the performance is still improved, noticeably decreasing latency and increasing performance over the two VCs case. Finally, as we increase the number of VCs, the Multi-Switch solution clearly outperforms the Multi-Stage one. As the network becomes bigger, the bottleneck moves from the ejection channels to the switch-to-switch channels, and in this case the replicated crossbars of the Multi-Switch architecture become an advantage. B. Performance comparison We derived similar curves for several trafﬁc patterns for each switch architecture. Those results are summarized in Figure 10. This ﬁgure shows the normalized maximum throughput of Multi-Switch with respect to Multi-Network. In this plot, a bar higher than 1 means an improvement on maximum throughput of Multi-Switch over Multi-Network. As mentioned in the previous section, there is almost no difference in performance between the Multi-Switch and the Multi-Stage solutions. In practice, Multi-Switch always achieves a performance similar to (or slighly better than) Multi-Stage, but never worse. Thus, the equivalent ﬁgure for their performance comparison is not shown here for the lack of space.       Although demonstrated with static VC allocation in this paper, our approach can be extended to reﬂect a dynamic allocation policy, where the new VC to be allocated is decided on the switch output port. However, delay of the decision logic adds up to the link delay, thus requiring a timing analysis which is left for future work. ACKNOW L EDGM EN T This work was supported by by the project NaNoC (project label 248972) which is funded by the European Commission within the Research Programme FP7, by the Spanish MEC and MICINN, as well as European Comission FEDER funds, under Grants CSD2006-00046 and TIN2009-14475-C04. "
Temperature-Aware Delay Borrowing for Energy-Efficient Low-Voltage Link Design.,"This paper presents a new technique that takes advantage of the differing temperature dependences in low-voltage interconnect links and higher voltage transceivers. The link and transceiver are dynamically retimed as the system temperature changes. This delay borrowing enables the link to maintain a frequency requirement despite temperature-induced frequency variations in excess of 200%, and enables the link to operate at lower voltages than possible with a non-temperature aware link. In addition to improved tolerance of environmental variations, the proposed approach achieves energy savings of up to 40% in a commercial 65 nm technology, including the energy overhead of the temperature-aware system. Further, the delay borrowing system is shown to decrease temperature-induced delay variations by 85%.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip Temperature-Aware Delay Borrowing for EnergyEfficient Low-Voltage Link Design  David Wolpert, Bo Fu, and Paul Ampadu  Dept. of Electrical and Computer Engineering  University of Rochester  Rochester, NY 14627 USA  <wolpert, bofu, ampadu>@ece.rochester.edu Abstract—This paper presents a new technique that takes  advantage of the differing temperature dependences in lowvoltage interconnect links and higher voltage transceivers. The  link and transceiver are dynamically retimed as the system  temperature changes. This delay borrowing enables the link to  maintain a frequency requirement despite temperature-induced  frequency variations in excess of 200%, and enables the link to  operate at lower voltages than possible with a non-temperature  aware link. In addition to improved tolerance of environmental  variations, the proposed approach achieves energy savings of up  to 40% in a commercial 65 nm technology, including the energy  overhead of the temperature-aware system. Further, the delay  borrowing system is shown to decrease temperature-induced  delay variations by 85%.  10−9 ) s ( y a e l D r e t r e v n I 10−10 0.6 0.7 0.8 Supply Voltage (V) 0.9 1 Fig. 1. Impact of temperature on a commercial 65 nm technology.  Keywords-Temperature, delay borrowing, interconnect, energyefficient, low voltage.  TABLE I.  TEMPERATURE-INDUCED DELAY CHANGE IN A 65 NM  TECHNOLOGY.   INTRODUCTION  I.  The network-on-chip (NoC) paradigm is a promising  solution to the global communication challenges of gigascale  systems; however, as billions of transistors continue to be  added to these nanoscale systems, power dissipation continues  to be a major design constraint. A great deal of research has  examined power  issues  in NoCs [1]-[5]. The network  infrastructure has a large cost—up to 36% of the power  dissipation in each networked tile [6]. To improve NoC power  consumption, common power saving techniques such as the use  of reduced swing voltages have been applied  to  the  interconnect links [7][8].   Unfortunately, the reduction of link voltage makes these  systems much more susceptible to variations. Temperature  variations have a particularly severe impact on delay in lowvoltage designs, shown in Fig. 1 for a 65 nm commercial  technology. The error bars in Fig. 1 are quantified in Table I,  where we see that the military specified temperature range  (-55°C–125°C [9]) can result in delay changes in excess of  200% at 0.6 V. Another important observation from Table I is  that the delay change from -55°C to 125°C is negative at 0.6 V  and positive at 1.0 V. The voltage region with a positive delay  change is referred to as the normal temperature dependence  region, while the region with a negative delay change is  referred to as the reverse temperature dependence region.  Between the two regions, there is a supply voltage where the  Compared  Temperatures  -55°C(cid:198)125°C  25°C (cid:198)125°C  45°C(cid:198)125°C  65°C (cid:198)125°C  0.6 V  -210.0%  -59.6%  -42.2%  -28.4%  Link Voltage  0.8 V  -8.2%  -4.5%  -3.5%  -2.7%  0.7 V  -59.3%  -22.2%  -16.5%  -11.5%  0.9 V  11.4%  4.5%  3.3%  2.23%  1.0 V  19.3%  8.9%  6.8%  4.9%  impact of temperature dependence on delay is minimized  [10][11], as indicated by the error bar at 0.8 V in Fig. 1. This is  referred to as the temperature-insensitive voltage VINS, and as  technology scales this voltage approaches nominal voltages  [12].  The difference between the temperature dependences at  high and low voltages provides an interesting opportunity for  systems with reduced link swing voltages—if the link voltage  is low enough to operate in the reverse temperature dependence  region, a change in temperature will cause the link delay to  vary in the opposite direction of the delay in the nominal  voltage router. These opposing delay variations make room for  innovative new approaches to lessen the impact of temperature  variations and improve system reliability and performance.   In this paper, we propose a temperature-aware delay  borrowing method that averages the impact of temperature  variation on the link and the transceiver. When the links are  operated below VINS, the average of the reverse temperature  978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.20 107               dependence in the link and the normal temperature dependence  in the transceiver results in a large reduction in the impact of  temperature variation on the communication system as a whole.  In the following sections we show how the proposed method  enables the use of lower link voltages to meet a given  frequency  requirement, providing  large energy savings  compared to conventional low-voltage links.  The remainder of the paper is organized as follows: in  Section II, we present an overview of related work in the areas  of temperature-aware systems. Section III details the impact of  temperature variation on the interconnect link, including both  devices and wire resistance. The implementation of the  proposed approach is provided in Section IV, with results and  comparison with prior approaches included in Section V. In  Section VI, we discuss other design considerations, and  conclusions are given in Section VII.  II. RELATED WORK  One of the simplest and most effective approaches for  reducing interconnect power consumption is to reduce the  supply voltage [7]. Reduced swing approaches vary widely in  complexity, from a simple  low-swing driver and  level  converter [7] to advanced signaling methods such as lowvoltage differential signaling (LVDS) [13], pulsed bus  signaling [14][15], and the use of a high-voltage boost to  improve low-voltage transition delays [1][4][16]. Adaptive  techniques such as dynamic voltage and frequency scaling are  also effective [2][17], although  they require additional  overhead systems. While each of these techniques reduces  power consumption,  low voltage  links have  increased  susceptibility to process, voltage, and temperature (PVT)  variations.   To tolerate variations, worst-case designs use PVT corner  analysis to ensure that systems function properly under the  majority of operating conditions. In older technologies, a single  temperature corner was sufficient to determine worst-case  requirements, but  the need  to  include  the reversal of  temperature variation has recently led to the temperature corner  being split into two separate corners [18]. Aside from the worst  case corners, a large amount of research has been performed on  temperature modeling to predict thermal issues at design time  and examine ways of avoiding potential problems [19]-[23].  These temperature models have resulted in a number of design  time temperature-aware techniques such as floorplanning [21],  routing [24], and coding [25]. The models have also facilitated  runtime techniques such as temperature-aware scheduling with  traffic throttling [26][27], and voltage/frequency throttling  [20]-[30]. These designs do not take into account the changes  in the temperature dependence at different voltages.    A number of methods have been proposed to create  temperature-insensitive designs, either taking advantage of the  temperature-insensitive voltage [11] or adjusting the threshold  voltage to achieve temperature insensitivity [31]. Additional  approaches to achieve temperature-insensitivity include the use  of multiple threshold designs to balance the dependences of  high-VT and low-VT logic cells [32]. These approaches are  restricted to a select range of voltages, limiting their potential  energy improvements. The approach proposed in this work  ( ) ( ) ( ) = T 0 [ TV ( R )0 ) ]0 takes advantage of low swing voltages and temperature-aware  system design to improve system energy while limiting the  impact of temperature variations.  III. TEMPERATURE VARIATION IN LOW-VOLTAGE LINKS  There are three major parameters affected by temperature in  low-voltage links—MOSFET mobility, MOSFET threshold  voltage, and wire resistance. The temperature relationships for  these parameters are provided in (1)-(3) [10][23]  μμ T = / TT = (1)  ) μα α + ( ( 0 0 (2)  TT − V TV T + α 0 1 TR R TT − (3)  where T is the temperature, T0 is the nominal temperature, μ0 is  the mobility at T0, VT0 is the threshold voltage at T0, R0 is the  resistance at T0; αμ, αVT, and αR are empirical terms named the  mobility temperature exponent, threshold voltage temperature  coefficient, and resistance temperature coefficient, respectively,  where αμ = -2, -1 mV/°C ≤ αVT ≤ -4 mV/°C, and αR in Cu is  0.004.   In the temperature region of concern (between -55°C and  125°C), both μ and VT decrease with increasing temperature;  however, examining the saturation drain current of a MOSFET,  = μ 1 WC 2 L we see that decreasing μ decreases ID, while decreasing VT  increases ID. To determine how ID changes with temperature,  we examine four values—(i) the change in μ for a change in  temperature, ∂μ/∂T, (ii) the change in VT for a change in  temperature, ∂VT/∂T, (iii) the impact of a change in μ on ID,  ∂ID/∂μ, and (iv) the impact of a change in VT on ID, ∂ID/∂VT.  The  temperature dependence of  the MOSFET can be  determined by summing the impacts of μ and VT on ID,  ∂ I ∂= I ∂+ I ∂= μ ∂⋅ I ∂+ V ∂⋅ I ∂ T ∂ T ∂ T ∂ T ∂ μ ∂ T ∂ V (4)  (5)  satD , )2 V V − ( GS I ox D D D D D T T Tot μ V T T In (5), ∂ID/∂T|μ is negative, and ∂ID/∂T|VT is positive. At  nominal voltages in the 65 nm technology used in this work,  ∂ID/∂T|μ > ∂ID/∂T|VT; thus, circuits at nominal voltages become  slower as temperature increases (shown by the positive delay  changes in Table I). At lower voltages, where the link operates,  ∂ID/∂T|μ < ∂ID/∂T|VT, thus the devices in the link get faster as  temperature increases (shown by the negative delay changes in  Table I). This reversal of the temperature dependence was first  noted by Park, et al. [33], and is also referred to as an inversion  of temperature dependence [34]; the original study found the  reverse temperature dependence to occur only at ~40% of  nominal voltages, and as technology has scaled the reverse  temperature dependence is approaching nominal voltages [12].  To determine how temperature affects the link delay as a  whole, (5) must be modified to include the change in wire  resistance. In the simulations in this paper, we include both the  temperature dependence of the MOSFET and the temperature  dependence of the wire resistance in all results.   108           IV. TEMPERATURE-AWARE LOW-VOLTAGE LINK DESIGN  Temperature variation is a particularly important design  consideration for low-voltage systems. One way to address  these variations is the use of temperature-aware systems, which  detect temperature-induced delay changes and adjust the supply  voltage to maintain a target frequency [35][28], as shown in  Fig. 2(a). When higher voltages are needed to maintain the  frequency, the energy efficiency of the system is reduced.   In this work, we propose an alternative method of  maintaining a  target  frequency  that  improves power  consumption and reduces the impact of temperature variation  on link delay. To achieve these improvements, we exploit the  different temperature dependences in the low-voltage link and  nominal voltage transceivers. As temperature decreases, link  latency increases and transceiver latency decreases. To offset  this increase in link latency, we adaptively borrow the  additional temperature-induced slack in the receiver buffers.   To implement the proposed method, we first determine the  number of stages in the transmitter and/or receiver that will be  available to borrow delay slack. Then, we select the lowest  voltage that can meet the target frequency at the ‘slowest’  temperature (e.g. -55°C for the reverse temperature dependence  region) with this pre-determined timing slack; this voltage must  also meet the timing requirement with no borrowed slack at the  ‘fastest’ temperature (e.g. 125°C for the reverse temperature  dependence region). The temperature range (-55°C–125°C) is  separated into a set of temperature regions, depending on the  desired granularity. In Fig. 2(b), we use four regions each  spanning a 45°C range. With these regions defined, we create a  look-up table that properly selects the transmitter and/or  receiver buffer clock phases depending on the operating  temperature. This timing borrowing greatly reduces the impact  of temperature variation on system delay, enabling the  proposed system to maintain a single low link voltage over the  entire range of temperatures, as shown in Fig. 2(b). Fig. 2  shows the timing slack in the link, which decreases with  decreasing temperature until some minimum slack point is  reached. At this point, in the conventional approach the link  voltage is raised, which increases the slack (and consumes  additional power); in the proposed approach, the decrease in  (a)  Fig. 2. Comparison of (a) conventional temperature-aware adaptive-voltage  system and (b) proposed temperature-aware delay borrowing system.  (b)   temperature  increases  the available delay slack  in  the  transceiver, allowing additional delay to be borrowed by the  link (shown in Fig. 2(b)). By maintaining a lower voltage, the  proposed system can achieve large energy savings over  conventional designs.  Fig. 3 shows the temperature-aware delay borrowing  system. The output buffer of the transmitter drives a lowvoltage link, with level shifters shown on the receiver end that  restore the signal swing for the nominal voltage receiver buffer.  Fig. 3. Diagram of temperature-adaptive delay borrowing system.  109         (a)  Fig. 4. Temperature sensor implementation.  (b)  Fig. 6. (a) Temperature sensor and (b) LUT operation.  Fig. 5. Look-up table to select delayed clock phase based on temperature  input.  Fig. 7. Timing diagram of temperature-aware delay-borrowing.   On the link, a temperature sensor provides regular status  updates to the delay-borrowing units in the transmitter and  receiver. Temperature changes at a rate of approximately  1 µs/°C [36], so the sensor polling rate is on the order of  1 MHz, depending on the desired accuracy. In the delayborrowing units, the temperature sensor output is converted  into a clock phase select signal by the LUTs, and the  appropriate phase is applied to the buffers. For example, at  nominal voltage the transceiver buffer Clk(cid:198)Q delay varies by  10.6% over  the  -55°C–125°C  temperature  range. As  temperature decreases, the link delay increases and the buffer  delay decreases; the nominal-voltage buffer will have up to  10.6% delay slack that can be borrowed by the link at low  temperatures. If the delay borrowing unit is designed to trade  delay with two buffers, the total amount of delay slack  becomes 21.2%, etc. As shown in the system diagram in Fig. 3,  additional buffers (or nominal voltage logic stages) may be  added to the delay borrowing units by adding a small delay  element to further offset the clock phase of subsequent stages.  The design of the temperature sensor and LUT are shown in  Figs. 4 and 5. The temperature sensor in Fig. 4 consists of two  sections, a temperature-dependent ring oscillator gated by an  enable signal, and a pulse counter to convert the oscillator  frequency into a digital output [37]. For each sensor reading,  the enable signal is applied for a fixed period depending on the  desired sensor resolution (as the enable period increases, the  impact of a small change in temperature causes a larger change  in the pulse count). When the enable signal is asserted, the  isolation circuitry at the bottom of Fig. 4 separates the  temperature sensor output from the rest of the system to  prevent unnecessary toggling. After the enable signal is  unasserted, the pulse counter passes a digital readout of the  temperature to the LUT.  The LUT in Fig. 5 sets the vector Out to ‘112’ when  temperatures lower than -10°C are detected, sets the vector Out  to ‘102’ when temperatures lower than 35°C are detected, and  so on as shown in Fig. 6, which is the simulated response of the  temperature sensor and look-up table in a commercial 65 nm  technology. As shown, both the temperature sensor and lookup table are very small circuits, with very little cost in terms of  complexity or overhead energy. For a 200 ns enable period, the  worst-case energy per sensor reading is 14.5 pJ, which provides  a worst-case temperature resolution of 5°C.   An example timing diagram of the system operation is  shown in Fig. 7. Assume that the initial temperature is 28°C,  resulting in a temperature sensor output Sensor_Output = ‘51’  and LUT_Value of ‘102’. A change in temperature will be  detected after some number of clock cycles (depending on the  sensor sampling rate and enable period), resulting in a change  in Sensor_Output. After the enable signal in the temperature  sensor is unasserted, the sensor output is passed to the LUT,  which determines if the system has transitioned to a new  temperature region and adjusts the delay-borrowing unit  accordingly. In Fig. 7, the LUT value transitions from the ‘102’  region to the ‘012’ region, applying a new clock phase to the  flip-flop. The system then continues polling for new changes in  temperature.   110                 Fig. 8. Piecewise rising and falling edge delay for a 600 mV link.   TABLE II. GLOBAL WIRE PARAMETERS.  Parameter  R (Ω / mm) @ 27°C  Substrate Capacitance, Cg (fF/mm)  Coupling Capacitance, CC (fF/mm)  Value  75  75  85  V. RESULTS  A. Experimental Setup  Each of the simulations presented in this paper was  generated using a commercial 65 nm process with low power  svt devices. The link was simulated using a distributed RC link  model of three parallel wires, with 1 mm segments and the  global wire and coupling parameter values in Table II. The  values in Table II were generated using the commercial 65 nm  process information and a global interconnect parameter  calculator [38]. A worst case delay input pattern was used  (‘010’(cid:198)’101’ transitions) for all simulations. A 0.5 switching  activity factor was used to calculate the power consumption.  The activity factor affects the magnitude of the power  dissipation in both the proposed and conventional methods;  however, the percentage power improvements achieved by the  proposed method are independent of the switching factor.  B. System Characterization  Before evaluating the benefits of the proposed delay  borrowing scheme, we study  the  impact of changing  temperature on the rise and fall time of a low voltage link. In  Fig. 8, we separate the rising and falling edge delays into five  segments—the Clk(cid:198)Q delay of the transmitter flip-flop, the  driver latency (Q(cid:198)Driver Out), the interconnect latency  (Driver Out(cid:198)Link End), the level shifter latency (Link  End(cid:198)LvlShift Out), and the setup time of the receiver flipflop. The rising and falling edge delays are normalized to  the -55°C condition to show which components increase or  decrease in delay (as well as the change in total path delay)  over the entire temperature range. The output of the transmitter  flip-flop has a full swing between 0 V and VDD = 1 V, while the  Fig. 9. Impact of delay borrowing on power consumption.  TABLE III. SUPPLY VOLTAGES SATISFYING 1 GHZ AND 500 MHZ  FREQUENCY REQUIREMENTS IN PROPOSED AND CONVENTIONAL  METHODS.  Target  Frequency  1 GHz  500 MHz  Method  Proposed  Conventional  Proposed  Conventional  Link Length (mm)  2 mm  645 mV  720 mV  595 mV  615 mV  3 mm  675 mV  780 mV  615 mV  640 mV  1 mm  605 mV  660 mV  565 mV  580 mV  low-voltage driver is an inverting buffer with supply voltage  VLow (in Fig. 8, VLow = 600 mV).   As expected, the nominal voltage flip-flop delay increases  as temperature increases, while the low-voltage driver delay  decreases as temperature increases. The interconnect delay also  increases with increasing temperature, as described by (3). The  level shifter used in these simulations is the conventional level  shifter described in [7]; its delay decreases with increasing  temperature. Finally, the setup time requirement for the  receiver flip-flop increases with increasing temperature. It is  important to be aware of the difference in rising and falling  delays to avoid race conditions when using delay borrowing.  C. Comparison with Conventional Low-Swing Link  As described in Fig. 2, delay borrowing enables us to  achieve a target frequency using a lower link voltage than  would otherwise be possible. The amount of voltage  improvement depends on the percentage of the clock cycle that  is borrowed, which in turn depends on the number of  transceiver buffers used to retime the link (as described in  Section IV). The power consumption in Fig. 9 is normalized to  the 0% delay borrowing case. As shown, the improvement in  power achieved by delay borrowing depends both on the target  frequency and the borrowed portion of the clock cycle. At a  1 GHz frequency, borrowing 60% of the cycle period results in  a power reduction of 34% in a 3 mm link. At 500 MHz, the  power reduction is considerably less. The benefits are reduced  at low frequencies because the target frequencies can be met at  lower voltages, leading to a smaller voltage difference between  the proposed delay borrowing technique and the conventional  technique (at low voltages, a small change in voltage causes a  large change in delay).   111         (a)  Fig. 11. Power savings of proposed method over conventional method over the  entire temperature range for a 3 mm link with target frequency of 1 GHz.  (b)  Fig. 10. Maximum power savings of the proposed method over conventional  low-swing approach vs. link length at (a) 1 GHz and (b) 500 MHz.  The voltage reductions achieved are shown in Table III for  link lengths of 1 mm, 2 mm, and 3 mm borrowing 60% of the  clock cycle. At the 500 MHz target frequency, delay borrowing  only improves the link voltage by 15 mV for a 1 mm link,  compared to a voltage improvement of 55 mV for a 1 GHz  frequency target. The data in Fig. 9, Fig. 10, and Table III were  generated by setting a target frequency and lowering the  voltage in 5 mV increments until the receiver flip-flop correctly  latched the data at -55°C (the worst-case temperature in the  reverse temperature dependence region). Fig. 10 compares the  power consumption of the proposed delay borrowing technique  and conventional low-voltage link at the frequencies and link  lengths from Table III. Reported power consumptions are for a  64-bit link with 0.5 activity factor at the voltages indicated in  Table III. As shown, the improvement in power consumption  increases for longer link lengths and faster frequencies; these  require larger voltages to meet the target frequency, resulting in  larger voltage differences between the methods and larger  improvements in power performance.   Fig. 11 shows the improvement in power consumption over  the entire temperature range for a 3 mm link achieving a 1 GHz  frequency target. The link voltages are optimized for the four  temperature regions in Fig. 6(b); in the proposed approach,  60%  of  the  clock  cycle  delay  is  borrowed  when  -55°C ≤ T < -10°C,  40%  delay  is  borrowed  when -10°C ≤ T < 35°C, 20% delay  is borrowed when  35°C ≤ T < 80°C,  and no delay  is borrowed when  80°C ≤ T ≤ 125°C. The maximum power savings of 40%  occurs at 5°C.  Fig. 12. Comparison of the delay variation of a conventional system operating  at 740 mV (VINS), a conventional system operating at 675 mV, and the  proposed system operating at 675 mV.   D. Comparison with Temperature-Insensitive Operation  Another common use of low-voltage design is to take  advantage of the temperature-insensitive voltage (VINS), at  which a change in temperature has very little impact on system  power and delay. This design decision is an excellent choice  for improving system reliability [11]; however, operating at  VINS imposes specific constraints on both power consumption  and latency. In the commercial 65 nm technology used in this  work, VINS occurs at ~740 mV; thus, systems operating at VINS  are  incapable of  taking advantage of  the high speed  performance of nominal voltage designs, while also being  unable to take advantage of the low power performance of lowvoltage designs.  One aspect of the temperature-aware delay borrowing  system that we have not yet discussed is the reduced impact of  temperature variation on the overall system (including the link  and any transceiver buffers being borrowed from). Fig. 12  compares the delay performance of the proposed approach  operating at 675 mV (the voltage achieving a 1 GHz target for  a 3 mm link, from Table III), a conventional link operating at  675 mV, and a conventional link operating at VINS.   The delays in Fig. 12 are normalized to the values at 125°C.  The system operating at VINS has a delay variation of 1.6% over  the entire temperature range, while the proposed method has a  3.4% delay variation over the entire range. Without the use of  112                   delay borrowing,  the  temperature-induced delay change  between -55°C and 125°C exceeds 23%. Thus, the delay  borrowing method  improves  robustness  to  temperature  variations by over 85%.  E. Area Overhead  Unit areas were calculated by summing the standard cell  widths for each standard gate and using the sum of the gate  width and the nwell design rule for the link driver and level  shifter, all from a commercial 65 nm process. The total area of  the temperature sensor in Fig. 4 is 122.4 μm2, the total area for  the LUT in Fig. 5 is 61.4 μm2, and the area for the each delay  borrowing unit in Fig. 3 (including the LUT, three 4:1  multiplexors, and eight buffers) is 131.7 μm2. Each link wire is  assumed to have three transmitter flip-flops and three receiver  flip-flops; combined with the driver and level shifter (ignoring  the global wire area), this results in each wire transceiver  having a total area of 247.4 μm2. Thus, for a 16-bit link, the  area overhead of the proposed approach shown in Fig. 3 (with  two delay borrowing units and one temperature sensor) is  9.7%. For a 64-bit link, the area overhead reduces to 2.4%.  Thus, multiple sensors may be integrated on each link at a  relatively low overhead cost.  VI. DISCUSSION  A. Integration of Multiple Temperature Sensors  For simplicity, we have assumed that the link temperature  profile is uniform, although this is clearly not always the case  [23][19]. For non-uniform temperature profiles, the proposed  system may still be used, although some additional overhead is  required. Depending on the expected temperature profiles,  multiple temperature sensors may be placed along the link or at  the link endpoints. To integrate these into the proposed system,  the look-up tables can be adjusted to select a weighted average  of the sensor inputs, and the clock phases must be adjusted to  ensure that the worst-case temperature requirements are met.  Exponentially-distributed thermal profiles have been shown to  induce up to 7% error in circuit delay calculations compared to  using an average link temperature, while a poorly located  temperature sensor can result in delay errors in excess of 20%  depending upon the gradient of a local hotspot [23].   B. Integration with Error Control Coding  One other important assumption that we have made is that  the link is reliable and not susceptible to errors; of course, this  is rarely the case. There are a number of solutions to improve  the reliability of on-chip interconnect links [39][40][41], such  as the use of error control coding. Error control encoders and  decoders slightly complicate the delay-borrowing scheme,  although the system can still function as described with a minor  modification. Rather than progressively borrowing the delay  from the buffers as mentioned in Section IV, the buffer clock  phases must be adjusted to skip the stage of the encoder and  decoder, which likely would not have as much slack as the  neighboring empty buffer stages.  Fig. 13. Level shifter delays at VLow = 0.6 V and VHigh = 1 V for two input  slew rates.   C. Level Shifter Design for Multiple Temperature  Dependence Systems  Level shifters are a particularly interesting topic when  discussing multiple temperature dependences, as they operate  at both VHigh and VLow. The internal inverter uses a low-voltage  supply, getting faster as temperature increases, while the fullswing cross-coupled inverters operate at nominal voltage,  decreasing in speed as temperature increases. In Fig. 13, we  examine the rising and falling edge delays (td_rise and td_fall,  respectively) for a level shifter with VLow = 0.6 V and VHigh =  1 V driven by an input with two different slew rates. As shown,  for a fast slew rate (10 ps) the rising delay increases with  increasing temperature, while the falling delay decreases with  increasing temperature. The different temperature dependence  of the rise and fall times is related to the different impact of  temperature on NMOS and PMOS devices [12]; NMOS  devices operate in the reverse temperature dependence region  at much higher voltages than PMOS devices. This causes the  falling edge delay to decrease with increasing temperature,  while  the rising edge delay  increases with  increasing  temperature. When the slew rate is increased to 1 ns, both the  rising and falling delays decrease with increasing temperature  (this matches the level shifter response shown in Fig. 8); thus,  the temperature response of the level shifter depends on both  the level shifter p-n ratios and the input slew rate. These  dependences (as well as other issues in multiple dependence  level shifters) will be examined in more detail in future work.  VII. CONCLUSIONS  In this paper we have presented a temperature-aware delay  borrowing system for low-voltage interconnect links. The  system  is shown  to achieve  improvements  in power  consumption of 40% as well as an 85% reduction in  susceptibility to temperature variation. The combination of  improved power performance and reliability come at a very  low overhead cost, less than 1% energy overhead for a 64-bit  link. In addition, the reliability improvements make the  proposed method a viable replacement for use of  the  temperature insensitive voltage in low-swing link designs,  allowing delay variations as low as 3.4% while enabling the  use of a wider supply voltage range to improve performance.  113   "
A Low-Cost Deadlock-Free Design of Minimal-Table Rerouted XY-Routing for Irregular Wireless NoCs.,"To bridge the widening gap between computation requirements of terascale application and communication efficiency faced by gigascale multi-processor system-on-chip devices, a new on-chip communication system, dubbed Wireless Network-on-Chip (WNoC), has been proposed. This work centers on the design of a high-efficient, low-cost, deadlock-free routing scheme for domain-specific irregular mesh WNoCs. A distributed minimal table based routing scheme is designed to facilitate segmented XY-routing. Deadlock-free data transmission is achieved by implementing a new turn classes based buffer ordering scheme. The simulation study demonstrates high routing efficiency, low cost and scalability of the routing scheme and the promising network performance of WNoC.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip A Low-Cost Deadlock-free Design of Minimal-Table Rerouted XY-Routing for Irregular Wireless NoCs Ruizhe Wu Yi Wang Dan Zhao The Center for Advanced Computer Studies University of Louisiana at Lafayette Email: {rxw2563,yxw4316,dzhao}@cacs.louisiana.edu Abstract—To bridge the widening gap between computation requirements of terascale application and communication efﬁciency faced by gigascale multi-processor system-on-chip devices, a new on-chip communication system, dubbed Wireless Networkon-Chip (WNoC), has been proposed. This work centers on the design of a high-efﬁcient, low-cost, deadlock-free routing scheme for domain-speciﬁc irregular mesh WNoCs. A distributed minimal table based routing scheme is designed to facilitate segmented XY-routing. Deadlock-free data transmission is achieved by implementing a new turn classes based buffer ordering scheme. The simulation study demonstrates high routing efﬁciency, low cost and scalability of the routing scheme and the promising network performance of WNoC1 . I . INTRODUCT ION With silicon technology scaling, multi-processor Systemson-Chip (MPSoC) are moving towards many-core structures with distributed architecture where a number of processing cores with memory are interconnected by a high-speed onchip communication network to support advanced computing trends such as terascale computing. Due to the stringent performance and power limitation, the state-of-the-art sharedbus and point-to-point connections have been shown unable to supply nanoscale MPSoCs (where hundreds or even thousands of cores are embedded) with both sufﬁcient bandwidth and low latency. Network-on-Chips (NoCs) are emerging as an alternative communication platform for complex MPSoCs. Various topologies have been proposed for NoCs, such as Mesh, Torus, folded-torus, Star, Octagon, and SPIN [1]. Among them, mesh and torus/folded-torus topologies are more popular due to its simplicity, regularity and power-throughput effectiveness [2], and thus are more suitable for homogeneous SoCs such as MIT RAW processor [3] and Intel’s 80-tile processor [4]. For such regular topologies, routing algorithms are usually classiﬁed into deterministic routing or adaptive routing. In the former, the routing path between a sourcedestination pair is ﬁxed, such as the XY-routing. In the latter, the routing path is partially or fully adaptive to the network trafﬁc and channel status to avoid network congestion or faulty routes, such as Odd-Even [5] and Negative First Turn [1] models. On the other hand, heterogeneous SoCs are rapidly emerging where various functional modules dedicated for accelerating speciﬁc computation of target applications are integrated 1 This research was supported in part by the National Science Foundation under grant number CNS-0821702 and CCF-0845983. on a single chip. As a result, irregular topologies are formed. Recently, several routing schemes have been proposed to tackle irregular sub-networks or networks, which can be classiﬁed into source or distributed routing. In source routing, the source node computes the routing path and inserts a sequence of intermediate routers in the packet header to reach the destination. In distributed routing, routing tables are distributed at the intermediate routers. As source routing involves high overhead stemmed from the centralized routing algorithm and large packet size, distributed routing is more appropriate for medium-scale static on-chip networks to achieve low routing algorithm complexity and simplify hardware implementation. Several approaches address routing in irregular mesh NoCs [6]–[8]. In [6], a linear programming based routing algorithm has been proposed that generates routers for the communication traces so as to minimize the total number of routers used in the topology. An adaptive routing algorithm has been developed in [7] based on the Odd-Even Turn model for partially irregular mesh. A routing table minimization scheme has been presented in [8] that reduces deviation tables for both distributed and source routing approaches under speciﬁc communication patterns. In the above mentioned approaches, routing is either part of ﬂoorplanning or placement optimization or is correlated to speciﬁc trafﬁc traces. Aiming at low-cost and high-efﬁcient implementation, we propose in this paper a distributed minimal table based routing scheme for network-on-chip under irregular topologies. Without loss of generality, we use wireless network-on-chip platform to illustrate the idea and evaluate the performance. Segmented XY-routing decision is made by utilizing the geographic distance between a node and its destination. Deadlockfree data transmission is achieved by implementing a new turn classes based buffer ordering scheme. A. Alternative NoC architecture: Wireless NoC With the high bandwidth, low power and ultra-short range communication provided by the recently developed UWB interconnect technology [9], we deploy wireless radios on chip in replace of wires to increase accessibility, to improve bandwidth utilization, and to eliminate delay and crosstalk noise in conventional wired interconnects [10]. The high ﬂexibility and free-of-wiring make UWB-I an attractive solution for the onchip communication of heterogeneous MPSoCs with diverse size, trafﬁc pattern and computation power. Thereafter, we propose to establish an irregular wireless NoC (WNoC) platform where a number of RF nodes, equipped with low-power 978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.29 199 and low-cost transceivers and tiny antennae, are dispersed onchip to form a multihop wireless nanonetwork for packetized data communication. For low cost on-chip implementation, a single wireless channel is shared by all RF nodes. A node may receive packets from any nearby nodes if they are within its transmission range, and thus are connected by wireless links. Due to the irregularity of the processor tiles, minimal number of RF nodes at the predetermined transmission range are placed to maintain full wireless coverage chip wide. One representative and hypothetical example of heterogeneous SoC ﬂoorplan and its correspondingly formed irregular topology is illustrated in Fig. 1 (a). Without loss of generality, each processor tile in a heterogeneous MPSoC contains a processor with cache and an interface to the WNoC. A set of RF nodes are distributed on chip with the transmission range of T . Any two nodes fall within each other’s transmission range are connected by a wireless link (dash line). A collection of RF nodes connected by wireless links form the irregular mesh for data communication in a domain-speciﬁc MPSoC. For the example WNoC with 21 RF nodes, some RF nodes work as hosts (e.g., node 1) attached with processor tiles while the others are routers (e.g., node 4) forwarding packets throughout the network. Each RF node mainly implements routing, channel arbitration, and buffering mechanisms for high-speed cost-efﬁcient on-chip communication. The rest of the paper is organized as follows. We propose a low-cost high-efﬁcient minimal table-rerouted XYrouting scheme for application-speciﬁc irregular mesh WNoC in Sec. II. We then propose a turn classes based deadlock avoidance scheme in Sec. III. In Sec. IV, the performance of the routing scheme is evaluated in terms of routing efﬁciency, cost and scalability. WNoC network performance are further assessed by experiments. Finally, we conclude this paper. I I . G EO -A S S I ST ED IRREGULAR -M E SH ROUT ING SCHEME Based on the insights obtained from the prior work, we explore highly efﬁcient low cost routing schemes in WNoC, especially under irregular network topologies. Considering an irregular mesh WNoC, we propose a minimal table rerouted XY-routing (MTR-XY) scheme by taking the advantage of geo-assisted approaches and at the same time avoiding the problems in prior routing schemes. A. The Proposed MTR-XY Routing Scheme We use the irregular mesh WNoC shown in Fig. 1 (a) as the starting point. In order to uniquely identify the RF nodes, each node is statically assigned an ID and a XY-coordinates address Ri (xi , yi ) that indicates its physical location in the topology. Initially, we apply the XY-routing2 where the data packets are routed through X-direction, nullifying the offset and then Y-direction from source to destination. For instance in Fig. 1 (b), the XY-routing from the source S (3, 1) to destination D 2XY-routing is a simple variation of dimension order routing for regular two dimensional mesh topologies. It produces minimal paths without redundancy. (4, 3) will start from node 7, move on horizontal direction to node 8 and then on vertical direction through node 13, and end at node 16. We deﬁne such a path a XY-routable path. In other words, S (node 7) is XY-routable to D (node 16). The main challenge is to address possible fatal routing on an irregular mesh. For example, no wireless link exists between nodes 3 and 11 due to the placement of processor tile. As a result, S1 (node 2) is not XY-routable to D1 (node 19). We denote such a case around-the-tile XY-routing failure. The other failure case is the dead-end paths where no further wireless links exist with the consideration of minimizing the RF nodes placement cost (e.g., there is no wireless connection between nodes 1 and 9 without the placement of RF node in between them). Thus, S2 (node 13) is not XY-routable to D2 (node 1). Hereafter, we name such fatal XY-routing paths, the XY-dead paths. A possible low-cost loop-free alternative is to properly apply distributed table-based routing to circumvent these dead paths which are not XY-routable. To this end, we address four challenging issues: identifying XYdead paths between any source-destination pairs; ﬁnding XYhop count-driven alternative paths to replace the dead paths; and construction of minimal routing tables for the RF nodes. 1) XY-Dead Path Replacement: We ﬁrst need to run XYrouting on all source-destination pairs in order to identify all XY-dead paths in an irregular mesh WNoC. All dead paths are marked by recording the corresponding sourcedestination pairs. We then apply the replacement algorithm (will be discussed next) to ﬁnd alternative paths for these source-destination pairs so as to replace the dead paths, and we name them XY-replacement paths. Note that, the obtained XY-replacement paths are not XY-routable. Meanwhile XYroutable paths guarantee path-shortest. Without loss of generality, we introduce two terminologies for the derivation of routing scheme. The bridge nodes are the intermediate nodes along a XY-replacement path of a sourcedestination pair which partition the path into several segments where each segment is XY-routable. For example, as shown in Fig. 1 (c), the path between source S (node 1) and destination D (node 19) is a XY-replacement path to replace the dead path due to the around-the-tile failure. If we allocate nodes N as the bridge node, the two partitioned segments of S→N and N→D are XY-routable separately. We further introduce XY-hop count. For any two nodes A and B, if a packet can be XY-routed from A to B, we say the XY-hop count between A and B is 1. In other words, within one XY-hop, B is XYroutable from A. If a replacement path needs the minimum of n bridge nodes to perform segmented XY-routing successfully, the resulted XY-hop count is n + 1. One critical design issue is to allocate minimal number of bridge nodes along a XY-replacement path or to reduce the XY-hop count of the path. In this way, we only need to create routing tables at the source and bridge nodes, while other nodes simply make XY-routing decision towards their next bridge node. Minimization of bridge nodes or XY-hop count thus achieves the best routing efﬁciency while minimizing the routing table cost. 200 (c) Illustration of a XY−replacement path S                   with a bridge node N D S D N N: bridge node D1: around−the−tile fatel routing D2: dead−end path fatel routing (b) Illustration of XY−dead paths 19 18 17 20 21 14 15 16 9 10 11 12 13 6 8 1 2 3 4 5 D S 7(3,1) (4,3) S1 S2 D2 S2 D1 S1 processor tile wireless link (a) Illustration of a irregular mesh WNoC 19 18 17 20 21 14 15 16 9 10 11 12 13 6 8 1 2 3 4 5 7 T RF node 17 14 18 19 20 21 16 13 12 11 10 15 9 1 2 3 4 5 7 8 6 5 6 7 8 9 10 11 17 18 21 13 12 16 15 14 19 20 D S1 S2 S3 S4 N (d) Routing table reduction 4 RT D    N 1 2 3 Fig. 1. Illustration of routing on an irregular mesh WNoC. In a nutshell, after we identify all dead paths, we determine their replacement paths in a way that minimizes the number of bridge nodes or XY-hop count on each replacement path in order to facilitate segmented XY-routing between the associated source-destination pairs. 2) XY-Hop Count-Driven Path Replacement Scheme: In order to improve routing efﬁciency while reducing routing table cost, we propose a minimum XY-hop count-driven replacement path ﬁnding scheme. Given an irregular mesh WNoC, we represent all XYroutable paths between any source-destination pairs in a directional XY-connectivity graph (GX Y C ) where each vertex Vi denotes a source or destination RF node in the WNoC and a directed edge Ei,j connects two vertices Vi and Vj if the path from the source Vi to the destination Vj is a XY-routable path. A weight Wi,j is associated with each edge Ei,j which is deﬁned as MAX HC ×lxyhci,j lhci,j indicate the XY-hop count and the (path) hop count of edge Ei,j respectively, and MAX HC is the maximum among the hop counts for all edges. Note that the XY-hop count for any link in graph GX Y C is one, thus lxyhci,j = 1. We deﬁne the edge weight in such a way that tackles the cases where two or more paths between a source-destination pair may be found that result in the same minimum number of XY-hop count but different hop count along different path. We would like to choose the one that achieves the minimal hop count among them. We then apply a shortest path algorithm (such as Dijkstra’s algo. [11]) to GX Y C to ﬁnd the shortest path in terms of XY-hop count to replace the XY-dead path between the associated source-destination pair. We record the intermediate nodes along the newly allocated XY-replacement path on GX Y C as the minimum needed bridge nodes to perform the segmented XY-routing from the source to the destination. Once all replacement paths are obtained and the bridge nodes are allocated on the replacement paths, the next step is to properly construct the routing table at each node so that the bridge nodes can be used as the intermediate destination nodes to perform XY-routing segmentally before approaching the destination. 3) Minimal Routing Table Construction: Each node may maintain a routing table (RT) with each entry corresponding to the next bridge node address (NBA) for a particular replacement path indexed by its destination address (DA). More + lhci,j , where lxyhci,j and speciﬁcally, the source node and the bridge nodes along a replacement path will construct and update their routing tables in a way that adds the next bridge node to their entry as shown in the Pseudocode of Algorithm 1. We use the path from S to D in Fig. 1 (c) as an example. By applying Algorithm 1, only the source S will maintain an entry of N while all other intermediate nodes including bridge node N don’t have the routing table entries for the path S → D . Each routing table contains two columns for DA and NBA respectively. Indexed by the DA, the current node may ﬁnd its NBA from the routing table so as to perform XY-routing towards the next bridge node. Note that, no entries exist for the none-bridge nodes and XY-routable paths. If none of the paths starting from a node is a XY-replacement path, no routing table is maintained by the node. Algorithm 1: Minimal Routing Table Construction Input: P = {Pi |i ∈ Np }, B = {Bi |i ∈ Np}; Output: S truct RF N {ID, Addr, S truct RT [Nent ]{DA, N BA}}, NRF N ; Nent = 0; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 for (i = 1 : Np ) do for (m = 1 : NRF N ) do if (RF N [m].ID == Si .ID) then break; end end Nent + +; RF N [m].RT [Nent ].DA = Di .Addr ; RF N [m].RT [Nent ].N BA = Bi [1].Addr ; for (j = 2 : N i for (m = 1 : NRF N ) do if (RF N [m].ID == Bj .ID) then b − 1) do break; end end Nent + +; RF N [m].RT [Nent ].DA = Di .Addr ; RF N [m].RT [Nent ].N BA = Bi [j + 1].Addr ; end end 4) Routing Protocol Design: Facilitated with the routing table and the XY-routing operator, we may perform the routing protocol at the RF nodes which successfully forwards the packets to their destinations with the minimum path cost as shown in Figure 2. Speciﬁcally, each packet contains in its packet header a one-bit tag, a destination address (DA) and a bridge-node address (BA). Upon receiving a packet, the node checks its packet header to make routing decision. The tag-bit is checked ﬁrst. If the tag is set, the node checks if its address matches with the bridge-node address or not. If they match, 201 the DA is used as the index to search in the node’s routing table. There are two possible cases. • If no entry corresponding to the DA is found, the tag is reset and XY-routing is performed from the current node to the destination. • Otherwise, the next bridge node along the replacement path is found. The BA in the packet header is updated with the next bridge node address obtained from the routing table entry. XY-routing is then performed from the current node to the next bridge node. If their addresses don’t match, the BA in the header is used as the intermediate destination address and XY-routing is performed towards this bridge node. If the tag is not set, the node is XY-routable to the destination. As can be seen, the DA in the packet header is used to run XY-routing to the next hop. MUX Selection Signals MUX Output TAG isMatch isFound 0 1 1 1 0 1 1 0 1 DA BA DA NBA Incoming Packet DA0 NBA0 LA DA . . . . . . . . . . . . . NBA DAn NBAn DA BA TAG LA =? isFound isMatch X U M XY Routing Next-hop ID DA BA TAG New Header Fig. 2. Illustration of the routing decision. B. Enhancement in Minimizing Routing Tables The routing tables can be further reduced by properly sharing the RT entries among several paths. For example, as shown in Fig. 1 (d), the four paths from sources S1, S2, S3, and S4 to destination D are not XY-routable, and need bridge node N to perform segmented XY-routing. Thus all four sources maintain a RT entry of N indexed by D. When we observe the four paths, we ﬁnd that the three paths of S1→D, S3→D and S4→D contain the path S2→D. As can be seen, node 2 is on the way of nodes 3 and 5 performing XY-routing to node 10. So instead of using the RT entry at node 3 or 5 to perform XY-routing to bridge node 10, we could simply force the packets initiated at node 3 or 5 to run XY-routing towards node 9. Once the packets arrive at node 2, they will use their destination D to check the routing table at node 2 and ﬁnd entry of node 10. Then XY-routing is performed towards node 10. In this way, we could remove the routing table entry of N at sources S3 and S4 for the two paths to D. While for the path of S1→D, as node 2 is the only neighbor of node 1, all packets initiated at node 1 must be forwarded to node 2, so no routing decision is necessary at node 1. We simply treat it as an special case and set node 2 as the next-hop of node 1. Again, we could remove the RT entry of N at source S1 and use the RT entry at node 2 to ﬁnish the remaining routing for the path S1→D. The routing table reduction scheme is described in the following pseudocode. We ﬁrst construct a reduced XYconnectivity graph RGX Y C (V , E ) which maintains the directed edges for all XY-replacement paths obtained by applying MTR-XY routing scheme. For each XY-replacement path Pij in graph RGX Y C (with |Pij | number of RF nodes on the path), we check all the intermediate nodes between source i and destination j by calling the function of Check RT which sets the ﬂag if the Check Node is on the way of i XY-routing to j before it is XY-dead and the Check Node has an entry to j . It is possible that multiple intermediate nodes are ﬂagged. We simply choose the latest ﬂagged Check Node as the Find Node and mark its RT entry to j by calling the function of Mark Entry. Finally, we remove all unmarked entries from the routing tables of the source nodes. By applying the routing tables reduction scheme, we may reduce the routing table cost by 42% in average (and up to 50%) as illustrated in Fig 3, where the routing table size after reduction is normalized to the one before reduction. Algorithm 2: Routing Table Reduction Input: RGXY C = (V , E ), and MTR-XY RTs; Output: Reduced MTR-XY RTs; for (i = 1 : |V |) do for (j = 1 : |V |) do if (i! = j && i is not XY-routable to j ) then Pij = XY-replacement path from i to j ; ﬂag1 = 0; for (k = 2 : |Pij | − 1) do C heck N ode = k|Pij ; ﬂag2 = Check RT(MTR-XY RTs, Check Node, j ); if (ﬂag2 == 1) then Find Node = k; ﬂag1 ++; end end if (ﬂag1 > 0) then Mark Entry(MTR-XY RTs, Find Node, j ); end end end end Remove unmarked entries from MTR-XY RTs of source nodes; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 1 e z i s l e b a t d e z i l a m r o N 0.8 0.6 0.4 0.2 0 4 6 8 10 12 14 Topology scale (n−by−n) 16 Fig. 3. Routing table cost reduction. I I I . D EADLOCK AVO IDANCE AND BU FFER ORGAN IZAT ION A. Dynamic Virtual Output Queuing For efﬁcient buffering, we propose a dynamic virtual output queuing strategy as shown in Fig. 4. More speciﬁcally, the buffers are organized in Nnei virtual output queues V Qi |i ∈ Nnei with a dynamically linked shared buffer, called free list. Each virtual queue corresponding to one of Nnei neighboring nodes temporarily stores the packets going to the dedicated neighbor. The shared buffer stores the incoming packet before its next-hop is determined by the RDL and consequently the packet is dynamically linked to the corresponding virtual output queue. In this way, Nnei dynamically sized virtual queues are formed which ensures dynamic allocation of upstream packets to a dedicated output queue corresponding to 202     a downstream node. In addition, the round-robin scheduler triggers the next non-empty and non-blocking output queue for fair queue management. Note that a backpressure based blocking scheme is implemented to control trafﬁc admission in the network. In particular, a blocking is set to a upstream node if the number of packets received from the node exceeds the blocking trigger level, e.g., Nc=1, and such blocking will be fed back to the source node to slow down the packet admission into the network. blocking signal to pre−hop Blocking signal generator Packet credit count register pre−hop ID next−hop ID packet header Routing Decision Logic Output Scheduler incoming packet free list Injection control Local PE n o i t m c i a a n c y o d a l l VQ1 VQ2 . . . . . VQn. . . . . . . Channel Arbitor outgoing packet blocking signal from next−hop Fig. 4. Conceptual RF microarchitecture. To minimize the buffering cost, we consider the worst case the same next-hop. In other words, all Nnei − 1 upstream scenario when multiple ﬂows crossing at a node contend for nodes of a node have packets to be forwarded to the same downstream node. To satisfy the worst case scenario, we need to maintain each virtual queue size of at least 1 (by setting the blocking trigger level to 1) and the shared buffer size of at least Nnei−2. As a result, the minimum buffer size is determined by Nnei + (Nnei − 2) = 2(Nnei − 1). With blocking trigger level of Nc , the minimum buffer requirement is 2Nc × (Nnei − 1). B. Turn Classes Based Deadlock Avoidance Scheme Routing in irregular mesh topology creates cyclic dependencies that may lead to deadlock. For example as shown in Fig. 5, data streams are delivered along three routing paths. RF node F wants to send packets to E, but E’s dedicated VQ is full and F has to wait. Meanwhile, nodes D and G are also waiting to send their packets as the dedicated VQs at nodes H and B are full respectively. Eventually, the three routing paths are blocked. The resulted circular dependency leads to deadlock, where multiple RF nodes involved in the cycle hold their packet buffer while waiting for the dedicated VQs at their downstream nodes to release buffer. A pkt@A C pkt@A pkt@H pkt@E pkt@E X D B X G H pkt@H pkt@E pkt@E pkt@A pkt@A X pkt@E E F pkt@A pkt@A pkt@A pkt@A pkt@A Fig. 5. Deadlock due to buffer dependency. We propose a new turn classes based buffer ordering scheme to resolve deadlock. The basic idea is similar to buffer ordering [12] with the introduction of a new set of buffer classes, namely turn classes to improve buffer ordering efﬁciency and reduce buffering cost. Speciﬁcally, each virtual queue maintains m units of buffer which are ordered into numbered classes, each associated with one buffer class. When a packet is generated at its source node, it is assigned the lowest buffer class of its dedicated VQ. Routing along the path, the packets are allocated in the buffer units from classes in ascending order. A packet belonging to a particular buffer class can only be allocated in its dedicated buffer unit. By imposing buffer ordering, routing is restricted to the use of buffers in ascending order so as to eliminate and prevent cyclic dependency. There are two popular ways to order buffers using either distance classes or dateline classes [12]. The former requires a packet at the ith hop from its source to acquire a buffer from class i, while the later assigns a higher order buffer to the packet whenever passing a dateline. Note that there exists multiple datelines in a complex irregular mesh. Both approaches involve large overhead especially when the MPSoCs scale towards 1000-processors integration. For example, distance classes require a number of packet buffers proportional to the diameter of the network (e.g., 2n for a n×n mesh) and dateline classes requires at least edge size of the network (e.g., n). For the example in Fig. 5, it requires 12 distance classes or 4 dateline classes to avoid deadlock. In order to reduce buffer classes to the minimum possible, we introduce turn classes, where a selection of two from eight turns deﬁned in the turn model [13] are used to order buffers into Ntc numbered classes (Ntc is determined by how often the selected two turns are made on a routing path). We develop a fast process to determine turn classes. We ﬁrst run our routing algorithm for any source-destination pairs and record the appearance frequency of each turn, excluding the four turns used by the XY-routing. Then we make a list of frequency for the remaining turns in ascending order. Following turn model regulation (that at least one turn from each of the two abstract cycles which are not at the diagonal positions must be eliminated to avoid deadlock), we will select one turn with the minimal frequency from each abstract cycle and use them to assign turn classes. In other words, each time when a packet makes either one of the two selected turns along its routing path, the buffer classes will be increased by 1. Based on this process, we minimize the turn classes by restricting the frequency of turns. For the example in Fig. 5, it only requires 2 turn classes which is 83% and 50% of improvement over distance classes and dateline classes, respectively. Assuming Ntc turn classes, the overall buffering cost will be 2Ntc × Nc × (Nnei − 1). IV. PER FORMANCE EVALUAT ION A. Simulation Setup In order to evaluate the performance of the proposed MTRXY routing scheme and the whole WNoC, irregular mesh 203 topologies are generated for the on-chip data communication within the MPSoC model. In WNoC, the MTR-XY routing scheme is implemented to determine the routing path for each sender and to propagate the packets through the network. Its performance is evaluated in terms of routing efﬁciency, cost and scalability. We further develop a WNoC simulator which also implements the single channel arbitration scheme as proposed in [10], dynamic buffering and trafﬁc admission control mechanism. An irregular mesh WNoC platform with identical and omnidirectional radio range is built to cover the communication among all processor tiles in a heterogeneous MPSoC. Each host RF node is associated with a trafﬁc generator which injects various trafﬁc patterns. We set the channel arbitration time of 3 clock cycles and the wireless bandwidth of 10Gbps. We introduce two parameters to study the impact of irregular topologies, Tscale and Lirr . The former represents the network scale while the later reﬂects the irregularity level of the topology at a given scale. We evaluate the network performance in terms of network throughput, aggregated bandwidth and end-to-end latency. The network throughput is deﬁned as the average data arrived at the destination during a unit time. The aggregated bandwidth is the overall amount of data being transmitted during a unit time. The end-to-end delay is deﬁned as the time spanning from the generation of a packet to the arrival of the packet at its destination. B. Routing Performance Evaluation We evaluate the performance of MTR-XY routing in terms of hop count, the overall routing decision time and routing table cost on various irregular WNoC mesh topologies. We compare our approach with the baseline table-driven shortest path routing scheme and Schafer’s irregular routing scheme [7]. The table-driven shortest path routing scheme keeps the whole topology information at each node to make the routing decision. It’s simple but involves large overhead, especially for large networks. It requires O(n2 ) routing table cost (where n is the number of nodes in the network). Both our approach and the Schafer’s scheme are compared to the baseline by normalizing their results to the baseline. We also study the impact of topology irregularity and network scalability. a) Evaluation Metrics: For all three approaches mentioned above, we run their routing schemes individually for all source-destination pairs on various topologies at the same level of topology irregularity and network scale, and record their overall hop count for comparison. In order to model the overall routing decision time of MTRXY along the paths, we deﬁne tX Y as the XY-routing decision time and tT ABLE as the distributed table routing decision time. We denote the portion of routing decisions made by XYrouting in pX Y and the overall hop count in N HM T R−X Y . The normalized routing decision time for all source-destination pairs is determined by, (pX Y · tX Y + (1 − pX Y ) · tT ABLE ) · N HM T R−X Y tT ABLE · N HSHORT ET (1) 204 where N HSHORT ET baseline. Our is the overall investigation shows that hop the count ratio of of N HM T R−X Y /N HSHORT ET is at most 103% and pX Y is more than 90% for all the topologies under study. Base on Eq. 1, we argue that the MTR-XY scheme will outperform the baseline approach if the XY-routing decision is 6% faster than the table routing decision. In the experiment, we set tX Y = 1 cycle and tT ABLE = 3 cycles based on a tentative ASIC implementation of both approaches. The overall routing decision time is calculated by, N(cid:2) N(cid:2) N H(cid:2) trouting = t(Nij−h , Nj ) (2) i=1 j=1 h=1 where N is the number of nodes in the network, N H is the hop count from node Ni to node Nj . The function t(Nij−h , Nj ) calculates the routing decision time at the hth hop on the path from node Ni to node Nj . The routing decision at the node can be made either by XY-routing or by table routing based on the whole routing path. tX Y , if applying XY-routing tT ABLE , if applying table routing t(Ni , Nj ) = (cid:3) (3) Further, the routing cost is compared in terms of routing table size counted by the number of table entries. The results are given in Fig. 6(b). As we can see, both MTR-XY and Schafer’s approach have higher hop count than the baseline. It’s because MTR-XY is designed in a way that minimizes the XY-hop count so as to minimize the routing table size. If the time in accessing the lengthy routing tables becomes critical, the reduction on the XY-hop count reduces the times to make table routing decisions and thus improves routing efﬁciency. However, the reduction in the XY-hop count pays at the cost of extra hop counts which increases the routing path cost and may affect the routing efﬁciency. Thus it’s important to study the tradeoff between path cost and table size. Schafer’s approach is based on Odd-Even turn model which cannot guarantee shortest path routing either. The hop count is increased when it takes the opposite direction to avoid forbidden turns. From the simulation results, MTR-XY makes the best performance-cost tradeoff over both baseline and Schafer’s approach. b) Adaptivity on Topology Variation: We further study the adaptivity of MTR-XY on topology variation in terms of RF nodes placement. The experiment is carried out on 10 topologies at the same irregularity level of Lirr = 25% and the same scale of 10 × 10 grids (i.e., 75 RF nodes placement). Fig. 6(a) shows that MTR-XY is well adaptive to topology variation. Among the ten topologies, the maximum hop count overhead compared to the baseline is about 4%. The best hop count performance is achieve at the 8th topology, where the overall hop count is only 0.7% more than the baseline. The standard deviation of the hop count overhead of all the topologies is just 0.84%, indicating good adaptivity upon the variation of topology structures. Note that the Schafer’s approach results in the longest routing time due the lengthy table accesses. The MTR-XY routing achieves averagely 36% reduction in routing time and 28.5% saving in table cost when 1 2 3 4 5 6 7 Topologies 8 9 10 0.9 0.95 1 1.05 N o r m a i l d e z p o h n u o c t MTR−XY Schafer (a) Normalized hop count. 1 2 3 4 5 6 7 Topologies 8 9 10 0 0.2 0.4 0.6 0.8 1 N o r m a i l d e z r u o i t g n i t m e MTR−XY Schafer (b) Normalized routing time. 1 2 3 4 5 6 7 Topologies 8 9 10 0 0.2 0.4 0.6 0.8 1 N o r m a i l d e z t e z s e b a l i MTR−XY Schafer (c) Normalized routing cost. Fig. 6. Routing performance under the same level of irregularity and scale compared to the baseline. It also presents a good adaptivity in both routing time and table cost, where the standard deviation of the normalized value of both metrics are 3.3% and 4.3% respectively. c) Adaptivity on Irregularity Levels: The MTR-XY scheme also presents sound adaptivity in irregularity level of topologies. Fig. 7 gives the simulation results of different topologies on the same 8 × 8 grids with irregularity level Lirr changing between 5% and 40%. Basically, the higher the Lirr , the more irregular is the topology. As less nodes are placed, more ﬂexural routing is resulted, and more table routing decisions are involved, which explains why both the routing time and routing cost increase in general with the increment in Lirr . As we can see, the hop count overhead is changing from 0 ∼ 1.4%. MTR-XY achieves signiﬁcant reduction in both routing time and table cost when compared to the baseline. When Lirr = 5%, up to 73% reduction in table cost and 41% reduction in routing time are reached. Among all the cases, the bottomline 26% reduction in table cost is still achievable. d) Adaptivity on Topology Scales: We also study the scalability of MTR-XY routing. The results are given in Fig. 8 for different topologies with Lirr ﬁxed at 10% and network scales ranging between 4 and 16. As the irregularity level is set relatively low, MTR-XY delivers the hop count performance very close to the baseline. The maximum hop count overhead is around 3%. As the topology scales up, more table-driven routings are introduced for making routing decisions, the reduction trend tends to slow down. Even that, a minimum of 38% of reduction in routing time is still observed. C. Deadlock Avoidance Efﬁciency and Cost Analysis The deadlock avoidance efﬁciency and overhead in terms of buffer classes are further evaluated. The comparison between our proposed turn classes and other two buffer classes, i.e., distance classes and dateline classes, are conducted under different topology irregularity and scale. As we can see from Fig. 9, our approach requires the smallest number of classes for buffer ordering. For example, when Lirr = 40% and at scale of 16 × 16 grids, the average turn classes needed is only 2.6 while 31 for distance classes and 16 for dateline classes. In other words, the turn classes based avoidance scheme achieves the highest efﬁciency at the lowest buffering cost. Even at the lowest irregularity level of Lirr = 5% and the smallest network scale of 4 × 4 grids, turn classes is maintained at 2 while 6 for distance classes and 4 for dateline classes. Two redeeming features of turn classes are excellent scalability and irregularity adaptivity with the fact that classes variation ranges between 0 and 0.6. 5 40 35 30 25 20 15 10 4 6 8 10 12 14 16 0 20 40 Irre g ularity le v el ( % ) Topology scale (n−by−n) Turn Distance Dateline Fig. 9. Comparison of turn classes vs. distance classes vs. dateline classes. D. WNoC Network Performance In order to study the impact of our proposed MTR-XY routing and turn classes based deadlock avoidance upon the overall WNoC network performance, we run experiments under various network scales. We study the scalability of WNoC and run the experiments with 4 different scales: 4 × 4, 6 × 6, 8 × 8, and 10 × 10. For fair comparison, we set their irregularity level to be the same Lirr = 10% under the uniform trafﬁc. As shown in Fig. 10(b), nice scalability is observed. By deﬁning the scaling factor S as the ratio of the number of nodes of two topology scales, the wireless bandwidth utilization scales at the same scaling factor. The network throughput increases at the rate of about S as the longer transmission paths will contradict the performance gains from higher concurrency level. In terms of the end-to-end performance, WNoC has latency less than 150 clock cycles at the largest network scale. The end-to-end delay scales by the factor of S following the increment trend of hop count. Before saturation, the transmission time at each hop dominates the end-to-end delay. While after saturation, the waiting time along the path due to contention dominates the end-to-end delay. √ V. CONCLU S ION In this paper, we presented a novel geo-assisted routing scheme under irregular mesh WNoCs for domain speciﬁc 205             5 10 15 20 25 30 35 40 Irregularity level (%) 0.9 0.92 0.94 0.96 0.98 1 1.02 N o r m a i l d e z p o h n u o c t (a) Normalized hop count. 5 10 15 20 25 30 Irregularity level (%) 35 40 0 0.2 0.4 0.6 0.8 N o r m a i l d e z r u o i t g n i t m e (b) Normalized routing time. 5 10 15 20 25 30 Irregularity level (%) 35 40 0 0.2 0.4 0.6 0.8 N o r m a i l d e z t e b a l e z s i (c) Normalized routing cost. Fig. 7. Routing performance on the same topology scale with different levels of irregularity 4 6 8 10 12 14 Topology scale (n−by−n) 16 0.9 0.95 1 1.05 N o r m a i l d e z p o h n u o c t (a) Normalized hop count. 4 6 8 10 12 14 Topology scale (n−by−n) 16 0 0.1 0.2 0.3 0.4 0.5 0.6 N o r m a i l d e z r u o i t g n i t m e (b) Normalized routing time. 4 6 8 10 12 Topology scale 14 16 0 0.1 0.2 0.3 0.4 0.5 0.6 N o r m a i l d e z t e b a l e z s i (c) Normalized routing cost. Fig. 8. Routing performance on different topology scales with the same irregularity level 0 0 0.1 0.2 0.3 Injection Rate 0.4 0.5 5 10 15 A g v . e n t w o r k t h r u p h g u o ( t G s p b ) 4x4 6x6 8x8 10x10 (a) Network throughput. 0 0 0.1 0.2 0.3 Injection Rate 0.4 0.5 20 40 60 80 100 A g v . g g a r a g e t d e d n a b w i d t h ( G s p b ) 4x4 6x6 8x8 10x10 (b) Aggregated bandwidth. 0 0 0.1 0.2 0.3 Injection Rate 0.4 0.5 100 200 300 400 A g v . d n e − o − d n e t y a e d l ( e c y c l ) 4x4 6x6 8x8 10x10 (c) End-to-end delay. Fig. 10. Network performance on different topology scales with same level of irregularity MPSoC applications. Distributed routing tables are constructed to orient segmented XY-routing. The approach achieves high routing efﬁcient in terms of short routing decision time, low routing cost by minimizing the routing table size, and high scalability. Further, a turn classes based buffer ordering scheme has been proposed to efﬁciently resolve deadlock at very low buffering cost. The WNoC network performance has been evaluated by implementing the RF microarchitecture. Extensive experiments have been carried out to demonstrate the promising performance of WNoC for high-speed on-chip data communication. "
Transient and Permanent Error Co-management Method for Reliable Networks-on-Chip.,"We propose a transient and permanent error co-management method for NoC links to achieve low latency, high throughput and high reliability, while maintaining energy efficiency. To reduce the energy overhead, a configurable error control coding adapts the number of redundant wires to the varying noise conditions, achieving different error detection capability. Infrequently used redundant wires are used as spare wires to replace broken links. Furthermore, a packet rebuilding/restoring algorithm that cooperates with a shortened error control coding method is proposed to support a low-latency splitting transmission. With this co-management method, we manage transient errors and a small number of permanent errors, without using extra spare wires, to reduce the need for adaptive routing. Simulation results show that the proposed method achieves up to 71% packet latency reduction and 20% throughput improvement, compared to previous methods. Case studies show that our method reduces the energy per packet by up to 68% and 48% for low and high permanent error conditions, respectively.","2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip Transient and Permanent Error Co-Management  Method for Reliable Networks-on-Chip  Qiaoyan Yu and Paul Ampadu  Department of Electrical and Computer Engineering  University of Rochester, Rochester, NY 14627, USA  <qiaoyan, ampadu>@ece.rochester.edu Abstract—We propose a transient and permanent error comanagement method for NoC links to achieve low latency, high  throughput and high reliability, while maintaining energy  efficiency. To reduce the energy overhead, a configurable error  control coding adapts the number of redundant wires to the  varying noise conditions, achieving different error detection  capability. Infrequently used redundant wires are used as spare  wires  to replace broken  links. Furthermore, a packet  rebuilding/restoring algorithm that cooperates with a shortened  error control coding method is proposed to support a low-latency  splitting transmission. With this co-management method, we  manage transient errors and a small number of permanent  errors, without using extra spare wires, to reduce the need for  adaptive routing. Simulation results show that the proposed  method achieves up to 71% packet latency reduction and 20%  throughput improvement, compared to previous methods. Case  studies show that our method reduces the energy per packet by  up to 68% and 48% for low and high permanent error  conditions, respectively.   Keywords-Network-on-chip; transient error; permanent error;  reliability; splitting transmission; spare wire.   INTRODUCTION   I.  To achieve the desired reliability and energy efficiency,  configurable error control coding methods have been used to  manage transient errors on NoC links that operate in a variable  noise environment [1]-[3]. Since the link width is typically  designed for the most powerful error control code, link  bandwidth is wasted when a simple code is used in a favorable  noise condition. Spare wire replacement [4-7], fault tolerant  routing [8-10] and splitting transmission [4, 5, 25] approaches  are advocated to manage permanent errors on NoC links.  Although spare wire approaches are simple, they increase the  need for redundant links. Fault tolerant routing methods  discards faulty links and re-route packets through alternative  paths, which may result in network congestion and increased  latency. Splitting transmission separates one flit into multiple  fractions, transmitting one fraction at a time. This method  proceeds using the links containing broken wires, but needs  multiple cycles to finish a single flit transmission.   Both transient and permanent errors reduce NoC reliability;  thus, co-management of these two errors is urgent. Various  methods for handling these errors have been investigated  separately; however, simply combining different error control  methods can result in large area and energy overhead and  degrade performance.   In this work, we make use of configurable error control  coding to manage variable transient errors, and employ the  infrequently used redundant wires to handle permanent errors.  The main contributions of this work are: (1) Cooperative  transient and permanent error management method. Based on  the noise condition, an appropriate error control coding is  selected to detect transient errors. Retransmission is required  when errors are detected. If there are some unused redundant  wires available (for the powerful transient error detection),  those wires are used as spare wires in permanent error  recovery. When there are not enough redundant wires for  broken wire replacement, the error control codes are shortened  to incorporate a low-latency splitting transmission for comanaging transient and permanent errors. (2) Efficient splitting  transmission method. Instead of abandoning half of the wires  on the broken link [5] or wasting healthy wires to equalize the  link width over the entire network [6], we improve the  utilization rate of non-broken wires and transmit a packet over  broken links using a packet rebuilding/restoring algorithm.   The remainder of this paper is organized as follows: Section  II provides an overview of related work. In Section III, the  transient and permanent error co-management method is  proposed. Simulation results are provided in Section IV.  Conclusions and future work are presented in Section V.  II. RELATED WORK  A. Transient Error Management  Transient errors are  typically  induced by external  radiation, spurious voltage spikes and crosstalk coupling [1114]. Because of its temporary behavior, transient errors can be  managed with various error control coding methods—such as  forward error correction (FEC), error detection combined with  automatic repeat request (ARQ) retransmission, and hybrid  ARQ [1-3, 15-18]. In low noise conditions, simple error  detection coding combined with retransmission is an energyefficient method to achieve the targeted reliability [1, 15]. As  the noise increases with deeply scaled technologies, the need  for powerful codes to handle multi-bit errors increases [3, 14,  18]. These more powerful codes designed for the worst case  typically result in increased hardware complexity and energy.   978-0-7695-4053-5/10 $26.00 © 2010 IEEE DOI 10.1109/NOCS.2010.24 145         Different applications have diverse constraints on  reliability, energy-efficiency, throughput and area. In addition,  different noise conditions have different system requirements  to ensure error resilience. As a result, configurable error  control becomes attractive in optimizing performance in  varied scenarios. Adaptation on the number of Hamming code  groups combined with interleaving can tradeoff bandwidth and  reliability  requirements  in on-chip  interconnects  [19].  Furthermore, Rossi et al achieve different quality-of-service  (QoS) needs by selecting error detection, correction, and  mixed modes at design time [20]. To reduce energy wasted by  error detection in favorable noise conditions, the error  detection capability is adapted based on the number of errors  found over a fixed time window by a victim line operating at  half the supply voltage in [1]. Recently, adaptive error control  coding (ECC) combined with hybrid ARQ was proposed to  adjust both error detection and correction, according to the  monitored link quality or the constraints on reliability and  throughput for a given application [3]. Adaptive error control  assumes that the noise is not always in the worst case, and thus  the adaptation can reduce average-case energy consumption.  Although the unused redundant wires do not consume  dynamic energy, link bandwidth is wasted in favorable noise  conditions. In this work, these infrequently used wires are  exploited to manage permanent errors.  B. Permanent Error Management  Permanent errors are usually caused by  imperfect  manufacturing process or device aging [7, 11, 21]. These errors  do not vanish unless the faulty components are abandoned or  replaced with new ones. Gossip algorithms are used to tolerate  errors⎯broadcasting multiple copies of packets to the network,  and selecting one error-free copy in the destination [22].  Although the gossip algorithm is simple, flooding multiple  identical copies over the network results in increasing network  congestion and unnecessary energy consumption. Fault-tolerant  routing algorithms [8, 9, 23] provide alternative paths for the  packet to avoid the faulty link/router. To facilitate this method,  information about link and router status need to be frequently  updated in the entire network. Recently, fault-tolerant routing  and multiple copies of packet methods were combined to  achieve high reliability and reduce overhead [9].   Alternatively, spare wires and splitting transmission have  been used to manage permanent errors. Broken wires can be  replaced with spare wires either in a testing stage [24] or at run  time [5, 7]. Because the approach in [24] cannot prevent  broken links during runtime, we are interested in runtime spare  wire replacement here. Splitting transmission improves the  utilization rate of broken links and reduces the needs of rerouting to reduce the network congestion and latency. Note, if  spare wires are exhausted or the router is nonfunctional, rerouting is needed. As shown in Fig. 1(a), half splitting  transmission [5] divides one flit into two fractions, transmitting  one fraction each cycle. This approach is not efficient if the  ratio of broken wires over all links is much smaller than 0.5. In  [6], the user reduces the phit size (=switch link width) to the  maximum number of healthy wires per switch-to-switch link  set. Fig. 1(b) shows that the phit size is reduced to MIN(n-k1, nk2, n-k3), even though not all links have permanent errors.  146 (a)                                                       (b)  Fig. 1 Two approaches for permanent error management (a) Half splitting  transmission [5] (b) Phit size reduction [6].  Fig. 2 Architecture for the proposed transient and permanent error comanagement method.   In this work, we propose a low-latency splitting transmission  method to improve the utilization rate of broken links.  Furthermore, this splitting transmission is integrated with  runtime spare wire replacement [7] to reduce the needs of faulttolerant routing, reducing the packet latency and improving  throughput. Redundant wires in configurable error control  coding for transient error management are used as spare wires,  so that permanent error control overhead is reduced.   III. PROPOSED CO-MANAGEMENT METHOD  A. Overview of Cooperative Transient and Permanent Error  Management   The proposed method is used to manage both transient and  permanent errors at runtime. The transmitter and receiver  architecture is shown in Fig. 2. Error control coding is used to  detect (or correct) transient errors. According to the instruction  of transient error monitor [1], a configurable error control  coding (ECC) encoder/decoder in each output/input port of a  router selects the most energy-efficient ECC to control  transient errors. The input width for ECC encoder, k, is equal to  flit width (here, we assume flit width is as same as phit size),  and the maximum codeword is n. Note, the link width between  routers for data transmission is the same as the maximum  codeword width. Triple modular redundancy (TMR) is applied  to the acknowledgement signal, TMR NACK, to ensure the  correctness of the retransmission request.         Fig. 3 Flowchart for cooperative transient and permanent error management.  From the configurable ECC point of view, there are no  permanently broken links, because permanent errors are  handled transparently after/before ECC encoding/decoding. Inline testing (ILT) unit proposed in [7] is used after/before the  configurable ECC encoder/decoder to detect permanent errors  and transparently replace broken wires with the spare wires. In  our method, no additional spare wires are needed for broken  wire replacement. In low noise conditions, a simple ECC code  is used to detect/correct transient errors. Thus, there are some  unused redundant wires (kept for parity check bits in the  powerful ECC) available for broken wire replacement. If there  are enough spare wires, the broken wires are automatically  replaced with healthy ones. When the number of broken wires  exceeds the total number of spare wires, splitting transmission  is invoked. This more likely occurs in the case of the router  using a powerful ECC to recover multi-bit transient errors than  in the case of using the simple ECC. To maintain permanent  error recovery capability without using additional spare wires,  we propose to rebuild the packet and encode the packet with a  shortened powerful error control code. The  splitting  transmission control module facilitates the packet rebuilding  and transmission. The cooperative management flowchart is  shown in Fig. 3.  Unlike handling transient and permanent errors in both  network interface and router [21], our method manages two  types of errors in router only. Thus, we can reduce the latency  caused by waiting for the end-to-end acknowledgement of  transient error detection. Our  transparent  spare wire  replacement reduces the probability of introducing network  congestion caused by re-routing, as well. Consequently, the  proposed co-management method is able to achieve better  latency performance.  B. Latency-Efficient Splitting Transmission  An efficient splitting transmission is proposed to cooperate  with configurable error control for transient errors, achieving a  smaller latency than phit size reduction and half splitting  transmission. Since the number of spare wires varies as noise  changes, we propose a packet rebuilding/restoring algorithm  and implementation in this section.  (a)  (b)  Fig. 4 Architecture for router output port supporting splitting transmission (a)  Output buffer and configruable ECC encoder (b) Split buffer for (k-k2)=4.  1) Output Port Architecture  Detailed architectures for the output buffer and configurable  ECC encoder (in Fig. 2) are shown in Fig. 4. Error control  codec selection signal (ECC_Sel) is used to select the right  output from the configurable ECC encoder and enable the tail  flit detector. In low noise conditions, a simple ECC code (n1,  k) is used to detect 1- and 2-bit errors on a link. Because the  codeword width n1 is less than the link width n, there are n-n1  wires available for broken wire replacement. Consequently,  the entire k-bit flit is fed to the simple ECC Encoder. In high  noise conditions, a powerful ECC code (n, k2) is used to detect  more multi-bit errors on links. In order to obtain an n-bit  codeword, the input width for the advanced ECC encoder, k2,  is less than the flit width k. The remaining (k-k2) bits are  accumulated to rebuild extra flits later, which are appended  after the original flits. If the remaining bits of one packet are  less than k2, zero bits are filled into the last appended flit. Note  that Fig. 4 only shows the case where the total accumulated  bits are less than the flit length. When more than one flit is  appended after the original packet, the splitting control logic  should be modified.  Splitting control signal, Splitting_Ctrl, indicates whether the  current flit is original data flits or rebuilt flits. If the number of  the appended flit is no more than one, Splitting_Ctrl becomes   147         Fig. 5 Packet rebuilding algorithm.  active one cycle after the tail fit leaves the Output Flit_Buffer,  as shown in Fig. 4. Typically, the flit type (e.g. header,  payload and tail) is indicated by a fixed position in a flit. Thus,  one simple logic gate and a delay element are enough to  generate the Splitting_Ctrl signal. If more than one appended  flits are needed, the tail detector should be replaced with a flit  counter to appropriately provide the Splitting_Ctrl signal.   The Split_Buffer stores the remaining bits after flit splitting,  and these remaining bits are combined into the appended flit  for later transmission. As shown in Fig. 4(b), this Split_Buffer  is organized in a pipelined fashion with a width of (k-k2) and a  depth equal to the packet length. With this architecture, we  can simultaneously store split flits and transmit shortened flits.  If the Splitting_Ctrl is active, the Split_Buffer contents are  used to construct a new flit after the last shortened flit is  transferred; otherwise, the output of Split_Buffer is discarded.   2) Packet Rebuilding and Restoring Algorithm  To eliminate the use of extra wires for transient error  management, the link width is set to the maximum codeword  width in the configurable ECC encoder. However, this setting  results in no spare wires available for the advanced ECC case.  We propose a packet rebuilding and restoring algorithm to  remove this limitation.   Pseudo code for the packet rebuilding algorithm is shown in  Fig. 5. This algorithm is enabled only when the advanced ECC  encoder/decoder is employed (i.e. ECC_Sel=1). Before the  Splitting_Ctrl signal is set to high, the input for the advanced  ECC encoder is the top entry of the Flit_Buffer, but discarding  S (=k-k2) bits. The S-bit information is saved in the  Split_Buffer. The new incoming flit pushes the contents in  output Flit_Buffer and Split_Buffer one stage forward.   Fig. 6 Packet restoring algorithm.  The arrival of the tail flit sets the Splitting_Ctrl signal to  high, and the content in the Split_Buffer is passed to the flit  out for the followed error control coding. Because the  Splitting_Ctrl is controlled by the tail flit, no flit counter is  needed. Thus, this algorithm supports packet switching with a  variable packet length. The newly constructed flits increase  latency, but this overhead is much smaller than the halfsplitting transmission.  The packet restoring algorithm is shown in Fig. 6. Similarly,  this algorithm is enabled when the ECC_Sel is high. Unlike in  the output port, there is no split buffer in the input port. Before  the tail flit arrives, the flit buffer in the input port of the next  router pops out a flit each cycle; the incoming shortened flit is  filled into parts of the input buffer. After the tail flits reach the  input buffer, the flit constructed by the packet rebuilding  algorithm is written to the rest of the input buffer (Note, other  content in the buffer remains unchanged). Because of this  unchanged state, one-cycle latency is introduced by this packet  restoring algorithm.  3) Latency Analysis  In this work, we define the packet latency as the time  interval between the header flit departing from the transmitter  and the last flit of the packet being accepted by the receiver.  Because of its efficiency, go-back-n [17] retransmission policy  is applied to recover transient errors. In this section, we  analyze the packet latency with/without splitting transmission  methods. The packet latencies for no-splitting, half-splitting [5]  and  the proposed  transmission approaches, Tpkt_NoSplit,  Tpkt_HalfSplit, Tpkt_Proposed, are expressed in (1)-(3).  link T T2 ECC    (1)  No( Error ) ( With Error ) + T3 link T _pkt NoSplit packet L2 ×α+ 2 ) ⎪ ⎧ ⎨ ⎪⎩ ( + L T ECC packet + + = 148       0 5 10 15 20 25 30 35 20 40 60 80 100 120 150 140 Number of Flits per Packet P e k c a t a L t y c n e No Splitting Half Splitting Proposed (a)                                    20 5 10 15 20 25 30 35 40 60 80 100 120 140 160 180 200 220 Number of Flits per Packet P e k c a t a L t y c n e No Splitting Half Splitting Proposed (b)  Fig. 7 Packet latency comparison of three splitting methods with different  packet sizes (a) without error (b) with error (α=1).  ( ) ⎪ ⎧ ⎨ ⎪⎩ + T + + T ×α+ L4 × 2 + = ) No( Error ) ( With Error T3 T2 L 2 T link ECC packet link ECC packet HalfSplit _pkt       (2)  ( ) ( ) ( ) ( ) ⎧ ⎪ ⎪⎪ ⎨ ⎪ ⎪ ⎪ ⎩ + + ⎞ ⎟ ⎟ ⎠ ⎛ ⎜ ⎜ ⎝ ⎤ ⎥ ⎥ ⎡ ⎢ ⎢ × k × k − kk + ×α+ + + ⎞ ⎟ ⎟ ⎠ ⎛ ⎜ ⎜ ⎝ ⎤ ⎥ ⎥ ⎡ ⎢ ⎢ kk − + L2 × = Error With T3 T2 L L 2 Error ) No( T T L T link ECC packet 2 packet link ECC packet 2 packet roposed P_pkt (3)  where, k is the number of bits in each flit; k2 is the input width  for the powerful ECC encoder; Lpacket is the number of flits per  packet (suppose one flit per cycle, the flit length is equal to the  basic packet latency); TECC is the ECC encoder and decoder  latency; Tlink is the propagation delay for one hop; α ∈ (0, 1] is  the ratio of the number retransmitted flits over the packet  length; ⎡ ⎤ is a ceil function. We assume that TECC is two  cycles (one for the encoder and another for the decoder) and  Tlink is one cycle. If retransmission is needed, the resend flits  are encoded/decoded again, resulting in 2·TECC. Including the  latency for waiting for NACK, 3·Tlink are needed for transient  error recovery. For simplicity, we assume using one  retransmission can obtain an error-free flit.  TABLE I.  LATENCY FOR DIFFERENT SPLITTING METHODS  k=32, k2=28  α=0.5  94.5  (100%)  107  (113%)  182  (193%)  k=128, k2=124  α=0.5  94.5  (100%)  99.5  (105%)  182  (193%)  α=0  73  (100%)  83  (114%)  143  (196%)  α=1  112  (100%)  127  (113%)  217  (194%)  α=0  73  (100%)  77  (105%)  143  (196%)  α=1  112  (100%)  118  (105%)  217  (194%)  No  splitting  Proposed  splitting  Half  splitting  Fig. 7 shows the packet latency comparison of different  transmission approaches. Compared to the half splitting  approach, the proposed method reduces packet latency by 42%  and 41% for without error and with error conditions,  respectively. Although it saves up to 12.1% (for the withouterror cases) and 11.8% (for the with-error cases) packet  latency compared to our method, the no splitting method does  not have permanent error recovery capability. In  the  experiment, k = 32, k2 = 28, and α = 1. Table I shows the  latency for the three methods using larger k and k2 and smaller  α. As can be seen, changing k, k2 and α does not affect the  latency of no splitting and half splitting, but our method  achieves better latency performance. In addition, Table I also  shows that larger k and k2 reduce the latency overhead of our  method. So does smaller α.  IV. CASE STUDY  A. Experimental Setup  A case study is used to evaluate the proposed comanagement. In this case study, two error control codes are  integrated as a configurable ECC: ECC1⎯Hamming(38, 32)  (shortened from Hamming(63, 57)) and ECC2⎯four groups  of Hamming(12, 8) (shortened from Hamming (15, 11))  combined with interleaving. Hereafter, 4xHamming(12, 8)  represents ECC2. In low noise conditions, Hamming(38, 32) is  used  to detect  two-bit  transient errors. In high noise  conditions, 4xHamming(12, 8) are used to detect multi-bit  (>2) transient errors. The transient noise monitor, such as the  one proposed in [1], guides the configuration on the error  control codec. Because the codeword for 4xHamming(12, 8) is  48 bits wide, the data link width between transmitter and  receiver is set to 48. When the Hamming(38,32) is in use,  there are ten wires available for permanent error recovery. If  transient noise increases, the 4xHamming(12, 8) is needed.  However, in this case, no spare wires exist for broken-wire  replacement. To remove this limitation, we further shorten  Hamming(12, 8) to Hamming(11, 7). For example, assume the  original packet is composed of seven flits, 32 bits for each flit.  After using 4xHamming(12, 8) and splitting transmission, one  additional flit is appended after the seventh flit. As the system  returns to low noise conditions, Hamming(38,32) can be used  again, and the splitting transmission is not necessary.   B. Implementaion of Configurable Error Control  To reduce the ECC configuration overhead, we construct  two special parity matrices for the shortened Hamming(12,8)  and Hamming(38, 32), PHM(12,8) and PHM(38,32), respectively.  The matrices are expressed in (4) and (5).  149                       ECC1:Hamming(38,32) ECC2:4xHamming(12,8) Shortened ECC2:4xHamming(11,7) 10−6 10−8 10−10 10−12 10−14 10−16 10−18 e t a R r o r r E t i l F l a u d s e i R 10−20 10−12 10−10 10−8 10−6 Bit Error Rate on Links 10−4 10−2 Fig. 9 Residual flit error rate of three error control methods.   ECC1; 7 Flits per Packet ECC2; 7 Flits per Packet Splitting+Shortened ECC2; 7 Flits per Packet ECC1; 21 Flits per Packet ECC2; 21 Flits per Packet Splitting+Shortened ECC2;21 Flits per Packet 10−4 10−6 10−8 10−10 10−12 10−14 e t a R r o r r E t e k c a P l a u d s e i R 10−16 10−11 10−10 10−9 10−8 10−7 10−6 10−5 10−4 10−3 Bit Error Rate on Links Fig. 10 Residual packet error rate of three error control methods.   Compared  to ECC1, ECC2 and  its shortened code  (4xHamming(11,7)) reduce the residual flit error rate by up to  five orders of magnitude in low noise conditions, as shown in  Fig. 9. To obtain a targeted reliability of 10-10 residual flit error  rate, we use ECC1 when the bit error rate is below 10-7;  otherwise switch to the shortened ECC2. Fig. 9 also shows that  the splitting transmission does not degrade the reliability. The  impact of packet length on splitting transmission is further  examined in Fig. 10. Residual packet erorr rate is defined as the  probability of a received packet still containing errors after  error control. As shown, the proposed error control can  effectively differentiate the transient error resilence between  two modes and achieves the targeted reliability if the bit error  rate is less than 10-5. The packet with a longer length may  require the switching point to be lowered.  D. Power and Energy Consumption  Power consumption for different splitting transmission  methods are compared in Table III and IV. The transmitter and  receiver are implemented with Verilog and synthesized by  Synopsys Design Complier with a TSMC 45 nm technology.  The supply voltage is 1 V and the clock frequency is 1 GHz.  Considering store-and-forwarding packet switching, the input  and output flit buffer is set to the same size of a packet. For  Fig. 8 Hardware-efficient configurable error cotnrol encoder.  TABLE II.  AREA FOR DIFFERENT ERROR CONTROL CODEC CIRCUITS  Area (μm2) HM(38,32)  Encoder  151  Decoder  239  Total  390   4xHM(12,8) Combined  189  340  353  592  542   932   Proposed  215  386  601   )8,12(HMP = 1110 1101 0111 0000 ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ 1110 1101 0111 1111 1110 1101 0111 0000 1110 1101 0111 1111 T ⎤ ⎥ ⎥ ⎥ ⎥ ⎦ )32,38(HMP = 1110 1101 0111 0000 0000 0000 ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ 1110 1101 0111 1111 0000 0000 1110 1101 0111 0000 1111 0000 1110 1101 0111 1111 1111 0000         (4)           (5)  T ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ 1110 1101 0111 0000 0000 1111 1110 1101 0111 1111 0000 1111 1110 1110 1101 1101 0111 0111 0000 1111 1111 1111 1111 1111 Because of the overlap in the parity matrices, most of the  codec circuit can be shared. The diagram of the configurable  ECC encoder is shown in Fig. 8. Because the code is linear,  the first 32 bits for Hamming(38, 32) are as same as the  inputs. The check bits for Hamming(38, 32) are the results of  XORing the check bits of the four Hamming(12, 8) encoders  and three auxiliary signals. When ECC_Sel is low, the  multiplexers select the outputs of the XOR gates at the last  stage; otherwise, the multiplexers select the check bits of the  four Hamming(12, 8). As  shown  in Table  II,  this  implementation using a CMOS 45nm technology only requires  11% more area for configurable ECC than the worst-case  codec, 4xHamming(12, 8), and reduces the area overhead by  35%, compared to the method that simply combines two  different ECC codec circuits.  C. Reliability  The reliability of the proposed configurable error control  combined with splitting transmission is evaluated at the switchto-switch level. Residual flit error rate is the probability of  errors remaining in the accepted flit after using error control.  The simple ECC (i.e. ECC1) using Hamming(38, 32) can  detect 1- and 2-bit random errors; the powerful ECC (i.e. ECC2)  using 4xHamming(12, 8) with interleaving can detect any 1-  and 2-bit random errors and 3- to 8-bit errors as long as each  Hamming(12, 8) decoder does not receive more than two  erroneous bits. To facilitate permanent error recovery, a  shortened Hamming code is employed in the proposed method.  150                                                                                                                                               Table III, the flit buffer is 32 bits wide and 7 flits deep. For  Table IV, the flit buffer is 64 bits wide and 14 flits deep. The  area and power in these two tables exclude the portion of the  configurable error control codec, because the selection of ECC  depends on the targeted reliability. As shown in Table III, the  transmitter power of our method is greater than that of the half  splitting approach, because of the extra splitting buffer and  control logic; however, our receiver power is close to that of  the half splitting approach, since no additional splitting buffer  is needed in the receiver. As a result, the proposed method only  consumes 3.6% more dynamic power than the half splitting  approach for a 32-bit flit. In addition, leakage power is reduced  by 4% in the proposed method. When the flit width increases to  64 bits, our method consumes the same dynamic power as the  half splitting transmission, and further reduces the leakage  power by 9%, as shown in Table IV.  The global link power is simulated using distributed RC  segments. We assume that the link switching activity is 0.5.  The global wire parameters and parasitic capacitance values  are shown in Table V. Resistance and parasitic capacitance  values (R, Cg, and CC) were calculated [26] using a 45 nm  global interconnect model [27]. No inductances are included  in the link model because of the short link length [28]. The  total energy is composed of the configurable error control  encoder and decoder, the splitting transmitter and receiver, the  switch-to-switch link, and the TMR NACK signal. Energy per  packet is used to measure the average energy needed for  receiving an error-free packet in the presence of transient and  permanent errors. The percentage of broken links is the ratio  of the total links experiencing permanent errors over the total  number of NoC links. In this experiment, we use a 4x4 torus  NoC and assume that the number of broken wires per link is  no more than four. In future work, we will examine the energy  on a larger NoC with more broken wires.  As shown in Fig. 11(a), the energy per packet of the fixed  ECC (4xHamming(12,8) for the worst case) with half splitting   method [5] increases with the percentage of broken links over  the entire NoC. In contrast, in the low noise region, our  method enters ECC1 mode and consumes 17% and 68% less  energy than the fixed ECC for 5% and 50% links experiencing  permanent errors, respectively. In ECC1 mode, our method  has ten redundant wires for the broken wire replacement, and  no splitting is needed if the number of broken wires per link is  less than ten. The energy per packet for these two methods  operating in a high transient noise condition is examined in  Fig. 11(b). Low-latency splitting transmission is used in ECC2  mode; thus, the energy consumption increases by 6%, as the  percentage of broken links changes from 5% to 50%.  However, the proposed method still achieves 48% energy  reduction, compared to the fixed ECC with half splitting  method.   TABLE III. AREA AND POWER COMPARISON OF CASE STUDY FOR 32-BIT FLIT  Methods  Area   (μm2)  Dynamic  Power(mW)  Leakage Power  (μW)  Proposed  Half Splitting  Proposed  Half Splitting  Proposed  Half Splitting  Splitting  Transmitter  2368  2320  1.402  1.312  68.65  75.19  Splitting  Receiver  2245  2280  1.298  1.295  75.37  74.53  Total   4613 (101%)  4600 (100%)  2.700 (104%)  2.607 (100%)  144.02 (96%)  149.72 (100%)  TABLE IV. AREA AND POWER COMPARISON OF CASE STUDY FOR 64-BIT FLIT  Methods  Area   (μm2)  Dynamic  Power(mW)  Leakage Power  (μW)  Proposed  Half Splitting  Proposed  Half Splitting  Proposed  Half Splitting  Splitting  Transmitter  9383  9273  5.378  5.223  290.0  314.2  Splitting  Receiver  9091  9461  5.152  5.307  304.0  331.1  Total   18474 (104%)  18734 (100%)  10.53 (100%)  10.53 (100%)  594.0 (91%)  645.3 (100%)  TABLE V. GLOBAL LINK PARAMETER  Parameter  Value Width, WL (μm)  0.31 Minimum Spacing, S (μm) 0.31 Thickness, t (μm)  0.83 Height, h (μm)  0.14 Parameter Dielectric Constant  R (Ω / mm)  Substrate Capacitance, Cg (fF/mm)  Coupling Capacitance, CC (fF/mm)  Value  2.1  85.5  77.4  70.3  151       3 0 10 20 30 40 50 3.5 4 4.5 5 5.5 6 6.5 7 Percentage of Broken Links (%) E e n r y g e p r P e k c a t ( J p ) Fixed ECC with Half Splitting Proposed 17% 68% 3 0 10 20 30 40 50 3.5 4 4.5 5 5.5 6 6.5 7 Percentage of Broken Links (%) E e n r y g e p r P e k c a t ( J p ) Fixed ECC with Half Splitting Proposed 3% 6% 48% (a)                                                                                                        (b)  Fig. 11 The comparison of energy per packet for two transient and permanent error co-management methods with different number of broken links (a) In low  transient noise region (b) In high transient noise region. Link length for this experiment is 1 mm.   0 1 2 3 4 5 10 15 20 25 Link Length (mm) E e n r e p y g r P e k c a t ( J p ) Proposed−− 5% Permanent Errors Fixed ECC with Half Splitting−−5% Permanent Errors Proposed−−25% Permanent Errors Fixed ECC with Half Splitting−−25% Permanent Errors 0 1 2 3 4 5 10 15 20 25 Link Length (mm) E e n r e p y g r P e k c a t ( J p ) (a)                                           (b)  Fig. 12 Energy per packet comparison with different link length (a) In low  transient noise region (b) In high transient noise region.  The link length affects the link energy of the NoC. As the  link length increases, the link energy increases. Fig. 12 shows  that our method achieves more energy reduction over the fixed  ECC with half splitting method, if a longer link is used in the  NoC. This applies to both low and high noise regions and low  and high permanent error scenarios.   E. Area Overhead  As shown in Table III, the proposed transmitter consumes  more silicon area than the half splitting approach, because of  the additional splitting buffer for packet rebuilding. In the  reciever, no splitting buffer is needed in our method and the  control logic is simpler than the half splitting transmission.  Consequently, the penalty on the total area of our method is  about 1% more than that of the half splitting approach. As the  flit size increases to 64 bits, the area overhead of our method  increases by 4% (shown in Table IV).   Fig. 13 Packet latency comparison of different splitting transmission methods.  F. Packet Latency and Throughput  The latency and throughput are evaluated using a 4x4 torus  NoC, implemented in the C programming language. A  deterministic XY routing algorithm and store-and-forwarding  packet switching technique are used in the NoC. The  simulation results presented below are based on uniformly  distributed traffic. A round-robin scheduling algorithm is  employed if there is resource contention. We examine the  average packet latency by changing the number of broken links  over the entire NoC. Each broken link has up to four broken  wires. The phit size reduction method proposed in [6] does not  have transient error detection/correction capability, but this  method can be an alternative method for permanent error  management. To thoroughly evaluate the impact of permanent  errors on different methods, we also provide the latency and  throughput for the phit size reduction method. In the ECC1  mode, our method does not need splitting transmission because  our method has enough spare wires. The results shown in  Fig.  13 and Fig. 14 are for the ECC2 mode.   152                                                 results show that the proposed method reduces latency by 20%  and achieves an up to 71% higher throughput, compared to  previous methods. The case studies confirm the energy  efficiency of the proposed co-management method.   The work is successfully applied to a NoC using a storeand-forward packet switching NoC. In future work, we will  extend this method to facilitate both transient and permanent  error management for a wormhole packet switching NoC. In  addition, we will also investigate the combination of the  proposed method with adaptive routing, so that permanent  errors can be recovered when not enough spare wires are  available in the NoC.  "
