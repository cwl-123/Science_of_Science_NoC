title,abstract,full_text
PROSA - protocol-driven NoC architecture.,"Nowadays chip multiprocessors tend to have an increasing number of cores, usually implementing a distributed shared last level cache. The network on chip (NoC) is in charge of interconnecting the cores, memory controller(s) and cache banks, largely impacting memory access latency. Packet switching in usually used in NoCs, but circuit switching may achieve better performance if the setup time of the circuit is shadowed (established before it is needed). In this paper we propose PROSA, a novel NoC architecture to improve memory access latency by using circuits. In PROSA, the coherence protocol steers the circuit establishment logic in order to setup circuits before needed and only for the time frame they are required. Also, a memory latency control unit (MLCU), implemented in the memory controller, assists PROSA by computing arrival time of memory blocks. A clustered router approach is followed where groups of routers are combined and attached to a PROSA circuit controller. We detail all the implementation issues of PROSA, including circuit establishment logic with acknowledgment messages, protocol modifications, and router modifications to setup circuits when required. Results from real applications demonstrate reduction of network flit latency by 34% which translates into a reduction of miss load (and store) latency of 21% (in 64-core systems). PROSA needs 9.66% more area, but reduces power by 3%.","PROSA: Protocol-Driven NoC Architecture Miguel Gorgues Alonso, Jos ´e Flich Cardo Universitat Polit `ecnica de Val `encia, Spain Email: migoral@disca.upv.es Abstract—Nowadays chip multiprocessors tend to have an increasing number of cores, usually implementing a distributed shared last level cache. The network on chip (NoC) is in charge of interconnecting the cores, memory controller(s) and cache banks, largely impacting memory access latency. Packet switching in usually used in NoCs, but circuit switching may achieve better performance if the setup time of the circuit is shadowed (established before it is needed). In this paper we propose PROSA, a novel NoC architecture to improve memory access latency by using circuits. In PROSA, the coherence protocol steers the circuit establishment logic in order to setup circuits before needed and only for the time frame they are required. Also, a memory latency control unit (MLCU), implemented in the memory controller, assists PROSA by computing arrival time of memory blocks. A clustered router approach is followed where groups of routers are combined and attached to a PROSA circuit controller. We detail all the implementation issues of PROSA, including circuit establishment logic with acknowledgment messages, protocol modiﬁcations, and router modiﬁcations to setup circuits when required. Results from real applications demonstrate reduction of network ﬂit latency by 34% which translates into a reduction of miss load (and store) latency of 21% (in 64-core systems). PROSA needs 9.66% more area, but reduces power by 3%. I . IN TRODUC T ION Chip Multiprocessor systems (CMPs) rely on networkson-chip (NoCs) [1] to achieve fast communication between resources, mainly processors, memory caches and memory controllers. It is by no doubt that NoCs play a vital role on CMPs performance, mainly as they lay along the critical path of communication, and thus, affect process communication latencies and most important, memory access latency. During more than one decade, NoCs have been researched (not only for CMPs) with two main goals in mind, network throughput and network latency. While network throughput is an important aspect of the NoC, in CMPs the average load of links found for accepted benchmarks (PARSEC, SPLASH, ...) is typically low. This means latency becomes the signiﬁcant metric when dealing with NoCs for CMP systems. Indeed, a memory transaction is started when one processor requests a load or a write on its private L1 data cache, and ﬁnishes when the data is delivered. The NoC is used to send request commands and data between cache levels and the memory controller (MC). Therefore, the NoC plays a vital role as the memory request gets blocked until data is delivered. In this paper we address the issue of latency reduction of memory transactions. We propose PROSA, a PROtocoloriented circuit Switch Architecture, which co-designs both the NoC and the coherence protocol and put them to play together. Our approach enhances the NoC with a new clustered 0 This work was supported by the Spanish Ministerio de Econom´ıa y Competitividad (MINECO) and by FEDER funds under Grant TIN201566972-C5-1-R component, the PROSA controller (PC), which is in charge of managing circuits and to resolve any possible conﬂict. The controller is in charge of four NoC PROSA routers (PR) and steers their local circuits when needed. Results show that PROSA outperforms the baseline design by reducing network latency by about 34% on average. PROSA reduces the load and store miss latency by 21%. PROSA correctly sets up 90% of the circuits ahead of time. The PROSA cluster takes 9.66% more area but saves more than 3% in power consumption. PROSA differs from previous circuit-switching methods by means of the feedback provided by the coherence protocol. Although some previous works also combined protocols and NoC designs [2], [3], [4], [5], [6], our proposal lets the coherence protocol to steer the network in programming circuits before they are needed, hiding the set up circuit delay. Moreover, PROSA does not need extra buffer resources at the router. Just a pair of multiplexer/demultiplexer is needed on each bidirectional router port. Section V describes related work. The rest of the paper is organized as follows. In Section II, we describe and analyze the coherence protocol. This will center our proposal and will provide justiﬁcation of its need. In Section III we describe PROSA. In Section IV, we provide the evaluation and its analysis. Related work is described in Section V and the paper is concluded in Section VI. I I . COH ER ENC E PROTOCO L ANA LY S I S We assume the CMP uses private L1 caches for each core and shared but distributed L2 banks along a tile-based organization. Two memory controllers (MC) are placed on each top corner of the system. A 4 × 4 mesh NoC uses a 4-stage pipelined 7-radix router design (four ports to connect to neighbor routers and three to connect L1, L2, and MC). The system implements the MOESI protocol [7] at L1 while at L2 blocks can be in P (private), S (shared), C (cached), or I (Invalid) state. In C mode no L1 cache has a copy of the block. Inclusive caches are assumed and a write back policy used. A static mapping policy of blocks to L2 banks is assumed. Figure 1 shows the simpliﬁed ﬁnite state machine (FSM) related to load and store operations. In the ﬁgure nodes are represented by circles. The current block state is represented with text placed just above the circle while the new state after the transaction is represented under the circle. Messages sent between nodes are represented by arrows. Messages for load and store operations are combined (e.g. GETS/GETX). Whenever a L1 load miss occurs, a GETS message is sent to the L2 Home bank. Based on the block state at L2 different actions are performed. If the block is in S or C state (Figures 1(a) and 1(c)), the L2 sends the data to the L1 requestor. If the 978-1-4673-9030-9/16/$31.00 c(cid:13)2016 IEEE (a) L2 in state C (b) L2 in state P (c) L2 in state S (d) L2 in state I Fig. 1. Coherence protocol transactions. block is in P state (Figure 1(b)), the L2 sends a forward (FWD) message to the L1 with the block and the block state at L2 changes to S. When the FWD message arrives, the L1 cache sends the data to the L1 requestor cache and the block state changes O. Finally, if at the L2 the block is in I state (meaning a miss occurs, Figure 1(d)), a REQ message is forwarded to the MC. The L2 receives the data and forwards it to the L1 cache requestor. The state of the block is set to P in the L2 and to E in the L1. Whenever a L1 store miss occurs, a GETX message is sent to the L2 Home bank. Similarly, if the block is in C state (Figure 1(a)), the L2 sends the data to the requestor and changes the block state to P. A different case occurs when the block is in P state (Figure 1(b)). The L2 sends an invalidation message (INV) to the L1 owner cache. When the INV message arrives, the data is sent to the L1 requestor and the block changes from E to I. When the block is in S state (Figure 1(c)), the L2 sends the data to the L1 requestor and sends INV messages to all the L1 sharers. When each L1 sharer receives the INV message, it sends and ACK message to the requestor and changes the block state to I. Finally, if the block is in I state (miss) (Figure 1(d)), then a REQ message is forwarded to the MC. Once the data is received, it is forwarded to the L1. The block is put in P state in L2 and in M state in L1. As we see, the protocol faces mainly four possible paths depending on the type of operation (load/store), L1 access type (miss/hit) and L2 access (miss/hit). Whenever a miss occurs at L1 and L2 the NoC gets involved and the memory transaction latency is increased. Factors than may affect signiﬁcantly memory latency are the distance between the L1 requestor and the L2 Home bank, distance between the L2 Home bank and the MC, and the network congestion. (a) total transac (b) total transac (%) Fig. 2. Memory transactions types for different SPLASH-2 applications. To better analyze these effects, Figure 2(a) shows memory transactions classiﬁed by type: MC transactions (due to miss both in L1 and L2 banks), RR transactions (request-response due to an L1 miss and an L2 hit to a clean block), FWD transactions (forward transactions due to an L1 miss and an L2 forward to the owner) and REPL transactions (replacement transactions due to L2 being full). Different SPLASH-2 applications are run in a system described in Section IV. First thing to note is the different amounts of transactions between applications. This depends on the complexities of the applications. However, when we normalize the numbers, Figure 2(b), we see similarities. A high percentage of REPL transactions exist in all applications (more than 50% of the transactions). This is mainly due to the L2 cache size and the dataset of the applications and these transactions can not be predicted from the coherence protocol point of view. Noteworthy, we can observe the small percentage of FWD transactions. All applications exhibit less than 3% of this type, and for OCEANNC and RADIX lower than 1%. This indicates shared blocks mainly follow a multiple sharers pattern and not a producer-consumer pattern. Now, we focus on a more interesting transaction type. Some applications (FFT and RADIX) trigger a large percentage of MC transactions while others (BARNES and WATERNSQ) have a marginal percentage. This difference is due to the L2 hit rate and the memory footprint of the application. Finally, RR transactions are highly representative in some applications (BARNES and WATERNSQ) while minimal in others (FFT). This is due again to the hit rate in L2 and the memory footprint. Thus, applications either have large percentage of MC transactions or large percentage of RR transactions. In both cases, MC and RR, trafﬁc needs between L2 and MC, and between L2 and L1 (requestor) can be predicted. PROSA will exploit this fact by using circuits for MC and RR transactions. I I I . PROSA PROSA sets up circuits between MC-L2 and L2-L1 before they are needed. First, we show the modiﬁcations performed in the coherence protocol and then, show the modiﬁcations in the NoC (the PROSA controller and PROSA router) and MC (the Memory Latency Control Unit, MLCU). A. PROSA Coherence Protocol In order to program circuits, we add a new action termed SE TCIRC . This action involves the source and destination of the circuit, and the time period the circuit will be needed (Delta T). The action is triggered by the coherence protocol in MC and RR transactions (see Figure 3(a)). In MC transactions, whenever a request is received by the MC, a Memory Latency Control Unit (MLCU; described in Section III-D) predicts when the data will arrive to the MC. Based on this delay, the MC triggers a SE TCIRC action after a delay period (DP). DP equals to the predicted memory latency minus the circuit setup period (CSP). The SE TCIRC action sets the circuit. By the time the block arrives to the MC the circuit has been set and is kept during the time the data will be transmitted. When SE TCIRC arrives to L2, the circuit is conﬁrmed but in parallel a new SE TCIRC between L2 and L1 is triggered. Thus, when the data arrives to L2, after accessing the L2, the block is sent also to L1 using a circuit. These two circuits are predictable circuits in the sense they will be necessary regardless of network status. Figure 3(b) shows the timing of both circuits. (a) Transaction (b) Cronogram Fig. 3. PROSA new actions triggered by the coherence protocol and circuit establishment time line for MC transaction. In RR transactions (between L1 cache and L2), whenever a GETS/GETX message is received, a SE TCIRC action is triggered between the L2 and the L1. Again, the connection will be set only for the time period the data is available. However, contrary to the two previous triggered circuit scenarios (in MC transactions) now this circuit is speculative, in the sense the circuit may not be needed if the block is in P or I state. Therefore, for this type of circuit we will need to automatically and efﬁciently tear down the circuit when transmission is ﬁnished or when it is clear the circuit is not needed.This modiﬁcation does not introduce out of order delivery, because for the same transaction the protocol never sends two messages to the same destination. B. PROSA Circuit Network PROSA follows a clustered approach where one PROSA Controller (PC) controls four routers (Figure 4(a)). When the SE TCIRC action is triggered in an L2 or MC, a request circuit (RC) message is forwarded to the local PC. If the RC wins the needed resources in the cluster, it advances to the next PC along the path. If not, a NACK response is sent to the source of the RC message. When one RC message reaches the PC that controls the destination node and wins the resources, then an ACK response is sent back to the source via the PC network. Note that routers are not affected by RC messages,nor ACKs neither NACKs. All this trafﬁc travels via PCs. On top of Figure 4(c), we show the RC message is composed by eight ﬁelds. P refers to the input port from which the RC arrives to the cluster. src and dst are the source and destination for the circuit. Delta T and Delta T’ carry the number of cycles to wait until the circuit will be established and torn down, respectively. Type carries the type of the RC message (REQUEST, ACK, NACK). ID identiﬁes the circuit inside the network, and ﬁnally, GT, Golden Token, refers to the distance between src and dst. This ﬁeld is used in the PC to assign priorities between requests. Below the RC structure, in Figure 4(c), we show the Resource Arbiter (RA) that controls whether a speciﬁc resource can be reserved for a particular period of time. A PC consists of several RA modules, one RA per output port in the cluster controlled by the PC. In total, each cluster has twenty eight RA, all shown in Figure 4(d) (e.g. R0S corresponds to the south output port at the north-west router in the cluster). An RA module arbiters between some requests, the number of requests depends on the number of inputs ports that have dependencies with the resource (the output port). This depends on the routing algorithm, in our case we assume DOR routing. When an RC message arrives to RA, the dst ﬁeld is checked to decide whether the output port controlled by the RA is along the path between src and dst. If so, the RC goes to the Static Arbiter (SA) which arbiters between the incoming requests at this point. SA gives priority to requests with larger values in the GT ﬁeld (longer paths have priority). Then, RC advances to the Time Comparator (TC) module where it checks that the request does not overlap in time with any previously programmed circuit. Finally, if there is no conﬂict with programmed circuits then the RC is stored in the Register Table and forwarded along the PC network. Notice that only one RC message gets access to the table. As we will see, this does not impact performance and reduces complexity. The register table (RT) keeps circuits information. For each circuit the following ﬁelds are stored: Delta T, Delta T’, ID, src, and ST. When an ordinary request wins the resource, Delta T, Delta T’, ID, src get the values from the RC message. ST keeps the state of the circuit, which can be unconﬁrmed, conﬁrmed, or empty. Delta T is decremented by one every cycle. When it reaches zero, the RA module activates the outputs Cir to P which control the circuit establishment in the PROSA router (described later). Delta T’ is also decremented by one every cycle until it reaches zero. When Delta T’ arrives to zero the signals and the register are cleared. When an ACK or a NACK arrives to RA, it follows the same path than an ordinary request, with some small differences. First, the RC message advances to SA. ACK/NACK messages have higher priority than ordinary requests. By construction we guarantee only one ACK/NACK enters one PC controller each cycle, thus they will always win the SA access. Then, in TC the RC message checks the information stored in the table. An ACK consolidates the stored information while a NACK removes it. Then, the RC message is forwarded (to the next RA along the path inside the cluster or to the next PC). The PC controller is shown in Figure 4(d). It contains 28 RAs, 7 per router. For the sake of simpliﬁcation, we group all outputs of a router (L1, L2, MC) into a single RA (RxL ), thus showing only twenty RAs. The PC controller implements two queues to store generated and incoming ACK/NACK messages. RC requests can be dropped. A demultiplexer located at the input port (left-hand side) separates requests from ACK/NACK messages (t y pe ﬁeld of the RC message is used as selector). When a request arrives, it continues through the PC. However, if the incoming request is an ACK/NACK, it is sent to the corresponding ACK or NACK queue. The next stage in PC is a multiplexer, which multiplexes between the incoming RC message and queued ACK/NACKs, giving priority to ACK/NACKs. In case of conﬂict the request is discarded (a) Prosa Network (b) Prosa Cluster (c) Resource Fig. 4. PROSA controller, detailed components. (d) Router circuit ”CC” (generating a NACK message which will be queued). Finally, the selected message enters the RA tree. RA modules are linked following the resources dependencies imposed by the routing algorithm (DOR in our case). An example of linked arbiters are R0E → R1S → R3S for messages coming through router PR0 and leading to a destination below router PR3. An RA module has several input and output dependencies as multiple routing combinations exist. Along this path, if a request wins all the required RA modules (from left to right) the request will be forwarded to the next PC, or will generate an ACK if the destination is controlled by the current PC. A comparator at the output of the PC module (right-hand side) checks whether one RC message won all the required resources (it was at the input side and also succeeds at the output side). In not, a NACK message is triggered and stored in the NACK queue. Figure 4(d) shows the ACK and NACK queues and its control logic block. The logic guarantees only one ACK/NACK message will access the PC controller each cycle, ensuring that they will always win all RAs. NACK messages have higher priority than ACK messages. Moreover, the logic changes the input port of the ACK response, computing the input port of the ordinary request corresponding to this ACK response. Thus, ACK messages cross the same RA path used by the associated request messages. Notice that NACK generated messages locally in a PC controller need to be re-injected in the RA tree again to remove all the reserved resources, and then sent back to the previous PC controller. C. PROSA Router Figure 5 shows the modiﬁcations performed in the baseline router. We only add one demultiplexer per input port, one multiplexer per output port, connections between those elements and an asynchronous repeater per output port. The repeater allows to forward the ﬂit very fast reducing wire delay, as is used in SMART [2]. This technology allows one ﬂit to cross all the network between source and destination in one cycle. The router works as usual until signals Circ to X are activated, where X is the output port (L1, L2, MC, N , E , W , S). In this situation, the corresponding input and output ports are switched in and the arrived ﬂits are blindly forwarded through them. At the same time, VA and SA arbiters associated to the output port are disabled. Notice that the circuits won’t use buffer resources, therefore circuits cannot introduce deadlocks. Fig. 5. PROSA router. Circ to X signals are generated by the PC controller, each generated by a single RA module (the one that manages the output port). Circ to X indicates the input port that has to be switched in to the ’X’ output port. One wire is used for each possible input port. Thus, the demultiplexer at an input port is selected from the ORing of Circ to X signals, and the multiplexer at an output port is selected from the ORing of all the wires from the associated Circ to X signal. Circuits are cleared in a distributed and silent mode. When T and T’ values reach zero on a resource arbiter (RA), the connection is eliminated as well. This is a valid point for addressing correctly miss peculation of circuits or when the data gets delayed more time than expected, for instance from the memory bank through the MC. D. Memory Latency Control Unit Figure 6 shows the MLCU module that assists the PC controller at the MC. It computes arrival time of memory blocks. MLCU works at bank level by monitoring their status (which row is open) and memory requests (which bank/row to access). The register keeps the information required per bank to correctly compute block arrivals: ID for the request identiﬁer, ROW for the bank row where data is located, and Tpred and Tend deﬁne the time period when data arrives to MC. When a request arrives to the MC, the MLCU computes the bank associated. Then, a comparator checks if the bank is idle. In that case MLCU proceeds to calculate the block arrival time (LAT COMP logic block) and the request is sent to main memory. Otherwise, the address and requestor ID are queued and the request is sent to main memory. Queued bank requests are dequeued when a new block arrives from the bank. Fig. 6. Memory Latency Predictor information. To properly compute arrival times (Tpred ), we need some timings from the memory, mainly the activation, precharge and read latencies. The last Tpred and Tend values are stored in the register (associated to the bank). If the current row is open the next Tpred is computed as the maximum between the last Tend and current time plus the read latency. Otherwise, Tpred is equal to the maximum between the last Tend and current time plus activation, precharge and read latency. Whenever a new Tpred is computed, the MC schedules a SE TCIRC action. This action is scheduled sixteen cycles before the data arrives to MC. When the ﬁrst ﬂit from the incoming data arrives to the MC, then the MLCU dequeues a request and computes its Tpred value. E. PROSA Circuit Setup Example Figure 7 depicts an example of a successful PROSA setup circuit process between MC and L2, following a SE TCIRC action triggered by the coherence protocol. The circuit establishment process starts when the REQ message arrives to the MC. Let’s assume that this event occurs at t0 and all resources are available (no circuit conﬂicts will arise). Also, the MC knows the requested data will be available in 10 cycles (from memory). The MC will process the request in one cycle, thus, in t1 , will send an RC message to its PC (PC0). Fig. 7. PROSA setup circuit example. At t1 , RC reaches PC0 and attempts to get all the necessary resources in the cluster along the path between MC and L2. The resources RC will compete for are R0E → R1E (east port of PR0 and PR1 ). At each RA the request will be stored. Delta T will be set to 9 (Delta T at request) in the R0E , R1E RA modules. Delta T’ will be set to the number of cycles needed for the transmission of the block. At t2 , the RC message is forwarded from PC0 to PC1 . Notice stored values of Delta T are decreased by one. At PC1 , the same process described for PC0 applies, but now for resources R2E → R3S → R7L . After winning the resources, PC1 generates an ACK message that will be stored in the ACK queue. Delta T ﬁelds at RA modules for R2E , R3S and R7L are set to 8. At t3 , all the resources with one circuit established (even in uncon f irmed state) decrease Delta T value by 1. Also, at t3 , PC1 processes the ACK message. As said above, ACKs and NACKs messages have higher priority, and then this ACK will win the necessary RA modules, thus conﬁrming the circuit at PC1 and sending back the ACK message to PC0 . Finally, at t4 , the ACK message arrives to PC0 , being processed at t5 and winning all the RA modules and conﬁrming the circuit at PC0 . The ACK message is forwarded to the MC node, conﬁrming the circuit at the Network Interface (NI). In case that any of the resources were not available during the circuit setup time, the affected PC would generate a NACK message and would enqueue it into the NACK queue. The resources would be freed the next cycle and the NACK message would be transferred to the previous cluster. If a NACK is received at the NIC, the packet will be injected using packet switching. Time advances and at time t11 , Delta T value at RA module associated with R0E reaches 0, thus the signal Circ to W (in R0E ) is activated and set to point to the local port. This signal switches in input port from MC and output port E at PR0 . This circuit will last the required number of cycles for the message (Delta T’). All other programmed output ports (R1E , R2E , R3S and R7L ) switch the input and output ports making the circuit just for Delta T’ cycles. In this cycle the MC receives the block from memory and injects the ﬁrst ﬂit. The ﬂit is forwarded and crosses the entire network reaching the L2 output port at PR7. Finally, when Delta T’ reaches 0, the circuit is torn down in a distributed manner on each router just when the last ﬂit of the message crossed the router. IV. PROSA EVALUAT ION For the performance analysis we use an event-driven cycleaccurate simulator that models any network topology and router architecture. We model a 2-stage pipelined router (Input Buffer (IB), Routing (R), VC allocator and Switch allocator Fig. 8. Performance results for different architectures (BASELINE, DEJAVU, PROSA in column order). (VASA), that three in only one stage, and Crossbar (X)) with VCs and ﬂit-level crossbar switching, as used in Garnet [8]. Table I shows the simulation parameters for the router and the cache hierarchy. L1 caches are private to the core and L2 cache is shared but distributed among all the tiles. Network 8x8 mesh 4/1 cycle 8/72 bytes 8/72 bytes Parameter Topology VCs/ﬂy link Message sizes Flit/Queue size sets/way/line size (B) cache/tag latency Num. MCs Topen/Tact/Tread L1 L2 per tile MC 32/4/64 2/1 128/16/64 4/2 2 16/16/16 PARAM E TER S AND VA LU E S U S ED FOR ROU T ER S AND CACH E S . TABLE I We evaluate three mechanisms: the baseline router, the Dejavu solution [9] and PROSA. In PROSA data sent through circuits take one cycle (using SMART). We analyze applications from SPLASH [10] and PARSEC [11]. Deja Vu pre-allocates the circuit between nodes in order to hide the setup latency by dividing the NoC in two planes: control and data plane. The control plane is in charge of conﬁguring the circuits. This plane has higher voltage and frequency, so is faster than the data plane. In this NoC, the request packet pre-allocates the path in backward direction as it approaches destination. The destination node can forward the response to the requestor node whenever the data is ready with no circuit setup. This approach can produce conﬂicts. Deja Vu conﬁgures the circuits in the order they are reserved. A. Results Figure 8(a) shows the application runtime. PROSA reduces, on average, BASELINE application runtime by 33.11%. PROSA reaches lower gains in applications with a low number of L2 misses (BODYTRACK), or with short application runtimes (LU) as it improves those application runtimes by 5.45% and 4.0% respectively. However, with balanced applications (OCEANNC), or applications with a high number of MC requests (FFT, CANNEAL, FMM, BLACKSCHOLES) PROSA reaches an improvement up to 50%. Deja Vu achieves negligible beneﬁts in performance, as already seen in [9]. Figure 8(b) shows latency results. DEJAVU outperforms on average BASELINE by 10%. PROSA, in applications with a low number of MC requests and small datasets (e.g. BODYTRACK), achieves a 7% improvement on end-to-end latency. However, PROSA outperforms BASELINE up to 39% on applications with a high number of L2 misses (e.g. CANNEAL). On average, PROSA outperforms BASELINE end-to-end latency by 31.14%. For network latency (solid color), on average, PROSA improves by 34.16 %. Now, we analyze results for L1 miss latency normalized to BASELINE (Figure 9). In this case, as it occurs with runtime, DEJAVU slightly improves BASELINE and PROSA achieves better performance. On average, our proposal improves BASELINE by 23 %. Table II shows statistics about PROSA circuits. The ﬁrst column shows the received ACKs (the number of circuits established successfully). In all cases this is higher than 85.58% and the average is 90.62%. The second column shows the received NACKs and is the sum of the next ﬁve columns, which list the different types of conﬂicts in the PC controller. The third and fourth column show the number of NACKs generated at the input of the PC controller as a normal request and an ACK/NACK conﬂicted at the same time. The last columns show conﬂicts generated at the RA arbiters due to (1) the register table is full (FB), (2) the required period of time of one request overlaps with one established circuit (TMP), and (3) two requests collided in the same RA module (ARB) due to the static arbiter. As we can see, most conﬂicts are generated because of temporal conﬂicts in generating circuits or because of concurrent requests enter with conﬂict with an ACK is the same resource. However, the current table size at RA modules (4 entries) seems to be properly sized (even could be reduced thus saving more area) as the number of conﬂicts due to the table being full are negligible. B. PROSA Implementation We have implemented all the PROSA infrastructure for a 4 × 4 CMP system. Each PC module has been implemented in Fig. 9. Memory latency results for different architectures. Normalized to baseline case. CONF. INPUT CONF. RA REC.ACK REC. NACK NACK ACK FB TMP ARB (OCNC) 85.98% (LU) 4.28% (LU) 0.01% (BOD) 0.63% 0% (WSPA) 1.92% (WSPA) 0.31% 90,62% 9,38% 0,04% 3,94% 0,00% 4,61% 0,79% (LU) 95.72% (OCNC)14.02% (WSPA) 0.18% (WSPA) 9.27% 0% (OCNC) 9.04% (OCNC) 1.70% MIN AVG MAX NUMB ER O F ACK AND NACK M E S SAG E S GEN ERAT ED IN PROSA AND NUMB ER O F CON FL IC T S GEN ERAT ED IN PC MODUL E . TABLE II Fig. 10. Power consumption results. First column represents BASELINE and second PROSA. Verilog and tested. We use a canonical router design with 64bit ﬂits, four 9-ﬂit depth VCs, and with seven ports to attach 2D-mesh ports and L1, L2, and MC (including the MLCU). We use Design Vision tool from Synopsys with 45nm Nangate open cell library [12]. Power results are obtained from Orion-3 power library [13]. Table III shows the area overheads of PROSA for different conﬁgurations. In all of them, and for the sake of comparison, we also account for the components to build a cluster of four routers. In the case of PROSA we consider all the components (including the PC and the four PRs). Conﬁguration Baseline router PROSA Router (PR) PROSA Controller (PC) Baseline cluster PROSA cluster Flattened Butterﬂy cluster 2xBaseline cluster area (µ m2 ) 243784 248200 76547 975136 1069347 1380109 1658560 TABLE III overhead 1.8% 9.66% 41% 70% AR EA OV ERH EAD S FOR D I FFER EN T ROU T ER AND NOC ORGAN I ZAT ION S . As we can see, the PROSA router takes only 1.8% more area than the baseline router. The PROSA controller (PC) takes less area than a baseline router (22% of baseline router area). However, this component is new and needs to be considered as an additional overhead. To make this comparison fair, the table shows area of cluster regions. In this case, the PROSA cluster takes 9.66% more area. The NIC’s overhead can be considered as negligible. PROSA overheads can be seen as high. However, we should consider the performance gains that PROSA circuits enable. In order, however, to better assess the overheads, the table shows two additional conﬁgurations worth being analyzed. The ﬁrst one, f l at t ened but t er f l y, is the overhead for a ﬂattened butterﬂy topology which has more connectivity along each dimension and direction. In this case, because of the larger number of input ports, the area overhead is increased by 41%. The second one is for the case where the baseline cluster is enhanced with double ﬂit size. This can lead to faster transfer times between the nodes. However, as we can see, overhead skyrockets to 70% additional area. This is mainly due to the larger buffer requirements. Figure 10 shows the energy results. Although the leakage energy increases by 42% the total energy consumption is reduced by 3%. This reduction is due to switching and internal energy, which are reduced by 10%. As described in [9] Deja Vu achieves a 30% energy reduction, which is similar to our results (but without the latency improvements). Notice that the 30% execution time reduction will translate in major power savings. V. R E LATED WORK Circuit-switching [14] has been used in a number of previous works in NoC architectures in order to reduce onchip communication latency. Once a circuit is set, data does not travel through the routing and arbitration stages on each router. However, setup time usually causes low resources utilization and performance degradation. On the other hand, packet switching improves resource utilization and network performance, splitting the entire message in smaller blocks and forwarding them along the network. Some works try to get beneﬁt from both mechanisms by implementing a hybrid circuit-packet switching strategy. Kumar [15] proposes Express Virtual Channels (EVC) allowing packets to bypass intermediate routers along their path. EVCs only allow to connect nodes along the same dimension, so circuits cannot turn from one dimension to another. PROSA, on the other hand, allows to connect nodes regardless their location, thus offering more ﬂexibility. Jerger [6] proposes circuit switched coherence, setting permanent circuits between pairs of frequent data sharers instead of tearing them down. It allows to quickly send data between the same nodes. However, if another circuit requires the resource, the data is switched to packet switching until it reaches destination. Yin [5] proposes a hybrid circuit-packet switched network in which the packet is forwarded along the packet network while the circuits can be set in parallel, using TDM. Yim’s proposal also expends time in the setup latency. Mazloumi [3] proposes another hybrid packet-circuit switched router. This mechanism setups the circuit along the network while the request message is being forwarded between the requestor and the destination. When the request reaches the destination and the data is ready, the mechanism sends a probe message activating the reserved circuit, after that the data is sent. All these mechanisms require a setup period. However, PROSA hides the setup circuit latency based on the coherence protocol and when the circuit usage ﬁnishes, resources are freed allowing the packet switching to forward packets without having to tear down any circuit, the data is sent without delay. Abousamra [9] proposes Deja Vu (brieﬂy described in Section IV). In Deja Vu, the selected order schema can produce underutilization of network resources. In [16] authors alleviate the problem by using a different order. However, it still requires the high frequency and voltage control plane. PROSA, as Deja Vu, preallocates the circuit in order to hide the setup circuit delay, However, as per our evaluation, PROSA achieves better performance results. Van Lear [4] proposes a coherence-based message predictor for optical interconnection networks. In the proposal a global predictor establishes the circuits between nodes. All the trafﬁc in the network has to cross the predictor, thus potentialy causing a bottleneck in the network. This proposal is also for optical interconnects where a full optical crossbar is assumed. This makes scalability a major issue. Krishna in [2] presents SMART, a multihop network with single-cycle data-path all the way from source to destination. Setup circuit is required one cycle before the data is sent and partial circuits can be established. An extra network is required to send SMART-hop Setup Request, SSR. This mechanism is not based on coherence cache as PROSA and our mechanism relies on SMART circuits. Peh [17] presents ﬂit-reservation ﬂow control. In this proposal the circuit is setup hop by hop. However PROSA uses clusters to setup circuits, improving the time required to establish the circuit. PROSA does not require to buffer messages, contrary to ﬂitreservation. Our proposal anticipates the circuit setup process. As a summary, PROSA allows to establish circuits between any pair of nodes hiding the setup latency. PROSA uses the coherence protocol information to establish these circuits. In addition, PROSA programs circuits for their exact period of time they will be needed, thus not conﬂicting with other circuits using the same resources (in other time periods). Indeed, PROSA circuits can be steered by coherence protocols or even for other higher-level applications where trafﬁc bursts can be predicted and requested ahead of time. The previous strategies do not rely on coherence protocols or they rely on more expensive architectures and technologies. V I . CONCLU S ION S In this paper we introduce PROSA, a circuit-switched enabled NoC architecture which allows the coherence protocol to steer circuit connections for future predictable and speculative connections between memory controllers (MC), L2 cache banks and L1 caches. Connection establishment is performed as single-packet based and established for the required time period for L2 to L1 an MC to L2. On a miss predict the circuit is silently removed. PROSA builds a separate infrastructure to manage circuits, thus the main NoC network is not affected by the additional control trafﬁc. Results show network latency is reduced up to 35% by correctly predicting and using circuits with PROSA. Also, PROSA outperforms the baseline designs by reducing application runtime by 33% and PROSA improves the DEJAVU performance results achieving similar energy results. PROSA takes an overhead of 9.66% more than the baseline proposal, however, it saves more than 3% in power consumption. Overhead of PROSA is reasonable given the beneﬁts in performance. In a future work, we plan to extend our proposal with partial circuits, increasing the number of MCs and studying the table size scalability. "
Sharing a global on-chip transmission line medium without centralized scheduling.,"We consider the design of a shared global on-chip communication medium using repeated equalized transmission lines (RETLs). Our design overcomes a number of limitations with previously proposed shared global mediums based on transmission lines. Prior solutions require wide-pitch transmission lines that occupy considerable area, do not support multicast or broadcast operations, and employ centralized schedulers that are difficult to scale. In this paper, we propose a novel design based on RETLs that utilizes thin transmission lines that require a much narrower pitch, about 6x narrower in comparison to previously proposed wide-pitch-based designs. In addition, our design supports multicast and broadcast operations, which are critical for the implementation of cache coherency protocols. Moreover, our design is based on fully distributed arbitration protocols that can achieve very high throughput and bandwidth utilization, but are simple to implement. We demonstrate a design using 32 lanes of 20 Gb/s differential RETLs that provides 640 Gb/s aggregated throughput and enables communications between any on-chip cores in under two core clock cycles, including multicast and broadcast communications. Given the narrow pitch of RETLs, our design can easily scale to multiple terabits per seconds with additional lanes. Simulation results with both synthetic and real benchmarks with up to 64 parallel threads demonstrate that our proposed distributed solutions are capable of achieving near ideal throughput.","Sharing a Global On-Chip Transmission Line Medium without Centralized Scheduling Yashar Asgarieh Computer Science and Engineering Department University of California, San Diego Email: yasgarie@cs.ucsd.edu Bill Lin Electrical and Computer Engineering Department University of California, San Diego Email: billlin@eng.ucsd.edu Abstract—We consider the design of a shared global on-chip communication medium using repeated equalized transmission lines (RETLs). Our design overcomes a number of limitations with previously proposed shared global mediums based on transmission lines. Prior solutions require wide-pitch transmission lines that occupy considerable area, do not support multicast or broadcast operations, and employ centralized schedulers that are difﬁcult to scale. In this paper, we propose a novel design based on RETLs that utilizes thin transmission lines that require a much narrower pitch, about 6x narrower in comparison to previously proposed wide-pitch-based designs. In addition, our design supports multicast and broadcast operations, which are critical for the implementation of cache coherency protocols. Moreover, our design is based on fully distributed arbitration protocols that can achieve very high throughput and bandwidth utilization, but are simple to implement. We demonstrate a design using 32 lanes of 20 Gb/s differential RETLs that provides 640 Gb/s aggregated throughput and enables communications between any on-chip cores in under two core clock cycles, including multicast and broadcast communications. Given the narrow pitch of RETLs, our design can easily scale to multiple terabits per seconds with additional lanes. Simulation results with both synthetic and real benchmarks with up to 64 parallel threads demonstrate that our proposed distributed solutions are capable of achieving near ideal throughput. I . IN TRODUC T ION Continuous improvements in CMOS technologies have led to an increasing number of cores on a chip. Although conventional hop-by-hop on-chip networks, such as mesh networks, are scalable in size, their effectiveness at global communications signiﬁcantly diminishes with increasing number of on-chip cores. Even in the absence of queuing delays, global communications can take many tens of cycles, which complicates the design of cache coherence protocols and can signiﬁcantly diminish the effectiveness of caching. Besides the latency problem, the trafﬁc load on each link also increases with the network diameter, which diminishes the effective throughput. To address these challenges, researchers have sought alternative technologies for global communications. One candidate technology for implementing a global onchip network is optical interconnect, which is made possible by recent advances in silicon nanophotonics [1], [2]. The data travels along optical waveguides at approximately the speed of light. However, conventional CMOS fabrication processes cannot be directly used to realize nanophotonics. An alternative candidate technology is the use of on-chip transmission lines (TLs) as a global shared medium [3]– [10]. Transmission lines can also deliver data at the speed of light across the shared medium and consumes much less power than conventional wires because the wave propagation eliminates full-swing charges and discharges on the wire and gate capacitance. Transmission lines are attractive because they can provide very low latency packet delivery across chip (order of ns), very high bandwidths (20+ Gb/s per TL pair), and high energy efﬁciency. Previously, Carpenter et al. [7], [8] proposed a globally shared-medium design for on-chip communications based on transmission lines. Their design comes with a number of limitations. First, they use transmission lines as a shared bus with differential signaling, but their design does not make use of equalization circuitry or repeaters. To overcome frequencydependent loss of transmission lines, their design requires wide-pitch transmission lines to ensure signal integrity at high data rates. The total pitch (including spacing and shielding) per differential pair of transmission lines is 45µm, which occupies considerable area1 . Second, their bus-based architecture is merely a shared medium that allows conﬁgurable point-to-point communications, but does not support multicast or broadcast operations, which are critical for cache coherency protocol implementations. The reason for only allowing a single receiver is to limit the distortion that would be caused by multiple receivers when no equalization circuitry or repeaters are used2 . Third, for access arbitration, the authors had assumed that a distributed arbitration protocol, such as carrier-sensing, would not be practical because some of the known protocols have poor bandwidth utilization properties. They cited a well-known collision detection protocol [11] as an example that can only achieve at most 36% bandwidth utilization. Therefore, they proposed instead to use a centralized scheduler for access arbitration. However, a key challenge in implementing a practical centralized arbitration scheme is the need for getting the requests from the cores to the centralized scheduler and the grants back to the cores. Unfortunately, if signiﬁcant latencies are incurred on these control lines, the performance of a centralized scheduler diminishes substantially, leading to 1As will be discussed in Section II, the total pitch for our design (including spacing and shielding) is only 7.8µm, which is about 6x narrower. 2 Previously, Ito et al. [4] proposed a bidirectional multi-drop transmission line interconnect that can support multiple simultaneous receivers, but these links can only reliably operate at much lower data rates (e.g., 8 Gb/s) due to attenuation caused by multiple receivers when no equalization circuitry or repeater structures are used. Further, their design is also based on wide-pitch transmission lines that have similarly signiﬁcant area overhead. 978-1-4673-9030-9/16/$31.00 c(cid:13)2016 IEEE substantially diminished system performance. In this paper, we propose a novel design of a shared global on-chip communication medium using repeated equalized transmission lines (RETLs) that overcomes the above limitations. Our contributions can be summarized as follows: • Our design is based on shorter-length segments of transmission lines that are repeated and equalized at each segment to form a global shared medium. The use of sophisticated equalization circuitry and a repeater structure [5], [6], [9] enables the use of thin wires to implement the transmission lines that can tolerate the resistive loss and inter-symbol interference at very high data rates (e.g., 20 Gb/s per TL pair with differential signaling). • The design also allows all receivers to simultaneously listen to the shared medium while ensuring signal integrity, which means multicast and broadcast operations can readily be supported. These operations are essential for cache coherency protocol implementations. • We propose several novel arbitration schemes that are fully distributed. These distributed schemes can achieve very high throughput and bandwidth utilization, but yet are simple to implement. Unlike arbitration schemes that have been proposed for nanophotonics that rely on the inherent ability of nanophotonics to divert light [1], [2], for which there is no equivalent for transmission lines, the schemes that we propose are based on the ability of multiple receivers to determine when a channel becomes available. • We demonstrate a design using 32 lanes of 20 Gb/s differential RETLs that provides 640 Gb/s aggregated throughput and enables communications between any on-chip cores in under two core clock cycles, including multicast and broadcast communications. Given the narrow pitch of RETLs, our design can easily scale to multiple terabits per seconds with additional lanes3 . • Comparing to ideal scheduling, we show that the proposed distributed arbitration schemes can nearly fully utilize the shared bandwidth, while keeping the average latency low. The rest of the paper is organized as follows: Section II presents an overview of our architecture based on repeated equalized transmission lines (RETLs). Section III presents several novel fully distributed arbitration schemes for coordinating access to the shared RETL medium. Section IV presents evaluation results, and Section V concludes the paper. I I . SY ST EM ARCH I T EC TUR E In this section, we ﬁrst present an overview of the repeated equalized transmission lines that we use to build our global shared medium. We then describe how the overall system is organized with respect to the processor cores and this shared medium. Finally, we describe how data transmission over this shared TL medium operates. A. Repeated Equalized Transmission Lines Unlike the wide-pitch TLs used in previous works (e.g., [4], [7], [8]), we utilize equalization techniques in our TL structure in order to signiﬁcantly reduce the pitch of the TLs as well 3 For example, a design with 128 lanes can achieve an aggregated throughput of 2.56 Tb/s. TABLE I RETL P ER FORMANC E M E TR IC S . Dimensions Delay Power Pitch (width+spacing): 2.6µm, Total pitch(a pair of differential TLs + power/ground shielding): 7.8µm, Length: 2.5mm each segment. Transmission line: 13 ps, Tx: 44 ps, Rx: 45 ps, Total: 102 ps, Normalized: 40 ps/mm. Total: 1.66 mW (Tx: 0.79 mW, Rx: 0.87 mW), Energy/bit: 0.08 pJ/b. as to achieve a high reliable data rate. In particular, we utilize the repeated equalized transmission line (RETL) design from our previous work [9]. We defer the reader to [9] for a more detailed discussion of this design. Here, we simply depict an abstraction of a point-to-point RETL segment in a bounding box shown in Fig. 1. Unlike conventional wires, a transmission line operates in the LC region at high-frequency (e.g. 20+ GHz) and carries low-swing waveforms at near the speed of light. The RETL design employed in this paper is based on a differential TL pair with terminated resistance, with the TL pair surrounded by power and ground lines for shielding. As described in details in [9], the transmitter (Tx) comprises a chain of tapered current-mode logic (CML) buffers as the driver. The number of CML stages and the tapered factor can be optimized based on the length of the TL segment and the expected load. On the receiver side (Rx), a continuous-time linear equalizer (CTLE) and a sense-ampliﬁer based latch are used to recover the transmitted signal by boosting the eyeopening (i.e., to compensate for resistive loss and inter-symbol interference). By co-optimizing the transmitter, the length and pitch of the differential TL pair, and the receiver together, we can achieve an optimized result for a target link throughput and latency. The reader is referred to [9] for a detailed description of this co-optimization ﬂow. With this co-optimization ﬂow, we can derive an RETL segment that is 2.5mm in length, which can achieve a 20 Gb/s throughput and a 40 ps/mm normalized latency. Equivalently, an RETL segment operates at a 20 GHz communication clock frequency. The performance metrics for this link segment design is shown in Table I. With the use of equalization techniques, thin wires can be used to implement the TLs. Our design supports a wire pitch of 2.6µm that includes the wire width and the wire spacing. The total pitch is 7.8µm, which includes the wires and spacing for a pair of differential TLs as well as the surrounding power and ground lines for shielding. This is about 6x narrower than the total pitch of 45µm required for the wide-pitch TLs used in previous works (e.g., [7], [8]). Note that these RETL segments are unidirectional. They can be cascaded together to form longer connections. B. Cluster Architecture We next describe the design of a system that comprises 64 cores. Each core comprises a processor, an L1 data/instruction cache, a slice of a shared L2 cache, and a slice of the cache coherency directory. To take full advantage of the high data rates that can be achieved over the shared global RETL medium, we group four cores together into clusters via 4:1 concentrators. This is depicted in Fig. 1. Each cluster of four cores shares a common interface to the shared global RETL medium for inter-cluster communications. All inter-cluster trafﬁc will be transmitted via the shared global RETL medium in FIFO order. For intra-cluster trafﬁc, they will be handled (a) Topological abstraction showing how the clusters are interconnected. Fig. 1. Clusters of four processor cores are grouped together via 4:1 concentrators. Clusters are interconnected by RETL segments. by the concentrator, which also acts as a local crossbar. For cache coherence, we assume a MOESI directory-based protocol over a snoopy-based approach because a MOESI directory-based approach has been shown to be more powerful and performance efﬁcient [12]. For our evaluations in Section IV, we assume a core clock frequency of 1.25 GHz, which corresponds to a 0.8 ns clock period. With the shared RETL medium operating at a 20 GHz communication clock frequency, data can be sent 16 times faster over the shared TL medium than the core clock rate. To bridge the two clock rates, 16:1 serializers (SER) and de-serializers (DES) are shown in Fig. 1 to perform the respective operations. These serializers/de-serializers can be efﬁciently implemented using a power-efﬁcient tree structure that includes a four-level tree of buffers and dividers [13]. The clusters are interconnected via cascaded RETL segments that form a global shared medium. When a cluster transmits data on this shared medium, all clusters can simultaneously listen to this shared medium, and each cluster can determine for itself if it should receive the data being transmitted based on the addressing information provided. Since all clusters can simultaneously listen to and receive from the shared medium, multicast and broadcast operations can readily be supported. To ensure that only one cluster can transmit at a time, an access arbitration scheme mechanism is needed. Our approach is based on fully distributed arbitration schemes. Each cluster employs a local arbiter that independently determines when the cluster can transmit. We defer to Section III for descriptions of several novel distributed arbitration schemes for coordinating access to the shared medium. C. Shared RETL Bus For a system with 64 cores, they are grouped together into 16 clusters. Fig. 2(a) shows how the 16 clusters are interconnected together to form a unidirectional ring. Fig. 2(b) depicts the ring topology in a chip layout with the 16 clusters. At any moment in time, the loop is broken by the cluster that is transmitting to form a unidirectional bus. The unidirectional bus starts at the transmitting cluster i and ends at cluster ((i + N − 1) mod N ), where N is the number of clusters. This is achievable because two cascaded RETL segments are separated by a selector switch. The transmitting cluster breaks the loop by setting its selector switch. This selector switch is depicted in Fig. 1. Just before a cluster starts transmitting, (b) Chip layout organization for a 64-core system organized into 16 clusters. Fig. 2. Overall system organization. The shared global RETL bus is unidirectional. Though the diagrams show the shared global RETL medium forming a ring, the loop is broken by the transmitting cluster via setting its selector switch accordingly. it conﬁgures its selector switch to connect the output of the serializer (SER) to the RETL transmitter (Tx). This selector switch conﬁguration also disconnects the preceding RETL segment from the forwarding path, hence breaking the loop. All other non-transmitting clusters have their selector switches conﬁgured for pass-through. The selector switch can be implemented using a standard CMOS pass-gate structure, which can be quickly reconﬁgured for connection and disconnection. We assume a chip size of 10mm×10mm, divided into sixteen 2.5mm×2.5mm clusters, with each of the four cores in a cluster occupying a 1.25mm×1.25mm area. This means the sixteen clusters can be interconnected by cascading sixteen 2.5mm RETL segments to form a ring. However, with the decoupling of the ring by transmitting cluster i, the longest distance that a signal needs to travel to reach the last reachable cluster ((i + N − 1) mod N ) is only N − 1 = 15 RETL segments away, or 37.5mm. With a normalized latency of 40 ps/mm, any cluster can reach any other cluster in just 1.5 ns, or under two core clock cycles at 1.25 GHz4 . For our evaluations in Section IV, we assume a design with 32 lanes of transmission lines, each lane capable of sending 20 Gb/s, which provides an aggregated throughput of 640 Gb/s. This means 64 bytes (which corresponds nicely to a cache line) may be sent over the shared RETL bus per core clock cycle. This design provides ample bandwidth for the benchmarks evaluated. Given the narrow pitch of our RETLs, our design can easily scale to multiple terabits per seconds with additional lanes. For example, a design with 128 lanes can achieve an aggregated throughput of 2.56 Tb/s. D. Timing of Operations We now examine the timing of data transmissions over the proposed shared RETL bus. The timing of data transmission 4 Two core clock cycles at 1.25 GHz is 1.6 ns. Fig. 3. Timing of operations for the proposed shared RETL bus. Fig. 4. Example of token-based arbitration timing. operations is depicted in Fig. 3. As will be discussed next in Section III, each cluster will monitor the shared RETL bus to determine whether or not it should start transmitting in the next core clock cycle. If it determines that it should start transmitting in the next core clock cycle, it conﬁgures its selector switch to disconnect the connection from the preceding RETL segment and instead redirects the connection from the output of its serializer (SER) to the RETL transmitter (Tx) of the next RETL segment. This selector switch conﬁguration occurs shortly before the start of the next core clock cycle. This way, at the start of the next core clock cycle, it can start transmitting data on the shared RETL bus without a loop. As shown in Fig. 3, cluster i transmits data to cluster a in the next core clock cycle, followed by data to cluster b in the following cycle, and data to cluster c in the cycle after. Cluster i can continue to send data to different receivers (or possibly multicast or broadcast to multiple receivers) as long as it has possession of the shared RETL bus. When cluster i ﬁnishes transmitting data, the shared RETL bus will need to be idle for a period of time to allow the signals to drain through the shared medium. Recall that a transmission line works by transmitting low-swing waveforms at very high frequencies (e.g., 20 GHz). These waves propagate through the shared transmission line medium. The next cluster cannot safely start transmitting on this shared transmission line medium until all in-ﬂight waves have propagated through. Recall from Section II-C that the longest distance that a wave needs to propagate is through 15 RETL segments, or a worst-case distance of 37.5mm, which takes 1.5 ns. Therefore, the worst-case draining period is under a two-cycle turnaround time. Referring again to Fig. 3, cluster j can start transmitting two cycles after cluster i stops transmitting. As the worst-case draining period is under two core clock cycles, cluster j can conﬁgure its selector switch shortly before the end of the two cycles so that it can start transmitting at the start of the next clock cycle, which is to cluster d, followed by cluster e. I I I . D I STR IBU T ED ARB I TRAT ION In this section, we provide several fully distributed schemes for solving the arbitration problem. The shared RETL bus5 requires arbitration to prevent two or more clusters from transmitting at the same time. We ﬁrst describe a token-based arbitration scheme. We then describe another scheme based on the idea of distributed randomized polling. In addition, 5 Throughout this section, we will occasionally refer to a shared RETL bus simply as a bus. we extend both schemes to work with multiple shared RETL buses that operate in parallel. In particular, multiple buses could be implemented using the same number of lanes by spatially partitioning the lanes into multiple buses so that each (narrower) bus is implemented with fewer lanes. The use of multiple parallel buses enables multiple clusters to transmit concurrently. As we shall see in Section IV, better performance can be achieved with multiple parallel buses even when the number of lanes used remains the same. A. Token-Based Arbitration A typical token ring scheme [14] is based on an acquire-andrelease mechanism. A token gets circulated from one sender (cluster in our case) to the next until it is acquired by a sender that has data to send (i.e., it has a non-empty queue). Such a sender is called a requester. The sender then transmits the data that it wishes to send, possibly for multiple clock cycles, depending on how much data that it wants to send. When it ﬁnishes sending the data, it releases the token for circulation to other senders. The token bypasses non-requesters (i.e., senders with empty queues) until it is acquired by a sender that has data to send. A straightforward implementation in our setting would be to implement the control token ring as a one-bit ring with conventional wires and latches, where a one-bit token would circulate from one cluster to the next at each clock cycle until the token is acquired by a requesting cluster. Once a requesting cluster has acquired the token, it may take multiple cycles to transmit its data, followed by a two-cycle draining period. After which, it releases the token, which may take multiple cycles to circulate through the control token ring until the token reaches the next requesting cluster. A problem with this approach is that it may take multiple cycles before the token reaches the next requesting cluster, which would leave the bus unnecessarily idle. Alternatively, token-based arbitration schemes have been proposed for nanophotonics [1], [2]. In these schemes, the token is broadcasted on an optical ring. These schemes rely on the inherent ability of nanophotonics to divert light. That is, the token travels along the optical ring, bypassing nonrequester, until it is diverted by a requester, at which point the light is completely removed from the optical ring to provide an exclusive grant for the corresponding optical channel. Unfortunately, there is no equivalent diverting ability for transmission lines. In our approach, we also implement the control token ring as a one-bit ring with conventional wires and latches. However, we extend this straightforward scheme with three ideas: • Send-ahead tokens: We allow the current cluster which has acquired the bus to send data for up to T consecutive cycles. In our design with 32 lanes at 20 Gb/s (640 Gb/s aggregated throughput), 64-bytes can be transmitted per core clock cycle at 1.25 GHz. Consider the example depicted in Fig. 4. At cycle t + 1, cluster i has already acquired the bus and transmits to cluster a. At the same cycle, cluster i sends ahead the token along the control token ring. Suppose cluster j , where j = i + 1, is a requesting cluster. Then it receives and acquires the token one cycle later at t + 2. Meanwhile, cluster i continues to transmit data to cluster b in cycle t + 2 and cluster c in cycle t + 3. • Completion sensing and explicit EOT indication: The second idea is completion sensing. Although cluster j has acquired the token in cycle tacquire = t + 2, it does not start transmitting. Instead, it monitors the bus to determine when the previous transmitter has ﬁnished. Rather than inferring completion by monitoring for idle cycles, we require cluster i to explicitly send an “End-of-Transmission” (EOT) status bit at the end of the cycle to indicate that it has completed its transmission. Referring again to Fig. 4, the EOT status bit is sent at the end of cycle t + 3. • Relative propagation time: The third idea is to help cluster j decide when it can start transmitting after it has detected the EOT status bit. As discussed in Section II-C, in our design, the last reachable cluster is at most N − 1 = 15 segments the longest distance that a signal needs to travel to reach away, corresponding to the propagation delay through 15 RETL segments, or 37.5mm, which takes just under two core clock cycles. Since the EOT status bit is sent along the transmission lines, it can take up to almost two core clock cycles for a cluster to detect the EOT status bit. However, clusters that are closer to the last transmitting cluster may detect the EOT status bit after just one core clock cycle. In particular, for clusters that are less than N/2 segments away, they will detect the EOT status bit just after one cycle, whereas clusters that are greater or equal to N/2 segments away will detect the EOT status bit two cycles later. In the example depicted in Fig. 4, since cluster j is only one segment away, it will detect the EOT bit in cycle tdetect = t + 4, one cycle after cycle t + 3 when cluster i sent the EOT bit. Since cluster j has been monitoring the bus, it also knows that the last transmitting cluster is cluster i since the data transmitted contains both the source ID and the destination ID. Based on a table lookup, cluster j can determine if it should check if it has already acquired the token already in cycle tcheck = tdetect , or if it should wait for another cycle until tcheck = tdetect + 1 for the data sent by cluster i to drain through the system before checking if it has acquired the token. In this example, cluster j knows that it is less than N/2 segments away from cluster i, and therefore, it will wait another cycle to tcheck = t + 5 to check if it has the token, and it will start transmitting at the next cycle t + 6. In another words, depending on the relative position of cluster j to cluster i, tcheck is either tdetect or Fig. 5. Example of distributed randomized polling-based arbitration timing. tdetect + 1. As depicted in Fig. 4, cluster j starts transmitting to cluster d at cycle t + 6, then to cluster e in cycle t + 7, and so on. It is worth noting that with the explicit EOT indication, a cluster that has already acquired the token can start transmitting at most two cycles after the last transmitting cluster has ﬁnished. Without the explicit EOT indication, a cluster may have to wait for one more cycle to be sure that the last transmitting cluster had ﬁnished – two cycles to ensure all inﬂight waves have drain through the system, plus another cycle to make sure that bus has been idled. If the token has not reached a requesting cluster (a cluster with a non-empty queue) by the end of cycle tcheck . then all requesting clusters will monitor the bus for two cycles to check if the bus is idle. If one of the requesting clusters has acquired the token during these interim two cycles, then it will start transmission after these two cycles. Otherwise, all requesting clusters will repeat monitoring for two more cycles until one of the requesting clusters has acquired the token. In our evaluation in Section IV, we assume the control token ring is clocked with the core clock. Conceivably a faster clock could be used, in which case, the performance may improve. B. Distributed Randomized Polling Instead of using a token ring, we can alternatively employ the concept of distributed randomized polling, based on the following ideas: • Like the token-based arbitration scheme described above, we also rely here on completion sensing and explicit EOT indication. Consider the example depicted in Fig. 5. Suppose the current cluster that has acquired the bus is again cluster i, and it transmits to cluster a in cycle t + 1, cluster b in cycle t + 2, and cluster c in cycle t + 3. • At the end of the cycle t + 3, we again require cluster i to send an EOT status bit. Meanwhile, all other clusters monitor the bus to detect the EOT status bit. Again, depending on the relative distance that a cluster is to the last transmitting cluster. In the example depicted in Fig. 5, cluster j is one segment away, so it will detect the EOT status one cycle later in cycle tdetect = t + 4. On the other hand, cluster k is 14 segments away, so it will detect the EOT status two cycles later in cycle tdetect = t + 5. (a) Single shared RETL bus. (b) Multiple parallel (but narrower) RETL buses. Fig. 6. With spatial partitioning into multiple parallel (but narrower) RETL buses. Multiple clusters can simultaneously transmit. (a) Cluster 0 transmits over all 32 lanes. (b) Cluster 0 transmits over two 8-lane buses b0 and b2 (shown in red), cluster 1 transmits over one 8-lane bus b1 (shown in green), and cluster 15 transmits over one 8-lane bus b3 (shown in blue). • Rather than using a token passing mechanism and having the clusters check if they have acquired the token, we use a pseudo-random number generator to poll the clusters. In particular, all clusters will implement the same pseudorandom number generator logic, for example using a linear feedback shift-register [15], which can be implemented with negligible cost. A pseudo-random number generator will generate a random sequence of numbers, changing from one random number to another random number each clock cycle. To ensure that all clusters will see exactly the same random number sequence, all the pseudo-random number generators can be initialized to the same seed. • With the availability of a pseudo-random number generator at each cluster, each cluster will see the same random number R in each cycle. If a cluster is less than N/2 segments away from the last transmitting cluster, for example cluster i in Fig. 5, then it will poll the random number R in cycle tcheck = tdetect + 1. On the other hand, if a cluster is greater or equal to N/2 segments away, for example cluster k in Fig. 5, then it will poll the random number R in cycle tcheck = tdetect . Each cluster can detect how far it is away from the last transmitter by doing a table lookup since it knows the source ID of the last transmitter. • In Fig. 5, both cluster j and cluster k will poll R in cycle t + 5 to see if (R mod N ) is equal to its cluster ID. In this example, cluster k matches (R mod N ), with N = 16. Therefore, it starts transmission in the next cycle t + 6 to cluster m, then to cluster n in cycle t + 7, and so on. Like the token-based arbitration scheme, it is possible that the cluster that matches the random number R at cycle tcheck has an empty queue. In this case, all clusters will monitor the bus for two cycles to check if the bus remains idle. If the bus has remained idle, then again all clusters will check their ID against the random number of R. If the matching cluster is still empty, then all clusters will repeat monitoring for two more cycles until one of the clusters that matches R is non-empty. C. Spatial Partitioning In this section, we partition the bus into multiple narrower buses that operate in parallel, and we extend both arbitration schemes to work in parallel buses. By having multiple parallel buses, multiple clusters can transmit concurrently. Consider the example depicted in Fig. 6. In our evaluation, we assume a 32-lane shared RETL bus, where each lane provides 20 Gb/s of data rate, for an aggregated throughput of 640 Gb/s. At the core clock frequency of 1.25 GHz, the 32-lane shared bus can transmit 64 bytes per cycle. This conﬁguration is depicted in Fig. 6(a), which shows cluster 0 transmitting. Fig. 6(b) depicts a spatial partitioning of the 32 lanes into four parallel (but narrower) 8-lane shared buses, b0 , b1 , b2 , and b3 . Instead of transmitting 64 bytes per core clock cycle, only 16 bytes can be transmitted over one of the four buses per core clock cycle. Though each of the four parallel buses provides less capacity, they permit up to four clusters to transmit concurrently. If the trafﬁc load is low-to-moderate, then we want an arbitration mechanism that will allow one cluster to use more than one bus (possibly all four buses) concurrently. Fig. 6(b) depicts cluster 0 transmitting over two 8-lane buses b0 and b2 (shown in red), cluster 1 transmitting over one 8-lane bus b1 (shown in green), and cluster 15 transmitting over one 8-lane bus b3 (shown in blue). If a cluster has acquired more than one bus, then it will load-balance its trafﬁc across the acquired buses. The 32 lanes can be partitioned into different number of parallel buses of different widths, for example, two 16-lane parallel buses or eight 4-lane parallel buses. Given the narrow pitch of RETLs, we can also add more parallel buses with more lanes per bus, as space permits. For our evaluations in Section IV, partitioning 32 lanes was found to be adequate. 1) Extending token-based arbitration: Using the example of four parallel buses shown in Fig. 6(b), we can extend the token-based arbitration scheme by implementing four separate control token rings, one per parallel bus. The operation and timing of each parallel bus would be the same as explained in Section III-A. If a cluster has acquired more than one bus, then it will load-balance its trafﬁc across the buses acquired. We can initialize the starting token position to a random location for the four control token rings. One problem with this approach is the following: Clusters closer to the currently active clusters have priority over clusters that farther downstream in acquiring tokens. If a cluster has acquired multiple buses, then the next requesting cluster downstream will likely acquire the same bundle of buses when the currently active cluster ﬁnishes. This synchronization could make it increasingly unlikely that multiple clusters will be transmitting simultaneously over different buses. It is conceivable that all four buses would only be used by one cluster at a time. As we shall see in Section IV, this extension does not perform better than without spatial partitioning. 2) Extending distributed randomized polling: We can also similarly extend the distributed randomized polling-based arbitration scheme by implementing four separate pseudo-random number generators at each cluster, one corresponding to each of the parallel buses (e.g., R0 , R1 , R2 , and R3 for buses b0 , b1 , b2 , and b3 , respectively). The operation and timing of each parallel bus would be the same as explained in Section III-B. If a cluster has acquired more than one bus, then it will loadbalance its trafﬁc across the acquired buses as well. Unlike the token-based arbitration scheme where clusters closer to the currently-active clusters have a higher priority, SY ST EM PARAM E TER S AND B ENCHMARK S . TABLE II Processor Caches Bus Synthetic Applications Core clock: 1.25 GHz; #Cores: 64; Core area: 1.25mm×1.25mm L1: private 4-way, 32KB/core; L2: private 8-way, 512KB/cluster. Coherence: MOESI(blocking) Frequency: 20 GHz; #Lanes: 32; Worst-case draining: under 2 core clock cycles. Control messages: 8-bytes; Data messages: 64-bytes uniform barnes (br), lu cb (lc), lu ncb (lnc), radix (rd), bodytrack (bt), cholesky (ck), facesim (fs), blackscholes (bs), swaptions (sw), water nsquared (wns), radiosity (rs), raytrace (rt) the randomized nature of the polling scheme means that all clusters have equal probability of acquiring each of the four buses, which makes it unlikely that any one cluster would acquire multiple buses when there are other clusters contending for them. As we shall see in Section IV, this spatial partitioning extension of the distributed randomized polling method leads to better results. IV. EVALUAT ION A. Experimental Setup Our experimental setup follows the baseline design described in Section II. In particular, we assume a design that comprises 64 cores that are organized into 16 clusters via 4:1 concentrators. Each core comprises a processor node, an L1 data/instruction cache, a slice of a shared L2 cache, and a slice of the cache coherency directory. The cores operate on a 1.25 GHz clock. We allocate a 32KB private 4-way L1 cache to each of the 64 cores, and we allocate a 512KB private 8-way L2 cache to each cluster that is shared by the four cores in the cluster. For cache coherence, we employ a MOESI directory protocol [12]. We assume a 10mm×10mm chip, divided into sixteen 2.5mm×2.5mm clusters, with each of the four cores in a cluster occupying a 1.25mm×1.25mm area. For the shared RETL bus, we assume a design with 32 lanes of RETLs, each lane capable of sending 20 Gb/s, which provides an aggregated throughput of 640 Gb/s. This means 64 bytes may be sent over the shared RETL bus per core clock period of 0.8 ns, which corresponds nicely to a cache line. The longest distance that a signal needs to travel on the bus is 15 RETL segments, or 37.5mm, which means the worstcase draining period is under two core clock cycles. We set the size of the control messages to be 8-bytes and the size of the data messages (cache lines) to be 64-bytes. These system parameters are summarized in Table II. Note that we intentionally limit our bus conﬁguration to 32 lanes to stress the limits of the bus capacity. As noted earlier, our design can easily scale to many more lanes given the narrow pitch of RETLs, which could deliver better performance or support heavier workloads. For example, a design with 128 lanes can achieve an aggregated throughput of 2.56 Tb/s. To evaluate the performance of our proposed design and arbitration schemes, we employ both synthetic and real application benchmarks. For synthetic trafﬁc, we use the Uniform trafﬁc model in which each cluster has equal probability of generating trafﬁc. For real-case scenarios, we use selected applications from the PARSEC and SPLASH-2 benchmark suites. These benchmarks are also summarized in Table II. To generate application traces from these benchmarks, we employ Fig. 7. Performance of the token-based and distributed randomized pollingbased arbitration schemes under the uniform trafﬁc model. Fig. 8. Performance of the token-based and distributed randomized pollingbased arbitration schemes for real application benchmarks. the Synfull framework [16]. The Synfull framework provides accurate trace models that account for caching behavior. B. Performance Evaluation We ﬁrst evaluate the token-based and distributed randomized polling-based arbitration schemes under the uniform trafﬁc model. The results are provided in Fig. 7. In this experiment, we assume all trafﬁc generated by a cluster will be destined to another cluster – i.e., all trafﬁc is inter-cluster trafﬁc that will go over the shared RETL bus, and the trafﬁc generated will be queued at a FIFO at the cluster. Under the uniform trafﬁc model, each cluster will generate a 64byte message with probability (Load × 1/N ) in each cycle, where 64-bytes/cycle correspond to the aggregated throughput of the shared RETL bus. In our setup, we have N = 16 clusters. The graph shown in Fig. 7 plots the average latency in cycles for the two arbitration schemes with respect to the total trafﬁc load, from Load=0.1 (10%) to Load=0.95 (95%). From Fig. 7, we can observe that the token-based scheme performs better than the distributed randomized polling scheme. Further, despite requiring a draining period of two core cycles between one transmitting cluster to another, the bus is still not yet saturated at Load=0.95 (95%). We next show in Fig. 8 the performance of the token-based and distributed randomized polling-based arbitration schemes for different PARSEC and SPLASH-2 benchmarks. For each benchmark, we evaluated the workload for 64 parallel threads, with one thread running on each of the 64 cores. From Fig. 7, we can observe that both arbitration schemes have comparable performance, with the randomized polling scheme performing slightly better in most cases. In all cases, the average latency is around or below 20 cycles. Finally, as discussed in Section III-C, though the proposed arbitration schemes are effective in achieving high throughput, they only allow one cluster to transmit at a time. By V. CONC LU S ION In this paper, we presented the design of a shared global communication medium using narrow-pitch repeated equalized transmission lines that overcomes a number of limitations associated with previously proposed designs based on transmission lines. Our design naturally supports multicast and broadcast operations and can scale to multiple terabits per seconds. We proposed several novel distributed arbitration schemes that can achieve very high throughput and bandwidth utilization, but are simple to implement. "
Multi-bit transient fault control for NoC links using 2D fault coding method.,"In deep nanometer scale, Network-on-Chip (NoC) links are more prone to multi-bit transient fault. Conventional ECC techniques brings heavy area, power, and timing overheads when correcting and detecting multiple transient faults. Therefore, a cost-effective ECC technique, named 2D fault coding method, is adopted to overcome the multi-bit transient fault issue of NoC links. Its key innovation is that the wires of a link are treated as its matrix appearance and light-weight Parity Check Coding (PCC) is performed on the matrix's two dimensions (horizontal matrix rows and vertical matrix columns). Horizontal PCCs and vertical PCCs work together to find the faults' position and then correct them by simply inverting them. The procedure of using the 2D fault coding method to protect a NoC link is proposed, its correction and detection capability is analyzed, and its hardware implementation is carried out. Comparative experiments show that the proposal can largely reduce the ECC hardware cost, have much higher fault detection coverage, maintain almost zero silent fault percentages, and have higher fault correction percentages normalized under the same area, demonstrating that it is cost-effective and suitable to the multi-bit transient fault control for NoC links.","Multi-bit Transient Fault Control for NoC Links Using 2D Fault Coding Method Xiaowen Chen† , ‡ , Zhonghai Lu‡ , Yuanwu Lei† , Yaohua Wang† , Shenggang Chen† †College of Computer, National University of Defense Technology, 410073, Changsha, China ‡Department of Electronic Systems, KTH - Royal Institute of Technology, 16440 Kista, Stockholm, Sweden ‡ {xiaowenc,zhonghai}@kth.se Abstract—In deep nanometer scale, Network-on-Chip (NoC) links are more prone to multi-bit transient fault. Conventional ECC techniques brings heavy area, power, and timing overheads when correcting and detecting multiple transient faults. Therefore, a cost-effective ECC technique, named 2D fault coding method, is adopted to overcome the multi-bit transient fault issue of NoC links. Its key innovation is that the wires of a link are treated as its matrix appearance and light-weight Parity Check Coding (PCC) is performed on the matrix’s two dimensions (horizontal matrix rows and vertical matrix columns). Horizontal PCCs and vertical PCCs work together to ﬁnd the faults’ position and then correct them by simply inverting them. The procedure of using the 2D fault coding method to protect a NoC link is proposed, its correction and detection capability is analyzed, and its hardware implementation is carried out. Comparative experiments show that the proposal can largely reduce the ECC hardware cost, have much higher fault detection coverage, maintain almost zero silent fault percentages, and have higher fault correction percentages normalized under the same area, demonstrating that it is cost-effective and suitable to the multi-bit transient fault control for NoC links. 1 I . IN TRODUC T ION As the chip technology goes into the deep nanometer era, i.e., its transistor feature size is reduced to be 45nm, 40nm, 28nm, and even smaller, integrated circuits characterized by high frequency and low voltage will be increasingly susceptible to transient faults and permanent faults. The occurrence of transient faults is considered to be roughly 80%[1]. Reliability of links challenges large-scale Network-on-Chip (NoC) design. In deep nanometer scale, transient fault tolerance of NoC links faces new phenomena: (I) The fault probability of a link wire becomes bigger. The fault probability (ε) of a link wire can be characterized by the classic fault model[2][3] with a Gaussian distribution as (cid:3) (cid:4) ∞ (cid:2) Vdd 2σN ε = Q = Vdd /2σN 1√ 2π e−y2 /2dy (1) where Vdd is supply voltage and σN is noise voltage. Fig. 1 depicts the trends of supply voltage and the ratio of noise voltage to supply voltage, according to the real technology data from TSMC(cid:2) foundry[4]. As the technology shrinks, the 1 The research is partially supported by the National Natural Science Foundation of China (No. 61502508), the Hunan Natural Science Foundation of China (No. 2015JJ3017), and the Doctoral Program of the Ministry of Education in China (No. 20134307120034). 978-1-4673-9030-9/16/$31.00 c(cid:2)2016 IEEE ) V ( e g a t l o V l y p p u S 3 2 1 0 2.5 6% (cid:109) 1.8 8.3% 12.5% 12.5% 1.2 1.2 17.6% 16.7% 0.15 15% (cid:111) 1 0.9 0.1 0.85 0.05 e g a t l o V e s o i N f o o i t a R 250nm 180nm 130nm 90nm 65nm 40nm 28nm TSMC Technology Fig. 1. Trends of supply voltage and the ratio of noise voltage from TSMC(cid:2) supply voltage decreases for the main purpose of reducing the chip power consumption. However, the proportion of voltage noise in supply voltage becomes bigger. Therefore, according to Equation (1), the increase of the ratio of noise voltage to supply voltage results in the increase of the fault probability (ε) of a link wire. (II) The fault probability of a link becomes bigger. Because technology shrinking leads to narrower wire and smaller distance between two adjacent wires and the width of on-chip link is not subject to the limited IO resources of a chip, on-chip link can be usually designed to be 256bit, 512-bit, and even more wider in order to improve the bandwidth performance. Equation (2) shows that, as the link width (notated as w) becomes bigger, multiple wires in a link may have transient faults concurrently, resulting in the increase of the fault probability (η ) of a link[5]. Multiple faults existing on the links have become more important[6][7]. η = 1 − (1 − ε)w (2) NoC links are more prone to multi-bit transient fault than ever before in deep nanometer scale, and it is a need to study multi-bit transient fault control for NoC links. Typically, fault tolerance can be achieved by redundancy. Redundancy is achieved by redundant components to cope with failing ones (spatial redundancy), by re-execution of a data transmission with the same component (temporal redundancy), and by adding information for fault detection and correction (information redundancy)[8]. In the paper, our scope is multi-bit transient fault control for on-chip communication links of large-scale NoCs via information redundancy. In information redundancy, ECC (Error Correcting Codes) [9][10] is a commonly used and effective protection technique.           x 104 7 6 5 4 3 2 1 250 200 150 100 50 ) w m ( n o i t p m u s n o C r e w o P ) s n ( y a e l D 32(cid:358)bit 64(cid:358)bit 128(cid:358)bit 256(cid:358)bit 512(cid:358)bit 0.75 0.7 0.65 0.6 0.55 0.5 0.45 0.4 0.35 ) 2 m (cid:80) ( a e A r SECDED DECTED QECPED OECNED SECDED DECTED QECPED OECNED SECDED DECTED QECPED OECNED Fig. 2. Hardware cost of traditional ECC techniques such as SECDED, DECTED, QECPED, and OECNED under TSMC(cid:2) 40nm technology Hamming algorithm is often used as the base to generate SECDED (Single-Error-Correction Double-Error-Detection) code, while BCH is as the base to form the codes that can detect and correct multiple faults, e.g., DECTED (2-ErrorCorrection 3-Error-Detection), QECPED (4-Error-Correction 5-Error-Detection), and OECNED (8-Error-Correction 9Error-Detection). Fig. 2 summarizes the hardware cost of SECDED, DECTED, QECPED, and OECNED under TSMC(cid:2) 40nm technology. (1) For the same link width, as the number of detected and corrected faults increases, more area, power consumption, and delays are required. (2) For the same detection and correction capability, wider link consumes more hardware cost. Therefore, scaling up conventional ECC techniques to cover multiple transient faults incur large hardware cost. For multi-bit transient fault tolerance, careful ECC design should be carried out by comprehensively considering its fault detection and correction capability and hardware cost, so as to obtain better cost-effectiveness. Therefore, we are motivated to adopt a 2D fault coding method to support multi-bit transient fault control for NoC links. The main contributions of the paper are summarized below: 1) The 2D fault coding method is adopted to organize the wires of a link as its matrix appearance, perform light-weight parity check coding on two dimensions (horizontal matrix rows and vertical matrix columns), and combine the horizontal and vertical fault coding information to detect and correct multiple transient faults. 2) The procedure of using the 2D fault coding method to protect a NoC link is proposed, and its correction and detection capability on NoC links is analyzed in detail. The proposal is implemented by hardware. 3) The cost-effectiveness of the proposal is proved by comparative experiments. Compared with the conventional ECC techniques, our method reduces the hardware cost largely, has higher fault detection coverage, maintains almost zero silent fault percentage, and has higher fault correction coverage normalized under the same area. I I . R E LAT ED WORK Information redundancy at the data link layer widely uses coding schemes such as Hamming code, BCH code and parity bits to catch faults in NoC links. Hamming code based SEC or SECDED is the most popular, since the occurrence of 1 fault has the highest probability. As the transistor feature size shrinks, the probability of multi-bit fault grows up, some researches adopt BCH code as the base to detect and correct w0 w1 w2 (cid:258) w62 w63 (cid:258) Wires of a link b0,0 b0,1 (cid:258) b0,7 b1,0 b1,1 (cid:258) b1,7 b7,0 b7,1 (cid:258) b7,7 (cid:258) (cid:258) (cid:258) (cid:258) Group 0 Group 1 Group 7 Fig. 3. An example of organizing the link wires into several groups that constitute the link’s matrix appearance two or more multi-bit faults[11]. However, BCH code has large extra hardware overhead. To reduce the hardware cost, some researchers combine a set of relatively simple SEC or SECDED codes with bit interleaving technique to correct adjacent multi-bit faults[5] [10][12]. For instance, in [5], the data is split into blocks to be interleaved, while a SEC code is applied for each block. Then an adaptive fault control method is used to select the ECC scheme dynamically. In [12], Lehtonen analyzes forward fault correction methods for nanoscale NoC. Another thought is developed by Dutta and Touba[13], who use an unequal coding scheme to protect different parts of the packet. It has similar costs as SEC codes while providing better fault detecting and correcting capability, thus the cost is relatively reduced. The ECC codes used by all these literatures are originated from Hamming code or BCH code. Due the algorithm structure itself, Hamming code with bit interleaving or BCH code can theoretically scale to detect and correct more faults, but the hardware cost will grow rapidly, so the number of detected and corrected faults by current literatures is small (< 4). We adopt a new technique (called 2D fault coding method) rather than Hamming code or BCH code to achieve fast multi-bit fault detection/correction while still maintaining high fault coverage with low cost. The 2D fault coding method organizes the wires of a link as a matrix appearance, and performs horizontal ECC along matrix rows and vertical ECC along matrix columns. Regarding ECC on two dimensions, prior work mainly concerns the protection of memory arrays. In [14], Mohr applies product codes to memory arrays and uses horizontal byte-parity codes to enable low-latency fault detection. This scheme only addresses detecting and correcting single-bit faults within a memory array. In [15][16], Argyrides proposed a matrix code to protect SRAM-based memories against Multiple Bit Upsets (MBU). The proposed method integrates hamming code and parity code together to assure the memory reliability with low area and performance overhead. His work is only at the algorithm level without concrete hardware implementation. Similar with Argyrides’s work, in [17], Kim proposed an fault coding scheme in embedded L1/L2 memories and enabled vertical fault coding across words in combination with conventional per-word horizontal fault coding, in order to achieve multibit fault protection. The vertical fault coding process is not             Link Width: 64 bits (cid:90)(cid:19) (cid:90)(cid:20) (cid:90)(cid:21) (cid:90)(cid:22) (cid:90)(cid:23) (cid:90)(cid:24) (cid:90)(cid:25) (cid:90)(cid:26) (cid:3) (cid:90)(cid:27) (cid:90)(cid:28) (cid:90)(cid:20)(cid:19) (cid:90)(cid:20)(cid:20) (cid:90)(cid:20)(cid:21) (cid:90)(cid:20)(cid:22) (cid:90)(cid:20)(cid:23) (cid:90)(cid:20)(cid:24) (cid:90)(cid:20)(cid:25) (cid:90)(cid:20)(cid:26) (cid:90)(cid:20)(cid:27) (cid:90)(cid:20)(cid:28) (cid:90)(cid:21)(cid:19) (cid:90)(cid:21)(cid:20) (cid:90)(cid:21)(cid:21) (cid:90)(cid:21)(cid:22) (cid:90)(cid:21)(cid:23) (cid:90)(cid:21)(cid:24) (cid:90)(cid:21)(cid:25) (cid:90)(cid:21)(cid:26) (cid:90)(cid:21)(cid:27) (cid:90)(cid:21)(cid:28) (cid:90)(cid:22)(cid:19) (cid:90)(cid:22)(cid:20) (cid:90)(cid:22)(cid:21) (cid:90)(cid:22)(cid:22) (cid:90)(cid:22)(cid:23) (cid:90)(cid:22)(cid:24) (cid:90)(cid:22)(cid:25) (cid:90)(cid:22)(cid:26) (cid:90)(cid:22)(cid:27) (cid:90)(cid:22)(cid:28) (cid:90)(cid:23)(cid:19) (cid:90)(cid:23)(cid:20) (cid:90)(cid:23)(cid:21) (cid:90)(cid:23)(cid:22) (cid:90)(cid:23)(cid:23) (cid:90)(cid:23)(cid:24) (cid:90)(cid:23)(cid:25) (cid:90)(cid:23)(cid:26) (cid:90)(cid:23)(cid:27) (cid:90)(cid:23)(cid:28) (cid:90)(cid:24)(cid:19) (cid:90)(cid:24)(cid:20) (cid:90)(cid:24)(cid:21) (cid:90)(cid:24)(cid:22) (cid:90)(cid:24)(cid:23) (cid:90)(cid:24)(cid:24) (cid:90)(cid:24)(cid:25) (cid:90)(cid:24)(cid:26) (cid:90)(cid:24)(cid:27) (cid:90)(cid:24)(cid:28) (cid:90)(cid:25)(cid:19) (cid:90)(cid:25)(cid:20) (cid:90)(cid:25)(cid:21) (cid:90)(cid:25)(cid:22) 8 8 (cid:69)(cid:19)(cid:17)(cid:19) (cid:69)(cid:19)(cid:17)(cid:20) (cid:69)(cid:19)(cid:17)(cid:21) (cid:69)(cid:19)(cid:17)(cid:22) (cid:69)(cid:19)(cid:17)(cid:23) (cid:69)(cid:19)(cid:17)(cid:24) (cid:69)(cid:19)(cid:17)(cid:25) (cid:69)(cid:19)(cid:17)(cid:26) (cid:69)(cid:20)(cid:17)(cid:19) (cid:69)(cid:20)(cid:17)(cid:20) (cid:69)(cid:20)(cid:17)(cid:21) (cid:69)(cid:20)(cid:17)(cid:22) (cid:69)(cid:20)(cid:17)(cid:23) (cid:69)(cid:20)(cid:17)(cid:24) (cid:69)(cid:20)(cid:17)(cid:25) (cid:69)(cid:20)(cid:17)(cid:26) (cid:69)(cid:21)(cid:15)(cid:19) (cid:69)(cid:21)(cid:15)(cid:20) (cid:69)(cid:21)(cid:15)(cid:21) (cid:69)(cid:21)(cid:15)(cid:22) (cid:69)(cid:21)(cid:15)(cid:23) (cid:69)(cid:21)(cid:15)(cid:24) (cid:69)(cid:21)(cid:15)(cid:25) (cid:69)(cid:21)(cid:15)(cid:26) (cid:69)(cid:22)(cid:15)(cid:19) (cid:69)(cid:22)(cid:15)(cid:20) (cid:69)(cid:22)(cid:15)(cid:21) (cid:69)(cid:22)(cid:15)(cid:22) (cid:69)(cid:22)(cid:15)(cid:23) (cid:69)(cid:22)(cid:15)(cid:24) (cid:69)(cid:22)(cid:15)(cid:25) (cid:69)(cid:22)(cid:15)(cid:26) (cid:69)(cid:23)(cid:15)(cid:19) (cid:69)(cid:23)(cid:15)(cid:20) (cid:69)(cid:23)(cid:15)(cid:21) (cid:69)(cid:23)(cid:15)(cid:22) (cid:69)(cid:23)(cid:15)(cid:23) (cid:69)(cid:23)(cid:15)(cid:24) (cid:69)(cid:23)(cid:15)(cid:25) (cid:69)(cid:23)(cid:15)(cid:26) (cid:69)(cid:24)(cid:15)(cid:19) (cid:69)(cid:24)(cid:15)(cid:20) (cid:69)(cid:24)(cid:15)(cid:21) (cid:69)(cid:24)(cid:15)(cid:22) (cid:69)(cid:24)(cid:15)(cid:23) (cid:69)(cid:24)(cid:15)(cid:24) (cid:69)(cid:24)(cid:15)(cid:25) (cid:69)(cid:24)(cid:15)(cid:26) (cid:69)(cid:25)(cid:15)(cid:19) (cid:69)(cid:25)(cid:15)(cid:20) (cid:69)(cid:25)(cid:15)(cid:21) (cid:69)(cid:25)(cid:15)(cid:22) (cid:69)(cid:25)(cid:15)(cid:23) (cid:69)(cid:25)(cid:15)(cid:24) (cid:69)(cid:25)(cid:15)(cid:25) (cid:69)(cid:25)(cid:15)(cid:26) (cid:69)(cid:26)(cid:15)(cid:19) (cid:69)(cid:26)(cid:15)(cid:20) (cid:69)(cid:26)(cid:15)(cid:21) (cid:69)(cid:26)(cid:15)(cid:22) (cid:69)(cid:26)(cid:15)(cid:23) (cid:69)(cid:26)(cid:15)(cid:24) (cid:69)(cid:26)(cid:15)(cid:25) (cid:69)(cid:26)(cid:15)(cid:26) h0,0 h1,0 h0,1 h1,1 h2,0 h2,1 h3,0 h3,1 h4,0 h4,1 h5,0 h6,0 h5,1 h6,1 h7,0 h7,1 Link Width: 64 bits (cid:90)(cid:19) (cid:90)(cid:20) (cid:90)(cid:21) (cid:90)(cid:22) (cid:90)(cid:23) (cid:90)(cid:24) (cid:90)(cid:25) (cid:90)(cid:26) (cid:3) (cid:90)(cid:27) (cid:90)(cid:28) (cid:90)(cid:20)(cid:19) (cid:90)(cid:20)(cid:20) (cid:90)(cid:20)(cid:21) (cid:90)(cid:20)(cid:22) (cid:90)(cid:20)(cid:23) (cid:90)(cid:20)(cid:24) (cid:90)(cid:20)(cid:25) (cid:90)(cid:20)(cid:26) (cid:90)(cid:20)(cid:27) (cid:90)(cid:20)(cid:28) (cid:90)(cid:21)(cid:19) (cid:90)(cid:21)(cid:20) (cid:90)(cid:21)(cid:21) (cid:90)(cid:21)(cid:22) (cid:90)(cid:21)(cid:23) (cid:90)(cid:21)(cid:24) (cid:90)(cid:21)(cid:25) (cid:90)(cid:21)(cid:26) (cid:90)(cid:21)(cid:27) (cid:90)(cid:21)(cid:28) (cid:90)(cid:22)(cid:19) (cid:90)(cid:22)(cid:20) (cid:90)(cid:22)(cid:21) (cid:90)(cid:22)(cid:22) (cid:90)(cid:22)(cid:23) (cid:90)(cid:22)(cid:24) (cid:90)(cid:22)(cid:25) (cid:90)(cid:22)(cid:26) (cid:90)(cid:22)(cid:27) (cid:90)(cid:22)(cid:28) (cid:90)(cid:23)(cid:19) (cid:90)(cid:23)(cid:20) (cid:90)(cid:23)(cid:21) (cid:90)(cid:23)(cid:22) (cid:90)(cid:23)(cid:23) (cid:90)(cid:23)(cid:24) (cid:90)(cid:23)(cid:25) (cid:90)(cid:23)(cid:26) (cid:90)(cid:23)(cid:27) (cid:90)(cid:23)(cid:28) (cid:90)(cid:24)(cid:19) (cid:90)(cid:24)(cid:20) (cid:90)(cid:24)(cid:21) (cid:90)(cid:24)(cid:22) (cid:90)(cid:24)(cid:23) (cid:90)(cid:24)(cid:24) (cid:90)(cid:24)(cid:25) (cid:90)(cid:24)(cid:26) (cid:90)(cid:24)(cid:27) (cid:90)(cid:24)(cid:28) (cid:90)(cid:25)(cid:19) (cid:90)(cid:25)(cid:20) (cid:90)(cid:25)(cid:21) (cid:90)(cid:25)(cid:22) 8 ECC bits: 29 v0,0 v0,1 v0,2 v0,3 v0,4 v0,5 v0,6 v0,7 v1,0 v1,1 v1,2 v1,3 v1,4 v1,5 v1,6 v1,7 ECC bits: 32 ECC bits: 57 (a) (93,64) QECPED  (b) 2D fault coding:  Horizontal: PCC2 + Vertical: PCC2 (c) (121,64) OECNED  8 (cid:69)(cid:19)(cid:17)(cid:19) (cid:69)(cid:19)(cid:17)(cid:20) (cid:69)(cid:19)(cid:17)(cid:21) (cid:69)(cid:19)(cid:17)(cid:22) (cid:69)(cid:19)(cid:17)(cid:23) (cid:69)(cid:19)(cid:17)(cid:24) (cid:69)(cid:19)(cid:17)(cid:25) (cid:69)(cid:19)(cid:17)(cid:26) (cid:69)(cid:20)(cid:17)(cid:19) (cid:69)(cid:20)(cid:17)(cid:20) (cid:69)(cid:20)(cid:17)(cid:21) (cid:69)(cid:20)(cid:17)(cid:22) (cid:69)(cid:20)(cid:17)(cid:23) (cid:69)(cid:20)(cid:17)(cid:24) (cid:69)(cid:20)(cid:17)(cid:25) (cid:69)(cid:20)(cid:17)(cid:26) (cid:69)(cid:21)(cid:15)(cid:19) (cid:69)(cid:21)(cid:15)(cid:20) (cid:69)(cid:21)(cid:15)(cid:21) (cid:69)(cid:21)(cid:15)(cid:22) (cid:69)(cid:21)(cid:15)(cid:23) (cid:69)(cid:21)(cid:15)(cid:24) (cid:69)(cid:21)(cid:15)(cid:25) (cid:69)(cid:21)(cid:15)(cid:26) (cid:69)(cid:22)(cid:15)(cid:19) (cid:69)(cid:22)(cid:15)(cid:20) (cid:69)(cid:22)(cid:15)(cid:21) (cid:69)(cid:22)(cid:15)(cid:22) (cid:69)(cid:22)(cid:15)(cid:23) (cid:69)(cid:22)(cid:15)(cid:24) (cid:69)(cid:22)(cid:15)(cid:25) (cid:69)(cid:22)(cid:15)(cid:26) (cid:69)(cid:23)(cid:15)(cid:19) (cid:69)(cid:23)(cid:15)(cid:20) (cid:69)(cid:23)(cid:15)(cid:21) (cid:69)(cid:23)(cid:15)(cid:22) (cid:69)(cid:23)(cid:15)(cid:23) (cid:69)(cid:23)(cid:15)(cid:24) (cid:69)(cid:23)(cid:15)(cid:25) (cid:69)(cid:23)(cid:15)(cid:26) (cid:69)(cid:24)(cid:15)(cid:19) (cid:69)(cid:24)(cid:15)(cid:20) (cid:69)(cid:24)(cid:15)(cid:21) (cid:69)(cid:24)(cid:15)(cid:22) (cid:69)(cid:24)(cid:15)(cid:23) (cid:69)(cid:24)(cid:15)(cid:24) (cid:69)(cid:24)(cid:15)(cid:25) (cid:69)(cid:24)(cid:15)(cid:26) (cid:69)(cid:25)(cid:15)(cid:19) (cid:69)(cid:25)(cid:15)(cid:20) (cid:69)(cid:25)(cid:15)(cid:21) (cid:69)(cid:25)(cid:15)(cid:22) (cid:69)(cid:25)(cid:15)(cid:23) (cid:69)(cid:25)(cid:15)(cid:24) (cid:69)(cid:25)(cid:15)(cid:25) (cid:69)(cid:25)(cid:15)(cid:26) (cid:69)(cid:26)(cid:15)(cid:19) (cid:69)(cid:26)(cid:15)(cid:20) (cid:69)(cid:26)(cid:15)(cid:21) (cid:69)(cid:26)(cid:15)(cid:22) (cid:69)(cid:26)(cid:15)(cid:23) (cid:69)(cid:26)(cid:15)(cid:24) (cid:69)(cid:26)(cid:15)(cid:25) (cid:69)(cid:26)(cid:15)(cid:26) v0,0 v0,1 v0,2 v0,3 v0,4 v0,5 v0,6 v0,7 v1,0 v1,1 v1,2 v1,3 v1,4 v1,5 v1,6 v1,7 v2,0 v2,1 v2,2 v2,3 v2,4 v2,5 v2,6 v2,7 v3,0 v3,1 v3,2 v3,3 v3,4 v3,5 v3,6 v3,7 h0,0 h1,0 h0,1 h1,1 h2,0 h2,1 h3,0 h3,1 h4,0 h4,1 h5,0 h6,0 h5,1 h6,1 h7,0 h7,1 ECC  bits: 48 (d) 2D fault coding:  Horizontal: PCC2 + Vertical: PCC4 Fig. 4. Comparison of fault coverage and extra ECC bits overhead between QECPED, OECNED and the 2D fault coding method fast due to the memory’s structural characteristic that multiple words can only be loaded out sequentially. And, their jobs focused on radiation-induced faults of memory cells and did speciﬁc optimization according to memory cell structure. I I I . TWO D IM EN S IONA L FAULT COD ING M E THOD FOR MU LT I -B I T TRAN S I EN T L INK FAU LT CON TRO L A. Idea The core idea of 2D fault coding method is to treat the wires of a link as a matrix appearance, perform light-weight parity coding on each matrix row and column, and combine both of the horizontal and vertical fault coding information to detect and correct one or more transient faults. Since light-weight parity codes are adopted, hardware cost can be reduced. Fig. 3 shows an example of organizing the wires of a link as its matrix appearance logically. Assuming that a link contains 64 wires labeled as w0 , w1 , ..., w63 . The wires are grouped into 8 groups. Group i (i = 0,...,7) contains the wires labeled as wi×8+j (j = 0,...,7), which are renamed as bi,j . All the wires in group i become the elements on the i-th row of its matrix, and all of the j -th wires in all groups become the elements on the j -th column of the matrix shown in Fig. 4. To illustrate the 2D fault coding idea, Fig. 4 compares the fault coverage and extra ECC bits overhead between two conventional ECC techniques (QECPED and OECNED) and the 2D fault coding method when they are used to protect a 64-bit link. In the ﬁgure, each box represents a bit of the link. The white ones are for data bits, while the grey ones are for ECC bits. The number of green boxes represents the maximum number of the fault that can be detected, while the number of green boxes enclosed by the red line represents the maximum number of the faults that can be corrected. Fig. 4 (a) shows the fault coverage and the number of extra ECC bits of QECPED that can detect at most ﬁve faults and correct at most four faults. To protect a 64-bit link, 29 ECC bits are needed so that the ECC overhead is 29/64 = 45.3%. As shown in Fig. 4 (b), the 2D fault coding method adopts horizontal PCC2 for each row and vertical PCC2 for each column. PCCn represents the n-way interleaved Parity Check Coding technique, which contains n parity bits. The i-th (i = 0, ..., n − 1) parity bit = data bit[i] ⊕ data bit[i + n] ⊕ data bit[i + 2 × n] ⊕ · · · . ⊕ represents an exclusive-OR operation. For instance, in Fig. 4 (b), the parity bit (h0,1 ) is equal to the result of performing exclusive-OR operation on the odd bits (b0,1 ,b0,3 ,b0,5 ,and b0,7 ) at row 0. PCCn allows to detect all continuous n-bit faults, which can increase the fault coverage along rows and columns. The combination of PCCn in the horizontal and vertical directions identiﬁes the location of erroneous bits. Once the erroneous bits are identiﬁed, they can be corrected by simply inverting them, so the hardware cost of their correction is trivial. For instance, in Fig. 4 (b), horizontal PCC2 and vertical PCC2 are adopted so that at most 28 faults (the green boxes) can be detected and at most four faults (the green boxes enclosed by the red line) can be corrected. From Fig. 4 (a) and (b), we can see that the 2D fault coding method with horizontal PCC2 and vertical PCC2 has higher fault coverage than QECPED. Although it requires 32 ECC bits that is a little more than 29 ECC bits of QECPED, PCC2 is very simple so that its total ECC hardware cost including ECC bits and ECC encoding and decoding logics are much smaller than QECPED. Fig. 4 (c) and (d) show the fault coverage and extra ECC bits overhead of OECNED and the 2D fault coding method with horizontal PCC2 and vertical PCC4, both of which can correct at most eight faults. For detecting and correcting more faults, the ECC overhead becomes larger. For instance, the ECC bits overhead of OECNED reaches 57/64 = 89.1%. The 2D fault coding method has less ECC bits than OECNED. Because BCH algorithm is more complex than PCCn, the 2D fault coding method can save more ECC hardware overhead than OECNED. Meanwhile, the 2D fault coding method has large fault coverage so as to detect more faults. B. Procedure of using the idea to protect a NoC link The generical procedure of how to use the 2D fault coding method to protect a NoC link is proposed, which can guide (a) correctable Situation A (b) correctable (c) detectable Situation B 8 8 8 8 8 8 (e) detectable Situation C 8 8 8 8 8 8 (d) correctable (f) undetectable Sv0,0 Sv0,1 Sv0,2 Sv0,3 Sv0,4 Sv0,5 Sv0,6 Sv0,7 Sv1,0 Sv1,1 Sv1,2 Sv1,3 Sv1,4 Sv1,5 Sv1,6 Sv1,7 Sv0,0 Sv0,1 Sv0,2 Sv0,3 Sv0,4 Sv0,5 Sv0,6 Sv0,7 Sv1,0 Sv1,1 Sv1,2 Sv1,3 Sv1,4 Sv1,5 Sv1,6 Sv1,7 Sv0,0 Sv0,1 Sv0,2 Sv0,3 Sv0,4 Sv0,5 Sv0,6 Sv0,7 Sv1,0 Sv1,1 Sv1,2 Sv1,3 Sv1,4 Sv1,5 Sv1,6 Sv1,7 Sv0,0 Sv0,1 Sv0,2 Sv0,3 Sv0,4 Sv0,5 Sv0,6 Sv0,7 Sv1,0 Sv1,1 Sv1,2 Sv1,3 Sv1,4 Sv1,5 Sv1,6 Sv1,7 Sv0,0 Sv0,1 Sv0,2 Sv0,3 Sv0,4 Sv0,5 Sv0,6 Sv0,7 Sv1,0 Sv1,1 Sv1,2 Sv1,3 Sv1,4 Sv1,5 Sv1,6 Sv1,7 Sv0,0 Sv0,1 Sv0,2 Sv0,3 Sv0,4 Sv0,5 Sv0,6 Sv0,7 Sv1,0 Sv1,1 Sv1,2 Sv1,3 Sv1,4 Sv1,5 Sv1,6 Sv1,7 Sh7,0 Sh6,0 Sh5,0 Sh4,0 Sh3,0 Sh2,0 Sh1,0 Sh0,0 Sh7,1 Sh6,1 Sh5,1 Sh4,1 Sh3,1 Sh2,1 Sh1,1 Sh0,1 Sh7,0 Sh6,0 Sh5,0 Sh4,0 Sh3,0 Sh2,0 Sh1,0 Sh0,0 Sh7,1 Sh6,1 Sh5,1 Sh4,1 Sh3,1 Sh2,1 Sh1,1 Sh0,1 Sh7,0 Sh6,0 Sh5,0 Sh4,0 Sh3,0 Sh2,0 Sh1,0 Sh0,0 Sh7,1 Sh6,1 Sh5,1 Sh4,1 Sh3,1 Sh2,1 Sh1,1 Sh0,1 Sh7,0 Sh6,0 Sh5,0 Sh4,0 Sh3,0 Sh2,0 Sh1,0 Sh0,0 Sh7,1 Sh6,1 Sh5,1 Sh4,1 Sh3,1 Sh2,1 Sh1,1 Sh0,1 Sh7,0 Sh6,0 Sh5,0 Sh4,0 Sh3,0 Sh2,0 Sh1,0 Sh0,0 Sh7,1 Sh6,1 Sh5,1 Sh4,1 Sh3,1 Sh2,1 Sh1,1 Sh0,1 Sh7,0 Sh6,0 Sh5,0 Sh4,0 Sh3,0 Sh2,0 Sh1,0 Sh0,0 Sh7,1 Sh6,1 Sh5,1 Sh4,1 Sh3,1 Sh2,1 Sh1,1 Sh0,1 syndrome bits syndrome bits syndrome bits syndrome bits syndrome bits syndrome bits (cid:69)(cid:19)(cid:17)(cid:19) (cid:69)(cid:19)(cid:17)(cid:20) (cid:69)(cid:19)(cid:17)(cid:21) (cid:69)(cid:19)(cid:17)(cid:22) (cid:69)(cid:19)(cid:17)(cid:23) (cid:69)(cid:19)(cid:17)(cid:24) (cid:69)(cid:19)(cid:17)(cid:25) (cid:69)(cid:19)(cid:17)(cid:26) (cid:69)(cid:19)(cid:17)(cid:19) (cid:69)(cid:19)(cid:17)(cid:20) (cid:69)(cid:19)(cid:17)(cid:21) (cid:69)(cid:19)(cid:17)(cid:22) (cid:69)(cid:19)(cid:17)(cid:23) (cid:69)(cid:19)(cid:17)(cid:24) (cid:69)(cid:19)(cid:17)(cid:25) (cid:69)(cid:19)(cid:17)(cid:26) (cid:69)(cid:20)(cid:17)(cid:19) (cid:69)(cid:20)(cid:17)(cid:20) (cid:69)(cid:20)(cid:17)(cid:21) (cid:69)(cid:20)(cid:17)(cid:22) (cid:69)(cid:20)(cid:17)(cid:23) (cid:69)(cid:20)(cid:17)(cid:24) (cid:69)(cid:20)(cid:17)(cid:25) (cid:69)(cid:20)(cid:17)(cid:26) (cid:69)(cid:20)(cid:17)(cid:19) (cid:69)(cid:20)(cid:17)(cid:20) (cid:69)(cid:20)(cid:17)(cid:21) (cid:69)(cid:20)(cid:17)(cid:22) (cid:69)(cid:20)(cid:17)(cid:24) (cid:69)(cid:20)(cid:17)(cid:25) (cid:69)(cid:20)(cid:17)(cid:26) (cid:69)(cid:21)(cid:15)(cid:19) (cid:69)(cid:21)(cid:15)(cid:20) (cid:69)(cid:21)(cid:15)(cid:21) (cid:69)(cid:21)(cid:15)(cid:22) (cid:69)(cid:21)(cid:15)(cid:23) (cid:69)(cid:21)(cid:15)(cid:24) (cid:69)(cid:21)(cid:15)(cid:25) (cid:69)(cid:21)(cid:15)(cid:26) (cid:69)(cid:21)(cid:15)(cid:19) (cid:69)(cid:21)(cid:15)(cid:20) (cid:69)(cid:21)(cid:15)(cid:21) (cid:69)(cid:21)(cid:15)(cid:22) (cid:69)(cid:21)(cid:15)(cid:23) (cid:69)(cid:21)(cid:15)(cid:24) (cid:69)(cid:21)(cid:15)(cid:25) (cid:69)(cid:21)(cid:15)(cid:26) (cid:69)(cid:22)(cid:15)(cid:19) (cid:69)(cid:22)(cid:15)(cid:20) (cid:69)(cid:22)(cid:15)(cid:21) (cid:69)(cid:22)(cid:15)(cid:22) (cid:69)(cid:22)(cid:15)(cid:23) (cid:69)(cid:22)(cid:15)(cid:24) (cid:69)(cid:22)(cid:15)(cid:25) (cid:69)(cid:22)(cid:15)(cid:26) (cid:69)(cid:22)(cid:15)(cid:19) (cid:69)(cid:22)(cid:15)(cid:21) (cid:69)(cid:22)(cid:15)(cid:22) (cid:69)(cid:22)(cid:15)(cid:23) (cid:69)(cid:22)(cid:15)(cid:24) (cid:69)(cid:22)(cid:15)(cid:25) (cid:69)(cid:22)(cid:15)(cid:26) (cid:69)(cid:23)(cid:15)(cid:19) (cid:69)(cid:23)(cid:15)(cid:20) (cid:69)(cid:23)(cid:15)(cid:21) (cid:69)(cid:23)(cid:15)(cid:22) (cid:69)(cid:23)(cid:15)(cid:23) (cid:69)(cid:23)(cid:15)(cid:24) (cid:69)(cid:23)(cid:15)(cid:25) (cid:69)(cid:23)(cid:15)(cid:26) (cid:69)(cid:23)(cid:15)(cid:19) (cid:69)(cid:23)(cid:15)(cid:20) (cid:69)(cid:23)(cid:15)(cid:21) (cid:69)(cid:23)(cid:15)(cid:22) (cid:69)(cid:23)(cid:15)(cid:24) (cid:69)(cid:23)(cid:15)(cid:25) (cid:69)(cid:23)(cid:15)(cid:26) (cid:69)(cid:24)(cid:15)(cid:19) (cid:69)(cid:24)(cid:15)(cid:20) (cid:69)(cid:24)(cid:15)(cid:21) (cid:69)(cid:24)(cid:15)(cid:22) (cid:69)(cid:24)(cid:15)(cid:23) (cid:69)(cid:24)(cid:15)(cid:24) (cid:69)(cid:24)(cid:15)(cid:25) (cid:69)(cid:24)(cid:15)(cid:26) (cid:69)(cid:24)(cid:15)(cid:19) (cid:69)(cid:24)(cid:15)(cid:20) (cid:69)(cid:24)(cid:15)(cid:21) (cid:69)(cid:24)(cid:15)(cid:22) (cid:69)(cid:24)(cid:15)(cid:23) (cid:69)(cid:24)(cid:15)(cid:24) (cid:69)(cid:24)(cid:15)(cid:25) (cid:69)(cid:24)(cid:15)(cid:26) (cid:69)(cid:25)(cid:15)(cid:19) (cid:69)(cid:25)(cid:15)(cid:20) (cid:69)(cid:25)(cid:15)(cid:21) (cid:69)(cid:25)(cid:15)(cid:22) (cid:69)(cid:25)(cid:15)(cid:23) (cid:69)(cid:25)(cid:15)(cid:24) (cid:69)(cid:25)(cid:15)(cid:25) (cid:69)(cid:25)(cid:15)(cid:26) (cid:69)(cid:25)(cid:15)(cid:19) (cid:69)(cid:25)(cid:15)(cid:20) (cid:69)(cid:25)(cid:15)(cid:21) (cid:69)(cid:25)(cid:15)(cid:23) (cid:69)(cid:25)(cid:15)(cid:24) (cid:69)(cid:25)(cid:15)(cid:25) (cid:69)(cid:25)(cid:15)(cid:26) (cid:69)(cid:26)(cid:15)(cid:19) (cid:69)(cid:26)(cid:15)(cid:20) (cid:69)(cid:26)(cid:15)(cid:21) (cid:69)(cid:26)(cid:15)(cid:22) (cid:69)(cid:26)(cid:15)(cid:23) (cid:69)(cid:26)(cid:15)(cid:24) (cid:69)(cid:26)(cid:15)(cid:25) (cid:69)(cid:26)(cid:15)(cid:26) (cid:69)(cid:26)(cid:15)(cid:19) (cid:69)(cid:26)(cid:15)(cid:20) (cid:69)(cid:26)(cid:15)(cid:21) (cid:69)(cid:26)(cid:15)(cid:22) (cid:69)(cid:26)(cid:15)(cid:23) (cid:69)(cid:26)(cid:15)(cid:24) (cid:69)(cid:26)(cid:15)(cid:25) (cid:69)(cid:26)(cid:15)(cid:26) E2 E1 E4 E3 E2 E1 E4 E3 E1 E2 E3 E4 E2 E4 E1 E3 E1 E2 E4 E3 E1 E2 E3 E4 (cid:69)(cid:19)(cid:17)(cid:19) (cid:69)(cid:19)(cid:17)(cid:20) (cid:69)(cid:19)(cid:17)(cid:21) (cid:69)(cid:19)(cid:17)(cid:22) (cid:69)(cid:19)(cid:17)(cid:23) (cid:69)(cid:19)(cid:17)(cid:24) (cid:69)(cid:19)(cid:17)(cid:25) (cid:69)(cid:19)(cid:17)(cid:26) (cid:69)(cid:20)(cid:17)(cid:19) (cid:69)(cid:20)(cid:17)(cid:20) (cid:69)(cid:20)(cid:17)(cid:21) (cid:69)(cid:20)(cid:17)(cid:22) (cid:69)(cid:20)(cid:17)(cid:23) (cid:69)(cid:20)(cid:17)(cid:24) (cid:69)(cid:20)(cid:17)(cid:25) (cid:69)(cid:20)(cid:17)(cid:26) (cid:69)(cid:21)(cid:15)(cid:19) (cid:69)(cid:21)(cid:15)(cid:20) (cid:69)(cid:21)(cid:15)(cid:21) (cid:69)(cid:21)(cid:15)(cid:22) (cid:69)(cid:21)(cid:15)(cid:24) (cid:69)(cid:21)(cid:15)(cid:25) (cid:69)(cid:21)(cid:15)(cid:26) (cid:69)(cid:22)(cid:15)(cid:19) (cid:69)(cid:22)(cid:15)(cid:21) (cid:69)(cid:22)(cid:15)(cid:22) (cid:69)(cid:22)(cid:15)(cid:23) (cid:69)(cid:22)(cid:15)(cid:24) (cid:69)(cid:22)(cid:15)(cid:25) (cid:69)(cid:22)(cid:15)(cid:26) (cid:69)(cid:23)(cid:15)(cid:19) (cid:69)(cid:23)(cid:15)(cid:20) (cid:69)(cid:23)(cid:15)(cid:21) (cid:69)(cid:23)(cid:15)(cid:22) (cid:69)(cid:23)(cid:15)(cid:24) (cid:69)(cid:23)(cid:15)(cid:25) (cid:69)(cid:23)(cid:15)(cid:26) (cid:69)(cid:24)(cid:15)(cid:19) (cid:69)(cid:24)(cid:15)(cid:20) (cid:69)(cid:24)(cid:15)(cid:21) (cid:69)(cid:24)(cid:15)(cid:22) (cid:69)(cid:24)(cid:15)(cid:23) (cid:69)(cid:24)(cid:15)(cid:24) (cid:69)(cid:24)(cid:15)(cid:25) (cid:69)(cid:24)(cid:15)(cid:26) (cid:69)(cid:25)(cid:15)(cid:19) (cid:69)(cid:25)(cid:15)(cid:20) (cid:69)(cid:25)(cid:15)(cid:21) (cid:69)(cid:25)(cid:15)(cid:23) (cid:69)(cid:25)(cid:15)(cid:24) (cid:69)(cid:25)(cid:15)(cid:25) (cid:69)(cid:25)(cid:15)(cid:26) (cid:69)(cid:26)(cid:15)(cid:19) (cid:69)(cid:26)(cid:15)(cid:20) (cid:69)(cid:26)(cid:15)(cid:21) (cid:69)(cid:26)(cid:15)(cid:22) (cid:69)(cid:26)(cid:15)(cid:23) (cid:69)(cid:26)(cid:15)(cid:24) (cid:69)(cid:26)(cid:15)(cid:25) (cid:69)(cid:26)(cid:15)(cid:26) (cid:69)(cid:19)(cid:17)(cid:19) (cid:69)(cid:19)(cid:17)(cid:20) (cid:69)(cid:19)(cid:17)(cid:21) (cid:69)(cid:19)(cid:17)(cid:22) (cid:69)(cid:19)(cid:17)(cid:23) (cid:69)(cid:19)(cid:17)(cid:24) (cid:69)(cid:19)(cid:17)(cid:25) (cid:69)(cid:19)(cid:17)(cid:26) (cid:69)(cid:20)(cid:17)(cid:19) (cid:69)(cid:20)(cid:17)(cid:21) (cid:69)(cid:20)(cid:17)(cid:22) (cid:69)(cid:20)(cid:17)(cid:23) (cid:69)(cid:20)(cid:17)(cid:25) (cid:69)(cid:20)(cid:17)(cid:26) (cid:69)(cid:21)(cid:15)(cid:19) (cid:69)(cid:21)(cid:15)(cid:20) (cid:69)(cid:21)(cid:15)(cid:21) (cid:69)(cid:21)(cid:15)(cid:22) (cid:69)(cid:21)(cid:15)(cid:23) (cid:69)(cid:21)(cid:15)(cid:24) (cid:69)(cid:21)(cid:15)(cid:25) (cid:69)(cid:21)(cid:15)(cid:26) (cid:69)(cid:22)(cid:15)(cid:19) (cid:69)(cid:22)(cid:15)(cid:20) (cid:69)(cid:22)(cid:15)(cid:21) (cid:69)(cid:22)(cid:15)(cid:22) (cid:69)(cid:22)(cid:15)(cid:23) (cid:69)(cid:22)(cid:15)(cid:24) (cid:69)(cid:22)(cid:15)(cid:25) (cid:69)(cid:22)(cid:15)(cid:26) (cid:69)(cid:23)(cid:15)(cid:19) (cid:69)(cid:23)(cid:15)(cid:20) (cid:69)(cid:23)(cid:15)(cid:21) (cid:69)(cid:23)(cid:15)(cid:22) (cid:69)(cid:23)(cid:15)(cid:23) (cid:69)(cid:23)(cid:15)(cid:24) (cid:69)(cid:23)(cid:15)(cid:25) (cid:69)(cid:23)(cid:15)(cid:26) (cid:69)(cid:24)(cid:15)(cid:19) (cid:69)(cid:24)(cid:15)(cid:21) (cid:69)(cid:24)(cid:15)(cid:22) (cid:69)(cid:24)(cid:15)(cid:23) (cid:69)(cid:24)(cid:15)(cid:25) (cid:69)(cid:24)(cid:15)(cid:26) (cid:69)(cid:25)(cid:15)(cid:19) (cid:69)(cid:25)(cid:15)(cid:20) (cid:69)(cid:25)(cid:15)(cid:21) (cid:69)(cid:25)(cid:15)(cid:22) (cid:69)(cid:25)(cid:15)(cid:23) (cid:69)(cid:25)(cid:15)(cid:24) (cid:69)(cid:25)(cid:15)(cid:25) (cid:69)(cid:25)(cid:15)(cid:26) (cid:69)(cid:26)(cid:15)(cid:19) (cid:69)(cid:26)(cid:15)(cid:20) (cid:69)(cid:26)(cid:15)(cid:21) (cid:69)(cid:26)(cid:15)(cid:22) (cid:69)(cid:26)(cid:15)(cid:23) (cid:69)(cid:26)(cid:15)(cid:24) (cid:69)(cid:26)(cid:15)(cid:25) (cid:69)(cid:26)(cid:15)(cid:26) (cid:69)(cid:19)(cid:17)(cid:19) (cid:69)(cid:19)(cid:17)(cid:20) (cid:69)(cid:19)(cid:17)(cid:21) (cid:69)(cid:19)(cid:17)(cid:22) (cid:69)(cid:19)(cid:17)(cid:23) (cid:69)(cid:19)(cid:17)(cid:24) (cid:69)(cid:19)(cid:17)(cid:25) (cid:69)(cid:19)(cid:17)(cid:26) (cid:69)(cid:20)(cid:17)(cid:19) (cid:69)(cid:20)(cid:17)(cid:20) (cid:69)(cid:20)(cid:17)(cid:21) (cid:69)(cid:20)(cid:17)(cid:22) (cid:69)(cid:20)(cid:17)(cid:23) (cid:69)(cid:20)(cid:17)(cid:24) (cid:69)(cid:20)(cid:17)(cid:25) (cid:69)(cid:20)(cid:17)(cid:26) (cid:69)(cid:21)(cid:15)(cid:19) (cid:69)(cid:21)(cid:15)(cid:20) (cid:69)(cid:21)(cid:15)(cid:21) (cid:69)(cid:21)(cid:15)(cid:22) (cid:69)(cid:21)(cid:15)(cid:23) (cid:69)(cid:21)(cid:15)(cid:24) (cid:69)(cid:21)(cid:15)(cid:25) (cid:69)(cid:21)(cid:15)(cid:26) (cid:69)(cid:22)(cid:15)(cid:19) (cid:69)(cid:22)(cid:15)(cid:20) (cid:69)(cid:22)(cid:15)(cid:21) (cid:69)(cid:22)(cid:15)(cid:24) (cid:69)(cid:22)(cid:15)(cid:25) (cid:69)(cid:22)(cid:15)(cid:26) (cid:69)(cid:23)(cid:15)(cid:19) (cid:69)(cid:23)(cid:15)(cid:20) (cid:69)(cid:23)(cid:15)(cid:21) (cid:69)(cid:23)(cid:15)(cid:24) (cid:69)(cid:23)(cid:15)(cid:25) (cid:69)(cid:23)(cid:15)(cid:26) (cid:69)(cid:24)(cid:15)(cid:19) (cid:69)(cid:24)(cid:15)(cid:20) (cid:69)(cid:24)(cid:15)(cid:21) (cid:69)(cid:24)(cid:15)(cid:22) (cid:69)(cid:24)(cid:15)(cid:23) (cid:69)(cid:24)(cid:15)(cid:24) (cid:69)(cid:24)(cid:15)(cid:25) (cid:69)(cid:24)(cid:15)(cid:26) (cid:69)(cid:25)(cid:15)(cid:19) (cid:69)(cid:25)(cid:15)(cid:20) (cid:69)(cid:25)(cid:15)(cid:21) (cid:69)(cid:25)(cid:15)(cid:22) (cid:69)(cid:25)(cid:15)(cid:23) (cid:69)(cid:25)(cid:15)(cid:24) (cid:69)(cid:25)(cid:15)(cid:25) (cid:69)(cid:25)(cid:15)(cid:26) (cid:69)(cid:26)(cid:15)(cid:19) (cid:69)(cid:26)(cid:15)(cid:20) (cid:69)(cid:26)(cid:15)(cid:21) (cid:69)(cid:26)(cid:15)(cid:22) (cid:69)(cid:26)(cid:15)(cid:23) (cid:69)(cid:26)(cid:15)(cid:24) (cid:69)(cid:26)(cid:15)(cid:25) (cid:69)(cid:26)(cid:15)(cid:26) (cid:69)(cid:19)(cid:17)(cid:19) (cid:69)(cid:19)(cid:17)(cid:20) (cid:69)(cid:19)(cid:17)(cid:21) (cid:69)(cid:19)(cid:17)(cid:22) (cid:69)(cid:19)(cid:17)(cid:23) (cid:69)(cid:19)(cid:17)(cid:25) (cid:69)(cid:19)(cid:17)(cid:26) (cid:69)(cid:20)(cid:17)(cid:19) (cid:69)(cid:20)(cid:17)(cid:21) (cid:69)(cid:20)(cid:17)(cid:22) (cid:69)(cid:20)(cid:17)(cid:23) (cid:69)(cid:20)(cid:17)(cid:24) (cid:69)(cid:20)(cid:17)(cid:25) (cid:69)(cid:20)(cid:17)(cid:26) (cid:69)(cid:21)(cid:15)(cid:19) (cid:69)(cid:21)(cid:15)(cid:20) (cid:69)(cid:21)(cid:15)(cid:21) (cid:69)(cid:21)(cid:15)(cid:22) (cid:69)(cid:21)(cid:15)(cid:23) (cid:69)(cid:21)(cid:15)(cid:24) (cid:69)(cid:21)(cid:15)(cid:25) (cid:69)(cid:21)(cid:15)(cid:26) (cid:69)(cid:22)(cid:15)(cid:19) (cid:69)(cid:22)(cid:15)(cid:20) (cid:69)(cid:22)(cid:15)(cid:21) (cid:69)(cid:22)(cid:15)(cid:22) (cid:69)(cid:22)(cid:15)(cid:23) (cid:69)(cid:22)(cid:15)(cid:24) (cid:69)(cid:22)(cid:15)(cid:25) (cid:69)(cid:22)(cid:15)(cid:26) (cid:69)(cid:23)(cid:15)(cid:19) (cid:69)(cid:23)(cid:15)(cid:20) (cid:69)(cid:23)(cid:15)(cid:21) (cid:69)(cid:23)(cid:15)(cid:22) (cid:69)(cid:23)(cid:15)(cid:23) (cid:69)(cid:23)(cid:15)(cid:24) (cid:69)(cid:23)(cid:15)(cid:25) (cid:69)(cid:23)(cid:15)(cid:26) (cid:69)(cid:24)(cid:15)(cid:19) (cid:69)(cid:24)(cid:15)(cid:20) (cid:69)(cid:24)(cid:15)(cid:21) (cid:69)(cid:24)(cid:15)(cid:22) (cid:69)(cid:24)(cid:15)(cid:23) (cid:69)(cid:24)(cid:15)(cid:25) (cid:69)(cid:24)(cid:15)(cid:26) (cid:69)(cid:25)(cid:15)(cid:19) (cid:69)(cid:25)(cid:15)(cid:20) (cid:69)(cid:25)(cid:15)(cid:22) (cid:69)(cid:25)(cid:15)(cid:23) (cid:69)(cid:25)(cid:15)(cid:24) (cid:69)(cid:25)(cid:15)(cid:25) (cid:69)(cid:25)(cid:15)(cid:26) (cid:69)(cid:26)(cid:15)(cid:19) (cid:69)(cid:26)(cid:15)(cid:20) (cid:69)(cid:26)(cid:15)(cid:21) (cid:69)(cid:26)(cid:15)(cid:22) (cid:69)(cid:26)(cid:15)(cid:23) (cid:69)(cid:26)(cid:15)(cid:24) (cid:69)(cid:26)(cid:15)(cid:25) (cid:69)(cid:26)(cid:15)(cid:26) (cid:69)(cid:19)(cid:17)(cid:19) (cid:69)(cid:19)(cid:17)(cid:20) (cid:69)(cid:19)(cid:17)(cid:21) (cid:69)(cid:19)(cid:17)(cid:22) (cid:69)(cid:19)(cid:17)(cid:23) (cid:69)(cid:19)(cid:17)(cid:24) (cid:69)(cid:19)(cid:17)(cid:25) (cid:69)(cid:19)(cid:17)(cid:26) (cid:69)(cid:20)(cid:17)(cid:19) (cid:69)(cid:20)(cid:17)(cid:21) (cid:69)(cid:20)(cid:17)(cid:22) (cid:69)(cid:20)(cid:17)(cid:23) (cid:69)(cid:20)(cid:17)(cid:25) (cid:69)(cid:20)(cid:17)(cid:26) (cid:69)(cid:21)(cid:15)(cid:19) (cid:69)(cid:21)(cid:15)(cid:20) (cid:69)(cid:21)(cid:15)(cid:21) (cid:69)(cid:21)(cid:15)(cid:22) (cid:69)(cid:21)(cid:15)(cid:23) (cid:69)(cid:21)(cid:15)(cid:24) (cid:69)(cid:21)(cid:15)(cid:25) (cid:69)(cid:21)(cid:15)(cid:26) (cid:69)(cid:22)(cid:15)(cid:19) (cid:69)(cid:22)(cid:15)(cid:20) (cid:69)(cid:22)(cid:15)(cid:21) (cid:69)(cid:22)(cid:15)(cid:22) (cid:69)(cid:22)(cid:15)(cid:23) (cid:69)(cid:22)(cid:15)(cid:24) (cid:69)(cid:22)(cid:15)(cid:25) (cid:69)(cid:22)(cid:15)(cid:26) (cid:69)(cid:23)(cid:15)(cid:19) (cid:69)(cid:23)(cid:15)(cid:20) (cid:69)(cid:23)(cid:15)(cid:21) (cid:69)(cid:23)(cid:15)(cid:22) (cid:69)(cid:23)(cid:15)(cid:23) (cid:69)(cid:23)(cid:15)(cid:24) (cid:69)(cid:23)(cid:15)(cid:25) (cid:69)(cid:23)(cid:15)(cid:26) (cid:69)(cid:24)(cid:15)(cid:19) (cid:69)(cid:24)(cid:15)(cid:20) (cid:69)(cid:24)(cid:15)(cid:21) (cid:69)(cid:24)(cid:15)(cid:22) (cid:69)(cid:24)(cid:15)(cid:23) (cid:69)(cid:24)(cid:15)(cid:25) (cid:69)(cid:24)(cid:15)(cid:26) (cid:69)(cid:25)(cid:15)(cid:19) (cid:69)(cid:25)(cid:15)(cid:20) (cid:69)(cid:25)(cid:15)(cid:22) (cid:69)(cid:25)(cid:15)(cid:23) (cid:69)(cid:25)(cid:15)(cid:24) (cid:69)(cid:25)(cid:15)(cid:25) (cid:69)(cid:25)(cid:15)(cid:26) (cid:69)(cid:26)(cid:15)(cid:19) (cid:69)(cid:26)(cid:15)(cid:20) (cid:69)(cid:26)(cid:15)(cid:21) (cid:69)(cid:26)(cid:15)(cid:22) (cid:69)(cid:26)(cid:15)(cid:23) (cid:69)(cid:26)(cid:15)(cid:24) (cid:69)(cid:26)(cid:15)(cid:25) (cid:69)(cid:26)(cid:15)(cid:26) Fig. 5. Analysis of fault detection and correction capability of a 64-bit link with its 8×8 matrix appearance, horizontal PCC2 and vertical PCC2 implementing the 2D fault coding method in practice. The procedure mainly contains three steps as described below. (I) Organize a link as a its matrix appearance. Assuming that the link width is N and its matrix appearance equation: N = R × C . Then, the wires (w0 , w1 , ..., wN ) in the contains R rows and C columns, N , R, and C must meet the link are grouped into R groups. Group i (i = 0,...,R) contains the wires labeled as wi×R+j (j = 0,...,C ), which are renamed as bi,j (0 (cid:2) i < R, 0 (cid:2) j < C ). All the wires in group i become the elements on the i-th row of its matrix, and all of the j -th wires in all groups become the elements on the j -th column of the matrix. The matrix appearance is illustrated in Fig. 4. (II) Choose appropriate PCC techniques for horizontal row data and vertical column data, according to the objective of fault correction and detection. If PCCm is chosen for each row data and PCCn for each column data, at most m × n faults can be corrected, and the fault coverage can span over m continuous columns and n continuous rows. hi,j and vi,j are notated as the parity bits for row data and column data respectively (see Fig. 4). They are calculated by Equation (3) and (4). hi,j = bi,j ⊕ bi,j+m ⊕ · · ·⊕ b i,j+(cid:4) C−1 m (cid:5)×m (cid:5) 0 (cid:3) i < R 0 (cid:3) j < m (cid:6) (3) vi,j = bi,j ⊕ bi,j+n ⊕ · · · ⊕ b i,j+(cid:4) R−1 n (cid:5)×n (cid:5) 0 (cid:3) i < n 0 (cid:3) j < C (cid:6) (4) In total, M parity bits are required. M is computed by Equation (5). M = m × R + n × C (5) (III) Correct erroneous bits or restart a transmission, according to the comparison of the PCC results between the original data and the received data. In order to detect the faults, the parity bits (hi,j and vi,j ) of the original data (bi,j ) is computed, and the parity bits (h(cid:3) and v (cid:3) i,j ) of the received data (b(cid:3) i,j ) are also calculated. Then, the horizontal syndrome bits (S hi,j ) and the vertical syndrome bits (S vi,j ) are obtained by Equation (6) and (7) respectively. (6) i,j S hi,j = hi,j ⊕ h(cid:3) S vi,j = vi,j ⊕ v (cid:3) i,j (0 (cid:2) i < R, 0 (cid:2) j < m) (0 (cid:2) i < n, 0 (cid:2) j < C ) i,j (7) A bit (b(cid:3) i,j ) of the received data has its corresponding pair equal to ’1’, the bit (b(cid:3) i,j ) is suspected as an erroneous one, notated as b? Condition A: If the maximum horizontal distance between any two of b? i,j is less than m and the maximum vertical distance between any two of b? i,j is less than n, all b? i,j are simply inverting it, as described by Equation (8). ∧ represents conﬁrmed to be erroneous bits. They can be corrected by an AND operation. of (S hi,j%m , S vi%n,j ). If both of S hi,j%m and S vi%n,j are i,j . corrected bi,j = (S hi,j%m ∧ S vi%n,j ) ⊕ b? i,j (cid:5) 0 (cid:3) i < R 0 (cid:3) j < C (cid:6) (8)                         Condition B: If Condition A is not achieved, the suspected bits can not be conﬁrmed. And, if any one of syndrome bits becomes 1, the link fault is detected and the transmission is restarted. Condition B can be formulated by Equation (9). ∨ represents an OR operation. That is to say, if there is at least a pair of (S hp1 ,p ,S hp2 ,p ) with the same p and the distance (p2 − p1 ) (cid:3) m, or if there is at least a pair of (S vq ,q1 ,S vq ,q2 ) with the same q and the distance (q2 − q1 ) (cid:3) n, the retransmission signal (notated as ξ ) is asserted to be active-high. (cid:7) (cid:7) ξ = ( s.t. (S hp1 ,p ∧ S hp2 ,p )) ∨ ( 0 (cid:2) p1 , p2 < R, 0 (cid:2) t < m, p2 (cid:3) p1 + m 0 (cid:2) q1 , q2 < C, 0 (cid:2) q < n, q2 (cid:3) q1 + n (S hq ,q1 ∧ S hq ,q2 )) (9) C. Analysis of fault detection and correction capability Fig. 4 intrinsically shows that, when protecting the NoC link, the 2D fault coding method has high fault coverage and can correct at most m × n transient faults. However, it cannot correct any m × n transient faults. This subsection analyzes the fault detection and correction capability of the 2D fault coding method under three situations categorized according to the locations of erroneous bits. Situation A: The maximum horizontal distance between any two of erroneous bits is less than m and the maximum vertical distance between any two of erroneous bits is less than n. In this situation, the combination of PCCm and PCCn can exactly obtain the positions of all erroneous bits, so that they can be corrected by simply inverting them. For instance, link appears as a 8×8 as shown in Fig. 5 (a), a 64-bit matrix, and PCC2 is performed on all rows and columns, i.e., m=n=2. If the distance between any two of the erroneous bits (E1,E2,E3,E4) is less than 2, all of their horizontal syndrome bits and vertical syndrome bits become 1 (The orange boxes represent that the syndrome bits are 1 in Fig. 5), so the exact positions of these erroneous bits can be obtained. Situation B: The horizontal distance of at least one pair of erroneous bits is less than m, or the vertical distance of at least one pair of erroneous bits is less than n. In this situation, the combination of PCCm and PCCn cannot exactly obtain the positions of all erroneous bits so as to correct them, but some of the erroneous bits can be detected deﬁnitely. As illustrated in Fig. 5 (b), if all of their horizontal syndrome bits and vertical syndrome bits become 1 (see the orange boxes), the erroneous bits can be positioned. However, if not all their syndrome bits are 1, some erroneous bits can not be corrected. For instance, in Fig. 5 (c), because PCC2 is adopted, a syndrome bit becomes 1 when its related odd bits are erroneous. Because the erroneous bits E2 and E4 contribute to the same vertical syndrome bit, the vertical syndrome bit of E2 and E4 is still zero. Therefore, E2 and E4 cannot be corrected, but they can be detected because E2’s horizontal syndrome bit and E4’s horizontal syndrome bit become 1. In Fig. 5 (b) and (c), because the distance of at least one pair of erroneous bits is less than 2, their syndrome bits become 2D ECC Decoder Retrans. buffer Input Buffer Retrans. logic credit signal A 2D ECC Encoder Ä TMR 2D ECC Decoder ACK/ NACK source router Retrans. buffer Input Buffer Retrans. logic sink router 2D ECC Encoder Fig. 6. Implementing the 2D fault coding method in fault-tolerant NoC 1 deﬁnitely. Although all faults can not be corrected, we can judge that this transmission has faults and restart it. Situation C: The horizontal distance of any two of erroneous bits is more than m and the vertical distance between any two of erroneous bits is more than n. In this situation, if all syndrome bits of all erroneous bits are 1, they can be corrected, as shown in Fig. 5 (d). If some syndrome bits are 1, they can be detected, as shown in Fig. 5 (e). However, the combination of PCCm and PCCn may not obtain the positions of any erroneous bits. For instance, in Fig. 5 (f), E1 and E2 contribute to the same horizontal syndrome bit, E3 and E4 contribute to the same horizontal syndrome bit, E1 and E3 contribute to the same vertical syndrome bit, and E2 and E4 contribute to the same vertical syndrome bit, so no syndrome bits become 1. The erroneous bits are silent in Fig. 5 (f), meaning that they cannot be detected and will cause the system fault. Based on the analysis under the above three situations, in the procedure introduced in Subsection III-B, fault correction operation is performed under Condition A, and retransmission operation is carried out under Condition B. IV. IM P L EM EN T ING 2D FAULT COD ING M E THOD IN FAULT- TO L ERAN T NOC A. Overview Fig. 6 illustrates how to implement the 2D fault coding method in NoC routers to protect the data on links. To highlight the implementation, only its related modules such as 2D ECC decoder, retransmission buffer, retransmission logic, and 2D ECC encoder are drawn, while the port count and the basic modules such as routing computation, VC allocation, switch allocation, etc, are omitted in the ﬁgure. Before a packet (see packet A in the source (left) router in Fig. 6) is sent out, it is stored in the retransmission buffer and encoded by the 2D ECC encoder. Then the encoded packet (see packet ¨A in the sink (right) router in Fig. 6) is transferred on the link to the sink router. After the sink router receives the encoded packet ¨A, the 2D ECC decoder is responsible for detecting and correcting one or more faults. If there are no faults or the faults are corrected, an ACK signal is sent to the source router and notiﬁes the retransmission logic to remove the packet A from the retransmission buffer. If faults are detected but uncorrected, an NACK signal is returned to inform the source router of retransmitting the packet A. The ACK/NACK signal for notiﬁcation is protected by Triple Modular Redundancy (TMR) to ensure its correctness. Fig. 7. 2D ECC Encoder B. 2D ECC Encoder The 2D ECC encoder is responsible for (I) calculating the horizontal parity bits (hi,j ) for each row data according to Equation (3) and the vertical parity bits (vi,j ) for each column data according to Equation (4), and (II) protecting the parity bits (hi,j , vi,j ) by TMR technique. Fig. 7 shows the circuit a 64-bit (8× 8) link. The circuit is simple, because it is detail of the 2D ECC encoder that uses PCC2 to protect only comprised of 32 4-input exclusive-OR gates for parity calculation and 96 ﬂip-ﬂops for TMR protection. C. 2D ECC Decoder The 2D ECC decoder is a little more complex than the 2D ECC encoder. It contains ﬁve functions: (I) The parity bits (h(cid:3) i,j ) of the received data (b(cid:3) i,j ) from the link are calculated according to Equation (3) and (4). In the example shown in Fig. 8, it contains 32 4-input exclusive-OR gates. (II) The majority voters are used to get the received parity bits (hi,j , vi,j ) from their three copies. (III) Based on (I) and (II), the syndrome bits (S hi,j , S vi,j ) are calculated according to Equation (6) and (7), which uses 32 2-input exclusiveOR gates in Fig. 8. (IV) Based on (III), the corrected bits (corrected bi,j ) are calculated according to Equation (8), which uses 32 2-input AND gates and 32 2-input exclusiveOR gates in Fig. 8. (V) According to Equation (9), the retransmission signal (ξ ) is generated. If the retransmission signal is active-high, an NACK signal is sent to the source router, and corrected bi,j will not be stored into the input buffers. Otherwise, corrected bi,j is stored into the input buffers, and an ACK signal is sent to the source router. i,j , v (cid:3) D. Retransmission buffer and logic The retransmission logic is responsible for starting the retransmission process and conﬁguring the multiplexer at the output port. When an encoded packet ¨A is sent out, its original packet A is stored into the retransmission buffer. If an NACK signal is returned, the retransmission logic conﬁgures the multiplexer so that the original packet A can be gotten from the retransmission buffer and then sent to the sink router again after going through the 2D ECC encoder. If an ACK signal is returned, the retransmission logic removes the original packet A from the retransmission buffer, and conﬁgures the multiplexer so that a new packet can be transmitted. V. EX P ER IM EN T S AND R E SU LT S A. Experimental Setup The focus of the experiments is to compare the effects of using different ECC techniques to protect a link, so the OR AND OR . . . . . . . . . Shx,0 Shy,0 x > y+1 x,y = 0...7 AND OR . . . . . . . . . Shx,1 Shy,1 x > y+1 x,y = 0...7 AND OR . . . . . . . . . Sv0,x Sv0,y x > y+1 x,y = 0...7 AND OR . . . . . . . . . Sv1,x Sv1,y x > y+1 x,y = 0...7 retransmission i = 0...7 j = 0...7 b'i,1 b'i,3 b'i,5 b'i,7 h'i,1 XOR b'i,0 b'i,2 b'i,4 b'i,6 h'i,0 XOR b'0,j b'2,j b'4,j b'6,j v'0,j XOR b'1,j b'3,j b'5,j b'7,j v'1,j XOR Voter hi,0 Voter hi,1 Voter v0,j Voter v1,j hi,1 v'0,j v0,j XOR Sv0,j v'1,j v1,j XOR Sv1,j h'i,0 hi,0 XOR Shi,0 h'i,1 XOR Shi,1 i = 0...7 j = 0...7 i = 0...7 j = 0...7 Shi,0 Sv0,j bi,j Shi,1 Sv0,j bi,j Shi,0 Sv1,j bi,j Shi,1 Sv1,j bi,j corrected_bi,j i = 0,2,4,6 j = 0,2,4,6 XOR AND corrected_bi,j i = 0,2,4,6 j = 1,3,5,7 XOR AND corrected_bi,j i = 1,3,5,7 j = 0,2,4,6 XOR AND corrected_bi,j i = 1,3,5,7 j = 1,3,5,7 XOR AND (cid:175) D   Q CK D   Q CK D   Q CK (II) (I) (III) (IV) (V) Fig. 8. 2D ECC Decoder 0.1 0.2 0.3 0.4 0.5 0.6 0.7 s n Delay DECTED SECDED 2 C C E D 1 _ 2 C C E D 2 _ OECNED QECPED 2 C C E D 4 _ 2 C C E D 8 _ 1 2 3 4 x 104 (cid:80) m 2 Area OECNED QECPED DECTED SECDED 2 C C E D 1 _ 2 C C E D 2 _ 2 C C E D 4 _ 2 C C E D 8 _ 20 40 60 80 100 120 140 w m Power Consumption OECNED QECPED DECTED SECDED 2 C C E D 1 _ 2 C C E D 2 _ 2 C C E D 4 _ 2 C C E D 8 _ Fig. 9. Comparison of the area, power, and timing overheads between the 2D fault coding method, SECDED, DECTED, QECPED, and OECNED experimental platform is built up to contain a source router, a sink router, and a link connecting the two routers, as shown in Fig. 6. For comparison, the ECC encoder and decoder are implemented by the 2D fault coding method and the conventional SECDED, DECTED, QECPED, and OECNED techniques respectively. In the experiments, The 2D fault coding method with horizontal PCC1 and vertical PCC1, which corrects at most 1 fault, is notated as 2DECC 1. The 2D fault coding method with horizontal PCC1 and vertical PCC2, which corrects at most 2 faults, is notated as 2DECC 2. The 2D fault coding method with horizontal PCC2 and vertical PCC2, which corrects at most 4 faults, is notated as 2DECC 4. The 2D fault coding method with horizontal PCC2 and vertical PCC4, which corrects at most 8 faults, is notated as 2DECC 8. The ECC encoder and decoder are synthesized by Synopsys(cid:2) Design Compiler under TSMC(cid:2) 40nm technology. In the experiments, the classic fault model (see Equation (1)) is used to generate an erroneous bit for a link wire randomly. The link width increases from 32, 64, 128, 256 to 512 bits. The supply voltage is set as 0.9V, which is the nominal voltage at TSMC(cid:2) 40nm technology. The voltage noise varies from 0.10V to 0.20V. 10,000 packet transmissions are carried out. B. Hardware Cost Analysis Fig. 9 compares the hardware cost in terms of area, power consumption, and delay between the 2D fault coding method and the conventional SECDED, DECTED, QECPED, and   32b 64b 128b 256b 512b 0 10 20 30 40 50 60 u a F t l C o r r c e i t n o P e r e g a n e c t Link Width SECDED 2DECC_1 32b 64b 128b 256b 512b 20 40 60 80 u a F t l C o r r c e i t n o P e r e g a n e c t Link Width DECTED 2DECC_2 32b 64b 128b 256b 512b 20 40 60 80 u a F t l C o r r c e i t n o P e r e g a n e c t Link Width QECPED 2DECC_4 32b 64b 128b 256b 512b 20 40 60 80 u a F t l C o r r c e i t n o P e r e g a n e c t Link Width OECNED 2DECC_8 32b 64b 128b 256b 512b 20 40 60 80 100 u a F t l D c e e t i t n o P e r e g a n e c t Link Width SECDED 2DECC_1 32b 64b 128b 256b 512b 20 40 60 80 100 u a F t l D c e e t i t n o P e r e g a n e c t Link Width DECTED 2DECC_2 32b 64b 128b 256b 512b 20 40 60 80 100 u a F t l D c e e t i t n o P e r e g a n e c t Link Width QECPED 2DECC_4 32b 64b 128b 256b 512b 0 20 40 60 80 100 u a F t l D c e e t i t n o P e r e g a n e c t Link Width OECNED 2DECC_8 32b 64b 128b 256b 512b 0 20 40 60 80 100 S l i n e t u a F t l P e r n e c t e g a Link Width SECDED 2DECC_1 32b 64b 128b 256b 512b 0 20 40 60 80 100 S l i n e t u a F t l P e r n e c t e g a Link Width DECTED 2DECC_2 32b 64b 128b 256b 512b 20 40 60 80 100 S l i n e t u a F t l P e r n e c t e g a Link Width QECPED 2DECC_4 32b 64b 128b 256b 512b 20 40 60 80 S l i n e t u a F t l P e r n e c t e g a Link Width OECNED 2DECC_8 Fig. 10. Comparison of fault correction/detection/undetection percentages between our proposal, SECDED, DECTED, QECPED, and OECNED under different link widths when noise voltage is 0.2V 0.10 0.12 0.14 0.16 0.18 0.20 20 40 60 80 100 u a F t l C o r r c e i t n o P e r e g a n e c t Noise Voltage (V) SECDED 2DECC_1 0.10 0.12 0.14 0.16 0.18 0.20 20 40 60 80 100 u a F t l C o r r c e i t n o P e r e g a n e c t Noise Voltage (V) DECTED 2DECC_2 0.10 0.12 0.14 0.16 0.18 0.20 40 60 80 100 u a F t l C o r r c e i t n o P e r e g a n e c t Noise Voltage (V) QECPED 2DECC_4 0.10 0.12 0.14 0.16 0.18 0.20 40 60 80 100 u a F t l C o r r c e i t n o P e r e g a n e c t Noise Voltage (V) OECNED 2DECC_8 0.10 0.12 0.14 0.16 0.18 0.20 0 20 40 60 u a F t l D c e e t i t n o P e r e g a n e c t Noise Voltage (V) SECDED 2DECC_1 0.10 0.12 0.14 0.16 0.18 0.20 0 20 40 60 u a F t l D c e e t i t n o P e r e g a n e c t Noise Voltage (V) DECTED 2DECC_2 0.10 0.12 0.14 0.16 0.18 0.20 0 20 40 60 u a F t l D c e e t i t n o P e r e g a n e c t Noise Voltage (V) QECPED 2DECC_4 0.10 0.12 0.14 0.16 0.18 0.20 0 10 20 30 40 50 60 u a F t l D c e e t i t n o P e r e g a n e c t Noise Voltage (V) OECNED 2DECC_8 0.10 0.12 0.14 0.16 0.18 0.20 0 10 20 30 40 50 S l i n e t u a F t l P e r n e c t e g a Noise Voltage (V) SECDED 2DECC_1 0.10 0.12 0.14 0.16 0.18 0.20 0 5 10 15 20 25 30 S l i n e t u a F t l P e r n e c t e g a Noise Voltage (V) DECTED 2DECC_2 0.10 0.12 0.14 0.16 0.18 0.20 0 5 10 15 20 25 30 S l i n e t u a F t l P e r n e c t e g a Noise Voltage (V) QECPED 2DECC_4 0.10 0.12 0.14 0.16 0.18 0.20 0 5 10 15 20 25 30 S l i n e t u a F t l P e r n e c t e g a Noise Voltage (V) OECNED 2DECC_8 Fig. 11. Comparison of fault correction/detection/undetection percentages between our proposal, SECDED, DECTED, QECPED, and OECNED under different voltage noises when link width is 64-bit OECNED techniques when the link width is 256-bit. We can obtain that: the 2D fault coding method has obviously smaller area, power consumption, and delay than the conventional ECC techniques. As the correction capability (i.e. the number of corrected faults) increases, the hardware cost of the conventional ECC techniques increases quickly, but the hardware cost of the 2D fault coding method increases slowly. C. Detection and Correction Coverage Fig. 10 and 11 compares the detection and correction capability of the 2D fault coding method and the conventional ECC techniques under different link widths and voltage noises, when they are used to correct at most 1, 2, 4, or 8 faults. From the two ﬁgures, we can see that: (1) The conventional ECC techniques have higher fault correction percentages due                                                                                                                                                                                                 32b 64b 128b 256b 512b 10 20 30 40 50 60 u a F t l C o r r c e i t n o P e r e g a n e c t Link Width SECDED 2DECC_1 32b 64b 128b 256b 512b 10 20 30 40 50 60 u a F t l C o r r c e i t n o P e r e g a n e c t Link Width DECTED 2DECC_2 32b 64b 128b 256b 512b 60 50 40 30 20 10 u a F t l C o r r c e i t n o P e r e g a n e c t Link Width QECPED 2DECC_4 32b 64b 128b 256b 512b 70 60 50 40 30 20 10 u a F t l C o r r c e i t n o P e r e g a n e c t Link Width OECNED 2DECC_8 Fig. 12. Comparison of fault correction percentages normalized under the same area between our proposal, SECDED, DECTED, QECPED, and OECNED to its strong strength that, for instance, QECPED can correct any 4 faults. The strong fault correction capability leads to its high complexity and hence large hardware cost. As the link width increases (see the 1st row in Fig. 10) and the voltage noise increases (see the 1st row in Fig. 11), the fault correction percentages of the 2D fault coding method and the conventional ECC techniques decreases, because the probability of more than 1, 2, 4, or 8 faults increases. (2) The 2D fault coding method has much higher fault detection percentages than the conventional ECC techniques, because, for instance, OECNED can not detect the link fault if the number of wire faults are beyond 9, but the 2D fault coding method can detect the link fault as long as one of its syndrome bits is 1. As the link width increases (see the 2nd row in Fig. 10) and the voltage noise increases (see the 2nd row in Fig. 11), the 2D fault coding method has more obvious fault detection performance. (3) Due to the same reason above, As the link width increases (see the 3rd row in Fig. 10) and the voltage noise increases (see the 3rd row in Fig. 11), the silent fault (i.e. undetected fault) percentages of the conventional ECC techniques increases. However, the silent fault percentages of the 2D fault coding method is almost zero, because the probability of the distribution of multiple faults that leads to all syndrome bits to be 0 is very small and can be negligible. Although the 2D fault coding method does not have higher fault correction percentage but has higher fault detection coverage and smaller hardware cost than the conventional ECC techniques. Further, Fig. 12 shows the fault correction percentages normalized under the same area. The fault correction percentage (δ ) of the conventional ECC techniques is re-calculated by Equation (10). δ = δ × AREA2D f ault coding method AREAconventional ECC (10) Under the same area, the 2D fault coding method has higher fault correction percentages, demonstrating that it is more costeffective than the conventional ECC techniques when solving the multi-bit fault problem. V I . CONC LU S ION Observing that the probability of the occurrence of multiple transient faults in deep nanometer scale rises up and the conventional ECC techniques brings heavy hardware overheads when correcting and detecting multiple transient faults, the paper adopts a cost-effective ECC technique, named 2D fault coding method, to solve the multi-bit transient fault issue. The procedure of using the 2D fault coding method to protect a NoC link is proposed, its correction and detection capability is analyzed, and its hardware implementation is also carried out. Comparative experiments demonstrate that the method is suitable to the multi-bit transient fault control for NoC links. This paper focuses on the proposal of adopting the 2D fault coding method to safeguard NoC links. In the future, we will turn to further evaluate and optimize its performance itself by considering end-to-end and hop-to-hop fault control schemes. "
TooT - an efficient and scalable power-gating method for NoC routers.,"With the advent in technology and shrinking the transistor size down to nano scale, static power may become the dominant power component in Networks-on-Chip (NoCs). Powergating is an efficient technique to reduce the static power of under-utilized resources in different types of circuits. For NoC, routers are promising candidates for power gating, since they present high idle time. However, routers in a NoC are not usually idle for long consecutive cycles due to distribution of resources in NoC and its communication-based nature, even in low network utilizations. Therefore, power-gating loses its efficiency due to performance and power overhead of the packets that encounter powered-off routers. In this paper, we propose Turn-on on Turn (TooT) which reduces the number of wake-ups by leveraging the characteristics of deterministic routing algorithms and mesh topology. In the proposed method, we avoid powering a router on when it forwards a straight packet or ejects a packet, i.e., a router is powered on only when either a packet turns through it or its associated node injects a packet. Experimental results on PARSEC benchmarks demonstrate that, compared with the conventional power-gating, the proposed method improves static power and performance by 57.9% and 35.3%, respectively, at the cost of a negligible area overhead.","TooT: An Efﬁcient and Scalable Power-Gating Method for NoC Routers Hossein Farrokhbakht, Mohammadkazem Taram, Behnam Khaleghi, and Shaahin Hessabi Department of Computer Engineering, Sharif University of Technology, Tehran Email: {hfarrokhbakht, taram, behnam khaleghi}@ce.sharif.edu, hessabi@sharif.edu Abstract—With the advent in technology and shrinking the transistor size down to nano scale, static power may become the dominant power component in Networks-on-Chip (NoCs). Powergating is an efﬁcient technique to reduce the static power of under-utilized resources in different types of circuits. For NoC, routers are promising candidates for power gating, since they present high idle time. However, routers in a NoC are not usually idle for long consecutive cycles due to distribution of resources in NoC and its communication-based nature, even in low network utilizations. Therefore, power-gating loses its efﬁciency due to performance and power overhead of the packets that encounter powered-off routers. In this paper, we propose Turn-on on Turn (TooT) which reduces the number of wake-ups by leveraging the characteristics of deterministic routing algorithms and mesh topology. In the proposed method, we avoid powering a router on when it forwards a straight packet or ejects a packet, i.e., a router is powered on only when either a packet turns through it or its associated node injects a packet. Experimental results on PARSEC benchmarks demonstrate that, compared with the conventional power-gating, the proposed method improves static power and performance by 57.9% and 35.3%, respectively, at the cost of a negligible area overhead. Index Terms—Power Gating, Network-on-Chip, Energy Consumption, Idle Time I . IN TRODUC T ION The advent of multi-core processors with the ever increasing number of cores emphasizes the need for scalable, fast and energy efﬁcient interconnects. However, existing scalable Networks-on-Chip (NoCs), e.g. 2D meshes, are critical consumers of the chip’s total power budget [1], [2]. As an example, Intel Teraﬂop’s on-chip network consumes up to 28% of the chip overall power [2]. Recent studies show that a signiﬁcant portion of the NoC power consumption arises from routers’ static power consumption [3], [4]. In addition, static power is predicted to be exacerbated by continuous scaling of transistor feature size. On the other hand, since the routers are provisioned for peak loads but usually operate at low average utilization levels [5], routers’ static power are often viewed as an attractive target for power reduction. Nevertheless, this power reduction must be achieved without trading-off performance and scalability of NoC, which are also of vital importance because NoC should be able to meet the performance demands of future many-core processors. There are several techniques for reducing static power in digital circuits [6]. Power gating is a widely used technique where idle blocks are powered-off for reducing static 978-1-4673-9030-9/16/$31.00 c 2016 IEEE power [7]. Although this method can effectively reduce the power consumption in many digital circuits, it has several shortcomings particularly when applied to NoC routers. First, when a packet encounters a powered-off router, it has to wait until the router wakes up. Hence, power gating may impose a signiﬁcant latency overhead to the NoC. The problem gets worse if the packet encounters further powered-off routers along its path. Second, although the purpose of power gating is to save static power when blocks are idle, power gating itself incurs power cost. In other words, powering on a router leads to a non-negligible power overhead. Thus, consecutive idle periods should be large enough to be able to compensate the imposed wake-up power overhead. While real applications have a relatively low average trafﬁc load, even in these low utilization rates it is hard to ﬁnd/predict large enough consecutive idle cycles as packets arrive intermittently and do not follow a particular arrival pattern [8], [9]. Several attempts have been made recently to overcome the power gating issues. A common optimization technique to alleviate the latency overhead of power gating is the early wake-up technique. In this technique, wake-up signals are generated multiple hops earlier before a packet arrives at the powered-off router. Thus, it can partially [10] or almost completely [11] hide the wake-up latency. However, in addition to its area overhead, this technique only addresses the wake-up latency issue associated with power gating, and still suffers from intermittent packet arrival. Other recent efforts in effective power-gating methods for routers, including [3], [12], [10], are either complex and impose signiﬁcant area and performance overhead, or non-scalable. As an example, NoRD [3] avoids waking up a powered-off router using a ring to maintain connectivity of the network. Nevertheless, the long bypass ring makes the NoRD non-scalable, inasmuch as a packet may traverse a ring with the size of the network to evade encountering powered-off routers. In this paper, we propose Turn-on on Turn, TooT, a scalable yet effective power gating method. TooT is based on a key observation, which will be justiﬁed later in this paper: most of the times a powered-off router needs to be woken-up is caused by arriving a straight packet. We deﬁne straight packets as the packets that a powered-off router receives from an adjacent router and do not have a turn on this powered-off router. For example, south to north and west to east packets are considered as straight packets. Based on this observation, we aim to devise a mechanism that minimizes the number of wake-ups by providing a bypass path for straight packets. In other words, we do not power on a router if the incoming packet does not turn through the powered-off router. Therefore, we efﬁciently mitigate both the latency and energy overheads of the power-gating technique. Based on our new design, we also introduce a new predictor for efﬁcient detection of long idle periods to further enhance power consumption. Simulation results using PARSEC benchmarks [13] show that: (1) TooT outperforms static power savings of the conventional power-gating by 63.6%, (2) Compared to the conventional power gating, TooT improves performance by 35.3%, and, (3) besides its simplicity (in terms of circuit overhead), TooT’s predictor improves power-efﬁciency by 5.5% compared with its conventional counterparts. The rest of the paper is structured as follows. Sec. II provides a condensed background on power gating, and motivates the need for a better approach. In Sec. III, we present TooT, our novel power-gating mechanism. Sec. IV discusses our evaluation methodology. Sec. V presents simulation results. Sec. VI reviews related work, and ﬁnally Sec. VII concludes the paper. I I . BACKGROUND AND MOT IVAT ION A. Requisiteness of Power Gating Due to shrinking of transistor feature size and reduction in supply voltage, the contribution of leakage power in the chip’s total power has been increased. On-chip networks, as a promising substitute for the old bus structures in today’s Multiprocessor Systems-on-Chip (MPSoCs), are not exceptions to this trend. While NoC by itself may consume up to 35% of a chip’s total power [14], it has been shown that the ratio of static power in a typical 8⇥8 NoC increases from 43% in 45nm technology to more than 63% in 22nm [15], making the NoC’s static power a major contributor to chip’s power. Therefore, developing methods to alleviate the role of static power is indispensable. Power gating is a widely accepted approach to eliminate the leakage power of unutilized/idle components in digital circuits. In order to efﬁciently apply power gating to a digital circuit, it requires idle time of the circuit to be long enough to compensate the corresponding wake-up energy overhead. While NoCs are adjusted to tolerate high inter-node communications to avoid performance bottleneck, real-world applications typically exhibit quiet trafﬁcs, leading to underutilized on-chip resources. Our analysis on PARSEC benchmarks on an 8⇥8 mesh topology reveals that, on average, 92.7% of routers are idle during application runtime. Altogether, ﬁrst, power gating is crucial for NoCs due to the contribution of its leakage power to the total chip power, and second, it seems promising because of abounding idleness in components. B. Coarse-Grained Power Gating of On-Chip Routers Power gating is carried out by placing a switch (e.g. high threshold transistor) between a circuit and its supply voltage. For on-chip routers, the switch is cut-off when all the datapaths, i.e. input buffers, output latches and crossbar of a Fig. 1: Power gating of on-chip routers. router are idle. It can be accomplished by a simple alwayson monitoring controller. However, the adjacent routers must be notiﬁed of a power-gated router, otherwise they may send packets that will be thrown away in the network. Thus, handshaking signals are necessary to inform the neighbors if a router is powered-off. In this case, the neighbor tags the corresponding output port as powered-off, and make it unavailable when allocating the switch. Similarly, the adjacent routers need handshaking signals to inform a powered-off router when they send a packet for it. As illustrated in Fig. 1, the right side router is power gated and notiﬁes its neighbors (only the left router is shown) by asserting the PG signals. Consequently, the neighbors tag the corresponding output buffer as powered-off (port E of the left router is tagged as powered-off). The neighboring routers prompt this router to power on via the wake-up (WU) signals. C. Challenges of Power Gating the On-chip Routers Cumulative wake-up latency: A router can remain in power-gated state until it confronts a packet. This happens when either a) its associated node injects a packet, b) the associated node is the destination of a packet (i.e., ejects a packet), or c) it transmits a packet to any neighbors. Each of the mentioned scenarios causes the router to wake up. Typically, waking up a router takes about 4ns [10], which corresponds to 8 cycles wake-up latency in 2 GHz frequency. The problem is exacerbated when a packet encounters several power-gated routers within its path. In conventional power gating method, when the network utilization is low, the majority of the routers are power gated, hence there is a high probability that such scenario would happen, referred to as cumulative wake-up latency. Break-even time: Each time a router is woken up, it imposes a considerable wake-up energy. The term Break-Even Time (BET) is deﬁned as the minimum number of consecutive cycles that a gated block (here a router) must remain poweredoff in order to compensate the wake-up energy overhead. For on-chip routers, the BET is about 10 cycles [10]. Therefore, power gating the idle intervals for less than 10 cycles imposes energy overhead rather than save. As mentioned before, we observed that routers are idle for 92.7% of time; however, 63.4% of idle intervals violate the BET role, i.e. are shorter than 10 cycles, that impose signiﬁcant energy overhead and deteriorate the efﬁciency of power gating. Based on the above-mentioned issues, applying efﬁcient power gating on on-chip routers requires an approach to reduce the wake-up latency and energy in circuit level and/or reduce Fig. 2: Breakdown of packets encountering a powered-off router. the number of wake-ups, which subsequently increases the idle intervals as well. While applying power gating in smaller granularities (e.g. buffer and port level) [16] may alleviate the BET issue, these methods are not efﬁcient because of their hardware overhead and complexity. In addition, these methods do not mitigate the wake-up energy overhead and just split it to smaller pieces. D. Key Observation To further analyze the wake-ups of routers, we investigated the type of packets that cause a powered-off router to wake-up in a commonly used mesh network with XY routing. Note that due to lower overhead of XY routing algorithm (compared to adaptive routing), it has become the dominant routing algorithm which is widely used in most commercial and research chips (e.g., [17], [18], [1]). In addition, adaptive routing algorithms are usually beneﬁcial for high loaded networks, but power-gating is usually used for low to medium trafﬁc load, where there is little recognizable difference between XY and adaptive routing algorithms [11]. Demonstrated in Fig. 2, straight packets are the majority of packets that interrupt power-gated routers, which on average contribute to 54.7% of wake-ups. The remaining part is distributed almost evenly between other types of packets, i.e., turn, inject and eject. Notice that the routing algorithm plays a key role here, since in the XY routing algorithm any packet is injected and ejected once, has at most one turn during its path, and mostly traverses straightly. As an obvious consequence, if we could evade waking-up the power-gated routers, performance overhead due to wake-up latency would be decreased signiﬁcantly. In addition, not only will the wake-up energy overhead be mitigated, but also further energy can be saved since router will be in power-gated state instead of being in power-consuming idle detection state if conventional power gating method had been applied. I I I . PRO PO SED M E THOD : TOOT A. Main Idea In the conventional power-gating method, a powered-off router is woken up if any packet encounters the router. On the other hand, as noticed in Sec. II-D, more than 54.7% of these packets are straight packets. Accordingly, if we avoid powering on the routers for this large portion of packets, the efﬁciency of power-gating will highly improve by a) improving the power efﬁciency by increasing the length of intervals a router Fig. 3: TooT router architecture. is powered-off, which not only increases the powered-off periods, but also reduces the energy-overheads incurred while powering-on a router as well, and b) reducing the accumulated wake-up latency by reducing the cases a packet waits for a power-gated router to be powered-on during its path from the source to the destination. We prevent powering on the power-gated routers by augmenting all the routers with a bypass path. In addition, by appropriately architecting the bypass route, we aim to exploit it for the ejecting packets, as well (details of the bypass will be described in Sec. III-B). Thus, it is not anymore necessary to involve a power-gated router datapath to route such packets. Note that the observation of Fig. 2 is principal in the proposed method because otherwise, i.e. if all the packet types were distributed evenly, its effectiveness would be diminished, especially taking the potential overheads into account. B. TooT Architecture Fig. 3 demonstrates the proposed architecture to employ TooT. For the sake of simplicity, only the X+ (the input from the west upstream router) to X  (the output to the east downstream router) path is represented in this ﬁgure. For every bypass path such as X+ to X  , one bypass latch and two 2x1 multiplexers  all having equal ﬂit widths  are needed. Once a ﬂit arrives at a TooT router, depending on active/power-gated state of the router, different scenarios happen, as detailed in what follows. Router is power-gated: Input ﬂit enters the dedicated bypass latch. Afterwards, the controller checks whether the ﬂit turns at the current router. To do that, it compares the destination coordination of the ﬂit with that of the current router. If the ﬂit is supposed to turn at this router, it must wait until the router wakes up (power-on signal is triggered by the controller). Otherwise, the ﬂit is routed to the downstream router using the S0 port of the Out Mux. Its selection signal is also controlled by the controller (represented by dashes in the ﬁgure). Note that for the output network interface, the output of the latch (denoted by XB + ) passes through the 5x1 Mux. Actually, in the power-gated mode, TooT router acts as a conventional router with one-ﬂit input buffers, capable of merely forwarding straight packets and ejecting. Hence, it uses the free-to-forward signals to notify neighbors whether the corresponding bypass latchs are empty. Similarly, if the Turn Inject Shift-Register Empty PG (b) (a) Fig. 4: (a) Powered-off to powered-on transition. (b) TooT’s predictor. input buffers of the next router (or the bypass latch if it is power-gated) are not empty, the ﬂit remains in bypass latch. To avoid any conﬂict or starvation, while the bypass latch is not empty, the controller always grants the priority to this latch, i.e. S0 port of Out Mux is passed to output, whether the router is power-gated or even active. The latter may happen when one or more ﬂits inside the bypass latches cannot route to the next hops (due to network/router congestion, etc.) and meanwhile the router is triggered by some turning/injecting packets and has been powered-on (or is powering on). In such cases, the controller disables the (input ports of the) bypass latches until the router is completely woken-up, and then grants the priority to ﬂit(s) that may reside in them. Finally, it returns the privilege to the basic router by selecting the port S1 of the dedicated multiplexers. This procedure is depicted in Fig. 4a, where the router has been powered-on but there is a ﬂit in the shown bypass latch. Both the ﬂits in bypass latch and multiplexer M 2 (or its corresponding input buffer inside the router) are about to send a ﬂit through the port in M 1. In this case, as mentioned above, priority is given to the latch (path is denoted by P 1). However, simultaneous with P 1, the non-straight path P 3 also can be established since there is no full bypass latch (East to West) that sends a ﬂit through M 3. Since the router is powered-on (or is powering on) the input of the bypass latch is disabled. Router is active: This scenario is similar to typical behavior of a router with the delay overhead of In Mux and Out Mux if the network frequency is the bottleneck. It is worth noting that as discussed above, when the router is active, all input ﬂits enter the input buffers (through S1 Port of In Mux). However, when there is a ﬂit in a bypass latch, its corresponding TooT router output will be assigned to this latch. For instance, as depicted in Fig. 3, if the bypass latch of X + is full, the XR must wait until the controller grants the S 1. C. TooT’s Predictor In conventional power-gating methods, if a router has been idle for 4 consecutive cycles, it is deduced that it will be idle for the next 10 cycles, as well, providing it with the powergating opportunity. However, since in the proposed method we have modiﬁed the wake-up conditions, it necessitates a change in predictor mechanism since it is not anymore necessary to keep a router on when the passing packets are straight/eject. Thus, we aim to power gate a router if no turning or injecting + ﬂit encounters it within 4 consecutive cycles. Therefore, as a key advantage of TooT, if frequent/bursty straight packets arrive at a router, the controller can simply powergate it after 4 cycles, which signiﬁcantly improves the TooT efﬁciency. This is a substantial beneﬁt to powergate a router during its operation because in on-chip networks, the trafﬁc on a router’s port is usually continuous. It is noteworthy that this 4cycle predictor can be implemented with a simple circuitry. As shown in Fig. 4b, we have implemented the predictor with a (4-bit) shift register and a few logic gates. Based on this ﬁgure, in every cycle, if any of the ﬂits traversing through the router is turn or inject, a logical 1 (one) is input into the shift register, which disables the power gating signal (P G = 0) for the next 4 cycles. On the other hand, if none of the outgoing ﬂits is turn or inject, a 0 (zero) is shifted in. Repeating this procedure for 4 consecutive cycles makes the output of NOR gate logical 1. Then, if all of the router buffers, pipeline, etc. are empty (Empty = 1), the power-gating signal will be triggered. IV. EVALUAT ION M ETHODO LOGY Simulation experiments are performed using gem5 [19] full-system simulator, with the Ruby memory model and a modiﬁed version of BookSim [20]. The benchmark applications are taken from PARSEC benchmark suite [13]. Gem5 executed each application within their region of interests (ROI). Additionally, we integrated DSENT [4] with Booksim to model static and dynamic power for a 45 nm process. we used Synopsys Design Compiler with a Nangate 45 nm Open Cell Library to obtain area overhead of TooT. Our baseline platform is a 64-core processor with a 2-level cache hierarchy. Each core has a private L1 cache, while L2 cache is a distributed cache which is maintained via a MESI directory cache coherence protocol. We used an 8⇥8 2D mesh, with each router attached to a single processor core. Wakeup latency is set to 8 cycles assuming a 4ns wake-up delay [10] and 2GHz frequency. The BET is set to 10 cycles based on the evaluations in [10]. We also evaluated TooT across the full range of the network utilizations using synthetic workloads (uniform, bitcomplement, and shufﬂe [20]). To this end, we warmed-up the simulator for 30,000 cycles and then collected network statistics for one milion cycles. For synthetic workloads, we used a mix of short 1-ﬂit and long 5-ﬂit packets. In order to assess the scalability of TooT, we also evaluate TooT under 4⇥4 and 16⇥16 network sizes. Table I summarizes the key parameters of the simulation setup. TABLE I: Key Simulation Parameters # of cores  64 on-chip, in-order, Alpha ISA, 2GHz  Private I/D L1$  32KB, 4-way, LRU, 2-cycle latency, MESI coherence protocol  Shared L2 per bank 8MB, 8-way, LRU, 8-cycle latency  Cache block size  64B  Memory  128 cycle access time, 4 on-chip memory controllers  Link bandwidth  128 bits/cycle  Network topology  4x4, 8x8, 16x16 mesh  Routing algorithm XY  Router  3-stage and 4-stage  Virtual channel  1 VCs/VN, 3 VNs, 4 flits/VC  Network topology 4x4, 8x8, 16x16 mesh  Virtual channel  3 VCs/port  Input buffer depth  4 flits/VC  Router  3-stage, 4-stage  Routing algorithm XY  	 	 	 Fig. 5: Router power breakdown. We compare the proposed method, TooT, with the following mechanisms: • NO-PG: The baseline of our evaluations in which the power-gating technique is not applied to the routers. • Conv: Conventional power-gating method which powersoff a router as soon as it goes to idle mode. • ConvOpt: Like Conv but uses the early wake-up technique [10] to partially hide wake-up latency. Further, this method waits for 4 consecutive idle cycles before powering-off an idle router. • PowerPunch Signal: A state-of-the-art power-gating method proposed by Chen et al. [11] which tries to completely hide wake-up latency. V. R E SU LT S AND ANA LY S I S In this section, we present the impact of TooT on power, energy and performance. Then, we analyze the TooT’s behavior across the full range of network loads. Afterwards, we present the results of the scalability and sensitivity analysis of the proposed method with respect to the number of pipeline stages and wake-up latency. Finally, we report the area overhead of the TooT controller and datapath. A. Impact on Power and Energy Fig. 5 shows the breakdown of router power for the real workloads. The power values are normalized to NO-PG case. The total router power consumption is composed of dynamic power, static power, and power gating overhead. All the power wasted for applying power gating, such as the power consumed for powering on the routers, and the power-gating controller are accounted for the power-gating overhead. In order to fairly compare the net savings in static power, in addition to static power itself, the power-gating overhead should also be considered. That is, the total of the bottom two bars in Fig. 5 shows the net static power. As it can be seen in Fig. 5, TooT reduces router static power consumption by 90% on average, while PowerPunch saves 75.9% and ConvOpt saves 75.5% of static power compared to NOPG. TooT’s considerable static power improvement is mainly achieved by the increased length of idle periods through bypass path, which has two consequences: 1) static power reduction through higher opportunity of powering off the routers, and 2) decreased power-gating overhead due to reduced number of wake-ups and violations of the BET role. Fig. 6: Static energy comparison. Fig. 7: Packet latency comparison. Fig. 6 shows the static energy of different power-gating schemes normalized to NO-PG case. The ﬁgure shows that on average, TooT achieves 88.9% static energy saving, while PowerPunch and ConvOpt save 74.3% and 72.4% of static energy, respectively. B. Impact on Performance In addition to achieving a signiﬁcant power saving over conventional power-gating methods, TooT is able to reduce packet latency, and hence improves performance. Fig. 7 shows the packet latency of the methods across the benchmarks. Results are normalized to the NO-PG case. It is noticeable that NO-PG leads to the least packet latency due to the fact that it does not power-off routers, and hence does not suffer from wake-up latency. On the other hand, Conv substantially increases average packet latency (104.4% on average). This is because the network utilization is low in these real workloads. Consequently, at any point in time a large number of routers are in powered-off mode. Therefore, packets should wait for many wake-ups, which leads to increase in average packet latency. Early wake-up technique which is applied in ConvOpt can partially hide the wake-up latency and reduce the large packet latency overhead incurred by power gating. However, this optimization technique cannot completely cover the wakeup latency and still leave non-negligible latency overhead (52.8% on average). TooT, on the other hand, on average imposes only 32.3% average packet latency overhead, and thus outperforms conventional power-gating methods. This improvement arises from the fact that TooT evades waking up powered-off routers for straight/eject packet. Thus, such packets do not have to wait for waking-up routers, which leads to a superior average packet latency compared to the conventional power-gating methods. PowerPunch is able to further reduce average packet latency, thanks to its signaling mechanism. However, as described in Sec. V-A, it is achieved at the cost of missing a signiﬁcant power saving opportunity. Fig. 8: Execution time comparison. Fig. 9: Average number of encountered powered-off routers. Execution time of benchmarks, as Fig. 8 depicts, follow a similar trend, although the network latency has different effects on different benchmarks. Some benchmarks, such as dedup and blackscholes, show more sensitivity to network latency than the others. Overall, the Conv, ConvOpt, PowerPunch and TooT increase the execution time by 14.4%, 7.3%, 1.2%, and 4.5% respectively, in order to reach the power saving described in Sec. V-A. To give a better insight on how TooT can decrease the adverse effect of power gating on average packet latency, we present further analysis by providing the results of the average number of wake-ups a packet encounters across its path in network from the source to the destination. As it can be seen in Fig. 9, TooT reduces average number of wake-ups to 1.35, which is signiﬁcantly less than the conventional schemes. C. Impact of TooT Predictor As discussed in Sec. III-C, we replace the conventional power-gating idle detection mechanism with a more effective predictor. In this section, we compare TooT’s predictor with conventional power-gating idle detection mechanism. Fig. 10 represents the static power savings of TooT using different predictors. As the ﬁgure shows, TooT’s predictor contributes to 5.5% of its static power improvement. TooT’s predictor uses the straight packets as indicator of arrival of the future straight packets. Thus, it can power a router off earlier than conventional idle detection mechanism, which leads to more power saving. D. Behavior across Full Range of Network Loads Fig. 11 depicts the behavior of the power-gating schemes across different injection rates using synthetic workloads. Fig. 11 (a-c) compare the net static power (including static power and power gating overhead) of the schemes normalized to NO-PG case, and Fig. 11 (d-f) compare the average packet latency of the schemes. The ﬁgure shows that the net Fig. 10: TooT’s predictor performance. static power and average packet latency of all three synthetic workloads follow similar trends. In low injection rates, where the power-gating schemes power off routers for majority of the time, conventional power-gating schemes can considerably reduce static power. Nevertheless, on occasions when a packet is injected into the network, it would encounter lots of wake-ups across its path to destination, which leads to a non-negligible average packet latency overhead. TooT, on the other hand, obviates the need for waking up a router when the packet is a straight packet, leading to a minor overhead and signiﬁcant static power saving compared to related methods. When injection rate gradually increases, ﬁrst we observe a slight increase in TooT’s average packet latency due to the congestion on TooT’s bypass latch. Then, by further increasing the load, as routers incline to be in power-on state, packets tend to traverse through powered-on routers instead of bypass latch, which results in lower congestion in bypass latch, thereby reducing average packet latency. Note that although in some loads TooT may have slightly higher packet latency than ConvOpt, it achieves considerable higher static power saving in those injection rates. Besides, as our simulation shows, real application loads typically have very low injection rates. As it can be observed, at some loads Conv even increases the net static power. This is because this method powers a router off as soon as it goes to idle mode (i.e., no packet in router’s pipeline). Thus, in higher injection rates, where packets come usually at a router with an intervals of less than BET, the wakeup energy exceeds the static power savings of Conv; hence, this methods defeats the purpose of power gating. Finally, in higher injection rates, where almost all routers are in powered-on state, net static power and average packet latency of the power-gating schemes conform to NO-PG case. E. Sensivity Analysis on Pipeline Stages and Wake-up Latency In order to investigate the effects of the number of pipeline stages and the wake-up latency on the average packet latency overhead of power-gating schemes, we set wake-up latency to 6, 8, 10, and 12. Then, we re-performed the simulations for routers with three and four pipeline stages. In these experiments we used uniform random workloads with an injection rate similar to the average injection rate of PARSEC benchmarks. Fig. 12 shows the results of these experiments. While the average packet latency of conventional power-gating methods substantially grows with increase of wake-up latency, (a) Net static power (uniform) (b) Net static power (bit-comp.) (c) Net static power (shufﬂe) (d) Average packet latency (uniform) (e) Average packet latency (bit-comp.) (f) Average packet latency (shufﬂe) Fig. 11: Packet latency and net static power of synthetic workloads. 6 8 10 12 Wake-up latency (a) 3-stage pipeline (b) 4-stage pipeline Wake-up latency Fig. 12: Sensivity analysis on pipeline stages and wake-up latency. TooT has considerably less average packet latency in all of the wake-up latencies. F. Scalability and Area Overhead Since TooT neither uses a centralized decision making mechanism nor a network-sized bypass ring, TooT is expected to be scalable. To examine the scalability of TooT, we performed simulations for 4⇥4 and 16⇥16 network sizes. In these experiments we used uniform random workloads with injection rate set to 0.01 ﬂits/node/cycle. TooT outperforms the static power savings of ConvOpt by 60.3% and 50.2% for 16⇥16 and 4⇥4 networks, respectively. Also, TooT can reduce the average packet latency of ConvOpt by 24.2% and 11.8% for 16⇥16 and 4⇥4 networks, respectively. TooT’s effectiveness (in terms of static power saving and average packet latency) is directly proportional to the size of the network. This is because in a mesh topology, a packet has at most one turn across its path. Thus, the ratio of straight packets is greater in larger network sizes, which leads to superior efﬁciency of TooT. We evaluated the area overhead of TooT using Synopsys Design Compiler and 45 nm technology node. Our experiments show that TooT’s static power savings come at the cost of only 3.12% area overhead compared to conventional scheme. V I . R ELAT ED WORK Coarse-grained Power-Gating: NoRD [3] eliminates the dependency of a node to its associated router in order to provide it with the ability of injecting and ejecting packets without powering on the router. To this end, it adds an inport and outport bypass to Network Interface (NI) of the router, hence, any sent/received packet can inject/eject to/from the network without waking-up the router. Similarly, forwarded packets can bypass a powered-off router by entering through the dedicated inport bypass and passing through its outport bypass. While this solution eliminates the wake-up latency once a packet encounters a powered-off router, it increases the packet latency by detouring it to a neighbour router. The situation becomes worse when a packet faces multiple powered-off routers. Therefore, NoRD is not scalable for large NoC (e.g. an 8⇥8 mesh) because a packet may traverse long bypass rings  up to size of the network  to evade from powering on the router(s), which imposes signiﬁcant packet latency for large NoC. In [11], in order to mitigate the wakeup latency, Chen et al. have proposed PowerPunch which aims to power on a powered-off router ahead arrival of the packets, using additional controlling signals. Since PowerPunch has nothing to do with the fragmented idleness period of routers and the number of wake-ups, this prevalent power-gating overhead is still an issue in this method. The authors in [12] propose Router Parking which powers off a subset  based on aggressive or conservative policies  of the routers that are connected to powered-off cores. It is also aware of maintaining the network connectivity and limiting the latency imposed by the packets that detour the powered-off routers. Based on the information of the network trafﬁc which is collected by a speciﬁc component, it chooses the subset of routers to be powered-off. It is only able to power-gate the routers connected to powered-off cores, so its efﬁciency is limited. In addition, Router Parking requires complex design because it updates the routing table once a router is powered-off, and modiﬁes the routing algorithm. In [21] power-gating efﬁciency in Multi-NoC has been investigated. In a Multi-NoC, wires and buffers are split-off into several sub-networks wherein the node is connected to all of its associated routers in all the sub-networks. Thus, it is promising for power-gating since a sub-network can be power-gated entirely without affecting the network connectivity. To achieve this goal, they propose a subnet selection policy that provides it long idleness interval to alleviate the BET problem, and adapts the network bandwidth to application demands. Fine-grained Power-Gating: FlexiBuffer has proposed partitioning a buffer into two unequal parts to facilitate the powergating [22]. The smaller part is always active to provide routability in small trafﬁcs, while the second part is poweredoff by default to save energy. When the number of ﬂits in smaller part surpasses a determined threshold, the second part of the buffer is powered-on. In [16], Matsutani et al. have proposed ultra ﬁne-grained power-gating for NoC routers in which each component (e.g. buffer, crossbar, etc.) can be individually powered-off, considering the network ﬂow. In order to alleviate the wake-up latency, it uses 2-hop lookahead signals to wake-up the buffers prior to arrival of the packets. While this method reduces the wake-up latency, it has no approach to reduce the number of wake-ups, i.e., idleness intervals of routers are still fragmented that leads to energy overhead because of wake-ups and also leaks powergating opportunity due to idle-detection times. Finally in [23], Panthre is proposed to provide long sleeping intervals to ﬁnegrained units to facilitate efﬁcient power-gating. This method is based on the observation that only 10% of network trafﬁc ﬂows through the 30% of (low utilization) links, providing a potential for power-gating. Power-gating is done in link-level granularity, i.e., links with utilization lower than an adaptive threshold are powered-off. In addition to complexity of added components, this method is inefﬁcient since it can exploit only about half of its power-gating opportunity. V I I . CONC LU S ION NoC power consumption is considered as a signiﬁcant and increasing portion of the overall multi-core chip power consumption. This work, Turn-on on Turn (TooT), focused on a novel power-gating mechanism in order to reduce the static power consumption of NoC routers. TooT, by providing a bypass path for straight and eject packets, avoids powering on a router when a straight/eject packet encounters a poweredoff router. Consequently, TooT improves the efﬁciency of power-gating methods by reducing the overall number of wake-ups which leads to signiﬁcant power and performance improvements. Full system simulations show that compared to an optimized conventional power-gating method, TooT is able to reduce static energy of NoC routers and average packet latency of network by 59.9% and 13.4%, respectively. "
Extending networks from chips to flexible and stretchable electronics.,"Emerging flexible hybrid electronics paradigm integrates traditional rigid integrated circuits and printed electronics on a flexible substrate. This hybrid approach aims to combine the physical benefits of flexible electronics with the computational advantages of the silicon technology. In this paper, we discuss the possibility to implement a physically flexible system capable of sensing, computation and communication. We argue that this capability can transform personalized computing by enabling the next big leap forward in the form factor design, similar to the shift from desktop and laptop computers to hand-held devices. Designing this type of a comprehensive system requires integrating many flexible and rigid resources on the same substrate. As a result, efficient interconnection network design rises as one of the major challenges similar to the system-on-chip experience. Therefore, we also discuss the interconnect design challenges and promising solutions for flexible hybrid systems.","Extending Networks from Chips to Flexible and Stretchable Electronics Invited Paper Ujjwal Gupta and Umit Y. Ogras School of Electrical, Computer, and Energy Engineering, Arizona State University Email: {ujjwal,umit}@asu.edu Abstract—Emerging ﬂexible hybrid electronics paradigm integrates traditional rigid integrated circuits and printed electronics on a ﬂexible substrate. This hybrid approach aims to combine the physical beneﬁts of ﬂexible electronics with the computational advantages of the silicon technology. In this paper, we discuss the possibility to implement a physically ﬂexible system capable of sensing, computation and communication. We argue that this capability can transform personalized computing by enabling the next big leap forward in the form factor design, similar to the shift from desktop and laptop computers to hand-held devices. Designing this type of a comprehensive system requires integrating many ﬂexible and rigid resources on the same substrate. As a result, efﬁcient interconnection network design rises as one of the major challenges similar to the system-on-chip experience. Therefore, we also discuss the interconnect design challenges and promising solutions for ﬂexible hybrid systems. Keywords — Flexible Electronics, Flexible Hybrid Electronics, Systems-on-Chip, Thin-Film Transistor (TFT) I . IN TRODUC T ION Flexible electronics have the potential to transform computing by enabling bendable and stretchable systems with arbitrary shapes [39]. Physical ﬂexibility combined with the cost and weight advantages opens a wide range of form factors and application areas, including wearable electronics, prosthetics, medical sensing, rollable displays, and internet of things (IoT) [32]. For instance, a brain-machine interface embedded on a hat or an electronic patch could enable controlling a prosthetic arm and interaction with IoT devices. However, performance and capabilities of purely ﬂexible electronics are very limited [35]. Therefore, ﬂexible hybrid electronics (FHE) has recently emerged as a promising technology to address these limitations [10]. FHE refers to physically ﬂexible systems that integrate commercial off-the-shelf rigid integrated circuits (ICs) on ﬂexible or stretchable substrates. Hence, the FHE concept combines the physical merits of ﬂexible electronics with the power, performance and area advantages of traditional silicon technology. Fig. 1: An illustration of the future SoP, i.e., System-onPolymer, concept. at a mobile form factor. We envision that the next big leap forward in form factor design can be enabled by ﬂexible hybrid electronics. More precisely, FHE can be used to design arbitrary shaped, ﬂexible or stretchable systems with integrated sensing, computing and communication capabilities, as illustrated in Figure 1. This capability can deliver the functionality of current smartphones and more in a truly pervasive form factor. We have recently coined the term system-on-polymer (SoP) to refer to this fully integrated solution [17]. An SoP implements the whole system similar to system-on-a-chip, but the ﬂexible circuits and off-the-shelf rigid ICs are integrated on ﬂexible or stretchable substrates using FHE. Combination of sensing, computation and communication in a single system is possible only with the integration of many rigid ICs and ﬂexible circuits. For example, the system illustrated in Figure 1 consists of a processor, offchip memory, sensor hub, display, radio frequency IC (RFIC), printed antenna and voltage regulation circuitry. The SoC design experience shows that communication between these resources is critical for both the system performance and power consumption [27]. Therefore, the interconnection network should provide sufﬁcient network and low latency such that communication does not become the performance bottleneck. Furthermore, battery constraints become even more stringent than SoCs due to the wearable form factor. This necessitates extreme energy-efﬁciency in all aspects of SoPs including the interconnection network design. During the last decade, tremendous advances in the systemon-chip (SoC) design enabled integration of a complete system including a large variety of processing elements and memory on a single die. This, in turn, gave rise to powerful yet lowpower embedded platforms that enabled personal computing The communication challenge is ampliﬁed further by the evolutionary nature of ﬂexible electronics. Most of the sensors, display and antenna are expected to be implemented using purely ﬂexible electronics. With the current technology, the application processor, off-chip memory and RFIC will be 978-1-4673-9030-9/16/$31.00 c(cid:13)2016 IEEE likely rigid, since ﬂexible electronics are not yet mature and competitive in these areas. At the same time, we witness great advances in the design of ﬂexible processors, RFIC, analog to digital conversion and memory design [3, 14]. As the ﬂexible electronics technology advances, the composition of ﬂexible and rigid resources is expected to evolve. Consequently, the capabilities of SoPs are expected to ﬂourish in time, while the interconnection networks adapt to the growing needs. SoP interconnection network design challenge has two aspects. The ﬁrst aspect is innovating reliable circuits and manufacturing solutions to interface rigid components with ﬂexible substrates. This problem is addressed by novel interface circuits [23, 37] and recently funded manufacturing institute [29]. The focus of this paper is the second aspect, which is designing a system level communication network similar to networkson-chip (NoCs) used in SoC and multicore chips [27]. We present system level communication challenges for SoPs, and discuss potential solutions to mitigate their effect. In particular, we analyze the system level interconnect solutions for two classes of systems. The ﬁrst class of systems is bendable along an arbitrary axis, as illustrated in Figure 2(a). We refer to this type of systems as “bendable” throughout the paper. The second class is more comprehensive in the sense that they are “stretchable” as well as bendable, as illustrated in Figure 2(b). The communication bandwidth reduces dramatically from rigid to bendable, and bendable to stretchable systems due to lower integration density and lower operating frequencies. Furthermore, global interconnects on ﬂexible substrates suffer from lower reliability, since they are prone to physical damage. As a result, wireless communication becomes advantageous both in terms of energy efﬁciency, performance and reliability. Therefore, we propose a hybrid combination of Low Voltage Differential Signaling (LVDS) [15] based communication and inter-chip wireless communication [6, 9]. The remainder of this paper is organized as follows. We overview the FHE technology and related literature in Section II. In Section III, we discuss the SoP interconnect design challenges and potential solutions. Section IV presents potential application areas of FHE. Finally, we conclude the paper by summarizing the key points in Section V. I I . BACKGROUND AND R ELAT ED WORK Flexible electronics refers to integrated circuits implemented on bendable, stretchable, rollable, conformable, or elastic substrates which are lighter, thinner, and less expensive to manufacture [39]. Major building blocks of ﬂexible electronics are the substrate, backplane electronics, frontplane and encapsulation. Common types of substrates are thin glass [26], plastic ﬁlm [28] and metal foil [39]. Backplane electronics are used for processing and power/signal delivery to the frontplane. Most common materials used for backplane include silicon thin-ﬁlm transistors (TFT) [25], organic thinﬁlm transistors [41] and transparent thin-ﬁlm transistors [24]. Interconnect and contacts are implemented using transparent conductive oxides, conducting organic polymers and stretchable wires. Finally, frontplane could be liquid crystal display, Neutral Position Bent (a) !""#$& !""#$% (b) !""#$& !""#$% !"" X !# !# !"" !"" X !# !# !"" Stretched & Bent Fig. 2: The relaxed and deformed examples of bendable and stretchable SoPs. (a) The forces are only applied in y direction. (b) The forces can be applied in all three directions x, y , z . Figure shows forces in direction x and y only. organic light-emitting display, an actuator such as an artiﬁcial muscle or sensor [25]. Flexible electronics technology has been used successfully to design displays, electronic paper, sensors, photovoltaic cells and batteries [33]. More recently, researchers have demonstrated wireless tags [20], programmable logic circuits [34], simple micro-controllers, analog-to-digital converters (ADC) and radio frequency transmitters [1]. However, performance and capabilities of ﬂexible electronics is still signiﬁcantly lower than that offered by CMOS technology [19]. Therefore, ﬂexible hybrid electronics have been proposed to exploit the advantages of rigid ICs [29, 33]. Integration of CMOS devices on ﬂexible substrates has been demonstrated recently [18, 35]. Moreover, circuits for interfacing ﬂexible electronics and CMOS ICs have been proposed [23, 37]. Design of metal wire interconnects for stretchable electronics is presented in [16, 40]. The authors in [16] discuss three different shapes of metal interconnects on a polydimethylsiloxanes substrate using ﬁnite element analysis. Each interconnect is stretchable matrix of shape elliptical, U-shape and horseshoe. When deformation occurs, only the shape of the interconnect changes without any signiﬁcant change in the resistance, because total length still remains the same. In addition to this, the work presented in [40] demonstrates a highly stretchable electronic system with free ﬂoating interconnect network. The network consists of serpentine shaped wires connecting assemblies like ICs that are suspended in a microﬂuid. Despite the impressive progress in the development and analysis of these interconnects, they have lower integration density and lower performance compared to on-chip interconnects. To mitigate these effects, approaches that use wireless communication for intra- and inter-chip communication can be used [6, 9], as detailed in Section III-B. I I I . SOP IN TERCONNEC T CHAL LENGE S AND POTENT IA L SO LU T ION S A. Limitations of Wired Interconnects on Flexible Substrates Bandwidth Limitations: The bandwidth of a communication channel can be expressed as BW = W ×fmax , where W is the number of links and f is the maximum frequency at which one bit can be transmitted over one link [22]. Compared to SoCs, the communication bandwidth on ﬂexible substrates is severely limited by three factors: 1) Low integration density: Modern CMOS technology and printed circuit board provide more than ten metal layers. Flexible and stretchable substrates, on the contrary, have limited number of layers putting larger constraints on the number of wires and wire pitch. Since a typical wire width is around 90µm [16], it can limit the wire pitch to ∼200µm for bendable and ∼800µm for stretchable systems. Moreover, the links on the substrate need to connect to the pins of the rigid ICs. Hence, even if smaller wire pitch becomes possible, still the physical dimensions of the rigid ICs impose limitations. 2) Physical limitations: Besides manufacturing challenges, adding more wiring layers is not desirable, since this decreases the ﬂexibility of the system, as depicted in Figure 3. The ﬂexibility reduces with more wires, since the elasticity of the interconnects is signiﬁcantly smaller than that of the substrates. For example, the Copper wires have a higher Young’s modulus of elasticity of 128GPa compared to Polyimide’s elasticity of 7.5GPa [11]. 3) Limited circuit support: Fast on-chip and on-board communication over long links (in the order of millimeters) is enabled by optimally inserted repeaters, designed using CMOS technology [7]. Since the TFTs are signiﬁcantly slower, these techniques may not be a viable option for ﬂexible interconnects. As a result of low integration density (1) and physical limitations (2), the number of wires that can ﬁt into a given cross-section (i.e. W ) is a few orders of magnitude smaller for ﬂexible electronics. Furthermore, the maximum signaling frequency fmax over long wires is limited due to limited circuit support (3). Consequently, the communication bandwidth on ﬂexible substrates using conventional approach is signiﬁcantly lower than that available for SoCs. Shape Changes: The interconnect design challenge in ﬂexible hybrid electronics is aggravated by the physical ﬂexibility. Our measurements on an experimental prototype and ﬁnite element method (FEM) simulations show that bending does not change the length and electrical properties of Copper wires signiﬁcantly. However, increasing the wiring area decreases the ﬂexibility signiﬁcantly, as depicted in Figure 3. In this ﬁgure, we show COMSOL simulation results for bending a Polyimide substrate. The abscissa equal to zero corresponds to a Polyimide substrate without any wires on it. We observe that the ﬂexibility normalized to this point decreases signiﬁcantly by adding Copper wires to the substrate. For example, if 30% of the total area is Copper interconnect, the maximum displacement of the substrate at the edge decreased by more than 22%, as illustrated on the plot. The effect of the shape changes is more pronounced for stretchable substrates. As illustrated in Figure 2, a serpentine interconnect is used to accommodate length changes due to t n e m e c a p l s i D d e z i l a m r o N 1.0 0.9 0.8 0.7 0.6 0.5 High Displacement Medium Displacement Low Displacement 0 10 20 30 40 50 60 70 Ratio of Interconnect Area to Total Area (%) Fig. 3: Change in the deﬂection of the SoP with increasing interconnect area. stretching [16]. This has two design drawbacks. First, the effective length of the wire is as long as the worst-case stretching, even when the substrate is in neutral position. Second, the serpentine shape further increases the area allocated for wiring reducing W , hence limits the available bandwidth. Reliability Limitation: Continuous physical deformation of the SoPs can decrease the reliability of ﬂexible circuits and their interconnects as the number of bending cycles increases. For example, after 160,000 bending cycles of ultraﬂexible pentacene Field Effect Transistors, the drain current changes by about 10% [31]. This can cause some portions of the circuit to slow down and trigger timing violations leading to the failure of the device. Moreover, the contacts to the chip pins can break or get permanently deformed if too much stress is applied. Thus, it is important to improve reliability at both material and system level. For instance, structural duplication like redundant interconnects and graceful performance degradation methods can be used for improving the reliability [30]. B. Inter-Chip Communication over Flexible Substrates Potential of short-range wireless transceivers for on-chip wireless communication has been demonstrated in [9, 42]. For example, the work presented in [9] shows that wireless communication is more energy efﬁcient than wired communication for distances over 7mm. As described in Section III-A, the interconnects on ﬂexible and stretchable substrates are more limited compared to on-chip interconnects. Therefore, using wireless communication is even more promising in the context of ﬂexible hybrid electronics. In the following, we present a high-level comparison of energy per bit between a LVDS interfaced channel and a wireless link. We perform this comparison as a function of the data rate to identify the operating regions in which each approach is more energy-efﬁcient. Wired communication energy: On-chip wires generally act as lossy RC transmission lines, while the off-chip wires behave as LC transmission lines due to larger wire lengths [8]. To drive these LC transmission lines, differential techniques like Low Voltage Differential Signaling, Emitter Coupled Logic, and Current Mode Logic are used. These differential techniques, use a pair of electrical signals to transmit data, to minimize the effect of external electromagnetic interference. We choose LVDS, which is a low voltage, low power, differential signaling standard [15]. The power consumption of LVDS based interconnect consists of two components. The ﬁrst component is the LVDS driver power consumption Pdriver , while the second is the power consumption of the channel Pchannel . ) t i b / J p ( y g r e n E 50 40 30 20 10 0 Comparison of Energy per bit  W ired link  W ireless link 1 3 5 7 9 11 13 PLVDS = Pdriver + Pchannel (1) The energy per bit can be obtained by dividing the power in Equation 1 by the data rate DR , Ewired = PLVDS DR (2) For high frequency operations, long LC transmission lines cause channel attenuation, due to the skin effect and dielectric absorption [8]. We adapted a sophisticated channel model and parameters from [7] for a target bit error rate (BER) of 10−12 . Then, we computed the wire parameters assuming a Copper wire on a ﬂexible substrate with the values summarized in Table I. TABLE I: Parameter values for the 0.5oz Copper wires used in this paper. Parameters Values (mm) Height Length Wire spacing 0.018 100 0.152 Wireless communication energy: The energy consumption for wireless transceiver depends on the power consumed in the transmitter and receiver circuits. We can express the power consumption for a wireless link Plink,wireless as the sum of the transmitter power PTx and the receiver power PRx , i.e., Plink,wireless = PTx + PRx . The transmitter power can be further decomposed as the sum of the power consumption of the voltage-controlled oscillator PVCO , frequency synthesizer Psyn and power ampliﬁer PP A . PTx =PVCO + Psyn + PP A =PVCO + Psyn + 1 η γP AL2DR (3) (4) where η is the PA efﬁciency, L is the transmission distance, and γP A is the path loss exponent [38]. Then, the energy consumed per bit can be found by dividing the total transceiver power by the data rate DR as: Elink,wireless = (PVCO + Psyn + 1 η γP AL2DR ) DR + PRx DR = 1 η γP AL2 + (PVCO + Psyn + PRx ) DR (5) We estimated the values in Equation 5 using an existing state-of-the-art on-off keying (OOK) CMOS transceiver design presented in [5]. Data rate (Gbps) Fig. 4: Comparison of energy consumption per bit for wireless and wired links. The two curves intersect at 4.1Gbps. Energy-Efﬁciency Comparison: Figure 4 shows the comparison between the energy consumption per bit as a function of the data rate. The wireless link energy decreases monotonically as the data rate DR increases, since the cost of the voltagecontrolled oscillator and receiver power are amortized over larger data rates. Likewise, the wired link energy decreases slightly when increasing data rate from 1Gbps to 4Gbps due to low channel attenuation of the signal. However, the energy per bit starts increasing after 4Gbps, because the voltage swing needs to be increased to overcome the channel attenuation, and support the required data rate. We observe that the wireless communication becomes more energy efﬁcient, when the data rate exceeds 4.1Gbps. The precise location of the cross-over point is a function of the transceiver and LVDS circuit design, interconnect parameters and the communication distance. However, the trends indicate that a wired or wireless data transmission may be more energy-efﬁcient depending on the data rate requirements. This result shows a great potential for optimizing the communication energy-efﬁciency in FHE systems, since the data rate requirements are typically diverse as observed in NoCs. For instance, the data rate between the sensor hub and SoC in Figure 1 is typically low, while the memory and display interfaces are very demanding. Furthermore, memory trafﬁc itself can show signiﬁcant variations over time as a function of the workload. Consequently, a hybrid combination of wired and wireless transmission strategies can be designed to minimize the communication energy. Moreover, dynamic management techniques can be utilized to switch between wired and wireless interfaces, when the data rate requirement ﬂuctuate over time. IV. POTEN T IA L A P PL ICAT ION AR EA S O F FHE Flexible devices can be worn, have arbitrary shapes to surround objects, and incorporate multiple functions. Moreover, combination with the CMOS technology enables integrated sensing, powerful computation and communication in a truly wearable form factor. Hence, FHE can help in transforming personalized computing by providing a systematic approach to design wearable systems and arbitrarily shaped objects, such as electronic patches. In particular, SoPs equipped with physiological, biochemical, and motion sensing capabilities can be used in wireless body area networks (WBANs), which interconnect a variety of sensing/actuating nodes in or on the body through an energy-aware wireless network [4].       antenna to another device with Bluetooth capability. Since the return loss of ﬂexible antennas, such as a bow-tie antenna, may increase with bending [13], we also tested the ﬂexible antenna on our prototype under different bending scenarios. We conﬁrmed that the center frequency of our inverted-F antenna and characterized the received signal power as a function of bending. This shows promise for future inter- and intra-SoP connectivity using wireless transceivers. V. CONC LU S ION Flexible hybrid electronics technology combines the advantages of traditional rigid ICs and printed electronics. Successful realization of a complete system capable of sensing, computation and communication can become possible with the advent of reliable, energy-efﬁcient and high performance interconnect solutions. In this paper, we overviewed the communication challenges for FHE, discussed potential solutions, and presented promising application areas. Our preliminary results show that a wired link is preferred at low data rates, while wireless links become more energy-efﬁcient when the data rate increases. Hence, a hybrid network of wired and wireless links can be designed as a function of speciﬁc bandwidth requirements. TM TM TM TM TM TM TM TM "
Printed circuits on flexible substrates - opportunities and challenges (invited paper).,"Printed electronics (PE) on flexible substrates is a promising technology for wearables and internet of things (IoT). To implement an integrated system on flexible substrates for applications ranging from medical imaging to disposable thermometers, robust design methodology based on unreliable printed components plays a critical role. This paper reviews robust design of printed circuits based on thin-film transistors (TFT). A novel design style known as Pseudo-CMOS [1], which can tackle several design challenges of TFT circuits, is also introduced. Design examples of Pseudo-CMOS circuits for applications in energy [2], healthcare [3], biomedical [4], and near-field communication (NFC) tags [5], [6] are discussed.","Printed Circuits on Flexible Substrates: Opportunities and Challenges (Invited Paper) Tsung-Ching Huang∗ , Kwang-Ting Cheng† , Raymond Beausoleil∗ ∗Hewlett Packard Labs, Palo Alto, USA Email: tsung-ching.huang@hpe.com †Hong Kong University of Science and Technology, Hong Kong Abstract—Printed electronics (PE) on ﬂexible substrates is a promising technology for wearables and internet of things (IoT). To implement an integrated system on ﬂexible substrates for applications ranging from medical imaging to disposable thermometers, robust design methodology based on unreliable printed components plays a critical role. This paper reviews robust design of printed circuits based on thin-ﬁlm transistors (TFT). A novel design style known as Pseudo-CMOS [1], which can tackle several design challenges of TFT circuits, is also introduced. Design examples of Pseudo-CMOS circuits for applications in energy [2], healthcare [3], biomedical [4], and near-ﬁeld communication (NFC) tags [5], [6] are discussed. Keywords—Printed Electronics, Flexible Electronics, Thin-Film Transistor (TFT) IN TRODUC T ION I . Flexible electronics is a promising technology for wearables and IoT. Flexible electronics can be ”printed” with lowcost high-volume manufacturing such as multi-head ink-jet printing or roll-to-roll imprinting. The non-recursive-expense (NRE) can therefore be reduced by several orders of magnitude compared to that of silicon chip fabrication. This advantage enables many applications such as human body-sized medical image scanner, disposable thermometer for patients carrying infectious diseases, and smart label for temperature-controlled logistics. By connecting heterogeneous components on ﬂexible substrates such as printed sensors, TFT ampliﬁers, and lowpower SOC, together with elastic printed interconnect, circuit and system designers are able to build an integrated system on ﬂexible substrates for use cases that are not achievable today using merely silicon chips due to area or cost constraints. However, the key elements of ﬂexible electronics, thin-ﬁlm transistors (TFTs), have many challenges such as slower operating speeds and poor air stability. Due to material properties, TFTs are usually mono-type either p- or n-type devices. Making air-stable complementary TFT circuits is very challenging [7], [8] or requires heterogeneous process integration of two different TFT technologies [9], [10]. Existing design methodologies for silicon electronics, therefore, cannot be directly applied to ﬂexible electronics. Other factors such as high supply voltages, large process variations, and lack of trustworthy device model also make designing large-scale TFT circuits a signiﬁcant challenge. The objective of this paper is to provide an overview of ﬂexible printed circuits toward building an integrated system on ﬂexible substrates. An overview of Pseudo-CMOS [1] which alleviates the negative impacts of large printing process 978-1-4673-9030-9/16/$31.00 (c)2016 IEEE TABLE I. COM PAR I SON AMONG TFT TECHNO LOG I E S variations to the circuit performance is also included. In the following, we ﬁrst introduce several TFT technologies that are suitable to be fabricated on ﬂexible substrates, as well as design examples for energy, healthcare, biomedical, display, and NFC tag applications [2], [3], [4], [5], [6], [11]. I I . F L EX IBL E TH IN F I LM TRAN S I STOR A. Organic TFT Table I lists different TFT technologies and their key characteristics from circuit and system points of view. Among TFT technologies, amorphous-Si, metal-oxide, and carbon nanotube TFTs show great promise for applications that require higher performance and device uniformity because of its compatibility to wafer-level lithography. On the other hand, ink-jet and shadow-mask printed organic TFT (OTFT) is ideal for lowcost and large-area applications due to its low NRE over a large area. The ink-jet printing process to fabricate ﬂexible organic TFT (OTFT) circuits is illustrated in Figure 1(a) and the microphoto of an ink-jet printed OTFT is shown in Figure 1(b). This OTFT is made with a material ink-jet printer, which can achieve 50-µm resolution, to deposit different functional and liquid-phase ”inks” on the desired locations. Due to the high sensitivity of the semiconducting material (ex. SP220) to the ambient air (ex. oxygen and water vapor), the top-gate structure is applied in which the gate dielectrics is deposited on top of the semiconductor layer and forms a protection Fig. 1. (a) Illustration of printing process using a material ink-jet printer, (b) microphoto of an ink-jet printed organic polymer TFT, (c) device structure of a 2V self-assembly monolayer (SAM) organic TFT, and (d) microphoto of a SAM organic TFT [1]. layer to prevent the semiconductor layer from exposing to the ambient air [12]. Since most soluble materials for ink-jet printing is polymers and the complete ink-jet printing process is accomplished under the room-temperature, the semiconducting (ex. ∼ 0.001cm2/V s) than small-molecule organic materials. material is usually amorphous and has lower carrier mobility On the other hand, the device structure of a 2V self-assembly monolayer (SAM) OTFT using small-molecule organic semiconducting materials (ex. Pentacene) is shown in Figure 1(c), and the device photo is shown in Figure 1(d). The gate dielectric (∼6 nm thick) is accomplished by ﬁrst exposing the gate terminal Al to O2 plasma ashing to grow AlOX (∼4 nm), and the complete sample is treated with the SAM (∼2 nm) solution (n-octadecylphosphonic acid) to provide high gate capacitance while keeping a low gate leakage current. The semiconductor material Pentacene is deposited using the thermal evaporation to achieve a higher carrier mobility (∼ 0.5cm2 /V s). Thanks to the high gate capacitance of SAM dielectric (∼ 600nF /cm2 ), the SAM OTFTs can be operated with a low gatesource voltage VGS at only 2 V, which makes SAM OTFTs capable of using the same supply voltage with CMOS-VLSI (ex. 180nm) for ﬂexible hybrid electronics (FHE). The measured drain current IDS of a 2V SAM-OTFT versus gate-source voltage VGS and drain-source voltage VDS is shown in Figure 2. B. Metal-Oxide TFT Figure 3(a) shows transparent Indium-Gallium-Zinc-Oxide (IGZO) TFTs on a low-cost plastic ﬁlm. Metal-oxide TFTs such as IGO TFTs are emerging as a promising candidate in replacement of hydrogenated amorphous-silicon (a-Si:H) TFTs for display and imaging applications because of the following reasons: 1) higher carrier mobility, 2) better transparency, and 3) better long-term reliability. Compared with a-Si:H TFTs (∼ 1cm2 /V s), metal-oxide TFTs have carrier mobility ∼ 10cm2/V s that enables faster-switching display pixels for a better quality of dynamic images and videos than conventional Fig. 2. (a) Measured drain current (IDS ) versus gate voltage (VGS ) plot and (b) measured drain current (IDS ) versus drain voltage (VDS ) plot of a 2V SAM organic TFT [1]. Fig. 3. (a) Transparent amorphous IGZO TFT on a plastic ﬁlm, and (b) measured I-V curve of a ﬂexible IGZO TFT [1]. a-Si:H TFT-LCDs. It also has higher conduction current to support the next-generation organic LED (OLED) displays that need a higher driving current for a better contrast ratio and dynamic response. Because of the wide bandgap of the semiconductor materials (∼ 3eV), most of the visible light can pass through the device, which makes the metal-oxide such as ZnO and IGZO TFTs transparent to visible light. The I-V curve of six IGZO TFTs fabricated on the same plastic ﬁlm is shown in Figure 3(b). The saturation conduction current (IDSAT ) is around 10 nA and the threshold voltage VTH is around 15 V. Fig. 4. Pseudo-CMOS inverters [1]. (Left) Pseudo-E type, and (right) PseudoD type. increases the input capacitance. These complementary TFT inverters, therefore, usually operate at slower speed or are less reliable in the ambient air. Conventional mono-type digital design styles, such as the diode-load or resistive-load designs, often suffer from high static power consumption and poor noise margin due to their ratioed-logic nature. Digital designs using either depletion-type OTFTs [15] or dual-gate OTFTs [16], [5] have been proposed to increase the static noise margin and reduce the static power consumption at the cost of TFT fabrication complexity. In addition to these proposals, a novel design style, Pseudo-CMOS [1], has been proposed to solve the difﬁculties of digital design with mono-type single-VTH TFTs to reduce the fabrication complexity as shown in Figure 4. The measured inverter transfer function versus the tuning voltage VSS is shown in Figure 5. The small-signal gain of a PseudoD SAM-OTFT inverter can reach ∼20 at 2V supply voltage with -1V VSS. The inverter noise margin and static power consumption are comparable to those characteristics of complementary inverters, while the circuit operation speed can be much faster than that of complementary designs because only fast p-type OTFTs are used [1]. Neither TFT manipulations nor high back-gate tuning voltages are required, thereby reducing the manufacturing cost and process complexity. In addition to inverter, the Pseudo-CMOS NAND gate is illustrated in Figure 6. Measured transfer function of Pseudo-CMOS NAND gate is shown in Figure 7. Moving forward, the oscillation frequency of a ﬁve-stage Pseudo-CMOS ring-oscillator based on 2V SAM organic TFTs is 4.27 kHz [17] and the oscillation frequency is greater than 1MHz for a seven-stage Pseudo-CMOS ringoscillator based on 20V amorphous IGZO TFTs [18]. Pseudo-CMOS frequency divider for an AC energy harvester and an insole pedometer are demonstrated in [2], [3]. With a level-shifting clock buffer for the gate-boost switches, the Pseudo-CMOS frequency divider is able to be operated at 2V supply voltage with 1Hz input, which speed can fulﬁl the need for counting the steps while harvesting the mechanical energy for its own use by a piezoelectric energy harvester. In addition to ﬂexible power meter and pedometer, Pseudo-CMOS is also used in ﬂexible NFC tags targeting internet of things (IoT) applications such as asset tracking [5], [6]. The highest data rate (396.5 kbit/s at 10V and 212 kbit/s at 5V) meets ISO standards (ISO/IEC 15693, 14443, and Sony Felica) for NFC tags which can start functioning at 0.5V supply with 14.3 kbit/s data rate for nominal 10V self-aligned IGZO TFTs. B. Analog Circuit Due to low carrier-mobility, the transconductance gm of an OTFT is usually several orders of magnitude smaller than that of a silicon MOSFET, which limits its signal bandwidth as well the signal gain [19]. The large process variations and the lack of accurate compact model for an OTFT also make designing a high-gain and variation-tolerant OTFT ampliﬁer a signiﬁcant challenge. Alternatively, by including a resistive feedback and an ac-coupling input capacitor with a Pseudo-CMOS inverter, a high-gain 2V organic ampliﬁer can be achieved with SAM-OTFTs [4]. The signal gain is larger than 400 with -1V VSS and 2V VDD. By replacing the input OTFT of the inverter with a FG-OTFT, the inverter trip point can be altered by applying the programming voltage without decreasing the signal gain. The trip-point variations of Pseudo-CMOS inverters was reduced from approximately Fig. 5. Measured inverter transfer functions with 2V SAM OTFTs [13]. (a) Pseudo-E inverter, and (b) Pseudo-D inverter. Fig. 6. Schematics of Pseudo-E and Pseudo-D NAND gates implemented in p-type TFTs. I I I . ROBU S T F LEX IBL E C IRCU I T AND I T S A P P L ICAT ION S A. Digital Circuit Complementary TFT inverters that apply both p- and n-type devices have also been demonstrated using either organic materials [7], [8], carbon nanotube [14], or heterogeneous hybrid integration (ex. metal-oxide and organic) [9], [10] at the cost of fabrication complexity. While p-type OTFTs can often achieve 0.5 cm2 /V s or higher in carrier mobility and are air stable, ntype OTFTs are usually one order or lower in carrier mobility than p-type OTFTs and often become unstable with prolonged exposure to ambient air. To compensate for the lower conduction current of the n-type OTFT, the channel width needs to be much larger than that of the p-type OTFT that inevitably Fig. 7. Pseudo-D NAND transfer function by ﬁxing one input and varying the other input. (a) When input B is kept high and (b) when input A is kept high. The nearly identical curves reveal a good input symmetry of the NAND gate. [8] H. Klauk, U. Zschieschang, J. Pﬂaum, and M. Halik, “Ultralow-power organic complementary circuits,” Nature, vol. 445, no. 7129, pp. 745– 748, Feb. 2007. [9] H. Chen, Y. Cao, J. Zhang, and C. Zhou, “Large-scale complementary macroelectronics using hybrid integration of carbon nanotubes and igzo thin-ﬁlm transistors,” Nat Commun, vol. 5, pp. –, Jun. 2014. [Online]. Available: http://dx.doi.org/10.1038/ncomms5097 [10] K. Myny, S. Smout, M. Rockele, A. Bhoolokam, T. H. Ke, S. Steudel, K. Obata, M. Marinkovic, D.-V. Pham, A. Hoppe, A. Gulati, F. Gonzalez Rodriguez, B. Cobb, G. Gelinck, J. Genoe, W. Dehaene, and P. Heremans, “30.1 8b thin-ﬁlm microprocessor using a hybrid oxideorganic complementary technology with inkjet-printed p2rom memory,” in Solid-State Circuits Conference Digest of Technical Papers (ISSCC), 2014 IEEE International, 9-13 Feb. 2014, pp. 486–487. [11] T.-C. Huang, K. Ishida, T. Sekitani, M. Takamiya, T. Someya, and T. Sakurai, “13.2: A ﬂoating-gate otft-driven amoled pixel circuit for variation and degradation compensation in large-sized ﬂexible displays,” SID Symposium Digest of Technical Papers, vol. 42, no. 1, pp. 149–152, 2011. [Online]. Available: http://dx.doi.org/10.1889/1.3621141 [12] H. Klauk, Ed., Organic Electronics: Materials Manufacturing, and Applications. Wiley-VCH, 2006. [13] T.-C. Huang, J.-L. Huang, and K.-T. Cheng, “Robust circuit design for ﬂexible electronics,” Design & Test of Computers, IEEE, vol. 28, no. 6, pp. 8–15, Nov.-Dec. 2011. [14] H. Wang, P. Wei, Y. Li, J. Han, H. R. Lee, B. D. Naab, N. Liu, C. Wang, E. Adijanto, B. C.-K. Tee, S. Morishita, Q. Li, Y. Gao, Y. Cui, and Z. Bao, “Tuning the threshold voltage of carbon nanotube transistors by n-type molecular doping for robust and ﬂexible complementary circuits,” Proceedings of the National Academy of Sciences, vol. 111, no. 13, pp. 4776–4781, 2014. [Online]. Available: http://www.pnas.org/content/111/13/4776.abstract [15] E. Cantatore, T. C. T. Geuns, G. H. Gelinck, E. van Veenendaal, A. F. A. Gruijthuijsen, L. Schrijnemakers, S. Drews, and D. M. de Leeuw, “A 13.56-mhz rﬁd system based on organic transponders,” Solid-State Circuits, IEEE Journal of, vol. 42, no. 1, pp. 84–92, 2007. [16] K. Myny, M. Beenhakkers, N. van Aerle, G. Gelinck, J. Genoe, W. Dehaene, and P. Heremans, “Unipolar organic transistor circuits made robust by dual-gate technology,” Solid-State Circuits, IEEE Journal of, vol. 46, no. 5, pp. 1223–1230, May 2011. [17] K. Fukuda, T. Sekitani, T. Yokota, K. Kuribara, T. Huang, T. Sakurai, U. Zschieschang, H. Klauk, M. Ikeda, H. Kuwabara, T. Yamamoto, K. Takimiya, K.-T. Cheng, and T. Someya, “Organic pseudo-cmos circuits for low-voltage large-gain high-speed operation,” Electron Device Letters, IEEE, vol. 32, no. 10, pp. 1448–1450, 2011. [18] Y. Chen, D. Geng, M. Mativenga, H. Nam, and J. Jang, “High-speed pseudo-cmos circuits using bulk accumulation a-igzo tfts,” Electron Device Letters, IEEE, vol. 36, no. 2, pp. 153–155, Feb. 2015. [19] N. Gay, W.-J. Fischer, M. Halik, H. Klauk, U. Zschieschang, and G. Schmid, “Analog signal processing with organic fets,” in Solid-State Circuits Conference, 2006. ISSCC 2006. Digest of Technical Papers. IEEE International, 6-9 Feb. 2006, pp. 1070–1079. [20] K. Ishida, R. Shabanpour, B. Boroujeni, T. Meister, C. Carta, F. Ellinger, L. Petti, N. Munzenrieder, G. Salvatore, and G. Troster, “22.5 db openloop gain, 31 khz gbw pseudo-cmos based operational ampliﬁer with a-igzo tfts on a ﬂexible ﬁlm,” in Solid-State Circuits Conference (ASSCC), 2014 IEEE Asian, 10-12 Nov. 2014, pp. 313–316. 400 mV before the programming voltage to 20 mV. With the channel length of 6 µm and 10mV input voltage, the maximum gain is 130 at 50 Hz, and drops to 10 at 1 kHz. The ampliﬁcation for a small piezoelectric signal generated by a ﬂexible PVDF-based touch-sensor sheet from 10 mV to 150 mV is also demonstrated [4]. In addition to the single-ended self-biased ampliﬁer, Pseudo-CMOS can also be used as the active-load to enhance the output impedance for a differential ampliﬁer with measured 22.5 dB open-loop gain and 5.6 kHz bandwidth [20]. IV. CONC LU S ION With the rapid development of ﬂexible TFT technologies such as organic, carbon nanotube, and IGZO TFTs, applications ranging from ﬂexible displays to wearable smart sensors now become feasible. Flexible electronics is also a promising candidate for emerging applications such as internet of things (IoT) and body area network (BAN) due to low NRE over a large area. By hybrid integrating with low-power thinned silicon chips for signal processing and communication, the printed conformable sensor can be easily attached to the uneven surface of the object such as the human body, which enables tremendous opportunities for ubiquitous sensing and computing. With a number of design examples demonstrated using Pseudo-CMOS [1], [2], [3], [4], [5], [6], we believe that many more possibilities for wearables and IoT based on the state-of-the-art ﬂexible TFTs will become ubiquitous and enable a connected network on ﬂexible substrates in the near future. "
The synchronous vs. asynchronous NoC routers - an apple-to-apple comparison between synchronous and transition signaling asynchronous designs.,"Several studies have been made on comparison between synchronous and asynchronous NoCs (Network-on- Chips). However, very few attempts have been made at fair comparison between synchronous NoCs designed by synchronous researchers and asynchronous ones designed by asynchronous researchers under the same functional specification using the same fabrication technology. In this paper, representative routers of both design styles are individually designed under the same conditions. Then, they are precisely compared in the viewpoint of area, latency, power, and fairness. As a result, we show ""areas of specialty"" of both routers, e.g., real-time control systems for asynchronous routers and multi-media data processing systems for synchronous ones.","The Synchronous vs. Asynchronous NoC Routers: An Apple-to-Apple Comparison between Synchronous and Transition Signaling Asynchronous Designs Masashi Imai Thiem Van Chu Kenji Kise Tomohiro Yoneda Hirosaki University Tokyo Institute of Technology Tokyo Institute of Technology National Institute of Informatics miyabi@eit.hirosaki-u.ac.jp thiem@arch.cs.titech.ac.jp kise@cs.titech.ac.jp yoneda@nii.ac.jp Abstract—Several studies have been made on comparison between synchronous and asynchronous NoCs (Network-onChips). However, very few attempts have been made at fair comparison between synchronous NoCs designed by synchronous researchers and asynchronous ones designed by asynchronous researchers under the same functional speciﬁcation using the same fabrication technology. In this paper, representative routers of both design styles are individually designed under the same conditions. Then, they are precisely compared in the viewpoint of area, latency, power, and fairness. As a result, we show “areas of specialty” of both routers, e.g., real-time control systems for asynchronous routers and multi-media data processing systems for synchronous ones. Key Words: NoCs, synchronous routers, asynchronous routers, XY routing, performance and power dissipation comparison. I . INTRODUCT ION NoCs (Network-on-Chips) are recently a hot and popular approach to implementations of many-core systems, and their application to various areas are proposed (e.g., [1], [2]). In the research area of this topic, in addition to traditional synchronous implementations, many asynchronous versions of NoC routers are proposed (e.g., [3–7]). On the other hand, asynchronous NoCs have a wide range of features from robustness to various variations to low power dissipation with high performance, depending on their design styles (i.e., a coding scheme, data-path implementation, signaling protocols, and so on). There are areas in which asynchronous NoCs are more suitable than synchronous ones, and vice versa. So far, however, not only the precise and direct comparison of these two router design styles but also efforts to ﬁnd out suitable application ranges of each one have not been done sufﬁciently, to the best of our knowledge. In this paper, we aim at learning “areas of specialty” of synchronous and asynchronous router design styles by performing the apple-to-apple comparison of them. For this purpose, we have built synchronous and asynchronous versions for the same functional speciﬁcation, architecturally optimized each one separately, and compared them fairly using actual layout results. Design methodology as well as synthesis tool ﬂows for them are not necessary the same, because the most adequate design approaches should be taken for each. Our synchronous router uses one unique chip-wide clock domain, mainly to avoid overhead needed to implement mesochronous or multisynchronous versions. It may not be feasible to design large scale NoCs. However, the size of NoCs evaluated in this paper is not so large, and thus, we consider that one clock domain version is acceptable. As for the asynchronous routers, we choose a design style based on a bundled-data path and a transition signaling protocol [8] in order to pursue high performance with low power dissipation. A typical and rather simple NoC router architecture as shown below is chosen in order to make the comparison and evaluation easier. • The network topology assumed for the routers is a 2D mesh, and the wormhole ﬂow control is used. • The routers have no virtual channels, and the same amount of input buffers. • The routing algorithm used is a dimension-order (XY) routing, where all X-moves are ﬁrst done, which is followed by Y-moves. The destination location is kept in some part of a head ﬂit payload. The evaluation for comparison is done based on intensive simulations of placed-and-routed 4×4 network designs under uniform random trafﬁcs. Two layouts with different router distance (1mm and 2mm) as well as two packet length (4 and 8 ﬂits) are evaluated on the same device technology (130nm bulk CMOS). It is an old technology node, however, the main purpose of this work is a fair comparison between synchronous and asynchronous routers. Thus, the absolute values are not important. What matters is rather relative performance. Generally, the timing margin of asynchronous circuits to tolerate systematic delay variations due to systematic process variations, voltage scaling, and temperature variations can be smaller than that of synchronous circuits since the latter must consider the worst case. On the other hand, in the deep sub-micron technology, when random delay variations due to random process variations, voltage droops, and crosstalks become dominant, the above advantage in asynchronous circuits may be lost. Thus, this work focuses on one particular device technology and tries to perform precise comparison of synchronous and asynchronous designs in the chosen device technology. Especially, an important point to emphasize is that the synchronous and asynchronous routers are separately designed for the same speciﬁcation in competition by a synchronous design team and an asynchronous one. Therefore, it can be claimed that the results of the comparison are fair to some degree and suited for our purpose, i.e., showing “areas of specialty” of both routers based on the apple-to-apple comparison of them. The rest of this paper is organized as follows. Section II refers to some related work. Section III and Section IV explain our synchronous and asynchronous router designs. In each section, several versions of routers by each design style are 978-1-4673-9030-9/16/$31.00 c(cid:13)2016 IEEE designed and compared with preliminary evaluations. Then, both representative routers are actually placed-and-routed, and are compared precisely in Section V. Finally, Section VI concludes our discussion. I I . RE LAT ED WORK Comparisons between synchronous and asynchronous routers are discussed mainly by asynchronous researchers. For example, [4] proposes an asynchronous interconnection network for GALS systems, and a performance comparison between asynchronous/synchronous networks is shown. Our asynchronous routers are similar to theirs, because the transition signaling protocol is used in both works. The network topology used in their work is a Mesh-of-trees, while our work is based on a popular 2D mesh topology. Thus, routing algorithms and complexities of routers/arbiters are very different. In [5], multi-synchronous DSPIN network is compared with asynchronous NoC (ANOC), and in [6], the ANOC is compared with several synchronous counterparts. In these studies, four phase signaling protocol with QDI logics are used for implementing asynchronous routers, which is different from ours. [6] shows the comparisons with respect to cycle times, static and idle power, and so on, but latency or performance comparison in different trafﬁc loads is not shown. Our work may be the most similar to [7], where a transition-signaling bundled data NoC switch is proposed. Our baseline asynchronous router is almost the same as theirs. We, however, design several improved versions of asynchronous routers. Their work also proposes a very interesting circular FIFO, but its impact to performance and area is not shown in [7], while our work includes several related comparisons. I I I . SYNCHRONOU S ROUT ER A. Baseline Design Our baseline is the input-queued router microarchitecture with a single FIFO buffer per input channel. We adopt the conventional wormhole ﬂow control in which routers’ buffers and channels are allocated on a ﬂit-by-ﬂit basis. In addition, at each router, we maintain credit counters for tracking state of the adjacent routers’ buffers. The baseline router is pipelined at the ﬂit level. The fourstage pipeline structure consists of Routing Computation (RC), Switch Allocation (SA), Switch Traversal (ST), and Link Traversal (LT). Only head ﬂits proceed through the ﬁrst two stages RC and SA. The remained two stages are performed for every ﬂit. When the head ﬂit of a packet arrives at a router, stage RC is performed to determine the output port to which the packet is passed. After that, the packet sends a request for the output port to the switch allocator. The switch allocator assigns each available output port to a requester. We currently use n arbiters where n is the number of output ports to implement the switch allocator. Since multiple packets may contend for the same output port, the packet may fail to get access to its desired output port. In this case, it has to wait until the output port becomes available to perform stage SA again. Once stage SA is completed successfully, each ﬂit of the packet traverses the crossbar switch and the output link towards the next router at stage ST and stage LT respectively. B. Pipeline Optimizations The router pipeline delay directly affects the overall latency of the network. The minimum number of cycles that it takes each head ﬂit to traverse a router is equal to the number of pipeline stages. Additional delay may arise due to the contention at stage SA. In general, a signiﬁcant performance gain can be obtained if the router pipeline depth can be reduced with only a slight impact (or without any impact) on the delay of each stage. Thus, we perform several pipeline optimizations described below. At the baseline design, the routing logic performs the routing computation for a packet after the head ﬂit has been written into the input buffer. However, because we use dimensionorder routing, a simple deterministic routing algorithm, the routing computation is very fast. Therefore, we eliminate stage RC and calculate the output port of a packet at the same time the head ﬂit arrives at the router. By this way, the number of pipeline stages is reduced from four to three. The pipeline depth can be further reduced by performing two stages SA and ST in one clock cycle. Head ﬂits traverse the crossbar switch in parallel with the switch allocation. At each crossbar output, a ﬂit is selected if and only if it has been granted by the switch allocator. Flits that failed the switch allocation are discarded. Because the switch allocation and the selection at the crossbar outputs are performed in series, the clock cycle may be slightly increased if the switch allocator is the critical path of the design. Nevertheless, a signiﬁcant improvement in the network performance may be achieved because the router pipeline depth is reduced to just two-stage. C. Buffer Allocation An input FIFO buffer can be allocated in two ways: atomic and regular. In the atomic allocation method, an input FIFO buffer is occupied by only one packet at any given time. The buffer cannot be reallocated until the tail ﬂit of the packet currently occupying the buffer departs. On the contrary, the regular allocation method allows multiple packets to be in the buffer at the same time. In particular, the buffer can be reallocated to a new packet once the tail ﬂit of the previous packet arrives and there is still at least a free space in the buffer. On a router that employs virtual channel ﬂow control [9], the atomic allocation method can provide better performance than the regular one does when the number of FIFO buffers per input channel is large, because the head-of-line blocking is eliminated. However, in our case, each input channel of the baseline router is associated with only one FIFO buffer. In addition, we use a deterministic routing algorithm. Therefore, the regular allocation method outperforms the atomic one. One simple way to implement the regular allocation method is to perform the routing computation at the front (output side) of the FIFO buffer, whereas it is performed at the rear (input side) of the FIFO buffer in the atomic allocation method. This can maintain simplicity of stage RC in the sense that only one packet should be handled for the routing computation in the input port controller. However, performing the routing computation at the front of the FIFO buffer (for the regular one) makes it impossible to eliminate stage RC for reducing the pipeline depth unlike the atomic one (see Section III-B), because the routing computation needs to start after the head 0 0.000 40 80 120 160 200 0.005 0.010 0.015 0.020 0.025 Packet Injection Rate (packets/node/cycle) 0.030 0.035 A e v r e g a P e k c a t L a t y c n e ( c y c l ) s e 4stage-atomic 3stage-atomic 3stage-regular 2stage-regular Fig. 1. Architectural evaluation of four synchronous NoC designs. ﬂit has been stored in the FIFO buffer. In general, in order to hide this routing computation delay in the regular allocation method, the input port controller needs to handle multiple packets for the routing computation. An approach that we adopt in this study is to perform the routing computation of every packet when it arrives at the input port, and maintain the routing computation results of all packets currently occupying the FIFO buffer. Although this incurs some overhead, its controller can be implemented rather straightforwardly (i.e., no re-computation of the routes is needed). D. Architectural Evaluation In this section, we describe an architectural evaluation of four 8×8 networks, each with one of the following four synchronous router designs. • 4stage-atomic. This is our baseline design which has been described in Section III-A. The baseline design employs the atomic buffer allocation method. • 3stage-atomic. The number of pipeline stages is reduced to three by performing the routing computation for a packet at the same time the head ﬂit is stored into the input buffer. This design also uses the atomic buffer allocation method. • 3stage-regular. This design is the same as 3stage-atomic, except that the regular buffer allocation method is used. • 2stage-regular. The pipeline depth is further reduced to two stages as described in Section III-B. Like 3stageregular, this design uses the regular buffer allocation method. Figure 1 shows the average packet latencies (in cycles) of the four NoC designs under uniform random trafﬁc. For each simulation, we simulate 100,000 warm-up cycles, 100,000 measurement cycles, and a drain phase. The average packet latency is calculated based on latencies of all packets generated during the measurement phase. Figure 1 shows that, at the architectural level, the design with two pipeline stages and regular buffer allocation (2stage-regular) outperforms the others. Although the operating frequency of this design may slightly decrease compared to the baseline due to the pipeline optimizations and the complexity of the regular allocation method as discussed in Section III-B and Section III-C, the network performance can be signiﬁcantly improved because the number of delay cycles at each router is reduced. Therefore, we choose 2stage-regular as the representative synchronous router to compare with asynchronous designs. E. Frequency Optimizations When the 2D mesh topology together with dimension-order (XY) routing are used, we can simplify the implementation of the switch allocator and crossbar switch, and thereby improving the operating frequency of the design, based on the following observation. A packet at the north or the south input port will not go to the east and the west output ports because the packet is routed ﬁrst in the X-dimension and then in the Ydimension to reach its destination. Using this observation, for example, we can reduce the number of inputs of two arbiters (for the east and the west output port) of the switch allocators. Although such optimizations do not directly help to shorten the router critical path, they result in a simpler circuit which may be faster than the original one thanks to area reducing and place-and-route easiness. IV. A SYNCHRONOU S ROUT ER A. Baseline Router In order to achieve higher performance with lower power dissipation, we design our asynchronous router using bundleddata method with transition signaling protocol [8]. Our asynchronous router consists of ﬁve input-channels (ICs), ﬁve cross-bar inputs (CBINs), and ﬁve cross-bar outputs (CBOUTs) as shown in Fig. 2. Each component in the baseline design is implemented as follows. 1) An IC gets the ﬂits sent to its input port, and puts them into the input queue. This queue is just an asynchronous linear FIFO constructed by four MOUSETRAP [10] pipeline stages. Since multiple packets can enter an IC, its allocation method is classiﬁed to be “regular.” 2) Figure 3 shows the details of a CBIN. When a CBIN receives a head ﬂit from its IC, it performs the routing computation (RC), and sends a request (arb req) to an arbiter in the CBOUT decided by the RC. Since the “RC comp. unit” is just a combinational circuit IC CBIN IC CBIN IC CBIN IC CBIN IC CBIN U O B C T I o P n t r 0 O u t o P t r 0 O u t o P t r 1 O u t o P t r 2 O u t o P t r 3 O u t o P t r 4 I o P n t r 1 I o P n t r 2 I o P n t r 3 I o P n t r 4 U O B C T U O B C T U O B C T U O B C T Fig. 2. Asynchronous router architecture.                                               R T M G R arb_done idata head(idata) current location arb_req to  CBOUT arbiters Data to  CBOUTs req to  CBOUTs ack from  CBOUTs arb_grant from  CBOUT arbiters D R T E M X U ireq ireq_d oack cur en1 en2 sel TR-gate RC comp. ireq_d : close @(reset)              open @(posedge arb_done)              close @(bothedge ireq_d) if tail(idata) = 1 Fig. 3. Implementation of a CBIN. for the XY routing, it may produce glitches before its output becomes stable. Thus, the output is gated by an enable input en1, which is obtained by delaying “head(idata)”, where head(d) becomes 1 only when signal d carries a head ﬂit (tail(d) is deﬁned similarly). “RC comp. unit” has another enable input en2 to be activated only by valid head ﬂits. ireq ⊕ oack can be used for this purpose. The head ﬂit should be held in the CBIN until the grant arb grant is given from the arbiter. This is done by gating the request signal ireq using a TR-gate, which can be implemented by a D-latch. When the grant is obtained, the TR-gate is opened by “@posedge(arb done)”, and ireq d is forwarded to the corresponding CBOUT through a TRDEMUX. The control circuits of the TR-gate have the function shown in the red box in Fig. 3. Although it can actually be implemented using multiple-clock multipleedge-triggered ﬂipﬂops [11], the details are omitted here. A TR-DEMUX is a DEMUX for the transition signaling protocol, and is implemented by both-edge sensitive toggle FFs with enable inputs. Note that the multipleclock multiple-edge-triggered ﬂipﬂops are only used for control blocks. The data-path block is implemented using the normal D-latches. When an acknowledgement is sent from the CBOUT, it is forwarded to the IC as oack. TR-MRG is just an exclusive-OR, and is used to merge transition signaling inputs. When oack of the head ﬂit is asserted, the request (arb req) to the arbiter is released, but arb grant is kept by a latch in CBOUT (see below). The following data and tail ﬂits are just passed from the IC to the CBOUT, because the TRgate is kept open. When ireq d of a tail ﬂit is sent to the CBOUT through TR-DEMUX, the TR-gate is closed by “@(bothedge ireq d) if tail(idata)=1” for the preparation of a head ﬂit of next packet. 3) A CBOUT has a four-input asynchronous arbiter [12]. The grant outputs of the arbiter are latched by a LevelLT (normal D-latches), and the latched grant outputs arb grant are sent back to CBINs. A CBOUT has a (normal) multiplexer in the data-path (its output is mdata) and it is controlled by arb grant, i.e., the data from the CBIN that has a grant is selected. This forms a cross-bar. The arbiter latch is initially open, and is closed when a head ﬂit arrives at mdata. This is ireq oack ireq oreq iack oreq icredit odata mdata oack 4 input  arbiter  Level-LT   Mousetrap  pipeline  stage Credit controller M X U R T M G R D R T E M X U Data from  CBINs arb_req from  CBINs arb_grant to  CBINs req from  CBINs ack to  CBINs arb_grant : open @(reset)                    close @(bothedge ireq) if head(cdata) = 1                    open @(bothedge oack) if tail(cdata) = 1 Fig. 4. Implementation of a CBOUT. because arb req is released after the head ﬂit goes through as mentioned previously. It is opened again for the next packet when the tail ﬂit acknowledged. A CBOUT also has one pipeline stage connected to the output port. Its request input is obtained by merging the request signals from four CBINs, and its acknowledgment output is distributed to four CBINs selectively through TR-DEMUX based on arb grant. “Credit controller” transforms the req/ack protocol to the credit-based ﬂow control protocol. in mdata is B. Adding a Pipeline Stage in CBINs The second version of our router has one MOUSETRAP pipeline stage within each CBIN in order to hide the computation time of “RC comp. unit.” This corresponds to “2 stage-regular” synchronous router. This design change requires one modiﬁcation in CBINs, in addition to the insertion of the pipeline stage. As shown in Fig. 5, a LVL-gate (just AND gates) is inserted for obtaining arb req signals. It is initially open, and is closed during a tail ﬂit is held in the pipeline stage, i.e., it is closed when a tail ﬂit is acknowledged by the pipeline stage, and is opened when it is acknowledged by the CBOUT. During this period, even if a head ﬂit arrives at this R T M G R arb_done ireq_d : close @(reset)              open @(posedge arb_done)              close @(bothedge ireq_d) if tail(idata) = 1 idata cdata head(idata) current location arb_req to  CBOUT arbiters Data to  CBOUTs req to  CBOUTs ack from  CBOUTs arb_grant from  CBOUT arbiters D R T E M X U ireq ireq_d oack arb_req : open @(reset)                 close @(bothedge oack) if tail(idata) = 1                 open @(bothedge iack2) if tail(cdata) = 1 i 2 k c a o 2 q e r i q e r o q e r k c a o i k c a cur en1 en2 sel TR-gate Mousetrap  pipeline stage LVL-gate RC comp. Fig. 5. Modiﬁcation of CBIN. CBIN, it must not try to acquire the arbiter. V. COM PAR I SON C. Some Optimizations As mentioned in Section III-E, from the nature of XY routing, the east (west, resp.) CBOUT receives arb req only from the west (east) and its own core’s CBINs. Thus, 2-input arbiter, which is just an ME (Mutual Exclusion) element, can be used for those CBOUTs. For the same reason, we can use smaller MUXs and TR-MRGs. They directly lead to performance improvement in asynchronous routers. Furthermore, we also implement a circular FIFO proposed in [7] for the input queue in IC. As shown later, it improves the router latency in low load, because the input data can go to the output directly unlike a linear FIFO. However, the performance is reduced in high load due to the overhead for buffer selecting controllers. We know that rigorous performance analysis and optimization of asynchronous circuits are quite complex. Further optimization of our asynchronous router is in the scope of our future work. D. Evaluation In order to compare the above router designs, the logicsynthesized designs of a 4×4 mesh network are simulated. The routers used for the comparison are as follows. The ﬁrst version is our baseline with no pipeline stage, and is denoted by “No pipe.” The “Pipe” version has a pipeline stage in CBINs. The third version is similar to the second one except that it uses the optimized arbiters and multiplexers in east and west CBOUTs with linear FIFOs for the input queue in IC. Finally, the fourth version is obtained from it by just replacing the linear FIFOs with the circular FIFOs. These versions are denoted by “Linear FIFO” and “Circular FIFO.” The simulation is performed under uniform random trafﬁc, and uses 50,000 warm-up cycles, 50,000 measurement cycles, and a drain phase. Figure 6 shows the results. Figure 6 (a) shows the average packet latency when the load is light. It is shown from these results that the effect of the pipelining is large. From Fig. 6 (a), we can see that the arbiter and multiplexer optimization improves the light-load performance. As mentioned above, the circular FIFO can improve the router latency in low load, while the performance is reduced in high load as shown in Fig. 6 (b). Thus, we choose the third and the fourth versions as the representative asynchronous routers to compare with synchronous designs.  24  23  22 ] s n  21 [ No pipe Pipe Linear FIFO Circular FIFO  500  400 ] s n [ No pipe Pipe Linear FIFO Circular FIFO  20 y c n e a t  300 y c n e a t l t i l f  19 e g a  18 r e v A  17  16  15  14  0 l t i l f e g a  200 r e v A  100  2  4  6 Packet injection rate [MHz]  8  10  0  0  5  10  15  20  25  30 Packet injection rate [MHz]  35  40  45 (a) (b) Fig. 6. Comparison of logic-synthesized designs of four routers. A. Physical Design In this section, the representative routers of synchronous and asynchronous designs are compared using the 130nm bulk CMOS technology. The size of NoC is 4 × 4. The distance between routers, which represents the distance between network interfaces of IP cores, is assumed as 1mm and 2mm. Note that the latter is an enough space to place a 32-bit processor core with 64KB memory between routers. At ﬁrst, basic cells for asynchronous design styles, i.e., MUTEX cells, Muller’s C-elements, and multiple-clock multipleedge-triggered ﬂipﬂops are designed and characterized. In the technology mapping design phase, both synchronous and asynchronous routers are synthesized by the Synopsys Design Vision. In asynchronous routers, hard macro blocks are also used since the proposed design style using multipleclock multiple-edge-triggered ﬂipﬂops is not supported by the commercial synthesis tools. On the other hand, in synchronous routers, the clock-gating optimization by the Synopsys Design Vision is performed in order to reduce total power. In this section, synchronous routers with/without the clock-gating optimization are evaluated. In the placed-and-routed design phase, a ﬂoorplan shown in Fig. 7 is imposed. Dummy I/O ports from/to IP cores are assigned to speciﬁed adjacent router regions represented by black boxes at a distance of 1mm (or 2mm). Then, all the distance: 1mm, 2mm router region dummy-I/O region Fig. 7. Floorplans of synchronous and asynchronous NoCs. cells of each router are assigned to the corresponding router regions as shown in Fig. 7. The utilization of cell area is speciﬁed as 0.75. In asynchronous routers, several place-androute optimization options in the Cadence EDI are disallowed. For example, deleteInst and downsizeInst options are disallowed since buffer cells which serve a matched delay must keep the size. After the place-and-route process is performed, a Verilog netlist ﬁle and an SDF (Standard Delay Format) ﬁle which contains delay information are generated by the tool. Finally, simulations using the placed-and-routed information are performed by the Synopsys VCS digital simulator. In synchronous routers, in the placed-and-routed design phase, timing constraints speciﬁed by a clock cycle are imposed from 2.2ns to 2.7ns by 0.1ns step in 1mm distance, from 2.4ns to 3.0ns by 0.1ns step in 2mm distance, respectively. Then, all the results are evaluated using the Synopsys VCS as described in the following sections. Finally, the fastest result is selected and compared with the asynchronous counterpart. B. Cell Area Comparison At ﬁrst, the cell area of 4 × 4 NoC is compared. Table I shows the cell area of synchronous and asynchronous NoCs.             Interestingly, the clock-gating optimization reduces the area of combinational circuits of synchronous routers. This is done purely by the synthesis tool, and the reason is not so clear. The table also shows that the area overhead of circular FIFOs for the asynchronous routers is very large. This is because the circular FIFO requires 5 four-to-one multiplexers whose area is almost the same as a crossbar. TABLE I C E L L A R EA O F S YN CH RONOU S AND A S YN CH RONOU S N OC S [µm2 ] . Synchronous without with gated-clk gated-clk 469747 305791 432727 439610 Asynchronous Linear Circular FIFO FIFO 234044 507092 353349 380568 87208 87208 674601 974868 234715 508403 353349 380568 87208 87208 675272 976179 235836 517729 353349 380568 87208 87208 676393 985505 Synthesized cmb. non-cmb. macro sum cmb. non-cmb. macro sum cmb. non-cmb. macro sum 902474 782347 433339 745401 534905 440119 1mm place &route 1215686 712054 432773 975024 559544 440011 2mm place &route 1144827 999555 cmb.: combinational circuits non-cmb.: non combinational circuit: ﬂipﬂops, latches In synchronous designs, the cell area increases after the placed-and-routed design phase since the placed-and-routed tool inserts clock tree buffer cells and signal buffer cells in order to satisfy the timing constraints. On the other hand, in asynchronous designs, the cell area slightly increases since the tool only inserts signal buffer cells to places where output load constraints of driver cells are not satisﬁed. As a result, in 1mm placed-and-routed designs, the cell area of synchronous NoC with the clock-gating optimization is about 1.44 times larger than that of the asynchronous linear FIFO NoC. Then, it is almost the same as that of the asynchronous circular FIFO NoC. On the contrary, the cell area of synchronous NoC without the clock-gating optimization is about 1.80 and 1.25 times larger than those of the asynchronous linear and circular FIFO NoCs, respectively. C. Latency Comparison The clock cycle of 1mm and 2mm placed-and-routed synchronous designs is speciﬁed as follows. First, the STA (Static Timing Analysis) is performed and the delay value of a critical path is obtained. Then, the clock cycle is set as the critical path delay and all the simulations, each with 50,000 warmup cycles, 50,000 measurement cycles, and a drain phase, are performed under uniform random trafﬁc. In our testbench, the correctness of transferred packets as well as the existence of lost packets are checked. Thus, if simulation results are all passed, the clock cycle is decreased by 0.01ns and the above steps are repeated until it violates the timing constraints or the testbench ﬁnds some errors. If the default clock cycle does not satisfy the above conditions, the clock cycle is increased until it satisﬁes them. As a result, in this evaluation, the clock cycles in 1mm and 2mm placed-and-routed designs without the clock-gating optimization are speciﬁed as 2.37ns and 2.74ns. Then, those with the clock-gating optimization are speciﬁed as 2.43ns and 3.37ns, respectively. For consistent comparison with synchronous design, these cycles are also used for specifying the warm-up and measurement duration in asynchronous designs. Figure 8 (a) and Fig. 9 (a) show the maximum link delay between synchronous routers. Figure 8 (b) and Fig. 9 (b) show the major path delays (larger than 2.2ns) between modules in the router. These analyses are performed in order to show the critical path in synchronous NoCs. Note that ﬁnding longest paths in asynchronous routers is not so useful since many paths including shorter paths affect their performance. IPn, OPn, and Out denote an input port control module, an output port control module, and an output buffer module, respectively. In Fig. 8 and Fig. 9, the maximum value is represented as a large italic font. In Fig. 9 (a), rn denotes “from rising edgetriggered ﬂip-ﬂop to negative level-sensitive latch,” where the negative level-sensitive latches are used for clock-gating. These times represent half-clock-cycle. Thanks to the time borrowing technique performed by the Cadence EDI, it is sufﬁcient to require that they should be less than the given cycle time. Consequently, as shown in Fig. 8 and Fig. 9, the link delays of the NoC are lower than the critical path delay inside the router since wire delays are not dominant in the 130nm process technology. For this reason, link pipelining is not needed for our comparison. On the other hand, the clock cycle required for the proper operation in 2mm placed-androuted design with clock-gating (3.37ns for 2mm design) is larger than the STA analysis (2.35ns as shown in Fig. 9). It is in the scope of future work to clear its reason and adjust the appropriate clock cycle. In this paper, however, the evaluation results under the above conditions are presented in R00 R10 R20 R30 R01 R11 R21 R31 R02 R12 R22 R32 R03 R13 R23 R33 2.29 2.47 1.89 2.35 2.43 2.37 2 . 4 2 1 . 9 6 2 . 9 3 2 . 7 2 2 . 5 2 1 . 5 3 1 . 9 8 1 . 8 6 1 . 1 7 2 . 6 3 2 . 9 2 1 . 3 7 1 . 9 8 2 . 0 3 1 . 2 6 1 . 5 6 2 . 0 3 1 . 8 7 2 . 0 3 1 . 6 5 1 . 7 4 1 . 5 3 2 . 4 3 1 . 8 7 2.36 2.55 2.00 2.34 2.36 2.34 2.27 2.26 2.36 2.41 1.96 1.95 2.45 2.44 1.82 2.39 2.38 2.35 IP4 2.63 Out IP0 OP0 IP1 OP1 OP2 IP2 IP3 OP3 OP4 2.53 2.37 2.50 2.42 2.35 2.65 2.70 2.67 2.65 2.60 2.67 2.61 2.59 2.27 2.55 2.46 2.22 2.54 2.42 R12 2.50 R03 2.62 2.20 2.60 2.41 2.42 R01 2.27 2.25 2.25 2.31 2.46 2.53 2.70 (a) Link delay (b) Major path delay in the critical path router (R02) Fig. 8. Path delay analysis (w/o gated clock, 2mm distance). R00 R10 R20 R30 R01 R11 R21 R31 R02 R12 R22 R32 R03 R13 R23 R33 1.80 1.82 1.83 1.78 1.81 9 0 2 . 8 6 1 . 0 9 1 . 8 3 1 . r 7 3 1 n . r 5 6 1 n . r 9 3 1 n . r 9 3 1 n . 0 9 1 . 0 8 1 . 0 1 2 . 8 6 1 . 9 8 1 . 5 8 1 . r 4 4 1 n . 3 4 1 . r 5 6 1 n . 5 7 1 . r 6 4 1 n . r 9 5 1 n . 0 5 1 . r 1 6 1 n . 5 3 1 . 1 7 1 . 1.71 1.79 1.76 1.66 1.96 1.77 1.80 1.82 1.76 1.81 1.93 2.12 1.79 1.96 1.81 1.79 2.06 1.93 rn1.78 IP4 Out IP0 OP0 IP1 OP1 OP2 IP2 IP3 OP3 OP4 2.41 2.35 2.32 2.27 2.32 2.32 2.31 2.34 2.34 2.33 rn1.78 2.33 2.31 2.32 2.30 2.26 2.29 2.29 2.27 2.28 2.27 2.30 2.28 2.25 2.25 2.25 2.25 2.26 2.25 2.25 2.28 2.26 R10 R01 2.09 (a) Link delay (b) Major path delay in the critical path router (R00) Fig. 9. Path delay analysis (with gated clock, 2mm distance). order to show the performance tendency of synchronous and asynchronous routers. Table II shows latencies from an input port to an output port of a router. In synchronous routers, the latency is calculated as two times clock cycle time since the representative router is two stage routers. On the other hand, in asynchronous routers, the latency is evaluated as the average latency when there is no trafﬁc congestion. As shown in Table II, in almost zeroload situations, the representative asynchronous circular FIFO router is the fastest. TABLE II L AT EN CY O F A ROU T ER [ns] . Synchronous without with gated-clk gated-clk 4.74 4.86 5.48 6.74 Asynchronous Linear Circular FIFO FIFO 4.84 4.22 5.13 4.43 1mm place&route 2mm place&route Figure 10 and Fig. 11 show the average ﬂit latency at a distance of 1mm and 2mm, respectively, where the ﬂit number of one packet is assumed as 4 and 8. All these results are calculated in the Verilog testbenches. The horizontal axis represents the average packet injection rate. Note that the actual packet injections happen in randomly varied intervals such that their average matches the given rate. The average ﬂit latency is calculated based on latencies of all the ﬂits generated during the measurement phase.  20  0  25  30  35  40  45  50  2  4  6 Packet injection rate [MHz]  8  10 A e v r e g a t i l f l a t y c n e [ s n ] Sync w/o gated-clock 8flits Sync w gated-clock 8flits Async (Linear FIFO) 8flits Async (Circular FIFO) 8flits Sync w/o gated-clock 4flits Sync w gated-clock 4flits Async (Linear FIFO) 4flits Async (Circular FIFO) 4flits (a)  0  0  100  200  300  400  500  10  20  30  40 Packet injection rate [MHz]  50 A e v r e g a t i l f l a t y c n e [ s n ] (b) Fig. 10. Average ﬂit latency at 1mm distance. Sync w/o gated-clock 8flits Sync w gated-clock 8flits Async (Linear FIFO) 8flits Async (Circular FIFO) 8flits Sync w/o gated-clock 4flits Sync w gated-clock 4flits Async (Linear FIFO) 4flits Async (Circular FIFO) 4flits  20  0  25  30  35  40  45  50  55  60  2  4  6 Packet injection rate [MHz]  8  10 A e v r e g a t i l f l y c n e a t [ s n ] (a)  0  0  100  200  300  400  500  10  20  30  40 Packet injection rate [MHz]  50 A e v r e g a t i l f l y c n e a t [ s n ] (b) Fig. 11. Average ﬂit latency at 2mm distance. We can see that, in almost zero-load cases, the representative asynchronous circular FIFO routers outperform a little the synchronous routers. In 2mm distance layout, both the asynchronous routers are slightly better than the synchronous ones. However, in high load cases, the asynchronous routers suffer from the saturation earlier than the synchronous routers. We have the following observations about this fact. • In asynchronous designs, in almost zero-load situations, ﬂits can go forward almost without being blocked. The forward latency of the MOUSETRAP asynchronous pipelines is very small, because the D-latches are open when the pipelines are empty. Furthermore, especially in the circular FIFO structure, the asynchronous buffers which are also constructed by MOUSETRAP pipelines can propagate ﬂits from their inputs to outputs quickly if they are empty. On the other hand, the synchronous router always uses one clock to store a ﬂit in an FIFO, even if it is empty. The clock period is also decided for handling the most complicated functions. Therefore, the asynchronous routers have better performance in those situations. • In asynchronous designs, in high load situations, almost every pipeline stage is ﬁlled, and ﬂits move only when an upstream stage gets space. Thus, it can be viewed as a “hole” goes backward. The MOUSETRAP pipelines have rather large backward latency. Also, in asynchronous designs, these holes move step-by-step according to the req/ack protocol, while the synchronous designs can propagate the information of the hole to several downstream stages in the same clock period, which makes it possible for those stages to restart simultaneously. We consider that the superiority of synchronous routers in high load situations comes from these facts. The similar observations may explain the reason why the synchronous routers can handle longer packets efﬁciently. D. Power Comparison Generally, power dissipation strongly depends on the switching activity of all the cells. Thus, in this paper, SAIF (Switching Activity Interchange Format) is used to calculate the power of synchronous and asynchronous NoCs. The simulation is performed under uniform random trafﬁc, and uses 50,000 warm-up cycles and 5,000 measurement cycles. Similarly, the power dissipation without packet injection is also evaluated. In asynchronous designs, it represents the leakage power evaluation since no transition occurs. On the other hand, in synchronous designs, it represents dynamic power of global clock signal distribution and leakage power. Figure 12 shows the total power dissipation of NoCs. In both synchronous and asynchronous designs, the power dissipation is almost proportional to the average packet injection rate when it is lower than the saturation rate. In addition, it is saturated in higher load situations. In the zero-load situation, the asynchronous routers consume almost no power, while the synchronous routers without clock-gating optimization consume about 211 (178) mW in 1mm (2mm, resp.) distance layout due to the global clock signal. Note that the total power of the longer link (2mm) version is smaller than that of the shorter link (1mm) version. This seems counter intuitive, but we consider that this is caused by the fact that the clock frequency of the former is smaller than the latter. This power consumed in the synchronous routers is reduced to 69.2 mW and 52.2 mW by the clock-gating optimization. Obviously, the clock-gating optimization is effective to reduce power while the peak performance of the clock-gating synchronous NoCs                         is lower than that of the non-clock-gating synchronous NoCs shown as Fig. 10 and Fig. 11. The fact that the total power of asynchronous NoCs is lower than that of synchronous NoCs is a large advantage of asynchronous routers.  350  300  250 ] Sync w/o gated-clock 8flits Sync w gated-clock 8flits Async (Linear FIFO) 8flits Async (Circular FIFO) 8flits Sync w/o gated-clock 4flits Sync w gated-clock 4flits Async (Linear FIFO) 4flits Async (Circular FIFO) 4flits  350  300  250 ] W m  200 r [ e W m  200 r [ e w o p w o p  150 l a  150 l a t o T  100  50  0  0 t o T  100  50  0  0  10  20  30 Packet injection rate [MHz]  40  50  10  20  30 Packet injection rate [MHz]  40  50 (a) 1mm distance (b) 2mm distance Fig. 12. Total power of NoC. E. Fairness Comparison The representative synchronous router uses a simple ﬁxedorder arbiter to achieve higher performance, while the asynchronous router uses a fully-asynchronous arbiter. This may cause some differences in arbitration fairness between two routers. For this evaluation, the variation of average ﬂit latency of each node is measured for different injection rates. The injection rates relative to the one that causes an NoC to be saturated are used in this evaluation for the fair comparison. That is, the injection rate that results in about 1000ns as the average ﬂit latency is obtained for each router, and the injection rates with various ratios to that value are considered. The packet size is ﬁxed to 4 ﬂits, and 1mm distance layout is used. Figure 13 shows the results. In this ﬁgure, the variation is represented by error-bars, i.e., the maximum, average, and minimum values of average ﬂit latency among 16 nodes are shown as a bar. From these results, it can be seen that the fairness of the synchronous and asynchronous arbiters is almost the same under the case where the relative injection rate is lower than 75%. However, in higher rates, the synchronous arbiter causes larger variations compared to the asynchronous one. Thus, if fairer arbitration is required, the synchronous router needs to use more complicated arbiters, which may lead to less performance.  350 Sync w/o gated-clock 4flits Sync w gated-clock 4flits Async (Linear FIFO) 4flits Async (Circular FIFO) 4flits  300 ] s n [  250 e d o n h c a e f o  200 y c n e a t  150 l t i l f e g a  100 r e v A  50  0  0  20  40  60 Ratio to saturation rate [%]  80  100 Fig. 13. Average ﬂit latency of each node. V I . CONCLU S ION We have designed a 2stage-regular synchronous NoC router and two corresponding asynchronous NoC routers based on the transition signaling protocol under the same functional speciﬁcation using the same fabrication technology, and precisely compared them in the viewpoint of area, latency, power, and fairness. The results show that the representative asynchronous routers have slightly better performance than the synchronous one in almost zero-load situations, while the synchronous router outperforms the asynchronous ones in high load situations. As for power and area, the asynchronous NoCs have great potential compared to synchronous NoCs. From these features of asynchronous and synchronous routers, we can consider their “areas of specialty.” Asynchronous NoCs are more suitable for applications where short packets should be quickly propagated rather sparsely within limited power budgets. Real-time control systems may be those applications. Synchronous NoCs can handle continuous large data packets efﬁciently, and thus, are suited for multi-media data processing and so on. It is in the scope of our future work to perform similar comparisons for routers with more complicated router architectures and routing algorithms. ACKNOW L EDGMENT This work was partially supported by JSPS KAKENHI Grand Numbers JP15K00179, JP15H02254, and JP15K12005. We thank Takahiro Hanyu and Hiroshi Saito for technical discussions. This work is also supported partially by VLSI Design and Education Center(VDEC), the University of Tokyo in collaboration with Synopsys, Inc and Cadence Design Systems, Inc. "
Toward exa-scale photonic switch system for the future datacenter (invited paper).,"This paper reviews various optical switch technologies, then discusses how to realize an optical circuit switching interconnect capable of more than 10 Tbps link bandwidth and more than 100,000 end points scalability for data centers or high performance computers. Introducing the wavelength division multiplexing transmission technology and a simple distributed-like control scheme of optical switches is proposed, with the associated challenges discussed.","Toward Exa-scale Photonic Switch System for the  Future Datacenter (Invited Paper) Kiyo Ishii and Shu Namiki Electronics and Photonics Research Institute National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Japan Email: kiyo-ishii@aist.go.jp Abstract—This paper reviews various optical  switch  technologies, then discusses how to realize an optical circuit  switching interconnect capable of more than 10 Tbps link  bandwidth and more than 100,000 end points scalability for  datacenters or high performance computers. Introducing the wavelength division multiplexing transmission technology and a  simple distributed-like control scheme of optical switches is  proposed, with the associated challenges discussed. Keywords—Optical path, inteconnect, wavelength label, optical  comb distribusion INTRODUCTION I. To keep continuous performance improvement of high  performance computers (HPCs) and dater centers (DCs), high  capacity and low latency interconnect network is essential. One  of the current main driving forces of the performance  improvement is the increase in the number of parallel  processing. Regarding flagship HPCs appearing on TOP 500, the number of CPU cores exceeds 3 millions and the number of  nodes reaches almost a hundred thousand [1]. The volume of  intra-datacenter traffic is reported to exceed 7 Zettabyte/year in 2019 [2]. Toward the post Moore era, further parallelism,  disaggregation, heterogeneous and specialized compute/ memory node architectures, and even non Von Neumann  architecture can be envisioned, which will necessitate enormously large capacity interconnect. Link or port rates of current servers and switches in DCs or  HPCs are mostly 10 to 100 Gbps while standardization of 400 GE is under discussion [3, 4]. On the other hand, over 100  Tbps transmission capacity in a single optical fiber is achieved  in long-haul optical communication networks using wavelength  division multiplexing  (WDM)  transmission and digital  coherent detection scheme with digital signal processing (DSP)  [5]. Without  introducing  these  technologies, short-reach  interconnect network with link rate exceeding Tbps-class could  not be achieved. To handle future interconnect networks with the link rate of Tbps-class, optical switch technologies will become inevitable.  An optical switch can pass optical signals input from an input  port out to an arbitrary output port without OE/EO conversion.  Due to the absence of OE/EO conversion, the optical switch  offers superior energy efficiency and bit rate transparency.  Optical switches typically offer transparent optical circuit  switch capability, not packet switch capability. That is, optical  978-1-5090-0172-9/15/$31.00 (c) 2015 IEEE switches typically do not include address recognition process in  its switching process, and thus it requires control signals to update the switch setup apart from the data signals, which is a  different characteristics of packet switches. While Ethernet or  InfiniBand switches based on packet switching are currently  utilized widely, bottlenecks of their switch throughput are  concerned due to the energy consumption increasing with the  throughput, front panel densities, and package sizes of the  ASICs [6].  This paper discusses optical  interconnect network  technologies employing WDM transmission and optical switch  technologies for future DCs and HPCs. Section II reviews  optical switch technologies. Section III describes a proposed approach for optical circuit switch interconnect networks toward post Moore era and discusses the associated challenges.  Section IV provides brief conclusion of the paper. II. OPTICAL SWITCH TECHNOLOGY OVERVIEW There are various optical switch technologies, and the  typical ones are listed in Table I. Since currently available  optical switches are for single mode fibers (SMFs) and not for multi-mode fibers (MMFs), we have no choice but to use SMF  transmission links. Though currently many MMF links are  installed in DCs/HPCs, the use of SMFs will increase to  enhance both of the link bandwidth and the transmission  distance in large DCs. piezoelectric beam-steering. A 192×192 and 384×384 optical  There are only a few optical switch technologies which  provide  large port count  (more  than 100). One  is  microelectromechanical systems (MEMS) and the other is  switch are currently commercially available, respectively [7, 8].  Since they employ free space optics, the insertion loss and  cross talk can be small while the physical size tends to be large [9]. The switching  time  is relatively  long as  tens of  milliseconds. In a previous study, a SDN-controlled hybrid  optical/electrical interconnect network using MEMS optical  switches has been proposed and the switch reconfiguring time  is measured as long as 1.2 to 2.2 seconds including overhead  due to the centralized control [10]. and its reliability is know to be satisfactory [11]. A 32× 32  Waveguide type devices are attractive due to their high  reliability as well as suitability for large-scale integration and  mass productivity. Silica-based planar  lightwave circuits  (PLCs) have already been in practical use in long-haul systems Technology Type Port Count Port Bandwidth Physical Size Insertion Loss Crosstalk Switching Time MEMS based  optical switch MEMS Fiber switch 192x192 (C.A.)* Ultra wide (tens of THz) Tend to be large About 3 dB Can be very small Tens of  milliseconds [7], [10] OPTICAL SWITCH TECHNOLOGIES TABLE I.  PLC based  optical switch PLC Fiber switch 32 x 32 16 x16 (C.A)* Fairly wide (more than 5 THz) 110 x 115 mm (chip size) 6.6 dB < -40dB Silicon photonics  optical switch Silicon waveguide Fiber switch 32x32 Fairly wide (more than 5 THz) 11 x 25 mm (chip size) About 20dB < -20dB < 3ms 30 (cid:541)s [14], [15] WSS Mostly LCOS Wavelength switch 1x20 (C.A.)* 1x40 Fairly wide (more than 5 THz) -3 - 6 dB < -40dB Tens of  milliseconds [16] AWG-R based PLC and  tunable laser -720x720 25 - 100GHz (Tradeoff with the  port counts) ---Hundreds of  microseconds  (Depends on TLS) [19], [20], [22] SOA based fast  multicast switch SOA -8x8 ----< 10ns [23], [24], [25]  [11], [12], [13] single chip of which size is 110×115 mm2 [12]. The insertion  optical switch based on PLCs has been developed with  integrating 2048 Mach-Zehnder interferometers (MZI) in a  loss is reported as 6.6 dB; the extinction ratio is reported as 55  dB. The waveguide crossing loss can be a limiting factor of the  port count in waveguide type optical switches. The switching  silicon photonics technology. Recently a 32 × 32 silicon  time of PLC switches is specified as less than 3 ms [13]. 11×25 mm2 [14, 15], which is much smaller than the silica  Another emerging waveguide type optical technology is a photonics switch has been demonstrated; the chip size is  based PLC. The switching speed is reported as about 30 (cid:541)s; the  on-chip loss as 19.7±0.8dB; the crosstalk as less than -20dB [14]. One of the most attractive features of the silicon  photonics technology is the CMOS-compatible fabrication  which leads to low cost, mass productivity, and high reliability. The cost is the common and significantly important issue for  optical technologies to be widely introduced into HPCs/DCs. The switch function of the above mentioned four optical  switches is optical fiber switch where the optical signals in an  input optical fiber are switched all at once. There is another  type of optical switch called wavelength selective switch  (WSS) where individual wavelengths in an input fiber are  switched independently to one of a number of output fibers. The WSS typically has one input port and multiple output ports or multiple input ports and one output port; the former works  1×9 or 1×20 WSSs are now commercially available [16]. The  as a programmable wavelength demultiplexer and the latter as  a programmable wavelength multiplexer with blocking  tens of milliseconds. Recently, large port count (1 × 40) or  discarded wavelengths. Liquid crystal on silicon (LCOS) based  multiple-input/output-port architecture (5 × 5) have been  WSS is now widely deployed in long-haul optical networks.  typical insertion loss is 6 dB and the typical switching time is  1x4 AWG (cid:79)1, (cid:79)2, (cid:79)3, (cid:79)4 In 1 Out 1 Out 2 Out 3 Out 4 (cid:79)1 (cid:79)2 (cid:79)3 (cid:79)4 Fig. 1.  An example of a 1x4 AWG behavior.  * C.A. stands for commercially available. demonstrated [17, 18]. To provide a large switch port count, another optical  switching scheme called arrayed waveguide grating router  (AWG-R) has been proposed [19, 20]. AWG is a passive  optical device which is typically used as static optical  multiplexer or demultiplexer. The AWG forwards the signal at  an input port to an output port depending on the wavelength of  the signal as shown in Fig. 1. That is, the output port can be  switched by tuning the input wavelength. Fast switching time  can be achieved by using fast tunable lasers; recently a tunable  laser with the tuning time of less than 650 us has been  720 × 720 AWG-R based switch has been demonstrated by combining 8×8 optical switches and 1×90 AWGs where the  demonstrated [21]. In this architecture, there is a tradeoff  between the number of ports and the port bandwidth since one  wavelength is assigned to a pair of an input and output port. A  bandwidth is 50 GHz/port [22]. Regarding a fast optical switch, semiconductor optical  amplifiers (SOA) based multicast switches have been proposed  mainly for the application of optical packet switching [23, 24].  The reported switching time is as fast as less than 10 ns [25]. On the other hand, the energy consumption can be high since a  number of SOAs is needed to construct high radix switch. In  addition, the pattern effect in SOA should be managed. There  are some other fast optical switch technologies providing the  switching time of tens of nanoseconds, such as PLZT, LN, and  DLP WSS [26-28]. However, the port counts are quite limited as 2 to 4. Thus these would not be very suitable for high radix  switch applications. III. OPTICAL NETWORKING ON HPC AND DC IN POST MOORE  ERA To provide a future optical interconnect network, it is  essential to achieve (1) large bandwidth such as Tbps-class per  port or link; (2) reasonably fast reconfiguring time according to  the  traffic; as well as  (3) high scalability such as  accommodating more than 100,000 end points. Fig. 2 shows a schematic diagram of our proposed system.  It consists of hybrid electrical/optical interconnect networks;  the former is used for transferring small granular traffic and the  later is used for bulk data transfer. In the optical network, a pair  Fast optical circuit switch based network Optical Switch Controller Optical Switch Electrical packet switch  based network 10-400Gbps link Compute  node-1 Compute  node-2 Compute  node-3 Compute  node-n Tbps-class link WDM source (shared optical comb source) Fig. 2  Schematic diagram of the proposerd optical interconnect network Optical source distribution Shared WDM  source Optical  transmitter unit Shared WDM  source Optical receiver  unit DEMUX Computing units/ Memory units/ Storage units/ etc. Optical  transmitter  unit Optical  receiver  unit Network interface for Electrical  packet network (a) X U M E D M U X D E M U X Optical blocker array Modulator array (b) Receiver array Mixer array (c) Fig. 3.  Examples of detailed architecture. (a) compute node, (b) optical transmitter unit, and (c) optical receiver unit with a coherent detection scheme. TABLE II.  AN EXAMPLE OF WAVELENGTH LABEL ASSIGNMENT Compute node Compute node-1 Compute node-2 Compute node-3 Compute node-4 Compute node-5 Compute node-6 Corresponding Wavelengths (cid:79)1 (cid:79)2 (cid:79)1 (cid:79)1 (cid:79)3 (cid:79)4 (cid:79)2 (cid:79)2 (cid:79)3 (cid:79)(cid:22) (cid:79)4 (cid:79)4 of end points are connected with a direct optical circuit, and  thus the latency is caused only by the optical signal  propagation of 5 ns/m. Regarding the optical switch technology,  hereafter we assume the silicon photonics switch is one of the most reasonable technologies due to the relatively good  balance among the switching time, port count, physical size,  expected productivity, and cost. To achieve a Tbps-class link with optical switches, the  WDM transmission is inevitable. At an end point (that is, a compute node), an integrated multi-channel modulator and  receiver array, and compact and an energy efficient WDM light  source are the keys to achieve a Tbps-class network port. To  achieve this, a shared optical comb source is introduced [29,  30]. The optical comb is distributed to each of a number of  compute nodes as the WDM light source. Fig. 3 shows detailed  configurations of a compute node, an optical receiver unit, and  an optical transmitter unit. Due to the shared WDM source, individual compute nodes do not have to carry lasers in their optical transmitter units. An optical transmitter unit without lasers can consist of passive optical devices and be developed with cost-efficient silicon photonics technologies. Polarizationmaintaining fibers (PMFs) have to be used for the optical comb  distribution because optical modulator is usually sensitive to  the polarization of input light. Though currently the cost of  PMFs is higher than that of SMFs, development of a low-cost  PMF is highly expected for this purpose. To achieve a reasonably fast reconfiguring time, two  approaches are necessary; (1) minimizing the control overhead  time; and (2) maximizing the traffic duration. The switching  time of the silicon photonics switch itself is 30 (cid:541)s and thus tens  of microseconds switching guard time including the control  overhead should exist. To achieve efficient circuit switching  network, the traffic duration time should be long enough  compared to the switching guard time. Traffic pattern needs to  be optimized with support from co-design of the computing  algorithm and computer architecture. To minimize the control  overhead time, control scheme in a distributed manner will be  desirable rather than in a centralized manner. To control the optical switch in like a distributed manner, a  “combinatorial method” is  introduced [31]. In  the  combinatorial method, a source compute node can select a  destination compute node by selecting a set of wavelengths  transmitting the data among the bunch of wavelengths. That is,  a combination of wavelengths is assigned to each compute  node such as it serves as its address. Table II shows an example  of wavelength label assignment in which two wavelengths are  selected out of four. The optical signals are distributed to the  optical switch controller through optical splitters as shown in  Fig. 2. The optical switch controller distinguishes  the  destination of the optical signals based on the wavelength  combination without a complex label recognition circuit. The  optical switch controller can be simply constructed with optical  demultiplexer, photo detector arrays, and table look up circuits.  The look-up tables describing optical switch settings according  to the destination addresses are uploaded in the optical switch  controller in advance, so that the optical switch can be  controlled locally. A source compute node can start to send  optical signals at any time without communicating control  signals to the optical switch controller. The optical transceiver  units shown in Fig. 3 do not require any additional elements for  implementing the combinatorial method. This simple scheme  can realize a distributed-like control mechanism of an optical  Fig. 4.  Scalability of the address space of the combinatorial method Link Bandwidth with 43 Gaud DP-16QAM [Tbps] 3 6 9 12 0 1,000,000 s e 100,000 s s e r d d A f o r e b m u N 10,000 1,000 100 10 1 N=8 N=20 N=32 N=16 N=24 N=40 0 10 20 30 40 Number of Wavelengths for the Data Lane (M) Fig. 5.  Link bandwidth flexibility and scalability of the combinatorial  method     switch and can minimize the switch control overhead time. The scalability of the combinatorial method is evaluated in  space as (cid:1829)(cid:3014)(cid:3015) Fig. 4 and Fig. 5. Supposing the number of WDM wavelengths  in a fiber is N, and M wavelengths of the N wavelengths are  used to connect between a pair of compute nodes, the  combinatorial method can offer the scalability of the address  . More than 100,000 addresses can be expressed  with N(cid:149)20 as shown in Fig. 4. The connection bandwidth  between two nodes can be easily expanded by adjusting the  value of M as shown in Fig. 5. For example, if we set N= 40  and M=35 and suppose the use of a signal format of DP16QAM with the symbol rate of 43 Gbaud, more than 100,000  addresses can be expressed and more than 12 Tbps link  bandwidth can be achieved. These values can be quite  reasonable considering that 40 to 80 channel WDM systems covering C-band are deployed with 100 or 50 GHz spacing in currently a 32 × 32 strictly non-blocking switch has been  current long-haul optical networks. Regarding the scalability of the silicon photonics switches,  demonstrated [14]. Though the improvement of the insertion  loss and crosstalk can be expected with the advances of  processing techniques, the expansion of the port counts to  For example, a 512×512 strictly non-blocking switch can be  constructed with 32×32 strictly non-blocking switches. Then a  hundreds to hundred thousand can be hardly expected.  131,072 × 131,072 strictly non-blocking switch can be  However, employing clos switch architecture can construct a  constructed with 512 × 512 strictly non-blocking switches.  large port count switch with many small port count switches.  Though a number of 32 × 32 switch chips is needed, each  switch chip  is  sufficiently small. Towards practical  implementation of such a large scale optical switch, the  is, the number of heaters in a 32×32 strictly non-blocking MZI  reliability and ease of maintenance need to be assured. If  switch is comparable to that in a 256 × 256 rearrangeably  blocking switch structures  including rearrangeably nonblocking switch structures are allowed, large port count silicon  photonics switches can be constructed much more easily. That  nonblocking switch. However, circuit connection contentions  due to the blocking switch structure should be managed in the  switch controller or an  interconnect network  resource  management system. IV. CONCLUSION We firstly reviewed various optical switch technologies,  and then discussed an optical circuit switching interconnect  architecture for future DCs and HPCs. To achieve the  capability of more than 10 Tbps link bandwidth and the  scalability of more than 100,000 end points , the WDM  transmission technology and silicon photonics optical fiber  switches were introduced. To minimize the switch control  overhead  time, distributed-like optical  switch control  mechanism called combinatorial method was introduced. Though there are many challenges remained to practically  achieve  the proposed architecture  including practical  implementation of large-scale optical switches, detailed circuit  establishment process avoiding resource starvation or deadlock,  and optimization of the network configuration co-designed  with computer architecture and applications, such a large  bandwidth interconnect network will play a key role in future  large computer systems particularly in post Moore era. ACKNOWLEDGMENT This work was supported in part by KAKENHI #16H02816. "
Design of high bandwidth photonic NoC architectures using optical multilevel signaling.,"Network-on-chip (NoC) is a key component for boosting the system performance of future chip multiprocessors. With the projected increase in the number of cores on the chip, the NoC is perceived to be the limiting component for performance and scaling. Photonic NoCs are under serious consideration for scaling future multicore architectures. In this paper, we propose two photonic NoC architectures based on an optical multilevel signaling technique that can double the transmission bandwidth and reduce the area requirements. Simulation studies show that the proposed methodology saves up to 53% of power and reduces the area overhead by as much as 81% compared with metallic-based NoCs.","Design of High Bandwidth Photonic NoC Architectures Using Optical Multilevel Signaling Tzyy-Juin Kao Ahmed Louri Department of Electrical and Computer Engineering University of Arizona Tucson, Arizona Email: tjkao@email.arizona.edu Department of Electrical and Computer Engineering George Washington University Washington, DC Email: louri@gwu.edu Abstract—Network-on-chip (NoC) is a key component for boosting the system performance of future chip multiprocessors. With the projected increase in the number of cores on the chip, the NoC is perceived to be the limiting component for performance and scaling. Photonic NoCs are under serious consideration for scaling future multicore architectures. In this paper, we propose two photonic NoC architectures based on an optical multilevel signaling technique that can double the transmission bandwidth and reduce the area requirements. Simulation studies show that the proposed methodology saves up to 53% of power and reduces the area overhead by as much as 81% compared with metallic-based NoCs. I . IN TRODUC T ION Silicon photonic devices are compatible with standard CMOS technology and have brought light onto a chip [1]. The photonic link features a high data-transmission rate and low propagation loss, making it especially suitable for replacing long-distance wires. In the wavelength-division multiplexing (WDM) technique, several dozen wavelengths share a waveguide without interference and can be modulated and received individually [2]. Recent advancements in NoC design have leveraged the beneﬁts of silicon photonics [3]–[5]; however, because designers put emphasis on improving both the speed and power, the chip area consumption was usually neglected. Many photonic NoC architectures require three-dimensional stacking technology [6] and an additional die to place excessive photonic devices, increasing the manufacturing costs. Current photonic NoC architectures adopt micro-ring resonators to modulate wavelengths with an on-off keying (OOK) format, utilizing only the presence and absence of a wavelength to represent logical 1 and 0 [2]. To increase aggregate communication bandwidth without adding more links, many modulation formats used in ﬁber-optical communication are more advanced than OOK, such as optical multilevel signaling (OMLS). Because OMLS has multiple levels of amplitude to assign more bits of data at once, the capability of the bandwidth can be increased. Furthermore, OMLS offers a lower implementation complexity [7]; thus, it is more feasible to implement on a processor chip owing to the area constraint. This paper presents a bottom-up approach to introduce OMLS-based NoC architectures. We brieﬂy describe the structure of the OMLS link with a transmission bandwidth twice as 978-1-4673-9030-9/16/$31.00 c(cid:13)2016 IEEE large as the conventional photonic link [8]. Then, we employ a Clos network topology [4], [9] to develop a target system that can best harness the advantages of OMLS. Because of the multistage routing of Clos, packets can be distributed evenly across the middle routers, providing high load-balancing capability and high path diversity. Finally, we describe the implementation approach and propose two OMLS-based NoCs using offchip and on-chip lasers for a 64-tile chip. Simulation results show that the proposed architectures consume a signiﬁcantly lower chip area. This is achieved with only a slight or no increase in power consumption compared with the current photonic OOK-based NoCs. I I . OMLS -BA SED NOC ARCH I T EC TUR E S In this section, we illustrate the implementation approach of the on-chip OMLS link by using a simple 4-tile architecture as an example and propose two 64-tile OMLS-based NoC architectures. Readers could refer to [8] for more details about the on-chip OMLS link. A. Implementation Approach Fig. 1(a) illustrates the structure of an OMLS link. A 2way asymmetric splitter [10] is placed at the beginning of the transmitter, diverting two-thirds of the input laser power to the upper waveguide; the rest remains in the lower waveguide. Hence, the upper waveguide commands a larger amplitude power than the lower waveguide. Each waveguide on the transmitter side includes an identical series of rings that modulate the same set of wavelengths simultaneously, and both the upper and lower waveguides perform OOK modulation according to the electrical inputs. After the modulation, a combiner connecting both waveguides as inputs merges these two OOK signals back into one four-amplitude-level (4-ASK) signal owing to the constructive interference, assuming both waveguides present identical phase delays. Each amplitude level represents one of the four combinations of two bits (00, 01, 10, and 11). Finally, the modulated wavelength reaching the destination is coupled into a ring and converted back into two electrical signals. As a result, because there are two output bits in each time interval, the bandwidth of each wavelength is doubled compared with the conventional link. Fig. 1. (a) OMLS link: an example of encoding a four-amplitude-level signal into a single wavelength with two input binary signals. (b) Number of rings required for a singular conventional link and an OMLS link. The conventional link can only achieve half the bandwidth of the OMLS link. Fig. 1(b) shows the overall numbers of rings required for the conventional and OMLS links, assuming that up to 128 wavelengths can be placed on a waveguide by transmitting them on opposite directions. A single OMLS link with rings modulated at 10 GHz can achieve a maximum bandwidth of 2.56 terabits per second, whereas a conventional link can achieve only half of that. Given the same transmission bandwidth, OMLS requires half as many wavelengths as OOK. As the number of wavelengths decreases, the number of rings required on both the transmitter and receiver sides also decreases. Therefore, although the OMLS link requires one additional ring per wavelength, the overall number of rings is less than that of the conventional link. We use a small Clos topology for illustration purposes to simplify the design and veriﬁcation. Fig. 2 shows the conventional and OMLS implementations of a 2-ary 3-stage Clos topology for a 4-tile chip. This Clos topology is composed of four tiles, six routers, and a total of sixteen links (Fig. 2(a)). In this example, because of the proximity of the tiles, two tiles are grouped into a cluster and assigned three routers—ingress, middle, and egress routers—which represent three different stages of the network. Tiles of the same cluster are electrically connected (gray lines) to their ingress and egress routers because of their adjacency. Routers in the same cluster are also electrically connected, whereas the inter-routers between different clusters are optically connected (colored lines). To implement four optical connections (colored lines in Fig. 2(a)) between two clusters, we perform two different approaches as shown in Fig. 2(b). Either two conventional links or one OMLS link is required to implement this network and achieve the same aggregate communication bandwidth. We use four colors to correspond to each optical connection of the Clos topology. In the conventional optical approach (left side of Fig. 2(b)), the left waveguide is used for connecting the ingress and middle routers from cluster 1 to cluster 2 and vice versa. The right waveguide uses the same method to connect the middle and egress routers. The right side of Fig. 2(b) shows another approach that compresses all four connections into one OMLS link. Each electrical input signal is dissected and wired to two identically colored rings that modulate the same set of wavelengths to generate a double-bandwidth signal. Fig. 2. (a) Basic 2-ary 3-stage Clos topology. The optical connections in the dashed box can be implemented with either two conventional photonic links or a single OMLS link (b). Each color of rings corresponds to the same color line in the topology. (c) Four-tile chip layouts for the links. Two physical layouts on a 4-tile chip are shown in Fig. 2(c). The chip is divided into two clusters: upper and lower. Three routers of the same cluster are grouped together and placed in the center of every cluster. To achieve a bidirectional connection in the waveguides, two external laser sources with different sets of wavelengths are required, which are connected to both sides of the chip. The total number of rings required Fig. 3. (a) 8-ary 3-stage Clos topology. The links are unidirectional, from left to right. The communication links between ﬁrst and last clusters are highlighted. (b) The 64-tile chip layout is divided into eight clusters. Each cluster includes one group of three routers located at its center. For clarity, the links are not shown. (c) U-shaped OMLS-based architecture for this Clos topology on the 64-tile chip with OMLS links. The U-shaped arrows—having four different lengths—each represent a waveguide or a waveguide bundle. A thicker blue line indicates a larger number of waveguides (1, 5, 9, and 13). The tables list a total of 28 links and their bidirectional connections between any two clusters. (d) O-shaped OMLS-based architecture. The clusters have their own on-chip lasers to generate light individually. The double arrows represent bidirectional waveguides and form a layout with minimal lengths and no crossing point. for the conventional optical and OMLS approaches is 512 and 384, respectively. Consequently, with the OMLS approach, the number of long-distance waveguides and rings are both reduced. This OMLS implementation approach is used to design 64-tile NoC architectures in the following subsections. B. U-Shaped Photonic Clos Architecture We propose a U-shaped photonic NoC architecture using the OMLS links. Fig. 3(a) shows an 8-ary 3-stage Clos topology for a 64-tile network, organized in eight tiles per cluster. Each cluster corresponds to three routers that are closely located, including one for each ingress, middle, and egress router. The network has the previously discussed implementation approach: inter-routers between different clusters are optically connected (total of 112 optical connections), and others are electrically connected. Because one OMLS link can be responsible for up to four optical connections, the number of OMLS links required is 28, regardless of the routing paths. Fig. 3(b) shows a chip layout that distributes 64 tiles and 8 groups of routers symmetrically. To implement this 8-ary 3stage Clos topology, 28 OMLS links illustrated by U-shaped double-sided arrows are placed, as shown in Fig. 3(c). The shortest arrow has only one waveguide, which is responsible for the communication between clusters 1 and 8. The other three arrows, having different thicknesses, indicate waveguide bundles, comprising 5, 9, and 13 waveguides from thin to thick. The corresponding communication links for each waveguide and waveguide bundle are presented in the tables in Fig. 3(c). To achieve bidirectional communication among clusters, two external laser sources are coupled to both sides of each waveguide and placed at the top of the chip. Therefore, the physical layout of the waveguides on a 64-tile chip forms a U-shaped structure. C. O-Shaped Photonic Clos Architecture We propose a second OMLS-based NoC architecture, wherein off-chip lasers are replaced by state-of-the-art on-chip lasers to further reduce the waveguide lengths and optimize the power efﬁciency. To design this architecture, we use the same 8-ary 3-stage Clos network topology, implementation approach, and 64-tile chip as the previously described architecture. On-chip lasers provide the beneﬁts of layout ﬂexibility, the capability of switching on and off sources, and the elimination of coupling power losses [11]. Because lasers can be placed anywhere on the chip, both sides of the waveguides can be located inside the chip and do not need to gather at the edge. The groups of routers are distributed across the chip, so the laser sources can be similarly distributed. Each cluster can have its on-chip lasers at the center; thus, bidirectional waveguides can start at one cluster and end at another. Consequently, all OMLS links are reallocated back-to-back without overlapping with the minimum-length requirements, forming an O-shaped architecture, as shown in Fig. 3(d). One of the shortest waveguides, highlighted in red, communicates between clusters 1 and 2; one of the longest waveguides, highlighted in yellow, communicates between clusters 4 and 8. We estimate the required waveguide lengths by assuming that each waveguide begins and ends at the center of its cluster and that the unit is the length of a tile. The results indicate that the O-shaped architecture improves the waveguide lengths signiﬁcantly because of the on-chip lasers. The shortest waveguides are 2 tile units in length and can be used for connecting two adjacent clusters. The longest waveguides in the U-shaped and O-shaped architectures are 18 and 10 tile units long, respectively. Although the total number of waveguides for both architectures is 28, the overall waveguide length is 62% lower for the O-shaped architecture. B. Area The electrical links are the dominating factor, as shown in Fig. 4(b); however, their effect can be minimized by using photonic technology. For P-Clos architectures, the remaining electrical links enable short-distance communications between tiles and routers within each cluster. After removing most of the long distance electrical links in E-Clos, U-PClos-C can save up to 70% of the area overhead. Substituting OMLS links in U-P-Clos-OMLS and O-P-Clos-OMLS further reduces the area of the waveguides by half because the waveguide bandwidth is doubled. U-P-Clos-OMLS reduces the area of photonic components to less than 4 mm2 , decreasing the area overhead by as much as 81% and 37% compared with EClos and U-P-Clos-C, respectively. This not only yields a similar power consumption to O-P-Clos-C but also reduces the number of waveguides and rings required Fig. 4. (a) Power breakdown and (b) area overhead for different architectures. IV. CONCLU S ION I I I . EVALUAT ION This section presents an architectural analysis of the power and area for NoCs operating at 5 GHz on a 400-mm2 chip. PClos architecture offers a uniformly low latency, leveraging the beneﬁts of both Clos topology (low hop counts) and photonic technology (fast transmission) [4]. To evaluate the power consumption, we modify the photonic models of the DSENT simulator [12] to include OMLS links and use its default parameters. All the devices are based on a 22-nm CMOS technology. Our results include two electrical architectures for baseline comparison: mesh (E-Mesh) and Clos (E-Clos). The proposed photonic Clos (P-Clos) architectures can be categorized into the following four conﬁgurations: 1) U-P-Clos-C: U-shaped photonic Clos architecture using conventional photonic links. 2) U-P-Clos-OMLS: U-shaped photonic Clos architecture using OMLS links. 3) O-P-ClosC: O-shaped photonic Clos architecture using conventional photonic links. 4) O-P-Clos-OMLS: O-shaped photonic Clos architecture using OMLS links. A. Power For the ring-modulation power, the depletion-mode ring, which can be tuned electrically, is adopted. Theoretically, OMLS requires 4.8 dB more laser power at the receiver circuit to differentiate 4-ASK signals and obtain the same bit error rate [7]. The power breakdowns for different network architectures are shown in Fig. 4(a). Particularly, because Clos networks are composed of many long distance links, they appear to be well-suited for optical interconnects. Therefore, when implementing conventional photonic links, U-P-ClosC eliminates 41% of the wire power and reduces 32% of the total power. By using OMLS links, U-P-Clos-OMLS reduces the overall power by 22% compared with E-Clos. For further improving the power efﬁciency by using the O-shaped architecture, O-P-Clos-OMLS can save up to 53% of the power compared with E-Clos. We propose a compact photonic OMLS structure and its application to NoC design. The OMLS link doubles the transmission bandwidth of each waveguide by transmitting data into a 4-ASK signal. It exhibits great potential for improving bandwidth, area, and cost of optical interconnects, and for NOCs in particular. To highlight the advantages of OMLS for NoCs, we propose detailed implementation of OMLS-based photonic architectures, and the simulation results indicate: 1) low area requirement; 2) high feasibility of monolithic integration; and 3) only a slight or no increase in power. Consequently, the use of OMLS for NoC design is a promising approach to satisfy communication demands of future multicore architectures. "
A heuristic method of generating diameter 3 graphs for order/degree problem (invited paper).,"We propose a heuristic method that generates a graph for order/degree problem. Target graphs of our heuristics have large order (> 4000) and diameter 3. We describe the observation of smaller graphs and basic structure of our heuristics. We also explain an evaluation function of each edge for efficient 2-opt local search. Using them, we found the best solutions for several graphs.","A Heuristic Method of Generating Diameter 3 Graphs for Order/Degree Problem (Invited Paper) Teruaki Kitasuka Graduate School of Science and Technology Kumamoto University Kumamoto 860-8555, Japan Email: kitasuka@cs.kumamoto-u.ac.jp Masahiro Iida Graduate School of Science and Technology Kumamoto University Kumamoto 860-8555, Japan Email: iida@cs.kumamoto-u.ac.jp Abstract—We propose a heuristic method that generates a graph for order/degree problem. Target graphs of our heuristics have large order (> 4000) and diameter 3. We describe the observation of smaller graphs and basic structure of our heuristics. We also explain an evaluation function of each edge for efﬁcient 2-opt local search. Using them, we found the best solutions for several graphs. Keywords—order/degree problem, graph generation, Petersen graph, average shortest path length, 2-opt I . IN TRODUC T ION One of the famous problems in the ﬁeld of combinatorics is the degree/diameter problem [1], [2], [3], [4]. The degree/diameter problem1 is the problem of ﬁnding the largest possible number n(d, k) of nodes in a graph of maximum degree d and diameter k . The maximum degree of a graph is the maximum degree of its nodes. The degree of a node is the number of edges incident to the node. The diameter k of a graph is the maximum distance between two nodes of the graph. On the other hand, the problem of the graph golf 2015 competition [5] is the order/degree problem. The order/degree problem is the problem of ﬁnding a graph that has smallest diameter k and average shortest path length (ASPL, l) for a given order and degree. Compared to the degree/diameter problem, order is given and diameter is not given in the order/degree problem. As the organizer of the competition mentioned, the order/degree problem has important role to design networks for high perfomance computing. Because, the number of nodes of the network is determined based on design constraints such as cost, space, budget, and applications. Solutions of the degree/diameter problem can be used to limited networks of particular number of nodes. Currently, there is no trivial way to increase or decrease the number of nodes from the optimal graph of the degree/diameter problem, while keeping its diameter close to the optimal graph. For example, Besta and Hoeﬂer [6] have presented diameter-2 and -3 networks with particular number of routers, and each endpoint is connected 1 The Degree/Diameter Problem, CombinatoricsWiki, //combinatoricswiki.org/wiki/The Degree/Diameter Problem#Undirected graphs 978-1-4673-9030-9/16/$31.00 c(cid:13)2016 IEEE http: to a router. The number of endpoints can be changed in a range. Matsutani et al. [7] have reduced the communication latency of 3D NoCs, by adding randomized shortcut links. We try to solve some order/degree problems. There are two contributions in this paper. 1) Showing heuristic algorithm to create a graph for given order and degree (Sec. III). Using this algorithm, we have created two best-known graphs; one has order n = 4096 and degree d = 60, the other has n = 4096 and d = 64. After 2015 competition, we also have created other graph of order n = 10000 and degree d = 60. 2) Developing evaluation function of edges for 2-opt local search (Sec. IV). Local search starts with a graph that has the given number of nodes and satisﬁes degree constraints. Swapping two edges is accepted, if swapped graph G(cid:48) is better than the previous graph G in terms of diameter and/or ASPL. For example, if two edges a-b and c-d are selected for swapping from graph G, we try to swap two edges such that two edges a-b and c-d are removed from and two edges a-c and b-d are added to the graph G. If diameter and/or ASPL of swapped graph G(cid:48) is smaller than G, this swap is accepted. We call the evaluation function “edge importance”. Lower-importance edge pair is selected as the candidate of swapping earlier than other pairs. For the existence of local minimum graph, we need to temporarily accept worse graphs in searching graph of order n = 256 and degree d = 16. I I . OB SERVAT ION O F SMA L L ORDER GRA PH S The observation of small order graphs brings us the idea of the heuristic algorithm shown in Sec. III. At the beginning of the 2015 competition, we drew graphs with small order and degree. The ﬁrst graph is order n = 16 and degree d = 3 as shown in Fig. 1. The second one is order n = 16 and degree d = 4 as shown in Fig. 2. The diameters of these two graphs are three (k = 3). Through drawing these two graphs, we found that these graphs contain many pentagons (5-node cycles), no or small number of squares (4-node cycles), and no triangle (3-node cycle). In Fig. 1, there is no triangle and no square. No triangle and four squares exist in Fig. 2. We think triangles and squares cause diameter ASPL (average shortest path length) to be larger for the case of k = 3. Through this observation, we deﬁne increasing the number of pentagons as our policy in Section (a) ring layout (a) ring layout (b) pentagon (5-node cycle) layout Fig. 1. The best known graph with order n = 16 and degree d = 4 III. In the degree/diameter problem, pentagons are appeared in the graphs of diameter k = 2, e.g., Petersen graph (shown in Fig. 3) and Hoffman-Singleton graph (n = 50 and d = 7). I I I . H EUR I ST IC A LGOR I THM A. Policy and Outline of Heuristic Algorithm Based on the observation of small order graphs described in Sec. II, we determine the outline of our heuristic algorithm as following two steps. 1) If target diameter is k , we connect small order graphs such that their diameter is k − 1. For example, if the target diameter k = 3 and order n = 10000, the 1000 Petersen graphs (Fig. 3, diameter k = 2) are connected. 2) We try to increase the number of (2k−1)-node cycles, when edges are added. For graphs of k = 3, we try to increase the number of pentagons (5-node cycles). Outline of our heuristic algorithm is shown in Algorithm 1. In the remaining of the paper, we discuss only k = 3 graphs. Our algorithm generates a graph which diameter is almost 3, for given order and degree. Algorithm 1 Outline of graph generation algorithm 1: procedure GRA PHG ENERATER(order n, degree d) Create a base graph G0 , such that order of G0 is n Create a graph G, by greedily adding edges one by one to G0 Return the graph G 2: 3: 4: 5: end procedure (b) pentagon (5-node cycle) layout (c) square (4-node cycle) layout Fig. 2. The best known graph with order n = 10 and degree d = 3 B. Create a Base Graph G0 A base graph G0 = (V , E0 ) has n nodes, i.e. |V | = n. The graph G0 is a connected graph, but its degree is ﬁve. Most nodes have ﬁve edges. Other nodes, i.e., some border and anomalous nodes have four edges. Graph G0 contains multiple Petersen graphs. The Petersen graph GP , which is shown in Fig. 3, is one of well-known Moore graphs [3], and has ten nodes and degree d = 3. The diameter of Petersen graph is two. When the nodes are numbered in Fig. 3, ﬁfteen edges of the Petersen graph are described as Fig. 3. The Petersen graph (n = 10, d = 3) Fig. 5. The 11-node graph (n = 11, d = 4) for base graphs 11-node graphs. The 11-node graph is the subgraph of Fig. 2. We heuristically select eleven nodes, 5, 6, 7, . . . , 15 from the graph of Fig. 2. When we connect a 11-node graph with adjacent Petersen graphs, we ignore node 10 and other ten nodes are connected similar to Fig. 4. The eleven nodes graph is shown in Fig. 5, nodes are renumbered, except for node 10. Nodes 5, 6, 7, 8, 9, 11, 12, 13, 14, 15 in Fig. 2 are renumbered to 2, 3, 4, 0, 1, 6, 7, 8, 9, 5 in Fig. 5, respectively. The base graph G0 is generated by CR EATEBA SEGRA PH procedure in Algorithm 2. The base graph G0 has n nodes. Each node of G0 has ﬁve edges, except for nodes in the ﬁrst and the last Petersen graphs and node 10 of 11-node graphs. These exceptional nodes have just four edges. C. Greedily Add Edges One by One to G0 In this step, we greedily add edges one by one to the base graph G0 . Our policies to add edges are the followings. 1) Increase the number of pentagons in the graph, to create a graph such that its diameter becomes three and its ASPL is close to two. 2) Add an edge, which has the smallest degree node on one side. 3) No track back, i.e., never remove edges from the graph. Under the policy 1), our heuristic searches two nodes such that distance of them is four, and adds an edge between these two nodes. By adding the edge, the number of pentagons is increased. Even if the small-degree graph of n = 16 and d = 4 in Fig. 2, there are many pentagons those include a particular edge. For example, an edge 1-2 is contained in eight pentagons; 1-2-3-4-0, 1-2-6-7-0, 1-2-15-14-0, 1-2-3-8-9, 1-2-3-13-12, 1-2-6-5-9, 1-2-6-7-12, and 1-2-6-10-9. The policy 2) is employed to uniformly increase the degree of nodes and save computation time. Our heuristic maintains the nodes that have the smallest degree, and selects a node from them as one side of a new edge. A node of the other side is selected based on the policy 1). Although we can select the new edge from all possible pair of nodes, to save computation time, our heuristic limits search space by ﬁxing one side of new edge. The policy 3) also saves computation time. As another reason, we do not ﬁnd any effective evaluation function to track back. Fig. 4. Connecting two adjacent Petersen graphs follows, for i ∈ {0, 1, 2, 3, 4} and j ∈ {5, 6, 7, 8, 9}. (i, i + 1 mod 5) (i, (2i mod 5) + 5) (j, (j + 1 mod 5) + 5) (k = 1, 2, 3, . . . , n/10). Adjacent If a given order n is multiple of ten, we generate (n/10) Petersen graphs, GP k Petersen graphs, GP k and GP (k+1) , are connected as shown in Fig. 4 by CONN ECT procedure in Algorithm 2. Fig. 4 shows only edges crossing two Petersen graphs. In the case of n = 10000, we generate 1000 Petersen graphs, and k-th graph are connected with (k − 1)-th and (k + 1)-th graph for 1 < k < 1000. Algorithm 2 Create a base graph G0 Create k-th petersen graphs 1: procedure CR EATEBA SEGRA PH(n) for each k ∈ {1, . . . , (n/10)} do 2: 3: 4: 5: 6: 7: for each k ∈ {1, . . . , (n/10) − 1} do CONNEC T(k) end for end for 8: end procedure 9: procedure CONNEC T (k ) Connect k-th and (k + 1)-th Petersen graphs as shown in Fig. 4 10: 11: end procedure When there is a remainder r > 0 divided by ten, i.e., n = 10 · (cid:98)n/10(cid:99) + r , we replace r Petersen graphs with r Here, we explain this step in Algorithm 3. In each loop iteration of lines 3 to 17, two nodes i and j are selected and add an edge i-j to the graph. Node i is chosen from the nodes of the smallest degree in G at line 4, based on policy 2). d(i, j ) denotes the distance between two nodes i and j . In the loop of lines 6 to 14, candidates of node j are evaluated, based on policy 1). After evaluation, node j that satisﬁes two following conditions (1) and (2) is selected in line 15, and an edges i-j is added to graph G in the next line. J (cid:48) is the subset of J such that each node in J (cid:48) satisﬁes condition (1). We have no particular tie-breaking rule. p1 (j ) = min p2 (j ) = max (cid:48) ) (cid:48) ) p1 (j j (cid:48)∈J j (cid:48)∈J (cid:48) p2 (j (1) (2) The COUN TPATH S function is used for the evaluation of j ∈ J . COUNTPATH S(i, j ) roughly counts the number of paths between two nodes i and j , those distance are three. Dm (i) is the set of nodes distant m from node i, i.e., for every node k ∈ Dm (i), k satisﬁes d(i, k) = m. For example, every node k ∈ D1 (i) there are two combinations2 of swapping, 1) a-c and b-d (G(cid:48) of line 7) and 2) a-d and b-c (G(cid:48)(cid:48) of line 9) . Diameter and ASPL of both G(cid:48) and G(cid:48)(cid:48) are calculated. The loop of lines 4 to 13 implies that edge importance is reused for swapped graphs. In our experience, after ﬁfty swaps, ordering still valuable to ﬁnd smaller ASPL graph. Algorithm 4 2-opt local search 1: procedure MU LT I P LE2O PT(G) Calculate edge importance of all edges in G Sort edges by edge importance while timeout do repeat Select an edge pair (ei , ej ) in a particular order Generate swapped graph G(cid:48) Calculate diameter and ASPL of G(cid:48) Generate another swapped graph G(cid:48)(cid:48) Calculate diameter and ASPL of G(cid:48)(cid:48) until G(cid:48) or G(cid:48)(cid:48) has smaller diameter or ASPL than Copy G(cid:48) or G(cid:48)(cid:48) to G G end while 14: end procedure Fig. 7. Edge importance of an edge j -k for node i Our edge importance (or edge impact) is a value given to each edge of a graph. As an intuitive explanation, less important edges probably be removed from the graph with little increase of ASPL than other edges. Then, we give higher priority to less important edges, when we select an edge pair for swap. The edge importance of an edge e = j -k is deﬁned by the following function. f (e) = f (j -k) = f1 (i, j -k), (cid:88) i∈V 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: where f1 (i, j -k) is the importance of edge j -k for node i. The examples of f1 (i, j -k) is shown in Fig. 7. We assume f1 (i, j -k) = f1 (i, k-j ) (symmetricity) and divide two cases of f1 (i, j -k) as follows. • If two nodes j and k have the same distance from i, i.e., • If two nodes j and k have the different distance from i, d(i, j ) = d(i, k), then f1 (i, j -k) = 0. (left of Fig. 7) i.e., d(i, j ) + 1 = d(i, k), then 0 < f1 (i, j -k) ≤ 1. We deﬁne node set J , each of which has an edge to k and its distance from i is equal to d(i, j ). (cid:48) |j Using J , f1 (i, j -k) is deﬁned as follows. (cid:48) ) = d(i, j ) and d(j (cid:48) ∈ V and d(i, j (cid:48) , k) = 1} J = {j f1 (i, j -k) = 1 |J | The center of Fig. 7 shows a subcase of |J | = 2, and the right of it shows the other subcase of |J | = 1. Note that, this case includes the case of i = j . If i = j , then |J | = 1 by the deﬁnition. B. Order of Edges Pairs for Local Search Two lower-importance edges are the candidate of swapping for 2-opt local search. All edges are sorted by edge importance and denoted by e0 , e1 , . . . , e|E |−1 . The edge e0 has the smallest importance. The 2-opt local search algorithm is shown in Algorithm 4 The loop of lines 5 to 11 is the main loop of local search. Line 6 is the important point using edge importance. We tries several orders to select a pair, which are described in the next paragraph. For pair ei (= a-b) and ej (= c-d) selected in line 6, We heuristically employ two searching orders of line 6 in Algorithm 4. Both orders satisfy (ei , ej ) < (ei , ek ) < (ej , ek ) for i < j < k . We think the order of the smallest ﬁrst is better than it of triangle, empirically. • the smallest ﬁrst: (ei , el ) < (ej , ek ) for i < j < k < l. (e0 , e1 ), (e0 , e1 ), (e0 , e2 ), . . . , (e0 , e|E |−1 ), (e1 , e2 ), (e1 , e3 ), (e1 , e4 ), . . . , (e1 , e|E |−1 ), (e2 , e3 ), (e2 , e4 ), (e2 , e5 ), . . . , (e2 , e|E |−1 ), . . . , (ei , ei+1 ), (ei , ei+2 ), (ei , ei+3 ), . . . , (ei , e|E |−1 ), . . . • triangle: (ei , el ) > (ej , ek ) for i < j < k < l. (e0 , e1 ), (e0 , e2 ), (e1 , e2 ), (e0 , e3 ), (e1 , e3 ), (e2 , e3 ), . . . , (e0 , ei ), (e1 , ei ), (e2 , ei ), . . . , (ei−1 , ei ), . . . Since ASPL calculation is time-consuming task, we additionally design ASPL recalculation method for 2-opt local search. The method stores the distance matrix of graph G and update the matrix for swapped graph G(cid:48) . We can elimiate recalculation of the distance between nodes which the swap does not affect. C. Graph Instances We run local search program during the competition and after competition. We show the smallest graph that we found in Table II. These graphs probably are the best-known graphs 2 If graph G already has one of edges a-c or b-d (a-d or b-c), we skip the swap since the degree of two nodes are decreased by one. GRA PH S BY LOCAL S EARCH ING A FT ER 2015 COM PE T I T ION TABLE II order n 256 4096 4096 10000 degree d 16 60 64 60 diameter k 3 3 3 3 ASPL l 2.09069 2.295216 2.242170 2.648977 l of Table I 2.12757 2.295275 2.242228 2.648980 Note: all graphs may be the best-known graphs. The winner’s graph of the competition has larger ASPL than these. for these four combinations of order and degree. For graphs of order n = 4096 and n = 10000, Algorithm 4 is directly applied. For the graph of order n = 256 and degree d = 16, our graphs fall into local optimal many times. To ﬁnd graphs of smaller ASPL, we accept worse post-graph G(cid:48) than preswap graph G in 2-opt local search. Fig. 8 shows the search history of the last 1000 graphs before reaching the best-known graph of l = 2.09069. Many branches from each graph is omitted. In this ﬁgure, we show ASPL of each graph and order of edges that we swapped. The order of swapped edges is distributed from 0 to 620, i.e., swapped edges are two of e0 , e1 , e2 , . . . , e620 in each graph. To reach the best-known graph, we need to run local search at least in the range of edge pair (ei , ej ) for 0 ≤ i ≤ 153 and 1 ≤ j ≤ 620). This range contains only 4 % of all edge pairs. So, the edge importance seems to be valuable function to prioritize edges for swapping. We brieﬂy explain the distribution of order of swapped edges. Since two edges are selected for each swap in Fig. 8, the total number of selected edges are 2000 for 1000 swaps. The half of these edges have the order smaller than 8. The order smaller than 108 contains 90% of these edges. V. CONC LU S ION In this paper, we explained the heuristic algorithm that creates a graph which has small average shortest path length (ASPL) for diameter 3 graphs. The algorithm intends to increase the number of pentagons (5-node cycles). Through the observation of small order graphs which has diameter 3, we focused on the number of pentagons. The heuristic algorithm can create two best-known graphs at the graph golf 2015 competition, and a best-known graph after the competition. These three graphs have order n = 4096 and degree d = 60, n = 4096 and d = 64, and n = 10000 and d = 60. We also explained the technique of 2-opt local search to reduce ASPL of a graph. The technique is based on the evaluation function called edge importance (or edge impact). Edges which have smaller importance compared to other edges are the good candidates for swap of 2-opt. We applied this technique to the graph of n = 256 and d = 16, and ﬁnd a best-known graph after competition. As future work, we will try to ﬁnd more elegant heuristic to create a small ASPL graph, for not only diameter 3 graphs but also larger diameter graphs. Data structures for fast ASPL computation also should be explored. (a) ﬁrst half (ASPL: 2.09163 < l < 2.09277) (b) last half (ASPL: 2.09068 < l < 2.09186) Fig. 8. Local search history of 1000 graphs of order n = 256 and degree d = 16 ACKNOW L EDGM ENT This work was partially supported by JSPS KAKENHI Grant Number 26330107. "
Safe and dynamic traffic rate control for networks-on-chips.,"Networks-on-Chip (NoCs) for real-time systems require solutions for a safe and predictable sharing of resources between transmissions with different quality-of service (QoS) requirements. In this work, we present a mechanism which allows to apply existing wormhole-switched and performance optimized NoCs in safety critical domains, without requiring complex hardware modifications. For this purpose, we introduce a global and dynamic admission control mechanism implemented in the form of an access layer, controlling the rates at which running applications can access the NoC. The mechanism allows to enforce behavioral models for different data streams as well as to dynamically adapt the rates values to the number of currently active applications. We prove this important feature using formal timing analysis. Our approach results in a higher performance and tighter guarantees while simultaneously decreasing hardware (up to 60%) and temporal overhead (up to 80%) when compared with existing solutions.","Safe and Dynamic Trafﬁc Rate Control for Networks-on-Chips Adam Kostrzewa, Sebastian Tobuschat, Rolf Ernst Institute of Computer and Network Engineering Technische Universit ¨at Braunschweig, Germany Email: {kostrzewa, tobuschat, ernst}@ida.ing.tu-bs.de Selma Saidi Institute of Embedded Systems Technische Universit ¨at Hamburg, Germany Email: Selma.Saidi@tuhh.de IN TRODUC T ION Abstract—Networks-on-Chip (NoCs) for real-time systems require solutions for a safe and predictable sharing of resources between transmissions with different quality-of service (QoS) requirements. In this work, we present a mechanism which allows to apply existing wormhole-switched and performance optimized NoCs in safety critical domains, without requiring complex hardware modiﬁcations. For this purpose, we introduce a global and dynamic admission control mechanism implemented in the form of an access layer, controlling the rates at which running applications can access the NoC. The mechanism allows to enforce behavioral models for different data streams as well as to dynamically adapt the rates values to the number of currently active applications. We prove this important feature using formal timing analysis. Our approach results in a higher performance and tighter guarantees while simultaneously decreasing hardware (up to 60%) and temporal overhead (up to 80%) when compared with existing solutions. I . Networks-on-Chip (NoCs) based multi- and manycore architectures are frequently considered as a solution for future real-time applications due to their superior efﬁciency and scalability. However, in safety critical domains, such as automotive and avionics, the predominant requirement is to provide temporal guarantees proving that the worst-case system’s behavior is predictable and conforms to the application’s timing constraints. Commonly used wormhole-switched NoCs with multistage arbitration are not designed to meet these requirements but rather to deliver high performance on average. In such networks, ongoing transmissions compete for output ports (link bandwidth) and virtual channels (buffer space). Therefore, without appropriate quality-of-service (QoS) mechanisms, resources are not reserved in advance, i.e. packets are switched as soon as they arrive, and all trafﬁc receive equal treatment. Moreover, some interference cannot be resolved locally by the router’s arbiter and requires input from the adjacent neighbours e.g. a joint allocation of the crossbar switch and the router output due to a possible lack of buffers at the output (inputbuffered router). This results in a complex spectrum of direct and indirect interferences between data streams which may endanger the system safety [1]. Nevertheless, standard NoC architectures still remain appealing for use since they are affordable, fast and ﬂexible [2]. In embedded real-time systems, the NoC must serve a variety of trafﬁc classes with different requirements e.g. maxi978-1-4673-9486-4/16/$31.00 c(cid:13) 2016 IEEE mum communication latency. Trafﬁc shaping using rate control is a well known method to support quality-of-service e.g. [3], [4], [5], [6]. In real-time systems, this approach allows additionally to bound the interference and ensure predictability by enforcing a well deﬁned application’s behavior e.g. limit the maximal interference between data streams which must be resolved locally in standard NoC’s routers. The major drawback of this solution is that the rates are adjusted statically according to the worst-case scenario i.e. assuming that all senders are running simultaneously with maximum possible transmission arrival rates. Therefore, this approach is not work conserving and sacriﬁces the interconnect utilization whenever senders expose dynamics in the execution time, release jitter or communication volume and the system is not highly loaded. In this paper, we propose a mechanism that allows to overcome these limitations. Our solution is composed of three components: monitors, an access control layer and a special scheduling unit. At each source node, the monitor regulates the rate with which the source can inject trafﬁc in the NoC. This regulation is performed dynamically (at runtime) according to the system load i.e. the number of simultaneously active applications. We use the information about modiﬁed rate value to derive the interference in the NoC induced by a processing node what allows to improve performance while meeting the real-time constraints. The dynamic rate regulation is performed using a protocol-based access layer implemented within the existing NoC architecture and synchronizing senders using a special scheduling unit called Resource Manager (RM). The RM has a global knowledge of the state of the system and adjusts the rate conﬁguration for interfering applications according to their real-time requirements and ongoing communications. This rate can be different between trafﬁc streams thereby offering nonsymmetric guarantees. Recent work, such as [7], [8], from the real-time analysis domain proved that standard NoCs, even with complex twostaged arbitration (e.g. iSLIP) and virtual channels (VCs), are efﬁciently analyzable allowing to compute the worst-case network latency. Based on these results, we prove that the proposed RM-based dynamic admission control also provides predictable timing bounds allowing to apply many existing standard NoC architectures in safety critical real-time systems without the need to modify the hardware. We demonstrate that dynamic admission control leads to signiﬁcantly lower temporal overhead as well as lower hardware cost when compared to currently used QoS solutions. Moreover, the RM simpliﬁes the efﬁcient integration of real-time and safety requirements in existing performance optimized NoC architectures. The rest of the paper is structured as follows: Section II provides a detailed discussion of the related work. Section III describes the ﬂow of the mechanism. Based on that, a formal timing analysis is presented in Section IV. Finally, Section V presents an experimental evaluation and validation of the timing properties along with the corresponding overheads. I I . R E LATED WORK Ensuring safety and QoS is a well known research problem in NoC design. The majority of existing work have focused on QoS mechanisms which allow to enforce predictability while simplifying the worst-case analysis and enabling temporal guarantees. In principle, existing solutions can be divided into two categories: Time-Division Multiplexing (TDM) [9] and local arbitration in routers with rate control [10]. The former approach considers the entire NoC to be a single shared resource protected by a global arbiter. Each application/transmission is granted, in a cyclic order, a dedicated time slot to have an exclusive access to the NoC. Some advanced architectures applying this approach are Aethereal [9] and PhaseNoC [11]. Although TDM-based solutions are easy to analyze and implement, they suffer from several limitations such as static non work-conserving scheduling, since the access to the NoC is allocated exclusively and statically to every application whether it is sending data or not and for the entire duration of the time slot. This causes average latencies to be very close to the worst case latencies [12] and offers low scalability. Moreover, the transmissions latencies are statically dependent on the total number of synchronized applications and not on the actual load of the system. Finally, TDM requires a NoC design supporting a global time synchronization. The second group of mechanisms relies on QoS arbitration schemes (e.g. rate control) implemented in routers, well introduced in the network design [10]. This scheme is based on a local arbitration performed independently in routers. The QoS is provided through dynamic scheduling between transmissions trying to acquire a shared output port in a router. In order to bound interference, different data streams are isolated on different virtual channel (VCs) and routers use a static or dynamic priority based arbitration to control the rate with which transmissions progress on different VCs. Examples of such architectures are [13], [14]. Although this approach offers worst-case guarantees and a work-conserving scheduling, it introduces high hardware overhead. Firstly, the arbitration is based on the assumption that packets can be forwarded as soon as they arrive [8] i.e. there is no back pressure and no correlation between routers, which requires large buffers [9]. Secondly, the number of critical applications must not exceed the number of VCs, otherwise interference occurs thereby reducing predictability and preventing modular integration [9]. Therefore, the increase in the number of different, safetycritical applications integrated into a single chip, as in e.g. a Flight Management System [15], results in a proportional increase of the hardware overhead. Finally, these approaches require a custom NoC design with monitoring and rate control support infrastructure in each router. In this work, we address simultaneously the shortcomings Fig. 1: Admission control using the RM for interfering transmissions. of the previously presented strategies. Our mechanism offers a hybrid approach combining local arbitration performed in routers with a global and dynamic arbitration for an end to end guarantees. It reduces hardware overhead compared to routers with independent and local arbitration (reduced number of VCs and small required buffering) achieved through rate control (trafﬁc shaping) performed only in the egress ports of the network interfaces connecting nodes to the NoC. It decreases also the average latencies compared to TDM, i.e. temporal overprovisioning, due to a ﬂexible and workconserving arbitration. Moreover, the introduced access-layer does not requires a new dedicated interconnect architecture. On the contrary, it simpliﬁes the efﬁcient integration of real-time and safety requirements in existing performance optimized NoCs such as, the MPPA processor [16] and Tilera [17]. In the context of NoCs, client-server admission control schemes are applied to improve average performance of the interconnect by limiting the access rates to the hot-modules, such as e.g. [18]. Recently, authors in [19], [20] proposed protocol-based synchronization of individual transmissions based on the global state of the interconnect. These solutions concentrate on the systems frequently using DMA-engines as they assume synchronization of entire logical transmission (i.e. multiple packets) to mitigate high overhead of the introduced protocol. Therefore, they are not feasible for the usecase considered by us, i.e., trafﬁc originating from general-purpose applications running on processors with caches with short transmissions (one packet long cachelines) and high transmission arrival rate, including burst activations. I I I . DYNAM IC RATE CON TROL In this work, we introduce a global and dynamic mechanism for admission control in performance optimized NoCs to make them applicable for real-time systems. It is based on an access layer applied within the existing NoC. The control layer is composed of a management unit (the Resource Manager (RM)) and a synchronization protocol. This can be implemented using a dedicated VC or an independent control NoC already available in commercial MPSoC architectures, such as [16]. In order to be predictable, applications must ﬁrst acquire access from the RM before sending data in the NoC. This decouples the mechanisms responsible for admission control (the access-layer) from the underlying infrastructure conducting switch arbitration between packets/ﬂits (the bottom-layer), as depicted in Fig. 1. A. Mechanism Description In order to comply with the timing requirements (i.e. bounding direct and indirect interference in routers), the access Fig. 2: A schematic description of the trafﬁc injection rate for a given application. The rate varies according to the system mode. layer is used to arbitrate between senders (applications) conducting interfering transmissions - transmissions whose paths cross/overlap in at least one router. Note that, the proposed mechanism is especially important for the synchronization of transmissions sharing the same VC since the effects of both back pressure and head-of-line blocking may seriously endanger the system safety [7]. We refer to the set of interfering transmissions as a synchronization scenario arbitrated/protected using a scheduling unit, the Resource Manager (RM). Applications must invoke the RM whenever they are activated or terminated. Therefore, the RM has a knowledge about the global state of the NoC - which sender is active and which resources are occupied. Using these information, the RM may decrease or increase the injection rates for a particular node, as depicted in Figure 2, dynamically depending on the current system mode. Each mode is deﬁned by the number of currently active applications, and determines the minimum time separating every two transmissions issued from the same application. The mechanism is capable of enforcing symmetric and non-symmetric guarantees. In the former, transmission rates decrease uniformly for all applications belonging to a synchronization scenario along with the increasing number of senders running in parallel (system mode). In the latter, transmission rates depend not only on the current system mode but also on the application’s importance and may differ between senders. The non-symmetric mode can be used in a mixed-criticality system to maintain the critical application guarantees while reducing best effort trafﬁc. Note that, for each application the rates per mode are set up during the system initialization phase, with the analysis provided in the Sec. IV, and are static at runtime. This is done to ensure predictability and increase the mode change speed. Consequently, the introduced mechanism enforces behavioral models required to meet the critical timing constraints of the system for each application at runtime . In order to support admission control, we extend standard NoC architectures by introducing local supervisors, called clients at each node. The role of clients is to i) prevent nonauthorized accesses ii) adjust the access rates to the NoC for each application iii) release the NoC resources (inform the RM whenever an application terminates) and iv) prevent unbounded NoC accesses. Note that, the actions of a client can be completely transparent to the running applications. The clients can be implemented fully in software or as an independent hardware module controlling accesses before they arrive to the NI. This allows the integration with existing, commercially available components. Fig. 3: Workﬂow of an application synchronization with the RM. B. Workﬂow The communication between applications and the RM is protocol-based and realized with special control messages. The protocol consists of four control messages: activation (actMsg), termination (terMsg), stop (stopMsg) and conﬁguration (confMsg). The RM must be informed about the activation and termination of each application belonging to a particular synchronization scenario. Therefore, whenever an application is activated and trying to conduct the ﬁrst transmission its request is trapped by the client. It remains blocked until acknowledged by the RM with a confMsg. Later, the corresponding client sends an activation message to the RM. Similarly, when a client detects the termination of an application it issues a terMsg message to the RM. Activations cannot be nested i.e. an application may send a new actMsg only after issuing a terMsg for the previous one. The activation and termination messages are processed by the RM in their arrival order. Each of them initiate the transition of the system to a different mode. In order to ensure predictability, these transitions are done sequentially i.e. the new one may start only after the previous re-conﬁguration is fully completed. Additionally, the RM must assure that the transition between modes is safe. Therefore, before changing the rates, the RM sends to the clients supervising active applications, a stop message (stopMsg) to block all accesses to the NoC from the corresponding node. Clients then waits for the confMsg communicating the current system mode. Consequently, the RM sends confMsg with a delay (cf. Sec. IV) to ensure that packets from previously ongoing transmissions have left the NoC i.e. provide logical isolation. After receiving the confMsg, clients adjust the rate and unblock transmissions. Additionally, confMsg initiates activated applications (ones which just issued actMsg) which can then use the NoC. Note that, in the considered embedded safety critical systems, the behavior and characteristics of real-time applications are usually well speciﬁed and tested [21], contrary to the offchip networks where the behavior of nodes is often unknown at design time. This allows a trade-off analysis during the design phase providing estimation of the overhead resulting from the global arbitration. For instance, the designer may decide to keep the rates for some applications on a constant level (without endangering their deadlines) in some or all modes, in order to limit the number of necessary synchronizations and re-conﬁgurations reducing the probability of global arbitration being the bottleneck of the design (cf. Sec. V). Additionally, in systems where multiple disjoint sets of interfering applications exist, e.g. due to multiple hot-modules, the designer may use different RMs to synchronize each of them independently. This allows to increase the system’s scalability and to decrease the synchronization overhead. IV. T IM ING ANA LY S I S i (M ); l+ i (M ) and l+ In this Section, we prove the predictability of the mechanism. We compute a bound on the worst case response time for each application running in the system and accessing the shared NoC using the RM. The proposed timing analysis is based on the busy window approach (see [22]) which constructs a critical instant, i.e. scenario with the worst-case arrival sequence of events, event’s duration and the scheduling policy, to maximizes the response time, denoting the duration from the activation of the application until its completion. Consequently, the proposed analysis considers the worst case scenario where applications run in parallel as long as possible, so that the duration of all modes decreasing the rate is maximized. Therefore, we maximize the interference in the NoC and response time of applications. The proposed analysis considers also the overhead of the protocol introduced by the mechanism. Deﬁnition 1 (Basic Network Latency Bounds). Let l− i (M ) of a transmission i be the minimum and maximum time required to transfer a packet in the NoC in a system mode M when no contention and maximal contention are respectively considered. The time during which a packet is physically present in the network in a given mode can then be bounded by [l− i (M )]. Note that, the latency may be different from one mode to another since each mode implies a different amount of interference on the NoC. These bounds can be obtained using the method from [8]. Real-time applications are activated according to a given activation pattern (for instance, periodic). This pattern deﬁnes the number of currently active applications and thereby the current mode of the system. We use event models [23] to capture the best-case and worst-case behavior of every possible application arrival/activation pattern. Deﬁnition 2 (Application Activation Pattern). Let η− i (∆t) be the minimum and maximum number of activations of a particular application i within a time window of size ∆t. Their pseudo-inverse counterparts δ− i (n) denote the minimum and maximum time interval between the ﬁrst and the last activation in any sequence of n activations of application i. Whenever an application is activated, it must perform a maximum number of N + i accesses/transmissions to the NoC. These transmissions are served by the RM according to a rate varying with the system mode (see Fig 2). Similarily, we use event models to capture the trafﬁc rate in each mode. Note that, this event model is enforced by the trafﬁc shaper in the egress port of the NI. Deﬁnition 3 (Transmissions Access Rate). Let r− and r+ i (∆t, M ) be the minimum and maximum number of accesses to the NoC (i.e transmissions) allowed by the RM in mode M for an application i within a time window of size ∆t. As explained previously, this rate is deﬁned by the minimum time di between two transmissions and the number of active applications M deﬁning the system mode. r− (resp. r+ i ) is i (n) and δ+ i (∆t) and i (∆t, M ) η+ i deﬁned by rounding down (resp. up) the following fraction, r− i (∆t, M ) = (cid:23) (cid:22) ∆t di × M i (∆t, M ) = (cid:100) ∆t ; r+ di × M i (n, M ) and (cid:101) ˜r+ their pseudo-inverse counterparts ˜r− Similarly, i (n, M ) , denote the minimum and maximum time interval between the ﬁrst and the last transmission in any sequence of n transmissions from application i in a mode M . Recall, that the proposed rate controlled arbitration introduced by the RM follows the following rules: i) the rate with which transmissions are conducted is decreasing with the increasing number of active senders in each synchronization scenario ii) the rate depends only on the number of active application iii) the rate may be different for different applications in each mode (i.e different value of di ) for non-symmetric guarantees. Note that, the number of system modes is bounded by the number z of applications in a synchronization scenario λ i.e. M = M1 , M2 , ...Mz (z ∈ N). Next, we construct a critical instant which marks the beginning of the busy window and considers the worst-activation sequence of transmissions. We assume conservatively that transmission requests from all synchronized senders arrive simultaneously to the RM, which may never be observed in a real execution. Deﬁnition 4. Let q-event busy windows ω− describe the minimum and maximum time interval required to complete q consecutive activation of an application i belonging to the synchronization scenario λ protected by a RM. Theorem 1. The worst-case time necessary to ﬁnish q activations of an application i is bounded by: i (q) and ω+ i (q) i (q) ≤ q · (C + i + 2 · l+ ω+ i (q), q · N + + Bi (w+ i ) + M CRi (w+ i (q)) i,ctrl ) (1) i i i (q) i (q), where, C + is the worst case processing time on the core, Bi is the maximum communication time which q · N + transmissions from application i may experience in the NoC during w+ MCR (mode change request) deﬁnes the maximum delay resulting from re-conﬁgurations of the system during the time w+ and l+ i,ctrl denotes the maximum latency of control messages for a transmission i. Proof: The theorem directly derives from the mechanism and protocol description (see Sec. III). The busy window of q activations of application i is bounded by the time necessary for each instance to execute on core (q · C + i ) and conduct transmissions on the NoC (Bi (w+ i ). We assume conservatively that a re-conﬁguration (MCR) happens every time an application is activated and therefore delays its execution. Finally, the application must send to the RM activation and a termination messages q times (2q · l+ Note that the ω+ i (q) appears on both sides of Eq. 1 which represents an integer ﬁxed-point problem. It can be solved iteratively starting with ω+ i,ctrl ). We assume constant latency of control messages (l+ i,ctrl ) (i.e independently of any mode). This can be assured by taking conservatively the longest latency from all possible modes. i (q) = q ·(C + i +2·l+ i (q), q · N + q · N + i,ctrl ). i i (∆t, q ·N + bm i ) = m=z if m = z i } i − (cid:80)k=m+1 k=z (2) where bm i deﬁnes the maximum number of transmissions which can happen in mode m. Bi (∆t, q · N + i (∆t, q · N + i ) = bm i ) · (˜r+ i (2, m) + l+ i (m)) min{ A , q · N + min{ A , q · N + i (∆t), k) } r+ i (Θk i (Θm i (∆t), m) and Θm if m (cid:54)= z (3) where A = r+ i (∆t) denotes the worstcase duration of the mode m (cf. Lemma 3). Proof: In each mode, the NoC access rate deﬁnes the maximum number of transmissions A conducted by an application i (see Def. 3). However, i) it may not be higher than the maximum number of transmissions conducted by a task (q · N + i ) ii) we must account for transmissions which must happen in other modes. This is conservatively assured by selecting the minimum from the r+ i and available number of transmissions (q ·N + i (∆t)) in mode m. The maximum duration of a single transmission is deﬁned by its network latency (l+ i (m)). We must also conservatively assume that a new transmission has arrived just before the previous one was granted an access to the NoC, therefore consider the maximum delay resulting from rate control (˜r+ Lemma 3. The longest duration of a mode m with respect to an application i in a time window ∆t can be bounded by: i −(cid:80)k=m+1 k=z r+ i (Θk i (2, n)). , m)(cid:1) if m = z  (cid:0)η+ (cid:0)˜r+ (cid:0)η+ min j (∆t) · ˜r+ j (N + j (∆t) · N + (∆t) , m(cid:1)(cid:1) min∀j∈λm ˜r+ j j (Θm+1 j ∀j∈λm=λ(cid:48) j − j if m (cid:54)= z Θm i (∆t) = Next, we determine how many accesses are performed in each mode according to the worst case scenario (i.e applications are running in parallel for maximal duration). We start with the highest possible mode (M = z ) to compute how many transmissions out of a total of q · N + i occur in this mode with the lowest rate, and then decrease it (M = z − 1, M = z − 2), ... etc). In a next step, we deﬁne the longest duration of each system mode. Lemma 2. The worst-case NoC communication time required by q · N + transmissions in a time window ∆t is bounded by: i 0(cid:88) (4) where λ(cid:48) denotes the set of all applications belonging to the same synchronization scenario as i i.e. λ(cid:48) = λ\{i}. λm deﬁnes the set of all applications active in the worst case scenario in mode m. It is created by removing from λ(cid:48) all applications which completed in system modes with a higher number of active senders i.e. < m + 1, z >. Proof: The longest duration of each system mode is deﬁned by the longest possible overlap between time periods during which particular applications are actively using NoC. Note that the overlap may happen at any moment of execution j (N + j (∆t)) of an application i i.e. not only at a critical instance. In order to deﬁne the periods, we must take into consideration the maximum number of activations for each application (η+ and the maximum time (per activation) during which it may use the NoC (˜r+ j , m)). This is done under the conservative assumption that each activation happens early enough to block i and that multiple activations happen one after another to extend the overlap, what may not happen in reality. The application with the shortest period deﬁnes the end of a mode i.e. when it terminates the number of active senders must decrease. Therefore, the duration of a mode is derived by selecting the minimum from the periods for all applications belonging to λm . Consequently, for modes m! = z these operation must be repeated iteratively starting from mode z to exclude applications which must have ﬁnished before the beginning of mode m. Lemma 4. The maximum delay which an application i will experience during the period ∆t due to the need for safe reconﬁguration of the system (i.e. safe mode transitions) can be safely bounded by: (cid:88) ∀j∈λ j (∆t) · (cid:0) max∀k∈λ 2 · η+ (l+ max (l+ k (m)) + l+ k,ctrl )+ ∀k∈λ&m∈M (cid:1) (5) i,ctrl M CRi (∆t) = Proof: We assume conservatively that each activation within the period ∆t of any application belonging to the synchronization scenario causes two (start and termination) re-conﬁgurations. During each mode change, the RM must assure the logical separation between transmissions i.e. that all transmissions provided an access to the NoC in the old mode leave the NoC before the new mode begins to avoid overload. As RM conducts multicast, we select conservatively the longest possible transmission time of stopM sg as well as the maximum from transmission times of the one (i.e. last) packet for all applications belonging to λ (taking the worst result from all modes). Finally, we must account for a transmission of conf M sg which unblocks i. Based on the computed busy-window wi (q), we derive the the worst-case response time Ri of an application i i.e. the longest time interval between its activation and completion: (6) {ωi (q) − δ− (q)|ωi (q) ≥ δ− (q + 1)} Ri = max∀q≥1 The response time Ri is represented by the difference between δ− (q). Later the schedulability test has to conﬁrm if the the busy window ωi (q) and the earliest possible activation constraint Ri ≤ Di is satisﬁed for every application in the system, where Di is the deadline of the application. Otherwise, guarantees for a predictable behavior cannot be provided. V. EX PER IM EN T S In this Section, we evaluate the proposed mechanism using a NoC with a conventional router microarchitecture [2] as our main goal is to efﬁciently apply COTS components for real-time systems. We use a 2D mesh network with wormhole switching, two stage arbitration (iSLIP), virtual channels (VCs) and a deterministic XY-routing. Consequently, we do not compare the proposed mechanism against architectures and mechanisms requiring custom modiﬁcations of NoCs such Fig. 5: Performance of CHSTONE benchmarks synchronized with MPEG-4. (a) (b) Fig. 4: (a) MPEG-4 average communication demands speciﬁed in MB/s, (b) mapping of MPEG-4 with interfering trafﬁc from general purpose (GP) applications. as e.g. [13], [9]. Alternatively, we use for comparison the solution supporting QoS through trafﬁc shaping performed independently in the egress ports of network interfaces which complies with generic trafﬁc shaping known from real-time systems e.g. interface based designs [3] or latency rate servers [4]. Consequently, it allows to apply the analysis proposed in [8], [7] to obtain QoS guarantees in the assumed performance optimized NoC architecture without a modiﬁcation of routers. Note that, it works similarly to other QoS mechanisms implemented independently in the NI of each node, such as TDM introducing periodic shaping with the minimum distance between messages equal to the duration of the TDM cycle. However, on the contrary to TDM, it does not require a common time base for the synchronized nodes. In the rest of the paper, we refer to this solution as decentralized trafﬁc shaping (DTS). We assume a common usage scenario in real-time systems where different application classes execute in parallel: streaming multimedia e.g MPEG-4 video decoder [24] and general-purpose applications. Moreover, we focus on network trafﬁc resulting from memory transfers as memories constitute the most common hot module in MPSoCs challenging both performance and scalability (cf. [25]). The MPEG-4 decoder’s demands with respect to the interconnect resources are presented in Fig 4(a). We identify three memory modules with high communication requirements: DRAM (7 senders), SRAM2 (4 senders) and SRAM1 (2 senders). In the sequel, we focus on the DRAM memory module with the highest interconnect demand. Each task from MPEG-4 is activated periodically and periods are calculated based solely on the required bandwidth including some release jitter (J=10% of P) whenever a task is activated, it requires to perform a burst of transmissions on the NoC according to the amount of data speciﬁed in Fig 4(a). This burst is modeled using a trafﬁc generator conducting 8kB long DMA transfers Fig. 6: Trace from dfmul benchmark with the number active senders showing the number of packet arrivals in time. to maximize the beneﬁt from DDRAM3 2133N [26]. For general purpose applications, we use activation traces of benchmarks from the CHSTONE suite [27]. Traces are extracted using the Gem5 simulator and an ARMv7-a core with a 32 kB L1 cache. Compilation is performed using standard gcc compiler (ver. 4.7.3). Simulations are carried out with OMNeT++ event-based simulation framework and HNOCS library [28]. Therefore, the memory trafﬁc of both MPEG-4 and general purpose applications share the interconnect and need to be synchronized using the RM in order to be predictable. Fig 4(b) shows the considered mapping for the DRAM synchronization scenario. We compute the worst-case guarantees with the analysis from Sec. IV implemented in the pyCPA framework [22]. A. Application and Benchmark-Based Results We start with the evaluation of the performance gain achieved through the proposed mechanism. Fig. 5 presents the average response times of CHSTONE benchmarks running in the considered scenario. Results concern the DTS arbitration (rates adjusted and ﬁxed according to the worst-case) as well as the RM approach (rates decreasing uniformly with the increasing number of active senders). We observe that benchmarks synchronized with the RM ﬁnish faster despite of the additional overhead resulting from the synchronization protocol. The improvement is signiﬁcant and varies among benchmarks starting from 50 % for blowﬁsh to reach a speedup value of nearly 85% for mips. This depends on the frequency and access patterns of initiated transmissions as well as on the activity of other senders. The speedup is also proportional to the number of memory accesses, i.e. an application conducting more transfers experiences higher improvement in the response time due to dynamic rate RM arbitration. It is visible that Fig. 7: Average latencies for DTS and RM in the system requiring non-symmetric guarantees. Fig. 9: Temporal overhead resulting from the RM. Fig. 8: Worst-case response time for different levels of system load. Fig. 10: Area overhead resulting from the RM. the protocol overhead remains very low compared to the transmission time (∼ 15%). For detailed explanation, Fig. 6 presents the evolution of the ﬁrst 1000 accesses of dfmul benchmark in the system using the RM along with the variation of the number of active senders (right Y-axis). It is visible that the benchmark’s speed depends on the actual system mode i.e. packets latency varies according to the number of senders running in parallel. In the simulation, we observe a maximum number of 5 active applications running in parallel with dfmul, for a total number of 8 synchronized applications. Moreover, we observe that for 60% of the time, there is only one application running together with the considered benchmark. This allows to signiﬁcantly increase the transmission rate and to provide high performance improvement at runtime. Next, we compare the average latencies of transmissions synchronized with RM against the system utilizing DTS when non-symmetric guarantees are required. We assume the same experimental setting as previously but in a priority based system, where MPEG-4 tasks communicating with the DRAM memory are assigned priorities: tasks with higher frequencies are assigned lower priority. Recall, that each task conducts periodically burst-accesses to the NoC of equal length. DTS used to enforce non-symmetric guarantees applies a static minimum distance function between two transmissions according to the priority i.e. shorter distance (high rate) in case of high priority transmissions. In case of the RM, we increase the rates of tasks whenever it is possible. The rate decreases with the increase in the number of active senders, however it cannot be lower than the priority rate which allows the application to meet its timing constraints. Fig. 7 presents the results for MPEG-4 decoder tasks. It is visible that when DTS is used, the average latencies are increasing along with the decreasing priorities of senders which get a low rate according to their priority level. In case of the RM, applications with lower priorities beneﬁt signiﬁcantly from the unused bandwidth by increasing their rate whenever it is possible (i.e when higher priority applications are not active), therefore achieving a speed up of up to 30% in case of UPSMAP module. The transmissions with higher priorities achieve similar average latencies as for DTS. B. Scalability. In order to assess the RM’s scalability, we measure the worst case response time and temporal overhead by varying the number of synchronized applications. Additionally, we change the overlap between synchronized senders deﬁned as the maximum period of time during which all senders may run in parallel. Recall, that the overlap is proportional to the i) number of transmissions ii) their maximum arrival rate and iii) the frequency of activations of the sender, as explained in Sec.IV. Fig. 8 summarizes the results presenting the worst-case response times in a system with DTS- and RM-based QoS control. The response times, for both scenarios, increase proportionally to the number of synchronized applications. However, for DTS they remain constant and independent from the possible overlap. In case of the synchronization with RM, we may proﬁt from the low overlap providing ﬁne-grained worst-case guarantees when it is sufﬁciently low. Finally, we may also observe that the RM offers slightly worse guarantees than DTS in fully loaded systems e.g. with high numbers of transmissions and high overlap. This is due to the protocol overhead. For detailed explanation, the protocol overhead is depicted in Fig. 9. It increases proportionally to the number of synchronized senders but remains constant w.r.t to the overlap. This is caused by the fact, that the main delay component results from the safe mode change (protection against system overload) which is conducted by RM whenever an application belonging to the synchronization scenario is activated or terminated. In subsequent series of experiments, we increase the activation rates of other synchronized senders (from 1 to 4 activations per activation of the benchmark). The overhead in such scenarios grows exponentially along with the number of senders and the frequency of their activations. Note that in the considered safety critical systems, the behavior of senders is usually predictable and well understood [21]. Therefore, their activation frequency is known in advance and often signiﬁcantly limited (e.g. periodic in ms). Additionally, clients introduced by the RM may monitor and bound the application’s activation frequency to decrease the overhead. Moreover, many commercially available NoCs e.g.[16] are already equipped with independent signal lines, prioritized VCs or a dedicated independent control layer to improve performance of latency sensitive transmissions. In such architectures, control messages are not blocked by any other trafﬁc what additionally decreases the overhead. C. Implementation Overhead. The implementation of the access layer is done in form of independent hardware modules controlling accesses to the NI without modiﬁcation of the NoC. The evaluation is done in the IDAMC platform [29] on a Virtex-6-LX760 Xilinx FPGA with 5 ﬂit buffers per VC per ingress port. The clients required 300 Look-up tables (LUTs), which corresponds only to 3% of the area of a Network Interface (NI) module whereas the RM required approx. 1200 LUTs which corresponds to 16% of the area of the NI. Fig. 10 presents the area overhead resulting from the implementation of the RM in NoCs for different sizes and employed arbitration mechanisms. Although DTSand RM-based arbitration methods introduce area overhead, they simultaneously decreases the number of required VCs compared to other existing QoS solutions. For instance, in the considered DRAM synchronization scenario, the RM requires 2 VCs (control messages and transmissions) whereas state-ofthe art mechanism based on non-blocking routers and local arbitration [13] requires 8 VCs (one for each data stream in case of dynamic trafﬁc shaping between different VCs for nonsymmetric guarantees). The RM-based approach introduces slightly higher overhead than DTS due to the central RM unit (approximately 15%). V I . CONC LU S ION S The efﬁcient and safe sharing of network resources is crucial for the success of MPSoCs in the market of safety critical and real-time systems. In this work, we propose a novel mechanism, which allows to apply the majority of existing, performance optimized NoCs in safety critical domains without complex hardware modiﬁcations. It allows to enforce behavioral models for different data streams as well as to adapt at runtime to the currently active NoC load. The proposed approach allows to decrease both temporal and hardware overhead while signiﬁcantly improving the overall performance of the system and meeting the strict timing constraints. "
Chip-scale si-photonics optical transceiverfor a photonics-electronics convergence system (invited paper).,"Here, we introduce the roadmap of the PETRA project. The first approach is a chip-scale silicon photonics transceiver using multimode optical wiring to realize low-cost and high-productivity optical interconnection for several shortreach applications. Hybrid-integrated low-power-consumption chip-scale optical transmitters/receivers called ""optical I/O cores"" are presented. Their performance can cover most reach demands for high-performance computers and data centers. The scope for advances in technology based on optical I/O cores is discussed for application of this concept to future photonics-electronics convergence at the chip level.","Chip-scale Si-photonics optical transceiver for a  photonics–electronics convergence system (Invited Paper) Kazuhiko Kurata, Ichiro Ogura, Kenichiro Yashiki, Yasuyuki Suzuki Mituru Kurihara, Yasuhiko Hagihara, and Takahiro Nakamura Photonics–electronics Technology Research Association (PETRA) West 7 SCR, 16-1 Onogawa, Tsukuba, Ibaraki, Japan 305-8569 Mmail:k-kurata@petra-jp.org Abstract—Here, we introduce the roadmap of the PETRA  project. The first approach is a chip-scale silicon photonics  transceiver using multimode optical wiring to realize low-cost and high-productivity optical interconnection for several shortreach applications. Hybrid-integrated low-power-consumption  chip-scale optical transmitters/receivers called “optical I/O cores”  are presented. Their performance can cover most reach demands  for high-performance computers and data centers. The scope for  advances in technology based on optical I/O cores is discussed for  application of this concept to future photonics–electronics convergence at the chip level. Keywords—silicon photonics, optical transceiver, chip-scale  integration, optical interconnection INTRODUCTION I. Owing to the rapid growth of information society, there is  a constant demand for wider bandwidth in communication  systems and higher performance in IT/NW equipment. Figure 1 shows the recent performance trends for high-performance  computers (HPCs). The peak performance has continued to  increase with advances such as multicore CPUs. On the other  hand, the network and memory bandwidths have not greatly  increased. This means that the effective performance has not greatly improved relative to the peak performance. Fig.1(cid:3) Trends of HPCs 978-1-4673-9030-9/16/$31.00(c) 2016 IEEE Securing enough bandwidth for off-chip interconnection  has become a vital concern. The I/O bottleneck issue has been  considered to achieve a sufficient interconnection bandwidth [1]. For electrical interconnections, realizing a higher signal  speed with a higher density faces a dilemma between the  transmission  length–power consumption relationship to  maintain the signal quality and the constraint of a smaller pad  size with a limited package size. Optical interconnection  technology has attracted much attention as a high-speed, highdensity, and energy-efficient solution. To further apply optical  interconnection technologies with shorter reaches in board-toboard and chip-to-chip regimes, industrial problems such as  cost efficiency and mass production will become critical issues.  Silicon photonics technology is one of the more attractive  options. Precise silicon wafer fabrication processes have been  used to realize high-speed (>25 Gbps) and small-size Siphotonics transmitter and receiver devices and photonics  integrated circuits (PICs) [2]. This technology is very suitable  for very small optical transceivers and mass production.  Although highly  integrated photonic circuits have been  developed  [2],  high-productivity optical  packaging  technologies are required to fulfill the industry demands with  regard to volume and cost. Meanwhile, precise optical axis  alignment remains an issue for realizing industrial mass  production of  conventional opt-electronics packaging.  Applying a multimode transmission line to silicon photonics  technologies is considered a practical solution for highproductivity packaging. In this paper, we propose a highdensity multi-mode wiring system with chip-scale optical  Tx/Rx based on silicon photonics from PETRA. After we  review the concept of a chip-scale optical transceiver called an  “optical I/O core” by using silicon photonics, we discuss the  experimental results for multimode fiber (MMF) transmission.  Finally, the applicability and future prospects for photonics– electronics convergence are described. ROADMAP OF PETRA PROJECT II. Figure 2 illustrates the roadmap of the PETRA project for  developing the required technical advances in packaging to facilitate photonics–electronics integration [3]. The plan targets the completion  of  photonics–electronics convergence  techniques to exceed electronics capabilities by 2022. To  address bottleneck  issues, an onboard electrical–optical convertor and optical PCB is to be integrated with the CMOS  LSI in the late 2010s. Advances in the system performance of  optical interconnections should lead to system penetration of  the photonics–electronics convergence. Optical Tx/Rx  functions, which feature a high density and low power  consumption, as the optical I/O will be required in the first  stage of the photonics–electronics convergence. PETRA has  developed a miniature optical chip-scale package that is called  an optical I/O core. This will serve as a building block for the optical transceiver, active optical cables, embedded optics or  optical engines, and built-in LSI package in the second stage.  The optical Tx/Rx functions will be fabricated on silicon as a  photonics–electronics convergence interposer in the third stage.  The photonics–electronics convergence occurs not in the  limited field of high-end systems but in many applications, such as industrial and consumer electronics. As optical  technology penetrates systems like chip-to-chip connections, a Fig.2 Roadmap of PETRA large number of optical channels must be connected to  waveguides or optical fibers. This may complicate the interface.  Because precise adjustment is a serious problem for lowering  the cost and optical interconnections are generally for shortreach data transmission, multimode wiring is favored to reduce the complexity and manufacturing cost. III. BASIC CONCEPT A: Transmission configuration. The advantage of Si photonics is that different types of  PICs can be fabricated with the same process rule. On the other  hand, more expensive LSI fabrication equipment is necessary  to produce a PIC. This means that large-scale production is  necessary to ensure cost effectiveness. Optical interconnections are in demand for several short-reach consumer and industrial  applications in addition to high-end IT/NW applications. These  require higher density, reduced noise, and lower weight. Highpotential applications for huge-scale production of short-reach  interconnections will be realized in the near future. Optical  interconnections using silicon photonics should be optimized  for short-reach applications to accelerate their deployment in  these areas. However, the cost of these applications needs to be  lowered. High-precision alignment has been a serious  challenge to lowering the cost of optical interconnections throughout the history of optical communications. In general, an accuracy of 1/10 of the core size is required for a stable  optical connection. Precise alignment forces the use of special  equipment such as an active alignment machine and increases  the cost for precise parts. There are two types of optical  transmission wiring: single mode for longer transmissions with  a high adjustment accuracy of 1 μm, and multimode for shorter  transmissions with a reduced adjustment accuracy of 5 μm. An  adjustment accuracy of less than 1 μm is usually applied for silicon photonics packaging and sacrifices mass production. On  the other hand, recent assembly machines with an accuracy of  5 μm have been used for mounting small electrical parts in  mass production. A multimode transmission configuration is considered suitable for mass production with assembly  machines. Therefore, we propose using a multimode optical  interconnection for the silicon photonics transceiver. Singlemode applications such as WDM/switches for long reaches may be merged to produce a sufficient volume for short-range  applications. In this case, a reasonably low-cost optical  transmission for short reaches up to long reaches of up to 2 km  can be realized by using unified processes for high-volume  manufacturing. B: Applied wavelength The applicable optical wavelength region ranges from 1.3 μm to 1.55 μm because of the transparency of the silicon  waveguide. Advances have realized a sufficiently low  propagation loss in this wavelength region [4]. For a shortreach interconnection, multimode optical fibers of 0.85 and 1.3 μm have already been standardized. Additionally, a polymer  waveguide and plastic optical fiber are also applicable for 0.85 and 1.3 μm wavelengths. A working wavelength of 1.3 μm was chosen for the optical I/O core. Experimental results of 25 Gbps and 820 m transmission distance have been reported for a graded index (GI) fiber optimized to 1.3 μm [5]. Because the  wavelength of 1.3 μm is permitted a higher power transmission than 0.85 μm in eye safety specifications, a wider dynamic  range is required for higher speed transmissions of over 25 Gbps in the future. IV. OPTICAL I/O CORE A: Packaging concept Our proposed optical I/O core is a new form-factor with  optical/electrical functions built on a chip-scale silicon  Fig.3 Concept of optical I/O core decoupling capacitors on a silicon photonics platform. A Fabry–Perot laser diode (FP-LD) was also mounted by using a passive alignment technique on the transmitter of the optical  I/O core. Optical pins are a newly developed optical coupling  structure and discussed later. The electrical pads were formed  photonics substrate. Figure 3 compares the concepts of the  optical I/O core and a conventional optical transceiver. In conventional optical  transceivers, each common  optical/electrical function and customized optical/electrical  component are assembled on a substrate. With the proposed  concept, optical/electrical functions are packaged in the optical  I/O core. The optical I/O core has a unified optical/electrical  interface. By attaching optical/electrical components designed  to fit the unified interface, various kinds of customized optical  transceivers can be formed. The optical I/O core can create  various kinds of optical interconnection systems by combining  peripheral components and equipment. B: Target specifications and fundamental structure of  optical I/O core Table 1 lists the target specifications of the optical I/O  Table 1 Target specifications Fig.4 Photographs and functional blocks  of optical I/O cores on the surface of the glass by using a through glass via (TGV)  formed with a cover glass. C: Optical coupling structure cores. The target data rate per channel ranged from 25 to 28 Gbps. Both the Tx and Rx were 5 mm × 5 mm in size and had 4-, 8-, or 12-channel I/Os. The power consumption per data rate of  ICs was designed to be 5 mW/Gbps to suppress heat on the  small chip. Low power consumption was achieved with 1 V CMOS circuits (driver and transimpedance amplifier (TIA)). The driver and TIA were basically driven at 0.9 V. A supply  voltage of 3.3 V was used to control the modulator bias and  photodiode (PD) bias. The wavelength of the light sources was 1.3 μm, and the transmission medium was MMF as mentioned  above. Figure 4 shows photographs of the optical I/O cores and  their fundamental functions. This figure shows a 4-channel transmitter and receiver for  each optical I/O core. Mach–Zehnder type modulators, grating  couplers (GC) for optical I/Os, and a spot size converter (SSC)  for coupling to LD were integrated on the silicon photonics  platform of the transmitter [7, 8]. Surface-illuminated PIN  photodiodes with a diameter of 30 μm were fabricated on the  silicon photonics of the receiver to connect the multimode  optical transmission line, as shown in Fig. 4 [9]. Figure 5  shows a cross-section of the optical I/O cores along with the  main functional structures, such as the modulator, GC, and  optical pins. The optical I/O core was formed by assembling  optical pins, a cover glass, a CMOS IC (driver or TIA), and  Fig.5 Cross-section of optical I/O cores Optical pins are a vertical polymer multimode waveguide array [9] and have been newly developed for optical I/O. Figure 6 shows the designed optical pins [10]. A narrower  pitch for the coupling of optical arrays was achieved by using  optical pins. In addition, the optical pins can create a flat and  wide connecting area on top of the optical I/O core to mount  optical connecting parts with the function of controlling the optical mode field. The height of the optical pins, which can be  adjusted to the height of the cover glass, was 300 μm to be higher than the mounted IC. Optical pins with a minimum pitch  of 125 μm were confirmed. Each optical pin consisted of a core  and cladding using two types of UV curable resin with  different refractive indices. The optical pins were fabricated on  the silicon photonics platform by using a photolithographic  technique. The optical pins were formed with a tapered shape  and tilt angle by tuning the UV exposure condition [8]. By  LX MMF were evaluated [5]. The transmission rate was  25.78125 Gbps. Figures 7(a) and (b) show Tx optical eye  patterns and Rx electrical eye patterns, respectively, after MMF  transmission. A pseudo-random binary sequence (PRBS) pattern of 231-1 was applied. Clear eye openings and error-free  operation were obtained up to 300 m[10]. Stable operation was also confirmed under temperature changes from 20 °C to 75 °C. Fig.7 Tx optical eye patterns (a) and Rx electrical eye patterns (b) applying the tapered shape, the beam size of the incident light  was controlled, and the optical coupling tolerance against outer  waveguides was optimized. In the Rx, the core size of the  optical pins on the outer-waveguide side was set to a diameter  of 70 μm to get enough coupling tolerance with the light from  the GI50 MMF. To illuminate the small photosensitive area of  the PDs, the core diameter on the silicon photonics platform  side was set to 30 μm. The incident light from the MMF was fully transmitted to the photodetector by using high refractive  index of >NA 0.4 between the core and cladding. In the Tx, the  core size of the optical coupling pin was set 35 μm, and the tilt  angle was set to 8° based on the output angle from the GC. The  output light from the GC was fully maintained in the optical  pin’s core, which had a smaller diameter than the GI50 MMF  core. By applying the tilt angle, the beam-divergence angle was kept small, and the generation of a higher-order propagation  mode at the optical pins was suppressed. Figure 6 plots the measured and calculated coupling  losses due  to  the  misalignment between the GI-50 MMF and Rx optical pin. The  dependence of the measured coupling loss on the displacement  was similar to the calculated results. The measured minimum  coupling loss was 0.41 dB, and the coupling tolerance was expected to be approximately 34 μm. Based on this result, the  actually fabricated optical pin had sufficient coupling tolerance  to satisfy the target for mass production. The figure shows the  measured and calculated coupling losses due to misalignment  between the GI50 MMF and Tx optical pin. The gap between  the GI50 MMF and glass plate on the optical pin was 10 μm,  which was filled with index matching oil for the measurement. The measured minimum coupling loss was 0.49 dB. The  alignment tolerance was 10 μm, which is smaller than the  calculated value but satisfies the alignment tolerance target. D: PERFORMANCE EVALUATION To test the performance of the optical I/O cores, the 25Gbps transmission characteristics with the Corning Clearcurve  Fig.8 BER characteristics Figure 8 shows the BER characteristics at each temperature. The minimum average received power at 20 °C was -8.5 dBm after a 300 m transmission[11]. This performance indicates that  optical I/O cores are applicable to interconnection reaches of over 300 m (e.g., in a data center). V. APPLICATION As shown in Fig. 9, optical I/O cores are suitable for many  applications: optical backplanes not only for rack-to-rack but  also board-to-board connections, built-in systems by the changing of peripheral parts such as optical/electrical  connectors, or custom-designed substrates. To achieve a lowcost module, fiber connectors should be mounted on optical I/O  cores by passive or visual alignment. To apply such alignment  Fig.6 Structure of optical coupling pin and Insertion loss techniques on a multi-mode waveguide, a transmission medium  like a GI50 MMF is indispensable because it has a large  alignment tolerance with regard to the optical components. A prototype of an active optical cable is presented as an example  application of the optical I/O core in the first stage of the  project. In the second step, multiple optical I/O cores were built  in an organic LSI interposer, and mid-scale integration was  obtained with roughly 3 Tbps. Optical I/O cores with a small  footprint and high-density I/O are suitable for the built-in LSI  package. Several optical I/O cores can be mounted around  wide-bandwidth LSIs, such as field-programmable gate arrays  (FPGAs). Connecting wide-bandwidth LSIs such as FPGAs or  CPUs by optical I/O cores will be effective at scaling up servers with associated CPUs and for disaggregation to achieve  free connection among CPUs or memories[12]. Fig. 9 shows  16 optical I/O cores mounted around an FPGA card. We  demonstrated that an electrical signal of 25 Gbps from the  wide-bandwidth LSI was converted to an optical signal by an  optical I/O core, and the optical signal was looped back to the  other optical I/O cores. A 25-Gbps/ch error-free transmission  Fig.9 Application using optical I/O cores with a wide-bandwidth LSI was  confirmed without clock data recovery (CDR). This application  should consume a low amount of power. Future work will  involve realizing an on-chip server consisting of CPU and  memories mounted on a silicon photonics interposer. Acknowledgment This research was partly supported by the New Energy and  Industrial Technology Development Organization (NEDO). "
Tighter time analysis for real-time traffic in on-chip networks with shared priorities.,"The Network-on-Chip (NoC) is the preferred interconnection medium for massively parallel platforms. Targeting real-time applications, fixed-priority based NoCs with virtual channels have been proposed as a promising solution. In order to verify if specific time requirements can be satisfied, schedulability tests are typically used. Several analysis approaches have been proposed targeting priority-based NoCs. However, due to the approximation considered in the analyses, the results may involve a large amount of pessimism. The applicability of the analyses is thus limited in practice. In this paper, we identify a number of properties of NoCs with shared priorities. An improved time analysis is proposed where pessimism can be significantly reduced for many cases. In order to evaluate the proposed analysis, a number of experiments have been generated along with a case study based on an automotive application. The improvement can be clearly observed from the evaluation results.","Tighter Time Analysis for Real-Time Trafﬁc in On-Chip Networks with Shared Priorities Meng Liu, Matthias Becker, Moris Behnam, Thomas Nolte M ¨alardalen University, V ¨aster ˚as, Sweden Email: {meng.liu, matthias.becker, moris.behnam, thomas.nolte}@mdh.se Abstract—The Network-on-Chip (NoC) is the preferred interconnection medium for massively parallel platforms. Targeting real-time applications, ﬁxed-priority based NoCs with virtualchannels have been proposed as a promising solution. In order to verify if speciﬁc time requirements can be satisﬁed, schedulability tests are typically used. Several analysis approaches have been proposed targeting priority-based NoCs. However, due to the approximation considered in the analyses, the results may involve a large amount of pessimism. The applicability of the analyses is thus limited in practice. In this paper, we identify a number of properties of NoCs with shared priorities. An improved time analysis is proposed where pessimism can be signiﬁcantly reduced for many cases. In order to evaluate the proposed analysis, a number of experiments have been generated along with a case study based on an automotive application. The improvement can be clearly observed from the evaluation results. I . IN TRODUC T ION The Network-on-Chip (NoC) is an interconnection medium for massively parallel platforms, which is typically used in a System-on-Chip (SoC). Compared to conventional bus and crossbar based interconnections, NoCs can achieve better scalability and power efﬁciency. Wormhole-switching is a commonly used technique for NoCs [1]. Under such a mechanism, each packet is divided into a number of ﬂits. Once a router receives a ﬂit, it can directly transmit the ﬂit without waiting for the arrival of the complete packet. Compared to a storeand-forward mechanism, wormhole-switching requires much smaller buffers, which suits the design principles of NoCs. A number of NoC designs have been proposed in the literature (e.g. [2][3][4]). In this paper, we focus on NoCs with Virtual-Channels (VC) [5]. Using the virtual-channel technique, each physical channel can be shared by multiple virtual-channels. A transmission control can be applied at the output port of each router, so that preemptions between virtual-channels are supported. Since a ﬂit is the minimum operation unit in NoCs, preemptions are typically allowed between transmissions of different ﬂits but not during the transmission of a single ﬂit. Arbitration between different VCs can be achieved in different ways (e.g. priority based, round-robin based, etc.). In this paper, we use priority-based arbitration which is favored for many real-time applications. Each VC is associated with one priority level, and multiple message ﬂows can share the same priority/VC. This work has been supported by the Knowledge Foundation (KKS) through the projects PREMISE and DPAC. 978-1-4673-9030-9/16/$31.00 c(cid:13)2016 IEEE In order to use NoCs for real-time applications, the time behavior of the network should be predictable, so that system designers can verify if speciﬁc time requirements can be satisﬁed or not. One typical time requirement is regarding transmission deadlines. In other words, the transmission latency (also called response time hereinafter) of a packet should not exceed the given deadline. This paper focuses on time analysis of NoCs targeting such type of requirements. Targeting priority-based NoCs with shared virtual-channels, a Response-Time-Analysis (RTA) has been proposed in the literature [6][7]. However, due to the approximation used in this RTA, the analysis results include a large amount of pessimism for certain cases. Since pessimistic results may cause waste of system capacity as well as unnecessary effort on redesigning the system, the applicability of the above analysis is thus limited in practice. Therefore, in this paper, we propose an improved RTA for NoCs with shared VCs where pessimism is reduced, so that the applicability can be improved accordingly. A. Contribution This paper presents the following contributions: • A number of properties are identiﬁed to reduce the pessimism included in the RTA of NoCs with shared VCs. Based on the identiﬁed properties, an improved RTA is proposed along with the proof of correctness. • A number of experiments as well as a case study based on an automotive application are generated, in order to evaluate the performance of the proposed RTA. Compared to the original RTA, a signiﬁcant improvement can be observed from the results. The remainder of this paper is organized as follows. Section II describes the main related results in the literature. In Section III, we present the system model considered in this paper. In Section IV, we recapitulate the existing RTA for NoCs with shared VCs. Then in Section V, we identify a number of properties to reduce pessimism, and propose an improved RTA accordingly. The evaluation of the proposed analysis is presented in Section VI. Finally, in Section VII we conclude the paper along with some ideas of future work. I I . R ELAT ED WORK A number of works have been presented in the literature targeting support of real-time applications on NoC based platforms. As discussed in [8], providing guarantees to a real-time application is typically achieved either by modifying hardware to support contention control, or by analyzing traverse times of ﬂows on existing NoCs. Based on the ﬁrst approach, a number of NoC designs have been proposed, such as the Æthereal NoC [9], time-triggered NoCs [10][11], the Back Suction ﬂowcontrol scheme [12], and ﬁxed-priority based NoCs [13][6]. In order to analyze real-time trafﬁc over NoCs, several approaches have been proposed. Targeting NoCs with a RoundRobin Arbitration (RRA) scheme, the authors in [14] present a Network-Calculus based approach to compute end-to-end latency of a NoC ﬂow. To reduce pessimism involved in the Network-Calculus based analysis, the authors in [15] propose a Recursive Calculus (RC) to calculate ﬂow latencies. This work is improved in [16] to a more general analysis with less pessimism. Targeting ﬁxed-priority based NoCs, the authors in [13] present a RTA based on the well-known analysis for task scheduling [17]. In [18], a stage-level latency analysis is proposed. It has been shown that this approach can compute tighter estimates compared to the analysis presented [13]. However, this analysis is based on a condition where the buffer is large enough such that a ﬂow cannot be delayed because the buffer at a downstream router is full. If buffer sizes are not sufﬁcient, this analysis can result in optimistic estimates. Unfortunately, such a condition for NoCs is not realistic. Practically, the buffer size is limited to hold only a few ﬂits (e.g. 3 ﬂits in [2], 24 ﬂits in [19]). Moreover, the analyses proposed in [13] and [18] only focus on NoCs with distinct priorities. In such type of NoCs, each ﬂow has a unique priority, which requires a separated virtual-channel. As a result, the buffer cost increases signiﬁcantly as the number of ﬂows goes up, which makes the above approaches less practical. To overcome this limitation, in [6], the authors use a priority sharing policy to reduce the number of required virtual-channels. Additionally, a corresponding RTA is also proposed. This RTA is further revised in [7] by removing optimism for certain cases. However, because of the approximation considered in this analysis, the results involve a large amount of pessimism for many cases. Therefore, in this paper, we identify a number of properties for ﬁxed-priority based NoC with shared VCs, and propose an improved analysis to provide tighter analysis results. I I I . SY ST EM MOD EL The system considered in this paper consists of m × m computing cores which are connected by a priority-based wormhole-switched NoC with virtual-channels (similar to [6]). An abstraction of the architecture is shown in Fig. 1. The NoC is based on a 2D mesh-based topology which has been used for many research works (e.g. [13][20][21]) as well as commercial implementations (e.g. [2][3]). Each VC is associated with one priority level, and has a speciﬁc buffer at each router. In this paper, we assume a low buffer cost model that the buffer of each VC holds one ﬂit. Flit-level preemptions through VCs are allowed. Therefore, a ﬂow using a VC with a higher priority can preempt the transmission of a ﬂow transmitted in a VC with a lower priority. Preemptions can occur between the transmissions of different ﬂits but not during the transmission Fig. 1. The abstracted architecture of an example NoC with 8 virtual-channels. of a single ﬂit. A buffer control is applied at each router, which can be achieved by a credit-based strategy (e.g. [22]). At a certain router, a ﬂow with a higher priority may not be able to transmit ﬂits to the output link, because the buffer at the next router is full. In this case, a ﬂow, which uses a VC with a lower priority, can get access to the output link. In other words, a ﬂow cannot occupy a physical link without transmitting any ﬂits. Each VC can be shared by multiple ﬂows. At each router, the arbitration of ﬂows using the same VC uses a First-In-FirstOut (FIFO) mechanism. The network consists of a set of n periodic or sporadic ﬂows F = { f1 , fn}. A ﬂow f i can be characterized by (Li , Ti , Di , Pi , Ri ). Li represents the packet size of f i (including the payload and the header). Ti denotes the period of a periodic ﬂow or the minimum inter-arrival time of a sporadic ﬂow. Di is the relative constrained deadline of f i (i.e. Di ≤ Ti ). A ﬂow is deﬁned as schedulable if its maximum latency (also called Worst-Case Response Time (WCRT) hereinafter) is no larger than its deadline. The whole network is deﬁned as schedulable if all the ﬂows in the network can meet their deadlines. Each ﬂow has a ﬁxed priority denoted by Pi , and a ﬁxed path (also called route) Ri which starts from its source node (denoted as Sri ) and ends at its destination (denoted as Dsi ). f2 , ..., IV. R ECA P I TU LATE THE EX I ST ING RTA In [6], the authors present the ﬁrst response time analysis for NoCs with shared priorities, which is later revised in [7] by removing the optimism. As discussed in [7], the response time of a NoC message is mainly affected by four factors: direct and indirect interference, direct and indirect blocking. f i is the ﬂow under analysis. As presented Assume that earlier, the arbitration between ﬂows with the same priority uses a FIFO mechanism, which means that the transmissions of these ﬂows are pipelined and they cannot bypass each other. Therefore, a ﬂow fn (Pn = Pi ), which shares at least one physical link with f i , can block the transmission of f i . This type of delay that fn causes to f i is called direct blocking. On the other hand, a ﬂow fm (Pm = Pi ), which does not share any link with f i , can still block the transmission time of f i . This situation occurs when an intervening ﬂow fn exists, where fn has the same priority with f i (Pn = Pi = Pm ) and shares physical links with both f i and fm . In this case, fm can directly block the transmission of fn . If fn is queued ahead of f i , the transmission of f i can be blocked as well because of the FIFO mechanism utilized within each virtual-channel. Such type of delay that fm causes to f i is called indirect blocking. A ﬂow f j (Pj > Pi ), which shares at least one physical link with f i , can preempt and delay the transmission of f i , because the access of physical channels uses priority-based arbitration. This type of delay that f j causes to f i is called direct interference. Sometimes, even if a ﬂow fk (Pk > Pi ) does not share any link with f i , fk can still affect the response time of f i . This situation happens when an intervening ﬂow f j exists, where f j has a medium priority (i.e. Pk > Pj > Pi ) and shares physical links with both f i and fk . In this case, fk can cause direct interference to f j which can decrease inter-arrival times between successive instances of f j . The direct interference that f j causes to f i can thus be affected. This type of delay that fk causes to f i is called indirect interference. The WCRT of a NoC ﬂow can then be acquired by ﬁnding the upper-bounds of the above four types of delay [7]. A ﬂow, which causes either direct or indirect blocking to f i , can completely delay the transmission of f i . In other words, f i cannot proceed until this ﬂow is transmitted. Therefore, ﬂows, that can cause direct or indirect blocking to f i , are treated in the same manner during the analysis. Given a time duration ∆t , the blocking that a ﬂow fn can cause to f i can be upperbounded by (cid:100) ∆t (cid:101) · Cn . If the WCRT of f i is Ri , the maximum blocking that f i may get within the time duration of Ri can then be calculated as Tn Bi (Ri ) = ∑ ∀ fn ∈SB i (cid:101) · Cn (cid:100) Ri Tn (1) i T j where SB is the ﬂow set including all the ﬂows which can cause direct or indirect blocking to f i . Given a certain time duration ∆t , the direct interference that f j can cause to f i can be upper-bounded by (cid:100) ∆t (cid:101) · C j . As discussed in [13], the effect of indirect interference can be captured by adding an extra jitter (called interference jitter) to the intervening ﬂow. Assume that indirect interference happens on f i through f j . By taking the effect of indirect interference into account, the interference from f j can be upper-bounded (cid:101) · C j . An upper-bound of the interference jitter j ) can be approximately computed as R j − C j , where R j (J I represents the WCRT of f j [13]. Moreover, if a ﬂow f p (Pp > Pn ) causes direct interference to fn while fn blocks f i fn ∈ SB (i.e. i ), the transmission of f p can delay f i as well. Therefore, any ﬂow, which can preempt the transmission of a ﬂow fn ( fn ∈ SB i ), is approximately treated as ﬂows that can cause direct interference to f i [7]. Accordingly, if the WCRT of f i is Ri , the maximum interference that f i may experience within the time duration of Ri can be computed as by (cid:100) ∆t+J I T j j Ii (Ri ) = ∑ ∀ f j ∈h p(i) h p(i) =( (cid:100) Ri + J I T j j SD n ) ∪ SD i (cid:91) ∀ fn ∈SB i (cid:101) · C j (2) where SD represents the ﬂow set including all the ﬂows that can cause direct interference to f i . i The WCRT of f i can then be calculated by1 Ri = Ci + Bi (Ri ) + Ii (Ri ) (3) where Bi (Ri ) and Ii (Ri ) are computed using Eq. 1 and Eq. 2 respectively. Ci is the basic transmission time of f i over Ri without experiencing any blocking or interference, which can be computed as (4) Ci = ((cid:100) Li (cid:101) + noh(Sri , Dsi ) − 1) · τ Rn+1 i = Ci , i = Rn i = Ci + Bi (Rn i ) + Ii (Rn ς where ς represents the size of a ﬂit, τ is the transmission time of a single ﬂit over one physical link (which typically takes one or a few cycles), and noh(Sri , Dsi ) denotes the number of hops within Ri . Since Ri appears on both sides of Eq. 3, it can be solved using ﬁxed-point iterations [17]. Eq. 3 can be reformatted as2 i ). The iteration starts with R0 and terminates with either of the following two conditions: (1) a convergence of Ri is reached (i.e. the smallest Rn+1 i is found); (2) the computed Ri exceeds its deadline (i.e. Ri > Di ). When the ﬁrst condition occurs, the WCRT of f i is found as Ri which is smaller than its deadline. f i is then determined to be schedulable. On the other hand, when the second condition happens, f i misses its deadline which is thus determined to be unschedulable. The correctness of the above analysis has been proven in [7]. However, in order to simplify the computation, the above analysis involves approximations. Consequently, the analysis results may include signiﬁcant pessimism for many cases (i.e. the computed upper-bounds are much higher than the actual WCRTs). The applicability is thus limited in practice, since the pessimism can result in waste of system resources. Therefore, in the following subsections, we reduce the pessimism involved in the analysis, in order to improve the applicability in general. V. IM PROV ED T IM E ANA LY S I S In this section, we ﬁrst identify a number of properties, which can be used to reduce the pessimism involved in the RTA. Then an improved RTA is proposed based on these identiﬁed properties. The main source of pessimism is the approximation of considering the whole path of f i as one exclusive resource. In other words, a ﬂow which passes any link within Ri is considered to traverse through the whole Ri . Such an approximation indeed simpliﬁes the analysis, but can result in a large amount of pessimism, since parallel transmissions are not well considered. The following deﬁnitions are used to simplify the presentation. 1Note that a ﬂow may get blocking from another ﬂow with a lower priority at a certain router because the transmission of a single ﬂit is non-preemptive. However, such a blocking delay is constant and quite small (i.e. at most τ). Without loss of generality, this type of blocking is incorporated into the basic transmission time of each ﬂow to simplify the presentation. 2We use superscript to present the count of iteration. For example, Rk represents the WCRT of f i computed after the kth iteration. i Deﬁnition 1. The i-level trafﬁc includes all the transmissions which can cause interference or blocking to f i , including the transmissions of f i itself. Deﬁnition 2. The i-level busy-period is the time duration since the amount of the i-level trafﬁc becomes greater than 0 until it turns back to 0. Accordingly, the computation of the WCRT of f i is actually the calculation of the i-level busy-period. A. Pessimism in Computed Blocking First, we identify a number of properties that can be used to reduce pessimism involved in the computation of the blocking delay (i.e. Eq. 1). fn (∀ fn ∈ SB As shown earlier, Eq. 1 considers that multiple instances of i ) can block f i which is under analysis. However, in [23], the authors have shown that this consideration is not necessary for certain ﬂows. Property 1. If a ﬂow f p ( f p ∈ SB i ) encounters f i at the source node of f i (i.e. Sri ), f p can block f i at most once [23]. For example, in Fig. 2, f3 can block f2 at most once, and f1 can block f2 multiple times. Accordingly, we ﬁrst revise Eq. 1 as Bi (Ri ) = ∑ ∀ f p ∈SB ∧Ni, p=Sri i Cp + ∑ ∀ fn ∈SB ∧Ni,n (cid:54)=Sri i (cid:101) · Cn (cid:100) Ri Tn (5) where Ni, p represents the node where f i and f p encounter for the ﬁrst time. Fig. 2. An example of three NoC ﬂows. All the ﬂows share the same VC. In Eq. 1, the computation is based on the consideration that all the ﬂows in SB i are released simultaneously with f i [7]. These ﬂows are then considered to be pipelined ahead of f i , and will be transmitted sequentially. However, this worst-case assumption cannot happen in reality. Now we assume f2 is under analysis in the example shown in Fig. 2. We notice that if f1 and f2 are released at the same time, the transmission of f1 over Link(C − D) is actually in parallel with the transmission of f2 over Link(B − C). Due to this type of parallel transmission, we can get the following property. Property 2. Before an i-level trafﬁc reaches Ni,n (∀ fn ∈ SB i ), the transmission of fn cannot affect the corresponding i-level busy-period. Proof. This property can be proven by contradiction. Assume that at time instant t1 , f p is transmitted on the links after Ni,n . At the same time, an i-level trafﬁc reaches another node Nx , where Nx locates before Ni,n in Ri . The i-level trafﬁc reaches Ni,n at t2 (t2 > t1 ). Now we assume that the transmission of fn can delay the i-level trafﬁc within the time duration [t1 , t (cid:48) 2 ] (t (cid:48) 2 < t2 ). This implies that the workload during this time interval on the link Link(Nx , Ni,n ) either cause blocking or interference to the i-level trafﬁc. According to Deﬁnition 1, this workload belongs to the i-level trafﬁc as well. This means that the ilevel trafﬁc already reaches Ni,n at t (cid:48) 2 which is earlier than t2 . This contradicts the condition that the i-level trafﬁc reaches Ni,n at t2 . Based on Property 2, we can get the following theorem. Theorem 1. A message fn ( fn ∈ SB i ) causes the maximum direct blocking to f i when fn and the i-level trafﬁc arrive simultaneously at node Ni,n . Proof. We deﬁne that the i-level trafﬁc arrives at Ni,n at time t0 , and fn arrives at Ni,n at time t1 . We need to prove that fn can cause more blocking to f i when t0 = t1 compared to the cases where t0 (cid:54)= t1 . Case 1: t0 < t1 In this case, the i-level trafﬁc arrives at Ni,n earlier than fn . When fn arrives, the output-link of Ni,n is already occupied by the i-level trafﬁc. Thus the whole packet of fn needs to compete for the link access with the i-level trafﬁc (i.e. there is no parallel transmission). Therefore, as long as fn arrives later than the i-level trafﬁc, fn can completely cause direct blocking to f i (i.e. the transmission of fn will increase the i-level busyperiod). Accordingly, if we change t1 to any other earlier time 1 (t0 ≤ t (cid:48) 1 ≤ t1 ), the i-level busy-period will not be decreased. In other words, fn can cause no less direct blocking to f i when t0 = t1 compared to the case when t0 < t1 . t (cid:48) Case 2: t0 > t1 In this case, the i-level trafﬁc arrives at Ni,n later than fn . When fn arrives at Ni,n , the i-level trafﬁc has not arrived yet, fn can thus start its transmission. The transmission of fn during the time interval [t1 , t0 ) is in parallel with the transmission of the i-level trafﬁc (i.e. Property 2). Therefore, the transmission of fn during [t1 , t0 ) will not increase the i-level period at all. This implies that fn can cause no less direct blocking to f i when t0 = t1 compared to the case when t0 > t1 . Based on the above proof, we can conclude that cause the largest direct blocking to f i when t0 = t1 . fn can According to Theorem 1, while computing the blocking that fn (∀ fn ∈ SB i ) can cause to f i in Eq. 5, the release time of fn can be postponed to the time instant when the header of the i-level trafﬁc reaches Ni,n . Assume that f i is released t1 (t0 ≤ t1 ). In order to maximize the response-time of f i , to the network at t0 . The i-level trafﬁc and fn reach Ni,n at parallel transmissions should be minimized, which means that t1 − t0 should be minimized. A lower bound of t1 − t0 can be computed by only considering the basic transmission time of the head ﬂit of the i-level trafﬁc from Sri to Ni,n without any blocking or interference. Therefore, Eq. 5 can thus be revised as (cid:100) Ri − βn Bi (Ri ) = ∑ Cp + ∑ (cid:101) · Cn Tn (6) ∀ f p ∈SB ∧Ni, p=Sri i ∀ fn ∈SB ∧Ni,n (cid:54)=Sri i βn =noh(Sri , Ni,n ) · τ where noh(Sri , Ni,n ) represents the number of hops from Sri to Ni,n . Property 3. Once the header of f i reaches Ni,n (∀ fn ∈ SB i ), the later arrived instances of fn cannot block f i any more. Property 3 can be easily proven by the feature presented in Section III that the arbitration between ﬂows sharing the same priority level uses a FIFO mechanism. Once the header of f i reaches Ni,n , the later arrived instances of fn will be queued in the buffer of Ni,n after f i , and their transmission cannot bypass f i . Therefore, these instances do not block f i at all. For example, in Fig. 3, once the header of f2 reaches node B, later arrived instances of f1 cannot block f2 any more. In order to maximize the blocking that fn can cause to f i , the header of f i should reach Ni,n as late as possible. Fig. 3. An example of four NoC ﬂows. All the ﬂows share the same VC. ς (cid:101) + Theorem 2. Assume that f i ﬁnishes its transmission (i.e. the last ﬂit of f i reaches its destination) at time t f . The header of f i reaches Ni,n (∀ fn ∈ SB i ) no later than t f − ((cid:100) Li noh(Ni,n , Dsi ) − 1) · τ. Proof. Assume that the header of f i reaches Ni,n at t1 , and that the basic transmission time of f i from Ni,n to Dsi without any blocking or interference is ∆C. Due to the fact that the t f − t1 ≥ ∆C. Similar to Eq. 4, ∆C can be computed as ((cid:100) Li basic transmission time of f i is non-avoidable, we can get noh(Ni,n , Dsi ) − 1) · τ. Accordingly, we can get t1 ≤ t f − ((cid:100) Li noh(Ni,n , Dsi ) − 1) · τ. Therefore, the header of f i reaches Ni,n no later than t f − ((cid:100) Li ς (cid:101) + noh(Ni,n , Dsi ) − 1) · τ. According to Property 3 and Theorem 2, while computing fn (∀ fn ∈ SB the maximum blocking that i ) can cause to fn that are released after t f − ((cid:100) Li f i , the instances of noh(Ni,n , Dsi ) − 1) · τ can be ignored. Eq. 6 can be further revised as (cid:100) Ri − βn ς (cid:101) + ς (cid:101) + ς (cid:101) + Bi (Ri ) = ∑ Cp + ∑ (cid:101) · Cn ∀ f p ∈SB ∧Ni, p=Sri i ∀ fn ∈SB ∧Ni,n (cid:54)=Sri i Tn (7) βn =(noh(Sri , Ni,n ) + ((cid:100) Li (cid:101) + noh(Ni,n , Dsi ) − 1)) · τ ς As discussed in Section IV, in Eq. 1, ﬂows that can cause direct or indirect blocking to f i are treated in the same manner. The reason is that a ﬂow which can cause direct or indirect blocking to f i can block the whole VC, which directly delays all the workload in the VC including f i . Such a consideration is based on the approximation that the whole path is treated as one exclusive resource. However, when we take more detailed route information into account, we notice that some of the indirect blocking ﬂows cause much less effect on f i compared to the delay computed in Eq. 1. To simplify the presentation, m,n ). Property 4. Assume that we introduce a notation N (cid:48) m,n which represents the last node in the shared path between fm and fn (i.e. the shared links between fm and fn starts from Nm,n and ends at N (cid:48) fm causes indirect blocking to f i through an intervening ﬂow fn . If Nm,n is in prior to Ni,n in Rn , the transmission of fm cannot block f i . Proof. Nm,n is in prior to Ni,n in Rn implies that when fn starts to block f i , the header of fn must have passed Nm,n and reached Ni,n . Assume that the header of fn reaches Nm,n at time t0 and starts to block f i at t1 , thus we know that t0 < t1 . The instances of fm which are released before or at t0 can block fn but not f i , because fn has not reached Ni,n yet. On the other hand, according to Property 3, the instances of fm released after t0 cannot block fn which cannot further block f i . This completes the proof. Based on Property 4, we can replace the ﬂow set SB with in Eq. 9 i SB(cid:48) i = {SB i \ fm | ∀ fm ∈ SIB i ∧ fn ∈ SB i ∧ Nm,n ≺ Ni,n } (8) i where SIB represents all the ﬂows which causes indirect blocking to f i , and fm causes indirect blocking to f i through fn . The operation A\a means removing the item ’a’ from A, and the expression Nx,k ≺ Ny,k denotes that node Nx,k is in prior to Ny,k in Rk . In the example shown in Fig. 3, while computing the blocking delay of f1 (i.e. B1 ), f3 can be excluded from SB 1 . Note that in Property 4, fm cannot block f i does not mean that fm cannot affect the WCRT of f i . fm can decrease the inter-arrival times of successive instances of fn , which further affects the WCRT of f i . Such a behavior is similar to the effect of indirect interference (see Section IV). Therefore, similar to the solution of handling indirect interference, we add an extra jitter to fn (called blocking jitter and denoted as J B n ) in order to take the effect caused by fm into account. Similar to interference jitter, an approximate upper-bound of J B n is computed as Rn − Cn . To this end, Eq. 9 can be ﬁnally revised as n − βn Bi (Ri ) = ∑ Cp + ∑ (cid:101) · Cn (cid:100) Ri + JB Tn ∀ f p ∈SB(cid:48) ∧Ni, p=Sri i ∀ fn ∈SB(cid:48) ∧Ni,n (cid:54)=Sri i (9) βn =(noh(Sri , Ni,n ) + ((cid:100) Li (cid:101) + noh(Ni,n , Dsi ) − 1)) · τ i ς where SB(cid:48) is presented by Eq. 8, and J B n = Rn − Cn . B. Pessimism in Computed Interference In this section, we present a number of properties to reduce the pessimism involved in the computed interference (i.e. Eq. 2). Property 5. Before an i-level trafﬁc reaches Ni, j (∀ f j ∈ SD i ), the transmission of f j cannot affect the corresponding i-level busy-period. This property can be proven by contradiction, which is the same as Property 2. Due to the space limitation, we do not repeat the proof here. Theorem 3. A ﬂow f j ( f j ∈ SD i ) causes maximum direct interference to f i , when f j and the i-level trafﬁc arrive simultaneously at node Ni, j . The proof of Theorem 3 is very similar to Theorem 1. It can be proven that the interference caused by f j when it arrives simultaneously at Ni, j with the i-level trafﬁc, is no smaller than the interference caused by any other release times of f j . Therefore, we ﬁrst revise Eq. 2 as j − β j (cid:101) · C j (10) Ii (Ri ) = ∑ (cid:100) Ri + J I T j β j =noh(Sri , Ni, j ) · τ ∀ f j ∈h p(i) As presented in Section IV, if a ﬂow f p causes direct interference to fn ( fn ∈ SB i ), f p will be included in h p(i) even if f p does not share any link with f i . In this case, Theorem 3 cannot be applied, thus β j cannot be computed. Therefore, we divide h p(i) into two sets: (1) SD including ﬂows that can cause direct interference to f i ; (2) SD(cid:48) including ﬂows that cause interference to f i through another ﬂow fn ( fn ∈ SB i ). SD(cid:48) be simply acquired as SD(cid:48) i = { f p |∀ f p ∈ h p(i) ∧ f p (cid:54)∈ SD i }. The can computation of interference caused by ﬂows in SD(cid:48) remains the same as in Eq. 2, since Theorem 3 (as well as the theorems presented later in this section) does not hold for these ﬂows. The computation of β j in Eq. 10 can then be modiﬁed as i i i i (cid:26) noh(Sri , Ni, j ) · τ, ∀ f j ∈ SD 0, ∀ f j ∈ SD(cid:48) i i β j = (11) Theorem 4. The tail of Property 6. When the tail of f i leaves the shared links with f j ( f j ∈ SD i ), the later arrived instances of f j cannot preempt f i any more. Property 6 can be easily proven by the fact that f j will not cause any direct interference to f i when they do not share links any more. According to Property 6, the maximum interference from f j actually depends on the latest time when the tail of f i leaves the shared links with f j (i.e. when the tail of f i leaves N (cid:48) f i leaves Node N (cid:48) i, j no later than t f − noh(N (cid:48) i, j , Dsi ) · τ, where t f is the time instant when the tail of f i arrives at its destination. Proof. Assume that f i leaves N (cid:48) i, j at tx . We know that the basic transmission time of one ﬂit from N (cid:48) i, j to Dsi is noh(N (cid:48) τ. This implies that when the tail of f i leaves N (cid:48) i, j , it takes at least a time duration of noh(N (cid:48) i, j , Dsi ) · τ to reach Dsi . In other words, t f − tx is no smaller than noh(N (cid:48) i, j , Dsi ) · τ. Therefore, we have tx ≤ t f − noh(N (cid:48) i, j , Dsi ) · τ. This completes the proof. According to Theorem 4, we can revise Eq. 10 and Eq. 11 j − β j − β(cid:48) i, j , Dsi ) · i, j ). as Ii (Ri ) = ∑ i (cid:101) · C j j ∀ f j ∈SD (cid:100) Ri + J I (cid:26) noh(Sri , Ni, j ) · τ, ∀ f j ∈ SD T j (cid:26) noh(N (cid:48) 0, ∀ f j ∈ SD(cid:48) i, j , Dsi ) · τ, ∀ f j ∈ SD i i i 0, ∀ f j ∈ SD(cid:48) i β j = β(cid:48) j = which can then be simpliﬁed as j − β j Ii (Ri ) = ∑ (cid:100) Ri + J I (cid:26) (noh(Sri , Ni, j ) + noh(N (cid:48) T j (cid:101) · C j ∀ f j ∈SD i β j = i, j , Dsi )) · τ, ∀ f j ∈ SD i 0, ∀ f j ∈ SD(cid:48) i (12) Another source of pessimism is the basic transmission time of f j ( f j ∈ SD i ) considered in Eq. 2. Theorem 5. Each instance of a ﬂow f j can cause direct interference to f i no more than ((cid:100) L j ς (cid:101) + noh(Ni, j , N (cid:48) i, j ) − 1) · τ. Proof. We know the fact that f j can cause direct interference to f i only when f i and f j are competing for the shared links (i.e. links between Ni, j and N (cid:48) i, j ). Therefore, we need to prove that f j can occupy the links between Ni, j and N (cid:48) i, j for a time duration no more than ∆t (∆t = ((cid:100) L j ς (cid:101) + noh(Ni, j , N (cid:48) i, j ) − 1) · τ). This can be proven by contradiction. Assume that f j can occupy the links between Ni, j and i, j for a time duration ∆t (cid:48) (∆t (cid:48) > ∆t ). Under the wormholeswitching mechanism, once a ﬂit of f j arrives at node, the ﬂit can be directly transmitted without waiting for the arrival of the whole packet. The basic transmission time of f j from Ni, j to N (cid:48) i, j is thus bounded by ((cid:100) L j ς (cid:101) + noh(Ni, j , N (cid:48) i, j ) − 1) · τ (i.e. ∆t ). As a result, the time duration of (∆t (cid:48) − ∆t ) is occupied by f j without transmitting any ﬂits. This contradicts the behavior of NoCs as presented in Section III. Based on Theorem 5, we can further revise Eq. 12 as j − β j N (cid:48) Ii (Ri ) = ∑ i j (cid:101) · C i ∀ f j ∈SD (cid:100) Ri + J I (cid:26) (noh(Sri , Ni, j ) + noh(N (cid:48) T j (cid:26) ((cid:100) L j 0, ∀ f j ∈ SD(cid:48) (cid:101) + noh(Ni, j , N (cid:48) i, j ) − 1) · τ, ∀ f j ∈ SD C j , ∀ f j ∈ SD(cid:48) i, j , Dsi )) · τ, ∀ f j ∈ SD ς i i i i β j = C i j = (13) Finally, the WCRT of f i can be computed by Eq. 3, where Ci , Bi (Ri ) and Ii (Ri ) are calculated by Eq. 4, 9 and 13 respectively. V I . EVA LUAT ION In order to evaluate how much improvement the proposed analysis can achieve compared to the existing RTA, we have generated a number of general experiments as well as a case study using an automotive application. A. Case Study The presented case study is based on an autonomous vehicle application, which has been used in [24][21]. The application has a number of tasks performing different functionalities such as obstacle detection via stereo photogrammetry, navigation control and stability control. The inter-task communications are achieved by 38 real-time ﬂows. The application is deployed on a 4 × 4 2D mesh-based NoC, which uses XY routing. The parameters of ﬂows as well as the task mapping are kept the same as presented in [24]. We apply both the improved RTA Fig. 4. The results of the case study. (denoted as imRTA) and the original RTA [7] (denoted as oRTA) on the above system. As shown in Fig. 4, for 21 out of 38 ﬂows, the imRTA and the oRTA give equal computation results. The equal estimates appear in a number of cases where the properties identiﬁed in this paper cannot be applied, such as: when a ﬂow does not experience any blocking or interference (e.g. f18 , f20 and f28 ); when a ﬂow can get blocking but not interference, however, the deducted β j in Eq. 9 is not sufﬁcient to reduce the arrival of blocking (e.g. f1 and f23 ); when a ﬂow gets interference but not blocking, however, the deducted β j in Eq. 13 is not enough to reduce the arrival of interference (e.g. f2 ). For the rest of the ﬂows (i.e. 17 out of 38 ﬂows), at least one of the identiﬁed properties can be applied, therefore the imRTA always shows shorter WCRTs than the oRTA. The largest difference can be observed from f4 and f32 , where the WCRT computed by the oRTA is around 5.6 times longer than the WCRT calculated by the imRTA. The main reason of such a signiﬁcant decrease of pessimism for these two ﬂows is Property 4. Apparently, if a ﬂow may experience both interference and blocking, it has a higher probability to get tighter estimates using the improved RTA compared to the cases where a ﬂow only experiences either blocking or interference. However, it is difﬁcult to conclude which property contributes the most to the reduction of pessimism in general, since it highly depends on the ﬂow setting. B. General Evaluation In addition to the case study, we also implement a number of general evaluations where the system settings are randomly generated. We evaluate the proposed RTA in three aspects: latency, schedulability3 and computation time. All these evaluations are deployed in a 16 × 16 2D-mesh based NoC with 8 virtual-channels (similar to the architecture presented in [19]). The source and destination of each ﬂow are randomly selected. The utilization of each ﬂow (i.e. denoted as Ui = Ci ) is randomly selected following a uniform distribution within a range of [0.001, 0.01]. The deadline of each ﬂow is set to be equal to its period or minimum inter-arrival time (i.e. Di = Ti ). 1) Results regarding Latency: The ﬁrst group of the evaluation is regarding transmission latency. The packet size of each ﬂow is randomly selected from [10, maxL] ﬂits following a uniform distribution, where maxL is selected from {500, 1000, 5000, 10000, 20000, 40000}. For each experiment setting, we randomly generate 200 systems, and each system includes 100 ﬂows (i.e. 20000 ﬂows are collected for each setting). Ti 3 Schedulability ratio refers to the percentage of schedulable systems in all the generated experiments for each setting. The results are shown in Table I. First, we notice that the improved RTA and the existing RTA get the same results for around 30% of the generated ﬂows (i.e. the results marked as ’Equal Percentage’). In other words, improvement achieved by the new RTA can be observed in 70% of the ﬂows. As the maximum packet size goes up from 500 to 40000, the average improvement4 decreases from 16.7% to 4.9%. The main reason is that when the maximum packet size becomes larger, the reduced latencies of many ﬂows increase slower than their WCRTs. Consequently, the proportion of the reduced latency of a ﬂow in its WCRT may become smaller. On the other hand, we cannot really observe any effect of the maximum packet size on the maximum improvement. In these experiments, we can observe that for some ﬂows, the WCRTs computed by the existing RTA are around 14 times larger than the results calculated by the improved RTA. TH E IM PROV EM EN T REGARD ING TRAN SM I S S ION LAT ENCY. TABLE I Packet Size (ﬂits) 500 1000 5000 10000 20000 40000 Improvement Max Mean Std 517.6% 16.7% 24.2% 663.5% 10.9% 24.3% 1402.1% 6.2% 28.9% 882.6% 5.7% 28.9% 1402.2% 4.8% 30.4% 1212.1% 4.9% 29.8% Equal Percentage 30.5% 31.2% 31.7% 31.0% 30.5% 31.0% TH E IM PROV EM EN T REGARD ING SCH EDU LAB I L I TY RAT IO . TABLE II Number of Flows 20 40 60 80 100 Schedulability imRTA oRTA 98.5% 98.5% 94.5% 94.5% 82.5% 81.5% 73.5% 72% 53% 50.5% Number of Flows 120 140 160 180 200 Schedulability imRTA oRTA 39% 35.5% 27% 23.5% 17% 12.5% 9.5% 6.5% 3.5% 1% 2) Results regarding Schedulability: The second group of evaluation focuses on the schedulability of the whole network. The packet size of each ﬂow is randomly selected from [10, 5000] ﬂits following a uniform distribution. The number of ﬂows in each generated network is selected from [20, 200] with a granularity of 20. For each experiment setting, we randomly generate 200 systems. The results are listed in Table II. As the number of ﬂows increases from 20 to 200, the utilization of the whole network becomes higher. Consequently, the schedulability ratio using 4 The improvement is computed as VoRT A −VimRT A , where VimRT A and VoRT A are the results computed by the improved and the original RTA respectively. VimRT A the improved RTA decreases from 98.5% to 3.5%, and the schedulability ratio achieved by the original RTA decreases from 98.5% to 1%. When the number of ﬂows is low (e.g. 20 and 40), both analyses return the same results. However, when the number of ﬂows becomes higher (i.e. larger than 40), the improved RTA always achieves higher schedulability ratio compared to the original RTA. 3) Results regarding Computation Time: As discussed earlier, the proposed RTA reduces pessimism by taking more detailed ﬂow information into account. Compared to the original RTA, the improved RTA requires more processing time which affects the applicability in practice as well. Therefore, in the third group of evaluation, we measure the processing time of the proposed RTA. The packet size of each ﬂow is selected following a uniform distribution within [10, 5000] ﬂits. The number of ﬂows in each generated network is selected from [20, 140] in steps of 20. We also generate 200 systems for each setting. As shown in Fig. 5, as the number of ﬂows goes up, the processing time of both the improved RTA and the original RTA increases. The improved RTA requires more processing time compared to the original RTA as expected. However, the difference is quite acceptable. When the network contains 140 ﬂows, the average processing time of the improved RTA is 8.15 s which is 1.18 s longer than the original RTA. Fig. 5. The results of processing time. Processing time refers to the time cost for analyzing a complete ﬂow set (i.e. the whole network). V I I . CONC LU S ION AND FU TUR E WORK S In this paper, we identify a number of properties to reduce pessimism involved in the existing time analysis of prioritybased NoCs with shared virtual-channels. An improved analysis is then proposed based on the identiﬁed properties along with the proof of correctness. In order to evaluate the proposed analysis, a number of experiments as well as a case study based on an automotive application are generated. According to the evaluation results, the proposed RTA provides lower WCRTs for around 70% of the tested ﬂows compared to the original RTA. Signiﬁcantly reduced pessimism can be observed from many cases as well. On the other hand, the proposed RTA requires more processing time as a price. However, according to the measurements, the extra time cost is quite acceptable in practice. In this work, we do not have any assumption on the buffer size of each router. In order to provide safe results, the worstcase scenario is always considered (i.e. each buffer can only hold one ﬂit). As future work, we may take exact buffer size into account, which can further decrease pessimism.  [1] L. M. Ni and P. K. McKinley, “A survey of wormhole routing techniques in direct networks,” Computer, 1993. [2] D. Wentzlaff, P. Grifﬁn, H. Hoffmann, L. Bao, B. Edwards, C. Ramey, M. Mattina, C.-C. Miao, J. F. Brown III, and A. Agarwal, “On-chip interconnection architecture of the tile processor,” IEEE Micro, 2007. [3] Epiphany Architecture "
A built-in self-testing framework for asynchronous bundled-data NoC switches resilient to delay variations.,"Most multi- and many-core integrated systems are currently designed by following a globally asynchronous locally synchronous paradigm. Asynchronous interconnection networks are promising candidates to interconnect IP cores operating at potentially different frequencies. Nevertheless, post-fabrication testing is a big challenge to bring asynchronous NoCs to the market due to a lack of testing methodologies and support for them. In particular, the unpredictable delay variability introduced by the manufacturing process may differentiate the delay of nominally-balanced I/O timing paths, thus making the order of the input patterns unpredictable and precluding the correct behaviour of signature-based test compactors. This paper tackles this challenge by proposing a testing framework for asynchronous NoCs which works effectively despite delay variations in and across timing paths of the NoC under test. Moreover, in order to mitigate the growing test application costs in modern ICs, we come up with a built-in self-testing infrastructure which automatically controls and delivers the outcome of the testing process without the intervention of an external automatic test equipment (ATE).","A Built-In Self-Testing Framework for Asynchronous Bundled-Data NoC Switches Resilient to Delay Variations Gabriele Miorandi, Alberto Celin, Michele Favalli and Davide Bertozzi University of Ferrara, Italy email: {gabriele.miorandi, michele.favalli, davide.bertozzi}@unife.it, alberto.celin@student.unife.it Abstract—Most multi- and many-core integrated systems are currently designed by following a globally asynchronous locally synchronous paradigm. Asynchronous interconnection networks are promising candidates to interconnect IP cores operating at potentially different frequencies. Nevertheless, post-fabrication testing is a big challenge to bring asynchronous NoCs to the market due to a lack of testing methodologies and support for them. In particular, the unpredictable delay variability introduced by the manufacturing process may differentiate the delay of nominally-balanced I/O timing paths, thus making the order of the input patterns unpredictable and precluding the correct behaviour of signature-based test compactors. This paper tackles this challenge by proposing a testing framework for asynchronous NoCs which works effectively despite delay variations in and across timing paths of the NoC under test. Moreover, in order to mitigate the growing test application costs in modern ICs, we come up with a built-in self-testing infrastructure which automatically controls and delivers the outcome of the testing process without the intervention of an external automatic test equipment (ATE). I . IN TRODUC T ION In many modern post-silicon systems testability requirements growed well beyond defect detection to allow the exploration of operating conditions and system reconﬁgurations. A typical example is provided by the Networks-on-Chip (NoC) domain, where a faulty circuit may be ﬁxed if the post-silicon testing framework is able to successfully locate the defect within the interconnection. Once a fault has been located, the designer can either use spares [3] or selectively exclude the use of the faulty region by adopting a proper routing policy [7]. To match the different frequency requirements of applications and the difﬁculties in distributing the clock signal across wide areas, the Globally Asynchronous Locally Synchronous (GALS) design paradigm [2] offers a promising solution to efﬁciently link the processing elements of a multi-core system using an asynchronous interconnection. Switching fabrics relying on clockless handshaking have recently become more appealing as 2-phase bundled-data asynchronous NoCs [13], [6] are proving capable of relieving the area and power overhead of traditional Quasi-Delay Insensitive (QDI) implementations. Therefore, it becomes necessary to develop a viable testing framework for bundled-data interconnection networks in manycore systems. Unfortunately, from a testing point of view, bundled-data designs are harder to test than delay insensitive circuits, since there are many kinds of faulty behaviors to be taken into account [11]. GALS designs are likely to require an independent testing support for the NOC, which works under a different paradigm than the cores. From this point of view, Built-In Self-Test (BIST) of NOC nodes and links is an attractive approach in terms of concurrency (i.e., test time) and fault location capability. A BIST detection method acting at the granularity of the NoC switch should wrap the switch I/O ports in order to inject 978-1-4673-9030-9/16/$31.00 c(cid:13)2016 IEEE. test patterns to any input port and evaluate the effect produced at the output interfaces. A low-cost test pattern generation circuit is usually provided by Linear Feedback Shift Registers (LFSR). Conversely, at output ports Multiple-Input Signature Registers (MISR) collect and elaborate all the response data produced by the circuit under test. In synchronous and multisynchronous Networks-on-Chip, signature-based analysis have been widely covered by literature [1], under the implicit assumption that the fault-free outputs of the system under test are deterministic, thus making the MISR evolution fully predictable. This is not obvious at all under the assumption of fully asynchronous operation: a deterministic relationship between the input and the output sequence of states is ensured only by very simple circuits such as linear pipelines, but this is not the case for NOC switches, which exhibit more complex circuit structures with diverging and/or reconverging paths. An asynchronous switch, in fact, ensures trafﬁc consistency, while the speciﬁc sequence of its output states depends on its internal timing (e.g., arrival order of requests at arbitration points), thus preventing the MISR to provide a deterministic signature under the effect of various kinds of delay variations. Any attempt to enforce a deterministic behavior is likely to result in restrictions on the input sequence, which may prevent the full veriﬁcation of parts of the component. To the best of our knowledge, this particular behaviour has been overlooked so far in literature. Instead, this work proposes an in-depth analysis of the aforementioned issue while targeting the following detailed goals: Our testing framework does not misunderstand circuit delay variability as an effect of a fault in the switch caused by manufacturing process variations. In fact, in asynchronous systems, delay variation around the nominal condition can be tolerated and cause nondangerous performance variations. Rather, our method preserves high levels of almost-constant fault coverage within realistic variation bounds. For this purpose, the main building blocks for a variability-resilient Design For Testability (DFT) infrastructure have been designed. • We propose a minimally-intrusive Built-In SelfTesting framework which does not break switch internal paths but injects test patterns and analyses test responses directly from the switch boundary. Nonetheless, both faults in the data path and in the control path are targeted. • We tackle the challenge of sustaining fault coverage under these constrained observability and controllability conditions. Indeed, clockless handshaking delivers an observability opportunity that was not there in synchronous design, where the acknowledge signal is replaced by the clock edge. In contrast, the BIST infrastructure for synchronous NoCs has been historically more intrusive into the switch architecture in order to get the coverage results achieved by this paper. • I I . R ELAT ED WORK S The application of standard synchronous test methodologies to asynchronous circuits is not easy from both the test generation and application point of views. The different asynchronous design styles often require ”ad-hoc” dedicated signal representation and dedicated automatic test generation tools [4], [11]. In several cases, asynchronous circuits also present a variety of isolated latch elements which can hardly be connected in scan-chains. In case an asynchronous circuit is embedded in a more complex system, directly controlling its inputs and observing its outputs is not trivial and design for testability approaches should be used. For instance, in [10] latches and C-Muller elements have been modiﬁed to allow the insertion of tests and the extraction of responses via an asynchronous scan chain. Asynchronous architectures based on micropipelines mitigate this problem because their regular structure helps the use of systematic test approaches. As far as bundled-data designs are concerned, a test generation technique for high-speed asynchronous pipelines is proposed in [5] that provides a detailed testability analysis of stuck-at and timing faults, and a test generation method. The authors of [12] address also transistor-level faults in a CMOS Mousetrap pipeline. All these techniques suppose that the pipeline I/O ports can be accessed by an Automatic Test Equipment ()ATE) and that the pipeline structure is linear or that its testing can be reconducted to the linear case via DFT insertion at joins [5]. This latter technique, however, in the case of NOC switches would result in a relevant overhead, because of multiple forks and joins at switch ports. Recently, [14] and [9] have proposed new methodologies to check the micropipeline’s protocols and timing violations online. These techniques, however, do not consider functional faults, and require latch modiﬁcations that may lead to relevant overheads in latch-dominated designs. The key contribution of this work over state of the art consists of the development of a testing framework resilient to circuit delay variability. This is a key enabler to apply robust testing methods not only to linear (or almost linear) pipelines, but also to more complex architectures relying on arbitrated resources, such as NoC switches. I I I . DFT ARCH I T EC TUR E In this work we propose to extend the 2-phase bundleddata switch design presented in [6] with Built-In Self Testing (BIST) logic. This switch is a fast responding solution based on a 2-phase MOUSETRAP pipeline. For our experiments, we implemented a 5-ported switch with an output buffering capacity of 6 slots and 32-bit ﬂit-width. In normal operation mode, it implements the XY-routing algorithm and wormhole switching. In this section, we brieﬂy outline the architectural requirements of BIST for network switches and we analyze the main differences between the synchronous and the asynchronous cases, thus motivating the architecture of the proposed testing approach. A standard BIST detection method with switch-level granularity is supposed to wrap the switch I/O ports in order to inject random trafﬁc patterns to any input port and evaluate the evolution at the output interfaces. A low-cost trafﬁc generation system is usually provided by the use of Linear Feedback Shift Registers (LFSRs), that generate pseudo-random trafﬁc with a minimum area cost. At the output ports, Multiple-Input Signature Registers (MISRs) collect and elaborate all the data produced by the circuit under test. Area and memory overhead is minimized by the MISR, that does not check the circuit correctness event by event, but gives a response just at the end of the testing process. In practice, the MISR collects all the output data and generates a ﬁnal signature summarizing the evolution of that speciﬁc output in a compressed form. The obtained signature is then compared with a golden signature generated by an error-free simulation stimulated with the same input pattern: if at the end of the process the signatures match with one another, the circuit under test is serviceable. An evolution of that paradigm is the cooperative BuiltIn Self Testing, which is a promising method to minimize the overhead of DFT logic, and has been successfully implemented in synchronous systems [1]. As depicted in Fig. 1(a), cooperative BIST requires a unique Test Pattern Generator (TPG) and a unique Automatic Test Analyser (ATA) per switch. All the TPGs generate the same trafﬁc patterns to all the directions, then ATAs are able to locate the fault performing a simple comparison between all the received data. Unfortunately, this method is not applicable to asynchronous switches because it is not possible to share a unique TPG. In practice, an asynchronous TPG should wait for all the acknowledgement signals to be received before being able to produce a new test pattern. Above all, in case one of the neighbour switches is unable to acknowledge the requested communication (e.g., switch on the right in Fig. 1(b)), the TPG would deadlock, thus compromising the testability of the other neighbour switches. Note that this is not an issue in synchronous systems, since there is no acknowledgement. For the aforementioned reasons, asynchronous systems must provide LFSR and MISR to the switch port level, as shown in Fig 2. In particular, in the proposed architecture both MISR and TPG are wrapped at switch input ports, thus the trafﬁc generated by an LFSR will be checked by a MISR located at the input port of the neighbour switch (see ﬁgure). We intentionally locate the DFT logic at the switch input ports to ensure that the longer cycle-time is located just before the MISR: this fact helps to cause congestion in the upstream switch and to facilitate the generation of overrun faults. Overrun faults are speciﬁc for MOUSETRAP micropipelines [8] and could be caused by a stuck-at-one fault on the latch enable signal. Since the FIFO’s design is a strongly latchbased architecture derived from the MOUSETRAP pipeline, Fig. 1. Cooperative testing framework. Fig. 2. Proposed BIST architecture. Fig. 3. A change in the ﬂit order makes signature analysis unreliable. it is important to check for overruns also within the FIFOs. A simple method proposed in [8] suggests to ﬁll up the pipelines and then to read them out. IV. A SYNCHRONOU S BU I LT- IN S EL F T E S T ING : TH E CHA LL ENGE The main issue with asynchronous circuits is that the evolution of arbitrated resources is not deterministic, since it depends on the arrival time by which the clients are requesting the arbiter. In Fig. 3 two different scenarios are presented as a case study. In both cases, the LFSRs are activated at the same time (this usually happens at system reset), but due to the internal circuit delays (∆) a change in the order of ﬂits arriving at the MISR interface may be experienced. For example, in the left scenario ∆W < ∆S < ∆N , while in right scenario ∆S < ∆W < ∆N . In both cases the switches are fault-free, but the MISRs generate different signatures because the test packets are received in different order. Since the circuit delays ∆X are correlated to unpredictable post-fabrication effects, it is never possible to establish a unique golden signature for an arbitrated asynchronous system. This is not just a matter of internal switch delays, but it also depends on the link delay. For example, considering the switch in Fig. 4, LFSRNorth generates packets A and B with destinations East and West respectively, LFSRSouth generates packet C with destination West. In scenario 1 MISRWest receives packet B before packet C: this latter is delayed since the output port West is already allocated by packet B. In scenario 2 MISRWest receives packet C before packet B, not because of the typical delays of packets B and C, but because a performance degradation is experienced by packet A when traversing the East link. Overall, an unexpected post-silicon increase of ∆linkE has postponed the generation of packet B, which loses the arbitration with packet C. Also in this case, Fig. 5. Pass/fail test of a fault-free asynchronous switch for different average link delays ± 50ps. the macroscopic effect is a change in the order of the packets arriving at the MISR’s interface. Fig. 5 shows how these effects may compromise circuit testability. We ﬁrst performed a “golden simulation” on a fault-free switch with no output environment delays, injecting random continuous trafﬁc. After that, we varied the average output environment’s delays from 50ps to 500ps (this actually happens in asynchronous designs, where each link has its own differentiated local delay, different from any other link delay). For each average value, we performed 100 different simulations considering a variability of ±50ps around the average value. Then, we veriﬁed if the data at the output interfaces match the exact data order obtained in the golden simulation. The result is dramatic: for small average delays, 18% of the simulations fail, at 250ps±50ps, 50% of the simulations fail, and at 350ps±50ps all the simulations fail, meaning that none of the switches can be tested with a standard “data order sensitive” Built-In Self-Testing framework. The break-even point between pass or fail tests in Fig. 5 is due to the fact that at 250ps the critical path moves from the crossbar to the link. V. PRO PO SED ID EA The key take-away so far is that a Built-In Self-Testing framework for asynchronous designs can not be “data order sensitive”. As shown in Fig. 4 the order of the packets may change quite severely depending on the real circuit delays. However, as it is clear from the same Fig. 4, ﬂits of the same packet never change their order, thus the proposed idea consists of developing a testing framework able to detect an error on a per-packet basis. This means that at the end of each packet the MISR is instructed to evaluate the data correctness of the ﬂits of that packet, then it proceeds to compute the next packet. As represented in Fig. 6, this is possible only if the MISR signature returns to the initial value (IN I T ) whenever a tail ﬂit is received, and then restarts the analysis when the next header ﬂit is received. Practically, data correctness is evaluated checking the condition M I SRS ignature = IN I T after each tail ﬂit. The real issue is to generate a pseudo-random Trafﬁc Pattern Generator (TPG) capable of bringing the MISR back to the initial state on a per-packet basis. Note the difference between an LFSR that generates simple random trafﬁc and Fig. 4. A change in the ﬂit order due to the post-silicon link delay variation. Fig. 6. OI-MISR responsiveness. Fig. 7. Proposed wrapper architecture including OI-TPG and OI-MISR. Fig. 8. Detailed Linear Feedback Shift Register (LFSR) architecture. the proposed TPG, that enables the development of an “orderinsensitive” Built-In Self Testing (OI-BIST). From now on, we refer to OI-MISR and OI-TPG to differentiate order-insensitive solutions from the standard order-sensitive ones. To exploit this capability, each OI-TPG must be provided with additional logic that enables the generation of a proper tail ﬂit. The target of our testing infrastructure consists of the detection of stuck-at faults. Therefore, when we consider delay variations and the resilience of the testing framework to them, we are indirectly assuming that the delay variations do not violate the bundling constraint (i.e., the request signal arrives once the data is stable). In case design instances violating this constraint were generated, our testing method would be able to detect some of these violations (e.g., when the request is so fast that the previous data is sampled twice), although not all of them (e.g., when setup and hold times of the latch are violated). However, such constraints are not the primary target of our infrastructure, but nothing prevents from extending it in future work or from complementing it with a testing framework targeting delay faults. V I . BU I LT- IN S EL F T E ST ING : IM P LEM EN TAT ION In the following subsections all the developed BIST building blocks are presented. The wrapper, to be placed at the switch input ports, is shown in Fig. 7 and interlaces the link with the switch input interface. The wrapper contains both the proposed orderinsensitive building blocks: the OI-TGP and the OI-MISR. A barrier of latches isolates the test pattern generator and the communication channel in case the test mode is OFF. A pair of XOR gates acts as 2-phase signal mergers to join the Req/Ack signals of the OI-BIST logic and the Req/Ack signals of the communication control channel (by default this channel is used when the test mode is OFF). There is an additional MISR associated with the OI-TPG: the “shadow MISR” that keeps trace of the TPG activity and enables the generation of the tail ﬂits on a per-packet basis as described in Sec. V. Additional circuitry (random shifters) has been included deep within the routing logic to improve the coverage for speciﬁc types of errors, it is going to be presented at the Fig. 9. Testing a 32-bit switch using 10-bit LFSR and MISR. end of this section. Circuits allowing the controllability from an external controller have been inserted too: the system can be safely switched from TestMode to NoTestMode, but these details are not covered in this paper for lack of space. The need for duplicating the MISR also within the OI-TPG may cause a severe area overhead. Therefore, to limit the cost of the DFT logic, we reduced the dimension of MISRs and TPGs from 32 bits (corresponding to the switch port width) down to 10 bits. In our case, a 10-bit random pattern generator is sufﬁcient to properly test the routing logic and therefore all the circuits on the control path (i.e., the arbiters). Routing logic is entirely coverable because it has 10 input bits, thus all the addresses will be produced. We have veriﬁed that distributing these 10 bits to the whole ﬁeld of data path bits is enough to obtain a satisfactory fault coverage at the switch level. This approach is enabled by the logic structure of the switch, that does not correlate the bits in the datapath: for example, bit 22 and bit 9 will cross the switch without being logically combined, therefore it is not necessary to create all the possible input combinations for these two bits. We distributed the 10 bits to a 30-bit ﬁeld in such a way to minimize the capacitive coupling effects on the link. The aforementioned mechanism is brieﬂy shown in Fig. 9. A 10-bit LFSR generates the random pattern, these bits are spread over a 30-bit data path which crosses the switch together with the 2-phase request and the F litT ype bits (header and tail), which are generated separately1 . At the switch output port interface the received data is precomputed by a XOR-based Bit-Triple Comparator, that converts the 30bit output data pattern into a 10-bit one by combining the bits three by three. Note that in a fault-free scenario the output of the Bit-Triple Comparator precisely matches the pattern generated by the input LFSR, thus it is ready to be analysed with the 10-bit MISR. The MISR computes the new ﬂit whenever a Req ﬂips its phase, meaning that the received data pattern is valid. F litT ypes bits are also included in the MISR analysis. A. Linear Feedback Shift Register (LFSR) A Linear Feedback Shift Register (LFSR) is used as source of 10-bit pseudo-random vectors within the OI-TPG. The LFSR micro-architecture is presented in Fig. 8. It is a FlipFlop based LFSR with characteristic polynomial (10,9,7,3,0). A new value is generated by triggering the Flip-Flops with a positive edge on the U pdateLF SR signal. B. MISR Core The MISR core is used as building block for both the OI-MISR and the “shadow MISR” in the OI-TPG. Its microarchitecture is represented in the top sub-block of Fig. 10(a). 1Our ﬂow control provides that the ﬂit type is encoded in the two least signiﬁcant bits of the ﬂit, based on the following rule: the 0th bit position indicates that this is an header ﬂit by setting H eaderB it = 1, while the 1st bit position indicates a tail bit by setting T ailB it = 1; ﬁnally, both the ﬂit type bits are set to low to transmit a payload. Fig. 10. Detailed implementation of the proposed BIST components: the shadow MISR (a), the order-independent MISR (b) and the order-independent TPG (c). D. Order-Independent TPG The OI-TPG micro-architecture is shown in Fig. 10(c). The main building blocks of the OI-TPG are: (i) the handshake controller to manage the Req/Ack communication channel interface, (ii) the shadow MISR that (as previously mentioned) is the essential component to enable an order-independent packet generation, (iii) other control circuits to manage the global OI-TPG operation (it includes a counter to synchronize the TPG operation and an LFSR to generate random test patterns). THE HAND SHAKE CONTROLLER generates new requests and manages the handshake with the switch input interface. The FlipFlop F F Gen generates the ﬁrst request as soon as ST ART signal is asserted high from the external controller. The generated request is propagated to the switch interface. Since DataOut must be stable until an Ack is received, the bottom OI-TPG circuitry is always triggered by a positive edge on AckReceived which happens whenever an Ack from the switch interface is received. Starting from the AckReceived positive edge, two concurrent events happen: (i) a new request is generated and (ii) the bottom circuitry updates DataOut to the next value. This generates a clear one-side timing constraint (Delay(i) > Delay(ii) ) starting from AckReceived node and ending at the output OI-MISR interface (input switch interface). For this reason the output Req is appropriately delayed in order to match the delay of the data path. LH old can further delay the request generation if disabled: this happens whenever a new header request is rapidly generated after a tail ﬂit, in this case LH old is still closed and the request propagation is prevented until the previous packet transmission is safely terminated and the bottom OI-TPG circuitry is reinitialized. Latch LH old can be also used to permanently stop the TPG operation, deasserting the GO signal low from an external controller. LFSR AND COUNTER generate random patterns and give a synchronization to the rest of the TPG. The LFSR is in charge to generate 10 random bits to be used for header and payload ﬂits (while special computation for the tail ﬂit is performed by the shadow MISR) and its micro-architecture has been presented in Sec. VI-A. The counter determines the length of the packet and the current ﬂit number. At each header ﬂit the counter initializes to a random value between 4 and 7 (for this purpose two of the generated random bits are fed from the LFSR to the counter) and then decrements at each positive edge of AckReceived. When zero, the T ailB it signal is asserted high. T ailB it is used to control the output multiplexer and univocally select the generated RandomP attern (during header and payload) or the C alculatedT ail. After the multiplexing stage the selected 10 bits are split to 30 bits and completed with the F litT ype bits in order to create the data pattern to be sent through the switch interface. The counter uses Gray encoding in order to prevent glitches on the generated output signals. SHADOW M ISR includes a MISR core and the Tail Logic. During header and payload ﬂit the MISR core receives all the random patterns generated by the LFSR plus the F litT ype bits and updates at each positive edge on AckReceived. Up to the second to last ﬂit, the MISR core operation is exactly the same of a standard MISR, same of what has been presented in the previous Sec. VI-C. Then the main issue concerns the generation of the TailFlit. The key operations occur within the Tail Logic. As depicted in Fig. 10(a), Tail Logic determines the C alculatedT ail by logically combining the current S ignature produced by the MISR core with the expected IN I T state value. To ensure that the xth bit of the ﬁnal signature will match the expected IN I T [x], it is necessary to act on the xth bit of DataI n. It also depends on the (x + 1)th signature value of the penultimate ﬂit. Therefore Fig. 11. OI-MISR timing diagram. It is composed by a 10-Flip-Flop LFSR with characteristic polynomial (10,7,0); the polynomial differs form the TPG’s one in order to minimize aliasing and correlation effects. The OI-MISR core computes the 10-bit DataIN bits plus the F litT ype bits. For the latter two bits there is no state holding logic because it is not possible to return to the initial MISR state on a per packet basis while honoring the ﬂit type protocol. However, stuck-at fault errors on the F litT ype bits can be observed by using three-input XOR gates to generate the LFSR feedback signal: this way an error on this ﬁeld affects the next signature and becomes visible. The additional logic shown at the bottom of Fig. 10(a) is used to implement the “shadow MISR” in the OI-TPG, and it will be further investigated in Sec. VI-D. C. Order-Independent MISR The OI-MISR micro-architecture is shown in Fig. 10(b). An handshake controller handles the Req/Ack interface with the link. The MISR operation begins with the MISR core in the initial state, this means that the produced S ignature matches the I nitialS tate(IN I T ). In case the S ignature does not match with IN I T , a 10-bit comparator is in charge to set the N otI nitialS tate high. The global OI-MISR operation is represented by the timing diagram in Fig. 11. First a new DataI n (header) arrives at the input interface, crosses the BitTriple Comparator and stabilises at the MISR core input interface. When a Req signal is received, it passes the XOR gate within the handshake controller, and sets the U pdateM I SR high: this causes the update of the MISR core2 . Concurrently the incoming Req is latched into the F F Req . Once Ack is generated U pdateM I SR is deasserted low, thus a new ﬂit can be received. In a fault free scenario, during header and payload ﬂits transmissions the MISR state evolves normally, the N otI nitialS tate is likely high during this period and the MullerC element within the Error Manager is reset thus maintaining E valuate high. When the tail is received, it is also computed after the U pdateM I SR positive edge, but since T ailB it is 1 the E valuate signal is deasserted low. To ensure a correct operation, no Ack is generated until a possible update of the N otI nitialS tate signal is experienced. For this purpose, an extra delay has been purposely inserted after F F Req . After the Ack generation, U pdateM I SR returns to 0 and E valuate is re-asserted high. In practice, a 1-0-1 pulse on the E valuate signal is experienced during the tail bit transmission, and if the inserted delay properly matches the time required to compute N otI nitialS tate, an E rror signal is asserted high after a positive edge on the E valuate signal (case N otI nitialS tate did not return to 0). Once an E rror is detected a feedback loop in the Error Manager ensures that it is maintained high. 2 It is being assumed that DataIN arrives earlier than the request at the input interface order to propagate properly up to the MISR core. This timing constraint can be fulﬁlled simply applying an additional delay on ReqIN in general: T ail[x] = IN I T [x] ⊕ S ignature[x + 1]. In case depends on S ig [0]: T ail[x] = IN I T [x] ⊕ S ignature[x + 1] ⊕ the MISR core has some TAPs, the next S ignature value also S ignature[0]. Note that in this case the produced function is inverted to take into account that T ailB it is 1 when the tail is received, which inverts the value of S ig [0]. Thanks to this function, the Tail Logic generates the input value that returns the OI-MISR to the INIT state after the Tail ﬂit. Fig. 12 represents the OI-TPG operation in proximity of the Tail ﬂit transmission. When the penultimate Ack is received, the payload is computed by the MISR core and S ignature = s5. Then the Tail Logic computes the C alculatedT ail value. Concurrently, the counter sets the T ailB it to high thus selecting the multiplexer for C alculatedT ail. Since the MISR core is not in the initial state, EnalbleT ransmission is deasserted low and latch LH old becomes opaque3 . When the Tail’s Ack is received, a pair of MullerC elements resets the MISR core (note that LH old is still opaque during this operation). When the MISR core reaches the IN I T state EnalbleT ransmission is set high, LH old is enabled and the Header’s request is propagated. The above use of LH old has been engineered to prevent extremely large values of the matched delay. E. Optimizing the fault coverage Despite the order-independent capabilities delivered by the proposed method, there are still two main open issues: 1) Difﬁculty in uniformly spanning all the possible addresses. This fact is mainly related with the routing logic function. In practice, it is not possible to uniformly distribute the trafﬁc among all the possible switch ports with a purely random trafﬁc injection. For example there is a single address value which causes the packet to be driven to the local port, and by injecting random trafﬁc it ends up being stimulated just in very few cases. This issue can impact dramatically on the coverage of the output local port logic. No coverage of misrouting errors. This is a side 2) effect of the order-independent mechanism. Since the OI-MISRs are returning to the initial value after each packet, they are not able to determine if that packet should be routed to another destination: this fault is detectable only with a standard BIST method which analyses several packets and reports the signature at the end of the simulation. This issue can compromise the coverage of the control path. We resolved both these problems with two circuits solutions. For the ﬁrst problem we provide the routing logic with a 3 The Tail request has already passed LH old latch, then it is delayed to match the Generation of DataOut. Fig. 13. Random routing logic shifter placed within the switch input port module. random shifter. As depicted in Fig. 13 the shifter takes the 1-hot vector produced by the routing logic and rotates it. The amount of the randomic rotation is performed based on the value of some S hif tB its extracted from the Header ﬂit. Since the content of the header ﬂit is random the set of generated output addresses is much better distributed among all the possible addressable output ports. In normal operation (no testing), the header ﬂit must have all the S hif tB its set to 0, this way the result of the routing logic is copied to the output with no shifting. As far as the routing issue is concerned, our OI-TPG has been provided with a packet counter. This counter is initialized to the number of packets that the TPG will generate, and it is decremented at each packet. When zero, it forces the OI-TPG Go signal to 0, which stops the generation of new packets. In a friendly scenario, after a time-out expires, the value of all the counters should be 0. If it does not happen, it is highly likely that an error affects the switch control path (routing logic or arbiter). We experienced that this method detects a very large number of errors. V I I . EX PER IM EN TA L R E SU LT S The proposed BIST solution has been synthesized using Synopsys Design Compiler and targeting a Low-Power 40nm industrial technology library. The design ﬂow is pretty much similar to the one proposed in [6]. Also, timing constraints are ﬁxed automatically by checking the real delays from the synthesized netlist, then matched delays are inserted where required by using set min delay constraints. Fig 14 shows the area impact of the proposed DFT logic. The baseline switch area cost represents 71.8% of the whole test-capable switch, which means that the area overhead due to the DFT logic is 39,2%. The OI-TPG occupies 12.8% of the total area, because of the high complexity of its building blocks: considering only the shadow MISR and the INIT state comparator, they consume two times more area than the LFSR. Despite the signiﬁcant overhead in terms of area, it must be highlighted that this is not a big deal in asynchronous systems. In fact, area overhead is not a synonym of power overhead in normal operation, because being the circuit completely asynchronous, the DFT logic will consume almost no power when inactive. Fig. 12. Representative timing diagram of the OI-TPG operation. Fig. 14. Proposed DFT switch area breakdown. total injected faults Detected faults Coverage (%) switch only 7346 7175 97.7 whole DFT switch 9356 8734 93.4 testable DFT switch 9144* 8734 95.5 TABLE I. FAULT COV ERAG E FOR THE PRO PO S ED SO LU T ION *212 injected faults are not considered in the testable solution because they are located on the wrapper multiplexing logic between the link and the switch that our solution is unable to cover, however they may easily become visible by adding simple network-level tests. As far as the coverage is concerned, table I shows the detection capabilities of the proposed system. With 7346 stuckat faults exhaustively injected in the switch, 7175 are successfully detected. Thus the stuck-at fault coverage considering the switch only is of 97.7%. This value was obtained by setting all link delays to the nominal value of 200 ps, which roughly corresponds to a link length of 1.5mm in our target technology. When the DFT logic is taken into account, the total number of gates increases up to 9356. In this latter set of additional gates, part of the wrapper logic is not reachable from our DFT framework, such as the multiplexing logic between the link and the switch input interface (this includes about 424 gates). For this reason, it is reasonable to consider only half of these gates in our total count (those that can be in principle detected), thus we corrected the total number of “testable” gates from 9356 to 9144. Under this assumption, the achievable coverage of the whole DFT switch (including IO-TPG and IO-MISR) is 95.5% Then, we considered 10 instances of the switch with link delay variability applied. Nominal link delays were modiﬁed by randomly selecting a delay in the range [150ps-250ps]. For each switch instance affected by delay variations, stuck-at faults were exhaustively injected, and the testing procedure was executed. We were therefore able to measure the percentage relative uncertainty of the coverage values reported in table I to be 0.0212%. This means that our testing method provides a coverage which is highly insensitive to delay variations. Finally, we performed the same fault-free simulations in Fig.5, and found out that the switch augmented with the proposed testing framework delivered a ”pass” test response in 100% of the cases, regardless the average delay. Interesting considerations concern the responsiveness of the proposed testing framework. Being the signature computed on a per packet basis, it is typically not necessary to wait till the end of the testing process to get an error indication from MISRs. Fig. 15 shows the time and the number of ﬂits required to cause an error in at least one of the OI-MISR. Interestingly about 60% of the injected stuck-at faults are detected in less than 30ns (equivalent to 30 clock cycles in a synchronous counterpart clocked at 1 GHz), that in other words is the time required to inject and evaluate approximately a hundred of ﬂits. This result is quite promising to bound the average testing time. From Fig. 15 it is also clear that it is not possible to reach the maximum coverage by using MISRs only (in this case the maximum coverage would peak at around 71%). To bridge the gap, misrouting counters must be read after a timeout: the counters are able to detect 32.3% of the errors and cooperate to achieve the optimal coverage. 8% of the faults are detected by both counter and OI-MISR. The ultimate consideration concerns the latency overhead of the DFT logic in normal operation (no testing mode). In the worst case, during the header ﬂit transmission both the wrapper and the routing logic shifter must be traversed; this causes a head latency degradation of +240ps on average (+16% with respect to the baseline switch header latency). For payload and tail transmissions the shifter is not traversed, therefore in this case the latency degrades on average of +150ps (+13% with respect to the baseline switch payload latency). V I I I . CONC LU S ION S In this work we performed an in depth analysis of the delay effects in the context a Built-In Self-Testing framework Fig. 15. OI-MISR responsiveness. for 2-phase bundled-data asynchronous NoC switches. It has been demonstrated how unpredictable delay variations (namely post-silicon ones) may cause signature-based analysis to fail. In this paper we solved this problem by proposing two novel DFT building blocks, the OI-TPG and the OI-MISR, which deliver a delay-independent BIST operation where analysis is performed on a per packet basis. The obtained coverage of stuck-at faults is about 95.5%, including coverage of the DFT logic, while in terms of test application time the results report ultra-high responsiveness of a few tens of equivalent synchronous cycles in most cases. In future work we will integrate the proposed BIST framework at the network level, by complementing it with a reconﬁguration method of the network routing function. "
Constructing large-scale low-latency network from small optimal networks.,"The construction of large-scale, low-latency networks becomes difficult as the number of nodes increases. In general, the way to construct a theoretically optimal solution is unknown. However, it is known that some methods can construct suboptimal networks with low-latency. One such method is to construct large-scale networks from optimal or suboptimal small networks, using the product of graphs. There are two major advantages to this method. One is that we can reuse small, already known networks to construct large-scale networks. The other is that the networks obtained by this method have graph theoretical symmetry, which reduces the overhead of communication between nodes. A network can be viewed as a ""graph"", which is a mathematical term from combinatorics. The design of low-latency networks can be treated as a mathematical problem of finding small diameter graphs with a given number of nodes (called ""order"") and a given number of connections between each node (called ""degree""). In this paper, we overview how to construct large graphs from optimal or suboptimal small graphs by using graph-theoretical products. We focus on the case of diameter 2 in particular. As an example, we introduce a graph of order 256, degree 22 and diameter 2, which granted us the Deepest Improvement Award at the ""Graph Golf"" competition. Moreover, the average shortest path length of the graph is the smallest in graphs of order 256 and degree 22.","Constructing Large-scale Low-latency Network from Small Optimal Networks (Invited Paper) Ryosuke Mizuno Department of Physics Kyoto University, Kyoto, Japan Email: mizuno@tap.scphys.kyoto-u.ac.jp Yawara Ishida Research Institute for Mathematical Science Kyoto University, Kyoto, Japan Email: yawara@kurims.kyoto-u.ac.jp Abstract—The construction of large-scale, low-latency networks becomes difﬁcult as the number of nodes increases. In general, the way to construct a theoretically optimal solution is unknown. However, it is known that some methods can construct suboptimal networks with low-latency. One such method is to construct large-scale networks from optimal or suboptimal small networks, using the product of graphs. There are two major advantages to this method. One is that we can reuse small, already known networks to construct large-scale networks. The other is that the networks obtained by this method have graphtheoretical symmetry, which reduces the overhead of communication between nodes. A network can be viewed as a “graph”, which is a mathematical term from combinatorics. The design of low-latency networks can be treated as a mathematical problem of ﬁnding small diameter graphs with a given number of nodes ( called “order” ) and a given number of connections between each node ( called “degree” ). In this paper, we overview how to construct large graphs from optimal or suboptimal small graphs by using graph-theoretical products. We focus on the case of diameter 2 in particular. As an example, we introduce a graph of order 256, degree 22 and diameter 2, which granted us the Deepest Improvement Award at the “Graph Golf ” competition. Moreover, the average shortest path length of the graph is the smallest in graphs of order 256 and degree 22. Index Terms—low-latency network, graph theory, the degreediameter problem, the order-degree problem, Brown’s construction, star product. I . IN TRODUC T ION In mathematics, graph theory is the study of graphs, which are mathematical objects obtained by abstraction of networks. A graph G is a network which consists of a set of vertices V and of edges E . Nodes and connections between nodes are translated to vertices and edges respectively. Two distinct vertices u and v of G are connected to each other if and only if (u, v) ∈ E . In this case, we say that u and v are adjacent and use the notation u ∼ v . For example, let V be a set of (i, j ) such that i = 0, 1 and j = 0, 1, . . . , 4, and let E be a set if i (cid:54)= k then j = 2l mod 5. The graph of the given V and E is called Petersen graph as shown in Figure 1. We can study networks by modeling them as graphs and formulate various problems in terms of graph theory. For example, the traveling salesman problem, the four color theorem, the max-ﬂow mincut theorem and the shortest path problem are well known of ((i, j ), (k , l)) such that if i = k then j = l ± 1 mod 5 or 978-1-4673-9030-9/16/$31.00 c(cid:13)2016 IEEE Fig. 1: Petersen graph. formulations of problems of networks. Knowledge gained by the formulation of graph theory is expected to be useful for the study of networks. The order N = |G| = |V | and the maximum degree of vertices ∆ are basic feature values of graphs, where the degree of a vertex is the number of adjacent vertices to it. The distance d(u, v) between u and v in V is the length of the shortest path between them. The diameter D of the graph G is the maximum distance of all pairs of vertices; D = max u,v∈V d(u, v). The diameter of Petersen graph is 2. Let us turn to some other famous examples of graphs. Let V be vertices of the ndimensional hypercube over a ﬁeld such that each component is 1 or 0, and the vertices x and y in V are adjacent if and only if 1 or −1 appears just once in the components of x − y , and other components of it are 0. This graph is called an n-dimensional hypercube graph, which is of N = 2n , ∆ = n, D = n. Let V be (Z/mZ)n such that m > 2, and two vertices x and y in V are adjacent if and only if 1 or m − 1 appears just once in the components of x − y , and other components of it are 0. This graph is called an n-dimensional torus grid graph, which is of is gauss’s symbol. These two examples are graphs obtained by abstracting known topological structures. On the other hand, we can also construct graphs directly by using algebraic methods. Let L be a set of distinct t labels and let the vertices N = mn , ∆ = 2n, D = n[m/2] where this bracket V be Ln . The vertices (a1 , a2 , . . . , an ) and (b1 , b2 , . . . , bn ) in V are adjacent if and only if ai = bi+1 or ai+1 = bi for i = 1, 2, . . . k − 1. This graph is known as undirected de Brujin graph of type (t, n) of N = tn , ∆ = 2t, D = n [1], [2]. The order, the maximum degree, and the diameter are important indices for measuring the efﬁciency of connections between vertices of a graph. If a graph of given diameter and maximum degree has more vertices, we consider it as a more efﬁcient network. This claim can be rephrased into two other forms. One is that a graph of given order and maximum degree is more dense if its diameter is smaller. The other is that if the maximum degree of a graph of given order and diameter is smaller, the distribution of the edges is more efﬁcient. These claims correspond to three optimization problems; the degreediameter problem, the order-degree problem and the orderdiameter problem. The degree-diameter problem is to ﬁnd the largest possible number of vertices in a graph of given degree and diameter [3]. Of the above three, this is the most attractive to mathematicians. A table of a lower bound of the largest possible number of vertices with given degree and diameter is available online at the Combinatorics Wiki website [4]. The order-degree problem is to ﬁnd graphs with the smallest diameter in graphs of given order and degree. This problem can be applied for designing of law-latency networks, but a table similar to the degree-diameter problem has not been constructed as of yet. Only a very small table is available online at the Graph Golf website [5]. The order-degree problem is to ﬁnd graphs with the smallest maximum degree in a graph of given order and diameter. Unfortunately, this problem has been somewhat overlooked compared to the others. These three problems are strongly interrelated. Therefore we can use the best-studied degree-diameter problem in order to study the others. In what follows, we review the degree-diameter problem and two methods for constructing graphs of diameter 2 for the order-degree problem. One is Brown’s construction, which was originally introduced by W.G. Brown [6] and later generalized by the co-author [7] of this paper. The other is graph-theoretical product known as star product [3], [8]. This allows us to introduce the graph of order 256, degree 22, and diameter 2. Moreover, the average shortest path length of the graph is the smallest in graphs of order 256 and degree 22, where the average shortest path length ASP L is deﬁned as follows; (cid:80) ASP L = u,v∈V d(u, v) |V |(|V | − 1) I I . TH E D EGRE E -D IAM ET ER PROB L EM As mentioned above, the degree-diameter problem is to ﬁnd the largest possible number n∆,D of vertices in a graph of given degree ∆ and diameter D [3]. If G is a graph with D−1(cid:88) degree ∆ and diameter D , then we get where |G| is the number of vertices of G. The right hand side of the above inequality is called Moore bound. The graph whose order equals Moore bound is called Moore graph. In the case of D = 2, Moore graph can exists when |G| ≤ n∆,D ≤ 1 + ∆ (∆ − 1)k k=0 ∆ = 2, 3, 7, 57 [9]. In fact, if ∆ = 2, 3, 7, then Moore graphs are a pentagon, Petersen graph, Hoffman-Singleton graph respectively. It is not known whether Moore graph exists if ∆ = 57. In the case of D ≥ 3, Moore graph is a cycle graph of length 2D + 1 [10]. Some general lower bounds of n∆,D are known( see [3] ). For example, if ∆ is an even number, the undirected de Brujin graph gives a lower bound of n∆,D ; (cid:18) ∆ (cid:19)D n∆,D ≥ 2 However, exact lower bounds of n∆,D are only known for small ∆, D . Thus, the order-diameter problem for almost all pairs of ∆, D is unsolved. Let us consider the case of ∆ = 8, D = 8, where Moore bound is 7686401. The 8dimensional hypercube graph, the 4-dimensional torus grid graph over Z/5Z, and the undirect de Brujin graph of type (4, 8) are examples of ∆ = 8, D = 8. The order of the 8dimensional hypercube graph is 28 = 256. The order of the 4-dimensional torus grid graph over Z/5Z is 54 = 625, which is more efﬁcient than 8-dimensional hypercube. The order of undirect de Brujin graph of type (4, 8) is 48 = 65536, which is more efﬁcient than the above two graphs. The order of the already known suboptimal graph of ∆ = 8, D = 8 is 734820 [11]. The graph is discovered by using a computer. However it seems to be too small because the percentage of the Moore bound for ∆ = 8, D = 8 is 9.56%. As another example, let us consider the case of ∆ = 20, D = 2. The order the undirected de Brujin graph of type (10, 2) of ∆ = 20, D = 2 is 102 = 100. The order of the already known suboptimal graph of ∆ = 20, D = 2 is 381, where the percentage of the Moore bound for ∆ = 20, D = 2 is 95%. It is obtained by Brown’s Construction, which we will discuss in more detail later on. As mentioned above, the hypercube graph, the torus grid graph, and the de Brujin graph are very simple and easy to construct, but these graphs are not close to suboptimal. Thus, when we want more dense or more efﬁcient graphs, these simple graphs are not suitable. In order to construct more dense or more efﬁcient graphs, there are roughly two ways; either by a deterministic mathematical construction, or by searching suboptimal graphs using computers. Furthermore, a deterministic mathematical construction can be divided into two ways. One way is to construct graphs with more vertices from scratch. The other is to construct graphs from small, already known suboptimal graphs. A. The case of diameter 2 For the case of diameter 2, Brown’s construction [3], [6] gives suboptimal graphs. This construction gives a graph for each q which is a power of a prime. Let Fq be a ﬁnite ﬁeld. Brown’s construction gives the graph B(Fq ) where the vertices are lines in F 3 q and two lines are adjacent if and only if they are orthogonal. It follows that; |B(Fq )| = q2 + q + 1, ∆(B(Fq )) = q + 1, D(B(Fq )) = 2. The degree of each vertex of B(Fq ) is q + 1 or q . The reason of D(B(Fq )) = 2 is that for all two vectors x and y there by natural numbers, namely V (Kn ) = {0, 1, . . . , n − 1}. Let (cid:126)E be {(k1 , k2 )|k1 < k2 } and φ be deﬁned as follows; (0, 1 − j ) (2, j + 1 mod 3) (1, j − 1 mod 3) φ(k1 , k2 )((i, j )) = (i = 0) (i = 1) (i = 2). Fig. 2: The G8 Construction. exists a vector z orthogonal to x and y even in ﬁnite vector spaces. Therefore, for ∆, if a power of a prime q exists such that ∆ = q + 1, we get n∆,D ≥ ∆2 − ∆ + 1. Among q2 + q + 1 vertices, q + 1 vertices are of degree q and q2 vertices are of degree q + 1. If q is a power of 2, there exists the graph of N = q2 + q + 2, ∆ = q + 1, D = 2 [12]. Therefore, if ∆ is a power of 2, we have n∆,D ≥ ∆2 − ∆ + 2. For the remainder ∆, we get suboptimal graphs by duplicating the vertex of the graphs obtained by Brown’s construction [3]. For any  > 0 there exists a constant c such that, for any ∆ the following holds; n∆,D ≥ ∆2 − c∆19/12+ . For the case of diameter 2 with large maximum degree, the graphs obtained by Brown’s construction are the best suboptimal graphs among already known graphs. Let us turn to another graph-theoretical technique known as star product, which was introduced by Bermond, Delorme and Farhi [3], [8]. Let G1 , G2 be graphs. We ﬁx an arbitrary orientation of all edges of G1 and let (cid:126)E be the corresponding set of the ﬁxed arrows of G1 . For each arrow (u, v) ∈ (cid:126)E , let φ(u, v) be a bijection on the set V (G2 ). The vertex set of the star product G1 ∗φ G2 is thus V (G1 ) × V (G2 ), and the vertex (u, v) is adjacent to (w, x) if and only if either u = w and (v , x) is an edge of G2 , or (u, w) is in (cid:126)E and x = φ(u, w)(v). Using this product, we can construct some efﬁcient graphs. For example, we can construct the graph that gives the exact lower bound of order for ∆ = 6, D = 2. The exact lower bound for ∆ = 6, D = 2 is 32, namely n6,2 = 32. Let the graph G8 of order 8 be as follows; V = {(0, 0), (0, 1), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)} and (i, j ), (k , l) in V be adjacent if and only if (i, k) in {(0, 1), (1, 0), (2, 2)}, or if (i, k) = (1, 2), (2, 1) then j = l. G8 is of order 8, degree 8, and diameter 2 as shown in Figure 2. Let Kn be a n-complete graph, where each vertex is connected to all other vertices. The vertices of Kn is labeled Then we get the star product Kn ∗φ G8 with φ, which is of order 8n, degree n + 2 such that if n ≥ 3, the diameter is 2. When n = 4, the K4 ∗φ G8 is the optimal graph of order 32 for the degree-diameter problem of ∆ = 6, D = 2. I I I . TH E ORD ER -D EGR EE PROB LEM The order-degree problem is to ﬁnd the graph with the smallest diameter in graphs of given order and degree. As mentioned above, the order-degree problem is correlated strongly with the degree-diameter problem. In fact, for given degree ∆ and diameter D , the optimal graph G for the degree-diameter problem is also optimal for the order-degree problem of order |G| and degree ∆. If G is not optimal for the order-degree problem, then there exists the graph G(cid:48) of order |G|, degree ∆ and diameter D (cid:48) < D . Thus, we get G(cid:48)(cid:48) of order |G| + 1, degree ∆ and D (cid:48) + 1 ≤ D by inserting one vertex into an arbitrary edge of G(cid:48) . This contradicts the assumption that G is the largest graph of given degree ∆ and diameter D . Therefore, the already known suboptimal graphs for degree-diameter problem seem to be suboptimal for order-degree problem. However, the order of optimal or suboptimal graphs of the degree-diameter problem does not cover an arbitrary order. For example, the facts of n6,2 = 32 and n7,2 = 50 implies nothing more than that if 33 ≤ |G| ≤ 49 and D = 2, then ∆ ≥ 7, or if 33 ≤ |G| ≤ 49 and ∆ = 6, then D ≥ 3. Namely, the graphs obtained from the degree-diameter problem cover a very limited number of pairs of order and degree for the order-degree problem. Thus, we have to construct graphs for missing pairs of given order and degree. As well as the degree-diameter problem, there are roughly two deterministic ways to construct graphs; to construct graphs from scratch or from already known small graphs. We introduce two ways corresponding to them; generalized Brown’s construction and multiple star product, which are the generalization of Brown’s construction and star product respectively. Using multiple star product, we construct the graph of order 256, degree 22, and diameter 2. A. Generalized Brown’s Construction We give a generalization of Brown’s construction [7] by replacing a ﬁnite ﬁeld Fq with a ﬁnite commutative ring R with unity, in particular Z/nZ. The vertices V (B(R)) is deﬁned as follows; (R3 \ {v |∃r ∈ R, r · v = 0})/ ∼ where v ∼ w if and only if there exists k ∈ R∗ such that k · v = w . The two vertices [v ] and [w] are adjacent if and only if v ·w = 0. It is clear that, if R is ﬁnite, then so is B(R). Moreover if R is a quotient of any Euclidean domain, then the numbers pi and natural numbers ki > 0 such that n = (cid:81) diameter of B(R) is 2. When R = Z/nZ, there exist prime and it follows that; i pi ki , |B(Z/nZ)| = (cid:89) (cid:0)pi ∆(B(Z/nZ)) = i (cid:89) 2ki + pi (cid:0)pi D(B(Z/nZ)) = 2 i ki + pi 2ki−2 (cid:1) 2ki−1 + pi ki−1 (cid:1) . Therefore, for given order N and degree ∆, if there exists a natural number n ≥ 2 and δ ≥ 0 such that N = |B(Z/nZ)|+δ and ∆ ≥ ∆(B(Z/nZ)) + δ , then we have the graph of order N , degree ∆ and diameter 2 by δ times duplicating vertices of B(Z/nZ) and adding some edges. We omit the proof of the above discussion because the details will appear in our next paper [13]. B. Multiple Star Product Let us construct the graph of order 256, degree 22, and diameter 2. It was necessary to develop a novel method for the construction of this graph because it could not be obtained by generalized Brown’s construction nor by the ordinal star product. For example, the order of K32 ∗φ G8 is 256 = 32 × 8 with an arbitrary φ but the degree is 34 = 31+ 3. We introduce the new concept of m-multiple star product. Let G1 , G2 be graphs. We ﬁx an arbitrary orientation of all edges of G1 and let (cid:126)E be the corresponding set of the ﬁxed arrows of G1 . For each arrow (u, v) ∈ (cid:126)E , let ψ(u, v , l) be a bijection on the set V (G2 ) where l = 0, 1, . . . , m − 1. Then the vertex set of the multiple star product G1 ∗ψ G2 with ψ is V (G1 ) × V (G2 ), and the vertex (u, v) is adjacent to (w, x) if and only if either u = w and (v , x) is an edge of G2 , or (u, w) is in (cid:126)E and there exists l such that x = ψ(u, w, l)(v). We take Ka , Kb ∗φ G8 as G1 , G2 respectively, according to the deﬁnition of φ as seen in the previous section. Let (cid:126)E be {(l1 , l2 )|l1 < l2 } and ψ be deﬁned as follows; ψ(l1 , l2 , 0)((k , (i, j ))) = ψ(l1 , l2 , 1)((k , (i, j ))) =   (k , (0, 0)) (k , (0, 1)) (k , (1, 0)) (k , (2, 1)) (k , (1, 1)) (k , (2, 2)) (k , (1, 2)) (k , (2, 0)) (k , (0, 1)) (k , (1, 0)) (k , (2, 1)) (k , (0, 0)) (k , (2, 2)) (k , (1, 2)) (k , (2, 0)) (k , (1, 1)) ((i, j ) = (0, 0)) ((i, j ) = (1, 0)) ((i, j ) = (1, 1)) ((i, j ) = (1, 2)) ((i, j ) = (0, 1)) ((i, j ) = (2, 0)) ((i, j ) = (2, 1)) ((i, j ) = (2, 2)) ((i, j ) = (0, 0)) ((i, j ) = (1, 0)) ((i, j ) = (1, 1)) ((i, j ) = (1, 2)) ((i, j ) = (0, 1)) ((i, j ) = (2, 0)) ((i, j ) = (2, 1)) ((i, j ) = (2, 2)) ψ(l1 , l2 , 2)((k , (i, j ))) = ψ(l1 , l2 , 3)((k , (i, j ))) =   (k , (1, 0)) (k , (2, 1)) (k , (0, 0)) (k , (0, 1)) (k , (1, 2)) (k , (2, 0)) (k , (1, 1)) (k , (2, 2)) (k , (2, 1)) (k , (0, 0)) (k , (0, 1)) (k , (1, 0)) (k , (2, 0)) (k , (1, 1)) (k , (2, 2)) (k , (1, 2)) ((i, j ) = (0, 0)) ((i, j ) = (1, 0)) ((i, j ) = (1, 1)) ((i, j ) = (1, 2)) ((i, j ) = (0, 1)) ((i, j ) = (2, 0)) ((i, j ) = (2, 1)) ((i, j ) = (2, 2)) ((i, j ) = (0, 0)) ((i, j ) = (1, 0)) ((i, j ) = (1, 1)) ((i, j ) = (1, 2)) ((i, j ) = (0, 1)) ((i, j ) = (2, 0)) ((i, j ) = (2, 1)) ((i, j ) = (2, 2)) Thus, we have the 4-multiple star product Ka ∗ψ (Kb ∗φ G8 ). It can be easily shown that the order and the maximum degree of the graph is 8ab and 4a + b − 2 respectively. We have to prove the remaining property that the diameter of the graph is 2. Proposition. The following equation holds. D(Ka ∗ψ (Kb ∗φ G8 )) = 2 Proof. The strategy of this proof is straightforward; traveling from one arbitrary vertex to any other takes two steps at most. We show that, for all l1 , l2 in V (Ka ), k1 , k2 in V (Kb ), u, v in V (G8 ), (l1 , (k1 , u)) ∼ (l2 , (k2 , v)) holds or there exists w in V (Ka ∗ψ (Kb ∗φ G8 )) such that (l1 , (k1 , u)) ∼ w ∼ (l2 , (k2 , v)). It is sufﬁcient to prove that l1 (cid:54)= l2 because the diameter of Kb ∗φ G8 is 2. Let us deﬁne A, B , C, D ⊂ V (G8 ) as follows; A = {(0, 0), (1, 0), (1, 1), (1, 2)} B = {(0, 1), (2, 0), (2, 1), (2, 2)} C = {(0, 0), (0, 1), (1, 0), (2, 1)} D = {(1, 1), (2, 2), (1, 2), (2, 0)} Thus, we get V (G8 ) = A (cid:116) B = C (cid:116) D . Additionally, let G be a graph and S, T ⊂ V (G), we use the notation S ∼ T if and only if for all s in S there exists t in T such that s (cid:54)= t, s ∼ t, and for all t in T there exists s in S such that t (cid:54)= s, t ∼ s. Using this adjacency of subsets of vertices, for example that C ∼ D holds in G8 . We use the further notation (l, k , S ) = {(l, k , s)|s ∈ S }. For k1 (cid:54)= k2 , the following holds; (l1 , k1 , A) ∼ψ (l2 , k1 , C ) ∼φ (l2 , k2 , C ) (l1 , k1 , A) ∼φ (l1 , k2 , B ) ∼ψ (l2 , k2 , D) (l1 , k1 , B ) ∼ψ (l2 , k1 , D) ∼φ (l2 , k2 , D) ACKNOW L EDGM ENT We would like to thank Miikael-Aadam Lotman and Masanori Kanazu. http: "
Logic-based implementation of fault-tolerant routing in 3D network-on-chips.,"The susceptibility of on-chip communication links and on-chip routers to faults has guided the research towards focusing on fault-tolerance aspects of 2D and 3D Network-on- Chips (NoCs). In this paper, we propose Logic-Based Distributed Routing for 3D NoCs (LBDR3D), a scalable, re-configurable and fault-tolerant mechanism, which utilizes only two virtual channels for implementing any deadlock-free turn model routing algorithm in partially vertically connected 3D NoCs. Such networks might emerge either due to the limitation of on-chip area for vertical links or due to occurrence of fault because of wear-out. LBDR3D guarantees live-lock freeness as well as connectivity regardless of the location and number of vertical links as long as faults do not disconnect the network. Our method relies on a limited set of bits which describe the topology and routing algorithm, updated using an offline algorithm. Our Experimental results show the comparison of LBDR3D with three previously proposed fault-tolerant mechanisms, Elevator-First, North-East To Z (NETZ) and East-Then-West (ETW). Compared to Elevator-First, our proposed mechanism is more flexible and in terms of packet latency, it performs better or equal under even extreme fault scenarios for vertical links. Furthermore, as long as the topology is supported by the routing algorithm, LBDR3D can tolerate faults on horizontal links in each layer. In contrast to NETZ and ETW, LBDR3D does not rely on the location of vertical links as long as the network is connected.","Logic-Based Implementation of Fault-Tolerant Routing in 3D Network-on-Chips Behrad Niazmand⇤ , Siavoosh Payandeh Azad⇤ , Jos ´e Flich† , Jaan Raik⇤ , Gert Jervan⇤ , Thomas Hollstein⇤‡ ⇤Department of Computer Engineering,Tallinn University of Technology, Tallinn, Estonia †Department of Computer Engineering, Universitat Polit `ecnica de Val `encia,Valencia, Spain ‡ Informatik und Ingenieurwissenschaften, Frankfurt University of Applied Sciences, Frankfurt, Germany {bniazmand, siavoosh, jaan, gert.jervan, thomas}@ati.ttu.ee, jﬂich@disca.upv.es, hollstein@fb2.fra-uas.de Abstract—The susceptibility of on-chip communication links links increases, therefore creating a communication bottleneck. and on-chip routers to faults has guided the research towards These missing vertical links can be either the result of faults, focusing on fault-tolerance aspects of 2D and 3D Network-onsuch as wear-out, or they can be related to saving area due Chips (NoCs). In this paper, we propose Logic-Based Distributed to the on-chip area constraints. Therefore, in order to run an Routing for 3D NoCs (LBDR3D), a scalable, re-conﬁgurable and application on such NoCs, a routing mechanism that is both fault-tolerant mechanism, which utilizes only two virtual channels for implementing any deadlock-free turn model routing algorithm fault-tolerant and adaptive, would help to mitigate the issue by uniformly distributing packets on the communication links in partially vertically connected 3D NoCs. Such networks might emerge either due to the limitation of on-chip area for vertical and bypassing the faulty links, while being re-conﬁgurable at links or due to occurrence of fault because of wear-out. LBDR3D the same time. guarantees live-lock freeness as well as connectivity regardless of the location and number of vertical links as long as faults do not A scalable logic-based distributed routing mechanism, disconnect the network. Our method relies on a limited set of named LBDR, supporting turn model adaptive routing algobits which describe the topology and routing algorithm, updated rithms, has been proposed in [6] for 2D NoCs, however, the pousing an ofﬂine algorithm. Our Experimental results show the tential exists for the case of 3D NoCs with partially vertically comparison of LBDR3D with three previously proposed faultconnected links. In this paper, we extend the logic of LBDR tolerant mechanisms, Elevator-First, North-East To Z (NETZ) to support 3D NoC topologies, and we name it LBDR3D. The and East-Then-West (ETW). Compared to Elevator-First, our mechanism is augmented with additional conﬁguration bits, i.e. proposed mechanism is more ﬂexible and in terms of packet latency, it performs better or equal under even extreme fault new connectivity bits for supporting 3D topologies and a new scenarios for vertical links. Furthermore, as long as the topology set of bits called as vertical bits. The proposed mechanism is is supported by the routing algorithm, LBDR3D can tolerate scalable in a sense that, unlike some previous works (e.g. [7]), faults on horizontal links in each layer. In contrast to NETZ and routers do not require to store the location address of nodes ETW, LBDR3D does not rely on the location of vertical links as with vertical links in each layer. In addition, the approach does long as the network is connected. not incur additional overhead to packets when steering them Keywords—fault-tolerance; routing algorithm; reconﬁgurabilacross the layers of a 3D NoC. ity; reliability; Network-on-Chip. LBDR3D gives the highest priority to the vertical links if the destination of a packet is not on the same layer as source, thus, steering it to the nearest node with vertical link, if needed. the nearest node is calculated using an ofﬂine algorithm (described later in Section III.B) and fed into the logic of the mechanism by setting the so called vertical bits using a re-conﬁguration framework [8]. In case the destination is on the same layer as the source, any deadlock-free adaptive and deterministic turn model routing algorithm (already supported by LBDR [6]) can be implemented using LBDR3D. By using only two Virtual Channels (VCs) and not allowing packets to change their VC, the mechanism separates packets going upwards and downwards (a separate VC is used for each direction), thus guaranteeing deadlock freeness for crosslayer trafﬁc ﬂows. In addition, the mechanism ensures livelock freeness using signals from the input ports (described in Section III.C), and guarantees connectivity as long as faults do not disconnect the network, therefor,e making it possible to tolerate scenarios with extreme faulty vertical links and faulty horizontal links (as long as LBDR supports the topology in each layer). The rest of the paper is organized as follows: Section II reviews the previous works in the ﬁeld of fault-tolerant I . Three-dimensional (3D) integration is one of the solutions that has gained momentum recently in order to alleviate the interconnect wire delay by stacking active silicon layers [1]. On the other hand, Network-on-Chip (NoC) is one of the trends that has been considered in research in order to overcome the communication bottleneck in previous architectures such as shared bus [2] [3], by providing scalability, ﬂexibility, transparency and modularity. In an on-chip network, processing cores communicate with each other on one layer and they might also need access to their memory blocks at the same time, therefore one approach can be placing the memory blocks on an adjacent layer in a 3D NoC architecture. Different research works have focused on the topic of 3D integration of NoCs by using stacked layers [1]. IN TRODUC T ION As the number of vertical links is reduced in a 3D NoC - thus, transforming them into vertically partially connected 3D NoCs [4] [5] - the utilization of the remaining vertical 978-1-4673-9030-9/16/$31.00 c 2016 IEEE routing algorithms for 3D NoCs. Section III is dedicated to the description of the LBDR3D logic, along with the proposed ofﬂine algorithm for computing the vertical bits. Section IV is dedicated to proofs of deadlock and live-lock freeness of LBDR3D, along with a proof of providing connectivity for every source-destination pair. Section V is dedicated to the experimental results and comparison with three other fault-tolerant routing mechanisms for 3D NoCs in terms of performance (average latency), reliability, ﬂexibility and area overhead. Finally, section VI concludes the paper. I I . R ELAT ED WORK There has been a number of works proposing fault-tolerant routing algorithms for Network-on-Chips. In this paper, we have mostly focused on the state-of-the-art regarding faulttolerant routing algorithms for 3D NoCs, however, our proposed mechanism also works for the 2D domain. Authors of [9] have introduced a fault-tolerant routing scheme in 3D NoCs, named 4NP-First, based on the Negative-First turn model. The drawback of the algorithm is the overhead of the packet replication if the number of faulty links in the network exceeds a threshold. In [10], a low-overhead fault-tolerant deﬂection routing algorithm is proposed for 3D Mesh NoCs. The limitation of this work is scalability due to using routing tables per layer. Authors of [11] have introduced AFRA, a deadlock-free and deterministic routing algorithm (based on an extension of ZXY routing algorithm) for 3D NoCs. However, the proposed algorithm has limitations regarding the location of faults occurring on vertical links. Ebrahimi et al. have proposed HamFA [12], which takes advantage of Hamiltonian paths in order to tolerate faults in 2D and 3D NoCs. Despite the advantages compared to [9] and [10], HamFA is not able to address faults on vertical links at the end of the Hamiltonian paths and also some of the horizontal links in each layer, as shown in [12]. Jiang et al. have presented an efﬁcient fully adaptive faulttolerant routing algorithm for 3D NoCs [13]. The algorithm consists of two phases: inter-layer and intra-layer routing. Two assumptions that limit this work are as follows: 1) Processing Elements (PEs) will never get faulty and 2) faults on links are considered as bidirectional. Also, the deadlock recovery mechanism used in this work can impose additional performance overhead. Authors of [14] have proposed a high-performance reliable and deadlock-free routing scheme (HARS), which follows a mid-node searching method in 3D NoCs without requiring any Virtual Channels (VCs). However, reliability results are only provided when up to 10% of the network vertical links are faulty. In [4] a distributed routing algorithm has been proposed for partially vertically connected 3D NoCs, named Elevator-First. The algorithm can tolerate faults on vertical links, regardless of the location and the number of faulty links. In order to guarantee deadlock freeness, the method depends on using two virtual channels along X and Y dimensions. Despite the advantages, the algorithm relies on an additional overhead in header ﬂits, when steering packets to nodes with vertical links (called as elevator nodes). Also, each router should store the location of at least one up and one down elevator node in its layer for fault-tolerance purposes which can impose additional memory overhead as the network scales up. The mechanism proposed in this work is based on an extension of Logic-Based Distributed Routing (LBDR) [6] to the 3D domain. LBDR is capable of implementing different deadlock free turn model routing algorithms and depends only on a set of routing and connectivity bits that describe the routing function and the topology, respectively. The mechanism removes the need for routing tables at routers, thus being scalable. LBDR is able to support 2D Mesh and topologies derived from the 2D Mesh (as shown in [6]), however, it lacks the support for 3D NoC topologies. In [15],  LBDR is introduced which is a congestion-aware version of LBDR, able to take routing decisions based on the trafﬁc status of links connecting each router to its neighbors, however, it only supports 2D topologies and support for the 3D is remained as future work. On the other hand, LBDR3D uses the concept of inter-layer and intra-layer routing, depending on the location of packet’s destination with respect to the source node. The proposed mechanism relies on 2 Virtual Channels (VCs) per input port at each router. Based on whether the packet should be steered upwards or downwards, a ﬁxed VC is assigned to it at the source node and therefore, up-going and down-going packets are separated until they reach their corresponding destinations, which can guarantee the prevention of deadlock even when different data ﬂows are transferred across different layers of a 3D NoC. The main contributions of this work are the following: 1) LBDR3D, A logic-based routing mechanism which supports implementing dead-lock free routing algorithms in partially vertically connected 3D Network-on-Chips, 2) an ofﬂine algorithm integrated with a framework (OSR-Lite [8]) for calculating and updating the new set of conﬁguration bits (i.e. vertical bits) of LBDR3D logic, 3) providing proof of dead-lock freeness, by using only two VCs at each router and utilization of input signals for prevention of live-lock, while at the same time guaranteeing connectivity between all communicating nodes. 4) Tolerating extreme cases of faulty vertical links, as long as faults on vertical links do not disconnected the network, thus not being dependent on the number and location of the faulty vertical links, unlike [5] [16]. As it will be shown in Section 5, LBDR3D is also capable of tolerating faults on horizontal links in each layer, as long as the topology of each layer is supported by the original LBDR mechanism. I I I . D E SCR I PT ION O F LBDR3D M ECHAN I SM In this section, ﬁrst the LBDR3D mechanism and its logic are described. Afterwards, the proposed ofﬂine algorithm which is used for calculating the vertical bits, is explained in detail. A. LBDR3D Mechanism Our proposed mechanism is based on LBDR, which supports 2D Mesh and some of the topologies derived from 2D Mesh [6]. However, in order to add support for 3D NoCs, the connectivity bits (Cx ) of the logic are extended to cover Up and Down directions, in addition to the existing ones for 4 cardinal 2D directions (North, East, West and South), therefore, leading to six connectivity bits per router, as follows: Cx : Cn , Ce , Cw , Cs , Cu , Cd LBDR3D uses the same number of routing bits (Rx y ) as LBDR for implementing the routing algorithm in each layer, as follows: 0 37 53 51 60 63 Fig. 1: A 4 ⇥ 4 ⇥ 4 3D Mesh with 88% faulty vertical links Rxy : Rne , Rnw , Ren , Res , Rwn , Rws , Rse , Rsw One of the new additions to the mechanism is a new set consisting of 8 bits per router, named as vertical bits, based on which the logic can determine whether there is at least one node with up/down vertical link in the corresponding direction or not (4 bits for up and 4 bits for down links). The vertical bits are as follows: N u, Eu, W u, Su, N d, Ed, W d, Sd The bits ending with u indicate that there is at least one vertical node with up link in the corresponding direction. The same applies to the bits ending with d, but for down links. For instance, if the Su bit is set, it means that there exists at least one vertical node with up link in the Southward direction in the current layer. In order to cover the situations in which the vertical node is located on a quadrant with respect to the current node, both the corresponding bits are set. For instance, if a router has a node on the North-East quadrant with the up vertical link, both Nu and Eu bits at the current router are set. Such approach for showing the existence of vertical nodes would save area, as there would not be any necessity to store the location (address) of the node(s) with vertical links (unlike the methods used in [4] and [5]). One important issue is the approach taken to compute the values of the vertical bits, which is addressed in the next subsection via the proposed ofﬂine algorithm that calculates the vertical bits at each router at the same time when connectivity and routing bits are initialized.The re-conﬁguration process is performed using the OSR-Lite framework [8] in a transparent way, without imposing signiﬁcant run-time latency and affecting normal operation of the network. B. Ofﬂine algorithm for calculating vertical bits In order to select a node (router) as a vertical node in a 3D NoC for steering a packet upwards or downwards (if the layer of the destination node is not the same as the current node), a policy is required for prioritization of the direction to take. As shown in Fig. 1, for example, in a 4 ⇥ 4 ⇥ 4 3D Mesh with Algorithm 1: Ofﬂine algorithm for calculating vertical bits at routers in each layer 1 forall the nodes router [curr ] 2 NoC do DVL = ; // Down Vertical Nodes List UVL = ; // Up Vertical Nodes List HTV = 0 // hops to Vertical node if C u[router [curr ]] = 1 then SetVerticalBits(”U”, router [curr ], 0) 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 else forall the router [i] 2 Layer[curr ] do if router [i] 6= router [curr ] then // for each router[i] in current layer except the current node if Cu [router[i]] = 1 then // router[i] is an up vertical node HTV = Distance (router [i],router [curr ]) append HTV to UVL SetVerticalBits(”U”,router [curr ],FindMin(UVL)) if C d[router [curr ]] = 1 then SetVerticalBits(”D”, router [curr ], 0) else forall the router [i] 2 Layer[curr ] do if router [i] 6= router [curr ] then // for each router[i] in current layer except the current node if Cd [router[i]] = 1 then // router[i] is a down vertical node HTV = Distance (router [i],router [curr ]) append HTV to DVL SetVerticalBits(”D”,router [curr ],FindMin(DVL)) // Functions Description: // 1. Distance (router [i],router [curr ]) returns | x +  y |= |xcurr   xrouter [i]|+ |ycurr   yrouter [i]| // 2. FindMin(VerticalNodesSet) returns the vertical node in set VerticalNodesSet with shortest distance to current node,ties are broken randomly // 3. SetVerticalBits(Dir,router[curr],VNode) sets the vertical bits of router[curr] in direction Dir(""U""=up,""D""=down), to vertical node VNode 88% faulty vertical links, if node 53 wants to send a packet to node 37, it has 3 choices for choosing a node on the current layer as an up vertical node (nodes 51, 60 and 63). As it can be seen, we can not guarantee that the total path the packet will take to reach its destination will be the minimal path. Therefore, instead of trying to take the minimal possible path from source to destination, the ofﬂine algorithm calculates the values of vertical bits at each router based on its Manhattan distance to a vertical node. As it can be seen in Fig. 1, two nodes can be chosen as candidates for up vertical nodes (nodes 51 and 60), values of vertical bits for the up direction for node 53 would be as follows (calculated by the ofﬂine algorithm): N u = 0, Eu = 0, W u = 1, Su = 1 Also, since node 53 is located on the bottom-most layer, all the down vertical bits for this node would be set to zero, as follows: N u = 0, Eu = 0, W u = 0, Su = 0 The pseudo-code for the algorithm that computes the packet. In order to prevent a packet from ﬂuctuating between two vertical nodes (which guarantees live-lock freeness), we have utilized four additional signals which are fed from the 2D input ports, as follows: ipX : ipN , ipE , ipW, ipS As an example, if a packet comes from the North input port, ipN signal is set to one. As the packet should not go back to the North direction again, it should not be possible for the packet to be steered towards North (N) direction in search of a vertical link. Next, the directions that the packet may take, are computed, that means the packet is transmitted on the plane using any kind of deadlock free turn model routing algorithm that can already be implemented using LBDR on a 2D NoC. In order to explain the logic of LBDR3D, we focus on one output port, for instance the North (N) output port logic. Similar deductions can be inferred for other output port signals. For the North port to be selected for forwarding the packet, one of the following conditions can hold: (1) The packet’s destination is located on the same layer as the current node and it is located towards the North direction (N 0 .U 0 .D 0 ), or (2) The current node is not a vertical node, but there exists at least one up/down vertical node on the same layer as the current node towards the North direction (i.e. on North direction or on North-East or North-West quadrant): U 0 .(N u0 + N E u0 + N W u0 ) + D 0 .(N d0 + N E d0 + N W d0 ) The second phase of the logic of LBDR3D is similar to the basic LBDR. For instance, in case of the North output logic, if one of the above-mentioned conditions hold, the North output port can be selected if either (1) the destination is located on the same column as the current node in the North direction or (2) it is located on the North-East (NE) or (3) North-West (NW) quadrant and the turn at the next router along North direction allows the packet to take the North to East (Rne = 1) or North to West turn (Rnw = 14), respectively. Finally, for the North port (N) to be considered as the output port for transmitting the packet, the corresponding connectivity bit of North port (Cn ) should also be set. Therefore, in the end, the packet will be forwarded to the North output port (if North is also chosen by the arbitration unit) and it will either reach its ﬁnal destination (if destination is on the same layer as current node) or it will reach the nearest node with up/down vertical link, depending on whether it has to go upwards or downwards (when destination is not on the same layer as current node). The only output port signals that have slightly different logics are U (Up) and D (Down) and the L (Local) output port signals. If a packet reaches a vertical node and has to be steered upwards or downwards, only U or D output port can become active, respectively, and other output port signals are automatically set to zero (based on the logic’s behavior and because the ofﬂine algorithm will calculate all the corresponding vertical bits as zero for a node with vertical link). It should be noted that depending on the topology, and based on the nearest vertical node, the vertical bits for a node might change during the lifetime of the system, if reconﬁguration would be needed. Also, regarding the Local output port (L), it is activated only when the packet has reached its destination (all the direction signals N 0 , E 0 , W 0 , S 0 , U 0 and D 0 are zero). In such case, the packet is forwarded to the Processing Element (PE) Fig. 2: Proposed logic of LBDR3D mechanism vertical bits for each router is summarized in Algorithm 1. These calculations are performed once for up and once for down vertical nodes. As mentioned before, the algorithm is executed ofﬂine and before the normal operation of the network, and afterwards, the values of vertical bits are reconﬁgured (if necessary) using the OSR-Lite framework [8]. Thereafter, the algorithm will only be executed if a new fault occurs in the network and there is a need for re-conﬁguration of the connectivity, routing and vertical bits. How the faults are detected on the links would be out of the scope of this paper. It is worth noting that if a node is an up (down) vertical node itself, all the up (down) vertical bits for that router are set to zero, as the current node can already be chosen by LBDR3D for steering the packets one layer up (down). C. Description of LBDR3D Logic The logic of LBDR3D is proposed based on the principle that packets should be steered towards a node with vertical link, making the packet getting closer to its destination (if possible), but it should not wander between different nodes with vertical links in one layer, since in that case, it can lead to live-lock and affect performance. The complete logic of LBDR3D mechanism is shown in Fig. 2. In the ﬁrst phase, the direction signals are computed by comparing the current address of the packet (stored in the current router) and the destination address of the packet (extracted from the header ﬂit of the packet), i.e. signals N 0 , E 0 , W 0 , S 0 , U 0 and D 0 are computed. Also, in this phase, ﬁrst the quadrants or directions that the packet cannot go are ﬁltered out temporarily for the 0 15 x x x x x x x 31 47 63 (a) x x x x x x x x x 31 47 63 x xx x x x (b) 0 x 15 x x x 0 0 3 51 3 51 63 (c) 61 (d) Fig. 3: Different topology scenarios considered for analysis of routing mechanisms: with (a) 20%, (b) 40%, (c) 84% faulty vertical links, and (d) 88% faulty vertical links with some faulty horizontal links for a 4 ⇥ 4 ⇥ 4 3D Mesh. It should be noted that in ﬁgures (a) and (b), the red vertical links are the faulty and the vertical links that are not shown are the healthy ones. However, in ﬁgure (c) and (d) only the healthy vertical links are shown for the sake of ﬁgure’s simplicity and the faulty ones are not shown. connected to the router through its Local port. IV. D EAD LOCK AND L IV E - LOCK FRE EN E S S AND CONN EC T IV I TY GUARAN TE E A. Proof of deadlock freeness In this section, a formal proof is provided regarding the fact that any deadlock free routing algorithm that is implemented using LBDR3D in each layer of a 3D NoC, would not lead to deadlock when packets are transmitted across the layers. To this end, similar to [4], virtual channels are introduced in this work. A deadlock situation can occur whenever nodes in the network try to access resources (communication links and/or buffers) in a circular way, which can cause the packets to get locked and not advance anymore. In 3D NoCs, Intra-layer communications might still lead to deadlock situation even if each layer already has a deadlock free routing algorithm. The proof of deadlock freeness for LBDR3D is as follows, using only two Virtual Channels (VCs) per each input port and the concept of Channel Dependency Graph (CDG) [17], [18]. A CDG is a directed graph whose nodes are network channels and edges represent dependencies between these channels. If we assume that a cycle exists in the CDG, the cycle can either (1) include vertical links at least one in each direction (up or down), or (2) not include any vertical links. If (2) is the case, then the cycle cannot be formed since all the packets follow the deadlock-free CDG-acyclic routing algorithm on the plane of the supposed cycle (as mentioned before, the routing algorithm used in each layer is deadlock-free). If (1) is the case, then there is at least one link in the up direction and one in the down direction. These two links have no dependency through the cycle since one is using VC1 (for up direction) and the other is using VC0 (for down direction). In that case, we need to demonstrate that there are no dependencies from the down resource (VC0) to the up resource (VC1). This would mean there is one layer where there is a dependency between link X of VC1 and link Y of VC0. Since we do not allow VC0 to VC1 and VC1 to VC0 transitions, therefore the cycle can not form, thus the proof. B. Proof of live-lock freeness A live-lock situation can occur when packets usually take non-minimal paths and never reach their destination. However, as mentioned in Section III, LBDR3D uses a set of signals from input ports when computing the candidate output port(s) for routing packets and it does not allow a packet that has come from a direction going back to the same direction (input signals are denoted as ipN , ipE , ipW and ipS , each corresponding to one of the 4 main cardinal 2D directions). Therefore, the logic automatically guarantees packets would not ﬂuctuate between different nodes with vertical links when they are being routed to another layer in a 3D NoC, thus, it guarantees live-lock freeness. C. Proof of connectivity Similar to the Original LBDR, the proposed mechanism also guarantees connectivity between every pair of communication nodes as long as faults do not disconnect the network. In other words, as long as for each source, at least one minimal path exists to its destination under the current routing algorithm, connectivity is assured. Our work outperforms [5] and [16] in a sense that there are no limitations on the location of vertical links in each layer for guaranteeing connectivity and there is no need for the existence of a pillar connecting all layers. V. EX PER IM EN TA L R E SU LT S A. Performance analysis In order to analyze the performance of LBDR3D, we have compared it with Elevator-First [4]. Both approaches are implemented in an extended cycle-accurate NoC simulator supporting 3D NoCs, i.e. Noxim simulator [19]. The extended version of Noxim supporting Virtual Channels (VC) and LBDR3D is maintained as an open-source project on [20]. In order to evaluate the effect of faulty links on the performance of the network using different routing mechanisms, we considered different scenarios for a 4 ⇥ 4 ⇥ 4 3D NoC with 20%, 40%, 84% faulty vertical links and 88% faulty vertical links with some faulty horizontal links, as shown in Fig. 3ad. The location of faulty vertical links are chosen randomly in the scenarios. However, it is noteworthy that LBDR3D is not limited only to these scenarios. The two other faulttolerant routing mechanisms are North-East to Z (NETZ) [16] and East-Then-West (ETW) [5], [21]. However, since NETZ relies on the existence of a pillar on the North-East column 0 50 100 150 200 250 300 0.005 0.007 0.009 0.011 0.013 0.015 0.017 Packet	Injection	Rate	(Packet/Node/Cycle) 0.019 0.021 A e v r e k c a P e g a t a L t n e y c ( ) s e c y C l Elevator-First	40%	Fault	rate LBDR3D	West-First	40%	Fault	rate Elevator-First	20%	Fault	rate LBDR3D	West-First	20%	Fault	rate LBDR3D	XY	40%	Fault	rate LBDR3D	North-Last	40%	Fault	rate LBDR3D	XY	20%	Fault	rate LBDR3D	North-Last	20%	Fault	rate (a) 0 50 100 150 200 250 0.002 0.0025 0.003 0.0035 0.004 0.0045 0.005 A e v r e k c a P e g a t a L t n e y c ( e c y C l ) s Packet	 Injection	Rate	(Packet/Node/Cycle) Elevator-First	84%	Fault	rate LBDR3D	XY	84%	Fault	rate LBDR3D	West-First	84%	Fault	rate LBDR3D	North-Last	84%	Fault	rate (b) 220 200 180 160 140 120 100 80 60 40 20 0 2 0 0 0 . 5 2 0 0 0 . 3 0 0 0 . 5 3 0 0 0 . 4 0 0 0 . 5 4 0 0 0 . 5 0 0 0 . 5 5 0 0 0 . 6 0 0 0 . 5 6 0 0 0 . 7 0 0 0 . 5 7 0 0 0 . 8 0 0 0 . 5 8 0 0 0 . 9 0 0 0 . 5 9 0 0 0 . 1 0 0 . 5 0 1 0 0 . 1 1 0 0 . 5 1 1 0 0 . 2 1 0 0 . 5 2 1 0 0 . A e v r e k c a P e g a t a L t n e y c ( ) s e c y C l Packet	Injection	Rate	(Packet/Node/Cycle) Elevator-First	40%	Fault	rate LBDR3D	West-First	40%	Fault	rate Elevator-First	20%	Fault	rate LBDR3D	West-First	20%	Fault	rate LBDR3D	XY	40%	Fault	rate LBDR3D	North-Last	40%	Fault	rate LBDR3D	XY	20%	Fault	rate LBDR3D	North-Last	20%	Fault	rate (c) 160 140 120 100 80 60 40 20 0 0.0009 0.0011 0.0013 0.0015 0.0017 0.0019 0.0021 0.0023 0.0025 Packet	Injection	Rate	(Packet/Node/Cycle) A e v r e k c a P e g a t a L t n e y c ( c y C l e s ) Elevator-First	XY	84%	Fault	rate LBDR3D	West-First	 84%	Fault	rate LBDR3D	XY	84%	Fault	rate LBDR3D	North-Last	 84%	Fault	rate (d) 0 20 40 60 80 100 120 140 5 0 0 0 . 5 5 0 0 0 . 6 0 0 0 . 5 6 0 0 0 . 7 0 0 0 . 5 7 0 0 0 . 8 0 0 0 . 5 8 0 0 0 . 9 0 0 0 . 5 9 0 0 0 . 1 0 0 . 5 0 1 0 0 . 1 1 0 0 . 5 1 1 0 0 . 2 1 0 0 . 5 2 1 0 0 . 3 1 0 0 . 5 3 1 0 0 . 4 1 0 0 . A e v r e k c a P e g a a L t t n e y c ( c y C l e s ) Packet	Injection	Rate	(Packet/Node/Cycle) Elevator-First	40%	Fault	rate LBDR3D	XY	40%	Fault	rate LBDR3D	West-First	40%	Fault	rate LBDR3D	North-Last	40%	Fault	rate Elevator-First	20%	Fault	rate LBDR3D	XY	20%	Fault	rate LBDR3D	West-First	20%	Fault	rate LBDR3D	North-Last	20%	Fault	rate (e) 180 160 140 120 100 80 60 40 20 0 0.001 0.0015 0.002 0.0025 0.003 0.0035 0.004 0.0045 0.005 A e v r e k c a P e g a t a L t n e y c ( c y C l e s ) Packet	 Injection	Rate	(Packet/Node/Cycle) Elevator-First	84%	Fault	rate LBDR3D	XY	84%	Fault	rate LBDR3D	North-Last	84%	Fault	rate LBDR3D	West-First	84%	Fault	rate LBDR3D	West-First	88%	Fault	rate	with	 faulty	horizontal	 links (f) Fig. 4: Average latency results for different fault scenarios under (a),(b) Random Uniform, (c),(d) Transpose and (e)(f) Bit-Reversal trafﬁc patterns of the network and also ETW relies on the existence of at least one vertical link on the East side of each layer, they are excluded from the performance analysis results (we have assumed random locations for faulty vertical links). Fig. 4a-f illustrate the simulation results of average packet latency for the compared algorithms, in case of 20%, 40%, 84% faulty vertical links(the percentages correspond to the fault rates written in the ﬁgure). The scenarios represent the ones shown in Fig. 3a-c. In addition, in order to show that LBDR3D supports faults on some horizontal links in addition to vertical ones, simulation results for one scneario is provided in Fig. 4f which corresponds to the topology scenario in Fig. 3d (88% faulty vertical links with some faulty horizontal links). Three different synthetic trafﬁc patterns, i.e. Random Uniform, Bit-Reversal and Transpose are considered in the simulation in order to assess the effect of different fault scenarios on the average network latency and analyzing network’s saturation point for latency. The simulation setup parameters (for Noxim simulator) and the considered scenarios for the experiments are summarized in Table I. It should be noted that during the experiments, each simulation is performed 10 times for each packet injection rate (pir) and the average value of the latency is considered. As it can be seen, in Fig. 4a, 4c, 4e, under 20% and 40% random faulty vertical link conﬁgurations (which correspond to the scenarios shown in Fig. 3a and Fig. 3b, respectively), LBDR3D is able to achieve slightly better average latency results compared to Elevator-First, under Random Uniform (Fig. 4a), Transpose (Fig. 4c) and Bit Reversal (Fig. 4e) trafﬁc patterns. Of course, as it can be observed, when the fault rate increases to 40%, the network starts saturating at a lower packet injection rate (pir) compared to the 20% fault rate case. In addition, when LBDR3D is programmed to a partially adaptive routing algorithm such as West-First and North-Last in each layer (as shown in Fig. 4a, 4c and 4e), under all considered trafﬁc scenarios except random uniform, it can achieve better or similar performance (average latency) results due to lowering the chance of congestion in the network, compared to deterministic routing (Elevator-First and LBDR3D-XY). This also conforms to the observation made 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 TABLE I: Considered Scenarios and simulation parameters Routing Algorithm Network Topology Number of VCs VC depth Network Frequency Simulation Time Warm-up time Trafﬁc patterns LBDR3D-XY,Elevator-First XY, LBDR3DWest First, LBDR3D-North Last 4 ⇥ 4 ⇥ 4 3D Mesh with 20%,40%, 84% faulty vertical links and 88% faulty vertical links with some faulty horizontal links 2 (per each router input port) 4 ﬂits 1 Ghz 10000 cycles (1 cycle = 1ns) 1000 cycles (1 cycle = 1ns) Random Uniform, Bit-Reversal and Transpose in [22] which illustrates that deterministic dimension-ordered routing algorithms such as XY achieve better latency results compared to the adaptive routing algorithms as they distribute the trafﬁc in a more uniform manner in the long run. Furthermore, as demonstrated in Fig. 4b, 4d and 4f, as the percentage of faulty vertical links increases to 84%, LBDR3D still achieves comparable average latency results to ElevatorFirst under the three considered synthetic trafﬁc patterns. In addition, Fig, 4b, 4d and 4f conﬁrm that as more links get faulty, there would be less path diversity for packets being transmitted between layers and therefore the network saturates at a lower packet injection rate. This is also due to higher utilization of vertical links for cross-layer trafﬁc ﬂows. It is noteworthy that all the scenarios demonstrated in Fig. 3 can be simulated using the conﬁguration ﬁles (for Noxim) provided available at [20], [23] for all the considered trafﬁc patterns and the packet injection rates shown in Fig. 4. B. Flexibility and scalability analysis In order to evaluate the ﬂexibility and scalability of LBDR3D, it is compared with Elevator-First. As mentioned earlier, both LBDR3D and Elevator-First follow the concept of inter-layer and intra-layer routing when the desintation of a packet is not on the same layer as the current node. However, unlike Elevator-First [4], in our approach, the packets are not augmented with additional intermediate destination address of the elevator nodes. Moreover, we remove the need for storing the location address of elevator node(s) per each router at the current layer, and substitute it by a ﬁxed limited set of vertical bits, thus making LBDR3D scalable (more details explained in Section V-C). From another point of view, in [4] only the deterministic XY routing is considered and for simplicity, the topology of each layer is assumed to be 2D Mesh. Thus, one might infer that the routing logic of Elevator-First is ﬁxed, therefore, if a fault occurs in the network and the topology changes or the routing algorithm needs to be re-conﬁgured, the whole routing logic needs to be re-designed, and only the addresses of the elevator nodes can be updated. However, in case of LBDR3D, if a fault occurs, during the re-conﬁguration phase, not only the vertical bits, but also the connectivity and routing bits can be updated, thus, in addition to add support for the new faulty topology, the routing algorithm can also be updated. As an example, Elevator-First with XY routing would not be able to support the topology shown in Fig. 3d, however LBDR3D when programmed to West-First routing, can still (a) (b) Fig. 5: Area consumption (in µm2 ) for different routing mechanisms, (a) for showing scalability of LBDR3D over Elevator-First, (b) for comparison of LBDR3D to ZXY, LBDR, NETZ and ETW. guarantee connectivity among all nodes and tolerate faults on horizontal links to some extent. Also, for such scenario, LBDR3D can obtain acceptable performance (average latency) results compared to the other scenarios, as shown in Fig. 4f (under Bit-Reversal trafﬁc pattern). C. Area consumption analysis In our experiments, we also made an area consumption comparison, by describing the RTL logic of LBDR3D for a 4 ⇥ 4 ⇥ 4 and 10 ⇥ 10 ⇥ 10 3D Mesh and also the logic of ZXY routing (which is a non-fault-tolerant mechanism), ElevatorFirst, NETZ and ETW, and synthesized them using Synopsys Design Compiler [24]. The results showed 34.6% increase in area for a 4 ⇥ 4 ⇥ 4 3D Mesh and 11.7% increment in area for a 10 ⇥ 10 ⇥ 10 3D Mesh, when comparing LBDR3D logic to Elevator-First using XY routing. The decrease in the area overhead can be a proof of the scalability of LBDR3D as it does not store the location of nodes with vertical links in each layer. In addition, when comparing LBDR3D programmed to XY routing (LBDR3D XY) with NETZ and ETW for the case of a 4 ⇥ 4 ⇥ 4 3D Mesh network, the area overhead was only around 5.02% and 5.1%, respectively. Another explanation for the area overhead of LBDR3D compared to ZXY, LBDR, Elevator-First, NETZ and ETW would be the additional set of vertical bits and the new logic for supporting 3D NoC topologies, but at the same time it brings the advantage of providing ﬂexibility and not relying on existence of any pillars in the topology. The area consumption results are illustrated in Fig. 5. For better readability, two groups are considered. In Fig. 5a LBDR3D is compared only to Elevator-First (both the general logic and also when programmed to XY routing). Also, its scalability comparison is performed in Fig. 5a when the size of the network grows. Our experiments showed that after the network size of 25 ⇥ 25 ⇥ 25, LBDR3D can even achieve better area results, as it does not rely on the location address of elevator nodes. The second group, which is shown in Fig. 5b, includes comparison of the ZXY routing, LBDR3D and LBDR3D XY, NETZ and ETW, all for a 4 ⇥ 4 ⇥ 4 network. It is noteworthy that the area results are obtained using NanGate Open Cell 45 nm Library [25] and synthesis of the RTL of the designs is performed using Synopsys Design Compiler [24]. D. Reliability analysis In terms of reliability and tolerance to faults on vertical links, we have compared LBDR3D with two other proposed approaches in [5], [21] and [16], named East-Then-West (ETW) and North-East To Z (NETZ). Both methods rely on the existence of a pillar, i.e. NETZ requires the existence of at least one pillar in the North-East column of the network and ETW also needs at least one vertical link on the East column of the network. However, our proposed mechanism does not impose such limitations. For instance, as mentioned before, LBDR3D supports all the topologies shown in Fig. 1 and Fig. 3a-d. In case of faults on horizontal links, LBDR3D and Elevator-First, both support all the topologies shown in Fig. 1 and Fig. 3a-c. But in the case of Fig. 3d, since the routing algorithm of Elevator-First in each layer is XY, the topology is not supported as some of the minimal paths between the nodes are broken. However, in such topology (Fig. 3d) LBDR3D can be re-programmed, for instance, to the West-First turn model. V I . CONCLU S ION In this paper, LBDR3D, a fault-tolerant, deadlock and livelock free logic-based distributed mechanism for implementing routing algorithms in 3D NoCs was introduced. The mechanism, which is an extension to the baseline LBDR approach, only relies on a ﬁxed set of conﬁguration bits describing the routing algorithm and the topology. In addition, an ofﬂine algorithm was utilized in order to calculate the new set of vertical bits for the proposed mechanism, re-conﬁgured in transparent way using a previously introduced framework. The proposed mechanism is able to tolerate extreme fault scenarios for vertical links in a 3D NoC as long as the network is not disconnected. Moreover, faults on horizontal links are tolerated, as long as the topology in each layer is already supported by LBDR. Experimental results, compared LBDR3D with three other proposed fault-tolerant routing algorithms for vertically partially connected 3D NoCs. It was shown that it can achieve similar performance results under synthetic trafﬁc patterns compared to the Elevator-First routing algorithm, without storing the location of elevator nodes and not augmenting packets with additional information. In addition, our proposal does not rely on the existence of pillars in the network, unlike NETZ and ETW. Finally, area consumption results showed the scalability of the mechanism compared to Elevator-First and its insigniﬁcant overhead compared to NETZ and ETW, which comes at the price of more ﬂexibility. ACKNOW L EDGM ENT S The work has been supported by EU FP7 STREP BASTION, EU’s H2020 RIA IMMORTAL, EU’s H2020 Twinning TUTORIAL, Estonian Science Foundation grant ETF9429, Estonian institutional research grant IUT 19-1, and funded by Estonian Ministry of Education and Research. [1] "
Using benes networks at fault-tolerant and deflection routing based network-on-chips.,"Fault tolerance and energy consumption are two key aspects of future Network-on-Chips (NoCs). The utilization of deflection routing and Banyan networks for fault-tolerant and energy efficient router architectures was proposed recently. Compared to traditionally deployed crossbars, Banyan networks can reduce hardware costs and improve the energy efficiency of router architectures. However, they also cause substantial disadvantages in terms of fault tolerance, as they are internal blocking, single-path networks. In contrast, Benes networks have some considerable advantages if they are used for fault-tolerant NoCs, as they are rearrangeable non-blocking, multi-path networks. In this paper, we identify the problems of Banyan networks when it comes to fault tolerance. Further, we show that a router architecture, which is instead based on a Benes network, can solve all identified problems, if constructed carefully. Comparisons to existing fault-tolerant and deflection routing based NoCs are presented. The achieved results show, that compared to Banyan networks, Benes networks can improve the performance at almost no additional hardware costs.","Using Benes Networks at Fault-tolerant and Deﬂection Routing based Network-on-Chips Armin Runge and Reiner Kolla Department of Computer Science University of W¨urzburg, Germany Email: {runge,kolla}@informatik.uni-wuerzburg.de Abstract—Fault tolerance and energy consumption are two key aspects of future Network-on-Chips (NoCs). The utilization of deﬂection routing and Banyan networks for fault-tolerant and energy eﬃcient router architectures was proposed recently. Compared to traditionally deployed crossbars, Banyan networks can reduce hardware costs and improve the energy eﬃciency of router architectures. However, they also cause substantial disadvantages in terms of fault tolerance, as they are internal blocking, single-path networks. In contrast, Benes networks have some considerable advantages if they are used for fault-tolerant NoCs, as they are rearrangeable non-blocking, multi-path networks. In this paper, we identify the problems of Banyan networks when it comes to fault tolerance. Further, we show that a router architecture, which is instead based on a Benes network, can solve all identiﬁed problems, if constructed carefully. Comparisons to existing fault-tolerant and deﬂection routing based NoCs are presented. The achieved results show, that compared to Banyan networks, Benes networks can improve the performance at almost no additional hardware costs. I. Introduction Technology scaling as well as growing communication requirements lead to Network-on-Chips (NoCs) as the dominant communication infrastructure for many-core systems. NoCs consist of a number of routers and links, connecting two routers to each other. A frequently used topology is the 2D mesh, at which every router has ﬁve ports (north (n), east (e), south (s), west (w), and one local port (l)). However, the energy consumption of NoCs of today’s generation is substantial and will be a primary barrier in the future [1], [2]. The NoC of Intel’s 80-Core Teraﬂops Research Chip, for example, consumes 28% of the system power [3]. A signiﬁcant proportion of energy and area used for the network is consumed by buﬀers [4], [5]. In contrast, deﬂection routing does not need buﬀers and allows simple local ﬂow control. Each router merely computes a permutation between all input and output ports. This principle leads to area and power savings, as well as to a short router latency. These beneﬁts are enhanced, if permutation networks are used instead of crossbars. The permutation networks which have been deployed so far at deﬂection routing based NoCs [6], [7], 978-1-4673-9030-9/16/$31.00 c(cid:13)2016 IEEE [8] are Banyan networks [9]. Those networks consist of two stages of permuter blocks (cf. Section III), if a 2D mesh topology and routers with four input and output ports are used. Unfortunately, Banyan networks have substantial drawbacks in terms of fault tolerance. The ever growing number of cores and resources per chip is caused by continued technology scaling. The ﬁner structural widths lead to increasing variability in performance and reliability. Aging eﬀects, such as electromigration, hot carrier injection, etc., lead to an irreversible device degradation over time. Therefore, the probability of link failures or even the breakdown of whole routers increases for structures in nanometer range [10]. Future NoCs should thus be able to cope with failures as they are ubiquitous in those systems. Whereas forward error correction (FEC) and retransmission can be used to handle transient and intermittent failures, the inherent redundancy of NoCs should be used to tolerate permanent failures. Therefore, we focus on permanent faults in this work. As an equal number of input and output ports is a prerequisite for deﬂection routing, we further assume bidirectional link failures. To the best of our knowledge, this is the ﬁrst work that identiﬁes the problems of Banyan networks at NoCs in terms of fault tolerance. Furthermore, we present Benes networks [11] as a solution for all identiﬁed problems. The remainder of this paper is organized as follows. In Section II we present related work. The beneﬁts and drawbacks of Banyan networks at NoCs are shown in Section III. Benes networks and the required adaptations for fault tolerance are presented in Section IV. Section V introduces the herein evaluated router architecture, which is based on a Benes network. An evaluation of performance and hardware requirements of this architecture, as well as a comparison to existing fault-tolerant and deﬂection routing based NoCs, is given in Section VI. Finally, this paper closes with a summary and conclusion in Section VII. II. Related Work In recent years, several approaches have been investigated to achieve fault-tolerant NoCs. An overview of failure mechanisms, fault models and fault tolerance methods is given in [12]. In buﬀered NoCs, special attention has to be paid to deadlock avoidance, as buﬀers are a shared resource, which may arise a circular dependence. This restricts the possible routing decisions for fault-tolerant routing algorithms even more. Deadlock free routing algorithms are often based on the so called turn model [13], where some turns are prohibited, or on virtual channels [14]. In contrast, deﬂection routing is inherently deadlock free. Though, routing algorithms have to prevent livelocks, which can be achieved by prioritizing all ﬂits according to their age. This makes deﬂection routing attractive for small, energy eﬃcient and fault-tolerant NoCs. However, only few fault-tolerant deﬂection routing based router architectures exist. A fault-tolerant routing algorithm which is based on deﬂection routing is presented in [15]. In [16], [17], the Nostrum NoC [18], which utilizes deﬂection routing, was extended to tolerate transient, intermittent and permanent faults. Their fault-adaptive “cost-based deﬂection routing” algorithm even employs the remaining functionality of partial defective routers. The main emphasis of their architecture lays on highest possible performance and optimal routing decisions. However, this is contrary to our approach, as our design focus lies on an energy eﬃcient and fast router architecture. This is why the cost based router architecture is omitted for the evaluation in Section VI. Feng et al. reported in [19] that the costbased routing algorithm causes a drastic decline of over 60% of the achievable frequency. They presented a faulttolerant deﬂection routing algorithm called FON which makes routing decisions based on fault information of twohop neighbor routers [19]. FON can only tolerate faultregions with a single convex/concave point. In [20], Feng et al. also presented a fault-tolerant deﬂection routing algorithm FTDR based on Q-learning. FTDR is a tablebased algorithm, which outperforms FON and ”cost-based routing” in terms of throughput and average hop count. However, even if the achievable frequency is quite similar to FON, the required area is yet higher than that of costbased routing. To reduce the routing table overhead, Feng et al. proposed FTDR-H, in which each switch contains a local and a region routing table. Additionally, a 3D version of FTDR was presented in [21], and a version that tolerates transient faults was presented in [22]. All these NoCs are based on router architectures that utilize a crossbar. Fallin et al. presented the non-fault-tolerant CHIPPER [6] router architecture which also uses deﬂection routing. CHIPPER is based on a permutation network instead of a crossbar, which enables a very fast and energy eﬃcient router architecture. MinBD [7], also presented by Fallin et al., is an extended version of [6]. Two important changes compared to [6] are the second ejection port and the side buﬀer, which reduces the number of deﬂections. Recently, FaFNoC [8], which is also based on CHIPPER but is additionally fault-tolerant, was presented. Instead of routing tables, as e.g. used in FTDR(-H), this architecture uses the concept of fault-aware ﬂits. The herein presented router architecture is based on FaFNoC [8], but utilizes a Benes network instead of a Banyan network (like FaFNoC) or a crossbar (like FON, FTDR, FTDR-H). Compared to Banyan networks, Benes networks allow easier fault handling and enable performance improvements at almost no additional hardware costs. III. Limitations of Banyan Networks st (pi )=0 st (pi )=1 st (pi )∈{0,1} don’t care Fig. 1. FaFNoC router architecture, consisting of the non-faulttolerant CHIPPER router architecture (depicted in black) and two additional components for fault tolerance (depicted in gray). Banyan networks are multistage networks which have exactly one path from any input to any output port. Such a network is build from four 2 × 2 permuter blocks pi (cf. Fig 1), whereas i ∈ {1, 2, 3, 4}. Each permuter block pi has the ability to swap both inputs (state st of pi is st (pi ) = 1) or pass the inputs to the corresponding outputs (st (pi ) = 0). Permuter blocks are self-routing, which means that every permuter block routes the packets using only local information. In CHIPPER [6], the permuter blocks of the ﬁrst stage (p1 and p2 ) just consider the higher prioritized ﬂit’s y -coordinate of the destination (dsty ). If dsty does not equal the router’s y -coordinate, the ﬂit is transferred to the vertical axis (p3 ), and otherwise to the horizontal axis (p4 ). The permuter block p3 (p4 ) just considers the dsty (dstx ) coordinate. If the dsty (dstx ) coordiante is higher than the current router’s y -coordinate (x-coordinate), the packet is transferred to the ﬁrst output, which is the n-port (eport). This means, packet prioritization, routing decision and the actual packet transmission is handled by the permuter blocks. In contrast to this distributed control, a crossbar based switch usually uses a central controller, which sorts all arriving ﬂits according to their priority and arbitrates the output ports. This yields in a long critical path due to the sequential dependence of ﬂit sorting and port arbitration. A Banyan network makes the crossbar and the corresponding arbitration logic superﬂuous, which enables a fast and simple router architecture. A. Drawbacks of Banyan Networks However, besides these beneﬁts, Banyan networks suﬀer from three disadvantages compared to traditionally deployed crossbars or other permutation networks like Benes networks: 1) internal blocking / impossible permutations 2) healthy links not utilizable in case of link failures 3) central component required for fault tolerance Internal blocking means that some permutations of inputs and outputs can not be realized, even if no collision occurs at the outputs. For a Banyan network it is only guaranteed that the highest prioritized ﬂit gets one hop closer to its destination. Fig. 2 shows two diﬀerent permutations. A permutation where both ﬂits are routed (a) Possible permutation (b) Impossible permutation due to internal blocking Fig. 2. A possible and an impossible permutation for a Banyan network. Please cf. the legend in Fig. 1. to their preferred output is depicted in Fig. 2a. Please note that this kind of permutation is very frequent if dimension order routing is used, as no turns are performed and both ﬂits follow a straight line. Fig. 2b shows an impossible permutation. Here, only the highest prioritized ﬂit is allocated to its preferred direction (the ﬂit from the north). This ﬂit determines the state of permuter block p1 and is sent to p3 . Hence, the less prioritized ﬂit from the east is deﬂected to permuter p4 . Therefore, the ﬂit from the east can not reach the north, as p4 is only connected to the e and to the w port. (a) Faulty n port (b) Faulty n and w port Fig. 3. Banyan networks with faulty links (depicted in gray). Please cf. the legend in Fig. 1. The second problem from which Banyan networks suﬀer are unusable but healthy links in case of link failures. This can be a serious problem for the performance as well as for the connectivity of a NoC [8]. We brieﬂy illustrate this by means of an example. Fig. 3a shows a Banyan network with a faulty north link. To be able to tolerate a link failure, the faulty link and port must not be used. Therefore, only one ﬂit is allowed to arrive at permuter block p3 , as otherwise the second ﬂit would have to use the north port. As two ﬂits can arrive at p2 , and one of them has to be forwarded to p3 , the ﬂit from the east has to be routed to p4 . In other words, the north input and the north output have to be connected to each other to prevent that the faulty link is used. Hence, the ﬂit from the east can be either routed to the east or to the west, but not to the south, even though the south link is healthy. This routing restriction can cause livelocks, as some routing decisions are impossible even if a physical intact connection between source and destination exists. The third problem of Banyan networks is the necessity of a central component for fault tolerance. If two link failures shall be tolerable, link failure situations could occur where every permuter block is connected to one healthy and one faulty link, as it is the case for the situation depicted in Fig. 3b. There, two routing decisions are possible: e → s, s → e and e → e, s → s. For both routing decisions, all four permuter blocks have to be in the same state. That means, if p1 routes the ﬂit from the east to p3 , then p2 has to route the ﬂit to p4 . Hence, the states of all four permuters have to be coordinated and in particular the states of p1 and p2 . IV. Benefits of Benes Networks In this section we show, that Benes networks can alleviate the ﬁrst, and even solve the second and third problem of Banyan networks presented in Section III. Therefore, Benes networks have important advantages for fault-tolerant router architectures. Further, we show, that several design properties should not be adopted from CHIPPER’s [6] or FaFNoC’s [8] Banyan network. Fig. 4. Benes network with CHIPPER input and output port order. Benes networks with four inputs and four outputs consist of six permuter blocks (cf. Fig. 4). Such networks are rearrangeable non-blocking, which means that the network can be rearranged to provide every possible permutation between all inputs and outputs. The order of input and output ports as well as the wiring between permuter blocks has also a signiﬁcant inﬂuence on the performance of a Benes network. At the Banyan network in [6], these properties were well-chosen for the following reasons: A Banyan network does not allow every possible permutation between all input and output ports. However, the order of the input ports ensures that the most frequent permutation of dimension order routing (n → s, e → w, s → n, w → e) is possible. The order of the output ports allows an eﬃcient implementation of the routing algorithm. As the output ports are grouped to vertical and horizontal ports, the permuter blocks of the ﬁrst stage (p1 , p2 ) just have to compare the y coordinate (x-coordinate) for y -ﬁrst (x-ﬁrst) routing. Even the permuter blocks of the second stage (p3 , p4 ) just have to compare either the y - or the x-coordinate. Further, the Banyan network of the CHIPPER architecture can be easily adjusted for fault tolerance [8]. To avoid the utilization of faulty links or ports, a faulty input and the corresponding output port must be connected to each other. At the Banyan network of [6] and [8], every input port is connected to its corresponding output port if and only if the state of every permuter block pi , which is connected to this input or the corresponding output port, is st (pi ) = 0. Therefore, the required adjustments for fault tolerance are straightforward and independent from the actual number of link failures: every permuter block pi which is connected to a faulty port / link has to pass the ﬁrst input to the ﬁrst output and the second input to the second output (st (pi ) = 0). Unfortunately, if the input and output port order as well as the wiring of the Benes network are adopted from CHIPPER’s Banyan network, as depicted in Fig. 4, the performance would decrease whereas the complexity of the design would increase. The ﬁrst problem of the shown network is internal blocking. Both ﬁrst stage permuter blocks (p1 and p2 ) would transmit a ﬂit, which should be routed along the vertical axis, to permuter block p3 . At p3 , such ﬂits would collide and one would be deﬂected to the horizontal axis (h). The second problem is the fact that the adjustments of the permuter block states for the identity permutation are more complicated than for the Banyan network. As this permutation is required for fault tolerance, this would complicate the design of the whole router architecture. Fig. 5. Fault-tolerant router architecture based on FaFNoC and a Benes network. Components for fault tolerance are depicted in gray. The herein evaluated router architecture is depicted in Fig. 5, which is explained in more detail in Section V. This architecture consists of a Benes network which solves the above mentioned problems. The arrangement of vertical and horizontal output ports is reasonable even for a Benes network, and is thus adopted from CHIPPER. The wiring between the ﬁrst stage and the second stage of the permuter blocks is changed to solve the problem of internal blocking. Internal blocking occurs at the Benes network depicted in Fig. 4 in the second stage of the permuter blocks. Both ﬁrst stage permuter blocks would transmit a ﬂit, which should be routed to the vertical axis, respectively the horizontal axis, to the same permuter block. The adjusted wiring between the ﬁrst and the second stage of permuter blocks ensures that p1 transmits ﬂits for the vertical (horizontal) axis to p3 (p4 ), whereas p2 transmits ﬂits for the vertical (horizontal) to p4 (p3 ). Benes networks allow every possible permutation between all input ports and all output ports. Therefore, the reason for the input port order of CHIPPER’s Banyan network does not exist for a Benes network. However, the input port order depicted in Fig. 5 ensures the following properties: 1) Input port i, (i ∈ {n, e, s, w}) is connected to its corresponding output port o = i if the state of every permuter block pj which is connected1 to port i is st (pj ) = 0. 2) Every permuter block requires only local information, even for fault tolerance. This includes that the adjustments of the permuter blocks’ states are additive. The state adjustments for two diﬀerent one-link fault situations should be the same than for the resulting two-link fault situation. (a) Faulty n port (b) Faulty e port (c) Faulty s port (d) Faulty w port Fig. 6. All one-link fault situations for a Benes network. Faulty ports are depicted in gray. Please cf. the legend in Fig. 1. Fig. 6 shows every possible one-link fault situation. To avoid the utilization of a faulty port, the states of exactly three permuter blocks are predeﬁned: The state of every permuter block pi which is connected to the faulty port has to be st (pi ) = 0. The three remaining permuters are in an arbitrary state. Please note, for two arbitrary but diﬀerent one-link fault situations, the intersection of two sets of permuter blocks with a predeﬁned state is one single permuter block. Fig. 7 shows all possible two-link fault situation. For every two-link fault situation the state of exactly one permuter block remains undeﬁned, as two diﬀerent onelink fault situations share exactly one permuter block with a predeﬁned state. Also for three link failures this principle works. As the state of every permuter block pi which is connected to a faulty port is st (pi ) = 0, the only healthy input port is connected to the corresponding output port. V. Router Architecture In this section, we brieﬂy explain the herein evaluated router architecture which is based on FaFNoC [8], or 1A permuter block pj is connected to an input port i if either pj is a ﬁrst stage permuter block and directly connected to i or if pj is connected to i when st (pk ) = 0, whereas pk are permuter blocks of the preceding stages. (a) Faulty n and e port (b) Faulty n and s port (c) Faulty n and w port (d) Faulty e and s port Algorithm 1 Routing algorithm of permuter block pi // f1 , f2 are the two input ﬂits // ﬁnd higher prioritized ﬂit // pi connected to faulty port 2: 3: 4: 1: procedure Routing(f1 , f2 ) hp ← priority (f1 , f2 ) if ﬁ 6= 0 then set state according to Fig. 6 and Fig. 7 else if hp(fs) 6= 0 then set st (pi ) for turn direction else // turns required for fault-region evasion 5: 6: 7: // according to [8] // fault free permuter and no turns required for hp // route hp to its preferred direction // lower prioritized ﬂit is deﬂected to remaining direction 8: 9: 10: 11: 12: 13: if dst (hp) in direction of out1 then route hp to out1 else route hp to out2 end if end if 14: end procedure // lower prioritized ﬂit to out2 // lower prioritized ﬂit to out1 (e) Faulty e and w port (f) Faulty s and w port Fig. 7. All two-link fault situations for a Benes network. Faulty ports are depicted in gray. Please cf. the legend in Fig. 1. rather CHIPPER [6]. However, the interconnection, which is a Banyan network in both existing router architectures, is replaced by a Benes network. The abilities of Benes networks in turn allow a reduction of the fault tolerance logic, which was partly required to compensate the weak points of the Banyan network. A fault-tolerant router architecture has to ensure two major aspects: 1) faulty links must not be used 2) complex fault situations must be able to overcome At FaFNoC, the ﬁrst aspect is ensured by the fault-handler (cf. Fig. 1), which adjusts the states of all four permuter blocks in case of link failures. The second aspect is handled by the concept of fault-aware ﬂits. Instead of utilizing routing tables, every ﬂit is aware of the encountered fault situation. Inspired by the simple algorithm to ﬁnd a way out of a labyrinth, the ﬂits try to overcome the faulty region by turning always left or always right. To realize this concept, the ﬂit structure in [8] is extended by several additional “fault-region evasion” bits which store the turn direction and the number of required turns. Furthermore, the faultstatus-handler is added to the router pipeline of [8], which is the component that adjusts these bits in the ﬂit structure. In case the fault-region evasion bits are set, the routing decision is adjusted by the faulthandler. A further task of the fault-handler is the returnﬂag handling. The return-ﬂag is required to solve the second problem of Banyan networks mentioned in Section III-A. For more information to the general fault tolerance concept of FaFNoC, please refer to [8]. As shown in Section IV, several drawbacks of Banyan networks do not exist for Benes networks. Therefore, a central component that adjusts the states of the permuter blocks as well as the return-ﬂag handling is not required anymore. Only the general concept of fault-aware ﬂits and the faultstatus-handler are adopted here (cf. Fig. 5). As a Benes network is rearrangeable non-blocking, every permuter block can operate independently of the other permuter blocks and no central component is required for fault tolerance. The general routing algorithm of every permuter block is depicted in Algorithm 1. This algorithm works fully decentralized, in particular in the case of link failures. Every permuter block has a two bit wide input ﬁ , which gives the fault information of the two corresponding input ports. If at least one of these two bits is set, the permuter block adjusts its state according to Fig. 6 and Fig. 7. In contrast to a Banyan network based router architecture, local fault information is suﬃcient for the state adjustments. This is possible as the adjustments are additive and no coordination between permuter blocks is required. Additivity means that the adjustments for two one-link fault situations are the same as for the resulting two-link fault situation. This is achieved by the adjusted input port order and the adjusted wiring between the ﬁrst and the second stage of permuter blocks. The coordination is not required because the routing decision is either predeﬁned (as for three or four link failures), or at least one healthy permuter block exists. A permuter block is called healthy if both connected input ports are healthy. This is one of the important advantages of Benes networks compared to Banyan networks in terms of fault tolerance. If both links are healthy, the fault-status bits of the higher prioritized ﬂit are checked, and if necessary, the routing decision is adjusted for fault-region evasion. In the fault-free case, simple y -ﬁrst routing is performed. VI. Evaluation In this section, we evaluate the performance and the hardware costs of the presented router architecture. Further, we compare the results to existing router architectures which are fault-tolerant and additionally based on deﬂection routing. A. Test method All router architectures are simulated for an 8 × 8 NoC using our in-house cycle accurate simulator implemented in VHDL. We reimplemented several fault-tolerant and deﬂection routing based router architectures, and compared the results against the herein presented router architecture. FON [19], FTDR [20], FTDR-H [20] are based on crossbars, FaFNoC [8] is based on a Banyan network. Every router is connected to a traﬃc generator, which is able to generate diﬀerent synthetic traﬃc. If a ﬂit could not be injected due to congestion, the ﬂit is stored in an injection queue and injected as soon as possible. Every simulation took 5000 clock cycles. For comparability, every reported value is an average value of three simulations. Hence, the plots in Fig. 8 consist of 54 simulations, the plots in Fig. 10-13 consist of 60 simulations. The location of the link failures, and also the traﬃc in case of random traﬃc, are generated by a uniform distributed random process with the same seed for every router architecture. Only connectivity was ensured at the link failure placement. To evaluate the hardware cost, we synthesized the NoCs using Xilinx’s XST. Please note that XST synthesizes a design for FPGAs and does not use a standard cell library like ASIC synthesis software. Nevertheless, this enables us to compare the hardware requirements and the achievable speed of the diﬀerent router architectures. B. Non-fault-tolerant architecture As shown in Section IV, Benes networks have signiﬁcant advantages over Banyan networks with regard to fault tolerance. However, even the pure interconnection architecture of a router has an impact on the hardware costs as well as on the performance. Theoretically, a Banyan network is the most restrictive architecture and the crossbar the least restrictive interconnection architecture. Table I TABLE I Possible and impossible permutations of different interconnection architectures. permutation n → s, s → n n → s, e → n n → e, s → e/w n → e, s → e Banyan Benes crossbar             shows four permutations, whereas the ﬁrst port denotes the input port and the port after the → denotes the preferred output port. The ﬁrst permutation is possible with all interconnection architectures, which means that both incoming ﬂits can be routed to their preferred output port. The second permutation is not possible with a Banyan network, as both ﬂits collide at p3 . At the third permutation, we assume that the second ﬂit, which arrives at s, has two productive output ports (e and w). As a Benes network has no central controller, both ﬂits would collide at p5 . The last permutation is not possible with any interconnection architecture, as both ﬂits have just one productive output port, which is the same port for both ﬂits. In order to compare the impact of the diﬀerent interconnection architectures of a router, we replaced the Banyan network of the non-fault-tolerant CHIPPER router architecture with a crossbar as well as with a Benes network and simulated various traﬃc scenarios. The simulation (a) Average hop count (b) Average latency Fig. 8. Simulation results for non-fault-tolerant router architectures with diﬀerent interconnection architectures, uniform random traﬃc and a variable injection rate. results for uniform random traﬃc with a variable injection rate are depicted in Fig. 8. The average hop count gives the average time in clock cycles a ﬂit spends in the network, which is limited by 255 hops. The average latency further considers the queue time, which is theoretically unlimited. A crossbar based router architecture achieves the least average hop count (cf. Fig. 8a) as well as the least average latency (cf. Fig. 8b). The worst values are achieved with the internal blocking Banyan network. As not all permutations are possible for such a network, this restricts the routing decisions. As Benes networks are rearrangeable non-blocking, every permutation between all inputs and outputs is theoretically possible. However, in contrast to a crossbar based architecture, a central component which adjusts the permutation network is avoided and all permuter blocks are self-routing. This means that all permuter blocks only use local information, which is a drawback in terms of performance. Fig. 9 shows the synthesis results for these non-faulttolerant architectures. The required look-up tables (LUTs) (cf. Fig. 9a) and the achievable frequency (cf. Fig. 9b) of the router architecture based on a Benes network lie in between the values of a crossbar based architecture and a Banyan network based architecture, just as the performance values. The reason for the signiﬁcant higher hardware costs of a crossbar are mainly because of the sorting and arbitration logic, which is not required for permutation network based architectures. (a) Hardware requirements for one router Fig. 9. Synthesis results for the non-fault-tolerant CHIPPER router with diﬀerent interconnection architectures. (b) Achievable frequency C. Fault-tolerant architecture The herein assumed fault model considers permanent link failures, as caused by aging eﬀects (e.g. electromigration). To evaluate fault tolerance, we investigated the impact of diﬀerent link-failure rates to the throughput (Fig. 10), the number of lost ﬂits (Fig. 11), the achievable average hop count (Fig. 12), and the achievable average latency (Fig. 13). Depicted here are the results for random traﬃc as well as for transpose traﬃc. We simulated even more synthetic benchmarks, however, as the results are similar to the depicted results, we omitted them because of space restrictions. Flits are injected at every router with an injection probability of 10%. Hence, every simulation tried to inject approx. 32000 ﬂits. If the 8 bit hop count ﬁeld of a ﬂit overﬂowed, the ﬂit was discarded and logged as a lost ﬂit. This is required as not all router architectures are able to tolerate a high fault rate. (a) Uniform random (b) Transpose Fig. 10. Throughput for an injection rate of 10% and a variable number of random link failures. Fig. 10 shows, that the throughput degrades for both benchmarks with an increasing fault rate. However, FTDR performs best, followed by the Benes network. The same order is achieved for the average hop count (cf. Fig. 12). As FTDR learns the interconnection topology, even very irregular topologies, as they occur for high fault rates, can be tolerated. Our Benes network based router architecture performs only second best. However, a signiﬁcant improvement for all performance metrics can be observed, compared to the remaining router architectures. As FON (a) Uniform random (b) Transpose Fig. 11. Number of lost ﬂits for an injection rate of 10% and a variable number of random link failures. Please cf. the legend in Fig. 10. (a) Uniform random (b) Transpose Fig. 12. Average hop count for an injection rate of 10% and a variable number of random link failures. Please cf. the legend in Fig. 10. (a) Uniform random (b) Transpose Fig. 13. Average latency for an injection rate of 10% and a variable number of random link failures. Please cf. the legend in Fig. 10. just uses two-hop fault information, only fault situations which do not exceed this number are tolerable. FTDR-H utilizes a local- and region-table at every router. Complex fault situations which cause that the only path between source and destination traverses at least two regions can therefore also not be tolerated. This is the reason why a crucial number of ﬂits are lost for FON as well as for FTDR-H (cf. Fig. 11). For FaFNoC, merely a relatively small number of ﬂits is lost. Only with our Benes network based router architecture as well as with FTDR no ﬂits are lost at all. Fig. 14 shows the synthesis results for all herein compared router architectures. It can be seen, that the required number of LUTs for FTDR and FTDR-H increases with the NoC dimension (cf. Fig. 14a), as both architectures utilize routing tables. The three remaining router architectures require a comparable amount of LUTs, which is constant for all NoC dimensions. In particular, this shows, the hardware requirements of the third permuter block stage of our Benes network based router architecture [5] D. Wentzlaﬀ, P. Griﬃn, H. Hoﬀmann, L. Bao, B. Edwards, C. Ramey, M. Mattina, C.-C. Miao, J. F. B. III, and A. Agarwal, “On-chip interconnection architecture of the tile processor,” IEEE Micro, vol. 27, no. 5, pp. 15–31, 2007. [6] C. Fallin, C. Craik, and O. Mutlu, “Chipper: A low-complexity buﬀerless deﬂection router,” Carnegie Mellon University, Tech. Rep. 2010-001, December 2010. [7] C. Fallin, G. Nazario, X. Yu, K. Chang, R. Ausavarungnirun, and O. Mutlu, “Minbd: Minimally-buﬀered deﬂection routing for energy-eﬃcient interconnect,” in Networks on Chip (NoCS), 2012 Sixth IEEE/ACM International Symposium on. IEEE, 2012, pp. 1–10. [8] A. Runge, “Fault-tolerant network-on-chip based on fault-aware ﬂits and deﬂection routing,” in Proceedings of the 9th International Symposium on Networks-on-Chip. ACM, 2015, p. 9. [9] A. Pattavina, Switching Theory, Architectures and Performance in Broadband ATM Networks. Wiley, 1998. [10] S. Borkar, “Designing reliable systems from unreliable components: The challenges of transistor variability and degradation,” IEEE Micro, vol. 25, pp. 10–16, November 2005. [11] V. Benes, “Permutation groups, complexes, and rearrangeable connecting networks,” Bel l System Technical Journal, The, vol. 43, no. 4, pp. 1619–1640, 1964. [12] M. Radetzki, C. Feng, X. Zhao, and A. Jantsch, “Methods for fault tolerance in networks on chip,” ACM Computing Surveys, vol. 1, p. 35, 2013. [13] D. Fick, A. DeOrio, G. Chen, V. Bertacco, D. Sylvester, and D. Blaauw, “A highly resilient routing algorithm for faulttolerant nocs,” in Proceedings of the Conference on Design, Automation and Test in Europe, ser. DATE ’09. 3001 Leuven, Belgium, Belgium: European Design and Automation Association, 2009, pp. 21–26. [14] M. Ebrahimi, M. Daneshtalab, J. Plosila, and F. Mehdipour, “Md: Minimal path-based fault-tolerant routing in on-chip networks,” in Proceedings of 18th Asia and South Paciﬁc Design Automation Conference (ASP-DAC), 2013. [15] M. Fattah, A. Airola, R. Ausavarungnirun, N. Mirzaei, P. Liljeberg, J. Plosila, S. Mohammadi, T. Pahikkala, O. Mutlu, and (a) Hardware requirements for one router (b) Achievable frequency Fig. 14. Synthesis results for the compared router architectures and diﬀerent NoC dimensions. are oﬀset by the more eﬃcient logic for fault tolerance. The achievable frequency (cf. 14b) is similar for the three crossbar based router architectures (FTDR, FTDRH, FON), and slightly higher for FaFNoC and the Benes network based router architecture, which are based on a permutation network. VII. Summary & Conclusion In this paper, we investigated the use of Benes networks at NoCs which are fault-tolerant and additionally based on deﬂection routing. In the past, Banyan networks have already been deployed in deﬂection routing based router architectures. However, in terms of fault tolerance, they suﬀer from several disadvantages, which were identiﬁed in this paper. Further, we showed, that the wiring between the ﬁrst stage and second stage of permuter blocks as well as the input port order should not be adopted from Banyan networks. To evaluate and compare a Benes network based router architecture, we replaced FaFNoC’s Banyan network by a Benes network. The beneﬁts of Benes networks enabled a signiﬁcant reduction of the fault tolerance logic. This reduction even compensates the logic of the two additional permuter blocks. Whereas 24% more logic is required for a non-fault-tolerant router architecture which is based on a Benes network instead of a Banyan network, only 3% more logic is necessary for the fault-tolerant router architecture. Furthermore, signiﬁcant performance improvements are observed, in particular for higher fault rates. Comparisons with existing fault-tolerant and deﬂection routing based NoCs show, that the herein presented router architecture is competitive in terms of hardware costs and performance. "
Powermax - an automated methodology for generating peak-power traffic in networks-on-chip.,"Early estimation of the peak power consumption of a system under development is crucial in assessing the design's reliability and thermal profile, and for benchmarking various architectural options and chip-level power management features. In this paper, we present a versatile power-virus generation technique for Networks-on-Chip (NoC), which allows the designer to quantify the realistically attainable peak power consumption, in order to efficiently guide the design process. The proposed PowerMax methodology generates appropriate network traffic patterns that cause peak power consumption within the NoC. More importantly, PowerMax is a fully automated high-level methodology that can be applied to any network topology and any routing algorithm. The proposed technique maximizes both the network utilization and the data switching activity, thereby causing, on average, 5.5x higher power consumption than synthetic traffic patterns with random behavior. PowerMax can be used as a stand-alone tool to test the power characteristics of the NoC, or it can be embedded in other system-level powervirus applications.","PowerMax: An Automated Methodology for Generating Peak-Power Trafﬁc in Networks-on-Chip Ioannis Seitanidis(cid:3) , Chrysostomos Nicopoulosy and Giorgos Dimitrakopoulos(cid:3) (cid:3)Electrical and Computer Engineering, Democritus University of Thrace, Xanthi, Greece. e-mail: fiseitani, dimitrakg@ee.duth.gr yElectrical and Computer Engineering, University of Cyprus, Nicosia, Cyprus e-mail: nicopoulos@ucy.ac.cy Abstract—Early estimation of the peak power consumption of a system under development is crucial in assessing the design’s reliability and thermal proﬁle, and for benchmarking various architectural options and chip-level power management features. In this paper, we present a versatile power-virus generation technique for Networks-on-Chip (NoC), which allows the designer to quantify the realistically attainable peak power consumption, in order to efﬁciently guide the design process. The proposed PowerMax methodology generates appropriate network trafﬁc patterns that cause peak power consumption within the NoC. More importantly, PowerMax is a fully automated high-level methodology that can be applied to any network topology and any routing algorithm. The proposed technique maximizes both the network utilization and the data switching activity, thereby causing, on average, 5.5(cid:2) higher power consumption than synthetic trafﬁc patterns with random behavior. PowerMax can be used as a stand-alone tool to test the power characteristics of the NoC, or it can be embedded in other system-level powervirus applications. I . INTRODUC T ION Technology scaling has enabled digital system designs with billions of transistors integrated on a single chip. Besides the abundance of resources, which has been the driving force behind the multicore archetype, key (micro-)architectural decisions are dictated by power constraints. Excessive power dissipation increases packaging/cooling costs, reduces battery life in mobile devices, and adversely affects hardware reliability, primarily due to elevated temperatures [1]. The increasingly stringent requirement to adhere to a given power budget has rendered power consumption a ﬁrst-class design constraint [2]. Hence, it is imperative for system architects to understand and accurately quantify their design’s power usage from the early stages of the design process [3]. One particular salient attribute is of paramount importance: the peak (i.e., worst-case) power consumption [4], [5]. Assuming a fake scenario whereby all circuit nodes switch simultaneously will inevitably result in an overly pessimistic estimate. Instead, it is crucial to accurately quantify the realistically attainable peak power consumption, in order to efﬁciently guide the design process. Note that both the system’s maximum performance and implementation costs (power delivery, packaging, and cooling) are directly impacted by this worstcase power consumption. A pessimistic peak power estimate will unnecessarily curtail performance, while an optimistic estimate could potentially lead to reliability problems. I.Seitanidis is supported by the PhD scholarship of Alexander S. Onassis foundation. 978-1-4673-9030-9/16/$31.00 c⃝2016 IEEE The accurate identiﬁcation of worst-case power consumption is extremely challenging, especially as chips become more complex. To further compound the problem, the worstcase power usage of a system is not simply the sum of the maximum power of each component, due to under-utilization and contention for shared resources. Hence, the peak power consumption must be estimated using a stimulus that is realistic and resides within the functionally feasible workload space of the system under evaluation. Typically, designers rely on hand-crafted, custom-made power viruses (also referred to as “stressmarks,” or “powermarks”) to estimate a system’s peak power consumption [6], [7], [8]. However, the task of manually constructing a program for a speciﬁc architecture is very cumbersome and error-prone. Most importantly, the generated virus is not guaranteed to yield the maximum possible power usage. Thus, automatic approaches to the generation of effective power viruses are highly desirable. Power viruses have been explored within the realm of CPUs, main memory, and off-chip I/O. One notable absentee is the on-chip network; there is currently no methodology to identify the peak power consumption in the system’s Network-on-Chip (NoC) backbone. The NoC has become the de facto communication medium in multi-core setups, due to its modularity and scalability traits. Being an integral part of the system, the NoC provides connectivity across the chip, and it synergistically contributes to overall system performance [9]. Given the NoC’s functional/performance criticality, it is obvious that peak-power analysis cannot ignore such an elemental actor. This paper presents, for the ﬁrst time (to the best of our knowledge), a fully automated high-level methodology to generate appropriate trafﬁc patterns that cause peak power consumption within the NoC, by leveraging the intertwined and complementary roles of high network utilization and data switching activity. The proposed framework, aptly called PowerMax, can generate a peak-power “trafﬁc virus” for any network topology and any routing algorithm. The generated trafﬁc patterns are realistic – i.e., feasible for the particular topology and routing algorithm – and ensure that the resulting peak power consumption accurately reﬂects the potential of the underlying NoC conﬁguration. The proposed methodology is two-pronged; it combines the contributions of high network utilization – through the generation of appropriate trafﬁc patterns – and data switching activity, in a user-controllable manner. The interaction of these two facets is non-trivial, and it is a key feature of the problem formulation. For example, very high network utilization alone is not enough to generate peak power, because, if the data payloads happen to be “favorable” (i.e., yielding low switching activity), the NoC power consumption may be quite low. Moreover, high network utilization is often confused with low saturation throughput, i.e., highly congested networks. However, high congestion may severely affect certain NoC regions, but may leave other regions under-utilized. Hence, formulating the generic problem of peak power consumption within the NoC involves the intricate interaction of all aforementioned nuances, which is one of the key contributions of PowerMax. PowerMax can be used within multiple contexts: (1) as a stand-alone tool to test the peak power characteristics of a NoC under development; (2) as a benchmark to assess lowpower NoC architectures [10]; and (3) in conjunction with other power viruses that stress other components of the system, when evaluating system-level peak power consumption. Extensive and detailed experimental evaluations using various NoC topologies and routing algorithms validate the effecstrated to cause an average of 5.5(cid:2) higher power consumption tiveness of the proposed methodology. PowerMax is demonthan randomly selected trafﬁc patterns, or patterns that result in high saturation throughput. I I . PROB LEM FORMULAT ION The dynamic power consumption of a NoC that operates at a pre-determined clock frequency and voltage is directly proportional to (a) the capacitance of every gate-level node of the circuit, and (b) the switching activity of these nodes [11]. NoC components also consume static/leakage power, but this becomes relevant only in idle components. The goal of any power virus is to cause full utilization of all components, thereby rendering static/leakage power irrelevant, since no components are left idle to consume static/leakage power. Consequently, leakage power is ignored when trying to discover a realistic peak-power consumption scenario. A. The interplay of contention and data switching activity The peak power consumption of a NoC jointly depends on (1) the network component utilization, and (2) the data switching activity caused by the trafﬁc ﬂowing inside the NoC every cycle. A network component – e.g., the links, the buffers, the crossbar, etc. – is considered utilized, as long as it performs a useful operation in each cycle. The amount of power consumed is directly proportional to the switching activity caused by the bits of the traversing ﬂits. For example, let us assume a NoC link that transfers useful ﬂits in every clock cycle, but those ﬂits happen to have almost the same bit-level proﬁle. In such a case, even though the experienced utilization is maximized, the actual power consumption remains very low, due to minimal switching activity. The same holds for most NoC components like the router’s buffers and the multiplexers of the crossbar. The power consumption of every NoC component – considering both utilization and data switching activity – is directly related to the effect of contention and multiplexing. Whenever at least two ﬂows compete for the same resource, e.g., a NoC link through a router’s output port, they will possibly gain access to the shared resource in a time-multiplexed manner, depending on the employed arbitration policy. In this case, there is no way to predict the data switching proﬁle seen at the output of the shared resource, since the output data stream is the result of multiplexing-in-time of two (or more) ﬂows that are unrelated in terms of their data properties. Fig. 1. The process of multiplexing different data ﬂows “corrupts” the data switching proﬁle of each individual incoming data stream, and can possibly lead to very low dynamic power consumption. An example of the unpredictability of the output data stream is shown in Fig. 1, assuming two 4-bit-wide data ﬂows traveling through a multiplexer. Although the two incoming streams exhibit considerable switching activity when viewed independently of each other, the arbitrated trafﬁc that passes to the output of the multiplexer exhibits very few bits switching in every clock cycle. Additionally, in NoCs that consist of routers with equal numbers of input and output ports (i.e., the most common case in the majority of designs), contention for the same output port inevitably leaves at least another output port unutilized. This implies that at least one NoC output link, together with the input buffers at the end of said link (in the downstream router), are not used for a certain number of cycles, which directly translates to zero dynamic power. Therefore, in order to guarantee that (a) all NoC components will be fully utilized in each cycle, and (b) the data switching activity caused within each component is directly controllable by the sources of the NoC (not affected by intermediate contention/multiplexing points), we need to derive contentionfree trafﬁc patterns, which utilize all network links. Full-link utilization effectively causes full buffer and crossbar utilization in each NoC router, thus approaching, as much as possible, the realistically attainable peak power of the NoC (when driven by appropriate data). The desired trafﬁc patterns would allow the network endpoints (sources/destinations) to transmit and receive a new ﬂit every cycle (100% throughput), while controlling the data switching proﬁle of the injected trafﬁc and avoiding the unpredictability caused by multiplexing of unrelated data streams. B. Permutation trafﬁc, contention, and network utilization To identify contention-free trafﬁc patterns, we start by removing contention at the endpoints of the network, i.e., the trafﬁc injected by each source is directed to a different destination. Therefore, out of all possible trafﬁc patterns, we need to identify those permutation trafﬁc patterns where (a) trafﬁc is exchanged between unique source-destination pairs, (b) no ﬂow contention is caused inside the network, and (c) all NoC links are simultaneously fully utilized. in Fig. 2(a) for a 3(cid:2)3 2D mesh NoC, where the paths An example of such a permutation trafﬁc pattern is shown of the injected ﬂows are determined by the XY routing algorithm. The terminal nodes are depicted as circles in the ﬁgure. The numbers beneath each terminal node indicate the source/destination pair of the ﬂow originating from that Fig. 2. Three different permutation trafﬁc patterns in a 3 (cid:2) 3 2D mesh: (a) A pattern that causes maximum NoC component utilization, which yields peak power consumption (i.e., the desired pattern); (b) A pattern that leaves certain links idle, even though it achieves full injection/ejection throughput and avoids contention; (c) A pattern that merely congests certain network channels, while leaving parts of the NoC unutilized. terminal node, e.g., the “1!8” designation below node 1 indicates that the trafﬁc generated at this node goes to node 8. Each source/destination pair is unique. The selected trafﬁc pattern allows for full NoC component utilization (injection and ejection throughput can be 100%), while, at the same time, the sources can control, from outside the network, the data switching activity inside the NoC. On the contrary, the permutation trafﬁc shown in Fig. 2(b) leaves 4 links idle (links 6!3, 3!6, 5!8, and 8!5), even though it achieves full injection/ejection throughput, without contention, on the remaining links. Peak power consumption should not be confused with network congestion and worst-case throughput. The permutation trafﬁc pattern shown in Fig. 2(c) triggers the worst-case throughput in this NoC. The trafﬁc that is produced using the method in [12], maximizes the load on certain channels, in order to stress the network and highlight its worst-case throughput. However, even though the NoC is congested, many portions of it remain under-/un-utilized, and exhibit lower power consumption. For example, the 1!0 and 4!5 links are not used at all. Moreover, some of the utilized links cycle. For instance, the 0!1 link is only used with 50% are not fully utilized, i.e., they do not receive a ﬂit every throughput, because the red ﬂow going from node 0 to node 8 encounters contention in the downstream node 1, and is forced to share the 1!2 link with the dark green ﬂow going from node 1 to node 5. Since the red ﬂow only uses 50% of the bandwidth of the 1!2 link, the throughput of the 0!1 link also falls to 50%, i.e., it remains idle for 50% of the time. Hence, the approach of simply triggering worst-case NoC throughput results in both idle links, and links that are not fully utilized, due to downstream contention. Once a link is un-/under-utilized, the router components connected to the link’s endpoints (multiplexers, pipeline registers, ﬂow control logic, and input buffers) also remain idle, or under-utilized. By computing the average link utilization in the network, one can get a feeling of how much the corresponding trafﬁc patterns stress the network, and how close they can drive it to its peak power consumption. For example, the trafﬁc patterns in Figs. 2(b) and (c) produce an average link utilization of 83% and 68%, respectively, counting also the utilization of the links connecting the terminal nodes with the network. C. Number of appropriate permutation trafﬁc patterns The permutation trafﬁc patterns that yield extremely high link utilization constitutes an extremely small subset of the entire set of possible trafﬁc patterns. To gain insight as to how small this subset is, we performed two experiments. In the ﬁrst case, we produced all (cid:25) 135 (cid:2) 103 permutation trafﬁc patterns for a 9-node 3(cid:2)3 2D mesh (self trafﬁc is not allowed), we computed the utilization of each link (or, equivalently, the channel load [12]), and then the average link utilization in the network, when trafﬁc is routed under XY routing. The distribution of all permutation trafﬁc patterns, in in Fig. 3(a). For the 3(cid:2)3 2D mesh, only 648 permutation trafﬁc terms of achieved average network link utilization, is depicted patterns (0.5% of the total) can achieve full link utilization. Fig. 3. The distribution of permutation trafﬁc patterns, in terms of achieved average network link utilization. (a) All permutations for a 3(cid:2)3 2D mesh, and (b) 108 randomly generated permutation trafﬁc patterns for an 8(cid:2)8 2D mesh network. In both cases, XY routing is assumed. In the second case, we assumed a larger 64-node network, organized as an 8(cid:2)8 2D mesh. In this case, we produced 10 million randomly-generated trafﬁc patterns out of (cid:25) 1:98 (cid:2) 1087 possible ones. As shown in Fig. 3(b), the majority of the permutation trafﬁc patterns cause an average link utilization of about 50%, while the number of trafﬁc patterns that yield link utilizations above 80% (desired when trying to cause peak power consumption) is quite low (only 147 out of 10 million were found). Obviously, the random generation of trafﬁc patterns is neither efﬁcient, nor effective; out of the 10 million randomly generated trafﬁc patterns, not a single one was found to yield 100% (or even 90%) link utilization. I I I . TH E POW ERMAX M ETHODO LOGY PowerMax identiﬁes permutation trafﬁc patterns that route packets from any source to a different destination. The paths selected by PowerMax are legal with respect to the employed routing algorithm, and they utilize all network links of the network. At the same time, the injected packet ﬂows are always conﬂict-free, i.e., they never compete with each other for any network resource. A. Enhanced channel dependency graph The proposed algorithm is applied to the network’s Channel Dependency Graph (CDG). Every vertex of the CDG corresponds to a link in the network, and every edge of the CDG corresponds to an allowed network turn (moving from one link to another through a NoC router). Cycles in the CDG are a sign for routing deadlocks, where in-ﬂight packets are holding onto a set of network resources in a cyclic manner, thereby inhibiting routing progress indeﬁnitely [13]. A routing algorithm is responsible to break cyclic dependencies, while still preserving the connectivity among all network nodes. To achieve this dual objective, the routing algorithm restricts the turns that a packet can make in the network (edge removals from the CDG) which in effect transforms the CDG to a cyclefree graph [14], [15]. (a) The channel dependency graph (CDG) of a 3(cid:2)3 2D mesh, Fig. 4. with certain turns across network links removed, following the XY routing algorithm. (b) An abstract form of the enhanced CDG, which includes additional pseudo-edges across sink and source nodes, and self-loop edges connecting the source and the sink node of the same terminal. An example CDG for a 3(cid:2)3 2D mesh, derived after applying the turn restrictions of the XY routing algorithm, is shown in Fig. 4(a). The CDG in said ﬁgure includes a vertex for each link in the network (black vertices), including the links that connect network routers to the injection and ejection ports of the terminal nodes (white vertices). Another routing algorithm would have used a different set of edges in the CDG, depending on the algorithm’s properties [16]. In PowerMax, the cycle-free CDG is enhanced with additional “pseudo-edges,” which connect all the sink nodes to all the source nodes of the network. Self-loop edges are also added (local 180o turns), which connect the source and the sink node of the same terminal. An abstract enhanced CDG that includes pseudo-edges and self-loops is depicted in Fig. 4(b). The enhanced CDG is no longer cycle free, and any possible cycle should pass – by construction – through the added pseudo-edges. Our goal is to discover a Hamiltonian path in the enhanced CDG, which begins from any source node and visits each and every vertex of the enhanced CDG exactly once, before arriving at any sink node. B. Discovering a Hamiltonian path in the enhanced CDG A Hamiltonian path in the enhanced CDG starts from a source vertex, moves through an arbitrary number of vertices of the “core CDG” (see Fig. 4(b)), and at some point it reaches a sink node. A path cannot evolve indeﬁnitely in the core CDG without visiting a sink node, since the core CDG is cycle-free by construction (the turn restrictions of the routing algorithm impose this feature). Once at a sink node, the Hamiltonian path inevitably moves back into the core CDG via a source node, after using one of the available pseudo-edges (which connect all sink nodes to all source nodes). By splitting the Hamiltonian path every time a pseudoedge is encountered, PowerMax derives multiple sub-paths that connect distinct source and destination pairs. Due to the properties of a Hamiltonian path, each vertex of the CDG appears in only one of those sub-paths. Equivalently, each link of the network will be used to transfer the ﬂits of only a single ﬂow across the network, thereby allowing for conﬂict-free data transfer across a source and a sink node. Furthermore, since the Hamiltonian path covers all vertices of the CDG, it is guaranteed that all links of the network will receive actual trafﬁc, thus yielding full utilization of the NoC components. The discovery of a Hamiltonian path is an NP-complete problem, solved via path enumeration and backtracking [17]. At each recursive step, the algorithm examines all outgoing edges of an already-visited vertex. When an edge does not lead to a Hamiltonian path, the algorithm backtracks and tries another outgoing edge. In order to avoid redundant backtracks (arising from examining outgoing edges in a random order), we select to follow the implicit order imposed by the routing algorithm. The connection between link ordering and routing algorithms is described in detail in [13], [18]. For example, in a 2D mesh with XY routing, X+ turns are visited ﬁrst, followed by X-, Y+, and Y- turns. In other topologies, a different order ﬁts best. For example, in a tree or a random topology that follows up/down routing, up edges are examined before down edges. In this way, the recursive algorithm starts augmenting the Hamiltonian paths by including vertices that “match” the path selection process of the routing algorithm. Fig. 5. The Hamiltonian path for a 5-ary 1-mesh, as derived by PowerMax via pseudo-edges and self-loops. The path is then transformed into non-conﬂicting ﬂows, through the removal of self-loops and pseudo-edges. When the vertices of the core CDG are already included in a large path, only some of the source and sink nodes remain unconnected. Due to the proposed addition of pseudo-edges and self-loop edges to the CDG, the algorithm augments the path by using said edges, instead of back-tracking. As a result, the algorithm swiftly includes the remaining unconnected source and sink nodes, thereby signiﬁcantly reducing the complexity of forming a Hamiltonian path. POW ERMAX RUN - T IM E S : T IM E N EED ED TO DER IVE P EAK - POW ER PERMU TAT ION TRA FFIC PATT ERN S . TABLE I Once a Hamiltonian path is formed, it connects network links (the visited vertices of the CDG) and source and sink nodes. Of course, the connections across the vertices of the CDG include pseudo-edges and, possibly, self-loops. To illustrate this scenario, let us assume a simple 5-node line network (5-ary 1-mesh), as illustrated in Fig. 5(a). The Hamiltonian path discovered by PowerMax in the network’s enhanced CDG is shown in Fig. 5(b). A Hamiltonian path that uses the local self-loop connections can be transformed to a self-loop-free Hamiltonian path by using a simple post-processing step, which is depicted in Fig. 5(c). The self-loop is removed, and then one edge of the CDG is selected and “broken” into two segments. The incoming segment is connected to the local sink node, while the outgoing segment is connected to the local source. The Hamiltonian path remains connected via the pseudo-edges. This simple post-processing step merely implies that the Hamiltonian path has included the source and sink nodes in a different order. Once the pseudo-edges are removed, the source-to-sink ﬂows are correctly identiﬁed (Fig. 5(c)). The proposed algorithm can solve the Hamiltonian path problem for well-known NoC topologies of hundreds of nodes in just a few minutes. The reason for this efﬁciency is twofold: (1) The use of self-loops across the source/sink nodes of the same terminal node enables PowerMax to avoid repeated backtracking when encountering already-visited CDG vertices. (2) The visiting order of the edges that resembles the implicit ordering of channels imposed by the routing algorithm. Without these two properties, solving the problem for mediumscale topologies would require days, thus rendering the tool impractical for modern systems. The run-times required to derive peak-power permutation trafﬁc patterns for various topologies and various network sizes are shown in Table I, using a Linux computer with a 2.3 GHz Intel Core i7-4712HQ Processor and 16 GBs of RAM. C. Applying PowerMax to various NoC topologies Max, when applied to the CDG of the 3(cid:2)3 2D mesh network Fig. 6(a) depicts the resulting sub-paths identiﬁed by Powerof Fig. 4(a), and after removing the pseudo-edges of the identiﬁed Hamiltonian path. The generated sub-paths include every vertex of the original CDG and only a subset of the edges of the CDG. In this way, it is ensured that all network links are utilized, by using nine distinct and non-conﬂicting trafﬁc ﬂows, as depicted for clarity in Fig. 6(b). PowerMax can be applied to any topology and any routing algorithm. Fig. 7(a) depicts the PowerMax-derived nonconﬂicting permutation trafﬁc that causes 100% link utilization in an asymmetric 2D mesh network (the asymmetry is the result of faulty router 3, which is decommissioned). In this case, the turning restrictions of the routing algorithm [19] (depicted as small arrows at certain turn-points within the network) guarantee connectivity and deadlock freedom. Equivalently, Fig. 6. (a) The sub-paths that lead to unique source/sink pairs, as identiﬁed by PowerMax for the CDG of Fig. 4(a). (b) The corresponding ﬂows depicted on the 3(cid:2)3 2D mesh. the peak power trafﬁc for a tree that applies the up/down routing algorithm is highlighted in Fig. 7(b). Fig. 7. (a) PowerMax applied to irregular networks, such as an asymmetric 2D mesh. (b) Mapped ﬂows (as generated by PowerMax) on a tree topology. Ring and tori topologies employ Virtual Channels (VCs) to ensure freedom from possible routing deadlocks. The CDG of VC-based networks is more complex: each vertex (i.e., link) appears multiple times within the CDG; as many times as the number of supported VCs per network link. In these cases, cycles within the CDG are prohibited at the VC level by appropriate turn restrictions (edge removals on the CDG) [20], [21]. Consequently, the resulting routing algorithm is deadlock-free, through the appropriate use of VCs. PowerMax requires only one additional feature to handle the case of VC-enhanced CDGs. The equivalent vertices that correspond to the VCs of the same physical link are treated as a hyper-vertex, which is used only once in the Hamiltonian path. In other words, only one VC is used per physical channel of the network to carry the generated peak-power trafﬁc. The derived trafﬁc ﬂows are allowed to change VC in-ﬂight, as long as this is dictated by the routing algorithm; in any other case, each trafﬁc ﬂow remains within the same VC. In this way, the algorithm exercises all possible turns at the VC level, but, ultimately, it selects only one VC-to-VC connection. PowerMax does not impose any speciﬁc rule for acquiring a VC, other than the ones imposed by the routing algorithm. Figure 8 illustrates a peak-power trafﬁc scenario for a hierarchical ring. It should be noted that the example of Fig. 8, and the run-times of Table I for the ring, hierarchical ring, and 2D torus, correspond to the application of PowerMax on the enhanced CDG with VCs. IV. MAX IM I Z ING TH E DATA SW I TCH ING AC T IV I TY The data patterns that cause the exact maximum power consumption can only be derived using speciﬁc gate-level greatest common divisor of K and B is equal to one. When B is odd, the 2-vector data pattern that also maximizes the power on the links is the proper choice. When B is even, we can select a repetitive data pattern of B + 1 words. The B + 1 words can safely include B=2 repetitions of the 2-vector data pattern 0101 : : : 01 ! 1010 : : : 10 ! 0101 : : : 01 ! : : : ! 1010 : : : 10, plus an all-zero vector. Depending on the NoC conﬁguration, the repetitive set of K data patterns can also extend across different packets, as long as the ﬂits of the packets ﬂow consecutively in the NoC. For PowerMax, using such data patterns guarantees maximum switching activity, since the trafﬁc is non-conﬂicting and each VC buffer accepts a new ﬂit every cycle. This also holds for the wires of the links, the input pins of each register of the input buffers, the internal crossbar wires, the crossbar’s multiplexers, and the output pipeline registers. Even if those data patterns have the potential of also maximizing the coupling capacitances of the internal logic of the routers, this cannot be ascertained, since the amount of coupling across two circuit nodes depends on the exact layout and placement of the internal wires, which may change across different designs. In terms of power, the trafﬁc injected can stay within the same VC from source to destination, as long as one ﬂit is written and read per cycle, and the data values written and read have the maximum bit-wise difference. Distributing trafﬁc across VCs for each non-conﬂicting ﬂow produced by PowerMax is possible, but it needlessly complicates the derivation of the appropriate data switching patterns that cause the maximum switching activity, without any true impact of the triggered power consumption. Even though the non-conﬂicting nature of the trafﬁc patterns generated by PowerMax can maximize the switching activity in the datapath of the NoC (links, buffers, crossbar), the arbitration part is kept working on the same requests and grants in each cycle. This causes minimum switching activity in this portion of the NoC. However, this is not a problem in NoCs with wide datapaths of 128 bits or more, where the power of the arbitration logic is low relative to the datapath portion. This argument is also veriﬁed by the experimental results that use random trafﬁc; the latter maximizes the switching activity in the arbitration part, due to the random nature of the input requests. Even when compared with this trafﬁc scenario, PowerMax achieves signiﬁcantly higher power consumption by only appropriately targeting the data switching activity. V. EX P ER IM EN TAL EVALUAT ION The goal of PowerMax is to trigger the peak-power consumption of a NoC by injecting appropriately selected trafﬁc patterns that maximize network component utilization and data switching activity. Therefore, the measured power is realistic, in the sense that it can be caused by a real trafﬁc scenario, even if it is rare under normal system operation. PowerMax is evaluated on 64-node NoCs following 2D mesh and hierarchical ring topologies. Other tested topologies show similar trends. In order to contain the number of possible conﬁgurations, we assume a tile-based chip ﬂoor-plan similar to the Scorpio chip [24]. Scorpio was built at 45 nm our implementations), using a tile size of approximately 2(cid:2)2 technology (which matches the technology library we used in mm. Based on the chosen NoC topology, the NoC routers can Fig. 8. Peak-power trafﬁc patterns derived by PowerMax for a hierarchical ring, which employs virtual channels for deadlock freedom. techniques [22], [23], which can be applied only in certain sub-modules of the NoC and cannot be extended to the entirenetwork level. Therefore, to tackle the problem of identifying the worst-case data patterns, we need to rely on the most common micro-architectural features of the majority of NoC designs. Subsequently, and in conjunction with the guaranteed full-network utilization achieved by the non-conﬂicting permutation trafﬁc patterns, we can approach (as much as possible) the peak power consumption of the NoC as a whole. For the links, a repetitive data pattern that switches between 0101 : : : 01 ! 1010 : : : 10 is enough to trigger worst-case power consumption. Each bit experiences a change in every cycle, either 0!1 or 1!0, which switches the corresponding capacitance of the wire to ground. Further, this data pattern ensures that neighboring wires always switch in the opposite direction, thereby causing the worst-case power consumption, due to the link’s coupling capacitance. However, for the VC buffers and the internal logic of the router, we cannot be sure of the exact switching activity caused by this 2-data vector pattern. Assume, for example, the case of VC buffers that are built using register-based (i.e., ﬂipﬂop-based) FIFO queues, or using SRAM blocks. In either case, power is consumed every time a new ﬂit is written to, or read from, the VC buffers. PowerMax satisﬁes the goal of always keeping the VC buffers active (reading/writing), since it allows all NoC links to be fully utilized every cycle. Link utilization translates to a new ﬂit being read (dequeued) from a VC buffer, and moving to an output link that is connected to another VC buffer (in the downstream router), which accepts (enqueues) the new ﬂit. On each write, a new ﬂit is written to only one VC. Inside the queue of each VC, the ﬂit is written into the register that corresponds to the address pointed to by the tail pointer of the enabled VC queue. Therefore, on each write, only the bits of one register can change value. The rest are not enabled, or remain clock-gated, to save power in the clock pins of the ﬂip-ﬂops. Therefore, to maximize power, we need to guarantee that (a) the new value written to the register is different from the one already stored, and (b) the two values (the old and the new one) differ by as many bits as possible. This can only occur if we know beforehand the speciﬁc slot of the VC queue into which the incoming ﬂit will be stored. When a repetitive data pattern of K words is placed – one word after the other – in a buffer with B slots, then we can guarantee that any incoming word will be written (stored) into a register that already stores a different value, as long as the have a variable number of input and output ports. For every conﬁguration, we assume that the NoC supports 4 VCs per input port, with 5 buffers/VC, and the NoC routers employ the 3-stage pipelined organization of Scorpio routers [24]. The inter-router NoC links carry 64 bits of data, plus some extra ﬂow control information. The header ﬂit, which also includes network-addressing information, carries fewer actual data bits. All NoC components used in the evaluation were implemented in SystemVerilog, mapped to a commercial low-power 45 nm 0.8 V standard-cell library, and placed-and-routed using the Cadence EDI ﬂow. Depending on the NoC topology, a different placement-and-routing round was conducted. Power was measured after performing timing-accurate simulations, when operating the NoC at 1 GHz and including all backannotated layout parasitics. Fig. 9. A 10K-cycle snapshot of the instantaneous power consumption of a 64-node 2D mesh (top) and a hierarchical ring (bottom), after the network reaches steady-state operation, using uniform-random trafﬁc and data. In the ﬁrst set of experiments, PowerMax is evaluated against random synthetic trafﬁc patterns, under various data switching and network-injection scenarios. The instantaneous power consumed by a NoC when the incoming trafﬁc causes contention across ﬂows with unrelated data (this occurs in almost all cases under normal operation) can vary signiﬁcantly over time, depending on the switching activity in various parts in Fig. 9, for an 8(cid:2)8 2D mesh and a 64-node two-level of the network in each cycle. This behaviour is highlighted hierarchical ring that consists of 8-node local rings connected via an 8-node global ring. Both networks receive uniform-random trafﬁc at a different rate (close to their saturation throughput), as reported in Fig. 9. The two NoCs have equal link width, i.e., 64 bits plus ﬂow-control bits, and both operate at 1 GHz. Therefore, the bisection bandwidth of the 2D mesh is larger than the bisection bandwidth of the hierarchical ring. The injected packets are 5ﬂit long and carry random data in their payload portion. In this experiment, the bit of each ﬂit when entering the network has equal probability of being 0 or 1, independent of the rest of the bits of the same ﬂit, or the previous ﬂits. The peak power consumption achieved by random trafﬁc is merely the peak instantaneous power observed during the simulation’s time frame. There is no guarantee that a large power value can be triggered during simulation, due to the unpredictability in switching activity, and the lower NoC utilization caused by contention among different ﬂows. Additionally, the observed peak power consumption simply represents an instantaneous peak. This cannot be sustained Fig. 10. A 10K-cycle snapshot of the instantaneous power consumption of a 64-node 2D mesh and a hierarchical ring (under steady-state network operation), using PowerMax-derived trafﬁc/data with full injection throughput. over a longer period of time, which would be required to observe possible temperature increases and identify thermal hot-spots in the system. On the contrary, PowerMax does not have such limitations. In Fig. 10, we report the instantaneous power consumed by PowerMax under 100% injection load for each case (2D mesh and hierarchical ring). The results are measured by injecting 5-ﬂit packets in the NoC, following the PowerMax-derived permutation trafﬁc patterns and carrying data payloads with the 2-vector data patterns described in Section IV. PowerMax keeps power consumption constantly and consistently very high. The minimal variance in the power consumption is due to the switching proﬁle of the header and ﬂow-control bits, which are not controlled by PowerMax. Fig. 11. Peak power consumption vs. injection load. The power consumption triggered by PowerMax-derived trafﬁc compared against the power consumed when using uniform-random and bit-complement trafﬁc patterns. Next, we compare the peak power consumption of PowerMax and random trafﬁc scenarios (uniform-random and bit-complement trafﬁc), under the same injection load. For each injection load, the maximum instantaneous power consumption value observed (over 500,000 cycles of simulation) was recorded. The results are depicted in Fig. 11, for the same 2D mesh and hierarchical ring topologies. The peak power consumption of random trafﬁc (blue curves) follows the throughput behavior of the network itself, and, after saturation (when the utilization of NoC components reaches its limit), the peak power consumption observed is rather constant. On the contrary, PowerMax can increase the power consumption to its true maximum value, due to its non-conﬂicting trafﬁc. The data switching activity is directly controllable by the input sources, and it covers all the intermediate router ports and network links that are utilized by the injected ﬂow. When compared against uniform-random trafﬁc (Figs. 11(a) and (c)), PowerMax triggers maximum power consumption, which can be more than 6(cid:2) higher than the one achieved under uniformrandom trafﬁc with random data (i.e., blue curves). This is also true when the NoC is driven by uniform-random trafﬁc that allows contention in the network, but the injected data patterns are the same as the ones used with PowerMax (by following the guidelines described in Section IV). This scenario is also depicted in Figs. 11(a) and (c) with the red curves. PowerMax still consumes signiﬁcantly more power (4(cid:2) higher), since it simultaneously takes into account both the network utilization and the data switching activity. Similar conclusions are derived when the power consumption of the NoC is triggered using other permutation trafﬁc patterns, such as bit-complement trafﬁc. In this case (Figs. 11(b) and (d)), the peak power consumption of PowerMax is 4(cid:2) larger than the largest power observed under the bit-complement trafﬁc patterns (red curves). P EAK POW ER O F POW ERMAX V S . TH E POW ER O F A FAK E SC ENAR IO , WH ICH A S SUM E S THAT EVERY C IRCU I T NOD E SW I TCH E S IN EV ERY CYC LE . TABLE II Finally, we compare the power triggered by PowerMax, versus the peak-power consumption that corresponds to the fake scenario of every circuit node switching in every cycle. In both cases depicted in Table II, the peak power triggered by PowerMax (measured at 100% injection rate) is lower than the one derived using the fake (un-realistic) approach. The difference between these two maximum power values depends on topology characteristics, and the power expended on the links vs. the power expended within the routers. In any case, the signiﬁcant conclusion out of this comparison is that fake peak-power scenarios overestimate the true maximum power proﬁle of the NoC and unnecessarily increase the overall system power budget. With PowerMax, worst-case power analysis is brought closer to what is realistically attainable. V I . CONC LU S ION S & FU TUR E WORK As chips become increasingly more dense and complex, power consumption becomes a primary design constraint. It is imperative for designers to realistically estimate a design’s peak power consumption, which directly impacts other salient system attributes, such as performance, implementation costs, battery life, and reliability. This paper introduces PowerMax, the ﬁrst fully-automated high-level methodology to generate appropriate trafﬁc and data patterns that cause peak power consumption within the NoC. The peak power consumption triggered by PowerMax is, on average, 5.5(cid:2) higher (up to 8(cid:2) higher) than what is observed after simulating random trafﬁc and data patterns. PowerMax guarantees full utilization of every NoC component. Therefore, even if some NoC components are not identical – in terms of their (micro-)architecture – they are still fully utilized, irrespective of their differentiated design parameters. For example, PowerMax imposes full (100%) utilization on every link of the network, irrespective of the link’s length. Both short and long wires are equally utilized; this attribute translates into longer wires consuming more power than shorter wires. Differentiated design parameters – e.g., the link length, the presence (or not) of pipelined links, the number of VCs per input port, and the number of buffer slots per VC – can all be handled by PowerMax, since every component (irrespective of its size) is utilized in every cycle. However, in the case of completely heterogeneous NoCs, where the inter-router link bandwidths can vary (i.e., there is a combined effect of link width and clock frequency), marginal extensions to the problem formulation and the corresponding proposed algorithm are required. Our future work will focus on extending PowerMax to cover such NoC architectures, including those that distribute the NoC components across multiple voltage/frequency domains. "
Average shortest path length of graphs of diameter 3.,"A network topology with low average shortest path length (ASPL) provides efficient data transmission while the number of nodes and the number of links incident to each node are often limited due to physical constraints. In this paper, we consider the construction of low ASPL graphs under these constraints by using stochastic local search (SLS) algorithms. Since the ASPL cannot be calculated efficiently, the ASPL is not suitable for the evaluation function of SLS algorithms. We first derive an equality and bounds for the ASPL of graphs of diameter 3. On the basis of the simplest upper bound of the ASPL, we propose to use 3Δ + 2□ as the evaluation function for graphs of diameter 3 where Δ and □ denote the number of triangles and squares in a graph, respectively. We show that the proposed evaluation function can be evaluated in O(1) time as the number of nodes and the maximum degree tend to infinity by using some data tables. By using the simulated annealing with the proposed evaluation function, we construct low ASPL regular graphs of diameter 3 with 10 000 nodes.","Average Shortest Path Length of Graphs of Diameter 3 Nobutaka Shimizu Ryuhei Mori Dept. of Mathematical Informatics, Graduate School of Information Science and Technology, The University of Tokyo Email: nobutaka shimizu@mist.i.u-tokyo.ac.jp Dept. of Mathematical and Computing Sciences, School of Computing, Tokyo Institute of Technology Email: mori@c.titech.ac.jp Abstract—A network topology with low average shortest path length (ASPL) provides efﬁcient data transmission while the number of nodes and the number of links incident to each node are often limited due to physical constraints. In this paper, we consider the construction of low ASPL graphs under these constraints by using stochastic local search (SLS) algorithms. Since the ASPL cannot be calculated efﬁciently, the ASPL is not suitable for the evaluation function of SLS algorithms. We ﬁrst derive an equality and bounds for the ASPL of graphs of diameter 3. On the basis of the simplest upper bound of the ASPL, we propose to use 3△ + 22 as the evaluation function for graphs of diameter 3 where △ and 2 denote the number of triangles and squares in a graph, respectively. We show that the proposed evaluation function can be evaluated in O(1) time as the number of nodes and the maximum degree tend to inﬁnity by using some data tables. By using the simulated annealing with the proposed evaluation function, we construct low ASPL regular graphs of diameter 3 with 10 000 nodes. Index Terms—Network topology, graph diameter, average shortest path length, order/degree problem, simulated annealing. I . INTRODUCT ION The network topology is signiﬁcant on the performance of the interconnection network, and thus various kinds of topologies have been proposed and analyzed in many areas such as Datacenter network, High Performance Computing (HPC), Peer-to-peer system, Network-on-Chip (NoC), and so on [1], [2], [3], [4], [5], [6], [7]. In these areas, topologies with small diameter and low average shortest path length (ASPL) are desired. For example, the Hypercube topologies and the de Bruijn graphs are known to be effective for NoC architectures [5] and peer-to-peer networks [1], [3], respectively. Some topologies based on randomness have been proposed and explored in the area of Datacenter network and HPC [2], [6], [7]. The problem of ﬁnding the maximum graph for given maximum degree and diameter is called the degree/diameter problem, and has been studied in graph theory [8]. However, in practice, the order (the number of nodes) is often limited due to several reasons. Hence, for many applications, we are given order and maximum degrees, and try to minimize the diameter and the ASPL. This problem is called the order/degree problem, and has not been studied sufﬁciently. Recently, the 978-1-5090-0172-9/16/$31.00 c⃝2016 IEEE importance of the order/degree problem is pointed out [9]. Some of the authors of [9] and their coworkers held a competition called “Graph Golf ” on the order/degree problem [10]. The stochastic local search (SLS) is a framework of approximation algorithms for general optimization problems. In this paper, we consider the construction of low ASPL graphs of given order and given maximum degrees by using SLS algorithms. In a SLS algorithm, an initial feasible solution is generated (possibly be a random solution). Then, the solution is iteratively replaced by one of its neighborhoods. For our problem, we use a simple local modiﬁcation procedure called switch for deﬁning the neighborhoods. In this paper, we assume that SLS algorithms use the evaluation function which represents a quality of feasible solutions [11]. Since huge number of feasible solutions must be evaluated in SLS algorithms in general, efﬁcient evaluation algorithms are strongly desired. However, the ASPL cannot be calculated efﬁciently at least in our knowledge, and hence is not suitable for the evaluation function. In this paper, we assume that for given n and d, the dregular random graph of order n has the diameter 3 with high probability, and propose the evaluation function for graphs of diameter 3 which can be evaluated efﬁciently. More precisely, ﬁrst, we derive an equality and bounds for the ASPL of graphs of diameter 3. On the basis of the simplest upper bound of the ASPL, we propose to use 3△+22 as the evaluation function of SLS algorithms where △ and 2 denote the number of triangles and squares in a graph, respectively. The proposed evaluation function can be evaluated in O(1) time as n and d tend to inﬁnity by using some data tables. By using the proposed evaluation function in the iterative ﬁrst improvement (IFI) and the simulated annealing (SA), we construct low ASPL regular graphs of diameter 3 with 10 000 nodes, which are the best graphs in the competition Graph Golf. The remainder of this paper is organized as follows. In Section II, notions and notations used in this paper are introduced. In Section III, the equality and bounds for the ASPL of graphs of diameter 3 are shown. In Section IV, we propose the new evaluation function, and show an efﬁcient algorithm for calculating the new evaluation function using data tables. In Section V, we show results of numerical experiments. Section VI concludes this paper. I I . PRE L IM INAR I E S A. Notations and deﬁnitions A graph G is a pair of two ﬁnite sets V and E where every element of E is a subset of V of size 2. Each element of V is called a node. Each element of E is called an edge. Let eij := fi; j g for fi; j g 2 E . The number n of nodes is called the order of G. For nodes u; v 2 V , u is said to be connected to v if there is an edge fu; vg 2 E . The degree dv of a node v is the number of nodes connected to v . A graph is d-regular if the degrees of all the nodes are equal to d. ′ ′ ′ ′ is a For two graphs G = (V ; E ) and G ′ (cid:18) V and E ′ (cid:18) E . If G subgraph of G if V ′ is a subgraph ′ . We say G is isomorphic to G ′ of G, we say G contains G if there is a bijection f : V ! V ′ that satisﬁes the following condition: ), G = (V ; E 8fu; vg (cid:18) V ; fu; vg 2 E () ff (u); f (v)g 2 E ′ : For two graphs G and H , the number of H in G is the number of subgraphs of G that are isomorphic to H . For two nodes s and t, a s-t path or a path is a graph ℓ(cid:0)1∪ P = (V ; E ) where V = fs = v0 ; v1 ; : : : ; vℓ = tg; ffvk ; vk+1gg E = k=0 for v0 ; v1 ; : : : ; vℓ which are distinct nodes. Here, ℓ is called the length of s-t path P . If P = (V ; E ) is a s-t path of length ℓ (cid:21) 2, a graph C = (V ; E [ ffs; tgg) is called a cycle. The length of the cycle C is ℓ + 1. We call a cycle of length 3 and 4 a triangle and a square, respectively. Deﬁnition 1. For k = 0; 1; : : : a k-multiple triangle is a graph Tri(k) = (V (k) ; E (k) ) where fi; j g; V (k(cid:0)1) [ fvk g; V (k) := ffi; j gg; k = 0 k > 0 Such a k-multiple triangle is called a k-multiple triangle sharing i; j when we specify the two nodes i and j . Deﬁnition 2. For k = 0; 1; : : : a k-multiple square is a graph V (k) := Squ(k) = (V (k) ; E (k) ) where fi; j; v0g; V (k(cid:0)1) [ fvk g; ffi; v0g; fv0 ; j gg; E (k(cid:0)1) [ ffi; vk g; fvk ; j gg; E (k) := k = 0 k > 0 k = 0 k > 0: Such a k-multiple square is called a k-multiple square sharing i; j when we specify the two nodes i and j . Fig. 1 shows k-multiple triangles and k-multiple squares sharing i; j for k = 0; 1; 2. Note that the 1-multiple triangle and the 1-multiple square are equivalent to the triangle and the square, respectively. The number of k-multiple triangles and Fig. 1. Some examples of k-multiple triangles and k-multiple squares. Fig. 2. An image of switch for eab and ecd . the number of k-multiple squares in a graph are denoted by △(k) and 2(k) , respectively. Let △ := △(1) and 2 := 2(1) . For a graph G = (V ; E ) and s; t 2 V , a s-t path whose length is minimum among all s-t paths contained in G is called the shortest path between s and t. The length of the shortest path between s and t is called the distance between s and t, denoted by dist(s; t). If G contains no s-t paths, we deﬁne dist(s; t) = 1. The diameter of G is deﬁned by maxfi;jg(cid:18)V ;i ̸=j dist(i; j ). ASPL(G) := 2 n(n (cid:0) 1) dist(i; j ): fi;jg(cid:18)V ; i ̸=j In this paper, we consider the following graph optimization problem: For given n and d, minimize: subject to: jV (G)j = n ASPL(G) dv = d; 8v 2 V (G): Here, V (G) denotes the node set of G. Note that the set of feasible solutions of this problem is the set of all d-regular graphs of order n. B. Stochastic local search for our problem In this paper, we consider SLS algorithms described below. First, an initial d-regular graph G of order n is chosen randomly. Then, G is iteratively replaced by a local modiﬁcation procedure called switch. In the switch procedure, a E (k) := E (k(cid:0)1) [ ffi; vk g; fvk ; j gg; k = 0 k > 0: Deﬁnition 3. For a graph G = (V ; E ), the average shortest path length of G is deﬁned by ∑ { { { { G pair of edges (eab ; ecd ) is chosen and replaced by (eac ; ebd ) ′ is not d-regular, or (ead ; ebc ) as in Fig. 2. If the new graph G i.e., at least one of the edges introduced by the switch already exists in G, a new pair of edges in G is chosen for the switch ′ is found. Here, the d-regular graph until a d-regular graph G ′ , which can be obtained by a single switch of G, is called a neighborhood of G. Once a neighborhood G ′ is found, G ′ is evaluated by the evaluation function. According to the value of ′ , it is determined whether or not the evaluation function on G ′ . This updating procedure the current graph G is replaced by G continues until further improvement is not expected. It is natural to deﬁne the evaluation function by the ASPL ′ . In order to calculate the ASPL of a d-regular graph of of G order n, the distances of all node pairs have to be calculated, which takes O(n2d) time by using the breadth ﬁrst search. Since it is not sufﬁciently efﬁcient, low ASPL graphs cannot be obtained within a reasonable time for large n and d. In this paper, we consider an optimization of d-regular graphs of diameter 3. The Moore bound implies that any graph of diameter 2 satisﬁes n (cid:20) d2 + 1 and any graph of diameter 3 satisﬁes n (cid:20) d3 (cid:0) d2 + d + 1 [8]. Furthermore, almost all n (cid:21) d2 and n is close to d2 , then a random d-regular graph of graphs of diameter 2 satisfy n < d2 [8], [12]. Therefore, if order n is expected to have diameter 3 with high probability. For such n and d, if we choose a random d-regular graph of order n as the initial graph of a SLS algorithm, the diameter of every graph appearing in the SLS algorithm is empirically 3. By assuming that the diameter of graphs found by the SLS algorithm is always 3, the ASPL can be calculated in O(d) time1 , which is still not sufﬁciently efﬁcient for large n. In this paper, we give an upper bound of the ASPL of graphs of diameter 3, and propose SLS algorithms which uses the upper bound as the evaluation function. It will be shown that the upper bound can be calculated in O(1) time. I I I . BOUND S FOR TH E ASPL O F GRA PH S O F D IAM E TER 3 In this section, an equality and bounds for the ASPL of graphs of diameter 3 are shown. They are stated not only for d-regular graphs but also for general graphs. Deﬁnition 4. For a graph G = (V ; E ), let V2 := ffi; j g : i 2 V ; j 2 V ; i ̸= j g. For fi; j g 2 V2 , Wij := (V nfi; j g) [ f0g: For fi; j g 2 V2 and k 2 Wij , { k = 0 k ̸= 0: S i;j k ffi; j g : fi; j g 2 E g; := ffi; j g : fi; kg; fk ; j g 2 E g; For m = 1; 2; : : : ; n (cid:0) 1, ∑ ∑ T (m) := fi;jg2V2 K(cid:18)Wij jK j=m (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) If there is a square consisting of four nodes a, b, c and d and of edge set feab ; ebc ; ecd ; edag, it holds fa; cg 2 S a;c b can be divided into two steps: the removal of eab ; ecd and the addition of eac ; ebd . First, we consider the removal of eab and ecd . Since no triangles contain both of eab and ecd , the number of triangles decreases by T2[a][b] + T2[c][d]: (11) Next, we consider the addition of eac and ebd to the graph obtained by the removal of eab and ecd . Note that the arrays T1, T2 and T3 remain unchanged at this time. After the removal of eab and ecd , the addition of eac increases the number of triangles by T2[a][c] (cid:0) T1[a][d] (cid:0) T1[b][c]: (12) Here, the terms T1[a][d] and T1[b][c] take account of nonexistent paths a-d-c and a-b-c, respectively. Similarly, the addition of ebd increases the number of triangles by T2[b][d] (cid:0) T1[a][d] (cid:0) T1[b][c]: (13) Hence, from (11), (12) and (13), one obtains △′ (cid:0) △ = (cid:0) T2[a][b] (cid:0) T2[c][d] + T2[a][c] + T2[b][d] (cid:0) 2(T1[a][d] + T1[b][c]): Next, we consider 2′ (cid:0) 2. Since the number of squares that contain both of eab and ecd is T1[a][d]T1[b][c] (Recall that eac and ebd are not in E (G)), the removal of eab and ecd decreases the number of squares by T3[a][b] + T3[c][d] (cid:0) T1[a][d]T1[b][c]: (14) After the removal of eab and ecd , the addition of eac increases the number of squares by T3[a][c] (cid:0) T2[a][d] (cid:0) T2[b][c]: (15) Here, the terms T2[a][d] and T2[b][c] take account of nonexistent paths a-(cid:3)-d-c and a-b-(cid:3)-c, respectively, where (cid:3) represents an arbitrary node (A non-existent path a-b-d-c is not counted since ebd =2 E (G)). Similarly, after the addition of eac , the addition of ebd increases the number of squares by (16) Here, the term T1[a][d]T1[b][c] takes account of the path b-c′ . Hence, from (14), (15) and (16), one obtains a-d in G T3[b][d] (cid:0) T2[a][d] (cid:0) T2[b][c] + T1[a][d]T1[b][c]: 2′ (cid:0) 2 = (cid:0) T3[a][b] (cid:0) T3[c][d] + T3[a][c] + T3[b][d] (cid:0) 2(T2[a][d] + T2[b][c] (cid:0) T1[a][d]T1[b][c]): ) (cid:0) g(G) in O(1) time We conclude that we can calculate g(G if T1, T2 and T3 for G are given. C. Array update When eab is removed, T2 can be updated by decrementing T2[t][s] and T2[s][t] for all s-t paths of length 2 using eab . Since the number of these paths is O(d), this calculation can be done in O(d) time. Also T3 can be updated by decrementing T3[s][t] and T3[t][s] for all s-t paths of length 3 using eab . The number of these paths is O(d2 ) and thus this calculation can be done in O(d2 ) time. When edges are added, the arrays can be updated in O(d2 ) time in the same way. ′ THE R ELAT IV E ASPL ERROR S O F A P PROX IMAT ION S . TABLE I (n; d) (4096; 60) (4096; 64) (10 000; 60) (10 000; 64) (cid:0)0:1206 t = 1 (cid:0)0:1544 (cid:0)0:0209 (cid:0)0:0270 t = 2 0:0355 0:0523 0:0024 0:0036 (cid:0)0:0074 t = 3 (cid:0)0:0124 (cid:0)0:0002 (cid:0)0:0003 V. NUM ER ICA L EX P ER IM EN T S A. Accuracy of approximations In this section, we show by numerical experiments that the upper bounds and lower bounds obtained by Theorem 8 are also good approximations for the ASPL. Table I shows the relative error (^ASPL(G) (cid:0) ASPL(G))=ASPL(G) of the approximations where ^ASPL(G) denotes the bounds obtained by Theorem 8 for t = 1; 2; 3, and where G denotes a random d-regular graph of order n where (n; d) = (4096; 60); (4096; 64); (10 000; 60); (10 000; 64). The diameters of these graphs are 3. Table I implies that the approximations are more accurate as t increases. Furthermore, Table I also implies that the approximations are more accurate for sparse graphs. It can be easily understood since △; 2; △(2) ; 2(2) ; : : : are small when a graph is sparse. B. Construction by Iterative First Improvement The Iterative First Improvement (IFI) algorithm is one of the simplest SLS algorithms. The IFI algorithm evaluates neighborhoods one after another. When the IFI ﬁnds the neighborhood whose value of the evaluation function is lower than that of the current solution, it replaces the current solution by the neighborhood and then starts to evaluate the next neighborhood. The evaluation and replacement continues until the IFI algorithm ﬁnds the local optimum. We consider the IFI algorithm with the O(1)-time evaluation function proposed in Section IV. In order to improve the efﬁciency of the IFI algorithm, we modify the IFI algorithm as follows: At every 50 replacements, the set of edges are sorted by the number of triangles and squares that contain the edge. More precisely, all edges are sorted in descending order of 3△e + 22e where △e and 2e denote the number of triangles and squares that contain the edge e, respectively. After the sort, all pairs of edges are chosen in the order of (e1 ; e2 ); (e1 ; e3 ); : : : ; (e1 ; ejE j ); (e2 ; e3 ); (e2 ; e4 ); : : : . This modiﬁcation is useful for ﬁnding efﬁciently a neighborhood improving the value of the evaluation function. Table II shows the ASPL, the Moore bound (7), denoted by L, and the ASPL gap (ASPL(G) (cid:0) L)=L for random d-regular graph G of order n. From these random graphs, we applied the above modiﬁed IFI algorithm, and obtained local optimals. Table III shows the ASPL, the ASPL gap for the local optimals and the required time for the modiﬁed IFI. These numerical experiments are performed on MacBook Pro, Intel Core i7 2.6GHz. The modiﬁed IFI found a local optimal within two days. The obtained local optimals have TABLE II ASPL O F RANDOM GRA PH S TABLE IV R E SU LT S O F TH E SA (n; d) (4096; 60) (4096; 64) (10 000; 60) (10 000; 64) ASPL Moore 2.3951 2.1062 2.3464 1.9841 2.6901 2.6340 2.6557 2.5840 ASPL gap 13:72 (cid:2) 10 18:26 (cid:2) 10 21:31 (cid:2) 10 27:76 (cid:2) 10 (cid:0)2 (cid:0)2 (cid:0)3 (cid:0)3 TABLE III R E SU LT S O F TH E MOD I FIED IF I (n; d) (4096; 60) (4096; 64) (10 000; 60) (10 000; 64) ASPL 2.3055 2.2536 2.6521 2.6120 ASPL gap 9:461 (cid:2) 10 13:58 (cid:2) 10 6:886 (cid:2) 10 10:85 (cid:2) 10 (cid:0)2 (cid:0)2 (cid:0)3 (cid:0)3 Time (minutes) 341 336 2429 2222 smaller ASPL gap than the original random graphs. Especially, for (n; d) = (10 000; 60); (10 000; 64), the ASPL gaps are signiﬁcantly improved. C. Construction by Simulated Annealing The Simulated Annealing (SA) is one of the most successful SLS algorithms. The SA in our numerical experiments works as follows. At each step, a neighborhood is chosen uniformly ′ of evaluation function at random and evaluated. If the value E for the neighborhood is lower than that E for the current solution, then the current solution is replaced by the neighborhood. Also, even if the neighborhood has the higher evaluation value ′ than the current solution, i.e., E > E , the current graph is also replaced by the neighborhood with probability P where ′ (cid:0) E { } P = exp : (cid:0) E T Here, T is a parameter called the temperature which decreases at each step of SA. The annealing schedule, which determines the temperature at each step, is crucial to the SA. In the numerical experiments, we set T (k) := 11= ln(k + 1) where k denotes the number of graphs which are evaluated until that time. Table IV shows the ASPL of graphs constructed by applying the SA 60 days, and then applying the modiﬁed IFI. They are the best graphs obtained in the Graph Golf [10]. V I . CONC LU S ION We derived the equality and bounds for the ASPL of graphs of diameter 3 by using the number of triangles, squares, and some other structures in a graph. By numerical experiments, we conﬁrmed that the upper and lower bounds obtained are also accurate approximation for the ASPL. On the basis of one of our bounds, we propose to use 3△ + 22 as the evaluation function of SLS for the ASPL minimization problem where n is at least and close to d2 , so that the random graph has diameter 3 with high probability. We show that the proposed SLS algorithms requires O(1) time for the evaluation and O(d2 ) time for the update. We construct low ASPL graphs of diameter 3 by IFI and SA using our evaluation function. While the upper bound evaluated by the proposed algorithm is represented only by the number of triangles and squares, the Graph (10 000; 60) (10 000; 64) ASPL 2.6502 2.6099 ASPL gap 6:2 (cid:2) 10 10:0 (cid:2) 10 (cid:0)3 (cid:0)3 other bounds represented by the number of more complicated structures such as △(2) ; 2(2) are conﬁrmed to be more accurate. Thus, if there exists O(1)-time evaluation algorithm that calculates one of these bounds, then we can use the bound as the evaluation function of SLS. It would construct graphs with lower ASPL. Finally, we note that our bounds of ASPL for graphs of diameter 3 can be generalized to larger diameter in a similar way. The construction of low ASPL graphs of larger diameter by similar algorithm is an interesting future work. ACKNOW LEDGM EN T This work was supported by MEXT KAKENHI Grant Number 24106008. "
An area-efficient TDM NoC supporting reconfiguration for mode changes.,"This paper presents an area-efficient time-division-multiplexing (TDM) network-on-chip (NoC) intended for use in a multicore platform for hard real-time systems. In such a platform, a mode change at the application level requires the tear-down and set-up of some virtual circuits without affecting the virtual circuits that persist across the mode change. Our NoC supports such reconfiguration in a very efficient way, using the same resources that are used for transmission of regular data. We evaluate the presented NoC in terms of worst-case reconfiguration time, hardware cost, and maximum operating frequency. The results show that the hardware cost for an FPGA implementation of our architecture is a factor of 2.2 to 3.9 times smaller than other NoCs with reconfiguration functionalities, and that the worst-case time for a reconfiguration is shorter or comparable to those NoCs.","An Area-Efﬁcient TDM NoC Supporting Reconﬁguration for Mode Changes Rasmus Bo Sørensen, Luca Pezzarossa, and Jens Sparsø Department of Applied Mathematics and Computer Science Technical University of Denmark, Kgs. Lyngby Email: [rboso, lpez, jspa]@dtu.dk Abstract—This paper presents an area-efﬁcient time-divisionmultiplexing (TDM) network-on-chip (NoC) intended for use in a multicore platform for hard real-time systems. In such a platform, a mode change at the application level requires the tear-down and set-up of some virtual circuits without affecting the virtual circuits that persist across the mode change. Our NoC supports such reconﬁguration in a very efﬁcient way, using the same resources that are used for transmission of regular data. We evaluate the presented NoC in terms of worst-case reconﬁguration time, hardware cost, and maximum operating frequency. The results show that the hardware cost for an FPGA implementation of our architecture is a factor of 2.2 to 3.9 times smaller than other NoCs with reconﬁguration functionalities, and that the worst-case time for a reconﬁguration is shorter or comparable to those NoCs. I . IN TRODUC T ION Packet-switched networks-on-chips (NoCs) have become the preferred paradigm for interconnecting the many cores (processors, hardware accelerators, etc.) found in complex application-speciﬁc multi-processor systems-on-chip [1], [2]. Hard real-time multicore applications rely on the guaranteedservice (GS) of the NoC, in terms of bandwidth and latency for end-to-end virtual circuits, in order to guarantee correct timing behavior. This class of applications often has multiple modes of operations that they switch between during normal operation. A mode change consists of a change of a subset of the executing software tasks and it can be triggered as part of the normal operation of the system or in response to external events [3, p.340]. To support applications that change between modes, the NoC must be able to reconﬁgure the GS connections during run-time, since a single conﬁguration of the time-predictable NoC may not support all modes of operation for a given real-time application. This paper proposes and evaluates a ﬂexible and resourceefﬁcient NoC for use in the hard real-time domain. The proposed NoC extends the existing NI of the Argo NoC [4] to support mode changes. This extension is the main contribution of the paper, and a key feature of the proposed NoC. Our NoC implements virtual end-to-end circuits using static scheduling and time-division multiplexing (TDM). The Æthereal family of NoCs [5], [6] provides similar functionality at a much higher hardware cost. The key idea of the Argo NI [4] is that the DMA controllers that drive the data transfers are integrated with the TDM 978-1-4673-9030-9/16/$31.00 c(cid:13)2016 IEEE scheduling in the NIs. This architecture avoids both physical virtual channel buffers in the NIs and credit-based ﬂow control among the NIs that are found in most other NoC designs [6]– [8]. Since these resources must be reconﬁgured as part of a mode change [5], [6], the NI architecture of [4] can be extended to support mode changes using little extra hardware as we show in this paper. This paper is organized in seven sections. Section II presents related work. Section III presents the overall functionality of the network interface (NI) of the presented NoC and Section IV describes its hardware architecture. Section V presents the controller that manages the reconﬁguration process. Section VI evaluates the presented architecture. Section VII concludes the paper. I I . R E LATED WORK A multicore platform for hard real-time systems has to provide time-predictable inter-processor communication. In the following we present some TDM NoCs that offer GS connections and support run-time reconﬁguration of the GS connections. The Æthereal family of NoCs [5], [6] uses TDM and static scheduling to provide GS. The original Æthereal NoC supports both GS and BE trafﬁc. The scheduling and routing tables controlling the GS trafﬁc are distributed into the NIs and routers. The data structures that are written into these tables are distributed using BE trafﬁc. The use of BE trafﬁc for reconﬁguration may compromise the time-predictability. Observing the high cost of distributed routing and combined support for BE and GS trafﬁc, the aelite NoC [5] supports only GS trafﬁc and source routing. Reconﬁguration is performed using GS connections (reserved for this purpose only) from a reconﬁguration master. Essentially, our NoC implements similar functionality using fewer hardware resources. The dAElite NoC, focusing on multicast, re-introduces distributed routing and adds a dedicated network with a tree topology to handle reconﬁguration. The Argo NoC [9] supports GS trafﬁc and uses TDM with source routing like the aelite NoC, but Argo does not support the reconﬁguration needed to do a mode change. In the Argo NoC, the TDM schedule drives the transmission of data between scratchpad memories (SPM) attached to the processors in each node of the platform. The Argo network interface (NI) contains a direct memory access (DMA) table, where the local processor can setup DMA transfers to remote SPMs. The TDM schedule activates a certain DMA entry when it is allowed to send a packet. The Argo NoC uses TDM schedules generated by the off-line scheduler presented in [10]. We assume that each mode consists of a set of communicating tasks assigned to processors, and that each mode has an associated bandwidth graph, from which the scheduler can generate a schedule that provides the GS requirements of each virtual circuit for each mode of operation. I I I . N E TWORK IN TER FAC E FUNC T IONA L I TY This section describes the key ideas of the reconﬁguration feature and outlines the NI architecture, shown in Fig. 1. In our resource-efﬁcient NoC, we decided to use the existing resources to transfer reconﬁguration information. This implies that virtual circuits dedicated for transmission of reconﬁguration information must be set up alongside the virtual circuits that are used for transmission of regular data; for example, a reconﬁguration channel from a master core to each of the other cores in the platform. As mentioned earlier, reconﬁguration of a NoC typically requires accessing and modifying some state in the NoC, as well as ﬂushing the virtual circuits that are torn down and performing some initialization of virtual circuits that are set up. Our NoC is a source routed NoC with simple routers without any buffers, ﬂow control or arbitration. For this reason, reconﬁguration involves only the NIs. Moreover, the end-to-end transmission of packets for which incoming packets are written directly into the local SPM of the destination node, in combination with the way the scheduler maps virtual circuits to time slots in the TDM schedule, means that the network of routers is conceptually drained from packets at the end of each TDM period. This opens for the very interesting perspective of instantaneously switching from one TDM schedule to another in a way that is fully transparent to virtual circuits that persist across the reconﬁguration. The last TDM period of one schedule can be followed immediately by the ﬁrst TDM period of a new schedule, and the virtual circuits that persist across a mode change can be mapped to different paths and different timeslots in the TDM schedule, without any interference on the data ﬂows. This again avoids the fragmentation of resources seen in the previously published solutions [5], [6] in which no changes can be made to virtual circuits that persist across a mode change. The architecture of the NI is split into the transmit pipeline and the receive pipeline. The two pipelines interact through the SPM arbiter, the maximum rate of both pipelines is one 64-bit transaction every two cycles. Therefore, the arbiter can safely prioritize read requests from the transmit pipeline, with just a single register in the receive pipeline interface. In the transmit pipeline, the TDM controller reads out scheduling information of the current time slot from the schedule table. The output from the schedule table activates the packet manager that assembles the packet header, updates the ﬁelds in the DMA table, reads the payload data from the local SPM, and sends the packet. An important extension of our NI compared to Argo, is that incoming packets can be written to the internal tables of the NI, which allows a remote core to change the settings of the NI. We have extended the packet format to 3 types of packets, data packets, conﬁguration packets, and interrupt packets. In the receive pipeline the RX unit handles the incoming packets according to their type. IV. HARDWAR E ARCH I T EC TUR E The packet format of the three packet types that are used in our NI, consists of a 32-bit header followed by 2n 32-bit payloads. Fig. 1 shows our NI architecture. The transmit pipeline contains the TDM controller, the schedule table, the DMA table, the packet manager, and the reconﬁguration controller. The reconﬁguration controller is described in Section V. At boot time, each core initializes its schedule table and conﬁguration table in the local NI. When all NIs are initialized, the master core signals all TDM counters to start counting synchronously, through a global reset signal. The TDM controller indexes into the schedule table. The T2N (time-to-next) ﬁeld contains the number of cycles until the TDM controller increments the index. The Pkt. len. ﬁeld is the number of 64-bit payloads following the packet header. The Route ﬁeld contains the route the scheduled packet must follow through the NoC. The DMA idx. ﬁeld is the DMA table index of the associated DMA transfer. The DMA table contains the DMA entries that the processor uses to transfer data through the NoC. The processor and the conﬁguration unit can set the Active bit, to indicate that the DMA is active and the packet manager can clear the Active bit when the last data is sent. The transmit pipeline logic reads the active bit to determine whether a packet should be sent. The Word cnt. ﬁeld is the number of 64-bit words remaining of the active DMA transfer. The Read ptr. points to the next local 64-bit word address of the DMA transfer. The Write ptr. is the write address for the next packet header. The Pkt. type is the packet type for the DMA transfer. When a packet is sent, Word cnt. is decremented and Read ptr. and Write ptr. are incremented all by the packet length. The receive pipeline, shown in Fig. 1, contains the RX unit and the interrupt request (IRQ) FIFO. According to the packet type, the RX unit writes the packet payload to either the SPM, the internal tables of the NI, or the IRQ FIFO. The IRQ FIFO unit contains two queues that store the interrupt IDs of the interrupts. A local interrupt is invoked by the NI when the last data packet of a DMA transfer arrives at the destination to indicate the completion of a data block transfer. A remote in interrupt is invoked when an interrupt packet arrives at the NI. V. R ECON FIGURAT ION CONTROL LER To support reconﬁguration, we add a reconﬁguration controller to the NI and connect the RX unit to the conﬁguration bus, as shown in in Fig. 1. The latter allows the RX unit to write incoming conﬁguration packets into all the tables connected to the conﬁguration bus. The schedule table may hold multiple schedules at the same time, each spanning a range of entries. Fig. 1. A block diagram of our NI. The block diagram is split in two parts: the transmit pipeline and the receive pipeline. Each range is represented by a pair of pointers, high and low, stored in a small table in the reconﬁguration controller. A reconﬁguration simply requires that the TDM counter is set to the start entry of the new schedule when the TDM counter reaches the end of the current schedule. A master processor invokes a reconﬁguration of the NoC by sending a conﬁguration packet to the reconﬁguration controllers of all slave processor NIs, announcing that they must switch to the new schedule. This packet contains two parameters: the index of the reconﬁguration table entry that holds the high and low pointers for the new schedule and the ID of the TDM period after which the new schedule should start. The ID of the TDM period is deﬁned in the reconﬁguration controllers in all the NIs, but due to pipelining of routers, packets sent during a TDM period arrive in a time window that is phase-shifted by some clock cycles. Our scheduler minimizes this phase shift: virtual circuits that cross many routers are not scheduled towards the end of a TDM period. With this constraint, the phase shift is determined by the pipeline depth through the routers on the shortest path between two NIs. In our design, the shortest path is through two routers and, using 3-stage pipelined routers, this results in a phase-shift of 6 clock cycles. If a master processor issues a reconﬁguration command in TDM period i, then the NI in the master node sends conﬁguration packets to all the nodes during the next TDM period, i + 1. This means that some conﬁguration packets may be received during TDM period i + 2 and therefore the new schedule can start after TDM period i + 2 has ﬁnished. V I . EVALUAT ION This section evaluates the proposed architecture in terms of worst-case reconﬁguration time (WCRT), hardware cost, and maximum operating frequency of the NI. For the WCRT evaluation, we use a 4-by-4 platform with a bi-torus network and 3-stage pipelined routers. The WCRT of a new schedule Cnew depends on the currently executing schedule Ccurr . The worst-case analysis of software depends on the processor that executes the software. Therefore, we consider the software overhead of setting up DMA transfers outside the scope of this paper. The preferred situation is that the Cnew is already loaded in the schedule table of the slave NIs. It can be resident or pre-loaded in the background. In this case the WCRT is only three times the TDM period of Ccurr , as explained in Section V. For the benchmarks presented in Table I, the WCRT is between 135 and 381 clock cycles, depending on the current benchmark. In case that the schedule Cnew is not already loaded in the schedule table of the slave NIs, we need to add the worstcase latency of transferring Cnew to the slave NIs. For this we assume that Cnew is loaded in the processor local SPM of the reconﬁguration master. The worst-case latency of transferring Cnew to the slave NIs is the maximum of the individual worstcase latency for each slave NI. We calculate the worst-case reconﬁguration time between the schedules of the MCSL benchmark suite [11] and an All2all schedule to any other of these. The results are shown in Table I. For space reasons, we leave out the column for H264-1080p, as the results are identical to H264-720p. We see that the WCRT in Table I is between 549 and 2298 clock cycles. The Sparse benchmark, as Ccurr , results in the lowest WCRT, as Sparse has the shortest TDM period, and thus the highest bandwidth to the slaves. The worst-case transfer time of Cnew dominates the WCRT. The time interval required by Æthereal and dAElite to set up a single virtual circuits is 246 and 60 clock cycles, respectively [6]. Moreover, these NoCs also need to tear down unnecessary virtual circuits. Assuming that any two modes differ by more than a handful of virtual circuits, the reconﬁguration time of our NoC is in general comparable or shorter than the one of Æthereal and dAElite. TABLE I TH E WOR S T-CA SE R ECON FIGURAT ION T IM E , INCLUD ING SCHEDUL E TRAN S F ER . S INC E TH E WOR S T-CA S E R ECON FIGURAT ION T IM E DE PEND S ON TH E CURR EN T SCH EDU LE AND TH E N EW ONE , W E SHOW A MATR IX O F TH E COMB INAT ION O F CURR ENT AND N EW SCH EDU LE S . Cnew Ccurr 1024 Fpppp RS-dec RS-enc H264FFT720p Robot Sparse All2all FFT-1024 – 1436 1169 1080 1077 1074 991 1611 Fpppp 1627 – 1244 1149 1152 1149 677 1722 RS-dec 1593 1497 – 1125 1128 1125 1032 1680 RS-enc 1491 1401 1140 – 1056 1059 966 1572 H264-720p 1590 1494 1221 1128 – 1122 1029 1689 H264-1080p 1590 1494 1221 1128 1131 1122 1029 1689 Robot 2165 2041 1660 1539 1536 – Sparse 777 684 594 552 All2all 1539 1452 1176 1095 1406 2298 549 1086 822 – 549 1092 – 1002 TABLE II A HARDWARE R E SOURC E AND MAX IMUM O PERAT ING FR EQU ENCY COM PAR I SON O F ON E ROUT ER AND ON E N I B ETW EEN THE PR E SEN TED ARCH I TEC TUR E AND THR E E S IM I LAR D E S IGN S . aelite 8 1 1375 1916 3861 0 119 Slots Conn Slices LUTs FFs BRAM fmax (MHz) 3-port router dAElite Our imp 8 8 1 2 774 350 2506 1279 3081 1074 0 0 122 124 5-port router Argo Our imp 256 256 64 64 289 338 1119 1267 871 924 4 4 146 161 In case the new schedule needs to be transmitted to the slave NIs, our approach is still comparable. The maximum WCRT shown in Table I is 2298 clock cycles. For our NoC, this transition represents the transmission and the reconﬁguration of 255 virtual circuits. In this time interval, Æthereal and dAElite can only set-up 9 and 38 virtual circuits, respectively. We compare the hardware cost and the maximum operating frequency of the presented design against the TDM-based NoCs aelite and dAElite [6] as well as the original Argo NoC [9]. Table II shows the synthesis results of the four designs for one router and one NI and the supported number of TDM slots and connections per node. The published numbers we compare with are all a 2-by-2 mesh topologies implemented on the Xilinx Virtex-6 FPGA architecture. We divided the numbers by four to get the hardware consumption of one 3-ported router and one NI. The results in Table II for a 3-port router show that overall our implementation is smaller than the other NoCs against which we compare and has a similar maximum operating frequency fmax . In terms of slices, our implementation is a factor of 3.9 times smaller than aelite and a factor of 2.2 times smaller than dAElite. Table II also presents ﬁgures for a network node with a 5-ported router and a reasonable number of TDM slots and connections for a larger platform. Comparing these results the Argo NoC we can see that our extended NoC is only around 17% larger than the original Argo NoC. The results in Table II for the maximum operating frequency fmax show that the maximum frequency of our implementation is comparable to the ones of aelite and dAElite for a 3-port router. For a 5-port router, our implementation is around 30% faster than the 3-port router, since it uses block RAM instead of distributed memory (FFs), and around 10% faster than Argo. V I I . CONC LU S ION This paper presented a resource-efﬁcient NoC that supports reconﬁguration for mode changes. The NoC extends the existing NI of the Argo NoC and provides guaranteed-service communication between processors. An implementation of the proposed architecture is evaluated in terms of worst-case reconﬁguration time, hardware cost, and maximum operating frequency. The results show that our NoC is between 2.2 and 3.9 times smaller than NoCs with similar functionality and that the worst-case reconﬁguration time is comparable or shorter to those NoCs. Acknowledgment The work presented in this paper was funded by the Danish Council for Independent Research | Technology and Production Sciences under the project RTEMP, contract no. 12-127600, (http://rtemp.compute.dtu.dk). "
Flow-centric computing leveraged by photonic circuit switching for the post-moore era.,"Artificial Intelligence (AI)-based big data analysis is emerging for broad application fields, including manufacturing, autonomous car, health care, agriculture, and so on. Data centers are under severe pressure to meet ever-increasing demands on clouds. On the other hand, the amount of data processing is bounded by the total power budget, where the reasonable number is up to 20 MW. Therefore, the energy efficiency becomes a critical issue both now and the future. A combination of special-purpose processing units communicating through huge bandwidth optical circuit switched network allows to dramatically improve the energy efficiency of big data processing. Based on this idea, we propose flow-centric computing, a software-defined data center architecture focusing on data flow processing. Compute, storage, and network resources are disaggregated and dynamically composes slices, i.e., data processing environments, based on workload-specific demands. We are conducting a feasibility study of the concept and developing system software technologies.","Flow-centric Computing Leveraged by Photonic Circuit Switching for the Post-Moore Era (Invited Paper) Ryousei Takano Information Technology Research Institute, National Institute of Advanced Industrial Science and Technology Tsukuba, Ibaraki 305-8560, Japan Email: takano-ryousei@aist.go.jp Tomohiro Kudoh Information Technology Center, University of Tokyo Bunkyo-ku, Tokyo 113-8658, Japan Email: kudoh@nc.u-tokyo.ac.jp Abstract—Artiﬁcial Intelligence (AI)-based big data analysis is emerging for broad application ﬁelds, including manufacturing, autonomous car, health care, agriculture, and so on. Data centers are under severe pressure to meet ever-increasing demands on clouds. On the other hand, the amount of data processing is bounded by the total power budget, where the reasonable number is up to 20 MW. Therefore, the energy efﬁciency becomes a critical issue both now and the future. A combination of specialpurpose processing units communicating through huge bandwidth optical circuit switched network allows to dramatically improve the energy efﬁciency of big data processing. Based on this idea, we propose ﬂow-centric computing, a software-deﬁned data center architecture focusing on data ﬂow processing. Compute, storage, and network resources are disaggregated and dynamically composes slices, i.e., data processing environments, based on workload-speciﬁc demands. We are conducting a feasibility study of the concept and developing system software technologies. Keywords—Optical Circuit Switched Network, Data center, Operating System IN TRODUC T ION I . Big data processing and extreme-scale computing in the post-Moore era call for revolutionary technologies that drastically improve the energy efﬁciency of data storage and processing. AIST launched the Initiative for Most Power efﬁcient Ultra Large Scale data Exploration (IMPULSE) Program (FY2013-2015) to realize novel low-power devices (nonvolatile memory, 3D build-up logic, optical network) based on new materials and principles [1]. The scope of IMPULSE also includes the creation of future computer architectures that take full advantage of the unprecedented performance increases that integration of these innovative devices will bring. Figure 1 shows the overview of IMPULSE program and a proposed data center architecture, where 3D stacked packages are connected through an optical network. From the viewpoint of system software research, the following research opportunities can be considered: 1) data center-scale resource management, that is, data center OS or a software-deﬁned data center, 2) integration of photonic circuit switching (optical circuit switched network) into data center networks, 3) non-volatile memory, and 4) heterogeneous programming framework. This paper focuses on the ﬁrst two topics. 978-1-5090-0172-9/15/$31.00 c(cid:13)2015 IEEE Fig. 1: The Overview of AIST IMPULSE Program I I . DATA C EN T ER IN TH E PO ST-MOOR E ERA CMOS scaling is ending, and the “free lunch is over” in the post-Moore era. As a result, improvement of single CPU core performance and memory density have stopped. Moreover, both high performance computing systems and hyper-scale data centers are facing power wall problem. For example, the performance development of supercomputers has recently ﬂatten as shown in the TOP 500 list [2]. A top-class supercomputer is approaching to exascale, and the peak power consumption almost reaches 20 MW. Improvement of energy efﬁciency is a critical issue because we cannot dramatically increase this power budget. Thus, to keep a sustainable performance scaling, a clean slate architecture is required. The key of more than Moore is diversiﬁcation [3]. Multiple kinds of task type-speciﬁc processing units are available, for example, GPU, FPGA, IBM TrueNorth, Google Tensor Processing Unit, and quantum annealing machine. Integration and utilization of heterogeneous task-speciﬁc processing units in a system are the keys. However, data movement between processing units is the bottleneck to performance due to memory wall, interconnect wall and as well as power wall. It means communication is relatively slower than computation in current computers. The bytes per FLOP ratio is getting smaller generation by generation. Therefore, data afﬁnity that processing data at the place data are is important for performance. Revolutionary, high-speed communication will change this scenario. If the data movement cost is dramatically reduced (i.e., data can be moved in a short time with low energy consumption), data can be aggressively moved to processing units appropriate for performing the required task, as shown in Figure 2. We call such a new computing paradigm as function afﬁnity processing in contrast with data afﬁnity processing. In other words, our approach is combining a set of ﬁne-grained task speciﬁc processing units in a pipeline manner, instead of a general purpose processing unit with large memory. Such a pipeline requires huge bandwidth between processing units. We anticipate that this bandwidth demand will be met by the potential of optical network technologies such as interconnect over Dense Wavelength Division Multiplexing (DWDM) and direct optical I/O connection to memory [5], [7]. Using DWDM, the interconnect bandwidth will reach 10 Tbps per ﬁber, e.g., 100 Gbps x 100 lambdas, by 2030. Although heat and cost of DWDM light sources have been barriers in introducing DWDM in data centers, a Wavelength Bank (WB) or optical comb source, which is a centralized generator of wavelengths for DWDM, is promising to overcome it. The wavelengths are distributed to computing nodes through optical ampliﬁers, thus each computing nodes are able to eliminate light sources. The distributed light is de-multiplexed into individual wavelengths, modulated, multiplexed again, and transmitted from each computing node. Inoue, et al., have proposed to use a WB with a silicon photonics modulator [8]. dynamically compose resources as a slice based on data ﬂowspeciﬁc demands. A slice is a “virtual” server, that is, logically partitioned set of resources, including processing units and storage class memory connected through an optical circuit switched network. This allows the user to enjoy performancepredictable streaming data processing on the slice. Intel Rack-Scale Architecture (RSA) [6] takes a similar approach and introduces the ability to pool resources for more efﬁcient utilization and operational efﬁciency. On the other hand, ﬂow centric computing focus on achieving guaranteed performance of big data processing by utilizing heterogeneous task-speciﬁc processing units based on workload-speciﬁc demands. Fig. 3: Flow-centric Computing Fig. 2: Paradigm Change of Computing in the Post-Moore Era: From Data Afﬁnity Processing to Function Afﬁnity Processing Fig. 4: Disaggregated Data Center I I I . F LOW-C EN TR IC COM PU T ING Based on the idea of function afﬁnity processing, we have proposed ﬂow-centric computing, a software-deﬁned data center architecture focusing on data ﬂows, as shown in Figure 3 [4]. A ﬂow-centric computing platform runs on top of a disaggregated data center, as shown in Figure 4. A current data center consists of a set of commodity servers connected through an electronics packet network, and the total performance scales out by increasing the number of servers. In the near future such systems should reach a barrier in terms of scalability due to the low utilization of resources and the complexity of system. However, the proposed system disaggregates compute, storage, and network resources and provides the ability to Figure 5 shows a schematic diagram of a computing node. The processor cores are provided with local memory and connected through a high bandwidth network-on-chip. The proposed architecture allows direct access to main memory from a remote node via DWDM interconnect. Main memory is divided into memory blocks, and each memory block can be accessed either from the processor or the I/O at a time. Multiple memory blocks can be sent and received simultaneously using multiple wavelengths. IV. F LOW OS : A DATA CEN TER R E SOURC E MANAG EM EN T SY S TEM An optical circuit switched network enables us to communicate with both huge bandwidth and very low power V. CONCLUD ING R EMARK S AND FU TUR E WORK S We have proposed ﬂow-centric computing, that is a new software-deﬁned data center architecture leveraged by optical circuit switched network. It is a solution of sustainable computing infrastructure in the post-Moore era. We plan to demonstrate the feasibility with a proof-of-concept pilot system. The hardware system consists of two types of node elements: an energy efﬁcient GPU computing node and an FPGA-based switching node. Flow OS dynamically composes a slice combining the two type nodes based on workloadspeciﬁc demands. Existing OpenCL programs [9] are seamlessly running on the slice without any modiﬁcation. ACKNOW L EDGM ENT S The authors gratefully acknowledge fruitful discussion with H. Amano, H. Matsutani (Keio University), M. Koibuchi (NII), S. Namiki, and K. Ishii (AIST). This work was partly supported by the IMPULSE project of AIST and JSPS KAKENHI Grant Number 16670626. "
Populating and exploring the design space of wavelength-routed optical network-on-chip topologies by leveraging the add-drop filtering primitive.,"Emerging technologies often carry different logic primitives for which contemporary logic synthesis techniques are not suitable. For this reason, the design space of circuit-level solutions relying on such technologies is often largely unexplored. One domain where this trend is evident consists of topologies for wavelength-routed optical networks-on-chip (WRONoCs). Current literature only reports isolated design points that are inspired only by the intuition of researchers: there are currently no methodologies that enable designers to populate the design space in a consistent way. As a result, the possibility of an automated synthesis flow guiding the designer to the most promising solution in the design space is far from coming. This paper aims at bridging the former gap by identifying the basic primitive which is at the core of each topology. Then, a methodology is consistently derived to combine basic primitives together, thus potentially yielding all points of the topology design space. To our knowledge, this is the first time the design space of wavelength-routed topologies is populated in a potentially exhaustive way through a systematic methodology. As a result, it becomes evident that for a specified quality metric, there exist better solutions than the topologies that have been found out so far by designers' intuition. This paper is thus the stepping stone for future work targeting automatic synthesis of WRONoC topologies.","Populating and Exploring the Design Space of Wavelength-Routed Optical Network-on-Chip Topologies by Leveraging the Add-Drop Filtering Primitive Mahdi Tala, Marco Castellari, Marco Balboni and Davide Bertozzi MPSoC Research Group email: {mahdi.tala, marco.castellari,marco.balboni, davide.bertozzi}@unife.it Engineering Department, University of Ferrara, ITALY Abstract—Emerging technologies often carry different logic primitives for which contemporary logic synthesis techniques are not suitable. For this reason, the design space of circuitlevel solutions relying on such technologies is often largely unexplored. One domain where this trend is evident consists of topologies for wavelength-routed optical networks-on-chip (WRONoCs). Current literature only reports isolated design points that are inspired only by the intuition of researchers: there are currently no methodologies that enable designers to populate the design space in a consistent way. As a result, the possibility of an automated synthesis ﬂow guiding the designer to the most promising solution in the design space is far from coming. This paper aims at bridging the former gap by identifying the basic primitive which is at the core of each topology. Then, a methodology is consistently derived to combine basic primitives together, thus potentially yielding all points of the topology design space. To our knowledge, this is the ﬁrst time the design space of wavelength-routed topologies is populated in a potentially exhaustive way through a systematic methodology. As a result, it becomes evident that for a speciﬁed quality metric, there exist better solutions than the topologies that have been found out so far by designers’ intuition. This paper is thus the stepping stone for future work targeting automatic synthesis of WRONoC topologies. I . IN TRODUC T ION Despite the networking paradigm for chip-level connectivity is currently mainstream, it comes with its own concerns, which future system scalability trends will increasingly bring to the forefront. In particular, the multi-hop nature of networkson-chip (NoCs) makes their performance latency-dominated and distance-sensitive. Moreover, interconnect energy does not keep up with the scalability trend of compute energy [18]. Last but not least, the extension of intra-chip communications to off-chip ones is not seamless. One promising solution consists of networking communication actors by means of an emerging interconnect technology. Among the possible candidates, silicon photonics stands out as the most promising one, due to the intrinsic capability of light to transport information over large distances at very high data rates and low latency with minor dynamic power dissipation. Photonic integration of multi- and many-core architectures has been extensively studied in literature, envisioning singlestage topologies from buses [1]–[4] to crossbars [1], [5]–[7], and different multistage topologies from quasi-butterﬂies [8]– [10] to tori [11]–[13]. Most proposals use different routing algorithms, ﬂow control mechanisms, optical wavelength organizations, and physical layouts. Despite the intensive research effort, most literature works assess speciﬁc architecture design points. This typically characterizes the early stage of an emerging technology, when the primary objective consists of making compelling cases for its utilization, so to foster the evolution from a research concept to a viable interconnect technology. However, at a later stage, another objective becomes equally important: bridging the gap between technology developers and system designers, who need to do design with the new devices. Only the development of proper design methodologies and synthesis tool-ﬂows can properly tackle this challenge, which needs to be anticipated by the exploration of multiple implementation variants [21], [23], and by their understanding in the context of a uniﬁed design framework [24]. That is, researchers need to familiarize with the design space, thus laying the groundwork for pruning strategies of such design space capable of returning the best solution that meets the input requirements. Optical networks-on-chip (ONoCs) are still lagging far behind in this respect, since the open literature reports a number of looselyrelated architecture design points that originate directly from researchers’ intuition. This is clearly unacceptable to develop an interconnect technology with practical relevance. A noticeable example is provided by wavelength-routed ONoCs (WRONoCs). Unlike other optical bus, crossbar and (space-routed) network solutions, they do not require any form of arbitration, but deliver contention-free global connectivity by construction. Although they make an extensive use of laser sources, WRONoCs are attractive whenever applications demand for performance guarantees, or to predictably ofﬂoad trafﬁc from an electronic NoC, or to convey time-critical control messages in hybrid interconnection schemes. Design of WRONoC topologies is subject to unique design constraints, due to the need to share wavelength channels across initiators as much as possible, while at the same time avoiding the interference of channels tuned to the same wavelength. So far, only few connectivity patterns have been proposed in literature, including λ-router [25], GWOR [15], Snake [14] or Ring [16] topologies. However, the key questions that are not currently getting any answer are whether these design points are the only feasible ones in the design space, whether other solutions do exist, and how they look like. One main issue that currently limits the exploration of synthesis methods to map a logical bus or network to nanophotonic devices consists of mixing up descriptive information of proposed design solutions at different abstraction layers. This artiﬁcially limits the design space, does not help others understand the design, prevents direct comparison of related proposals, and the application of well-known synthesis techniques and/or interconnection network optimization techniques. As far as WORONoC topologies are concerned, drawing their connectivity patterns on a 2D plane yields very clean interconnection schemes, without any waveguide 978-1-4673-9030-9/16/$31.00 c(cid:13)2016 IEEE crossings outside photonic switching elements (PSEs), and with apparent symmetrical properties. However, such schemes are drawn under naive assumptions on to the placement of initiators and targets. Therefore, when such topologies undergo placement and routing subject to the ﬂoorplanning constraints of the system at hand, lots of inter-PSE waveguide crossings materialize, and the symmetrical structure may be impaired [14]. Therefore, it is becoming urgent to clearly separate the logic speciﬁcation from physical views, and to search for predictable design solutions from the ground up. In this respect, a non-symmetrical logic connectivity pattern does not necessarily map to an inefﬁcient physical implementation: it depends on the space budget on the chip layout. The clean separation of abstraction layers and associated topology views is the milestone on top of which this paper builds its synthesis methodology for WRONoC topologies. The basic intuition of the paper is that each topology logic scheme can be seen as a different combination of basic primitives, which we identify in the add-drop ﬁltering functions. Once the underlying abstraction has been identiﬁed, the paper deﬁnes the basic steps that lead from a generic connectivity speciﬁcation to a topology design point that meets the requirement. In this way, the paper is able for the ﬁrst time to populate the design space of WRONoC topologies built out of 2x2 PSEs, and to characterize the essential features of design solutions. Of course, the well-known WRONoC topologies from literature can be easily derived with the proposed methodology as well. Finally, the paper demonstrates that the discovered design points may feature different design predictability gaps when undergoing place&route, and that a number of solutions do exist that provide higher energy efﬁciency than known topologies. This paper is a key enabler for future work on the synthesis toolﬂow for WRONoC topologies, capable of pruning the design space toward the most promising design points that meet preassigned input requirements. I I . R ELAT ED WORK S Emerging technologies are revitalizing the attention on design methodologies and tool-ﬂows that today are challenged to handle both new digital designs and new models. Furthermore, novel logic abstractions and synthesis techniques are essential to unlock the true value and to fully utilize the expressive power of the candidate emerging technology, otherwise unseen by plain synthesis methodologies. Thus, logic synthesis, in addition to be a technology supporter, turns into a technology enabler, too. While this process is already booming considering the new digital nanotechnologies [27], for ONoCs, this discipline is still in its formative stage, although there are relevant indicators that this is becoming a vital research ﬁeld. In this respect, focusing on the back end, there are a lot of contributions in literature that tackle different design issues coming to physical aware designs, but still missing to address the overall picture. Relevant examples include the analysis and modeling of crosstalk noise, signal-to-noise ratio and biterror rate of optical routers and networks [30], temperature control and stabilization of silicon photonic devices [28] and of the photonic die as a whole [29], accounting for the layout constraints in a 3D architecture arising from the existence of interfaces between electronic and photonics [22]. At the same time, the back end analysis is completed by emerging tools that are investigating and optimizing systems technologies and evaluating novel optoelectronic components, providing also simulation frameworks, for the analysis and optimization of integrated photonic waveguides and optical ﬁbers. In this category we can mention VPIphotonic [31], Lumerical [32] and PhoeniX [34]. Finally, Lumerical and Mentor Graphics, along with PhoeniX Software, have collaborated to develop a complete design ﬂow for silicon photonics [35], but all these approaches only allow to generate physical designs once the circuit topology has been speciﬁed. Moving to the front end synthesis ﬂow, in [39]–[41] routing algorithms are presented but without the placement of PSEs and laser power consumptions. The authors in [38] presented a placement and routing algorithm but did not consider the laser power consumption of the system. [42] conducted a synthesis of 3D hybrid nanophotonic-electric NoC but without addressing the routing problem. Finally, PROTON [20] implemented layouts of Filter-Based WR-ONoC Topologies receiving as input a path-oriented description of the topology and providing the corresponding layout by implementing several objective functions. Another framework named VANDAL [33] provides a semi-automatic assistance for placing silicon photonic devices. The above works do not unfortunately cover the issue of synthesizing architectural design points starting from abstract connectivity requirements. Bridging this gap is fundamental to trigger the industrial uptake of silicon photonic technology, following the lesson learnt with electronic networks-on-chip [36], [37]. So far, just few works tackle this challenge. In [17], authors examine how a scalable and fully connected ONoC topology can be reduced to ﬁt speciﬁc connectivity requirements thus reducing the number of required wavelengths, laser sources, photodetectors and optical switches as well as the length of the longest optical path, Although pointing out interesting ways to customize a topology, the work does not present an exhaustive exploration of the design space. In [16], instead, authors propose an optical Ring design, showing that leveraging spatial and wavelength division multiplexing it is possible to reuse the number of wavelength channels between different waveguides, optimize losses, and ﬁnally reduce the overall laser power. In this case the study is limited to the Ring topology, without addressing ﬁlter-based topologies as a target. In this work we bring a disruptive contribution to the synthesis of ﬁlter-based WRONoC topologies by identifying the basic primitive which is behind all the topology design points and by populating the design space through the combination of such primitive. As result, we bring to the forefront the existence of topologies that are different from the few design points known today in literature and the deﬁnition of either better logic properties or more efﬁcient physical implementations with respect to them. I I I . WAV EL ENGTH -S EL EC T IVE ROUT ING A. Design constraints Wavelength-routed optical NoCs (WRONoCs) rely on the principle of wavelength-selective routing, which associates a wavelength channel to each source-destination pair. In particular, master M 1 uses n wavelengths λ1 to λn to reach slaves S 1 to Sn respectively. However, instead of allocating an additional set of wavelengths for the communications of master M 2 to all the slaves, the initial set of n wavelengths is reused across masters, as illustrated in Fig.1. This wavelength reuse requirement gives rise to two design constraints for Fig. 1. Wavelength-selective routing principle in a 4x4 ONoC. WRONoC topologies, which take on the burden to make such reuse feasible: • Optical channels originating from different masters and tuned to the same carrier wavelength should never overlap in topology waveguides. In fact, this would give rise to a signal conﬂict which would impair the information encoded in each channel. This constraint should be met by careful engineering of the connectivity pattern among photonic switching elements, that is, by enforcing that any waveguide is crossed by optical channels tuned to different wavelengths. • Each slave should receive wavelength channels from different masters on different carrier wavelengths, in order to avoid any signal conﬂict at the receiver side (see Fig.1). In practice, each initiator uses a different wavelength to reach each target, and each target receives packets from the different initiators on different wavelengths. The only alternative would be to receive samewavelength channels on different output waveguides at the same receiver, which would imply some form of spatial division multiplexing. Overall, wavelength-selective routing ends up delivering contention-free all-to-all connectivity, since signal contention for resources is avoided at design time rather than solved at run time. Therefore, no arbitration of ONoC resources is needed. B. Wavelength-Selective Routing In order to meet the above constraints, the wavelengthselective routing function fulﬁlled by each WRONoC topology (a) Drop function of a WRONoC topology. (b) Add function of a WRONoC topology. Fig. 2. Logical tasks performed by a WRONoC topology. Fig. 3. The tightly-intertwined add and drop functions at work in a λ-router WRONoC topology. Numbers refer to wavelength identiﬁers (IDs), while letters refer to master/slave IDs. can be logically viewed as consisting of two sub-functions: • Drop function (Fig.2a). Each master receives from the power distribution network a wavelength-division multiplexed (WDM) optical signal consisting of multiple carriers with wavelengths λ1 to λn . Each carrier is modulated and delivered to the topology for routing to a speciﬁc and distinct slave. Then, the ﬁrst task the topology should perform consists of resolving the individual wavelengthchannels from the multiplexed compound signal, so that each resolved component can be routed to a different destination. In practical terms, this task can be accomplished by using add/drop optical ﬁlters, which are tuned to a speciﬁc wavelength, and therefore split the associated optical channel from the compound signal. • Add function (Fig.2b). Resolved wavelength channels from the different masters and heading to the same slave should be recombined together into a WDM optical signal propagating onto the output waveguide of that slave. This way, a selective ﬁltering stage can eject the desired wavelength channel and feed it to a photodetector stage. In practice, this task can be accomplished by using different inputs of the same add/drop optical ﬁlters. In a WRONoC topology, the add and drop functions are tightly intertwined: as the WDM input signal from a given master propagates down the topology, its wavelength channels are progressively and selectively resolved and coupled with channels tuned on different wavelengths and originating from different masters. This process is pictorially illustrated in Fig.3 for the λ-router WRONoC topology. IV. SYN TH E S I S M ETHODO LOGY Next, we address the problem of synthesizing a generic wavelength-routed optical NoC topology, and demonstrate that the topology design space can be populated much beyond the isolated design points that are available in current literature. We identify a four-step synthesis methodology: wavelength resolution, technology mapping, wavelength assignment, topology connection. A. Wavelength Resolution We view topology synthesis as resulting from the combination of basic primitives. In this speciﬁc context, we identify such basic primitive in the 1x2 add/drop ﬁlter (ADF). In a ﬁrst step, the WDM input signal from masters should undergo the drop function by going through n − 1 ADFs, when assuming the connectivity of n masters with n slaves. This process can be pictorially illustrated in a graph, which we name the wavelength resolution graph (WRG), reported in Fig.4a for a 4x4 generic WRONoC topology. For the moment, we keep the graph completely parametric, and do not specify the resonant (a) Generic wavelength resolution graph. (b) Technology mapping example. (c) Wavelength assigment example. Fig. 4. Three-step synthesis methodology of WRONoC topologies. wavelengths of the ADFs λx , λy , λz , λt on each row. The resolution pattern may differ in each row, as illustrated in the ﬁgure, therefore the drop order of wavelength channels for master A can be different from that of master B , and so on. Also, on each row there is always one wavelength channel (not necessarily the same, and depending on the speciﬁc choice of resonant wavelengths on that row) which is not on resonance with any of the ADFs. Fixing the parameters of the WRG indirectly prunes the design space and biases the methodology toward a speciﬁc topology design point. However, in order for the design point to be legal, one constraint should be enforced on the WRG: the resonant wavelengths of the ADFs along a single row should be different from each other. That is, λx , λy , λz , λt is a valid drop order for the WDM signal from a given master, while λx , λx , λz , λz is not. There is one fundamental reason for posing this constraint. When a set of unresolved wavelength channels goes through consecutive ADFs tuned to the same carrier wavelength (λ1 in the example of Fig.5), the ﬁrst ADF drops channel λ1 , while the set of wavelength channels entering the second ADF reaches its output unaffected. Since the minimum number n−1 of ADF primitives has been instantiated for each row of the WRG, this causes two or more unresolved channels from the same master to reach one output of the topology, as can be clearly observed in Fig.5. Overall, a violation of the constraint causes one or more masters to reach the same slave through different wavelength channels, which also means that one or more slave will be unreachable from that master. B. Technology Mapping The WRG of Fig.4a could be in principle implemented by means of 1x2 ADFs, and by using directional optical couplers to combine dropped wavelength channels heading to the same slave onto the same output waveguide. However, this solution would be rather inefﬁcient, due the large area overhead for the couplers, and to the insertion loss of these structures. Therefore, a more efﬁcient technology mapping can be devised, which consists of grouping the 1x2 ADFs into compact 2x2 photonic switching elements (PSEs, those used in Fig.2a and Fig.2b). In future work, more complex switching cells Fig. 5. Incorrectly-populated WRG by dropping the same wavelength channel twice on the same row. can be explored. In order to map ADFs to 2x2 PSEs, pairs of ADFs should be identiﬁed on the WRG, and replaced by a PSE. However, there exists a fundamental constraint for such mapping process: the legal mappings are the combinations without repetitions C (n, 2) for picking 2 unordered outcomes from n possibilities. In practice, when assuming 4 masters (A,B,C,D) connected to 4 slaves (the same A,B,C,D), then the number of legal mappings C(4,2)=[n(n-1)....(n-k+1)]/2! is 6, namely AB , AC, AD , BC, BD , CD . Since for each master there are n − 1 ADFs on the associated WRG row, there are (n − 1)x(n − 1) ways to group an ADF ﬁlter of A with an ADF ﬁlter of B , of which only 1 should be selected for each topology design point. One legal mapping example is illustrated in Fig.4b. To validate the correctness of the example, one might notice that the ﬁrst ﬁlter of A is grouped with the ﬁrst ﬁlter of B (option AB taken), the second of A with the second of D (option AD taken), the third of A with the second of C (option AC taken), and so on, until all the 6 legal combinations are taken. There is a fundamental reason for posing the above constraint, associated with the add function. As can be observed in Fig.6a, when the WDM signal from a generic input (say, master C ) enters a 2x2 PSE, only the remaining unresolved channels propagate on the same row to complete the drop function, while the resolved wavelength channel is coupled onto another row, where it overlaps with the WDM signal from another master. Therefore, the correct operation of the add function consists of coupling a distinct wavelength channel onto each row (Fig.6a). In contrast, when the WDM signal from a speciﬁc master is coupled onto another row twice (despite the two PSEs are correctly tuned to different wavelengths, see Fig.6b), the dropped channels end up being recombined onto the target row, which violates the philosophy of the drop function performed with the minimum number of ﬁltering primitives. With this understanding, it is possible to compute the number of required PSEs to provide n × n contention-free full connectivity through a generic WRONoC topology: C (n, 2) = n ∗ (n − 1)/2 (1) Similarly, the number of WRONoC topologies in the design space amounts to: [(n − 1) ∗ (n − 2) ∗ ..... ∗ (n − n + 2)]n (2) A 4x4 WRONoC can therefore be implemented in 1296 different ways, one of them being the λ−router or the snake topology. For n > 4, the full enumeration of all possible design points becomes computationally unaffordable. 7: 8: 9: 10: 11: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: C. Wavelength Assignment Please notice that so far resonant wavelengths have been assigned to ADFs just to make examples, but their actual assignment is not strictly necessary to draw the WRG and to perform technology mapping. However, once mapping of ADFs to 2x2 PSEs has been performed, the actual assignment of resonant wavelengths to PSEs becomes mandatory. Clearly, both micro-ring resonators inside PSEs share the same resonant wavelength. In order to perform the complete assignment, we came up with a greedy algorithm, illustrated in Algorithm 1. The basic assumption is that we do not consider real wavelengths (e.g., 1550 nm) but symbolic ones, such as λ1 to λn in an n × n WRONoC.1 In general, each master needs 1 distinct channel to communicate with each one of the n slaves, and such n wavelengths are reused across masters. We consider each row of the WRG, one at a time. On each row, resonant wavelengths should be different not to violate the mutually-exclusive property, and not to end up in case like that of Fig.5. When a wavelength is assigned to an ADF in row i, it is automatically assigned to the companion ADF on another row (say, row k), since they are mapped to the same PSE. For this reason, before assigning a wavelength to an ADF in row i, not only should the algorithm check for wavelengths already assigned in that row, but also on the companion row. For istance, λ1 may not be in use by other ADFs in row i, but may be in use in row k . Overall, the mutually-exclusive property of resonant wavelengths should hold on both companion rows. When multiple wavelength options are available for assignment, the algorithm selects the one with lowest identiﬁer (e.g., λ1 is preferred over λ2 ). In fact, we noticed that under this assumption, the algorithm typically optimizes the number of microring resonator types. When applying our algorithm to the technology mapping of Fig.4b, the assignment in Fig.4c is derived. D. Topology Connection Given the synthesis stage in Fig.4c, it is possible to draw the connectivity pattern of the topology, as well as the features of its PSEs. In particular, the reader can easily prove himself that the topology in Fig.4c is exactly the λ-router topology presented in Fig.3. There is only a minor difference: in the baseline λ-router the last stage is tuned to λ4 , thus resulting in the use of 4 resonator types. Instead, our algorithm tunes the last stage to λ2 because of its preferred choice for minimum wavelength channel identiﬁers. This way, we provide the exact 1 The exact choice of wavelength values is left for future work, since requires a strategy to avoid routing faults. (a) Legal technology mapping of the (b) Illegal mapping: ADFs of masADFs of master C . ters C and B are coupled twice. Fig. 6. Technology mapping at work. Algorithm 1 Tune resonant wavelengths of PSEs. 1: Input Data: WRG, technology mapping, all wavelengths 2: Output Data: matrix of resonant wavelengths of ADFs 3: (cid:46) Initialization 4: selected wavelength ← 0 5: available wavelengths ← EM P T Y 6: for each row i in the W RG do used wavelengths row i ← EM P T Y available wavelengths row i ← all wavelengths for each ADF j in row i do ADF i j assigned ← 0 end for 12: end for 13: (cid:46) wavelength assignment iterative procedure 15: 16: 17: 14: for each row i in the W RG do for each ADF j do if N OT ADF i j assigned then available wavelengths row i ← all wavelengths − used wavelengths row i (k , t) = get companion ADF (i, j ) (cid:46) ADF i j is coupled with companion ADF k t (cid:46) which is the ADF of index t in row k available wavelengths row k ← all wavelengths − used wavelengths row k (cid:46) compute unassigned wavelengths (cid:46) in both row i and row k available wavelengths = intersection (available wavelengths row i, available wavelengths row k) if available wavelengths = EM P T Y then (cid:46) Unable to assign wavelengths end if ERROR : Exit selected wavelength ← minimum(available wavelengths) put selected wavelength in used wavelengths row i ADF i j assigned ← 1 put selected wavelength in used wavelengths row k ADF k t assigned ← 1 available wavelengths ← EM P T Y end if end for 37: end for connectivity pattern of the λ-router while making use of 3 resonator types only. In addition, our synthesis methodology can potentially populate the complete design space of WRONoC topologies relying on 2x2 PSEs as their basic building block. V. EX PER IM EN TA L R E SU LT S A. Customization Opportunities The proposed methodology for WRONoC synthesis paves the way for customization opportunities of existing topologies. Two examples are hereafter provided. In Fig.7a we can observe the breakdown of the optical paths in a 4x4 λ-router. We can see that this topology has two critical paths with three crossings and one drop into a ring resonator. In contrast, there are a couple of overly short paths, featuring one crossing and one drop. With the understanding given by this paper, it becomes possible to investigate where such features comes from. By looking at Fig.4c, we can see that the third ADF ﬁlter of A drops component λ3 to the third row, where it will then cross the PSE tuned to λ2 . Clearly, we have 3 crossings inside PSEs and one drop into a ring resonator. A similar consideration holds for the λ3 component from D . In contrast, components λ3 from both B and C ﬁnd (a) Path breakdown in the λ-router. (b) Technology mapping for path rebalancing. (c) Balanced λ-router. (d) Balanced path breakdown. (e) WRG and mapping for augmented GWOR. Fig. 7. Customization opportunities provided by the proposed methodology. (f) Augmented GWOR topology. PSEs on their way that bring them from ADFs in the second stage to ADFs in the last stage, where they are immediately output by the topology. Thus, they experience just one crossing and one drop. In some cases, the unbalancing in the path structure leads to differentiated signal-to-noise ratios for each optical path. We thus found out a different technology mapping that can potentially result in a more balanced breakdown of the optical paths (Fig.7b). In this mapping, all ADFs are grouped vertically with ADFs at the same ﬁltering stage on different rows. This has the twofold implication that the previously shorter paths as well as longer ones become balanced, since they now all feature two crossings and one drop. However, we clearly have 4 optical paths taken by the off-resonance wavelength λ4 that incur three crossings (but no drop). The new path breakdown is illustrated in Fig.7d, and features a higher degree of balancing in the path structure. We can name the new topology the balanced λ-router. When we visualize such a topology (Fig.7c), we can see that its drawing on a 2D plane becomes more intricate, since it now exposes crossings even outside PSEs. We would like to stress that these crossings should be considered as apparent, since at this stage we are drawing the logic topology, not the physical one. For the sake of laying the foundation of automatic synthesis ﬂows for emerging technologies, we ﬁnd it very important to differentiate between abstraction layers. In fact, when we consider ﬂoor-planning constraints for topology P&R, a number of non-idealities comes to the forefront. For instance, the position of masters and slaves is not the one projected in the logic topology drawing, or there is overlapping with other sub-networks and with the power distribution network. As a result, the number of additional inter-PSE crossings will be large, and not necessarily the lambda-router of Fig.3 will outperform the topology of Fig.7c. The evidence of this will be given in section V-C. Another well-known topology from literature is the GWOR. When designed for a 4 × 4 network, its main issue is that it does not provide self-communication. For this reason, it uses just 3 wavelength carriers. We manually derived the WRG and the technology mapping of the GWOR, which consists of the 4 PSEs tuned to λ1 and λ2 in Fig.7e. We then added another wavelength carrier, and introduced the third stage of ADFs visible in Fig.7e. By leaving the mapping of the baseline GWOR unaffected, and by mapping the new ﬁlters based on the untaken simple combinations without repetitions, we derived the modiﬁed GWOR with support for self-communication, which is illustrated in Fig.7f. B. Populating the Design Space We designed scripts to automatically run the synthesis methodology. We exhaustively populated the design space of 4 × 4 WRONoC topologies, while a sample of 10000 design points was derived for 8 × 8 ones. In this case, in order to Fig. 8. Complete design space for 4 × 4 WRONoC topologies. depends on decisions on the power distribution network and on the tuning of the laser sources that go beyond the scope of this paper. Second, even sticking to the worst-case assumption, the small variation of the critical path across logic topologies does not remain such as these latter are brought down to physical design. To prove this, we used the PROTON tool [20] for the place&route of generic WRONoCs to lay out the population of 4 × 4 topologies. In order to avoid trivial layouts associated with the small radix of considered topologies, we use them in the context of partitioned NoCs. In practice, we consider a 3D-stacked optical plane with 4 hubs (providing ONoC access to 4 clusters of processing cores in the electronic plane) and 4 memory controllers. Such ﬂoorplanning constraints are borrowed from [19]. In this context, we use three 4 × 4 networks (same topology) to provide 8×8 connectivity: one for memory requests (from hubs to controllers), one for memory responses (from controllers to hubs), and one for inter-hub communications. All 1296 4×4 WRONoC topologies are tried in this context. Post-place&route results are reported in Fig.10, where the critical path (max. number of crossings across all optical paths) distribution is reported. Interestingly, now the critical path length ranges from 18 to 39 crossings, thus featuring a much larger variability than in logic schemes. This raises the issue of placement-aware logic topology synthesis, which is consolidated for electronic design, but is completely new for optical NoCs. After checking the key differences between best and worst topologies, we found that the best ones feature most of their optical paths distributed between two or three crossings. Instead, the worst ones often have many paths with 0 (i.e., 1 drop) or 1 crossings, and other ones at 3 or 4 crossings. Probably, short and long paths are intertwined through the same PSEs, and the P&R tool handles the short paths ﬁrst, but then needs to route waveguides all the way back to drop optical carriers taking the long paths. However, in some cases we found topologies with the same path distribution performing either very well or very bad. The reason is that despite the distribution is the same, the topology connectivity is different, therefore one topology may have x crossings in path no.1 and y crossings in path no.2, while another one may feature the reverse situation. What ultimately changes, and makes the difference, is the association of these paths to a connection between masters and slaves. For instance, if path no.1 connects master A with slave B , this will go through a path with x crossings in the ﬁrst topology, and of y crossings in the second topology, thus leading to different results. Finally, we applied a power model to infer the static power of laser sources under ideal assumptions for the power distribution network (PDN). In essence, we choose the worst case power requirement for every wavelength channel, since this later enables to infer a PDN with 3dB splitters. That is, among the insertion losses of the n channels at wavelength λ1 provided by PROTON, we choose the worst one and we add the other loss contributions which are equal in every communication path, i.e, modulator loss (1dB), receiver coupler loss (1dB) and receiver loss (1dB) [2]. From the maximum insertion loss, we derive the laser power requirement for a given receiver sensitivity (-20dBm) [2], laser efﬁciency (20%) [14], coupling efﬁciency (90%). Then, we feed the derived optical power to all the wavelength channels at λ1 , thus deriving the total power for that laser source as the sum of all channel powers. We do similarly for the other wavelength Fig. 9. Sample of 10000 randomly-generated 8 × 8 WRONoC topologies. sample the design space, we took random decisions whenever a choice had to be made in the synthesis ﬂow (e.g., technology mapping), thus avoiding to bias the sample. The plot in Fig.8 shows the complete design space for 4 × 4 WRONoC topologies built out of 2x2 PSEs. Most logic topologies have 4 crossings on the critical path, while 15% of the population has only 3 (the λ-router belongs to this latter category). Interestingly, when the critical path is the longest, then it is likely to be unique in the topology (46% probability). The worst case consists of 4 critical paths of 4 crossings. In contrast, when the critical path is shorter (3 crossings), the topology is likely to have more than one. In fact, when the critical path is 3-crossings deep, the probability to have 8 of them is 60%. Although only 10000 topologies could be generated for 8 × 8 WRONoCs, this sample of the design space seems to conﬁrm a similar trend. Results are reported in Fig.9. Two features are noticeable. First, most of the topologies features the longest critical path of 12 crossings. This means that without proper methods to explore and prune the design space, random walks through it are very likely to materialize a sub-optimal design point. Second, a high number of critical paths materializes only when the critical path length is shorter than the maximum one (e.g., 10 or 11 crossings). C. Physical Design The previous section may lead to think that despite the large number of feasible design points, most of them are basically equivalent, since they feature only few values of critical path length. This is not true for a couple of reasons. First, the total static power of a topology may depend not only on the critical path of the topology as a whole, but also on the critical path among the n paths taken by each wavelength carrier. This Fig. 10. Distribution of the critical path after physical mapping. [7] S. Pasricha and N. Dutt, ”ORB: An on-chip optical ring bus communication architecture for multi-processor systems-on-chip,” in Asia South Paciﬁc Design Automat. Conf., Seoul, Korea, Jan. 2008, pp. 789-794. [8] H. Gu, J. Xu, and W. Zhang, ”A low-power fat-tree-based optical network- on-chip formultiprocessor system-on-chip,” in DATE2009. [9] A. Joshi et al., ”Silicon-photonic Clos networks for global on-chip communication,” in Int. Symp. Networks-on-Chip, 2009. [10] Y. Pan et al., ”Fireﬂy: Illuminating on-chip networks with nanophotonics,” in Int. Symp. Comput. Architecture, Austin, TX, 2009. [11] A. Shacham, K. Bergman, and L. P. Carloni, ”Photonic networkson-chip for future generations of chip multiprocessors,” IEEE Trans. Comput., vol. 57, no. 9, pp. 1246-1260, Sep. 2008. [12] M. J. Cianchetti, J. C. Kerekes, and D. H. Albonesi, ”Phastlane: A rapid transit optical routing network,” in Int. SCA2009. [13] Z. Li et al., ”Iris: A hybrid nanophotonic network design for highperformance and low-power on-chip communication,” J. Emerg. Technol. Comput. Syst., vol. 7, no. 2, p. 8, Jun. 2011. [14] L. Ramini, P. Grani, S. Bartolini, D. Bertozzi “Contrasting wavelengthrouted optical NoC topologies for power-efﬁcient 3d-stacked multicore processors using physical-layer analysis”, DATE2013. [15] X. Tan et al., “On a Scalable, Non-Blocking Optical Router for Photonic Networks-on-Chip Designs”, SOPO 2011. [16] S. Le Beux et al., “Optical Ring Network-on-Chip (ORNoC): Architecture and Design Methodology”, DATE2011 [17] S. Le Beux a, I. OConnor,G. Nicolescu, G. Bois, P. Paulin “Reduction Methods for adopting network on chip topologies to 3D Architectures ”, Journal of Microprocessors and Microsystems, 2012. [18] S.Borkar, “Exascale Computing — a Fact or a Fiction?”, IPDPS 2013. [19] L. Ramini, D. Bertozzi and L P. Carloni “Engineering a BandwidthScalable Optical Layer for a 3D Multi-core Processor with Awareness of Layout Constraints”, NOCS2012 [20] A. Boos, L. Ramini, U. Schlichtmann, D. Bertozzi “PROTON: An automatic place-and-route tool for optical Networks-on-Chip”, International Conference on Computer-Aided Design (ICCAD), 2013 IEEE/ACM [21] S. Le Beux, Hui Li, G. Nicolescu, J. Trajkovic, I. O’Connor “Optical crossbars on chip, a comparative study based on worst-case losses ”, Concurrency and Computation Practice and Experience, October 2014. [22] S. Le Beux, J.Trajkovic, I.O’Connor and G.Nicolescu, “Layout Guidelines for 3D Architectures including Optical Ring Network-on-Chip (ORNoC)”, VLSI-SoC2011 [23] P.Grani and S.Bartolini, “Design Options for Optical Ring Interconnect in Future Client Devices”, (JETC): ACM Journal on Emerging Technologies in Computing Systems, Vol. 10, Issue 4, May 2014. [24] C. Batten, A. Joshi, V. Stojanovic, and K. Asanovic ”Designing chiplevel nanophotonic interconnection networks”, IEEE Journal on Emerging and Selected Topics in Circuits and Systems,2012. [25] M. Briere, B. Girodias, et al., ”System level assessment of an optical NoC in an MPSoC platform,” DATE2007 [26] Tiansheng Zhang, et al. ”Thermal Management of Manycore Systems with Silicon-Photonic Networks” in DATE 2014 [27] De Micheli et al., ”New Logic Synthesis as Nanotechnology Enabler” Proceedings of the IEEE — Vol. 103, No. 11, November 2015 [28] Kishore Padmaraju and Keren Bergman, ”Resolving the thermal challenges for silicon microring resonator devices”, Nanophotonics, 2013. [29] Tiansheng Zhang, et al., ”Thermal Management of Manycore Systems with Silicon-Photonic Networks” in DATE 2014. [30] Mahdi Nikdast, et al., ”Crosstalk Noise in WDM-Based Optical Networks-on-Chip: A Formal Study and Comparison.” IVLSI Syst. 2015. [31] http://www.vpiphotonics.com/Tools/ [32] https://www.lumerical.com/tcad-products/interconnect/ [33] G. Hendry, J. Chan, L.P. Carloni, K. Bergman VANDAL: A Tool for the Design Speciﬁcation of Nanophotonic Networks DATE2011. [34] http://www.phoenixbv.com [35] https://www.lumerical.com/solutions/partners/eda/mentor graphics/ [36] D. Bertozzi et al, ”NoC Synthesis Flow for Customized Domain Speciﬁc Multiprocessor Systems-on-Chip”. Trans. Parallel Distrib. Syst. 2005 [37] S. Murali , P. Meloni, F. Angiolini, D. Atienza, S. Carta, L. Benini, G. De Micheli, L. Raffo, ”Designing Application-Speciﬁc Networks on Chips with Floorplan Information” ICCAD 2006 [38] S. Seo, A. Chatterjee, ”A CAD tool for system-on-chip placement and routing with free space optical interconnect”, Conference on Computer Design: VLSI in Computers and Processors, 2002 J. Minz, S. Thyagara, S. Lim, ”Optical Routing for 3D System-onPackage”, IEEE Transactions on Components and Pack. Tech., 2007 [40] D. Ding, Y. Zhang, H. Huang, T. Chen and D.Z. Pan ”O-Router: an optical routing framework for low power on-chip silicon nano-photonic integration”, DAC2009 [41] C. Condrat, P. Kalla, S. Blair ”Crossing aware channel routing for integrated Optics ”, ACM Great Lakes Symposium on VLSI, 2014 [42] S. Bahirat, S. Pasricha ”Design and synthesis of hybrid nanophotonic application speciﬁc 3D network-on-chip architectures ”, SiPhotonics 2014 [39] Fig. 11. Distribution of static power of topologies. carriers. Total laser power results for each 4 × 4 topology, replicated three times in the complex ﬂoorplanning scenario above, is reported in Fig.11. Logic schemes are categorized as nx cy , which means that a topology has x critical paths of length y crossings. Most of the topologies in each category end up in the same power range, from 50 to 130mW. When the category is densely populated (like n1 c4 or n2 c4), the distribution tends to a gaussian curve, although there is long and non-marginal tail toward high power values. For less dense categories (n3 c4, n6 c3, n8 c3), the distribution tends to become more uniform, and the tail shorter. Interestingly, both categories n8 c3, n6 c3 and n1 c4 are able to materialize solutions with the lowest power (range 40-50mW). Also, a few solutions exist that bring the total power to more than 300 mW, thus making the point for smart pruning of the design space toward the most efﬁcient solutions. D. Conclusions This paper proposes a methodology that systematically synthesizes all the points of the WRONoC topology design space by building upon the combination and aggregation of basic ﬁltering primitives. The derived abstraction is quite powerful, since it enables to populate a design space previously limited to few design points devised by researchers’ intuition. We demonstrate that with the proposed abstraction it is possible to customize existing solutions for path rebalancing or to provide self-communication. Moreover, we demonstrate that a small logic design space can be mapped onto a much wider physical design space, thus raising the need for placement-aware topology synthesis. Overall, this work is a stepping stone into future work targeting automatic synthesis of WRONoC topologies that meet the connectivity requirements and the ﬂoor-planning constraints of the system at hand. "
Run-time laser power management in photonic NoCs with on-chip semiconductor optical amplifiers.,"Photonic network-on-chip (PNoC) architectures are projected to achieve very high bandwidth with relatively small data-dependent energy consumption compared to their electrical counterparts. However, PNoC architectures require a non-trivial amount of static laser power, which can offset most of the bandwidth and energy benefits. In this paper, we present a novel low-overhead technique for run-time management of laser power in PNoCs, which makes use of on-chip semiconductor amplifiers (SOA) to achieve traffic-independent and loss-aware savings in laser power consumption. Experimental analysis shows that our technique achieves 31.5% more laser power savings with 12.8% less latency overhead compared to another laser power management scheme from prior work.","Run-Time Laser Power Management in Photonic NoCs with  On-Chip Semiconductor Optical Amplifiers  Ishan G Thakkar, Sai Vineel Reddy Chittamuru, Sudeep Pasricha  Department of Electrical and Computer Engineering  Colorado State University, Fort Collins, CO, U.S.A.  {ishan.thakkar, sai.chittamuru, sudeep}@colostate.edu  Abstract— Photonic network-on-chip (PNoC) architectures are projected to achieve very high bandwidth with relatively small data-dependent energy consumption compared  to their electrical counterparts. However, PNoC architectures require a non-trivial amount of static laser power,  which can offset most of the bandwidth and energy benefits.  In this paper, we present a novel low-overhead technique for  run-time management of laser power in PNoCs, which  makes use of on-chip semiconductor amplifiers (SOA) to  achieve traffic-independent and loss-aware savings in laser  power consumption. Experimental analysis shows that our  technique achieves 31.5% more laser power savings with  12.8% less latency overhead compared to another laser  power management scheme from prior work.  Index Terms—Photonic Networks-On-Chip, Power Management, Laser Source, Optical Signal Loss  I. INTRODUCTION  In the many-core era, processors with hundreds of cores on a  single chip are gradually becoming a reality. The performance  of these many-core processors is driven by the effectiveness of  the underlying electrical network-on-chip (ENoC) fabrics that  are becoming increasingly crosstalk- and energy-limited [1]. To  this end, due to the recent developments in the area of silicon  photonics, photonic network-on-chip (PNoC) fabrics have been  projected to supersede ENoCs. PNoCs offer a multitude of benefits over ENoCs such as: 1) higher bandwidth density; 2) distance-independent bit-rate; and 3) smaller data-dependent energy. However, irrespective of network traffic and utilization,  PNoCs dissipate a non-trivial amount of static power in their  laser source. The high laser power overheads can offset the  bandwidth and energy advantages of PNoCs. Therefore, it is imperative to forge new techniques that can reduce the static power  consumed in the laser sources of future PNoC architectures.  Several techniques have been proposed in prior works, e.g.  [1]-[6], that aim to reduce static power in the laser sources of  PNoCs. All of these techniques leverage temporal and spatial  variations in network traffic and opportunistically adjust the  power in laser sources by tuning or distributing the available  network bandwidth. These methods tend to notably reduce the  power in laser sources during low network load conditions.  However, if the losses encountered by optical signals in the network between the source and destination are high, these methods would still require excessive laser power to compensate for  the high losses, even under low network load conditions. This  observation motivates the need for a technique that can provide  traffic-independent and loss-aware savings in laser power.   In this paper, we present a novel low overhead technique for  run-time management of laser power in PNoCs, which makes  use of on-chip semiconductor amplifiers (SOA) to achieve  978-1-4673-9030-9/16/$31.00 (c) 2016 IEEE  traffic-independent and loss-aware savings in laser power. We  refer to our SOA-enabled laser power management technique as  SOA_LPM. Unlike the techniques proposed in prior works [1][6], SOA_LPM draws minimum power from the off-chip laser  source and offloads the burden of loss-aware run-time laser  power management to on-chip SOAs, which in turn enables  significant savings in laser power with minimal overheads.  Moreover, SOA_LPM is orthogonal to all the other laser power  management techniques reported in prior works, and can be  used in combination with any of them. Our novel contributions  in this paper are summarized below:  • We propose a low overhead, SOA-enabled, and loss-aware  technique (SOA_LPM) to manage and optimize the laser  power overhead in PNoC architectures at run-time;  • We implement our SOA_LPM technique on  a  multiple-write  -multiple-read (MWMR) photonic waveguide architecture;  • We present device-level analytical models of on-chip SOAs,  and based on this models, we analyze the energy, area and  performance overhead of our SOA_LPM technique;  • We evaluate SOA_LPM by implementing it on a well-known  PNoC architecture Flexishare [7] and compare it with another  laser power management technique from prior work [3].  II. BACKGROUND  PNoC architectures (e.g., [17]-[19]) employ multiple highbandwidth photonic links, each of which connects two or more  nodes (e.g., cores). Typically, a large number of wavelengths  are dense wavelength division multiplexed (DWDM) in a single  photonic link. Each wavelength corresponds to a channel that is  used for serial data transfers. Additionally, a photonic link employs microring resonator (MR) modulators (that are in resonance with the utilized wavelengths) at the source node to modulate electrical signals onto photonic signals that travel through  the link, and MR detectors at the destination node to detect photonic signals and recover electrical signals. In general, the use  of multiple wavelengths (or channels) in a photonic link enables  high bandwidth parallel data transfers across the link.  The amount of laser power required by a source node to transfer data across the parallel wavelength channels of a photonic  link to a destination node can be expressed as:  (cid:1842)(cid:3013)(cid:3028)(cid:3046)(cid:3032)(cid:3045) − (cid:1845) ≥ (cid:1842)(cid:3013)(cid:3042)(cid:3046)(cid:3046) + 10 log(cid:2869)(cid:2868) (cid:4666)(cid:1840)(cid:3090) (cid:4667),  (1)  where, PLaser is the required laser power in dBm, Nλ is the  number of wavelength channels in the link, PLoss (in dB) is the  total optical loss faced by a photonic signal along the link from  the source to the destination, and S is the sensitivity of the  detector (assumed to be -20dBm [8]). PLoss includes optical  signal losses such as through loss in MR modulators and  detectors, modulating losses in modulator MRs, detection loss  in detector MRs, propagation and bending loss in waveguides,                    and splitting loss in splitters. Overall, PLaser thus depends on two  main factors: 1) link bandwidth in terms of Nλ, which controls  the network utilization and traffic, and 2) the total optical loss  PLoss during photonic signal propagation [20]-[21].   As implied from Eq. (1), if a link is underutilized (due to low  traffic), its laser power consumption PLaser can be reduced by  decreasing Nλ associated with the link. This is exactly what is  done in prior works [1], [2], and [3] to reduce the total laser  power consumption in PNoCs. Some other prior works also propose laser power management techniques, e.g., [4], [5], wherein  they achieve significant power savings using dynamic reconfiguration of photonic networks. In spite of requiring periodic evaluation of network traffic and expensive run-time decision-making, these methods (presented in [1]-[5]) achieve notable savings in laser power. But the savings are highly contingent on  information related to network traffic and losses. In contrast to  the approaches from [1]-[5], Wang et al. [6] proposed a technique that achieves loss-aware savings in laser power. However,  this technique requires the network to function in the TDM communication paradigm only, which incurs architecture specific  overheads, limiting the scope of this technique.   In this paper, we present a complementary, low overhead,  and SOA-enabled technique called SOA_LPM that can provide  traffic-independent and loss-aware laser power savings for  PNoC architectures. For transmitting a packet between source  and destination nodes, SOA_LPM first allocates only the minimum amount of laser power to the source node that is enough  for correct detection at the destination node. It then accounts for  losses to be faced by the packet on its path from the source to  the destination and enables the source to amplify the allocated  laser power to the necessary level by using an on-chip SOA. As  will be evident in the following sections, SOA_LPM proves to  be more energy efficient than previously proposed techniques.  Table 1: Definitions and values of parameters for our SOA model  Parameter  Definition  Value  a1  Constant  6.7×10-16 cm2  n0  Transparency carrier concentration  1.2×1018 cm-3  α  Loss in SOA active region  10 cm-1  Γ  Light confinement factor  0.4  L  Length of SOA active region  10μm  Δλ  SOA gain linewidth  95nm  I0  Threshold input current  5μA  III. SEMICONDUCTOR OPTICAL AMPLIFIERS: OVERVIEW  We first give a brief explanation of the structure and behavior  of SOAs, before presenting analytical models for SOA gain  (Section IV.A) and overheads (Section IV.B).  A detailed description of the structure, functionality, and  modeling of SOAs is given in [10]. Briefly, an SOA is an optoelectronic device, which can be heterogeneously integrated with  a silicon-on-insulator (SOI) based silicon photonic chip, and under suitable operating conditions can amplify an input broadband light signal. The structure of an SOA consists of an active  waveguide region made of an intrinsic narrow bandgap material  (e.g., AlInGaAs, InGaAsP), which is sandwiched between ntype and p-type cladding materials (e.g., n-InP, p-InP) with  wider bandgaps. Free carriers are injected into the active waveguide region from the applied bias current, which in turn causes  population inversion in the active region. The population inversion of free carriers in the active region results in stimulated  emission of light, which imparts “gain” to the input optical signal. The operational characteristics and the gain spectrum of an  SOA depend on its structure and materials used.  A. Analytical Model for SOA Gain  As mentioned earlier, an SOA can provide broadband optical  gain in its active region through stimulated emission. This gain  obtained in the active region of unity length is called material  gain (gm). The effect of gm on SOA output power can be exponentially increased by increasing the length of the active region  to provide a very high value of single-pass bulk SOA gain (G).  Both gm and G are functions of wavelength (λ) and input bias  current (I), which can be expressed as [13]:  L, I0 and Δλ are constants that depend on the structure and operHere, gm and G are in cm-1 and dB, respectively; and a1, α, n0, Γ,  ating conditions of the SOA. We took the typical values of these  constants from [13], as shown in Table 1. We modeled the SOAs  used in this work using Eq. (1), (2) and the constants in Table 1.   (cid:1859)(cid:3040) (cid:4666)(cid:2019), (cid:1835)(cid:4667) = (cid:3428)(cid:3436)(cid:1985)(cid:1853)(cid:2869)(cid:1866)(cid:2868) (cid:3420) (cid:1835)(cid:1835)(cid:2868) − 1(cid:3424)(cid:3440) − (cid:2009)(cid:3432) (cid:4680)1 − 2(cid:4666)(cid:2019) − 1570(cid:4667)(cid:2870) (cid:1986)(cid:2019)(cid:2870) (cid:1833) (cid:4666)(cid:2019), (cid:1835)(cid:4667) = 10 log(cid:2869)(cid:2868)(cid:3435)(cid:1857) (cid:3013)∗(cid:3034)(cid:3288) (cid:4666)(cid:3090),(cid:3010)(cid:4667)(cid:3439),  (cid:4681) ,  (2)  (3)  B. Overhead Analysis   From the description in the previous sections, the SOA gain  is proportional to SOA input current (I). As a result, use of SOA  to amplify a DWDM signal comes with a non-zero power overhead corresponding to the non-zero SOA input current. The  power consumed by an SOA can be calculated by multiplying  the target I with the operating voltage. Based on guidelines for  bulk SOA design [13], we design the length (L) of the active  region and operating voltage of our SOAs to be 15μm and 1.5V,  respectively. Thus, we assess the power overhead of an SOA by  multiplying its target I with 1.5V. Moreover, an SOA takes  about 20-50ps to adjust to the target gain [13]. In summary, an  SOA incurs power overhead equal to input current (I) × 1.5V,  and latency overhead of at most 50ps (0.25 cycles at 5GHz).  Note that the broadband gain profile of an SOA is subject to  fluctuations due to temperature variations and noise. To  compensate for these fluctuations, our SOA_LPM technique  operates the SOAs at such input current levels that can yield 3dB  more SOA gain than the desired gain.   IV. SOA ENABLED LASER POWER MANAGEMENT  Our proposed SOA_LPM  technique uses SOAs  in  combination with comb switches [9] and lookup tables to enable  loss-aware run-time  laser power management  in PNoC  architectures. SOA_LPM can be easily ported to different  PNoCs and its implementation depends on the type of bus  waveguides (BWGs) used in PNoC architecture. To evaluate  our proposed SOA_LPM in this work, we implement it on a  crossbar based PNoC architecture Flexishare [7]. The Flexishare  PNoC uses multiple write multiple read (MWMR) type of  BWGs. Each node (e.g., a processing element) connected to a  crossbar requires both read and write access to the other nodes.  A detailed analysis of SOA_LPM implementation for MWMR  BWGs is presented in the following sub-section.                       A. Implementation for MWMR Bus Waveguide  In an MWMR BWG, multiple nodes are capable of sending  and receiving data using their modulating and detecting MR  banks respectively. Therefore, MWMR BWGs require arbitration among multiple sender nodes, and also receiver selection  among multiple receiver nodes [7][11].         Fig. 1 illustrates the implementation of SOA_LPM on a typical MWMR BWG based PNoC. As shown in the figure, the  PNoC is comprised of multiple MWMR BWGs. In general,  MWMR BWG based PNoCs with N nodes have N sender nodes  and N receiver nodes (i.e., all N nodes can send as well as receive), with implementation-specific K MWMR BWGs. Each  MWMR BWG employs a comb switch [9] (a broadband MR  switch that can switch the entire DWDM spectrum) and an onchip SOA [10]. Thus, the PNoC requires K SOAs and K comb  switches as there are K BWGs in the PNoC. The SOA of each  MWSR BWG can be controlled by any of the N sender nodes  depending on which sender node wins the arbitration.   As shown in the figure, each sender node of each MWMR  BWG has an SRAM based lookup table that stores N-1 values  of loss (corresponding to N-1 receivers). After arbitration, the  authorized sender node initiates a receiver selection phase, at the  end of which the sender node accesses a corresponding entry  from the lookup table to determine the total loss value that the  signal will face on its way to the target receiver. Then, the sender  node adjusts the gain of the SOA to be equal to the loss value.  Next, the sender node controls the comb switch to extract only  the minimum laser power equal to S=-20dBm (detector sensitivity) from the power waveguide and provide it as an input to  the SOA. The SOA amplifies the allocated laser power by a  value that is equal to the accessed loss value and provides it to  the sender node, which then modulates it for communication  with the target receiver.  Note that we assume each entry of the  lookup table to be of 8 bits, therefore, each sender has N-1 number of 8-bit entries in its SRAM based lookup table.  Fig. 1: Implementation of SOA_LPM on MWMR BWG based PNoC.    V. EXPERIMENTAL EVALUATION  A. Evaluation Setup   We target a 256-core many-core system for evaluating our  SOA enabled laser power management (SOA_LPM) technique.  We evaluate SOA_LPM on a well-known crossbar-based PNoC  architecture Flexishare [7]. Flexishare uses 32 groups of  MWMR BWGs with a 2-pass token stream arbitration. Each  MWMR BWG in Flexishare architecture is capable of transferring 512 bits of data from a source node to a destination node.   We modeled and simulated the architectures at cycle-accurate  granularity with a SystemC-based NoC simulator. We used realworld traffic from applications in the PARSEC benchmark suite  [12]. Full-system gem5 simulation of parallelized PARSEC  benchmarks [13] was used to generate traces that were fed into  our cycle-accurate NoC simulator. We set a “warm-up” period  of 100M instructions and captured traces for 1B instructions.  We targeted a 22nm process node and 5GHz clock frequency  for the 256-core system. We considered BWGs with 64 DWDM  wavelengths sharing the working band 1510nm–1590nm.   The static and dynamic energy consumption of electrical  routers and concentrators in Flexishare architecture is based on  results from the DSENT tool [16]. We model and consider area  and performance overheads for SOA_LPM enabled laser power  management. We estimated electrical area and power overhead  using gate-level analysis and the open-source CACTI tool [15]  for the SRAM-based lookup tables. The electrical area overhead, the electrical power overhead, and the photonic area overhead is estimated to be 0.8mm2, 0.24mW, and 237.5µm2 respectively for the Flexishare PNoC architecture.  To compute laser power, we considered detector sensitivity  of -20dBm, MR through loss of 0.02 dB, waveguide propagation  loss of 1dB/cm, waveguide-bending loss of 0.005dB/900, and  waveguide coupler/splitter loss as 0.5dB [14]. We calculated  photonic loss in components using these values, which sets the  photonic power budget and correspondingly the electrical power  for the off-chip laser source. Moreover, we take the energy/power and latency overhead values of SOAs and comb  switches as discussed in Section III.A.  B. Comparison with Prior Work   We compared SOA_LPM with a dynamic laser power management technique (BW_LPM) from prior work [3].  BW_LPM  performs a weighted time-division multiplexing of the photonic  network bandwidth, and leverages the temporal fluctuations in  network bandwidth to opportunistically save laser power.  BW_LPM is designed to perform laser power management in  MWMR BWGs [3]. Therefore, we focus on the Flexishare  PNoC architecture with MWMR BWGs for our evaluation.   We analyzed power consumption and average packet latency  for the SOA_LPM and BW_LPM techniques when they were integrated into the Flexishare PNoC architecture. For a fair comparison with BW_LPM, it is important to enable weighted timedivision multiplexing of the network bandwidth in the Flexishare PNoC. Therefore, we enhanced the baseline Flexishare  PNoC to enable weighted time-division multiplexing of the network bandwidth using token stream arbitration (TS) in its  MWMR BWGs through the laser controller. We refer to this enhanced Flexishare PNoC as Flexishare-TS.   We implemented the BW_LPM technique on Flexishare-TS,  and refer to the resulting PNoC configuration as Flexishare-TSBW_LPM. Similar to the BW_LPM enhanced PNoC architecture  presented in [3], the Flexishare-TS-BW_LPM configuration also  has four laser sources along with a laser source controller, which  is capable of switching ON/OFF the laser sources based on the  executing application bandwidth requirements.  We implemented SOA_LPM on Flexishare-TS to obtain the  Flexishare-TS-SOA_LPM PNoC configuration, and compared it  with the BW_LPM enhanced Flexishare-TS-BW_LPM PNoC  configuration. We also implemented BW_LPM and SOA_LPM  together and show the combined benefits for the resulting  Flexishare-TS-BW_LPM-SOA_LPM PNoC configuration.          Fig. 2 shows average latency results of this comparison study,  with all results normalized with respect to the baseline Flexishare PNoC. As evident from the figure, it can be observed that  on average, Flexishare-TS-SOA_LPM has 7.7% higher latency  compared to Flexishare-TS. The inferior values of latency are  due to the additional time/cycles required for switching of laser  power using comb switches (0.5ns), for accessing the lookup table entries (1ns), and for amplification using SOAs (50ps). Further, Flexishare-TS-SOA_LPM has 12.8% lower average latency compared to Flexishare-TS-BW_LPM. The increase in average latency for Flexishare-TS-BW_LPM is because of additional cycles required for periodic computation of average  packet latency in the BW_LPM technique. Flexishare-TSBW_LPM-SOA_LPM also has 25.8% higher latency compared  to Flexishare-TS. The latencies contributed by BW_LPM and  SOA_LPM cumulate to render worse average latency for Flexishare-TS-BW_LPM-SOA_LPM.   Fig. 2: Average latency for different variants of the Flexishare PNoC  architecture. Results are normalized w.r.t baseline Flexishare PNoC.  Fig. 3: Average laser and SOA power consumption comparison for different configurations of the Flexishare PNoC architecture. Results are  normalized w.r.t. the baseline Flexishare PNoC architecture.  Fig. 3 presents a comparison of the laser and SOA power  consumption values averaged across 12 applications of the  PARSEC benchmark suite for the baseline Flexishare, Flexishare-TS, Flexishare-TS-BW_LPM, Flexishare-TS-SOA_LPM  and Flexishare-TS-BW_LPM-SOA_LPM PNoC configurations.  The error bars in the figure represent maximum and minimum  power values across the 12 applications. Flexishare-TSSOA_LPM has 50.8%, 43.4%, and 30.1% lower total power consumption compared to Flexishare baseline, Flexishare-TS, and  Flexishare-TS-BW_LPM  respectively. For Flexishare-TSSOA_LPM and Flexishare-TS-BW_LPM-SOA_LPM, more than  50% of the total power consumption is due to the power overhead of SOAs. Even then, these PNoC configurations possess  less total power consumption than all other variants. These results corroborate the excellent capabilities of our SOA_LPM  technique in reducing laser power and hence total power consumption. Moreover, it can be inferred that SOA_LPM can be  combined with other laser power management techniques such  as BW_LPM to save even more laser power.  In summary, our SOA_LPM laser power management technique saves significant power with nominal latency overheads.   VI. CONCLUSIONS  We presented a low overhead, run-time laser power management technique called SOA_LPM, which made use of on-chip  semiconductor amplifiers (SOA) to achieve traffic-independent  and loss-aware savings in laser power consumption. Experimental analysis shows that our technique achieves 31.5% more  laser power savings with 12.8% less latency overhead compared  to another laser power management scheme from prior work.  Thus, SOA_LPM represents an attractive solution to reduce laser  power consumption in emerging PNoCs.  ACKNOWLEDGMENT  This research is supported by grants from SRC, NSF (CCF1252500, CCF-1302693), and AFOSR (FA9550-13-1-0110).  "
Hybrid large-area systems and their interconnection backbone (invited paper).,"Hybrid systems combine Large-Area Electronics (LAE) with high-performance technologies (e.g., silicon CMOS) [1]. With architectural concepts for hybrid systems broadening to match the range of emerging applications, this paper examines modular approaches for multi-sheet, multi-technology integration. It identifies the interfaces required as a critical backbone. For interfaces associated with various system functionalities (sensing, processing, powering), specific approaches are surveyed and analyzed, taking from insights derived from several previous experimental demonstrations of complete hybrid systems.","Hybrid Large-Area Systems and their Interconnection  Backbone (Invited Paper) Naveen Verma, L. Aygun, Y. Afsar, Y. Hu, L. Huang, T. Moy, J. Sanz-Robinson,   W. Rieutort-Louis, S. Wagner, J. C. Sturm  Princeton University  EQuad Rm. B-226  Olden Street  +1 609-258-1424  nverma@Princeton.EDU ABSTRACT  Hybrid systems combine Large-Area Electronics (LAE) with  high-performance technologies (e.g., silicon CMOS) [1]. With  architectural concepts for hybrid systems broadening to match the  range of emerging applications, this paper examines modular  approaches for multi-sheet, multi-technology  integration. It  identifies the interfaces required as a critical backbone. For  interfaces associated with various system functionalities (sensing,  processing, powering), specific approaches are surveyed and  analyzed, taking from insights derived from several previous  experimental demonstrations of complete hybrid systems.    Keywords  Large-area electronics, hybrid systems, thin-film transistors.  1. INTRODUCTION  Through five decades of Moore’s-Law scaling, computation is  available to us on an immense scale. This is in large part thanks to  a platform technology for realizing computational functions,  namely silicon (Si) CMOS. Having a platform technology has  enabled the formulation of principled design approaches as well  as focused engineering efforts directed at these. This, in turn, has  led to sustained advancements of both the entire technological  stack and the resulting applications.   However, even with the great number of diverse applications we  have today, it is likely that the application space where  computation offers potential for impact is even broader. In this  paper, we imagine the possibility of applying computation to all  forms of physical signals. Signals that exist all around us and  indeed within us (on/inside our bodies, in the spaces we work and  play, within the infrastructure we rely on, etc.) are likely of high  importance to us; the application of computation to these can  enable systems of correspondingly high value. What limits this  today is that, unlike computation, we do not have an established  platform technology for sensing the broad-range and largenumber of physical signals of relevance. In this paper we focus on  perhaps the first potential platform technology for creating  extensive and diverse interfaces to the physical world, namely  Large-Area Electronics (LAE).   Below, we describe  some attributes of LAE. Though  transformational for creating large numbers of distributed, formfitting sensors and energy harvesters, we note that LAE is  inadequate for creating complete systems, which need various  additional functionalities. We go on to describe the model of  hybrid systems, taking a modular approach, wherein various LAE  functionalities are combined with other  required system  functionalities, implemented using high-performance technologies  (namely, Si CMOS). Then, we explore the primary challenge that  978-1-4673-9030-9/16/$31.00 (c)2016 IEEE  emerges with hybrid systems, namely interfacing the various  functionalities realized by the different technologies. We survey  various circuit and architectural solutions that have recently  emerged, and also look at algorithmic concepts for enhancing  these. Finally, we provide some conclusions.    2. LARGE-AREA ELECTRONICS (LAE)  Large-area electronics (LAE) is a technology based on processing  thin-films at low temperatures (<200C, compared to >1000C for  Si CMOS). Low-temperature processing enables compatibility  with a wide range of materials, providing a basis for diverse  transducers for sensing and energy harvesting, as well as their  direct fabrication on substrates such as glass, plastic, paper, etc.  Further, the associated materials and their processing methods  (e.g., deposition of the thin-films) can be achieved over large-area  substrates, today reaching over 10m2. Figure 1 provides a brief  visual survey showing the types of sensors and form factors, i.e.,  large-area flexible sheets, that have been achieved.  Though low-temperature processing enables compelling attributes  for creating sensors, sensing systems require the integration of  Figure 1. Survey of sensor types and form factors achieved in LAE.  numerous other functionalities (instrumentation, computation,  power management, communication). To realize these, circuits  based on transistors are needed. Though thin-film transistors  (TFTs) are possible, unfortunately, low-temperature processing  results in these having orders-of-magnitude lower performance  and energy efficiency than typical Si-CMOS transistors. For  illustration, Table 1 provides a comparison of key metrics for SiCMOS transistors and amorphous-silicon (a-Si) TFTs (the current  workhorse LAE technology in flat-panel display applications).  For this reason, there is increasing focus on hybrid systems [1],  which combine LAE with high-performance technologies (e.g., Si  CMOS), in order to realize complete systems. Though the  integration of  two complementary  technologies presents a  compelling path forward, and suggests, at least on a high level,      Table 1. Comparison of Si-CMOS and LAE (thin-film) transistors.  how functionality should be distributed, the critical challenge lies  in interfacing the technologies. This is especially true as the scale  of systems increases. Next, we describe a modular approach to  creating hybrid systems, and survey  the key  interfacing  technologies required. As we detail, these will raise the need for  selective functions implemented using TFT circuits.  3. MODULAR HYBRID SYSTEMS  The modular approach to hybrid systems presented is derived  from experiences experimentally demonstrating several hybrid  systems, including the following: (1) a self-powered strainsensing sheet for structural monitoring via densely-distributed  strain gauges [2]; (2) a three-dimensional human-gesture-sensing  sheet via large-area capacitance-sensing electrodes [3]; (3) a  multi-speaker voice sensing and separation sheet via a largeaperture microphone array [4]; (4) a form-fitting EEG sensing and  processing array via distributed thin-film low-noise amplifiers [5];  (5) a large-area image sensing and detection surface via photosensors and embedded classifiers [6]; and several others.   3.1 Functionality and Dimension Scalability  Generally, we recognize two ways in which applications are  driving system scaling. The first is scaling of functionality and the  second is scaling of physical dimensions.  In fact, these two are  intimately related; specific functionalities required within the  system are mapped selectively to specific technologies within the  hybrid architectures, and the technologies determine the physical  dimensions. For  instance, high-performance  functions are  delegated  to centralized Si-CMOS die; energy-harvesting  functions are delegated to LAE over dimensions dictated by  power requirements; and sensing functions are delegated to LAE  over dimensions dictated by the distribution of information  signals.   To address scaling of functionality and dimensions, we focus on  the modular, multi-sheet integration approach shown in Figure 2  [7]. This represents a generalization of the hybrid-system concept,  where a separation of functionalities is employed not only  between Si-CMOS and LAE, but also between various  functionalities within LAE when the type and physical dimensions  are dissimilar. We note that generally such a separation of  functionalities raises interfacing complexities (in terms of energy,  performance, assembly). However, given  the  range of  functionalities envisioned  in  future hybrid systems, such  modularity could prove critical from a fabrication perspective and  highly preferred from a design perspective. As an example, from a  fabrication perspective, even when  the materials for  two  functionalities are the same, their processing may be dissimilar  (e.g., a-Si TFTs and a-Si solar cells), or even when the dimensions  for two functionalities are the same, their materials and processing  may be dissimilar (e.g., a-Si TFT backplanes and organic LED  front planes, in flat-panel displays). Thus, a modular, multi-sheet  approach presents a path forward for hybrid systems, and yet  raises an important focus on the interfacing between sheets.          978-1-4673-9030-9/16/$31.00 (c)2016 IEEE  Figure 2. Modular, multi-sheet approach to hybrid systems [7].  3.2 Multi-sheet Assembly  With the multi-sheet approach suggested, a question that arises is  how to physically interface the sheets (i.e., mechanically and  electrically). Mechanically, lamination methods have proven  successful in many-layered bonded-display assemblies, and are  believed to scale to larger dimensions, more layers, and broader  types of layers (e.g., flexible displays). Electrically, large-area  and/or flexible systems today employ metallurgical bonds.  However, generally speaking there is no reliable high-volume  process for bonding many small Si-CMOS ICs on large, flexible  sheets. Today, such ICs are either bonded in small numbers or via  discrete flexible connectors, and their assembly typically poses  dominating cost and reliability concerns.   In hybrid systems involving a relatively small number of such  interfaces at a localized area, metallurgical approaches similar to  those used today or those being developed for flexible systems  present a path forward. However, hybrid systems where a large  number of distributed interfaces are envisioned may need to  consider an alternative. One alternative is the use of non-contact  interfaces based on planar inductors and capacitors patterned on  the sheets [2]. Such an approach is illustrated in Figure 3a. Here  small Si-CMOS ICs can be bonded onto a credit-card-sized  flexible carrier, for which high-volume processes do exist today  (e.g., as in RFID-based smart cards), and mating inductors and  capacitors can be patterned on this and the flexible LAE sheet  stack. The inductor/capacitor sizes, shown to be large on the order  of centimeters, are determined by the coupling requirements  between driver and load. Roughly speaking, for capacitors the  density achieved is 100pF/cm2, while for inductors, where there is  an important tradeoff with series resistance as described below  (i.e., for thin traces enable higher inductance but also high  resistance),  the  inductance-to-resistance  ratio achieved  is  400nH/Ωcm2 (with respect to the radius squared). Hence,  reducing the number of interfaces remains an important concern  for density.  (a)  (b)  Figure 3. Non-contact interfaces, where inductive coupling raises a  dependence on frequency.  Of course, with non-contact interfaces, a challenge is that signals  (for both power and information) must be AC modulated at the  transmitter, and potentially demodulated at the receiver. This          raises the need for specialized architectures, as well as the  supporting circuits and devices, as described in the next section.  However, within the architectures, important tradeoffs between  inductive and capacitive coupling must also be considered. From  the perspective of robustness, inductive coupling presents superior  tolerance to proximity and alignment variations at the dimensions  envisioned. From  the perspective of matching driver/load  requirements, inductors present the option of voltage/current step  up/down. However, from the perspective of power efficiency,  inductors suffer from larger loss mechanisms (series resistance)  compared to capacitors (parallel resistance). This raises the  dependence of power-transfer  efficiency on modulation  frequency, shown in Figure 3b. For reference, the maximum fT  typically achieved with a-Si TFTs is also shown. This suggests  that for inductive coupling, specialized topologies capable of  operating near or beyond the TFT fT are necessary.     4. INTERCONNECTION BACKBONE  With the modular, multi-sheet approach to hybrid systems  presented above, interfacing power, information, and control  signals becomes a critical aspect. This section explores  architectures for such interfacing, by dividing the interfaces into  three categories based on signal type: (1) sensor signals; (2)  processed-data signals; and (3) power signals.  4.1 Sensor Signals  Sensor signals originate within hybrid systems at LAE sensors.  These have the attributes that they can be (1) large in number and  distributed (exploiting the ability to create large, expansive arrays  of transducers in LAE), and (2) generally sensitive to noise due to  small amplitudes. Figure 4 shows key system elements we will  consider for interfacing these, namely scanning circuits and active  matrices, low-noise-amplifier circuits, and modulator circuits.  Scanning circuits are required because we envision a large  number of LAE sensors; however, the low-performance of LAE  TFTs prevents signal-processing and -analysis functions that are  required over these, necessitating interfaces to Si-CMOS ICs.  From discussions in Section 4, the number of such interfaces must  be constrained. A noteworthy option is to employ active matrices,  which enable sensor accessing through row and column signals,  thereby reducing the interfaces by roughly a square-root factor.  Indeed, this is done today in flat-panel displays, X-ray imagers,  etc. In fact, scanning circuits may be used in conjunction with  active matrices. However, generally, for the sensing applications  envisioned one aspect that could limit the use of active matrices is  that the optimal choice of sensor placement will be determined by  the distribution of information signals, which may not be regular,  as in the case of dense displays and imagers. In this case, row/column-accessing signals will present unsuitable  routing  complexities. Though scanning circuits can be more amenable to  irregular sensor placements, they present significant challenges  also. Most notably, time multiplexing of a large number of signals  for maximal scalability raises sampling rate constraints, which  may be too severe for Nyquist acquisition, considering the  bandwidth of information signals. As an example, typical  scanning speeds are in the range of 1-10kHz, due to the limited  performance of TFTs [8]. Later in this paper we will mention how  algorithmic methods may overcome this.  Low-noise amplifiers are required because generally LAE sensors  will be distributed but Si-CMOS ICs will be centralized. This  raises the need for long interconnects on the sensing sheet, which  will be susceptible to coupling of stray noise sources. The  challenge with embedding low-noise TFT amplifiers near the  978-1-4673-9030-9/16/$31.00 (c)2016 IEEE  Figure 4. Interfacing sensor signals in hybrid systems.  distributed sensors is that, generally, these will have significantly  lower power efficiency (transconductance efficiency) and higher  noise than Si-CMOS amplifiers [5]. Specifically, the higher noise  comes about due to 1/f noise sources originating from the  increased density of carrier traps in the semiconductor and  semiconductor-dielectric  interface of TFTs. For  illustration,  Figure 5 compares cases within a voice sensing-and-localization  system based on piezoelectric-film microphones [4], analyzing the  total noise with and without a TFT amplifier, as stray coupling  noise is increased. As seen, for low levels of coupling noise, the  total noise referred to the microphone is worse due to the TFT  amplifier; however, as coupling noise increases beyond a level of  150mVRMS, the TFT amplifier becomes preferable. Recently, the  possibility of employing circuit techniques, such as chopper  stabilization [5], to reduce the 1/f noise of TFT amplifiers has  been explored. While this comes at the cost of power efficiency,  analysis and experimental validation show  that significant  reduction of 1/f noise is possible and viable, making distributed  low-noise amplifiers a promising component in hybrid systems.        Modulator circuits are required if non-contact interfaces are  employed for transmitting sensor signals to another sheet. TFT  circuits based on Gilbert-multipliers have been used successfully  and are readily implemented [2]. These combine with scanning  circuits  for multi-sensor accessing, and  typically employ  capacitive interfacing due to the limited speed of TFT-based  Gilbert modulators. Further, the modulation signal from a SiCMOS IC can also be provided through capacitive interfacing.  This enables readout by the IC in a synchronous manner (i.e,,  using the same signal for modulation and demodulation), leading  to simplified topologies [2].     4.2 Processed Data Signals  Processed data signals arise in hybrid systems in the following  scenarios, summarized  in Figure 6: (1)  to communicate  information/control signals between Si-CMOS ICs and the LAE  domain for control and synchronization functions (sensor  instrumentation already having been addressed in Section 4.1); (2)  to communicate information/control signals between distributed  Si-CMOS ICs, for instance for data aggregation; and (3) to  communicate information/control signals between a multi-sheet  Figure 5. Analysis of noise in microphone acquisition system [4].      hybrid system and remote systems. We explore specific cases and  architectures for each scenario below.  Figure 6. Interfacing processed data signals in hybrid systems.  One example of communication between the Si-CMOS domain  and the LAE domain mentioned previously is the CMOS-derived  modulation signal feeding a Gilbert-cell for non-contact sensordata coupling. Another example alluded to is the control of  scanning circuits. This case highlights a common challenge for  voltage signals, which is that the voltage levels in the Si-CMOS  domain are typically much lower than those in the LAE domain.  LAE TFTs require large voltages for adequate drive current, due  to the lower mobilities and lower gate-channel coupling (because  of reduced gate-dielectric scalability). Hence, signals from the  LAE domain to the Si-CMOS domain are easily transmitted, for  instance with voltage step-down via capacitive coupling. For  signals from the Si-CMOS domain to the LAE domain, where  voltage step up is required, two options can be considered. First, if  a high-level power supply is available in the LAE domain, explicit  TFT level converters can be employed, as in [8]. However, in  systems where there is no such power supply, voltage step-up can  also be achieved via inductive coupling [2]. As before, this  necessitates AC modulation at the transmitter, which is easily  achieved in Si-CMOS ICs at frequencies where inductive losses  can be tolerable. To maximize the voltage swing, operating the  inductive interface at resonance is highly preferred. High resonant  frequencies to also minimize inductor losses require minimizing  the tank capacitance, which may for instance be formed by LAE  thin-film diodes for demodulating the control signals [2]. Hence  the design of such diodes, as in [9], where current density is  maximized and forward voltage-drop is minimized is crucial.  Even after these optimizations, the power loss in an inductive  interface is quadratic with inductor resistance, since this can be  represented as a parallel resistor at resonance. Further, since  voltage step-up is achieved by increasing the turns ratio, for a  given inductor size, the traces must be thinner, causing the  resistance itself to increase roughly linearly. As a result, the  overall power loss for a given inductor size is roughly cubic with  the desired voltage step-up [2].  Communication between distributed Si-CMOS ICs can exploit  LAE for long-range interconnects, avoiding the need for more  energy-intensive wireless communication. With metallurgical  connections, LAE interconnects can be driven directly with data at  baseband, incurring the energy of capacitive switching (e.g.,  CV2). With non-contact  interfaces, AC-modulation of  the  communication data is required to robustly drive the interconnects  (though schemes for driving data at baseband have also been  explored [10]). Inductive interfaces again benefit from modulation  at the resonant frequency of the interconnect network, determined  by the inductance as well as the coupled and self-capacitance.  Since these parameters are often difficult to know a priori,  topologies that self-calibrate to the resonant frequency have been  proposed [2]. As before, inductive coupling also offers the benefit  of selective voltage step-up/-down. For instance, voltage stepdown can be employed at the transmitter, in order to reduce the  voltage amplitude and thus the power loss incurred due to the  resistance of the long interconnects (which appears as a parallel  resistance a resonance); while, voltage step-up can be employed at  the receiver to maximize the receive SNR [2].   978-1-4673-9030-9/16/$31.00 (c)2016 IEEE  Finally, for communication between a multi-sheet hybrid system  and remote systems, wireless transceivers are necessary. For  these, LAE offers the benefit of large antennas, which lead to  improved radiation efficiency. For  the  transceiver circuits  themselves, where distributed functionality is typically not  required, Si-CMOS implementations are preferred due to the  higher performance and energy efficiency of transistors. We point  out that TFT-implemented transceivers have been explored to  create distributed arrays of frontends for wireless communication  [11]. While experimental demonstrations show that despite low  TFT performance, 10’s of meters range is possible by taking  advantage of the large antenna achievable in LAE, the energyefficiency is lower than Si-CMOS-implemented transceivers.    4.3 Power Signals  Transmission of power signals across multiple sheets is an  important aspect  in hybrid systems, especially  if energyharvesting capabilities of LAE are to be exploited. Given that  ambient power signals can be distributed in many applications,  several discrete, distributed energy harvesters may be needed.  While this could correspondingly raise the need for discrete  interfaces to other sheets, the numbers are likely to be small.  Unlike information signals from sensors, there is typically not a  need to maintain separation of the power signals from discrete  sources and susceptibility to noise is not as critical. Thus power  from distributed sources can be combined into a small number of  interfaces for transmission. This makes metallurgical bonding for  power interfaces somewhat more viable.   However, for systems requiring substantial scalability, for  instance by incorporating many, possibly distributed, Si-CMOS  ICs, the need for a large number of power interfaces to each IC  can arise [2]. Here, non-contact interfaces, avoiding metallurgical  bonds, may once again be preferred. As usual, AC signal will be  required for coupling. Given that many LAE harvesters output DC  power (e.g., solar cells, thermo-electrics), power converters  (inverters) must then also be implemented. Significant power  transfer then becomes challenging due to the low current-handling  capabilities of TFTs. For instance, topologies such as switching  converters are used extensively in Si-CMOS and other highperformance technologies because of their high power-conversion  efficiencies and large output currents; however, these are not  suited for TFT-implementation due to the low currents of TFT  switches and the typical lack of complimentary (NMOS/PMOS)  transistors required for efficient switch control [12].   An alternate topology, which has proved to be much more suitable  for LAE implementation is a free-running LC power oscillator  [13], such as that depicted in Figure 7. Because this circuit  requires no control circuitry, its power-transfer efficiency can be  near theoretical-maximum levels, even with TFT implementation.  The reason for this is that resonant operation depends strongly on  the quality of inductors, which can be high in LAE, thanks to the  ability to pattern physically-large planar coils (i.e., many turns for  high inductance and thick traces for low resistance). Further, the  use of inductors enables power transfer via inductive coupling,  and the ability to resonate with the TFT capacitances allows for  frequencies well beyond the TFT fT (instead limited by fMAX,  which can be much larger for TFTs) [14]. Inductive coupling has  the advantage that the low currents but high voltages of TFTs can  be converted to high currents at the voltage levels of Si-CMOS  ICs through the turns ratio, thereby enabling significant power  transfer, for fully-self-powered systems [2].        Figure 7. TFT-based LC-oscillator power inverter [13].   5. ALGORITHMS FOR INTERFACING  The primary focus of this paper has been on the platform  methodologies and components required for interfacing multisheet hybrid systems. While several device, circuit, and  architectural solutions have been presented, offering a range of  tradeoffs, we generally see that interfacing poses a fundamental  concern in the approach of hybrid systems. Further, the problem is  most pronounced for interfacing sensor signals, because most  importantly we would like to exploit the ability of LAE to create  large numbers of distributed sensors and the ability of Si-CMOS  to perform processing and analysis over the signals. In this  section, we briefly point to algorithmic concepts that have  recently emerged, possibly offering the potential to dramatically  reduce the number of interfaces between LAE sensors and SiCMOS ICs.     Specifically, the algorithmic approaches are driven by the  recognition that over very large numbers of sensors, individual  instances of data are likely to not be of high importance. Rather,  higher-level features or information over aggregated sets of  sensors are most likely to be of greatest value. This points to  methods from the domains of machine-learning and statistical  signal processing for creating representations of the sensor data  that reduce the interfacing requirements. Two examples are  described below.  5.1 TFT-based Classification  In [15], an approach for reducing the data from a large number of  sensors to a small number of inference decisions is presented. The  approach relies on a machine-learning algorithm knows as ErrorAdaptive Classifier Boosting (EACB). EACB  itself  is an  extension of another machine-learning algorithm knows as  Adaptive Boosting (AdaBoost). In AdaBoost, a strong classifier,  able to fit to arbitrary data distributions, is formed from multiple  weak classifiers, unable to fit arbitrary data distributions. Training  of the weak classifiers is performed iteratively, at each step  biasing the training set to emphasize instances affected by fitting  errors. EACB, extends this by biasing the training set not only  based on fitting errors, but also based on errors due to non-ideal  implementation of the weak classifiers. In this way, errors due to  the large TFT variations typically encountered are overcome  through data-driven training. The weak classifier decisions are  then provided to the Si-CMOS domain for making strong  classifier decisions. Figure 8 shows the architecture of the system.  TFT-based  linear classifiers are  formed using a pseudo  differential, two-transistor stack, appropriately biased, to provide  an approximation to multiplication between a sensor input and a  weight derived from training (then stored via non-volatile charge  trapping in the bottom TFT). On the right, the representative  performance is shown for detecting ‘L’ shaped images (from a set  containing five different shapes) from LAE photo-sensors over  multiple EACB iterations.   978-1-4673-9030-9/16/$31.00 (c)2016 IEEE  Figure 8. TFT-based weak classifiers used with EACB to achieve  performance [true-positive (tp) and true-negative (tn) rates] near a  MATLAB-implemented support-vector-machine (SVM) classifier[15].  5.2 TFT-based Compression  In [5], a system is presented for acquiring EEG data from an array  of flexible electrodes. For providing the data to a Si-CMOS IC for  further processing, it is preferable to multiplex signals from all the  electrodes to a single interface, in order to address assembly  complexity and robustness. However, TFT-based scanning  circuits are limited to a frequency of  ~10kHz [7]; with each EEG  channel having a bandwidth of ~2kHz after practical filtering, the  number of channels that can be support is very small, considering  Nyquist-sampling requirements. Instead, the system exploits  compressive sensing, using analog modulation via TFT circuits to  acquire pseudo-random measurements over an epoch of data.  Given that EEG exhibits sparsity in a Gabor basis, sparse  reconstruction can be used to recover estimates of the original  signals. However, the system in [5] also explores the extraction of  spectral-energy features, which are a widely employed biomarker  in EEG, directly from the compressed samples. This is done via  linear estimation of the original signal followed by linear filtering.  Experimental results show that the error from linear estimation is  substantially  tolerable when  the features are subsequently  employed for classification, such as detection of epileptic  seizures.  Specifically,  seizure-detection  performance  is  maintained out to compression rates in excess of 60×.   6. CONCLUSIONS  This paper focused on the approach of hybrid systems. Hybrid  systems combine LAE, which can serve as a platform technology  for sensing, with high-performance technologies such as SiCMOS, which can severe as a platform technology for other  functionalities  required within a system  (instrumentation,  computation, power management, communication). Though  hybrid systems thus provide a path to implement complete  systems, they face the challenge of interfacing signals between the  technology domains. This paper examined the challenges and  possible solutions for interface three categories of signals: (1)  sensor signals; (2) processed data signals; and (3) power signals.  In all cases, circuits and subsystem architectures capable of  addressing the interfacing needs at scale were explored, with  respect to the various tradeoffs they face. Looking beyond the  circuit and architectures, algorithmic concepts for easing the  interfacing requirements (especially for sensor signals) were  examined. Experimental demonstrations show that the various  interfacing strategies are promising for a broad range of hybrid  systems, raising the possibility for such systems to address a range  of compelling new applications.     7. ACKNOWLEDGMENTS  This work was supported by NSF (grants ECCS-1202168 and  CCF-1218206) and Systems on Nanoscale Information fabriCs  (SONIC), one of six SRC STARnet Centers, sponsored by  MARCO and DARPA.        8. "
Bubble budgeting - throughput optimization for dynamic workloads by exploiting dark cores in many core systems.,"All the cores of a many-core chip cannot be active at the same time, due to reasons like low CPU utilization in server systems and limited power budget in dark silicon era. These free cores (referred to as bubbles) can be placed near active cores for heat dissipation so that the active cores can run at a higher frequency level, boosting the performance of active cores and applications. Budgeting inactive cores (bubbles) to workloads to boost performance has the following three challenges. First, the number of bubbles varies due to dynamic workloads. Second, communication distance increases when a bubble is inserted between two communicating tasks, leading to performance degradation. Third, budgeting too many bubbles as cooler to running applications leads to insufficient cores for future applications. In order to address these challenges, in this paper, a bubble budgeting scheme is proposed to budget free cores to each application so as to optimize the throughput of the whole system, including the execution time of each application and the waiting time incurred for newly arrived applications. Essentially, the proposed algorithm determines the number and locations of bubbles to optimize the performance and waiting time of each application, followed by tasks of each application being mapped to a core region. Experiments show that our approach achieves 50% higher throughput when compared to state-of-the-art thermal-aware runtime task mapping approaches.","Bubble Budgeting: Throughput Optimization for Dynamic Workloads by Exploiting Dark Cores in Many Core Systems Xiaohang Wang South China University of Technology Email: xiaohangwang@scut.edu.cn Amit Kumar Singh University of York Email: amit.singh@york.ac.uk Bing Li South China University of Technology Email: l.b07@mail.scut.edu.cn Yang Yang Sun Yat-sen University Email: yangy266@mail.sysu.edu.cn Terrence Mak University of Southampton Email: tmak@ecs.soton.ac.uk Hong Li South China University of Technology Email: hongli@scut.edu.cn Abstract—All the cores of a many-core chip cannot be active at the same time, due to reasons like low CPU utilization in server systems and limited power budget in dark silicon era. These free cores (referred to as bubbles) can be placed near active cores for heat dissipation so that the active cores can run at a higher frequency level, boosting the performance of active cores and applications. Budgeting inactive cores (bubbles) to workloads to boost performance has the following three challenges. First, the number of bubbles varies due to dynamic workloads. Second, communication distance increases when a bubble is inserted between two communicating tasks, leading to performance degradation. Third, budgeting too many bubbles as cooler to running applications leads to insufﬁcient cores for future applications. In order to address these challenges, in this paper, a bubble budgeting scheme is proposed to budget free cores to each application so as to optimize the throughput of the whole system, including the execution time of each application and the waiting time incurred for newly arrived applications. Essentially, the proposed algorithm determines the number and locations of bubbles to optimize the performance and waiting time of each application, followed by tasks of each application being mapped to a core region. Experiments show that our approach achieves 50% higher throughput when compared to state-of-the-art thermal-aware runtime task mapping approaches. I . IN TRODUC T ION Many-core chips are widely used in servers, datacenters, clusters to provide high throughput computation services. In such systems, applications or user requests dynamically arrive and leave the system with various workload characteristics. One phenomenon observed in such high-performance many-core system is that, there are plenty of free cores which are either not utilized or even shut down from time to time. We have referred these free and powered-off cores as dark cores or bubbles. Free cores exist due to two reasons. First, in datacenters, the CPU usage is lower than 100% at most of the time, the average CPU utilization is as low as 50%, as shown in Figure 1 [7]. Therefore, some cores are not running useful applications at certain time period. Second, the high density integration of chips leads to a possible dark silicon issue [11], where a large portion of the cores have to be turned off to meet the thermal and power constraints. Several efforts have been made to exploit the bubbles (dark cores) to boost performance of active cores and applications, by determining the number, position, and voltage/frequency levels of the active cores This research program is supported by the Natural Science Foundation of China No. 61376024 and 61306024, Natural Science Foundation of Guangdong Province No. S2013040014366 and 2015A030313743, Basic Research Programme of Shenzhen No. JCYJ20140417113430642 and JCYJ20140901003939020, Special Program for Applied Research on Super Computation of the NSFC-Guangdong Joint Fund (the second phase), and the Science and Technology Research Grant of Guangdong Province No. 2016A010101011. 978-1-4673-9030-9/16/$31.00 c(cid:2)2016 IEEE Fig. 1. Week Workload CPU Demand Trace, from [7]. [13], [18]. An active core can run at a higher level of frequency if bubbles are located near it for heat dissipation. This helps to achieve higher performance while meeting the temperature constraint. However, in a server system with dynamic workloads, the following challenges need to be addressed so as to optimize the overall system performance. First, for a system handling dynamic workloads, since the number of available/free cores changes with the arrival and departure of applications, the position of bubbles and voltage/frequency of active cores need to be adjusted at run-time under the temperature constraint in response to arrival and departure of applications. Most of the approaches (e.g., [13], [18]) consider static workloads only, i.e., a ﬁxed set of applications known in advance and ﬁxed number of bubbles, which does not reﬂect the dynamic feature of several systems, e.g., a server. Next, communication overhead between the active cores executing communicating tasks is largely affected by placing bubbles near them. Communication distance between two tasks increases if the corresponding two cores have bubbles (dark cores) inserted between them for heat dissipation. Therefore, although active cores can possibly run at a higher frequency level if bubbles are placed near them, the applications might suffer from increased communication overhead, resulting in poor performance. Existing approaches (e.g., [13], [18]) ignore such communication overhead. Furthermore, if most of the bubbles are placed near active cores and used for heat dissipation, a newly arrived application might need to wait for a longer time due to insufﬁcient free cores. Therefore, the decision of whether a free core should be shut down for heat dissipation as a bubble, or to be turned on to run tasks affects both the execution time of current application and the possible waiting time for future applications. Existing approaches ignore waiting time incurred for each newly arrived application, which also affects the overall system throughput. Contribution: This paper addresses the aforementioned challenges by proposing a lightweight dynamic resource management approach that handles dynamic workloads, where applications containing dependent tasks arrive at different moments of time. This work tries to determine the number and location of both free and active cores, so as to optimize performance, communication cost and waiting time. The main contributions of the approach are as follows: 1) We propose performance and waiting time models targeting dynamic workloads, where applications arrive and depart in the system at different times. Therefore, the number of free cores vary in the system. These models can be updated online. 2) We propose an online algorithm to select the number and locations of free cores for each application. Instead of optimizing each individual application’s performance, this algorithm tries to optimize the system throughput in terms of number of executed applications within a given time, which depends on the waiting time for each newly arrived application and the execution time of each application. Both computation and communication performances are optimized when determining the number and location of bubbles and active cores. I I . R E LAT ED WORK Allocating system resources to the tasks of multiple applications on on-chip many-core system has been an emerging research direction [25]. Several resource allocation approaches have been proposed while following different policies. Most of these approaches map communicating tasks of each application close to each other such that communication overhead and power are reduced [2]–[4], [6], [17], [24]. Some of these approaches also reduce computation power of the cores by employing voltage/frequency scaling [4]. However, these approaches do not consider a power budget for the whole chip, which is desired in the dark silicon era. There has been some efforts to perform the mapping by taking the power budget into account [14], [21]. Some of these efforts just try to respect the power budget, whereas others try for the thermal design power budget, which guarantees reliable operation for given thermal characteristic of the system [21]. However, considering only the power budget in the mapping process may result in thermal violations, which deteriorate performance and reliability of the system [18]. Therefore, temperature of the cores need to considered to avoid the thermal violations. Thermal-aware resource allocation approaches have been explored to reduce peak temperature and temperature gradient while directly considering the temperature of cores [5], [19]. However, these approaches do not impose any thermal constraint in the allocation process. Some approaches considering thermal constraint while optimizing for the performance have been reported [9], [20]. However, in [9], heat conductance amongst the neighboring cores is ignored to simply the problem and [20] considers only one application. In general, there is lack of resource allocation approaches considering multiple applications. Further, these thermal-aware resource allocation approaches do not consider dark silicon problem. To address dark silicon problem while considering multiple applications, recently, some resource allocation approaches have been introduced [16], [18]. The approach in [18] identiﬁes the number, location and voltage/frequency levels of active cores for each application to optimize the overall system performance [18]. However, static workload has been assumed, i.e., a ﬁxed set of applications are allocated at the same time and thus the number of active/dark cores are ﬁxed. For dynamic workloads, the number of active/dark cores will vary depending upon the arrival and departure of the applications. . 1 p p A Applications t1 t2 t3 . 2 p p A t1 t2 . . . N . p p A t2 t1 t4 r ) e g a n a s n o i t a c i l t M p p e A c o r u s e o c s e u o s e R r r m t e r a c o o f t l l a A l ( P t3 e r o C e r o C . . . e r o C t c e n n o . c r . e . n t I e r o C e r o C . . . e r o C Arrival Time Many-core Platform Fig. 2. System model. Further, applications containing dependent tasks are not considered and thus communication overhead between the active cores containing the communicating tasks is ignored. In such cases, the appropriate location of active cores not only help to optimize the peak temperature but also the communication time/performance. In [16], [27], dynamic workload is considered and the approach aligns active cores along with dark cores that can evenly distribute heat dissipation across the chip. However, the distribution of active/dark cores amongst multiple applications at the same time is not considered, which might degrade overall system throughput. Our approach addressed above concerns by appropriately identifying the number and location of active cores for the applications containing dependent tasks and arriving at different moments of time as a dynamic workload. I I I . SY S T EM MOD E L AND PROB L EM D E FIN I T ION Figure 2 shows our target system model. The system contains a many-core platform that executes a set of applications arriving at different moments of time. The applications are submitted to the platform resource manager that allocates resources to them. This section provides a brief overview of the platform, workload and thermal power capacity model along with the problem deﬁnition. A. Many-core Platform Model The many-core platform contains a set of cores connected by an interconnection network, which is modeled as a 2D mesh network with bidirectional links. The right hand side of Figure 2 shows an example platform. Each core consists of a processing unit, a cache and a network interface. It is represented as a directed graph G(T , L), where T is the set of cores and L represents the connections amongst the cores. The application allocation and resource management is done by a centralized platform resource manager. B. Application Model Each application i is represented as a directed graph AGi = (Ai , Ei ), where Ai is the set of tasks of the application and Ei is the set of directed edges representing dependencies amongst the tasks. The left hand side of Figure 2 shows some example application graph models. Each task a ∈ Ai has a weight: execution time (ExecTime), when mapped onto a core. The ExecTime for each task is considered as its worst-case execution-time (WCET) and remains ﬁxed at a given frequency. Each edge e ∈ Ei represents data volume communicated between the dependent tasks. A mapping function M (a) = t, for a ∈ Ai , t ∈ T binds tasks to the cores, such that task a is mapped to core t. Each edge e ∈ Ei has a weight of transmission time, when the two communicating tasks are mapped. The transmission time between two tasks depends on the communication distance between the cores on whom they are mapped and the trafﬁc volume. For each edge e = (ai , aj ), the transmission time T (e) = f (v(ai , aj ), D(M (ai ), M (aj ))), where v(ai , aj ) is the trafﬁc volume between the two tasks ai and aj , and on whom tasks ai and aj are mapped. The function f (·, ·) models D(M (ai ), M (aj )) is the distance (hop counts) between two cores                           the transmission time versus the trafﬁc volume and the hop count distance of the two tasks, which can be found by a linear regression as follows. to be allocated to each application, together with the task-to-core mapping of each application. The response time of each application is computed as follows: T (e) = α · v(ai , aj ) + β · D(M (ai ), M (aj )) (1) σi = Ai ﬁnish − Ai arrive (4) (5) where α and β are regression coefﬁcients. The transmission time model can be trained ofﬂine by transmitting packets to measure the latencies. The execution time of each application i is the makespan of task graph, denoted as ETi . set of bubbles Bi = {t1 , t2 , ...} are also associated with application The set of all existing free cores in the system is denoted as Γ. A i, where t1 , t2 , ... are powered off cores for cooling. C. The Thermal Power capacity Model We deﬁne the thermal power capacity (TPC) of a core as the maximum power the core can consume given the power distribution of other cores, such that the whole chip’s maximum temperature and thermal gradient do not exceed their respective thresholds. The TPC of each core can be determined at ofﬂine. In the rest of the paper, we use PM (np ) and PM (x, y) to denote the power capacity of the core np at the location (x, y) interchangeably. The TPC of a core is bounded by the cooling capacity of the system, and the power consumption or temperature of other cores, i.e., thermal correlation. The cooling effect of the system can be modeled by the thermal RC circuit as given in [12]. The thermal correlation, indicating the inter-dependency of the temperature of different cores, can be modeled by a linear regression [15]. The temperature T t+1 at time instance t + 1 of a core located at (x, y) can be determined by the temperature values of those cores located at (x ± l, y ± l) at time t [15], x,y T t+1 x,y = φ(T t (2) where φ(·) is a linear function, and l can be 0, 1, representing core (x, y)’s neighboring cores. Similarly, the TPC of a core np can be found as, x±l,y±l ) q αq · P (nq ) PM (x, y) = θ(P (x ± l1 , y ± l2 )) = (3) where P (x ± l1 , y ± l2 ) is the power consumption of the core nq located at (x ± l1 , y ± l2 ), which is thermally correlated with np . The function θ(·) can also be found by autoregressive model (AR), using the lasso method [10]. In particular, for each core at (x, y) we only keep the coefﬁcients of adjacent cores as non-zero. That is, (x ± l1 , y ± l2 ) with l1 and l2 equal to 0, 1, i.e., cores that are neighboring to the core (x, y). These cores have the highest thermal correlations with the core (x, y). We set the coefﬁcients of other cores to be 0, for core i. The frequency fj of a core j can be determined according to the power-frequency proﬁle of a speciﬁc CPU [23]. That is, given a maximum power consumption threshold, select the highest frequency such that the power consumption does not exceed the threshold. When a core containing a task has increased frequency, the task’s execution time changes accordingly. Since dynamic power consumption is proportional to frequency, the increase in TPC PM (x, y) by inserting a nearby bubble leads to the same percentage of decrease in the execution time of the task running on core (x, y). D. Problem Statement Within a given time period, for N applications arriving at different moments of time, the objective is to minimize the response time of each application in order to optimize throughput that is computed as the number of applications executed within a ﬁxed amount of time. The decision variables are the position and number of the bubbles (cid:2) arrive and Ai ﬁnish where, σi is the response time of application Ai , Ai are the arrival time and the ﬁnishing time of application Ai . For each application, its response time is related to both the execution time and the waiting time. Waiting occurs when an application arrives at the system but there are no sufﬁcient cores to run it. Execution time is related to both the communication and computation performances of the application. The response time of running N applications within a given time is then computed as: σ = AN ﬁnish where N is the number of applications arrived at the system within a given time, and AN ﬁnish represents ﬁnishing time of N th application within this given time. The objective is to min σ (6) The constraint is that, the temperature of the chip should be under a threshold. IV. PRO PO S ED DYNAM IC R E SOURC E A L LOCAT ION A P PROACH A. Overview Fig. 3 shows the overview of our proposed approach. Applications dynamically arrive in the system. The bubble count (number of bubbles) included in each application’s core region (the region including active cores and bubbles) is used as a control variable, which determines both the communication distance and the running frequency of the active cores such that the system thermal constraint is not violated. A virtual mapping process is ﬁrst called to estimate the performance of each application when using different number of bubbles. For each application, core regions with different numbers of bubbles are selected, such that the region’s core count is possibly larger than the number of tasks in the application. The tasks of the application are mapped virtually to this core region in order to estimate the performances given different bubble counts (bn ) for the application, i.e., the table from the performance model achieved as shown in Fig. 3. The running frequency of each active core can be determined to conﬁne to thermal constraint. During virtual mapping, no task is running on the cores, i.e., the tasks are not actually mapped to the cores. The waiting time model also generates a table indicating the waiting time given different bubble counts. Finally, during the real or ﬁnal mapping, the bubble count for each application is chosen which can result in the minimum application execution time (including communication and computation performances) and waiting time. Once the application ﬁnishes execution, the cores in the region is released and send back to the available resource pool. The reason to use the virtual mapping is as follows. The communication performance of each edge depends on the distance of the two cores running the communicating tasks, and the computation performance of each task depends on the frequency and TPC of each core, which is affected by the bubble count and location. Therefore, the calculation of execution time of an application requires knowing the task-to-core mapping scheme. To ﬁnd the core region with the optimal number of bubbles, we need to consider both the execution time and waiting time of each application with different number of bubbles (the decision variable). Virtual mapping serves for this purpose. It iterates the bubble number 0, 1, ..., min{|Ai |, Γ} for each Choose  the best  bn fo r  min  res ponse  t im e Secti on 4.5 WT Wa iting time 100 . . . 3000 ET Exe cution time 200 . . . 120 W ait ing t ime m odel Secti on 4.2 Secti on 3.3 Therm al power  capaci ty model bn 1 . . . min{|Ai|, Γ}  Secti on 4.4 V ir tual  mapp ing  Secti on 4.3 Per fo rm ance m odel bn 1 . . . min{|Ai|, Γ}  MS bn 1 . . . min{|Ai|, Γ}  Mapping results . . . Mapp ing r esu lts database Final (real) mapp ing  res ul ts Fig. 3. Overview of the proposed approach. application to generate the performance and waiting time models as shown in Fig. 3. The waiting time and performance models are stored in tables whose entries are < bn, W Ti > and < bn, ETi >, respectively. That is, given bn bubbles to be inserted into the application i, the two tables return the corresponding expected waiting time and execution time of the application. The mapping scheme with bn is also stored in a database. Based on the two models, during the ﬁnal or real mapping, the system can choose the best bn value (bubble number) in the core region and the corresponding mapping scheme from the database for each incoming application, which can result in the minimal expected response time. The various steps of the proposed approach are introduced in subsequent sub-sections and highlighted in Fig. 3. B. Waiting Time Estimation We target server systems whose workloads exhibit periodical behaviours [7], such that we can predict the waiting time from history data. In many server systems, there are some peak time when the CPU utilization is close to 100%, and some off-peak time when there are fewer running applications. In addition, waiting time depends on various system parameters including the application arrival rate (average number of applications arriving in the system per cycle), system size, and how many free cores are used as bubbles. The waiting time can thus be modeled by a polynomial regression model as in Eqn. (7), where |T | is the network size, |Ai | is the average number of tasks in each application, r is the average percentage of bubble count in an application’s core region, deﬁned as bubble count divided by the core count in each application’s core region, h is the average execution time of the tasks, and λ is the average application arrival rate. Using this model, r can be a decision variable such that, when the waiting time is estimated to be high, a smaller r is preferred. z(cid:2) ηi = z(cid:2) z(cid:2) z(cid:2) ci · |T |j + dj · |Ai |j + ej · rj + j=1 j=1 j=1 gj · λj + a0 j=1 z(cid:2) j=1 fj · hj + (7) To ﬁnd the coefﬁcients of c, d, e, f , g ’s, the maximum likelihood methods can be used [10]. C. Performance Estimation To estimate the performance of each application, we need know the communication performances of the edges and the computation performances of tasks in the task graph. These performances can only be determined after the tasks are mapped to cores. The number of bubbles in a core region is an important control variable which is related to both the communication distance and the computation power of each core/task. Given a virtual mapping of tasks to a core region with j bubbles, the execution time of each task and transmission time of each communication edge can be determined as in Sections III-B and III-C. The execution time of each task is related to the instructions to be executed and the running frequency and power of the core while satisfying the thermal constraint, which can be derived from Section III-C. The communication time of each edge in task graph can be determined by Eqn. 1. The performance of the application (referred as makespan) can be determined by ﬁnding the maximum execution path along the application’s task graph. Therefore, the performance estimation needs the virtual mapping algorithms which will be introduced in Section IV-D. The output of the performance model as shown in Fig. 3 is a table ET where each item ET[j] is the execution time with j bubbles. D. Virtual Mapping Algorithms During the mapping process, we virtually ﬁnd core regions whose core count equals to |Ai | plus j bubbles, where j = 0, 1, 2, ..., min{|Ai |, Γ}. At each iteration with j bubbles, the applications are virtually mapped to the core region and the execution time is stored in the performance model table. Once the iteration stops, the performance model generates a table indicating the execution times with j bubbles, where j = 0, 1, 2, ..., min{|Ai |, Γ}. The corresponding mapping schemes with up to j bubbles are also stored in a database. Note that, this process only virtually maps the tasks to the cores to get the performance model table and the mapping scheme database as shown in Fig. 3. Tasks are not actually bound to and run on the cores. No migration is involved. Other running application is intact. The virtual mapping process has two objectives, i.e., minimizing the communication distance and maximizing the computation frequency/performance of the tasks. These two objectives might be contradicting in the sense that, communication distance is minimal when tasks are mapped in close proximity, while each task’s frequency or computation performance is maximized when the temperature is low indicating hot tasks are distant from each other. We propose a heuristic based virtual mapping algorithm, where the two optimization objectives are tried to be achieved simultaneously. Algorithm 1 shows the virtual mapping ﬂow. At each iteration with j bubbles, the tasks are mapped to a core region of size |Ai | + j . The results are the two lookup tables ET and MS, where ET[j] returns the execution time when inserting j bubbles and MS[j] returns the best virtual mapping scheme when inserting j bubbles, respectively. Our proposed virtual mapping algorithm has the following steps. 1) Determine the computation to communication rate (CCR), which is deﬁned as the average computation workload (instructions to be executed) divided by the data volume to be sent in one application. 2) If CCR is over a threshold, call the computation biased virtual mapping sub-routine. Otherwise, call the communication biased virtual mapping sub-routine. A larger CCR indicates each task computation performance contributes more to the overall application performance, while a small CCR means communication has more contribution to the application performance. Based on the CCR value, two virtual mapping sub-routines are called which are computation and communication biased, respectively. Both of the two mappings have two steps as follows. An initial mapping is set up ﬁrst, followed by an iterative ALGORITHM 1: Online Virtual Mapping Input: j : The bubble number. Output: ET[j]: The execution time when inserting j bubbles. MS[j]: the best mapping scheme when inserting j bubbles. Function: Find the best virtual mapping scheme and the execution time for an incoming application given the bubble number is j , where 0 ≤ j ≤ min{|Ai |, Γ}. if CCR < T hreshold then Call the Communication Biased Virtual Mapping Sub-routine; begin end else end end Call the Computation Biased Virtual Mapping Sub-routine; replacement procedure to optimize computation and communication performances. The inputs to both of the virtual mapping subroutines are 1) the task graph of the incoming application, 2) the available cores in the system, and 3) the bubble number j , where j = 0, 1..., min{|Ai |, Γ}. 1) Communication Biased Virtual Mapping Sub-routine: Algorithm 2 shows the communication biased virtual mapping sub-routine. a) Initial Mapping: In the initial mapping, the objective is set to be minimal communication distance. A convex core region is ﬁrst found, followed by tasks with larger communication volume mapped in closer proximity virtually. The mapping algorithm in [8] is used as the initial mapping with minimal communication distance as the optimization objective. b) Inserting Bubbles: In each iteration, j bubbles are virtually tation performance of certain tasks, where j = 0, 1..., min{|Ai |, Γ}. inserted into the core region of this application to boost the compuThe application’s core region is bounded by a convex hull. At each iteration with j bubbles, ﬁrst, a location (x1 , y1 ) inside the current convex hull is found, then a location (x2 , y2 ) outside the convex hull is found that is adjacent to its boundary, and has the minimum distance to (x1 , y1 ). The bubble is virtually moved from (x2 , y2 ) to (x1 , y1 ) using the path migration algorithm in [22]. As an example, Fig. 4 shows the process of inserting two bubbles iteratively. At each iteration, when a new bubble is to be inserted, each task is selected as the candidate to be replaced by the bubble. A bubble with the minimal distance to each task is virtually replaced with the task. Then, the maximum power/thermal budget and frequencies of the cores running the tasks are updated following the thermal power capacity model. After determining the frequency of each core and the communication distance of each edge in task graph, the computation and communication performances are updated following the application model in Section III. The task replacement with the minimal execution time is recorded. For example, in Fig. 4, in the ﬁrst step to insert one bubble, suppose replacing task 1 with a bubble leads to the minimal execution time. So task 1 is moved to the location of the bubble. The region is enlarged each time a bubble is inserted. 2) Computation Biased Virtual Mapping Sub-routine: Algorithm 3 shows the computation biased virtual mapping sub-routine. a) Initial Mapping: If the task computation performance contributes more to the application performance, the initial mapping begins with a region of min{2× |Ai |, Γ} cores, where |Ai | or Γ cores are powered off as bubbles. The tasks are sorted by their weight (each Task graph 2 1 4 3 Convex hull Bubbles Active Node Busy Node Nodes in  other regions 1 Adding one bubble 1 2 3 4 3 4 1 2 Initial mapping Adding two bubbles 1 2 3 4 1 2 3 4 Fig. 4. Adding bubbles virtually to get the expected maximum speedup. ALGORITHM 2: Communication Biased Virtual Mapping Subroutine Output: ET[j]: The execution time when inserting j bubbles. MS[j]: the best mapping scheme when inserting j bubbles. Function: Find the best mapping scheme and the execution time for an incoming application given the bubble number is j , where 0 ≤ j ≤ min{|Ai |, Γ}. begin /* Inital Mapping Map the tasks with communication-awareness by using [8] without bubble insertion; ET[j] = INFINITY; // Recording the best performance for j = 0, ..., min{|Ai |, Γ} do /* Inserting Bubbles = 0, 1, ..., min{|Ai |, Γ}, start with the hotest location */ for each active core tk inside the core region do /* k */ */ Find a bubble b on the boundary of the core region returned by the mapping with the minimal distance to tk ; Virtually move b to tk using [22]; Update the performance Ex; if Ex < ET[j] then ET[j] = Ex; Virtually migrate b to tk using [22] and update MS[j]; end end end end node’s worst case execution time in the task graph) in descending order. The tasks are mapped as distant as possible to each other. The mapping can be done as follows. For each unmapped task ai in the (cid:3)i−1 sorted list, ﬁnd a core t with maximal distance to the mapped tasks, i.e., k=1 D(M (ak ), t). D(M (ak ), t) is the distance of the core to a previous virtually mapped task. This equation ﬁnds the core that has the maximum distance to those running the virtually mapped tasks. ALGORITHM 3: Computation Biased Virtual Mapping Subroutine Output: ET[j]: The execution time when inserting j bubbles. MS[j]: the best mapping scheme when inserting j bubbles. Function: Find the best mapping scheme and the execution time for an incoming application given the bubble number is j , where 1 ≤ j ≤ |Ai |. begin /* Inital Mapping Find a core region with size of min{2 × |Ai |, Γ}; for each unmapped task ak do Virtually map ak to core t such that t has the maximum distance to other mapped tasks; */ end ET[j] = INFINITY; // Recording the best performance for j = 1, ..., |Bi | do /* Removing Bubbles for edge ek = (am , an ) do Virtually move an to tk , i.e., a core clostest to M (am ) using [22]; Update the performance ET; if ET < ET[j] then ET[j] = ET; */ Virtually migrate an to tk using [22] and Update MS[j]; end end end end Edge 1(cid:252)3, fix task 1 bubbles 3 4 1 2 3 1 2 4 Initial mapping Moving 3 toward 1 Edge 2(cid:252)4, fix task 4 3 1 2 4 Moving 2 toward 4 1 3 2 4 Fig. 5. Migrating bubbles virtually to optimize the communication distance. b) Removing Bubbles: To get the performances with different bubble counts for each application, the bubbles are virtually migrated out from the initial mapping region one by one at each iteration. The communication edges in the task are sorted by their volume in descending order. For each edge e = (am , an ), an is migrated to a free core virtually and the application performance is recalculated. If the performance is improved, an is virtually migrated to that free core and the bubble is migrated to the original location of an . Then, the bubble is excluded from the application. Fig. 5 shows two steps of virtually migrating task 3 towards task 1, and tasks 2 towards task 3, respectively. After virtually migrating one task to a bubble, the original core hosting it is excluded from the region of this application. Then, the computation and communication performances are updated following the application model at each iteration. 3) Complexity Analysis: The worst case complexity of the virtual mapping process can be analyzed as follows. In the communication biased virtual mapping algorithm, the initial mapping step has a complexity of O(|Ai |2 ·|Ei |·|T |) [8]. In the second step, the algorithm has to iterate up to |Ai | times, corresponding to the bubble count. For each bubble count j , it takes O(|Ai |2 ) steps to virtually migrate the tasks. In the computation biased virtual mapping algorithm, the initial mapping step has a complexity of O(|Ai |2 · |T |). In the second step, it also has to iterate up to |Ai | times, corresponding to the bubble count. For each bubble count j , it takes O(|Ei |) steps to virtually migrate the tasks. Overall, the worse case complexity is O(max (cid:5)Ai |2 · |Ei | · |T |). E. Choosing the Best Number of Bubbles Given the waiting time and the performance models versus bubble count, we can determine the number and locations of bubbles for each incoming application such that the overall system performance is optimized. To achieve the same, the following two steps are performed. First, using the above two models, we can select the number of bubbles |Bi | for each application i with the minimum sum of execution time and waiting time, i.e., min{ETi + ηi }, with 0 ≤ |Bi | ≤ min{|Ai |, |Γ|}, where |Γ| is the total number of free cores. Second, with a bubble count of |Bi |, the mapping results can be retrieved from the database MS[|Bi |] as shown in Fig. 3. V. EX P ER IM EN TA L EVALUAT ION A. Experimental Setup Experiments are performed on an event-driven C++ simulator, with DSENT integrated as the power model and Hotspot is used as the temperature simulator. Task graphs are modeled in this simulator, which can dynamically arrive at the system. The simulator system has a network simulator which can model the package delay and energy of the communications in a cycle accurate manner. The conﬁguration of the network-on-chip is listed in Table I. The many-core system ﬂoorplanning can be found in [26]. The temperature threshold is 60 oC. We compare our approach with the following two runtime thermalaware mapping algorithms that aim to dark silicon era, (1) DsRem [18], where the cores on/off patterning are identiﬁed followed by tasks mapped to active cores, and (2) PAT [16], where a core region including inactive cores is found for each application. Both random and real applications are used in the experiments as tabulated in Table I in order to evaluate the performance of the proposed and relevant algorithms considered for comparison. In particular, we compare throughput (deﬁned as the average number of applications ﬁnished within a time unit), communication cost, and average waiting time for each application which occurs when there is insufﬁcient cores to run the tasks that arrive in the system at run-time. The communication cost is deﬁned as the network energy consumption, which is be measured by DSENT. The run-time execution costs of the algorithms are also evaluated. B. Validation of the Estimations For errors in the waiting time estimation, Fig. 6 compares the linear regression and polynomial regression models. Fifty experiments are run with |T |, |Ai |, r , h and λ set randomly. The error of a single experiment is deﬁned as, |W T − (cid:4)W T | × 100% ε = W T (8) where W T and (cid:4)W T are the waiting times obtained from the simulator and the waiting time estimate model, respectively. From this ﬁgure, one can see that quartic regression has the lowest error. Therefore, in the following experiments, we use the quartic regression model as the waiting time estimation. This indicates that the maximum order of the terms in Eqn. 7 is four. TABLE I S IMU LAT ION CON FIGURAT ION S Network parameters 128 bits Router 2 cycles, link 1 cycle 4 ﬂits XY routing 8 × 8 Random benchmark parameters Flit size Latency Buffer depth Routing algorithm Baseline topology Number of tasks Communication volume Degree of tasks Task number distribution [15, 45] [10, 200] (Kbits) [1, 15] Bimodal, uniform Real benchmarks AES enc, AES dec [28], E3S [1] 40 30 20 10 0 Errors of the waiting time estimation model (%) linear quadratic cubic quartic quintic Fig. 6. Errors of different regression models C. Finding the CCR Threshold Our approach (Algorithm 1) calls different sub-routines based on the CCR threshold and we have identiﬁed its value. Fig. 7 evaluates the CCR threshold which is used to classify an application as computation or communication biased. The communication volumes of the applications range from 20 to 1000 Kbits. CCR is deﬁned as the sum of edge weights divided by the sum of node weights in each application’s task graph. From this ﬁgure, one can see that, a CCR threshold of 1 generates the best performance. Therefore, in the following experiments, we set CCR threshold to be 1. D. Performance Comparison 1) Evaluation on Random Benchmarks: Fig. 8 compares the throughput, waiting time, and communication cost at different network sizes, for the three methods. One can see that, when the network size is large, e.g., 12 × 12, our approach can improve throughput by 1.5× and 3× over DsRem and PAT, respectively. The reason is that, our approach can optimize both the communication and computation intensive applications. For communication intensive applications, tasks with high trafﬁc volumes are mapped closer, while for computation intensive applications, more bubbles are inserted. Therefore, our approach can achieve better performance. Fig. 8 also shows that the waiting time of our approach is shorter than the other two approaches because our approach balances the waiting time and the execution time of each application when inserting bubbles. The other two approaches only consider the performance of each individual application. Among the three approaches, DsRem has the worst communication cost, since it does not take the communications among the tasks into account. Fig. 9 compares the considered metrics at different application communication volumes when the three methods are employed. It can be seen that when each application’s average communication volume increases, e.g., 150Kbits, our approach’s throughput is about 1.52× and 1.7× over DsRem and PAT, respectively. As DsRem does not consider communications among the tasks, it’s performance gets 2.5 2 1.5 1 0.5 Normalized execution time 1 CCR threshold 1.5 Fig. 7. CCR threshold selection. Normalized waiting time 10 5 0 Normalized comm cost 8 × 8 2 1 × 2 1 0 1 × 0 1 (c) proposed 1 0.5 0 Normalized throughput 8 × 8 2 1 × 2 1 0 1 × 0 1 (a) 10 5 0 8 × 8 0 1 × 0 1 2 1 × 2 1 (b) PAT DsRem Fig. 8. The throughput, waiting time, and communication cost comparison at different network sizes. worse when communication volume is large. Although PAT considers communication among the tasks, it does not consider budgeting the bubbles that affects the waiting time of future applications. Therefore, the waiting time of PAT is worse than ours, as in Fig. 9, leading to a degraded throughput performance. 2) Evaluation on Real Benchmarks: Fig. 10 compares the throughput, waiting time, and communication cost at different average number of tasks when the three methods are employed. When each application’s average number of tasks is large, e.g., 32 tasks, our approach’s throughput is about 1.67× and 1.5× over DsRem and PAT, respectively. Our approach also reduces waiting time by 50% and 44% over DsRem and PAT,respectively. Fig. 11 compares the considered metrics at different arrival rates for the three methods. When the arrival rate is high, e.g., 1 application arrives in the system per 100 cycles, our approach’s throughput is about 2.15× and 2.15× over DsRem and PAT, respectively. A higher arrival rates means more applications arrive at the system, indicating the system workload is high. In such cases, DsRem and PAT might lead to long waiting time when applications arrive, since the free cores are used as coolers for currently running applications. Further, DsRem and PAT optimize only for each individual application’s performance. On the other hand, when the system workload is high, our approach budgets fewer bubbles to currently running applications and thus more free cores can be used to run the incoming applications, reducing their waiting time. E. Cost Analysis The runtime cost of our algorithm is in the order of 1M cycles. This is averaged by running the algorithm ﬁfty times with different system parameters. After the evaluation, it has been observed that the running times of DsRem and PAT are also in the order of 1M cycles. Therefore, the runtime overhead of the proposed algorithm is acceptable. V I . CONC LU S ION We proposed an online algorithm to budget free cores (referred as bubbles) to each application so as to optimize the system throughput. The system throughput is related to each application’s communication and computation performances, as well as the waiting time incurred 1 0.5 0 Normalized throughput 50 150 100 (a) 3 2 1 0 Normalized waiting time 50 100 150 (b) 2 1 0 Normalized comm cost 50 100 150 (c) 1 0.5 0 Normalized throughput 0.1 0.4 (a) 1 30 20 10 0 Normalized waiting time 2 1 0 Normalized comm cost 0.1 0.4 (c) 1 0.1 1 0.4 (b) PAT DsRem proposed DsRem PAT proposed Fig. 9. The throughput, waiting time, and communication cost comparison at various communication volumes (in K bits). Normalized waiting time 1 0.5 0 Normalized throughput 8 16 (a) 32 3 2 1 0 2 1 0 Normalized comm cost 8 16 (c) 32 8 32 16 (b) PAT DsRem proposed Fig. 10. The throughput, waiting time, and communication cost comparison at different average number of tasks in each application. when it ﬁnds insufﬁcient cores to run its tasks. Performance and waiting time models are ﬁrst set up for the applications. An online algorithm was proposed to ﬁnd the best number and locations of the bubbles to each application, according to whether the new application is computation or communication intensive. The algorithm also trades the execution performance of each running application with the waiting time of new applications. Our experiments conﬁrmed that, compared with two existing runtime resource management approaches, our approach can improve the system throughput by as much as 50%. The runtime overhead of our approach is moderate, making it a suitable runtime resource management approach to achieve high system throughput for many-core systems running dynamic workloads. "
Improving NoC performance under spatio-temporal variability by runtime reconfiguration - a general mathematical framework.,"Real-world applications exhibit time varying traffic volumes and patterns (structures). However, regular or application specific networks-on-chip (NoC) optimized at design time are predominantly developed either under the assumption of time independent application structures and dynamics, or without considering them at all. The limited adaptability of the monolithic NoCs to spatio-temporal application variability results in poor performance and power inefficient designs. Most of prior reconfiguration approaches are evaluated and validated through a subset of experimental instances out of the entire problem space, resulting in a constrained applicability to general cases. Towards this end, we propose a general mathematical framework for reconfigurable NoC design and runtime optimization under spatio-temporal varying workloads. Our proposed mathematical modeling framework can be applied to NoCs of arbitrary topologies, sizes and routing protocols. More precisely, we first propose the general mathematical modeling for reconfigurable NoC systems. Then, as a case study, we formulate the runtime NoC reconfiguration as an optimization problem assuming applications modeled via time dependent graphical models. We show this problem is NP-hard and prove its submodularity. This submodularity property provides guarantees that our proposed greedy algorithms attaints optimality region. To further justify the mathematical framework, we investigate a reconfigurable NoC architecture and evaluate it under real-world applications. Experimental results demonstrate a 52.3% latency improvement and a 30.2% energy reduction compared to baseline design.","1 Improving NoC Performance under Spatio-Temporal Variability By Runtime Reconﬁguration: A General Mathematical Framework Yuankun Xue, Paul Bogdan University of Southern California, Los Angeles, CA, USA, {yuankunx,pbogdan}@usc.edu Abstract—Real-world applications exhibit time varying trafﬁc volumes and patterns (structures). However, regular or application speciﬁc networks-on-chip (NoC) optimized at designtime are predominantly developed either under the assumption of time independent application structures and dynamics, or without considering them at all. The limited adaptability of the monolithic NoCs to spatio-temporal application variability results in poor performance and power inefﬁcient designs. Most of prior reconﬁguration approaches are evaluated and validated through a subset of experimental instances out of the entire problem space, resulting in a constrained applicability to general cases. Towards this end, we propose a general mathematical framework for reconﬁgurable NoC design and runtime optimization under spatio-temporal varying workloads. Our proposed mathematical modeling framework can be applied to NoCs of arbitrary topologies, sizes and routing protocols. More precisely, we ﬁrst propose the general mathematical modeling for reconﬁgurable NoC systems. Then, as a case study, we formulate the runtime NoC reconﬁguration as an optimization problem assuming applications modeled via time dependent graphical models. We show this problem is NP-hard and prove its submodularity. This submodularity property provides guarantees that our proposed greedy algorithms attaints optimality region. To further justify the mathematical framework, we investigate a reconﬁgurable NoC architecture and evaluate it under real-world applications. Experimental results demonstrate a 52.3% latency improvement and a 30.2% energy reduction compared to baseline design. I . INTRODUCT ION AND RE LAT ED WORK Workloads induced by real world applications demonstrate strong spatio-temporal variability. Heterogeneous application tasks create inter-coupled trafﬁc patterns with time-varying data and control dependencies exacerbating the workloads requirements running over a large scale NoC-based manycore platform [1]. For instance, big-data applications like biological simulations [14] usually exhibit high dimensionality in their task structures [15], which are also varying vastly as a function of widely ranged objectives and unpredictable user input. Consequently, the failure to capture such variations through the development of a best-ﬁt NoC-based system, translates to systems that are prone to computation, communication and power inefﬁciencies. To address the spatio-temporal behavior of applications, prior research efforts have focussed on two design methodologies, namely the ofﬂine analysis of applications and optimization of application speciﬁc NoC architectures and the development of on-line optimization techniques. For instance, application-speciﬁc NoC synthesis has been well studied to achieve best-possible performance given a speciﬁc application. The synthesis usually generates well optimized NoC conﬁgurations and application mappings that are superior to baseline design in terms of energy efﬁciency and performance [11][13], application-speciﬁc performance (NoC-based accelerators)[7], reliability [16], lifetime cost [8]. Such optimization considers one or a subset of applications and captures their trafﬁc pattern and task structures. The synthesis is static with no adaptivity and performed prior to its practical deployment, assuming the application structure is not changing over time. Thus performance is very well optimized if only a ﬁxed set of applications are considered throughout the life cycle of the platform and the applications are almost temporally homogenous. However, the assumptions made before the synthesis usually do not hold. In reality, it is rarely the case that a dedicated NoC is solely developed for exclusive tasks processing. Instead, the NoC is usually used as fundamental communication infrastructure that integrates a set of heterogeneous processing entities that are expected to run a wide range of applications upon deployment. Most of applications are very diverse in terms of communication patterns and, more importantly, the trafﬁc patterns are also changing over time. Thus, the statically customized network structure could hardly be a feasible trafﬁc carrier that brings good performance in general cases. As an alternative to application speciﬁc NoC, the reconﬁgurable NoCs address this problem by changing their structural features [2][4], routing algorithm [10], resource management strategy [6][5], task assignments [12] to ﬁt to time-varying application requirements. In spite of their successful application in diverse settings, we still lack of a solid theoretical foundation upon which an analytical design methodology could be built to optimize the reconﬁguration with optimality guarantees in general cases. Most previous reconﬁgurable NoCs are proposed, evaluated and validated through a subset of experimental instances without looking at entire problem space from a mathematical perspective. To address the above-mentioned problems, we propose a general mathematical modeling framework considering the spatiotemporal characteristics of workloads and make the following novel contributions: • We propose a mathematical framework for capturing the dynamic nature of reconﬁgurable NoC and applica978-1-4673-9030-9/16/$31.00 c(cid:13)2016 IEEE Architectural Domain@time t Section III Architectural study Section II-A System modeling System Proﬁling Application Proﬂing Circuit Component Greedy Construction App 0 App 2 App 1 App 3 2 OS Domain@time t App 4 App 6 App N+1 ... App 5 ... App N App N+2 P (k , t ) k t Compatible ? N Relaxation Data Data Control Control Section II-C Submodular Optimization Section II-B Application Modeling Fig. 1: Overview of the proposed mathematical framework. The reconﬁgurable NoC system and its associated applications are characterized through the proposed system and application graphical models. Then the optimization to the network structural conﬁguration is performed by exploiting the submodular property of the problem. In case of a valid solution does not exist, we introduce the relaxation on problem constraints to obtain a feasible solution while preserve the optimality bound. tions that enables the formulation of major reconﬁgurable NoC optimization problems. Our analytical formalism can be applied to arbitrary network topologies and sizes, routing, or heterogeneous resource allocation problems. • We illustrate the efﬁcacy of this formalism by formulating the NoC reconﬁguration as a dynamic optimization problem. We prove that this optimization is NP-hard and demonstrate that our mathematical formulation satisﬁes the submodularity property justifying that our proposed greedy based algorithms can attain the optimality region. • We evaluate the impact of the proposed mathematical formalism by solving the NoC reconﬁguration problem. The experimental results show a 52.3% reduction in network latency, increased capability of handling heavy trafﬁc and 30.2% in energy reduction for our lightweight reconﬁgurable NoC when compared to baseline design. The paper is organized as follows: In Section II, we formally set up the reconﬁgurable NoC platform and introduce several deﬁnitions that help to enrich the model expressivity. Section III presents a case study of the NoC runtime reconﬁguration problem considering the case where application is characterized through a time-dependent graphical model (e.g., time-varying application graph). We show this optimization problem is NP-hard and prove its submodularity. By exploiting the submodularity property, we propose greedy heuristics with bounded optimality. Sections IV and V summarize our experimental results and our main achievements. I I . MATHEMAT ICAL MODE L ING AND O P T IM IZAT ION FRAMEWORK A. Architectural modeling framework Deﬁnition 1: A reconﬁgurable network-on-chip (NoC) is deﬁned as a connected dynamic directed graph G(t) = (N (t), E (t); γ (t), C ) at time t, where ni ∈ N (t) is a tile and represents a collection of functional units, and E (t) denotes the collection of physical links between different nodes in N (t). The ei,j ∈ E (t) denotes the link from ni to nj . We should note that N (t) is the set of enabled network functional units (e.g., DSPs, general processors, customized processing elements, memories or communication transceivers). The composition of N (t) can change as the subset of nodes are enabled or disabled over time. Also, the reconﬁgurable NoC usually takes advantage of different switching techniques for best possible performance under diverse workloads. Therefore, ei,j could be a regular link between two routers or a direct link established between tiles without interfacing with routers. To distinguish between regular and circuit links, we introduce the following deﬁnition: Deﬁnition 2: Edge ei,j is a circuit link if there exists a direct link from ni to nj that allows circuit switching. E (t) induces a function I : E (t) → R such that ei,j is a circuit link if and only if I (ei,j ) = 1. The circuit switching link is set up in the regular network to improve throughput over critical trafﬁc path in reconﬁgurable NoCs. Circuit switching reserves the entire bandwidth of the dedicated link and skips all routing stages, thus improving greatly the communication throughput when used between a pair of nodes. By connecting different circuit links together, it is possible for a subset of nodes to communicate over such dedicated circuit links for faster data exchange. To characterize such collection of nodes, we introduce the circuit component concept as follows: Deﬁnition 3: A subset N ′ (t) ⊆ N (t) is a circuit component T if and only if for any pair of nodes ni and nj ∈ N ′ (t), there exists a circuit link between them. A circuit component T in G(t) forms a connected subgraph enabling data transfer over dedicated links with augmented bandwidth and reduced latency. For example, the simplest circuit component is a pair of nodes with a direct link that skips the routers at both ends. Generally speaking, the reconﬁguration of NoC can be viewed as a process of enabling circuit components in the regular network to provide express ways for critical data transmission. Deﬁnition 4: The γ (t) : N (t) × N (t) → G(t) is the routing function at time t that maps a pair of nodes ni and nj to a C(n )0 ={   }n 2 C(n )3 ={   ,   } n 1 n 5 :X + X <=1 1 5 0 1 2 3 4 5 0 Invalid link Possible link Fig. 2: Augmented connectivity sets are shown for n0 and n3 . n3 is conﬁgurable to set up link with n1 and n5 whereas both links can not be established at the same time. Similar link to n0 is invalid due to physical limitation. subgraph G′ (t) of G(t) such that: i) ni , nj ∈ G′ (t) and ii) there exists at least a path from ni to nj . Of note, the (N (t), E (t); γ (t)) forms a 3-tuple that deﬁnes a regular NoC without reconﬁguration features, where N (t) and E (t) characterize its structural properties. γ (t) denotes all possible paths for data exchange. To be able to model and express the reconﬁguration capability, we deﬁne the augmented connectivity function C : Deﬁnition 5: C : N (t) → 2N (t) is the augmented connectivity function (ACF) that associates each node ni ∈ E (t) at time t with a subset of nodes C (ni ) = {nj |nj ∈ N (t)}, to which ei,j could be possibly set up. The set C (ni ) is called augmented connectivity set (ACS). By deﬁnition, C (ni ) denotes all the possible links that could be alternatively established for ni other than the current links in {ei,j |ei,j ∈ E (t)}. We should note that the detailed form of C and its range are limited by the physical constraints in a speciﬁc architecture. For instance in Figure 2, a physical link can not be inserted between n3 and n0 that are too far away from each other given physical limitations (e.g., propagation delay should be less than one cycle). Moreover, a link might not be set up from ni to nj even if nj ∈ C (ni ). Such constraint comes mostly from architectural limitations which forbid some links from being set up simultaneously as shown in Figure 2. To consider such cases, we introduce a set of constraints associated with C (ni ). Let Xj be a binary variable such that Xj = 1 if nj ∈ C (ni ) and ei,j is set up. Otherwise, Xj = 0. Σαj Xj ≤ K (1) where, αj ∈ {0, 1} represents whether Xj is masked or not in the constraints. K ∈ N denotes the maximum number of links that can be established simultaneously. Equation (1) deﬁnes a constraint, which encodes the physical limitation that a subset of nodes in C (ni ) can not connect to ni at the same time. By changing the conﬁguration of {αj }, eq. (1) forms an array of constraints that capture all such physical limitations. Thus, we can deﬁne the reconﬁgurable NoC in the general case: Deﬁnition 6 (reconﬁgurability): A node ni is reconﬁgurable if and only if C (ni ) 6= ∅. An NoC system G(t) = (N (t), E (t); γ (t), C ) is reconﬁgurable at time t if there exists at least one node ni ∈ N (t) that is reconﬁgurable. Deﬁnition 6 formally deﬁnes the reconﬁgurability. An NoC system G(t) is reconﬁgurable as long as any one of its nodes has non-empty ACS. By picking up nodes from ACS and constructing new edges over time, E (t) evolves as a function of reconﬁguration decisions made prior to time t. Of note, 3 Deﬁnition 1 enforces the connectivity of a given NoC system. Therefore, the routing algorithm γ (t) will also change accordingly to guarantee all the nodes are reachable. Combined with the dynamics of N (t), G(t) = (N (t), E (t); γ (t), C ) describes a dynamical system with time-varying structural characteristics, i.e., N (t), E (t) and γ (t), driven by the reconﬁguration decisions made upon C (ni ) over time. By introducing attributes of interest and associating them with G(t) and application tasks, we are able to construct the theoretical basis on which general NoC reconﬁguration could be formulated as optimization problems. To provide an illustrative example of this framework, we present the runtime NoC reconﬁguration optimization problem given time-dependent workloads characterized by graphical models. B. Application modeling framework Generally speaking, any runtime NoC reconﬁguration is an optimization process that searches for the best-ﬁt network structural conﬁgurations and application task assignments (i.e, mapping of tasks to speciﬁc tiles), given an objective function and a set of constraints. In contrast to application-speciﬁc NoC synthesis, runtime optimization is an online process repeatedly synchronized with the time-dependent characteristics of the running applications. This process usually consists of two phases: an execution phase and an optimization phase. In the optimization phase, the optimization will take the proﬁle of applications as input and decide on a best possible network conﬁguration for the execution phase. Proﬁling the application is a very complex research topic and several models were developed in different contexts. A detailed discussion is beyond the scope of this work. Next, we consider proﬁling applications via graphical models although the proposed NoC system model G(t) has no constraints on proﬁling techniques. Deﬁnition 7: An application A(t) is a dynamical directed graph A(t) = (V (t), C (t); P (Σ, F )) at time t. Each vertex vi ∈ V (t) is a task of the application. ci,j ∈ C (t) represents a directed data/control dependency from task vi to task vj . Each application A(t) induces a proﬁling system P (Σ, F ). Σ is the ﬁnite functionality alphabet given speciﬁc application context. It contains symbols that characterize the functionality of interest. Functionality proﬁling function F : V → 2Σ relates each task vi of A(t) at time t with a ﬁnite set of symbols deﬁned in Σ as functionality requirement set F (vi ). To provide some intuition, Σ can be as simple as an integer set {0, 1, 2, 3} in context of numeric computation where 2 and 3 represent “addition” and “multiplication”. 0 and 1 represent “integer” and “ﬂoating-point”, respectively. F will induce a functionality requirement set for each node vi . A node vi with F (vi ) = {1, 2, 3} requires ﬂoating-point addition and multiplication operations, while a node vj with F (vj ) = {0, 2} requires only integer addition. By changing alphabet Σ based on application context, i.e., application task proﬁled with interested details, P (Σ, F ) is able to characterize each task with sufﬁciently many mathematical details. Moreover, we will use the same proﬁling system P (Σ, F ) to characterize the “capabilities” of each tile ni ∈ G(t) such that we can easily compare capabilities of the NoC G(t) with 4 the functionality requirements from the application domain. This constitutes the foundation for application task assignment. For each ni of G(t), we deﬁne F (ni ) as capability set such that, an application task vi with functionality requirement set F (vi ) can be mapped to ni only if F (vi ) ⊆ F (ni ). Each directed edge ci,j ∈ C (t) is characterized by a data generation process Pci,j (k , t) = P {N (t) = k , k ∈ N } where N (t) is a counting process which denotes the number of packets generated in time interval [t, t + τ ]. Pci,j (k , t) captures the time-varying behavior of the application. To provide some intuition, the data generation process could be a Poisson process if no memory-effect is present or it could be a fractal process governed by power-laws exhibiting longrange memory dependency. Therefore, for an execution phase of length T , the average trafﬁc volume q(ci,j ) from task vi to task vj could be calculated as follows, q(ci,j ) = Z t+T t Z ∞ −∞ kPci,j (k , t)dkdt (2) We deﬁne by b(ci,j ) the minimal bandwidth requirement for communication from vi to vj . It should be noted that this requirement comes from the execution time constraints posed by the associated tasks. We deﬁne B (ei,j , I (ei,j )) as the bandwidth provided by the link ei,j ∈ E (t) given NoC system G(t). Attribute I (ei,j ) is introduced in B to consider the bandwidth difference between regular and circuit links. We check the validity of assigning a task to one tile in NoC system by comparing the functionality requirement and capability set, a pair of tasks vi , vj can be mapped to ni and nj only if b(ci,j ) ≤ B (ei,j , I (ei,j )) and b(cj,i ) ≤ B (ej,i , I (ej,i )). C. Runtime reconﬁguration – problem formulation Based on the mathematical description of the reconﬁgurable NoC and the application, the runtime NoC reconﬁguration is performed in optimization phase by modifying the structural properties of G(t), i.e., changing the E (t) and constructing circuit components T , based on ACF C given application A(t) and execution phase horizon T . To formally state the optimization problem, we ﬁrst introduce the deﬁnition of compatible partition of an application A(t) given G(t). Let π = {π1 , . . . , πn} be a partition of a given application A(t) = (V (t), C (t); P (Σ, F )) such that, Deﬁnition 8: The partition π is compatible with NoC system G(t) = (N (t), E (t); γ (t), C ) at time t if and only if for any partition element πk there exists a circuit component Tk in G(t) such that, ∪vi ∈πk F (vi ) ⊆ ∪ni∈Tk F (ni ), |πk | ≤ |Tk | Ti (3) (4) such that, the number of circuit components is maximized to sustain the trafﬁc requirements while also improving the energy efﬁciency, i.e., the dedicated links provide superior bandwidth and reduce energy consumption by skipping multiple routing stages. Next, we show that this problem is NP-hard and propose a greedy algorithm that solves the problem while also considering the convergence guarantees to the optimality. D. Complexity and algorithm analysis ⋄ Theorem 1: The runtime NoC reconﬁguration problem described in (7) is NP-hard. Proof: The proof follows by noticing that there exists an NoC system G(t) = (N (t), E (t); γ (t),C ) that, for any partition π of A(t), it is compatible. Therefore, the problem reduces to a quadratic assignment problem, that is NP-hard, between partition {πk } and circuit components {Tk } that minimizes (7). Because (7) contains (as subclass of problems) one that is NP-hard, it follows that (7) is also NP-hard. Next, we show the feasibility space of (7) given by the compatibility constraint, see Deﬁnition 8, is submodular. To prove the optimization problem (7) is submodular, it should be noted that (7) could be equivalently deﬁned as its dual maximization problem as: Runtime NoC reconﬁguration – dual problem formulation: Given a reconﬁgurable NoC system G(t) = (N (t), E (t); γ (t), C ), an application A(t) = (V (t), C (t); P (Σ, F )) at time t and execution phase horizon T . Find a partition π of A(t) and corresponding circuit components {Ti } that maximize the following cost function: max π ,T XTi (fa (Ti ) q(Ti ) qΣ + 1 λ E∆ (Ti )) (8) Subject to: Partition π is compatible. Of note, the q(Ti ) is the overall trafﬁc volume in Ti and qΣ denotes overall trafﬁc volume of A(t) given execution phase horizon T . The fa (Ti ) is the intra-component bandwidth factor, i.e., ratio of bandwidth between circuit link and regular link. Thus, the ﬁrst term in the summation considers the bandwidth gain obtained by assigning dedicated circuit link to application workloads. E∆ (Ti ) represents the amount of energy saved by using dedicated links for trafﬁc q(Ti ), compared to the energy consumed by using regular link instead. E∆ (Ti ) depends on how each task in A(t) is assigned to tiles in G(t). Lemma 1: The runtime NoC reconﬁguration objective function described in (8) is submodular. Proof: Given an application partition π , let us deﬁne two compatible circuit component sets TA ⊆ TB where TA or TB is a collection of circuit components that are compatible with a subset of partition elements in π . Let Te ∈ T be an arbitrary circuit component to which a partition πk will be assigned. We denote G (TA ) as the objective function deﬁned in (8) given TA . Thus, if Te /∈ TB , ⋄ G (TA ∪ Te ) − G (TA ) = fa (Te ) G (TB ∪ Te ) − G (TB ) = fa (Te ) q(Te ) qΣ q(Te ) qΣ + + 1 λ E∆ (Te ) (9) 1 λ E∆ (Te ) (10) 5 Otherwise, if Te ∈ TB , G (TB ∪ Te ) − G (TB ) = 0. Therefore, G (TB ∪ Te ) − G (TB ) ≤ G(TA ∪ Te ) − G (TA ) (11) holds for any TA ⊆ TB ⊂ T and Te ∈ T . Hence, the objective function (8) is submodular. Moreover, eq. (9) shows G is also monotonic. For monotonic submodular functions[9], we have the following theorem, Theorem 2: Given a monotonic submodular function G , G(∅) = 0, the greedy maximization algorithm returns: G (π , Tgreedy ) ≥ (1 − 1/e) max |T |≤N G(π , T ) (12) where N is maximum number of circuit components that are possibly constructed. Thus, even though the runtime NoC reconﬁguration problem is NP-hard, we can propose a greedy heuristic Alg. 1 with guaranteed optimality. In Algorithm 1, G(t) and A(t) are ﬁrst constructed for an execution phase of length T . Then the algorithm will partition the application A(t) constrained by the physical limitations posed by G(t) such that the maximum possible amount of trafﬁc is covered within all partition components {πk }, i.e., the cut weight is minimal. We assume the Fiduccia-Mattheyses algorithm [3] that is able to handle unbalanced partitions. The physical limitations majorly prevent infeasible partitions. A partition is considered infeasible if any of its partition components is beyond a predeﬁned size such that no compatible circuit component is possibly found, see Deﬁnition 8, (3), (4), and (5). Then a greedy heuristic will construct a circuit component Te that maximizes the incremental gain of G and add it to T . The cycle repeats until π is compatible. Algorithm 1 returns a solution to (8) with bounded optimality as long as it exists. Otherwise, i.e., no circuit components can be constructed to be compatible with given partition, or it takes indeﬁnite time to reach one, Algorithm 1 might practically fail. To obtain an approximated solution in such cases, we propose a relaxation process. Deﬁnition 9: An l-relaxed circuit component is a subset of nodes N ′ (t) ⊂ N (t) if and only if for any pair of nodes ni , nj , there exists a link ei,j between them such that, at most l of all such links are regular links. Deﬁnition 10: Partition π is l-compatible with NoC system G(t) = (N (t), E (t); γ (t), C ) at time t if and only if for any partition element πk there exists a l-relaxed circuit component Ti in G(t) such that (3), (4), and (5) are met. The deﬁnitions 9 and 10 set the relaxation on the concepts of circuit component and compatible partition. Ideally, we reconﬁgure the NoC structures to ﬁt the application workloads with the hope that all trafﬁc paths are able to exploit their dedicated bandwidth, i.e., circuit links. However, there is a chance that such circuit links are difﬁcult to construct given architectural and physical constraints. Therefore, we need to relax the Deﬁnition 8 in such cases, while still requiring a link to exist between any pair of nodes, to allow some of such links to be regular links. Deﬁnition 9 generalizes the circuit component such that not only the dedicated links, i.e., circuit link, but also the express links , i.e., one-hop regular links between routers, are considered. Thus, the constraint for (8) Algorithm 1 Greedy maximization algorithm to (8) Input: Application A(t) = (V (t), C (t); P (Σ, F )); Reconﬁgurable NoC G(t) = (N (t), E (t); γ (t),C ); Execution phase length T ; Output: Compatible application partition π and constructed circuit components set T 1: π=Paritition(A(t), G(t)) 2: repeat 3: T =T ∪ arg max (G (π , T ∪ Te ) − G (π , T )) Te 4: until π is compatible or T == N (t) Algorithm 2 l-relaxed Greedy maximization algorithm to (8) Input: Application A(t) = (V (t), C (t); P (Σ, F )); Reconﬁgurable NoC G(t) = (N (t), E (t); γ (t),C ); Execution phase length T ; Output: Compatible application partition π and constructed circuit components set T 1: l = 0 2: π=Paritition(A(t), G(t)) 3: repeat repeat 4: 5: 6: 7: 8: 9: T =T ∪ arg max (G (π , T ∪ Te ) − G (π , T )) Te until π is l-compatible or T == N (t) if π is l-compatible then Return T and π end if 10: l = l + 1; 11: until l == | arg max πk |πk || could be relaxed to l-compatible as stated by Deﬁnition 10. It is noted that the relaxation on the constraint does not affect the submodularity of G , thus the optimality bound in (12) holds. In what follows, we will instantiate a reconﬁgurable NoC architecture as optimization case study to evaluate the efﬁcacy of the proposed mathematical framework with realistic workloads characterized by graphical models. I I I . ARCH IT ECTURAL CA S E S TUDY To understand how the proposed mathematical framework could be effectively applied to a reconﬁgurable NoC system, we consider a simple reconﬁgurable NoC with switch boxes to provide dedicated links, i.e., the circuit link, between different network tiles. More precisely, we study a regular mesh NoC system G(t) = (N (t), E (t); γ (t)) to which a subnet of dedicated links is attached. Formally, for each node ni ∈ N (t), ACF C is induced to generate a set of nodes to which a link could be set up (i.e., ACS, see Deﬁnition 5). Therefore, we have a simple reconﬁgurable NoC system characterized by quadruple G(t) = (N (t), E (t); γ (t), C ). To detail the construction of ACS for each node ni , we set up low-level architectural features for the switch box. 6 A switch box is a set of programmable pass gates. For simplicity of illustration, Figure 3.(B) shows only the NMOS part of it. To allow circuit links to be set up between different tiles, a switch box is organized by an array of one-bit switch boxes. The length of the array is equal to the bitwidth of a circuit link. A one-bit swap box consists of 6 pass gates such that any pair of ports can be directly connected by a dedicated link. A tile connects to a switch box through a set of similar links controlled by the pass gates. By cascading such links among a subset of nodes, it is possible to establish circuit links between them. Figure 3.(B) shows the conﬁguration of a set of switch boxes such that, a set of nodes {n10 , n11 , n13 , n14} becomes a circuit component Tk . To simplify the hardware setup and minimize the area overhead, we assume each node in the circuit component should time-multiplex the link, i.e., one driver for any circuit link during a slotted time assigned. More precisely, for an execution time of length T , each node ni ∈ Tk is assigned with a bandwidth, i.e., being a driver for the circuit link, proportional to its trafﬁc share in q(Tk ). Therefore, each switch box could work in 3 possible modes, namely, ”Driver”, ”Sink” and ”Pass” based on the ownership of the link. A switch box is in ”Pass” mode if the tile connected to the switch is not the driver or sink of the data being transferred, thus ”passing” the data. Otherwise, it is in ”Driver” or ’Sink’ mode. The working modes can be identiﬁed by the conﬁguration pattern of switch box as shown in Figure 3.(C). A switch box is in ”Pass” mode if i) the tile is disconnected to the switch box and ii) switch box is programmed as any one of three conﬁgurations on top in Figure 3.(C). Otherwise, it will be in ”Driver” or ”Sink”. As a simple reconﬁgurable NoC, we enforce the statically assigned time slot for each node in the circuit component. So there is chance that a node becomes the driver yet with no data to transmit. To maximize the link utilization in such case, it is necessary to make possible the shift of circuit link ownership during such an idle ”Driver” phase. We thus adopt a self-negotiated link access control (SNAC) as shown in Figure 3.(D). We build up a one-bit bi-directional negotiation link (NL) between tiles. A simple negotiation protocol is implemented over the circuit link between a driver-sink pair to bargain over the ownership. More precisely, a ”Driver” during its assigned slot will automatically obtain the access to NL and the circuit link. When a data transmission is in progress, no negotiation is necessary. Otherwise, if the driver has no data to send, either a transmission is complete before the expiration of assigned time slot or no planned transmission, driver will send ACK sequence ”010” to sink to yield the control over the circuit link to the sink. Upon receiving this sequence, if the sink has anything to send to the current driver, a negotiation happens: the sink will send REQ sequence ”101” and wait for ACK sequence. Upon valid ACK, the sink will obtain the rest of time slot and use it for transmission to the previous driver, i.e., the sink and driver switch their roles. Otherwise, after a preset waiting threshold, the negotiation is a failure and the sink will abort the request. In the following discussion, we will consider a set of real world applications and perform the optimization to construct circuit components by exploiting the submodularity properties of the problem as stated in (8). 7 12 13 14 15 One -b i t sw itch bo x 13 N 14 W E S 10 11 T im e = t D a t a T im e = t+ tu rn a ro un d_ t im e D a t a T i l e 11 T i l e 13 T i l e 11 N eg t_ l i n k 14 P a s s N eg t_ l i n k 13 14 P a s s T i l e 13 13 9 5 1 8 4 0 A ) 10 11 B ) 6 2 7 3 “Pa ss ” Circuit link C ) “D r iv e ” “S ink ” Conﬁguration pattern D r i ve n 10 11 10 11 P a s s P a s s S in k Circuit link ownership @ t im e t N eg o t ia t io n ownership link@ t ime t 0 0 0 0 1 1 ACK N eg t_ l i n k Tu r n a r o u n d N eg t_ l i n k A b o r t N eg t_ l i n k Y i e l d D) 1 1 1 0 0 0 REQ 0 REQ 1 1 1 T im e ou t 0 1 ACK Fig. 3: A case study: low-level architecture details IV. EX P ER IMENTAL RE SULT S Experiment setup: We consider real world workloads induced by 6 SoC applications that are characterized by the proposed graphical model A(t) = (V (t), C (t); P (Σ, F )). The applications include video object plan decoder (VOPD), multi-window display (MWD), MP3 encoder/decoder, H.263 encoder/decoder and MPEG-4. The number of tasks ranges from 11 to 16. We set up a 4x4 reconﬁgurable NoC described in section III as target system. The NoC system is implemented using fully synthesizable Verilog and synthesized under SMIC 65nm process using Synopsys Design Compiler with a ﬁxed frequency constraint of 200MHz. All simulations are done using Synopsys VCS ported with Tcl scripts to load in the application trafﬁc workloads. Throughout the simulations, NoC adopts wormhole switching for regular data transmission under variable ﬂit-width ranging from 16-bit to 256-bit such that, we can test the network performance under different ﬂit injection rates given a ﬁxed data generation rate. Each port in the router has 4 4-ﬂit virtual channels. We do not insert repeaters to the circuit links and assume the propagation delay should be within 1 cycle under 200MHz. Thus, we constrain the feasible size of one circuit component to be less than 5. Power estimation is done by feeding the Switching Activity Interchangeable File (SAIF) to Design Compiler during the synthesis. We extract the switching statistics by RTL simulation in VCS and transform it into SAIF ﬁles. Algorithm 2 is implemented using C++. We use Fiduccia-Mattheyses algorithm for partition of the applications. Performance evaluation: Figure 4 shows the optimized network conﬁgurations for all 6 applications obtained by solving the submodular maximization problem in (8). Application partition π is ﬁrst obtained to minimize the cut cost, i.e., trafﬁc between different partition components, see (7) for detailed explanation. Then, we run the l-relaxed greedy maximization algorithm to construct circuit components T such that a circuit component conﬁguration is obtained, which is compatible with the partition π . As shown in Figure 4, our algorithm identiﬁes the critical trafﬁc paths of all applications (see solid rectangles) and constructs circuit components (dashed rectangles) to cover most of them. An important observation is that the conﬁguration of the network varies greatly for different applications, which suggests the spatio-temporal variability of the applications when ported to the NoC system over time. To evaluate the performance under different trafﬁc pressures, we run the applications on networks with different physical interconnection bandwidth (i.e., ﬂitwidth=16-bit to 256bit) under 200MHz to measure the network latency. In such settings, the ﬂit injection rate has to be increased/decreased to meet the application bandwidth requirements as the physical bandwidth shrinks/grows. We compare the network latency for reconﬁgurable NoC optimized by Algorithm 2 and the baseline regular mesh NoC. The results are reported in Figure 5.(a). For applications with smaller bandwidth requirements like MP3 Encoder, H263 Decoder and Encoder, both networks demonstrate no saturation phase transition under experimental settings. However, the optimized reconﬁgurable network shows on average 52.3% latency reduction compared to the baseline design. This is because most of the communication with heavy trafﬁc loads are identiﬁed and take advantage of the dedicated links without traveling through multiple routing stages. For applications with heavy trafﬁc requirements like VOPD, MPEG4 and MWD, the optimized network not only shows improved network latency before phase transition, but also exhibits its capability to endure greater trafﬁc pressure, i.e., the network becomes heavily congested under a greater ﬂit injection rate compared to the baseline design. This improvement comes from the fact that, the communication paths with heaviest trafﬁc are covered mostly by the circuit links, thus alleviating the trafﬁc pressure posed on the regular network. To show the improved energy efﬁciency of our optimized network, we report the normalized energy savings under different network settings in Figure 5.(b). Combined with Figure 5.(a), we have the following key observations: i) Our optimized network shows improved energy efﬁciency ranging from 22% to 38% with an average of 30.2%, under all workloads and ii) The energy efﬁciency increases as the network becomes more congested. These observations are supported by the fact that the trafﬁc over the circuit links consumes less energy by skipping multiple routers. The energy savings are even greater when the network is congested as the recursive switching within a router for blocked packets can be avoided. V. CONCLU S ION In this work, we lay the theoretical foundation for modeling the reconﬁgurable NoC in general and propose a mathematical framework for optimization of the NoC reconﬁg8 5 8 7 6 4 3 2 16 1 4 1 5 1 9 1 0 11 1 2 1 3 5 6 7 8 1 2 3 4 1 4 1 5 1 6 9 1 0 11 1 2 1 3 7 0 3 62 3 62 3 62 4 9 2 7 3 57 3 53 3 00 3 13 1 6 1 6 1 6 1 6 1 6 1 57 1 6 3 13 9 4 5 00 (A ) VOPD (B ) MP3 Encod e r 3 7 6 8 8 1 1 2 2 3 5 4 4 11 1 9 9 1 0 1 0 1 2 1 2 5 6 7 8 1 2 3 4 9 1 0 11 1 2 1 3 0 .0 25 2 .0 83 4 .0 6 0 .5 1 1 2 .0 83 0 .0 1 0 .5 4 .0 6 0 .8 7 0 .1 8 0 .1 5 5 6 7 8 1 2 3 4 1 4 9 1 0 11 1 2 1 3 0 .0 25 0 .0 25 0 .2 5 0 .1 87 0 .5 0 .1 3 .6 72 3 .6 72 3 .6 72 0 .3 8 4 .0 6 0 .5 0 .0 1 2 .0 83 0 .5 5 7 6 3 2 1 4 12 13 8 1 0 11 14 9 (D ) H .2 63 D e code r 13 4 5 6 7 8 1 2 3 9 1 0 11 1 2 6 5 7 7 8 8 1 1 2 2 3 3 4 4 9 1 0 1 0 11 11 1 2 1 2 0 .1 93 0 .0 25 3 8 . 00 1 3 8 . 01 6 2 4 . 63 4 4 6 . 73 3 3 7 . 95 8 4 .0 6 0 .5 0 .0 1 3 8 . 00 1 2 .0 83 C ) H . 26 3 En cod e r 5 6 7 8 1 2 3 9 1 0 11 1 2 64 4 1 28 9 6 9 6 1 28 9 6 9 6 9 6 9 6 6 4 6 4 9 6 5 5 6 6 7 7 10 1 1 2 2 3 3 4 12 11 4 1 0 8 9 E ) MWD 4 5 10 8 6 1 11 2 3 7 1 2 5 6 7 8 1 2 3 4 9 1 0 11 1 2 F ) MPEG - 4 0 .5 1 90 0 .5 6 0 4 0 2 50 5 00 1 73 6 70 9 10 6 00 3 2 Application p a r t i t ion E s t ab l i sh ed circuit link Commun i ca t ion bandw id th (MB / s ) 3 2 ( Circuit component 9 Unused tiles Fig. 4: Optimized network conﬁguration for real world applications 0 0.0001875 8 24 32 0.000375 0.00075 0.0015 0.003 Baseline Reconﬁgurable Noc MP3 Encoder !1 0 0.0002 7.5 15 22.5 30 0.0004 0.0008 0.0016 0.0032 Baseline Reconﬁgurable Noc H.263 Decoder 0 0.003 12.5 25 37.5 50 0.006 0.012 0.024 0.048 Baseline Reconﬁgurable Noc H.263 Encoder A e v r e g a l y c n e a t ( s e c y c l ) 0 0.02775 750 1500 2250 3000 0.0555 0.111 0.222 0.444 Baseline Reconﬁgurable Noc VOPD 0 1000 2000 3000 4000 0.0416 0.0833 0.166 0.333 0.666 Baseline Reconﬁgurable Noc MPEG4 !1 0 350 700 1050 1400 0.0145 0.029 0.058 0.116 0.232 Baseline Reconﬁgurable Noc MWD N o r m a i l o p d e z w e r s g n v a s i Baseline Optimized@Flitwidth=16 Optimized@Flitwidth=16 Optimized@Flitwidth=32 Optimized@Flitwidth=64 Optimized@Flitwidth=128 Optimized@Flitwidth=256 VOPD MP3 H263.Enc H263.Dec MWD MP4 Injection rate(ﬂits/node/cycle) 1 0.75 0.5 0.25 0 16 Fig. 5: Network latency and energy savings comparison between the baseline and optimized reconﬁgurable NoC under different trafﬁc workloads.(a) Network latency measurements and (b) corresponding normalized energy consumptions. uration. We formulate the NoC reconﬁguration as an optimization problem and prove its submodularity. Based on our theoretical analysis, we propose a greedy algorithm bounded by guaranteed optimality. As a case study, we propose a simple reconﬁgurable NoC as architectural instance to validate the framework. We perform the proposed optimization and evaluate it with real-world workloads. The results show a 52.3% reduction of network latency on average, increased capability of handling heavy trafﬁc and 30.2% in energy reduction compared to baseline design. V I . ACKNOW L EDGEMENT The authors are thankful to reviewers for their valuable feedback. We acknowledge the support by US National Science Foundation (NSF) under Grant 1331610 and 1453860. "
"Inter/intra-chip optical interconnection network - opportunities, challenges, and implementations.","Recent advances in photonics technologies have made optical interconnection network an attractive option for computing systems from high-performance computers and data centers to automobiles and cell phones. Optical interconnection network promises ultra-high bandwidth, low latency, and great energy efficiency to alleviate the inter-rack, intra-rack, intra-board, and intra-chip communication bottlenecks in multiprocessor systems. Silicon-based photonics technologies piggyback onto developed silicon fabrication processes to provide viable and cost-effective solutions. Both industry and academia have invested significant efforts to develop and commercialize optical interconnection network technologies. This paper reviews the latest progresses and provides insights into the challenges and future developments.","Inter/Intra-Chip Optical Interconnection Network: Opportunities, Challenges, and Implementations Peng Yang†1 , Shigeru Nakamura†2 , Kenichiro Yashiki3 , Zhehui Wang1 , Luan H. K. Duong1 Zhifei Wang1 , Xuanqi Chen1 , Yuichi Nakamura2 , and Jiang Xu1 1Hong Kong University of Science and Technology 2System Platform Research Laboratories, NEC Corporation, Japan 3Photonics Electronics Technology Research Association (PETRA), Japan Abstract—Recent advances in photonics technologies have made optical interconnection network an attractive option for computing systems from high-performance computers and data centers to automobiles and cellphones. Optical interconnection network promises ultra-high bandwidth, low latency, and great energy efﬁciency to alleviate the inter-rack, intra-rack, intraboard, and intra-chip communication bottlenecks in multiprocessor systems. Silicon-based photonics technologies piggyback onto developed silicon fabrication processes to provide viable and cost-effective solutions. Both industry and academia have invested signiﬁcant efforts to develop and commercialize optical interconnection network technologies. This paper reviews the latest progresses and provides insights into the challenges and future developments. I . IN TRODUC T ION Computing systems, from HPC and data center to automobile and cellphone, are integrating growing numbers of processors and memories to meet the burgeoning performance requirements of new applications under tight energy and thermal constraints. Traditional electrical interconnects are becoming the bottleneck in intra-rack and intra-board communications. Photonic technologies have been successfully used in WAN and LAN, and showed strengths in SAN. Thanks to recent progresses in silicon-based photonics devices, optical interconnects promise ultra-high bandwidth, low latency, and low energy consumption to evolve inter-rack, intra-rack and intra-board communications at a low cost. Integrated optical transceivers, optical switches, and onchip waveguides have been implemented and demonstrated [1] [2]. On-chip waveguides can be connected with offchip waveguides, such as optical ﬁbers, to alleviate chip pin constraints [3]. Based on photonic devices, many design explorations and analyses have been carried out for optical interconnects and optical interconnection networks. OE interface and PHY-level link modeling and comparison [3] [4], show the signiﬁcant advantages of optical interconnects over electrical interconnects. Optical interconnection network † Peng Yang and Shigeru Nakamura are both ﬁrst authors of this paper. This research is partly supported by New Energy and Industrial Technology Development Organization (NEDO) and National Institute of Information and Communications Technology (NICT). 978-1-5090-0172-9/15/$31.00 c(cid:2)2016 IEEE architectures have been extensively studied. Optical networkon-chip (ONoC) has been introduced to replace electrical network-on-chip for intra-chip communications [5]–[9]. Some studies target to inter-chip communications [10]. In order to take the full advantage of optical interconnects and remove the gaps caused by packages, uniﬁed inter/intra-chip optical network designs are proposed without EO/OE conversions between the intra-chip and inter-chip optical networks [11] [12]. [13] and [14] analyze optical crosstalk noise and propose effective techniques to relieve its impacts. Optical devices are sensitive to temperatures. [15] and [16] present a holistic analysis and develop techniques to restrain the thermal effects on optical interconnects. I I . S I L ICON PHOTON IC S D EV IC E S For the optical interconnect, continuous increase in bandwidth and penetration from inter-rack to intra-rack and intraboard are envisioned. To support such evolution, silicon photonics is quite attractive because of its capabilities of miniaturizing and integrating various optical devices by using sophisticated large silicon wafer process technology. But, toward actual application, assembling including the policy for optical coupling to external ﬁbers or waveguides is also an important point. In this section, we will describe examples of optical transceivers and optical switches which have been demonstrated in the form of assembled modules. A. Silicon Photonics Transceiver To increase the optical interconnect bandwidth, increasing signal bit rate per port and increasing I/O port density are required. Compact and low power optical transceivers which can be placed closely to a LSI host chip are required as shown in Fig. 1(a). For such so-called mid-board optical transceivers, silicon photonics based technology called ’optical I/O core’ has been developed by PETRA [1] [17] [18]. Fig. 1(b) shows an optical transmitter (Tx) chip and an optical receiver (Rx) chip. 12 channels of 25 Gbps optical transmitters including hybrid integrated light sources and ﬂip-chip mounted drivers or 12 channels of 25 Gbps optical receivers including ﬂipon the 5 mm × 5mm silicon chip, respectively. As targeting chip mounted trans-impedance ampliﬁers (TIAs) are formed at volume zone application with shorter reach, multi-mode ﬁbers (MMFs) are employed for optical transmission from Tx to Rx. ’Optical pin’ structure is introduced on a silicon chip for optical coupling to ﬁbers with ﬂat chip surface. Large misalignment tolerance and ﬂat chip surface are quite useful for cost-effectively assembling optical I/O core chips for optical coupling and electrical contact. Fig. 1. (a) Expected evolution from board-edge optical I/O to mid-board optical I/O. (b) Photographs of Tx and Rx chips of optical I/O core. (c) Measured 25 Gbps eye patterns at temperatures of 20◦C and 75◦C. On the Tx chip, CW light at a wavelength of 1300 nm band from a quantum-dot (QD) Fabry-Perot laser diode (LD) hybrid integrated on the Tx chip is modulated through a silicon waveguide based Mach-Zehnder type optical modulator and then emitted from the chip through a grating coupler. In the Mach-Zehnder modulator, 25Gbps electrical signal is supplied to MOS junction phase shifters through the ﬂip-chip mounted driver. Modulated light emitted from the grating coupler is then coupled to the MMF through the optical pin. The QDLD features little temperature dependence on its light output, which is useful for placing the optical I/O core closely to the LSI. The QD-LD also shows high tolerance to optical feedback from back-reﬂection [18], which is useful for isolator-free assembling. On the Rx chip, optical signal from the MMF is lead to a surface-illuminated Ge photodiode with a diameter of 30 μm through the optical pin and then converted electrical signal is extracted through the ﬂip-chip mounted TIA. Using this Tx and Rx combination, the total power consumption of the driver and the TIA was reported to be 5 mW/Gbps. Errorfree operation of 300-m MMF transmission at 25 Gbps was conﬁrmed [17]. Fig. 1(c) shows the eye patterns of 25 Gbps optical signal emitted from the Tx chip and 25 Gbps electrical signal extracted from the Rx chip when the temperature of the Tx chip is set at 20 ◦C or 75 ◦C. At both temperatures, clear eye opening was conﬁrmed [1], indicating stable operation under temperature change. B. Silicon Photonics Switches To construct more efﬁcient optical networks in larger scale datacenters, introducing the hybrid scheme consisting of electronic packet switches and optical circuit switches as shown in Fig. 2(a) is intensively investigated [19]–[21]. By preparing bypasses with optical switches for longer ﬂow trafﬁc, lower power consumption and lower latency are expected. Toward such optical switches, silicon photonics technology is attractive. The integration of many thermo-optical (TO) switch elements using silicon photonics can offer compactness, low driving power, and high speed. A switching time of the order of 10 μs can be offered by the silicon TO switch. For practical use, we need to meet many other important requirements including low loss, polarization insensitivity, and low cross-talk over wide wavelength range. For the optical switches, optical coupling to single mode ﬁbers is needed. Low loss optical coupling to ﬁbers is still a major issue in the silicon photonics ﬁeld. Fig. 2(b) shows a promising example of optical switches developed by NEC [2]. On the optical switch chip, a silicon waveguide circuit including 152 TO switch elements for the 8×8 multi-cast switch function is formed within a chip size of 12 mm × 14 mm. The port count of 8×8 is actually not so high, but it would be a realistic approach to construct higher port count optical switches by using reasonable scale optical switches as a unit and connecting them in parallel or multi-stage. The optical switch function was formed using rib waveguides with a silicon thickness of 1.5 μm, in which the guided mode ﬁeld diameter (MFD) was about 1.3 μm. There is a large mismatch between the MFDs of the silicon waveguides and the standard single mode ﬁbers (SMFs). Simple overlap integral estimation shows that the above MFD difference should correspond to about 10 dB coupling loss. Thus, spot size converters (SSCs) for adiabatically expanding the MFD at the facet were also formed on a chip. Here the MFD was Fig. 2. (a) Hybrid scheme consisting of electronic packet switches and optical circuit switches. (b) Schematic of 8×8 silicon photonic switch. (c) Photograph of 8×8 silicon photonics switch module. (d)-(f) Measured optical loss, PDL, and cross-talk for all 64 (=8×8) paths of the module. expanded to 4 μm at the chip facet. Then the array of 16 narrow-core ﬁbers having SMFs on the other end was attached on the chip facet. Optical coupling loss per facet was estimated to be less than 1 dB. The assembled 8×8 switch module had a compact size of 2 cm × 5 cm, as shown in Fig. 2(c). Fig. 2(d)-(f) shows measured optical loss, polarization dependent loss (PDL), and cross-talk for all 64 (=8×8) paths of the module. Average optical loss was 15 dB, indicating that module excess loss where 9 dB loss from 1×8 splitting was excluded was 6 dB. Averaged PDL was about 0.6 dB, and cross-talk was less than -35 dB. Low loss, low PDL, and low cross-talk have been conﬁrmed. I I I . PHY- L EV E L O P T ICA L IN T ERCONN EC T MOD E L AND ANA LY S I S In high performance computing systems, interconnect plays an increasingly important role. Optical interconnects promise high bandwidth, low latency, and could improve chip pin performance for manycore processors. We modelled PHY level interfaces for inter/intra-rack, intra-board, and intra-chip optical interconnects, and developed OEIL, a modeling and analysis tool for optical and electrical interfaces and links [3]. Two types of high-bandwidth interconnects are developed for modern computing systems. Low-swing differential electrical interconnects have been widely used in high speed I/O designs. On the other hand, optical interconnects are becoming potential alternatives for electrical interconnects. The basic structures of electrical and optical inter-chip interconnects are shown in Fig. 3. Models are mathematically built for their crosstalk noises, attenuation and receiver sensitivities [3]. 7UDFH 3 U H  P $ S L I L O H U ,Q ' L U H Y U ( O F H L U W D F O 3 L Q / L P P $ L I L L W O L J H Q S U ( D X T L O H ] U ( O F H L U W D F O Q 3 L E   U H G R F Q ( E   L U H 6 D L O U H ] (cid:258) 5 & O U H G Y Q Q D R N F F H R ' D \ W D E   E   ' U H G R F H ' L U H V H D L O U H ] 2XW (cid:258) (a) Electrical Interconnect 3 U H  P $ S L I L O H U ' L U H Y U 2 S L W D F O 3 L Q ( D X T L O H ] U 7 U Q D V L P P $ H F D G Q H H S S I L O U 2 S L W D F O 3 L Q R K 3 W H G R W F H W R U / H V D U )LEHU :DYHJXLGH / L P P $ L I L L W O L J H Q S U 5 & O U H G Y Q Q D R N F F H R ' D \ W D E   E   ' U H G R F H ' L U H V H D L O U H ] (cid:258) ,Q E   U H G R F Q ( E   L U H 6 D L O U H ] 2XW (b) Optical Interconnect Fig. 3. Both the electrical interconnects and optical interconnects have similar components. Additionally, optical interconnects have lasers, photodetectors and transimpendance ampliﬁers, which are used as EO-OE interfaces.             ,Q  :DYHJXGLH DZ N F R & O * W D U H Q H U R DZ DZ DZ ,Q  ,Q  ,Q                          2XWSXW 'ULYHU $1' *DWH /DVHU WE WE WE WE (a) Optical weaving E-O interface             2XW  DZ ' ' ' ' H F U X R 6 N F R & O 3' DZ 3' DZ 3' DZ 3' 2XW  2XW  2XW                          ,QSXW 'ULYHU 5= WR 15= :DYHJXGLH WE WE WE WE WE WE $PSOLIHU WE (b) Optical weaving E-O interface Fig. 4. Optical weaving E-O interface weaves data from multiple electrical channels into one optical channel by an array of microresonators. We propose a mechanism called optical weaving interfaces, which are able to serialize and deserialize optical signals at 	 	 	 	 lower power consumption [4]. The structure of a 4:1 optical weaving E-O interface, which consists of an AND gate, a clock generator, a laser source, a driver and a modulator, is shown in Fig. 4. Different from electrical funneling interfaces, there are four microresonators implemented along the waveguide to modulate the optical signals. These microresonators are designed to modulate the same optical wavelength. In optical TDM systems, they will not interfere with each other because, at any time, only one of the microresonators is activated. The output optical signals have the same waveform as electrical funneling interfaces, which are also NRZ coded.                      (a) Electrical Interconnect          (b) Optical Interconnect Fig. 5. When the frequency or interconnect length is increased, the energy consumption of electrical interconnect is rapidly increased;When the frequency or interconnect length is increased, the energy consumption of optical interconnect is gradually increased. optical interconnect is comparable to that of electrical interconnect if the transmission bandwidth or the interconnect length is less than threshold points. On the other hand, it will be nearly 100% less than that of electrical interconnect if the transmission bandwidth or the interconnect length is greater than threshold points. The energy consumptions of the electrical funneling and optical interfaces are shown in Fig. 6. It shows that optical weaving interfaces enjoy lower energy consumption, smaller area and lower latency than electrical funneling interfaces. Particularly, if the parallel-to-serial ratio of the interfaces is increased from 1 to 64, the energy spent per bit on electrical funneling interfaces is increased by 241%, while the energy spent per bit on optical weaving interfaces is decreased by 62.3%. IV. UN I FIED IN T ER / IN TRA -CH I P O P T ICA L N E TWORK In most designs [5]–[9], either inter-chip or intra-chip optical network is considered. In I2CON [12], we design both intra-chip and inter-chip optical networks jointly to make them cooperate efﬁciently with the carefully designed data channels and arbitration scheme. The overview architecture is as shown in Fig. 7. It contains an inter-chip network and an intra-chip optical network for each chip. Cache/memory subsystem is included in this design. 12.5 10.0 7.5 5.0 2.5 ) t i b / J p ( n o i t p m u s n o C y g r e n E 0 N=64 N=32 N=16 Electrical Funneling OI(64,N) Electrical Funneling OI(32,N) Electrical Funneling OI(16,N) N=1 N=8 N=4 N=2 Optical Weaving OI(64,N) Optical Weaving OI(32,N) Optical Weaving OI(16,N) Fig. 6. Comparison of energy consumptions of electrical funneling OI(M,N) and optical weaving OI(M,N). There are 64/32/16 parallel electrical interconnects and 1 to 64 optical wavelengths. Unit current Io =0.2mA/Gbps. Solid bars stand for energy electrical modules and empty bars stand for optical modules. The length of the optical waveguide is 50cm. The data rate per electrical interconnect is 2Gbps. Data are obtained from analytical models. We developed OEIL, a modeling and analysis tool for optical and electrical interfaces and links [3]. The energy consumptions of the electrical and optical interconnect are shown in Fig. 5. It shows that the energy consumption of ,QWHUFKLS OLQN ,QWUDFKLS OLQN 3URFHVVRU PHPRU\ FKLS &RUHPHPRU\ QRGH Fig. 7. The architecture overview of I2CON. The intra-chip networks facilitate both the on-chip communication and off-chip communication. The architecture of the chip with intra-chip network [22] is as shown in Fig. 8. 3D technology is used to stack an optical layer on the top of electrical layer. On the optical layer, the waveguides snake across the chip, connecting all cluster nodes and memory nodes through optical transceivers, making up the optical data channels. There are private L1 I/D cache for each core and a L2 cache bank for each node. Each node also includes a optical network interface to transmit and receive data and control signals from data channels and control system, respectively. Each node has a node agent, which physically locates in the center of the chip and communicate via electrical links. These node agents deal with the requests from the corresponding nodes logically distributed. In this way, the control and arbitration complexity can be reduced dramatically so that the scalability of intra-chip optical network is better.     Eŝ Eϭ EϬ Eũ EŬ Ϭ ũ ͘ ͘ ͘ ͘ ͘ ͘ ͘͘͘ ͘͘͘ ϭ Ŭ :DYHJXLGH &RUH 1RGH L 1RGH DJHQW M &HQWUDO &RQWURO 0RGXOH (OHFWULFDO :LUH Eŝ ũ Fig. 8. The intra-chip network ﬂoorplan. Data channels are used to transmit optical signals among different nodes through the parallelly aligned waveguides. The waveguides are closed loops, which can facilitate bidirectional transmission together with the dedicated design of optical network interface and optical transceiver. Channel segmentation and channel grouping are the important techniques used to improve the data channel throughput further. The waveguides are virtually segmented to separate sections so that in different sections of the same data channel payload can be transmitted bidirectionally and concurrently. Channel grouping divides all data channels into different groups, only speciﬁc trafﬁc patterns allowed within each group. All possible trafﬁc patterns are spread among these groups. Although the ﬂexibility of data channels is limited by channel grouping, it can relieve the arbitration burden and the network throughput is lifted much. The inter-chip network is composed of the data channels and the corresponding control fabrics. There are N (the maximum number of nodes on a chip) data channels which are parallel to each other with the same design and connection. The control fabrics is composed of the control channels and an arbiter chip. Before accessing the data channel, the nodes are required to send requests to the arbiter chip through the control channels. The arbiter chip will make the arbitration and also conﬁgure the data channels by sending out control information to the data channels. The inter-chip data channel and arbitration also take the advantages of that in intra-chip network design so that the arbitration can be simpliﬁed and the off-chip can make better use of the intra-chip networks and boost the inter-chip performance greatly. The evaluation of performance and power efﬁciency is conducted between I2CON and point-to-point and limited point-to-point network in [10]. The performance evaluation is done for both synthetic and real trafﬁcs. In most cases under same conﬁgurations, I2CON achieves much higher throughput and lower average packet delay. The maximum throughputs 1 2 3 4 5 6 7 8 Uniform Guassian BitComp Tornado Transpose Neighbor Average N o r m a i l h T d e z r u p h g u o t I2CON Point-to-point Limited point-to-point Fig. 9. The maximum throughputs of three designs. 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 FFT RS_dec Fpppp MD N o r m a i l d e z o p w e r u s n o c m p i t n o I2CON Point-to-point Limited point-to-point Fig. 10. Power consumptions of three designs under real applications. of three designs under synthetic trafﬁcs are as shown in Fig. 9. To show the power in the architectures, we adopt the nanophotonic power model proposed in [23]. Fig. 10 shows normalized power consumption of the three networks under real applications. I2CON achieves lowest power consumption for all applications. These results demonstrate the advantages of uniﬁed inter/intra-chip optical interconnect design. V. O P T ICA L CRO S S TALK NO I S E ANA LY S E S Crosstalk noise is an intrinsic characteristic of photonic devices. Crosstalk noise is classiﬁed into two types: coherent and incoherent crosstalk. If the optical propagation delay differences in the network exceed the coherent time of the laser, the crosstalk is incoherent. Conversely, when these differences do not exceed the coherent time of the laser, the crosstalk is coherent. In [24], the coherent crosstalk is proved to either contribute to the noise or cause ﬂuctuation in the signal power. In the following work, the worst-case results of crosstalk noise is shown, where coherent crosstalk is treated as noise. 05 Ȝ  ȜM  ȜQ 05 (OHFWULFDO VLJQDO 6L*H GRSHG Ȝ ȜM ȜQ   05 (OHFWULFDO VLJQDO Ȝ  ȜM  ȜQ 05 ,QSXW 2XWSXW Ȝ ȜM ȜQ  F ,1$&7,9(VWDWH %20( G $&7,9(VWDWH %20( H 3$66,1*PRGH %2)( 6L*H GRSHG I '(7(&7,1*PRGH %2)( 2SWLFDO VLJQDO ZLWKRXW FURVVWDON QRLVH 2SWLFDO VLJQDO ZLWK FURVVWDON QRLVH &URVVWDON QRLVH DZ 2))VWDWH 05 DZ 21VWDWH 05 2SWLFDO ZDYHJXLGH DZ J 2))VWDWH %26( ,QSXW 7KURXJK $GG 'URS Ȝ ȜM ȜQ   K 21VWDWH %26( DZ Ȝ ȜM ȜQ     Ȝ ȜM ȜQ ,QSXW 2XWSXW Ȝ ȜMȜQ   ,QSXW 7KURXJK $GG 'URS ,QSXW 2XWSXW ,QSXW 2XWSXW 5HIOHFWLRQ ,QSXW ,QSXW 2XWSXW  Ȝ ȜQ 7HUPLQDWRU D 2SWLFDO WHUPLQDWRU E :DYHJXLGH FURVVLQJ Fig. 11. Basic optical elements For the analyses of crosstalk noise in interconnection networks, we analyze the crosstalk in basic optical elements and       photonic devices, which are given in Fig. 11. Fig. 11(a) and (b) have depicted the optical terminator and the waveguide crossing respectively. As can be seen, crosstalk noise can be generated with the waveguide crossing, and the reﬂection is considered for the optical terminator. Fig. 11(c) to (h) have respectively demonstrated each optical element in its possible stage: the Basic Optical Modulating Element (BOME), the Basic Optical Filtering Element (BOFE) and the Basic Optical Switching Element (BOSE) [25]. It is noted that, in this ﬁgure, the MR is modulating/detecting/switching with resonant wavelength as λj . Also, in our work, all the Basic Optical Elements follow the Lorentzian transfer function [16], [26]. To verify the impact of crosstalk noise in the optical interconnects, we utilize SNR, the ratio between the received signal and the crosstalk noise power at the detectors, or . We can see that the SNR is indeed independent of the input signal power since the input power of the signal power and the crosstalk noise power cancel each other out. For the noise, our works has considered both coherent and incoherent second-order crosstalk. Besides, Bit error rate (BER) is also an important factor for the performance of an optical interconnection network, which can be calculated based on the SNR [27], and the overall BER of the whole inter-/intra-chip interconnection network has also been derived [13]. Psignal Pnoise Fig. 12. The worst-case SNR in 8×8 WDM Mesh-based, folded-torus-based and fat-tree-based Optical Interconnection Networks We analyzed the worst-case signal power loss, crosstalk noise and SNR in popular network topologies: mesh, foldedtorus, fat-tree and ring. Fig. 12 demonstrates the results of the worst-case SNR in the ﬁrst three topologies, with a network size of 8x8 under different number of wavelengths [14]. Incoherent crosstalk noise is considered in these results. From Fig. 12, crosstalk noise may negatively affect the performance of the WDM interconnection network. The more wavelengths utilized, the lower the worst-case SNRs can be. The demonstrated quantitative results in Fig. 12 are analyzed by CLAP, an automated Crosstalk and Loss Analysis Platform. CLAP can perform the analyses of crosstalk noise, signal power loss and SNR of WDM optical interconnection networks using arbitrary optical routers [28]. Regarding the results of ring-based optical interconnection networks, our results in Table I [13] have provided an overall comparison of the performance of the I2CON [12] intra-chip and Corona [5], with the same network size of 8×8. In this table, we have calculated the results of the ideal and real bandwidth together with the power consumption. A modulation rate of 10Gbit/s is assumed for the ideal case [5], where the worstcase bandwidth results are referred from the worst-case SNR considering both types of crosstalk noise of each structure. Two sets of the worst-case bandwidth have been calculated: one based on the Shannon limit - the maximum theoretical bandwidth which can be achieved under the constraint of crosstalk noise; and the other is achieved by Reed-Solomon error correction coding. ID EA L AND AC TUA L PER FORMANC E : CORONA AND I2 CON IN TRA -CH I P TABLE I Optical structure I2 CON intra-chip Corona data channel Corona broadcast bus Corona control Without crosstalk Bandwidth (Tb/s) Energy (pJ/bit) BER With crosstalk (Worst-case) Bandwidth (Tb/s) Shannon limit RS(255,149) Energy (pJ/bit) 160 160 0.64 0.64 0.005 0.03 118.85 93.49 0.01 137.50 0.04 114.19 93.49 235.32 34.19 0.02 0.5 0.37 58.51 71.42 More importantly, we have analyzed both the inter- and the intra-chip interconnects of I2CON. Our results in [13] have demonstrated that there is a trade off between the number of chips and the number of cores on a chip. For example, with a total of 256 cores, using one chip results in the worstcase BER of 0.03, while using two chips reduces the BER to 0.008. However, if the number of chips is four, the worst-case BER increases to 0.01. Using a single chip is not an optimal solution, while using so many number of chips may degrade the performance of the network. V I . TH ERMA L -O P T ICA L E FFEC T S AND ANA LY S I S Thermal effect is one of the intrinsic characteristics of photonic devices, which potentially results in performance degradation and even functional failure for optical interconnect for inter/intra-rack, intra-board, and intra-chip communication. Thus, a comprehensive study on thermal effects for the whole interconnect network and a speciﬁc design to eliminate or reduce such effect are necessary for the design of the optical interconnect. Fig. 13(a) shows the overview of an M-wavelength WDMbased optical link in optical interconnection network. In general, an optical link in WDM-based network is composed of an E-O interface, an optical path consisted by numerious active and passive switching elements, and an O-E interface. Temperature ﬂuctuation inﬂuences the performance of the optical interconnect mainly by thermo-optical effect, which shows a nearly linear relationship between temperature and refractive index of the material. Since resonant phenomenon occurs in the laser cavities and MRs, both the emitting wavelength of lasers λVCSEL and the resonant wavelength of MRs λMR vary with the change in temperature, as shown in Eq. (1) and (2), where λVCSEL,0 and λMR,0 are the resonant wavelength of the VCSEL and MR at room temperature, TVCSEL and TMR are the temperature of the VCSEL and MR, and ρVCSEL and ρMR are the wavelength shift coefﬁcient of the VCSEL and MR, respectively. λVCSEL = λVCSEL,0 + ρVCSEL (TVCSEL − T0 ) λMR = λMR,0 + ρM R (TMR − T0 ) (1) (2) If the emitting wavelength of the VCSEL and the resonant wavelength of the MR do not align with each other due to the temperature-dependent wavelength shift, the loss of BOMEs, BOSEs, and BOFEs will dramatically increase. For a single wavelength, the change in the loss of active BOSE, where the MR is on-resonant, can be described as Eq. (3) [26], where 2δ is the 3-dB bandwidth of the MR, κ2 p is the round-trip power loss, and κe , κd are the in and out coupling coefﬁcients of the MR. When multiple WDM channels are utilized, the loss of the BOME, active BOSE, parking BOSE, where the MRs are off-resonant, and BOFE can be calculated recursively, and the detailed expressions are shown in [15]. LBOSE a = −10 log (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:3) 2κ2 κd κ2 e + κ2 d + κ2 p (cid:4)2 δ2 (λVCSEL − λMR )2 + δ2 (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (3) In addition to the wavelength shift, temperature ﬂuctuation also has an impact on the emitting power of VCSELs, as shown in Eq.(4), where α is the minimum threshold current, Tp is the temperature at which the maximal gain can be obtained, ε is the slope efﬁciency at 0◦C , β and γ are the coefﬁcients related to the gain property and slope efﬁciency of the VCSEL. PTX = (I − α − β (TVCSEL − Tp )2 )(ε − γ · TVCSEL ) (4) Fortunately, the sensitivity of the receiver, SRX , does not have obvious variation with temperature ﬂuctuation, and the CW oper at ion BOME Laser Laser  D ir ect modu lation (b) E-O inte rface wi th  dire ct m odulat ion (c) E-O inte rface wi th  separa te m odulat ors 0 ... M-1 PD PD BOFE 0 ... M -1 (d) O-E i nterfa ce  . . . E l e c t r o n i c d o m a n i 1 A ctive sw itch ing 0 A ctive sw itch ing 1 A ctive sw itch ing N1-1 E-O  interface E l e c t r o n i c d o m a n i 2 O-E  interface  ... ... Pas sive s wi tch ing 0 Pas sive swi tch ing i ... Pas sive s wi tch ing N2-1 ... ... 0 ... M-1 0 ... M -1 0 ... M-1 0 ... M -1 0 ... M-1 0 ... M -1 BOSE (a) An overview of t he M-wavel ength WDM-based opt ica l li nk in ONoCs  Fig. 13. An overview of an M-wavelength WDM-based optical link in ONoCs connecting two electronic domains. dark current of the photodetector is still kept sufﬁcient low even with large temperature change [29]. If we assume that the optical signal passes N1 active BOSEs and N2 parking BOSEs, the emitting power of the VCSEL need to satisfy Eq. (5) in order to make sure the receiver can properly receive the optical signals PTX ≥SRX + LBOME + N1(cid:2) i=1 LBOSE a,i + N2(cid:2) i=1 LBOSE p,i + LBOFE + L0 , (5) where L0 is the loss from some other sources, including the propagation loss, bending loss and crossing loss, which are nearly thermal-independent. λM R i = λV CSEL i + ρV CSEL − ρM R 2 · (Tmax − Tmin − 2T0 ) (6) With OTemp [16], the thermal effect analysis platform for both WDM-based and single-wavelength optical interconnect, the power consumption of the optical interconnect can be obtained when temperature ﬂuctuates. Since the losses of the BOME, BOSE and BOFE vary with temperature, huge emitting power of the VCSEL is required when the mismatch of the emitting wavelength of the VCSEL and the resonant wavelength of the MR is large, even if we do not consider the limitation of the maximal power that the VCSEL is capable to emit and the nonlinearity of Si waveguide. Furthermore, from the analysis result, an optimal initial device setting can be obtained as shown in Eq. (6) to minimize the power consumption, where λM R i and λV C SEL i are the resonant wavelength of i-th MR and the emitting wavelength of i-th VCSEL at room temperature, respectively. 0 10 20 30 40 50 60 101 102 103 104 Temperature variation ΔT (oC) P o w e r u s n o c m p i t n o ( J p / b ) t i Total power w/o thermal adjustment Total power w/ thermal adjustment (w/ guard rings) Total power w/ thermal adjustment (w/o guard rings) On−chip power w/o thermal adjustment On−chip power w/ thermal adjustment (w/ guard rings) On−chip power w/ thermal adjustment (w/o guard rings) Fig. 14. Worst-case power consumption for one wavelength in an 8wavelength WDM optical link, w/ off-chip laser source, channel spacing 1nm. Fig. 14 shows the worst-case total power consumption for one wavelength in an 8-wavelength WDM optical link. As shown in the ﬁgure, thermal adjustment is indispensable to make the power consumption of the optical interconnect acceptable. In addition, when the temperature ﬂuctuation is large, adding additional guard rings [15] is able to reduce power consumption further. Moreover, some other techniques, including moderately increasing the spacing of the adjacent             WDM channels, minimizing the number of active BOSEs in the optical link and employing parallel-coupled MRs instead of single MR, will decrease the thermal sensitivity of the optical interconnect as well [15], [16], [30]. V I I . CONC LU S ION This paper reviews the latest developments on key aspects of intra-rack, intra-board, and intra-chip optical interconnection network, including photonic devices, PHY-level interconnect modeling, uniﬁed inter/intra-chip optical networks, crosstalk noise, and optical thermal effects. We discussed the opportunities and insights into efﬁcient optical interconnection networks designs, highlight the existing challenges ahead. "
