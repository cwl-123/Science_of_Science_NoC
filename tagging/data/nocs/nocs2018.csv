title,abstract,full_text
Critical Packet Prioritisation by Slack-Aware Re-Routing in On-Chip Networks.,"Packet based Network-on-Chip (NoC) connect tens to hundreds of components in a multi-core system. The routing and arbitration policies employed in traditional NoCs treat all application packets equally. However, some packets are critical as they stall application execution whereas others are not. We differentiate packets based on a metric called slack that captures a packet's criticality. We observe that majority of NoC packets generated by standard application based benchmarks do not have slack and hence are critical. Prioritising these critical packets during routing and arbitration will reduce application stall and improve performance. We study the diversity and interference of packets to propose a policy that prioritises critical packets in NoC. This paper presents a slack-aware re-routing (SAR) technique that prioritises lower slack packets over higher slack packets and explores alternate minimal path when two no-slack packets compete for same output port. Experimental evaluation on a 64-core Tiled Chip Multi-Processor (TCMP) with 8×8 2D mesh NoC using both multiprogrammed and multithreaded workloads show that our proposed policy reduces application stall time by upto 22% over traditional round-robin policy and 18% over state-of-the-art slack-aware policy.","Critical Packet Prioritisation by Slack-Aware Re-routing in On-Chip Networks Abhijit Das∗ , Sarath Babu† , John Jose∗ , Sangeetha Jose† and Maurizio Palesi‡ ∗ Dept. of Computer Science and Engineering, Indian Institute of Technology Guwahati, India † Dept. of Information Technology, Government Engineering College, Idukki, India ‡ Dept. of Electrical, Electronic and Computer Engineering, University of Catania, Italy {abhijit.das, johnjose}@iitg.ac.in, {saratbabu0410, sangeethajosem}@gmail.com, maurizio.palesi@dieei.unict.it Abstract—Packet based Network-on-Chip (NoC) connect tens to hundreds of components in a multi-core system. The routing and arbitration policies employed in traditional NoCs treat all application packets equally. However, some packets are critical as they stall application execution whereas others are not. We differentiate packets based on a metric called slack that captures a packet’s criticality. We observe that majority of NoC packets generated by standard application based benchmarks do not have slack and hence are critical. Prioritising these critical packets during routing and arbitration will reduce application stall and improve performance. We study the diversity and interference of packets to propose a policy that prioritises critical packets in NoC. This paper presents a slack-aware re-routing (SAR) technique that prioritises lower slack packets over higher slack packets and explores alternate minimal path when two no-slack packets compete for same output port. Experimental evaluation on a 64-core Tiled Chip Multi-Processor (TCMP) with 8×8 2D mesh NoC using both multiprogrammed and multithreaded workloads show that our proposed policy reduces application stall time by upto 22% over traditional round-robin policy and 18% over state-of-the-art slack-aware policy. Index Terms—Quality-of-Service (QoS), slack estimation, adaptive routing, input selection, stall time reduction I . IN TRODUC T ION After the paradigm shift towards multi-core systems, limitation in global wires, shared buses and monolithic crossbars are exposed. Packet based NoCs now connect tens to hundreds of components in TCMP based multi-core systems. NoCs are scalable and reliable with predictable and well controlled communication properties [1]. The most fundamental challenges in the design of general purpose TCMPs include devising efﬁcient resource sharing and scheduling policies. Behaviour and interference of applications for fundamental shared resources like NoC [2][3][4], last level cache (LLC) [5][6][7] and memory bandwidth [8][9][10] are explored in different capacities. NoC trivially becomes the most critical shared resource as it is the communication backbone for the entire system. Even other shared resources including LLC and memory bandwidth are dependent on NoC directly or indirectly. We explore the impact of NoC because it has various hidden and indirect This research is supported in part by Department of Science and Technology (DST), Government of India vide project grant ECR/2016/000212. 978but signiﬁcant performance deﬁning factors. Important factors include queueing delay, memory level parallelism (MLP), irregular trafﬁc patterns and unpredictable application interferences. These network-level factors can have a signiﬁcant impact on the application-level performance. A TCMP generally consists of processing elements organised as tiles. Each processing element houses a simple processor, a private L1 cache and a slice of shared distributed L2 cache. Typically, each L1 cache miss triggers an NoC request packet and corresponding reply packet. In an NoC, packets of different applications mainly interact with one another in the routers. The arbitration policy in these routers decide which application’s packet is to be prioritised over others when they request the same output port. Traditional arbitration includes round-robin and age-based policies which are application oblivious, i.e. they treat all application packets equally. However, applications can be heterogeneous in nature with different QoS requirements and hence each of these packets will have different impact on the application-level performance. One of the main reasons for this differential impact is the presence of MLP. Servicing multiple memory requests in parallel reduces the application stall time and criticality of each of these requests to the application depends on MLP to a large extend. Consider the following example: Assume that an application issues two network requests (cache misses), one after another, ﬁrst to a distant tile in the network, and second to a closer tile. The application can continue execution only after the reply of these requests are received. The ﬁrst request packet travels far and hence take more time to return, whereas the second request packet travels less and come back before the ﬁrst packet. Even after the second reply packet arrives, the application continues to stall because the ﬁrst reply packet is expected. Clearly, the second packet is less critical and can be delayed for multiple cycles without adding any stall to the application’s execution. This is because the latency of second packet is hidden under the ﬁrst packet, which takes more time. Thus, the delay tolerance of each packet can be different with respect to its impact on the application’s performance. We study the diversity and interference of packets to design packet-aware NoCs for general purpose TCMPs. We differ978-1-5386-4893-3/18/$31.00 ©2018 IEEE entiate packets based on a metric called slack which is a measure of packet’s criticality. Slack of a packet is deﬁned as the number of cycles the packet can be delayed in the network without affecting application execution [3]. Therefore, packets with available slack (slack-1) are non-critical compared to the packets with no available slack (slack-0). Increasing the latency of slack-0 packets stalls application execution. We propose an NoC architecture that prioritises critical packets in the network by a slack-aware re-routing (SAR) technique. Our SAR routers prioritise lower slack packets over higher slack packets like in Aergia [3]. But when two slack-0 packets have a port conﬂict, we re-route one of them through an alternate minimal path towards destination. Experimental analysis show that our policy effectively improves applicationlevel performance compared to the existing policies. Our main contributions of this paper can be summarised as follows: • We estimate slack of cache miss requests at runtime based on MLP of predecessor misses and incorporate this slack value on NoC packets as a priority. • We adopt a look-ahead routing to facilitate re-routing of slack-0 packets through alternate minimal paths. • We modify baseline routers to prioritise lower slack packets during routing and arbitration and re-route slack0 packets when the desired output port is unavailable. • We qualitatively and quantitatively compare our proposal to traditional round robin and state-of-the-art Aergia [3] policies to assess the performance. I I . MOT IVAT ION Modern TCMPs employ different MLP based methods like out-of-order execution, runahead execution etc. to reduce the penalty of load misses. These methods basically issue parallel memory requests with an intention to overlap future load misses with current load misses. If the application’s behaviour shows MLP in NoC, the latencies of outstanding packets overlap and introduce slack cycles. A. Exploiting Slack and its Diversity If the NoC routers are aware of the available slack, they can take routing and arbitration decisions by prioritising lower slack packets. We identify few cases where exploiting slack information of packets can reduce application stalls. Case 1: Interference between Different Slack Packets: Consider a 64-core TCMP as given in Figure 1. Two applications, one in Core-A (tile 57) and other in Core-B (tile 46) run simultaneously. Core-A encounters two load misses and generates two packets (A0 and A1). The ﬁrst packet A0 is sent to tile 7 and is not preceded by any outstanding packet, hence it has a latency of 13 hops and a slack of 0 hops. In the next cycle, the second packet A1 is sent to tile 40 with a latency of 3 hops. Since packet A1 is preceded (and thus overlapped) by the 13-hops packet A0, it has a slack of minimum 10 hops (13 - 3 hops). Similarly, for Core-B the ﬁrst packet B0 has a latency of 7 hops and a slack of 0 hops while the second packet B1 has a latency of 3 hops and a slack of 4 hops. Figure 1: Illustrative example of slack Figure 2: Slack-0 packets in SPEC CPU2006 benchmarks Packets A0 and B1 interfere at 2 points (tiles 47 and 39) as shown in Figure 1. A traditional application oblivious slack-unaware routing and arbitration policy that prioritises B1 over A0 degrades the application-level performance as A0 is more critical than B1. In contrast if the NoC is slackaware, it will prioritise packet A0 over B1 and reduce the stall time of Core-A without actually increasing the stall time of Core-B. Workload characteristics in Aergia shows that there exists sufﬁcient diversity in slack of packets across various benchmarks [3]. This observation led to the slackaware routing techniques in NoC [11][12]. Case 2: Interference between Slack-0 Packets: Consider another two applications in Core-C (tile 16) and Core-D (tile 18) which also run simultaneously with Core-A and Core-B on the same 64-core TCMP as shown in Figure 1. Core-C generates a packet C0 with a latency of 7 hops and a slack Figure 3: Quadrant and Region of 0 hops. While Core-D generates a packet D0 that has a latency of 5 hops and a slack of 0 hops. Packets C0 and D0 also interfere at 3 points (tiles 18, 19 and 20) as shown in Figure 1. Since both C0 and D0 are equally most critical, delaying either of them degrades the applicationlevel performance. In this case, as per Aergia [3] only one of the packets will get productive port and other will be delayed at least for 1 cycle. There are cases where slack-0 packets are delayed upto 8 cycles due to this port conﬂict. In contrast if the NoC is slack-aware and can forward one of the slack-0 packets through an alternate minimal path, both C0 and D0 can progress in parallel. This reduces the stall time of both Core-C and Core-D. Figure 2 presents the percentage of slack-0 packets in a representative set of 18 SPEC CPU2006 benchmarks. This set is a mix of heterogeneous applications with different network related characteristics. X-axis lists all the benchmarks and Yaxis shows the percentage of slack-0 packets in them. A trend is clearly visible. Most of them have more than 50% slack-0 packets. Therefore, an NoC architecture with simple slackaware routing policy [3] will not guarantee performance. We observe from a 64-core workload mix running on an 8x8 2D NoC that there are upto 34% cases where two slack-0 packets have port conﬂicts in the NoC routers. We address this issue with a novel re-routing technique. I I I . SAR ARCH I T EC TUR E Our proposed SAR routers perform online estimation of slack and alternate minimal path for routing and arbitration. A. Slack Estimation We estimate slack with respect to outstanding network transactions (L1 miss requests). We deﬁne slack of a packet as the difference between the maximum expected latency of its predecessor (i.e. any outstanding packet that was injected before this packet) and its own expected latency with proper adjustments on injection time. This latency is based on the minimum distance to be traversed in the network by a packet. Literature has other indirect metrics like L2 cache access status (hit or miss), number of miss predecessors (predecessors Figure 4: Alternate minimal path re-routing of a packet that are L2 cache miss) etc. which also correlates with slack and criticality of packets. However, such estimations become computation (hit/miss predictor) and storage expensive (miss predecessor’s list). We intuitively assume all L2 cache access are hits and avoid the off-chip slack computation as it is irregular and cannot be quantised accurately. We modify the structure of L1 miss status handling registers (MSHRs) to include predecessor related information. Before a cache miss request packet is injected into the network, slack is computed using this information from MSHRs. The slack is then quantised as a 1-bit value (Slack) and stored in the packet header. All the slack-0 packets are quantised as 0 and all higher slack packets are quantised as 1. This Slack bit is used to enable priority based routing and arbitration. B. Minimal Path Estimation Slack based priority policy is used to make sure that a slack0 packet is not delayed in any router. But when two slack-0 packets compete in a router for a single output port one has to be delayed. Rather than delaying a slack-0 packet, we explore the possibility of assigning another productive port to one of the slack-0 packets by re-routing. Re-routing is a technique where a packet is forwarded to an intermediate router within the minimal quadrant of current router and destination router. This makes sure both the conﬂicting slack-0 packets get a productive port. SAR routers use some additional metrics to estimate alternate minimal path for packet forwarding. When a packet with destination router T leaves a router S to N (N is neighbour of S), two 1-bit metrics; Quadrant and Region is computed and quantised in its header. Quadrant and Region bits indicate the relative position of T with respect to N. If N and T are on an axis of the N, i.e. on the same row or same column then the Quadrant bit is set to 0 else 1 as shown in Figure 3. If Quadrant=1 then Region=0 indicates T is in upper region of N and Region=1 indicates T is in lower region of N. If Quadrant=0 then Region bit is irrelevant. This Quadrant and Region bit update happens on each router before the packet moves to its crossbar stage. Region helps to identify an alternate minimal path towards destination if the desired output port is not available at N. A packet with Quadrant bit set to 1 can be re-routed at N. Figure 5: SAR priority vector 0 1 0 1 0 1 Slack-0 packets All other packets Destination is on axis (X/Y) of N Destination is on quadrant Destination is on upper region Destination is on lower region Slack Quadrant Region Inter Dest 000 - 111 Last router in Y direction if YX routing is used at N Red Flag Loc Port 0 1 0 1 Packet is not re-routed Packet is re-routed Nothing Packet is sent to local port at Inter Dest Table 1: SAR priority vector description Another metric called Inter Dest stores the address to be used for re-routing using alternate minimal path. It is the last router in Y direction from N if YX routing is used to reach destination of the packet. Figure 4 shows Inter Dest (router I) with an example and veriﬁes its position on the minimal path towards destination. For our evaluation of an 8×8 2D mesh, a 3-bit Inter Dest along with a 1-bit Red Flag is used. Only the column number (3-bits) is stored, as Inter Dest (router I) is on the same row as that of actual destination (router T). If the Red Flag bit is 0, Inter Dest is invalid. Another 1-bit metric Loc Port is used for deadlock prevention (will be discussed when deadlock is addressed). An 8-bit SAR priority vector (as shown in Figure 5) that incorporates all the above discussed metrics is added on the head ﬂit of each packet. Table 1 describes the ﬁelds of SAR priority vector. C. Router Microarchitecture Modiﬁcation Architectural block diagram of SAR router microarchitecture is presented in Figure 6. Like a generic 2D mesh baseline router, SAR router also has 5 input and 5 output ports/channels; one from each direction (east, west, north and south) and one from the local tile (through network interface). North and south input ports have additional demultiplexers (D1 and D2) to redirect packets to local input port. Multiplexers (M1/M2/.../Mn) in local input port send the re-routed packets to the appropriate VCs. SAR routers use XY routing and wormhole switching where only the head ﬂit participates in routing and arbitration. We use round-robin policy in baseline routers and slack-aware re-routing policy in SAR routers for performance comparison. The Routing Computer (RC), VC Arbiter (VA) and Switch Arbiter (SA) units are same as that of baseline routers. Two additional units Packet Pre-processor (PP) and Look Ahead Re-router (LR) facilitates the technique of SAR. Figure 6: SAR router microarchitecture Packet Pre-processor Unit (PP): This unit is an addition to the baseline routers and works in parallel across all input ports. The ﬁelds in SAR priority vector is used by this unit for initiating re-routing operations for every incoming head ﬂits. This unit works in conjunction with 4-bit Output Port Select (OPS) structure to identify port conﬂicts of slack-0 packets. PP unit identiﬁes all slack-0 packets and direct them towards productive output ports by enabling re-routing if required and possible. The working of PP unit is presented in Algorithm 1. Input Algorithm 1: Working of Packet Pre-processor unit (PP) : 8-bit SAR priority vector, Output Port Select (OPS), destination (T ) Output: Identiﬁcation of minimal output port Loc Port = 0 if Slack == 0 && Quadrant == 1 then if desired output port (E/W) not marked on OPS then Mark East (E) or West (W) output port on OPS else if Region == 0 && N not marked on OPS then Mark North (N) output port on OPS Swap column bits of T with Inter Dest Red Flag = 1 else if Region == 1 && S not marked on OPS then Mark South (S) output port on OPS Swap column bits of T with Inter Dest Red Flag = 1 else break else break Look Ahead Re-router Unit (LR): This is another additional unit in SAR routers. LR unit uses the next router information from RC unit and calculates alternate minimal path related metrics in advance only to be used by the PP unit of the next router. Algorithm 2 describes the working of LR unit in proposed SAR routers. Input Algorithm 2: Working of Look Ahead Re-router unit (LR) : 8-bit SAR priority vector, next router (N ), destination (T ) Output: Quadrant, Region and Inter Dest if Slack == 0 then if T == N && Red Flag == 1 then Replace column bits of T with Inter Dest Red Flag = 0 Loc Port = 1 else if T != N then row diff = row of T - row of N col diff = column of T - column of N if row diff == 0 (cid:107) col diff == 0 then Quadrant = 0 else Quadrant = 1 if row diff > 0 then Region = 0 else Region = 1 Temp Dest = N + row diff * network radix Inter Dest = column bits of Temp Dest else break else break LR unit works in parallel with VA and SA units since the metrics it calculates are used only by the next router. Since LA unit is not in the critical path of router pipeline it incurs no additional delay. In our SAR routers, each virtual channel has an extra priority ﬁeld which stores the 1-bit Slack value of the head ﬂit when it reserve the channel. This ﬁeld is used by the body ﬂits for priority based arbitration. An illustrative example of packet re-routing from router D is given in Figure 7. For packet C0, the dashed line through routers D, P, and T indicate the original path and the solid line through routers D, I and T indicate the re-routed path. D. Comparison and Design Challenges We compare the effectiveness of our technique with Aergia [3] that estimates slack in packets and prioritises lower slack packets over higher slack packet during VC and switch arbitration. Aergia also uses batching to prevent higher slack packets from starvation. All the same slack packets within a batch are treated as equal in Aergia and prioritised at random. But we have seen in Figure 2 that almost across all benchmarks slack-0 packets dominate. Hence, Aergia suffers from performance degradation when one slack-0 packet is prioritised over other. Figure 7: Illustrative example of slack-aware re-routing In contrast, our SAR policy works similar to Aergia to prioritise lower slack packets but also re-routes slack-0 packets in alternate minimal path when required. We do not use batching to prevent starvation as our 1-bit slack based priority does not add any signiﬁcant unfairness to the proposed architecture. Our proposal can be used as a complimentary policy with any other packet prioritisation technique. Starvation: When we evaluate our proposed SAR architecture we observe that our 1-bit slack based priority does not add any signiﬁcant unfairness to the system when compared to traditional round-robin policy. Thus, we do not use any additional metric for starvation prevention. Our proposed policy can always be extended with techniques like batching [2][3]. Livelock: In SAR routers, a packet always travels on a minimal path towards destination whether or not it is rerouted. Lower slack packets are prioritised over higher slack packets and slack-0 packets are re-routed through alternate minimal path; but forward progress is always ensured. Hence, the proposed SAR architecture is livelock free. Deadlock: Our SAR routers use XY routing where packets are ﬁrst routed in X direction followed by Y direction. However, when a packet is re-routed through an alternate minimal path, it takes an early Y-direction as shown in Figure 7 (rather than routers D, P, and T, packet C0 takes routers D, I, and T). After reaching Inter Dest (router I), if the packet attempts to take X-direction again, then it violates XY routing which may lead to deadlock. To prevent this situation, a 1-bit Loc Port is used in SAR priority vector. When Loc Port is set to 1, the packet after reaching router I is sent to the local input port VC. The demultiplexers (D1 and D2) placed in north and south input ports will extract the packet and add it to local port Processor L1 cache L2 cache NoC Packets Benchmarks 64 OoO x86 cores 32KB, 4-way, 64B lines, private 512KB×64 cores, 16-way, 64B lines, shared SNUCA 8×8 2D mesh, 4 VCs/port, 128-bit ﬂit channel 1-ﬂit request, 5-ﬂit reply SPEC CPU2006 (multiprogrammed), PARSEC (multithreaded) Table 2: Simulation conﬁguration # 1 2 3 4 5 6 7 Benchmark Slack-0% MPKI cactusADM 25.47 Low soplex 29.82 Low povray 32.73 Low specrand 45.25 Low namd 50.49 Low lbm 52.14 High bzip2 52.36 High # Benchmark 8 gobmk 9 libquantum 10 milc 11 blackscholes 12 ferret 13 streamcluster 14 x264 Slack-0% MPKI 66.15 High 68.03 Low 70.12 Low 44.59 Low 46.72 High 48.28 Low 48.65 High Table 3: Benchmark characteristics VC via multiplexers (M1/M2/.../Mn). From this local input port, the packet can take X-direction towards destination like a newly injected packet. Thus, even though we use both XY and YX routing by incorporating local port VC, we prevent deadlock in the proposed SAR architecture. IV. EX PER IM EN TA L S E TU P In this section, we describe the experimental framework, the metrics and application workloads used for performance evaluation and the trade-offs in the choice of design parameters. A. Simulation Setup We implement the proposed SAR architecture on cycleaccurate, trace-driven BookSim [13] simulator. The memory traces are generated by event-driven gem5 simulator with Ruby [14]. We extend the Ruby memory model on gem5 and modify the structure of L1 MSHR to include latency based slack calculation. Every newly injected packet refers these modiﬁed MSHR entries to get predecessor related information. Modiﬁed router microarchitecture is modelled in BookSim to enable slack-aware re-routing policy for priority based routing and arbitration. BookSim driven by gem5 traces forms the simulation framework for our performance evaluation. Table 2 provides the conﬁguration details of our simulation including processor, cache and NoC parameters. B. Evaluation Metrics We evaluate the existing and proposed policies using different performance metrics. We deﬁne network stall time (NST) as the number of cycles an application stalls waiting for a network packet. We assume all L2 access are hits as we want to identify the effects of NoC alone. We deﬁne usage wait time (UWT) as the number of cycles a reply packet waits from arrival at the source tile until being used by an application. UWT shows how early or late reply packets arrive at the source tile than necessary. We also deﬁne a metric called reply difference time (RDT) as the number of cycles between the arrival of ﬁrst and last ﬂits of the reply packet at the source Figure 8: Effect on network stall time (NST) tile. RDT shows how long an application may stall due to the delayed arrival of remaining ﬂits of a packet after the head ﬂit has arrived. Ideally, we need lower NST, UST and RDT for better application-level performance. C. Application Categories and Characteristics We use both multiprogrammed (MP1–MP7) and multithreaded (MT1–MT4) application workloads for performance evaluation. For multiprogrammed workloads, we use SPEC CPU2006 benchmarks where each core runs a separate application. For multithreaded workloads, we use PARSEC benchmarks where each core runs a separate process/thread but of a single application. In total we study 14 different benchmarks, 10 multiprogrammed and 4 multithreaded. To evaluate our proposal, we create different workloads of varying network characteristics with SPEC CPU2006 and PARSEC benchmarks. We estimate percentage of slack-0 packets to identify the criticality of benchmarks. We also calculate misses per kilo instructions (MPKIs) to estimate the network load contributed by the respective benchmarks. The characteristics of benchmarks are presented in Table 3. The workload formation is presented in Table 4 with description. For example, workload MP1 consists of 32 instances of high MPKI benchmarks (16 cores run bzip2 and another 16 cores run lbm) and 32 instances of high slack-0% benchmarks (16 cores run milc and another 16 cores run libquantum). V. P ER FORMANC E EVA LUAT ION We compare SAR to baseline round robin and state-of-theart Aergia policies based on NST, UWT and RDT for both multiprogrammed and multithreaded application workloads. The plotted result for each workload is averaged over 8 different spatially scheduled combinations. We also present router critical path, area and power overheads of SAR routers. A. Effect on NST Figure 8 shows the normalised NSTs of workloads with respect to the baseline round-robin policy. Round-robin delay packets during port conﬂicts irrespective of their load and criticality. This is because local round-robin policy is applicationoblivious. SAR reduces stall time for all workloads. Signiﬁcant reduction in stall time can be seen for workload mixes of Workload MP1 MP2 MP3 MP4 MP5 MP6 MP7 MT1 MT2 MT3 MT4 bzip2(16) bzip2(16) specrand(16) specrand(16) milc(16) milc(16) cactusADM(16) Representative Benchmark Combinations lbm(16) milc(16) lbm(16) cactusADM(16) namd(16) milc(16) namd(16) cactusADM(16) libquantum(16) cactusADM(16) libquantum(16) gobmk(16) soplex(16) povray(16) blackscholes(64) ferret(64) streamcluster(64) x264(64) libquantum(16) soplex(16) libquantum(16) soplex(16) soplex(16) povray(16) gobmk(16) Workload Characteristics 32 high-MPKI with 32 high-slack-0% benchmarks 32 high-MPKI with 32 low-slack-0% benchmarks 32 low-MPKI with 32 high-slack-0% benchmarks 32 low-MPKI with 32 low-slack-0% benchmarks 32 high-slack-0% with 32 low-slack-0% benchmarks 48 high-slack-0% with 16 low-slack-0% benchmarks 16 high-slack-0% with 48 low-slack-0% benchmarks 64 threads of the benchmark; 1 per core 64 threads of the benchmark; 1 per core 64 threads of the benchmark; 1 per core 64 threads of the benchmark; 1 per core Table 4: Core-wise application scheduling for various workload mixes Figure 9: Effect on usage wait time (UWT) Figure 10: Effect on reply difference time (RDT) high load and high slack critical benchmarks. We observe highest NST reduction (22% over round-robin and 18% over Aergia) for workload MP6 as it consists of 75% of slack-0 rich benchmarks (refer Table 4). Similarly, for MP1 also we see a very good NST reduction as it is a mix of high load and high slack critical benchmarks. In MP3 we achieve only 5% reduction over Aergia because it has fewer port contentions due to low rate of packet injection (low MPKI benchmarks). For multithreaded workloads (MT1–MT4), due to the inherent DNUCA based assignment of L2 address space, majority of L1 cache misses travel less to corresponding L2 cores. This results in either no slack or very little slack for cache misses. Hence there are very less opportunities to apply slackaware re-routing leading to marginal NST reduction with our technique. Even Aergia gets little improvement on 3 out of 4 multithreaded workloads. B. Effect on UWT Figure 9 shows the normalised UWTs of workloads with respect to the baseline round-robin policy. A reply packet has UWT if it reaches the source tile earlier than needed. While the packet has reached, at least one of its predecessors is still in the network. This might happen by penalising peer packets during port conﬂict at intermediate routers. Another possibility of having a UWT is that the packet may have very high slack which is not fully used during port conﬂicts. By our prioritisation technique a slack-1 packet will never delay a slack-0 packet. We observe that SAR reduces UWT signiﬁcantly. It varies from 5% reduction over round-robin in MP3 to 25% reduction over round-robin in MP1. For multithreaded workloads, SAR perform much better than Aergia which uses multi-bit slack value with batching. Results show that even with single-bit slack value without batching we can avoid starvation. In our case any slack above 0 whether it is between 1 and 5 (low slack) or above 5 (high slack), all are represented as slack-1 packets. In MT2 we ﬁnd that low slack packets are over-penalised leading to higher UWT than round-robin and Aergia. In MT1, MT3 and MT4 all slack-0 and slack-1 packets are received just in time. C. Effect on RDT Since we consider 128-bit ﬂit channel and 64B cache lines (blocks), every 1-ﬂit cache miss request packet generates a 5-ﬂit reply packet (1 head ﬂit, 4 body ﬂits) that bring the cache block from L2 tile to the L1 tile. Application can resume execution only if all the four body ﬂits of reply packet reach the source tile. Our technique facilitates forwarding body ﬂits of reply packets as soon as possible. Hence RDT is a very important metric that contribute to performance evaluation. Figure 10 shows the normalised RDTs of workloads with respect to the baseline round-robin policy. In our prioritisation policy, reply ﬂits get forwarded without any interleaving. Due to which the body and tail ﬂits reach the source tile without much delay. Aergia too have good RDT reduction on an average. SAR achieves an average RDT reduction of 14% over Aergia in multiprogrammed workloads. For multithreaded workloads since there is not much slack diversity; all the packets are more or less equal and hence the reply packets are received just in time. Round-robin performs better than Aergia because there is no slack diversity. The unnecessary level of slack based priority and negative effects of batching is the reason for this behaviour. D. Effect on Router Critical Path, Area and Power We implement SAR router microarchitecture in Verilog and synthesize using Synopsys Design Compiler with 65nm cell library to obtain timing characteristics. We assume 65nm technology for an NoC operating at 1GHz frequency with an inter-router link delay of 1 cycle. We use the traditional 2cycle pipelined router with ﬁrst cycle for PP and RC units (refer Figure 6). Even though PP is in the critical path, the combined combinational delay of PP and RC is 7% lower than the combined combinational delay of VA and SA units that constitute the second cycle stage. Our LR unit works in parallel to VA and SA units. The experimental observation that VA and SA stage determines the pipeline latency is already established [15]. Hence, SAR routers can be operated with the same pipeline frequency. Our additional units incur a router area overhead of 1.7% and static energy consumption of 2.1%. We compute the dynamic power dissipation estimates of SAR using Orion 2.0 [16]. Dynamic power consumption of NoC using SAR is 7.5% lower than using Aergia routers due to effective re-routing and reduction in latency of slack-0 packets. This compensates for the minor area and hardware overhead. V I . R ELAT ED WORK Criticality: Available literature has proposals that target criticality of data and instructions [17][18][19][20]. Cache miss criticality is explored with MLP based proposals [21][22]. Memory scheduling is explored with bank level parallelism [23][24]. Slack based criticality is studied for both performance and power optimisation [3][11][12]. But the impact of slack-0 packets are not observed before. Prioritisation: Other than traditional round-robin and agebased prioritisation, literature also has QoS [25][26][27] and application-aware [2][4][28] prioritisation policies. There are prioritisation proposals based on latency-sensitivity of NoC packets [29][30]. While most of the available proposals aimed for guaranteed service or fairness, our aim is to reduce application stall time and improve system performance. Furthermore, available proposals assign static priority to improve real-time performance. In contrast, our proposal computes dynamic priority for routing and arbitration. V I I . CONC LU S ION By understanding the diversity and interference of packets we propose a policy that prioritises critical packets in NoC. We present SAR, a slack-aware re-routing technique that prioritises lower slack packets over higher slack packets and re-routes slack-0 packets through alternate minimal path towards destination. Experimental analysis show that our policy improves system performance over existing policies for both multiprogrammed and multithreaded workloads. The performance gain is achieved with only a negligible area and static power overhead. We believe SAR routers can be good design alternative for TCMPs that run time critical applications. "
AxNoC - Low-power Approximate Network-on-Chips using Critical-Path Isolation.,"Various parallel applications, such as numerical convergent computation and multimedia processing, have intrinsic tolerance to inaccuracies that allow soft errors, i.e. bit flips, on a chip. However, existing Network-on-Chips (NoCs) guarantee error-free data transfer; thus, encountering limits to reduce the power consumption. In this context, we propose an approximate dual-voltage NoC, called AxNoC. An AxNoC router uses a per-flit look-ahead power management so that headers and important-data flits are perfectly transferred at a high voltage while the remaining flits may incur bit flips by decreasing the supply voltage. An AxNoC router isolates the critical path when the supply voltage is low since such a critical path is enabled only at high voltage. The critical path isolation enables low-voltage operation to work at the same operating frequency at high voltage. An AxNoC router was implemented using a 28nm process and the evaluation results illustrate its efficiency to reduce the power consumption reaching up to 43% while incurring a small area overhead that does not exceed 6.2%. We also demonstrate that AxNoC exhibits an acceptable accuracy illustrated in a sufficiently small geomean of error.","AxNoC: Low-power Approximate Network-on-Chips using Critical-Path Isolation Akram Ben Ahmed∗ , Daichi Fujiki† , Hiroki Matsutani∗ , Michihiro Koibuchi‡ , and Hideharu Amano∗ ∗Keio University, Yokohama, Japan †University of Michigan, MI, USA ‡National Institute of Informatics, Tokyo, Japan Email: blackbus@am.ics.keio.ac.jp Abstract—Various parallel applications, such as numerical convergent computation and multimedia processing, have intrinsic tolerance to inaccuracies that allow soft errors, i.e. bit ﬂips, on a chip. However, existing Network-on-Chips (NoCs) guarantee error-free data transfer; thus, encountering limits to reduce the power consumption. In this context, we propose an approximate dual-voltage NoC, called AxNoC. An AxNoC router uses a per-ﬂit look-ahead power management so that headers and importantdata ﬂits are perfectly transferred at a high voltage while the remaining ﬂits may incur bit ﬂips by decreasing the supply voltage. An AxNoC router isolates the critical path when the supply voltage is low since such a critical path is enabled only at high voltage. The critical path isolation enables low-voltage operation to work at the same operating frequency at high voltage. An AxNoC router was implemented using a 28nm process and the evaluation results illustrate its eﬃciency to reduce the power consumption reaching up to 43% while incurring a small area overhead that does not exceed 6.2%. We also demonstrate that AxNoC exhibits an acceptable accuracy illustrated in a suﬃciently small geomean of error. I. Introduction Computers approximate numerical values of input data where multiple processors access a shared variable in a nondeterministic order. It is thus diﬃcult to completely remove potential computation errors. As a result, it has become important to work around the presence of errors rather than providing completely error-free designs. In this context, approximate computing [1] has been proposed to exploit the trade-oﬀ between quality of results, throughput and power consumption with the acceptance of unreliable computer components, such as memory [2], [3] and processors [4], [5]. In fact, approximate computing can be suitable for emerging applications in many domains like machine learning, image/video processing, pattern recognition and big data analytics [6], [7]. Such applications produce acceptable results in spite of soft errors, i.e. bit ﬂips. Therefore, approximate computing can be used as a viable solution to achieve better performance and energy eﬃciency while relaxing the accuracy constraints of these applications within a tolerable error margin. In contrast, Network-on-Chips (NoCs) have not attempted to exploit approximate computing so as to reduce the power consumption of a chip. The power consumption of NoCs has become a dominant factor to design a chip since the total power consumption can become a major obstacle for improving the performance. For instance, in the MIT RAW microprocessor [8] the interconnect consumes 36% of the overall system power, while in the case of the Intel TeraFLOPS processor [9], it consumes up to 28%. Generally, there is a design trade-oﬀ between power consumption and performance; thus, a typical way to reduce the power consumption relies on the reduction of the operating frequency and voltage [10]. In this work, we take a radically diﬀerent approach, approximate computing, to reduce the power consumption of NoCs, while guaranteeing no performance penalty. Based on the study listed in the Appendix, we analyzed the relationship between the Bit Error Rate (BER) and supply voltage (Vdd) for a conventional NoC One of the striking observations from this analysis is that the reduction of supply voltage dramatically causes higher BER. For instance, reducing the voltage by 0.1 V increases the BER by almost 100×. These analysis results are consistent with those of a prior work [11]. Consequently, since some parts of approximate applications have inherent relaxed BER, there is no need to transfer this relaxed data at a perfect voltage. Instead, it can be transferred at a relaxed (lower) supply voltage. In this fashion, the power consumption of NoCs can be drastically reduced. Motivated by the aforementioned relationship between the execution accuracy and power consumption, we propose a router that is capable of providing a per-ﬂit dual-voltage power management for approximate and perfect data transfer. Headers or critical ﬂits are perfectly transferred at high voltage. The remaining ﬂits are approximated and may include bit ﬂips, since a tolerable error margin can be allowed, by decreasing the voltage with the same operating frequency. This eﬃciently supports the inter-processor-core, cache-line and memory access in which a bit ﬂip is acceptable, only in the low-rank fraction bits of the IEEE 754 standard ﬂoatingpoint number, on various CMP applications. Our main goal is to make a case for error-prone NoCs, called AxNoC, that takes into consideration the inherent relaxed BER of approximate applications. In other words, as long as there is approximate data, AxNoC can transfer it in a more power-eﬃcient way while guaranteeing undegraded performance. 978-1-5386-4893-3/18/$31.00 ©2018 IEEE The main contributions of this work are listed as follows: • Router Design: We design a dual-voltage ﬁxed-pipeline and ﬁxed-frequency router that isolates the critical path to transmit accurate ﬂits at high voltage (1.1 V) and approximate ones at low voltage (0.7 V). Using a 28nm process, the proposed router is placed and routed, then the post-layout timing analysis is performed at a 1GHz frequency to extract detailed timing, power and area characteristics. • Router Behavior: Each on-chip router uses a look-ahead per-ﬂit dual-voltage method so that the data path works at a high voltage just before an incoming header ﬂit arrives. Such policy prevents any performance degradation. • Application: We tune the Quality-of-Results (QoR) by varying the bit protection level and BER for various approximate-computing applications. Bit ﬂips in the larger fraction bits of the IEEE 754 standard ﬂoatingpoint number can be acceptable. Our recommendation is to set the BER between 10−5 and 10−7 for approximatecomputing applications when supplying Vdd-low in an AxNoC. II. Related Work A large number of studies have been conducted to reduce the power consumption in NoCs, such as Dynamic Voltage and Frequency Scaling (DVFS) and power gating. They usually explore the trade-oﬀ between performance and power consumption/area, i.e. the reduction of the power consumption at the expense of the area and performance penalty. DVFS [10] is a primary power saving technique that regulates the operating frequency and supply voltage in response to the applied load. However, DVFS introduces some diﬃculties when there is a large number of entities (or power domains), in which the supply voltage and clock frequency are controlled individually in a chip and the operating frequencies of two neighboring power domains must be 1:k (k is a positive integer). Otherwise, an asynchronous communication protocol, which introduces a signiﬁcant overhead, is required between them. Local DVS was presented in [12] to provide each pipeline stage with its own voltage controller, thus allowing each stage to choose its own optimized voltage. Runtime power gating [13] is another approach to reduce the power consumption of routers. However, it suﬀers from a wakeup latency to activate the sleeping components. Therefore, a sophisticated wakeup mechanism is required to mitigate the wakeup latency. When it comes to data approximation, most of the already conducted works in this domain mainly focus on exploiting approximation for data storage [2], [3] or computation [4], [5]. In other works, oﬀ-chip approximate networks are considered for wireless local area networks [14], consumer applications on the Internet [15], and for optical interconnection networks in data-centers or HPC systems [16]. The above works, in addition to many others, have shown the ability of approximation in reducing the energy consumption or enhancing the performance in conventional memory storage and computation processors. However, how NoC systems can take advantage of such beneﬁts remains a question that has not been properly addressed. To the best of our knowledge, the only work that tried to exploit the beneﬁts of approximation in NoC systems is the one presented in [17]. The authors proposed a hardware data approximation framework, named APPROX-NoC, for highperformance NoCs which alleviates the impact of heavy data communication stress by leveraging the error tolerance of applications. The main idea behind APPROX-NoC is to reduce the transmission of approximately similar data in the NoC by delivering approximated versions of precise data to improve the data locality for higher compression rate [17]. APPROXNoC showed promising results as it could achieve up to 9% latency reduction and 60% throughput enhancement when compared to conventional NoCs. While APPROX-NoC targeted the performance improvement by reducing the amount of data traveling the network, our goal is diﬀerent. Our proposed AxNoC exploits the data approximation to reduce the power consumption without any performance penalty. Moreover, it could work together with previously proposed approximate-computing techniques. III. Proposed AxNoC Router A. Pipeline Structure We considered a 128-bit wormhole router that has ﬁve physical channels (X+, X-, Y+, Y- and Core direction). Each input physical channel has four Virtual Channels (VCs), each of which contains a 4-ﬂit input buﬀer, while each output physical channel has a single 1-ﬂit output buﬀer. The packet processing in the router can be divided into two cycles: Next Route Computation (NRC) and Virtual-channel / Switch Allocation (VSA) in parallel, and Switch Traversal (ST). Besides, a single cycle is required for the link traversal (LT) between routers. B. Dual-Vdd for Critical Path Isolation An AxNoC router manipulates two diﬀerent supply voltages: Vdd-high for boosting the critical path and Vdd-low for the remaining paths. In this fashion, a trade-oﬀ between power consumption and critical path delay can be wisely balanced. The adopted critical-path isolation requires that our ﬁxedpipeline-depth router should only meet two conditions for a given operating frequency: • Per-packet operation (NRC,VSA) satisfying its critical path delay at Vdd-high. • Per-ﬂit operation (ST,LT) satisfying its critical path delay at Vdd-low. The VSA stage is typically on the critical path. Therefore, the per-packet operation always works at Vdd-high in an AxNoC router, and it should be always isolated from the router’s components that are operating at Vdd-low. Its per-ﬂit behavior is described in the remaining parts of this section. C. Per-ﬂit Power Control To support the per-ﬂit dual-voltage operation in AxNoC, we adopt the Fine-Grained Variable Vdd (FG-VV) method [18]. In an AxNoC router, illustrated in Figure 1, the voltage levels (a) (b) Fig. 2. Look-ahead voltage transition: (a) timing diagram (b) waveform snapshot obtained from the HSIM simulation. Consequently, it is always transferred at Vdd-high, as depicted in Figure 2. In this fashion, its correctness can be guaranteed while traveling from one router to another. As for the remaining payload ﬂits, their transmission mode (i.e., accurate or approximate) depends on the running application. In fact, once the number of approximate payload ﬂits is decoded from HEAD, the current router conﬁgures the voltage accordingly depending on the type of the expected payload ﬂits. For instance, DATA1, DATA2 and DATA3 in Figure 2 are assumed to be approximate. For this reason, the voltage drops to Vddlow as soon as the HEAD is forwarded to the next router (i.e., during the LT stage). The voltage is kept at Vdd-low when receiving DATA2 DATA3 and it is changed back to Vddhigh only when receiving the HEAD of the next packet. Since the voltage transition is triggered by the upstream router one cycle before the HEAD ﬂit arrives at the downstream router, the high-to-low voltage transition in an upstream node must be performed at the same time as the low-to-high transition in the corresponding downstream node. This behavior can be observed in RouterA and RouterB (Cycle-3) as well as in RouterB and RouterC (Cycle-6). F. Protection Metadata The AxNoC provides ﬂit-level data protection by high voltage supply. To eﬃciently operate this bit-protection scheme, we rearrange the data alignment in a packet payload. We ﬁrst assume that one packet conveys a 64-byte cache-line, and one ﬂit contains a 128-bit payload. As we previously mentioned, the ﬁrst ﬂit (i.e., HEAD) needs to be protected every time, as it contains important data (e.g., routing information and payload accuracy level). The packet payload is partitioned over the following four body-ﬂits. In case that the entire data of the payload is error-tolerant, all body-ﬂits can be transferred at Vdd-low. On the other hand, if some parts of data are essentially protected, e.g. exponent of a ﬂoating point, the error-sensitive bits are orchestrated on top of the packet payload. The router needs to be notiﬁed how many cycles Fig. 1. AxNoC router architecture. “VS"" and “LS"" stand for Voltage Switch and Level Shifter, respectively. of each input and output channel are independently varied. In fact, Vdd-high is only enabled along the active data-path in which a packet proceeds on a router while the other parts of the router remain working at Vdd-low. Additionally, the crossbar in this router is decomposed and merged with each output channel to simplify the voltage control of the input and output channels. In this fashion, all the router’s components can work at the same frequency, even when the ﬁne-grained power control is applied. To perform this ﬁne-grained power control, a “Power Controller” is required to manipulate the added “Level Shifters” and “Voltage Switches”. D. Look-ahead Power Management Switching the supply voltage entails a certain delay. In particular, when the voltage is reconﬁgured from Vdd-low to Vdd-high. A naive implementation keeps an error-sensitive wait for a certain number of clock cycles; however, this would signiﬁcantly degrade the overall performance. Although the used “Voltage Switches” in AxNoC can keep this delay to a single clock cycle, we should reduce the performance overhead caused by the waiting time for the voltage switching. To this aim, the look-ahead power management is introduced. A router can use the look-ahead wake-up method [13] to detect which input and output channels in the router are used for the incoming packets. With this method, the router can preliminarily start the voltage shift after the next packet’s arrival is detected. To process the per-packet operations at Vdd-high, the voltage transition from Vdd-low to Vdd-high should be started 1-cycle before the packets reach the router. Moreover, for deterministic routing (e.g., XY routing), lookahead routing computation [19] can be used to detect the packet arrival two hops away. E. Behavior To understand how AxNoC works, Figure 2 illustrates a simpliﬁed example of transferring a packet from RouterA to RouterC. We assume in this ﬁgure that this packet consists of four ﬂits: HEAD, DATA1, DATA2 and DATA3. In AxNoC, the head ﬂit (HEAD) is always considered as a critical ﬂit. double approx_func() { APPROX double a,b; double c; load_approx_value(&a, &b); c = ENDORCE(a + b); return c; } Fig. 3. EnerC type annotation. it has to keep the supply voltage at Vdd-high. Therefore, we extend the “Type"" ﬁeld of the header ﬂit to propagate the protection control metadata, containing the rearrangement pattern and the number of important ﬂits. The HEAD ﬂit is thus modiﬁed to: TYPE:PROTECTION:SRC:DST:LEN:VC_ID:SEQ_ID The protection metadata is interpreted by the arbiter and propagated to the next downstream router in advance by using the look-ahead method. IV. Approximate Execution Model A. Software Annotation Scheme We target the NoC communication for approximation. In particular, we approximate packets to/from L2 caches of shared memory model CMPs (with shared L2 cache banks) and inter-core communication of private memory model CMPs. As for the approximation of L2 cache access packets, we focus on store and load instructions in Instruction Set Architecture (ISA) to introduce approximate instructions that access the cache or memory in operand via an approximate channel, which uses a lower supply voltage for the wire. To provide a safe programming model for applications that run on our proposed environment with approximate data channels, we employed the EnerC annotation scheme [20]. This scheme uses a validated, general and safe type system that can isolate the critical portions of the program from the errortolerant portions, by regulating explicit delineation of precision ﬂow, particularly approximate to precise. A sample EnerC program is illustrated in Figure 3. EnerC permits implicit precision ﬂow of precise-to-approximate, while the opposite is prohibited unless it is explicitly delineated by ENDORCE(). We utilized Clang front-end extended for EnerC annotation system and its type checker LLVM pass from their work. B. AxNoC Compiler Our AxNoC oﬀers approximate memory access that can be associated with today’s ISAs with a few approximate aware extensions. Since the instruction stream can have a mix of approximate and precise instructions, the Memory Management Unit (MMU) needs to be notiﬁed the type of memory access (i.e., approximate or precise) and operates the memory data access. When accessing an L2 cache or a lower-ranked cache/memory, it allows the core’s interface to the router to create a packet with the desired precision type. AxNoC supports approximate memory access at a cache line granularity by exposing several ﬂits to error-prone communication channels. As in previous works targeting approximate memory architectures, this precision control is supported by having an approximate bit per line in each page, which indicates whether the corresponding line is precise or approximate. A cache controller and a memory controller determine the precision type of packets based on this bit. Note that this overhead is small (around 0.2%) for a typical 64-byte cache line. V. Evaluation This section is dedicated to evaluate the proposed AxNoC router in terms of area overhead, results’ accuracy, and power consumption. We compared AxNoC with a baseline 3-stage pipeline router (NRC/VSA, ST, LT), having the same parameters described in Section III-A, which is always working at Vdd-high to satisfy the timing requirements of the critical path. The baseline router always provides perfect packet transfers and does not support approximate data transfer. Note that, as previously mentioned, the main goal of AxNoC is to reduce the power consumption while maintaining the performance unchanged. Therefore, all the evaluations were conducted while making sure that AxNoC is working at the same operating frequency (1GHz) and providing the same performance as the baseline router. In other words, the critical path delays of both routers were satisﬁed since their maximum operating frequencies were slightly above the adopted 1GHz operating frequency. For this reason, no performance evaluations were included in this section. A. Overhead Evaluation 1) Deciding Vdd-high and Vdd-low: The proposed AxNoC router was designed in Verilog HDL, synthesized in STMicroelectronics 28nm (ST28nm) FDSOI process with Synopsys Design Compiler and placed and routed using Synopsys IC Compiler. Based on the trade-oﬀ analysis between the BER and supply voltage conducted in the Appendix, the Vdd-low and Vdd-high voltage supplies can be ﬁxed. As mentioned in Section I, we assume in this work that 10−5 and 10−12 are the BER thresholds for approximate and accurate data transfer, respectively. Based on these values, the relationship between Vdd-low and Vdd-high can be calculated. From these calculations, and constrained by the available technology libraries provided by the ST28nm process, we select 1.1 V and 0.7 V for Vdd-low and Vdd-high voltages, respectively. 2) Power Switch: The voltage switch design required a circuit level estimation; therefore, SPICE netlists were extracted from the GDS ﬁles of the routers by using Mentor Calibre. After that, the time and energy overhead of both low-tohigh and high-to-low voltage transitions were measured using Synopsys HSIM. First, to determine the number of transistors used in the voltage switch cells, the voltage transition delay was evaluated in circuit level simulation with various numbers of transistors. Here, the voltage transition latency is deﬁned as TABLE I Total area of transistors [µm2 ] Baseline AxNoC Router 852846 906249 Level shifter 0 1948.44 Voltage Sw. 0 534.06 Total 852846 908740.5 of the approximate mode with Vdd-low is only 47.5 mW. This is a natural result since the power consumption is proportional to V dd2 . This means that remaining in the approximate mode with Vdd-low for 1.00 second can save 8.19 mJ of energy. Therefore, the input and output channels should remain in the approximate mode for at least 23 cycles. This is because, it takes 22.47 nsec to compensate for the energy overhead of 36.8 pJ. It is worth noting that during our evaluation we observed a few cases where the BET condition was not respected. Such situations manifest when two critical ﬂits arrive at a router within the BET period of 23 cycles. However, AxNoC is designed in a way that it gives strict priority to transfer accurate ﬂits at a high voltage over respecting the BET condition. In this fashion, the correctness of the ﬂit transfer is guaranteed while incurring a negligible power overhead. 4) Area: Table I shows the total area of transistors used in each router. Placing and routing the input and output ports in separate macros, to allow the dual power supply in AxNoC router, requires only 5.9% extra area when compared to the baseline router. Moreover, the AxNoC router requires additional transistors and level-shifters to guarantee the voltage transition. Nevertheless, the area overhead of all the used voltage switching cells and level-shifters only constitutes less than 0.2% of the total AxNoC router area. In comparison to the baseline router, the proposed one only incurs 6.15% additional area overhead. We believe that this overhead can be considered as negligible when observing the power consumption reduction that the proposed router can oﬀer, as shown later in Section V-C. B. System-Level Evaluation 1) Methodology: We implemented the AxNoC model and its infrastructure using the SNIPER simulator [21] and the LLVM compiler infrastructure. Annotated source programs written in C or C++ are compiled to the LLVM intermediate language with the metadata information of approximation using the approximate compiler infrastructure [20]. It is then optimized and compiled to binary codes with a special runtime function that let the SNIPER simulator trace the memory address that can be approximated. The ornamented binary codes are then passed to the SNIPER simulator. We modiﬁed an MMU model and a cache model of the SNIPER simulator to simulate and analyze the access of the approximate channel to the L2 cache. We also implemented a detailed failure model of the proposed AxNoC router. The erring behavior is accurately simulated by inserting the error injection functions. For our evaluation, we used approximate C/C++ applications from ApproxBench [20], listed in Table II. We analyzed Fig. 4. Low-to-high voltage transition latency and energy overhead for a single input-port. the required time for Vdd to reach ±0.05V of the target voltage level after the “voltage-select"" signal is changed. Figure 4 shows the voltage transition delay vs. the number of voltage switch cells for a single input port in the AxNoC router. As explained in the previous subsection, Vdd-low and Vdd-high are ﬁxed to 0.7 V and 1.1 V, respectively. To complete the voltage transition in one clock cycle (1 nsec for 1GHz frequency), the number of transistors for an input port is decided to be 160, as depicted in Figure 4. When performing a similar experiment, we observed that 10 transistors are required to perform the low-to-high voltage transition for a single output port. The same measurements were also performed to extract the number of transistors required for the high-to-low voltage transitions in both the input and output ports. From these results, we decided to use 170 transistors for a single input and output port pair voltage switching cells. Since there are ﬁve input/output ports, 850 transistors in total are used. The area of a single power switch is much larger than that of a logic process; nevertheless, it is still considered very small and is about 1.44 µm2 . In terms of energy, results showed that the energy overhead of a pair of voltage transitions (i.e., low-to-high + high-to-low) is 34.2 pJ in an input channel. Similarly, a single transition in the output channel consumes 2.6 pJ. Therefore, 36.8 pJ is consumed when the power supplies for the input and output ports are changed at the same time. This energy overhead is very important as it aﬀects the power consumption evaluation in the system-level simulation. 3) Break Even Time: The Break Even Time (BET) is deﬁned as the minimum period that can compensate for an energy overhead (i.e., 36.8 pJ) by remaining at the Vdd-low level. If AxNoC router remains at Vdd-low for a shorter time than the BET, the energy overhead becomes larger than the obtained beneﬁt, and the total power consumption increases. To calculate BET, the placed-and-routed design of the proposed AxNoC router was simulated at 1GHz. The power analysis based on the switching activity information indicated that the total power consumption of the router in the accurate ﬂit transmission mode with Vdd-high is 123.4 mW. While that Fig. 5. Errors in ApproxBench applications. the cache and memory access of those programs using the previously mentioned simulators. For the Quality-of-Result (QoR) evaluation, we used the criteria of the past work on this benchmark [20]. 2) Quality of Result: Our objective is to obtain a good trade-oﬀ between accuracy and network energy consumption. In this evaluation, we discuss the possible degradation of data ﬁdelity and its impact on the result quality. Because the impact of errors diﬀers according to the error propagation model inside the application, and the acceptable range of quality loss varies according to the type of program (i.e., numerical calculation vs. multimedia processing), we show that we can achieve attractive trade-oﬀs between the energy consumption and the result by tuning the bit protection level. Figure 5 illustrates the QoR evaluation results. We used the metrics introduced in [20], and set the default bit protection level to 16. We considered BER to be equal to 10−5 and 10−7 , and assumed error-free communications for the packet header and protected data by the use of Vdd-high. As can be seen in the ﬁgure, the results’ quality improves (the error becomes smaller) with small BERs, although the impact on the quality depends on the applications. We can further control the results’ quality by changing the bit protection level as discussed below. The canneal application did not show any QoR degradation when executed on AxNoC. This is because the approx memory regions are small and created just on the stack by compiler optimizations, resulting in no approximate data communication to L2 caches. Figures 6 shows the QoR results of blackscholes and sobel. We evaluate the sensitivity to the bit protection level by changing this latter when BER is ﬁxed to 10−5 . We argue TABLE II Used approximate applications. Application blackscholes canneal inversk2j sobel streamcluster Description Investment pricing VLSI routing Inverse kinematics Sobel edge detector Online clustering Quality Metric Average Relative Error Particle distance Average Relative Error Image Diﬀ (RMSE) Cluster center distance Fig. 6. QoR vs. Bit Protection Level. that even with a ﬁxed BER determined by the hardware conﬁguration, we can ﬁnely tune the QoR by changing the bit protection level, as can be seen in Figure 6. While both of the programs use double precision ﬂoating point datatype for the approximate region of the memory, the trend driven by the bit protection level is contrastive. We observe higher robustness to the error position for sobel, in contrast to the gradual quality improvement of blackscholes with bit protection level increase. This is due to the following two factors. First, since sobel uses an approximate data region to store intermediate pixel data results, each data is independent and the error propagation is quite limited. Second, because of the limited impact of exposed error and the characteristic of error measurements for the image data, the overall quality is largely dependent on the frequency of errors, but not to the bit-position of the errors. We exploit these features to get better energy-accuracy trade-oﬀ. In this case, we do not have to protect the signiﬁcant bits for sobel (protection level = 0), and this decreases the overall power consumption, as demonstrated in the next subsection. Such tendency can be observed in many approximate applications in the image processing domain. Our results suggest that AxNoC provides attractive tradeoﬀs between result quality and performance. The “quality control ﬂexibility” discussed in the approximate computing literature can be achieved with the selection of the bit protection level, and the operating supply voltage being the “control knob.” C. Power Consumption In this ﬁnal evaluation, we calculate the power consumption when running the previously mentioned approximate benchmarks on AxNoC. We used the trace ﬁles obtained from the simulation conducted for the QoR evaluation to calculate the power consumption of both accurate and approximate data communication. The power consumption results obtained with the AxNoC router are compared with the baseline 3-stage pipeline router, which is always working at Vdd-high for both (a) (b) Fig. 7. Power consumption results: (a) AxNoC power breakdown (b) comparison with a baseline router. approximate and accurate data transfer. To perform this calculation, we took into consideration the power overhead results, previously presented in Section V-A. For fair comparison and to highlight the beneﬁts of approximation in NoCs, the standby power (i.e., when the router is not processing any ﬂits) is not included in this evaluation. When adding this portion of power, AxNoC could achieve a much signiﬁcant amount of power reduction. Figure 7 (a) illustrates the power breakdown results when running the adopted approximate benchmarks on AxNoC. Here, the right axis represents the percentage of approximate data that was extracted from the trace ﬁles. The ﬁrst observation from this ﬁgure is the very small percentage of power caused by the voltage transition between the Vdd-high and Vdd-low, which is almost invisible in the plot. The second one is that the amount of power consumed by approximate communication varies from one application to another. This variation is highly dependent on the percentage of approximate data, which is represented by the blue line in the ﬁgure. As previously stated, as the percentage of approximate data transfer increases, the number of ﬂits traveling the network at Vdd-low increases as well. As a direct consequence, a signiﬁcant amount of power consumption can be saved when compared to the baseline router, where all the ﬂits traveling the network traverse routers only supplied with Vdd-high, regardless whether these ﬂits carry accurate or approximate data. In fact, Figure 7 (b) demonstrates that the power saving obtained with AxNoC is around 1% for canneal and can reach up to 43% with sobel. From the previously conducted evaluations, we can conclude that the eﬃcient use of approximation endorsed with an adequate router architecture can provide a signiﬁcant amount of power saving in NoC systems while assuring an acceptable result accuracy. Not forget to mention that such beneﬁts do not stand as an obstacle for maintaining an undegradable performance. VI. Conclusions In this paper, we presented AxNoC that exploits data approximation to provide power eﬃciency in Network-on-Chips. Using dual-voltage look-ahead power management, AxNoC is capable of isolating the critical path in a way that headers and critical data ﬂits are perfectly transferred at high voltage, while the remaining ﬂits may include bit ﬂips by decreasing the supply voltage. An AxNoC router was implemented using ST28nm process and various experiments were conducted to extract its power and accuracy characteristics. The power comparison results conﬁrmed the existence of a tight relationship between the amount of saved power and the level of approximation in the simulated applications. When compared to a conventional baseline router working at a high supply voltage, the proposed router was able to provide a signiﬁcant amount of power saving that reached the 43%. This power saving was obtained while guaranteeing undegradable performance and with less than 6.2% area overhead. Furthermore, the Quality-of-Results (QoR) evaluation showed that AxNoC exhibits an acceptable accuracy illustrated in a suﬃciently small geomean of error (0.000738 when BER = 10−5 ); thereby, providing an eﬃcient trade-oﬀ between error and power consumption. Acknowledgment This work was partially supported by JSPS KAKENHI B Grant Number 18H03215. Also, the authors would like to thank the VLSI Design and Education Center (VDEC) at the University of Tokyo for the EDA tools support. Appendix – Analysis of Flit Error Rate In this appendix, we aim to provide a reliable analysis allowing us to depict the relationship between the Bit Error Rate (BER) and the supply voltage levels in NoC systems. Based on this analysis, the suitable supply voltage for a given BER can be extracted and used to design low power approximate NoC designs. Noise Source: Although NoCs have developed scaled technology that is advantageous in structural regularity, scalability, modularity and eﬃcient communication, they face reliability challenges brought by various noise sources [22]. The noise can be classiﬁed into two: transient errors and permanent errors. The permanent errors are induced mainly by the manufacturing process and device wear-out. On the other hand, the transient errors, especially bit ﬂips on which we focus in this paper, have a short lifetime. The causes of the transient errors include voltage glitches, crosstalk coupling, and particle strike. As technology scales, the probability of alpha particle-induced errors increases along with the increasing number of circuit nodes and decreasing critical charge [22], while predicting the error noise spectral density is not necessarily easy because it is inﬂuenced by many factors, including location, time, supply voltage and temperature. Error Model: The Gaussian pulse function is a widely accepted model for bit ﬂips in logic circuit [23], with the assumption that the noise voltage follows a normal distribution. (cid:32) Vdd Here, the wire bit error rate  is represented as:  = Q (cid:33) (cid:90) ∞ 2σN (1) (2) Q( x) = 1√ e− y2 2 dy, x 2π where σN denotes the standard deviation of the supply voltage Vdd . Let ˜ be the target BER of reduced Vdd (Vdd ,L ). Here we deﬁne the normal supply voltage Vdd ,H as the one providing reliable BER 0 . Note that ECC techniques may lessen the required voltage and that our model is compatible to any of them. Equation 1 can be rearranged as follows for both Vdd ,H andVdd ,L . Vdd ,H 2σN Vdd ,L 2σN = Q−1 (0 ) = Q−1 ( ˜ ). (3) (4) = = (5) rVdd Vdd ,L Vdd ,H By eliminating σN from these equations, we obtain the Vdd ratio rVdd of Vdd ,L and Vdd ,H as follows. Q−1 ( ˜ ) Q−1 (0 ) Note that the possible Vdd ,L value may be manifested by a plethora of factors in hardware implementation, and that both target BER and aﬀordable slack at a given frequency can set a lower bound of the Vdd ratio. Results: Based on Equation 5, the voltage ratio between the Vdd ,L and Vdd ,H for approximate and reliable data communications, respectively, can be extracted. As the error rate decreases, the voltage ratio increases. Assuming that ˜ and 0 are equal 10−5 and 10−12 , rVdd is estimated to be equal to 0.606. Using this value, the relation between Vdd ,L and Vdd ,H can be calculated. "
Securing NoCs Against Timing Attacks with Non-Interference Based Adaptive Routing.,"Timing channel attacks use interference from contending application flows to cause information leakage, and thereby either covertly transmit secrets, or create Denial-of-Service (DoS) attacks to undermine the on-chip hardware security. Protecting against timing channel attacks is very challenging since unseen vulnerabilities emerge in newer technology that can be cleverly exploited by malicious applications by intentionally gaming resources to artificially induce interference. In this paper, we propose to secure Network-on-Chips (NoCs) against timing attacks with non-Interference based adaptive routing where we efficiently separate network traffic to not only improve application performance and prevent information leakage. In our performance analysis, we show that the proposed architecture maintains non-interference between security domains, prevents DoS, and improves performance by 2-20% over existing techniques with only a 1.84% power consumption penalty.","Securing NoCs Against Timing Attacks with Non-Interference Based Adaptive Routing Special Session Paper Travis H. Boraten Avinash K. Kodi Dept. of Electrical Engineering and Computer Science Ohio University Athens, Ohio tb286706@ohio.edu Dept. of Electrical Engineering and Computer Science Ohio University Athens, Ohio kodi@ohio.edu Abstract—Timing channel attacks use interference from contending application ﬂows to cause information leakage, and thereby either covertly transmit secrets, or create Denial-ofService (DoS) attacks to undermine the on-chip hardware security. Protecting against timing channel attacks is very challenging since unseen vulnerabilities emerge in newer technology that can be cleverly exploited by malicious applications by intentionally gaming resources to artiﬁcially induce interference. In this paper, we propose to secure Network-on-Chips (NoCs) against timing attacks with non-Interference based adaptive routing where we efﬁciently separate network trafﬁc to not only improve application performance and prevent information leakage. In our performance analysis, we show that the proposed architecture maintains non-interference between security domains, prevents DoS, and improves performance by 2-20% over existing techniques with only a 1.84% power consumption penalty. I . IN TRODUC T ION In the future, Multiprocessor System-on-Chip (MPSoC) architectures are expected to exceed hundreds to thousands of cores [2]. To improve performance, applications must parallelize workloads across many cores, and distribute applications that simultaneously execute on separate portions of the chip. To efﬁciently utilize each core, researchers are likely to rely on packet-based Network-on-Chips (NoCs) architectures. Unlike busses, NoCs allow many-to-many shared communication channels and are capable of integrating processing cores, caches, memory controllers and I/O devices into a single scalable, power efﬁcient and reliable fabric [4]. Requiring NoCs to coordinate and control all on-chip and off-chip data transfers, however, presents a variety of challenges that researchers must tackle to ensure that NoC resources are shared in a secure, and efﬁcient manner. For example, consider the case when two applications are running simultaneously on the MPSoC. If computing resources are dynamically shared, by contending for and winning the same resources, it is possible for one application to temporarily starve or interfere with the communication ﬂow of another application. When one application can interfere with another, the interference creates what is called a timing channel. This scenario leads to the following two security challenges for NoCs: (1) side channel, and (2) Denial-of-Service (DoS) attacks. In a side channel attack, timing channels are exploited to leak information from a high security domain to a low security domain by observing how runtime characteristics such as power [16], timing [12], and performance [18] are affected by an interfering application. One approach to preventing interference, is to isolate domains and use time division multiplexing (TDM) [19], [20], to fairly arbitrate among requests to access the shared resources. While TDM techniques eliminate network interference, they do so with inefﬁcient resource management which further degrades performance. Alternate approaches have explored the combination of application demand and priority based allocation of resources to improve resource management with spatial and temporal partitioning [18]. Priority based techniques throttle lower priority domains and could potentially degrade overall performance. As performance is traded-off to improve security, we need new research techniques that simultaneously enhances security and sustains performance. In this paper, we propose non-interference based adaptive (NIBR) routing to secure NoCs from side channel and DoS attacks. In the proposed architecture, we employ adaptive routing algorithms to dynamically and spatially disperse network trafﬁc among under-utilized routers by classifying trafﬁc types into a set of sub-security domains. To ensure non-interference between each security domain, we create a set of policies to govern the direction of information ﬂow and allocation of shared resources. In our analysis we will show that the proposed architecture maintains non-interference between security domains, prevents DoS, and improves performance by 2-20% over existing techniques with only a 1.84% power consumption penalty. The major contributions of this paper are as follows: 1) Adaptive Routing: We propose the use of multiple adaptive routing techniques to efﬁciently utilize network resources. Instead of employing inefﬁcient bandwidth throttling techniques, we propose to re-route trafﬁc by employing multiple routing techniques, and thereby improve performance while decreasing the overhead needed to prevent timing channels. 978-1-5386-4893-3/18/$31.00 ©2018 IEEE 2) 1-D and 2-D Non-interference: To improve performance in future MPSoC architectures, applications must parallelize workloads across many cores, while other applications execute simultaneously on separate portions of the chip. Research is needed to make sure NoCs provide a secure and reliable interconnect. In this paper we explore the impact of both 1-D and 2-D dimensional timing-channel attacks. As MPSoC core counts continue to scale, workloads 3) Reduced overhead: In our performance analysis, we show that the proposed NIBR architecture maintains non-interference between security domains, prevents DoS, and reduces application performance penalties when compared to prior static partitioning and bandwidth throttling techniques. The paper is organized as follows: In section II, we discuss related work, in section III, we deﬁne timing-channels and interference in NoCs, in section IV, we detail our proposed solution, in section V, discuss our performance evaluation and in section VI, we conclude our paper. I I . R E LATED WORK In previous work, side-channel attacks create and monitor disturbances of integrated circuits through timing [10], power [11], and fault injection [1] in order to leak information, cause DoS, and covertly transmit data [3]. As a secondary effect, such attacks could negatively affect application performance although at that times that is not their primary intention. In our proposed solution we are primarily concerned with the threat from timing-channels caused by contention within the network micro-architecture. Quality of Service: To secure NoCs against timing-channels and prevent interference between low and high security domains, solutions often include Quality-of-Service (QoS) [7]– [9] and dynamic resource allocation. It is important to note that although there may be some overlap in techniques that improve fairness among different domains, non-interference and QoS have distinct goals and thereby operate under different throughput and latency constraints. For example, dynamic resource allocation may completely utilize resources in a QoS design for a single domain when another domain has less demand. When another application requests resources, the domain with full utilization will then need reduce its bandwidth/throughput and result in interference. Non-interference in NoCs: In [17], Wang and Suh proposed the following three static partitioning methods for noninterference in NoCs: spatial network partitioning (SNP), temporal network partitioning (TNP), and reversed priority with static limits (RPSL). With SNP, physical isolation is created by allocating routers to subnetworks and only trafﬁc belonging to those networks can be transmitted within them. In TNP, the granularity of allocation is limited to within the router micro-architecture. Virtual channels are statically assigned and temporal scheduling is used on a per-cycle basis for switch allocation and link traversals. While both SNP and TNP prevent interference in both directions (high to low, low to high), they both have inherent performance limitations due to their static partitioning. RPSL on the other hand, tradesoff interference in one direction by allowing the demand of the low security domain to control the bandwidth allocation. This allows RPSL to almost completely allocate resources to a single domain dynamically when the other domain’s demand is low. If both domains have high demand, then the high security domain is throttled whereas the low security domain takes precedence in arbitration but has static limits to prevent denialof-service. To ensure complete non-interference among domains, one approach is to statically allocate network resources and timestep packet progression with a technique called Time Division Multiplexing (TDM) [6], [13]. In traditional TDM scheduling, timing policies restrict packet progression to a single domain for a given number of cycles. If a domain does not utilize its time epoch, the epoch is wasted and no other domain can utilize it. The innate performance bottleneck of TDM gets worse as the number of application domains increase. To improve the efﬁciency of TDM the authors of SurfNOC [20], propose a provable low-latency TDM scheduling technique by scheduling domains in wave patterns to reduce the overhead incurred by cycle-by-cycle time multiplexing. While SurfNoC does signiﬁcantly improve the zero-load latency for packets that ride the wave, the improvement in average latency and throughput are marginal as only one packet can ride the wave at a time, and packets that miss the wave must wait for the next. In our paper, we propose a non-interference based routing solution to prevent information leakage from timing-channels and improve application performance by improving spatial network utilization. Unlike previous work our attack model explores the impact of both 1-D and 2-D dimensional timingchannel interference and propose ways to improve network utilization and sustain performance when compared to existing solutions that may incur performance penalties through throttling to prevent interference. I I I . T IM ING CHANN EL S AND IN TER F ER ENC E IN NOC S To overcome security threats in NoCs, we need to identify the attacks, localize the attack, and then enable mitigation techniques which could include enhancing encoding techniques for obfuscation, authentication, and veriﬁcation. For timing-channels in NoCs, interference can be created when multiple application ﬂows compete for allocation of network sources and thereby cause contention. In the router microarchitecture this includes virtual channel (VC) allocation and switch traversal (ST) pipeline stages that rely on arbitration circuits to control movement and storage of packets as they traverse the network. If two ﬂows compete for the same buffers and input or output ports (switch), interference can occur between the two ﬂows. In Figure 1, we show three conceivable outcomes when two simultaneous application ﬂows compete within the router micro-architecture for the same buffer space and switching paths. The top two plots for each scenario, ﬂow A (red) and Fig. 1. In the top two rows we show the injection rate over time for two simultaneous applications. In row three we show the achieved throughput for each application in the following scenarios: (a) Case 1 - Two-way interference, (b) Case 2 - No interference, and (c) Case 3 - One-way interference. ﬂow B (purple), show the achieved injection rate for each application when these are run individually. In these scenarios, ﬂows A and B represent applications in low and high security domains, respectively. For example, ﬂow B may represent a secure application attempting to perform encryption and ﬂow A represents a malicious application trying to gain information pertaining to the encryption method [17]. To achieve the attackers goal, the low security ﬂow must interact with the high security ﬂow in a manner that is observable to the attacker, for example measuring achieved throughput. In the 3rd (bottom) subplot of each scenario, we show the resulting throughput obtained for each low and high security domain ﬂows. Case 1 - Two-way interference: In the ﬁrst outcome of Figure 1 (a), interference occurs in both directions as the achieved throughput of both ﬂows is obstructed due to the other domain and their desired injection rates are not satisﬁed. When both ﬂows conﬂict, each is allocated roughly half the bandwidth. Of the three outcomes, this is the worst scenario as information can be leaked from either direction and each application has a door way to affect the runtime performance of the other domain and could potentially create denial-of-service. When timing-channels occur, this type of outcome is likely due to the security ﬂaws in fully dynamic QoS techniques that try to provision resources dependent on the load. Case 2 - No interference: In the second outcome of Figure 1 (b), we show the ideal and opposite scenario. In this case, no interference is caused by the malicious application as a result of the different injection rate proﬁles for each application. Had the sum of the injection rates exceeded the channel bandwidth (1 ﬂit/cycle) like the prior case, we would observe interference in the achieved throughput and have the same outcome as above. This is the ideal scenario. Case 3 - One-way interference: In the third outcome shown in Figure 1 (c), we show an example of information leakage restrained to a single direction. In this case, the injection rate proﬁles conﬂict but the high security domain is the only domain throttled due to fair arbitration. Since the low security domain is unaffected by the high security domain, the attack does not leak any sensitive information, however as a secondary effect, the high security domain does incur a performance penalty. When this scenario occurs, it is most likely intentional similar to [17] to improve channel efﬁciency. In this section, we have described how two competing ﬂows can create timing-channels that hampers performance of another application and cause information leakage within the local micro-architecture. In the next sections we will take a step back and discuss how timing-channels affect the NoC from a global perspective. In Figure 2, we illustrate two scenarios that are likely in a mesh topology, one-dimensional and two-dimensional timing-channel attack. A. 1-D Interference In Figure 2 (a), we show trafﬁc ﬂows from two applications, one in a high (green) and the other in the low (red) security domain. When trafﬁc from the high security domain reaches the second router, the second router must arbitrate requests from both domains. In an architecture that arbitrates packets fairly via round robin or ﬁrst-come ﬁrst-serve (FCFS), the Fig. 2. Examples of 1-D and 2-D timing-channels. competing ﬂows will be forced to share the link bandwidth, decrease the throughput of each ﬂow, and increase latency for all trafﬁc traversing that link (affected high region). While the interference occurs on one link, it has the potential to affect the performance of trafﬁc to any downstream router. In most noninterference approaches, prior work focused on prevention of interference and the point of interference, with little emphasis on the overall affected region. B. 2-D Interference In Figure 2 (b), we expand the problem to a two-dimensional network. The same link (1 of 36) is the cause of interference and the affected region is still 50%. In this example, we assume dimension order routing (x-y). Since all existing techniques to our knowledge require some form of ﬂow throttling to securely eliminate the interference, the point of attack has the potential to become a major bottleneck. Additionally, six of the twelve routers are spatially under-utilized. In the above scenario it may be beneﬁcial to reroute or deﬂect packets from the high security domain to an alternate path. The low security domain should not be rerouted as we do not want to interfere with the low security domain. Rerouting the high security domain around the attack could allow packets to avoid throttled areas, improve network utilization, and reduce performance overhead. In Figure 2 (c), we show a second example of twodimensional interference. In this case, both the high and low security domains are transmitting trafﬁc across both dimensions. As core counts continue to increase in MPSoCs, attacks spanning multiple cores will become more likely. Timing channel attacks may be used to target entire chip regions, memory controllers, or application speciﬁc cores. The unpredictable nature of task scheduling may further increase the distance between a malicious application and its target. Having a solution that adjusts buffer allocation, bandwidth allocation and routing could be imperative. In the next section we will discuss our proposed method can handle these type of timing-channel attacks and efﬁciently adapt routing to better utilize the network when compared to existing approaches. C. Non-interference Based Routing In future MPSoCs, applications will need to parallelize workloads across many cores. In a secure NoC, information leakage between applications competing for the same network resources must be isolated to maintain non-interference. In previous literature, this has shown to increase in-efﬁcient and wasteful allocation of hardware that may utilize temporal locality but not spatial locality. Restricting ﬂows by isolating them at the point of attack, also has the potential to develop back pressure and degrade application performance. Meanwhile, adjacent routers and other regions of the chip may be under-utilized and could potentially relieve congestion if trafﬁc is rerouted. To improve spatial network utilization and sustain application performance we propose Non-interference Based Routing (NIBR) and switch between the following three routing algorithms on-demand: x-y dimension order routing (X-Y) [21], orthogonal one turn (O1TURN) [15], and randomized oblivious minimal multi-phase (ROMM) [14]. Level-1: In dimension order X-Y routing, packets are restricted to move in one dimension at a time, explicitly the x-dimension ﬁrst and then allowed to move in the y-direction. For NoCs, X-Y routing has been heavily studied and is one of the more popular deterministic routing algorithms because of the low cost implementation and good performance. Additionally, X-Y has shown to perform the best for ﬂoodbased DoS attacks [5] until saturation. For these reasons we chose X-Y DOR to be a suitable default routing algorithm for all network trafﬁc in NIBR regardless of security domain. Level-2: In level-2, trafﬁc is routed using O1TURN, where packets have the option to move in either dimension ﬁrst as long as they follow either X-Y or Y-X paths. For trafﬁc in NIBR, domains can switch to O1TURN to elevate packet priority. By using O1TURN for level-2, domain ﬂows can achieve higher throughput and better utilize the network spatially if congestion exists more heavily in a particular dimension. Level-3: In NIBR, ROMM is reserved for trafﬁc with the highest priority (level-3) because it offers the highest path diversity of the three minimally routed algorithms. It too relies on DOR at its core but packets are routed to randomly selected intermediate destinations, called phases. When a packet reaches the destination for each phase, another intermediate destination is chosen and routed until the packet reaches its ﬁnal destination. An intermediate destination is valid as long as it falls within the minimal path to the ﬁnal destination. To support multiple turns as packets switch phases, packets are restricted to speciﬁc VCs allocated per phase. In Figure 4, we illustrate seven different minimal routing paths that exists between two routers on opposite sides of the network that is a maximum of four hops away. By being able Fig. 3. Routing options to avoid interference and improve network utilization. Fig. 4. Minimal path routing options in NIBR to avoid interference and improve network utilization. to switch routing algorithms per domain and on-demand in NIBR, domains can spatially utilize network resources more efﬁciently. As NoCs scale to larger mesh sizes, more paths will exist and each of them will be possible with ROMM by increasing phase counts. D. Micro-architecture To fully support non-interference with NIBR, we need micro-architectural support in the router pipeline to isolate switching interference and buffer allocation from separate security domains. In our performance analysis section, we will show that routing alone cannot eliminate non-interference entirely and therefore bandwidth throttling is still required to completely eliminate improper information leakage in scenarios when congestion cannot be avoided. To ensure information leakage cannot occur in our proposed design, we give reverse priority (low ¿ high) with static limits to low security trafﬁc ﬂows, a technique originally proposed in [17]. This will allow NIBR to sustain non-interference even in scenarios when the network is saturated. In Figure 3 and 5, we give an overview of each domain policy and how routing decisions are made to reduce the amount of trafﬁc that needs to be throttled. Before an application can transmit data, it must ﬁrst be assigned a security domain by the operating system. In NIBR, when a high security domain conﬂicts with a low security domain, the high security domain is throttled to give more bandwidth to the low security domain. This causes information leakage in one direction, but in a direction that is allowed (low to high). Additionally, static limits are used to prevent the low security domain from denying complete service to the high security domain. When the high security domain has its trafﬁc throttled, Fig. 5. Micro-architectural support for a multi-tiered security domain network that allows 1-way interference and routing based on trafﬁc domain. if back pressure is detected in the high domain (through credit counts and VC allocation), the domain can request routing privileges to be elevated. The following questions must then be answered during the RC stage to be granted elevation: 1. Has back pressure developed? 2. Is alternative port available? 3. Does the other domain have priority? If the answers to each question is ’yes’, then routing privileges can elevate. By default both domains start with X-Y routing. High domains can then elevate to O1TURN and eventually ROMM. E. Dead-lock prevention In NIBR we chose X-Y DOR, O1TURN, and ROMM for multiple reasons. They each build upon DOR, the overall implementation is modular, they each offer a variety of different paths, and give ﬂexibility to VC allocation which is required to prevent deadlocks. To prevent deadlocks in X-Y DOR a single domain requires at least one VC. For domains that are elevated to O1TURN routing, a minimum of two VCs is required, one each for X-Y and Y-X routes. In ROMM, the VC requirement is dependent on the number of routing phases that packets can use before they reach their destination. For example if we enable ROMM with two phases, we need a total of two VCs, at least one allocated per phase. As NoCs scale, more phases can be added to scale Fig. 6. Throughput of A as B is varied on the X-axis. Fig. 7. Throughput of B as A is varied on the X-axis. COM PAR ED NON - IN TER F ER ENC E T ECHN IQU E S . TABLE I IV. P ER FORMANC E EVA LUAT ION In this section, we evaluate our proposed non-interference base routing scheme and present results for scenarios involving one and two dimensional side-channel attacks. As a ﬁrst order analysis, we will examine the effect that each routing algorithm has on timing interference and then on the overall application performance. We will then evaluate multiple baselines, one for each routing algorithm, to identify and breakdown the source of performance changes (overall technique or routing algorithm). After our initial baseline, attack, and routing analysis, we will evaluate our proposed design to the following non-interference techniques to prevent timing channels: time division multiplexing (TDM), a SurfNoC [20] like TDM, temporal network partitioning (TNP) [17], and oneway information leakage with reserved priority with static limits (RPSL) [17]. For each design we will compare results in terms of application performance, throughput, and power consumption. In Table I, we give an overview of each noninterference technique we will compare against in this section. Power and area were synthesized and optimized using the Synopsys Design Compiler tool using the TSMC 40 nm technology libraries with a 1.0 V supply voltage and 2 GHz operating frequency. Network evaluation was conducted using an in-house cycle accurate network simulator and used synesthetic and real trafﬁc distributions from the PARSEC and SPLASH-2 benchmark suites on a 64-core, 16 router mesh-based NoC with each router connected to 4 cores. Each router has two unidirectional links connecting adjacent routers with 6 VCs per port, 4 64-bit buffer slots per VC, and a 5-stage (BW/RC,VA,SA,ST,LT) router pipeline implemented and round robin arbitration. The routing algorithm used in the baseline is x-y dimension order routing, but we also evaluate baselines with 1-turn, and ROMM routing. For our evaluation, we implement one-dimensional and two-dimensional timing-channel attack models. In the onedimensional attack, a low level priority application sends trafﬁc from router 1 to router 3 to force a timing-channel. In the two-dimensional attack, the attack application is sending a ﬂood of trafﬁc from router 0 to router 10 to cause a multidimensional timing-channel. The injection rate for both attacks is ﬁxed at 1 ﬂit per cycle per core and routing is restricted to x-y for the low priority domain. An illustration of both attacks is shown in Figure 2. A. Interference and Routing In this section we evaluate the potential impact that each routing algorithm in NIBR has on its own to prevent interference amongst security domains. In Figure 6 and 7, we plot the achieved throughput (ﬂit/cycle) of each security domain as one domain is held constant (plot series) and the domain’s injection rate is varied. In Figure 6, we show the achieved throughput for the low (A) security domain as the low (B) security domain is varied between X-Y, O1TURN, and the ROMM routing algorithms. In other words, the results indicate the interference observed by domain A caused by the interference of the two domains and any dip in achieved throughput indicates the magnitude of the interference. For example, if we look at the results from X-Y routing, as the injection rate of (B) is increased, the throughput of (A) decreases. It is not until the injection rate of (B) exceeds 0.5 ﬂit/cycle that the throughput of (A) stops diminishing. This due to the lack of protection from fair VC and SA arbitration techniques such as FCSF and round robin that split bandwidth allocation. As expected X-Y DOR suffers the largest performance penalty when encountered by interference because no alternate paths exists. For O1TURN and ROMM routing, the results show a very different picture. While interference is observed in Figure 6 for domain (A), signiﬁcantly less interference is seen for the high (B) domain in Figure 7. In some cases completely preventing any interference with the exception of the 1.0 ﬂit/cycle injection rate case. Therefore, a threshold exists for an injection rate some where between the 1.0 and 0.8 ﬂit/cycle where 2-way interference occurs. When below, interference is only observed in the low (A) domain and nearly zero performance penalty exists for the high (B) domain. The key takeaway from these results is that it shows that while routing can aid in the prevention of interference, it does not eliminate interference entirely in both directions and therefore some amount of throttling of TDM is needed to eliminate it completely. However, since routing can reduce the magnitude of interference and in some cases completely remove interference, security domains do not need to be throttled as much, which means NIBR has the potential to reduce the performance penalty over existing non-interference techniques. B. Baselines and Attack Before we compare the results from NIBR with TNP, and RPSL we wanted to evaluate if switching between routing algorithms would skew our data since it is common for routing algorithms to exhibit different performance characteristics. In Figure 8, we show each real trafﬁc benchmark for the each routing algorithms used in NIBR. To our surprise, the results indicated that each application ran within less than 1% of each other. While we expected some larger performance differences from our large simulations (millions of cycles), any performance penalty we see during the attack in the next section, is likely due to NIBR mitigating the threat and not the performance advantage of the routing algorithm itself. Next in Figure 9, we show benchmark performance results for each application while under one and two dimension attacks with a baseline of X-Y under no attack. For these results we wanted to highlight how much performance drop that we will see in the next section was due to the attack itself and not the performance penalty of the mitigation technique. For the 1-D attack, all benchmarks showed less than 1% performance penalty. For the 1-D case, any performance penalty we will see later will be due to the mitigation techniques. We should note that while the 1-D attack did not affect the overall benchmark performance (less than 1%), the attack did cause interference throughout the simulation. On the other hand, the second 2D attack had more of an impact on overall performance and Fig. 8. Performance of baseline with different routing algorithms. Fig. 9. Application performance of the baseline architecture when under attack different. caused upwards to 15-20% performance penalties. Therefore, in the next section when we show results for the mitigation technique, we will be able to analyze where and how the performance penalties were caused. C. 2-D Attack In this section we will evaluate NIBR, TNP, and RPSL mitigation techniques from benchmark performance perspective. We already know that each design is capable of providing noninterference, the unknown is how each stacks up in terms of performance. Due to our analysis in the Baselines and Attack subsection we have omitted results for 1-D attack scenario due to space limitations and are only presenting results for the 2-D attack scenario. In Figure 10, we show the performance of each mitigation technique for six real trafﬁc benchmarks and the average. For each benchmark the proposed NIBR solution successfully reduced the performance penalty caused by the attack and NIBR did not introduce its own penalty. On average NIBR performed 2-20% better than the other compared techniques and reduced the overall impact of the attack by 2%. For TNP and RPSL, performance of each mitigation technique was worse than the attack itself and saw a 12% and 27% performance penalty on average. The performance drops from TNP and RPSL were caused by the time division multiplexing and static allocation limits in each design, respectively. D. Power Consumption In addition to performance, in Figure 10, we show the overall network power consumption for each non-interference [2] L. Benini and G. De Micheli. Networks on chips: a new soc paradigm. Computer, 35(1):70–78, 2002. [3] J. Chen and G. Venkataramani. Cc-hunter: Uncovering covert timing channels on shared processor hardware. In Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO-47, pages 216–228, 2014. [4] W. Dally and B. Towles. Principles and Practices of Interconnection Networks. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2003. [5] D. Fang, H. Li, J. Han, and X. Zeng. Robustness analysis of mesh-based network-on-chip architecture under ﬂooding-based denial of service attacks. In Networking, Architecture and Storage (NAS), 2013 IEEE Eighth International Conference on, pages 178–186, 2013. [6] K. Goossens, J. Dielissen, and A. Radulescu. Aethereal network on chip: concepts, architectures, and implementations. IEEE Design Test of Computers, 22(5):414–421, Sept 2005. [7] B. Grot, J. Hestness, S. Keckler, and O. Mutlu. A qos-enabled on-die interconnect fabric for kilo-node chips. IEEE Micro, 32(3):17–25, May 2012. [8] B. Grot, S. W. Keckler, and O. Mutlu. Preemptive virtual clock: A ﬂexible, efﬁcient, and cost-effective qos scheme for networks-onchip. In 2009 42nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 268–279, Dec 2009. [9] B. Grot, S. W. Keckler, and O. Mutlu. Topology-aware quality-of-service support in highly integrated chip multiprocessors. In Proceedings of the 2010 International Conference on Computer Architecture, ISCA’10, pages 357–375, Berlin, Heidelberg, 2012. Springer-Verlag. [10] P. C. Kocher. Timing attacks on implementations of difﬁe-hellman, rsa, dss, and other systems. In Proceedings of the 16th Annual International Cryptology Conference on Advances in Cryptology, CRYPTO ’96, pages 104–113, 1996. [11] P. C. Kocher, J. Jaffe, and B. Jun. Differential power analysis. In Proceedings of the 19th Annual International Cryptology Conference on Advances in Cryptology, CRYPTO ’99, pages 388–397, 1999. [12] J. Li and J. Lach. At-speed delay characterization for ic authentication and trojan horse detection. In Hardware-Oriented Security and Trust, 2008. HOST 2008. IEEE International Workshop on, June 2008. [13] S. Liu, Z. Lu, and A. Jantsch. Highway in tdm nocs. In Proceedings of the 9th International Symposium on Networks-on-Chip, NOCS ’15, pages 15:1–15:8, New York, NY, USA, 2015. ACM. [14] T. Nesson and S. L. Johnsson. Romm routing on mesh and torus networks. In Proceedings of the Seventh Annual ACM Symposium on Parallel Algorithms and Architectures, SPAA ’95, pages 275–287, New York, NY, USA, 1995. ACM. [15] D. Seo, A. Ali, W.-T. Lim, and N. Raﬁque. Near-optimal worstcase throughput routing for two-dimensional mesh networks. In 32nd International Symposium on Computer Architecture (ISCA’05), pages 432–443, June 2005. [16] X. Wang, H. Salmani, M. Tehranipoor, and J. Plusquellic. Hardware trojan detection and isolation using current integration and localized current analysis. In Defect and Fault Tolerance of VLSI Systems, 2008. DFTVS ’08. IEEE International Symposium on, Oct 2008. [17] Y. Wang and G. Suh. Efﬁcient timing channel protection for onchip networks. In Networks on Chip (NoCS), 2012 Sixth IEEE/ACM International Symposium on, pages 142–151, 2012. [18] Y. Wang and G. E. Suh. Efﬁcient timing channel protection for onchip networks. In 2012 IEEE/ACM Sixth International Symposium on Networks-on-Chip, pages 142–151, May 2012. [19] H. M. G. Wassel, Y. Gao, J. K. Oberg, T. Huffmire, R. Kastner, F. T. Chong, and T. Sherwood. Surfnoc: A low latency and provably noninterfering approach to secure networks-on-chip. In Proceedings of the 40th Annual International Symposium on Computer Architecture, ISCA ’13, pages 583–594, 2013. [20] H. M. G. Wassel, Y. Gao, J. K. Oberg, T. Huffmire, R. Kastner, F. T. Chong, and T. Sherwood. Surfnoc: A low latency and provably noninterfering approach to secure networks-on-chip. SIGARCH Comput. Archit. News, 41(3):583–594, June 2013. [21] W. Zhang, L. Hou, J. Wang, S. Geng, and W. Wu. Comparison research between xy and odd-even routing algorithm of a 2-dimension 3x3 mesh topology network-on-chip. In 2009 WRI Global Congress on Intelligent Systems, volume 3, pages 329–333, May 2009. Fig. 10. Application performance and normalized power consumption of each compared non-interference design. technique. For each benchmark the proposed NIBR solution consumed the least amount of power with an average power consumption penalty of 1.84% over the baseline. While NIBR consumed more power than the baseline, our proposed solution was able to reduce the power overhead by 10% on average when compared to the TNP (TDM). When compared to RPSL, NIBR reduced power consumption by up to 4x. This is because RPSL throttles the trafﬁc for the high security domain to allow low domain trafﬁc to take priority. While we throttle trafﬁc in NIBR the same way as RPSL, NIBR is able to avoid the bottlenecks by re-routing packets through alternative minimal paths. However, if a network becomes saturated the advantage of re-routing trafﬁc will diminish over time and NIBR would perform similar to RPSL in the worst case scenario. V. CONC LU S ION In this paper, we proposed to secure NoCs against timing attacks with Non-Interference Based Routing (NIBR) where we efﬁciently disperse network trafﬁc to improve application performance by increasing network utilization through spatial locality, but maintaining strict VC allocation policies to enforce non-interference and prevent information leakage. In our analysis we showed that the proposed technique maintains non-interference between security domains, prevents DoS, and improves performance by 2-20% over existing techniques with only a 1.84% power consumption penalty. For future research, we hope to expand on this work by extending NIBR with further ﬁne-grain routing decisions based on both the type of trafﬁc within an application as well as the sub-security domains. ACKNOW L EDGM ENT This work was partially supported by National Science Foundation grants CCF-1420718, CCF-1513606 and CCF1703013. "
Exploration of Memory and Cluster Modes in Directory-Based Many-Core CMPs.,"Networks-on-chip have become the standard interconnect solution to address the communication requirements of many-core chip multiprocessors. It is well-known that network performance and power consumption depend critically on the traffic load. The network traffic itself is a function of not only the application, but also the cache coherence protocol, and memory controller/directory locations. Communication between the distributed directory to memory can introduce hotspots, since the number of memory controllers is much smaller than the number of cores. Therefore, it is critical to account for directorymemory communication, and model them accurately in architecture simulators. This paper analyzes the impact of directorymemory traffic and different memory and cluster modes on the NoC traffic and system performance. We demonstrate that unrealistic models in a widely used multiprocessor simulator produce misleading power and performance predictions. Finally, we evaluate different memory and cluster modes supported by Intel Xeon-Phi processors, and validate our models on four different cache coherence protocols.","Exploration of Memory and Cluster Modes in Directory-Based Many-Core CMPs Subodha Charles University of Florida Gainesville, FL, USA charles@cise.uﬂ.edu Chetan Arvind Patil Arizona State University Tempe, AZ, USA chetanpatil@asu.edu Umit Y. Ogras Arizona State University Tempe, AZ, USA umit@asu.edu Prabhat Mishra University of Florida Gainesville, FL, USA prabhat@uﬂ.edu Abstract—Networks-on-chip have become the standard interconnect solution to address the communication requirements of many-core chip multiprocessors. It is well-known that network performance and power consumption depend critically on the trafﬁc load. The network trafﬁc itself is a function of not only the application, but also the cache coherence protocol, and memory controller/directory locations. Communication between the distributed directory to memory can introduce hotspots, since the number of memory controllers is much smaller than the number of cores. Therefore, it is critical to account for directorymemory communication, and model them accurately in architecture simulators. This paper analyzes the impact of directorymemory trafﬁc and different memory and cluster modes on the NoC trafﬁc and system performance. We demonstrate that unrealistic models in a widely used multiprocessor simulator produce misleading power and performance predictions. Finally, we evaluate different memory and cluster modes supported by Intel Xeon-Phi processors, and validate our models on four different cache coherence protocols. I . IN TRODUC T ION Continuous advances in manufacturing technologies enable integrating an increasing number of general purpose as well as specialized processors on the same chip. For example, Intel Xeon Phi processors, code-named “Knight’s Landing” (KNL), feature 64-72 Atom cores and 144 vector processing units [1]. A larger number of cores can be exploited only if each core has fast and high bandwidth access to memory. Therefore, a low-latency network-on-chip (NoC) interconnects the cores with each other and a suite of integrated memory controllers (MC), which provide interfaces to multi-channel DRAM (MCDRAM) and main memory (DDR) [2]–[4]. Modern chip multiprocessor (CMP) architectures commonly employ directory-based cache coherence protocols and multiple levels of cache. The ﬁrst and second level caches (L1 and L2) are collocated with each core, while the last level cache (LLC) and tag directory are distributed throughout the chip, as illustrated in Figure 1. An L2 miss triggers a request to the directory that keeps track of the corresponding memory address. In case of a hit, the data is returned from (or written to) the LLC slice collocated with the directory. Otherwise, the request is forwarded to one of the MCs. Due to pin limitations and packaging constraints, the number of MCs is This work was partially supported by the National Science Foundation (NSF) grants CNS-1526687 and CNS-1526562. 978-1-5386-4893-3/18/$31.00 ©2018 IEEE much less than the cores. For example, Intel Xeon Phi has 8 MCs interfacing MCDRAM and two MCs interfacing DRAM, while the system has 72 cores [1]. Similarly, AMD Opteron 6386 SE has 16 cores with 1 MC and 4 memory channels. Therefore, LLC-Memory communication exhibits a many-tofew communication pattern while Core-LLC communication is many-to-many. In other words, memory trafﬁc is likely to introduce hotspots, whereas Core-LLC trafﬁc is relatively uniform. As a result, these hotspots and poor design choices can cause signiﬁcant performance degradation [5], as demonstrated in our experimental results. As an example, Intel Xeon Phi processor provides different cluster modes that deﬁne the afﬁnity of directories to speciﬁc MCs due to the importance of LLC/directory to memory trafﬁc. In the all-to-all mode, any directory can send requests to any MC on the chip. In contrast, quadrant mode divides the chip into four virtual partitions, where the directories in each partition are paired with speciﬁc controllers in their own quadrant. As a result, the quadrant mode localizes the trafﬁc in an attempt to improve the memory performance. Similarly, sub-NUMA cluster modes SNC-2 and SNC-4 divide the processor in two and four virtual sockets, respectively. In addition, the MCDRAM can be used as a cache (Cache mode), Fig. 1: Representative illustration of a many-core CMP. L1/L2: Private ﬁrst and second level cache, LLC: Distributed shared last level cache, TD: Tag directory distributed along with LLC, PCIe: PCI-express controller for I/O units, R: Router, PCU: Power control unit. an extension to DDR (Flat mode) or in a Hybrid mode [1], giving three memory mode options. Each of these choices lead to a different NoC trafﬁc pattern as a function of the workload. The optimum cluster and memory mode is a strong function of the target application. Applications whose threads and memory footprint ﬁt to a single quadrant can take advantage of the strong locality of quadrant and sub-NUMA modes. However, highly parallel applications with a large number of threads and memory footprint may beneﬁt from all-to-all and ﬂat memory modes. Analyzing the power consumption and performance impact of cluster and memory modes is important for two reasons. First, it enables us to use the existing platforms optimally. Second, it can help in making better architectural choices. This analysis is not feasible on existing hardware platforms, since the trafﬁc between the cores and memory is not observable. Furthermore, there are no public simulators capable of performing this exploration. For example, gem5 [6], which is one of the most widely used architecture simulators, assumes that there is an interface from each tile to the main memory. Consequently, the memory access latency is modeled, but the actual trafﬁc from LLC/directory to memory is not captured. This makes the default gem5 model unsuitable for cluster and memory mode exploration. This paper analyzes the impact of cluster and memory mode choices on the NoC trafﬁc. We demonstrate that congestion on the NoC links affects not only the communication latency, but also power consumption and application execution time. We also show that any exploration that involves LLC/directory to memory trafﬁc requires a simulation framework that models the cache coherence protocols accurately. We demonstrate both qualitatively and quantitatively that neglecting the LLC/memory trafﬁc, as it is done in gem5, gives highly optimistic results in terms of the network load, latency and power consumption. We also show that this inaccuracy can lead to misleading conclusions in terms of optimal MC placement. Then, we describe how the LLC/directory to memory trafﬁc, originating from directory-based cache coherence, can be modeled in architectural simulators. Using the corrected gem5 model, we evaluate the power consumption and performance impact of quadrant and all-to-all cluster modes together with cache and ﬂat memory modes which conﬁgure the directory-MC afﬁnity. The major contributions of this paper are as follows: • We demonstrate the importance of modeling the directory-memory trafﬁc, and show that considering only core to directory trafﬁc gives highly optimistic results. • We describe how to accurately model the LLC/directory to memory trafﬁc, and contrast it with the assumption adopted in gem5 [6] full system simulator. • We explore speed-up achieved by different cluster and memory modes supported by the state-of-the-art CMPs using the enhanced version of gem5. We conﬁrm that the trends are the same with those obtained on the real platform. • We demonstrate the impact of this work on four different cache coherence protocols. The rest of the paper is organized as follows. Section II presents related work on NoC design and exploration. Section III gives a background on memory and cluster modes. Section IV presents our NoC modeling and exploration framework. Section V presents the experimental results. Finally, Section VI concludes the paper. I I . R ELAT ED WORK Prior work on trafﬁc exploration on NoC and optimization motivates the need for better memory and processor placement to reduce contention and latency. Early work in this area suggests the efﬁcient distribution of memory trafﬁc to provide quality-of-service guarantees [7]. Abts et al. [5] tackle the problem of optimum MC placement where m cores need to be placed with n MCs. The placement is decided by examining the variation in latency experienced by cores to access each MC. “Diamond” placement is found to be the best for an 8x8 mesh with 16 MCs, while further improvements are achieved by introducing a class-based deterministic routing algorithm. Xu et al. [8] leverage this idea to ﬁnd an optimal placement for the same conﬁguration. The minimum number of MCs and their placement required to achieve a given performance goal was explored by taking Intel SCC [9] as a case study [10]. Once the number of MCs are decided and placed, it creates opportunity for optimization by dynamically mapping workload data to appropriate MCs [11]. The effect of modeling the main memory access through the directory was discussed by Duraisamy et al. [12]. They explore the trafﬁc patterns of two-level MESI directory protocol and AMD’s Hammer-based HyperTransport (HT) [13] protocol to design an efﬁcient multicast aware wireless NoC. Ros et al. analyzed area and trafﬁc trade-offs associated with cache coherence protocols [14]. To optimize power and performance, Schuchhardt et al. [15] propose a method to place directories closer to their shared data and thereby eliminating many network traversals. Other coherence trafﬁc-based optimization techniques include coherence protocol deactivation for private block accesses to reduce directory accesses [16], and a bloom ﬁlter mechanism for tagless coherence directory [17]. In contrast to our work, none of the prior studies rigorously explore the afﬁnity between PE, MC and directory in a system running directory-based cache coherence and optimization with different cluster and memory modes. As shown in our experimental results, the conclusions of the optimum MC placement study by Abts et al. [5] and Xu et al. [8] are no longer valid, when the LLC/directory to MC trafﬁc is considered. Hence, our proposed correction is vital and crucial for emerging NoCs with wireless [12], optical [18] and 3D networks [19]. I I I . M EMORY AND C LU S TER MODE S IN MODERN CMP S A. Memory Modes in Xeon-Phi Architecture Xeon-Phi architectures have a high-bandwidth MCDRAM memory and a larger low-bandwidth DDR memory [1]. These two memory types can be conﬁgured at boot time in different ways, as illustrated in Figure 2. • Flat Mode: In the ﬂat mode, both the MCDRAM and DDR memory are mapped in the same system address space. This mode is ideal for applications with data that can be separated into categories of a larger, lowbandwidth region, and a smaller, high-bandwidth region. • Cache Mode: In the cache mode, MCDRAM acts as a last level cache which is placed in between the DDR memory and L2 cache. The cache is direct mapped with a cache line size of 64-bytes. All memory requests ﬁrst go to the MCDRAM for a cache memory lookup, if there is a cache miss, they are sent to the DDR memory. • Hybrid Mode: In the hybrid mode, part of MCDRAM (half or quarter) is used in cache mode while the rest is used as ﬂat mode memory. The DDR memory will be served by the cache portion. This works well for a variety of applications that take advantage of storing frequently accessed data in ﬂat memory while also beneﬁting from regular caching. Fig. 2: Three memory modes in Xeon-Phi architectures [1]. B. Cluster Modes in Xeon-Phi Architecture The mesh interconnect in KNL supports three cluster modes, which have signiﬁcant impact on the NoC trafﬁc behavior [1]. Similar to memory modes, cluster modes can also be selected from BIOS during boot time. • All-to-all mode: In this mode, there is no afﬁnity between the processing element (PE), MC and directory. That is, a memory request can go from any directory to any MC. As a result, this mode does not exploit locality, unlike the other two cluster modes. • Quadrant mode: In the quadrant mode, the chip is divided into four quadrants. There is an afﬁnity between the directories and MC in the same quadrant. However, there is no afﬁnity between the PE and directory, i.e, a processor can send the memory request to any directory, but the directory will always forward that request to an MC on the same quadrant. • Sub-NUMA mode: This mode takes one more step forward by enforcing afﬁnity between all three components - PE, MC and directory. A request from a PE lands on a directory on the same quadrant, and the directory can forward that request to an MC on the same quadrant. The optimal combination of memory and cluster modes depends on the application characteristics and, largely affects the power and performance statistics. Figure 3 illustrates the trafﬁc ﬂow of these memory and cluster modes using examples. The quadrant and sub-NUMA clustering modes improve the locality of memory trafﬁc. For instance, Figure 3c illustrates the quadrant mode in KNL [1]. The initial request from a core can go to any directory (1). However, each directory is associated with the MCs within the same quadrant. The memory request marked with (2) can go to integrated MC on the right side or to MCDRAM controllers at the upper right corner. This afﬁnity helps in localizing the directory-memory trafﬁc, which in turn improves memory access latency. IV. ACCURAT E MODE L ING O F LLC /D IR EC TORY TO M EMORY COMMUN ICAT ION In this section, we ﬁrst describe how the transactions between core, LLC/directory and memory occur in modern CMPs. Then, we contrast it to the assumption made by gem5 and highlight the consequences. Next, we present an accurate NoC modeling and implementation of cluster and memory modes in gem5. Finally, we demonstrate that our framework is vital to accurately model and explore modern CMPs. A. Memory Controller Placement in CMPs Due to pin limitations and package constraints, it is unrealistic to attach a memory controller to each core in a CMP. For example, Intel Core i7-900 processor has only one MC, and 27.3% of its total pins are dedicated to the MC [20]. Similarly, the Tilera Tile64 processor integrates 64 cores in an 8x8 mesh with four on-chip MCs [3]. This results in a core to MC ratio of 16:1. The total number of cores and memory controllers in several modern CMPs are summarized in Table I. TABLE I: Comparison of cores and number of MCs in modern many-core CMPs. Processor # Memory Controllers # Cores Intel Xeon Phi 7210 [21] Tilera Tile64 [3] Intel Xeon 8160M AMD Opteron 6386 SE 64 64 24 16 8 MCDRAM & 2 DDR4 4 DDR2 in 16 ports 2 DDR4, 6 channels 1 DDR3, 4 channels Several studies have shown that relative placement of cores and MCs plays an important role in network trafﬁc distribution [5], [8]. This impact is more signiﬁcant in topologies, such as 2D Mesh, which do not have edge symmetry. Thus, it is evident that connecting MCs to every tile gives a highly optimistic estimate of the realistic scenario. Moreover, a large fraction of trafﬁc in a CMP originates not from actual data transfers, but from communication between cores to maintain data coherence [15]. As soon as the directory component comes into play, the trafﬁc distribution is not the same as processor to processor trafﬁc or processor to memory trafﬁc. Therefore, the afﬁnity between the cores, directories and MCs affect the performance of architectures that employ a distributed directory-based cache coherence algorithm. Consequently, it is crucial to accurately account for the communication ﬂow between the cores, tag directories and memory controllers. Arguably, the most widely used architectural simulator gem5 [6] makes an unrealistic assumption that there is an interface to main memory from every tile of the NoC. This eliminates the exploration of afﬁnity between directory and (a) Example of L2 miss in ﬂat memory mode and all-to-all cluster mode: (1) L2 cache miss. Memory request injected on the network to check the tag directory, (2) request forwarded to any memory controller after miss in tag directory, (3) data read from memory and sent to the requester. (b) Example of L2 and MCDRAM miss in cache memory mode and all-to-all cluster mode: (1) L2 cache miss. Memory request injected on the network to check the tag directory, (2) request forwarded to MCDRAM which acts as a cache after miss in tag directory, (3) request forwarded to memory after miss in MCDRAM, (4) data read from memory and sent to the requester. (c) Example of L2 miss in ﬂat memory mode and quadrant cluster mode: (1) L2 cache miss. Memory request injected on the network to check the tag directory, (2) request forwarded to memory controller on the same quadrant, (3) data read from memory and sent to the requester. Fig. 3: Trafﬁc models in ﬂat and cache memory modes and all-to-all and quadrant cluster modes in KNL architecture [1]. MC. Furthermore, the effects of memory modes cannot be captured in the current gem5 setup. B. LLC/Directory to Memory Communication in CMPs A miss in the local cache triggers a sequence of transactions in many-core architectures with distributed directories, as demonstrated Figure 4a. The order of these transactions are as follows: 1) The request is forwarded to the directory controller which contains the memory address information, 2) If data is not available in any of the caches, the request is forwarded to an MC, 3) The data is retrieved from the memory, 4) The MC forwards the data to the requester. The last two steps are signiﬁcant, since they introduce many-to-few communication pattern due to the smaller number of MCs, as summarized in Table I. As a result, they not only increase the number of packets in ﬂight, but also lead to hotspots which contribute to increased latency. C. Unrealistic Assumptions in gem5 on LLC/Directory to Memory Communication gem5 is one of the most popular many-core architecture simulators [6]. Instead of following these steps given in Section IV-B, it models the memory accesses directly from the directory (home node) itself, as illustrated in Figure 4b. The ﬁrst step is the same as shown in Figure 4a. That is, the request goes from the core to the tag directory responsible for the corresponding memory address. If there is an LLC miss, the data needs to be fetched from the memory, as expected. However, the memory access is modeled within the home directory (2), without explicitly modeling the trafﬁc from the directory to memory controller. In other words, each “directory Fig. 4: (a) Life cycle of a memory request and resulting transactions in real distributed directory systems. (b) The same transactions modeled in default gem5. We modiﬁed gem5 to have this realistic ﬂow. In the quadrant mode, there is an afﬁnity between a tag directory and the MC in that quadrant. Our modiﬁcation enables to accurately model this architectural feature, while the default model shown in (b) cannot differentiate the afﬁnity. controller” implies both a directory (i.e. state) and an MC [6]. The model accounts for the delay to main memory, but it does not have a separate MC node in the NoC. Therefore, the NoC trafﬁc to and from the memory controllers is not modeled. In contrast, the data is forwarded directly from the directory to the requester (3). Comparing the two scenarios, we can see that step 2 in Figure 4a does not exist in the current gem5 model. Moreover, the data (step 3) is sent from the directory, not from the MC as in the realistic model. The modeling choice in Figure 4(b) essentially establishes a virtual link between the tag directory and memory controllers. Therefore, the request and data packets to and from MCs are completely missed in this simulation model. This affects not only the communication Impact on NoC Trafﬁc: latency of a given transaction but also the utilization of the links and routers on the path. Consequently, the latency of all the NoC trafﬁc that goes through those routers will be lower in simulation than their actual values. Hence, this will result in optimistic performance estimates. D. Modeling and Exploration of Intel Xeon-Phi architecture An accurate NoC simulation model should explicitly capture PE to directory, directory to memory and memory to PE trafﬁc. Mapping Addresses to Memory Controllers: Since all the cores share the MCs, we need a mechanism to allocate different address ranges to the available MCs. To achieve this, the physical address of a memory location is mapped to an MC according to the function shown in Listing 1. It allocates a certain set of bits from the address to select the MC by deﬁning the range of bits from small to big and dividing the addresses uniformly among MCs. In this formulation, addr is the address to map, small is calculated as (numa high bit num memories bits + 1), and big is numa high bit. Here, numa high bit and num memories bits are calculated depending on the number of MCs. These expressions enable an even distribution of memory addresses among the MCs, which is similar to the decisions in a modern CMP [22]. Listing 1: Address hashing function used to map an address to a memory controller Addr bitSelect(Addr addr, unsigned int small, unsigned int big) { assert(big >= small); if (big >= ADDRESS_WIDTH - 1) { return (addr >> small); } else { Addr mask = ˜((Addr)˜0 << (big + 1)); Addr partial = (addr & mask); return (partial >> small); } } Simulation Framework: We employ a cycle-accurate fullsystem simulator - gem5 [6] and “GARNET2.0” [23] interconnection network model. The default gem5 model is modiﬁed to include separate MCs and to model PE to PE, PE to directory, directory to memory as well as memory to PE trafﬁc. The gem5 implementation handles the trafﬁc ﬂow through coherence protocols. In a distributed cache coherence protocol, in case of a cache miss, the request is forwarded to the coherence protocol controller. It makes the necessary state transitions and pushes the message in the appropriate virtual network to the network interface. The network interface then converts the message into network packets and sends them to the network via the connected router. The network then routes the ﬂits to the destination node using X-Y deterministic routing protocol. When the home directory receives the packet, it checks its state machine to see if another cache shares that data. If yes, it forwards the packet to the owner and then to the requestor (PE) and if not, it initiates a memory fetch depending on which memory and cluster modes are being used. If it is all-to-all and ﬂat mode, addresses are uniformly distributed across MCDRAM and DDR memory spaces. Which MCDRAM/DDR memory controller to forward to is decided using the function in Listing 1. If it is quadrant and ﬂat mode, only MCs in that quadrant are considered as candidates for forwarding the memory requests. In all-to-all and cache mode, MCDRAM space is treated as a last-level cache. Therefore, the request is sent to an MCDRAM controller for a cache lookup. If it is a miss, the memory request is again forwarded to the appropriate MC (selected using Listing 1 without considering MCDRAM controllers), and memory fetch request is placed through there. Once the requested data is fetched from either the DDR or MCDRAM memories, it is forwarded back to the PE after making the necessary coherence transitions. We explicitly differentiate the behavior of MCDRAM memory in cache and ﬂat modes. In cache mode, MCDRAM cache modules are instantiated and can be accessed only through the designated MCDRAM controller locations. In ﬂat mode, this cache module is not used, and an MC similar to the MCs interfacing DDR memory is connected to the designated nodes. We emphasize that without our modiﬁcation of the gem5 model, it is not possible to explore the power consumption and performance impact of different cluster and memory modes. The next section highlights two important aspects of our exploration framework. Our proposed NoC model is realistic since the power and performance numbers are comparable with the results from the Xeon-Phi hardware board. Moreover, our framework can be used to accurately model and explore a wide variety of current and future NoC architectures. V. EX PER IM EN TA L R E SU LT S A. Experimental Setup Architecture Model: In our studies, we use the Intel Xeon Phi 7210 platform [21] and model the same on gem5 [6]. It mainly targets high performance computing and other parallel computing segments. The architecture offers high memory bandwidth and massive parallelism options which enables it to run memory and processor intensive workloads with high throughput. A 64-core CMP is modeled with gem5 using a mesh topology. Each tile is composed of a core that runs at 2 GHz, private L1 cache, tag directory and a router. Each cache is split into data and instruction caches with 16kB capacity each. GARNET2.0 [23], which leverages the routing infrastructure provided by ruby memory system, models a router with a crossbar switch, switch allocation, virtual circuit selection and 4 input buffers giving a 3-cycle pipeline. Each router is connected to four other routers with internal links and to an L1 cache and a directory controller through individual network interfaces via external links. The complete set of simulation parameters are summarized in Table II. NoC Power Model: Since dynamic power consumption of an NoC is a function of the trafﬁc ﬂow, we need to use an energy model that captures the changes in the trafﬁc ﬂow. We use the model in [24] to estimate the power consumption. Acutilization at each router as shown in Figure 5. Figure 5a shows a 4x4 mesh where the MCs are connected to each directory which is the default implementation of gem5. Trafﬁc is uniform except for the tile 0 where the PE resides (tile numbers are as shown in Figure 1). Figure 5b shows a realistic scenario where every other parameter is kept the same, but MCs are connected to boundary routers. This does not display the uniform trafﬁc distribution as shown in Figure 5a. Trafﬁc patterns show hotspot columns due to MC placement which increases latency and saturates the throughput. The 4x4 mesh and MC placement conﬁgurations used in Figure 5 are for illustration only. Experiments are carried out using the parameters mentioned under section V-A. As a result of this congestion and more packets being sent through the network, the realistic model shows a 54.9% more network ﬂit latency on average across FFT, FMM, RADIX and LU benchmarks compared to the unrealistic model with a similar topology. A comparison of network latency, NoC power usage and execution times with different benchmarks is shown in Figure 6. As stated before, the default gem5 model does not permit cluster mode exploration as MCs are collocated with directories at every tile. Even then, if the default model is used for exploration, the results in Figure 6 show that it gives highly optimistic results for NoC latencies and power. (a) MCs modeled at each tile. cording to the energy model, there are two main contributors to NoC power; 1) Number of packets injected into the network - this is directly related to the number of cache misses in L1 and L2 caches, and in cache memory mode, misses in MCDRAMs. 2) Average hops traversed by packets - depends on the relative placement of PE, MCs and directories. The afﬁnity between these components which are conﬁgured using the cluster modes also contributes to the number of hops. We feed the output statistics from gem5 to the McPAT power modeling framework [25]. Power consumption of other components - caches, processor, off-chip memory and directories, are estimated using the energy models in McPAT. TABLE II: System conﬁguration parameters used in our simulations. L1 cache Topology Routing scheme Router Cache coherence Memory size Access latency Processor Conﬁguration Number of cores 64 Core frequency 2 GHz Instruction set architecture x86 Memory System Conﬁguration private, separate instruction and data cache. Each 16kB in size. distributed directory-based protocol 4GB DDR 300 cycles Interconnection Network Conﬁguration 8x8 Mesh (formed by rings in rows and columns) X-Y deterministic 4 port, 4 input buffer router with 3 cycle pipeline delay Link latency 1 cycle Parameters that change when implementing KNL Number of cores 32 (in 32 tiles each with one core) Core frequency 1.4 GHz L1 cache private, separate instruction and data cache. Each 32kB in size. shared, direct mapped cache MCDRAM Benchmarks: We use benchmarks from SPLASH2 [26] and MiBench [27] benchmark suites to run on gem5. B. Parameters used to model KNL The number of cores in gem5 must be a power of 2. We have 32 tiles with cores similar to KNL. However, each tile contains a single core unlike KNL, since gem5 does not support tiles with two cores. To match the number of cores, we deactivate one core in each tile in our Xeon-Phi platform. We also place the MCs to match the KNL architecture shown in Figure 3. Moreover, we set the core frequency to 1.4 GHz when comparing the simulation results against the hardware measurements to match our Xeon-Phi platform frequency. The parameters used in implementing KNL are summarized in Table II. C. Network trafﬁc analysis of realistic and unrealistic models To compare the effects of realistic (proposed approach) and unrealistic (default gem5) models, we observe the buffer (b) MCs modeled only at places marked in blue. Fig. 5: Buffer utilization in routers when RADIX benchmark running on core 0. Value in each tile is normalized to the highest buffer utilization value in the modiﬁed gem5 implementation (Figure 5b). Color coded to show the distribution of utilization across tiles (dark green - lowest and dark red highest). D. Trafﬁc Variation with different cache coherence protocols Another factor that effects the NoC trafﬁc behavior is the cache coherence protocol [28]. The default gem5 NoC implementation already captured these variations as it correctly implemented the PE to directory afﬁnity. We explored the effects of different cache coherence protocols - (1) MI, (2) MESI Two-Level, (3) MOESI CMP Directory, (4) MOESI (a) Normalized network latency. (a) MI (b) MI (c) MESI Two Level (d) MESI Two Level (b) Normalized NoC power usage. (e) MOESI CMP Dir. (f) MOESI CMP Dir. (c) Normalized execution time. Fig. 6: Power and performance comparison for different models: a) Default: unrealistic gem5 model which collocates MCs with directories at each tile. b) KNL (all-to-all): gem5 KNL model with all-to-all cluster mode and ﬂat memory mode. c) KNL (quadrant): gem5 KNL model with quadrant cluster mode and ﬂat memory mode. Hammer [6]. Figure 7 shows trafﬁc variation with different cache coherence protocols when RADIX benchmark is running on a 4x4 Mesh NoC. Figure 7a, show buffer utilization at each router when MI cache coherence protocol is used with the default gem5 implementation, which assumes a memory interface at each tile. Similar to the observation of Figure 5a, the trafﬁc shows a uniform gradient across the routers without any congestion. Figures 7b shows the same results with our modiﬁed implementation. We observe that the patterns remain consistent across cache coherence protocols. As evident from Figure 7c & 7d, Figure 7e & 7f and Figure 7g & 7h pairs, this observation remains the same across other 3 cache coherence protocols as well. Therefore, the observations made in Section V-C hold irrespective of the cache coherence protocol. This is expected as our modiﬁcation only affects the afﬁnity between directory and MC. With increased sharing in cache coherence protocols, the trafﬁc in NoC increases. But, the hotspot locations and the trafﬁc distribution remain the same. (g) MOESI Hammer (h) MOESI Hammer Fig. 7: Buffer utilization in routers when RADIX benchmark is running on a 4x4 Mesh with different cache coherence protocols. The color code is similar to what is used in Figure 5. (a), (c), (e), (g) show results with default gem5 implementation and (b), (d), (f), (h) show the same after our modiﬁcation. The buffer utilization in each tile is normalized to the highest value in the modiﬁed gem5 implementation for a given cache coherence protocol. when realistic MC placement conﬁgurations in [8] as well as gem5 default (unrealistic) model are tested across different benchmarks. As further evidence for the highly optimistic nature of the default gem5 model, we can see that the latency is signiﬁcantly less when compared to realistic MC placement models. In contrast to the conclusion in [8], “Optimal” is no longer the best placement, when the directory-based coherence is introduced. The results not only depend on MC placement, but also on PE placement and coherence protocol. Considering only the realistic placements described in [8], Column 2/5 conﬁguration turns out to be the best by 9.0% compared to the worst conﬁguration (Slash) when running BASICMATH. Column 2/5 also beats “Optimal” by 5.3% on average across all benchmarks. The trafﬁc congestion caused by adjacent MCs in Column 2/5 conﬁguration is compensated by the reduced hop counts, since it gives smallest average hop count. E. Network Latency Comparison for different MC placements Xu et al. explored network trafﬁc behavior with different MC placements (Column 0/7, Column 2/5, Diamond, Slash and Optimal) and concluded that the “Optimal” was best for similar benchmarks [8]. Figure 8 shows network ﬂit latency F. Exploration of memory and cluster modes and validation with results from the KNL hardware platform As seen from results in Figure 8, the afﬁnity between the PE, MC and directory plays a major role in network trafﬁc behavior. To explore this further, we experimented with PE placement and coherence protocol to come to realistic conclusions. Finally, our experimental results demonstrated that the afﬁnity between MC and directory controller can be manipulated with different cluster and memory modes such as quadrant, all-to-all, ﬂat and cache modes introduced in Intel’s KNL architecture to achieve better performance and power results. Our proposed exploration framework is vital for emerging NoCs with wireless, optical and 3D networks. "
Abetting Planned Obsolescence by Aging 3D Networks-on-Chip.,We set up a security analysis framework by aging the Network-on-Chip (NoC) to study planned obsolescence by the original equipment manufacturer (OEM). An NoC is the communication backbone in a manycore System-on-Chip (SoC). Planned obsolescence may adopt any vulnerability in the NoC to cause the SoC to fail. We show how an OEM can craft workloads to generate electromigration-induced stress and crosstalk noise in TSV-based vertical links in the NoC to hasten failure. We analyzed three malicious workloads and confirm that a crafted workload that injects 3-10% more traffic on to a few selected critical vertical links can shorten the lifetime of the NoC by 11%-25% averaged over the benchmarks considered in this work.,"Abetting Planned Obsolescence by Aging 3D  Networks-on-Chip  Sourav Das§,1, Kanad Basu¥,1, Janardhan Rao Doppa§,2, Partha Pratim Pande§,3, Ramesh Karri¥,2, Krishnendu Chakrabarty#  §1,2,3School of EECS, Washington State University, Pullman , WA; ¥1,2Department of ECE, New York University, Brooklyn, NY;   #Department of ECE, Duke University, Durham, NC.  Email: §{1sourav.das, 2jana.doppa, 3pande}@wsu.edu, ¥{1kb150, 2rkarri}@nyu.edu, #krishnendu.chakrabarty@duke.edu  Abstract— We set up a security analysis framework by aging  the Network-on-Chip (NoC) to study planned obsolescence by the  original equipment manufacturer (OEM). An NoC is the  communication backbone in a manycore System-on-Chip (SoC).  Planned obsolescence may adopt any vulnerability in the NoC to  cause the SoC to fail. We show how an OEM can craft workloads  to generate electromigration-induced stress and crosstalk noise in  TSV-based vertical links in the NoC to hasten failure. We  analyzed three malicious workloads and confirm that a crafted  workload that injects 3-10% more traffic on to a few selected  critical vertical links can shorten the lifetime of the NoC by 11%25% averaged over the benchmarks considered in this work.        Fig. 1: (a) 3D NoC uses TSV-bundles as vertical links (VL) between adjacent layers. The lifetime of VLs widely varies across the regions. (b) Illustratio n of VL  utilization profile and ‘cascading-effects’ of VL failures with simplified graphs at time t= (i) t1, (ii) t2, (iii) t3, (iv) t4 (t4> t3> t2> t1). (c) Typical ut ilization profile  and lifetime distribution of VLs in a 64-core NoC. High traffic utilization of the VLs results in significant reduction in lifetime and low MTTFs.  voltage tuning, and body biasing to alleviate NBTI related  issues [11], [12]. Hot Carrier Injection [13] and gate-oxide  breakdown [14] are the other phenomena that degrade the  performance of ICs. Error detection and recovery circuits have  been incorporated into processors to protect against timing  errors induced by aging and parametric variations [15]. Agingaware scheduling [16] and aging-aware cell library design have  been developed to curb aging in manycore processors [17].   The production of a chip entails design, synthesis,  manufacturing and testing steps by different actors within the  globally distributed electronics supply chain. This creates  opportunities  for  introducing Hardware Trojans,  IC  counterfeiting and overproduction [18], [19]. Third-party IPs  are another source of concern [20]. Reliability-based Trojans  can exploit NBTI and HCI effects [21]. Electromigrationinduced IC failure by aging metal wires and corresponding  circuits by introducing hardware Trojans was proposed in [22].   Prior research in hardware security has focused on the cores,  microarchitectures, and circuits. However, attacks on the  interconnects and in particular an NoC, a critical component in  a manycore SoC, has not received much attention. SECA, a  hardware NoC-based firewall,  that detects and  isolates  malicious requests, was proposed for bus-based NoCs [23].  Gossip-NoC explores side channel- and discrete timing-based  attacks by exchanging sensitive information among networks  [24]. ACeNoC partitions a NoC into reconfigurable security  zones and confines the communications depending on the  attack [25]. Security concerns of third-party IPs and NoC  components were explored in [26].    LeukoNoC counts the number of flits passing through TSVs  to detect attacks and isolates VLs to maintain system  performance [27]. However, it doesn't consider workloadinduced electromigration and crosstalk simultaneously.  Moreover, LeukoNoC assumes malicious workloads are  already present and does not offer guidance on constructing the  malicious programs and quantifying their effects on the  performance of the NoC.  Complementary to prior work on processor aging, we study  the effects of crafted malicious workloads on 3D NoCs. We  focus on electromigration stress and crosstalk noise in TSVs  and create malicious workloads that stress critical VLs to fail.  Accelerating the aging of the VLs degrades performance ,  causing the system to fail. Our choice of this attack vector is  motivated by the fact that the crafting of malicious workloads  is an easy path to planned obsolescence for an OEM to adopt.   III.  TSV-ENABLED MANYCORE SOCS  3D NoC architectures can improve the energy-efficiency  and execution time of manycore SoCs compared to planar  interconnect-based counterparts. Most 3D NoCs utilize  through-silicon-via (TSV) bundles as the VLs between the  planar dies [29]. The performance gain of 3D NoCs is  undermined when TSVs fail [29]. The uneven workload  distribution among VLs is a major reason for TSV failure in a  3D NoC. The resulting stress causes electromigration in TSVs  and increases their resistance and associated delay of the links.  The cross-coupling effects from the neighbors also increases  the worst-case delay of TSVs in the VL bundle [6]. The  combined effects of the workload-induced stress and the  crosstalk noise degrade timing in the TSV-based VLs.    Among existing 3D NoCs, a small-world network-enabled  3D NoC (3D SWNoC) outperforms other 3D NoCs in terms of  performance and reliability [29]. Hence, we focus on the 3D  SWNoC-based manycore SoC in this work.   Fig. 1(a) shows an example 3D SWNoC. The wear -out in  such a 3D NoC is a function of VL utilization and is caused by  the electromigration of the TSVs in the VL. The uneven  utilization of TSVs causes non-uniform wear-out of the VLs  varying the lifetime unevenly across the NoC [6]. For example,  in Fig. 1(a), we plot the lifetime of the VLs of the 3D SWNoC  when the CANNEAL benchmark from the SPLASH-2 suite is  running on the SoC [37]. The uneven lifetime distribution opens  up possibilities of planned obsolescence strategies targeting  VLs with relatively low lifetime.   MAGIC [28] uses malicious programs to expedite aging of  the processor pipeline. MAGIC demonstrated 17%  performance degradation in 6 months in the OpenSPARC T1  processor. MAGIC analyzes aging effects on circuit nodes,  critical paths, and microarchitecture components when running  malicious programs.    From a planned obsolescence perspective, the utilizationdependent wear-out of TSVs offers opportunities for security  attacks. By shaping the utilization profile of targeted TSVbased VLs via carefully crafted malicious workloads, the  lifetime of those VLs can be reduced. Failure of one VL has a  cascading effects on the lifetime of neighboring VLs and the                  Fig. 2:(a) A TSV-based vertical link between two routers from adjacent layers. (b) A simple model of TSV-based VL to study the workload-induced stress. The  TSV is connected to a serializer and a de-serializer.    lifetime of the SoC. Fig. 1(b) shows a simple six-node NoC.  The utilization and failure of links at different time s at t= (i) t1,  (ii) t2, (iii) t3, and (iv) t4 where t4>t3>t2>t1 are shown. The color  bar on the side represents the utilization intensity, red denoting  the highest utilized link and green denoting the least utilized  one. The highest utilized VL fails first and is represented by the  dotted black-color link. Once the most utilized link fails,  neighboring links start to fail at t = t2, t3 and so on. The  neighboring links were not heavily utilized prior to failure of  the first failed link. Once a link fails, the utilization of  neighboring links increases causing them to undergo heavy  wear-out and they start failing. This is called the ‘cascading  effects’ of link failure that degrades the NoC performance.  High utilization increases wear-out and lowers the lifetime  of VLs. Fig. 1(c) shows an example of normalized link  utilization and lifetime of the VLs for the CANNEAL  benchmark. The higher end of the utilization profile translates  to the lower end of the lifetime distribution, and vice versa.  Consequently, for 3D NoCs, attacking the TSV-enabled VLs  reduces the system lifetime significantly.  A. Model of a TSV-based VL   Figs. 2(a)-(b) show the schematic of a TSV-based VL. To  reduce the area overhead associated with the pitch and the  physical dimensions of TSVs, a serializer and a deserializer are  used. In our study, we employ a 4:1 serialization ratio as it  achieves the best data-rate per unit of energy consumed [30].   1) Workload Induced Electromigration Stress in TSVs  The workload-induced stress from the nonhomogeneous  TSV utilization (Fig. 1(c)) causes nonuniform electromigration  in the landing pads of the TSVs. This creates a void at the TSV  landing pad reducing the effective TSV cross-section area and  forcing the current to pass through the TSV barrier material  (e.g., TiN). The net outcome of the increased usage of VL is  that the resistance increases over time [31]-  𝑡 𝑅(𝑡) − 𝑅0 = 𝐴 𝑙𝑛 ( 𝑡0 )                       (1)  Here, t0 is the time when the size of the void at the landing  pad equals that of the TSV cross-section area. A is the aging  coefficient that depends on the TSV barrier material. The  parameter, R0, is the initial resistance of the TSV and t is the  active utilization time of TSVs.  2) Effect of Crosstalk on TSVs  The delay through a TSV-based VL is influenced by the  crosstalk capacitance. The effective value of the crosstalk in a  TSV bundle depends on three factors -- the configuration of the  TSVs in the bundle, the pitch of the TSV in the bundle, and the  bit patterns through the TSVs. The most popular way to bundle  TSVs is the grid-pattern, e.g. 3x3, 3x4, 4x4 and so on. (as  shown in Fig. 2(a)). The center TSV in such a bundle faces the  strongest amount of crosstalk (10x the crosstalk due to a single  transition between adjacent TSVs).  Consequently, the delay  through the center TSV is higher relative to the others.  Depending on the placement configuration of the TSVs, the  delay through the TSVs in the bundle varies by an order of  magnitude [6].  3) Model of TSV under Workload-induced Stress and Crosstalk  To study the effects of workload-induced stress and  crosstalk noise in the TSV-based VLs, we model the TSV as a  lumped RC π-network [32]. Fig. 2(b) shows the complete  model. The parameter, RTSV and CT are the TSV resistance, and  self-capacitance respectively. The parameter, Cc, models the  crosstalk capacitance of a TSV. The resistance corresponding  to the leakage current of the TSV is represented by Rs. The  values of these TSV parameters are calculated using the model  from [32] and [33]. The effects of workload-induced  electromigration of the TSV material is modeled as an  additional resistance, REM, in series with the TSV resistance.  REM is zero at t = 0, and depending on the workload  characteristics increases over time.   4) Lifetime of TSVs: Mean-time-to-failure (MTTF)  The combined effects of workload-induced stress and the  crosstalk increase the delay of the TSV. At one point, the delay  increases beyond the acceptable limit imposed by the timing  constraint of the SoC. This scenario is considered as a failure of  the TSV, and the corresponding time is termed as the meantime-to-failure (MTTF). In general, and in this work, a 10%  increase in delay is considered as the instant when the TSV fails  and reaches its MTTF [31], [34].  B. Effects of VL-failures on the 3D NoC  The effects of VL failure are three-fold. First, failure of VL  increases the diameter (average hop count per message) of the  network. Hence, the energy consumption in the NoC routers  and links increases. Second, the VL failure increases the  queuing latency at each node forcing each message to wait  longer. Third, due to the cascading effects, the utilization of the  neighboring VLs increase. A combination of all these factors  results reduces the lifetime of TSV-based VLs.  IV.  PLANNED OBSOLESCENCE IN 3D NOCS   In this section, we first discuss the unified metric to evaluate  the security and performance of the 3D SWNoC. Subsequent  sections elaborate different NoC attacks.               Fig. 3: NoC planned obsolescence framework to study the effects of malicious workloads on the aging of NoCs. The OEM who has intimate knowledge of the  SoC architecture including the NoCs and router configurations is the attacker. The OEM identifies critical links, regions and develops harmful workloads that  target the vulnerable links and regions of the NoC to accelerate the wear-out.   A. Security Metric: NoC Lifetime   In an NoC, the performance (latency, energy consumption,  energy-delay-product (EDP) per message), reliability, and  security are interrelated. The inter-node communication and  traffic injection rates vary and can be controlled to launch  different NoC attacks.   Without any VL failure, the 3D SWNoC, optimized for  achieving high performance, initially (at t = 0) shows lower  EDP values compared to a 3D MESH NoC [29]. However,  when the VLs fail, the network configuration changes and the  network  latency  and  energy  consumption  increase  progressively. At a certain instant, the EDP of 3D SWNoC  exceeds that of a fault-free 3D MESH. The 3D SWNoC  becomes less efficient than a fault-free 3D MESH (without any  link failure), and the corresponding time instance is the lifetime  of the 3D SWNoC. The failure of VLs reduces the lifetime of  3D NoC significantly. Formally the lifetime of 3D SWNoC is-  lifetime 3D SWNoC = {t: (EDP 3D SWNoC(at t=t) = EDP3D MESH(at t=0) )}     (2)  Lifetime of 3D SWNoC (or any other 3D NoC), indicates  the period of time up to which it operates with EDP less than  for a 3D MESH NoC. In this context, the EDP of 3D MESH is  considered a reference point for comparative study. However,  alternative definition of lifetime of 3D SWNoC (e.g. 10%  increase in EDP value), is applicable as well.  The lifetime of 3D NoC unifies performance (EDP) and  reliability of the NoC in a single metric. When the 3D SWNoC  reaches its lifetime, its performance is no longer as efficient as  the fault-free case. Consequently, the NoC lifetime measures  the severity of the external attack.   B. Threat Model: Planned Obsolescence by Aging NoCs  (POISON)  Fig. 3 depicts the considered POISON threat model. The  OEM can generate a malicious program that stresses the TSVs  and eventually deteriorates the SoC lifetime.  This malicious  program is delivered as software update. Once the program is  installed, the OEM has complete access to the IC. They can  apply these programs to accelerate the aging of the TSV-based  VLs and reduce the lifetime of the IC.  approaches to planned obsolescence is to introduce malicious  programs that target the NoC without affecting the cores. More  specifically, attacking the TSV-enabled VLs in an NoC  significantly degrades the system performance (latency, energy  and EDP) causing the chip to fail.   For planned obsolescence, the OEM is the attacker. The  OEM possesses an intimate knowledge and complete control  over the SoC configuration, NoC architecture, distribution and  placement of TSV-based VLs, router structure and routing  mechanism. In addition, the distribution of workloads among  the VLs in the NoC, dispersal of critical paths, locations of  heavily utilized links and regions are known as well. The OEM  develops malicious workloads by abusing this knowledge and  deploys them by exploiting its control over the manycore SoC.   C. NoC Attack Methodology  In this section, we introduce various security attacks and  highlight the characteristics of each of them. Each attack affects  the NoC in a different way. The aim of the OEM is to quicken  aging of the VLs to assure the deterioration of the manycore  chip. As explained in Section 3.1, a simple means to accelerate  the aging of the NoC is by marking some VLs and increase their  individual workloads. Three different attack workloads can  accelerate aging of the 3D NoC. These are- (i) uniform random  attack, (ii) attacking critical regions, and (iii) attacking the  critical VLs. In the subsequent sections, we outline the details  of the NoC attacks and the effects on the quality of service for  the end user.   1) Uniform Random Attack Workload  This is the baseline attack. All routers in the NoC and VLs  are subjected to a random increase in their message injection  rates. The rogue workload chooses source-destination pairs and  injects more messages between them randomly. Both the  horizontal and vertical links are used more and they wear out  faster. The resulting increase in the utilization profile for all the  VLs is uniform (if it takes place over a longer period of time).   The wear outs of the links are almost uniform regardless of their  impacts on the NoC performance. Hence, the degradation of  NoC lifetime and performance are not drastic.  2) Workloads that Attack on Critical Regions  Planned obsolescence accelerates aging of the chip leading  to an earlier-than-expected failure. One of the efficient  Instead of infecting all nodes, we target a specific region.  When a VL in the target region breaks down, the effect cascades              P D E d e z i l a m r o N 2.6 2.1 1.6 1.1 0.6 1 Uniform Critical Region Critical VL P D E d e z i l a m r o N Uniform Critical Region Critical VL 1.3 1.2 1.1 1 0.9 1.2 1.4 Normalized message injection rates 1.6 1.8 2 1 1.2 1.4 1.6 1.8 Normalized message injection rates  2 Fig. 4(a): Normalized EDP as a function of injection load for the three NoC  attacks for CANNEAL benchmark.  Fig. 4(b): Normalized EDP as a function of injection load for the three NoC  attacks for DEDUP benchmark.  (increased traffic through the VLs) onto the abutting VLs and  adjoining regions speeding up degradation of the SoC.  the ST 28-nm standard cell library to synthesize the driver  circuitry necessary for TSV-based VLs.   In the critical region of the NoC, the average traffic load is  higher than the rest of the NoC. In a 4-layer 3D NoC, this region  is between the two intermediate layers. VLs carry higher  amount of traffic, are utilized more, and wear out faster. The  resultant MTTFs of the VLs in this region are lower than from  other regions. As illustrated in Fig. 1(a), the lifetimes of the VLs  in the critical region (between layers 2 and 3) are less than the  other regions. Most of the VLs in the critical region correspond  to the left-bottom corner (with lower MTTFs) of the MTTF  distribution in Fig. 1(c).   3) Workloads that Target Critical Vertical Links   In the targeted VL attack, only a selected number of links in  the NoC are targeted and attacked. These critical VLs have their  workloads comparatively higher than the rest of the VLs and  the corresponding source-destination pairs connected via these  links are spread all over the NoC. Depending on the benchmark  characteristics, the critical VLs are spread over a wider region  than the critical region. As shown in Fig. 3, the OEM/attacker  exploits his/her access to the NoC the most. As we will see in  Section V, the attacker needs to inject a smaller number of  additional messages  to achieve similar NoC  lifetime  degradation than the uniform random and critical region  attacks. Referring to Fig. 1(c) for the CANNEAL benchmark,  critical VLs are located in the right- and left-bottom corners of  the utilization and MTTF graphs, respectively.  Concentrating on the critical VLs and abusing them  expedites their aging. The targeting all critical VLs has a similar  impact as that of the uniform random attack. Hence, we need to  investigate the trade-offs between the number of critical VLs  targeted and the overall lifetime and performance degradation  of the 3D NoC.   V.  RESULTS AND ANALYSIS  In this section, we describe the experimental setup and  evaluate the performance and vulnerability of 3D NoCs to the  various planned obsolescence-based attack scenarios.  A. Experimental Setup  We examine a 4-layer Chip Multiprocessor (CMP)  comprising of 64 cores and 64 routers subdivided into four  layers. In each layer, we distribute 16 cores (and associated  routers) in a grid pattern. TSVs are used as the vertical  interconnects. We used the Cadence Virtuoso environment and  To evaluate the performance of the 3D SWNoC, we use a  cycle-accurate simulator that can simulate arbitrary 3D  architectures. The NoC simulator uses wormhole routing,  where the data flits follow the header flits once the routers  establish a path. For the 3D SWNoC, the topology-agnostic  Adaptive Layered Shortest Path Routing (ALASH) algorithm  is used [35]. The energy consumption of the network routers  was obtained from  the synthesized netlist by running  Synopsys™ Prime Power, while the energy dissipated by the  wireline links was obtained using HSPICE simulations.   We consider CANNEAL, DEDUP and VIPS benchmarks  from the PARSEC [36], and FFT and WATER from the  SPLASH-2 suite [37]. These benchmarks vary in their  characteristics  ranging  from  computation-intensive  to  communication-intensive.   As mentioned in Section 3, we consider a small-world  network-enabled 3D NoC (3D SWNoC) as it outperforms other  regular and  irregular 3D NoCs e.g. 3D MESH from  performance and reliability angles [29]. While the evaluations  presented in this paper are only for 3D SWNoC, this analysis  can be carried out on other 3D NoCs as well.   B. Performance Degradation Under NoC Attacks  In this section, we analyze the effects of security attacks on  the performance of the NoC. We examine the energy-delayproduct (EDP) as the relevant performance metric. The EDP is  the product of the network latency and energy per message. To  measure the harshness of the attack, we look at the deterioration  of EDP when the malicious workload is injected at t=0 and  afterwards.   1) EDP vs. Injection Loads  To analyze the effects of the three different malicious  schemes, we consider the total number of messages injected  into the network and the NoC performance. Figs. 4(a) and (b)  show the normalized EDP profile at t=0 by varying the injection  load for CANNEAL and DEDUP benchmarks respectively. We  have chosen these benchmarks as they have wide variation in  their respective  traffic  injection rates from very high  (CANNEAL)  to  low (DEDUP). The EDP values are  normalized with respect to the baseline (without any attack) 3D  SWNoC with the nominal injection rate for each benchmark  (injection rate =1.0). In the considered NoC attack scenarios,  the malicious program increases the injection rates of the                  benchmark. The NoC performance for the three attacks e.g.  uniform random, the critical regions and the critical vertical  links are marked as Uniform, Critical Region and Critical VL  respectively.  From the figures (Figs. 5(a)-(b)), as the rate of malicious  message injection increases, the EDP of the NoC for the three  attack scenarios is affected differently. For Critical VL, the  EDP increases significantly. For the Uniform and Critical  Region attacks, the rates of increase are slower and depend on  the benchmark. For CANNEAL, Critical Region attack  degrades the performance more than the Uniform attack, while  for DEDUP opposite trend is observed due to its lower injection  load compared to CANNEAL.   The Critical Region attack is localized in a particular region.  Also, the nodes within the critical region are a few hops away  from each other. Hence, the EDP degradation is on average  lower than that of the Critical VL. However, depending on the  benchmark, the EDP degradation varies compared to the  Uniform attack. For the Critical VL attack, additional messages  are injected between source-destination pairs that use critical  VLs as intermediate paths. For this reason, the hop count is  higher than for any other NoC attacks. Expectedly, Critical VL  attack results in the maximum performance degradation.   2) EDP Profile vs Time  To analyze how the performance of 3D SWNoC degrades  over time under the three types of NoC attack, Figs. 5(a) and  (b) show the temporal variation of normalized EDP profile for  CANNEAL and DEDUP respectively for an injection load  higher than the nominal. As an example, we consider 10%  additional load as a case study. The 3D SWNoC lifetime line  (with EDP of 3D MESH at t = 0) is also shown in the figure.  The horizontal axis represents 3D SWNoC lifetime normalized  with respect to a 3D SWNoC without any attack. The EDP  increases with time as the number of failed VLs increases as a  result of attacks. In addition, attacking the Critical VL shows  the highest EDP degradation among all of them at any instant  of time. This is because when the Critical VLs are attacked, the  average hop count for each message increases as the sourcedestination pairs are spread all over the chip. Attacking the  Critical Region, initially (at t = 0) has lower EDP than the  Uniform attack as the network traffic is constrained to a smaller  region. However, the VLs in Critical Region attack fail earlier  than Uniform and hence, the EDP of Critical Region starts  degrading (increasing) earlier and increases more rapidly over  time compared to the Uniform attack.   C. Aging in TSV-based Vertical Links  To characterize the aging effects of the VLs under different  NoC attacks, we consider the MTTF distribution of all VLs.    The MTTF of any TSV is directly related to the utilization  profile of a VL. Higher utilization increases electromigration in  the TSV material and accelerates wear out of the VLs, and vice  versa. From our experiments, it is seen when the Critical VLs  are attacked, the TSV utilization increases more than the others.  This is because, for this type of attack, very specific VLs are  targeted and the corresponding source-destination pairs are  spread out all over the NoC. However, when attacking the  Critical Region, only a particular area is targeted, where the  source-destination pairs are constrained within a specific  region. The utilizations of the links in this particular area  increase whereas the links located outside this region are not  stressed. Consequently, the increase in VL utilization for  Critical Region is less than that of the Critical VL. For Uniform  attack, the usage of TSVs increases randomly and hence  average increase in utilization is the lowest.   Fig. 5(b): Temporal profile of the normalized EDP for the three NoC attacks  for DEDUP benchmark.  Fig. 5(a): Temporal profile of the normalized EDP for the three NoC  attacks for CANNEAL benchmark.  0.8 1 1.2 1.4 1.6 0 0.2 0.4 0.6 0.8 Time in normalized 3D SWNoC lifetime 1 N o r m a i l z E d e D P Uniform Critical Region Crtical VL EDP of 3D MESH at t=0 0.8 1 1.2 1.4 1.6 0 0.2 0.4 0.6 0.8 Time in normalized 3D SWNoC lifetime 1 N o r m a i l z E d e D P Uniform Critical Region Critical VL EDP of 3D MESH at t=0 Fig. 6(b): MTTF distribution for vertical links for the three NoC attacks for  DEDUP benchmark.  Fig. 6(a): MTTF distribution for vertical links for the three NoC attacks for  CANNEAL benchmark.  0 0.5 1 1.5 2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Index of critical VLs N o r m a i l z d e M F T T s Nominal Uniform Critical Region Critical VL 0 1 2 3 4 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Index of critical VLs N o r m a i l z d e M F T T s Nominal Uniform Critical Region Critical VL                           To perform the reliability analysis of the VLs under  different NoC attacks, Figs. 6(a) and (b) show the MTTF  distribution of the critical VLs for CANNEAL and DEDUP  benchmarks respectively. As a case study, we fixed the  additional injection load to 10%. To analyze the MTTF  distribution, we selected the maximum number of VLs that,  when failed causes the 3D SWNoC to reach its lifetime (the  EDP of the 3D SWNoC matches that of fault-free 3D MESH).   All MTTFs are normalized with respect to the lowest MTTF  value for the respective benchmark using nominal traffic load.  From Figs. 6(a)-(b), we see that the for most of the cases,  the MTTF values of the considered VLs for the Critical VL  attack are significantly lower than the Nominal and the other  two attacks. As the link utilization (for TSVs) increases  significantly when attacking the Critical VL, it noticeably  reduces the MTTF of VLs. On average, the MTTF is 26% lower  for the critical VLs compared to the Nominal. If the injection  loads are increased further, the reduction will be more  pronounced. The reduction in MTTFs of the considered links  for the Critical VL attack is followed by attacks on the Critical  Region and the Uniform. When attacking the Critical Region,  some VLs have higher MTTFs than Uniform attack, because of  the fact that some critical VLs are located outside the targeted  critical region and are not stressed during the attack. Hence,  they wear out slowly relative to the Uniform random attack.    D. Lifetime of NoC  The lifetime of an NoC depends on its EDP and the MTTF  distribution of the critical VLs. To evaluate the security profile  under different NoC attacks, Figs. 7(a) and (b) show the  normalized NoC  lifetime for CANNEAL and DEDUP  benchmarks respectively. The lifetimes are normalized with  respect to the NoC lifetime of the benchmarks with nominal  traffic injection (the message injection load of 1 as shown in the  horizontal axis of Fig. 7).  As the injection load increases, the lifetime of the NoC  decreases progressively for all types of malicious workloads. In  addition, the malicious workload that targets the Critical VLs  yields the lowest lifetime among all rogue workloads for any  injection load. Alternatively, the Critical VL requires the least  number of additional messages to be injected to yield similar  NoC lifetime degradation among all types of attacks. This  directly results from Fig. 5 and Fig. 6. For workloads that attack  the Critical VLs the EDP degradation is higher than others (Fig.  5) and concurrently, the MTTF of the critical VLs are lower  (Fig. 6). Consequently, the EDP of the 3D SWNoC quickly  increases to the reference value (EDP of 3D MESH NoC  without any VL failure at t = 0) and reaches its lifetime. Hence,  attacking the Critical VLs yields the lowest NoC lifetime,  followed by attacks on the Critical Region and Uniform attack.   E. Effect of Workload Characteristics   We discuss the effects of the benchmark characteristics on  the NoC  lifetime. To analyze the effects of different  benchmarks, Fig. 8 shows the normalized NoC lifetime and  their respective nominal traffic injection loads. We normalized  the injection loads and NoC lifetime with respect to CANNEAL  as it has the maximum traffic injection load in the nominal case  (without additional injections).   We consider five benchmarks e.g. CANNEAL, FFT,  DEDUP, VIPS, FUILDANIMATE (FLUID). Expectedly,  CANNEAL shows the minimum NoC lifetime while VIPS,  having the lowest traffic injection rate among the benchmarks  considered, has the maximum NoC lifetime. For high injection  load benchmarks, any malicious attack that increases the  workload of the VLs induces significant wear out, degrading  the performance of the NoC, reducing the NoC lifetime. From  the NoC attack perspective, to saturate the NoC lifetime as seen  in Figs. 7(a) and (b), higher traffic injection is required for  benchmarks with low nominal load (DEDUP) and vice versa.   VI.  CONCLUSION  We have analyzed the planned obsolescence in 3D NoCs by  accelerating the aging of TSV-enabled vertical links. By  exploiting the physical vulnerabilities of TSVs due to  workload-induced stress and crosstalk noise, the vertical links  of 3D NoCs are targeted using craftily constructed deleterious  workloads. Three types of attacks have been studied; these  attacks target critical nodes, regions, and vertical links of the  NoC. We found that malicious programs that target the heavily  utilized critical vertical links degrade the performance the most.  Stressing targeted vertical links with high message injection  Fig. 8: Effects of benchmark characteristics and injection loads on the NoC  lifetime.  0 4 8 12 16 0 0.25 0.5 0.75 1 1.25 CANNEAL FFT DEDUP VIPS FLUID N o r m a i l z d e L i f e t i m e N o r m a i l z d e r t a f f i c i e n j c t i n o r a t e s Lifetime Injection rate Fig. 7(b): Lifetime of 3D SWNoC for the three NoC attacks for DEDUP  benchmark.  Fig. 7(a): Lifetime of 3D SWNoC for the three NoC attacks for CANNEAL  benchmark.  0 0.2 0.4 0.6 0.8 1 1.2 1 1.2 1.4 1.6 1.8 Normalized message injection rates  2 N o r m a i l z d e N C o L i f e t i m e Uniform Critical Region Critical VL 0 0.2 0.4 0.6 0.8 1 1.2 1 1.1 1.2 1.3 1.4 Normalized message injection rates 1.5 1.6 N o r m a i l z d e N C o L i f e t i m e Uniform Critical Region Critical VL                               rate shortens their expected lifetime and introduces significant  performance penalty and deteriorates the reliability. The  analysis demonstrates that the most effective malicious  programs reduce the NoC lifetime on average by 11%-26% by  injecting about 3%-10% additional traffic for the SPLASH-2  and PARSEC benchmarks.  These results will motivate further  research on planned obsolescence attacks and mitigation  techniques for manycore SoCs that use a 3D NoC as the  communication fabric. Future work also includes studying  automated techniques to detect and thwart obsolescence attacks  by analyzing the communication patterns in NoC over time and  suitable mitigation strategies for those attacks.   VII.  "
FreewayNoC - A DDR NoC with Pipeline Bypassing.,"This paper introduces FreewayNoC, a Network-on-chip that routes packets at Dual Data Rate (DDR) and allows pipeline bypassing. Based on the observation that routers datapath is faster than control, a recent NoC design allowed flits to be routed at DDR improving throughput to rates defined solely by the switch and link traversal, rather than by the control. However, such a DDR NoC suffers from high packet latency as flits require multiple cycles per hop. A common way to reduce latency at low traffic load is pipeline bypassing, then, flits that find a contention-free way to the output port can directly traverse the switch. Existing Single Data Rate (SDR) NoC routers support it, but applying pipeline bypassing to a DDR router is more challenging. It would need additional bypassing logic which would add to the cycle time compromising the DDR NoC throughput advantage. FreewayNoC design restricts pipeline bypassing on a DDR router to only flits that go straight simplifying its logic. Thereby, it offers low packet latency without affecting DDR router cycle time and throughput. Then, at low traffic loads, besides the few turns that a flit would take on its way from source to destination, all other hops could potentially offer minimum latency equal to the delay of the switch and link traversal. Post place and route results in 28 nm technology confirm the above and also show that zero-load latency scales to the hop count better than current state-of-the-art NoCs.","FreewayNoC: a DDR NoC with Pipeline Bypassing Ahsen Ejaz Vassilios Papaefstathiou Ioannis Sourdis Chalmers Univ. of Technology Gothenburg, Sweden ahsen@chalmers.se Foundation for Research & Technology – Hellas (FORTH) Heraklion, Crete, Greece papaef@ics.forth.gr Chalmers Univ. of Technology Gothenburg, Sweden sourdis@chalmers.se Abstract—This paper introduces FreewayNoC, a Networkon-chip that routes packets at Dual Data Rate (DDR) and allows pipeline bypassing. Based on the observation that routers datapath is faster than control, a recent NoC design allowed ﬂits to be routed at DDR improving throughput to rates deﬁned solely by the switch and link traversal, rather than by the control. However, such a DDR NoC suffers from high packet latency as ﬂits require multiple cycles per hop. A common way to reduce latency at low trafﬁc load is pipeline bypassing, then, ﬂits that ﬁnd a contention-free way to the output port can directly traverse the switch. Existing Single Data Rate (SDR) NoC routers support it, but applying pipeline bypassing to a DDR router is more challenging. It would need additional bypassing logic which would add to the cycle time compromising the DDR NoC throughput advantage. FreewayNoC design restricts pipeline bypassing on a DDR router to only ﬂits that go straight simplifying its logic. Thereby, it offers low packet latency without affecting DDR router cycle time and throughput. Then, at low trafﬁc loads, besides the few turns that a ﬂit would take on its way from source to destination, all other hops could potentially offer minimum latency equal to the delay of the switch and link traversal. Post place and route results in 28 nm technology conﬁrm the above and also show that zero-load latency scales to the hop count better than current state-of-the-art NoCs. I . IN TRODUC T ION As technology scaling allows the number of cores on a chip to keep increasing, on-chip interconnects become a more critical system component. Network on chip (NoC) latency and throughput affect signiﬁcantly systems performance [1]–[3]. On one hand, applications that exhibit small transfers at low loads are often more sensitive to network latency [4]. On the other hand, systems that run concurrent scale-out applications are more demanding, push the network close to its saturation point, and are more sensitive to network throughput [4], [5]. Many existing techniques reduce packet latency by modifying the network topology [6] or the routing algorithm [7]. At the router architecture, an effective approach is to allow incoming ﬂits that ﬁnd no contention to bypass router’s stages and proceed directly through switch traversal to the output port [2], [8]. Pipeline bypassing can signiﬁcantly reduce network latency at low trafﬁc loads avoiding unnecessary router stages. Network throughput can be improved employing techniques at different layers of the network: using new allocation techniques [9], wider datapaths, richer network topologies [10], or multiple subnetworks [11], [12] to name a few. More recently, a Dual Data Rate (DDR) NoC router architecture was proposed This work was supported by the European Commission under the Horizon 2020 Program through the ECOSCALE project (grant agreement 671632). that increases network throughput [13]. DDRNoC, exploits the fact that the datapath of a router is faster than its control and allows two ﬂits to share the same path within a cycle using both clock edges. In essence, routers control uses an entire cycle to make decisions for ﬂits that are routed in two clock phases. In doing so, DDRNoC achieves to route packets at a rate determined solely by the delay of the switch- and link-traversal (ST, LT) rather than by the control improving throughput. Compared to Single Data Rate (SDR) NoCs, DDRNoC supports 25-45% higher throughput, but suffers higher latency as ﬂits require two full cycles per hop. In this work, we address the above drawback proposing FreewayNoC, a new DDR router architecture that offers pipeline bypassing, providing low packet latency. The main challenge is that pipeline bypassing adds complexity to the datapath of the router and this in turn affects DDRNoC throughput. More precisely, an incoming ﬂit, candidate for pipeline bypassing, would need to compete with other concurrently arriving ﬂits, so checking whether the path is free adds logic to the switch traversal (ST). On the other hand, the main DDRNoC performance advantage lies on the fact that its critical path which determines the rate at which packets are routed (half a cycle), is deﬁned by the ST (or LT) without any additional control delay. FreewayNoC attempts to preserve the throughput of DDR routers by restricting pipeline bypassing options to only ﬂits that go straight. Then, an incoming ﬂit is not competing for pipeline bypassing with other ﬂits that arrive concurrently and whether a path is free and available for bypassing is known a cycle earlier. Allowing bypassing only to ﬂits that go straight in a network router restricts the beneﬁts of the technique, however for most routing algorithms1 the number of turns a ﬂit takes during its journey from source to destination are only two or three. Then, for larger average number of hops and as network sizes increase the overhead of slower turns becomes a smaller fraction of the total packet latency. Concisely, the contributions of this paper are the following: • a new NoC architecture that routes ﬂits at DDR and offers simpliﬁed pipeline bypassing achieving performance deﬁned solely by its datapath delays, more precisely: – for all, but the turning hops, it has a latency per hop at best equal to the datapath delay (ST+LT), and – its throughput is based on the routing rate deﬁned by the longest datapath stage (ST or LT). 1 Pipeline bypassing is effective at low trafﬁc loads, where simple and deterministic routing algorithms are sufﬁcient. 978-1-5386-4893-3/18/$31.00 ©2018 IEEE   • a post P&R evaluation that conﬁrms the above and a comparison with previous NoCs showing that FreewayNoC: – matches DDR NoCs throughput and substantially improves packet latency; – substantially improves SDR NoCs throughput and provides a packet latency that scales better to the hop count, hence being lower in larger networks. The rest of this paper is organized as follows. Section II discusses related work in more detail. Section III describes the FreewayNoC architecture. Section IV details the experimental setup and presents evaluation results. Finally, Section V summarizes our conclusions. I I . R E LATED WORK An overview of router architecture techniques for reducing packet latency is presented ﬁrst. Then, high throughput Dual Data Rate (DDR) router designs are described focusing on the recent DDRNoC which is used as the baseline of this work. Michelogiannakis et al. showed how ideal latency could be approached on a Network on-chip [14]. Their design achieves ultra low latency for ﬂits that follow preferred, pre-conﬁgured paths, but has to tolerate misrouted, eagerly forwarded, dead ﬂits. A more conservative approach suggested speculative switch allocation (SA) performed in parallel to virtual channel allocation (VA), saving a cycle in the router pipeline [15]. Later, researchers used lookahead signaling in NoCs, inspired by off-chip network designs such as Spider [16]; there control information about a ﬂit could be generated early and travel forward one cycle ahead of the ﬂit itself on a separate, narrow link. Then, the control (i.e., allocation) in the downstream router could consider the ﬂit a cycle earlier [2], [17]. Finally, NoC routers were proposed that allowed some of their stages to be bypassed in the absence of contention reducing the number of cycles spent in the router [18]. A Single Data Rate (SDR) NoC combining several of the above techniques (speculative allocation, lookahead signaling, pipeline bypassing), among other features, is Scorpio [2]. Implemented in a 36-core chip, Scorpio demonstrates low latency and short cycle time. More recently, ShortPath, a NoC router that employs a bypassing mechanism without relying on speculative lookahead signals was proposed demonstrating lower latency than Scorpio [8]. Therefore, ShortPath is considered the current state-of-the-art SDR NoC and is used in this paper for performance comparison. Several approaches employed DDR paths to increase throughput. Hoskote et al. used a single DDR crossbar to support two network lanes [19]. Rapidlink time-shared links at DDR between 2-VC routers of two subnetworks [12] to increase throughput compared to a network composed of 4VC routers. Finally, DDRNoC proposed a router which allows its datapath parts to be shared between two ﬂits every cycle each using a different clock phase [13]. DDRNoC exploits the slack the datapath of a router has compared to its control latency; for instance, ShortPath, the fastest SDR NoC, allows its control to add about 25% latency to the datapath delay (ST) [13]. DDRNoC modiﬁes its control to handle each Fig. 1: The FreewayNoC router architecture. cycle ﬂits for two timeslots (in the two halfs of the cycle). In doing so, DDRNoC removes the control delay from the critical path. Then, routers datapath (ST or LT) is the one that determines the rate at which ﬂits are routed (one per half a cycle). DDRNoC demonstrated that it is able to fully capitalize the gap between control and datapath increasing equally its network throughput. For example, compared to ShortPath, DDRNoC exhibits about 25% higher throughput [13]. Despite the improved network throughput, DDRNoC has high packet latency. In order to keep ﬂits aligned, a DDRNoC ﬂit spends an entire cycle in ST and another in LT, although it actually needs only half of the cycle for each. That sums up to two complete cycles per hop. Another cycle is spent for DDRNoC control (allocation), which is however hidden using lookahead signaling (called control forwarding). In practice, DDRNoC control is forwarded a cycle earlier to the downstream router to trigger allocation before the actual ﬂit arrives. In summary, DDRNoC supports about 25% higher throughput than ShortPath but has about 50% higher packet latency [13]. It requires two cycles per hop, each cycle being two times longer than the ST/LT delay. This is about two times longer than the ideal latency of a hop, assuming the delay of ST and LT (using registered control bits) cannot be avoided. FreewayNoC addresses this DDRNoC drawback. It attempts to improve DDR routers latency using pipeline bypassing while maintaining their higher throughput over SDR routers. I I I . TH E FR EEWAYNOC ROUT ER ARCH I T EC TUR E The FreewayNoC is an on-chip interconnect composed of routers that have a double-pumped datapath and offer pipeline bypassing. As opposed to the previously proposed DDRNoC, FreewayNoC allows incoming ﬂits to directly traverse the switch and link without undergoing an allocation step when their way to the output port is free. FreewayNoC exploits a control forwarding (lookahead signaling) mechanism, similar to the one described in the DDRNoC [13], which allows the control of ﬂits to precede the ﬂit data, enabling allocation a cycle earlier; in our design control forwarding is also used for pipeline bypassing (PB) checks as well as for next-route computation (NRC). FreewayNoC uses a virtual channel selection (VS) mechanism at the output ports rather than VC allocation to better accommodate PB. It further restricts PB to ﬂits that go straight minimizing the complexity added to generating the forwarded control signals. FreewayNoC achieves to not add any complexity to routers datapath preserving the DDR packet rate, while reducing packet latency. Without loss of generality, our FreewayNoC design considers a 2D-mesh network, with look-ahead XY-routing, composed of routers with virtualchannels and credit-based ﬂow control. The top-level view of the FreewayNoC router is shown in Figure 1. The datapath is composed of two stages: Switch Traversal (ST) and Link Traversal (LT), separated by the input VC buffers and the two output registers (one for each clock edge multiplexed before the link). Each stage is able to handle two ﬂits per cycle, one at the high and one at the low phase of the clock. In addition, the FreewayNoC router has the following control blocks: a combined allocator composed of the Virtual Channel Select (VS) and a Switch Allocation (SA), Next-Route-Computation (NRC) and Pipeline Bypassing check logic at the input and output ports. A. Router Datapath The datapath of a FreewayNoC router, illustrated in Figure 1, can support two ﬂits per cycle, one at each phase of the clock. This requires the multiplexers along the datapath to be able to change their select twice within the cycle and the buffers to register ﬂits at either edge of the clock, as also described in the DDRNoC router [13]. It is the control and the timing of the ﬂits that differs in the FreewayNoC router, both when ﬂits are routed regularly as well as in PB. An input port can receive up to two ﬂits, one at each clock phase. Then, each register in an input VC buffer can be written selectively either at the positive or at the negative edge of the clock to allow storing up-to two ﬂits in the buffer per cycle, as described in [13]. The multiplexer of a VC buffer (not shown in Figure 1) is also able to read up to two consecutive ﬂits per cycle when the DDR VC en signal is on, which enables the select of the buffer multiplexer to change at each clock phase. This way a single packet can be forwarded at DDR. At each clock phase, the subsequent input and output multiplexers can also choose a different input VC and input port, respectively, operating at DDR and implementing the two SA decisions for the cycle. This is enabled by DDR IN en and DDR OUT en signals which allow the selects of the input and output multiplexers to change twice in the cycle. After the output multiplexer, ﬂits are registered in one of the two output registers (Reg+ and Reg−), one using the positive edge of the clock and the other the negative. Then, the contents of these registers are multiplexed to the link and sent to the downstream router. Sending two ﬂits to the link in DDR mode is enabled by the DDR LINK en signal otherwise only one register is selected for the cycle. Note, that the two output registers and their subsequent multiplexer can be replaced by a dual edge triggered ﬂip-ﬂop potentially reducing delay, but, such ﬂip-ﬂop was not available in our implementation library. In summary, FreewayNoC datapath supports the following DDR routing of ﬂits: (i) DDR per packet, sending up to two ﬂits of the same input VC, (ii) DDR sharing of input port, selecting two ﬂits of different input VCs, (iii) DDR sharing of output port, selecting two ﬂits from different input ports, (iv) DDR at the link, when two ﬂits make it to the output registers they are sent to the downstream router at DDR. In case a datapath part operated at SDR, the select of the respective multiplexer2 will remain unchanged through the entire cycle. Control information for each of the two incoming ﬂits is forwarded to the downstream router one cycle ahead. This mechanism is termed as control forwarding. The Forwarded Control (FC) uses a separate path (crossbar and link wires) to perform control switch traversal (CST) and control link traversal (CLT) in a single cycle as depicted in the bottom part of Figure 1. The FC bits carry the type of the ﬂit (head, body or tail), the id of the assigned downstream VC, the next route of the ﬂits and the destination address. Moreover, there is a separate path for FC signals, which bypasses the regular CST connecting each input port with its straight output port (N to S, E to W and vice versa), except for the local port. This path is used to carry the FC signals of a bypassing ﬂit to the output port through the Bypass FC mux and send them for CLT instead of the regular CST signals. Two sets of FC signals exist, one for the ﬂit propagating in the high phase of the next cycle (FC+) and one for the ﬂit propagating in the low phase (FC-), each set of FC signals uses a separate CST, CLT and bypass path. The pipeline bypassing logic checks the respective input and output port availability, as well as for available VC and credits, to determine whether bypassing is possible. This is performed in parallel to the regular CST shown in Figure 1. Then, depending on the PB check, a 2:1 multiplexer is used to send the bypassed FC to CLT instead of the regular FC. This 2:1 multiplexer is the only addition to the FC path delay required for PB. Still, CST and CLT are not in the critical path because, as opposed to data ST and LT, they do not need a register between them and the FC path is substantially narrower than the data-path requiring a smaller crossbar. B. Timing The timing of a FreewayNoC router is described using an example of a ﬁve-ﬂit packet traversing three routers, illustrated in Figure 2. In the source and destination router, the packet ﬂits take a turn as they move from and to a local port, respectively. In our example, we consider that all routers are aligned and therefore ﬂits passing through the second router go straight and have the opportunity for pipeline bypassing. Note that the 2 There are four types of multiplexers along the FreewayNoC datapath, each one of which may allow two ﬂits per cycle to pass, one at each clock phase: the multiplexer of a VC buffer (not shown in Figure 1), the multiplexer for the input arbitration, the one for the output arbitration, and the multiplexer that selects one of the two output registers before the link. LT and ST for ﬂits that traverse the link in the high clock phase are marked in the example with “+”, and for ﬂits that traverse the link in the low clock phase are marked with “-”. In cycle zero, router-1 receives the forwarded control (FC) of the two ﬁrst ﬂits (H, B1) which carries ﬂit types, assigned VC, next route and destination. The packet comes from the local port, so it takes a turn and therefore there is no option for bypassing. Then, ﬂits enter the allocation in cycle 1 using the next route information. In the same cycle. the head ﬂit enables the NRC computation as well as the VC selection, which provides an output VC. In parallel, the data ﬂits of H and B1 arrive and get stored in the input VC buffer in the ﬁrst and second half of cycle 1, respectively. Considering that there are no other competing packets, both H and B1 ﬂits get a grant for the switch at the end of cycle 1. Then, during cycle 2 the control information of the two ﬂits can be forwarded to router 2 (CST & CLT). In the second half of cycle 2 the H ﬂit traverses the switch of router-1 (ST+) and is registered in Reg+ before it performs LT in the ﬁrst half of cycle 3 (LT+). B1 ﬂit follows half a cycle later. It performs ST and LT in the ﬁrst and second half of cycle 3, respectively (ST-, LT-). The remaining ﬂits are routed through router-1 in a similar way. Following again H and B1 ﬂits, their control information arrives in router-2 at the end of cycle 2, enabling PB or allocation in cycle 3. PB check is positive, therefore the ﬂits are not considered for allocation. As the ﬁrst ﬂit is a header, NRC and VC select (VS) are performed, too. NRC, VS, and PB check are performed in parallel to form the control information of the bypassing ﬂits which will bypass the regular CST and traverse the link (CLT) to reach router-3 at the end of cycle 3. Bypassing ﬂit H arrives in router-2 after the ﬁrst half of cycle 3 and subsequently goes through ST and LT in the second half of cycle 3 and the ﬁrst half of cycle 4. Bypassing ﬂit B1 follows half a cycle later. Flits B2 and B3 pass through router-2 similarly with bypassing, however the tail ﬂit ﬁnds the requested output busy and therefore has to go through allocation spending an additional cycle in router-2. The control information for H and B1 arrive in router-3 at the end of cycle 3, enabling allocation in cycle 4 as they go to a local port and PB is not an option. Then, in cycle 5 their control is forwarded to the local port and in cycle 6 they traverse the local link. A cycle later B2 and B3 do the same. Finally, the tail ﬂit arrives with an extra cycle delay due to unsuccessful bypassing in router-2. C. Zero-Load Latency Analysis The Zero-Load Latency (ZLL) of a packet in FreewayNoC is as follows: one cycle for the SA of the ﬁrst hop as there is no control forwarding yet to cover it; all hops spend at least a cycle; turning hops require an additional cycle as PB is not allowed; the serialization latency is half a cycle per ﬂit for all but the ﬁrst 2 ﬂits that arrive on the ﬁrst cycle: Z LLFreewayNoC = 1 + hops + hopsturn + (N − 2)/2 cycles The zero load latency of a packet in the DDRNoC [13] is: Z LLDDRNoC = 1 + 2 ∗ hops + (N − 2)/2 cycles Fig. 2: Timing diagram showing the ﬂow of a ﬁve ﬂit packet through three router of the FreewayNoC. Fig. 3: ZLL analysis of FreewayNoC, DDRNoC and ShortPath with respect to the number of hops versus an ideal network that has a delay of ST+LT per hop. Moreover, the ShortPath router which uses dynamic pipeline bypassing to reduce pipeline stages to 2 (VA\SA\ST in one cycle and LT in the other) has a zero load latency of: Z LLShortPath = 2 ∗ hops + N − 1 cycles The ZLL of an ideal network with two stages per hop (ST and LT) can also be derived from the Shortpath ZLL equation, although its cycle time would be shorter. As shown in Section IV, the FreewayNoC cycle time is equal to the DDRNoC cycle and about 60% longer than the ShortPath cycle. The cycle time of an ideal network would be equal to its longest stage; considering ST and LT are balanced then its cycle would be 50% of the FreewayNoC cycle. Taking into account these cycle times and the above ZLL equations we perform a sensitivity analysis of the ZLL with respect to the number of hops a packet traverses depicted in Figure 3. We assume a 3-ﬂit packet and that the packet besides the two local port turns also takes a third one (X to Y or vice versa) if the hop count allows. The DDRNoC ZLL scales worse than the other two networks and is up to 50% higher for large number of hops. For low hop counts, ShortPath is better than FreewayNoC, i.e., 50% better for 2-3 hops, but above 14 hops FreewayNoC exhibits lower packet latency, i.e., 5.4% and 10.5% for 20 and 32 hops, respectively. It is worth noting that FreewayNoC scales to the number of hops better than ShortPath. Actually, FreewayNoC scalability is equal to the ideal network. It can be observed that FreewayNoC ZLL has a constant offset of 3 to 4 ST+LT delays, depending on the number of turns, with respect to the minimum latency of the ideal network. This enables FreewayNoC to approach a ZZL that is only 10% higher than the ideal network for 32 hops. one presented in [13]. The SA can grant the two phases of the next cycle to ﬂits of different or same packets. 2) Pipeline Bypassing: The FreewayNoC reduces end-toend packet latency by allowing packets to skip the SA stage under low load. At the same time it also aims for the critical path of the router to be deﬁned only by the datapath and be independent of the control logic. FreewayNoC achieves this by simplifying the bypass mechanism, by providing bypass paths for the FC signals in the router as well as by operating all control components of the bypass logic in parallel. Having full pipeline bypassing (PB) from one input port to any other output port requires multiplexing of output port status bits (busy, credit and available VC counters) to verify whether the incoming ﬂits can safely bypass SA in the current cycle. These checks introduce control logic in the datapath which deﬁnes the clock period and goes against one of the design goals of the FreewayNoC. Moreover, arbitration or kill logic would also be needed to either resolve contention or disable multiple ﬂits attempting pipeline bypassing in the same clock cycle. To avoid the latter problems, FreewayNoC supports pipeline bypassing only for ﬂits going straight through the router, i.e., move along one direction. Then, the output port to which each input port allows pipeline bypassing (in practice SA bypassing) is already known. This means that registered status bits at the output (and input) port can be directly used to decide if pipeline bypassing can occur. Finally, contention resolution is not required at the output port as it only allows bypassing from one input port. After receiving the FC bits of incoming ﬂits, the input port determines the eligibility of an incoming ﬂit to bypass by checking whether: (a) The ﬂit is going straight (i.e. not turning); (b) The input VC of the ﬂit is empty; (c) For incoming f lit+ (f lit− ), the ST + (ST − ) timeslot for the input multiplexer of the crossbar is not allocated to any input VC; (d) For a head ﬂit the output port has available VCs (VC Selector signal: Available VC Count (cid:54)= 0); (e) For a non-head ﬂit, the allocated output VC has credits. In case these input PB checks are successful, the new FC of the bypassing ﬂit(s) is updated with the following, before it is forwarded to the Bypass FC mux at the output port: (i) the NRC result computed using the destination included in the incoming FC, (ii) an output VC-id provided by the VS. Although, NRC, VS, and input PB checks are performed in parallel based on registered information, they would add delay to the FC path (CST & CLT) and would make it slower. FreewayNoC remedies this by providing a bypass path around CST from each input port (except the local port) to its opposite output port as shown in Figure 1. The input PB checks, NRC and VS now safely occur in parallel to each other and in parallel to the normal CST path after which the FC bits reach the Bypass FC mux shown in Figure 1. Parallel to input PB checks, the output PB check uses output port availability to set the select of Bypass FC mux. If the output port is free, the output PB check allows the bypassing FC to undergo CLT after passing input PB checks. It also informs input VC controller, SA and VS about successful PB. (a) VC availability check and count (b) VC selection Fig. 4: FreewayNoC VC selector. D. Router Control 1) Combined VC and Switch Allocation: The FreewayNoC utilizes a combined allocator to grant the requesting ﬂits access to the switch and to assign available output VCs to the head ﬂits of new packets which are inherited by the packet. It is composed of a VC Selection (VS) and a Switch Allocation (SA) module as depicted in the top part of Figure 1. The VS maintains a bit-vector per output port keeping track of the free VCs (VCs currently not assigned to a particular packet) in the downstream router. A free VC with available credits constitutes an available VC. The VS keeps a count of the available VCs per output port as shown in Figure 4a. Moreover, the bit vector indicating available VCs undergoes a V:2 round-robin priority arbitration to select up to two available VCs which are provided to new packets when their head ﬂits win SA, as illustrated in Figure 4b. Parallel to the VS, the SA receives requests for switch access from ﬂits going to particular outputs. In case of a head ﬂit request, the allocation logic only considers it if VS indicates one or more available output VCs, otherwise the request is ignored. If the head ﬂit is granted access to the switch, then it is also assigned an available output VC, selected by the VS arbiter, which is then marked as busy. Note that the VS arbitration and the SA are performed in parallel. A nonhead ﬂit request, is ﬁrst checked for availability of credits at the already assigned output VC before it is considered by the SA. For a router with P ports and V VCs per port, this requires (P-1)V:1 multiplexing on the credit counters. Besides ﬁltering out requests that lack a VC or credits, SA ignores requests from ﬂits that are allowed to bypass the pipeline. So the logic that determines whether a ﬂit can perform pipeline-bypassing provides input to the SA. This logic is parallel to the the logic that computes the number of available output VCs and communicates their credits. All remaining requests undergo switch allocation using a separable output ﬁrst dual grant switch allocator based on P PV:2 output arbiters (OA) and 2×P V:1 input arbiters (two per input port) with round-robin priority arbitration. Head ﬂits selected after OA for a particular output port are at most two and less than VC available count for that output. The switch allocator design is identical to the After a successful input and output PB check, the SA aligns the input and output multiplexers of the crossbar to the input VC from where the bypassed incoming ﬂits will undergo ST during the second half of the current cycle or the ﬁrst half of the next, as shown in the example of Figure 2, cycles 3-4, router 2. The FreewayNoC performs the above checks in the clock half that precedes the arrival of the incoming ﬂit3 and the time margin is sufﬁcient. As soon as the ﬂit arrives, it has been decided whether it will wait in VC buffers for SA or it will immediately proceed to ST. A normal SA allocation request is always created when an incoming ﬂit is announced by its preceding FC bits. Then, the SA checks the availability of output VCs (for head ﬂits) or credits (non head ﬂits) and in parallel the input PB checks are performed. If the input PB checks pass, the VC controller speculates that output PB checks will also pass and sets the PB Req to block the respective SA request before it enters the arbitration stages. This adds only a small delay of a 2-input AND gate after the downstream buffer availability checks are performed by the SA. In essence, an SA request that passes the input PB check is canceled speculatively without conﬁrming that the output PB check is also positive. We opted for this design decision to avoid creating a longer path before SA arbitration. We have observed that this speculation has negligible impact on performance because: (i) under low load the speculation is mostly correct (output PB check is positive), (ii) under high load the input PB checks are not successful and thus their respective SA requests are seldom canceled, (iii) in case of mis-speculation, other ﬂits can still allocate the output port reducing the chance of bandwidth waste. 3) Next Route Computation: Next Route Computation (NRC) is based on the destination carried by the forwarded control (FC) of a ﬂit after the FC is registered. In case the ﬂit is allowed to perform a pipeline bypassing, the result of the NRC is put to the new FC that bypasses the regular FC. Otherwise, the NRC result is registered and used in the regular FC a subsequent cycle. The NRC is performed in parallel to the PB check so it does not add to its delay. 4) Flow Control and Minimum Buffer Size: FreewayNoC uses credit based ﬂow control, with one wire per VC indicating to the upstream router the release of VC credits and an additional wire per port to signal the release of two credits of one VC. Moreover, in case of a regular ﬂit propagation, credits can be reused after four clock cycles. However, between nodes propagating ﬂit in pipeline bypassing mode, credits can be reused after two clock cycles because the ﬂits do not require SA pipeline stage in either of the two routers. 5) Control Path analysis: Figure 5 illustrates the various register to register paths of the control logic. The FreewayNoC architecture offers an entire cycle for these paths to complete, as opposed to the datapath parts (ST, LT) which have half a cycle available to transfer a ﬂit. It can be observed that, Input 3 For an FC that arrives at a rising clock edge, its ﬁrst out of the two data ﬂits, f lit+ , is registered in an input VC at the next falling edge, and can perform ST (ST + ) in the second half of the cycle. PB has completed its checks in the ﬁrst half of the cycle. Fig. 5: FreewayNoC control path analysis. PB check, NRC, and VS selection are in parallel and give input to the CLT. They are also in parallel to the other checks of the SA needed for ﬁltering SA requests before SA arbitration. Finally, the CST is in sequence with the output PB check, the Bypass FC mux and the CLT, while VC selection arbitration is in sequence with a P × P crossbar for sending the output VC-ids to the input ports. IV. EVALUAT ION The FreewayNoC is evaluated and compared against DDRNoC and ShortPath, the two current state-of-the-art NoC designs that achieve the highest throughput and lowest packet latency, respectively. First, we discuss the experimental setup and present post place-and-route implementation results for operating frequency and silicon area. Then, we evaluate and compare the performance and energy of FreewayNoC using synthetic trafﬁc and traces of application-driven workloads. A. Experimental Setup The FreewayNoC and DDRNoC are fully implemented in Register Transfer Level (RTL) abstraction, to accurately measure operating frequency, area and power consumption. Moreover, ShortPath’s critical path, as presented in [8], is implemented in RTL to obtain its maximum operating frequency for the implementation technology used. All networks were also modeled in SystemC at a cycle accurate level to obtain network performance results for longer simulations. Table I lists the implementation details of all network routers. The networks were implemented in a 28nm CMOS FDSOI (Fully Depleted Silicon on Insulator) 1.10V standard cell technology. The designs were placed and routed (P&R) with Cadence Design Systems Innovus Implementation System 17.1. We consider tiles with their longest side being 2.85 mm based on the Chip multiprocessors parameters used by Sewell et al. after scaling down to 28 nm (CPU core with 32kB L1 I- and D-cache and a 512kB L2 cache slice) [20]. Finally, registers in the datapath support clock gating to reduce power consumption when idle. Power analysis is performed simulating post-P&R netlists of the NoCs in Questasim with back-annotated delays. Then, gate-level switching activity for each router in a network is recorded in a VCD ﬁle which was then used to get power estimates of the entire NoC using Synopsys Primetime PX. Performance was measured by injecting synthetic and application-driven trafﬁc using 80 bytes (5 ﬂits) packets for TABLE I: Implementation parameters of the Networks. Design Router arch. Router arch. Flow ctrl Link cycles/hop VC Conﬁg. Buffer size VC allocator Switch allocator Routing DDRNoC [13] 2-stages: ST, LT (VA/SA parallel to LT of upstr. router) None FreewayNoC 2-stages: ST, LT (VS/SA parallel to LT of upstr. router) Dynamic pipelinestage bypassing Credit based ﬂow control 159 bits: 128b data, 147 bits: 128b data, 2×3b 2×3b ﬂit type, ﬂit type, 2×2b VC-id, 2×2b 2×2b VC-id, 2×2b NRC, 2×6b dest. N2 RC, 4b+1b addr, 4b+1b credits credits Min: 1, Max: 4 Min: 2, Max: 4 4 VCs per input port. 5-ﬂit ﬂip-ﬂop based VC buffers. 4 VC Select with V:2 Out-ﬁrst separ. alarbiter, RR priority loc., RR, PV:1 out arbitr., V:1 in arbitr Speculative out-ﬁrst separ. alloc., RR priority, PV:2 out arbitr., V:1 in arbitr. XY with N2 RC Out.-ﬁrst separ. alloc., RR priority, PV:2 out arbitr., V:1 in arbitr. XY with NRC ShortPath [8] 4-stages: VA, SA1, SA2/ST, LT Dynamic pipelinestage bypassing 138 bits: 128b data, 3b ﬂit type, 2b VCid, 4b ﬂit-credits, 1b pkt-credit Min: 2, Max: 4 V:1 in arbit., P:1 out arbitr. VC alloc. ReqQ depth = 10 2-stage pipel.: SA1: In-arbtr. V:1, SA2: Out-arbtr. P:1. SA ReqQ depth = 8 XY with NRC (a) 16×16, uniform random (b) 16×16, hotspot (c) 16×16, bit reverse (d) 16×16, nearest neighbor TABLE II: Post place & route implementation results. Voltage 1.10 V Design FreewayNoC DDRNoC ShortPath Area (No. of Gates) Max Op. Frequency (GHz) 276732 1.47 249505 1.47 NA 2.38 data and 16 bytes (1 ﬂit) for control. The following synthetic trafﬁc patterns are considered having 50% data packets and 50% control: (i) uniform random (UR), (ii) hotspots (HS) with 25% of the trafﬁc going to 4 hotspots, one at each NoC corner, and the rest of the trafﬁc being uniform random, (iii) nearest neighbour (NN), and (iv) bit reverse (BR). In addition, traces based on application driven workloads are obtained using SynFull [21]. These traces capture the application behaviour of various PARSEC [22] and SPLASH-2 [23] benchmarks including messages generated by the cache coherence protocol and message dependencies. Simulations run until completion, all below 100 million cycles, for 32 node (4×8) networks, considering processor speed of 5 GHz for all benchmarks except for radix and fft benchmarks which saturate all networks and are simulated considering a processor speed of 2.5 GHz. B. Implementation results Table II summarizes the post P&R results of a single router for the networks. Balancing ST and LT delays, FreewayNoC datapath parts have the same delay as DDRNoC in the same technology; that is 340 ps for ST and 340 for LT. The target cycle time of FreewayNoC is then double the above delay as two ﬂits per cycle traverse ST (or LT). Post P&R results conﬁrm that FreewayNoC control is not in the critical path and ﬁts in a clock period of 680ps letting FreewayNoC match the maximum DDRNoC operating frequency (1.47 GHz). According to Psarras et al. the critical path of ShortPath is determined by the delays of credit-check, an N:1 arbiter (for the second part of SA), two 2:1 multiplexers and the crossbar delay [8]. Partly implementing ShortPath in RTL to analyze the above path in 28 nm technology results to an estimated 4 This is sufﬁcient for ShortPath as it has a credit-round-trip-time of 5 cycles and for FreewayNoC and DDRNoC as they handle up to 5 ﬂits packets. (e) 8×8, uniform random (f) 32×32, uniform random Fig. 6: Packet latency for 4 trafﬁc patterns on 16×16 network and for 3 network sizes on UR trafﬁc. clock period of 420ps operating at a maximum frequency of 2.38GHz. Finally, the area of FreewayNoC is about 11% higher than DDRNoC mostly due to the additional bits needed in the forwarded control that is sent across routers. Since the RTL implementation of ShortPath was not available, we are did not compare with it in terms of area. C. Performance evaluation Figure 6 reports the performance of FreewayNoC, DDRNoC and ShortPath on different trafﬁc patterns and network sizes. On a 16×16 network, the latency of FreewayNoC is 4-7% higher than ShortPath at low injection rates for UR, HS and BR trafﬁc patterns where the average hop count is about 11. For NN trafﬁc, where each packet makes two turning hops, the FreewayNoC latency is 49% higher than ShortPath at low injection rates. On the other hand, FreewayNoC throughput is about 25% higher than ShortPath in all trafﬁc patterns. On the same network size, compared to DDRNoC, FreewayNoC does not improve throughput, but reduces packet latency by about 30% in all trafﬁc patterns, except NN where FreewayNoC has only marginal improvement over DDRNoC. In a smaller, 8×8 network and UR where the average hop count is 5.33, FreewayNoC latency is 24% higher than ShortPath at low load but its throughput is 25% better, while compared to DDRNoC, FreewayNoC latency improves by 22% and throughput is 2% higher. In a larger 32×32 network and UR trafﬁc, where the average hop count is 21.33, FreewayNoC latency is 5% lower *Geometric Mean for ShortPath does not include latencies of banes and FFT in which the network saturates. Fig. 7: Normalized packet latency for application driven trafﬁc. than ShortPath while its throughput is 23% better. Compared to DDRNoC in this network size, FreewayNoC latency improves by 41% and throughput is about 2% higher. Figure 7, shows the packet latency comparison between the three networks, normalized to the DDRNoC, when injecting application-driven trafﬁc through SynFull. As the networks are smaller (4×8), FreewayNoC beneﬁts of PB are limited. Still, the combination of FreewayNoC high throughput and low latency yields on average 8% and 18% lower packet latency compared to DDRNoC and ShortPath, respectively, excluding the benchmarks where ShortPath saturates. The large FreewayNoC latency improvement over Shortpath, is attributed to FreewayNoC higher bandwidth that prevents congestion in several benchmarks. D. Energy efﬁciency Figure 8 summarizes the energy efﬁciency results on an 8×8 network with UR trafﬁc for FreewayNoC and for DDRNoC, which is the only related NoC we had a full RTL implementation. The following are measured: total power, energy per transferred bit, energy-delay product (EDP), and energy throughput ratio (ETR). FreewayNoC has up to 70% higher power at zero load and 16% higher at high injection rates due to its wider forwarded control and the pipeline bypassing logic. This is reﬂected in the energy per transferred bit. EDP of the two networks is similar due to FreewayNoC lower packet latency. ETR has a similar increase with the power consumption as FreewayNoC throughput does not improve over DDRNoC. V. CONCLU S ION S FreewayNoC is the ﬁrst on-chip interconnection network that achieves a performance solely dependent on datapath delays. FreewayNoC routes packets at DDR maximizing throughput at a rate deﬁned by the longest datapath stage (ST or LT). It further provides simpliﬁed pipeline bypassing to improve latency, which per hop is at best equal to the sum of the ST and LT delays. FreewayNoC matches the throughput of previous DDR NoC and achieves a zero-load latency that scales to the number of hops better than current state-of-theart NoCs and equally well with an ideal network that has no control overheads. This enables FreewayNoC to achieve lower packet latency than existing networks at higher number of hops per packet and larger network sizes. (a) Total Power (c) Energy Delay product (b) Energy per bit Fig. 8: Energy and power costs on a 8×8 network and UR. (d) Energy Throughput Ratio "
A Low-Overhead Multicast Bufferless Router with Reconfigurable Banyan Network.,"In modern Multi-Processors System-on-Chip (MPSoC), it is highly desirable to provide hardware support for efficient multicast traffic. Recently, bufferless router has become a promising solution for NoC due to its simplicity and low overhead. However, existing multicast bufferless routers utilize the serialized switch allocator to allocate both unicast and multicast packets based on the packet priority one by one, which makes the router have a long critical path and lowers the frequency of the router. In this paper, we propose a low-overhead multicast bufferless router with a reconfigurable Banyan network (called Banyan_PR, PR is short for packets replication). The Banyan switch of the router can be configured as four modes (straight, exchange, U-multicast and L-multicast) according to the type of the incoming packets. For the U-multicast and L-multicast configurations, the multicast packet can be replicated adaptively to reduce the multicast latency. Using a 4 × 4 Banyan network instead of the serialized switch allocator, the Banyan_PR router has shorter critical path length and less area overhead. Synthesis results under a 28nm technology show that the Banyan_PR router can achieve the frequency of 1GHz and save 65% less area and 89% less power consumption than the existing deflection-routing-based multicast bufferless router (called DRM PR all) with the serialized switch allocator. Simulation results illustrate that the Banyan_PR router achieves 25%, 28% and 19% less latency on average than that of the router without packets replication (called Banyan_noPR) and 39%, 42% and 35% less latency on average than that of the DRM_PR_all router under three synthetic traffic patterns respectively.","A Low-overhead Multicast Bufferless Router with Reconﬁgurable Banyan Network Chaochao Feng College of Computer National University of Defense Technology Changsha, China fengchaochao@nudt.edu.cn Zhenyu Zhao College of Computer National University of Defense Technology Changsha, China zyzhao@nudt.edu.cn Zhuofan Liao College of Computer and Communication Engineering Changsha University of Science & Technology Changsha, China csulzf@gmail.com Xiaowei He College of Computer National University of Defense Technology Changsha, China hxw@nudt.edu.cn Abstract—In modern Multi-Processors System-on-Chip (MPSoC), it is highly desirable to provide hardware support for efﬁcient multicast trafﬁc. Recently, bufferless router has become a promising solution for NoC due to its simplicity and low overhead. However, existing multicast bufferless routers utilize the serialized switch allocator to allocate both unicast and multicast packets based on the packet priority one by one, which makes the router have a long critical path and lowers the frequency of the router. In this paper, we propose a low-overhead multicast bufferless router with a reconﬁgurable Banyan network (called Banyan PR, PR is short for packets replication). The Banyan switch of the router can be conﬁgured as four modes (straight, exchange, U-multicast and L-multicast) according to the type of the incoming packets. For the U-multicast and L-multicast to reduce the multicast latency. Using a 4 (cid:2) 4 Banyan network conﬁgurations, the multicast packet can be replicated adaptively instead of the serialized switch allocator, the Banyan PR router has shorter critical path length and less area overhead. Synthesis results under a 28nm technology show that the Banyan PR router can achieve the frequency of 1GHz and save 65% less area and 89% less power consumption than the existing deﬂectionrouting-based multicast bufferless router (called DRM PR all) with the serialized switch allocator. Simulation results illustrate that the Banyan PR router achieves 25%, 28% and 19% less latency on average than that of the router without packets replication (called Banyan noPR) and 39%, 42% and 35% less latency on average than that of the DRM PR all router under three synthetic trafﬁc patterns respectively. Index Terms—low-overhead, multicast, bufferless banyan network, Network-on-Chip router, I . INTRODUC T ION Network-on-Chip (NoC) has already become a promising communication paradigm for Multi-Processors System-onChip (MPSoC) due to its reusability, ﬂexibility and scalability features [1]. Most of NoCs adopt the wormhole/virtual-channel router which contains buffers in each input port to improve the bandwidth efﬁciency when packets contention occurs. However, buffers in routers also consume signiﬁcant energy and chip area [2] [3]. To reduce the buffers in NoC router and simplify the design complexity, different types of bufferless routers are proposed to provide a low-overhead solution for NoC [4-8]. Deﬂection routing is used in most bufferless routers to solve the packets contention problem [5] [9]. At early stages, the serialized switch allocator was used in deﬂection routing, which made the bufferless router with a long critical path to achieve a lower frequency [6] [7]. To overcome this disadvantage, bufferless routers with Banyan network [10] were proposed in [6-8]. Using a banyan network to replace the switch allocator and crossbar requires fewer hardware resources and achieves higher performance. Optimized unicast trafﬁc (also called one-to-one packet ﬂow) has already been implemented in existing on-chip networks. However, it is important to support efﬁcient collective communications, i.e., multicast (one-to-many packet ﬂow) and broadcast (one-to-all packet ﬂow), for NoC-based MPSoC to run various applications. There are statistic data to show that the multicast trafﬁc accounts for 5-10% of the total network trafﬁc in communication traces of different cache coherence protocols and has a serious impact on system performance [11]. Thus, it is better to support efﬁcient multicast in NoC for optimizing the performance of the MPSoC system. There are many mechanisms for NoC with buffered routers to improve the efﬁciency of multicast trafﬁc [11-16]. Due to the fact of unpredictable routing path of deﬂection routing in bufferless router, existing multicast schemes in buffered routers cannot be applied for the bufferless router directly. Recently, a few researches focus on supporting multicast in bufferless routers [17-19]. However, these multicast schemes both utilize serialized switch allocator to allocate unicast and multicast packets based on the packet priority one by one, which makes the router have a long critical path and lowers the frequency of the router. To the best of our knowledge, no other bufferless routers with Banyan network support multicast routing. In this paper, we propose a low-overhead multicast bufferless router with 978-1-5386-4893-3/18/$31.00 ©2018 IEEE a reconﬁgurable Banyan network (called Banyan PR, PR is short for packets replication). The Banyan switch of the router can be conﬁgured as four modes (straight, exchange, U-multicast and L-multicast) according to the type of the incoming packets. For the U-multicast and L-multicast conﬁgurations, the multicast packet can be replicated adaptively to reduce the multicast latency. Using a 4 × 4 Banyan network instead of the serialized switch allocator, the Banyan PR router has shorter critical path length and less area overhead. Synthesis results under a 28nm technology show that the Banyan PR router can achieve the frequency of 1GHz and save 65% less area and 89% less power consumption than the existing deﬂection-routing-based multicast bufferless router (called DRM PR all) in [17]. Simulation results illustrate that the Banyan PR router achieves 25%, 28% and 19% less latency on average than that of the router without packets replication (called Banyan noPR) and 39%, 42% and 35% less latency on average than that of the DRM PR all router under three synthetic trafﬁc patterns respectively. The rest of paper is organized as follows. The related work is reviewed in Section II. Section III describes the lowoverhead multicast bufferless router with Banyan network in details. Hardware implementation is presented in Section IV. In Section V, experimental results are presented and analyzed, followed by the conclusion in Section VI. I I . R ELATED WORK Three common multicast mechanisms (unicast-based, pathbased and tree-based) appear in most buffered routers. In the unicast-based multicast scheme, i.e., proposed in [20], the source node ﬁrst divides a multicast packet into multiple unicast packets based on the multicast destinations and then sends the unicast packets to each destination one by one. Although unicast routers can support this type of multicast without any changes, the scheme has larger multicast latency and lower network throughput since multiple copies of one multicast packet are injected into the network. To overcome the drawback of the unicast-based scheme, path-based and tree-based multicast schemes are proposed. In path-based multicast routing, the multicast packet is routed to each multicast destination along a pre-established path without deadlock. In [12], a connection-oriented multicast scheme, which is a path-based scheme, has been proposed in wormhole-switched NoC. In this scheme, the multicast process consists of three steps: multicast setup, communication and multicast release. Only one single copy of multicast packets is sent to each multicast destination along a path which is established in the multicast setup step. The pathbased multicast scheme is simple to implement in hardware. However, the drawback of the scheme is the large packet latency when the number of multicast destinations increases. The tree-based multicast scheme overcomes the disadvantage of the path-based scheme by routing the multicast packet ﬁrst along a common path (also called the tree trunk) as far as possible, then replicating and routing the copied multicast packets when arriving at the tree branches. A tree-based multicast mechanism, called Virtual Circuit Tree Multicasting (VCTM), has been proposed in [11] for NoC to send multicast packets according to a pre-established multicast tree stored in a virtual circuit tree table. Similar as VCTM, two powerefﬁcient tree-based multicast algorithms (Optimized tree and Left-XY-Right-Optimized tree) have been proposed in [13] to improve the throughput and reduce the power consumption. However, hardware overhead is increased since the multicast tree information requires additional storage resources. The Recursive Partition Multicast (RPM) scheme proposed in [14] outperforms the VCTM scheme due to the fact that it can replicate multicast packets based on a destination partition method. A fully-adaptive multicast mechanism, called MRR, is proposed in [15]. The mechanism selects multicast policy adaptively according to the trafﬁc loads. It adopts a tree-based multicast at low or medium loads and adopts a path-based one when the network approaches the saturation. However, large memory resources are needed to store the routing information, which increase the hardware overhead, and in addition the mechanism is too complex to implement in hardware. For bufferless router, three deﬂection-routing-based multicast (DRM) schemes have been proposed in [17]. DRM noPR is a non-deterministic path-based multicast scheme without packets replication. DRM PR src scheme can only replicate packets at the source node and DRM PR all scheme can replicate packets at both source and intermediate nodes according to a region partition rule. All three schemes are based on the router architecture shown in Fig. 1, which uses a serialized switch allocator to allocate unicast and multicast packets one by one according to the packet priority. The serialized switch allocator becomes a long critical path in the DRM router, which lowers the frequency the router can achieve. DRM PR all scheme is also extended to 3D bufferless NoC in [18]. Carpool router is another bufferless router supporting both multicast and hotspot communications [19]. Although Carpool router is based on the Chipper router [6], it does not use Banyan network to replicate multicast packet and the allocation scheme is still similar as DRM schemes. In addition, Carpool router has two pipeline stages, which also increase the multicast latency. Fig. 1. Bufferless router architecture for DRM schemes [17]. I I I . LOW-OV ERH EAD MU LT ICA ST BU FFER LE S S ROUT ER W I TH BANYAN N ETWORK A. Baseline Unicast Bufferless Router Architecture The bufferless router with Banyan network for 2D mesh NoC proposed in [7] is chosen as the baseline router, which is shown in Fig. 2. Except the input registers for 5 input ports, there are no other buffers in the router. The addresses of four incoming packets are sent to packet ejector to check whether a packet has arrived at the destination or not. The packet ejector generates the selecting signal of the 4:1 multiplexer for the arrival packet. It also decides whether the local node can inject a packet into the network or not. If the number of incoming packets is less than 4, the local input packet can be injected through one free input port of the Banyan network. The 4 × 4 Banyan network includes two stages. Each stage contains two 2 × 2 Banyan switches. If two incoming packets of a Banyan switch compete for the same productive output port, the packet with larger hop counts (higher priority) can be routed to the desired output port of the Banyan switch, while the other packet will be deﬂected to the other output port. The header updater module updates the address and hop count ﬁelds of the packet. We will extend the packet format of the baseline router to support both unicast and multicast packets in the next subsection. Fig. 2. Baseline unicast bufferless router architecture [7]. B. Packet Format The Banyan PR router supports two packet types: unicast and multicast, the format of which is shown in Fig. 3 (a) and (b) respectively. The ﬁelds of two packet formats are explained as follows: • Type ﬁeld (2 bits): indicate the type of the packet (“01”: unicast packet; “10”: multicast packet; “00”/“11”: empty/invalid packet). • Dst addr ﬁeld: for unicast packet, relative address to the destination is used in this ﬁeld, which has 6 bits for row and column addresses respectively. For multicast, an n-bit vector, where n is the number of nodes in the network, is adopted is in this ﬁeld. A bit of ‘1’ in this vector means the corresponding node is one of multicast destinations. • Rev ﬁeld (n-12 bits): used in the unicast packet to maintain the same bit width with the multicast packet. • Src addr ﬁeld (12 bits): use the relative address to the source node in this ﬁeld (6 bits for row and column addresses respectively). • Hop counter ﬁeld (10 bits): this ﬁeld records the number of hops the packet already been routed in the network, which is used as packet priority to avoid livelock. • Payload ﬁeld: the packet payload ﬁeld is set to 64 bits, which can also contain more bits according to different applications. Fig. 3. Packet format for unicast and multicast. C. Multicast Bufferless Router with Banyan Network contains a 4 × 4 Banyan network. Different from the baseline Fig. 4 shows the Banyan PR router architecture, which unicast bufferless router, four incoming packets instead of only addresses are sent to the packet ejector. The packet ejector checks whether a packet has arrived at the destination or not and also decides whether the local node can inject a packet into the network or not. For packet ejection, we assume the multicast packet has the higher priority than the unicast packet, which means if a unicast packet and a multicast packet reach the destination at the same time, the multicast packet will be chosen to eject to the local node, while the unicast packet will be deﬂected to one free output port of the router. The packet ejector generates the selecting signal (L sel) of the 4:1 multiplexer to select the arrival packet to eject out. For the multicast packet, the packet ejector also checks if it reaches the ﬁnal destination or not. If it does not reach the ﬁnal destination, it will still be transmitted to the next router. The Dst addr ﬁeld of the arrival multicast packet will be updated by packet ejector before it enters into the Banyan network. If the number of packet outputting from the packet ejector is less than 4, the packet ejector will generate the selecting signal (inject sel) to decide the local input packet injecting through one free input port of the 4 × 4 Banyan network. The 4× 4 Banyan network includes two stages. Each stage contains two 2 × 2 Banyan switches. The details of the Banyan switch conﬁguration algorithm will be described in Section III.D. The header updater module following after the Banyan network is responsible for updating the address and hop counter ﬁelds. For the unicast packet, the Dst addr and Src addr ﬁelds are updated with the relative address from the next router to the destination and source nodes respectively. For the multicast packet, the Dst addr ﬁeld is already updated by the packet ejector and only the Src addr ﬁeld is updated. For both packets, 1 hop is added to the Hop counter ﬁeld. Fig. 4. Multicast bufferless router with Banyan network. D. Banyan Switch Conﬁguration Algorithm In the baseline unicast bufferless router with Banyan network, the Banyan switch can only have straight and exchange conﬁgures, as shown in Fig. 5 (a) and (b). For our multicast router, the Banyan switch can support additional two conﬁgures (called U-multicast and L-multicast) shown in Fig. 5 (c) and (d), which means the multicast packet can be replicated to both output ports of the Banyan switch when one of input ports is empty. Fig. 5. Four conﬁgurations of Banyan switch for multicast router. The pseudo code of the Banyan switch conﬁguration algorithm is shown in Fig. 6. The algorithm ﬁrst gets the productive direction vectors (pro dir0 and pro dir1) of the two input packets by the procedure get productive dir in line 1 and 2 respectively. For the unicast packet, the productive direction is calculated based on the relative address (d addr) from current router to the destination. For the multicast packet, the procedure get productive dir ﬁrst selects a destination with the minimum manhattan distance to the current router from the destination bit string (d addr) as the best candidate, then calculates the productive direction(s) according to the position of the best candidate node to the current node. From line 3 to 57, the algorithm makes different conﬁgurations according to the type of the incoming packets input into the Banyan switch. If the upper input packet is a unicast packet and the lower input is empty, only when the productive direction of packet0 is east or west, the Banyan switch is conﬁgured as the exchange mode, otherwise it is the straight conﬁguration (Line 3 to 8). Similar situation from line 9 to 14 is corresponding to the upper input being empty and lower input being a unicast packet. If both inputs are not empty, the packet with larger hop counts (higher priority) is routed to its desired port according to the productive direction, while the Fig. 6. Banyan switch conﬁguration algorithm. with deﬂection routing must avoid livelock by limiting the number of deﬂections. In our multicast router, both unicast and multicast packets are prioritized based on its age (the number of hops already routed in the network) recorded in the Hop counter ﬁeld. In the case of the unicast packet and the multicast packet having the same age, the multicast packet has the higher priority than the unicast packet. The packet with the highest priority will always be routed to the productive direction through the Banyan network, which guarantees it can move towards its destination deterministically. Thus livelock can be avoided. IV. HARDWAR E IM PL EM EN TAT ION The multicast bufferless router with Banyan network is developed at register transfer level (RTL-level) with VHDL and synthesized by a commercial RTL compiler tool with a 28nm technology at the typical condition (25◦C, 0.9v). In order to make a comparison, we also implement a multicast bufferless router with Banyan network without multicast packets replication (called Banyan noPR) and the DRM PR all router described in [17]. Different from Banyan PR router, the Banyan switch of Banyan noPR router can only support the straight and exchange conﬁgurations. All three multicast bufferless routers contain 5 ports, which are used in an 8 × 8 2D mesh NoC. As the NoC size is 8 × 8, there are 64 bits in the Dst addr ﬁeld of the multicast packet. So the total bit width of the packet is 152. Synthesize results for timing, area and power consumption are shown in TABLE I. The Banyan PR router achieves the same frequency (1GHz) as the Banyan noPR router with only 16% additional area overhead. Due to the long critical path of the serialized switch allocator, the DRM PR all router can only achieve the frequency of 500MHz. Compared with the DRM PR all router, the Banyan PR router can save 65% less area and 89% less power consumption. HARDWAR E CO S T COM PAR I SON FOR THR E E MU LT ICA ST BU FFER LE S S ROUT ER S TABLE I Banyan noPR Banyan PR DRM all PR Frequency 1GHz 1GHz 500MHz Area((cid:22)m2 ) 62239 72180 208217 Power(mW ) 14 15 139 V. P ER FORMANCE EVALUAT ION In this section, we use a cycle-accurate NoC simulator developed in VHDL to evaluate the performance of three multicast bufferless routers (DRM PR all, Banyan noPR and Banyan PR) with different synthetic trafﬁc patterns. A. Experimental Setup The experiments are performed on an 8 × 8 2D mesh network. A packet generator is attached to each router and uses a FIFO to buffer the packets which cannot be injected into the network immediately due to the fact that there is no free input port of Banyan network to route the packet. The Fig. 7. Region partition method [17]. other packet takes the other port (Line 15 to 28). For example, if packet1 has larger hop counts than packet0, only when the productive direction of packet1 is north or south, the Banyan switch is conﬁgured as the exchange mode, otherwise it is the straight conﬁguration. For the situation of only one input is a multicast packet, the Banyan switch can be conﬁgured as the U-multicast or L-multicast mode (Line 29 to 54). Take the upper input being a multicast packet as an example (Line 29 to 41). The multicast address is replicated into two parts (rep addr NS and rep addr EW ) without destination overlapping according to the region partition method [17] shown in Fig. 7. For example, if the current router is 6, the replicated address rep addr NS contains destinations in regions R1 and R3 shown in Fig. 7, while the replicated address rep addr EW contains destinations in regions R2 and R4 shown in Fig. 7. If both rep addr NS and rep addr EW contain multicast destinations, two copy multicast packets are sent to out0 and out1 respectively (Line 32 to 34), which achieves the Umulticast conﬁguration. If just one replicated address contains multicast destinations, the multicast packet is sent according to the productive direction without replication (Line 35 to 40). The L-multicast conﬁguration can be achieved similarly from line 42 to 54. The default conﬁguration is straight mode (Line 55 to 57). E. Deadlock and Livelock Avoidance The multicast bufferless router with deﬂection routing is inherently deadlock-free since all packets (both unicast and multicast packets) does not need to wait in a router. When a packet arrives at a router, it will be sent to the next router immediately regardless of the productive direction is busy or not. However, whenever a packet is deﬂected because of the higher priority packet occupied the productive direction, livelock may occur which means the packet will never reach the destination node. Thus the multicast bufferless router trafﬁc workloads include both unicast and multicast packets. For unicast trafﬁc, we use three synthetic trafﬁc patterns: uniform random, transpose and bit complement. In uniform random trafﬁc, each node sends packets randomly to other nodes with an equal probability. For transpose trafﬁc, source node positioned at (i; j ) sends packets to destination node (j; i) for all i ̸= j . In bit complement trafﬁc, the 6-bit source node ID {si |i ∈ [0; 5]} sends packets to the destination with node ID {¬si |i ∈ [0; 5]}. For multicast trafﬁc, the destination positions are uniformly distributed. The number of destinations and the percentage of multicast trafﬁc can be conﬁgured at the beginning of the simulation. The packet latency T is deﬁned by equation (1), where Tpkt gen time is the clock cycle at which the packet is generated and Tpkt arrival time is the clock cycle when the packet arrives at the destination node. Pclk cycle (with unit ns=clock cycle) is the clock cycle time of each router synthesized under a 28nm technology, which is converted from the frequency column shown in Table I. The Pclk cycle of the Banyan noPR and Banyan PR routers is 1ns=clock cycle, while the Pclk cycle of DRM all PR router is 2ns=clock cycle. T = (Tpkt arrival time − Tpkt gen time ) × Pclk cycle (1) B. Results at Various Injection Rates Fig. 8 (a)-(c) show the average packet latency of the three multicast routers with three synthetic unicast trafﬁc patterns at various injection rates respectively. The multicast trafﬁc is 10% of the total trafﬁc and the number of multicast destinations is 8. From the ﬁgures it can be observed that the Banyan PR router achieves the smallest average packet latency among the three routers. In the case of the network not being saturated, the average packet latency of the Banyan PR router is 25%, 28% and 19% less than that of the Banyan noPR router and 39%, 42% and 35% less than that of the DRM PR all router under three synthetic unicast trafﬁc patterns respectively. The Banyan PR router outperforms the Banyan noPR router due to the reason that the Banyan switch of the Banyan PR router can replicate multicast packets adaptively. The saturation point of the network with the DRM PR all router is a little bit larger than that of the network with the Banyan noPR and Banyan PR routers. The reason lies in that the multicast packet replication scheme of DRM PR all router is more ﬂexible than that of the Banyan PR router. However, the serialized switch allocator makes the DRM PR all router have a long critical path, which lowers the frequency of the router. In addition, the network is never fully loaded under real application workloads, so the Banyan PR router is a better multicast solution for bufferless NoC than the other two routers. C. Results with Various Percentage of Multicast Trafﬁc Fig. 9 (a)-(c) reveal the average packet latency of the three multicast routers with various percentage of multicast trafﬁc under three synthetic unicast trafﬁc patterns respectively. The percentage of multicast trafﬁc varies from 10% to 25% with the step of 5%. The number of multicast destinations is 8 and the packet injection rate is 0.1 packets/cycle/node. The Banyan PR router achieves 32%, 34% and 18% less latency on average than the Banyan noPR router and achieves 41%, 45% and 22% less latency on average than the DRM PR all router under three synthetic unicast trafﬁc patterns respectively. For the bit complement unicast trafﬁc, the DRM PR all router performs even better than the Banyan noPR router when the percentage of multicast trafﬁc is 25%. D. Results with Various Number of Multicast Destinations Fig. 10 (a)-(c) illustrate the average packet latency of the three multicast routers with various number of multicast destinations (8, 16, 24 and 32) under three synthetic unicast trafﬁc patterns respectively. The percentage of multicast trafﬁc is 10% and the packet injection rate is 0.1 packets/cycle/node. As the number of multicast destinations increases from 8 to 32, the average latencies of the DRM PR all and Banyan PR routers do not vary so much, while the average latency of the Banyan noPR router increases up to 2 times. When the number of multicast destinations exceeds 16, the DRM PR all router even performs better than the Banyan noPR router. Compared with the DRM PR all router, the Banyan PR router achieves 39%, 42% and 35% less latency on average under three synthetic unicast trafﬁc patterns respectively. V I . CONC LU S ION This paper proposes a low-overhead multicast bufferless router with a reconﬁgurable Banyan network. Speciﬁc contributions of this paper can be summarized as follows: • A multicast router with a reconﬁgurable Banyan network is proposed to support efﬁcient multicasting in bufferless NoC. The router uses a 4 × 4 Banyan network instead of the serialized switch allocator for the ﬁrst time to improve the performance and reduce the area and power overhead. Different from the baseline unicast bufferless router with Banyan network, the Banyan switch of the multicast router can be conﬁgured as four modes (straight, exchange, U-multicast and L-multicast) according to the type of the incoming packets, which can replicate multicast packets adaptively. • The multicast bufferless router is implemented in a 28nm technology, which can achieve the frequency of 1GHz. Synthesis results show that compared with the existing DRM PR all router, the Banyan PR router can save 65% less area and 89% less power consumption. Simulation results on an 8 × 8 2D mesh NoC illustrate that the Banyan PR router achieves 25%, 28% and 19% less latency on average than the Banyan noPR router and 39%, 42% and 35% less latency on average than the DRM PR all router under three synthetic trafﬁc patterns respectively. Fig. 8. Average packet latency at various injection rate (10% multicast trafﬁc, 8 destinations). Fig. 9. Average packet latency with various percentage of multicast trafﬁc (8 destinations, injection rate: 0.1 packets/cycle/node). ACKNOW LEDGM EN T The research is partially supported by the National Major Project HGJ No. 2017ZX01028103 and the National Natural Science Foundation of China under Grant No. 61402056, No. 61471375 and No. 61303066. "
Reconfigurable Network-on-Chip for 3D Neural Network Accelerators.,"Parallel hardware accelerators for large-scale neural networks typically consist of several processing nodes, arranged as a multi- or many-core system-on-chip, connected by a network-on-chip (NoC). Recent proposals also benefit from the emerging 3D memory-on-logic architectures to provide sufficient bandwidth for neural networks. Handling the heavy traffic between neurons and memory and also the multicast-based inter-neuron traffic, which often varies over time, is the most challenging design consideration for the networks-on-chip in such accelerators. To address these issues, a reconfigurable network-on-chip architecture for 3D memory-on-logic neural network accelerators is presented in this paper. The reconfigurable NoC can adapt its topology to the on-chip traffic patterns. It can be also configured as a tree-like structure to support multicast-based neuron-to-neuron and memory-to-neuron traffic of neural networks. The evaluation results show that the proposed architecture can better manage the multicast-based traffic of neural networks than some state-of-the-art topologies and considerably increase throughput and power efficiency.","Reconfigurable Network-on-Chip for 3D Neural  Network Accelerators  Special Session Paper Arash Firuzan   Department of Computer  Engineering  Science and Research Branch  Islamic Azad University  Tehran, Iran  a.firuzan@srbiau.ac.ir Mehdi Modarressi  School of Electrical and  Computer Engineering,  College of Engineering,  University of Tehran  and IPM School of Computer  Science, Tehran, Iran  modarressi@ut.ac.ir  Masoud Daneshtalab  Intelligent Future Tech.  Mälardalen University   Sweden  masoud.daneshtalab@mdh.se Midia Reshadi  Department of Computer  Engineering  Science and Research Branch  Islamic Azad University  Tehran, Iran  reshadi@srbiau.ac.ir  Abstract— Parallel hardware accelerators for large-scale  neural networks typically consist of several processing nodes,  arranged as a multi- or many-core system-on-chip, connected by  a network-on-chip (NoC). Recent proposals also benefit from the  emerging 3D memory-on-logic architectures to provide sufficient  bandwidth for neural networks. Handling the heavy traffic  between neurons and memory and also the multicast-based interneuron traffic, which often varies over time, is the most  challenging design consideration for the networks-on-chip in  such accelerators. To address these issues, a reconfigurable  network-on-chip architecture for 3D memory-on-logic neural  network accelerators  is presented  in  this paper. The  reconfigurable NoC can adapt its topology to the on-chip traffic  patterns. It can be also configured as a tree-like structure to  support multicast-based neuron-to-neuron and memory-toneuron traffic of neural networks.  The evaluation results show  that the proposed architecture can better manage the multicastbased traffic of neural networks than some state-of-the-art  topologies and considerably increase throughput and power  efficiency.  Keywords— Network-on-Chip, Neural Networks, Hardware  Accelerator, Reconfiguration  INTRODUCTION  I.  Neural networks have shown tremendous promise in  solving challenging problems in various domains including  image and speech recognition, data mining, and natural  language processing [1].  To meet the excessive computational power demand of  deep learning applications, the semiconductor industry has  introduced neural network accelerator chips as an emerging  class of processors in recent years [1]. These hardware  implementations vary significantly in size and range from  small-scale embedded processing units [2][3] to large-scale  massive parallel architectures that run thousands to millions of  neurons at real-time [4-6].  A specialized hardware architecture for neural networks  typically consists of an array of customized neural engines  arranged as a multi- or many-core system-on-chip. In such  architectures, each processing element (PE) is allocated part of  the neurons to execute and interacts with other PEs in a  dataflow style: a PE operation is triggered by data arrival, and  upon finishing with the calculation, the results are forwarded to  those neural network partitions that, according to the neural  network topology, take the results as input.  In such systems, PEs need to communicate to exchange  data based on the dataflow model of the neural network (e.g.  sending the result of one neuron to all neurons of the next  layer), get input data from memory, and get a new set of  weights to (re)configure their datapath. The latter case occurs  when the target neural network is too large to fit the  multiprocessor architecture and different neural network  partitions share the PEs in time. In this case, upon switching to  a new neural network partition, the corresponding array of  weights should be fetched from memory.  Early multiprocessor neural network architectures often use  a primitive bus topology, as a simple and inexpensive  mechanism, to connect PEs [7]. However, a bus can hardly  provide sufficient bandwidth for data movement, which  increases with the degree of computational parallelism, due to  its limited scalability in area, power, and latency. To address  this challenge, the benefits of using networks-on-Chip (NoCs)  for neural network multiprocessor accelerators have been  discussed in many prior works [5-10].  Most neural network NoCs come with some variations of  the well-known mesh topology [7-8]. The mesh topology has a  regular interconnection structure and benefits from ease of  circuit-level on-chip implementation. However, as the network  size increases, the latency of mesh prohibitively grows due to  the growing network diameter. In addition, as a considerable  portion of the traffic of neural networks is in the form of  multicast of short messages (to exchange partial results), a  NoC that supports multicast routing is critical for a neural  network parallel accelerator.   Without multicast support at the topology level, a mesh  should either implement multicast by multiple unicasts or  implement multicast at the switching level.  The former will  soon saturate network and the latter makes the router  architecture complex.   978-1-5386-4893-3/18/$31.00 ©2018 IEEE     In addition, the spatial traffic pattern between the PEs and  memory and also the PEs themselves varies significantly per  application and also in time for a single application. This  variety prohibits using application-specific  full-custom  topologies due to their poor flexibility.  To address these shortcomings, we propose a cluster-based  reconfigurable NoC architecture for parallel neural network  accelerators. The PEs, which act as the nodes of the proposed  NoC, are arranged as several clusters. The nodes inside a  cluster are connected by broadcast-based topology, whereas the  clusters themselves are connected by a reconfigurable network.  The reconfigurable architecture can dynamically adapt the  inter-cluster topology to the current inter-PE and memory-PE  connections. Scalability (by providing customized long links  between remotely located clusters) and flexibility are the key  advantages of the proposed architecture over conventional  topologies.  The superior performance of a reconfigurable topology to  handle inter-neuron communication has been shown in our  prior work [11][12], but  this paper  targets a realistic  architecture considering both memory and PEs.  In addition to inter-layer data transfer, data transfer  between the PEs and memory is another source of traffic in  neural network accelerators [8][13]. As the memory capacity  and bandwidth demand of modern multilayer perceptron  (MLP) and convolutional neural network (CNN) models  increases, data movement between the PEs and the memory is  now considered as one of the most critical performance and  energy bottlenecks in neural network accelerators.  For example, the majority of the MLP and CNN models  used in various smart services of Google feature between 5M  to 100M synaptic weights [14] which impose both excessive  memory capacity and memory/network bandwidth. In this  paper, we implement the reconfigurable NoC on the logic layer  of a 3D memory-on-logic structure. In this scheme, multiple  memory layers are stacked on top of a logic layer. All layers  are divided into multiple partitions and the vertically adjacent  memory and logic form an independent memory channel. The  logic layer of each channel accommodates a memory controller  and multiple PEs. The proposed topology considers the PEs  inside a channel as a cluster and interconnects the clusters by a  reconfigurable topology.   This architecture is provided with a mapping and topology  construction algorithm that maps different partitions of an  input neural network into the NoC clusters (and cluster nodes)  and finds an appropriate inter-cluster topology.   The focus of this paper is the communication part of a  neural network parallel accelerator and has no restriction on the  computation part: it is orthogonal and can come with any type  of computational units, ranging from complex cores to small  multiply-and-accumulate (MAC) functional units.  The rest of the paper is organized as follows. In section 2,  some concepts are introduced. Section 3 surveys some related  work. Sections 4 and 5 present the proposed architecture and  design flow, respectively.  Section 6 presents experimental  results and finally, Section 7 concludes the paper.  II. PRELIMINARIES  a.  Neural networks  The most common form of neural networks is the  feedforward Multi-Layer Perceptron (MLP) model; an n-layer  MLP consists of one input layer, n-2 intermediate (hidden  layers), and one output layer. Each layer consists of a set of  basic processing elements called neurons. An individual  neuron is connected to several neurons in the previous layer,  from which it receives data, and several neurons in the next  layer, to which it sends data. Consequently, all neurons in any  given layer i receive the same set of inputs from layer i-1.  Associated with each input, each neuron keeps a weight that  specifies the impact of the input in the final output. Each  neuron computes the dot product of the input and weight  vectors and passes the result to an activation function to  produce the final output. The weight vector of each neuron is  determined during an offline or online training phase. The  focus of this paper is on the traffic management of the  execution (inference) phase of the neural networks.   b. Processing in memory  We implement our reconfigurable NoC in a processing-inmemory architecture, in which PEs and memory banks are  integrated and co-exist on a single module in a 3D fashion. We  model our system as a 3D logic-on-memory design based on  Micron’s Hybrid Memory Cube  (HMC), which  is a  commercial implementation of a 3D stacked DRAM with an  integrated logic layer [15]. In this 3D design, the logic layer  can benefit from the abundant memory bandwidth provided by  the high-speed TSVs that interconnect the layers. HMC has up  to 8 DRAM layers for a total of 8GB capacity. Each DRAM  layer is divided into 32 partitions and all vertically adjacent  partitions (that form a column of stacked partitions) are called a  vault. Each vault has an independent DRAM controller, or  vault controller, implemented on its logic die. Vaults act as  independent memory channels and can be accessed  simultaneously.  In addition to implementing memory controller, HMC 2.0  implements several simple in-memory arithmetic and logic  instructions at its logic layer that can be called by the host  processor by some specific packet types.   In some prior work, the logic die is filled by the neural  network PEs to accelerate neural information processing [13].  This paper uses the same architecture for the logic layer. While  prior work considers a 2D mesh network, we show that our  reconfigurable NoC is a promising architecture to connect PEs  to each other and also to memory controllers.  c. Cluster-based reconfigurable NoC  The reconfigurable topology adopted in this paper is a  modified version of the baseline reconfigurable topology we  presented in a prior work [11]. In the reconfigurable NoC in  [11], routers (squares in Figure 1) are not connected directly to  each other, but through a simple logic, called configuration  switch (circles in Figure 1) that allow changing the inter-router  connections dynamically.        A Configuration switch consists of several transistor  switches, like the switch boxes in an FPGA, which can be  configured to set up permanent long links between different  routes. Actually, they can be considered as simple 4×4  crossbars and are by far simpler and smaller than a router since  they have no network interface, buffer, arbitration, and  routing/flow control  logic. Further, unlike  the  internal  connections of a router crossbar that may potentially change at  each cycle,  the FPGA-like configuration switches keep  permanent  input-output connections during application  execution time to further save power.   By adding a register to some configuration switches (the  dark configuration switches in Figure 1), a long link formed on  multiple configuration switches is spelt into fixed length parts  on which flits can travel in a pipelined fashion (one segment  per clock).  Core Router Configuration  Switch Figure 1. The reconfigurable NoC structure [11]  This conservative link traversal prohibits any potential  timing violation that long links may cause. To get a more  general topology, we can construct a hierarchical topology in  which the routers are arranged as several clusters with a fixed  topology, but the clusters themselves are interconnected by  configuration switches.   By changing the three main parameters of the topology, i.e.  cluster size (number of nodes in each cluster), network corridor  width (the number of switches between two adjacent clusters at  each row and column), and the number of clusters in the  network, various reconfigurable NoCs can be implemented.   This architecture can benefit from the interesting properties  of mesh, while mitigates its main drawback, i.e. the lack of  short paths between remotely located nodes in the topology.  The reconfigurable NoC offers up to 35% latency reduction  and 25% power reduction while imposing 10-35% area  overhead due to the extra links and configuration switches it  uses. More implementation details and of this architecture can  be found elsewhere [11].  III. RELATED WORK  In recent years, with the emergence of multi- and manycore  chips,  several NoC-based  neural  network  implementations have been proposed [5-10]. The role of NoC  in such systems is to manage the excessive inter-neuron and  memory-to-neuron traffic.  SpiNNaker is a large multiprocessor that simulates spiking  neural networks  [6]. Each node  in  the SpiNNaker  multiprocessor is a many-core chip consists of a 2-D array of  processing nodes. These nodes are connected by a modified  version of the torus topology, whereas the inter-chip topology  is a 2D triangular mesh with 6-port routers.   EMBRACE [5] is another example of a large-scale neural  network  architecture which  tries  to  integrate  the  programmability features of FPGAs to the scalability of NoCs.  The EMBRACE architecture is a 2-D array of interconnected  neural tiles surrounded by I/O blocks. It adopts H-NoC, a  hierarchical mesh-based topology, that consists of three layers;  10 neural cells are connected to a router at the first layer to  form a neuron module, up to 10 neuron modules are connected  to a upper router to form a tile, and up to 4 tiles are connected  to an upper router at the third layer to form a cluster. If a neural  network does not fit in one cluster, multiple clusters are  interconnected by a mesh at a higher level.  In [7], a number of NoC topologies are examined for  hardware implementation of neural networks. The paper then  shows that a mesh NoC is preferred over the other analyzed  architectures (fat tree, point to point and shared bus). The paper  highlights the importance of reconfigurable topologies as a  promising future research direction.   In [9], a design flow that implements a convolutional neural  network by a library of basic processing units (convolutional  layer filters and neurons) is presented. The components are  connected by two simple data distribution and data reduction  networks that have been reported to offer better performance  than mesh and crossbar.  A recent research proposal shows that the power-efficiency  of some customized version of the indirect interconnection  networks, such as the Clos [10] and dragonfly [16] topologies  outperforms many conventional topologies in managing the  multicast-based traffic of neural networks. However, they only  cares about the inter-neuron traffic and does not consider the  traffic between PEs and memory.  Neurocube is a memory-aware accelerator for neural  networks that implements neural networks on the logic die of a  3D memory-on-logic structure [13]. A mesh NoC on the logic  die is used to interconnect PEs. Our paper uses the same  platform, but shows that a reconfigurable topology can yield  better power-efficiency than the mesh and other conventional  topologies.  IV. PROPOSED NOC-BASED NEURAL NETWORK  To handle the multicast-based traffic of neurons, we use a  bus as the intra-cluster topology of the reconfigurable NoC to  connect the PEs and the memory controller inside a cluster.  Figure 2 shows the customized architecture with the corridor  width of 2 and the cluster size of 8.  Bus is generally known as the most cost-efficient option to  support collective communication but suffers from poor  throughput scalability in large systems. By limiting the bus  agents to the PEs of a single cluster, we can keep the bus traffic  low and benefit from its low-cost multicast with no scalability  issues. To support the bus operation, the routers are equipped  with a simple bus controller on one of their ports that can            read/write from the shared bus in each cluster. The regular  packet-switched part of the router connects it to the routers of  other clusters via configuration switches. The bus arbitration  uses a simple FIFO arbitration policy that grants nodes in the  order at which they place their request.  Figure 2. The proposed bus-based recofigurable NoC. The circls are  configuration switches and sqaures represent routers.  As Figure 2 shows, the routers in this design have three  bidirectional ports to the bus, local PE/memory, and the  adjacent configuration switch. When a router receives a packet,  it can take three actions: (1) send the packet to its local  processor, (2) broadcast it over the local bus to send it to other  nodes of the cluster, and (3) send it to its inter-cluster output  port to pass it to other clusters. A router may also take a  combination of these actions, e.g. if a data should be sent to  both the bus and ejection port.  Figure 3 shows how each cluster of our design is mapped  onto a vault.   Figure 3. The structure of a vault  Figure 3 displays the structure of a vault with a 8-node busconnected cluster. One of the nods is the memory controller  which is connected to the memory slices of the vault, whereas  the rest are PEs that run neurons.  In this paper, we ignore the initialization and control  procedures that host initiates to control the entire memory-side  neural information processing and focus on managing the onchip communication during the inference phase of neural  network execution.  We assume that a higher level protocol marks packets with  appropriate tags and sequence numbers in order for PEs to  distinguish between neuron weights and input data and also  reorder the received input data appropriately to multiply them  to the right weight.  NOC TOPOLOGY CUSTOMIZATION  V.  We propose a design flow to implement a neural network  on the reconfigurable NoC architecture. The main steps of the  design flow are partitioning the neural network based on the  number of neurons that each PE can process in parallel,  mapping neurons onto cluster nodes, and customizing the intercluster topology for the neural network by establishing  reconfigurable links between clusters. If the neural network  size is larger than the system’s capacity, a scheduling scheme  is further needed to determine the order of execution of  different neural network partitions on the architecture. In this  case, upon switching the partitions, the synaptic weights of the  new partition should be read from the memory that makes the  on-chip traffic more complex.  a. Neuron to network mapping   Partitioning. In a massive parallel machine, a neural  network may not fit entirely on a single node, as each PE (NoC  node) has a certain processing power that specifies the  maximum number of neurons it can run in parallel. Thus, we  should first divide neural networks across several NoC nodes.  In this work, we use a layer-wise partitioning approach:  assuming each PE can host at most n neurons, starting from the  first neuron of the first layer and proceeding layer by layer, the  neurons are grouped into n-neuron partitions. Figure 4.a shows  this parting scheme for PEs with the capacity of two neurons.   Partition grouping. Once the neurons are partitioned, the  partitions should be mapped onto the NoC nodes. The mapping  is done by a two-step procedure. In the first step, the partitions  that should be mapped onto the same cluster are selected and  bundled into partition groups. Afterwards, each partition group  is assigned to a NoC cluster.  Among different approaches, the grouping policy that this  paper applies puts adjacent partitions in the same group, so  partition groups, and hence clusters, mostly contain neurons  from the same layer. This way, the PEs of the same cluster  have common input data. It can facilitate traffic management,  in that one PE of each cluster can get the data and broadcast it  to the others internally.   Partition group to cluster mapping. The next step is the  mapping algorithm that specifies to which cluster each                partition group should be mapped to minimize communication  overhead. For this step, we use a modified version of the  NMAP algorithm that used for the baseline reconfigurable  topology [11]. It is a heuristic mapping algorithm that makes  an appropriate trade-off between the execution speed and the  quality of the mapping.  This algorithm takes a parallel multi-task application, in the  form of a communication task graph (CTG), as input and maps  each task onto a NoC node in such a way that the total traffic  load across all NoC links is minimalized. In other words,  partition groups must be mapped onto network clusters in a  way that groups with heavier communication are mapped onto  adjacent (or nearby) clusters. The details of the NMAP  algorithm can be found elsewhere [11].  For the mapping step, we ignore the configuration switches  and consider the NoC clusters as the nodes of a mesh network,  as this step just cares about reducing the Manhattan distance  between the partition groups that communicate more frequently  (and with higher data rates).   CTG is a weighted directed graph G(V,E), in which each  vertex iv V∈ represents a task and each edge  E∈ shows the  communication flow from task vi to task vj. The weight of  each edge t(ei,j) represents the data transfer rate from node i to  node j. for our design flow, we modify a baseline CTG by  tagging the edges that carry different copies of the same data.  This will be used during the topology construction step to  support such connections by a multicast tree.  CTG edges represent three sources of traffic: (1) the neuron  output that should be sent to the neurons of the next layer, (2)  neural network input that goes from a memory controller to the  PEs that host neurons of the first layer, and (3) the neuron  synaptic weights which are stored in memory and go to PEs to  (re)configure them. Figure 4.b shows the CTG generated for  the neural network of Figure 4.a. Without loss of generality, we  assume the neural network input data is mapped onto the  memory vault where most of the first layer neurons are mapped  (C1). The synaptic weights are also mapped onto the memory  vaults of C1.  je ,i b. Topology generation   Topology construction is the last step of the proposed  design flow. The inter-cluster links are configured at this step.  Figure 4.c shows how a set of customized inter-cluster links are  constructed for the neural network.  Given a cluster-based reconfigurable NoC, a mapping of  neurons onto  the NoC clusters, and  the  inter-cluster  connections provided by the CTG, the topology construction  step aims at setting up a connection for all inter-cluster  communications  through  the  routers and configuration  switches in such a way that (1) each connection is set up along  a shortest path between the source and destination clusters, and  (2) the link usage (total number of utilized links) is minimized.  Minimizing the link usage will lead to forming multicast trees  over the configuration switches: if the same data should be sent  to two or more clusters, the connections can use common  configuration switches and links for part of their path.   Topology  construction  algorithm. The  topology  construction uses a modified version of the Dijkstra’s shortest  path algorithm to find a shortest path with minimum cost for  the edges of the neural network’s CTG. For this purpose, the  CTG edges are selected in some order and a path between their  endpoint nodes is constructed. In this work, the edges are  selected in the order of their communication rate. If two or  more edges have the same communication rate (e.g. the edges  that send the output of a neuron to some other neurons) the  order will be set randomly. The Dijkstra’s algorithm works on  an architecture graph that is constructed based on the  reconfigurable topology. In the architecture graph, all clusters  and configuration switches of the topology are considered as  nodes. Figure 5 displays the architecture graph of the  reconfigurable topology of Figure 2.  (a)  (b)  (c)  Figure 4. (a) The neural network partitioning, (b) CTG extraction,  and (c) mapping and topology construction steps           The architecture graph is a directed weighted graph and the  cost of a path on it is calculated as the cumulative cost of the  clusters and configuration switches that the path contains.  We assign a cost of 1 to a link segment ending to a  configuration switch (link segment X in Figure 2) and a cost of  4.5 to a link segment ending to a cluster (link segment Y in  Figure 2). These cost values reflect the power/latency ratio of  forwarding a packet through the switches and 8-node clusters  and are obtained by architecture-level power/latency  evaluation. This weight assignment policy pushes the paths  towards using more configuration switches that in turn, yields  more direct paths between the communication endpoint nodes.  Figure 5. The equivalent architecture graph for the reconfigurable  topology of Figure 2. Larger circle represent clusters  A path may contain multiple configuration switches and  clusters. Once a path is found, it will be established by  configuring configuration switches and routing tables of the  cluster routers. The algorithm, then, continues with the next  communication.  The  links  that are constructed  for  two distinct  communications are not allowed to overlap on configuration  switches; so, if a switch is already configured in previous  iterations of the algorithm for other communications (e.g., by  connecting its N input port to S output port), the algorithm is  not allowed to use conflicting turns on that switch (e.g.,  connecting N input port to W input port or E input port to S  output port). This is done by setting the cost of such turns to  infinity. However, overlapping paths are allowed on clusters,  as they can connect any input to any output through their  internal bus.   To restrict Dijkstra to return topological shortest paths, it  only considers the links that progress towards the destination  node, i.e. the links that reduce the distance to the destination by  one hop at each step.  During the neural network execution, packets may pass  several buses and configuration switches. For example, Figure  6 shows the path constructed between node X in cluster Cs and  node Y in cluster Cd. The source node first uses the bus to send  its data to node U in the same cluster, and from there to node V  in an intermediate cluster. In that cluster, the packet is sent on  the bus to reach node W and from there to the final destination  node  through  the direct connection provided by  the  configuration switches. Inside the clusters, packets are routed  by routing tables filled during connection setup.  Multicast support. If multiple edges of the CTG carry the  same data, the algorithm tries to build a multicast tree for them  to reduce the link usage. The tree construction is still done  using the Dijkstra’s algorithm, but tries to overlap the paths as  much as possible to save links for subsequent edges.  To this end, all CTG edges of a node that multicast a data  are sorted in the increasing order of the Manhattan distance to  their destination clusters. Then, the path for the edge with the  shortest distance is set up by the Dijikstra’s algorithm. Suppose  this path is constructed from cluster Cs to Cd1 and passes  through N intermediate nodes IN1 to INN.  Intermediate nodes  can be either cluster or configuration switch. For the second  edge that goes from Cs to Cd2, the shortest path algorithm not  only finds the path with the minimum weight between clusters  Cs and Cd2, but also calculates the minimum weight from all  of the intermediate nodes of the previous path, i.e. IN1 to INN,  to Cd2. Then, the path with minimum weight is selected. If  multiple paths have the same weight, the path that minimizes  the link usage, i.e. connects Cs to Cd1 and Cd2 with the  minimum number of links is selected. For each of the next  multicast edges, the intermediate nodes of all previously  constructed edges take part in minimum path calculation as a  potential branching point.  Figure 6 shows how a multicast tree from node A to nodes  B and C can save links, compared to the case where each  communication has its dedicated path. To accelerate the  algorithm, this step only considers those intermediate nodes of  the previous path that lie along one of the shortest paths  between Cs and Cd2. If the new path expands the previous path  at node IN1 and IN1 is a configuration switch, the internal  crossbar of the switch should connect the input from which the  path enters to two outputs, each related to one edge.  Figure 6. A multi-hop connection between nodes X and Y and the  multicast tree from node A to nodes B and C  Link reconfiguration. As mentioned before, flexibility is a  key advantage of the proposed architecture. The inter-cluster  topology can change dynamically  to a pre-calculated  configuration when the neural network corresponding to that  configuration starts execution.   EXPERIMENTAL RESULTS  VI.  We implement the proposed NoC architecture, along with  the HNoC [5], dragonfly [16], and concentrated mesh (Cmesh)  topologies for comparison purpose, in a modified version of  the widely-used BookSim NoC simulator [17]. We have  modified the baseline simulator to support the reconfiguration  switches.                  The routers in all considered networks have 8-flit deep  buffers. The zero-load latency of all switches is two cycles  with one cycle for switch output arbitration and look-ahead  routing and one cycle for internal crossbar and link traversal.  The bus access latency is also set to one cycle. The link bitwidth is 32 which is wide enough to carry the 16-bit fixedpoint neuron output plus some routing/ordering information in  one flit.  We use several large neural networks as benchmark. The  size and topology of each benchmark is listed in Table 1. The  first three benchmarks are pattern recognition applications  while the forth is a classification application. The benchmarks  are taken from [18]. The table displays the number of layers  and the number of neurons per layer.   For the two smaller benchmarks (NN3 and NN4), the  reconfigurable cluster-based NoC has 32 clusters of size 8 and  corridor width of 2. H-NoC has 8 neurons in each neuron  facility, 8 neuron facilities in each tile, and 8 tiles in each  cluster, and 4 clusters. The mesh is 8×8 with a concentration  degree of 4. The dragonfly has 256 PEs arranged as 16 groups,  each containing 16 PEs. For the larger neural networks (NN1  and NN2), we use 1024 PEs by increasing the cluster count of  HNOC and reconfigurable NoC and the group count of  dragonfly by a factor of 4. For the both network sizes, if the  neural network is too large to fit the NoC, the neural network is  partitioned and its execution is scheduled on the NoC.   All NoCs has 32 vault controllers that are distributed across  the topology. Neuron input data is written to vault 0 by a DMA  engine and the starting address of synaptic weights starts right  after the input data and may spread across multiple vaults.  As the focus of this paper is on the network, we use a  simplified model for our PEs: Each PE (NoC node) in all  topologies has a maximum capacity of 8 neurons and can  complete 8 parallel multiply-and-accumulate operations in a  single cycle. The power/temperature analysis results of [13]  shows that the logic layer of a vault has enough space to  accommodate the eight (and even 16) PEs of our design and  this configuration do not make sever thermal problem for the  target 3D chip.    In the simulations, a stream of inputs is fed to the neural  networks and is processed in pipelined fashion (when the first  input vector is running by the second layer, the second input  vector is being processed by the first layer, and so forth). The  neural network input is always ready and enters the network  once the neurons of the first layer complete the processing of  the previous data.  Packet  latency analysis. Table 1 compares  the  performance of the four considered topologies. As the table  indicates,  the  lower (and adaptable) hop count of the  reconfigurable topology offers to a shorter latency across all  injection  rates. The average packet  latency of  the  reconfigurable topology is followed by dragonfly (with the  average hop count of 4 to 6 [12]), HNOC (with the average hop  count of 6 to 8 [5]). With each network size, larger neural  networks (N2 and N4) have longer latency as the overhead of  fetching synoptic weights from memory (to switch between  neural network parts) is higher for larger neural networks.  TABLE 1 . THE S IZE AND TOPOLOGY OF BENCHMARK S AND AVERAGE  PACKET LATENCY (CYCLE S) COMPAR ISON . THE NUMBER OF NEURON S IN  D IFFERENT LAYER S O F EACH NEURAL NETWORK  ARE SEPARATED BY THE  “:” DEL IM ITER  Benchmark  NN1:  784:500:800:500:1  NN2:  400:3136:784:1200:300:11  NN3:  256:256:256  NN4:  234:1024:61  NoC  size Cmesh HNOC Dragon. Reconf. 1024  163  99  81  73  1024  184  110  256  256  63  68  44  50  96  37  41  80  29  32  Figure 7 compares the power consumption and neural  network throughput for the considered NoCs. The results are  normalized to the concentrated mesh’s results, in order to give  a better insight into the relative performance of the NoCs.  Throughput is defined as the inverse of end-to-end latency of a  neural network that in turn, is defined as the time elapsed from  when an input set is delivered to the neurons at the input layer  until the last neuron at the output layer receives its input. The  results only consider the NoC-related latencies and assumes  that the PEs process each received data in a single cycle.  As Figure 7 shows, the reconfigurable topology can  provide higher processing throughput than the dragonfly, HNoC and Cmesh. Yielding lower network latency and taking  advantage of the broadcast capability of bus (while mitigating  its scalability problem by reconfigurable long links) is the main  source of the obtained throughput improvement. The better  network latency of the reconfigurable topology brings about a  maximum of 71% improvement over the Cmesh.   2 Cmesh HNOC Dragon. Reconf. t u p h g u o r h t d n a r e 1 w o p d e z i l i a m r o N 0 r e w o P t u p h g u o r h T r e w o P t u p h g u o r h T r e w o P r e w o P t u p h g u o r h T NN1 NN2 Figure 7. power and throughput comparison  NN3 t u p h g u o r h T NN4 In terms of power consumption, following the same trend  as the performance, our design provides better results, mainly  due to the shorter average hop count the reconfigurable links  can provide. Power consumption, however, shows marginal  improvement for some benchmarks, mainly due to the dynamic  and static power of the configuration switches. The power and  area results are obtained by an architecture-level power model  in 22nm technology point for 32 bit links.   However, the architecture of the reconfigurable NoC is  larger than concentrated mesh, dragonfly and HNOC by 30%,                [11] M. Modarressi, et al., “Application-aware Topology  Reconfiguration for On-Chip Networks,” IEEE Trans. VLSI Syst,  vol. 19, No. 11, 2012, pp. 2010-2022.  [12] A. Firuzan, et al., “Reconfigurable communication fabric for  efficient implementation of neural networks”, in Proceedings  ReCoSoC, 2015, pp. 1-8.  [13] D. Kim, et al., “Neurocube: A Programmable Digital  Neuromorphic Architecture with High-Density 3D Memory,” in  Proceedings of ISCA, 2016, pp. 380-392.  [14] N. P. Jouppi et al., “In-datacenter performance analysis of a  tensor processing unit,” in Proc. of ISCA, 2017, pp. 1-12.  [15] “Hybrid memory cube specification 2.0,” Hybrid Memory Cube  Consortium, Tech. Rep., Nov. 2014.  [16] N. Akbari and M. Modarressi, “A High-Performance Networkon-Chip Topology for Neuromorphic Architectures,” in Proc. of  IEEE EUC, 2017, pp. 9-16.  [17] P. Mehrvarzy, et al., “Power-and performance-efficient clusterbased  network-on-chip with  reconfigurable  topology,”  Microprocessors and Microsystems, vol. 46, pp. 122-135, 2016.  [18] H. Esmaeilzadeh, et al., “Neural network stream processing core  (NnSP) for embedded systems”, in Proc. of ISCAS, pp. 27732776, 2006.  22%, and 13%, respectively. The performance results,  nonetheless, suggest that the reconfigurable topology makes  more appropriate  tradeoff between area and power  efficiency/throughput.  CONCLUSION  VII.  Among different hardware accelerators  for neural  networks, those methods that use NoC as communication  infrastructure give better performance and scalability, as they  can better manage the heavy multicast-based inter-neuron and  memory-to-neuron traffic.   In this paper, we presented a reconfigurable cluster-based  NoC architecture for neural networks. The architecture is  supported by a design flow that maps the neural network onto  NoC clusters and establishes a topology on the reconfigurable  fabric based on the inter-cluster connectivity pattern of the  network. The evaluation results show that the proposed  architecture accelerates neural processing by offering up to  71% higher throughput than a conventional mesh.  VIII.  ACKNOWLEDGMENT  This work has, in part, been supported by KKS within the profile  DPAC and DeepMaker projects.   "
On-Chip Wireless Channel Propagation - Impact of Antenna Directionality and Placement on Channel Performance.,"Long range, low latency wireless links in Networks-on-Chip (NoCs) have been shown to be the most promising solution to provide high performance intra/inter-chip communication in many core era. Significant advancements have been made in design of both Wireless NoC (WNoC) topologies and transceiver circuits to support wireless communication at chip level. However, a comprehensive understanding of wireless physical layer and its impact on performance is still lacking. There is still a lot of scope for thorough analysis of the affects of intra-chip wireless channel and antenna characteristics on signal transmission and link reliability in WNoCs. To this end, we analyse signal propagation through wireless channel by accurately modelling the intra-chip environment. We analyse the effects of antenna placement across chip plane and its directionality on the signal loss, delay and dispersion properties. The analysis shows that directional antenna exhibits better delay characteristics, while omnidirectional antennas have low loss for signal transmission in the channel. Furthermore, the placement of antenna shows considerable impact on channel characteristics due to reflections from chip edges and constructive or destructive interference between the multiple signal components. This work provides crucial insights into propagation characteristics of on-chip wireless links for better design of transceiver components and their performance.","On-Chip Wireless Channel Propagation: Impact of Antenna Directionality and Placement on Channel Performance Special Session Paper Sri Harsha Gade Department of ECE IIIT Delhi, New Delhi, India harshag@iiitd.ac.in Sidhartha Sankar Rout Department of ECE IIIT Delhi, New Delhi, India sidharthas@iiitd.ac.in Sujay Deb Department of ECE IIIT Delhi, New Delhi, India sdeb@iiitd.ac.in Abstract—Long range, low latency wireless links in Networkson-Chip (NoCs) have been shown to be the most promising solution to provide high performance intra/inter-chip communication in many core era. Signiﬁcant advancements have been made in design of both Wireless NoC (WNoC) topologies and transceiver circuits to support wireless communication at chip level. However, a comprehensive understanding of wireless physical layer and its impact on performance is still lacking. There is still a lot of scope for thorough analysis of the affects of intra-chip wireless channel and antenna characteristics on signal transmission and link reliability in WNoCs. To this end, we analyse signal propagation through wireless channel by accurately modelling the intra-chip environment. We analyse the effects of antenna placement across chip plane and its directionality on the signal loss, delay and dispersion properties. The analysis shows that directional antenna exhibits better delay characteristics, while omnidirectional antennas have low loss for signal transmission in the channel. Furthermore, the placement of antenna shows considerable impact on channel characteristics due to reﬂections from chip edges and constructive or destructive interference between the multiple signal components. This work provides crucial insights into propagation characteristics of on-chip wireless links for better design of transceiver components and their performance. Index Terms—inter-chip wireless, multichip system, heterogeneous architectures I . IN TRODUC T ION Limitations of Dennard scaling on power consumption have ushered the chip designs into the era of multi-core and many core architectures for high performance computing. These architectures necessitate the implementation of Networks-onChip (NoCs) for achieving efﬁcient communication performance. However, as number of cores integrated on a single chip increases, traditional NoCs are limited by high latency and energy wired communication. Improving characteristics of metal wires does not satisfy network performance requirements in the long term. Furthermore, multi-chip wired links severely penalize the broadcast and multicast data transfers, required by several control data like coherence messages in many core processors. New interconnect paradigms are needed to meet such communication demands of emerging processing architectures. To this end, several novel approaches like 3D integration, on-chip photonic links, RF interconnects (RF-I) and wireless interconnects have been proposed. While each of these interconnects have their pros and cons, wireless interconnects (operating at mm-wave and sub-THz frequencies) offer the most feasible solution due to their compatibility with CMOS manufacturing processes. Wireless NoCs (WNoCs) typically augment existing wired NoC architectures with low latency and low energy on-chip wireless links for providing high performance communication. The wireless links enable single hop communication over long distances across the chip to overcome limitations of wired interconnects. Besides providing high performance, wireless communication inherently supports broadcasts, highly suitable for control messages in many core architectures. Several topological and system level implementations have shown the performance and energy improvements of WNoCs [1]–[3]. Similarly, signiﬁcant advances have been made in transceiver designs [4]–[6] and on-chip wireless links operating at different frequencies [7], [8] for WNoCs. While these works highlight the feasibility and advantages of WNoCs, they still suffer from fundamental challenges and limitations. High power consumption in transceiver components, efﬁcient resource sharing and allocation mechanisms, link analysis, etc. are some of the major limiting factors for WNoC implementations. A foremost concern in WNoCs is the on-chip wireless channel and is the focus of this work. The wireless channel determines and impacts greatly the major characteristics of signal transmission like loss, delay, etc. The channel characteristics has its effects experienced across the entire spectrum of WNoC design from link performance to analog transceiver speciﬁcations to wireless node placement and system level design. However, most existing topological analyses assume free space channel transmission for performance evaluation. But, on the contrary, on-chip wireless channel is plagued by multipath propagations, near ﬁeld effects, interference wired links, etc. deviating its char978-1-5386-4893-3/18/$31.00 ©2018 IEEE acteristics signiﬁcantly from free space channel. Hence, there is a fundamental need for detailed investigations into on-chip wireless channel models for both efﬁcient design and accurate evaluation of on-chip wireless communication infrastructure. In the context of channel modelling, the objective of this work is to accurately model the on-chip environment for wireless channel and understand different propagation paths of the received signal. We especially evaluate the impact of antenna placement and its directionality on number of prevailing signal paths and channel characteristics. We accurately model the intra chip wireless channel environment to include the silicon substrate, metal interconnect layers for wired communication and dielectric materials, etc. On-chip wireless channel is a highly multipath channel with different signal components arising from reﬂections off of different chip components and edges. Using the chip model, we ﬁrst derive different propagation paths between the transmitter and receiver. The same model is then simulated using CST MicroWave Studio tool to analyse electromagnetic propagation between antennas with different placements. The study can provide crucial insights to system designers for optimizing antenna parameters and placement for reliable wireless transmission in WNoCs. Especially, our observations show that received signal characteristics vary signiﬁcantly with different antenna placements and must be incorporated while choosing the optimal placement for wireless interfaces in WNoC architecture. I I . R E LAT ED WORK Understanding channel and signal transmission characteristics is pivotal for efﬁcient design and evaluation of wireless communication systems. In WNoCs, the on-chip environment, akin to urban environment for mobile communication, is comprised of several structures of different geometries, dimensions and material properties like substrate, metal interconnects, dielectric and passive materials, etc. The complex nature of on-chip wireless channel rises to several propagation paths between transmitter and receiver. This results in several adverse effects like small scale fading, high loss, dispersion and propagation delay. Furthermore, the signal transmission may fall into near ﬁeld or transition regions depending on the frequency of operation. There have been endeavours to analyse the characteristics of on-chip wireless channel and its impact on transmission properties. Authors of [9], through manufactured integrated antennas, have studied the effect of silicon substrates on their performance. Design guidelines for improving antenna efﬁciency in presence of metal interference structures (in the form of wired links, power grid, etc.) have been proposed in [10], [11]. Different conﬁgurations for silicon substrate and ground shield have been explored in [4] for improving transmission characteristics of the integrated antennas. While these works use integrated antennas manufactured in CMOS chips and measurements from them, the objective of their work is to understand and improve antenna characteristics. A few works have analysed the signal propagation through on-chip wireless channel to derive properties like path loss and propagation delay. A study in [12] has used Green’s functionbased channel model to establish different propagation paths between transmitter and receiver in intra chip wireless channel. Intra chip wireless interconnect analysis in [13] has analysed wireless channel through manufactured integrated antennas to derive link performance and BER. However, their analysis is comprised of simple chip geometry with antenna over silicon substrate and SiO2 dielectric. They ignore the impact of metal interference structures inside the chip which can signiﬁcantly alter both antenna and channel efﬁciency. To the best of our knowledge, analysis by Zhang et. al. in [14] is the only work that has attempted to study metal structures’ impact on channel performance. They have performed a time and frequency domain analysis of on-chip wireless channel using measurements from manufactured antennas to derive loss and delay measures. To achieve detailed understanding of intra chip wireless channel including all major components and the impact of antenna placement and directionality, we develop a realistic 3D representation of the CMOS chip and provide analytical and simulation models of the channel. I I I . S IMU LAT ION MOD E L AND EVA LUAT ION M E THODO LOGY In this section, we brieﬂy describe the model developed and methodology for analysing the on-chip wireless channel. The three dimensional multi-layered model as shown in Fig. 1 captures all the major components of the intra chip environment that impact the wireless channel characteristics. Each layer in the 3D chip model represents a hardware functional layer and are stacked along the Y-dimension and spans the area across the XZ-plane. The bottom most layer of the model is the Silicon Substrate layer, which represents the bulk of the chip in which all the transistors and devices are fabricated. As the dimension of each transistor is signiﬁcantly smaller than the bulk, we ignore them and model the layer as a single homogeneous material of silicon. A Poly layer above silicon substrate represents the poly-silicon used for gate contact. These layers basically embodies the digital and analog circuit components forming the heart of any CMOS chip. On top of this circuit layers, several layers of metal are formed embedded within the Silicon-di-Oxide (SiO2 ) material as shown in Fig. 1. These metal layers represents the wired interconnects within the chip, which connect different circuit components and deFig. 1. 3D on-chip environment with a silicon substrate, copper interconnects and silicon-di-oxide for wireless channel estimation CH I P MOD E L AND AN T ENNA S P EC I FICAT ION S FOR CHANN E L MOD E L ING TABLE I Component Dielectric Constant Conductivity Description Silicon Substrate 11.9 2.5x10−4 Metal Interconnects SiO2 Layers Folded Dipole PLPA — 3.9 — — 5.8x107 10−10 5.8x107 5.8x107 633µm bulk with circuit devices 0.1-8µm M1 to M9 layers Embedded between two metal layers Omnidirectional antenna from [15] Directional antenna adopted from [16] vices. Conventionally, consecutive metal layers inside a CMOS chip are aligned perpendicular to each other and span different lengths. We model the layers similarly with every alternate layer either aligned along X or Z dimension respectively. In our modelling, we have used nine metal layers (M1 to M9), which is common in chips with billions of transistors (possibly more layers). Alternate metal layers are separated by a SiO2 layer. The dimensions of each layer in the model are shown in Table I. These different layers represent the major and most commonly occurring components of any CMOS chip. The antennas are etched into the top metal layer as shown in the Fig. 1 to reduce interference from other metal interference structures of the chip. This is followed from the observation in [10], [11], that metal structures above the antenna are to be avoided for better performance. Furthermore, the implementation of antenna is done to ensure that the region surrounding it is devoid of any metal structures. The antennas and top of model are exposed to free space, representative of vacuum inside the chip. We have used two different antenna structures, Folded Dipole (FD) [15] and Planar Log Periodic Antenna (PLPA) [16], to understand the impact of antenna directionality along with the placement of the antennas. The FD antenna has near omnidirectional radiation pattern and can result in signiﬁcantly high number of signal components arising from reﬂections off of different edges. On the other hand, PLPA is a directional antenna with 300 radiation pattern, thereby limiting the possible number of reﬂections from edges and consequently signal components at the receiver. The transmitter and receiver antennas in case of directional antenna are oriented to be in the end-ﬁre region of each other. The dimensions and material properties of different layers in 3D chip model are speciﬁed in Table I. In our evaluations, we we ignore the effects of chip packaging because, in most cases, the spacing between the top layer and packaging is signiﬁcantly higher (of the order of a few millimetres) than the thickness’s of other layers. The channel characteristics within this model are obtained by simulating the electromagnetic propagation between the antennas using Finite Differences Time Domain (FDTD) method. We ﬁx the transmitter antenna and evaluated the received signal at different placements of the receiver antenna across the chip. The propagation analysis is performed at mm-wave frequencies by exciting the transmitting antennas with a Gaussian pulse at 60GH z frequency and bandwidth covering the frequency range of 55−65GH z . The frequency band is chosen because the dimensions of the antennas radiating within this frequency range can be easily etched on top metal layer in existing CMOS processes. The proposed model and analysis can be easily extended to sub-THz and THz frequencies with little modiﬁcation. Primarily, while signal propagation in mmwave frequencies may fall into near ﬁeld/transition region, transmission at sub-THz frequencies and above is mostly in far ﬁeld region allowing conventional models of wireless communication. Using this setup, the impact of antenna placement and directionality is analysed primarily by looking at the number of different propagation paths arising due to reﬂections from both different chip layers and chip edges. IV. IM PAC T O F CH I P LAY ER S ON PRO PAGAT ION PATH S In this section, we discuss how chip components and design parameters such as the copper interconnect conﬁguration, the thickness of the passive layer and the material properties of the passive layer give rise to different propagation modes in the on-chip channel. For this purpose, we consider the two dimensional lateral view of the 3D chip model as shown in Fig. 2. To study the impact of different chip layers, we consider different conﬁgurations of the model. In the ﬁrst case, the simulation space shown in Fig. 2 consists of the silicon substrate and the dielectric layers, but without the metal interconnects. In the second case, we introduce the interconnect layer and study it’s impact on the channel. We compare the time and frequency domain behaviours of signal transmission between two antennas in both the cases, along with that of free space channel. The transmitter and receiver antennas are kept at a distance of 10mm from each other. The transmitter is excited using an impulse signal to evaluate the impulse response of the channel in both time and frequency domains and extract different signal components from the received signal. Fig. 2. Electromagnetic wave propagation in a two-dimensional on-chip environment with a silicon substrate, copper interconnect layer and dielectric layer. The propagation includes following paths: (1) direct free space wave signal from transmitter to receiver; (2) signal reﬂected off the free space and dielectric layer interface; (3) surface wave propagating through dielectric layer and (4) reﬂected waves from the metal interconnect layers 0 0.2 0.4 0.6 0.8 1 Time (nsec) 1.2 1.4 1.6 1.8 −0.02 −0.01 0 0.01 0.02 0.03 0.04 0.05 d e z i l a m r o N h t g n e r t S Continuous Copper Distributed Copper Multilayered Copper No Copper Free Space (a) −35 0 100 200 300 Frequency (GHz) 400 500 −30 −25 −20 −15 −10 | E | z ( B d ) Continuous Copper Distributed Copper Multilayered Copper No Copper Free Space (b) −3 0 100 200 300 Frequency (GHz) 400 500 −2.5 −2 −1.5 −1 −0.5 0 E ∠ z r ( n a d a i ) Continuous Copper Distributed Copper Multilayered Copper No Copper Free Space (c) Fig. 3. The transmitter and receiver are 10mm apart in a two-dimensional on-chip environment with different types of interconnect layer conﬁgurations. The ﬁgure shows (a) the time domain response (b) frequency domain magnitude response and (c) phase response of the received signal A. Time and Frequency Domain Analysis The time domain and frequency domain response of the received signal at 10mm distance from the transmitter for different conﬁgurations is shown in Fig. 3. In free space, the time domain received signal peaks at 0.036nsec as shown in Fig. 3(a), which corresponds to the distance travelled by the signal in free space between the transmitter and receiver. The frequency domain magnitude and phase responses of this signal are shown in Figures.3(b) and 3(c). The Gaussian nature of the transmitted signal is reﬂected in the magnitude response of the received signal, which is weaker than the source. When we consider the case with the silicon substrate and the dielectric layer without the metal interconnect layers, we observe that the received signal shows greater delay (0.095nsec) than the free space case. The magnitude response of the received signal in Fig. 3(b) shows approximately 5dB additional attenuation in signal strength at mm-wave frequencies. Next, we include the metal interconnect layers and consider three conﬁgurations in the simulation space as shown in Fig. 2. The three conﬁgurations are considered to evaluate the impact of different layers on the transmission characteristics. Especially taking into the account, the skin effect at different frequencies, if all the metal layers appear as single homogeneous conducting interface or if signal propagates through any of the metal layers. Firstly, as observed from Fig. 3(a), the delay with all three conﬁgurations is considerably greater than that of free space propagation, but lesser than that of when the interconnect layer is absent. Also the strength of the signal (as seen from the time domain response in Fig. 3(a)) is high and comparable with the free space case. Note that this aspect is not reﬂected in the magnitude response in Fig. 3(b) up to 500GH z . For mm-wave frequencies, the signal strength is considerably lower than that of free space attenuation. At frequencies above mm-wave frequencies (not shown here), though, the magnitude response becomes comparable to the free space results. Most importantly, the three copper conﬁgurations do not show considerable variation in the delay, strength or phase response. This implies that for all frequencies in mm-wave and sub-THz range, all the metal interconnect layers appear as single homogeneous conducting layer, which totally reﬂects the transmitted signal. Hence, no −55 0 100 200 300 Frequency (GHz) 400 500 −50 −45 −40 −35 −30 −25 −20 | E | z ( B d ) Total Direct + Dielectric Reflection Surface Wave Metal Layer Reflection (a) 0 100 200 300 Frequency (GHz) 400 500 −3 −2 −1 0 1 2 3 E ∠ z r ( n a d a i ) Total Direct + Dielectric Reflection Surface Wave Metal Layer Reflection (b) Fig. 4. (a) Magnitude and (b) Phase response of different received signal components at 10mm distance of from the transmitter in on-chip environment signal transmission exists within the substrate layer of the chip. B. Propagation Components In order to understand the detailed physics behind the observations listed above, we analyse the time and frequency domain behaviours of the received signal to extract different propagation components. The analysis is done only for the case with all layers including metal interconnects are considered. Fig. 4 shows the magnitude and phase response of each of these primary signal components. The major propagation components are - the direct wave from the source to the receiver position through free space (ray 1 in Fig. 2), the reﬂected component from the free space - dielectric layer interface (ray 2), the surface wave through the dielectric layer (ray 3) and the reﬂected component from the metal interconnects (ray 4). Due to the signiﬁcant thickness of the copper layer, the signal does not penetrate into the silicon substrate layer. When the source and receiver positions are signiﬁcantly far apart (greater than a few wavelengths), the incident signal on the free space - dielectric layer interface is close to 90◦ and hence the reﬂection coefﬁcient, Γ, is close to −1 as per (1). Γ = cos θi − ǫ2 cos θt cos θi + ǫ2 cos θt (1) Here, ǫ2 is the relative permittivity of dielectric layer. θi and θt are incident and refraction/transmission angles respectively.                                     dimensional chip model from Fig. 1 and wireless signal propagation between antennas is simulated using transient solver from CST MicroWave Studio (MWS), which is based on FDTD techniques. The dimensions of the chip are set to 15mmx15mm in the XZ plane and the dimensions of different chip layers are set as shown in Table I. We have considered both FD and PLPA antennas in our evaluations. The location of the transmitting antenna and the different receiver antenna placements evaluated are shown in the top view of the model in Fig. 5. In each evaluation, only one location of the receiver antenna is considered with ﬁxed location of the transmitting antenna. The receiver antenna, at any placement, is oriented to be in the end-ﬁre radiation region of the transmitting antenna. The placements of the receiver antenna are chosen to include different scenarios with antenna away from the edges and closer to one or more edges of the chip. We evaluate the placement and channel characteristics in terms of time domain response, signal strength, delay spread and propagation delay metrics for both the antennas. A. Time Domain Analysis The time domain response of received signal at different placements of the receiver antenna for FD and PLPA antennas is shown in Fig. 6 and Fig. 7 respectively. As can be observed, several components exist in the received signal due to reﬂections off of both chip layers and edges. While several unique received signal components exist, both antennas exhibit different characteristics due to their respective directionality. Omnidirectional antennas give rise to many reﬂections from all edges of the chip unlike directional antennas. Figures 6(a) and 7(a) shows the response at three locations along the diagonal extending from the transmitter (locations 4, 15, 21) as shown in Fig. 5. Irrespective of the antenna directionality, receiver signal at corner placement of the receiver antenna is severely impacted by a high number of signal components. Furthermore, these signal components interfere with each other, giving rise to many components even at different frequencies. Furthermore, the constructive interference between these components leads to signal components comparatively stronger at longer distances. The strength of strongest signal component at location 21 (17mm distance) is approximately twice that of location 4 (8mm transmission distance). A similar variation in time domain response of the received signal is observed in Fig. 6(b) and Fig. 7(b) for locations that are spread along the diagonal orthogonal to transmitting antenna location. Though several signal components exist for antennas placed at chip edges, the number of signal components is lesser as compared to previous case as reﬂections occur only from one of the chip edges. The number of signal components as the placement of antenna nears chip edges. The major observation from the time domain analysis of received signal with different antenna placement is - while path loss in on-chip wireless channel is signiﬁcantly high, the antenna placement results in a unique variation of loss with distance due to constructive/destructive interference between signal components. The directionality of the antenna plays a Fig. 5. Top View of the Transmitting and Receiver Antenna Placements Note that the reﬂection coefﬁcient at such large incident angles (θi ) is mostly independent of the constituent parameters of the dielectric layer. The path lengths of the direct free space wave and the reﬂected wave off the free space-passive interface are nearly equal at these incident angles and the two components destructively interfere. As a result, the direct free space wave contributes negligibly to the transmission of the signal from the source to the receiver at large distances and the received signal experiences greater delay when compared to free space conditions. One of the possible techniques for preventing the destructive interference of the direct free space wave is to introduce a meta-material based passive layer of zero refractive index ǫ2 = 0. This will result in a reﬂection coefﬁcient of 1 at the free space-dielectric layer interface as per (1), resulting in constructive interference between the components. The most signiﬁcant component of the received signal is the surface wave through the passive layer as seen in the magnitude response in Fig. 4(a). The surface wave travels slower than the free space wave and hence is associated with greater delay in the time domain. The surface wave component consists of multiple modes. The modes in the reﬂected component from the metal layers are weak at mm-wave frequencies, but become dominant at higher frequencies. From the phase response in Fig. 4(b), we can infer that the different electromagnetic wave components are not in phase with each other and so the total ﬁeld is weaker than its components. The analysis presented in this section shows the impact of different chip components on signal propagation through on-chip wireless channel. The different prevailing signal components are same irrespective of antenna structures, their properties and placement and frequencies considered for evaluation. In the subsequent section, we analyse the signal propagation in 3D environment with different antenna structures and their placement. In addition to signal components from different chip layers, this adds signal components from reﬂections off of various chip edges, further deviating channel characteristics from free space propagation. V. IM PAC T O F AN T ENNA P LAC EM EN T AND D IR EC T IONA L I TY ON PRO PAGAT ION COM PON EN T S In this section, we present the analysis of the impact of antenna placement on on-chip wireless channel and different propagation components of the received signal. The three0 2000 4000 6000 8000 10000 Time (picosec) 0 0.1 0.2 0.3 0.4 N o r m a i l S d e z r t g n e t h Location 4 Location 15 Location 21 (a) 0 2000 4000 6000 8000 10000 Time (picosec) 0 0.1 0.2 0.3 0.4 0.5 0.6 N o r m a i l S d e z r t g n e t h Location 1 Location 4 Location 7 (b) Fig. 6. Time Domain Representation of the Received Signal for Folded Dipole Antenna at (a) three locations diagonally across the chip from nearest to the farthest distance and (b) three locations orthogonal to the antenna 0 1000 2000 3000 4000 5000 6000 7000 Time (picosec) 0 0.005 0.01 0.015 0.02 0.025 0.03 N o r m a i l S d e z r t g n e t h Location 4 Location 15 Location 21 (a) 0 1000 2000 3000 4000 5000 6000 7000 8000 Time (picosec) 0 0.005 0.01 0.015 0.02 0.025 N o r m a i l S d e z r t g n e t h Location 1 Location 4 Location 7 (b) Fig. 7. Time Domain Representation of the Received Signal for Planar Log Periodic Antenna at (a) three locations diagonally across the chip from nearest to the farthest distance and (b) three locations orthogonal to the antenna role of the number of prevailing components, but has less impact as antenna placement reaches chip corners. B. Propagation Delay Fig. 8 shows the variation of ﬁrst peak delay with different antenna placements for both FD and PLPA antennas. The propagation delay, with either antennas, at any distance and placement is considerably high as compared to that of free space antenna and agrees with the observation made in Fig. 3. The line of sight component between transmitter and receiver provides negligible effect due to destructive interference from reﬂections off of chip layers. C. Delay Spread A major implication of the highly multipath propagation in wireless channel is time dispersion and phase distortion in the received signal. Delay spread gives the statistical measure of time dispersion nature of channel and is caused by signals from different propagating paths experiencing different propagation delays. Fig. 9 shows the variation of maximum delay spread with different antenna placements for both FD and PLPA antennas. The delay spread values are obtained by measuring the time difference between ﬁrst peak and last peak, whose magnitude is greater than pre-deﬁned threshold. The threshold 0.5 0 7 6 5 4 -7 3 -6 2 -5 1 -4 Length (mm)) -3 0 -1 -2 -3 -4 -5 -6 -7 -2 Width (mm) -1 01 2 3 45 6 7 D e l y a ( c e s n ) (a) 0.5 0 7 6 5 4 -7 3 -6 2 -5 1 -4 Length (mm)) -3 0 -1 -2 -3 -4 -5 -6 -7 -2 Width (mm) -1 01 2 3 45 6 7 D e l y a ( c e s n ) (b) Fig. 8. Variation of First Peak Delay of the Received Signal with Different Placements for (a) Folded Dipole and (b) Planar Log Periodic Antenna             0 7 6 5 4 -7 3 -6 2 -5 1 -4 Length (mm)) -3 0 -1 -2 -3 -4 -5 -6 -7 -2 Width (mm) -1 01 2 3 4 D e l y a p S r d a e ( c e s n ) 5 6 7 5 (a) 0.5 0 7 6 5 4 3 -7 2 -6 1 -5 Length (mm)) -4 0 -1 -2 -3 -4 -5 -6 -7 -3 -2 Width (mm) -1 0 1 23 4 56 7 D e l y a p S r d a e ( c e s n ) (b) Fig. 9. Variation of Delay Spread in the Received Signal with Different Placements for (a) Folded Dipole Antenna and (b) Planar Log Periodic Antenna is set to 40dB below transmitted signal peak, taking signal loss and noise sources inside the chip into consideration. Similar to the observations from time domain analysis, delay spread near the chip center is small and it increases signiﬁcantly as antenna placement nears the chip edges or corners. The directionality of the antenna shows a great impact on the overall variation of the delay spread, along with antenna placement. As can be seen from Fig. 8(b), delay spread for PLPA presents an exponentially variation with distance and placement of the antenna. In locations far away from the chip edges, the maximum delay spread approaches zero as (i) no reﬂections exist due to the directional nature of the antenna and (ii) any prevailing reﬂections from the edges are signiﬁcantly smaller in strength and hence provide no contribution to the overall receiver signal. For antenna locations near the chip edges or corners, several reﬂected components reach the receiver, leading to higher delay spread even with directional nature of the antenna. On the contrary, delay spread for FD is higher at all antenna placements, reaching to the highest values as antenna becomes closer to the chip corners. The high delay spread is attributed to several reﬂections from all chip edges and interference among them and the maximum delay spread at corners goes as high as 5nsec. This clearly shows that on-chip wireless channel is highly fading in nature, especially small scale fading, with high dispersion and possibly frequency components not present in transmitted signal. D. Path Loss The path loss in the wireless channel determines the power requirements for reliably transmitting and receiving the signal over wireless link. As observed from magnitude response in Fig. 3 and Fig. 4, signal propagation in on-chip wireless channel experiences high path loss. Fig. 10 shows the path loss at different placements and distances of receiving antenna for both the antennas and validates the same that path loss in on-chip wireless channel is considerably high. Between both the antennas, FD with omnidirectional pattern experiences lower path loss as compared to that of PLPA. This is due to the multiple signal paths, which constructively interfere with each other leading to stronger signal at the receiver, while using FD antennas. Hence, delay spread and path loss have trade-offs depending on the directionality and placement of the antenna. Omnidirectional antennas have better loss characteristics, while directional antennas have better delay spread and vice-versa. Another major implication of on-chip wireless channel on path loss is that it does not follow a decreasing trend with distance from the transmitter. As observed from Fig. 10, the 0 7 6 5 4 3 2 1 0 -1 -2 -3 -4 -5 -6 -7 Length (mm)) -7 -6 -5 5 -4 Width (mm) -3 -2 -1 012 34567 P a t s s o L h ( d B ) 10 (a) 0 7 6 5 4 3 2 1 0 -1 -2 -3 -4 -5 -6 -7 Length (mm)) -7 -6 -5 -4 -3 Width (mm) -2 -101 23 4567 20 P a t s s o L h ( d B ) (b) Fig. 10. Variation of Path Loss with Different Placements for (a) Folded Dipole Antenna and (b) Planar Log Periodic Antenna                 path loss at the corners of the chip (longer distances from transmitter) is smaller than that at the center of the chip for both the antennas. The high number of multiple paths at the corners of the chip reach the receiver at almost similar path times and interfere constructively leading to lower path loss at the chip corners. This path loss characteristics show an inverse and complex trend as compared to traditional loss variation with distance from the antenna. As observed from delay spread and path loss characteristics, both the antenna directionality and placement play a major role in channel characteristics and have to be considered for topological design of WNoCs. E. Impact on WNoC The major takeaways from the analysis of on-chip wireless channel and impact of the antenna placement are - (i) on-chip wireless channel is highly fading in nature due to several propagation paths and (ii) antenna placement and directionality give rise to unique variations in signal strength and delay spread. While directional antennas provide better performance in terms of channel delay spread, antennas placed at chip corners are still severely impacted. This mandates the incorporation of antenna characteristics and channel effects in the system level design and placement of transceivers in WNoCs. In addition, different transceivers experience different signal characteristics, requiring either careful tuning of each transceiver or design for the worse case. A careful analysis of all physical level parameters have be considered for both design and evaluation of WNoCs. Furthermore, the high delay spread results in small coherence bandwidth, leading to fading effects even at very low frequencies and necessitates fading resilient modulation schemes, bandpass ﬁlters at receivers, etc. V I . CONC LU S ION This work presents a study of on-chip wireless channel and the impact of antenna placement and directionality on channel characteristics. The analysis of intra chip environment shows that it varies considerably from free space and gives rise to small scale fading effects from multiple propagation paths from both chip layers and chip edges. Taking this into account, we analyse signal propagation with two antennas, one omnidirectional and other directional, to understand different multipath components and their effect on path loss, delay and delay spread of the channel. Directional antennas perform better as compared to omnidirectional antennas in terms of delay spread, but also experience high path loss. However, antenna placements reaching towards the corners or edges of the chip result in high delay spread, resulting in severe fading effects irrespective of antenna directionality. Furthermore, constructive or destructive interference between different signal components gives rise to a unique variation in signal loss with distance. While, signal strength near the chip center decays with distance, the overall loss around the chip edges can be smaller due to constructive interference between the signal components depending on the placement of the antenna. The insights provided in this work warrants and mandates the incorporation of physical level characteristics of wireless links on design and evaluation for WNoCs for achieving an efﬁcient and realistic implementation. "
Accurate Congestion Control for RDMA Transfers.,"High-performance interconnects need congestion control to deal with traffic bursts. In this paper, we propose ACCurate, a congestion control protocol that assigns exact max-min fair rates to flows, without relying on costly per-flow state inside the network. ACCurate keeps the backlogs outside of the network, protects innocent flows, and promptly recovers the flows' rates after congestive episodes. Comparisons with TCP and PAUSE-only RDMA under datacenter-resembling workloads further show that ACCurate provides up to 10× faster flow completion times. ACCurate relies on simple hardware that can be readily implemented inside switches. In our implementation, the additional circuitry needed in a 16×16 switch occupies less than 2% of FPGA resources.","Accurate Congestion Control for RDMA Transfers Dimitris Giannopoulos, Nikos Chrysos, Evangelos Mageiropoulos, Giannis Vardas, Leandros Tzanakis, Manolis Katevenis {dimian, nchrysos, emageir, vardas, janakis, kateveni}@ics.forth.gr Computer Architecture and VLSI systems Lab, Institute of Computer Science Foundation for Research and Technology Hellas Abstract—High-performance interconnects need congestion control to deal with trafﬁc bursts. In this paper, we propose ACCurate, a congestion control protocol that assigns exact maxmin fair rates to ﬂows, without relying on costly per-ﬂow state inside the network. ACCurate keeps the backlogs outside of the network, protects innocent ﬂows, and promptly recovers the ﬂows’ rates after congestive episodes. Comparisons with TCP and PAUSE-only RDMA under datacenter-resembling workloads further show that ACCurate provides up to 10x faster ﬂow completion times. ACCurate relies on simple hardware that can be readily implemented inside switches. In our implementation, the additional circuitry needed in a 16×16 switch occupies less than 2% of FPGA resources. I . IN TRODUC T ION In modern computing clusters, a large number of servers work collectively in order to complete challenging tasks. Current research examines ways to replace the expensive and power-hungry processors of today with thinner, costefﬁcient computing units [1]–[3]. These systems can consist of many tightly coupled end-nodes and fast in-node storage devices increasing the intensity of inter-server trafﬁc. Already happening in datacenters today, VM migration, checkpointing and storage trafﬁc contribute to sudden bursts, which are responsible for transient congestion. To mitigate the bad effects of congestion, many datacenters and several supercomputers still use lossy Ethernet and rely on TCP for congestion control. However, traditional TCP in lossy networks is sluggish and suboptimal, and the industry is looking for alternative solutions [4], [5]. An increasing number of datacenters and HPC installations employ Remote Direct Memory Access (RDMA)-based networks. Multi-channel RDMA engines enable user-level initiated transfers that bypass the kernel network stack and its large latency and CPU utilization [6]–[9]. Without the kernel involvement, handling congestion at the network hardware level is both necessary and a good opportunity to outperform TCP. For similar reasons, modern RDMA NICs already provide reliable packet delivery. RDMA networks typically use link-level ﬂow control in order to improve performance [10] and simplify the hardware responsible for re-transmissions. In lossless networks, congestion manifests with large in-network backlogs, which can decrease system throughput and increase the ﬂow completion time (FCT) of latency-sensitive ﬂows, even of those not contributing to congestion. Using proactive congestion control, we can prevent congestion from occurring in the ﬁrst place, by reserving resources before injecting trafﬁc [11], [12]. However, this introduces a connection setup latency, which gets more pronounced with increasing system size due to a larger number of hops. In this paper, we instead favour latency-optimized protocols that allow ﬂows to start immediately at full speed. In principle, a congestion management scheme needs to regulate the injection rate of ﬂows in order to avoid overloading network links. Additionally, it must distribute the available capacity fairly among network users and control the in-network backlogs to decrease the FCTs. To achieve these goals, in this paper we propose Accurate Congestion Control (ACCurate), a new scheme suitable for efﬁcient hardware implementation. At each network interface, we use a Remote Direct Memory Access (RDMA) engine with multiple channels. Each such channel executes a data transfer (ﬂow) and can control its transmission rate on a per-packet basis using a rate limiter. Our congestion control relies on a contention point (CP) in front of every link. On top of that, we employ a novel heartbeat mechanism that allows CPs to keep track of the active ﬂows (RDMA transfers) passing through them, without relying on expensive per-ﬂow state. We assume that time is slotted in network-scale time periods (from a few to a few tens of microseconds). In every time period, every ﬂow injects exactly one “heartbeat” that travels to the destination, effectively communicating its current transmission rate with every link on its path, as shown in Fig. 1. This protocol allows links to compute their fair shares at the end of every period. Our congestion control algorithm uses these estimates to accurately rate-limit each ﬂow to the fair share of its dominant bottleneck. To further reduce the in-network backlogs, ACCurate reserves enough headroom so that links can breath out sudden backlogs [13].With shallow in-network buffers (∼2.5KB per input port in our experiments), this can adequately control the in-network latencies. Our results show that ACCurate: (i) improves the FCTs of latency-critical ﬂows by a factor of 10 compared to TCP under typical datacenter workloads carrying a mix of bulky 978-1-5386-4893-3/18/$31.00 ©2018 IEEE many high-performance networks, such as Inﬁniband. Note though that there can be many transfers between a source and a destination; these transfers may follow different paths, following ECMP-style routing [14]. The contention points (CPs) rely on three state variables: (i) current fair share rate of the link (F SR), (ii) number of ﬂows bottlenecked on this link (M ), and (iii) total capacity of ﬂows bottlenecked elsewhere (B ). Receiving an FRP, the CP compares the incoming Current Rate(CR) ﬁeld against its current FSR: • If FSR ≤ CR, the ﬂow is considered as bottlenecked on this link; for such locally bottlenecked ﬂows, the CP equates the CR ﬁeld of the FRP packet to FSR, and increments M by one. • Otherwise, the ﬂow is considered as bottlenecked elsewhere; in this case, the FRP is left unmodiﬁed, while the CP updates B = B + CR. When the destination node receives an FRP packet, it swaps the source-destination header ﬁelds of the packet, and injects it back into the network – see Fig. 1. It also sets a special bit in the FRP header to indicate that this is an FRP-response packet. The CPs do not process FRP-response packets. Receiving an FRP response, the ﬂow source sets its rate limit to the value indicated by the CR ﬁeld. At the end of each RRP, every CP updates its current FSR as: (1) F SR = C · (1 − α) − B M where C is the capacity of the link, and α is a parameter, typically set between 0.01 and 0.1. Intuitively, the algorithm computes the bandwidth that the (M ) locally bottlenecked ﬂows would receive by a fair scheduler, considering also that the present link cannot (and should not) modify the rates of ﬂows bottlenecked elsewhere. In our algorithm, a portion of the link capacity, C · α, is reserved and not allocated to ﬂows, creating headroom to accommodate for control overhead and drain backlogs; increasing parameter α will allow the network queues to drain faster the backlogs formed during congestive episodes. Fig. 2 presents a CP processing a number of FRP messages. In this example, the RRP period is 20 microseconds, the link capacity C = 10 Gb/s, and the FSR starts at 3 Gb/s. Within the ﬁrst RRP period, the CP receives several FRPs and updates their CR ﬁeld. At the end of the FRP, the CP re-calculates the FSR according to Eq. 1: having M = 2 ﬂows bottlenecked here and B = 5 Gb/s from ﬂows bottlenecked elsewhere, the new FSR becomes (10 - 5)/2 = 2.5 Gb/s – for clarity, we assumed that α = 0. To enable prompt reaction to trafﬁc changes, the RRP must be set low. However, if we set it too small, then FRP overhead will increase. In our experiments, the length of standalone FRP packets is 20 bytes, and the RRP is set at 20 microseconds. Thus the overhead of a single active ﬂow is 20 bytes every 20 microseconds, i.e. 8 Mb/s. This overhead is manageable for up to 100 ﬂows (0.8 Gb/s) passing through a 10 Gb/s link. Fig. 1. Heartbeats (one per period per ﬂow) disseminate ﬂows current rates, allowing all links along the ﬂow’s path to keep track of the number of active ﬂows and to properly compute their fair shares at the end of each period. The current rate of each heartbeat is updated on every hop such that it does not exceed the fair share of a previously visited link, and is bounced back to the source after reaching the destination, throttling the injection rate of the ﬂow. Short-circuit feedback can shorten the reaction delay when the rate difference is large. and latency-sensitive ﬂows; (ii) throttles the offensive ﬂows by assigning them a globally feasible fair share, and (iii) converges fast (in 20-40µs in our experiments) to a max-min fair rate allocation without relying on per-ﬂow state. The remainder of this paper is organized as follows. We begin with Section II, describing the new congestion control scheme. In Section III, we use extensive computer simulations to evaluate its performance under various workloads. Then, in Section IV, we present our hardware implementation and its complexity. In Section VI we present related work, and we conclude in Section VII. I I . CONG E ST ION CON TRO L U S ING ACCURAT E ACCurate Congestion Control relies on contention points (CPs) located in front of network links and reaction points (RPs) placed at ﬂow sources, which can control the injection rate of individual ﬂows. As shown in Fig. 1, time is divided into periods or epochs, which we call Rate Re-evaluation Periods (RRPs). In every RRP, each ﬂow issues one special control packet (“heartbeat”), which we call Flow Rate Packet (FRP). It is important to note that every active ﬂow issues one FRP per period. Following the same path as the payload packets of the corresponding ﬂows, FRPs inform the CPs along the ﬂow’s path about the ﬂow’s current transmission rate. Initially the current rate of an FRP is equal to the maximum transmission rate of the ﬂow as enforced by its source rate limiter. Travelling in its path, the current rate ﬁeld of FRPs may be updated by contention points, as we describe below. We assume single-path routing for the packets belonging to the same ﬂow, i.e. all packets of a transfer (or ﬂow) follow the same path inside the network, which is typical for set equal to the current FSR. Therefore, if the source gets an FRP with DR greater than the CR, it receives a positive feedback that it can increase its current sending rate to the value indicated by the DR value. This allows a ﬂow’s rate to recover as soon as the congestion episode elapses. I I I . P ER FORMANC E EVA LUAT ION In this section, we use computer simulations in order to test the ACCurate scheme. We use event-driven simulations based on Omnet++ [15] on 10 Gb/s networks. With ACCurate, the ﬂows are initiated by ﬁring up transfers in the multi-channel RDMA engine. The RDMA engine generates network packets with maximum size of 256B, parameter α = 0.05, and RRP = 20µsec. The FRP messages travel with high priority in order to bypass backlogs formed inside the network. We also use the implementation of TCP that comes together with the INET library in Omnet. The TCP is conﬁgured with maximal receiver buffer capacity of 21KB, no delayed ACKs, maximum segment size of 1500 Bytes and TCPReno congestion control. We test lossless TCP, i.e. TCP in a network with ﬂow control that prevents packet drops, and two types of lossy TCP, one performing tail-drop at switch inputs and one at switch outputs. A. Solving the parking lot problem In this experiment, we examine the fairness properties of ACCurate under the parking lot benchmark [16]. We consider a fat-tree network as shown in Fig. 3(a). Two hosts (H1 and H2) connected to the same leaf of the tree send packets (ﬂows f1 and f2) to H4 which resides in a remote leaf. These two transfers cross the same link due to routing conﬂict (e.g. caused by ECMP or d-mod-k routing). H4 also receives packets from ﬂow f3, coming from a local host (H3). All ﬂows initially send packets at full link speed. Ideally all ﬂows should get an equal share of the link connecting to H4. First we examine how lossless TCP performs in this scenario. Our results are presented in Fig. 3(b). As can be seen, this conﬁguration fails to deliver desired rates: the remote ﬂows get almost 2x lower bandwidth. Unfortunately, popular hardware congestion control solutions, like DCQCN [8] and QCN [5] also experience similar problems, because the remote ﬂows, facing two bottlenecks, can receive two times more congestion notiﬁcations. With ACCurate (Fig. 3(c)), the rate allocation is globally fair. All ﬂows receive their max-min fair share 10/3 Gb/s – they get slightly less than that because α = 0.05. B. Victim ﬂow protection, max-min fairness, and rate recovery In this experiment, we conﬁgured three RDMA ﬂows in the network topology depicted in Fig. 4(a) –for clarity, only a subset of the network links are shown. Here we test three key properties of ACCurate: • protect victim ﬂows that do not contribute to congestion, • redistribute the unused bandwidth in a max-min fair way, • recover the rate of previously congested ﬂows fast. Fig. 2. Timeline of a CP processing a number of FRPs within an 20µsec RRP period; capacity units are measured in Gb/s. A. Short-circuit notiﬁcations for prompt reaction If a ﬂow starts transmitting with a very high rate, before it receives an FRP reply, it may cause congestion on all links that it passes through. To slow down such a sudden ﬂow, we use short-circuit notiﬁcations: when a CP observes a large difference between the current rate of a ﬂow and the FSR variable of the link, it immediately generates an FRP reply and forwards it towards the source. As shown in Fig.1, the source can reduce the ﬂow’s rate without having to wait for the destination response, which will arrive after a network round-trip time (RTT). Note that the CP always propagates the FRP to the next switch, in order to allow the ﬂow discover other bottlenecks along its path. Thus, multiple switches may generate a short-circuit notiﬁcation for the same FRP. B. Flow init & ﬂow stop messages When a new ﬂow commences, the CPs inside the network are still unaware of its presence. What will happen in this case is that within the next RRP the ﬂow will issue an FRP. Subsequently, the affected CPs will update their FSR at the end of this RRP, and will start throttling ﬂows in the next RRP, taking into account the presence of the new ﬂow. This reaction latency can induce large backlogs inside the network. To accelerate this procedure, ﬂows inject a ﬂow-init FRP together or in advance of the ﬁrst network packet. Flow-init FRPs trigger a prompt update of the link’s fair share, and may also cause short-circuit notiﬁcations. If a ﬂow wants to stop transmitting, it sends a Flow Stop FRP message. With this message, it lets the CPs know that its bandwidth can be distributed right on the next RRP, so that ﬂow rates can reach an optimal state, and utilization can reach maximum. C. Ultra-fast rate recovery So far we have described the ACCurate mechanisms used to throttle congestive ﬂows. In order to allow throttled ﬂows to recover their rate once congestion has settled, ACCurate uses a positive feedback mechanism: FRP messages also carry a Desired Rate (DR) ﬁeld, which is set by the FRP source and lets the CPs know at what rate the ﬂow wants to transmit, regardless of its current rate (CR). The CPs look at the DR ﬁeld of FRP packets, and if it is smaller than the FSR, they pass the DR as is; otherwise, the DR ﬁeld of the packet is (a) Topology & trafﬁc scenario (b) Lossless TCP results (c) ACCurate results Fig. 3. Performance under the parking lot scenario. (a) Topology and trafﬁc scenario (b) Results for Pause-only (c) Results for ACCurate Fig. 4. Trafﬁc scenario and time-series depicting the evolution of ﬂows rates. Flows f1 and f4 are sourced from host H1, but diverge inside the leaf switch. Flows f2 and f3 source from hosts H3 and H4, respectively, in the leaf switch; they travel through the same link with f4 to the spine, where they all diverge to different destinations. The active periods of the ﬂows are described below. • f1 and f2 are active throughout the experiment, • f3 is activated at time 170 µsec, and stays on till the end, • f4 is active from 370 to 570 µsec. Before 170 µsec, only ﬂows f1 and f2 are active. Because these ﬂows have disjoint paths, they can reach full link rate. Problems emerge with ﬂow f3 (170 µsec), which together with f2 congest the leaf to spine link. As shown in Fig. 4(b), in a network with link-level ﬂow control (Pause-only), once f3 becomes active, f2 and f3 get 5 Gb/s because they share a link. Subsequently, a backlog is formed in front of this link, which triggers link-level ﬂow control that moves the backlog upstream at the network interfaces of H2 and H3. However, the backlog does not reach H1, which sources ﬂow f1, because the path of f1 is disjoint from that of f2 and f3. Effectively, the rate of f1 is unaffected. Flow f1 is impacted once ﬂow f4 is activated at time 370 µsec. Since f2, f3 and f4 share a link, they all get a rate of 3.33 Gb/s. Now, due to link-level ﬂow control, the backlog of f4 reaches H1, wherein it strikes the victim ﬂow f1, limiting its rate to 3.33 Gb/s. This happens due to head-of-line blocking inside the queue that f1 shares with f4: this queue drains f4 packets at a rate of 3.33 Gb/s, effectively limiting the departure rate of f1 packets as well. Next, we examine whether ACCurate can correct this behavior. Our results are shown in Fig. 4(c). When the congestion begins at 370 µsec, ﬂow f1 is initially throttled to 5 Gb/s. This is expected because it shares a link with f4. Note that ACCurate removes the head-of-line blocking that could get f1 down to 3.33 Gb/s; hence, ACCurate protects the victim ﬂow. In addition, ACCurate allows ﬂow f1 to take advantage of f4’s bottleneck, and use the excess bandwidth: one RRP time after its appearance, ﬂow f4 informs the CP in front of the link connecting H1 with the leaf switch that its current rate is 3.33 Gb/s. Effectively, the CP can in the next RRP period update the desired rate (DR) ﬁeld of f1’s FRPs to 6.66 Gb/s. In this example, ACCurate converges to max-min fair rates in two RRP periods, i.e. just 40 µsec.1 When f4 becomes inactive again at 570 µsec, we want the rates of the other ﬂows to increase to the levels they were before 370µsec. With ACCurate, the rate recovery using the DR positive feedback is very fast – it takes only two RRP periods. With Pause-only, the ﬂows can recover their previous rates only after the network backlogs have drained, 1 The exact rates are a bit lower due to α begin equal to =0.05. (a) Topology: 2 Blades hosting 4 Multi-Node Groups, each of which hosts 4 nodes. N* nodes are also responsible for network communication. (b) Average Flow Completion Time (FCT) for critical ﬂows. (c) Slowdown relative to ACCurate for the 99.9th percentile of FCTs: All ﬂows. (d) Slowdown relative to ACCurate for the 99.9th percentile of FCTs: Critical ﬂows. Fig. 5. Simulation results under datacenter-resembling workload in multinode topology. which takes approximately 150 µsec. With ACCurate, these in-network backlogs are avoided, and the positive feedback from the network allows them to recover the rate limiters. C. Flow completion times under datacenter workloads In this experiment, we use an adversarial trafﬁc pattern, inspired by modern datacenter workloads. As described in [4], datacenter workloads exhibit a heavy-tail ﬂow size distribution. In our experiments, the bursty ﬂows (20%) have a size of 1 MB, whereas the small, latency-critical ones (80%) have a size of 20 KB. We use uniformly-destined trafﬁc, and we vary the injection load.We also run experiments with richer packet size distributions (additional sizes) and the main tradeoffs did not change. We use the hierarchical direct topology shown in Fig. 5(a). Each computing node generates trafﬁc either from an RDMA interface or an Ethernet interface (TCP tests) at 10 Gb/s. Four (4) computing nodes are grouped together in a MultiNode Group (MNG). The nodes inside each MNG are fullyconnected, using 10 Gb/s High Speed Serial (HSS) links. One node in each MNG additionally implements inter-MNG connectivity (shown as N*). At the inter-MNG level, sets of four (4) MNGs form a Blade group. Each blade/group has its MNGs fully-connected through HSS links. In this experiment, we conﬁgured two blades, connected using two HSS links. In Fig. 5(b) we depict the latency of critical (small) ﬂows. For uniformly destined trafﬁc, the links connecting the two blades are congested once we reach the 0.25 injection load per node – half of the trafﬁc is local within each blade, therefore the 16 local nodes generate 16 × 10 2 Gb/s trafﬁc that crosses the two links, generating a 4x overload on each one. As can be seen, ACCurate outperforms all other variants. The TCP variants perform quite poorly even at very low loads. As we approach the saturation point, ACCurate delivers 5x to 10x lower latency than all other schemes. Next, we measure the tail latency. In these results, ACCurate’s performance shines thanks to precise rate limiting that keeps backlogs out of the fabric. In Fig. 5(c), we plot the slowdown in 99.9-th percentile latency for all schemes compared to ACCurate. As can be seen, lossy TCP performs poorly even at low loads. Pause-based schemes (with or without TCP) perform better at low loads, but their performance suffers under high load. By avoiding backlogs and congestion spreading, ACCurate reduces the tail latency of all ﬂows. In Fig. 5(d), we consider only small latency-critical ﬂows. As can be seen, the slow down is now considerable starting from low loads. Only PAUSE keeps up, but again the slowdown is almost 4x even at low loads. Overall, ACCurate reduces the latency variations that contribute to large tail latency. IV. HARDWAR E IM PL EM EN TAT ION One concern with congestion control mechanisms is that they often require complex hardware blocks inside the network or interconnect [17]. ACCurate also implements some functions inside network nodes, but does not to use expensive per-ﬂow state. To demonstrate the simplicity of hardware block needed, we have described the CP functionality in Verilog, and placed and routed the design in a Zynq Ultrascale+ FPGA. The contention points sit on every switch output. Their purpose is to throttle the monitor FRP packets, and to update their CR and DR ﬁelds. The packets that pass through a CP are divided into four groups: • Normal, payload packets that pass through the switch are not processed. • FRPs that are processed as explained in Section II and further detailed in this section. • FRP-responses, i.e. FRPs that reached their destination and are now traveling back to their source, are not processed. • Flow-init and ﬂow-stop messages that inform the network about a new or a ﬁnished transmission and need special processing. The hardware circuit that implements the CP is coarsely depicted in Fig. 6. Our implementation is generic and can be made to work for any custom interconnect or popular Layer-2 network, such as Ethernet or Inﬁniband, which may encode the FRP packet ﬁelds differently. We deliberately ignore needed signals, such as start-of-packet, end-of-packet, and reset. Inputs to our circuit are the ﬁelds of an FRP packet and the RRP clock. The CP also maintains variables M , B and FSR, as explained in Section II. The circuit estimates the FSR of the link, and updates the CR and DR ﬁelds of FRP packets. Upon receiving an FRP packet, a CP ﬁrst determines if it is a ﬂow-init or a ﬂow-stop message. New ﬂows must not wait for an RRP clock event in order to get a rate assigned to them; nstead, the CPs may give them a feasible rate immediately. Unfortunately, in every RRP clock event, the M and B variables of the CP are reset to zero (0) and therefore the CP cannot assign a legitimate rate to these new ﬂows without waiting for all ﬂows to send their FRPs, which will only happen at the end of the RRP. To counter this, the CP keeps the old (previous RRP) FSR, M and B values in registers. It then assigns a new sending rate to the new ﬂow, considering what its rate should be if it was active in the previous RRP: the CP compares the CR ﬁeld with the old FSR, and correspondingly updates the old M and B values. It then calculates a new FSR using these updated values. If the message is a forward FRP, the CP compares the current FSR with the CR. If FSR ≤ CR, the CP increases M and updates the CR ﬁeld of the packet with FSR. Otherwise, it increases B by the CR value of the packet. At the end of the RRP, the CP calculates the new FSR value and keeps the old values M , B and FSR variables in its “old” registers; these are not displayed in the ﬁgure for clarity. We have run the place and route process for the CP circuit for a Zynq Ultrascale+ FPGA [18] using Vivado. Our design occupies less than 0.1% of the FPGA (286 LUTs and 90 ﬂipﬂops), and can achieve the frequency of 156.25 MHz that we targeted in our implementation; thus, our design can handle a new FRP message every 6.4 nanoseconds. In a crossbar switch Fig. 6. Outline of RTL implementation of contention points. consisting of 16 ports, the CPs will occupy less than 2% of the FPGA resources. A. Handling corner cases Our circuit also handles a number of corner cases which we describe next. The aggregate capacity (B ) of ﬂows bottlenecked elsewhere may exceed the total capacity(C ) when many ﬂows start transmitting simultaneously, thus not allowing the system to realize where the real bottleneck is. In this case, Eq. 1 yields a negative FSR. To handle this case, we temporarily set the FSR equal to C divided by the total number of ﬂows passing through. Another corner case is when M = 0. In this case, Eq. 1 will attempt division by zero. We handle this as follows: if B = 0, then no ﬂow is active; if instead B (cid:54)= 0, we use the maximum of the bottlenecked elsewhere rates, bmax (the circuit keeps this in a register, and resets it in every RRP), set M = 1, and decrement B by bmax . B. Reaction points The reaction points in ACCurate can be implemented on the multi-channel RDMA sources residing on network interfaces. Below we outline their operation, leaving a more detailed description for future work. We consider that every ﬂow (i.e. transfer) is allocated a separate RDMA channel. The reaction point injects an FRP message for each active channel per RRP, and limits its injection rate to not exceed the value speciﬁed by FRP responses. In principle, it is preferable to limit ﬂow rates on a per packet-basis (packet packing), allowing only small bursts (1-2 packets) to jump into the network. Rate limiting can also be combined with ﬂow scheduling. The idea is to maintain a next-service-time (NST) per ﬂow, and block a ﬂow with NST > T , where T is the current time. After serving a packet from R , where S is the packet size and R is the current rate limit. A packet scheduler selects the ﬂow (i.e. DMA channel) with the minimum NST; if this minimum NST exceeds the current time, then no rate-limited ﬂow is eligible for service. a ﬂow, N ST = T + S V. D I SCU S S ION In this section, we discuss a number of design decisions taken by ACCurate. Reaction control delay: With ACCurate, ﬂows adjust their rates on RRP. With a relatively short RRP (a few tens of microseconds, depending on the number of ﬂows expected – 20µsec in our experiments), we maintain a fairly fast reaction to network changes. The added beneﬁt is that allowing rate adjustments only on RRP intervals, we maintain accurate rate assignments, which take all ﬂow rates into account. Multi-path routing: ACCurate assumes that all packets of a rate-controlled transfer follow the same path. To enable multipath routing, different transfers between a pair of endnodes may use different paths. In order to avoid sudden path overshoots caused by large transfers, we can break the transfer in smaller ones that follow a single path each. Congestion control for Network-on-chip (NOC): In this paper, we described and evaluated ACCurate for off-chip interconnects. However, ACCurate is also applicable for onchip networks, implementing wormhole or virtual cut-through routing. NOCs traditionally adopt RDMA mechanisms for data transfers. Typically, they have smaller buffers, but this does not mean that congestion control is not needed [19], [20]. Unlike QCN and DCQCN, ACCurate does not rely on backlog size to quantify congestion, and therefore is applicable for ultra-shallow buffer networks, like NOCs. Furthermore, it is straightforward to implement a global clock needed for the RRP in NOCs. Inactive or small/slow ﬂows: Flows that are too slow or small may not send FRP messages, as they do not interfere with other ﬂows. For instance, one may allow ﬂows smaller than 4KB to proceed without sending FRPs. The same can be conﬁgured for ﬂows with small injection bandwidth. In this way, the CPs will not be informed about the current rate of these ﬂows. As described in [4], small ﬂows contribute little in overall trafﬁc, as most of network trafﬁc comes from large bulky ﬂows, which we control with ACCurate. Max-min fair versus shortest-job-ﬁrst scheduling: Our protocol allocates equal shares of the link capacity to competing ﬂows using processor sharing (PS). A competing paradigm is shortest-job-ﬁrst (SJB) scheduling, which can reduce the ﬂow completion times [11]. We consider that when two large storage / data ﬂows conﬂict on a link, it is preferable to share the bandwidth half-half at all times (PS), rather than having one waiting for the other to complete (SJB). Eliminating the in-network backlogs using rate regulation already reduces the ﬂow completion times. In addition, we do not rate-limit small messages. In future work, we will consider how to further optimize the FCT of latency critical ﬂows. Reducing the FRP overhead: As said previously, the overhead of FRP is one FRP per RRP per link. For an FRP size of 20 Bytes and RRP of 20 µsecs, the trafﬁc per rate-limited ﬂow is just 8 Mb/s. As described in [4], a large portion of the ﬂows found in a datacenter are relatively small, and we do not have to limit their rate. To further reduce the overhead, one may increase the length of the FRP period, taking into account that doing so will increase the reaction delay of the scheme. Also, one may try to send the FRP information using available bits in the header or footer of a packet. Encoding the current and the desired rates using 8 bits for each, and adding 4 more bits to encode the type of packet (FRP, FRP-response, start/end ﬂow), we need 20 bits. The downside is that an FRP may delay due to backlogs inside the network. Broadcasting a common RRP clock event: Our protocol assumes that a central clock generates synchronous events to all modules with a frequency of 50kHz for an RRP of 20 µsecs. In general LAN networks, the Precision Time Protocol (PTP) is already approaching this level of accuracy [21]. For rack-scale interconnects, a custom protocol can be used to achieve this distribution of a common clock. For instance, the BXI interconnect from Atos broadcasts a common clock with an accuracy of 2 µsecs [22]. As said above, broadcasting the RRP clock event is straightforward in NOCs. Implementing CP circuits using Open-Flow: The CP logic required inside switches by ACCurate is very simple. If custom hardware is not an alternative, it may still be possible to implement it using advanced, programmable OpenFlowcapable switches [23]. V I . R E LATED WORK Quantized Congestion Notiﬁcation control (QCN) has been proposed for lossless Converged Enhanced Ethernet (CEE) [5]. QCN and its derivatives detect congestion based on buffer queue occupancy and occupancy variation inside routers, and send notiﬁcation messages to the sources in order to slow down in the event of congestion. One big advantage of our scheme is that it is more proactive: we do not wait for queues to built up in order to detect congestion and throttle ﬂows. In addition, in our scheme we use absolute rate assignments, whereas QCN uses a stochastic process in order to decide which ﬂow to throttle, and relies on additive-increase multiplicativedecrease (AIMD) feedback control in order to adjust the ﬂows’ transmission rates. The combination of the two bring uncertainty which can have detrimental effects on performance for certain workloads. Like ACCurate, QCN generates congestion notiﬁcations; however, QCN decides to throttle a random subset of the incoming ﬂows, which can lead to grossly unfair schedules, whereas the present scheme selectively activates the prompt feedback paths, exactly on the cases when this is mostly needed, producing fair allocations. Today, the datacenters are dominated by TCP congestion control. Recently, lossless TCP has been tested as a way to avoid re-transmission latency and effectively to optimize the latency of critical ﬂows [10]. Lossleness is also a key requirement of RDMA networks. Our results in this paper showed that ACCurate provides up to 10x faster completion times than lossless TCP. Other TCP derivatives have also been proposed that reduce the backlogs created inside the networks, thereby reducing latency [4]. The software stack of TCP however places a large overhead on communication, which we override completely using RDMA engines. Recently, DCQCN was proposed for hardware-level congestion control in L3 datacenter networks [8]. DCQCN combines features from QCN and DCTCP, providing hardware rate limiters, and can be used for RDMA transfers that bypass the linux network stack. One drawback of the scheme is that it has a large number of control variables, which may be difﬁcult to tune in different environments. In addition, DCQCN cannot provide max-min fair allocation, thereby needlessly penalizing some ﬂows in multi-bottleneck (parking lot) scenarios, which ACCurate handles smoothly. Similar to QCN, DCQCN does not use positive feedback, and relies on additive increase, which typically takes longer to recover rates. In this paper, we showed that with ACCurate, ﬂows can recover to new fair share rates within one or two RRP times. Another alternative is the Inﬁniband Congestion Control. This congestion control uses a table at sources to store congestion signals from the network, and uses this feedback to throttle ﬂows. However, Inﬁniband has a lot of parameters that may be difﬁcult to tune correctly with the risk of being unfair [24]. A proactive congestion control scheme is presented in [12]. This scheme relies on per-ﬂow state inside the network, and uses a complicated formula to compute link fair shares, without demonstrating its implementation. In addition, in [12] packets carry a rate demand per link, which creates additional overhead. In contrast, ACCurate relies on minimal hardware resources, making it applicable in demanding environments. V I I . CONC LU S ION Modern high-speed networks shift away from traditional TCP-based communication, adopting RDMA transfers in order to achieve lower latency. These networks need hardware-based congestion management in order to deal with saturation trees. In this work we described ACCurate, an efﬁcient and fair congestion control scheme that can be readily implemented in hardware. Our simulation results demonstrate that ACCurate reacts promptly to congestion, throttles the offensive ﬂows by allocating bandwidth in a max-min fair manner, and reduces the ﬂow completion time by more than one order of magnitude under demanding realistic workloads. In our implementation for a Xilinx Ultrascale+ FPGA, the CPs needed at the outputs of a 16-port switch occupy less than 2% of the FPGA resources. V I I I . ACKNOW LEDGM ENT This work is supported by the European Union’s Horizon 2020 Research and Innovation Programme, ExaNeSt, under grant agreement No. 671553, as part of the EU FETHPC initiative. "
NoC-Based Support of Heterogeneous Cache-Coherence Models for Accelerators.,"On-chip shared memory is the primary paradigm for multi-core SoC designs and poses the most critical challenges to their scalability. Choosing the appropriate coherence model for accelerators not only can improve the overall system performance, but can also decrease energy consumption by reducing the accesses to DRAM. We propose an extension of a standard directory-based cache-coherence protocol and present its design as part of a scalable memory hierarchy implemented over a NoC. To evaluate our contribution we designed a many-accelerator SoC architecture that can support three main cache-coherence models for accelerators: non-coherent, last-level-cache-coherent, and fully-coherent. This SoC can run Linux SMP with split last-level cache and multiple DRAM controllers. Our FPGA-based experiments show that the optimal cache-coherence selection varies at run-time, based on the running accelerators and the memory footprint of the applications. Therefore, we support runtime selection of the cache-coherence model for each accelerator, as an alternative to a design-time decision.","NoC-Based Support of Heterogeneous Cache-Coherence Models for Accelerators Davide Giri Department of Computer Science Columbia University New York, NY, USA davide giri@cs.columbia.edu Paolo Mantovani Department of Computer Science Columbia University New York, NY, USA paolo@cs.columbia.edu Luca P. Carloni Department of Computer Science Columbia University New York, NY, USA luca@cs.columbia.edu Abstract—On-chip shared memory is the primary paradigm for multi-core SoC designs and poses the most critical challenges to their scalability. Choosing the appropriate coherence model for accelerators not only can improve the overall system performance, but can also decrease energy consumption by reducing the accesses to DRAM. We propose an extension of a standard directory-based cache-coherence protocol and present its design as part of a scalable memory hierarchy implemented over a NoC. To evaluate our contribution we designed a many-accelerator SoC architecture that can support three main cache-coherence models for accelerators: non-coherent, last-level-cache-coherent, and fully-coherent. This SoC can run Linux SMP with split lastlevel cache and multiple DRAM controllers. Our FPGA-based experiments show that the optimal cache-coherence selection varies at run-time, based on the running accelerators and the memory footprint of the applications. Therefore, we support runtime selection of the cache-coherence model for each accelerator, as an alternative to a design-time decision. I . IN TRODUC T ION As systems-on-chip (SoCs) integrate ever more components and become distributed systems [1], the network-on-chip (NoC) is superseding the more traditional interconnects [1]– [3]. There are many examples of this shift both in industry and academia [4], [5]. While NoCs provide more scalable on-chip communication, shared memory is the dominant programming model of multicores [6]. Shared memory and cache coherence pose critical scalability challenges, which have been widely addressed for NoC-based homogeneous multicores [7], [8]. Despite the NoC being identiﬁed as a proper and scalable solution also for heterogeneous SoCs [9], [10], the interaction between accelerators and a cache hierarchy distributed over a NoC has received limited attention by researchers. We identiﬁed three main cache-coherence models for accelerators: non-coherent, fully-coherent, and LLC-coherent [11], [12]. In the non-coherent model the accelerator operates through direct-memory access (DMA), bypassing the caches. Conversely, with the fully-coherent model, memory requests must be coherent with the entire cache hierarchy. This approach can endow accelerators with a private cache, thus This work was supported in part by DARPA (C#: R0011-13-C-0003), the National Science Foundation (A#: 1546296) and C-FAR (C#: 2013-MA2384), an SRC STARnet center. 978-1-5386-4893-3/18/$31.00 ©2018 IEEE Fig. 1. 4x4 instance of our NoC-based architecture with focus on the content of accelerator, memory, and processor tiles and their interface with the NoC. Numbers 1 to 6 refer to separate physical NoC planes. requiring no modiﬁcations to the coherence protocol. The third model represents an intermediate position: memory requests issued by the accelerator are coherent with the LLC, but not with the private caches of the processors. In this case, DMA transactions address the shared LLC, rather than external memory. Cota et al. were the ﬁrst to describe the LLC-coherent model for accelerators and to estimate its beneﬁt in simulation [11]. To date, however, neither a cache-coherence protocol nor an SoC architecture have been presented to support LLCcoherent accelerators over a NoC. To this end, we propose an extension of the MESI directory-based protocol and integrate LLC-coherent accelerators into the SoC architecture illustrated in Fig. 1, which leverages the concept of tile-based architecture over a packet-switched NoC [13]–[15]. This is the ﬁrst NoCbased system that allows all three models of coherence for accelerators to coexist and operate simultaneously with support for run-time selection. Furthermore, by supporting atomic test-and-set and compare-and-swap operations over the NoC, we can run multi-processor and multi-accelerator applications on Linux SMP. With a set of experiments on FPGA, we prove the beneﬁts of selecting the coherence model at run-time. The experimental results conﬁrm that the LLC-coherent model signiﬁcantly reduces, and in some cases eliminates, the number of accesses to main memory. In addition, LLC-coherent accelerators can have better performance than non-coherent ones, as long as the accelerated application doesn’t incur thrashing of the LLC. The fully-coherent model can be the optimal selection when the memory footprint ﬁts in the accelerator’s private cache. Non-coherent DMA is optimal, instead, whenever accelerators operate on large amounts of data. I I . NOC -BA SED ARCH I T EC TUR E Fig. 1 shows a 4-by-4 instance of our scalable SoC. Similarly to other tile-based architectures [14], [15], each tile can host a general-purpose processor, an accelerator, or an interface with main memory. Our design is based on an instance of Embedded Scalable Platforms (ESP) [15], [16]. We enhanced the ESP architecture by adding a cache hierarchy to support both symmetric multiprocessing as well as loosely-coupled LLC-coherent and fully-coherent accelerators, alongside the more typical non-coherent loosely-coupled accelerators. SoC Integration. A processor tile contains a single core and a private write-back L2 cache. The latter implements the directory-based coherence protocol over the NoC, thus decoupling the processor-speciﬁc L1-cache design from the rest of the system. For instance, our processor tile hosts a Leon3 core [17], which is tightly integrated with the writethrough L1 caches. Memory requests issued over the local bus are intercepted by the L2, whereas memory-mapped I/O operations are directly forwarded to the NoC. Fig. 1 shows the local ports of the NoC planes used to route all types of messages in the system. Memory tiles are the access points to off-chip memory. They feature a memory controller and an LLC tightly coupled with a directory. One of the memory tiles hosts shared system resources, including the interrupt controller, the system timer, and a debug interface. The accelerator tile can host any accelerator complying with a simple interface. This consists of memory read and write ports, conﬁguration ports, and a done signal. As shown in Fig. 1, the tile includes a set of memory-mapped conﬁguration registers. These are accessed by the operating system through a device driver. The latter, in turn, is invoked by an application to ofﬂoad a task. Some registers are accelerator speciﬁc and hold the conﬁguration parameters, including the size of the workload. Others are common to all accelerators and hold information such as the page table handle, or the selected cache-coherence model. Based on these registers, a small TLB translates the accelerator’s requests to accesses in physical memory and passes the transaction information to either the DMA controller or the private cache. We leverage a fast translation scheme based on a scatter-gather list that partitions the accelerator space in large equally-sized pages and generates a small page table [16]. Thus, accelerator tiles handle virtual memory without interrupting the processor cores. System Invariants. For functional correctness across all coherence models, our system maintains two invariants. First, we use locks to enforce mutual exclusion: during the execution of any accelerator no other component can access its data. Second, during the execution of non-coherent accelerators, we ensure that there exists only a single copy of the data, thanks to an efﬁcient ﬂush mechanism. This second invariant is relaxed for LLC-coherent accelerators, because the data can be present both in DRAM and in the LLC. Indeed, most of the lines stored in the LLC are expected to be valid, if the software application was working on it before invoking the accelerator. Note that only the ﬁrst invariant is necessary for fully-coherent accelerators. These can access data owned or shared by any private cache in the system, but mutual exclusion is still necessary because they do not share lock variables with the operating system and cannot perform atomic operations. NoC Planes. We designed a packet-switched NoC with a 2D-mesh topology and look-ahead dimensional routing. In order to prevent protocol deadlock and provide sufﬁcient bandwidth for both coherence and DMA messages, the NoC has multiple physical planes [18]. The tiles inject packets into each plane based on the type of message. Every hop takes a single clock cycle, because arbitration and next-route computation are performed concurrently. The channel width is conﬁgurable, but it is ﬁxed at 32 bits for this work. Directory-based protocols impose two main requirements on the interconnect to avoid deadlock: point-to point ordering and three separate channels for request, forward and response messages [19]. Hence, we devote three NoC planes to the cache coherence messages (planes 1, 2, 3 in Fig. 1). For the same reason, we route DMA requests and responses between accelerator tiles and memory tiles on two different planes (see planes 4 and 5 in Fig. 1). While we could reuse the coherence planes as DMA planes, we prefer to allocate additional ones to increase the communication bandwidth. Finally, the plane labeled IO/IRQ is dedicated to short packets for interrupts and memory-mapped I/O. Interrupts are not broadcasted: the interrupt controller receives all interrupts and injects in sequence one speciﬁc interrupt-level message for each processor that must be notiﬁed. While interrupt handling on the NoC inevitably incurs higher latency than on a bus, hard real-time deadlines can still be met. Cache Design. Our cache hierarchy implements a MESI directory-based protocol over two levels of caches: a private and write-back L2 cache present on every processor tile (and, optionally, on any accelerator tile) and a combined LLC and directory that can be split across multiple memory tiles. Each partition of the LLC and directory handles requests for the same address ranges pertaining to the memory controller placed on the tile. Since the LLC and directory are tightly coupled together, we refer to them interchangeably to indicate the combination of the two. We designed the caches in SystemC and implemented them with high-level synthesis (HLS). All caches are conﬁgurable in the number of sets and ways, as well as in the number of sharers and owners. For our experiments, D IR EC TORY CONTROL LER ’ S EX T END ED MES I PROTOCO L . TABLE I GetS read mem, Excl. Data to req, owner = req / E Excl. Data to req, owner = req / E Data to req, sharers += req GetM read mem, Data to req, owner = req / M Data to req, owner = req / M Data to req, Inval. to sharers, owner = req, clear sharers / M Fwd-GetS to owner, sharers+=req+owner, clear owner / SD Fwd-GetS to owner, sharers+=req+owner clear owner / SD Fwd-GetM to owner, owner = req / M Fwd-GetM to owner, owner = req I V S E M SD stall EID stall MID stall stall stall stall REQUESTS PutS PutM Evict Put-Ack to req Put-Ack to req Put-Ack to req Put-Ack to req Put-Ack to req, sharers -= req / V (if last sharer) Put-Ack to req, sharers -= req / V (if last sharer) Put-Ack to req, if req is owner: - clear owner / V Put-Ack to req write LLC, Put-Ack to req, if req is owner: - clear owner / V write LLC, Put-Ack to req, if req is owner: - clear owner / V [write mem] / I [write mem], Inval. to sharers, clear sharers / I Fwd-GetM to owner, clear owner / EID Fwd-GetM to owner, clear owner / MID Put-Ack to req, sharers -= req Put-Ack to req, sharers -= req stall Put-Ack to req, sharers -= req Put-Ack to req, sharers -= req Put-Ack to req, sharers - = req Put-Ack to req, sharers -= req DMA REQUESTS Read Write read mem, [read mem], write LLC, Data to req / V / V Data to req write LLC RESPONSES Data Inv-Ack [write mem] / I write LLC, / V (if no sharers), / S (otherwise) write mem / I write mem / I (i.e. for each iteration of the main loop) is exactly 4 cycles for a 16-ways LLC. The miss penalty, due to off-chip memory access, adds up to the ﬁxed delay, when applicable. The cache storage is also implemented through banked SRAMs, offering up to 16 ports. Hence, a whole set can be read in a single clock cycle when the LLC has at most 16 ways. The design of the private L2 cache is similar to the LLC, with the addition of a register bank that keeps all information about lines that are currently in a transient state. These registers support a conﬁgurable number of ongoing requests, for which the cache is only updated when a line transitions to a stable state of the MESI protocol. We integrate the caches into their corresponding tile as follows. In the case of fully-coherent accelerators, read and write transactions are directly translated into cache-speciﬁc requests. On processor tiles, load and store operations are issued on a local bus, where the L2 cache acts as a slave device. The LLC, instead, acts as a master of a local bus on the memory tile and issues operations to address the memory controller. The ﬂush mechanism relies on memory-mapped registers that allow the device driver to request a selective ﬂush of any cache in the hierarchy and check for completion before invoking an accelerator. When a ﬂush is due, the private caches wait for the processor’s L1 to be ﬂushed and then start a synchronized ﬂush across all cache levels and all processor cores to guarantee the consistency of shared data. Note that only the levels of caches selected by the device driver are ﬂushed and the duration of the ﬂush phase becomes a negligible overhead when the workload of the accelerator is large enough. Fig. 2. Cache-coherence state and exchanged messages. we target an FPGA but the system can be synthesized for an ASIC target as well. Fig. 2 shows all the message types that can be sent or received by the caches. Those that involve the DMA controller are only required by LLC-coherent accelerators; they are not part of the regular MESI protocol. The cache lines and all the meta-data are stored in SRAM banks and register ﬁles. The directory controller is speciﬁed as a single SystemC process. At each iteration of the main loop, the controller checks with a ﬁxed priority if there is any incoming message or if there is any previously stalled message that can now be processed. If this is the case, then the controller reads a whole cache set and, if needed, it updates a cache line and its meta-data. The controller might also send out one or more messages according to the protocol. The maximum latency for handling a request I I I . CACHE -COHER ENC E PROTOCO L We modiﬁed a classic MESI directory-based cachecoherence protocol, as deﬁned by Sorin et al. [19], to make it work over a NoC and, most importantly, to support LLCcoherent accelerators. The extension to the cache coherence protocol does not affect the private caches, but rather only the LLC. A. Directory controller Table I shows in full the new protocol for the directory controller. The format is similar to the original table [19]. The colored cells and the bold text highlight our additions and modiﬁcations. Each column corresponds to a message type that the LLC can receive. The only exception is Evict, which is not a message, but rather a possible consequence of a miss in the LLC. The rows, instead, represent the possible states of the cache line addressed by the incoming message. Each entry indicates the actions performed and, after the /, the new state of the cache line. Actions within square brackets may or may not occur. The empty boxes indicate situations that never take place, while the stall cells require the incoming message to be stalled until the pertinent cache line resolves to a stable state. req refers to the requestor, which is the private cache that sent the message. mem stands for off-chip memory. Write-back. First, we explicitly specify the protocol as write-back, i.e. a Put message does not cause a write back to main memory. Only an eviction caused by a Get request or a ﬂush can do so. For this purpose, we add a stable state that we call Valid (V ), which refers to cache lines that contain valid data but have an empty sharers’ list and no owner. As shown in Table I, the only difference between Valid and Invalid is that misses for Valid lines do not cause memory accesses and that Valid lines can be evicted. We added the orange cells to describe the management of the Valid state. In order to explicitly deﬁne the write-back behavior, the bold fonts identify all the read and write operations to memory or to the LLC cache lines. Dirty bit. In our implementation we only write back to memory if the cache line to be evicted is dirty, thus reducing off-chip accesses. Most of the write mem operations enclosed in square brackets in Table I take place for dirty lines only. Recalls. We support sending recalls from the LLC to the private caches. These happen when a Get request (or a DMA request) causes a miss and there are no Invalid or Valid lines in the set. A line must be sacriﬁced and its sharers or owner have to be informed. GetS and GetM can only cause recalls when the LLC is not inclusive, while DMA requests may always trigger recalls. A recall requires two additional transient states corresponding to the last two rows of Table I. When a line is in either of these states, the cache is waiting for the response to a recall. The latter is sent either in the form of an Invalidate message, when transitioning from the Exclusive state, or a Fwd-GetM message, when transitioning from the Modiﬁed state. The purple cells in Table I deﬁne the behavior of recalls. Flush. From the protocol viewpoint, a ﬂush is a series of evictions. In our implementation, a ﬂush only evicts Valid lines. This simpliﬁcation, which greatly reduces the performance penalty, is possible because a ﬂush is only needed before a non-coherent accelerator starts executing. In this situation we make sure that the ﬂush of the private caches is completed before the ﬂush of the LLC starts. This guarantees that all cache lines needed by the accelerator are either present in the LLC with Valid state, or stored in DRAM only. DMA requests. In Table I, the green columns indicate how we extend the protocol to handle LLC-coherent DMA requests. These can only address Valid or Invalid cache lines, thanks to the invariants speciﬁed in Section II: the private caches are ﬂushed and no other component can access the accelerator’s data before completion. Note that thanks to recalls, ﬂushing the private caches could be avoided. However, the amount of generated messages would incur a much higher trafﬁc and performance overhead, when compared to ﬂushing. DMA requests cause a memory access only in three possible scenarios: eviction of dirty lines, read miss, or misaligned write miss. The last condition generates at most two memory accesses, corresponding to the ﬁrst and the last cache line involved in the misaligned DMA transaction. All other lines are completely overwritten and require no write allocation. B. Private cache controller Recalls. Recalls from the LLC are supported and implemented as forced evictions. L1 invalidation. The processors integrated in our SoC are Leon3 cores conﬁgured with a write-through split L1 cache that supports snooping-based coherence over the AMBA AHB bus [17]. Hence, for every cache line that is evicted or invalidated in the L2, the corresponding line in the L1 is invalidated by performing a fake-write operation on the bus. Invalidation is not necessary for accelerator tiles, where the write-back L2 is the ﬁrst and single level of private cache. Atomic operations. To run an unmodiﬁed version of Linux SMP with the Leon3 processors, we support test-and-set and compare-and-swap operations. A processor issues these operations as one or more loads addressing a cache line that may or may not be followed by a store targeting the same cache line. A lock signal is set to prevent preemption of the bus. Over a NoC, standard directory-based protocols alone do not guarantee the atomicity of such operations. Hence, we add a transient state to the private cache protocol to capture the fact that a line has been read by an atomic operation, but not written yet. We call this state XMW , which resolves to Modiﬁed when any of the following requests arrives on the bus: an atomic write request for the XMW cache line, a non-atomic request, or a request for a different cache line. Additionally, when the atomic load arrives, the private cache sends a GetM to the directory to gain ownership of the cache line. Once it gains ownership, no forward requests are accepted for this cache line until it resolves to Modiﬁed. When the processor issues an atomic read request, if the related cache line is in either Exclusive or Modiﬁed state, a read hit is followed by a state update to XM W . CHARAC TER I ZAT ION O F TH E TARG ET ACC EL ERATOR S . TABLE II Accelerator FFT 1D Sort FFT 2D SPMV Memory Footprint 032kB - 256kB 128kB - 04MB 256kB - 16MB 025kB - 10MB PLM (kB) 40 24 128 12 FPGA Resources LUT FF BRAM 7,537 4,310 10 36,868 31,300 6 3,965 2,190 48 8,136 4,476 24 IV. ACC ELERATOR S We used SystemC and Cadence Stratus HLS to design and synthesize all loosely-coupled accelerators in our experiments. A loosely-coupled accelerator provides a major speedup over a software implementation of the same algorithm thanks to its highly parallel architecture and aggressively banked private local memory (PLM) [11], [20]. While the PLM occupies most of the accelerator area, typically it cannot contain the whole dataset processed by an accelerator for a given invocation. Hence, the data subsets (chunks) must be continuously exchanged between the PLM and the rest of the memory hierarchy. To achieve optimal performance, communication and computation phases must be overlapped as much as possible using ping-pong buffering and pipelining: the pipeline input stage loads a chunk of data into the PLM; one or more compute stages process the input data and save an output chunk into the PLM; the output stage issues a store for the partial results to the next level of the memory hierarchy. If the compute stage takes at least the same time as the input and output stages, the communication phase is completely hidden and the accelerator achieves its maximum sustainable throughput. The ability to have perfectly balanced accelerator stages is highly dependent on the speciﬁc memory access patterns, as well as on the system interconnect and the memory hierarchy, including the selected cache-coherence model. The SoC designer must consider spatial and temporal locality (if any), length of read/write transactions, and the offsets across multiple transactions, which could be statically known, or data dependent. Some accelerators, for example, never read the same data from memory twice; others, instead, may reuse data extensively. While many loosely-coupled accelerators present a streaming access pattern with long contiguous transactions, some issue irregular and short requests to memory. More importantly, the size of the dataset can vary greatly across different accelerators, and across multiple invocations of the same accelerator. Our experiments conﬁrm that the memory footprint of the workload is indeed the most relevant metric when selecting the appropriate cache-coherence model. We implemented four representative accelerators to carry out the experiments described in Section V. We accelerate four ubiquitous algorithms: Sort, Fast Fourier Transform (FFT) 1D, FFT 2D, and Sparse Matrix-Vector Multiplication (SPMV). The initial software implementation for SPMV is taken from the MachSuite [21]. We report below a brief qualitative description of each accelerator. Table II summarizes some quantitative data on their memory footprint and resource utilization on FPGA. Fig. 3. SoC integrator GUI conﬁgured for the experiments. FFT 1D accelerates the FFT algorithm over one vector of up to 32K complex numbers. The computation stages of its pipeline process two non-contiguous portions of the input vector. The offset between the two depends on the current iteration of the FFT algorithm. The number of transactions with the system memory hierarchy is proportional to the logarithm of the vector’s length. Sort can process up to 1024 vectors of 1024 ﬂoating-point numbers. Each vector ﬁts in the PLM and it is sorted in-place. Note that no data is accessed twice and temporal locality is exploited within the accelerator’s PLM. FFT 2D operates in two phases. First FFT 1D is executed on every row of a two-dimensional matrix. While input data are read in row-major order, the output is written back in columnmajor order, thus transposing the resulting matrix. The second phase repeats the same operation on the transposed matrix, thus completing FFT 2D. Similarly to the FFT 1D, the number and length of the read transactions depends on the size of a row. Conversely, write transactions consist of a sequence of two-word store operations, each offset by a row. SPMV multiplies a sparse matrix by a dense vector. The matrix is compacted in the compressed row storage format, which removes all zero entries. This dot product causes few irregular accesses to memory. The compute-to-memory ratio is very low due to the overhead of performing short read transactions compared to a simple computation stage: elementwise dot product. The elements of the matrix are read only once, while portions of the dense vector can be reused. V. EVALUAT ION For the evaluation we use an FPGA-based infrastructure built on top of the one proposed by Mantovani et al. [22]. Fig. 3 is a snapshot of the SoC conﬁguration that we use for all the experiments. The CAD ﬂow from the graphical user interface to the bitstream for FPGA is fully automated. The SoC has two Leon3 cores, two memory controllers and twelve accelerators, of which only two have a private cache (see the Cache selection). Through the GUI, we also select the size of the caches: 16kB for the L1 caches of the cores; 64kB for each L2 private cache; 1M B for each partition of the LLC, for a total of 2M B . The bandwidth towards external Fig. 4. Comparison of speedup w.r.t. software of non-coherent (NC) and LLC-coherent (LLC) accelerators. Bars are annotated with the memory access count. memory is throttled by the AMBA AHB bus to one 32-bit word access per cycle. The DDR3 memory is conﬁgured to operate at its slowest possible frequency of 320 MHz. This reduction is meant to give an off-chip memory access penalty similar to that of an equivalent ASIC implementation. A. Single-accelerator We start by evaluating single-accelerator SoCs, to test each of the four types of accelerators in isolation. We allocate the accelerator’s data on one memory partition and force the operating system to run on the other partition only. Hence, we can measure the statistics of the accelerators without the non-deterministic interference of software execution. Each test runs a user-space application that prepares the input data for the accelerator and processes its output data. Hence, caches are always hot before invoking an accelerator. Results for both non-coherent and LLC-coherent accelerators are summarized in Fig. 4. Bars represent the geometric mean of the accelerator speedup with respect to single-core software execution over several runs of the same test. For each bar, the corresponding label shows the total DRAM access count in thousands. The charts highlight clear trends relatively to the memory footprint of the dataset. In the case of FFT-1D, Sort and FFT2D, when the accelerator’s memory footprint is smaller than the LLC size (< 1M B ), the LLC-coherent model always has higher speedup than the non-coherent one. For larger datasets, however, the non-coherent option returns higher speedups because the LLC-coherent accelerators trigger many evictions. When considering SPMV, the charts in Fig. 4 report a much larger speedup for the smallest dataset than for the other ones. Notice that SPMV heavily beneﬁts from LLC-coherence when the dataset ﬁts in the LLC. The gap between the ﬁrst dataset and the others for the non-coherent run is determined by the size of the dense vector only: SPMV has a 32kB PLM dedicated to the dense vector, which is used only when the vector ﬁts in it (382KB dataset). In this case, the sparse accesses of single words to the dense vector are performed within the PLM. Because of the irregular access pattern, the LLC-coherent model continues to deliver slightly better performance, even for memory footprints larger than the LLC. In general, despite the performance hit for large datasets, the beneﬁts of LLC-coherence in terms of DRAM accesses are indisputable: when the dataset ﬁts in the LLC, they are completely eliminated, with the exceptions of compulsory misses. For instance, FFT-2D operates on a temporary memory buffer, which is not accessed by software prior to invoking the accelerator. Additionally, note that the largest workloads of FFT 2D and SPMV capture the worst-case scenario of the LLC-coherent model: during these tests, most LLC-coherent DMA transactions are either a read miss that evicts a dirty line or a short misaligned write request that evicts a dirty line and doesn’t write an entire cache line. Hence, each operation causes two memory accesses, as opposed to one access needed by the non-coherent DMA. In any other scenario, such as for Sort, the number of accesses to memory for LLC-coherent accelerators is always less or equal to the number of accesses required by a corresponding non-coherent accelerator. In summary, the relative speedup of LLC-coherent accelerators, compared to non-coherent ones, ranges between 0.5x and 4x. The memory access count, instead, ranges from none to at most 2x with respect to the non-coherent model. We also observe that the non-coherent model monotonically improves performance when increasing the size of the dataset. The LLCcoherent behaves similarly, but with a jump back when the dataset becomes larger than the LLC. These results conﬁrm that a run-time selection of the cache coherence model, based on the memory footprint of the workload, can be beneﬁcial. B. Many-accelerator For a single accelerator we have shown that the effectiveness of the LLC-coherent model is strictly correlated to the ratio between the size of the workload and the capacity of the LLC. With the next set of experiments, we collect data for 4, 8 and 12 accelerators running concurrently (1, 2 and 3 instances for each accelerator type). We pick a small workload ranging from 256kB to 512kB per accelerator, such that only the aggregate dataset of 8 and 12 accelerators is larger than the LLC. For these experiments we use a dedicated memory controller for each memory partition to avoid saturation of the Fig. 5. Speedup of 4, 8, and 12 accelerators executing simultaneously. Each bar is normalized to the speedup of the corresponding accelerator when running in isolation. The dataset per accelerator ranges between 256kB and 512kB. memory bandwidth. This is critical to make sure that results are not affected by under-provisioned I/O. Fig. 5 shows the speedup of each accelerator’s execution w.r.t. its execution in isolation, averaged over multiple runs. The rightmost bar on each cluster is the geometric mean of the speedups across all running accelerators. Note that the invocation of each accelerator should cause a ﬂush of some caches depending on the coherence model. However, when a ﬂush is issued while another one is pending, we don’t reexecute it. Therefore, some accelerators beneﬁt from a smaller invocation overhead (e.g. Sort in Fig. 5). This performance advantage would not be appreciable on large workloads, when the overhead for ﬂushing becomes negligible. The average performance degraded by up to 38% and 10% for LLC-coherent and non-coherent accelerators, respectively. As expected, the performance of LLC-coherent accelerators is the most penalized and the speedup degradation increases with the number of accelerators. Based on the system layout, accelerators with a dedicated path to memory perform better. In addition, accelerators operating on short and frequent transactions, like SPMV, incur larger penalties. In fact, other accelerators are likely to be granted the NoC links and lock them during long DMA transfers. Next to performance degradation, the LLC-coherent model shows an increased number of memory accesses when running many-accelerator workloads. Nevertheless, it still maintains a considerable advantage over the non-coherent model: 44x improvement with 4 accelerators and about 5x with 8 and 12 accelerators running concurrently. As expected, these results also conﬁrm that the selection of the cache-coherence model must account for the ratio between the workload aggregate memory footprint and the capacity of the LLC. C. Fully-coherent model Finally, we consider a case with a very small dataset and select the two accelerators in the system equipped with a private L2 cache. In this scenario, the fully-coherent model can have similar or better performance than the non-coherent and LLC-coherent ones. Similarly to LLC-coherent accelerators, Fig. 6. Comparison of speedup w.r.t software for tiny workloads. the fully-coherent ones have the beneﬁt of reducing or eliminating the memory accesses. Additionally, this model does not require ﬂushing the processors’ caches, which could disrupt the work of other component of the SoC. In Fig. 6, we show side by side the accelerator speedups with respect to a software execution on a processor core for the three cache-coherence models. The fully-coherent model yields better performance only for the smallest datasets. Given these results and since fully-coherent accelerators are widely present in the literature (e.g. [?], [23], [24]), we support the run-time selection of the fully-coherent model as well. Our SoC generator enables this feature through the selection of an optional private cache in each accelerator tile. V I . R ELAT ED WORK Cache Coherence Models for Accelerators. What we deﬁned as non-coherent and fully-coherent models represent the two main cache-coherence models for loosely-coupled accelerators in the literature [25]. Fully-coherent accelerators have started to receive growing interest from the industry both as off-chip [24], [26] and on-chip [27] components, but in busbased systems only. Instead, we integrated all of the models in a NoC-based SoC. Previous works deﬁne some bus-based variations of what we refer to as fully-coherent accelerators. These accelerators have no private cache and memory requests are issued directly on the bus. By adapting a snooping-based protocol, both the LLC and the private caches respond to the accelerator’s requests enforcing coherence [28], [29]. A similar approach over a NoC would require a costly multi-cast of invalidation and recall messages to the private caches. Among the proposed solutions to support non-coherent accelerators over a NoC, some suggest to keep a separate memory space for the accelerators [9], while more recent approaches agree on maintaining shared memory to avoid copying data across the two address spaces [10], [30]. Our implementation follows the most recent approach. The few studies that compare cache-coherence models for accelerators differ with respect to our work in that they study bus-based systems, experiment on single-accelerator workloads and do not compare all three models that our architecture supports. With Fusion, Kumar et al. presented three designs of the fully-coherent model [23]. Shao et al. analyzed the noncoherent and fully-coherent models [12]. Finally, Cota et al. evaluated LLC-coherent and non-coherent accelerators [11]. While these works rely mostly on simulation, our study is based on FPGA implementations. This allows us to run complex multi-threaded applications, on top of Linux SMP, that invoke multiple accelerators operating on large workloads. Similarly to Fusion, other works explored the case of multiple accelerators that share the same private L1 cache or scratchpad [9], [10]. Arguably, a group of accelerators sharing the same L1 cache or PLM can be deﬁned by its aggregate communication pattern and workload size. Hence, our conclusions would still apply. Cache Hierarchy and NoC Optimization. Researchers proposed several ways to optimize the cache hierarchy in NoC-based multicores. However, the implication of cachecoherence over a NoC for accelerators has received limited attention. Some approaches for homogeneous multicores propose a modiﬁcation of the directory-based protocol [8], [31]. Others suggest to restructure the interconnect and embed the cache coherence protocol in the NoC, thus completely removing the directory [7]. Alternatively, Cong et al. [32] propose a hybrid NoC interconnect as the backbone for many-accelerator architectures. These types of optimization are orthogonal to our work. Our protocol can be implemented on different types of NoC. In fact, the only restrictions that apply to the network are point-to-point ordering and the availability of three distinct planes, or virtual channels. V I I . CONC LU S ION We proposed an extension to the MESI directory-based cache coherence protocol over NoC to support LLC-coherent accelerators. We presented the ﬁrst NoC-based SoC enabling non-coherent, LLC-coherent and fully-coherent accelerator models to coexist and operate simultaneously. By implementing atomic test-and-set and compare-and-swap, our SoC can run complex accelerated applications on top of Linux SMP. Experiments on FPGA prove the importance of run-time selection of the accelerator coherence model. In particular, the results show how supporting LLC-coherent accelerators can deliver up to 4× the performance of non-coherent accelerators, while reducing (or in some cases eliminating) the number of accesses to external memory. "
Architecting a Secure Wireless Network-on-Chip.,"With increasing integration in SoCs, the Network-on-Chip (NoC) connecting cores and accelerators is of paramount importance to provide low-latency and high-throughput communication. Due to limits to scaling of electrical wires in terms of energy and delay, especially for long multi-mm distances on-chip, alternate technologies such as Wireless Network-on-Chip (WNoC) have shown promise. WNoCs can provide low-latency one-hop broadcasts across the entire chip and can augment point-to-point multi-hop signaling over traditional wired NoCs. Thus, there has been a recent surge in research demonstrating the performance and energy benefits of WNoCs. However, little to no work has studied the additional security and fault tolerance challenges that are unique to WNoCs. In this work, we study potential threats related to denial-of-service, spoofing, and eavesdropping attacks in WNoCs, due to malicious hardware trojans or faulty wireless components. We introduce Prometheus, a dropin solution inside the network interface that provides protection from all three attacks, while adhering to the strict area, power and latency constraints of on-chip systems.","Architecting a Secure Wireless Network-on-Chip Special Session Paper Brian Lebiednik Sergi Abadal Hyoukjun Kwon Tushar Krishna Georgia Institute of Technology Univ. Polit `ecnica de Catalunya Georgia Institute of Technology Georgia Institute of Technology Atlanta, GA, USA brian.lebiednik@usma.edu Barcelona, Spain abadal@ac.upc.edu Atlanta, GA, USA hyoukjun@gatech.edu Atlanta, GA, USA tushar@ece.gatech.edu Abstract—With increasing integration in SoCs, the Networkon-Chip (NoC) connecting cores and accelerators is of paramount importance to provide low-latency and high-throughput communication. Due to limits to scaling of electrical wires in terms of energy and delay, especially for long multi-mm distances onchip, alternate technologies such as Wireless Network-on-Chip (WNoC) have shown promise. WNoCs can provide low-latency one-hop broadcasts across the entire chip and can augment pointto-point multi-hop signaling over traditional wired NoCs. Thus, there has been a recent surge in research demonstrating the performance and energy beneﬁts of WNoCs. However, little to no work has studied the additional security and fault tolerance challenges that are unique to WNoCs. In this work, we study potential threats related to denial-of-service, spooﬁng, and eavesdropping attacks in WNoCs, due to malicious hardware trojans or faulty wireless components. We introduce Prometheus, a dropin solution inside the network interface that provides protection from all three attacks, while adhering to the strict area, power and latency constraints of on-chip systems. I . IN TRODUC T ION Network-on-Chip (NoC) is currently the paradigm of choice to interconnect the different components of System-on-Chips (SoCs) or Chip Multiprocessors (CMPs). As the levels of integration continue to grow, however, current NoCs face signiﬁcant scalability limitations and may become a performance bottleneck in manycore systems. One promising solution to this problem is the introduction of new interconnect technologies as an extension of the NoC paradigm. Among them, wireless on-chip communications have garnered considerable attention due to their low latency, architectural ﬂexibility, and inherent broadcast capabilities [1], [2]. Architecting manycore systems with Wireless Network-on-Chips (WNoCs) is an active area of research [3] since low-latency broadcasts can facilitate scalable coherence and consistency. The adoption of the WNoC paradigm brings up new challenges, including the implementation and efﬁcient integration of high-performance and low-cost transceivers or the development of appropriate communication protocols. Security is another important aspect to address as the broadcast nature of the wireless transmissions introduces new points of entry The authors gratefully acknowledge support from the Spanish MINECO under contract TEC2017-90034-C2-1-R (ALLIANCE project). The anti-spooﬁng portion of this paper is an extended version of our previous work in [10]. The other two components are extensions onto the system. for an attacker to compromise the chip. If accesses to the wireless medium are not protected, smart Hardware Trojans (HTs) placed within the network or in third-party components can degrade the system performance, write corrupt data in memory, or steal sensitive information. This paper focuses on the security aspects of WNoC, and we build on two observations. First, the communication mechanism of a WNoC is essentially different from that of wired on-chip networks. This introduces new threats, such as the possibility of eavesdropping or generating a global Denialof-Service (DoS) attack from any node’s Medium Access Control (MAC) module, and prevents the use of existing NoC protection strategies, e.g. [4]–[7]. Second, the performance requirements of a WNoC are radically different from that of conventional wireless networks, driving the need for fast and lightweight solutions and preventing the use of existing strategies for wireless security, e.g. [8], [9]. The main contribution of this paper is Prometheus, a tripartite solution to three threats to the WNoC: DoS, spooﬁng, and eavesdropping. Prometheus is placed within each node’s Network Interface (NIF), as shown in Figure 1, and is capable of deactivating transceivers affected by HT to mitigate their impact. DoS attacks are detected through observation of the medium accesses and the collection of network statistics from the NIF. To combat spooﬁng attacks, Prometheus incorporates our previous work Veritas [10], which identiﬁes spoofers opportunistically by comparing the reception power proﬁles of the presumed and actual source of a message. Lastly, eavesdropping is prevented by securing the communications with very low-cost encryption schemes. Through performance and cost analysis, Prometheus can protect a WNoC from advanced HTs with reasonable power, area, and network performance overheads. To the best of the authors’ knowledge, this is the ﬁrst work jointly addressing DoS, spooﬁng, and eavesdropping in Wireless NoCs. Existing works either address a single threat [11] or incur unacceptable overheads [12]. This paper is set forth as follows. First, we provide some background on WNoC in Section II. Next, we discuss the threat model in Section III and our proposed architecture solution in Section IV. Then, we evaluate the performance and cost of our proposal in Sections V and VI, respectively. Finally, we discuss related work in Section VII, and conclude the paper in Section VIII. 978-1-5386-4893-3/18/$31.00 ©2018 IEEE minant of performance in any wireless network as two simultaneous accesses to the same channel will fail. In WNoCs, the MAC mechanism becomes fundamental as the medium will be densely populated and the load will be probably high. Related works in WNoC consider a variety of MAC designs. We classify them into two kinds, each with its own pros and cons. Contention-free schemes [1], [2], [15] avoid collisions via arbitration or the use of different frequency bands or time slots. These techniques can deliver high throughput, but do not work well under variable workloads since bandwidth is statically allocated. A popular implementation of this type of MAC is the token passing protocol, where only the node holding the token can transmit [1]. Contention-based schemes [3], [16], [17] allow all nodes to attempt to transmit on the shared medium at any time instant. This provides ﬂexibility and reduces overall latency, but comes at the cost of limited throughput due to the potential collisions. These schemes use various mechanisms to minimize collisions and to recover from them. In wireless networks in general, Carrier Sensing Multiple Access (CSMA) protocols where nodes listen to the medium before transmitting and perform a random backoff if they collide are widely considered. These type of protocols have been adapted to the WNoC scenario as well [17]. Variations of CSMA, such as MACAW [18], can further improve on the performance of contention-based protocol by introducing explicit fairness measures. We based our CSMA protocol on [17] with the fair backoff of MACAW. As we will see, both types of protocols have their own security implications. To cover the most representative cases, we will consider both token passing and CSMA protocols in our analysis. Malicious nodes will attempt to cause a denial of service by generating collisions in both schemes. C. Network Interface (NIF) The NIF performs address translation and admission control tasks. In our target WNoC, it can implement load balancing and Quality of Service (QoS) functions. To this end, we assume the NIF to be composed of a controller, which determines the path (wired or wireless) to follow by each message, and two speciﬁc interfaces connected to their respective network planes. At the interfaces, network performance statistics can be collected to assist congestion avoidance and fairness mechanisms or, in our case, security policies. I I I . THREAT MODE L Insecure wireless NoCs can potentially be vulnerable to various kinds of attacks. Next, we discuss the main assumptions related to security and then describe the threats addressed in this work: DoS, spooﬁng, and eavesdropping. A. Assumptions As a ﬁrst assumption, we consider a system with a single point of attack. We will see that this assumption is enough to provide signiﬁcant harm in the WNoC scenario and avoids considering an scenario where an unbounded attacker could disrupt the system. Another common assumption we make is Fig. 1: Schematic representation of Prometheus within a wiredwireless NoC architecture. I I . BACKGROUND Constant downscaling of Radio Frequency (RF) circuits have opened the door to the conception of a wide variety of WNoC architectures. In this approach, wireless interfaces are co-located with cores and generally complement the wired NoC. Due to the relatively large size of the RF passives, most WNoC proposals assume that wireless interfaces are shared among several cores [1], whereas more aggressive proposals consider per-core integration as exempliﬁed in Figure 1 [3]. Next, we provide some background on the components of a WNoC and discuss their security implications. A. Physical Layer (PHY) The PHY deﬁnes how bits are transmitted over the wireless links. In a WNoC, the PHY module serializes processor messages, modulates the bits at a frequency much higher than the processor clock, and delivers the modulated signals to the antenna. The inverse operation is performed at reception. Current transceivers for chip communication operate at 60 GHz with simple modulations to minimize area and power. As a reference, the 65-nm CMOS design from [13] uses on-off keying and achieves 16 Gbps with a bit error rate of 10-15 while taking 31.2 mW and 0.25 mm2 . Future trends point to higher RF frequencies to further reduce area and to increase bandwidth [14]. Coding in WNoC must be fast and simple but capable of detecting small bursts of errors [1]. These PHY design decisions have important security implications. Encryption mechanisms should also be fast to avoid becoming a performance bottleneck. It is also necessary to achieve a large signal-to-noise ratio (18 dB in [13]) to maintain the error rate requirements of the scenario. We can thus assume that thermal noise ﬂuctuations hardly affect the signal and that it is feasible to distinguish between thermal errors (sparse) and those caused by a wireless collision (bursty). Such distinction is important to provide security at the MAC level. B. Medium Access Control (MAC) The MAC1 layer implements mechanisms to ensure that nodes can access the medium reliably. This is a key deter1 In the security literature, MAC could refer to Message Authentication Code. But to remain consistent with RF networks, we use MAC to mean Medium Access Control. Fig. 2: The wireless intra-chip channel. that Prometheus, our solution, cannot be compromised [19]. It is also common practice to assume that the HT is placed in a digital circuit. Therefore, HTs cannot alter the PHY layer. We further consider the propagation of RF signals within the system, which determines the origin of potential attacks in WNoC. The chip (or system) package can be seen as a metallic box that prevents RF signals from leaking to or coming from outside the package. This is important to security because it is safe to assume that DoS, spooﬁng, or eavesdropping can only be performed from the inside. To better justify this last assumption, we illustrate the typical structure of a processor within a ﬂip-chip package in Figure 2. A similar analysis can be performed in other packages [20]. Circuits are placed within an insulator on top of a silicon substrate. This structure is ﬂipped, connected to the rest of the system via an array of metallic microbumps, and often covered with a metallic lid that acts as a heat sink. In this conﬁguration, signals propagate from the transmitting antenna (TA) to the receiving antenna (RA) via multiple paths, but cannot scatter outside the package due to the presence of the heat sink and the micro-bumps [21]. Despite not being a solid chunk of metal, the micro-bumps and subsequent metallizations disrupt propagation because their pitch is generally much lower than the wavelength of the RF waves (i.e., 10–100 µm against ∼1 mm in mmWave bands). A ﬁnal reasonable consideration is that, in such a controlled and enclosed environment, signals do not suffer from humidity effects. B. Denial of Service Any misconﬁguration at the MAC layer can cause severe problems inside the WNoC. Two nodes transmitting on the same channel cause a collision, corrupting the message. Continued collisions lead to loss of bandwidth or breakdown of the wireless network. We do not consider the case where the attacker waits until someone transmits to then create a collision on purpose, as this requires the HT to be placed at the PHY layer. Yet, malicious or faulty entities could exploit vulnerabilities at the MAC layer to perform a DoS attack. We quantify the potential harm by modeling a game [22] with every node playing one of six possible conﬁgurations (Strategy × Trafﬁc) shown in Fig. 3(a) for a CSMA system. “Selﬁsh” nodes try to maximize their utility by playing unfairly. In the context of WNoCs, an unfair play could be (a) sending out of turn in case of a token-based arbitration scheme or (b) lowering the backoff delay in case of a CSMA scheme. In either scenario, this can result in bandwidth stealing from normal (“healthy”) nodes. It can be quite hard to detect this behavior since the selﬁsh nodes can hide behind the protocol and pretend to be “hotspot” nodes with more trafﬁc to send. This is especially so in the CSMA protocol, which by itself cannot distinguish between genuine contention and DoS. Fig. 3(b) sweeps the design space modeled above by varying the injection rate x in a 16-core contention-based system, and plots the relative bandwidth of each node as a function of the total injected load. With healthy nodes, hotspot nodes utilize higher bandwidth than normal nodes, as expected. With selﬁsh nodes, at normal loads, the relative bandwidth occupancy is almost the same as that of healthy nodes, which shows that the CSMA protocol itself is robust. However, once the selﬁsh nodes start injecting at moderate to heavy loads, their relative bandwidth occupancy becomes signiﬁcantly larger than that of the healthy nodes (both normal and hotspot). Fig. 3(c) plots the latency versus throughput of all nodes across the same set of conﬁgurations. Here, we can see that the latency of healthy nodes is quadrupled and the throughput drops by over 70% in the presence of selﬁsh hotspot nodes, thereby slowing down forward progress of any parallel application signiﬁcantly. Note that this attack is unique to a WNoC domain and no known solution exists to the best of our knowledge. C. Spooﬁng Since the WNoC naturally acts as a shared medium [23], any node can broadcast information. This can be leveraged by malicious cores to cause system-level problems by manipulating the source address of ﬂits. Spooﬁng could be employed to bypass memory access protection by impersonating a core that has permission to write, eventually writing in prohibited regions of memory to steal sensitive information or disrupt execution. Spooﬁng may also be leveraged to respond to legitimate requests originally intended for a given node n. Before n can answer with the requested information, another rogue node r responds with false information, causing the application to crash. A more complex r might respond with incorrect data that does not cause the application to crash, but rather provide incorrect outputs or loss in performance. We fully explore spooﬁng and provide solutions with results in [10] and expand upon our results in Section VI. D. Eavesdropping Broadcast messages are inherently vulnerable to eavesdropping attacks because all nodes are always listening. Different processes or Virtual Machines (VMs) may need to keep the passed information secret from other processes because they are running at different permission levels or different users. Also, large cloud processing NoCs can have proprietary information running for clients that they want kept secret. In traditional NoCs, the trafﬁc is kept separate between applications using QoS, but this is not suitable in a broadcast environment [24]. Also, if the admission control mechanism of the NIF is compromised, such secret information will be exposed anyway. Thus, there is a need for messages to be sent securely through the WNoC. Fig. 3: Potential DoS Attack. (a) Types of wireless nodes in the system. Each node plays a particular Strategy × Trafﬁc in each game. (b) Channel bandwidth occupied by healthy and selﬁsh nodes. (c) Throughput and latency characteristics for healthy and selﬁsh nodes (Hotspot nodes shown in circles). IV. S ECUR E WNOC M ICROARCH I T EC TUR E : PROM ETHEU S We design Prometheus, a set of hardware solutions, to address the three threat models described earlier in Section III. Each of these can be added as a module into the NIF, as shown in Fig. 4, to protect the system from the particular threat model at hand –be it due to a HT, or a faulty MAC hardware. Upon detection of a potential threat, a ﬂag is raised and the ID of the malicious node is sent to the OS which disables (or resets) its WNIF, forcing it to only use its wired NoC either temporarily or indeﬁnitely. Although Prometheus works well independently of the number and location of transceivers, we henceforth assume a single transceiver per core in a homogeneous processor. A. DoS protection via unfairness detection We consider DoS attacks in both contention-free and contention-based schemes as described earlier in Section II-B. 1) Contention-free MAC: In contention-free MAC protocols (see Section II), there should never be a collision since only the node possessing a token [1] is allowed to transmit. In this scenario, a selﬁsh node (either malicious or faulty) would transmit out of turn and potentially cause a collision leading to data corruption. PrometheusDoS proposes to have a node monitor collisions, and using the wired NoC, prompt the owner of the token to suppress its own transmissions for a ﬁxed period of time. Transmissions by the selﬁsh node can now be identiﬁed by its ID, and turned off by the OS. If the selﬁsh node spoofs its ID, PrometheusSpoof (Section IV-B) kicks in. 2) Contention-based MAC: Detecting a DoS attack is more challenging in contention-based protocols such as CSMA because collisions in the channel are an inherent part of the protocol. A selﬁsh node (Figure 3(a)) can cause repeated collisions but is indistinguishable from a healthy node with hotspot trafﬁc. We identify that the key difference between the two cases is unfairness. Healthy nodes should exhibit the following properties: at low-loads, they should experience a high injection throughput (TT X ), a low reception throughput (TRX ), and low backoff delay (B ), translating to low latency and high bandwidth from the channel. At high-loads, they should experience moderate TT X , high TRX , and a high value of B , translating to moderate latency and bandwidth. In case of a DoS attack, as Figure 3(b) and (c) demonstrated, a selﬁsh node will always experience a high TT X , low TRX , and low B . Fig. 4: Prometheus Microarchitecture Unfairness Ratio Γ. We propose to use TT X , TRX , B , and the wireless channel capacity C (in Gbps) to determine whether the node is healthy or selﬁsh in a distributed way. This is in contrast to typical fairness ratios, which generally use a centralized approach to compare a single metric as they assume that the protocol is not compromised [25]. PrometheusDoS periodically probes the WNIF and MAC for these statistics as Fig. 4 shows. Over three thousand samples across several hundred simulations of different node behavior combinations, as described in Fig. 3(a), we derive a metric called the unfairness ratio Γ Γ = TT X C + TRX × B 2 1 + B 2 min (1) where Bmin is the minimum backoff (1 cycle in our case). Intuitively, Γ will increase for nodes that are experiencing disproportionately high amounts of injection, indicating a potential DoS attack. As TT X increases and TRX decreases, Γ increases. A selﬁsh node will knowingly reduce B to more successfully obtain the channel. Nodes with high TT X , that are healthy, should naturally experience a higher B , lowering the overall Γ. Healthy nodes will have a Γ near zero (as TT X is close to TRX ) relative to selﬁsh nodes that have a higher Γ. If Γ is greater than a conﬁgurable threshold, it signals the OS about the unfairness. False Positives and Negatives. As Γ is a heuristic, naturally it can have both false positives (a healthy hotspot node being tagged by its NIF as selﬁsh) and false negatives (selﬁsh nodes go undetected). Also, Γ may vary across different WNoC conﬁgurations. Section V evaluates the robustness of Γ through an example. B. Spooﬁng protection via RF power analysis Since the WNoC naturally acts as a shared medium, any node can broadcast information. Leveraging this, a HT node may choose to masquerade as another node to make unauthorized memory access or to cause performance loss. In off-chip wireless networks, authenticity can be guaranteed with asymmetric keys. Asymmetric key encryption, however, is orders of magnitude slower than symmetric encryption [26], which already takes several clock cycles per byte of information [27]. Thus, authenticity via asymmetric encryption becomes impractical in NoC and WNoC environments. Fast and lightweight alternatives are required instead. The WNoC paradigm offers a unique possibility of using the received RF power levels to determine the identity of the source of a given packet. In conventional wireless communications, propagation is modeled as a stochastic process as it depends on many random factors such as the environment, mobility, or blocking, among others. On the contrary, the WNoC scenario is static, highly controlled, and conﬁned. As a result, the wireless channel becomes time-invariant [28] and quasi-deterministic –aspects such as humidity effects can be neglected, multipath and path loss are static and can be known beforehand. Moreover, since we expect a high signal-to-noise ratio (see Section II), we can consider path loss measurements to be temperature-invariant as well. Building on these observations, PrometheusSpoof (formerly Veritas [10]) converts the received power into an effective source address and compares it with the ID contained in the packet header. A mismatch raises a spooﬁng alert. To prevent from repeating the same research, we leave the full implementation details of PrometheusSpoof to [10]. C. Eavesdropping protection via low-cost encryption Encryption keeps messages secret between sender and receiver.We consider symmetric key encryption because of the large overhead of asymmetric keys and hashing. The National Institute for Standards and Technology (NIST) prescribes encryption based on the use and the encryption strength needed. For network communications, speciﬁcally IPSec, NIST recommends the Advanced Encryption Standard (AES)-128 in Cipher Block Chaining (CBC) mode [29]. The cryptographic community considers AES, the standard for network communication, as a fast encryption scheme and the only known attacks are side channel attacks [26], such as observing CMOS gates to decipher the encryption key [30]. Such an attack would be near impossible in a WNoC. Challenges with AES-128 in WNoCs. Though AES is fast by modern network standards for off-chip wireless networks, using AES for on-chip communication comes with delay, area, and power penalties, as we show in this work. Instead, we believe that a secure WNoC can use other faster but less secure encryption algorithms such as stream ciphers to encrypt data. Proposed Solution: Stream Ciphers. Stream ciphers perform an operation on each bit (ﬂip or not ﬂip) using synced timing and symmetric keys. We consider two stream ciphers, RC4A and Py [31] (pronounced “roo”). Py improves on RC4A TABLE I: Known attacks, length of the attack (128-bit ﬂits), and processing delay of the proposed encryption schemes. Encryption Attack Length Delay per Byte N/A 251 ﬂits 265 ﬂits 20 cycles [27] 7 cycles [33] 2.85 cycles [34] AES RC4A Py Side Channel Distinguishing Distinguishing security and speed by using rolling arrays that rotate every rotation step by one unit. RC4A and Py are only vulnerable to linear distinguishing attacks, meaning that given a certain number of bytes an attacker can distinguish between a random stream of bytes and a stream encrypted with RC4A. Many consider this an academic break of the encryption because an attacker only knows that the sender used RC4A to encrypt the message [32]. Table I provides processing time and known attacks for each algorithm with the associated number of ﬂits needed for each attack. PrometheusEavesdrop . We implement Py in PrometheusEavesdrop since it has the lowest performance, power, and area overhead, as we show in our evaluations. It uses the following key distribution scheme. We add a key distribution center (KDC) where nodes request keys if the information needs conﬁdentiality from other nodes in the system. When the system starts, each node negotiates a new key with the KDC using a preshared potion of memory only know to the KDC and the respective node. The communication between the KDC and each node is done with this unique key. This requires only n additional keys where the node would share that key with the key distribution scheduler only. That ensures that the system does not need to use asymmetric encryption. Using this key, each node can negotiate a key with the KDC everytime it wants to created an encrypted session with another node. The other node does likewise so only the KDC, node n1, and node n2 know the key. The key request could delay the communication slightly, but once established the key can be used for up to 265 ﬂits before requesting another key to prevent linear distinguishing attacks. Lastly, we also encrypt the Cyclic Redundancy Check (CRC) along with the message. This makes the encryption scheme resistant to plaintext and ciphertext attacks. Like messages will have the same CRC, so sending a message without encrypting the CRC will reveal the underlying message. V. EVA LUAT ION S A. PrometheusDoS Simulation Methodology. We modeled and simulated PrometheusDoS in the PhoenixSim framework [35]. PhoenixSim is an event-driven NoC simulator that, while oriented to photonic NoCs, incorporates a complete set of accurate models for the evaluation of electrical NoC designs. Those models include several parameterized NIF and router designs, as well as a remarkable amount of routing protocols and topologies. On top of this, Abadal et al. implemented the necessary modules for the simulation of wireless on-chip communication, including different PHY, MAC, and wireless NIF designs [23], [36]. Fig. 5: Unfairness ratio Γ of healthy/selﬁsh nodes across normal/hotspot trafﬁc at varying loads in a 16-core system. Relevant to the evaluation of PrometheusDoS , each node in our simulator is conﬁgured to be a Healthy/Selﬁsh node sending Normal/Hotspot trafﬁc (Fig. 3(a)) at varying injection rates and burst rates to sweep the design space. Our experiments simulate 16 antennas, a number consistent with current WNoC designs [1], [36]. In the interest of space, we do not present results for contention-free protocols which are easier to safeguard. Instead, we implement the contention-based protocol from [17] augmented with the well-known explicit fairness mechanisms of MACAW [18], and the contention-free token-passing variant from [23]. Results. We evaluated the validity of PrometheusDoS with the unfairness ratio Γ described in Sec. IV-A. Figure 5 plots Γ as a function of network load across a suite of trafﬁc patterns. All points with the same color and symbol represent different injection and burst rates at that particular conﬁguration. We make three key observations from our results: • Selﬁsh nodes with moderate to heavy hotspot trafﬁc have a Γ > 0.01, which can be set as the threshold to raise a ﬂag to the OS. Note that the exact value of the threshold may vary as the size of the WNoC changes and can be conﬁgured after stress testing as our simulations have. • Healthy nodes with moderate to heavy hotspot trafﬁc have a low Γ, all of them below 0.01 in this case, demonstrating that we completely avoided false positives. • Selﬁsh nodes with normal trafﬁc have a low Γ below the threshold, showing that there could be false negatives. However, these false negatives are harmless since these selﬁsh nodes do not actually steal bandwidth from other nodes (as selﬁsh hotspot nodes do) due to the inherent robustness of the protocol. Thus, we ﬁnd that a selﬁsh node must have heavy trafﬁc to unfairly use the channel, at which point Γ can detect it. B. PrometheusSpoof Impact of chip package. To extend on the work in [10], further investigation was required in the package design. The chip package has a strong impact on the channel response. Therefore, it is pertinent to evaluate PrometheusSpoof in different package conﬁgurations. For instance, since lowresistivity silicon introduces substantial losses, some works have proposed to thin the silicon die [37]. To illustrate the potential impact of such decisions on PrometheusSpoof , we evaluate the dynamic range and resolution requirements as a function of the thickness of the silicon die. We refer the reader to [10] for details on the simulation methodology. Fig. 6: Dynamic range and resolution requirements of PrometheusSpoof as a function of the silicon thickness. Figure 6 shows the results of the analysis. It is observed that reducing the silicon die indeed brings the dynamic range requirements down because losses are minimized, whereas the resolution requirements oscillate around acceptable levels (5 dB). These results demonstrate that PrometheusSpoof would beneﬁt from a priori co-design efforts to (i) avoid bad design points, like the unacceptable dynamic range over 100 dB for a 0.45-mm silicon die; and (ii) point towards designs with relaxed requirements, i.e. at 0.15 mm the dynamic range is 36.57 dB and the resolution requirement is 4.11 dB. C. PrometheusEavesdrop Simulation Methodology. We use PhoenixSim [35] to evaluate the performance overheads of various encryption schemes using 16 antennas distributed over the chip with uniform random broadcast trafﬁc. We model the encryption process as a pipelined delay at both the transmitter (encrypting) and receiver (decrypting). The delay of each pipeline stage is the number of cycles required in hardware to produce a block or bit of the encrypted message. The rest of the network remains consistent with a standard WNoC: see Section V-A and [23], [36] for more details on the PhoenixSim and the modiﬁcations made to accommodate a WNoC. Results. Figure 7 plots the average message latency as a function of injection rate across the encryption schemes and assuming different wireless transmission speeds. On average across channel bandwidths, we observed that adding Py encryption saturates the network at 90% of the unencrypted trafﬁc saturation rate, whereas the RC4A and AES implementations saturate much earlier, at 50% and 24% respectively. The secure-hash implementation proposed in [12] is reportedly even slower than AES, making it a non-starter. Current technologies allow for a 16 Gbps channel for wireless transmissions [13]. With this bandwidth, the network with encryption saturates at the same injection rate as the standard network and, therefore, it does not suppose a bottleneck at the moment. With respect to latency, Py adds less than 5 cycles of overhead. Such delay is tolerable in manycores, where long-range transfers take several tens of cycles, as well as in broadcast-oriented architectures [36]. RC4A and AES increase latency by 80% and 260% compared to unencrypted trafﬁc, respectively, becoming a heavy burden. As we increase the channel bandwidth to 32 Gbps, which would be possible in the future by using more complex modulations or frequencies in the 90 GHz band or beyond, Py becomes the only option that does not reduce throughput. Py TABLE II: Breakdown of the area and power overheads of Prometheus at 15 nm Module PrometheusDoS PrometheusSpoof Digital logic Power detector Data converter Digital logic Py implementation Power Area 1.46 mW 0.002 mm2 0.8 mW 0.006 mm2 0.67 mW 0.004 mm2 0.37 mW 0.004 mm2 4.67 mW 0.007 mm2 7.97 mW 0.023 mm2 6.13 mW 0.009 mm2 20.96 mW 0.023 mm2 16 mW 0.1 mm2 PrometheusEavesdrop Total (at the corners) Total (other wireless interfaces) Router Wireless Transceiver (estimated) TABLE III: Power and Area for proposed encryption modules Encryption Scheme Power Area AES RC4A Py 91.76 mW 0.085 mm2 4.81 mW 0.008 mm2 4.67 mW 0.007 mm2 techniques to minimize the cost. Our 15-nm implementation yields a power of 1.46 mW and an area of 0.002 mm2 . PrometheusSpoof also incurs minor area and power overheads. Since the anti-spoof mechanism needs to be placed only in a few locations independently of the network size, the cost is scalable. Individually, we estimate that each PrometheusSpoof module will consume less than 0.02 mm2 of area and 2 mW of power. The main contributions to this cost are from the Power Detector (PD) and the Analog-to-Digital Converter (ADC). Regarding the PD, designs as small as 0.006 mm2 consuming less than 1 mW are capable of meeting the dynamic range requirements set in Section V [39]. Regarding the ADC, prototypes in 40-nm CMOS operating at ∼1 GS/s with 6bit resolution have been reported to occupy 0.004 mm2 and consume 5.3 mW. Since spooﬁng protection is performed on a per-packet basis, the frequency requirements can be relaxed, thereby reducing the power consumption. At 16 Gbps, 128-bit ﬂits are wirelessly transferred in 8 nanoseconds, which yields a sampling requirement of 125 MS/s. We therefore assume a power consumption of 0.67 mW. The overheads of PrometheusEavesdrop come from the implementation of the encryption/decryption modules. Our evaluation results with FreePDK are summarized in Table III. On the one hand, AES increases power by 90 mW and area by 0.085 mm2 per wireless interface, which represents an overhead of 538% and 464% with respect a NoC router. Thus, AES and similar solutions presented in related work [12] are unacceptable. On the other hand, the power and area overheads of RC4A and Py are 4.67 mW and 0.007 mm2 , which are modest numbers considering the size and power of cores in a manycore processor. We obtained such similar results because RC4A and Py are stream cyphers (Py is built from RC4A). V I I . R ELAT ED WORK Secure Wireless Networks-on-Chip. Ganguly et al. [11] leverage 24 wireless channels to create small-world topologies capable of mitigating, but not preventing, a DoS attack from a single node. Besides 24 channels being unrealistic, this scheme cannot handle a distributed DoS attack or an attack on a Fig. 7: Average message latency (in cycles) using proposed encryption schemes in (a) 16, (b) 32 and (c) 64 Gbps channel. starts to affect throughput as the bandwidth is further pushed up to 64 Gbps, currently unfeasible. Speciﬁcally, throughput is reduced by 30% and the 5-cycle added delay supposes a 46% increase over the unencrypted delay. The main conclusion of this scalability analysis is that lightweight encryption mechanisms like Py are a reasonable option in the mid term for WNoC, but that faster alternatives will be required further down the road. A workaround to this problem would be to not encrypt non-sensitive messages that need to traverse the network, leaving the sensitivity decision to Prometheus and the NIF. Finally, secure-hash and AES result in unacceptable performance penalties in any case. V I . IM P LEM EN TAT ION CO ST Prometheus incurs power and area overheads stemming from the implementation of its three mechanisms. To evaluate the overheads of the digital part of Prometheus, we synthesized RTL implementations of each module and a state-of-the-art mesh router targeting a clock frequency of 1 GHz and using Synopsys Design Compiler with the Nangate 15nm FreePDK library [38]. For the analog parts, area and power have been estimated using designs from the literature that meet the requirements of Prometheus. Conservatively, we do not scale down the cost of analog components to 15 nm. The results are summarized in Table II. Prometheus, when containing the three protection modules, occupies 0.025 mm2 of silicon area and consumes less than 8 mW at 15 nm. However, this is only required in a few cores; whereas the rest can be safe by incorporating PrometheusEavesdrop only, which results in an overhead of 4.67 mW and 0.008 mm2 . In comparison, a 5-port 128-bit 3-VC router synthesized at 15 nm would consume 20.96 mW and 0.023 mm2 , whereas recent 22nm estimations of a wireless transceiver point towards a cost of more than 16 mW and 0.1 mm2 [3], [14]. Prometheus thus incurs a reasonable overhead, especially taking into consideration that several cores share a single transceiver in typical WNoC designs. The overhead decreases proportionately to the sharing degree and can be further cut down if any of the submodules is not really needed. The cost of PrometheusDoS is affordable as the module only consists of a logic circuit that evaluates Eq. 1 and a comparator. The circuit can be broken down into several ﬂoating point registers, adders and multipliers. Since the operation of PrometheusDoS is not time-critical, we apply optimization single shared wireless channel, both of which PrometheusDoS handles. To prevent eavesdropping, a hash-based authentication [12] has been proposed which incurs unacceptable latency overheads as we demonstrated in Section V-C. There has been no work on countering spoof attacks in a WNoC to the best of our knowledge. Prometheus is the ﬁrst work in WNoCs on a comprehensive solution to provide security. Secure Wired Networks-on-Chip. In Wired NoCs, researchers have looked into mitigating HTs DoS attacks via performing deep packet inspection [4] and injecting faults in an attempt to activate HTs in order to detect them and prevent them from creating DoS packets [19]. prevent HT DoS attacks [40]. Spoofs can be handled by standards for securing NoCs and the access rights to memory units [5]. Anti-eavesdropping using an AES-like symmetric key encryption combined with an asymmetric key encryption has also been explored [7]. Secure Wireless Sensor Networks. Most wireless networks use heavyweight software-based solutions that are unsuitable in a WNoC environment. Lightweight solutions are used in wireless sensor networks. One proposal combats spooﬁng, secrecy, and DoS attacks by means of neighbor speciﬁc keys, node speciﬁc sink keys, and data delivery techniques [9]. Another proposal use the RC6 stream cipher [8]. V I I I . CONC LU S ION In this paper, we proposed a new microarchitecture to secure WNoCs from vulnerabilities associated with wireless communications. Our scheme, called Prometheus, is a lowcost drop-in hardware-only solution that detects and reconciles HT or faulty hardware in the WNoC. Results show that with small increases in power, area, and latency, Prometheus can protect the chip against DoS, spoof, and eavesdropping. PrometheusDoS detects selﬁsh nodes using network broadcast predictions for token-based arbitration and network statistics for CSMA networks. PrometheusSpoof detects a node attempting to spoof the source address of another node using its received RF power proﬁle. Lastly, PrometheusEavesdrop secures communication by properly encrypting messages sent between the two end-nodes. The three components of Prometheus protect and defend the WNoC from attacks that could lead to performance loss and eventual breakdown of the network. "
Accurate Channel Models for Realistic Design Space Exploration of Future Wireless NoCs.,"Wireless Networks-on-Chip (WiNoC) are being explored for parallel applications to improve the performances by reducing the long distance/critical path communications. However, WiNoC still require precise propagation models to go beyond proof of concept and to demonstrate it can be considered as a realistic efficient alternative to wired NoC. In this paper, we present accurate 3D models based on measurements in Ka band and Electromagnetic (EM) simulations of transmission on silicon substrate in the V band and the Sub-THz band. Using these EM results, a time-domain simulation is performed using an On-Off Keying (OOK) modulation based transmission with different PA/LNA configurations. Our results highlight the type of performances and tradeoffs to be considered according to different parameters such as power output and amplifier's gain. By improving the knowledge about the signal propagation, one can conduct precise design space exploration for parallel applications. We discuss the realistic channel modeling and we present also hybrid solutions and associated limitations of WiNoC architectures. We conclude the paper with research directions to be explored to make WiNoC a reality.","Accurate Channel Models for Realistic Design Space  Exploration of Future Wireless NoCs  Special Session Paper  Ihsan El Masri  Lab-STICC / UBO  Brest, France  ihsan.elmasri@univ-brest.fr  Pierre-Marie Martin  Lab-STICC / UBO  Brest, France  pmartin@univ-brest.fr  Hemanta Kumar Mondal  Lab-STICC / UBS  Lorient, France  kumar-mondal@univ-ubs.fr  Rozenn Allanic  Lab-STICC / UBO  Brest, France  allanic@univ-brest.fr     Thierry Le Gouguec     Lab-STICC / UBO     Brest, France     legouguec@univ-brest.fr  Cédric Quendo  Lab-STICC / UBO  Brest, France  quendo@univ-brest.fr  Christian Roland  Lab-STICC / UBS  Lorient,  France  christian.roland@univ-ubs.fr  Jean-Philippe Diguet  Lab-STICC / CNRS  Lorient,  France  jean-philippe.diguet@univ-ubs.fr power and area efficient. Currently, CMOS-compatible NoC  with millimeter-wave (mm-wave) wireless  links proves  reasonable bandwidth and energy efficiency. With technology  advances, mm-wave  transceiver can also provide high  bandwidth to fulfill the application needs.[2]  However, WiNoC still suffer from important limitations.  The first one is the power consumption of the analog part of  the transceivers with its order of magnitude (tens of mW)  larger than the power consumption of digital components.  Apart from the transceiver component in WiNoC, channel  modeling is one of the major concerns. The multiple important  radio parameters with strong impacts on communication  quality and efficiency are not considered or based on simple  and non-meaningful models. Accurate models are complex but  signal propagation in air or silicon layers must be taken into  account with its physical phenomena implications such as  multipath interferences, metal interference structures and timedomain dispersion [3]. Thus, there is a need of a detailed  investigation on channel modeling and antenna models before  proceeding to a complete realization of the wireless links at a  network level.  The BBC project (on-chip wireless Broadcast-Based  parallel Computing) [4] has as an objective the realization of  wireless interconnects for parallel computing application. A  complete study is entailed from the radio and physical layer  (i.e. the design and study of an integrated antenna /transceiver  and of the propagation channel) through the MAC layer to the  digital layer (broadcast protocol/architecture).  In this article, we present in the context of BBC an accurate  model based on measurements and electromagnetic (EM)  simulations of transmission on silicon substrate, in order to  facilitate  the  efficient  design  of  on-chip wireless  communication infrastructure. After validating the problems  caused by the silicon substrate by measurement, we propose a  solution and test its feasibility by EM simulation on 2 different  simulation tools and deduce its implications and benefits on the  overall system. The retained multilayer architecture for BBC  project is illustrated in Fig. 1.  Abstract— Wireless Networks-on-Chip (WiNoC) are being  explored for parallel applications to improve the performances  by reducing the long distance/critical path communications.  However, WiNoC still require precise propagation models to go  beyond proof of concept and to demonstrate it can be considered  as a realistic efficient alternative to wired NoC. In this paper, we  present accurate 3D models based on measurements in Ka band  and Electromagnetic (EM) simulations of transmission on silicon  substrate in the V band and the Sub-THz band. Using these EM  results, a time-domain simulation is performed using an On-Off  Keying (OOK) modulation based transmission with different  PA/LNA configurations. Our results highlight the type of  performances and tradeoffs to be considered according to  different parameters such as power output and amplifier’s gain.  By improving the knowledge about the signal propagation, one  can conduct precise design space exploration for parallel  applications. We discuss the realistic channel modeling and we  present also hybrid solutions and associated limitations of  WiNoC architectures. We conclude the paper with research  directions to be explored to make WiNoC a reality.    Keywords—Dipole antennas, electromagnetic propagation,  network-on-chip, parallel application, wireless channel modeling   I. INTRODUCTION   The emergence of the High-Power Computing (HPC)  applications,  leads  to manycore  architectures, which  necessitates the use of efficient network-on-chip. However, the  future manycore architectures will require large NoC that can  lead to prohibitive latencies due to long multi-hop paths.  Parallel computing relies on cache coherency protocols and  synchronization mechanisms (locks, barriers, conditions). The  efficiency of cache coherence protocols and synchronization is  strongly penalized by multi-hop paths and by sequential  messages. There is a great need of alternative solutions to  avoid this penalization in parallel computing.   Wireless Networks-on-Chip (WiNoC) is considered as a  promising solution in the context of manycore architectures to  replace the multi-hop communications [1]. The use of wireless  links allows single-hop long distance communication between  distant nodes. WiNoC also allows taking benefit of real  broadcast capabilities. Most existing WiNoC architectures are  based on On-Off Keying (OOK) modulation technique as it is  978-1-5386-4893-3/18/$31.00 ©2018 IEEE wireless channel [21]. In [3], propagation mechanisms in  intra-chip channels is explored and shows the variation of path  loss factor and propagation delay in free space transmission.  In [22] the WiNoC propagation problems involved with  channel model on Silicon is addressed. However, no effective  solutions were provided without modifying  the CMOS  structure.  None of the existing works thoroughly examine the effects  of the complex intra chip geometries on wireless channel and  signal propagation characteristics that can have significant  impact on circuit design, network and system performance  notably when the carrier frequencies increase. Thus, we  provide an initial deep EM analysis of the channel, to be  followed later by the effects of the metal interferences  structures.  III. PROPAGATION CHANNEL MEASUREMENT AND ANALYSIS IN  FREQUENCY DOMAIN   A first study concerns the design and the electromagnetic  analysis of the antenna and of the propagation channel.  Usually, in frequency domain, the channel is characterized  through the scattering parameters (S-parameters) representation  (S12 and S21 are the transmission coefficients and S11 and S22 are  the reflection coefficients). Such parameters are computed by  using EM field solver simulation  tools or by direct  measurement from realized prototypes. In this paper, Sparameters are used to investigate and estimate the channel  behavior in several configurations of carrier frequencies. First  of all, we validate the numerical electromagnetic model by  comparison with measurements in Ka band and then we  highlight the problem of multipath reflection of the EM wave  on the boundary between air and silicon.  A. Propagation Channel Measurements in Ka Band  To be able to satisfy high data rate and small dimension  antennas, the use of Extremely High Frequency (EHF) band  (30-300 GHz) as the frequency range for propagation is  investigated in this work. This choice leads to high data rates  due to the high bandwidths expected. It is also interesting for  the reduction of the size of the antenna due to the wavelength  decrease. In order to validate the EM-Simulation, we have  realized antennas in Ka band (26-40 GHz).  Based on the architecture retained (Fig. 2) and to limit the  complexity of the environment, we have studied the simplest  propagation channel model on a simplified CMOS structure.  It consists of printed dipole antennas as they are adequate to  the WiNoC  in broadcast applications (omnidirectional  radiation pattern and simple design). The antennas were  designed based on CMOS-like Structure with grounded High  Resistivity silicon (HR-Si) with: a relative permittivity           εr = 11.9, a resistivity of ρ = 2.5 kΩ.cm and a thickness:    Hsub = 655 μm. A small layer of 330 nm of SiO2 (εr = 4) is  placed on the top level as an isolation. The dipole’s length is  close to Ld = λg/2, with λg is the guided wavelength. The layer  and the design of dipole antenna are presented in Fig. 2. In  order to facilitate the probe measurements a transition  between GSG (Ground Signal Ground) pads and the dipole  antenna has been designed.  Fig. 1 : The different layers of a Wireless network on Chip  The remainder of the paper is organized as follows: a brief  overview of existing channel modeling works is presented in  Section II. Section III describes the propagation channel  measurement and analysis in frequency domain. The time  domain analysis of propagation channel is illustrated in Section  IV. Overall summary of this work including feasibility,  scalability and overheads is discussed in Section V. We  conclude our work in Section VI.   II. RELATED WORKS  Emerging  interconnect architectures  tend  to  reduce  performance  limitations of multi-hop communication  in  conventional wired NoCs. Four emerging interconnects are  explored to address the long-range multi-hop communication  bottleneck [5]: three-dimensional (3D) [6], photonic [7],  RF/wireless NoC [8]. The mm-wave based WiNoCs have  emerged as one of the promising solutions for scalable,  energy-efficient CMOS compatible  technology. Another  alternative to wireless interconnects is an inductive/capacitivecoupled 3D integration technology [9], [10], however it  produces electromagnetic interferences through the unwanted  coupling. Thus, we choose to use mm-wave interconnects as it  is the most adequate for broadcast application.  The WiNoC system has two main components on the  physical layer: an antenna and a transceiver. The OOK  modulation is generally envisaged for WiNoC as associated  low power transceivers are easily implementable in CMOS  technology [11], [12], [13], [14]. The OOK modulation is the  simplest amplitude-shift keying modulation with a data rate up  to half of the available bandwidth. It is convenient for many  high-speed applications and short-distance systems as in  WiNoC case.   The on-chip integrated antennas for WiNoC operating in  the GHz-range have been implemented in [15], [16], [17],[18],  [19], [20]. Though several works highlight the advantages of  WiNoC architectures, however, it is also important to note that  some existing analyses are not based on physical  implementations of on-chip wireless links and some analyses  make the fundamental assumption that signal propagation is  through the free space. Apart from these, few works have  analyzed the behavior of signal propagation in intra-chip          Fig. 2: Design of dipole antenna: (a) stack layer; (b) 3D view of dipole  antenna with feeding transition  The test structures have been realized at the Laboratory of  GREMAN in Tours – France, based on the procedure  explained in [23]. The measurements have been done at the  Lab-STICC laboratory in Brest – France using a probe  station.  The results presented in Fig. 3 and Fig. 4 show a good  agreement between the simulations (on the EM simulation  Software HFSSTM) and the measurement results which  validate the EM-simulation tools and modeling.  The matching of the antennas is presented Fig. 3. In both  cases measurement and  simulation, a minimum of   S11= -37 dB is obtained at 37 GHz. The presence of multiple  resonance frequencies on the reflection parameter S11 is  observed. The difference between the -10 dB matching  bandwidth in simulation Fh1-Fl1 = 6.3 GHz (17.4%) and in  measurement Fh2-Fl2 = 11.1 GHz (30.8%) is due to the  uncertainty on silicon losses, the antenna environment and  the measurement conditions. [24]  A good agreement between the simulated and measured  S12 transmission parameter is shown in Fig. 4. With a  separation between the 2 antennas of 7.5 mm, the maximum  of S12 is -12.5 dB which is similar to other measurements  found in literature [2]. If we consider a variation of -3 dB in  comparison of the maximum of S12 parameter, a 4 GHz  bandwidth is obtained. Notice that this band is clearly smaller  than the -10 dB matching criterion previously presented.  Fig. 3: Comparison between measurement and simulation of the reflection  parameter S11 of the structure  Fig. 4: Comparison between measurement and simulation of the transmission  parameter S12 of the structure  Similarly to the resonances observed in adaptation case,  we notice the presence of the transmission dips in the  transmission parameter S12. The dips limit the -3 dB  bandwidth. They are due to a cavity effect caused by the  difference of relative permittivity between the silicon and the  surrounding air (11.9 vs. 1).  This interface silicon/air behaves like magnetic walls  around the silicon; associated with finite dimensions, it  implies destructive  interferences  [22]. The difference  between rays traveling through different paths, mainly the  path that goes directly in the air and the path that goes in the  silicon also implies transmission dips.   B. EM Simulations in V band    To really highlight this phenomenon of cavity effect and  multiple paths which induced dips in transmission we have  conducted several EM simulations with different scenarios as  depicted in Fig. 7. To clearly highlight the cavity effect, two  simulation cases are compared in this section in the V band  (40-75 GHz). At the end of this paragraph, a solution to  overcome the negative behavior of the cavity is proposed.  As already was seen in measurement, the problem resides  in the cavity formed along the silicon substrate and due to the  reflections on the dielectric/air interface, the ideal case would  be to consider the lateral dimensions of the dielectric substrate  as infinite (i.e. radiation boundaries on the air box surrounding  the substrate). In reality, the substrate will have finite  dimensions and surrounded by air.   So, the 2 cases simulated and compared are:  1. Ideal scenario: Antenna on HR-Si with infinite lateral  dimensions simulated using radiation boundary on the  silicon substrate edges Fig. 7 1).   2. Real scenario: Antenna on HR-Si with finite lateral  dimensions simulated using an air box surrounding the  silicon substrate Fig. 7 2).  We have studied these scenarios considering 4 clusters of  cores on a chip, and therefore 4 antennas are placed on the  silicon substrate as per the Fig. 5. The distance of separation  between 2 adjacent antennas is set to 10 mm (the distance  between diagonal antennas is of 14.1 mm). The characteristics  of the substrate were kept the same as those used in  measurements in Ka band.                   Fig. 5: Scenario used for the 4 antennas  The comparison between the 2 cases has been conducted on  the EM SW CST Microwave Studio© (Similar results were  validated on HFSS as well) and the Scattering S-parameters are  presented in Fig. 6.  The reflection parameters S11 ((cid:1)Sii) (Fig. 6-a) shows a very  different behavior between the two cases. In the case (1) with  infinite dimensions a single resonance corresponding to the  dipole resonance is observed as well as a large matching  bandwidth (BW) that extends from 42 to 74 GHz (>50%). In  case of finite dimensions, multiple resonances are present and  it is difficult to identify the resonance due to the dipole.  The transmission parameter between diagonal antennas S21  looks stable on a large BW in the case (1) and we can note a -3  dB BW over 30 GHz. While the chaotic parameter obtained in  the case of a finite substrate dimension (2) will make the  transmission impossible.  For the transmission parameter between adjacent antennas  S31 (and its equivalents S41), the same remarks can be said as  for the diagonal transmission. Nevertheless, in the case (1),  more  fluctuations are observed which may  limit  the  transmission bandwidth (multiple bands of 4 GHz around  multiple frequencies are discernible). In the next section, we  present a solution to avoid the cavity behavior.  C. Proposed Solution Simulations in V band   As the ideal case (1) is practically not realizable because of  the limited dimensions of the chips, and to get back to this case  we suggest surrounding the Silicon layer by an absorbing  material layer. In this absorbing layer, the waves will be  evanescent before reaching the air and thus the reflections  would be reduced significantly. This proposed solution is  called thereafter case (3) and is illustrated in Fig. 7-3. We can  see in the literature propositions for absorbent materials like  graphene as an example [25].  A comparison between the cases (1) and (3) have been  made using EM-simulation and the S-parameters results are  presented in Fig. 8. The use of an absorbing layer permits to  limit the number of resonances and to obtain a smooth  variation like the infinite dimensions case. The contribution of  the EM-absorbent can also be shown on the electric field  distribution as depicted in Fig. 9. The fields’ cartography in the  case (2) seems chaotic, a huge number of modes interfere  which is typical of an oversized cavity while in the case (3)  fields distribution can be compared to the propagation in an  infinite environment.  Lower  levels of  transmission are observed between  adjacent antennas than between the diagonal antennas despite  that the separation distance is higher for the diagonal antennas  than for the adjacent ones. This is due to the radiation pattern  of the grounded dipole antenna which presents lower gain in  the adjacent antenna’s direction than in the diagonal antenna’s  direction. Nevertheless, the proposed solution helps to bypass  the silicon substrate problems. To avoid this directivity  problem, a solution is to use monopole antennas placed inside  the silicon substrate.  D. EM Simulations for Monopole Antennas in Sub-THz band  As can be seen in the above paragraph, the antenna  directivity and placement plays an important role in improving  the transmission for the WiNoC channel. This hypothesis was  studied in [17], [26], [27]. But in each of these publications no  deep EM analysis was provided.  Fig. 6: S-parameters comparison for the ideal (1) and real (2) cases: (a) S11;  (b) S21; (c) S31  Fig. 7: The 3 scenarios under comparison                  EM simulation has been done considering a Silicon  substrate surrounded by an absorbing layer as explained  previously. The distance of separation between 2 adjacent  antennas is set to 10 mm and between the diagonal antennas  would be 14.1 mm.  The simulations have been conducted on HFSSTM and the  results of S-parameter are shown in Fig. 11 for the reflection  parameter and in Fig. 12 for the transmission parameters.   A good matching (S11<-20db) is obtained and the -10 dB  bandwidth is extended to 62 GHz which represents a relative  band of 31%.  As for the transmission parameters S21 & S31, we can see  fluctuations that stay in a level between -25 and -35 dB. This  behavior may be explained by  the multiple reflections  vertically through the layers [32] [33] than by the horizontal  reflections on the edges and borders of the substrate interfaces.  The use of omnidirectional antennas  implies higher  transmission between the adjacent antennas than the diagonal  antennas as the adjacent antennas are closer.  However, using high carrier frequency implies lower  transmission levels for a same distance. This is due to the  FRIIS equation who expresses the ratio of received power Pr to  transmitted power Pt in 3D space as:       (1)  Where Gt and Gr represent the antenna gains, D is the  separating distance between the two elements and l is the  wavelength and a is the channel attenuation. So, the greater the  frequency, the lower is this ratio.  Fig. 8: S-parameters comparison for the ideal (1) and the solution (3) cases:  (a) S11; (b) S21; (c) S31  Fig. 9: Fields distribution in the cases (2) and (3) in V band  As broadcast applications are addressed, omnidirectional  vertical monopoles are good candidates [28]. In order to have a  reasonable height compared to the substrate height, these  monopoles have been designed in the Sub-THz band (at  200 GHz on silicon the wavelength is of ≈435μm, so a quarterwavelength monopole would be after optimization of ≈143μm  height excited by a coaxial cable conform with  their  dimensions) as it is presented Fig. 10. These monopoles could  be metallic if the technology permits, otherwise carbon  nanotubes could be integrated [29], [30], [31].   Fig. 11: S11 parameter for the monopole antenna  Fig. 10: Monopole geometry for 200 GHz  Fig. 12: S12/S13 parameter for the monopole antenna                        To evaluate the channel behavior in time domain, the S  parameters obtained by measurements or EM simulations have  been used in a very simple circuit modeling to obtain the eye  diagram characteristics. This part is presented in the next  section.   IV. TIME DOMAIN ANALYSIS OF THE PROPAGATION CHANNEL  A. Transceiver Design and Time Domain Simulations in Ka  Band   To assess the channel performances, we have used the  block diagram of a simple OOK transceiver shown in Fig. 13.  The transmitter consists of a binary sequence generator and a  sine wave generator multiplexed in an OOK modulator. The  channel is formed by the touchstone file (.sNp) extracted  from the measurements or the EM simulation between 2 or  more antennas. The receiver includes a Low Noise Amplifier  (LNA) of variable gain G and an envelope detector to recover  the signal. Eye diagrams help us to check the state of the  signal after each step and precisely at the receiver side.  The specific advantage of this transceiver is besides its  simple design, the absence of a power amplifier at the  transmitter side which will help us later to decrease the power  consumption.  Based on the above design, the performances of the  measured Ka band channel have been evaluated using ADS©  circuit simulator. The aim of this simple study is to estimate  the Signal to Noise Ratio (SNR) in several configurations of  binary rate and amplifier gain. The noise figure of the LNA  have been set arbitrarily at 10 dB and the filter of the envelop  detector is simply modeled by RC cells. The simulation  results are summarized in the Table 1. The representation of  the 1V level: L1, the 0V level: L0, the height H and the width  W are presented in Fig. 14.  The SNR is estimated using:  (2)  Where d1 and d0 are the noise levels for L1 and L0  respectively (also illustrated on Fig. 14).  The SNR depends both on the binary rate Fbin and on the  global gain G of the transmission chain. The higher the data  rate Fbin is, the higher the global gain G should be and thus  the higher the power consumption.   Fig. 13: Electric circuit for OOK simulation and eye diagram modeling  Fig. 14: Basic parameters obtained on ADS for the channel evaluation  Table 1: Main parameters of the Eye Diagram of the measured channel in Ka  band  Global (PA +  F bin.  (mV) W(ps) SNR  H  LNA)   (dB)  (Gbps)  Gain (dB)  0  20,0  447.5  17.5  5  110,0  470,0  20.9  10  280,0  462,5  21.0  x  160,0  5.1  58,0  205,0  12,7  181,0  207,5  13,7  L1  (mV)  28,0  140,0  355,0  Not significant  122,0  319,0  L0  (mV)  0,0  1,0  5,0  2  4  5  10  6,0  22,0  In Ka band, based on the channel measurements results, we  set the carrier frequency to 35 GHz, for a data rate of 2 Gbps,  no global gain implies an SNR of around 17 dB which seems  sufficient for WiNoC applications [5].For a data rate of 4  Gbps, the global gain needed to obtain an SNR greater than 12  dB is around 5 dB. In both binary rate cases, the use of 10 dB  global gain ensures a good level of SNR implying a bit error  rate  lower  than 10-12 which  is adequate for WiNoC  applications.  B. Time Domain Simulations in the mm Wave Band   Based on the EM simulations, we use here the same  transceiver design used earlier for the measurement in Ka  band, to realize the time domain simulations for the case (3) in  mm Wave band for the monopole antennas designed with a  surrounding absorbing layer and a carrier frequency of  200 GHz.  The results of the eye diagram simulations are resumed in  the Table 2 and Table 3 corresponding respectively to the  adjacent antennas transmission case (10 mm distance) and the  diagonal antennas transmission case (14.1 mm distance). The  noise figure of the different amplifiers is set to 10 dB.   Due to the Friis relation, using higher frequencies implies  the need of a greater amplifier gain. This is why to achieve a  data rate greater than 4 Gbps, it is necessary to use at least a  global gain of 15 dB; the greater the data rate, the higher the  gain needed. As an example, if 8 Gbps is needed, a global gain  of 25 dB minimum is required to achieve a SNR > 9dB.  The ripple of the S-parameters limits the band, while if the  response is flat, higher data rates are possible. This drawback  can be compensated by using a numerical equalizer as it has a  limited power consumption compared to the analog part. The  lower the transmission parameters level, the higher the global  gain in the chain must be.                  L0  (mV)  L1  (mV)  Table 2: Eye diagram characteristics for adjacent antenna transmissions in the  mm Wave band  F bin.  Global (PA +  LNA)  (Gbps)  Gain (dB)  15  20  30  20  25  30  (mV) W(ps) SNR  H  (dB)  -  -  7.9  33  138  11  195  145  12  33  105  11  93  105  13  204  105  13.8  18  70  405  62  167  363  3  7  53  3  12  33  6  8  6  L0 (mV)  L1  (mV)  7  50  330  Table 3: Eye diagram characteristics for diagonal antenna transmissions in the  mm Wave band  F bin.  Global (PA  (mV) W(ps) SNR  H  + LNA)  (Gbps)  Gain (dB)  (dB)  15  1  -  -  7  20  5  7  110  8.6  30  37  140  130  11,5  20  Not significant  25  124  10  36  30  283  32  108  V. DISCUSSIONS   100  101  9.5  9,1  8  In this study we first observe that reflections and cavity  effects must be considered to build a realistic WiNoC. We  show that an absorption layer is required to get a workable  solution and  to avoid  intractable placement constraints.  Moreover considering technology constraints and cost, it is  crucial that this layer is CMOS compliant. Another well  identified issue is the channel uniformity with horizontal dipole  antenna. We show that vertical quarter-wavelength monopole  is a possible solution in the Sub-THz Band that can be  integrated in a 200μm silicon layer.  The second point is related to power consumption, which is  one of the main impediments to WiNoC implementation. PA  and LNA are of the most significant sources of power  consumption. PA efficiency is extremely low (17% in [34])  and worse than LNA.  Recent work in 45nm show very  impressive power gain for PA [34] and LNA[35], namely 3.9  and 1.3mw respectively, and the LNA efficiency is three times  better (0.08 vs 0.24pJ/bit). So, we explore the possibility to get  rid of the PA. Our simulations show that a 25dB gain provides  8 Gbps data rate for a 9.5dB SNR, which is enough to retrieve  the information with a simple transmission scheme. If we  assume a classic linear power model with respect to the gain,  we can perform rough estimations. Let’s consider a solution  without a PA and with an LNA only from [34]. We must  increase the gain of the LNA from 16 to 25dB, which means a  power consumption of about 2mW. This estimation must be  compared to 5.2mW that is required for the LNA and PA case  and a global gain of 32dB. On the contrary, note that if we  consider a solution with a single PA from [34], we must  increase the gain of the PA from 16 to 25dB and obtain a  power consumption of about 6mW.  So, it seems to be possible  to use an LNA only, this is a promising perspective since  significantly  lower power consumption means possible  implementation and scalability. In other words, we can expect  WiNoC with a significant number of routers.   Thirdly, let’s consider the frequency Band. 200GHz allows  designing vertical antennas but the drawback is the energy  efficiency that decreases proportionally with the frequency.  Compared to 60GHz, we get a loss of about 6dB, however it is  already included in our simulation scheme and in our results.  Moreover digital communications methods can additionally be  applied to reduce power with lower SNR. This option is  motivated by the power consumption of the digital part, which  is an order of magnitude lower than the analog part. Basically,  (DSSS) [36] with low spread factor and Error-Correcting  Codes (ECC) [37] techniques with high code ratio in order to  maintain high data rates. Finally, digital equalization [36] can  also be used to improve the bandwidth and so the data rate.  we can expect to introduce Direct Sequence Spread-Spectrum  VI. CONCLUSION  Our study highlights the crucial need for realistic channel  models that consider cavity effects and multi-paths. However,  it also shows that WiNoC remains a promising alternative to  prohibitive wired multi-hops paths and to provide efficient  broadcast capabilities if appropriate design, frequency band,  absorbing  layer, antenna design and digital processing  techniques are applied.   The  immediate perspectives are firstly an analytical  accurate channel model, thus we are currently running timeconsuming simulations. Then we intend to provide the details  and the experimental results of the CMOS-compliant absorbing  layer that was briefly introduced in this paper.  Finally, we  intend to combine accurate RF models and digital techniques to  evaluate the consistency of the complete proposed solution.   ACKNOWLEDGMENT   The authors would  like  to  thank  the ""Laboratoires  d'Excellence"" COMINLABS and the TECHYP (the High  Performance Computing Cluster of the Lab-STICC).  NoCs,” in 2014 Design, Automation & Test in Europe Conference &  "
Brownian Bubble Router - Enabling Deadlock Freedom via Guaranteed Forward Progress.,"Deadlocks are a bane for network designers, be it a Network on Chip (NoC) in a multi-core or a large scale HPC/datacenter network. A routing deadlock occurs when there is a cyclic dependence between the buffers of network routers. Most modern systems avoid deadlocks by placing routing restrictions or adding extra virtual channels, in turn hurting performance and adding overhead respectively. In this work, we demonstrate that instead of placing such restrictions, we can, in fact, design routers to themselves guarantee deadlock-freedom, by (i) ensuring that every router always has at least one bubble (i.e., free buffer slot) at any input port, and (ii) this bubble pro-actively moves between input ports. We call this a Brownian Bubble Router (BBR). A BBR guarantees forward progress in any network topology, without requiring any routing restrictions or additional virtual channels. With our BBR design we provide 4× better throughput over state of art deadlock recovery schemes and 40% better throughput over traditional deadlock avoidance schemes in a 8×8 Mesh at negligible area and power overheads.","Brownian Bubble Router: Enabling Deadlock Freedom via Guaranteed Forward Progress Mayank Parasar Ankit Sinha Tushar Krishna School of ECE, Georgia Institute of Technology, Atlanta, GA, USA mparasar3@gatech.edu ankit.sinha@gatech.edu tushar@ece.gatech.edu Abstract—Deadlocks are a bane for network designers, be it a Network on Chip (NoC) in a multi-core or a large scale HPC/datacenter network. A routing deadlock occurs when there is a cyclic dependence between the buffers of network routers. Most modern systems avoid deadlocks by placing routing restrictions or adding extra virtual channels, in turn hurting performance and adding overhead respectively. In this work, we demonstrate that instead of placing such restrictions, we can, in fact, design routers to themselves guarantee deadlock-freedom, by (i) ensuring that every router always has at least one bubble (i.e., free buffer slot) at any input port, and (ii) this bubble pro-actively moves between input ports. We call this a Brownian Bubble Router (BBR). A BBR guarantees forward IN TRODUC T ION progress in any network topology, without requiring any routing restrictions or additional virtual channels. With our BBR design we provide 4× better throughput over state of art deadlock recovery schemes and 40% better throughput over traditional deadlock avoidance schemes in a 8x8 Mesh at negligible area and power overheads. Index Terms—Computer architecture, Network-on-chip, Interconnection network, Deadlock I . In interconnection networks, a deadlock is deﬁned as a cyclic dependence between router buffers that renders forward progress impossible, since every upstream packet in the cycle waits indeﬁnitely for the downstream packet to leave. Fig. 1(I) shows an example. Network designers put a lot of effort in making sure that the network is guaranteed to be deadlock free. This is because deadlock freedom is a correctness issue rather than a performance problem. Naturally, networks need to be formally proven to be deadlock free [1], [2] for any trafﬁc ﬂow, as one may not know, in general, what kind of trafﬁc/runtime condition the network would be subjected to when deployed. In addition to dynamic trafﬁc conditions, there could also be runtime changes in the network topology due to faults, or power gating of network components. This makes the problem of deadlocks even more challenging [3], [4], [5]. Almost all modern networks use one of the following two techniques to avoid deadlocks - turn model or vc-partitioning. A turn model leverages Dally’s theory [1] to ensure that a cyclic dependence will never get created for any trafﬁc pattern, by restricting certain turns from never being taken. This guarantees deadlock-freedom, but comes at the cost of reduced path diversity. VC-partitioning based solutions leverage Duato’s theory [2] and ensure that there is at least one escape VC that maintains turn restrictions (and is thus guaranteed to be deadlock-free via Dally’s theory [1]) while the remaining VCs can allow any turn. This guarantees deadlock-freedom, but comes at the cost of more resources (VCs) within each router. For example, an escape-VC based solution requires at least two VCs in a mesh, and at least three in a dragon-ﬂy [6]1 . Bubble Flow Control (BFC) [7], [8], [9] is another technique for deadlock avoidance that works on the principle that ensuring the presence of one bubble within a ring via controlled injection can guarantee forward progress. Unfortunately, it only works in ring (and by extension Torus) topologies. There has been another class of solutions arguing for deadlock detection and recovery, since deadlocks are quite rare [10], [11], [12]. However this does not come for free, and requires expensive circuitry and overhead to detect deadlocks and extra buffers added at design-time to be used for the recovery process [10], [11], [12]. In this work we make a case for a new perspective for guaranteeing deadlock freedom. Instead of avoiding deadlocks or recovering from them, we show that we can allow cyclic dependences and deadlocks to form, but ensure that they never persist by guaranteeing forward progress through every router. We deﬁne forward progress as a requirement for every packet to eventually leave the current (upstream), router and move to its downstream router. This begets a key question: How can a router ensure forward progress when that depends on the state of the network at the downstream routers (and may in fact cycle back on to the upstream router in case of a deadlock). We show that this can be ensured if we guarantee the following invariant: for every buffered packet, there will eventually appear at least one free buffer (i.e., bubble) at the desired input port of its downstream router. Theoretically, this is sufﬁcient to guarantee deadlock-freedom, since the packet can keep moving forward. To guarantee this invariant, we introduce the novel idea of a brownian bubble (BB), which is a bubble that keeps moving through the input ports of the router at a certain frequency. The bubble is not an additional buffer - instead one of the empty VCs (say VCi ) at one of the input ports (say port pa ) of the router acts as a BB at a certain time. Bubble movement essentially means moving a packet from a full VC (say VC j at port pb ) to the empty VCi within the router. From this point on, VC j at port pb is now the BB. BBR works with arbitrary topologies and routing algorithms, requires no turn-restrictions, or additional VCs. It can enable fully-adaptive routing with just 1 VC, unlike current solutions. We observe 1.4-4 × higher throughput on average across trafﬁc patterns on a 64-core mesh compared to the state-of-the-art deadlock avoidance and recovery schemes respectively. 1Here we are talking about the minimum number of VCs required purely for the sake of avoiding routing deadlocks. A network will use more VCs to avoid protocol-level deadlocks or to avoid head-of-line blocking 978-1-5386-4893-3/18/$31.00 ©2018 IEEE Fig. 1: Walkthrough [Left to Right] shows how brownian bubble movement helps in breaking deadlock cycles. It allows a deadlocked packet to move to some other port in its router, and other packets, not part of the deadlock ring, to acquire its place and eventually leave the router, thus breaking the deadlock ring. In this example, it takes two bubble movements to break the deadlock. I I . BACKGROUND AND R ELAT ED WORK A rich body of work has been explored in NoCs to provide deadlock-freedom. We classify these into three broad categories: Deadlock avoidance, Deadlock detection and recovery and Deﬂection and brieﬂy discuss prominent techniques in each. A. Deadlock Avoidance 1) Turn Model: Routing algorithms are designed such that there are no cycles in the extended Channel Dependency Graph (CDG). Example of such routing algorithms are dimensionordered XY routing, West-First routing [13], in a Mesh and Up-Down [14] routing in irregular topologies. These routing algorithms work by restricting certain turns that a packet can take, thus effectively restricting the formation of deadlock cycle. The main shortcoming of the Turn model routing is that the path diversity is not fully utilized. Moreover, in case of dynamic faults in the network, this technique can still deadlock [3], [11]. 2) Virtual Channel (VC) Partitioning: VC partitioning overcomes the shortcomings of the turn model by adding virtual channels (VCs) and ensuring that there is no cyclic dependence between the VCs. Packets need to change VCs on every turn, or on some turns. Each VC by itself implements a turn-model, and is thus deadlock-free. Examples of this implementation include O1TURN [15] in a mesh and UGAL [16] in dragon ﬂy. A more optimized version of this design is an escape VC (eVC) based design that allows all turns to be taken by all VCs except one - the Escape VC, which implements a turn-model. VC partitioning techniques allow better link utilization since together all VCs do allow all links to be used. However, they introduce area overhead as now we need at least 2 (or more) VCs per input port in the network. 3) Flow-Control-based Schemes: Bubble Flow Control [7] is used in rings and provides deadlock avoidance is by runtime injection restrictions. There is no turn restriction imposed on the path taken by the packet, so the CDG may be cyclic. However, injection into the ring is restricted such that there is at least one bubble in every router [7], [9] or in the ring [8]. BFC can also be extended to a torus by viewing a turn (say X to Y) as an injection into the Y ring [9]. Unfortunately BFC only works in rings. Extending the idea of injection restriction to arbitrary topologies would require prior knowledge of the trafﬁc ﬂow, and is thus not general-purpose. Adaptive ﬂit dropping [17] allows ﬂits to be dropped if they fail to traverse a switch beyond a threshold number of times, which frees up buffers, thereby providing deadlock-freedom (and reducing congestion). BBR leverages the same principle as BFC in terms of requiring a bubble to ensure forward progress, However, in BFC [7] and its variants [9], [8], the bubble moves only when a packet moves from one router to the other, while in BBR, our brownian bubble pro-actively moves through different ports of the router which allows us to dynamically introduce bubbles in deadlocked rings. BBR thus works with arbitrary topologies and does not require any injection restriction. 4) Spanning Trees: In irregular network topologies - either by design or due to faults or power-gated components, deadlocks are avoided by constructing spanning trees over the topology [3], [18], [19], [20]. These techniques ensure deadlock-freedom by routing packets via the root thereby avoiding cyclic paths, but leads to non-minimal paths between certain source-destination pairs, and also introduces additional complexity for spanning tree construction [3], [18], [19], [20]. B. Deadlock Recovery Deadlock recovery proposes to detect the deadlock during runtime and recover from it [21], [11], [12]. The rationale is that deadlocks are often quite rare - therefore turn-model restrictions or VC over-provisioning done by avoidance schemes is an overkill. However, because of complicated logic for tracking dependence cycles in the network and then providing guaranteed mechanism of recovering from it, none of these scheme have made it to commercial designs. C. Deﬂection Deﬂection-based networks misroute packets whenever there is contention [22], [23], [24]. Since a packet never stalls, these are naturally deadlock-free. However, mis-routing creates congestion in the network [22], [23], and higher energy consumption in the links due to non-minimal routes. A. Key Concept 1) Walk-through Example of Bubble Movement: Fig. 1 illustrates functioning of of BBR in detail. As shown in Fig. 1(I), packets present in the South, West, North and East input ports of router 1, 2, 3 and 4 respectively are currently in a deadlock. The direction (N/S/E/W) written on the packets is the direction in which the packet is destined to go, in order to reach to its destination. The empty VC at the West input port is tagged as a “bubble”. For the purposes of this example, consider only Router 3 (R3). In Fig. 1(II), the packet at the North input port of R3 is moved into the bubble - thus the packet now sits in the West input port, while the VC at the North input port is now free (and tagged as the BB). As explained earlier, to maintain the invariant of keeping one BB per router, no external packet (from another router) is allowed to enter the BB. Thus even though there is a free VC in the deadlocked loop, the deadlock persists. Next, in Fig. 1(III), the bubble moves to the East input port, and the packet sitting there comes to the North input port.‘ This packet wanted to go North (to router 2) which is unblocked (since Router 2’s South input port is free), and will thus leave the router. This is shown in Fig. 1(IV). At this point, it will free the VC at the North input port, into which the packet sitting in Router 2’s West input port can move, leading to forward progress. Fig. 1(IV) shows the ﬁnal state of the network, where the four packets originally part of the deadlock ring have made forward progress and now there is no deadlock. Note that the direction initial written on the packets still represents the direction in which the packet needs to move in order to reach to its destination. 2) Proof of Deadlock Freedom: Suppose there are one or more deadlocked rings going through a router. Deﬁnition: Unblocked Packet. A packet that will eventually leave the router because of a free credit at its downstream router. Theorem: As long as a router has (a) at least one empty input VC (not the BB) or (b) at least one unblocked packet, BBR breaks any cyclic dependence going through this router. Proof: Bubble movement can move one of the deadlocked packets into the empty VC (in case (a)), or into the VC originally occupied by the unblocked packet (in case (b)). In case (b), the unblocked packet will eventually leave the router, leaving an empty VC equivalent to case (a). The introduction of an empty VC into the deadlocked ring can guarantee forward progress, breaking the deadlock. The walk-through example in Fig. 1 had an unblocked packet in Router 3. However, this might not always be true. Next, we discuss how an unblocked packet can be introduced into the router, in case it does not exist, via a Bubble Exchange. 3) Bubble Exchange: With the current BM technique discussed so far, it is still not guaranteed that deadlock will be broken, depending on the output directions of the buffered packets. Consider the three cases present in Fig. 2. Each column in the ﬁgure shows a deadlock scenario, bubble exchange and ﬁnal state of the router after exchange. Like before, the initial of the direction present on the packet is the direction in which the packet is intended to go. Colored packets in the router are the ones which are involved in the deadlock ring, there state Fig. 2: Bubble-Exchange: Deadlock corner cases can still occur with simple bubble movement technique (§III-A1). In each column, the ﬁrst row shows the deadlock ring with involves 2, 3 and 4 routers respectively; the second row shows the bubble-exchange state in action, and third row ﬁnally shows the routers state after deadlock is broken. I I I . BROWN IAN BUBBL E ROUT ER Brownian Bubble Router proposes to instrument routers with the ability of moving a bubble across the input ports of the router. A bubble refers to an empty packet-sized input VC in a virtual cut through router. This provides an opportunity to a packet that is currently part of a deadlock cycle, to move into an empty VC in the router, potentially breaking the deadlock cycle. We have assumed each VC to be as deep as 1-packet in this work. See §III-B for extensions. At the beginning of the network run, one of the VCs (VC-0 for simplicity) at one of the input ports (excluding the injection port) of each router is tagged as the “brownian bubble (BB)”. The invariant of BBR is that, there will always be one BB present per router. To maintain this invariant, no packet from any other router is allowed to enter the BB. Otherwise the bubble might get consumed while the deadlock continues. At a certain user-speciﬁed frequency, the BB is moved across input ports. Moving the bubble essentially means tagging some target VC at some other input port as the BB. If the target VC is non-empty, all ﬂits of its packet are explicitly moved into the original bubble - for e.g., for a 5-ﬂit packet, this step takes 5 cycles. If the target VC is empty, some additional credit signals are required, as we discuss later in §III-B2. The frequency of bubble movement (BM) is based on an epoch counter. We use BBR-k to refer to a BM every k cycles, where k is user-speciﬁed. The target VC that the bubble is moved into is also a design choice. For the purpose of simplicity, in our implementation the target VC is randomly selected by an arbiter with higher priority being given to empty over nonempty VCs, to avoid explicit packet moves. is show before deadlock, during bubble exchange and after deadlock in row-I, II and III respectively. Consider Row-I. Case A shows a 2 router deadlock. Packets intending to go East are sitting in the east input port at the ﬁrst router, and those going west are sitting at the West input port of the neighboring router, leading to a deadlock dependence. This would not occur in a baseline design if u-turns are not allowed. However, with bubble movement, this scenario is possible recall that in Fig. 1(III), the North input port of Router-3 houses a packet that wants to go North. In Row-1, Case A, no amount of bubble movement in the two routers can resolve the deadlock, since all packets in the respective routers point to the same direction. From the necessary condition described in §III-A2, this is because of the lack of an unblocked packet in either of the routers. Row-1, Case B shows a 3-router deadlock. Here bubble movement is possible with the packets requesting different output ports. We do not enumerate all possible scenarios here in the interest of space, but all bubble movements will still end up with a 2-router or a 4-router deadlock. Finally, Row-2 Case C shows a 4-router deadlock, where no amount of bubble movement can resolve it, again due to the absence of an unblocked packet in any of the routers. To resolve deadlocks in such scenarios, we introduce the concept of bubble exchange (BE). The idea is to force forward progress of one of the packets by moving it into the bubble at its downstream router, and then recover the bubble by moving one of the packets at the downstream router into this router. BE is initiated by the upstream router when all the neighboring downstream routers that the packets of this upstream router want to get to have an occupancy of N − 1, where occupancy is deﬁned as the number of nonempty VCs in the router across all ports (except local), and N = num in port × num vcs per in port exce pt l ocal . In other words, BE is initiated when the downstream routers are completely full, except their BBs. BE takes two steps: 1(cid:13) Upstream router routes one of the packets to the downstream router to sit at the BB of the downstream router. This is equivalent to the upstream router consuming the BB of the downstream router. This leads to a situation where there are 2 bubbles present at upstream router and none at the downstream router, breaking the BB invariant temporarily. 2(cid:13) The downstream router mis-routes one of its packet to upstream router’s original bubble, to recover its bubble back. The input VC of the packet chosen to mis-route is selected at random, and becomes the BB at the downstream router. Note that steps 1(cid:13) and 2(cid:13) described above, are performed in tandem on bi-directional link connected between the upstream and downstream routers involved in the bubble exchange. Row-2 in Fig. 2 shows bubble exchange in action for all three cases. The deadlocks in all cases are broken in Row 3. Why does Bubble exchange guarantee deadlock freedom? As discussed in §III-A2, BBR works only if a router has an unblocked packet that is guaranteed to eventually leave. Bubble exchange forces one of the packets in the router to make forward progress towards its destination, essentially making it an unblocked packet for the purposes of the proof. In the worst case, a packet might move all the way to its destination via bubble exchange, where it will eventually get consumed2 . Does Bubble exchange require deadlock detection? No. It is important to note that the bubble recovery algorithm is a heuristic for exchanging bubbles between neighboring routers. We do not actually detect the deadlock, thereby do not pay its associated overheads in terms of timeout counters and probes [11]. This implies that there can be false positives. Why is Bubble exchange performed at an occupancy of N-1? In BBR, the necessary condition for a deadlock is an occupancy of (N-1), since the brownian bubble is always empty. Since explicit deadlock-detection is not performed, an occupancy of N-1 triggers a guaranteed forward movement of one of the blocked packets via bubble exchange. This guarantees that there will never be any false negatives, though there may be false positives (i.e., an occupancy of N-1 due to congestion and not a true deadlock). Does Bubble exchange lead to mis-routing? Sometimes, but not always. Bubble exchange always leads to forward progress of at least one packet. In certain cases, the other packet might be moved to a neighbor that is not its actual preferred output port. However, in other cases, such as Fig. 2Case A, both packets might end up making forward progress. Does Bubble exchange lead to livelocks? It is theoretically possible, though extremely unlikely for the same packet to keep getting misrouted as part of the bubble exchange condition at every router it enters, never reaching its destination, leading to a livelock. Livelocks can be avoided by disallowing more than a certain number of misroutes for any packet, like prior works on mis-routing have explored [23]. We do not implement it in BBR for simplicity. Our evaluations show that the number of misroutes is actually quite low. B. Implementation Fig. 3 shows the BBR microarchitecture. In addition to modules such as VC Allocator, Route Compute, Switch Allocator and Crossbar which have their usual function as in a baseline router, we introduce a few additional ones to implement BM and BE. We implemented the BBR modules in RTL, and observed around 7.4% area overhead (Fig. 3) and 4.3% power overhead over a 4-VC baseline router [25] post-layout at 28nm. BBR introduces an additional mux in front of each VC, but meets timing at 1GHz like the baseline. Thus BBR’s additions are extremely light-weight. We also show a a ﬂow-chart of the BBR operation in Fig. 3. Each functional unit speciﬁc to BBR is color-coded with the same color as its microarchitecture counterpart. We describe key components next. 1) Bubble Movement Epoch Unit: Based on the conﬁgurable epoch parameter k, the Bubble Movement Epoch unit triggers a BM every k cycles. BM may be aborted in a special case discussed later in the credit management unit. Bus. We add a small bus inside the router connecting the outputs of all the VCs excluding the port VCs. This is to 2We assume that the protocol is deadlock-free, and any packet in a router may stall but will eventually get consumed by its destination. Fig. 3: Figure showing router micro architecture on the left for Brownian Bubble Router router and ﬂow diagram illustrating the order in which Brownian Bubble Router speciﬁc actions are performed on right. Note that Brownian Bubble Router router concept is generic to any underlying topology, hence number of ports are kept as N for generality of the idea. Here VC stands for virtual channel. Speciﬁc details about each module are discussed in §III-B. The area consumed by the router at 28nm is also shown. facilitate bubble movement between two ports. We chose as bus based on the insight that at any point in time there is only one packet which would be moved to the BB to perform BM. This implies that there will never be contention on this interconnect media, which suits the bus. Also, since we randomly move bubble across input ports by giving preference to empty input ports over non-empty input ports, a bus which connects all input ports ﬁts our purpose. The input to the VCs can be multiplexed between the input link and the bus. This is determined by the arbiter which handles all the multiplexers using control signals (MOV(1), MOV(2), ...). There will never be any contention for the input port of a VC between the link and the bus, since the VC into which a packet is being written from the bus was the BB, and will never receive a packet from the input link. Arbiter. If a BM is triggered, the arbiter chooses the input port for BM by choosing an input VC at an input port in a round-robin manner. Priority is always given to a empty VC, if available. The bus-arbiter unit sends out two signals, the MOV EN and the MOV signals. The MOV EN signal is sent to the bus to indicate that a BM is impending in the next cycle while the MOV signal is used to select the port to where this movement will happen (in other words, the port from where a packet will be read out and put on the bus to be inserted into the current BB). This signal remains active till all ﬂits of the packet have been moved. 2) Credit Management Unit: Credit management is an integral part of the BBR. As mentioned earlier, no packet is allowed to come and occupy the VC tagged as the brownian bubble. This is ensured by not sending a credit for this VC to the upstream router so that it believes that this VC is actually occupied. Thus a BB simply looks like a full VC to the upstream router. During BM, two cases arise. Case I: The bubble is moved to a full VC (i.e., the packet from the full VC is moved into the bubble). In this case, no credits need to be sent to the respective upstream routers. This is because both upstream routers connected to the downstream router believe that the VCs are full - it is agnostic to the fact that one has an actual packet, one is the BB, and the bubble moved between them. Case II: The bubble is moved to an empty VC. The VC that becomes the BB needs to send a decrement credit signal to the upstream router to inform it that this VC is actually not empty. The original VC which was the BB needs to send an increment credit signal to the upstream router signaling that this VC is now free. This is done after one cycle of delay to manage a corner case where the upstream router may have already started sending ﬂits for a new packet into this VC, which is currently on the link. This is handled by aborting the bubble movement as follows: (a) if the upstream router receives a decrement credit for a VC that it has already started sending ﬂits to, it ignores the decrement credit, (b) if the downstream router receives ﬂits into a VC that became the BB in the previous cycle, the original (empty) VC is tagged as the BB again. (c) the original VC sends its increment credit signal after waiting for a cycle only if the above scenario does not occur. 3) Bubble Exchange Unit: Each router keeps track of its occupancy, which was deﬁned earlier in §III-A3. If the occupancy of the router reaches (N-1), where N is the total number of input VCs at all ports of the router (except local), it collects the occupancy of its neighbors. If the neighbors have an occupancy greater than a certain threshold (max threshold is N-1), BE is triggered by setting the EXC ﬂag. This reads one TABLE I: Network Conﬁguration. Network Topology Router latency Num VCs Buffer Organization 8x8 Mesh 1-cycle 1, 2, 3, 4 Virtual Cut Through Single packet per virtual channel Target Networks West-ﬁrst and Escape VC Static Bubble [11] and SPIN [12] BBR-k (k= BM frequency) Deadlock Avoidance Deadlock Recovery Brownian Bubble Router of the packets from the router, and sends it out of the crossbar and output link to the neighboring router. The neighbor in turn sends a packet to this router which is added into this router’s BB. A subtle point to note is that BE does not steal any useful link bandwidth since the occupancy of (N-1) at both routers means that they were unable to send packets to each other via regular switch allocation due to the lack of credits, and so the links between them were anyway idle. BE is a measure of how reactive the router is towards recovering from the deadlock by exchanging the bubble. The occupancy metric we use to trigger BE is just a heuristic. From a correctness point of view, BE can be triggered more proactively or at a ﬁxed time epoch in alternate implementations. C. Adding BBR over Alternate Router Microarchitectures BBR’s underlying mechanism of periodic bubble movement across input ports within a router, followed by occupancydriven bubble exchanges between neighboring routers, can be applied to any input buffered VC router to guarantee deadlock freedom in the network, as we showed in §III-A2. This makes it agnostic to the underlying topology (mesh/high-radix/irregular/reconﬁgurable [26]), routing algorithm (XY/adaptive [27]) and router bypass optimizations [25]. BBR can also work with wormhole routers, but will require additional complexity (such as packet truncation [22]) to manage ordering since parts of the same packet might end up at different input ports of the same router due to BM. IV. EVALUAT ION A. Methodology We model Brownian Bubble Router in the Garnet [28] cycleaccurate NoC simulator. For BBR, we implement a minimal fully adaptive random routing algorithm. We use credits at the downstream router to decide the direction if more than one choice exists. Table I lists the system conﬁgurations we evaluated. We contrast BBR against both classic deadlockavoidance (West-ﬁrst and escape VC) and state-of-the-art deadlock-recovery (Static Bubble [11] and SPIN [12]) schemes. All networks use a single-cycle router. Recall that the rate of bubble movement (BM) is a knob given to the network designer to tune the frequency of bubble movement within the router. We evaluate BBR with multiple BM epoch values and report results with 1 (i.e., every cycle), 64 (every 64 cycles) and 1024 (every 1024 cycles). For BE, we empirically set the occupancy threshold at downstream routers to 4. B. Correctness The primary claim of Brownian Bubble Router is to make sure there is no deadlock that persists in the network. We show Fig. 4: Correctness of Brownian Bubble Router ˙For a ﬁxed number of packets for the simulation, x-axis shows total packets injected in network per node per cycle and y axis shows %age of total packets received at the end of simulation. Different trafﬁc patterns deadlock with fully-random routing, but BBR never deadlocks with any trafﬁc. this in Fig. 4 for a 8 × 8 mesh topology. On y-axis we plot the percentage of packets received over ﬁxed packets injected in the network. X-axis shows the injection rate at which these packets are injected in the network. All trafﬁc patterns use fully random routing. Fig. 4 also shows how sensitive the network is towards the number of VCs present per input port in the router. We see that network deadlocks at much lower injection rate when number of VCs is 1 compared to when it is 4. This is especially stark in bit complement trafﬁc which deadlocks almost immediately in a 1 VC design. BBR performs consistently by delivering 100% packets that are injected in the network at all injection rate for all trafﬁc patterns. C. Performance Next, in Fig. 5, we evaluate BBR against state-of-the-art deadlock freedom techniques for for a 8 × 8 mesh at different VC counts. In the interest of space, we only present results for transpose, shufﬂe, uniform-random and bit rotation trafﬁc pattern for 2 and 4 VCs respectively. With 4 VCs, we observe 37% throughput improvement over WestFirst and escapeVC (deadlock avoidance) on average and 3× improvement over Static Bubble and SPIN (deadlock recovery). With 2 VCs, we observe 44% improvement over WestFirst and escape VC, and .2.5× improvement over Static Bubble and SPIN. For many of the patterns, the performance improvements are higher at 2 VCs as opposed to 4 because of the path diversity provided by fully adaptive routing enabled by BBR in all VCs. In contrast, West-ﬁrst lacks path diversity in the west direction, while escape VC only provides full path diversity within one of its VCs (the other one restricted to west-ﬁrst). With 1 VC, however, we found the latency with BBR to be erratic at a few injection rates. This is because the input port where the bubble resides essentially gets blocked for the upstream router for a period of time, leading to uneven and unpredictable delays until packet reaches its destination. However, it is important to note that a 1 VC BBR design is deadlock-free, as we showed earlier in Fig. 4. We also performed an experiment to understand the performance overhead BBR adds on top of an already deadlock-free routing algorithm, such as XY. In Fig. 6, we plot the reception rate for a baseline XY scheme and various BBR schemes with 2 and 4 VCs. We notice that an aggressive BBR-1 (that tries to perform bubble movement every cycle) leads to a 25% drop in throughput for uniform random and 35% drop in thoughput for bit complement averaged over all VC count . But with higher Fig. 5: Performance of Brownian Bubble Router technique compared against recently proposed deadlock recovery schemes and well known deadlock avoidance schemes such as escapeVC and WestFirst Routing, proving its superiority. Here x-axis shows total packets injected in network per node per cycle and y-axis shows the average latency incurred by packets in cycles. Fig. 6: Overhead introduced when adding BBR over a baseline deadlock-free XY routing algorithm. Here x-axis shows the packets injected in network per node per cycle and y-axis, similarly shows packets received in the network per node per cycle. Fig. 7: Bubble Movement Frequency: y-axis shows ratio of buffer reads (or writes) due to BM over the baseline buffer reads (or writes) and x-axis shows the packets injected in the network per node per cycle. BBR-1 shows the highest BM for bit-reverse compared to other BBR-k; this behavior is opposite in uniform random trafﬁc. This shows distribution of BM across BBR-k is highly trafﬁc dependent. values of the epoch, the drop is only 9%. Recall that at high loads, BE kicks in once the upstream and downstream routers start becoming full, which leads to a performance differential at high loads, even if the BM epoch is set very high. If we restrict BE to occur on a very high ﬁxed threshold, rather than based on occupancy, BBR would have essentially no overhead if the underlying algorithm is inherently deadlock free. D. Bubble Movement and Bubble Exchange Frequency Having shown the correctness and performance beneﬁts of BBR, next we study the potential overhead. As discussed earlier in §III-B, the area overhead for implementing BBR is negligible. However, each bubble movement involves reading a packet out of its current VC and writing it into an empty VC, increasing the total buffer activity. The same occurs in during a bubble exchange as well, across neighboring routers. This can naturally have energy implications. We quantify this overhead in the next set of experiments. In Fig. 7 and Fig. 8 we plot the ratio of additional buffer reads Fig. 8: Bubble-Exchange Frequency: here y-axis shows ratio of buffer reads (or writes) due to BEs over the baseline buffer reads (or writes) and x-axis shows the packets injected in the network per node per cycle. We see that BBR-1 has highest BE over any other BBR-k. (or writes) due to BM and BE respectively over the baseline buffer reads (or writes) for various values of BBR-k, where k is the frequency of BM. In the interest of space, we plot the behavior for two patterns - uniform-random and bit-reverse which show contrasting characteristics. Other trafﬁc patterns showed similar behavior to one of these patterns. Note that a BM between empty VCs does not count as a buffer read/write. In Fig. 7, we highlight that there is no bubble movement up to a certain injection rate; this is because at low loads, more than two VCs are empty across all ports of the router in most cases, so a BM does not need an explicit packet read and write. This shows that BM actually adds no energy overhead, especially at low to medium loads which is the common operating point for most NoCs. At high injection rates, the network is more susceptible to deadlock, and naturally the number of BMs go up as well. One might expect low values of k (i.e., high frequency of BM) to lead to higher number of buffer reads/writes. This can be seen as true in bit rotation, where BBR-1 shows up to 4x more buffer reads/writes than BBR-64 post saturation. Counter intuitively, though, the opposite is seen in uniform random trafﬁc, where a higher values of k actually end up leading to more bubble movements overall. This is because with a low frequency of BM, deadlocks persist for longer and end up requiring more BMs to provide forward progress. This shows a subtle yet important feature of BBR that it is adaptive; hence the energy consumption will be more only if the trafﬁc is susceptible to deadlock. In Fig. 8, we see that BBR-64 and beyond, the number of BEs is negligible. A notable exception is BBR-1 (moving bubble every cycle) which shows the highest number of BEs over any other BBR-k (lower frequency of bubble movement). Fig. 9: A 4x4 Mesh with a faulty link (shown with X). XY routing can no longer work. Traditional deadlock avoidance (Spanning Tree) will disable the use of the grey link to avoid cycles, leading to non-minimal routes. Thus BBR provides higher saturation throughput. This can be understood as follows - high BM ends up moving packets to other free VCs in the router very frequently, leading to more occupancy within the router. This in turn triggers BE more frequently. In contrast, at low BM frequency, the router tends to remain emptier (especially at low loads), leading to fewer forward movements due to BE. In summary, the impact of the BM frequency depends heavily the trafﬁc pattern and injection rate. For uniform random trafﬁc, we observe that BE, not BM actually tends to dominate. Moreover, the energy overhead can be controlled via various knobs exposed to the designers such as the BM frequency and BE occupancy threshold. E. BBR for Irregular Topologies Next, we study BBR performance with an irregular topology as shown in left most sub-ﬁgure in Fig. 9. Here the link between routers 5 and 6 in a mesh is broken, which could be due to various reasons such as power gating or dynamic faults in the network [3]. A challenge with irregular topologies is that traditional turn-restrictions (such as XY) will not longer work - for e.g., any packet from 5 to {10, 11, 14, 15} will have to make a Y to X turn at 9. Similarly, a Y to X turn at 2 will have to be allowed. Such turns could lead to a 5 → 9 → 10 → 6 → 2 → 1 → 5 deadlock. In such scenarios, the deadlock free routing option is to construct a spanning tree [3], [18], [19], [20], which will not allow the use of the link between router 1 and 2 (highlighted in grey) to avoid cycles. BBR does not need any such restrictions and enjoys the full path diversity. This translates to around 40% higher throughput on average over the spanning tree routing algorithm as shown in Fig. 9. V. CONC LU S ION We propose a novel deadlock-freedom scheme called Brownian Bubble Router (BBR) which guarantees forward progress of packets through every router in an interconnection network. This is done by a clever mechanism of circulating one bubble through all ports of the router, and periodically exchanging it between neighbors. This prevents any deadlock from persisting in the network. BBR provides the following advantages over baseline schemes: (i) it is topology and routing algorithmagnostic since BM occurs within the router, and BE between direct neighbors. This makes it highly applicable across domains - homogeneous many-cores, heterogeneous SoCs, faulty (or power-gated) NoCs with missing links/routers, (ii) it requires no turn restrictions or escape VCs, thereby providing full path diversity - which in turn translates to higher throughput, (iii) is extremely light-weight. On a 8x8 mesh, across a suite of trafﬁc patterns, BBR provides 40% and 4× higher throughput on an average, respectively, over state-of-the-art deadlock avoidance and recovery techniques. "
Securing Photonic NoC Architectures from Hardware Trojans.,The compact size and high wavelength selectivity of microring resonators (MRs) enable photonic networks-on-chip (PNoCs) to utilize dense-wavelength-division-multiplexing (DWDM) in photonic waveguides to attain high bandwidth on-chip data transfers. A Hardware Trojan in a PNoC can manipulate the electrical driving circuit of its MRs to cause the MRs to snoop data from the neighboring wavelength channels in a shared photonic waveguide. This introduces a serious security threat. This paper presents a framework that utilizes process variation based authentication signatures along with architecture-level enhancements to protect data in PNoCs from data-snooping Hardware Trojans. Evaluation results indicate that our approach can significantly enhance the hardware security in DWDM-based PNoCs with minimal overheads of up to 17.3% in average latency and of up to 15.2% in energy-delay-product (EDP).,"Securing Photonic NoC Architectures from Hardware Trojans  Special Session Paper  Sudeep Pasricha1, Sai Vineel Reddy Chittamuru3, Ishan G Thakkar2, Varun Bhat1  1Department of Electrical and Computer Engineering, Colorado State University, Fort Collins, CO, U.S.A.  2Department of Electrical and Computer Engineering, University of Kentucky, Lexington, KY, U.S.A.  3Micron Technology, Inc, Austin, TX, U.S.A.  sudeep@colostate.edu, schittamuru@micron.com, igthakkar@uky.edu, varunb@colostate.edu   Abstract – The compact size and high wavelength selectivity of  microring resonators (MRs) enable photonic networks-on-chip  (PNoCs)  to utilize dense-wavelength-division-multiplexing  (DWDM) in photonic waveguides to attain high bandwidth onchip data transfers. A Hardware Trojan in a PNoC can manipulate the electrical driving circuit of its MRs to cause the MRs to  snoop data from the neighboring wavelength channels in a shared  photonic waveguide. This introduces a serious security threat.  This paper presents a framework that utilizes process variation  based authentication signatures along with architecture-level enhancements to protect data in PNoCs from data-snooping Hardware Trojans. Evaluation results indicate that our approach can  significantly enhance the hardware security in DWDM-based  PNoCs with minimal overheads of up to 17.3% in average latency  and of up to 15.2% in energy-delay-product (EDP).  Keywords – Process Variations, Hardware Security, Photonic NoCs  I.  INTRODUCTION  To cope with the growing performance demands of modern Big  Data and cloud computing applications, the complexity of hardware in modern chip-multiprocessors (CMPs) has increased. To  reduce the hardware design time of these complex CMPs, thirdparty hardware IPs are frequently used. But these third party IPs  can introduce security risks [1]-[2]. For instance, the presence of  Hardware Trojans (HTs) in the third-party IPs can lead to leakage  of critical and sensitive information from modern CMPs [3].  Thus, security researchers that have traditionally focused on software-level security are now increasingly interested in overcoming  hardware-level security risks.  Many CMPs today use electrical networks-on-chip (ENoCs)  for inter-core communication. ENoCs use packet-switched network fabrics and routers to transfer data between on-chip components [4]. Recent developments in silicon photonics have enabled  the integration of photonic components and interconnects with  CMOS circuits on a chip. Photonic NoCs (PNoCs) provide several prolific advantages over their metallic counterparts (i.e.,  ENoCs), including the ability to communicate at near light speed,  larger bandwidth density, and lower dynamic power dissipation  [5]. These advantages motivate the use of PNoCs for inter-core  communication in modern CMPs [6].   Several PNoC and photonic core-to-memory interface architectures have been proposed to date (e.g., [7]-[9], [13], [31]-[34]).  These architectures employ on-chip photonic links, each of which  connects two or more gateway interfaces. A gateway interface  (GI) connects the PNoC to a group of processing cores. Each photonic link comprises one or more photonic waveguides and each  waveguide can support a large number of dense-wavelength-division-multiplexed (DWDM) wavelengths. A wavelength serves as  a data signal carrier. Typically, multiple data signals are generated  at a source GI in the electrical domain (as sequences of logical 1  and 0 voltage levels) which are modulated onto the multiple  DWDM carrier wavelengths simultaneously, using a bank of  modulator MRs at the source GI [10], [19]. The data-modulated  carrier wavelengths traverse a link to a destination GI, where an  array of detector MRs filter them and drop them on photodetectors  to regenerate electrical data signals.   In general, each GI in a PNoC is able to send and receive data  in the optical domain on all of the utilized carrier wavelengths.  Therefore, each GI has a bank of modulator MRs (i.e., modulator  bank) and a bank of detector MRs (i.e., detector bank). Each MR  in a bank resonates with and operates on a specific carrier wavelength. Thus, the excellent wavelength selectivity of MRs and  DWDM capability of waveguides enable high bandwidth parallel  data transfers in PNoCs. Similar to CMPs with ENoCs, the CMPs  with PNoCs are expected to use several third party IPs, and therefore, are vulnerable to security risks [11]. For instance, if the entire PNoC used within a CMP is a third-party IP, then this PNoC  with HTs within the control units of its GIs can snoop on packets  in the network. These packets can be transferred to a malicious  core (a core running a malicious program) in the CMP to determine sensitive information.  Unfortunately, MRs are especially susceptible to security  threatening manipulations from HTs. In particular, the MR tuning  circuits that are essential for supporting data broadcasts and to  counteract MR resonance shifts due to process variations (PV)  make it easy for HTs to retune MRs and initiate snooping attacks.  To enable data broadcast in PNoCs, the tuning circuits of detector  MRs partially detune them from their resonance wavelengths [8],  [12]-[13], such that a significant portion of the photonic signal  energy in the data-carrying wavelengths continues to propagate in  the waveguide to be absorbed in the subsequent detector MRs. On  the other hand, process variations (PV) cause resonance wavelength shifts in MRs [14]. Techniques to counteract PV-induced  resonance shifts in MRs involve retuning the resonance wavelengths by using carrier injection/depletion or thermal tuning [6],  implemented through MR tuning circuits. An HT in the GI can  manipulate these tuning circuits of detector MRs to partially tune  the detector MR to a passing wavelength in the waveguide, which  enables snooping of the data that is modulated on the passing  wavelength. Such covert data snooping is a serious security risk  in PNoCs.  In this work, we present a framework that protects data from  snooping attacks and improves hardware security in PNoCs. Our  978-1-5386-4893-3/18/$31.00 ©2018 IEEE           framework has low overhead and is easily implementable in any  existing DWDM-based PNoC without major changes to the architecture. To the best of our knowledge, this is the first work that  attempts to improve hardware security for PNoCs. Our novel contributions are:    • We analyze security risks in photonic devices and extend this  analysis to link-level, to determine the impact of these risks  on PNoCs;  • We propose a circuit-level PV-based security enhancement  scheme that uses PV-based authentication signatures to protect data from snooping attacks in photonic waveguides;   • We propose an architecture-level reservation-assisted security enhancement scheme to improve security in DWDMbased PNoCs;  • We combine the circuit- and architecture-level schemes into  a holistic framework called SOTERIA; and analyze it on the  Firefly [8], Flexishare [9], and SwiftNoC [13] crossbar-based  PNoC architectures.  II. RELATED WORK  Several prior works [11], [16], [17] discuss the presence of security threats in ENoCs and have proposed solutions to mitigate  them. In [11], a three-layer security system approach was presented by using data scrambling, packet certification, and node  obfuscation to enable protection against data snooping attacks. A  symmetric-key based cryptography design was presented in [16]  for securing the NoC. In [17], a framework was presented to use  permanent keys and temporary session keys for NoC transfers between secure and non-secure cores. However, no prior work has  analyzed security risks in photonic devices and links; or considered the impact of these risks on PNoCs.  Fabrication-induced PV impact the cross-section, i.e., width  and height, of photonic devices, such as MRs and waveguides. In  MRs, PV causes resonance wavelength drifts, which can be counteracted by using device-level techniques such as thermal tuning  or localized trimming [6]. Trimming can induce blue shifts in the  resonance wavelengths of MRs using carrier injection into MRs,  whereas thermal tuning can induce red shifts in MR resonances  through heating of MRs using integrated heaters. To remedy PV,  the use of device-level trimming/tuning techniques is inevitable;  but their use also enables partial detuning of MRs that can be  used to snoop data from a shared photonic waveguide. In addition, prior works [18], [19], [29] discuss the impact of PV-remedial techniques on crosstalk noise and proposed techniques to mitigate it. None of the prior works analyze the impact of PV-remedial techniques on hardware security in PNoCs.      Our proposed framework in this paper is novel as it enables security against snooping attacks in PNoCs. Our framework is network agnostic, mitigates PV, and has minimal overhead, while  improving security for any DWDM-based PNoC architecture.  III. HARDWARE SECURITY CONCERNS IN PNOCS  A. Device-Level Security Concerns  Process variation (PV) induced undesirable changes in MR  widths and heights cause “shifts” in MR resonance wavelengths,  which can be remedied using localized trimming and thermal tuning methods. The localized trimming method injects (or depletes)  free carriers into (or from) the Si core of an MR using an electrical  tuning circuit, which reduces (or increases) the MR’s refractive  index owing to the electro-optic effect, thereby remedying the  PV-induced red (or blue) shift in the MR’s resonance wavelength.  In contrast, thermal tuning employs an integrated micro-heater to  adjust the temperature and refractive index of an MR (owing to  the thermo-optic effect) for PV remedy. Typically, the modulator  MRs and detectors use the same electro-optic effect (i.e., carrier  injection/depletion) implemented through the same electrical tuning circuit as used for localized trimming, to move in and out of  resonance (i.e., switch ON/OFF) with a wavelength [7].  A HT can  manipulate this electrical tuning circuit, which may lead to malicious operation of modulator and detector MRs, as discussed  next.  Fig. 1(a) shows the malicious operation of a modulator MR. A  malicious modulator MR is partially tuned to a data-carrying  wavelength (shown in purple) that is passing by in the waveguide.  The malicious modulator MR draws some power from the datacarrying wavelength, which can ultimately lead to data corruption  as optical ‘1’s in the data can lose significant power to be altered  into ‘0’s. Alternatively, a malicious detector (Fig. 1(b)) can be  partially tuned to a passing data-carrying wavelength, to filter  only a small amount of its power and drop it on a photodetector  for data duplication. This small amount of filtered power does not  alter the data in the waveguide so that it continues to travel to its  target detector for legitimate communication [12]. Thus, malicious detector MRs can snoop data from the waveguide without  altering it, which is a major security threat in photonic links. Note  that malicious modulator MRs only corrupt data (which can be  detected and corrected) and do not covertly duplicate it, and are  thus not a major security risk.                                (a)                                                   (b)   Fig. 1: Impact of (a) malicious modulator MR, (b) malicious detector MR  on data in DWDM-based photonic waveguides.  B. Link-Level Security Concerns   Typically, a photonic link is comprised of one or more DWDMbased photonic waveguides. A DWDM-based photonic waveguide uses a modulator bank (a series of modulator MRs) at the  source GI and a detector bank (a series of detector MRs) at the  destination GI. DWDM-based waveguides can be broadly classified into four types: single-writer-single-reader (SWSR), singlewriter-multiple-reader (SWMR), multiple-writer-single-reader  (MWSR), and multiple-writer-multiple-reader (MWMR). As  SWSR, SWMR, and MWSR waveguides are subsets of an  MWMR waveguide, and due to limited space, we restrict our linklevel analysis to MWMR waveguides only.  An MWMR waveguide typically passes through multiple GIs,  connecting the modulator banks of some GIs to the detector banks  of the remaining GIs. Thus, in an MWMR waveguide, multiple            GIs (referred to as source GIs) can send data using their modulator  banks and multiple GIs (referred to as destination GIs) can receive  (read) data using their detector banks. Fig. 2 presents an example  MWMR waveguide with two source GIs and two destination GIs.  Fig. 2(a) and 2(b), respectively, present the impact of malicious  source and destination GIs on this MWMR waveguide. In Fig.  2(a), the modulator bank of source GI S1 is sending data to the  detector bank of destination GI D2. When source GI S2, which is  in the communication path, becomes malicious with an HT in its  control logic, it can manipulate its modular bank to modify the  existing ‘1’s in the data to ‘0’s. This ultimately leads to data corruption. For example, in Fig. 2(a), S1 is supposed to send ‘0110’  to D2, but because of data corruption by malicious GI S2, ‘0010’  is received by D2. Nevertheless, this type of data corruption can  be detected or even corrected using parity or error correction code  (ECC) bits in the data. Thus, malicious source GIs do not cause  major security risks in DWDM-based MWMR waveguides.  Let us consider another scenario for the same data communication path (i.e., from S1 to D2). When destination GI D1, which is  in the communication path, becomes malicious with an HT in its  control logic, the detector bank of D1 can be partially tuned to the  utilized wavelength channels to snoop data. In the example shown  in Fig. 2(b), D1 snoops ‘0110’ from the wavelength channels that  are destined to D2. The snooped data from D1 can be transferred  to a malicious core within the CMP to determine sensitive information. This type of snooping attack from malicious destination  GIs is hard to detect, as it does not disrupt the intended communication among CMP cores. Therefore, there is a pressing need to  address the security risks imposed by snooping GIs in DWDMbased PNoC architectures. To address this need, we propose a  novel framework SOTERIA that improves hardware security in  DWDM-based PNoC architectures.  (a)  (b)  Fig. 2: Impact of (a) malicious modulator (source) bank, (b) malicious detector bank on data in DWDM-based photonic waveguides.  IV. SOTERIA FRAMEWORK: OVERVIEW  Our proposed multi-layer SOTERIA framework enables secure  communication  in DWDM-based PNoC architectures by  integrating circuit-level and architecture-level enhancements. Fig.  3 gives a high-level overview of this framework. The PV-based  security enhancement (PVSC) scheme uses the PV profile of the  destination GI’s detector MRs to encrypt data before it is  transmitted via the photonic waveguide. This scheme is sufficient  to protect data from snooping GIs, if they do not know about the  target destination GI. With target destination GI information,  however, a snooping GI can decipher the encrypted data. Many  PNoC architectures (e.g., [11], [27]) use the same waveguide to  transmit both the destination GI information and actual data,  making them vulnerable to data snooping attacks despite using  PVSC. To further enhance security for these PNoCs, we devise an  architecture-level  reservation-assisted security enhancement  (RVSC) scheme that uses a secure reservation waveguide to avoid  the stealing of destination GI information by snooping GIs. The  next two sections present details of our PVSC and RVSC schemes.  Fig. 3: Overview of proposed SOTERIA framework that integrates a circuit-level PV-based security enhancement (PVSC) scheme and an architecture-level reservation-assisted security enhancement (RVSC) scheme.  V. PV-BASED SECURITY ENHANCEMENT  As discussed earlier (Section III.B), malicious destination GIs  can snoop data from a shared waveguide. One way of addressing  this security concern is to use data encryption so that the malicious  destination GIs cannot decipher the snooped data. For the encrypted data to be truly undecipherable, the encryption key used  for data encryption should be kept secret from the snooping GIs,  which can be challenging as the identity of the snooping GIs in a  PNoC is not known. Therefore, it becomes very difficult to decide  whether or not to share the encryption key with a destination GI  (that can be malicious) for data decryption. This conundrum can  be resolved using a different key for every destination GI so that  a key that is specific to a secure destination GI does not need to  be shared with a malicious destination GI for decryption purpose.  Moreover, to keep these destination specific keys secure, the malicious GIs in a PNoC must not be able to clone the algorithm (or  method) used to generate these keys.   To generate unclonable encryption keys, our PV-based security  (PVSC) scheme uses the PV profiles of the destination GI’s detector MRs. As discussed in [14], PV induces random shifts in the  resonance wavelengths of the MRs used in a PNoC. These resonance shifts can be in the range from -3nm to 3nm [14]. The MRs  that belong to different GIs in a PNoC have different PV profiles.  In fact, the MRs that belong to different MR banks of the same GI  also have different PV profiles. Due to their random nature, these  MR PV profiles cannot be cloned by the malicious GIs, which  makes the encryption keys generated using these PV profiles truly  unclonable. Using the PV profiles of detector MRs, PVSC can  generate a unique encryption key for each detector bank of every  MWMR waveguide in a PNoC.  Our PVSC scheme generates encryption keys during the testing  phase of the CMP chip, by using a dithering signal based in-situ  method [15] to generate an anti-symmetric analog error signal for  each detector MR of every detector bank that is proportional to  the PV-induced resonance shift in the detector MR. Then, it converts the analog error signal into a 64-bit digital signal. Thus, a  64-bit digital error signal is generated for every detector MR of                    each detector bank. We consider 64 DWDM wavelengths per  waveguide, and hence, we have 64 detector MRs in every detector  bank and 64 modulator MRs in every modulator bank. For each  detector bank, our PVSC scheme XORs the 64 digital error signals  (of 64 bits each) from each of the 64 detector MRs to create a  unique 64-bit encryption key. Note that our PVSC scheme also  uses the same anti-symmetric error signals to control the carrier  injection and heating of the MRs to remedy the PV-induced shifts  in their resonances.  To understand how the 64-bit encryption key is utilized to encrypt data in photonic links, consider Fig. 4 which depicts an example photonic link that has one MWMR waveguide and connects the modulator banks of two source GIs (S1 and S2) with the  detector banks of two destination GIs (D1 and D2). As there are  two destination GIs on this link, PVSC creates two 64-bit encryption keys corresponding to them, and stores them at the source  GIs. When data is to be transmitted by a source GI, the key for the  appropriate destination is used to encrypt data at the flit-level  granularity, by performing an XOR between the key and the data  flit. This requires that the size of an encryption key match the data  flit size. We consider the size of data flits to be 512 bits. Therefore, the 64-bit encryption key is appended eight times to generate  a 512-bit encryption key. In Fig. 4, every source GI stores two  512-bit encryption keys (for destination GIs D1 and D2) in its local  ROM, whereas every destination GI stores only its corresponding  512-bit key in its ROM. Note that we store the 512-bit keys instead of the 64-bit keys as this eliminates the latency overhead of  affixing 64-bit keys to generate 512-bit keys, at the cost of a reasonable area/energy overhead in the ROM. As an example, if S1  wants to send a data flit to D2, then S1 first accesses the 512-bit  encryption key corresponding to D2 from its local ROM and  XORs the data flit with this key in one cycle, and then transmits  the encrypted data flit over the link. As the link employs only one  waveguide with 64 DWDM wavelengths, therefore, the encrypted  512-bit data flit is transferred on the link to D2 in eight cycles. At  D2, the data flit is decrypted by XORing it with the 512-bit key  corresponding to D2 from the local ROM. In this scheme, even if  D1 snoops the data intended for D2, it cannot decipher the data as  it does not have access to the correct key (corresponding to D2)  for decryption. Thus, our PVSC encryption scheme protects data  against snooping attacks in DWDM-based PNoCs.  Fig. 4: Overview of proposed PV-based security enhancement scheme.  Limitations of PVSC: The PVSC scheme can protect data from  being deciphered by a snooping GI, if the following two  conditions about the underlying PNoC architecture hold true: (i)  the snooping GI does not know the target destination GI for the  snooped data, (ii) the snooping GI cannot access the encryption  key corresponding to the target destination GI. As discussed  earlier, an encryption key is stored only at all source GIs and at  the corresponding destination GI, which makes it physically  inaccessible to a snooping destination GI. However, if more than  one GIs in a PNoC are compromised due to HTs in their control  units and if these HTs launch a coordinated snooping attack, then  it may be possible for the snooping GI to access the encryption  key corresponding to the target destination GI.   For instance, consider the photonic link in Fig. 4. If both S1 and  D1 are compromised, then the HT in S1’s control unit can access  the encryption keys corresponding to both D1 and D2 from its  ROM and transfer them to a malicious core (a core running a  malicious program). Moreover, the HT in D1’s control unit can  snoop the data intended for D2 and transfer it to the malicious  core. Thus, the malicious core may have access to the snooped  data as well as the encryption keys stored at the source GIs.  Nevertheless, accessing the encryption keys stored at the source  GIs is not sufficient for the malicious GI (or core) to decipher the  snooped data. This is because the compromised ROM typically  has multiple encryption keys corresponding  to multiple  destination GIs, and choosing a correct key that can decipher data  requires the knowledge of the target destination GI. Thus, our  PVSC encryption scheme can secure data communication in  PNoCs even if the encryption keys are compromised, as long as  the malicious GIs (or cores) do not know the target destinations  for the snooped data.   Unfortunately, many PNoC architectures, e.g., [11], [27], that  employ photonic links with multiple destination GIs utilize the  same waveguide  to  transmit both  the  target destination  information and actual data. In these PNoCs, if a malicious GI  manages to tap the target destination information from the shared  waveguide, then it can access the correct encryption key from the  compromised ROM to decipher the snooped data. Thus, there is a  need to conceal the target destination information from malicious  GIs (cores). This motivates us to propose an architecture-level  solution, as discussed next.  VI. RESERVATION-ASSISTED SECURITY  ENHANCEMENT  In PNoCs that use photonic links with multiple destination GIs,  data is typically transferred in two time-division-multiplexed  tion wavelength. In Fig. 5(a), (cid:2755)1 and (cid:2755)2 are the reservation selec(TDM) slots called reservation slot and data slot [11], [27]. To  minimize photonic hardware, PNoCs use the same waveguide to  transfer both slots, as shown in Fig. 5(a). To enable reservation of  the waveguide, each destination is assigned a reservation selection wavelengths corresponding to destination GIs D1 and D2, respectively. Ideally, when a destination GI detects its reservation  selection wavelength in the reservation slot, it switches ON its detector bank to receive data in the next data slot. But in the presence  tectors to snoop (cid:2755)2 from the reservation slot. By snooping λ2, D1  of an HT, a malicious GI can snoop signals from the reservation  slot using the same detector bank that is used for data reception.  For example, in Fig. 5(a), malicious GI D1 is using one of its decan identify that the data it will snoop in the subsequent data slot  will be intended for destination D2. Thus, D1 can now choose the  correct encryption key from the compromised ROM to decipher  its snooped data.  To address this security risk, we propose an architecture-level  reservation-assisted security enhancement (RVSC) scheme. In  RVSC, we add a reservation waveguide, whose main function is              to carry reservation slots, whereas the data waveguide carries data  slots. We use double MRs to switch the signals of reservation slots  from the data waveguide to the reservation waveguide, as shown  in Fig. 5(b). Double MRs are used instead of single MRs for  switching to ensure that the switched signals do not reverse their  propagation direction after switching [29]. Compared to single  MRs, double MRs also have lower signal loss due to steeper rolloff of their filter responses [29].   The double MRs are switched ON only when the photonic link  is in a reservation slot, otherwise they are switched OFF to let the  signals of the data slot pass by in the data waveguide. Furthermore, in RVSC, each destination GI has only one detector on the  (cid:2755)1 and (cid:2755)2, respectively, on the reservation waveguide. This makes  it difficult for the malicious GI D1 to snoop (cid:2755)2 from the reservation  reservation waveguide, which corresponds to its receiver selection wavelength. For example, in Fig. 5(b), D1 and D2 will have  detectors corresponding to their reservation selection wavelengths  sponding to (cid:2755)2 on the reservation waveguide. However, the HT in  slot as shown in Fig. 5(b), as D1 does not have a detector correD1’s control unit may still attempt to snoop other reservation  wavelengths (e.g., λ2) in the reservation slot by retuning D1’s λ1  detector. But succeeding in these attempts would require the HT  to perfect the timing and target wavelength of its snooping attack,  which is very difficult due to the large number of utilized reservation wavelengths. Thus, D1 cannot identify the correct encryption key to decipher the snooped data. In summary, RVSC enhances security in PNoCs by protecting data from snooping attacks, even if the encryption keys used to secure data are compromised.  (a)  (b)  Fig. 5: Reservation-assisted data transmission in DWDM-based photonic  waveguides (a) without RVSC, (b) with RVSC.  VII.  IMPLEMENTING SOTERIA ON PNOCS  We characterize the impact of SOTERIA on two popular PNoC  architectures: Firefly [8], Flexishare [9], and SwiftNoC [13], all  of which use DWDM-based photonic waveguides for data communication. We consider Firefly PNoC with 8×8 SWMR crossbar  [8] and Flexishare PNoC with 32×32 MWMR crossbar [9] with  2-pass token stream arbitration. Moreover, we also consider  SwiftNoC PNoC with 32×32 MWMR crossbar [13] with improved concurrent token-stream arbitration. SwiftNoC PNoC [13]  features improved multicast-enabled channel sharing, as well as  dynamic re-prioritization and exchange of bandwidth between  clusters of cores, to increase channel utilization and system performance compared to the other crossbar based PNoC architectures (e.g., Firefly [8] and Flexishare [9] PNoCs) from prior work.  We adapt the analytical equations from [29] to model the signal  power loss and required laser power in the SOTERIA-enhanced  Firefly, Flexishare, and SwiftNoC PNoCs. At each source and  destination GI of the SOTERIA-enhanced Firefly, Flexishare, and  SwiftNoC PNoCs, XOR gates are required to enable parallel encryption and decryption of 512-bit data flits. We consider a 1 cycle delay overhead for encryption and decryption of every data  flit. The overall laser power and delay overheads for all PNoCs  are quantified in the results section.  Firefly PNoC: Firefly PNoC [8], for a 256-core system, has 8  nodes (N1-N8) with 32 cores in each node. Firefly uses reservation-assisted SWMR data channels in its 8×8 crossbar for internode communication. Each data channel consists of 8 SWMR  waveguides, with 64 DWDM wavelengths in each waveguide. To  integrate SOTERIA with Firefly PNoC, we added a reservation  waveguide to every SWMR channel. This reservation waveguide  has 7 detector MRs to detect reservation selection wavelengths  corresponding to 7 destination GIs. Furthermore, 64 double MRs  (corresponding to 64 DWDM wavelengths) are used at each reservation waveguide to implement RVSC. To enable PVSC, each  source GI has a ROM with seven entries of 512 bits each to store  seven 512-bit encryption keys corresponding to seven destination  GIs. In addition, each destination GI requires a 512-bit ROM to  store its own encryption key.      Flexishare PNoC: We also integrate SOTERIA with the Flexishare PNoC architecture [9] with 256 cores. We considered a 32node Flexishare PNoC with eight cores in each node and 16 data  channels for inter-node communication. Each data channel has  four MWMR waveguides with each having 64 DWDM wavelengths. In SOTERIA-enhanced Flexishare, we added a reservation waveguide to each MWMR channel. Each reservation waveguide has 16 detector MRs to detect reservation selection wavelengths corresponding to 16 destination GIs. To enable PVSC,  each source GI requires a ROM with 16 entries of 512 bits each  to store the encryption keys, whereas each destination GI requires  a 512-bit ROM.  SwiftNoC PNoC: We also integrate SOTERIA with a 256-core  32-node SwiftNoC PNoC [13] with eight cores in each node and  16 MWMR data channels for inter-node communication. Furthermore, these 256 cores are divided into 4 clusters (each cluster has  64 cores) to enable dynamic re-prioritization and exchange of  bandwidth between clusters of cores. Each MWMR data channel  has four MWMR waveguides and it connects 16 source GIs and  16 destination GIs. Out of the four MWMR waveguides per  MWMR data channel, three waveguides have 64 DWDM wavelengths and one waveguide has 68 DWDM wavelengths. In SOTERIA-enhanced SwiftNoC, we added a reservation waveguide to  each MWMR data channel. Each reservation waveguide has 16  detector MRs to detect reservation selection wavelengths corresponding to 16 destination GIs. To enable PVSC, each source GI  requires a ROM with 16 entries of 512 bits each to store the encryption keys, whereas each destination GI requires a 512-bit  ROM.               VIII. EVALUATIONS  A. Evaluation Setup  To evaluate our proposed SOTERIA (PVSC+RVSC) security  enhancement framework for DWDM-based PNoCs, we integrate  it with the Firefly [8], Flexishare [9], and SwiftNoC [13] PNoCs,  as explained in Section VII. We modeled and performed simulation based analysis of the SOTERIA-enhanced Firefly, Flexishare,  and SwiftNoC PNoCs using a cycle-accurate SystemC based NoC  simulator, for a 256-core single-chip architecture at 22nm. We  validated the simulator in terms of power dissipation and energy  consumption based on results obtained from the DSENT tool [22].  We used real-world traffic from the PARSEC benchmark suite  [23]. GEM5 full-system simulation [24] of parallelized PARSEC  applications was used to generate traces that were fed into our  NoC simulator. We set a “warmup” period of 100 million instructions and then captured traces for the subsequent 1 billion instructions. These traces are extracted from parallel regions of execution of PARSEC applications. We performed geometric calculations for a 20mm×20mm chip size, to determine lengths of  SWMR and MWMR waveguides in Firefly, Flexishare, and  SwiftNoC. Based on this analysis, we estimated the time needed  for light to travel from the first to the last node as 8 cycles at 5  GHz clock frequency [13]. We use a 512-bit packet size, as advocated in the Firefly, Flexishare, and SwiftNoC PNoCs. Similar to  [29], we adapt the VARIUS tool [20] to model random and systematic die-to-die (D2D) as well as within-die (WID) process variations in MRs for the Firefly, Flexishare, and SwiftNoC PNoCs.  The static and dynamic energy consumption values for electrical routers and concentrators in Firefly, Flexishare, and SwiftNoC  PNoCs are based on results from DSENT [22]. We model and  consider the area, power, and performance overheads for our  framework implemented with the Firefly, Flexishare, and SwiftNoC PNoCs as follows. SOTERIA with Firefly, Flexishare, and  SwiftNoC PNoCs has an electrical area overhead of 12.7mm2,  3.4mm2, and 3.4mm2, respectively, and power overhead of  0.44W, 0.36W, and 0.36W, respectively, using gate-level analysis  and CACTI 6.5 [25] tool for memory and buffers. The photonic  area of Flexishare, Firefly, and SwiftNoC PNoCs is 19.83mm2,  5.2mm2, and 49mm2 respectively, based on the physical dimensions [21] of their waveguides, MRs, and splitters. SwiftNoC  PNoC has significantly more photonic area compared to the other  PNoCs, as it employs significantly more MRs and power delivery  waveguide to support dynamic bandwidth reconfigurability [13].  For energy consumption of photonic devices, we adapt model parameters from recent work [26], [28], [30] with 0.42pJ/bit for  every modulation and detection event and 0.18pJ/bit for the tuning circuits of modulators and photodetectors. The MR trimming  power is 130μW/nm [10] for current injection and tuning power  is 240μW/nm [10] for heating.  B. Overhead Analysis of SOTERIA on PNoCs  Our first set of experiments compare the baseline (without any  security enhancements) Firefly, Flexishare, and SwiftNoC PNoCs  with their SOTERIA enhanced variants. From Section VII, all 8  SWMR waveguide groups of the Firefly PNoC, all 16 MWMR  waveguide groups of the Flexishare and SwiftNoC PNoCs each  are equipped with PVSC encryption/decryption and reservation  waveguides for the RVSC scheme.  We adapt the analytical models from [29] to calculate the total  signal loss at the detectors of the worst-case power loss node  (NWCPL), which corresponds to router N4R0 for the Firefly PNoC  [8] and node R63 for the Flexishare [9] and SwiftNoC [13] PNoCs.  Fig. 6(a) summarizes the worst-case signal loss results for the  baseline and SOTERIA configurations for the three PNoC architectures. From the figure, Firefly PNoC with SOTERIA increases  loss by 1.6dB, Flexishare PNoC with SOTERIA increases loss by  1.2dB, and SwiftNoC PNoC with SOTERIA increases loss by  1.18dB on average, compared to their respective baselines. Compared to the baseline PNoCs that have no single or double MRs to  switch the signals of the reservation slots, the double MRs used  in the SOTERIA-enhanced PNoCs to switch the wavelength signals of the reservation slots increase through losses in the waveguides, which ultimately increases the worst-case signal losses in  the SOTERIA-enhanced PNoCs. Using the worst-case signal  losses shown in Fig. 6(a), we determine the total photonic laser  power and corresponding electrical laser power for the baseline  and SOTERIA-enhanced variants of Firefly, Flexishare, and  SwiftNoC PNoCs, shown in Fig. 6(b). From this figure, the Firefly, Flexishare, and SwiftNoC PNoCs with SOTERIA have laser  power overheads of 44.7%, 31.40%, and 33.20% on average,  compared to their baselines.  (a)  (b)  Fig. 6: Comparison of (a) worst-case signal loss and (b) laser power dissipation of SOTERIA framework on Firefly, Flexishare, and SwiftNoC  PNoCs with their respective baselines for 100 process variation maps.  Fig. 7 presents detailed simulation results that quantify the average packet latency and energy-delay product (EDP) for the two  configurations of the Firefly, Flexishare, and SwiftNoC PNoCs.  Results are shown for twelve multi-threaded PARSEC benchmarks. From Fig. 7(a), Firefly with SOTERIA has 5.2%, Flexishare with SOTERIA has 10.6%, and SwiftNoC with SOTERIA  has 17.3% higher latency on average compared to their respective  baselines. The additional delay due to encryption and decryption  of data (Section VII.A) with PVSC contributes to the increase in  average latency compared to the baselines. The baseline SwiftNoC PNoC has lower average latency compared to the Firefly and  Flexishare PNoCs, and therefore, the fixed latency overhead of  encryption/decryption events in SOTERIA-enhanced SwiftNoC  results in higher latency overhead in terms of percentage value.   From the results for EDP shown in Fig. 7(b), Firefly with SOTERIA has 4.9%, Flexishare with SOTERIA has 13.3%, and  SwiftNoC with SOTERIA has 15.2% higher EDP on average compared to their respective baselines. Increase in EDP for the SOTERIA-enhanced PNoCs is not only due to the increase in their          average packet latency, but also due to the presence of additional  RVSC reservation waveguides, which increases the required photonic hardware (e.g., more number of MRs) in the SOTERIA-enhanced PNoCs. This in turn increases static energy consumption  (i.e., laser energy and trimming/tuning energy), ultimately increasing the EDP.  number of SOTERIA enhanced MWMR waveguides increases  number of packets that are transferred through the PVSC encryption scheme, which contributes to the increase in average packet  latency across these variants. From the results for EDP shown in  Fig. 8(b), FLEX-ST-4, FLEX-ST-8, FLEX-ST-16, and FLEXST-24 have 2%, 4%, 7.6%, and 10.8% higher EDP on average  compared to the baseline Flexishare. EDP in Flexishare PNoC increases with increase in number of SOTERIA enhanced MWMR  waveguides. Increase in average packet latency and signal loss  due to the higher number of reservation waveguides and double  MRs increase overall EDP across these variants.  (a)  (a)  (b)  Fig. 7: (a) normalized average latency and (b) energy-delay product  (EDP) comparison between different variants of Firefly, Flexishare, and  SwiftNoC PNoCs that include their baselines and their variants with SOTERIA framework, for PARSEC benchmarks. Latency results are normalized with the latency results of Firefly PNoC. Bars represent mean  values of average latency and EDP for 100 PV maps; confidence intervals  show variation in EDP across 100 PV maps.  C. Analysis of Overhead Sensitivity  Our last set of evaluations explore how the overhead of SOTERIA changes with varying levels of security in the network.  Typically, in a manycore system, only a certain portion of the data  that contains sensitive information (i.e., keys) and only a certain  number of communication links need to be secure. Therefore, for  our analysis in this section, instead of securing all data channels  of the Flexishare PNoC, we secure only a certain number channels  using SOTERIA. Out of the total 32 MWMR channels in the Flexishare PNoC, we secure 4 (FLEX-ST-4), 8 (FLEX-ST-8), 16  (FLEX-ST-16), and 24 (FLEX-ST-24) channels, and evaluate the  average packet latency and EDP for these variants of the SOTERIA-enhanced Flexishare PNoC.   In Fig. 8, we present average packet latency and EDP values  for the five SOTERIA-enhanced configurations of the Flexishare  PNoC. From Fig. 8(a), FLEX-ST-4, FLEX-ST-8, FLEX-ST-16,  and FLEX-ST-24 have 1.8%, 3.5%, 6.7%, and 9.5% higher latency on average compared to the baseline Flexishare. Increase in  (b)  Fig. 8: (a) normalized latency and (b) energy-delay product (EDP) comparison between Flexishare baseline and Flexishare with 4, 8, 16, and 24  SOTERIA enhanced MWMR waveguide groups, for PARSEC benchmarks. Latency results are normalized to the baseline Flexishare results.  IX. CONCLUSIONS  We presented a novel security enhancement framework called  SOTERIA that secures data during unicast communications in  DWDM-based PNoC architectures from snooping attacks. Our  proposed SOTERIA framework shows interesting trade-offs between security, performance, and energy overhead for the Firefly,  Flexishare, and SwiftNoC PNoC architectures. Our analysis  shows that SOTERIA enables hardware security in crossbar based  PNoCs with minimal overheads of up to 17.3% in average latency  and of up to 15.2% in EDP compared to the baseline PNoCs.  Thus, SOTERIA represents an attractive solution to enhance hardware security in emerging DWDM-based PNoCs.  ACKNOWLEDGMENTS  This research is supported by grants from SRC, NSF (CCF1252500, CCF-1302693, CCF-1813370), AFOSR (FA9550-131-0110), and Micron Technology, Inc.                          [21] S. Xiao, M. H. Khan, H. Shen, and M. Qi, “Modeling and measurement of losses in silicon-on-insulator resonators and bends,”  in Optics Express, 15(17), pp. 10553-10561, 2007.  [22] C. Sun, C.-H. O. Chen, G. Kurian, L. Wei, J. Miller, A. Agarwal,  L.-S. Peh, and V. Stojanovic, “DSENT - a tool connecting  emerging photonics with electronics for opto-electronic networks-on-chip modeling,” NOCS, 2012.  [23] C. Bienia, S. Kumar, J. P. Singh, and K. Li, “The PARSEC  Benchmark Suit: Characterization and Architectural Implications,” in PACT, Oct. 2008.  [24] N. Binkert, B. Beckmann, G. Black, S. K. Reinhardt, A. Saidi,  A. Basu, J. Hestness, D. R. Hower, T. Krishna, S. Sardashti, R.  Sen, K. Sewell, M. Shoaib, N. Vaish, M. D. Hill, and D. A.  Wood, “The gem5 Simulator,” in CA News, May 2011.  [25] CACTI 6.5, http://www.hpl.hp.com/research/cacti/  [26] S. V. R. Chittamuru, I. Thakkar, and S. Pasricha, “Analyzing  Voltage Bias and Temperature Induced Aging Effects in Photonic Interconnects for Manycore Computing,” in Proc. SLIP,  June. 2017.   [27] C. Chen and A. Joshi, “Runtime management of laser power in  silicon-photonic multibus NoC architecture,” in Proc. IEEE  JQE, 2013.  [28] I. Thakkar, S. V. R. Chittamuru, and S. Pasricha, “Mitigation of  Homodyne Crosstalk Noise in Silicon Photonic NoC Architectures with Tunable Decoupling,” in Proc. CODES+ISSS, Oct.  2016.   [29] S. V. R. Chittamuru, I. Thakkar, and S. Pasricha, “HYDRA: Heterodyne Crosstalk Mitigation with Double Microring Resonators and Data Encoding for Photonic NoCs,” in TVLSI, vo1. 26,  no. 1, 2018.  [30] I. Thakkar, S. V. R. Chittamuru, and S. Pasricha, “A comparative analysis of front-end and back-end compatible silicon photonic on-chip interconnects,” in Proc. SLIP, June 2016.  [31] S. Pasricha and S. Bahirat, “OPAL: A Multi-Layer Hybrid Photonic NoC for 3D ICs,” in Proc. ASPDAC, Jan. 2011.  [32] S. Bahirat and S. Pasricha, “METEOR: Hybrid Photonic RingMesh Network-on-Chip for Multicore Architectures,” in ACM  JETC, 13(3), 2014.  [33] I. Thakkar and S. Pasricha, “3D-Wiz: A Novel High Bandwidth,  Optically Interfaced 3D DRAM Architecture with Reduced  Random-Access Time,” in Proc. ICCD, Oct 2014.  [34] I. Thakkar and S. Pasricha, “3D-ProWiz: An Energy-Efficient  and Optically-Interfaced 3D DRAM Architecture with Reduced  Data Access Overhead,” IEEE TMSCS, 1(3), Sep. 2015.  "
DAPPER - Data Aware Approximate NoC for GPGPU Architectures.,"High interconnect bandwidth is crucial to achieve better performance in many-core GPGPU architectures that execute highly data parallel applications. The parallel warps of threads running on shader cores generate a high volume of read requests to the main memory due to the limited availability of data cache space at the shader cores. This leads to scenarios with rapid arrival of reply data from the DRAM, which creates a bottleneck at memory controllers (MCs) that send reply packets back to the requesting cores over the NoC. Coping with such high volumes of data requires NoC architectures that possess high power overhead. To accomplish high bandwidth and low energy communication in GPGPUs, we propose Dapper, a data-aware approximate NoC architecture that increases the utilization of the available bandwidth by using low power single cycle overlay circuits for the reply traffic between MCs and shader cores. Dapper also incorporates a novel MC architecture that leverages the inherent approximability of the data values of certain applications and reduces the number of reply packets injected into the NoC by the MCs. Experimental results show that Dapper reduces the energy consumed in the GPGPU by up to 50% with up to 99% application output accuracy and minimum performance overheads compared to a state-of-the-art approximate NoC architectures.","DAPPER: Data Aware Approximate NoC for   GPGPU Architectures  Venkata Yaswanth Raparti, Sudeep Pasricha  Department of Electrical and Computer Engineering  Colorado State University, Fort Collins, CO, U.S.A.  yaswanth@rams.colostate.edu, sudeep@colostate.edu  Abstract – High interconnect bandwidth is crucial to achieve better  performance in many-core GPGPU architectures that execute highly  data parallel applications. The parallel warps of threads running on  shader cores generate a high volume of read requests to the main  memory due to the limited availability of data cache space at the  shader cores. This leads to scenarios with rapid arrival of reply data  from the DRAM, which creates a bottleneck at memory controllers  (MCs) that send reply packets back to the requesting cores over the  NoC. Coping with such high volumes of data requires NoC  architectures that possess high power overhead. To accomplish high  bandwidth and low energy communication in GPGPUs, we propose  Dapper, a data-aware approximate NoC architecture that increases  the utilization of the available bandwidth by using low power single  cycle overlay circuits for the reply traffic between MCs and shader  cores. Dapper also incorporates a novel MC architecture that  leverages the inherent approximability of the data values of certain  applications and reduces the number of reply packets injected into  the NoC by the MCs. Experimental results show that Dapper reduces  the energy consumed in the GPGPU by up to 50% with up to 99%  application output accuracy and minimum performance overheads  compared to a state-of-the-art approximate NoC architectures.    Index Terms—GPGPU, approximate computing, network-on-chip  I. INTRODUCTION  For today’s high-performance computing workloads, general purpose  graphical processing units (GPGPU) have become highly popular due to  their support for high thread and data level parallelism. GPGPUs  typically have many computational units within their streaming  multiprocessors (SM) [1] (also known as shader cores) that execute  hundreds of threads simultaneously. Libraries such as OpenCL [2], and  CUDA [3] have enabled programmers to efficiently parallelize a program  and reap the best performance out of the available parallel computational  resources on a GPGPU. However, highly parallel applications generate  high volumes data traffic between memory controllers (MCs) connected  to the main memory and the compute cores in the SMs. Traditional meshbased NoC architectures are not designed to handle such high volumes of  traffic. To facilitate the high traffic rates of GPGPUs, NoC channel  widths should be increased multifold compared to conventional NoCs,  which leads to a significant increase in NoC power dissipation. Figure 1  gives a breakdown of power consumed by various components of a  GPGPU when executing data parallel CUDA applications from the  CUDA SDK sample code suite. For memory intensive applications that  generate high volumes of memory requests, the NoC dissipates up to 20%  of the overall power. Thus, there is a critical need for a NoC architecture  that supports high volumes of data generated in GPGPUs, yet dissipates  low power (and energy) without sacrificing application performance.  Recent works [4]-[5], have demonstrated the impact of a new paradigm  called approximate computing that trades-off computation accuracy for  978-1-5386-4893-3/18/$31.00 ©2018 IEEE savings in energy consumption. Many emerging application domains like  machine learning, imagine processing, and pattern recognition-based  applications are today exploring approximate computing techniques to  save energy or improve application performance while tolerating a small  range of output errors. There is an interesting potential for using the  approximate computing paradigm in GPGPUs to design low power NoC  architectures. The applications that execute on GPGPUs also operate on  large input data sizes with a significant scope for data approximability.  The main goal of our work is to exploit approximation techniques  intelligently to minimize the energy consumed by the NoC in moving data  between MCs and cores without slowing down the application in manycore GPGPU platforms.  Figure 1: Breakdown of power dissipated by different components of a  GPGPU when executing various parallel workloads  In a many-core GPGPU platforms, the NoC traffic consists of  load/store (LD/ST) data with LD replies forming a majority of the traffic  that causes MC bottlenecks [6]. Typically, the traffic in a NoC is skewed  with memory requests that follow a many-to-few pattern from cores to  MCs and replies that follow a few-to-many pattern from MCs to cores.  With high volume of LD reply data at MCs, a bottleneck is created which  leads to high wait times for the thread blocks executing on shader cores,  and hence a slowdown in overall performance. Hardware designers have  come up with high radix NoCs with intelligent routing schemes [7], or  complicated warp scheduling techniques [8] to hide, or minimize, the  high latency due to the MC bottleneck. However, these techniques incur  high power and area overheads. Several applications that use GPGPUs  generate high volumes of data with redundant or similar values that can  be approximated to reduce the number of packets transmitted from MCs  to cores. In this paper, we leverage this observation and propose a highspeed circuit overlay NoC architecture, called Dapper, that overcomes  the MC bottleneck issue and minimizes energy consumption by up to  50% compared to the state-of-the art techniques with around 99%  accuracy in the output. Our novel contributions are as follows:  • We introduce a novel memory controller (MC) architecture that  approximates the read reply data arriving from main memory. The  approximable data is processed and flagged for transmission on the  fast overlay circuits created by NoC;  • We also propose a novel asynchronous fast overlay circuit  architecture for transmitting both general and approximate data            between MCs and cores in 3 cycles. The transmission along X and  Y axes are done in 1 cycle each, stopping only at turns;   • We further propose a novel NoC router architecture, and a global  overlay manager (GOM) that are utilized in setting up and tearing  down overlay circuits that transmit data between MCs and cores;   • We conduct rigorous experimentation of our proposed Dapper NoC  architecture with CUDA applications to compare the performance  and energy consumption against the state-of-the art.  II. RELATED WORK  Several prior works have addressed the issue of NoC energy  consumption for traditional many-core processors. In [9], the authors  propose a small world NoC that utilizes machine learning to establish fast  connections between cores that generate high volumes of data. In [10],  the authors propose an application criticality-aware packet routing  scheme that prioritizes memory requests of time-critical applications. In  [11], a fault-tolerant and energy-aware NoC routing schemes are  proposed. In [25]-[28] reliability and energy-aware NoC routing schemes  are proposed. These techniques perform well in CPU based platforms  with lighter traffic conditions, but they cannot be used to minimize the  bottleneck caused at MCs in GPGPU platforms.    Approximate (or inexact) computing has been studied by researchers  from academics and industry to save energy by trading off the correctness  of the output. A few works [19]-[20] have demonstrated the use of  compiler support for programmers to mark and treat approximate  variables differently from other variables and enabling inexact computing  on those variables at the hardware level. A few prior works have used  approximate computing at the circuit level [12]-[13] to trade-off accuracy  of the logic circuits for shorter critical paths and low operating voltages  that save power and area footprint. A few other works have introduced  approximate last level caches [14]-[15]. In these works, the authors  reduce the amount of data stored in the caches, and increase the cache hit  rate by associating tags of multiple approximately similar blocks to the  same data line. In [17], the authors use approximate computing in  conjunction with spatial and temporal locality to increase the benefit of  instruction reuse in GPGPUs. None of these techniques can be applied  for energy efficient data transmission between MCs and cores, to resolve  MC bottleneck issues in GPGPUs.   In [18], the authors attempt to minimize the NoC congestion in manycore CPUs by introducing a new method of approximate compression and  decompression of packets at network interfaces of each router to reduce  the number of flits entering the NoC. This work utilizes dictionary-based  compression/decompression technique that consumes high energy and  leads to performance overheads when the network traffic is high. To  address these shortcomings, we propose Dapper, a data-aware  approximate low power NoC that establishes overlay circuits using  asynchronous links and repeaters. As part of Dapper, we also introduce  a NoC router architecture that ensures the traversal of flits on the overlay  circuits, and an approximation-aware MC architecture.  III. BACKGROUND AND MOTIVATION  A. Baseline Configuration  We use a heterogeneous accelerator-based system with an x86 CPU  and a grid of shader-cores of GPGPUs that have private L1 caches  (shared, texture, instruction, and data) to support data parallel  multithreaded application execution. A shader core consists of parallel  integrated pipelines with a common instruction fetch unit that executes a  single instruction on multiple data (SIMD) simultaneously. Each  integrated pipeline has an integer arithmetic logic unit and a floatingpoint unit. A shader core also has several load store units that fetch data  from a private L1 cache or from the main memory. A GPGPU based  accelerator has a shared L2 cache bank located at the MCs that caches  data coming from the main memory. All the shader cores and MCs are  connected to an on-chip interconnection network.   Typically, there is little to no communication between the shader cores  on the chip. The communication between CPU and GPU cores takes  place through main memory. The shader cores send read/write requests  (via LD/ST instructions) to MCs over the NoC. A memory reply takes  several cycles based on the location of and availability of data (either at  L2 or DRAM). The baseline NoC architecture between the shader cores  and the MCs has a channel width of 128-bits, twice the size of 64 bit NoC  channels in traditional CPU based platforms, and consists of 4-stage  routers (stage 1: buffer write; stage 2: route computation, stage 3: virtualchannel/switch allocation; stage 4: switch/link traversal). There are 5  virtual channels (VCs) per input port and 4 flit buffers for each VC,  connected to each shader core. Flits are routed along the XY path from  source to destination. In this work we only focus on optimizing the NoC  that connects shader-cores and MCs. Henceforth, the term cores implies  shader-cores for the remainder this article.  (a)  (b)  Figure 2: (a) Example image showing similar data values in pixels (b) RGB  values of the marked locations as stored in texture memory   B. Data value approximability  Several types of data parallel applications are typically executed on  GPGPU platforms. Many of them belong to the domain of image  processing, signal processing, pattern recognition, and machine learning.  There also exist other scientific and financial applications that use  GPGPUs that operate on large input data sets. The data used for executing  image and signal processing applications in many cases is highly  approximable. For example, as shown in figure 2(a), the areas in boxes  contain pixels that are very similar to their adjacent pixels. The RGB  values of two pixels for each box are shown in figure 2(b). These values  (for each box) are quite similar and it is not energy efficient to save and  transmit cache lines containing similar RGB values separately from  DRAM to the cores. Instead, if cache lines with same or similar data can  be identified at MCs, we can avoid their repeated transmissions to the  cores, to save energy. Dapper is designed to realize this idea.  The next logical question one may ask is: how approximable are  typical applications that run on GPGPUs? Figure 3 (a) shows the  percentage of approximable data in different parallel CUDA applications.              Image processing applications such as DCT, Hist, ConvTex have up to  80% of approximable data that can be exploited to save NoC energy  consumption. By making use of the programming paradigm proposed in  EnerJ [19], one can label the variables using the @approx keyword as  highlighted in green in figure 3(b). These approximate variables are  predetermined to contain approximate values. Approximable variables  will have a distinct set of instructions for load and store that are used  when compiling the C++ code to machine code. These instructions can  be used by cores to identify approximable data, and accept inexact values  for them from main memory.   formed between an MC to the cores stays for a fixed time window during  which it transmits the reply packets of that MC, before switching to the  next overlay circuit (for another MC). On overlay circuits, shown in red  and green colored arrows in the figure, each MC transmits flits of packets  waiting in its buffers in just 3 cycles using asynchronous links and  repeaters that pass through NoC routers. Flits traverse in X and Y  directions in one cycle each, stopping only at the turns. Hence, flits  traversing over overlay circuits do not go through switch arbitration and  route compute logic at every hop. This leads to a low energy consumption  NoC that establishes high-throughput, congestion-free paths for read  reply packets. More details about how overlay circuits are established and  used for data traversal are discussed in the following sections.  (a)  (b)  Figure 3: (a) Normalized percentage of approximable data transmitted from  main memory to cores in different CUDA applications. (b) Example of  marking approximable variables using ENERJ [19]  C. Overlay circuits for low latency traversal   As discussed earlier, in GPGPUs, read reply data is the main source of  MC bottlenecks. Minimizing read reply latency is crucial for application  performance. Also, read reply data forms most of the traffic from MCs to  cores in few-to-many traffic scenario. To ensure that MCs do not get  clogged by a heavy influx of reply data, it is intuitive to design a NoC  with all-to-all connections. However, the area and power constraints of a  chip limits such architectures. Hence, researchers have come up with  smart solutions in [6], [7] where they propose intelligent NoC routing  schemes in tandem with MC placement to create conflict free paths in  NoCs for packets to traverse from MCs to cores. However, the complex  router design in such architectures add to the NoC power dissipation.   In our work, we make use of the few-to-many traffic pattern of the  reply packets and utilize an overlay circuit topology that forms dedicated  circuits for each MC. An overlay circuit connects an MC to all the cores,  forming return paths for read reply packets. Figure 4 shows how the  circuits are established from two MCs (represented as colored circles) to  cores. These overlay circuits are established from each MC to all the  cores, on top of the underlying 2D mesh-based NoC. An overlay circuit  Figure 4: A 4x4 NoC showing overlay circuit for each MC in a   few-to-many traffic scenario  IV. DAPPER OVERVIEW  In Dapper, we use a combination of approximate computing together  with overlay circuits to reduce the number of packets injected into the  NoC, and to provide the bandwidth required by MCs to deliver read reply  packets to cores, without increasing the energy consumption of the NoC.  Dapper has two components, (1) Data approximation support at the  memory controller (MC), that identifies the approximable data waiting in  its buffers and marks them for coalescence, and (2) overlay NoC that  delivers flits of coalesced packets to the respective destinations  simultaneously on overlay circuits that are established for each MC. We  define coalescence as a group of data packets whose read reply data from  MCs are approximable and within an error threshold. These packets are  then coalesced into a single packet, to reduce the number of packets  transmitted into the NoC for delivery to multiple destinations.   Dapper employs a 2D-mesh based NoC to connect its cores and MCs.  The network is divided into two planes, request and reply planes, each  with 64-bit channel width, to avoid protocol deadlock. The NoC routes  packets with an XY turn-based routing scheme to avoid routing deadlock.  The request packets are transmitted on a request plane and the reply  packets are transmitted via overlay circuits of the reply plane. A majority  of the reply plane traffic consists of read reply packets. Hence, Dapper  performs packet coalescing using approximation only on read reply data  that is received from DRAM or L2 cache. The reply plane of the NoC  contains a novel router architecture that enables establishing and tearing  down of the overlay circuits. More details of the above-mentioned  modules are discussed in the following sub-sections.  A. Data approximation at the memory controller  MCs are connected to the NoC through a network interface (NI) and  a router. An MC receives memory requests from cores and creates  commands to be sent to the DRAM or L2 cache. Figure 5 shows how an  MC handles a read request. Before sending a read request to an MC, a  core sets an approximable flag for the memory request, if the load  operation is on an approximable variable. The NI that connects the core  to the router generates flits from the memory request packet and adds an  approximable flag to the header flit. The receiver side NI generates a                memory request from these flits and sets an approximable flag for the  memory request before sending to the memory subsystem, as shown in  figure 5. Once the reply is received from DRAM or L2 cache, it is  matched with the corresponding request and saved in the output buffer.  In Dapper, each MC is equipped with a data approximator. The function  of the data approximator is go through each of the data packets waiting  at the output buffer of the MC, find the approximable data among the  waiting data, and check if any data can be coalesced before sending to  the NI for delivery to the destination cores. The granularity of  approximable data waiting in the output buffer is an L1 cache line  because a read reply from MC contains a cache line of the address that  leads to a cache miss which should be replaced. The data approximator  tries to coalesce cache lines waiting in buffer entries only if each of the  variable contained in the cache line is approximable.   Figure 5: Overview of approximation done at memory controller (MC)  Algorithm 1: Data Approximator  Inputs: Error threshold, Check_depth   1:    while Output_buffer.size( ) > 0 do  2:        ð ← 1  3:        dest_list ← Ø  4:        pkt ← Output_buffer. Front( )  5:        if pkt.approximable == 0 then  6:            send to NI(pkt)  7:        else  8:            While ð < Check_depth do  9:                  nxt_pkt ← Output_buffer.get(ð )  10:                nxt_data ← nxt_pkt.get_data( )  11:                if nxt_pkt.approximable == 1 then  12:                    if (pkt.data – nxt_data)/pkt.data < Error threshold then  13:                        dest_list. Add(nxt_pkt.dest)  14:                        Output_buffer.erase(ð)  15:                    end  16:                end  17:                ð++  18:           end  19:           pkt.add_dest(dest_list)  20:      end   21:      send to NI(data)  22:      Output_buffer.pop( )  23:  end      Algorithm 1 explains the steps involved in data approximation in our  framework. Algorithm 1 takes information about the MC output buffers,  as well as two parameters (error_threshold, check_depth) as inputs. It  iterates over each entry in the output buffer and checks if the data is  approximable (line 5). If the data is not approximable, it sends the data  unaltered to the NI for delivery (line 6). If the data is approximable, the  algorithm iterates through subsequent data entries to find other  approximable data (lines 8-9). If an approximable data is found, the  difference between the first approximable data and the found data is  calculated (lines 11-12). If the difference is within the error_threshold,  the found data entry is erased from the output buffer, and its destination  address is saved (lines 13-14). This iteration process stops once it reaches  the check_depth, which is defined as the maximum number of packets  that are coalesced if their data values are within an error threshold. The  list of destination addresses along with the original address is appended  to the first reply (line 19). The approximated data and the list of addresses  are then sent to the NI (line 21) to generate a packet that is broadcast to  the list of destination nodes on the overlay circuits as explained in the  following sections. The cleared entries in the output buffer are reused for  incoming data from DRAM or L2 cache.  The data types considered for approximation include the integer and  floating point types. For floating point data, only the mantissa is  approximated for a faster computation. The overhead involved in data  approximation is minimal. Algorithm 1 takes a small (constant) time for  execution, and requires logic for division, a comparator, and storage for  the parameters check_depth and error_threshold. The NI connected to an  MC is often fully occupied at run-time, resulting in a wait time for data  at MCs in most cases, before they are sent to the NI. This wait time helps  to mask the overhead involved in data approximation. A packet is only  sent to the NI after it passes the data approximation stage.   B. Overlay circuits for memory controllers  The reply plane of Dapper establishes dedicated overlay circuits from  MCs to cores as shown by green and red arrows in figure 4. Each MCs  overlay circuit is a predefined mapping of input and output ports in the  reply plane NoC routers, during which flits traverse from source (MC) to  destinations (cores) in 3 cycles. An overlay circuit assigned to an MC  lasts for a time duration determined at run-time. To establish overlay  circuits, the reply plane NoC is equipped with (i) a Global overlay  controller that decides the time duration for which an overlay circuit is  established, and (ii) modified routers called bypass routers through which  flits traverse in X or Y axes in a single cycle stopping only at a turn.  B.i. Global overlay controller (GOC)  The job of a global overlay controller (GOC) is to determine and  assign time durations for overlay circuits for each MC. The execution  time is divided into epochs, and each epoch is further divided into time  windows which are computed at the beginning of every epoch by the  GOC. A time window for an MC is determined at run-time based on the  number of reply packets waiting at the MCs output buffers, and the reply  arrival rate at the MC from the previous epoch. Each MC sends the stats  collected during an epoch to the GOC at the end of that epoch, using the  overlay circuits. The GOC uses this received information to compute a  weight function as shown in equation (1):   (cid:2022)((cid:1865)) = (cid:2009) . (cid:1827)((cid:1865)) + (cid:2011) . (cid:1828)((cid:1865))						                                 (1)  (cid:1846)(cid:1861) = (cid:1837) ∗ (cid:2022)((cid:1861))/[(cid:2022)(1) + (cid:2022)(2) + (cid:2022)(3) + ⋯ + (cid:2022)((cid:1865))]																									(2)  where A(m) is the reply arrival rate and B(m) is the average buffer  occupancy at the MC m in the previous epoch. α and γ are coefficients of  where Ti is the time window of the ith MC overlay and (cid:2022)((cid:1861)) is its weight  the weight function. The GOC compares ξ’s of each MC and computes  time window durations T1, T2, T3…, Tm for the next epoch as:  function. The ratio of the weight functions is then multiplied by a constant  K which is equal to the periodicity of the time windows in an epoch. The  time windows repeat periodically for E/K iterations in an epoch, where E  is the epoch interval duration and K is the periodicity of the time window  set. By having the time windows repeat and overlay circuits switch                         multiple times in an epoch, MCs send flits in multiple bursts across an  epoch. The time window durations are broadcast by the GOC at the  beginning of an epoch, which is saved by the reply plane routers in  dedicated buffers, and then used for setup and tear down of circuits.  B.ii. Bypass router architecture  The reply plane NoC is made up of bypass routers that support flits  passing through them without stopping at each hop for arbitration or route  computation. Figure 6 shows an overview of a bypass router architecture.  A bypass router has asynchronous links connecting output ports to input  ports and latches via an asynchronous switch and a crossbar. Each input  port is connected to a latch to save the flit that is coming over the X axis.  The router also saves time windows of each overlay circuit received from  the GOC in a local overlay controller as shown in figure 6. The crossbar  is configured at the beginning of each time window and reset at the end  of the window to establish a different overlay configuration. The crossbar  configuration of a router is based on its location on the 2D NoC and is  selected dynamically by the selection unit based on the current overlay  circuit. The output ports of a bypass router are connected to either input  ports or input latches to enable the 3 cycle flit transmission. In the first  cycle, a flit traverses along the bypass routers along the X axis through  asynchronous links and gets latched at the input latches. In the second  cycle, when the output ports are connected to input latches, the flit  traverses along the bypass routers in the Y direction and gets latched  again at the input latches. In the third cycle, the flit is sent from an input  latch to the local (core) port. Two important components of a bypass  router are the overlay controller and selection unit. The overlay  controller keeps track of the time windows and calls the selection unit to  dynamically configure the crossbar to establish the overlay circuits.   Figure 6: Overview of bypass router architecture  Algorithm 2: Overlay controller operation  Inputs: {time windows}, epoch_duration  1:     e_counter ← 0; // reset counter value  2:    {T1…Tk} ← {time windows} // initialize time window durations  3:     selection unit ({T1…Tk})  4:      for every cycle do         5:           if (e _counter < epoch_duration) then         6:               e _counter ++  7:           else if (e _counter  == epoch_duration) then  8:               {T1…Tk} ←  read_values(GOC_input) // save time windows  9:               selection unit({T1…Tk})  10:             e _counter ← 0; // reset counters  11:         end if  12:    end for  Algorithm 2 provides an overview of the overlay controller operation.  The controller gets the list of time window durations and epoch duration  as inputs from the GOC. It sends the list of time windows to the selection  unit as input (line 3). At every cycle, it increments a local counter to keep  track of the epoch duration (lines 5-6). At the end of an epoch duration,  updated time window values are received from the GOC which are sent  to the selection unit, and the local counter is reset (lines 8-10).   Algorithm 3: Selection unit operation  Inputs: {T1…Tk}, {MC1…MCk}  1:   ŧ←0, i←1  2:   for each cycle do  3:       if ŧ == Ti then  4:            i ← (i+1)%k   5:            if local.(X,Y) == MCi.(X,Y) then                          // scenario 1  6:                Lin→Eout,Wout,La(L) and La(L) →As(Nout,Sout)  7:            else if local.(Y) == MCi.(Y) then                           // scenario 2  8:                 Ein→Wout, La(E) and Win→Eout, La(W)   9:                 La(E) →As(Nout,Sout,Lout) and La(W) →As(Nout,Sout,Lout)  10:          else if local.(Y) > MCi.(Y) then  11:               Nin→Sout, La(N) and La(N) →As(Lout)              // scenario 3   12:           else if local.(Y) < MCi.(Y) then                             // scenario 4  13:               Sin→Nout, La(S) and La(S) →As(Lout)  14:           end if  15:      else   16:            ŧ ++  17:      end if  18:   end for  Algorithm 3 provides an overview of the selection unit operation. The  inputs to the selection unit are the time window durations {T1…Tk} for k  overlay circuits (corresponding to the k MCs). The selection unit  possesses knowledge of the coordinates of the MCs in the 2D mesh NoC  along with its own coordinates. With the given inputs, the selection unit  creates connections for an overlay circuit. At the beginning of each time  window, the selection unit compares the coordinates of the current router  and the MC for which the overlay is being established. Based on the  location of the router, there are 4 possible scenarios for each MC as  shown in figure 6. Scenario 1: If the router and MC are at the same NoC  point, the local input port is connected to the east and west output ports  and local latch (line 5). Scenario 2: If the router and the MC are along  the same X axis, the east and west inputs are connected to the west and  east output ports, and to their corresponding latches (line 7). Scenario 3:  If the router and MC are not along the same X axis and the router’s Y  coordinate is greater than the MCs Y coordinate, the north input is  connected to the south output and north latch (line 10). Scenario 4: If the  router and MC are not along the same X axis and the routers Y coordinate  is less than the MCs Y coordinate, the south input is connected to the  north output and south latch (line 12). All the input latches are connected  to output ports through an asynchronous switch, represented as  ‘As(Nout/Sout/Lout)’ in Algorithm 3, and are used for routing the flits along  the Y axis. At the end of a time window, the selection unit implements  connections for the overlay circuit of the next MC and its corresponding  time duration. This entire operation takes around 2 cycles at the  beginning of each epoch.   Routing: For conflict free routing using bypass routers, each row in the  2D NoC should only have one MC. A packet that is approximable might  have more than one destinations based on the check depth parameter of  the MC. To accommodate more than one destinations, the header flit of  the packet has more than one destination fields, and a destination length  field. When a packet is ready to be transmitted at the NI of an MC, it is  sent to the NI interface buffer based on the availability of the buffer  space. At each cycle, the router transmits flits along the asynchronous  links along the X direction and saves them at the corresponding input  latches of each bypass router (shown via the yellow line in figure 6). The  Y-compare unit (figure 6) then compares the destination Y coordinates  of the flits with its own coordinates and sends a signal to the  asynchronous switch to establish connections between input latches, and                    the north/south/local output ports, based on the location of the router.  Once the connections between latches and output ports are established,  the flits traverse in the Y direction (shown via the red line in figure 6) and  reach the destination router where they are sent to the local output port.  When the tail flit passes the bypass router, the asynchronous switch is  reset to tear down the connections between latches and output ports.   Overheads: The overhead involved with the overlay controller,  selection unit and Y-compare are very minimal. The overlay controller  uses a counter, and a register to store the time window values received  from the GOC, while the selection unit uses a counter, a cyclic register,  and five 3-bit comparator circuits. The bypass router also has an  additional Y-comparator, and an asynchronous switch. All of these  components together take up a very small fraction of the power and area  of a router. The bypass routers also do not have input buffers to save  incoming flits, VCA, SA, and route computation logic. Hence, a bypass  router consumes up to 50% less power compared to traditional router.   A. Experimental setup  V. EXPERIMENTS  We target a 16-core GPGPU based accelerator to test the performance,  energy consumption, network latency, and output error of Dapper  compared to the state-of-the-art. Table 1 lists the platform configurations.  We used GPGPU-Sim [21] to collect detailed application traces and  simulated the network and memory traffic on a customized Noxim NoC  simulator [22] that integrates our Dapper architecture model. We use  trace driven simulation as it is fairly accurate for architectural analysis.  We obtained traces for 6 CUDA-based applications [1], 4 of which  belong to the image processing domain {Discrete Cosine Transform 4x4,  Histogram, Convolution Texture, Direction texture compressor}, and the  other two are scientific applications {Blackscholes, Fast Walsh  Transform}, each with a different number of kernels and memory  intensity. As mentioned earlier, we use the programming paradigm  proposed in EnerJ [19] to specify the variables in these applications that  are potentially approximable. We set the epoch duration in Dapper as  10,000 cycles, the values of α, γ coefficients of the weight function from  equation (1) to 0.6, 0.4, and K = 1000 in equation (2). The power,  performance, and area values for our NoC architecture, modified MCs,  and cores at the 22nm node are obtained using the open source tools  DSENT [23] and GPUWATTCH [24], along with gate-level analysis.  We compare our proposed Dapper architecture with the baseline NoC  of the configuration shown in Table 1, and a prior work that utilizes  dictionary-based approximate compression and decompression at each  network interface, called Approx-NoC [16]. The baseline NoC has  channel width of 128-bits which is twice the size of the L1 cache line, to  provide high throughput to MCs. Approx-NoC uses dictionary-based  inexact compression using approximation at the sending NI to reduce the  number of flits that are injected into the NoC and save the overall energy  consumed. All the three techniques use XY routing, with Dapper  incorporating overlay circuit-based flit transmission in the reply plane.  The size of the MC output buffer is set to accommodate 66 data packets,  as used in GPGPU-Sim, for all the comparison works.   TABLE 1 GPGPU-SIM PARAMETERS  Parameters  Value  Shader Cores/MCs  12 / 4 (16-core)  Shader core pipeline  1536 Threads, warp size = 32  Shader registers  32768 per core  Constant / Texture Cache  8KB / 8KB per core  L1, L2 cache  16KB L1 per core, 128KB L2 per MC  NoC Topology  4×4, XY Routing  Channel width  128 bits   Base case router architecture  4-stage router, 5 VC/port, 4 buffers/VC  B. Dapper Sensitivity Analysis  We first carry out experiments to find the best values of buffer check  depth and error threshold parameters used in our data approximation  stage in the MCs in Algorithm 1. We compare Dapper with different  check depths and error thresholds to analyze the tradeoffs between energy  saved and the output error observed at the end of the application  execution. Output error is computed using equations (3) and (4):                                         (cid:1857) = ((cid:1848) − (cid:1848)′)/(cid:1848)                                         (3)                                             (cid:1857)(cid:1870)(cid:1870)(cid:1867)(cid:1870) = (cid:2869)(cid:3014) ∗ ∑ |(cid:1857)(cid:3036) |                                   (4)  (cid:3014)(cid:3036)(cid:2880)(cid:2868) where for each point in the output that comprises of images or matrices,  V and V’ are the actual and inexact points and M is the total number of  output points. Equation 4 gives the value of output error obtained at the  end of the simulation using approximation.  (a)  (b)  Figure 7: Comparison of energy consumption and output error observed with  Dapper across (a) check depths (CDs), and (b) error thresholds   Figure 7(a) shows the plots of normalized energy consumption and  normalized output error observed with Dapper across different values of  output buffer check depths (3, 6, 9) for data approximation at MCs. Check  depth is the maximum number of packets that are coalesced if their data  values are within an error threshold. The bars in the figure show the  energy consumption for the three configurations being compared and the  red dots indicate the output error. Both results are normalized to the result  for the check_depth = 3 configuration. From figure 7(a), on average, a  check depth of 6 gives up to 10% less output error compared to check  depths of 3 and 9. All the check depths give similar energy consumption  within a 1-2% range. The best check depth value varies for each  application based on the spatial locality of the data being requested from  memory and the computation being performed on the approximable data.  The DXTC application shows a decreasing trend in output error as the  check depth increases, as the number of inexact output values and their  sum do not increase proportionally to increase the error value in equation  (4).  For the FWT application, the trend for output error is opposite to that  observed for the DXTC application.   Figure 7(b) shows the plots of normalized energy consumption and  output error values observed by Dapper across different error thresholds                      (10%, 15%, 20%). The bars in the figure show the energy consumption  for the three configurations being compared and the red dots indicate the  output error. Both results are normalized to the result for the 10% error  threshold configuration. Not surprisingly, increasing the error threshold  leads to more output error for all the applications. For the DCT and Hist  applications, the increase in output error is high due to the high volume  of approximable variables present in the memory reply data. For DXTC,  the increase in output error is due to the computational error that increases  exponentially with the magnitude of error in the data values received.   Although the number of packets injected into the reply plane network  changes with changing check depth and error threshold values, due to the  low power consumption by bypass routers, the NoC consumes similar  energy across the three chosen values of check depth and error threshold.  Based on our sensitivity analysis, we use an error threshold of 10% and  a check depth of 6 for our Dapper architecture, when comparing with  other NoC architectures, as discussed in the following subsections.   (a)  (b)  compared to the baseline. In contrast, Dapper has only ~1% slowdown  in application execution time compared to the baseline. As the baseline  NoC is twice the size of a traditional NoC, it consumes higher power and  area, to provide the bandwidth necessary to cope with the high data  arrival rate at MCs. Although Approx-NoC reduces the number of flits  using approximate compression, the latency overhead for compressing  and decompressing packets at the source and destination NIs consumes a  significant chunk of the time saved by transmitting fewer flits. This  impact is particularly high in the Hist application where the latency  overhead becomes prohibitively large.   Figure 8(b) shows the energy consumption of the comparison works  across different benchmarks. On average, Dapper consumes up to 50%  less energy compared to the Baseline architecture, due to its low power  router architecture in the reply plane, and transmission of fewer flits  compared to baseline. Approx-NoC on the other hand consumes higher  energy due to higher power consumption in its routers that need  additional logic and CAS-based storage for saving the most used values  for compression and decompression. In Dapper, energy savings are  higher for applications that generate lower to medium ratio of  approximable data such as Hist and ConvTex. For DCT, Blackscholes,  and DXTC, which have a high percentage of approximable data, energy  savings are relatively lower due to the high volume of reply data, where  the resulting congestion ends up dominating overall latency.   Figure 8(c) shows the network latency of the comparison works across  various application. Network latency is high when the data influx rate is  high. For applications with high incoming data rates, such as DCT, FWT,  and DXTC, Approx-NoC shows high network latency due to the high  compression and decompression overheads involved for the high volume  of packets. Dapper on the other hand shows network latency similar to  the baseline NoC although with less energy consumption as shown in  figure 8(b). Hence, we can conclude that Dapper in GPGPUs achieves  similar performance to the baseline, while consuming 50% less energy.  (c)  Figure 8: Comparative analysis of Dapper with Approx-NoC [16], and a  baseline NoC architecture for (a) application execution time, (b) energy  consumption, and (c) network latency, across various CUDA applications.  C. Comparative Analysis  Figure 8(a) shows the application performance of different CUDA  applications executing on the GPGPU with three different NoC  architecures: Baseline, Dapper, and Approx-NoC [16]. It can be observed  that Approx-NoC has a 5-10% slowdown in application execution time  Figure 9: (a) Comparison of output error values across executed applications  for Approx-NoC [16] and Dapper configurations.  D. Error percentage   Obviously, it is also important to analyze the error in applications that  are subjected to approximation. Figure 9 shows the comparison of output  error % for Approx-NoC and three configurations of Dapper with  different error threshold values. Note that the baseline NoC is not shown,  as it has no output error. The error percentage is computed as 100 × error  from equation (4). Approx-NoC shows high percentage of error in its  output due to a flaw (right shift division) in its error threshold  computation which is used for identifying the approximable data before  compression. This results in a very high error percentage when data  values are incorrectly marked for approximation. Dapper on the other  hand incurs less than 1% error with 10% threshold, around 2% error with  a 15% threshold, and around 3% error with a 20% threshold due to more  accurately marking approximable data that can be coalesced before                          sending to destinations. Thus, Dapper presents a promising NoC-centric  solution to save energy consumption in GPGPUs for applications that  have a potential for being approximated (i.e., applications that can  tolerate some output error). As an illustrative example, Figure 10 shows  the DCT application output under no error (when using the baseline  NoC), and when using the Dapper. It can be observed from figure 10 that  the output for the configuration of Dapper with 10% error threshold is  virtually indistinguishable from the no error case, while saving almost  40% energy (figure 8(b)). This example highlights the exciting potential  of Dapper to save energy in GPGPUs.                                         (a)                                                (b)                                       (c)                                                (d)  Figure 10: DCT output: (a) original (no error), (b) with 10% error threshold  in Dapper, (c) with 15% error threshold in Dapper (d) with 20% error  threshold in Dapper  VI. CONCLUSIONS  In this paper, we propose a novel data-aware approximate NoC for  GPGPU architectures that identifies the approximable data waiting in the  output buffers of memory controllers and coalesces them to reduce the  number of packets injected into the NoC. Also, we advocate for a fast  overlay circuit based NoC architecture in the reply plane of the NoC for  the reply packets to reach the destinations in 3 cycles. To enable the  establishment of overlay circuits, we propose a novel low power router  architecture in the reply plane NoC called bypass router. Experimental  results show that Dapper dissipates up to 50% less energy compared to  baseline NoC, with only 1% slowdown in application performance and  less than 1% error in the application output. Thus, for the class of  emerging applications that have the ability to tolerate a small error in their  outputs, Dapper can provide significant savings at the NoC level. These  savings are orthogonal to (and can be combined with) further  approximation strategies at the computation components (e.g., using  approximate adders) to further expand the design space of trade-offs  between application output error, energy costs, and execution time.  "
A Diversity Scheme to Enhance the Reliability of Wireless NoC in Multipath Channel Environment.,"Wireless Network-on-Chip (WiNoC) is one of the most promising solutions to overcome multi-hop latency and high power consumption of modern many/multi core System-on-Chip (SoC). However, the design of efficient wireless links faces challenges to overcome multi-path propagation present in realistic WiNoC channels. In order to alleviate such channel effect, this paper presents a Time-Diversity Scheme (TDS) to enhance the reliability of on-chip wireless links using a semi-realistic channel model. First, we study the significant performance degradation of state-of-the-art wireless transceivers subject to different levels of multi-path propagation. Then we investigate the impact of using some channel correction techniques adopting standard performance metrics. Experimental results show that the proposed Time-Diversity Scheme significantly improves Bit Error Rate (BER) compared to other techniques. Moreover, our TDS allows for wireless communication links to be established in conditions where this would be impossible for standard transceiver architectures. Results on the proposed complete transceiver, designed using a 28-nm FDSOI technology, show a power consumption of 0.63mW at 1.0V and an area of 317 μm 2 . Full channel correction is performed in one single clock cycle.","A Diversity Scheme to Enhance the Reliability of Wireless NoC in Multipath Channel Environment Joel Ortiz Sosa Univ Rennes, Inria Lannion, France joel.ortiz-sosa@inria.fr Olivier Sentieys Univ Rennes, Inria Rennes, France olivier.sentieys@inria.fr Christian Roland UBS - Lab-STICC Lorient, France christian.roland@univ-ubs.fr Abstract—Wireless Network-on-Chip (WiNoC) is one of the most promising solutions to overcome multi-hop latency and high power consumption of modern many/multi core System-onChip (SoC). However, the design of efﬁcient wireless links faces challenges to overcome multi-path propagation present in realistic WiNoC channels. In order to alleviate such channel effect, this paper presents a Time-Diversity Scheme (TDS) to enhance the reliability of on-chip wireless links using a semi-realistic channel model. First, we study the signiﬁcant performance degradation of state-of-the-art wireless transceivers subject to different levels of multi-path propagation. Then we investigate the impact of using some channel correction techniques adopting standard performance metrics. Experimental results show that the proposed Time-Diversity Scheme signiﬁcantly improves Bit Error Rate (BER) compared to other techniques. Moreover, our TDS allows for wireless communication links to be established in conditions where this would be impossible for standard transceiver architectures. Results on the proposed complete transceiver, designed using a 28-nm FDSOI technology, show a power consumption of 0.63mW at 1.0V and an area of 317 µm2 . Full channel correction is performed in one single clock cycle. Index Terms—Wireless Network-on-Chip, Channel Effects, Multi-Path Propagation, Communication Reliability, Digital Transceiver Architecture I . IN TRODUC T ION The emerging manycore architecture era comes up with critical issues in communication latency and power consumption. Network-on-Chip (NoC) was a very promising solution to solve these challenges. However, with the scaling of advanced CMOS technologies, the number of cores is continuously growing. As a consequence, the number of hops needed to communicate between cores creates a critical bottleneck in the communication backbone by increasing latency and power consumption due to large volume data transmission. To deal with the surge of high performance computing systems and to overcome this communication bottleneck, new interconnect technologies, such as 3D-NoC [1], Radio-Frequency Interconnects (RF-I) based on waveguides [2], Optical NoC [3] [4] and Wireless NoC [5], have emerged. Although it is true that Wireless Network-on-Chip (WiNoC) is one of the most promising solutions for the communication infrastructure of chip multiprocessors (CMP) and multiprocessor System-on-Chip (MPSoC), the reliability of on-chip wireless links is still an important and unsolved issue [6]. Indeed, most of the approaches from the literature estimate performance using a very simplistic wireless channel model, which only considers path loss through free space and Additive White Gaussian Noise (AWGN). However, a realistic wireless channel model should consider some of the most important parasitic phenomena introduced by the physical structures of the chip (e.g. multi-path propagation), especially, when communication data rate is aiming at several tens of gigabits per second. The main contributions of this paper are as follows: • We demonstrate, by simulation, the signiﬁcant degradation of wireless communications facing multi-path propagation for current Wireless NoC architectures. Simulation is performed by adopting a proposed semi-realistic wireless channel models, based on the wireless intra-chip communication literature. • We study traditional channel cancellation techniques to improve communication reliability and adapt them to the on-chip wireless communication context. • We propose a digital transceiver architecture for each of the adopted techniques and evaluate their performance, power consumption and area for a 28 nm FDSOI technology and show that this overhead is very small compared to the gain in communication reliability. This paper is organized as follows. Section II highlights the main limitations in the state-of-the-art of current WiNoC architectures. In Section III, we present a semi-realistic channel model, while in Section IV, we study the traditional wireless interfaces used in WiNoC and show their weaknesses. After introducing some techniques to improve communication reliability, we propose a new transceiver architecture in Section V. Finally, in Section VI, we show some performance and synthesis results of the proposed architecture before to conclude in Section VII. I I . R ELAT ED WORK Wireless Network-on-Chip (WiNoC) was proposed as a more feasible and promising solution that can scale with transistor technology and the ever-increasing number of cores in a NoC [7]. Most of the contributions in the literature attempt to demonstrate that the WiNoC paradigm can overcome the limitations of conventional multi-hop NoC architectures for core-to-core [8], [9], and core-to-memory (e.g. cache coherency) [10], [11] communications. Many works 978-1-5386-4893-3/18/$31.00 ©2018 IEEE have also investigated different network topologies to take advantage of long distance links [12], [8]. WiNoC can thus allow for the reduction of energy consumption involved by transporting information packets through the network routers. The topology also impacts the total latency and throughput of the network [7]. Despite all these efforts to propose efﬁcient WiNoC architectures, the considered wireless channel model is very simple (i.e., using an additive white Gaussian noise) and therefore not representative of actual on-chip wireless channels. In particular, this basic model does not consider the frequency and time behavior of WiNoC for isotropic antennas, such as multi-path propagation and bandwidth limitation. However, few prior works have introduced some interesting channel features that could be considered for WiNoC simulations. As an example, the authors of [13] give a general overview of the main WiNoC characteristics considering a two-ray channel model. Besides, their analysis concludes that on-chip wireless channel resembles to familiar terrestrial settings. An analytical channel model is proposed in [14]. This model introduces the concept of molecular absorption attenuation (MAA), which makes the wireless communication through the air unsustainable. In any case, both analyses consider multi-path propagation as an unavoidable phenomenon for intra-chip wireless communications. However, to the best of our knowledge, there is no work that considers during performance simulations the signiﬁcant impact of multi-path propagation on the reliability of WiNoC communications. In contrast, the use of highly directive antennas based on carbon nanotube (CNT) is proposed in [15], which could avoid multipath propagation. Nevertheless, CNT antennas are still facing many challenges to be fully integrated in current chip fabrication process. Moreover, with such solution, broadcasting wireless signals will be hardly achievable without a dense antenna array. Other research studies around antennas designed for on-chip communications (e.g., [16], [17], [18]) also provide relevant information to build more realistic WiNoC channel models. However, the experimental setup only considers simple scenarios such as point-to-point communications without effects of chip package and real multicore context as it should exist in WiNoC. Effects of wave propagation caused by using monopole or dipole antennas fabricated on silicon wafer as well as channel analysis for this simple type of communication are also proposed [19], [20], [21], [22]. I I I . CHANN EL MODE L ING In any wireless communication system, it is very important to estimate the channel characteristics. Depending on the channel, the design of the transceiver needs to ensure a certain degree of wireless link reliability, as well as to reach a trade-off in design complexity. WiNoC, as a speciﬁc scheme of intrachip wireless communication, also requires a precise estimation of the channel. The channel must be modeled considering the parasitic phenomena introduced by the physical structure and electrical properties of all the components inside the chip, as well as the chip-package [14]. As a result, building an accurate model of a WiNoC channel is important but it is also a complex task. A wave propagated through a channel can suffer reﬂection, refraction and diffraction. Accordingly, waves can be propagated using different media at the same time. This approach was modeled and validated in [18], creating different ray paths through the silicon wafer. Besides, Zhang et al. [17] demonstrated that at least three types of waves on which the signal is propagated over intra-chip channel must be considered: space wave (air), surface wave (air-wafer interface), and guided wave (through silicon substrate). Likewise, their study concludes that the dominant path is not transmitted by space wave, but mainly by surface wave. Furthermore, the resistivity of the silicon substrate is a very important parameter for intra-chip wireless communication. Propagating waves through a low-resistivity ( ρ = 10 − 20 Ω · cm) or a high-resistivity ( ρ ≥ 5 kΩ · cm) material, can signiﬁcantly affect signal attenuation. Moreover, as demonstrated in [19], the inﬂuence of multi-path fading effects is more important for low-resistivity silicon substrate and for shorter distances. Also, a delay spread was measured for different antennas (zigzag, meander, and linear) in [17]. It was observed that the delay spread generally increases with the transmitting and receiving antennas (T-R) separation, although delay spread for a speciﬁc channel impulse response (CIR) depends on the surrounding of the T-R pair. For instance, a delay spread for a distance of 10 mm, using zigzag monopole-pair with some interference structures, is 120 ps . In this paper, to estimate and analyze the impact of a semi-realistic channel, the provided simulations are based on the previously discussed models deﬁned for intra-chip wave propagation. We consider that a channel impulse response should include at least two propagation paths for realistic WiNoC simulations together with Additive White Gaussian Noise (AWGN). Obviously, the number of paths is dependent on the symbol duration. For instance, considering a data rate up to 32 Gbit/s for WiNoCs, the symbol duration will be as small as 32.25ps , which is four time smaller than the delay spread for a distance of 10 mm. This indicates that the number of signiﬁcant paths could be up to three, each one separated by a symbol duration. On the other hand, due to the low complexity of its implementation, the classical modulation scheme considered for WiNoCs is On-Off Keying (OOK) with a maximum data rate of 16 Gbit/s. However, other modulation schemes (BPSK, QPSK, 16-QAM, etc.) can provide better spectral efﬁciency and higher resilience to multi-path propagation, although consuming more power and area than OOK. In order to test the reliability of current WiNoCs facing multi-path propagation, Fig. 2 represents the Bit-Error Rate (BER) of an OOK transceiver as a function of the Signalto-Noise Ratio (SNR) per bit in various multi-path conditions. Our simulations consider a two-ray model, and a time-invariant frequency-selective channel impulse response (CIR) composed of only two paths (the direct and the reﬂected paths), separated Keying (OOK) transceiver module, which has a very low complexity compared to other modulation schemes. There exist several architectures proposed in literature for the OOK transceiver, considering coherent or non-coherent detection [23], [24], [25]. Moreover, power consumption reduction mechanisms can be applied [5], which would allow to optimize the power according to the distance. Figure 3: A conventional Wireless Interface of a WiNoC. The communication between the router/switch and the WI begins with the source core setting the corresponding message into the Network Interface (NI) module. The NI packetizes the message by breaking it into several ﬂits (such as header, payload, tailer). The router/switch is then used to forward the data ﬂits into the wired interconnection network or towards the WI, whether the token ﬂow controller (TFC) authorizes the use of the wireless medium. The data ﬂits will be converted into a serial data stream using the serializer (SER) block. Then the OOK modulator adapts the baseband frequency signal into a high frequency one (carrier frequency). The signal obtained is transmitted to the power ampliﬁer (PA), which delivers the required transmitting power. Finally, this ampliﬁed signal is spread through the wireless medium using a dedicated or shared antenna. In case of a shared antenna, a nonmagnetic nonreciprocal passive CMOS circulator [26] allows for full-duplex (frequency-division duplexing) or halfduplex communication. At the receiver side, once the signal reaches the antenna, it is intensiﬁed by a low noise ampliﬁer (LNA). Then, a demodulator shifts the high frequency signal into a baseband one. The next step is to amplify and ﬁlter the serial baseband signal, which is converted into a parallel data stream of ﬂits using the deserializer (DESER) block. Finally, considering that the destination core is connected directly to the router/switch, the received ﬂits will be conducted towards the NI to retrieve the message forwarded by the source core. Figure 1: Channel impulse response with α = 60% by a symbol duration, as shown in Fig. 1. Also, the gain of the reﬂected path, deﬁned as a percentage of the direct one, varies from α = 0% to α = 60%. As observed in Fig. 2, the communication starts to degrade rapidly as soon as the reﬂected-path gain increases, even with small values of α . The degradation becomes signiﬁcant even if the amplitude of second path is only 30% of the main path. For this reason, multi-path cancellation techniques and their respective architectures are required to increase reliability of WiNoC communications. Figure 2: Scaling of BER performance according to the SNR per bit (Eb /N0 ) for different reﬂected-path gains in the OOK scheme. IV. O PT IM I Z ING TH E W INOC COMMUN ICAT ION SCHEM E In this section, we ﬁrst detail the basic communication scheme for WiNoCs and then introduce our modiﬁcations to increase communication reliability. Moreover, to overcome the CIR presented in the above section, we introduce some techniques used in traditional wireless communication systems to decrease channel effects. A. Traditional Wireless Interface A traditional Wireless Interface (WI) connects a conventional router/switch with the wireless medium. The WI can be divided into three main parts: antenna, analog and digital blocks, as shown in Fig. 3. The digital domain mainly incorporates the channel access token controller and a Serializer/Deserializer. The analog domain includes an On-Off B. Improving Communication Reliability To the best of our knowledge, current transceiver solutions from WiNoC literature do not consider some channel effect cancellation mechanisms, even during performance simulations. In this paper, we therefore study, simulate and synthesize traditional channel cancellation techniques adapted to onchip wireless communications. One reasonable hypothesis of 0 · · ·  b2· · ·   = (1)  0 0 0  0· · · 1· · ·  0 0 · · · · · · 0 · · · · · · this work is that the WiNoC channel is time invariant. This characteristic prevents the use of complex adaptive ﬁltering algorithms for WiNoC communications. Finally, the theoretical and mathematical approaches of each considered technique is obtained from [27]. 1) Equalization Techniques: are widely studied in conventional wireless communication to compensate for Inter Symbol Interference (ISI) created by multi-path propagation. Equalization can be linear (e.g., Zero Forcing, Minimum Mean Square Error) and non-linear (e.g. Decision Feedback Equalizer). The effectiveness of multi-path cancellation varies widely in practice, according to the type of equalizer applied. Furthermore, hardware cost and computation complexity are important issues when equalization is adopted. In this section we analyze the impact of Zero-Forcing (ZF) linear equalization, which is less complex than other equalization techniques. ZF restores the received signal applying the inverse of the channel frequency response computed off-line or on-line. Normally, a linear transverse ﬁlter is used when this technique is adopted. The coefﬁcients bi of the ﬁlter can be calculated in the time domain from the CIR, by resolving the following matrix system h0 h1 h2 · · · · · · h0 h1 b0 b1 · · · 0 h0 hk −1 h2 h1 h0 bk −1 where hi are the multi-path components of the CIR and k the total number of ZF ﬁlter coefﬁcients is an odd integer value. As an example following Fig. 1, h0 = 1, h1 = α and hi = 0, ∀ i = 2 . . . k − 1. The impact of using the ZF technique is simulated with different number of coefﬁcients (i.e., taps) k using the CIR with α = 60% as deﬁned in Section III. As shown in Fig. 4, the multi-path cancellation efﬁciency improves with the number of coefﬁcients. However, a large number of taps occupies signiﬁcant area and consumes a relatively high amount of power, especially when computed at high frequency. Furthermore, even with a large number of coefﬁcients, the simulated BER will never match the theoretical OOK BER. This phenomenon occurs because the zero-forcing equalizer ampliﬁes the noise power during the signal ﬁltering process. 2) Direct Sequence Spread-Spectrum (DSSS): is a transmission technique used in wireless network communications where a data signal at the sending node is combined with a code sequence (e.g. Gold sequence, M-sequence, Hadamard codes) running at high frequency, the chip rate. DSSS combines users’ data based on a spreading factor (S F ) deﬁned as the chip rate divided by the symbol (data) rate. The main advantages of DSSS are a resistance to jamming and interference, less background noise, and a certain degree of multipath resilience depending on the code sequence characteristics. Furthermore, DSSS allows for multiple users to access the same channel simultaneously in synchronous or asynchronous mode. In the WiNoC literature, this technique was adopted by Figure 4: Demodulation performance BER according to the SNR per bit (Eb /N0 ) using Zero-Forcing equalizer with different number of ﬁlter taps. [28], demonstrating promising results over conventional NoCs. However, simulations and performance estimations provided in [28] were only considering an ideal wireless channel. 3) Time-Diversity Scheme (TDS): implies that the same data is transmitted repeatedly at different time schedule. Thus, considering a channel with multi-path propagation, there are multiple copies of the same signal transmitted according to the number of multi-path components. Therefore, as depicted in Fig. 5, DSSS also takes advantage of this characteristic code sequences delayed by i = 0 . . . L − 1 chip periods. The assuming L correlators between the input data signal y (t ) and correlator outputs are combined using a diversity combining rule with their respective weighting coefﬁcients (G i ). On the other hand, since this technique collects the energy from the received signal components to provide diversity, it is often called the RAKE receiver. Figure 5: Diversity demodulator for spread-spectrum signals over multi-path channels. V. ARCH I T EC TUR E O F TH E W IR EL E S S IN TER FAC E This section presents the WiNoC and wireless transceiver architectures considered in this work. A. WiNoC Architecture 1) Topology: The adopted topology is based on a wired 2-D mesh conventional NoC architecture, divided into N clusters including a wired sub-network. Each cluster has one Wireless Interface (WI) connected to a dedicated router/switch, responsible for providing wireless access to the cores placed in the same cluster. Fig. 6 illustrates a 20mm × 20mm chip example composed of 8 × 8 cores. The WiNoC architecture is divided into N = 4 clusters of 16 cores with a WI located at the center of each cluster. 2) Wireless Multiple Access: Due to the availability of multiple channels, the synchronous DSSS technique is adopted for channelization to achieve simultaneous multiple communications among the WIs. Each cluster i uses a dedicated code sequence Ci for data transmission and knows the code sequence of the other clusters for data demodulation at the receiver side. Besides, we consider that signal synchronization is performed only once in the WI to open the different simultaneous channels. Figure 6: Wireless interface distribution among a clustered WiNoC hybrid topology with N = 4 clusters. B. Wireless Interface Architecture The adopted WI architecture is similar to the one explained in Section IV-A. However, the digital domain part is modiﬁed to comply with the proposed techniques. Fig. 7 depicts the global architecture of the digital transceiver, which is divided into three parts classiﬁed by clock frequencies: low 1 , medium 2 and high 3 . Furthermore, zero-forcing equalization and time diversity scheme are implemented using DSSS as a baseline architecture. Therefore, in our simulation and synthesis results, we tested three main architecture conﬁgurations detailed in this section and shown in Fig. 7: DSSS, TDS with DSSS, and ZF with DSSS. Additionally, DSSS needs an analog-to-digital converter (ADC) block to distinguish each code-channel. Consequently, to avoid excessive area cost and power consumption, we propose to adopt a 10 Giga-samples-per-second Time-Interleaved Successive Approximation Register (SAR) ADC [29] instead of a ﬂash ADC. This ADC provides eight 4-bit words in parallel at 1.25 GHz corresponding to eight consecutive samples. Despite the use of serializer (SER) and de-serializer (DESER) blocks is mandatory in any wireless communication, previous WiNoC literature rarely details information about these blocks. Therefore, we propose to adopt a SER/DESER binary MUX tree topology with half-rate architecture [30], [31], which uses a 625 MHz half-rate clock source to generate 1.25 Gb/s data rate. In addition, this architecture is a good compromise between higher data rates and risks of duty-cycle distortion and clock skew. Figure 7: Enhanced digital-domain transceiver architecture with three decoder conﬁgurations. 1) DSSS Architecture: is divided into two main functional blocks: encoder and decoder. Furthermore, the code sequence adopted in this architecture is a Hadamard binary code on 8 bits (S F = 8). As each cluster has a dedicated code sequence, the design of the encoder block is very simple. The encoder is composed of registers whose outputs are arranged according to the Hadamard code sequence assigned to the considered cluster. The decoder, depicted in Fig. 8, is a dedicated block used to retrieve the information forwarded by other clusters. As a Figure 8: DSSS decoder architecture. consequence, the decoder is composed of one correlator for each block with which it communicates, using the respective code sequence assigned to the considered cluster. As an example, for the architecture of Fig. 6 with four clusters, three correlators will be used in each cluster. Each correlator receives eight 4-bit words D j in parallel from the ADC, with D j ∈ {0, 1, . . ., 15} and j = {0, 1, . . ., 7} . These D j input values are directly compared with the j th bit of code Ci, j related to the i th cluster. Consequently, the value of D j is turned to negative if the j th bit is 1, and remains positive otherwise. Then, the correlator performs addition or subtraction operations to (2) calculate C or r = ±D0 ± D1 ± D2 ± D3 ± D4 ± D5 ± D6 ± D7. The ﬁnal step is to make a decision based on the resulting value of C or r , using a threshold detector. The optimal threshold value for DSSS with OOK modulation using Hadamard codes is 0. In our case, the decoder output is 1 when cor r < 0 and 0 when cor r > 0. This decision operation is simpliﬁed by connecting the sign bit (the MSB as we consider two’s complement representation) of the resulting value of C or r directly to the decoder block output. 2) TDS with DSSS architecture: uses the same encoder block architecture, as explained in the previous section. However, the decoder, depicted in Fig. 9, increases the number of correlators used by cluster code sequence according to the number of signiﬁcant multi-path components (MPC) estimated from the CIR. As the architecture is designed using the CIR (3) combiner as follows: C or r 0 = −D0 − D1 − D2 − D3 + D4 + D5 + D6 + D7 C or r 1 = D0 − D1 − D2 − D3 − D4 + D5 + D6 + D7 M RC = G0 × C or r 0 + G1 × C or r 1 3) TDS Decoder Optimization: is performed by grouping the similar bits of the code sequences and their time-shifted version, as shown in Fig. 10. Generally, Hadamard codes contain similarities with their time-shifted version. Therefore, this characteristic is exploited to decrease the number of operations in a correlator by rewriting Eq. 3 as C or r 0 = −D1 − D2 − D3 + D5 + D6 + D7 C or r 1 = −D0 + D4 M RC = G0 × (C or r 0 + C or r 1 ) + G1 × (C or r 0 − C or r 1 ) 4) Zero Forcing with DSSS Architecture: uses the same encoder block as the simple DSSS architecture. The decoder block, as illustrated in Fig. 11, is composed of three main subblocks: a circular buffer (CB), a ZF ﬁlter to mitigate multi-path propagation and a DSSS correlator with a code sequence. The coefﬁcient values of the ZF ﬁlter are calculated from Eq. 1 using the CIR of Section III with α = 60%. Figure 9: Decoder architecture of the time diversity scheme. studied in Section III with α = 60%, two correlators are used by code sequence to retrieve the forwarded information from one cluster. The ﬁrst one compares the input values D j with the j th bit of code Ci, j related to the i th cluster and the second one compares D j with a time-shifted version of Ci, j . Then the correlation results are combined using a diversity combining rule, called Maximum Ratio Combining (MRC). The MRC uses weighting coefﬁcients obtained from the CIR to combine the correlator outputs. Finally, the sign bit (MSB) of the resulting combination value obtained from MRC computation is connected directly to the decoder block output. To illustrate the correlator and MRC operations, Fig. 10 displays one Hadamard code with its one time-shifted version. These codes are processed by the correlators and the MRC Figure 10: Time-diversity scheme optimization. Figure 11: ZF equalizer with DSSS decoder architecture. V I . R E SU LT S To evaluate the performance metrics of the different techniques studied in Section V, an end-to-end wireless communication system has been implemented for simulation and hardware synthesis. The effectiveness of each technique is evaluated adopting the multi-path channel characteristics described in Section III. The system under study is ﬁrst simulated in MATLAB to estimate the BER performance. Then, the different architectures are modeled in C/C++ for High-Level Synthesis (HLS). Each architecture is synthesized to RTL using Catapult HLS and to the gate level by Synopsys Design Compiler. A 28-nm FDSOI technology library is used during hardware synthesis as a target with a supply voltage of 1 Volt. Finally, power consumption is estimated with Synopsys PrimeTime tool after activity extraction from a back-annotated gate-level simulation using 3.2 × 105 random input data. A. Bit Error Rate Evaluation The end-to-end system modeled in MATLAB considers a half-duplex communication and consists of four wireless interfaces to open three code-channels simultaneously. Each code-channel uses a dedicated orthogonal Hadamard code of 8 bits. The channel model employed in simulation was tworay time-invariant frequency-selective channel with AWGN. The modulation scheme simulated was coherent OOK. The simulated model considers the quantization noise (QN) added in the demodulation process by a 4-bit ADC at the receiver side [32]. The average BER, which considers three parallel code-channels, is estimated by Monte-Carlo simulation with a conﬁdence interval of 95%. Simulation results are reported in Fig. 12 for four different architectures: DSSS, DSSS with ZF using 3 taps and 11 taps, and DSSS with TDS. The considered techniques are compared with the single channel simulated BER of OOK with multi-path using α = 60%, and without multi-path channel (OOK Theory in Fig. 12). The results indicate that the average BER of the DSSS technique, without channel compensation, stays high even with an SNR per bit of 27 dB. This poor BER performance is due to the impact of the CIR on the code orthogonality, and consequently on the misdetection rate. Nevertheless, associating DSSS with TDS or ZF improves performance compared with the technique without compensation. Indeed, part of the channel degradation was corrected, thus decreasing the mis-detection rate. Furthermore, comparing ZF ﬁlter using 3 taps with the other architectures on an average BER of e.g. 10−6 , shows that TDS and ZF with 11 taps improve the SNR by 5 and 8 dB, respectively. However, despite ZF with 11 taps provides best performance, its complexity is much higher compared to the other architectures. Figure 12: Demodulation performance BER scaling according to SNR for the considered techniques with α = 60%. B. Synthesis Results and Discussion The different blocks of the digital transceiver as depicted in Fig. 7 have been synthesized from C/C++ using HLS and logic synthesis. Digital interfaces with the router are designed considering a 32-bit bus width, which is serialized at 1.25 Gbit/s by the 32-bit SER/DESER blocks described in Section V-B. The DSSS Encoder block encodes each data bit in a parallel stream of 8 bits, before being serialized at 10 Gbit/s by the 8-bit SER block. Four different architectures (DSSS, DSSS with 3-tap ZF, DSSS with 11-tap ZF, and DSSS with TDS) are synthesized separately. Each architecture conﬁguration is capable to retrieve data from N − 1 clusters among N . Synthesis results of each architecture with N = 4 and other blocks of the wireless interface are summarized in Table I. Codec in the table means both the encoder and the considered decoder architecture conﬁguration. The Optimized TDS Codec corresponds to the scheme proposed in Section V-B3. The total power consumption of each block reports both the static and dynamic power consumed. However, static power represents less than 1% of the total power, mainly thanks to the dynamic power, a Pseudo Random Bit Sequence of 4 × 104 28-nm FDSOI low leakage technology. To estimate an accurate bits is injected into the encoder block, which generates a total of 3.2 × 105 bits for the decoder block. The ADC is a key block in OOK transceivers based on DSSS and ﬁlter compensation (e.g. zero-forcing) since it consumes the bulk of area and power of the WI. Nevertheless, as performance simulations in literature only consider an ideal wireless channel, no ADC is used for digital channel compensation. The parallel architecture of time-interleaved SAR ADC, mentioned in Section V-B, allows to decrease the frequency for digital ﬁlters and therefore to improve power consumption. Based on [29], Table I also provides area and power for the ADC block. For reference, ﬁgures extrapolated from [33] of an OOK transceiver are also reported. However, Section III already showed the poor performance of this transceiver in case of multi-path CIR. Table I: Synthesis results (area and power consumption) of the different Wireless Interface architectures using 28-nm FDSOI. WI Block DSSS Codec TDS Codec Optimized TDS Codec 3-tap ZF with DSSS Codec 11-tap ZF with DSSS Codec 8-bit Serializer (10 Gbps) 32-bit Serializer (1.25 Gbps) 32-bit Deserializer (1.25 Gbps) 4-bit ADC [29] OOK Transceiver [33] Area (µm2 ) 313.18 401.63 317.42 490.41 1967.37 21.8 49 50 9000 not speciﬁed Power (mW ) 0.43 0.82 0.63 0.98 4.04 0.1741 0.04391 0.044 16 20.8 The DSSS codec shows lower area and power consumption overhead than the other techniques. However, as shown in Fig. 12, its average BER is lower than for the other techniques with channel effect compensation. On the other hand, the ZF with 11 taps shows the best performance in terms of average BER, at the cost of highest power and area overhead. Finally, the average BER performance of the Time Diversity Scheme (TDS) based on the RAKE receiver is situated between ZF with 11 taps and 3 taps. In addition, the area and power consumption overhead of the optimized TDS codec is better than any k -tap ZF codec. Furthermore, as shown in Fig. 2, decreasing α , the amplitude of the second path of the CIR, improves BER performance. Therefore, any technique applied with α less than 60% will enhance BER too. However, the area and power consumption of each architecture will be approximately the same (overhead may depend upon the coefﬁcient values), but the TDS performance will become closer to the 11-tap ZF. Under these conditions, the TDS technique offers the best trade-off between BER performance and area/power over the other techniques. V I I . CONCLU S ION S Emerging WiNoC communication technology is considered as a very viable solution for facing scalability and energy issues in multicore architectures. However, communication reliability through a realistic wireless channel cannot be ensured by adopting conventional transceiver architectures. Therefore, in this paper, we show the impact of a semi-realistic multipath wireless channel over conventional WiNoC modulation scheme. Our study demonstrates the signiﬁcant unreliability of an OOK transceiver, even for a channel with low dispersion. To overcome this issue, we propose to combine the DSSS coding scheme with channel compensation blocks. Among the solutions studied, the TDS receiver represents the best trade-off between bit-error rate performance and area/power overhead, when compared with different Zero-Forcing equalizers. The area (and power) overhead of the TDS block for a four-cluster receiver is only 317µm2 (and 0.63mW ), which represents less than 1% (2%) of the wireless interface. Therefore, the gain in performance of our proposed scheme is signiﬁcant at the cost of a very low area/power overhead. The TDS, unlike an error-correcting code, offers more throughput as the number of users sharing the wireless channel increases. We believe that the introduction of the proposed solution opens interesting scenarios to combat channel effects in WiNoC. "
Channel Characterization for Chip-scale Wireless Communications within Computing Packages.,"Wireless Network-on-Chip (WNoC) appears as a promising alternative to conventional interconnect fabrics for chip-scale communications. WNoC takes advantage of an overlaid network composed by a set of millimeter-wave antennas to reduce latency and increase throughput in the communication between cores. Similarly, wireless inter-chip communication has been also proposed to improve the information transfer between processors, memory, and accelerators in multi-chip settings. However, the wireless channel remains largely unknown in both scenarios, especially in the presence of realistic chip packages. This work addresses the issue by accurately modeling flip-chip packages and investigating the propagation both its interior and its surroundings. Through parametric studies, package configurations that minimize path loss are obtained and the trade-offs observed when applying such optimizations are discussed. Single-chip and multi-chip architectures are compared in terms of the path loss exponent, confirming that the amount of bulk silicon found in the pathway between transmitter and receiver is the main determinant of losses.","Channel Characterization for Chip-scale Wireless Communications within Computing Packages Special Session Paper Xavier Timoneda Albert Cabellos-Aparicio Dionysios Manessis Universitat Polit `ecnica de Catalunya Universitat Polit `ecnica de Catalunya Fraunhofer Institute for Reliability and Microintegration Barcelona, Spain xavier.timoneda@upc.edu Barcelona, Spain acabello@ac.upc.edu Berlin, Germany Dionysios.Manessis@izm.fraunhofer.de Universitat Polit `ecnica de Catalunya  Universitat Polit `ecnica de Catalunya  Sergi Abadal Barcelona, Spain abadal@ac.upc.edu Eduard Alarc ´on Barcelona, Spain eduard.alarcon@upc.edu Abstract—Wireless Network-on-Chip (WNoC) appears as a promising alternative to conventional interconnect fabrics for chip-scale communications. WNoC takes advantage of an overlaid network composed by a set of millimeter-wave antennas to reduce latency and increase throughput in the communication between cores. Similarly, wireless inter-chip communication has been also proposed to improve the information transfer between processors, memory, and accelerators in multi-chip settings. However, the wireless channel remains largely unknown in both scenarios, especially in the presence of realistic chip packages. This work addresses the issue by accurately modeling ﬂip-chip packages and investigating the propagation both its interior and its surroundings. Through parametric studies, package conﬁgurations that minimize path loss are obtained and the trade-offs observed when applying such optimizations are discussed. Single-chip and multi-chip architectures are compared in terms of the path loss exponent, conﬁrming that the amount of bulk silicon found in the pathway between transmitter and receiver is the main determinant of losses. Index Terms—Wireless Network-on-Chip; Flip-chip package; Channel Characterization; mm-Wave Propagation. I . IN TRODUC T ION Recent years have witnessed a rising interest towards heterogeneous multi-chip architectures and the so-called 2.5D integration. The reasons are various, but mostly have their origin in the diminishing returns of transistor scaling and the cost of manufacturing large chips. In this context, heterogeneous multi-chip architectures allow to increase performance of multicore processors beyond the limits of a large monolithic chip [1]; reduce their manufacturing cost by disintegrating a large monolithic chip into a network of smaller ones, but with much The authors gratefully acknowledge support from the Spanish MINECO under grants PCIN-2015-012 and under contract TEC2017-90034-C2-1-R (ALLIANCE project) that receives funding from FEDER, from the EU’s H2020 FET-OPEN program under grant No. 736876 (VISORSURF), and by the Catalan Institution for Research and Advanced Studies (ICREA). better yield [2]; and provide versatility or even modularity in response to the increasing appeal of co-integrating diverse components such as CPUs, GPUs, memories, or accelerators within a single package [3]. The new integration trends have strong consequences on the design of the communications backbone within the package. On the one hand, with the recent introduction of silicon interposers in 2.5D processes, there has been a reduction of the performance and cost difference between on-chip and offchip communication [4]. Also, interposers may be capable of hosting off-chip routers in the future [2]. On the other hand, increased system heterogeneity implies higher versatility requirements as the actual communication needs will depend on the actual constituents of the architecture. It is expected that within-package networks will exploit the interposer advantages and rely on a tighter integration of the on-chip and off-chip sub-systems [2]. However, as off-chip transfers keep being expensive, it remains unknown whether such approach alone will sufﬁce to meet the requirements of this scenario. As a result, emerging interconnect technologies are being explored as well [5], [6]. Among them, wireless chip-scale communications show great promise due to its inherent lack of path infrastructure. This allows to overcome pin limitations and contribute to versatility by providing lowlatency broadcast capabilities across the package [7]. Research in highly-integrated wireless communications has exploded in the last decade [8]–[17]. The heterogeneous integration tendency has also impacted on this ﬁeld, leading to several works of that explicitly consider wireless communications across chips for CPU-GPU coordination [18], integrated memory access [19], [20], or in a more generic intra-/interchip framework [21]. A missing piece in the wireless chip-scale puzzle is, however, the characterization of the wireless channel. The theory is well laid out [22] and a wide variety of works exist at on-chip 978-1-5386-4893-3/18/$31.00 ©2018 IEEE (a) 3D integration (b) Silicon interposer (c) Multi-chip module Fig. 1. Different heterogeneous integration techniques in examples with CPUs, GPUs, and memory. In a wireless approach, selected components would be equipped with one or several integrated antennas for wireless communication within the package. [23]–[25], off-chip [26]–[28] and PCB board levels [29]–[31]. However, as detailed in Section V, very few studies include the chip package in their simulations or measurements and, those that do it, are limited to low frequencies or lack proper justiﬁcations on the antenna type and placement [32]–[34]. Note that, without proper understanding of the wireless channel within package, the path loss and dispersion assumptions may be overly optimistic. This affects the transceiver design and leads to inaccurate performance and efﬁciency reports. As most architectural studies rely in such ﬁgures, the impact of the wireless chip-scale paradigm cannot be really assessed. This paper aims to address this issue by providing a characterization of wireless channels compatible with heterogeneous 2.5D packaging. We rigorously model the package in a variety of single-chip and multi-chip conﬁgurations and discuss the antenna placement. By means of full-EM simulation, we extract the ﬁeld distribution and coupling between antennas and then derive path loss models. Through parametric studies, we obtain optimal package dimensions for path loss minimization and also analyze the impact of the die-package transitions in multi-chip conﬁgurations. Although the methodology is applicable to any antenna type and frequency range, we particularize it for the promising case of Through-Silicon Vias (TSVs) used as monopoles in the range of 50–150 GHz. The remainder of the paper is as follows. Section II presents the system model, including details of the chip package and main assumptions. Section III describes the simulation methodology and subsequent channel modeling, whereas Section IV presents the main results. Finally, Section V analyzes related works and Section VI concludes the paper. I I . SY ST EM MODE L This work considers a variety of multi-chip conﬁgurations for the channel characterization, summarized in Figure 1. All cases assume ﬂip-chip integration. Although heat dissipation schemes are generally applied on a per-chip basis, here we propose the addition of a single heat spreader common to all chips and a heat sink on top. Next, we provide more details. A. Multi-chip Integration The (heterogeneous) integration of multiple chips currently takes place either vertically or horizontally. The former, represented in Figure 1(a), consists on the stacking of several chips that have been previously thinned down below 100 µm [35]. Once stacked, the chips are interconnected through a forest of vertical TSVs with very ﬁne pitch. This provides a huge bandwidth density and efﬁciency due to the very short link lengths. On the downside, 3D integration suffers from evident heat dissipation issues and the available area of integration basically depends on the dimensions of the chip at the base, i.e. around 20×20 mm2 . Contrary to 3D stacking, heterogeneous 2.5D integration takes a co-planar approach and interconnects chips either through a common platform [4]. Depending on the level of integration, this common platform may be silicon interposer (Fig. 1(b)) or the package substrate in a more classical Multi-Chip Module (MCM) approach (Fig. 1(c)). Such an arrangement alleviates the heat dissipation issue of 3D stacking and also increases the available area, as the limit is now set by the interposer (24×36 mm2 in [2], 40×40 mm2 in [4]) or the substrate (77×77 mm2 in [1]). It also reduces the cost of the interconnects, as the pitch of TSVs is signiﬁcantly coarsened. The main downturn of the approach is the reduction of bandwidth density and efﬁciency due to pin limitations and the need for longer links. As for heat dissipation, it is worth noting that heat dissipation schemes are generally applied to each chip individually and then covered by a common lid. Instead, in his work we propose the addition of a single heat spreader common to all chips of a multi-chip conﬁguration, and then a single heat sink on top. This would enhance heat dissipation further and favor inter-chip propagation through a common layer, reducing losses due to reﬂections at the chip-package interfaces. Molding compounds are sometimes used to ﬁll the gaps between chips and below the heat spreader [36]. However, due to its poor thermal behavior, we advocate to the direct interfacing of the chip with the heat spreader. The lateral space between chips is assumed to be ﬁlled with air or vacuum. This paper explicitly considers wireless communication in 2.5D environments. To this end, we model the interposer and MCM cases and comparing them with single-chip architectures. Therefore, the 3D stacking case is also indirectly represented: a single-chip architecture with thin silicon can be seen as a 3D stack as long as the antenna is placed on the top Fig. 2. Schematic of the layers of a ﬂip-chip package (a) Improvement over 0.7-mm Si (b) Improvement over no AIN CHARAC TER I S T IC S O F TH E LAY ER S IN A COM PU T ING PACKAG E TABLE I Heat sink Heat spreader Silicon die Interconnections Bumps Interposer Ceramic carrier Thickness 0.5 mm 0.8 mm 0.2 mm 13 µm 87.5 µm 0.1 mm 0.5 mm Material Aluminum Thermal cond. Bulk Silicon Cu and SiO2 Cu and Sn Bulk Silicon Alumina εr 8.6 11.9 3.9 11.9 9.4 tan(δ ) 3·10-4 0.2517 0.03 0.2517 4·10-4 layer, just before the heat spreader. B. Flip-chip Package Although a recent work suggests a packageless architecture [37], dies have historically included a package to (i) act as a space transformer for I/O pins, provide mechanical support to the dies, and (iii) for ease of testability and repairability. Some packages include a molding compound around the chip to improve mechanical stability [36], but its typically poor thermal conductivity discourages its use in hot architectures. In most cases, even the packageless one [37], the die is contacted by a thermal interface material with a metallic heat sink on top, avoiding the use of the molding compound. This work considers a ﬂip-chip package with solder bumps. The packaging procedure is summarized here; we refer the reader to [38] and references therein for more details. During the manufacturing process, the solder bumps are deposited on the chip pads, which already carry a valid under bump metallization (UBM) like nickel/gold (Ni/Au). Then, the chip is ﬂipped over and its solder bumps are aligned precisely to the pads of the package carrier external circuit. This is in contrast to wire bonding of chips on the package substrate or the interposer, in which the chip is mounted upright and wires are used to interconnect the chip pads to external circuitry [33]. Flip chip is generally preferred over wire bonding due to (i) its much lower inductance given by the shorter interconnect length [39], (ii) lower power–ground inductance due to direct routing of power, and (iii) higher power density given by the use of the whole chip surface. An instance of the resulting complete package is shown in Figure 2. The layers are described from top to bottom as summarized in Table I. On top, the heat sink and heat spreader dissipate the heat out of the silicon chip, as they both have good thermal conductivity. Bulk silicon serves as the foundation of the transistors. This layer has low resistivity (10 Ω·cm), which is convenient for the operation of transistors, but not for electromagnetic propagation [40]. The interconnect layers, which occupy the bottom of the silicon die as shown Fig. 3. Adjusting the Silicon and heat spreader thickness result in huge improvements in the path loss. in the inset of Fig. 2, are generally made of copper and surrounded by an insulator such as silicon dioxide (SiO2 ) [41]. Depending on the case, we ﬁnd a silicon interposer or a package substrate below the micro-bumps. C. Package Optimization for EM Propagation In our previous work, we discussed the impact of the different materials of a chip package on electromagnetic propagation. As pointed out above, the chip substrate and the heat spreader are the main determinants of the path loss and, by modifying their thicknesses, we can optimize propagation. The bulk silicon used in the chip substrate generally has low resistivity, which means a high loss tangent, and therefore we proposed to thin it to minimize propagation at this layer. To quantify the gains of this process, in [38] we studied the path loss for different silicon thickness values in a single-chip package. We took 100 µm as lower limit, frequently assumed in 3D stacking, although chip makers can reportedly reduce that further to tens of microns [42]. As we can see in Fig. 3(a), the path loss difference between the 0.1-mm and 0.7mm cases is over 40 dB. Henceforth, we take 200 µm as the value by default. The materials used as heat spreaders have good thermal properties and, coincidentally, low electrical losses [40]. To study their potential impact on electromagnetic propagation, in [38] we simulated a chip package with different heat spreader thicknesses –our choice was Aluminum Nitride (AIN). As observed in Fig. 3(b), thickening the heat spreader reduces losses up to 33 dB with respect to not having any heat spreader. Therefore, it is a parameter to consider in package engineering efforts. Henceforth, we consider a 800 µm AIN layer as the heat spreader by default. D. Antenna Integration The antenna placement within a ﬂip-chip package is another important design consideration. Placing the radiating element as far from the lossy silicon as possible, as proposed in several works [23], [43], [44], is not realistic because the antenna would be short-circuited by the array of micro-bumps. Instead, printed dipoles [33] or patch antennas [45] may be implemented in the metal layers closest to the silicon. However, the proximity of the antennas to the virtual ground plane formed by the array of micro-bumps reduces their efﬁciency, whereas co-planarity between antennas further increases losses. of S is used as a benchmark to evaluate the worst case for several material thickness combinations. It is expressed as: (1) Smin = min Sij . i,j (cid:54)=i With the S-parameters, the channel frequency response Hij (f ) can be then evaluated for each antenna pair as GiGj |Hij (f )|2 = |Sj i (f )|2 (1 − |Sii (f )|2 ) · (1 − |Sj j (f )|2 ) , (2) Fig. 4. Schematic representation of an on-chip monopole, expected radiation pattern, and return loss for instantiations optimized at 50–140 GHz. Finally, one could use TSVs as quarter-wave monopole antennas for several reasons: (i) the antenna would radiate laterally, directly towards the receiving antennas; (ii) advanced TSV and electroplating techniques [46] would allow ﬁnetuning the antenna to the desired frequency; and (iii) the array of micro-bumps would naturally act as a ground plane, allowing to see the quarter-wave monopole effectively as a dipole. Vertical on-chip monopoles have been proposed recently [47], but using non-standard fabrication and packaging. Given the promising performance of monopoles in the chipscale environment, we will consider them throughout this work. Figure 4 shows a sketch together with the expected radiation pattern within the package. I I I . M ETHODO LOGY The canonical structure of Fig. 2 is introduced in CST MWS [48] with the parameter values from Table I. We then modify the structure to model the different scenarios depicted in Fig. 1 and to perform package optimizations. To reduce the computational burden, we perform several approximations that do not affect the accuracy of the results. For instance, given their ﬁne-grained pitched at mm-Wave frequencies, the micro-bump array placed between the chip and the package substrate/interposer is modeled as solid metallic element [38]. The monopole antenna is modeled as a thin and long cylindrical metallic structure, placed vertically passing through the silicon. Through optimization-driven simulations, the length of the monopole is adjusted to minimize the return loss at the central frequency of interest. By default, simulations are by default centered at 60 GHz with 20 GHz bandwidth. However, explorations at higher frequencies are also performed after the respective monopole length adjustments. Figure 4 shows the return loss of the different monopole instances. Simulations consider a number of antennas evenly distributed across the chips. The outcomes are the ﬁeld distribution, the antenna gain, and the coupling between antennas. Let Sij be the average of the coupling between transmitter j and receiver i over the whole frequency band. The minimum where Gi and Gj are the transmitter and receiver antenna gains, Sj i is the coupling between transmitter i and receiver j , whereas Sii and Sj j are the reﬂection coefﬁcients at both ends [49]. Once evaluated, a path loss analysis can be performed by ﬁtting the attenuation over distance to LdB = 10n · log10 (d) + C, (3) where d is the distance between antennas and n is the path loss exponent [23]. The path loss exponent is around 2 in free space, below 2 in guided or enclosed structures, and above 2 in lossy environments. IV. S IMU LAT ION R E SU LT S In the following, we show the results of an extensive simulation study that explores the channel characteristics in single-chip and multi-chip settings. We perform package optimizations to minimize path loss and assess the impact of having a multiple chips on the optimal design point. Additionally, we explore the scaling with frequency. Unless noted, the dimensions are those shown in Table I. A. On-chip wireless channel in single-chip package We start by exploring the channel within a single-chip package. This models conventional processors, but also serves for 3D stacking if we assume that antennas are placed at the top chip –the one interfaced by the heat spreader. For this scenario, we consider a single square chip 22-mm long and wide, surrounded by conducting walls representing the package boundaries. The conducting walls are placed 5 mm away of the chip limits. The space between the chip and the walls is modeled as vacuum, although it could be ﬁlled with molding compound as discussed in Section II. Package Optimization at 60 GHz. As indicated in Section II-C, it is preferable to keep the silicon thickness to a minimum and to increase that of the heat spreader. When introducing the antenna, results may oscillate and an antenna-package co-design may be required for optimization. Fig. 5(a) shows the results of such co-design, which keeping the monopole matched at 60 GHz at all times. It is found that the optimal silicon and AIN thicknesses are 0.10 mm and 0.85 mm, respectively, as they yield the highest mean of the worst case coupling. This shows that ﬁne-grained optimization can provide extra 5–10 dB of path loss reduction. Path loss at 60 GHz. To further highlight the importance of package optimization, we performed a path loss analysis at 60 GHz. We considered three different cases: default dimensions as speciﬁed in Table I, optimal dimensions as obtained in Fig. (a) 60 GHz (b) 100 GHz Improvement of worst-case coupling Smin in the single-chip case. The baseline dimensions are speciﬁed in Table I. Fig. 5. Fig. 6. Path loss as a function of distance, including linear regression ﬁtting, for the single-chip case at 60 GHz. 5(a), and a quite suboptimal design point. Remind that the path loss decouples the antenna effects and leaves just losses due to propagation. The results, plotted in Figure 6, shows how optimization reduces not only the path loss overall, but also the path loss exponent. For the default case, the path loss exponent is 1.78, slightly lower than the free space path loss, thanks to having a conﬁned environment. In the optimal case, we are able to cut the exponent down to 0.75, thereby showing a strong waveguiding effect in propagation. The suboptimal case, with an exponent higher than 2, demonstrates that the losses introduced by silicon cannot be neglected. Frequency sweep. Increasing the frequency of operation leads to smaller antennas and, potentially, smaller transceivers. Also, the absolute bandwidth is generally improved. Therefore, it is of great interest to study the scaling trends of the onchip channel. Figure 7 shows how Smin scales over frequency. This sweep was performed with the default silicon and heat spreader widths, 0.2 mm and 0.8 mm, respectively. We can observe that in overall, the loss between links increases with frequency probably due to two reasons: the antennas have smaller apertures, and the propagation losses at the dielectrics are larger. Nevertheless, this effect is compensated in part by the enclosed nature of the on-chip scenario, mitigating the impacts of frequency scaling. Impact of frequency on package optimization. Since our methodology performs a joint antenna-package optimization, it is reasonable to think that the optimal point will change with frequency. To illustrate this, we performed the exploration with the monopole tuned at 100 GHz. Figure 5(b) shows how the optimal point has slightly changed, but the tendency of higher losses with a thicker silicon is indeed, increased. This Fig. 7. Worst-case coupling Smin as a function of frequency in the singlechip case. can be explained by the fact that losses on the silicon are frequency sensitive. Still, the improvement with respect to the default case is 10 dB and can be achieved even with 0.15mm of silicon. B. On-chip wireless channel in multi-chip package Let us now consider a single chip isolated in a package without lateral walls. This would a priori model the worst case for on-chip propagation in a multi-chip package, where lateral walls are far away and neighboring chips absorb most of the incoming energy. Indeed, the absence of reﬂecting elements nearby is expected to lead to a reduction of the energy that returns to the chip after leaving, therefore increasing the path loss. To evaluate this scenario, we consider a 22×22 mm2 chip whose boundaries model a perfect matching layer, which models an inﬁnitely wide package without walls. Impact of boundaries on loss between links. Without lateral walls, the loss of the worst-case link is increased up to 27 dB higher for the thicknesses of Table I (from -43 dB to -70 dB). This is due to the harsh reduction of the power received by the antenna at the opposite corner of the radiating one, as the majority of the power to this antenna was coming from the now non-existent reﬂections at the package walls. This results strongly suggests that, being either due to losses or security reasons (so that wireless signals cannot escape outside the package), having package walls is beneﬁcial. Impact of boundaries on package optimization. As we can see on Figures 8(a) and 8(b), the variation of Smin with respect to non-optimized case is large and more sensitive to the silicon thickness. Due to the absence of walls, package optimization plays an even more important role than with walls (Fig. 5). The improvement is larger than 20 dB in several cases. (a) 60 GHz (b) 100 GHz Improvement of worst-case coupling Smin in the multi-chip case (without lateral walls). The baseline dimensions are speciﬁed in Table I. Fig. 8. Note that the exploration also unveiled very detrimental Smin dips at certain thickness combinations. See, for instance, the 0.1-mm silicon and 0.7-mm AIN case at 60 GHz in Fig. 8(a). antennas increases. This conﬁrms that the amount of silicon that waves need to traverse is the main determinant of losses. V. R E LATED WORK C. Off-chip wireless channel in multi-chip package We ﬁnally consider a full multi-chip packages as represented in Figures 1(b) and 1(c). In the interposer case, we evaluate an array of 2×2 small chips, 10 mm in length each, placed inside a 33×33 mm2 package with a separation of 5 mm between chips. The silicon interposer has 0.1 mm of thickness and 33 mm of length and width. In the MCM case, we simulate 2×2 chips of standard size, placed inside a bigger package whose length and width is 59 mm. We place four antennas per chip, regardless of the chip size, in order to evaluate all the possible combinations, i.e., close or distant antennas in close or distant chips. All the multichip package simulations are performed with enclosing conducting walls and a common heat spreader for all chips. Small chips vs single standard chip. The silicon interposer case allows us to evaluate the impact of processor disintegration [2] on the wireless channel characteristics. To this end, we compare the coupling between antennas in a single large chip, Fig. 9(a), and multiple small chips, Fig. 9(b). The plots do not illustrate large changes overall –a slightly better coupling is observed in the interposer case (only a few dBs in most antenna pairs, including Smin ). There are two effects that seem to be canceling out: on the one hand, propagation occurring in the vacuum space between chips instead of in lossy silicon would lead to better coupling in the interposer case. On the other hand, reﬂections due to media changes (silicon–vacuum– silicon) are also higher in the interposer case, which may be leading to lower coupling in far away antennas. This may also explain the better coupling at nearby antennas (port 2 and 5). Path loss: interposer vs MCM. The path loss exponent is calculated for the two multi-chip scenarios and compared with that of 3D stacking, previously shown in Fig. 6. In all cases, we considered the default thicknesses speciﬁed in Table I. The path loss exponent of the interposer scenario, plotted in Figure 10(a), is 1.55. This is slightly lower than the exponent of the single-chip case (1.78), thanks to the lack of silicon between chips. The path loss exponent increases up to 4.27 for MCM case, plotted in Fig. 10(b), due to the crescent losses due to propagation through lossy silicon as the distance between Channels within a Chip Package: the existence of a wireless channel in ﬂip-chip packages was suggested in [32] and then experimentally validated at 15 GHz in [33]. Recently, the work by Narde et al. analyzes the S-parameters for one and two chips within the same package assuming planar zigzag antennas at 60 GHz [34]. Although becoming part of the radiative structure by capacitive coupling, the effect of bumps on the antenna response is not discussed. They also assume high-resistivity silicon, which may be unlikely in processor dies. Our previous works, instead, discuss different 60-GHz antenna types and perform ﬁeld distribution and path loss analysis within a single-chip package with bulk silicon [38], [50]. The present work extends the frequency range up to 150 GHz and considers multiple single-chip and multi-chip conﬁgurations, both MCM and interposer-based. Channels within a Metallic Casing: Matolak et al. suggested that the wireless chip-scale communications would act as a micro-reverberation chamber with metallic walls [22], although they did not discuss the package. Others have explored a similar scenario over a large PCB board, including DRAM modules and other components within a computer case up to 300 GHz [26], [29]–[31]. In [51], the authors explored waveguide-like millimeter-wave channels within a reconﬁgurable metamaterial that integrates multiple chips on a PCB. Finally, the 60-GHz channel has been also studied in larger enclosed environments such as printers [52] or data center cabinets [53], which also act as reverberation chambers. Although these structures may capture the enclosed nature of a chip package, they have substantially different dimensions, materials, and antenna placement restrictions. Chip-scale Channels without Package: studies of the onchip and off-chip wireless channels have been often conducted without considering any particular package, assuming free space over the insulator layer. Yan et al. provided a theoretical basis at millimeter-wave frequencies [54], whereas others provided simulation-based studies [27], [55], [56] or actual measurements using planar antennas [23], [57] and bondwire antennas [8]. In the terahertz band analysis of [25], the package structure is described, but then neglected for (a) Single standard chip on package substrate. (b) Four small chips on silicon interposer. Fig. 9. Mean of the S-parameter for each antenna pair assuming transmission from port 1 at 60 GHz. Smin is highlighted. (a) Four small chips on silicon interposer. (b) Four standard chips on MCM package. Fig. 10. Path loss as a function of distance, including linear regression ﬁtting, for the multi-chip cases at 60 GHz. simplicity. Finally, monopoles placed in a loosely deﬁned superstrate have been also studied in recent works [24], [47]. All these works, however, do not provide a faithful view of a chip package and cannot be re-used for the scenario at hand. V I . CONCLU S ION The characterization of the wireless channel in scenarios compatible with standard chip packages is largely missing in the literature. To start bridging this gap, here we have performed a frequency-domain analysis of mm-wave propagation in the 3D stacking, silicon interposer, and multi-chip module schemes. We highlight the importance of package optimization to ensure the feasibility of the WNoC approach, as it is capable of reducing path losses by several tens of dBs. We also conclude that such optimization is dependent on the frequency of operation and the elements surrounding the chips. Finally, a path loss analysis conﬁrms that propagation length within silicon is the main determinant of losses, and that ﬁnding the right package dimensions ensure the scalability of the approach. "
Testing WiNoC-Enabled Multicore Chips with BIST for Wireless Interconnects.,The following topics are dealt with: network-on-chip; multiprocessing systems; network routing; system-on-chip; integrated circuit interconnections; microprocessor chips; telecommunication traffic; wireless channels; cache storage; and telecommunication network routing.,"Testing WiNoC-Enabled Multicore Chips with BIST  for Wireless Interconnects Abhishek Vashist   PhD Program in Engineering   Rochester Institute of Technology   Rochester, USA  av8911@rit.edu  Amlan Ganguly   Computer Engineering   Rochester Institute of Technology   Rochester, USA  axgeec@rit.edu  Mark Indovina   Electrical and Microelectronic Engineering  Rochester Institute of Technology   Rochester, USA  maieee@rit.edu  Abstract—Complex multicore Systems-on-Chips  (SoCs)  require large testing time and consume high amounts of power  during post manufacturing test. With the advent of Network-onChip (NoC) based interconnections, the NoC has been envisioned  to be reused as the Test Access Mechanism (TAM) to reduce  hardware overheads for testing. Therefore, testing the TAM is  critical as it provides the interconnection backbone for such chips  and detecting a fault in the TAM can flag a faulty SoC early in the  testing process. However, multi-hop data transfer over traditional  NoCs can result in slow test times especially, for large multicore  SoCs. In recent years, conventional NoC fabrics augmented with  wireless transceivers to form Wireless NoCs (WiNoCs) have  improved message latency and energy consumption in on-chip  data transfer. Therefore, in this paper we propose to augment  NoC-based TAMs with wireless interfaces to form a WiNoC based  TAM (WiTAM) providing single-hop test delivery paths in the  NoC. As the wireless transceivers themselves can be faulty, we  propose a novel fault model and Built-In-Self-Test (BIST) based  test methodology for the wireless interconnects. We present a  model to estimate the test time and evaluate the testing time and  power of the WiTAM. We demonstrate that the WiTAM can  reduce the test time as well as the energy-delay product of testing  of the WiNoC. Moreover, we show that test time for testing cores  of a multicore SoC with WiTAM can also be reduced compared to  a wired NoC.  Keywords—NoC; Wireless Interconnect; BIST  I.  INTRODUCTION  The probability of manufacturing defects in Integrated  Circuits (ICs) increase with aggressive scaling of the technology  nodes due to shrinking geometries and higher process variation  [1]. This makes post-manufacturing  testing of complex  multicore System-on-Chips (SoCs) a challenging and very  important step. To reduce the time-to-market the design  paradigm for complex SoCs and multicore chips have adopted a  scalable interconnection backbone called Network-on-Chip  (NoC)  [2]. The NoC  is a packet switched network  interconnecting the cores in a SoC with on-chip switches and  links. As all the cores are already connected through the NoC,  the idea of reusing the NoC infrastructure is proposed to be used  as the Test Access Mechanism (TAM) for multicore SoCs to  eliminate additional hardware overhead for test delivery [3].  However, this implies that the NoC based TAM needs to be  tested before using it as the interconnect fabric. This also  This work was supported in part by the US National Science Foundation  (NSF) CAREER grant CNS-1553264.  reduces the testing time by enabling early detection of a faulty  NoC in the test process.  The data transfer over a conventional NoC requires multihop communication over the metal interconnects. This can cause  slow propagation of the test vectors in metal wireline based  traditional architectures increasing the overall test time, testing  energy consumption and hence, overall test cost. Research in  recent years has demonstrated  that on-chip wireless  interconnects are capable of establishing radio communications  between cores in multicore chips. Wireless data communication  links with multi GigaHertz bandwidths in millimeter-wave  (mm-wave) bands are fabricated and demonstrated [4]. Using  such on-chip antennas embedded in the chip, Wireless Networkon-Chip (WiNoC) architectures [5] are shown to improve  energy efficiency and latency of on-chip data communication in  multicore chips [6].  In this paper, we propose to use the WiNoC interconnection  architecture as a TAM for multicore chips. Reusing the same  Wireless Interface (WI) that are used for data communication as  TAM can reduce the test time significantly since the WIs can  provide single-hop communication paths between distant nodes  on the chip. Moreover, it has been noted in many earlier works  that the zig-zag mm-wave wireless antennas are not directional  and hence can be used for message broadcast over the shared  wireless channel [6]. This property gives an additional  advantage as wireless interconnects can provide a broadcastcapable medium to distribute the test vectors efficiently. This  will further reduce the overall testing time. However, the mmwave wireless transceivers enabling the WiNoC are high-speed  analog components and themselves need to be tested before they  can be used as a part of the TAM. Testing analog components is  a complex and time-consuming task, especially in mm-wave  frequencies. Therefore, in this work we propose a fault model  and a Built-In-Self-Test (BIST) based testing methodology for  the wireless interconnects. To the best of our knowledge this is  the first time a testing methodology for on-chip wireless  interconnects in a WiNoC environment is proposed.  The contributions of this paper are the testing methodologies  for multicore chips using a WiNoC based TAM (WiTAM) with  specific fault model and BIST procedure for the wireless  interconnections. In addition to estimating test data delivery  time over the WiTAM, we estimate its power consumption and  energy-delay product (EDP). We also demonstrate that the  978-1-5386-4893-3/18/$31.00 ©2018 IEEE   WiTAM reduces the test time and EDP of testing multicore  chips compared to conventional wireline NoC based TAMs.   II. RELATED WORK  As NoC paradigm emerged as the SoC interconnection  backbone, various researchers proposed the idea of reusing the  NoC interconnection as a TAM [3]. In [7] the authors assumed  NoC as fault free and used it for delivering the test vectors to the  functional cores. However, assuming the interconnection fabric  as fault free is not a realistic assumption. For testing the wired  inter-switch link a BIST approach is presented in [8], where a  high-level fault model is used that targets the crosstalk effects  between the links. In [9] authors presented a unicast and  multicast based approach for testing of NoC components, where,  in multicast mode, the test packets have multiple destinations  and improved test time compared to unicast messaging. In [10]  authors use the regularity in the NoC switches to send test  packets. Test vectors are sent simultaneously to all identical  ports, and for all identical routers. In [11] authors presented an  RF based wireless test network to test SoCs. In that work authors  only presented wireless network to transmit control signals.  However, in that work actual test data was not delivered using  the wireless interconnects, thereby limiting the potential  benefits. Authors in [12] proposes test scheduling based on [11].  In this work, we design the WiTAM and evaluate the reduction  in test time by using the WiNoC as a TAM for delivering test  data for both the WiTAM itself and the functional cores.  III. ADOPTED WITAM FABRIC  Several WiNoC architectures have been proposed over the  past few years [6]. While any of those architectures can be  adopted to design the WiTAM in principle, in this section, we  describe one variant that is demonstrated to provide low-power  and high-bandwidth on-chip data transfer compared to wireline  counterparts and is briefly discussed here.   A. Topology of WiNoC-based TAM  The WiNoC topology is a wireline NoC overlaid with  wireless interconnects. We adopt a mesh architecture for the  wired NoC topology as it is relatively easy to design, verify and  manufacture due to uniformity of link lengths. However, any  other NoC topology can be chosen if required by the system  design constraints. In addition to these wired links a few NoC  switches are equipped with an additional port connected to the  WI to give it access to the mm-wave channel.   Based on several previous works [6] we divide the mesh into  multiple subnets to deploy the WIs among the NoC switches. A  central switch in each subnet is then equipped with a WI to  facilitate test data delivery using the wireless medium. The  mechanism for test data routing based on this topology is  discussed in Section IIIC. The selection of subnet size (or the  number of WIs) offers a trade-off between test data delivery time  and WI testing overhead which we analyse in Section VIIA.  B. Wireless Interconnect Design  We propose the use of on-chip embedded miniature antennas  operating in the 60 GHz mm-wave band unlicensed by Federal  Communications Commission (FCC), which can establish direct  communication channels between the WIs. Several on-chip  antenna designs in the mm-wave bands have been investigated  [4]. We intend the chosen antenna to be compact as well as nondirectional so that they can radiate in all directions. This is  because we want to leverage the benefits of broadcast  capabilities for fast transfer of test vectors. Using prototypes  fabricated using CMOS processes, a metal mm-wave zigzag  antenna has been demonstrated to possess these characteristics  as they are more compact compared to a linear dipole due to zigzag folding of the arms [4]. Such mm-wave 60GHz antennas are  shown to have a bandwidth of 16GHz for intra-chip [5]  communications links through typical dielectric packaging  materials. We have adopted the design of mm-wave zig-zag onchip antennas from [13].  To ensure high throughput and energy efficiency, we adopt  the transceiver design from [14][15] where low power design  considerations are taken into account. Non-coherent On-Off  Keying (OOK) modulation is chosen, as it allows relatively  simple and low-power circuit implementation without the need  for power-hungry carrier  recovery and high-frequency  synchronization circuitry. The  transceiver  in  [14][15]  demonstrate a physical bandwidth of 16Gbps at 2.06pJ per bit  with a signal to noise ratio (SNR) providing a bit-error rate  (BER) of less than 10-12 while occupying an area of 0.12mm2.  Each WI is a combined transceiver with a single antenna  enabling half-duplex communication. Parallel data from a NoC  switch is serialized using a Parallel In Serial Out (PISO) register  before transmission and deserialized by a Serial In Parallel Out  (SIPO) register after reception. Due to the chosen nondirectional antennas tuned to the 60GHz channel and the simple  OOK modulation technique, the wireless channel is a shared  medium, requiring a contention free Medium Access Control  (MAC) mechanism. To avoid non-scalable central arbitrations  and power-hungry synchronization across the chip we adopt a  distributed wireless token passing mechanism to grant access of  the shared wireless channel to the WIs. To enable autonomous  token passing among the WIs with fairness in accessing the  wireless medium, the WIs are numbered sequentially in a virtual  token ring. The token circulates autonomously between the WIs  as a wireless flit in a round robin fashion. When a WI receives a  token, it transmits data over the shared wireless medium and  then releases the token to the next WI. Each WI can only occupy  the token for a pre-determined maximum time that can be  optimized based on application traffic. Only the WI possessing  the token can transmit using the wireless medium thus avoiding  collision and interference with other WIs.  C. Test Data Routing in WiNoC-based TAM  We adopt Virtual Channel (VC) based wormhole switching  protocol for routing data in both test and functional modes where  packets are broken into smaller flow control units or flits which  can be routed between NoC switches in a single cycle [16]. To  accommodate the broadcast of test data we choose a tree based  routing policy for the WiTAM. If the routing paths for the  possible destinations diverge at an  intermediate switch,  duplicate packets containing test data are generated and routed  towards the divergent branches of the routing tree. The list of  destinations is also split between the packets carrying the test  data  to prevent subsequent repeated splitting for same  destinations. The routing tree is constructed using Dijkstra’s  algorithm which extracts a Minimum Spanning Tree (MST)  providing the shortest path between any pair of nodes in a graph.  Fig. 1. Proposed BIST architecture for tesing wireles interconnects. Color coding shows how the test flit is created and received by the LFSRs. BIST  components shown in bold.   Consequently, deadlock is avoided by a combination of  avoiding repeated duplication of divergent destinations and by  transferring flits along the shortest path routing tree, as it is  inherently free of cyclic dependencies. A forwarding-table  based routing over pre-computed shortest paths determined by  Dijkstra’s algorithm is adopted to minimize the test time. Only  the header flits of the messages packets are routed using the  forwarding table to the next hop reducing both computational  complexity and eliminating the need for maintenance of global  routing information. The same routing protocol is used for  routing data packets in functional mode as well [6]. Due to the  same routing hardware and port structure, the WiNoC switches  have similar architectures. Next, we discuss the proposed fault  models for the WiNoC followed by the testing methodology  using the WiTAM.  IV. PROPOSED FAULT MODEL OF THE WINOC  As described in the previous subsection the WiNoC fabric  typically consist of wireless  interconnects  realized by  transmitters and receivers. These transmitters and receivers  operate based on a MAC protocol for reliable contention-free  data transfer over the on-chip wireless channel. Besides the  wireless transceivers, there are NoC switches with routing logic,  arbitration, ports and wired interconnects as the WiNoC is a  hybrid system. Fault models for the wires links and switches are  well developed over the last several years and so, here we will  focus on the fault models of the on-chip wireless interconnects  only [8]. The wireless interconnects have both data signal and  control signals. The data signals are flits that are transferred  between the transmitters and receivers whereas the control  signals are the MAC signals which determine which transmitter  has access over the channel to send the data. We propose fault  models for both and develop testing methods for both types of  faults. Here, we focus only on faults that result from physical  manufacturing defects.  Faults of the data signals can be caused by failures at the  transceiver such as out-of-tune antennas. Process variations and  manufacturing faults can distort the shape of metallic structures  in an IC [17]. The antenna elements being fabricated as metallic  structures in the CMOS process can suffer from such distortions.  This in turn results in loss of tuning of the antenna element to  the specific carrier frequency.  Such loss of tuning essentially  makes the antenna unable to communicate using the wireless  channel resulting in permanently broken wireless links. A  permanent failure of an antenna will disable its WI. The  oscillators, modulators, drive amplifiers of the transceivers and  the LNAs and demodulators of the receiver at RF and mm-wave  frequencies consist of inductors responsible for impedance  matching and signal coupling. As these inductors are also  fabricated out of metal spirals in the IC they are susceptible to  the same fault scenarios. All these factors can cause transceiver  failure resulting in permanent failure of a transceiver to either  transmit or receive data over the wireless channel. Although the  transceivers are analog circuits we model these faults as data  stuck-at faults. In particular in an OOK transceiver scenario, if a  transmitter is unable to send data it results in a stuck-at-0 fault  for the transmitter. If the switching transistor in the modulator in  always stuck-short, then the transmitter will always send out the  carrier signal causing a stuck-at-1 fault. An analogous scenario  may arise in the receiver when it fails to receive signal resulting  in a stuck-at-zero data fault. Therefore, data faults can be stuckat-0 or stuck-at-1 between the transceivers. If a transmitter or  receiver has a stuck-at fault, multiple wireless links connected  to that transmitter or receiver will also have the same stuck-at  fault. This is analogous to a fan-out or fan-in scenario in digital  testing. Therefore, we model the analog transceiver circuit  failures as digital data stuck-at faults at the WIs. This enables us  to simplify the testing infrastructure and save testing time as  discussed in Section VI. Moreover, it must be noted that each  wireless link is a serial interconnect. Therefore, a stuck-at fault  at a transceiver means all bits transmitted or received at that WI    will be stuck-at. However, the serializer/deserializer buffers are  also parts of the WIs and those maybe faulty as well. In case of  serializer/deserializer buffer faults, only bits corresponding to  the faulty buffer locations will have stuck-at faults.    The MAC enables a particular transmitter to send data over  the wireless medium. We model the MAC control of the  transmitters as analogous to a tristate buffer circuit. Therefore,  we model possible faults of the MAC circuit as either stuckenabled or stuck-disabled. This implies that because of  manufacturing defects, the MAC circuit loses the controllability  and manifests the defect by either always enabling or disabling  a particular transmitter. In the proposed testing methodology, a  single stuck-enable and a single stuck-disable fault is assumed.  V. WINOC-BASED  TESTING METHODOLOGY  The WiNoC is capable of providing low latency and energyefficient in on-chip data transfer. This facilitates the idea of  reusing the WiNoC to deliver test data. However, the WiNoC  itself needs to be tested before using it as a TAM for on-chip  cores. This helps in early detection of manufacturing defects in  the WiNoC as the interconnection mechanism. The testing of  WiTAM is divided into two phases: 1) testing of WIs, and 2)  testing of switches and inter-switch links. Here, we discuss how  we propose to test the WIs and the adopted test strategy for the  switches and wired links.  A. Proposed WI Testing Method with BIST  WIs provide wireless communication capability to WiNoC  network by transmitting and receiving data packets over the  wireless channel. The WIs consist of serializer/deserializer  buffers as well as analog RF components such as the transmitters  (Tx), receivers (Rx) and the antennas. Moreover, the WIs in a  WiNoC operate in accordance with a token based MAC protocol  that enables the WIs to transmit data over the shared wireless  channel without contention between multiple transmitters.  Testing the wireless interconnection requires testing the physical  hardware of the WIs (data) as well as the token based MAC  (control).   In general mm-wave transceivers are tested using analog  BIST infrastructure [18] However, this would need digital  stimulus generators and DACs to convert the digital stimulus  into the analog test waveforms. The response from the  Components Under Test (CUTs) will need to be converted using  ADC and then compared. DACs and ADCs at mm-wave  frequencies of 20-60GHz are extremely power hungry and  occupy large real estate. The DAC in [19] occupies five times  the area of the mm-wave WiNoC transceivers reported in  [14][15]. As a WiNoC has many WIs on the same die to ensure  good performance, such analog BIST based methodology is  prohibitively expensive in terms of chip real estate in this  scenario. Therefore, we propose a digital BIST based  infrastructure that is capable of testing the WIs.  1) Data testing: The proposed digital BIST uses a  combination of random and pre-determined testing for random  testing of both data and control according to the fault model  described in Section IV. The BIST infrastructure consists of  Linear Feedback Shift Register (LFSR) based Test Pattern  Generator (TPG), called Data-LFSR at each transmitter and a  Multiple Input Shift Register (MISR) based Output Response  analyzer (ORA) to perform the signature analysis at each  receiver. The data generated by the Data-LFSR in a WI is  packetized with some pre-determined data padding. The  predetermined data bits consist of the flit type and source  address for a header flit. For body flits, if used for testing, the  pre-determined bits are used for the flit type only. The flit types  are pre-encoded for the header, body and tail flits and therefore  need to be predetermined. Similarly, the source address  depends on the WI-Under Test (WUT), which is chosen  randomly as discussed below. The destination address does not  matter as there is no routing block to be tested as a part of the  WIs and are therefore pseudo-random. This composition of a  test header flit is shown in Fig. 1 with color coding indicating  the specific LFSRs used to compose the test flits. In this way  the test packet sent from a WUT can be received by all the other  WIs simultaneously due to the shared physical layer and nondirectional antennas. Therefore, all  the wireless  links  originating at the WUT can be tested concurrently. The routing  logic is tested as outlined in Section VB.   2) Control testing: In addition to testing the physical links  (data-testing) the proposed testing infrastructure is capable of  testing a token based MAC (control-testing). To test the MAC,  one more LFSR, called MAC-LFSR, is used to grant wireless  channel access to the WIs in a pseudo-random sequence. In the  test mode, the sequence generated by the MAC-LFSR is first  passed into a decoder, which will convert the LFSR output  sequence into a one-hot encoded sequence. This one-hot  encoded sequence is then passed through a parallel load-shift  register into the scan flip-flops, which form a scan chain. This  scan chain will be used to set the token possession register, T at  the WiNoC switches. Therefore, the length of scan chain is  same as the number of WIs in the WiNoC. The scan chain,  setting up the test vectors for the token registers, is  interconnected between WIs with wired links which maybe  pipelined if the wire length between the WIs cannot be  traversed in one clock cycle. The clock of the serializer buffer  of the transmitter in the WIs are gated with the content of the  token register. Therefore, the MAC-LFSR sequence converted  into one-hot sequence will enable a single WI to transmit the  data generated by its Data-LFSR. All the receivers will receive  this data packet and pass it to their respective MISRs. The DataLFSRs in all the WIs are constructed using the same  characteristic polynomial. This would make  test data  transmission from any sequence of WIs indistinguishable from  another. However, the test packet has the source address of the  WI randomly chosen by the MAC-LFSR. Therefore, the MISR  captures not only, the pseudo-random test data from the DataLFSR but also that of the MAC-LFSR requiring the MISR  length to be same as that of the sum of the lengths of the Data  and MAC LFSRs. In this way if any WI is stuck-disabled then  that fault will be captured as that WI will not transmit when the  MAC-LFSR enables it and the BIST mechanism is able to test  both data and control concurrently.   To test for any WI being stuck-enabled, in addition to the  random testing by the MAC-LFSR, the WIs will be tested with  a special sequence, which is all zeros. This will disable all the  WIs and ideally no receiver should receive any data. However,  if any WI is stuck-enabled, the receivers will receive the packets  generated by the Data-LFSR of that WI. The BIST cycle for the  WI testing comprises of running the MAC-LFSR at the rate of  CLK_pkt for a desired number of sequences. The longer this  sequence, the higher will be the fault coverage and test quality.  It must be ensured that each WI is enabled at least once to ensure  full control fault coverage for a single stuck-enabled or stuckdisabled fault scenario. At the end of the test schedule, a  difference in the signature at any WI compared to a goldstandard signature is interpreted to denote a fault. If all the WIs  have matching signatures with that of a gold standard signature,  the whole wireless interconnection is determined to be faultfree. Otherwise, it is flagged to be faulty.  3) BIST controller: The Wireless BIST Test Controller  (WiBIST-TC) shown in Fig. 1 will set the values of control  signals according to the mode of operation. During testing, the  WiBIST-TC will set the Test_Mode signal to logic ‘1’ and this  will select the test data as input at the transmitter and will enable  capturing the test data output at the receiver. In the functional  mode, Test_Mode is disabled and is set to logic-0, this will  select data inputs and data outputs of the respective WiNoC  switches. The WiBIST-TC will also enable the Data-LFSR and  MAC-LFSR to generate the test data. These two LFSRs will  work on different clock frequencies. The Data-LFSR will  generate test flits at the target data rate with CLK_data. The  MAC-LFSR will generate the data at packet rate with CLK_pkt.  This ensures that in the test mode, the MAC-LFSR enables  different WIs at packet-level temporal granularity. In this way,  as the MAC-LFSR activates each WI that transmits a packet of  random data to all the other WIs resulting in testing both the  control and data signals simultaneously.   In this way, having completed the testing of the wireless  interconnects, the overall test controller will enter the switch  testing phase. During this phase of testing the WIs will be used  for delivering test data to the other electronic components of the  WiNoC and their testing will proceed as discussed next.  B. Switch Testing  Testing of switch consists of testing of routing block,  arbitration logic and FIFO buffers. First the FIFOs are tested and  then using the already tested FIFOs we deliver the test vectors  to the logic blocks of the switch. We have opted for distributed  BIST approach for the FIFO testing where each FIFO will have  a local response analyzer (LRA), test data generator and control  circuit [9]. WiNoC FIFOs have one write-only (wo) port, and  one read-only (ro) port due to the full duplex nature of WiNoC  wired link communication and are also referred to as (wo-ro) 2P  memories. So, we adopt the fault modeling based on [20] to test  the FIFO buffers. This implies there is no need to deliver test  data for the FIFO buffers from the ATE. For the switches  equipped with a WI, there is an additional port for the WI with  corresponding FIFO buffers. The FIFO buffers for the WI are  tested in the same way as the buffers in the other ports.  Test vectors for routing and arbitration logic is obtained by  scan insertion technique [9]. The test data for these logic blocks  are generated by an Automatic Test Equipment (ATE). The test  data from the ATE is first received at the Test Access Switch  (TAS) which can be any switch equipped with a WI. As all  switches in the network are identical, the same test data can be  reused by all the switches [3]. For same test vector, the output  response produced by all switches will be same. We use this  approach to compare test data response between adjacent  switches and generating a pass/fail signal which is sent back to  the ATE.  C. Inter-switch wired Link Testing  For testing inter-switch wired links we have opted for  Maximum Aggressor Fault (MAF) model that was proposed in  [21]. The MAF model captures the crosstalk effects between the  aggressor and victim lines. The MAF models six possible  crosstalk errors: rising/falling delay, positive/negative glitch,  and rising/falling speed-up. As the wireline architecture in this  work is a mesh, all wired links are of the same length and flitwidth, requiring the same tests. In our methodology, we adopt  BIST to test inter-switch links as proposed in [8]. There is a  single WI testing phase followed by multiple switch and link  testing phases, recursively radiating from previously tested  components to untested ones.   VI. TEST TIME ANALYSIS  In this section, we present analytical model for evaluating  the test time of the WiTAM.  A. Test Time for WiNoC Components  1) WI Test Time: For an individual WUT the test time,  TWUT, will include scan-chain latency to set up the token  registers, LScan-Chain, the test generation time of the Data-LFSR,  TData-LFSR, and the wireless link latency of the test data packet,  LWI. The scan chain latency is due to the fact that each new onehot test vector decoded from the MAC-LFSR needs to be  passed to the token registers of all the WIs before a testing cycle  for a WI can begin. As the vectors are one-hot the entire  previous vector needs to be flushed out increasing this latency.  The MISR response compaction time is masked by the test data  generation time except for the last flit. Receiving the last flit  may take multiple cycles of CLK_data if the LFSR/MISR  length is less than that of the flit width. Therefore, TWUT , in  cycles of the CLK_data, is given by,  (cid:1846)(cid:3024)(cid:3022)(cid:3021) = 	 (cid:1838)(cid:3020)(cid:3030)(cid:3028)(cid:3041)(cid:2879)(cid:3004)(cid:3035)(cid:3028)(cid:3036)(cid:3041) + (cid:1846)(cid:3005)(cid:3028)(cid:3047)(cid:3028)(cid:2879)(cid:3013)(cid:3007)(cid:3020)(cid:3019) + (cid:1845)(cid:3043)(cid:3038)(cid:3047) (cid:1838)(cid:3024)(cid:3010) + (cid:1865)          (1)  Where, m is the number of CLK_data cycles necessary to  generate a flit from the LFSR or conversely, the number of  cycles necessary to receive a flit at the MISR. All the timing  parameters in (1) are expressed in number of CLK_data cycles.  As the wireless links cannot be pipelined, its latency, LWI, is  incurred by every transmitted flit separately and is therefore  multiplied by the packet size, Spkt in (1). The total time required  to test all WIs in the network is given by,  (cid:1846)(cid:3024)(cid:3010) = ((cid:1840) + 1)(cid:1846)(cid:3024)(cid:3022)(cid:3021) .                                   (2)  This is because, the total test time consists of testing N stuckdisabled faults of WUTs while testing stuck-enabled fault for all  the WUTs requires testing with only one additional predetermined test vector (logic ‘0’s) for all the WUTs.  2) Switch Test Time: Testing the switch consists of testing  its routing and arbitration logic as well as the FIFO buffers. Let  WiNoC  Size  64 cores  256 cores  1024 cores  TABLE I. TEST TIME (CYCLES OF CLK_DATA) FOR DIFFERENT WINOCS WITH VARYING NUMBER OF WIS  Number of WIs  Wireline  3744  7488  14976  4 (cycles)  3834  7342  11704  8 (cycles)  3772  6580  10987  16 (cycles)  3352  4756  10372  32 (cycles)  2980  3916  6724  64 (cycles)  2704  3640  5044  128 (cycles)  -  3556  4492  256 (cycles)  -  3856  4792  512 (cycles)  -  -  5860  1024 (cycles)  -  -  8464  STV-Switch denote the number of test vectors required to test  routing and arbitration logic component. Then the time required  to test the routing and arbitration logic of a switch, TRouting,  Arbitration, can be calculated as  (cid:1846)(cid:3019)(cid:3042)(cid:3048)(cid:3047)(cid:3036)(cid:3041)(cid:3034) ,(cid:3002)(cid:3045)(cid:3029)(cid:3036)(cid:3047)(cid:3045)(cid:3028)(cid:3047)(cid:3036)(cid:3042)(cid:3041) = ((cid:1842)(cid:3020)(cid:3024) + (cid:1838)(cid:3024) ) + (cid:1845)(cid:3021)(cid:3023)(cid:2879)(cid:3020)(cid:3050)(cid:3036)(cid:3047)(cid:3030)(cid:3035) − 1.       (3)  Where, PSW is the number of pipelined stages in the switches  and LW is the wired link latency. Equation (3) captures the delay  of propagation of the test data over a wired link from the  previous switch through the switch under test across its  pipelined stages. As the switch is pipelined the subsequent test  vectors propagate through the switch under test at the rate of one  every cycle which is captured by the addition of the size of test  data (number of test vectors, each applied per cycle) less the first  vector. For testing FIFOs, we have adopted functional testing  [16], and the total test time required for testing the FIFO buffers  amounts to:  (cid:1846)(cid:3007)(cid:3010)(cid:3007)(cid:3016) = 6(cid:1866) + 8 + 3(cid:1830)(cid:1857)(cid:1864)                                (4)  Where, n represents the depth of the FIFO buffers and Del is  the time after which cell loses its data for data retention faults  [20]. Also it can be seen that the TFIFO is not affected by the size  of the WiNoC system, rather it is a constant overhead  irrespective of the specific test strategies adopted for testing the  combinational logic of the switch. Hence, the total test time for  a switch under test can be computed as:  (cid:1846)(cid:3020)(cid:3050)(cid:3036)(cid:3047)(cid:3030)(cid:3035) = (cid:1846)(cid:3019)(cid:3042)(cid:3048)(cid:3047)(cid:3036)(cid:3041)(cid:3034) ,(cid:3002)(cid:3045)(cid:3029)(cid:3036)(cid:3047)(cid:3045)(cid:3028)(cid:3047)(cid:3036)(cid:3042)(cid:3041) + (cid:1846)(cid:3007)(cid:3010)(cid:3007)(cid:3016)                 (5)  3) Inter-switch Link Test Time: The inter-switch links  between switches are tested according to MAF fault model. The  wired link test time can be computed as:  (cid:1846)(cid:3013)(cid:3036)(cid:3041)(cid:3038) = 	 (cid:1838)(cid:3024) + (cid:1845)(cid:3021)(cid:3023)(cid:2879)(cid:3013)(cid:3036)(cid:3041)(cid:3038)(cid:3046) − 1                                   (6)  Where, STV-Links is the number of test vectors for testing the  links. Each test vector is applied per cycle. Using (2), (5), and  (6) we can model the overall test time for a WiNoC system.  B. Test Time for the Whole WiNoC  Here we present the analytical model for computing the test  time for the whole WiNoC. The total test time for the entire  WiNoC, TWiNoC, will be the total time required for the WI testing  phase and that for the multiple recursive switch and wired link  testing phases along with the corresponding time for test data  delivery over the WiTAM. This is given by,   (cid:1846)(cid:3024)(cid:3036)(cid:3015)(cid:3042)(cid:3004) = (cid:1846)(cid:3024)(cid:3010) + (cid:1838)(cid:3024)(cid:3010) (cid:1845)(cid:3021)(cid:3023) + (cid:1834)(cid:3004) ((cid:1846)(cid:3020)(cid:3050)(cid:3036)(cid:3047)(cid:3030)(cid:3035) + (cid:1846)(cid:3013)(cid:3036)(cid:3041)(cid:3038) )            (7)  Where, HC is the hop count between the switch farthest from  the WI in a particular subnet as this switch will finish the testing  last. Each hop corresponds to the traversal of a switch and link  segment to reach the next switch. TSwitch, and TLink can be  computed using (5), (6). The first term in (7) denotes the time  required to test the WIs. The second term represents the time  required to broadcast all the test vectors required for switch  testing from TAS to other WIs. Finally, the third term is the test  time for the switch farthest from the WI in its subnet. The hop  count captures the recursive nature of the testing methodology  as the testing progresses hop-by-hop only after completion of  testing of switches and links in the previous hop. Therefore, the  hop count is multiplied with the testing time of a single switch  and link (one hop) in (7). Only the testing time of a single subnet  is factored into (7) as all other subnets are getting tested  concurrently as each WI propagates the test data in its own  subnet. This is the main advantage of the proposed WiTAM. In  the next section, we evaluate the proposed WiTAM.  VII. EVALUATION OF THE WITAM  In this section, we evaluate the proposed WiTAM. We have  assumed different sizes of the WiNoC vis-à-vis, 64 cores, 256  cores, and 1024 cores. The WiNoC switches are considered to  have three stage pipeline design and the size of the FIFO buffers  is set to four flits per VC, with four VCs per port. A flit size of  32 bits is considered in all the experiments. The switches for the  proposed framework are synthesized from a RTL level design  using 65nm standard cell libraries from Chip Multi Projects  (http://cmp.imag.fr), using Synopsys and post-synthesis delay  and power consumption is used to estimate the results in this  section. The digital components are synthesized with nominal  65nm parameters of 1V VDD and 2.5GHz system clock. The  latency of all wired mesh links is one cycle. The delay and power  consumption of the wired links are obtained from layout models  of the wires in Cadence Virtuoso using specific lengths for a  mesh topology of the various sizes considered in this paper  considering a 20mmx20mm die size respectively. The wireless  transceiver is adopted from [14][15] which demonstrate a  physical bandwidth of 16Gbps at 2.06pJ per bit with a signal to  noise ratio (SNR) providing a bit-error rate (BER) of less than  10-12 while occupying  an  area of 0.12mm2. The  serializer/deserializer buffers associated with each WI is  considered to be 32 bits long to accommodate an entire flit  before/after transmission.  A. Evaluation of Test Time  For comparison of the test time with our proposed WiTAM,  a mesh-based wireline NoC with same switch and buffer  configuration is considered. The test time is measured in terms  of total time required to delivery all test data to all the  components and is computed according to (7). As we are  targeting post-manufacturing testing, it is assumed that test  vectors for switches are already packetized and available at  ATE. Table I shows the total test time for a 64, 256, and 1024  core WiNoCs with varying number of WIs. It can be seen from  the table that compared to wireline NoC, there is decrease in  total test time for 64 core WiNoC with increase in number of  WIs except for 8 WIs for the 64-core system. This is because the  small number of WIs in the small size WiNoC are unable to  reduce the test time significantly by using the broadcast  communication over the wireless channel. Additionally, the  testing time overhead of the WIs themselves increase the overall  test time in this case. We have considered each WUT to send a  single flit packet every time it is tested by the MAC-LFSR.  However, as number of WIs for the 64 core WiNoC increases  beyond 8 WIs, the total test time decreases. This decrease in test  time is because the WiTAM employs the broadcast capability of  increasing number of WIs in the WiNoC fabric. The test time  overhead for the WIs also increases with increase in the WIs  because all the WIs need to be tested before the entire WiTAM  can be declared fault-free. However, total test time including the  testing time required for WI testing is lower than the completely  wireline mesh TAM when the WiTAM has more than 8 WIs.  In the 256 core WiTAM, the test time reduction with  wireless is more evident compared to 64 core WiNoC. This is  due to the fact that the WiTAM provides long range shortcuts to  reach more distant switches in the fabric. It can be seen from  Table I, the maximum reduction in test time of 52.5% is obtained  for 128 WIs. After increasing number of WIs further the test  time increases due to the extra testing overhead of WIs. This  behavior can also be seen in the 1024 core WiNoC where, after  increasing number of WIs beyond 128, the test time increases,  but still the overall test time is lower than that of the  corresponding wireline NoC. This is due to the higher relative  advantage provided by the wireless interconnection for a large  multicore SoC where test data in a mesh based wireline  communication requires several hops to reach from the TAS to  all destinations. The number of WIs after which the test time  starts increasing can be viewed as the optimal number of WIs  required to minimize the test time of our proposed testing  methodology. From our analysis, we find the optimal WIs to be  64, 128 and 128 for 64, 256 and 1024 core WiNoCs respectively.  B. Power and Energy-Delay Product of proposed WiTAM  In this subsection, we evaluate the average power consumed  during the test delivery process for both wireline NoC and  WiTAMs. While the WIs help is broadcasting test data and  therefore reducing test time, they also consume power and can  increase testing power. Depending on the phase of the test as  discussed in Section V the instantaneous power consumption  will vary as WIs, all links and switches are not engaged in testing  at the same time. Therefore, we present the average power  consumed by the TAM during the complete test delivery. The  average power is the power consumed by the TAM in one clock  cycle averaged over the entire duration of the test delivery  duration. It can be seen from the Fig. 2 that the average power  consumption normalized with respect to the wireline TAM,  increases for WiNoC testing with the number of WIs. Moreover,  e l a c S d e z i l a m r o N 3 2 1 0 EDP 64 cores EDP 256 cores Average Power 64 cores Average Power 256 cores Wireleine 8 16 Number of WIs 32 64 Fig. 2. Energy delay product for 64 and 256 core systems  TABLE II. CORE STATISTICS AND TEST DATA VOLUME Core Names  Core  Test Data Volume  Class (bits)  b19  1  269088  ethernet  2  414634  Vga_lcd  3  150375  Netcard*  4  33900  *Testing for only stuck-at faults and not transient faults  Test Data  Packets 1052  1620  588  133  with higher number of WIs the average testing power increases  as all the WIs are used in test data broadcast. However, as we  have seen in the previous subsection, higher number of WIs  reduces the overall test time. Therefore, we also evaluate the  Energy-Delay Product (EDP) for the test delivery as the product  of average power with squared test time. From Fig. 2 we can see  that the EDP decreases for WiNoCs compared to the wireline  NoC based TAM for both 64 and 256 core systems with increase  in number of WIs. This reduction in EDP for WiTAM is due to  reduction in the test time with increase in number of WIs.  C. Reduction in Core Test Time  To evaluate the reduction in test time for core test data using  the WiTAM, we have used two different configurations,  namely, interleaved and clustered. In both configurations, we  have considered the multicore SoC to consist of 4 different types  of cores. These cores and their test data requirements for both  stuck-at and transient faults are shown in Table II [22]. In the  systems considered for the evaluation 16 and 64 cores of each of  the 4 types are considered for the 64 and 256 core WiNoCs  respectively. In the interleaved configuration, the 4 types of  cores are considered to be evenly distributed over the multicore  SoC in groups of 4. This will require core test data for each type  of core to be distributed all over the chip. In the clustered case,  each of the four types of cores are clustered into four quadrants  and each quadrant has only one type of core. Fig. 3 shows the  reduction in test time for 64 and 256 core systems with WiNoC  TAM normalized to a wireline mesh TAM.  We have considered the optimal 64 and 128 WIs in the 64  and 256-core systems respectively as noted in Section VIIA. In  both interleaved and clustered configurations, the WiNoC TAM  reduces the test time compared to the wireline TAM. This is  because our proposed WiTAM is able to broadcast the test  packets to reach far away cores more quickly compared to  wireline NoC based TAM. Also, the comparative (normalized)  reduction in test time is more when the cores are clustered. This  is because when the cores are clustered, the farthest the test data  has to travel is the same for all core-types and which are the  cores in the corner of   the chips. In contrast, when the cores are  Clustered Interleaved 1 0.8 0.6 0.4 0.2 64 Cores 256 Cores Fig. 3. Core test time for clustered and interleaved configuration  e m i t t s e T d e z i l a m r o N 0         TABLE III. AREA OVERHEAD AND POWER OF BIST PER WI Metric  Area (µm2)  Power (mW)  With 8-bit  Data-LFSR  652.57  0.153  With 16-bit  Data-LFSR  1025.4  0.1674  With 32-bit  Data-LFSR  1507.96  0.1873  interleaved in groups of four, the farthest that the test data has to  travel is different for different core types, resulting in overall  lower test time using wireline mesh based TAM. The test time  with the WiTAM are however nearly equal in both cases, as the  WIs directly broadcast the test data relying only marginally for  wireline test data transfer within each small subnet. Therefore,  the normalized reduction with the WiTAM is more for the  clustered configuration.  D. Area overheads for the Wireless BIST  For a Data-LFSR of length 32 bits, the area overhead of the  BIST infrastructure per WI will be 0.0015 mm2 as seen from  Table III. Typical area of such mm-wave WIs are reported to be  0.12mm2, making the overhead 1.25%. Each WI occupies an  area of 0.12mm2 [14][15], while a WiNoC switch occupies  0.34mm2. As seen from Section VIIA the optimal number of WIs  is 64 for the minimum test time in a 64-core system. However,  from a performance optimization perspective, the number of  WIs in a WiNoC can be even be less than 10 depending on  system size, the adopted MAC protocol, and WI placement  strategy [23][24]. Therefore, testing time optimization needs to  be considered while optimizing the number of WIs in a WiNoC  for Design-for-Testability (DFT).  VIII. CONCLUSION  A novel approach of using wireless interconnects as a TAM  for testing the WiNoC fabric for large multicore chips have been  proposed. The novelty of our testing mechanism lies in  proposing a novel fault model and BIST mechanism for the  wireless interconnects in a WiNoC and the idea of reusing  wireless interconnects for test data delivery. The efficiency of  the test method was evaluated for WiNoCs of different sizes.  Optimal number of WIs required for maximum reduction in test  time for different system sizes is presented. The results show  reduction in test time from 27% up to 70%. Using the proposed  WiTAM we also compute the test time for testing cores, and a  maximum reduction of 57.5% is obtained for 256-core system.  "
Exploiting Dark Cores for Performance Optimization via Patterning for Many-core Chips in the Dark Silicon Era.,"All the cores of a many-core chip cannot be active at the same time, due to reasons like low CPU utilization in server systems and limited power budget in dark silicon era. These free cores (referred to as bubbles) can be placed near active cores for heat dissipation so that the active cores can run at a higher frequency level, boosting the performance of active cores and applications. In the literature, this approach is referred as static patterning. Patterning for performance boost has the following challenges. First, communication distance increases when a bubble is inserted between two communicating tasks, leading to performance degradation. Second, budgeting too many bubbles as cooler to running applications leads to insufficient cores for future applications. In addition, task-migration-based dynamic patterning can further improve the performance of the system. In this paper, a static and a dynamic patterning approaches are proposed to budget free cores to each application so as to optimize the throughput of the whole system. Essentially, the proposed static patterning algorithm determines the number and locations of bubbles to optimize the performance and waiting time of each application, followed by tasks of each application being mapped to a core region. The dynamic patterning algorithm first selects the best pattern, the bubble number and the core region shape for each application that results in maximal performance, followed by choosing the location for each application's core region. Experiments show that our approach achieves 50% higher throughput when compared to state-of-the-art thermal-aware runtime task mapping approaches.","Exploiting Dark Cores for Performance Optimization via Patterning for Many-core Chips in the Dark Silicon Era Special Session Paper Xiaohang Wang Amit Kumar Singh Shengyan Wen South China University of Technology, China Email: xiaohangwang@scut.edu.cn University of Essex, UK Email: a.k.singh@essex.ac.uk South China University of Technology, China Email:w.shengyan@mail.scut.edu.cn Abstract—All the cores of a many-core chip cannot be active at the same time, due to reasons like low CPU utilization in server systems and limited power budget in dark silicon era. These free cores (referred to as bubbles) can be placed near active cores for heat dissipation so that the active cores can run at a higher frequency level, boosting the performance of active cores and applications. In the literature, this approach is referred as static patterning. Patterning for performance boost has the following challenges. First, communication distance increases when a bubble is inserted between two communicating tasks, leading to performance degradation. Second, budgeting too many bubbles as cooler to running applications leads to insufﬁcient cores for future applications. In addition, task-migration-based dynamic patterning can further improve the performance of the system. In this paper, a static and a dynamic patterning approaches are proposed to budget free cores to each application so as to optimize the throughput of the whole system. Essentially, the proposed static patterning algorithm determines the number and locations of bubbles to optimize the performance and waiting time of each application, followed by tasks of each application being mapped to a core region. The dynamic patterning algorithm ﬁrst selects the best pattern, the bubble number and the core region shape for each application that results in maximal performance, followed by choosing the location for each application’s core region. Experiments show that our approach achieves 50% higher throughput when compared to state-of-the-art thermal-aware runtime task mapping approaches. I . IN TRODUC T ION Many-core chips have been the core infrastructure of cloud computing, big data services, etc. A challenge posed to high performance many-core chips is the so called “dark silicon” issue, whereas a portion of the chip has to be powered off to meet the power constraint. We have referred these free and powered-off cores as bubbles. According to the ITRS projection, a large portion of the cores have to be turned off to meet the thermal and power constraints [10]. The adverse impact of dark silicon is low utilization, that is, a portion of transistors cannot work. To improve system performance, a few work proposed the so called “dark silicon patterning”, where “bubbles” (inactive cores) are placed near active core for heat dissipation [11], [14]. An active core can run at a higher frequency level if bubbles are located near it for heat dissipation. This helps to achieve higher performance while meeting the temperature constraint. Fig. 1 (a) and (b) show the impact of dark silicon patterning on system performance. Results from [12] shows that, the system performance of the pattern with active cores interleaved with dark cores (Fig. 1(b), consuming 87.6W power) is higher than that of the square pattern in Fig. 1(a) (consuming 76.2W power), since the dark cores can be used for heat dissipation. This research program is supported by Natural Science Foundation of Guangdong Province 2018A030313166, the Science and Technology Research Grant of Guangdong Province No. 2017A050501003, Pearl River S&T Nova Program of Guangzhou No. 201806010038. 978-1-5386-4893-3/18/$31.00 ©2018 IEEE (a) (b) t0 t1 t0 t1 (c) (d) Fig. 1. (a) The square pattern and it’s temperature proﬁle. The system consumes 76.2W power. (b) The chessboard pattern and it’s temperature proﬁle. The system consumes 87.6W power and has higher performance. (c) The square shifting dynamic pattern. (d) The chessboard shifting dynamic pattern. However, in a server system with dynamic workloads, the following challenges need to be addressed with the above patterning approach. First, since the number of available/free cores changes with the arrival and departure of applications, the position of bubbles and voltage/frequency of active cores need to be adjusted at run-time under the temperature constraint in response to arrival and departure of applications. Most of the approaches (e.g., [11], [14]) consider static workloads only, i.e., a ﬁxed set of applications known in advance and ﬁxed number of bubbles, which does not reﬂect the dynamic feature of several systems, e.g., a server. Next, communication overhead between the active cores executing communicating tasks is largely affected by placing bubbles near them. Communication distance between two tasks increases if the corresponding two cores have bubbles (dark cores) inserted between them for heat dissipation. Therefore, although active cores can possibly run at a higher frequency level if bubbles are placed near them, the applications might suffer from increased communication overhead, resulting in poor performance. Existing approaches (e.g., [11], [14]) ignore such communication overhead. In addition, the patterning in Fig. 1 (a) and (b) is static, that is, the task-to-core mapping cannot change during the program execution. If dynamic task-to-core mapping (task migration) is performed, we see a completely different scenario. With dynamic patterning, the tasks are ﬁrst mapped by following a pattern, e.g. square, and then migrated from the active cores to dark ones at each control time interval. For example in Fig. 1(c) the tasks hop between the active cores and the dark cores periodically. Similarly, in Fig. 1(d) the tasks are migrated to another region of dark cores periodically. Our experiments (more details in Section VI) have shown that chessboard shifting patterning (Fig. 1(d)) leads to worse performance than the square shifting pattern (Fig. 1(c)), due to the reason that in the chessboard shifting pattern, the communication distance between tasks is larger. From Fig. 1(c) and (d), one can see that, with dynamic patterning, the communication cost of the application is kept the same during the migration. Once the active core region is overheated, the tasks are migrated to other cores and active cores are turned off. Therefore, dynamic patterning brings more options for performance optimization. Contribution: This paper addresses the aforementioned challenges and opportunity by proposing both static and dynamic patterning based resource management approaches. The patterning approaches try to determine the number and location of both free and active cores, so as to optimize performance, communication cost and waiting time. The main contributions of the approach are as follows: 1) We propose an online static patterning algorithm to select the number and locations of free cores for each application. Both computation and communication performances are optimized when determining the number and location of bubbles and active cores. 2) In addition, we propose a dynamic patterning algorithm based on task migration. This dynamic patterning algorithm aims to optimize both communication power (by keeping low distance of communicating tasks) and computation performance (by setting active cores to run at a high frequency and migrate them to cold cores in case of overheating). Each application is restricted to migrate within a core region, which can alleviate fragmentation (a situation where free cores are scattered and not forming a contiguous region). The rest of the paper is organized as follows. Section II reviews related work, followed by system model deﬁnition in Section III. Sections IV and V introduces the static and dynamic patterning, respectively. Section VI evaluates the performances of the patterning algorithms, and ﬁnally Section VII concludes the paper. I I . R E LAT ED WORK Allocating resources to the tasks of multiple applications on onchip many-core system has been an emerging research direction [23]. Several resource allocation approaches have been proposed while following different policies. Most of these approaches map communicating tasks of each application close to each other such that communication overhead and power are reduced [3], [5], [13], [20]. However, these approaches do not consider a power budget for the whole chip, which is desired in the dark silicon era. Thermalaware resource allocation approaches have been explored to reduce peak temperature and temperature gradient while directly considering the temperature of cores [4], [15]. To address dark silicon problem while considering multiple applications, recently, some static patterning approaches have been introduced [12], [14]. The basic idea is to place the inactive cores around active cores to boost the frequency and performance of active cores. One major disadvantage of these application-mapping-based static patterning is that, the communication latency might be increased due to the increased hop counts between active cores. Further, thermal hotspots created during execution cannot be avoided. Dynamic task migration can be used to avoid thermal hotspot at run time. Task migration has been used for the purpose of thermal management [2], [6], [8], [22]. Thermal-aware task migration schemes migrate tasks from overheated cores to cooler ones. According to the migration path, existing task migration schemes [21] can be categorized into 1) global coolest migration, where hot tasks are migrated to a globally coolest core, 2) random migration, where hot tasks are migrated to a randomly-picked cool core, and 3) neighbor swapping [16], where hot tasks are migrated to neighbor cores if the latter is cooler. Among the three migration strategies, the global coolest migration might lead to the highest communication distance. The neighbor swapping schemes minimize communication overhead, but might lead to irregular application core region. In such a case, the free core regions are scattered and might not form a contiguous region, leading to the fragmentation problem which will increase the communication latency of future applications. Therefore, existing migration either leads to increased communication latency or fragmentation to future applications. I I I . SY S T EM MOD E L AND PROB L EM D E FIN I T ION The system contains a many-core platform that executes a set of applications arriving at different moments of time. A. Many-core Platform Model The many-core platform contains of a set of cores connected by an interconnection network, which is modeled as a 2D mesh network with bidirectional links. Each core consists of a processing unit, a cache and a network interface. It is represented as a directed graph 𝐺(𝑇 , 𝐿), where 𝑇 is the set of cores and 𝐿 represents the connections amongst the cores. The application allocation and resource management is done by a centralized platform resource manager. B. Application Model Each application 𝑖 is represented as a directed graph 𝐴𝐺𝑖 = (𝐴𝑖 , 𝐸𝑖 ), where 𝐴𝑖 is the set of tasks of the application and 𝐸𝑖 is the set of directed edges representing dependencies amongst the tasks. Each task 𝑎 ∈ 𝐴𝑖 has a weight: execution time (ExecTime), when mapped onto a core. The ExecTime for each task is considered as its worst-case execution-time (WCET) and remains ﬁxed at a given frequency. Each edge 𝑒 ∈ 𝐸𝑖 represents data volume communicated between the dependent tasks. A mapping function 𝑀 (𝑎) = 𝑡, for 𝑎 ∈ 𝐴𝑖 , 𝑡 ∈ 𝑇 binds tasks to the cores, such that task 𝑎 is mapped to core 𝑡. Each edge 𝑒 ∈ 𝐸𝑖 has a weight of transmission time, when the two communicating tasks are mapped. The execution time of each application 𝑖 is the makespan of the application task graph, denoted as 𝐸𝑇𝑖 . set of bubbles 𝐵𝑖 = {𝑡1 , 𝑡2 , ...} are also associated with application The set of all existing free cores in the system is denoted as Γ. A 𝑖, where 𝑡1 , 𝑡2 , ... are powered off cores for cooling. C. Problem Statement Within a given time period, for 𝑁 applications arriving at different moments of time, the objective is to minimize the response time of each application in order to optimize throughput that is computed as the number of applications executed within a ﬁxed amount of time. For static patterning, the control knobs are the position and number of the bubbles to be allocated to each application, together with the task-to-core mapping of each application. For dynamic patterning, the control knobs are the number of bubbles, the dynamic task migration pattern, and the shape of each application’s core region. The response time of each application is computed as follows: 𝑖 − 𝐴arrive 𝜎𝑖 = 𝐴ﬁnish 𝑖 (1) where, 𝜎𝑖 is the response time of application 𝐴𝑖 , 𝐴arrive are the arrival time and the ﬁnishing time of application 𝐴𝑖 . For each application, its response time is related to both the execution time and the waiting time. Waiting occurs when an application arrives at the system but there are no sufﬁcient cores to run it. and 𝐴ﬁnish 𝑖 𝑖 Choose  the best  bn for  min  response  time Section IV.E WT Wa iti ng tim e 100 . . . 3000 ET Exe cuti on tim e 200 . . . 120 Wai ting time mode l Section IV .B bn 1 . . . Section IV.D Virtual  ma pping  mi n{|Ai|, Γ}  Section IV .C Performa nce model bn 1 . . . mi n{|Ai|, Γ}  MS bn 1 . . . mi n{|Ai|, Γ}  Mapping re sul ts . . . Mapping resul ts database Final (rea l) ma pping  results Fig. 2. Overview of the proposed approach. Execution time is related to both the communication and computation performances of the application. The response time of running 𝑁 applications within a given time is then computed as: 𝜎 = 𝐴𝑁 ﬁnish (2) where 𝑁 is the number of applications arrived at the system within a given time, and 𝐴ﬁnish 𝑁 represents ﬁnishing time of 𝑁 𝑡ℎ application within this given time. The objective is to min 𝜎 (3) The constraint is that, the temperature of the chip should be under a threshold. IV. S TAT IC PAT T ERN ING A. Overview Fig. 2 shows the overview of our proposed approach. Applications dynamically arrive in the system. The bubble count (number of bubbles) included in each application’s core region (the region including active cores and bubbles) is used as a control variable, which determines both the communication distance and the running frequency of the active cores such that the system thermal constraint is not violated. A virtual mapping process is ﬁrst called to estimate the performance of each application when using different number of bubbles. For each application, core regions with different numbers of bubbles are selected, such that the region’s core count is possibly larger than the number of tasks in the application. The tasks of the application are mapped virtually to this core region in order to estimate the performances given different bubble counts (𝑏𝑛 ) for the application, i.e., the table from the performance model achieved as shown in Fig. 2. The running frequency of each active core can be determined to conﬁne to thermal constraint. During virtual mapping, no task is running on the cores, i.e., the tasks are not actually mapped to the cores. The waiting time model also generates a table indicating the waiting time given different bubble counts. Finally, during the real or ﬁnal mapping, the bubble count for each application is chosen which can result in the minimum application execution time (including communication and computation performances) and waiting time. Once the application ﬁnishes execution, the cores in the region is released and send back to the available resource pool. The various steps of the proposed approach are introduced in subsequent sub-sections and highlighted in Fig. 2. B. Waiting Time Estimation model as in Eqn. (4), where ∣𝑇 ∣ is the network size, ∣𝐴𝑖 ∣ is the average The waiting time can thus be modeled by a polynomial regression number of tasks in each application, 𝛾 is the average percentage of bubble count in an application’s core region, deﬁned as bubble count divided by the core count in each application’s core region, ℎ is the average execution time of the tasks, and 𝜆 is the average application arrival rate. Using this model, 𝛾 can be a decision variable such that, when the waiting time is estimated to be high, a smaller 𝛾 is preferred. 𝑧∑ 𝜂𝑖 = 𝑧∑ 𝑐𝑖 ⋅ ∣𝑇 ∣𝑗 + 𝑑𝑗 ⋅ ∣𝐴𝑖 ∣𝑗 + 𝑒𝑗 ⋅ 𝛾 𝑗 + 𝑓𝑗 ⋅ ℎ𝑗 + 𝑧∑ 𝑧∑ 𝑗=1 𝑗=1 𝑗=1 𝑔𝑗 ⋅ 𝜆𝑗 + 𝑎0 𝑗=1 𝑧∑ 𝑗=1 (4) To ﬁnd the coefﬁcients of 𝑐, 𝑑, 𝑒, 𝑓 , 𝑔 ’s, the maximum likelihood methods can be used [9]. C. Performance Estimation To estimate the performance of each application, we need to know the communication performances of the edges and the computation performances of tasks in the task graph. These performances can only be determined after the tasks are mapped to cores. The number of bubbles in a core region is an important control variable which is related to both the communication distance and the computation power of each core/task. Given a virtual mapping of tasks to a core region with 𝑗 bubbles, the execution time of each task and transmission time of each communication edge can be determined as in Sections III-B. The execution time of each task is related to the instructions to be executed and the running frequency and power of the core while satisfying the thermal constraint using the thermal power capacity model in [25]. The performance of the application (referred to as makespan) can be determined by ﬁnding the maximum execution path along the application’s task graph. Therefore, the performance estimation needs the virtual mapping algorithms which will be introduced in Section IV-D. The output of the performance model as shown in Fig. 2 is a table ET where each item ET[j] is the execution time with 𝑗 bubbles. D. Virtual Mapping Algorithms whose core count equals to ∣𝐴𝑖 ∣ plus 𝑗 bubbles, where 𝑗 = During the mapping process, we virtually ﬁnd core regions 0, 1, 2, ..., min{∣𝐴𝑖 ∣, Γ}. At each iteration with 𝑗 bubbles, the applications are virtually mapped to the core region and the execution time is stored in the performance model table. Once the iteration stops, the performance model generates a table indicating the execution times with 𝑗 bubbles, where 𝑗 = 0, 1, 2, ..., min{∣𝐴𝑖 ∣, Γ}. The corresponding mapping schemes with up to 𝑗 bubbles are also stored in a database. Note that, this process only virtually maps the tasks to the cores to get the performance model table and the mapping scheme database as shown in Fig. 2. Tasks are not actually bound to and run on the cores. No migration is involved. Other running application is intact. The virtual mapping process has two objectives, i.e., minimizing the communication distance and maximizing the computation frequency/performance of the tasks. These two objectives might be contradicting in the sense that, communication distance is minimal when tasks are mapped in close proximity, while each task’s frequency or computation performance is maximized when the temperature is low indicating hot tasks are distant from each other. We propose a heuristic based virtual mapping algorithm, where the two optimization objectives are tried to be achieved simultaneously. 𝑗 bubbles, the tasks are mapped to a core region of size ∣𝐴𝑖 ∣ + 𝑗 . The Algorithm 1 shows the virtual mapping ﬂow. At each iteration with ALGORITHM 1: Online Virtual Mapping Input: 𝑗 : The bubble number. Output: ET[j]: The execution time when inserting 𝑗 bubbles. MS[j]: the best mapping scheme when inserting 𝑗 bubbles. Function: Find the best virtual mapping scheme and the execution where 0 ≤ 𝑗 ≤ min{∣𝐴𝑖 ∣, Γ}. time for an incoming application given the bubble number is 𝑗 , begin if 𝐶𝐶𝑅 < 𝑇 ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 then Call the Communication Biased Virtual Mapping Sub-routine; Call the Computation Biased Virtual Mapping Sub-routine; end else end end results are the two lookup tables ET and MS, where ET[j] returns the execution time when inserting 𝑗 bubbles and MS[j] returns the best virtual mapping scheme when inserting 𝑗 bubbles, respectively. Our proposed virtual mapping algorithm has the following steps. 1) Determine the computation to communication rate (CCR), which is deﬁned as the average computation workload (instructions to be executed) divided by the data volume to be sent in one application. 2) If CCR is over a threshold, call the computation biased virtual mapping sub-routine. Otherwise, call the communication biased virtual mapping sub-routine. A larger CCR indicates each task computation performance contributes more to the overall application performance, while a small CCR means communication has more contribution to the application performance. Based on the CCR value, two virtual mapping sub-routines are called which are computation and communication biased, respectively. Both of the two mappings have two steps as follows. An initial mapping is set up ﬁrst, followed by an iterative replacement procedure to optimize computation and communication performances. The inputs to both of the virtual mapping subroutines are 1) the task graph of the incoming application, 2) the available cores in the system, and 3) the bubble number 𝑗 , where 𝑗 = 0, 1..., min{∣𝐴𝑖 ∣, Γ}. 1) Communication Biased Virtual Mapping Sub-routine: Algorithm 2 shows the communication biased virtual mapping sub-routine. a) Initial Mapping: In the initial mapping, the objective is set to be minimal communication distance. A convex core region is ﬁrst found, followed by tasks with larger communication volume mapped in closer proximity virtually. The mapping algorithm in [7] is used as the initial mapping with minimal communication distance as the optimization objective. b) Inserting Bubbles: In each iteration, 𝑗 bubbles are virtually tation performance of certain tasks, where 𝑗 = 0, 1..., min{∣𝐴𝑖 ∣, Γ}. inserted into the core region of this application to boost the compuThe application’s core region is bounded by a convex hull. At each iteration with 𝑗 bubbles, ﬁrst, a location (𝑥1 , 𝑦1 ) inside the current convex hull is found, then a location (𝑥2 , 𝑦2 ) outside the convex hull is found that is adjacent to its boundary, and has the minimum distance to (𝑥1 , 𝑦1 ). The bubble is virtually moved from (𝑥2 , 𝑦2 ) to (𝑥1 , 𝑦1 ) using the path migration algorithm in [19]. As an example, Fig. 3 shows the process of inserting two bubbles iteratively. At each iteration, when a new bubble is to be inserted, each task is selected as the candidate to be replaced by the bubble. A bubble with the minimal distance to each task is virtually replaced with the task. Then, the maximum power/thermal budget and frequencies Task graph 2 1 4 3 Convex hull Bubbles Active Node Busy Node Nodes in  other regions 1 Adding one bubble 1 2 3 4 3 4 1 2 Initial mapping Adding two bubbles 1 2 3 4 1 2 3 4 Fig. 3. Adding bubbles virtually to get the expected maximum speedup. ALGORITHM 2: Communication Biased Virtual Mapping Sub-routine Output: ET[j]: The execution time when inserting 𝑗 bubbles. MS[j]: the best mapping scheme when inserting 𝑗 bubbles. Function: Find the best mapping scheme and the execution time for an incoming application given the bubble number is 𝑗 , where 0 ≤ 𝑗 ≤ min{∣𝐴𝑖 ∣, Γ}. begin */ /* Inital Mapping Map the tasks with communication-awareness by using [7] without bubble insertion; ET[j] = INFINITY; // Recording the best performance for 𝑗 = 0, ..., min{∣𝐴𝑖 ∣, Γ} do /* Inserting Bubbles for each active core 𝑡𝑘 inside the core region do 0, 1, ..., min{∣𝐴𝑖 ∣, Γ}, start with the hotest /* 𝑘 = location */ Find a bubble 𝑏 on the boundary of the core region returned by the mapping with the minimal distance to */ 𝑡𝑘 ; Virtually move 𝑏 to 𝑡𝑘 using [19]; Update the performance Ex; if Ex < ET[j] then ET[j] = Ex; Virtually migrate 𝑏 to 𝑡𝑘 using [19] and update MS[j]; end end end end of the cores running the tasks are updated following the thermal power capacity model. After determining the frequency of each core and the communication distance of each edge in task graph, the computation and communication performances are updated following the application model in Section III. The task replacement with the minimal execution time is recorded. For example, in Fig. 3, in the ﬁrst step to insert one bubble, suppose replacing task 1 with a bubble leads to the minimal execution time. So task 1 is moved to the location of the bubble. The region is enlarged each time a bubble is inserted. 2) Computation Biased Virtual Mapping Sub-routine: Algorithm 3 shows the computation biased virtual mapping sub-routine. a) Initial Mapping: If the task computation performance contributes more to the application performance, the initial mapping begins with a region of min{2× ∣𝐴𝑖 ∣, Γ} cores, where ∣𝐴𝑖 ∣ or Γ cores are powered off as bubbles. The tasks are sorted by their weight (each node’s worst case execution time in the task graph) in descending order. The tasks are mapped as distant as possible to each other. The ALGORITHM 3: Computation Biased Virtual Mapping Subroutine Local coolest core Output: ET[j]: The execution time when inserting 𝑗 bubbles. MS[j]: the best mapping scheme when inserting 𝑗 bubbles. Function: Find the best mapping scheme and the execution time for an incoming application given the bubble number is 𝑗 , where 1 ≤ 𝑗 ≤ ∣𝐴𝑖 ∣. begin /* Inital Mapping Find a core region with size of min{2 × ∣𝐴𝑖 ∣, Γ}; for each unmapped task 𝑎𝑘 do Virtually map 𝑎𝑘 to core 𝑡 such that 𝑡 has the maximum distance to other mapped tasks; */ end ET[j] = INFINITY; // Recording the best performance /* Removing Bubbles for 𝑗 = 1, ..., ∣𝐵𝑖 ∣ do for edge 𝑒𝑘 = (𝑎𝑚 , 𝑎𝑛 ) do */ Virtually move 𝑎𝑛 to 𝑡𝑘 , i.e., a core clostest to 𝑀 (𝑎𝑚 ) using [19]; Update the performance ET; if ET < ET[j] then ET[j] = ET; Virtually migrate 𝑎𝑛 to 𝑡𝑘 using [19] and Update MS[j]; end end end end Edge 1(cid:252)3, fix task 1 bubbles 3 4 1 2 3 1 2 4 Initial mapping Moving 3 toward 1 Edge 2(cid:252)4, fix task 4 3 1 2 4 1 3 2 4 Moving 2 toward 4 Fig. 4. Migrating bubbles virtually to optimize the communication distance. mapping can be done as follows. For each unmapped task 𝑎𝑖 in the ∑𝑖−1 sorted list, ﬁnd a core 𝑡 with maximal distance to the mapped tasks, i.e., 𝑘=1 𝐷(𝑀 (𝑎𝑘 ), 𝑡). 𝐷(𝑀 (𝑎𝑘 ), 𝑡) is the distance of the core to a previous virtually mapped task, where 𝐷(⋅, ⋅) denotes Manhattan distance between two cores. This equation ﬁnds the core that has the maximum distance to those running the virtually mapped tasks. b) Removing Bubbles: To get the performances with different bubble counts for each application, the bubbles are virtually migrated out from the initial mapping region one by one at each iteration. The communication edges in the task are sorted by their volume in descending order. For each edge 𝑒 = (𝑎𝑚 , 𝑎𝑛 ), 𝑎𝑛 is migrated to a free core virtually and the application performance is recalculated. If the performance is improved, 𝑎𝑛 is virtually migrated to that free core and the bubble is migrated to the original location of 𝑎𝑛 . Then, the bubble is excluded from the application. Fig. 4 shows two steps of virtually migrating task 3 towards task 1, and tasks 2 towards task 3, respectively. After virtually migrating one task to a bubble, the previous core running this task is excluded from the region of this application. Then, the computation and communication performances are updated following the application model at each iteration. 3) Complexity Analysis: The worst case complexity of the virtual mapping process can be analysed as follows. In the communication biased virtual mapping algorithm, the initial mapping step has a complexity of 𝑂(∣𝐴𝑖 ∣2 ⋅∣𝐸𝑖 ∣⋅∣𝑇 ∣) [7]. In the second step, the algorithm Applicat ion's core region Applicat ion's core region 1 8 7 2 6 3 4 5 (c) t0 t1 (a) t2 (b) Fig. 5. (a) The square shifting pattern. (b) The conﬁned coolest shifting pattern. The hot task can be migrated to any cool core inside this core region. (c) The conﬁned neighbor swapping pattern. The hot task can be swapped with a neighbor cool core. has to iterate up to ∣𝐴𝑖 ∣ times, corresponding to the bubble count. For each bubble count 𝑗 , it takes 𝑂(∣𝐴𝑖 ∣2 ) steps to virtually migrate the tasks. In the computation biased virtual mapping algorithm, the initial mapping step has a complexity of 𝑂(∣𝐴𝑖 ∣2 ⋅ ∣𝑇 ∣). In the second step, it also has to iterate up to ∣𝐴𝑖 ∣ times, corresponding to the bubble count. For each bubble count 𝑗 , it takes 𝑂(∣𝐸𝑖 ∣) steps to virtually migrate the tasks. Overall, the worse case complexity is 𝑂(max ∣𝐴𝑖 ∣2 ⋅ ∣𝐸𝑖 ∣ ⋅ ∣𝑇 ∣). E. Choosing the Best Number of Bubbles Given the waiting time and the performance models versus bubble count, we can determine the number and locations of bubbles for each incoming application such that the overall system performance is optimized. To achieve the same, the following two steps are number of bubbles ∣𝐵𝑖 ∣ for each application 𝑖 with the minimum performed. First, using the above two models, we can select the sum of execution time and waiting time, i.e., min{𝐸𝑇𝑖 + 𝜂𝑖 }, with 0 ≤ ∣𝐵𝑖 ∣ ≤ min{∣𝐴𝑖 ∣, ∣Γ∣}, where ∣Γ∣ is the total number of free cores. Second, with a bubble count of ∣𝐵𝑖 ∣, the mapping results can be retrieved from the database MS[∣𝐵𝑖 ∣] as shown in Fig. 2. V. DYNAM IC PAT T ERN ING When an application arrives at the system, the following four parameters, 1) a core region, 2) an initial mapping, 3) dark core number and 4) the migration pattern should be selected for it. During the application’s execution, it follows the migration pattern until it ﬁnishes execution. The dynamic patterning algorithm has two steps. For a newly arrived application, Step 1) ﬁnds a pattern for the application according to it’s characteristic, Step 2) ﬁnds bubble number, aspect ratio (the ratio of the width to the height) of the core region, followed by ﬁnding the location of the core region. A. Pattern Deﬁnition and Performance Model There are many patterns for adapting the task-to-core mapping. Therefore, we deﬁne a subset of dynamic patterns that has most inﬂuence on the performance and temperature, as shown in Fig. 5. P 1 square shifting (denoted as SS): migrating the task region as a whole. This pattern requires that the number of bubbles equals to the number of tasks. P 2 conﬁned local coolest shifting (denoted as LC): migrating each hot task to a coolest core within the core region. P 3 conﬁned neighbor swapping (denoted as CN): migrating each hot task to a coolest and nearest neighbor core with the core region. The three patterns serve applications of different characteristics. Square shifting is beneﬁcial for both computation- and communication-intensive applications when the system workload is light (with sufﬁcient dark cores). The advantage is that the relative location of the tasks are ﬁxed; therefore, the communication latency does not increase during the task migration and the application’s execution process. Conﬁned neighbor swapping is beneﬁcial for applications with high communication volume when the system workload is heavy (with insufﬁcient dark cores). The hot tasks are only swapped with neighbors to restrict the increase in communication latency. Conﬁned local coolest shifting is beneﬁcial for computationintensive applications when the system workload is heavy (with insufﬁcient dark cores). The hot tasks are migrated to the coolest core within the region. Therefore, they can run at a higher frequency. Denote the execution time of an application as Π𝑖 . Π𝑖 can be modeled by the dynamic pattern 𝑝𝑖 , the aspect ratio (the ratio of the width to the height of a core region) 𝑟𝑖 and bubble number 𝑏𝑖 for application 𝐴𝑖 , ⎧⎨ ⎩ 𝑛1∑ 𝑛3∑ 𝑘=1 𝑛5∑ 𝑘=1 𝑘=1 𝛼1 𝑘 𝑏𝑖 𝑘+ 𝛼2 𝑘 𝑏𝑖 𝑘+ 𝛼3 𝑘 𝑏𝑖 𝑘+ 𝑛2∑ 𝑛4∑ 𝑘=1 𝑛6∑ 𝑘=1 𝑘=1 Π𝑖 = 𝛽 1 𝑘 𝑟𝑖 𝑘 , 𝑝 = 𝑆𝑆 𝛽 2 𝑘 𝑟𝑖 𝑘 , 𝑝 = 𝐶𝑁 𝛽 3 𝑘 𝑟𝑖 𝑘 , 𝑝 = 𝐿𝐶 (5) 𝑘 -𝛽 3 𝑘 -𝛼3 where 𝛼1 𝑘 and 𝛽 1 𝑘 are coefﬁcients, 𝑛1 , 𝑛2 , 𝑛3 are orders of the polynomials. We run the simulation to get the performance with different 𝑝𝑖 ’s and 𝑏𝑖 ’s and use a polynomial regression model by using the maximal likelihood method [9]. Each application has an initial task-to-core mapping, performed by an existing mapping algorithm like the one in [7]. The training of the performance model is done ofﬂine for each application. B. Algorithm to Search for the Best Patterns 𝑘 For each application, a search-tree-based algorithm is used to determine the bubble number, the migration pattern, the aspect ratio of the core region, the starting point and orientation of the core region. For a new application 𝐴𝑖 , two levels of new tree nodes are generated. The level 1 tree node 𝜋 𝑖,1 is characterized by the tuple (𝑝𝑖 , 𝑏𝑖 , 𝑟𝑖 ). In total, there are 2(∣𝐴𝑖 ∣ + 1) ⋅ ∣𝐴𝑖 ∣ + 1 possible combinations of the (𝑝𝑖 , 𝑏𝑖 , 𝑟𝑖 ) values. That is, for LC and CN patterns, the ranges of 𝑏𝑖 and 𝑟𝑖 are [0, ..., ∣𝐴𝑖 ∣] and [1, ..., ∣𝐴𝑖 ∣], respectively. For the SS pattern, 𝑏𝑖 = ∣𝐴𝑖 ∣ and 𝑟𝑖 = 1. Each level 1 tree node is also associated with a cost function 𝑣(𝜋 𝑖,1 𝑘 ), which can be computed from the performance model as in Eqn. 5. Only the level 1 tree node with the minimal cost function value are kept, i.e., all the other level 1 tree nodes are discarded. For this level 1 tree node, a new level of tree nodes are generated (level 2) 𝜋 𝑖,2 𝑘 , which are characterized by the pairs of (𝑠𝑖 , 𝑜𝑖 ), where 𝑠𝑖 is the start point coordinate (we set as the left bottom node of the core region), and 𝑜𝑖 is orientation (whether or not to be rotated by 90 degree). Each level 2 tree node is also associated with a cost function 𝑣(𝜋 𝑖,2 waiting time model in Eqn. 4. In total, there are 2∣𝑇 ∣ combinations 𝑘 ), which is computed by the of the (𝑠𝑖 , 𝑜𝑖 ) values. That is, the range of 𝑠𝑖 is [0, ∣𝑇 ∣] and there are two possible orientations, i.e., no rotation (0𝑜 ) and rotated by 90𝑜 . Only the level 2 tree node with the minimal cost function is kept, with all the other level 2 tree nodes discarded. For next application 𝐴𝑖+1 , the branching is based on this level 2 tree node of 𝐴𝑖 . The algorithm is shown as in Algorithm 4. It works as follows. For each application 𝐴𝑖 , new level 1 tree nodes are branched. For each node 𝜋 𝑖,1 𝑘 , select and keep the node with the minimal cost function (execution time), i.e., 𝑣(𝜋 𝑖,1 𝑘 ). Based on this node, branch new level 2 tree nodes. For each node 𝜋 𝑖,2 𝑘 , select and keep the node that has the cost function (minimal waiting time), i.e., 𝑣(𝜋 𝑖,2 The iteration proceeds application by appilcation. After the leaf tree node is reached (all the applications ﬁnd their patterns), an existing mapping algorithm [7] can be used for the initial mapping. During the application execution, the tasks of the applications follow the dynamic migration patterns as speciﬁed. 𝑘 ). ALGORITHM 4: Finding the pattern, shape, and location of the core region for each application Input: 𝑆 : the set of unmapped applications; Output: 𝑀 𝐷𝑖 and 𝑁 𝑂𝐿𝑖 : MD and NOL values of each application 𝐴𝑖 ; 𝑊 𝑄: A working queue, initialized to be empty; 𝐵𝑁𝑖,1 : The best level 1 tree node for application 𝐴𝑖 ; 𝐵𝑁𝑖,2 : The best level 2 tree node for application 𝐴𝑖 ; 𝑣∗,𝑖,1 = ∞: The minimal execution time for application 𝐴𝑖 among 𝑣∗,𝑖,2 = ∞: The minimal waiting time for application 𝐴𝑖 among 𝐴𝑖 ’s level 1 tree nodes; 𝐴𝑖 ’s level 2 tree nodes; while 𝑊 𝑄 is not empty do pop the top node 𝑁𝑞 out of 𝑊 𝑄; if 𝑁𝑞 is not a leaf node then /* Find pattern for application 𝐴𝑖 branch new level 1 tree nodes; for each newly branched nodes 𝜋 𝑖,1 𝑘 do compute 𝑣(𝜋 𝑖,1 𝑘 ) then */ 𝑘 ); if 𝑣∗,𝑖,1 > 𝑣(𝜋 𝑖,1 𝑘 ); 𝑣∗,𝑖,1 = 𝑣(𝜋 𝑖,1 𝐵𝑁𝑖,1 = 𝜋 𝑖,1 𝑘 ; branch new level 2 tree nodes 𝜋 𝑖,2 based on 𝐵𝑁𝑖,1 ; for each newly branched nodes 𝜋 𝑖,2 𝑘 do 𝑘 ) then 𝑘 if 𝑣∗,𝑖,2 > 𝑣(𝜋 𝑖,2 𝑘 ); 𝑣∗,𝑖,2 = 𝑣(𝜋 𝑖,2 𝐵𝑁𝑖,2 = 𝜋 𝑖,2 push 𝜋 𝑖,2 𝑘 𝑘 ; in 𝑊 𝑄; end end end App 1: 8 ta sks App 2: 2 ta sks Different values  of <p1, b1, r1>’s root <p1,b1, r1> ... <LC,4, 4:3> active co re  reg ion App 1 App 2 Different values  of <s1, q1>’s <(0,1), 0 o> App 1 cor e  reg ion Different values  of <p2, b2, r2>’s <p2,b2, r2> ... <CN,1, 1:2> active  cor e  reg ion Different values  of <s2, q2>’s <s2, q2> ... <(4,1), 0 o> App 1 App 2  cor e  reg ion . . . Fig. 6. A search tree of the dynamic patterning algorithm. The grey tree nodes are kept. Fig. 6 shows an example of the search tree. Suppose Applications 1 and 2 has 8 and 2 tasks respectively. To determine the pattern for 𝐴1 , level 1 tree nodes are branched with different < 𝑝𝑖 , 𝑏𝑖 , 𝑟𝑖 > values. Suppose the < 𝐿𝐶, 4, 4 : 3 > node is selected, indicating a conﬁne local coolest pattern (LC), 4 dark cores, and the aspect ratio to be 4:3. Based on this node, level 2 tree nodes are branched with different < 𝑠𝑖 , 𝑞𝑖 > values. Suppose the node < (0, 1), 0𝑜 > is selected, indicating the core region’s starting point (left bottom corner) is at coordinate (0, 1), and no rotation. Now, to ﬁnd a pattern for application 2, new level 1 tree nodes are branched. Suppose the node < 𝐶𝑁 , 1, 1 : 2 > is selected, indicating a conﬁne neighbor swapping, 1 dark cores, and the aspect ratio to be 1:2. New level 2 tree nodes are branched based on it. Suppose the node < (4, 1), 0𝑜 > is selected, indicating the core region’s starting point (left bottom App3 App3 App3 App4 A p p 5 App4 A p p 5 App4 A p p 5 t3 App arrival rate t4 t5 High arrival rate App 3: confined loc al cooles t shifting App 4:confined  neighbor swapping App 5: confined  neighbor swapping  (no dark core) Low arrival rate Time  App2 App1 App2 App1 App2 App1 App 1: squa re  shifting App 2: squa re  shifting t0 t1 t2 Fig. 7. An example showing the choice of patterns according to application arrival rates. corner) is at coordinate (4, 1), and no rotation. arrival rate is low (at times 𝑡0 − 𝑡2 ), both applications 𝐴1 and 𝐴2 can Fig. 7 shows an example of choosing the best patterns. When the use the square shifting patterns, as there are sufﬁcient dark cores in the system. When the arrival rate becomes high (at times 𝑡3 − 𝑡5 ), the computation-intensive application𝐴3 uses the conﬁned local coolest shifting pattern with two dark cores, and the communication-intensive applications 𝐴4 and 𝐴5 use the conﬁned neighbor swapping pattern. The worst complexity of the dynamic patterning algorithm is 𝑂(max ∣𝐴𝑖 ∣2 ⋅ ∣𝑇 ∣, ∣𝑊 𝑄∣), which is bounded by the length of the working queue WQ. V I . EX P ER IM EN TA L EVA LUAT ION A. Experimental Setup Experiments are performed on an event-driven C++ simulator, with DSENT integrated as the power model and Hotspot is used as the temperature simulator. Task graphs are modeled in this simulator, which can dynamically arrive at the system. The simulator system has a network simulator which can model the package delay and energy of the communications in a cycle accurate manner. The conﬁguration of the network-on-chip is listed in Table I. The many-core system ﬂoorplanning can be found in [24]. The temperature threshold is 80 𝑜C. Both random and real applications are used in the experiments as tabulated in Table I in order to evaluate the performance of the proposed and relevant algorithms considered for comparison. In particular, we compare throughput (deﬁned as the average number of applications ﬁnished within a time unit), communication cost, and average waiting time for each application which occurs when there is insufﬁcient cores to run the tasks that arrive in the system at run-time. The communication cost is deﬁned as the network energy consumption, which is be measured by DSENT. The run-time execution costs of the algorithms are also evaluated. B. Evaluation of the Static Patterning Algorithm We compare our approach with the following two runtime thermalaware mapping algorithms that aim to dark silicon era, (1) DsRem [14], where the cores on/off patterning are identiﬁed followed by tasks mapped to active cores, and (2) PAT [12], where a core region including inactive cores is found for each application. 1) Evaluation on Random Benchmarks: Fig. 8 compares the throughput, waiting time, and communication cost at different network sizes, for the three methods. One can see that, when the network size is large, e.g., 12 × 12, our approach can improve throughput by 1.5× and 3× over DsRem and PAT, respectively. The TABLE I S IMU LAT ION CON FIGURAT ION S Network parameters 128 bits Router 2 cycles, link 1 cycle 4 ﬂits XY routing Flit size Latency Buffer depth Routing algorithm Baseline topology Random benchmark parameters [15, 45] [10, 200] (Kbits) [1, 15] Bimodal, uniform Real benchmarks AES enc, AES dec [26], E3S [1] Number of tasks Communication volume Degree of tasks Task number distribution 8 × 8 Normalized waiting time Normalized comm cost Normalized throughput 1 0.5 0 8 × 8 2 1 × 2 1 0 1 × 0 1 (a) 10 5 0 10 5 0 8 × 8 2 1 × 2 1 0 1 × 0 1 (c) proposed 8 × 8 0 1 × 0 1 2 1 × 2 1 (b) PAT DsRem Fig. 8. The throughput, waiting time, and communication cost comparison at different network sizes. reason is that, our approach can optimize both the communication and computation intensive applications. For communication intensive applications, tasks with high trafﬁc volumes are mapped closer, while for computation intensive applications, more bubbles are inserted. Therefore, our approach can achieve better performance. Fig. 8 also shows that the waiting time of our approach is shorter than the other two approaches because our approach balances the waiting time and the execution time of each application when inserting bubbles. The other two approaches only consider the performance of each individual application. Among the three approaches, DsRem has the worst communication cost, since it does not take the communications among the tasks into account. 2) Evaluation on Real Benchmarks: Fig. 9 compares the throughput, waiting time, and communication cost at different average number of tasks when the three methods are employed. When each application’s average number of tasks is large, e.g., 32 tasks, our approach’s throughput is about 1.67× and 1.5× over DsRem and PAT, respectively. Our approach also reduces waiting time by 50% and 44% over DsRem and PAT, respectively. C. Evaluation of the Dynamic Patterning Algorithm In this section, we compare the dynamic patterning with the global coolest approach [18] (where each hot task is migrated to a globally coolest core), the neighbor coolest [17] (where each hot task is migrated to a neighbor coolest core), and the static patterning approach proposed earlier in this paper. The ﬁrst two sets of experiments were performed with random benchmarks. Fig. 10(a) compares the throughput with different communication volumes, for the four methods. One can see that, when the communication volume is high, e.g., 200𝐾𝐵 𝑖𝑡𝑠, our approach’s throughput is about 1.45×, 1.42× and 1.6× over global coolest, neighbor coolest and static patterning, respectively. The reason is that, our approach can dynamically adjust the patterns of the task-tocore mapping, such that the communication cost is minimized and the computation performance is improved. Therefore, our approach can achieve better performance. Fig. 10(b) compares the throughput with different network sizes when the four methods are employed. One can see that, when the network size is large, e.g., 12 × 12, our approach Normalized throughput 1 0.5 0 8 16 (a) 32 3 2 1 0 Normalized waiting time Normalized comm cost 2 1 0 8 16 (b) 32 8 16 (c) 32 DsRem PAT proposed Fig. 9. The throughput, waiting time, and communication cost comparison at different average number of tasks in each application. Normalized throughput 1 0.5 0 50 100 150 200 (a) Normalized throughput 1 1 0.5 0 0.5 0 0 1 × 0 1 2 1 × 2 1 (c) 4 1 × 4 1 Normalized throughput 8 × 8 0 1 × 0 1 2 1 × 2 1 (b) dynamic patterning global coolest neighbor coolest static patterning Fig. 10. The execution time comparison with different (a) communication volumes (measured as K bits) and (b) network sizes for random benchmarks. (c) The execution time comparison with real benchmarks. can improve throughput by 1.9×, 1.35× and 2× over global coolest, neighbor coolest and static patterning, respectively. Static patterning increases the V/F level of the hot tasks/cores, and thus has the lowest performance. Both global coolest and neighbor coolest patterns leads to increased communication latency and fragmentation during the task migration process, and thus also leads to performance degradation. Next, real applications were used in the experiments. Fig. 10(c) compares the throughput with different network sizes with real benchmarks. One can see that, when the network size is large, e.g., 14 × 14, our approach can improve throughput by 1.7×, 1.4× and 1.9× over global coolest, neighbor coolest and static patterning, respectively. D. Cost Analysis The runtime costs of both static and dynamic patterning algorithms are in the order of 1M cycles. This is averaged by running the algorithms ﬁfty times with different system parameters. After the evaluation, it has been observed that the running times of DsRem and PAT are also in the order of 1M cycles. Therefore, the runtime overhead of the proposed algorithm is acceptable. V I I . CONC LU S ION We proposed a static and a dynamic patterning algorithm to budget free cores (referred as bubbles) to each application to optimize the system throughput. The system throughput is related to each application’s communication and computation performances, as well as the waiting time incurred when it ﬁnds insufﬁcient cores to run its tasks. Ofﬂine performance and waiting time models are ﬁrst set up for the applications. In the static patterning algorithm, an online algorithm was proposed to ﬁnd the best number and locations of the bubbles to each application, according to whether the new application is computation or communication intensive. In the dynamic patterning algorithm, the task migration pattern, the dark core number, and the shape and location of the core region are selected for each application to optimize it’s computation and communication performance. The runtime overhead of our approach is moderate, making it a suitable runtime resource management approach to achieve high system throughput for many-core systems running dynamic workloads. "
Towards Energy-Efficient High-Throughput Photonic NoCs for 2.5D Integrated Systems - A Case for AWGRs.,"Silicon Photonics (SiPs) can overcome the energy and bandwidth limitations of electrical interconnects in networks-on-chip (NoCs) and enable efficient global all-to-all connectivity-the ideal from a performance perspective. Unfortunately, state-of-the-art SiP switching fabrics impose power overheads for thermo-optical control of microring resonators (MRs), excessive crosstalk, or challenging physical layout. The Arrayed Waveguide Grating Router (AWGR) is a SiP device that provides layout-efficient and scalable all-to-all connectivity through wavelength-routing on a passive and compact platform. Recent technological advances now enable AWGR integration with significantly reduced footprint (<;1mm 2 ), crosstalk (<;-38dB), and loss (<;2dB), making AWGRs emerge as a major enabler for energy-efficient all-to-all connectivity in NoCs. This paper, for the first time, populates the design space of AWGR-based NoC topologies and compares AWGRs to state-of-the-art SiP fabrics and aggressive electrical baselines. Our results make a compelling case for AWGRs in interposer-based systems with processor disintegration, which have high bandwidth demands, large on-chip distances, and less stringent area constraints. AWGR-based NoCs can offer an average of 1.67× power savings, 1.23× speed-up, and 1.5× lower energy-delay-product (EDP) on PARSEC3.0/SPLASH-2× workloads compared to electrical baselines, and fewer waveguides, wavelengths, or MRs than alternative SiP crossbar fabrics.","Towards Energy-efﬁcient High-throughput Photonic NoCs for 2.5D Integrated Systems: A Case for AWGRs Sebastian Werner, Pouya Fotouhi, Roberto Proietti, Xian Xiao, S.J. Ben Yoo Department of Electrical and Computer Engineering, University of California, Davis Email: {swerner,pfotouhi,rproietti,xxxiao,sbyoo}@ucdavis.edu Abstract—Silicon Photonics (SiPs) can overcome the energy and bandwidth limitations of electrical interconnects in networkson-chip (NoCs) and enable efﬁcient global all-to-all connectivity– the ideal from a performance perspective. Unfortunately, state-ofthe-art SiP switching fabrics impose power overheads for thermooptical control of microring resonators (MRs), excessive crosstalk, or challenging physical layout. The Arrayed Waveguide Grating Router (AWGR) is a SiP device that provides layout-efﬁcient and scalable all-to-all connectivity through wavelength-routing on a passive and compact platform. Recent technological advances now enable AWGR integration with signiﬁcantly reduced footprint (<1mm2 ), crosstalk (<-38dB), and loss (<2dB), making AWGRs emerge as a major enabler for energy-efﬁcient all-to-all connectivity in NoCs. This paper, for the ﬁrst time, populates the design space of AWGR-based NoC topologies and compares AWGRs to state-of-the-art SiP fabrics and aggressive electrical baselines. Our results make a compelling case for AWGRs in interposer-based systems with processor disintegration, which have high bandwidth demands, large on-chip distances, and less stringent area constraints. AWGR-based NoCs can offer an average of 1.67x power savings, 1.23x speed-up, and 1.5x lower energy-delay-product (EDP) on PARSEC3.0/SPLASH-2x workloads compared to electrical baselines, and fewer waveguides, wavelengths, or MRs than alternative SiP crossbar fabrics. I . IN TRODUC T ION 2.5D integrated systems place multiple processor and memory dies on an interposer within the same package to improve both performance and energy consumption by reducing the amount of off-package signaling [1]. In particular, processor disintegration, which exploits the interposer to integrate multiple smaller chips instead of one large(r) chip, is economically efﬁcient due to the overall higher manufacturing yield of smaller dies [2]. Therefore, it is reasonable to expect future systems with growing numbers of memory and processor dies on interposers, especially as numerous commercial designs have already started to adopt 2.5D integration [3]. Recent studies have revealed that the high on-chip trafﬁc in interposer-based systems puts signiﬁcant strain on the NoC [1]. Unfortunately, shrinking power budgets coupled with poor technology scaling of electrical interconnects, increasing die sizes, and growing numbers of dies make the design of energyefﬁcient high-bandwidth NoCs in 2.5D integrated systems extremely challenging. SiPs could overcome this challenge by enabling the distance-independent, high-bandwidth (provided by dense wavelength-division multiplexing (DWDM)) properties of optical communication inside the chip. In particular, SiPs are more energy-efﬁcient than electrical interconnects in implementing global all-to-all connectivity–the ideal from a performance perspective–which could have a large impact on performance in interposer-based systems. SiP-based all-to-all NoCs proposed to date use either optical buses [4] or wavelength-routed photonic NoCs (WRPNoCs)[5]. While bus-based designs quickly become impractical and cost-inefﬁcient due to either large numbers of waveguides or wavelengths, WRPNoCs typically require hundreds (if not thousands) of MRs to perform wavelengthselective routing which leads to high power overheads for thermo-optical control of MRs and challenging physical layout [5][6][7]. The ideal wavelength routing fabric would eliminate the need for MRs for routing without requiring an excessive number of waveguides or wavelengths, while enabling a practical physical implementation. AWGRs provide such a communication fabric. AWGRs enable scalable, low-loss wavelength routing between all input and output ports by utilizing N wavelengths and N input and output waveguides in support of an all-to-all N × N interconnection. Recent fabrication advances in CMOScompatible Silicon Nitride (SiN) AWGRs enable footprints of less than 1mm2 , which is highly efﬁcient since it is the only device needed for routing (as opposed to 100s of MRs in WRPNoCs). Their only drawback is that increasing bandwidth between source-destination pairs through bitparallelism requires implementing multiple AWGRs in parallel since current AWGR demonstrations only switch a single wavelength between each input and output port (DWDM cannot be leveraged to boost bandwidth inside AWGRs); however, recent demonstrations of sub-pJ Pulse-Amplitude Modulation (PAM4) transceivers at 40Gbps data rate [8], multiple AWGRs fabricated atop [9], or next to each other [10], enable highbandwidth fabrics with very few AWGRs at high energy efﬁciency. Based on these recent advancements, this paper makes the following novel contributions: • Discusses challenges of 2.5D integrated systems with disintegrated processors and how AWGRs can be exploited to solve them. • Analyzes the suitability of AWGRs for on-chip commu978-1-5386-4893-3/18/$31.00 ©2018 IEEE nication and explores the design space of AWGR-based NoC topologies. • AWGRs provide an average speed-up of 1.23x, 1.67x power savings, and 1.5x lower EDP on PARSEC3.0/SPLASH-2x workloads compared to aggressive electrical baselines. I I . 2 .5D IN TEGRATED SY ST EM S Fig. 1: A Reference SiP Link 2.5D integrated systems placing processor and memory dies (mostly 3D-stacked DRAM like High Bandwidth Memory (HBM) [1]) on a silicon interposer within the same package have gained popularity due to the energy and latency savings of reducing off-package signaling and tighter integration [1][11]. In addition, Kannan et al. [2] have made the case for 2.5D integrating multiple small(er) dies instead of one large many-core die as it reduces design costs through overall higher manufacturing yield–a system architecture referred to as processor disintegration. A. Challenges of 2.5D Integrated Systems Despite the beneﬁts of disintegrated 2.5D integrated systems, Loh et al. [1] identiﬁed numerous design challenges, many of which are yet to be solved. First, the trend towards growing numbers of HBM stacks inside the package, more channels per HBM, and wider DRAM buses to increase memory bandwidth will lead to higher bandwidth demands on the NoC which will make the implementation of electrical NoCs within acceptable power envelopes extremely challenging–especially in combination with the large distances imposed by interconnecting several dies on an interposer. Second, the NoC’s clock network must deal with die-to-dieto-interposer process variations, possibly even with different technology generations of different dies or heterogeneous integration of multiple different dies. Loh et al. [1] propose to decompose the NoC into smaller, independent clock domains to have easier timing and to support dynamic voltage and frequency scaling (DVFS), indicating that topologies should ideally support clustering or be hierarchical. Thirdly, large distances (e.g., AMD’s FURY is 1011mm2 [3]) and routing between dies increases link latency, suggesting that disintegration comes with a performance-cost trade-off. Routing electrical signals over such distances at satisfactory speed can only be attained with power-consuming repeater circuitry, resulting in more of the power budget being dedicated to the NoC and less to the compute (assuming a system operating under a power cap) [1]. Electrical NoCs tailored to interposer-based systems were shown to be more efﬁcient than conventional NoCs for monolithic chips [12], but cannot fully overcome these limitations. B. Solving Design Challenges with SiPs and AWGRs Recent evaluation results have revealed that integrated optical interconnects enabled by SiPs offer signiﬁcantly better scalability and energy efﬁciency for growing numbers of dies on the interposer [13]. In fact, the relatively distanceindependent energy consumption of SiPs coupled with its high-bandwidth links is ideal to overcome the aforementioned performance-cost trade-off and bandwidth challenges. Besides, SiPs enable energy-efﬁcient high-bandwidth all-to-all connectivity which minimizes the network diameter (and, in turn, zero load latency) and provides sufﬁcient bisection bandwidth to sustain core clustering for efﬁciently supporting separate clock domains and DVFS. In particular, we propose the use of AWGR-based optical interconnects which inherently provide energy-efﬁcient all-to-all connectivity with just one device, does not require on-chip heating power for thermo-optical control, and can utilize an off-chip laser, all of which ultimately results in an interconnection fabric that reduces on-chip power and allows more of the power budget to be dedicated to the compute. In the following, we will introduce SiP links and AWGRs, demonstrate their suitability and beneﬁts inside a 2.5D integrated target design with processor disintegration, and explore the design space of AWGR-based topologies. I I I . ENABL ING T ECHNOLOG I E S A. Photonic Networks-on-chip Figure 1 depicts a reference SiP link with one sender and one receiver–referred to as Single-Writer-Single-Reader (SWSR) bus. An off-chip laser generates light which is coupled into the chip and waveguide. Modulators perform electrical-to-optical (EO) signal conversion by encoding bits onto wavelengths, which are ﬁltered out by the receiver prior to being converted back into the electrical domain (OE) by a photodetector. Data can be transmitted on multiple wavelengths in parallel using DWDM with separate modulators and ﬁlters each tuned to one distinct wavelength. MR heating and laser power are signiﬁcant contributors to power consumption in SiPs. MRs are used to build modulators and ﬁlters and are susceptible to temperature and process variations, thus requiring integrated/co-located heaters and control circuitry to ensure correct operation. Laser operating power depends on the number of wavelengths, optical path losses, receiver sensitivity, and laser efﬁciency. Constructing a NoC with point-to-point SWSR buses is possible but area- and layout-inefﬁcient. Alternative approaches exploit DWDM to assign subsets of wavelengths on a single waveguide to different sources with one destination connected to the waveguide, referred to as Multiple-Writer-SingleReader (MWSR) buses [4]. The dual to this approach is the Single-Writer-Multiple-Reader (SWMR) bus where one source can send to multiple destinations simultaneously on different wavelengths [4]. WRPNoCs reduces the number of waveguides by utilizing MR ﬁlters to perform wavelengthselective routing [5]. These state-of-the-art approaches successfully connect all nodes in a NoC, but, as we will show in the following, do so less efﬁciently than AWGRs. B. AWGR Figure 2a depicts the unique wavelength routing from input to output ports in an 8 × 8 AWGR. All wavelengths (λ0 ..λ7 ) entering a given input port are evenly distributed across all output ports of the AWGR–one wavelength to one unique output port. One intriguing property of AWGRs is that multiple signals on the same wavelengths entering from different input ports can traverse the AWGR without interfering with each other. Therefore, multiple input waveguides can be connected to an AWGR whose wavelengths are evenly distributed to the output ports. As a result, an AWGR provides all-to-all connectivity between all input and output ports–without the need for MRs or a large number of waveguides. Figure 2b shows the schematic of an AWGR device. Wavelengths entering from the input waveguides traverse the freespace propagation region and subsequently the grating waveguides, which have a constant length increment (∆L). Each wavelength undergoes a constant change of phase attributed to the constant length increment in the grating waveguides. Wavelengths diffracted from each waveguide of the grating interfere constructively and get refocused at the output waveguides/ports depending on the experienced array phase shift. Figure 2c illustrates a picture of a fabricated SiN 8 × 8 AWGR. AWGRs are a mature technology and have already been used in the telecom industry [10], allowing for years of fabrication know-how with high-yield manufacturing. The novelties that makes AWGRs suitable for on-chip communication are the advancements in CMOS-compatible SiN-based AWGRs, which not only exhibit very low loss and crosstalk, but also extremely reduced footprint [14]. For instance, the AWGR in Figure 2c has a footprint of < 1mm2 . In principle, AWGRs are bidirectional, i.e. light can be sent through the AWGR in both directions without interference, effectively forming a bidirectional all-to-all switching fabric with just one device (see Figure 4 on the right). The switching matrix in Figure 3 allows for a deeper understanding of the working mechanism of a bidirectional AWGR by listing the wavelength distribution between the input and output ports. An 8×8 AWGR is necessary to provide 4×4 bidirectional all-to-all connectivity (each node needs a separate input and output port to avoid ﬁltering out its own signals). In general, an X×X bidirectional crossbar needs a 2X×2X AWGR; however, since we only connect four nodes on each side of the AWGR, half of the wavelengths that could theoretically be switched by the AWGR are not needed, thus not imposing signiﬁcant laser power overheads. The wavelengths in blue are the ones actually used to connect the input to the output ports in each direction. Given the cyclic wavelength distribution inside AWGRs, the same wavelengths are used in each direction, which would allow each side of the AWGR to reuse wavelength from a shared laser, thereby reducing system costs. 1) DWDM and PAM4: The efﬁcient wavelengthdistribution mechanism of AWGRs coupled with its small area footprint and low losses make it an ideal candidate for global all-to-all connectivity, especially as input/output waveguides can be directly routed to the senders/receivers, thereby minimizing path lengths and laser power. SiP typically leverage multi-wavelength DWDM signals to increase bitparallelism and, in turn, link bandwidth, within a single waveguide. AWGRs can only distribute a single wavelength between each input-/output-port pair, which prevents multiwavelength communication between nodes. Consequently, bandwidth can only be increased through spatial-division multiplexing, i.e., through implementing multiple AWGRs in parallel. This would have lead to a large number of AWGRs in previous NoC studies which mostly assume On/Off keying (OOK) modulation (1 bit per symbol), a modulation rate of 10Gb/s, and DWDM levels between 16-64 (this design point was shown to provide the highest energy efﬁciency [15]). Advanced modulation techniques that increase the data rate by encoding multiple bits into one symbol were shown to consume too much energy (∼3pJ/bit [16]). Fortunately, Moazeni et al. [8] recently demonstrated a new PAM4 transceiver (2 bits per symbol) which only requires a ‘spoked’ MR (and driver circuitry) with just 5µm in radius and 0.197pJ/bit to convert two electrical input bits into a PAM4 signal at 20Gb/s modulation rate–effectively enabling a data rate of 40Gb/s per wavelength. Not only does this enable a four-fold reduction in DWDM level to attain the same link bandwidth as previous studies, but it also improves bit-error-rate and energy-per-bit compared to OOK [16]. The reductions in DWDM level provided by high-data-rate PAM4 allow to construct a crossbar with high bisection bandwidth with a very small number of parallel AWGRs (as we will see in Section IV-A, four AWGRs sufﬁce to serve the bandwidth demands of an entire 64-node NoC), making the combination of these two technologies a highly promising solution. In fact, the physical implementation of parallel AWGRs is not area-constrained on interposers, which could easily accommodate a large number of AWGRs. The loss incurred by waveguide crossings of parallel AWGRs is fairly low and can largely be mitigated with tapering [10]. Alternatively, stacking AWGRs on different SiP layers is also feasible and removes any area or loss concerns altogether [9]. C. AWGRs vs. State-of-the-art SiP Fabrics Providing global all-to-all connectivity is much more efﬁcient with SiP in term of scalability, layout, and energy efﬁciency than with electrical interconnects [4], and is ideal from a performance perspective (diameter of one, constant zero load latency, high path diversity). Figure 4 illustrates a 4 × 4 bidirectional all-to-all connectivity implemented with state-ofthe-art SiP interconnection fabrics. The number of waveguides in crossbars consisting of SWSRs grows quadratically with the number of nodes, which is area-inefﬁcient and complicates (a) Wavelength distribution inside an AWGR (b) Schematic (c) Physical Layout Fig. 2: Switching Functionality, Structure, and Layout of an 8×8 AWGR losses), which complicates layout as heating circuitry must be co-located. Numerous studies dedicated just for investigating efﬁcient WRPNoC layouts underline this issue (i.a. [5][7]). Using an AWGR alleviates all of the aforementioned issues. First, one input and output waveguide per node is required which allows placing all of the transceiver circuitry close to the nodes, thus simplifying layout. Second, wavelength routing does not rely on MRs and AWGRs do not require on-chip heating, thus completely eliminating heating circuitry and power for routing. Also, an AWGR-based crossbar does not exhibit any waveguide crossings, which leads to higher losses in WRPNoCs [5]. Finally, AWGRs are bidirectional which allows constructing a bidirectional all-to-all fabric using a single passive component consuming no power. Fig. 3: Switching matrix for both sides of a 4×4 bidirectional all-to-all switching fabric with an 8×8 AWGR layout. SWMRs or MWSRs overcome these issues by requiring only one waveguide per sender or receiver, respectively; however, assigning waveguides to senders/receivers complicates the physical implementation as more nodes are added to the NoC (e.g., in the SWMR case, each receiver must place MRs at each of the senders waveguides to ﬁlter out signals). Waveguide pitches, MR radii, and spacing between components are in the range of ∼5µm [17]. This results in designs in which MRs are placed fairly far away (could be > 100µm) from the actual nodes, complicating placement of driver and heating circuitry, causing non-negligible energy consumption on the interconnect, and limiting scalability. WRPNoCs overcome this issue as each node only needs one waveguide for sending and receiving, respectively. MR ﬁlters are strategically placed between waveguides to route wavelengths through the network to the correct destinations. A widely studied WRPNoC topology is the λ-Router [5][6][18] illustrated in Figure 4 (for illustration purposes, this ﬁgure only shows one side of the topology, i.e. from left to right). A sender merely has to modulate its data on the correct wavelengths to ensure that its data packet will arrive at the destination. WRPNoCs require fewer and shorter waveguides to create a crossbar than buses but rely on MRs for routing which consumes heating power. Moreover, MRs are typically distributed across the chip (depending on which layout provides the lowest IV. EVALUAT ION A. Target Design Figure 5 illustrates our target design of a 2.5D integrated chip with processor disintegration, HBM memories, and an AWGR-based NoC fabric. We assume a 64-core processor, split into four 16-core dies. On each die, eight cores are clustered at each router, and the routers are interconnected by an 8 × 8 AWGR. We assume conventional electrical links between the dies and HBMs as electrical links seem to be able to satisfy current and future memory bandwidth demands of HBMs, thus posing a less eminent problem to be solved by a technology shift from electrical to optical links [13]. Although a recent study has proposed SiPs for processor-HBM communication [10], the very nature of the HBM interface (low frequency, wide IO [1]) is rather incompatible with SiP interconnects which excel at high-speed serial links and would require an impractical amount of SERDES circuitry. The AWGR shown in Figure 5 operates like the bidirectional all-to-all fabrics in Figure 4. From an X-Y-axis perspective, each router is directly connected through the AWGR with each router in X-direction, no connections exist in Y-direction. Therefore, for sending to nodes in the same column, one additional hop through the AWGR is needed (resulting in a diameter of two). In order to explore the design space Fig. 4: Bidirectional all-to-all NoC implementations with SiP switching fabrics offers an ideal diameter of one. B. Methodology We ﬁrst compare the three crossbar topologies introduced in the previous section (8C, 4C, and all-to-all) implemented with AWGRs to a 2D Mesh (‘Mesh’) and a 2D Mesh with a clustering factor of four (‘Mesh4C’)–both of which have found wide use in commercial products (i.a., [20][21]). We also include a regular 2D folded Torus (‘FoldedTorus’) as well as a clustered version with four cores per router (‘FoldedTorus4C’), which were shown to have superior performance metrics than meshes [2]. Subsequently, we compare our AWGR-based topologies to implementations with SWSRs, MWSRs, and λRouter to identify the best SiP interconnection fabric. Note that in our performance study, we only report the AWGR NoCs because changing the interconnect fabric (i.e., AWGR vs. SWSR vs. SWMR) does not change the connectivity pattern or link bandwidth. Though path lengths may vary, signal propagation of light in silicon is ∼11.4ps/mm [6]–a speed at which we found the path lengths differences to be negligible for our target system dimensions and clock frequency. Our target system (Figure 5) places four HBMs with 8GB capacity each and a 1024-bit wide interface at 1GHz. Each tile on the dies has a private 32kB L1I/D and 256kB L2 cache using the MESI coherence protocol. Both routers and cores are clocked at 2GHz, electrical links and routers are 128-bit, and optical links 64-bit wide. For the SiP NoCs, a 64-bit wide optical link at 2GHz is enabled by four wavelengths with 32Gbps PAM4 modulation per wavelength with 0.197pJ/bit [8] (thus requiring four AWGRs on the interposer). Tile dimensions are 2mm in height/width, with 2mm spacing between dies/HBMs. We used gem5 [22] with Garnet2.0 [23] for performance evaluation, and DSENT [15] with a 22nm technology node for power and latency modeling of the CMOS circuitry. We modeled laser power based on the analytical model by Li et al. [17] and 20% laser efﬁciency, -21dBm receiver sensitivity, 0.5dB MR-drop loss, 0.01dB MR-through loss, 0.005dB/90◦ bending loss, 0.12dB waveguide crossing loss, 1dB coupler loss, 3.1dB splitter loss (i.e. 0.1dB loss of the splitter itself and 3dB for a 50:50 power split), AWGR loss of 1.7dB, and assume 20µW/MR heating power [6][14][24][15]. Fig. 5: Target Design (a) AWGR8C (b) AWGR4C (c) AWGR8C alltoall (d) Crossbar with Buses Fig. 6: SiP bidirectional all-to-all NoCs under investigation of AWGR-based NoCs, we study two additional topologies: AWGR4C and AWGR8C alltoall (see Figure 6b and 6c). AWGR4C is the same topology as AWGR8C, but clusters only four cores at each router and uses a 16 × 16 AWGR for bidirectional connectivity. A lower clustering factor boosts performance and improves load balancing, but also requires a higher-radix AWGR, possibly requiring a performance-power trade-off. AWGR8C alltoall clusters eight cores at each router, but rather than being bidirectional it provides true all-to-all connectivity between each source-destination pair in the NoC, which has been investigated for data center switches [19] and (a) Average packet latency (cyc) vs. injection rate (pkts/cyc/node) (a) Average packet latency (cyc) normalized to the 2D Mesh (b) Power consumption (W) vs. injection rate (pkts/cyc/node) Fig. 7: Synthetic Workload Results (b) Application execution speed-up relative to the 2D Mesh Fig. 8: PARSEC3.0/SPLASH-2x Performance Results We collected results for synthetic trafﬁc with different trafﬁc patterns (uniform random, transpose, tornado) to stress different corner cases of the topologies using Garnet’s synthetic trafﬁc engine which varies packet sizes between 8 and 72 bytes [23]. For realistic workloads, we performed full-system simulations by booting a complete Linux Operating System with gem5, on top of which we executed multi-threaded applications from the PARSEC3.0 and SPLASH-2x [25] benchmark suites with simlarge input sets. We collected statistics during the parallel phase of the applications after cache warm-up. C. Synthetic Trafﬁc Results Figure 7a shows that the lower diameter in the AWGR topologies leads to reductions in average packet latency of at least 2.5x compared to all electrical baselines prior to network saturation. The mesh/torus baselines generally saturate earlier except when compared to the low-overhead AWGR8C, which saturates slightly earlier than the Mesh/Torus. Figure 7b shows the total power consumption for varying injection rates and different trafﬁc patterns. Note that only the on-chip power consumption is included, not the off-chip laser, meaning that these power values give an estimate on the on-chip power budget required for those NoCs; however, as we will see in the following section, the AWGR topologies still improve the total system efﬁciency even when laser power is included. Aside from AWGR4C, which consumes slightly more power than the Mesh4C/Torus4C, AWGR8C and AWGR8C alltoall dominate all electrical baselines, and become increasingly efﬁcient with higher injection rates due to the low dynamic power of optical links for the large distances in the disintegrated processor and the lower average hop count in the AWGR topologies. AWGR4C has slightly higher power consumption than Mesh4C/Torus4C as it has a much higher number of links while having the same numbers of routers, leading to higher leakage power; however, it still considerably reduces power compared to a regular mesh. D. Realistic Trafﬁc Results Figure 8a depicts the average packet latency for PARSEC3.0/SPLASH-2x workloads, which AWGR topologies reduce by more than 2.5x on average through a very low average hop count. Figure 8b shows that the lower packet latency of the AWGR topologies translates to an average speed-up of 1.23x. In general, applications exhibiting a high degree of data sharing and cache miss rates (resulting in higher on-chip trafﬁc) beneﬁt the most from the AWGR topologies, which is in line with the throughput and latency beneﬁts observed for synthetic trafﬁc. Based on these results, it is reasonable to assume that applications with higher onchip trafﬁc would beneﬁt from AWGR topologies even more. Nevertheless, AWGR4C and AWGR8C alltoall only improve the execution time marginally compared to AWGR8C for these workloads despite offering higher bandwidth, showing that AWGR8C offers enough bandwidth for these workloads. Figure 9 illustrates a power breakdown of the different NoCs. AWGR-based switching fabrics (low loss and no heating) using PAM4 modulation to attain the same bandwidth with fewer wavelengths is an energy-efﬁcient solution to keep MR heating and laser power low. While the regular mesh/torus is clearly outperformed, Mesh4C can compete with AWGR4C and AWGR8C alltoall, which have larger leakage power due to their higher-radix routers. Leakage power is known to dominate the budget for NoCs with buffers and virtual channels for technology nodes of 22nm and lower [26]. Although power gating can almost halve leakage power [27], the dynamic longer paths compared to AWGRs and, as a result, higher wgl oss . Therefore, the main question in terms of laser power is whether the wgl oss incurred by the U-shaped buses is higher than the loss of the AWGR which offers much shorter paths but additional loss within the AWGR. The chip/tile dimensions and the wgl oss per mm are the determining factors to answer the vast majority (∼90%) of the that question (note that AWGR loss is caused by the free space propagation slab– different wgl oss values for the grating waveguides have very little impact on the overall AWGR loss). Tiles have been becoming increasingly larger. In fact, 2mm tile dimensions assumed in our study are well within those in current commercial designs [21]. Interposer-based systems with processor disintegration further increase distances as dies must be placed with sufﬁcient spacing between each other. Assumptions on wgl oss vary signiﬁcantly in previous studies, with values ranging from 0.3dB/mm [28] loss to 0.027dB/mm [29] due to different assumptions on materials (e.g. SiN vs. SOI waveguides) and/or optimized fabrication. To have a pessimistic estimation on the efﬁciency of AWGRs compared to other SiP fabrics, we chose 0.027dB/mm wgl oss , which is the lowest reported loss value to date. Generally, the differences in total optical power of all SiP switching fabrics is insigniﬁcant. Only λ-Router imposes noticeable power overheads, mainly due to the additional power required for thermal control of the MRs in its switching fabric. For AWGR, SWSR, and MWSR, the differences in power consumption are negligible, especially as laser and MR heating power represent less than 10% of the total power in the NoCs (see Figure 9). Besides, we noticed during our study that wgl oss values higher than the aggressive 0.027dB/mm would make the AWGR the clear winner in terms of power. From a design cost and layout point-of-view, AWGR and SWSR, though consuming slightly more power than MWSR, require much fewer wavelengths in the NoC, which makes them the more practical as this allows for either fewer lasers in the system (for single-wavelength lasers) or for a laser with a smaller wavelength spectrum (for comb-lasers). SWSRs require slightly less power than the AWGR, but require a large number of waveguides, making physical layout more challenging compared to the AWGR, which offers a much more elegant layout with just a single device, shorter waveguides through direct connections, close proximity between the waveguides and transceiver circuitry, and fewer total waveguides. V. R E LATED WORK Since the emergence of CMOS-compatible SiP devices, a large number of photonic NoC architectures have been proposed, typically aiming to ﬁnd the most efﬁcient way of integrating SiPs into NoCs, be it through a combination of electrical and optical interconnects in a NoC topology [30][31], WRPNoCs [5][6][32], circuit switching [33], or wavelength sharing mechanisms [17][?]. The physical SiP interconnects studied in this paper form the basis of all of these proposals, most of which implementing a global crossbar, which we showed is more efﬁcient with AWGRs. Fig. 9: Power breakdown for PARSEC3.0/SPLASH-2x workloads (some omitted due to space constraints) Fig. 10: EDP for PARSEC3.0/SPLASH-2x workloads power savings of the AWGR topologies (up to 1.54x) would still make them favorable. Figure 10 shows the EDP (total energy x execution time) for the different NoCs. The large number of links in AWGR4C causes too much power overhead compared to its performance improvements. AWGR8C offers similar speed-up compared to AWGR4C and AWGR alltoall, but does so with much lower power. Consequently, AWGR8C is the best design and halves the EDP compared to the best electrical baseline FoldedTorus4C (on average). AWGR8C alltoall offers a 1.25x lower EDP and would thus provide a power-efﬁcient solution for latency-critical NoCs as it provides an ideal diameter of one. For workloads exhibiting higher on-chip trafﬁc these topologies will likely become even more favorable (as suggested by the latency and dynamic power improvements for synthetic trafﬁc), especially if power gating de-emphasizes leakage power are deployed. In fact, cores are known have a low average injection rate in PARSEC3.0/SPLASH-2x applications, leading to fairly low average dynamic power throughout the execution of an application. Nevertheless, AWGRs still considerably reduce power and execution time, making a compelling case for AWGR-based topologies in interposerbased many-core processors. E. AWGR vs. Alternative SiP Interconnection Fabrics Table I lists optical path losses (ILmax ), number of wavelengths, laser power, and MR heating power for the different SiP interconnects. The main difference between the AWGR and the bus fabrics regarding ILmax is waveguides loss (wgl oss ) and AWGR loss. NoCs based on buses typically deploy Ushaped waveguide layout to enable connectivity between all nodes, eliminating waveguide crossings but also leading to TABLE I: Optical power comparison between different all-to-all switching fabrics AWGR 8C SWSR MWSR λ-Router AWGR 4C SWSR MWSR λ-Router AWGR 8C All to All SWSR MWSR λ-Router ILmax (dB) Number of wavelengths Laser Power (mW) Number of MRs MR heating power (mW) Total Optical Power (mW) 4.07 4 12.3 256 5.12 3.7 4 11.9 256 5.12 10.43 10.05 3.19 16 10.62 256 5.12 8.75 3.9 16 12.5 384 7.68 13.18 4.6 8 54.1 512 10.24 64.34 4.0 4 53.6 512 10.24 63.84 3.9 32 50.1 512 10.24 60.24 4.2 32 53.59 1408 28.16 81.75 4.2 7 21.45 448 8.96 30.41 3.57 4 20.2 448 8.96 3.44 28 19.6 448 8.96 29.16 28.56 3.48 28 19.86 1344 26.88 46.74 SiPs evolve quickly and new devices enable new opportunities for NoC architectures. For instance, compelling demonstrations of on-chip lasers enable low-latency/energy adaptive laser control which can save large amounts of laser power [34]. The AWGR-based topologies proposed in this paper could, in fact, be efﬁciently combined with adaptive lasers to further improve power efﬁciency. The all-to-all style topologies enabled by AWGRs offer high path diversity, which could be exploited to perform adaptive bandwidth scaling by shutting down or turning on lasers on different paths. SiPs have also been investigated for solving the memory wall problem by using optical processor-to-DRAM links, which offer orders of magnitude higher bandwidth per pin than electrical interconnects [10][35][36]. Other studies use SiPs to connect several chips on a PCB board to form a virtual chip, like Oracle’s Macrochip [37] or Galaxy [38]. AWGRs could also be used to connect chips or memories, thereby offering a low-loss and mature switching fabric for future systems. V I . CONCLU S ION This paper investigated the use of AWGRs inside NoCs for interposer-based disintegrated processors, populated the design space of AWGR-based NoC topologies, and compared them to state-of-the-art SiP interconnects and aggressive electrical baselines. Our results show that AWGRs are ideal to implement global all-to-all connectivity and provide signiﬁcant speed-up and power savings. Based on our results, many future studies could be conducted around dynamic bandwidth allocation and adaptive laser sources, which can use AWGRs as the physical interconnection fabric and could further improve the total energy efﬁciency of photonic NoCs. "