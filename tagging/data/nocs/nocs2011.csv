title,abstract,full_text
Energy and reliability oriented mapping for regular Networks-on-Chip.,"We formulate the problem of energy consumption and reliability oriented application mapping on regular Network-on-Chip topologies. We propose a novel branch-and-bound based algorithm to solve this problem. Reliability is estimated by an efficient Monte Carlo algorithm based on the destruction spectrum of the network. Simulation results demonstrate that reliability can be improved without sacrificing much of energy consumption.","Energy and Reliability Oriented Mapping for Regular Networks-on-Chip Cristinel Ababei Electrical and Computer Eng. Nor th Dakota State University Fargo, ND 58102, USA cristinel.ababei@ndsu.edu Hamed Sajjadi Kia Electrical and Computer Eng. Nor th Dakota State University Fargo, ND 58102, USA hamed.sajjadikia@ndsu.edu Jingcao Hu Tabula, Inc. 3250 Olcott St. Santa Clara, CA 95054, USA jhu@tabula.com Om Prakash Yadav Industrial and Manuf. Eng. Nor th Dakota State University Fargo, ND 58102, USA om.yadav@ndsu.edu ABSTRACT We formulate the problem of energy consumption and reliability oriented application mapping on regular Networkon-Chip topologies. We propose a novel branch-and-bound based algorithm to solve this problem. Reliability is estimated by an eﬃcient Monte Carlo algorithm based on the destruction spectrum of the network. Simulation results demonstrate that reliability can be improved without sacriﬁcing much of energy consumption. Categories and Subject Descriptors B.8 [Performance and Reliability]: Reliability, Testing, and Fault-Tolerance General Terms Algorithms, Reliability Keywords Regular Network-on-Chip, mapping, energy, reliability 1. INTRODUCTION Network-on-Chip (NoC) has emerged as a new communication infrastructure for multiprocessor Systems-on-Chip (MPSoCs) [1, 2]. Because increasingly adverse process variations and wearout mechanisms result in an increased number of transient, intermittent, and permanent errors, NoC reliability is a growing challenge in the design of MPSoCs [3]. Fault tolerance is one of the oldest resilience areas [4, 5] and relies on redundancy as a technique to compensate for the random failure of components, consequently improving reliability [6]. Previous work has focused mainly on processing elements as the computation units of multiprocessor SoCs. They addressed reliability by employing fault tolerant techniques based on error detection [7], failure prediction Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS’11, May 1-4, 2011 Pittsburgh, PA, USA Copyright 2011 ACM 978-1-4503-0720-8 ...$10.00. [8], and error masking [9]. While in the ﬁeld of computer networks there has been a lot of work done on reliability, there have been only few recent attempts to estimate or indirectly optimize NoC reliability. The organization in layers of NoCs resembles the open system interconnection (OSI) protocol stack. Thus, it is convenient to discuss resilience techniques for NoCs in association with this organization. At the physical layer, wires may be sub ject to delay variations [10], while routers may be impacted by single event upsets (SEUs) [11]. The data-link layer can provide the functional and procedural means to detect and possibly correct errors that may occur in the physical layer, by employing error correcting codes (ECCs) [12, 13, 14], data encoding [15, 16], and redundancy based reconﬁguration [17]. At the network layer, reliability of the routing algorithms can be enhanced by routing multiple copies of the same packet via multiple paths [18, 19, 20, 21, 22] or by adaptive re-routing [23]. The system software provides an abstraction of the underlying hardware platform, which can be leveraged by the application developer to eﬀectively exploit the hardware’s capabilities via reconﬁguration [24]. Modeling of link failures due to process variations is studied in [25]. The reliability of custom switch architectures is analyzed in [26] while of various NoC topologies is analyzed in [27]. Several recent studies have investigated NoC fault tolerant architectures and techniques [28, 29, 30, 31, 32, 33, 34]. For a given set of resilience techniques, its eﬀectiveness in achieving the desired system-level reliability must be evaluated and its associated costs such as system-level energy and performance costs must be quantiﬁed. However, evaluation of the reliability of NoC based SoCs is a challenging task because reliability is aﬀected by numerous factors including aging/wearout mechanisms, process variations, dynamic power and thermal management, workload conditions, and system architecture and conﬁguration. In the context of reliable computing systems, high-level reliability metrics have been proposed including reliability function R(t), mean time to failure (MTTF), mean time to repair (MTTR), architectural vulnerability factor (AVF), availability, data integrity, etc. The reliability of NoCs has been evaluated using various metrics including fraction or probability of correctly delivered packets [21, 17, 32], percentage of lost packets or undetected errors [22], reliability factor [26], number of corrected errors [28, 29], minimum edge cutset [32], probability of correct operation [34], and path reliability [35]. Previous work does not evaluate and address reliability of NoCs directly during the optimization process of various design steps1 . Instead, reliability is achieved indirectly via fault tolerance and redundancy based techniques. Moreover, the ma jority of current resilience techniques are concentrated on individual levels of the system stack. Because such single-level approaches are based on redundancy, they impose high overheads in power, area, and performance. A diﬀerent approach is to design solutions that span multiple abstraction layers. This is the key idea of our reliability optimization solution, which is a collaborative approach of multiob jective mapping (design tool level) and adaptive routing (network layer). To this end, our contribution lies in (1) We formulate for the ﬁrst time the problem of mapping for regular NoCs with energy consumption and reliability as direct ob jectives. We propose a simple reliability model, which relates the path diversity determined by the shape of the bounding box of a given s − t communication pair to reliability (as s − t connectivity). (2) We develop an eﬃcient branch-and-bound algorithm to solve the mapping problem. This algorithm provides a tunable tradeoﬀ between the objectives of energy consumption and reliability via the parameter α. This can be utilized to generate Pareto optimal curves. (3) We validate the proposed algorithm via simulation based on an eﬃcient Monte Carlo reliability estimation technique. 2. BACKGROUND ON NETWORK RELIABILITY In our discussion, for a given network N, edges represent the unreliable components of the network. We assume that each edge can be in two mutually exclusive states, up and down: down means failure and up means normal functioning. An important network reliability parameter is the so called probability of terminal connectivity. If we refer to only a single source-destination pair s − t, then, we are interested in the probability of so called s − t connectivity. In general, for an arbitrary number k of source-destination pairs, we will use the term probability of source-terminal connectivity. Let us assume that the probability of each edge being up is p and of being down is q = 1 − p. When each of the source-destination pairs is connected through at least a path of edges which are up, we say that the network N is UP; otherwise we say that the network is DOWN. The probability of the network being in the UP state is utilized as a direct measure for reliability. In other words, the static network reliability can be computed straightforwardly by the expression [36]: r(cid:2) R(N) = P (N is U P ) = P (Si ) (1) i=1 where Si , i = 1, ..., r is a particular set of edges which are up, such that the network is UP. In the context of NoCs, we are concerned with the connectivity between the sources and destinations of the communication pairs of the application. This connectivity must be realized via paths formed by edges in the up state. For simplicity, in this paper we consider routing paths which are monotonic XY-YX (i.e., paths are formed by traversing 1The study in [32] only does topology selection and longrange link insertion to improve network reliability. (a) (b) (c) Figure 1: (a) Source destination pair with path length 2. (b) Path length 3. (c) Path length 4. links only in the same direction along x and y for a given source-destination pair). For example, in Fig.1.a, there is a single source-destination pair s − t. Hence, we are interested in the probability of s − t connectivity. The UP states of the network with four edges from Fig.1.a are S1 = {e1 , e2 }, S2 = {e1 , e2 , e3 }, S3 = {e1 , e2 , e4 }, S4 = {e1 , e2 , e3 , e4 }, S5 = {e3 , e4 }, S6 = {e1 , e3 , e4 }, and S7 = {e2 , e3 , e4 }. These states have probabilities P (S1 ) =P (S5 ) =p 2 q2 , P (S2 ) = P (S3 ) =P (S6 ) = P (S7 ) = p3 q , and P (S4 ) =p 4 . Therefore the static network reliability can be computed by equation 1: P (Si ) = 4p 3 2 q + 2p 2 q 4 + p (2) 7(cid:2) R(N) = i=1 which, for let us say p = 0.95, equals 0.9904. This is higher than the reliability of the network in the upper part of Fig.1.a R(N) = p2 = 0.9025. The diﬀerence is due to the two paths, which connect s and t in the network from the bottom of Fig.1.a as opposed to only one path in the network from the top of Fig.1.a. It is important to note that in the context of NoCs, the consideration of the two existing monotonic XY-YX paths is possible under the assumption that the NoC architecture is equipped with minimal adaptivity. This adaptivity means the capability to detect and locate link failures and to update the routing tables such that the remaining healthy paths are utilized for routing packets. The simplicity of the adaptive routing will result in minimal area overhead. This will be discussed in more details in a later section. The above discussion also applies to the networks in Fig.1.b and Fig.1.c. In general, it can be proven that the reliability of the networks at the bottom of these ﬁgures is higher than that of the networks at the top of these ﬁgures. Moreover, the reliability increases as the topology of the network is closer to a square shape. For example, the reliability of the network at the bottom left of Fig.1.c is higher than the reliability of the network at the bottom right of Fig.1.c, which in turn is higher than that of the network at the top of Fig.1.c. Note that the Manhattan distance (as number of reliability costs. In addition, the overall energy cost would increase too much. The constant C onst is speciﬁc to each given NoC mesh size and is selected as the minimum possible value such that the reliability cost rcost (d) remains positive for any possible bounding box within the given NoC mesh size. The overall network reliability cost can then be written as: (cid:2) Rcost = ∀ pair (ti ,tj ) rcost (dij ) (4) The proposed reliability model basically relates the path diversity determined by the shape of the bounding box of a given s − t communication pair to reliability (as s − t connectivity). As mentioned earlier, this relationship is possible due to the fact that the NoC architecture is equipped with hardware support for minimal routing adaptivity. This is the key idea of the proposed reliability optimization, which is done collaboratively: during mapping (design tool level) and with adaptive routing (network layer) to directly address link failures. 3.3 Energy Modeling To model the total communication energy consumption of the network, we adopt the bit energy metric proposed in [38]. Using this metric, the energy cost can be written as: (cid:2) (cid:2) aij ∈A Ecost = ELbit v(aij )dist(ti , tj ) + ERbit aij ∈A v(aij )[dist(ti , tj ) + 1] (5) where ERbit and ELbit represent the energy consumed when one bit of data is transported through one router and one physical link between two neighboring routers of the network. v(aij ) is the communication volume between two cores of the APCG. dist(ti , tj ) represents the Manhattan distance between tiles ti and tj . Equation 5 states that the energy consumption is determined by the Manhattan distance between source and destination tiles and is proportional to the total communication volume. 3.4 The Problem of Energy and Reliability Oriented Mapping By using the models introduced in the previous subsections, the problem of energy and reliability oriented mapping can be formulated as follows. Prob. statement − Energy and Reliability Oriented Mapping Given an APCG G(C, A) and the regular mesh NoC topology formed by the routers R; Find a mapping function Ω : C → R which maps each IP core ci ∈ C in the APCG to a router (hence tile) rk ∈ R and a deterministic, deadlock-free, minimal routing function R(); Such that the following ob jective function is minimized: cost + (1− α)E (cid:2) (cid:2) M in : αR (6) Ω(ci ) (cid:4)= Ω(cj ), ∀ci (cid:4)= cj ∈ C S.t. (7) B (lk ) ≥ v(aij )f (lk , R(Ω(ci ), Ω(cj ))), ∀lk (8) (cid:2) cost aij ∈A Figure 2: Illustration of the problem of mapping for regular mesh NoCs. hops) between s and t is the same in each case. It is precisely this relation between network reliability and the shape of the bounding box for a given source-destination pair, which we exploit to include reliability as an optimization goal in the mapping algorithm described in the next section. 3. ENERGY AND RELIABILITY ORIENTED MAPPING FOR REGULAR NOCS 3.1 The Problem of Mapping for Regular NoCs In this paper, we assume a regular mesh NoC topology composed of a 2D regular array of identical tiles. Each tile contains a processing element (PE) and a router. We utilize the terminology introduced in [37]. An application is given as an application characterization graph vertex ci ∈ C represents an IP core, and each directed arc (APCG) G(C, A), which is a directed graph, where each aij ∈ A represents the communication between source core ci and destination core cj . Each aij can be tagged with application-speciﬁc information (such as communication volume in bits v(aij ) or communication rate) and speciﬁc design constraints (such as communication bandwidth, latency requirements, etc.). An APCG is derived from an application communication task graph (CTG) whose concurrent tasks have been already assigned and scheduled onto a list of selected IP cores. The mapping problem then is to decide how to topological ly place the selected set of IP cores onto the PEs (or tiles) of the NoC array such that the metrics of interest are optimized (see Fig. 2). 3.2 Reliability Modeling max As we discussed in the previous section, for a given path length or Manhattan distance d between tiles ti and tj , the reliability is higher when the two tiles deﬁne a bounding box closer to a square shape. Therefore, one can use as a reliability cost: cost (d − 1) + (C onst − wbbox · hbbox ) rcost (d) = r where wbbox and hbbox represent the width and height of the bounding box deﬁned by the source and destination tiles. cost (d − 1) is a biasing term that equals the The ﬁrst term rmax d − 1. This term is needed to keep rcost (d) proportional maximum value of the reliability cost for a smaller distance to d = wbbox + hbbox . If it is not used, then, longer Manhattan distances will be associated incorrectly with smaller (3) ing communication volumes. The LBC of an internal node (which is a value that is the lowest cost that its descendant leaf nodes can possibly have) is decomposed into three components. LBC = LBCm,m + LBCu,u + LBCm,u (10) where LBCm,m is the partial cost calculated exactly (by equation 6) for the cores which are already mapped. LBCu,u is the partial cost due to the communication among the unmapped cores. It is calculated by assuming the closest possible locations between unmapped cores (on available unoccupied tiles). LBCm,u represents the cost due to the communications between mapped and unmapped cores. It is calculated also by assuming the best possible mapping of the unmapped cores on unoccupied tiles with respect to the already mapped cores. The pseudocode of the energy and reliability oriented mapping algorithm is as follows. Algorithm − Energy and Reliability oriented Mapping 1. Set α to desired value 2. Set M inU BC = ∞, B estC ost = ∞ 3. Create root node; Insert root node into priority queue P Q 4. Pop node from P Q 5. For each unoccupied tile ti do 6. Generate child node nc 7. Compute LBC and UBC as combined energy and reliability costs for node nc 8. If nc .LBC > M inU BC then Go to Step 10 9.1. If nc is leaf node then If nc .cost < B estC ost then Record B estC ost = nc .cost Record nc as best mapping so far 9.2. Else If nc .U BC < M inU BC then M inU BC = nc .U BC Insert nc into P Q 10. Repeat Steps 4-9 until P Q becomes empty 11. Report best mapping 4. RELIABILITY ESTIMATION The reliability of each mapping solution must be evaluated. Note that while equation 4 provides an easy mechanism to incorporate reliability into the cost function of mapping, it does not represent a means to estimate the reliability per se. While for small networks direct reliability calculation by equation 1 is simple, for large networks with multiple source-destination pairs, reliability estimation becomes challenging. Therefore, we propose a novel technique to estimate network reliability based on the network destruction spectrum, which in turn is estimated by an eﬃcient Monte Carlo algorithm [36]. This algorithm must be modiﬁed and adapted in the context of NoCs, where the network N represents the NoC architecture with the application cores mapped to it. In this representation, nodes of network N correspond to NoC routers while edges of N correspond to NoC links. The network status is UP when each communication pair sk − tk , k = 1, ..., n can be connected via a valid routing path formed by edges (or links in the context of NoCs) which are up. Figure 3: Illustration of the search tree. The internal node 103 represents the partial mapping of cores c0 c1 c2 to tiles t1 t0 t3 . (cid:2) (cid:2) where R cost and E cost are the normalized reliability and energy costs given by the equations 4 and 5. The normalization is necessary due to the potential large diﬀerence between the absolute values of the two cost components. It is done with respect with the worst case mapping, which one can achieve by placing individually the source and destination parameter α ∈ [0, 1] (which can be speciﬁed by the user) is of communication pairs in opposite corners of the NoC. The an ob jective weight, which is utilized to put more emphasis on reliability or energy ob jectives. B (lk ) is the bandwidth utilized or not by the routing strategy R: of link lk and the binary function f records if the link is if lk /∈ pathi,j if lk ∈ pathi,j f (lk , pathi,j ) = (9) (cid:3) 0, 1, 3.5 Branch-and-bound based Mapping To solve the energy and reliability oriented mapping problem, we propose a branch-and-bound (BB) based algorithm [39]. In our implementation, we extend the mapping algorithm studied in [40]. The BB algorithm enumerates systematically all candidate solutions out of which many are discarded by using upper and lower estimated bounds of the cost function being optimized. The enumeration process utilizes a search tree data structure. The root node represents the state when no IP core is mapped yet. The tree is constructed during branching by creating new child nodes as IP cores are mapped sequentially on the remaining available tiles of the NoC architecture (see Fig. 3). Each internal node represents a partial solution while leaf nodes correspond to complete solutions. For each partial solution associated with an internal node, the bounding procedure computes upper and lower bounds for the minimum value of the cost function. The key idea of the BB algorithm is that if the lower bound for a currently created node is greater than the upper bound for some other node, then, the current node can be discarded from the search (its child nodes will not be created and explored). This step is called pruning. The calculation of the upper bound cost (UBC) and lower bound cost (LBC) from [40] is extended to also include the reliability cost component of equation 4. The UBC of an internal node (which is a value that is no less than the minimum cost of its legal descendant leaf nodes) is computed by greedily mapping temporarily the remaining (not mapped yet) cores. During this greedy mapping, each of the remaining cores is assigned to a tile whose location is computed as the weighted center of gravity with respect to the already mapped cores. The weighting is done by the correspondTo describe the Monte Carlo algorithm we adopt the terminology introduced in [36]. For simplicity, we assume that all edges have the same lifetime τe ∼ F0 (t). Note that the authors of [36] present also an algorithm for estimating the network lifetime when edges are characterized by arbitrary distributed lifetimes [41]. One can employ such an algorithm also in the NoC context as long as the individual edge lifetime distributions are known. Then, the proposed technique to estimate NoC reliability for any F0 (t) and t is as follows. Algorithm − NetlifeSpectrum NoC 1. Set N k 1 = N k 2 = ... = N k pair sk − tk , k = 1, ..., n m = 0 for each source destination 2. Simulate random permutation π for the numbers 1, 2, ..., m, π = (i1 , i2 , ..., im ) 3. Assign weight r to the r-th edge ir , r = 1, ..., m 4. Find the maximal-weight spanning tree for each source destination pair sk − tk , k = 1, ..., n 5. Find the minimal-weight edge in each of the spanning trees. Let these weights be w k , k = 1, ..., n 6. Let qk = w k ; Put N k qk + 1, k = 1, ..., n 7. Repeat Steps 2-6 M times 8. For r = 1, ..., m put ˆf k r = N k r /M , k = 1, ..., n qk = N k ∗ ∗ 9. Estimate F k N (t) as ˆF k N (t) = k ˆf r F(r) (t), k = 1, ..., n m(cid:2) r=1 10. Let the maximum among these be ˆF N (t) = max 11. Estimate RN (t) as ˆRN (t) = 1 − ˆF N (t) ∗ k=1,...,n k ˆF N (t) ∗ in a 4 × 4 regular mesh NoC architecture m = 24) and n is where m is the number of edges in the network (for example the number of communication pairs. ˆfr , r = 1, ..., m is the estimated D-spectrum. F(r) (t) is the cumulative distribution function (CDF) of the r-th order statistic of the edge lifetimes and is given by the following expression [36]: m(cid:2) F(r) (t) = j=r m! j !(m − j )! j [F0 (t)] [1 − F0 (t)] m−j (11) testcase mpeg4 is from [43] and the second testcase telecom is from [40]. The third and fourth testcases ami25, ami49 were constructed from the classic MCNC benchmarks [44]. The initial connectivity between the modules was used to compute the communication volume in the communication task graph associated with each benchmark. Table 1: Testcases and their characteristics. Num. of APCG Testcase cores connectivity mpeg4 9 low telecom 16 medium ami25 25 high ami49 49 high Min/max comm. vol. 1/942 11/71 1/4 1/14 5.1 Energy Only versus Combined Energy and Reliability Oriented Mapping In the ﬁrst set of experiments, for the BB based algorithm, we compare the case when both energy and reliability are included as ob jectives into the cost function against the case when only energy is optimized. The results are shown in Table 2. Energy is calculated by equation 5 and reliability is estimated using the NetlifeSpectrum NoC algorithm from the previous section. For example, the mapping solutions for testcase telecom are shown in Fig. 4. Table 2: Branch-and-bound based algorithm. Testcase mpeg4 telecom ami25 ami49 CPU runtime (s) 0.23 2.1 7.25 124.1 Ob jective: energy Ob jective: energy & reliab. Energy Reliability Energy Reliability (J) (%) (J) (%) 5.73 97.96 5.74 98.953 13.969 95.163 14.794 98.064 6.907 96.91 7.185 97.932 15.402 91.378 15.942 94.111 In our implementation, the above algorithm is used to compute the system static reliability for diﬀerent mapping solutions. The default values of M and of the probability reliability is static − t is ﬁxed or not present). This algoof link failure are M = 10, 000 and F0 (t) = q = 0.01 (hence rithm represents a callable routine inside the program which implements the proposed mapping algorithm. Note that one could use this routine to replace equation 4 and thus estimate directly the reliability of a partial mapping solution. In this way the reliability cost component from equation 6 will be more accurate. However, the computational runtime of the mapping algorithm will increase because equation 6 must be evaluated thousands of times inside the branch-and-bound algorithm. 5. EXPERIMENTAL RESULTS The proposed mapping algorithm was implemented as a computer program in C++. The program and testcases can be downloaded from [42]. Simulations were done on a Linux machine running on a 2.8 GHz Intel Quad processor with 2 GB memory. We investigate the performance of the proposed algorithm on the testcases shown in Table 1. The ﬁrst (a) (b) Figure 4: (a) Application characterization graph (APCG) of testcase telecom. (b) Mapping solutions with energy only as ob jective and with combined energy and reliability ob jective. ) J ( y g r e n E 18.00 17.00 16.00 15.00 14.00 13.00 95.00 α = 0 telecom 0.1 96.00 97.00 Reliability (%) α = 0.9 0.8 0.7 0.3...0.6 0.2 98.00 99.00 Figure 5: Pareto frontier for testcase telecom. Data points are displayed from left to right for α varied between 0 and 0.9 in increments of 0.1. It can be observed that when reliability is also included into the cost function, the overall reliability of the mapped application is improved. However, the improvement in reliability is at the expense of increasing the energy consumption. Because, here we refer to the energy consumption of the NoC (which accounts for only up to 28% of the overall energy consumption of a multiprocessor SoC [45]), the energy consuption overhead is still within reasonable limits. The tradeoﬀ between reliability and energy consumption can be controlled by α. The closer α is to 1, the higher the reliability improvement. For example, the Pareto frontier when α is varied between 0 and 0.9 in increments of 0.1 is shown in Fig. 5 for testcase telecom. The results from Table 2 are reported for α = 0.6, which oﬀers a good balance between reliability improvement and energy increase. In our experiments, we noticed that reliability decreases as the connectivity of the APCG increases (for example testcases ami25 and ami49 have almost all cores communicating with each other). This can be explained in part by the fact that each link is utilized by more routing paths. Thus, a link failure has a bigger impact on the state of the network. Also, in highly connected APCGs there are more straight routing paths (as in the upper parts of Fig. 1.a-c), which increase the chances of the network to go DOWN. Another factor which aﬀects overall reliability is the probability of link failure q . The results from Tables 2 were achieved for reliability − the variation of reliability as a function of the q = 0.01. We also investigated how q impacts the network link failure probability q ∈ [0.0001, 0.5] is shown in Fig. 6. Even though it may seem that reliability improvement of y t i l i b a i l e R 1.2 1 0.8 0.6 0.4 0.2 0 mpeg4 telecom ami25 ami49 Probability of link failure q Figure 6: Variation of the static network reliability as a function of the probability of link failure. only a few percentages is not much, this improvement cannot be directly compared as a percentage against the percentage of energy or performance overhead. Instead, the signiﬁcance of such an improvement of reliability lies in its economic potential (what is the cost savings when for example 3% out of the total population of systems do not fail − i.e., when yield is improved by 3%), consequences (which can be severe in critical applications), and user satisfaction. 5.2 Comparison with Simulated Annealing based Mapping In this section we compare the proposed branch-and-bound algorithm against simulated annealing (SA). The SA based algorithm has been ﬁne-tuned by adjusting the cooling schedule and the number of moves per temperature. The basic move within the annealing process consists of swapping two cores. The results are shown in Table 3. It can be observed that while the energy and reliability results are similar to those in Table 2 the computational runtime of the SA algorithm is much longer. Table 3: Simulated Annealing based algorithm. Testcase mpeg4 telecom ami25 ami49 CPU runtime (s) 8.17 21.47 306.2 7089.9 Ob jective: energy Ob jective: energy & reliab. Energy Reliability Energy Reliability (J) (%) (J) (%) 5.865 96.951 6.321 98.953 13.722 95.208 14.877 98.927 6.861 96.054 6.815 97.929 13.888 90.121 14.104 94.988 5.3 Discussion of Hardware Implementation The collaborative nature of the proposed reliability optimization is based on NoC architectural support for costeﬀective adaptive routing. That is, the NoC architecture integrates support to detect link failures and to update appropriately the routing tables such that all s − t communication pairs of the APCG are connected via healthy monotonic XY-YX routing paths. The routing tables update is done periodically by rerouting controllers located in each router (Fig. 7.a), when the NoC is forced into testing mode (TM). This is similar to the method proposed in [46]. The testing mode has two phases. (a) (b) Figure 7: (a) The modiﬁed router integrates a rerouting controller, which updates the routing tables when link failures are detected. (b) Illustration of the broadcasting process: router 5 broadcasts, the information is received after one cycle by routers 2, 4, 8, after two cycles by routers 1, 3, 7, and after three cycles by routers 0, 6.   In the ﬁrst phase, each router with failed adjacent links broadcasts the state of its adjacent links2 . When, for example router i broadcasts, all other routers will receive the information from router i after a number of cycles which depends on the distance from router i (Fig. 7.b). This process is done sequentially in that the router j which also has failed adjacent links will start broadcasting the state of its adjacent links only after the information from router i has reached all other routers. In the second phase of the testing mode, the routing tables are updated by rerouting controllers. To minimize the hardware overhead, the routing tables update is done such that the next path closest to the previous monotonic XYYX routing path is selected. For example, in Fig. 8 for the s − t communication pair, path 2 is selected after failure 1 occurs and path 3 is selected after failure 2 occurs. Figure 8: Illustration of path selection via routing tables update when link failures occur. If at the end of the testing mode period all routing tables are updated successfully, the normal operation of the NoC is resumed. If there are too many link failures and at least one routing table cannot be updated successfully, then, an error signal is asserted to notify the software layer that the NoC cannot be utilized with the current application mapping. The support for minimal adaptivity will incur hardware overhead, which translates in turn into area overhead. To estimate the area overhead due to routing controllers, we have implemented in Verilog and synthesized using Xilinx ISE [47] routers for NoC topologies with sizes 2×2, 3×3, and 4 × 4. The router architecture is similar to that in [1]. It has ﬁve input and ﬁve output ports, two virtual channels, roundrobin based arbiters, input buﬀer size of ten, and packets with ﬁve ﬂits for links with 32 bits wide. In all cases, the area overhead is less than 5%. 5.4 Discussion of Limitation 5.4.1 Deadlock and livelock Routing algorithms for NoCs are desired to be free from deadlock and livelock [48]. The initial routing paths of the mapping solution achieved by the proposed algorithm are computed by XY or west-ﬁrst and odd-even routing algorithms [40], which guarantee the deadlock-free property. However, after the ﬁrst link failure occurs and paths are updated as monotonic XY-YX routing paths, the deadlock-free property cannot be guaranteed. On the other hand, monotonic XY-YX routing has the advantage of simple hardware implementation. 2The actual failure and diagnosis techniques are outside the scope of this paper. Such topics have been extensively studied in other works. Deadlock could be guaranteed if the routing tables update were done by specialized adaptive routing algorithms [49]. Alternatively, the probability of deadlock could be minimized by employing techniques for routing paths recalculation [23] or partial deadlock removal [50]. However, the implementation of such adaptive routing algorithms and techniques is more challenging and will require higher area penalty. If such specialized routing algorithms are employed, the proposed mapping algorithm can still be utilized − only the reliability estimation procedure must be changed (i.e., computation of spanning trees) to account for the particular type of routing paths. 5.4.2 Bandwidth constraints The initial mapping as found by the proposed mapping algorithm is under bandwidth constraints (see equation 8). Once link failures start to occur and routing tables get updated, the bandwidth constraints may be violated. We assume that in such situations, the whole NoC based SoC will be clocked at a lower frequency, thereby decreasing network performance. Operation with lower performance may still be desirable than no operation at all. Alternatively, this issue could be addressed by re-running the mapping algorithm or only its routing paths calculation as a software executable on the processing element of a central manager tile. In this way, the new routing paths could be found such that all bandwidth constraints are satisﬁed. However, there is no guarantee for that (failed links will ultimately demand decrease in performance) and the complexity of such an approach is higher. 5.5 Future Work It would be interesting to study the combined eﬀect of the proposed mapping algorithm and repairable links on reliability. If links are designed with redundant spare wires, then, several initial link failures could be addressed by link self-repair. After that, additional link failures could be addressed by the proposed routing tables update. Moreover, one could formulate other interesting reliability problems. For example, one can ask for mapping with minimization of the power consumption under reliability constraints (e.g., reliability higher than 0.99). Also, one can ask to ﬁnd the most reliable mapping within certain power consumption envelope. As already discussed, one could replace equation 4 with calls to the NetlifeSpectrum NoC routine from Section 4. However, the computational runtime of the mapping algorithm will increase because equation 6 must be evaluated many times inside the branch-and-bound algorithm. An investigation of the tradeoﬀ between the increase in runtime and the improvement in reliability is left for future work. 6. CONCLUSION We proposed a multi-ob jective branch-and-bound based mapping algorithm for regular NoCs. The main advantages of the proposed algorithm are: (i) it provides a tunable tradeoﬀ between the ob jectives of energy consumption and reliability via the parameter α; it was demonstrated that reliability can be improved without sacriﬁcing much of energy consumption and (ii) it is very eﬃcient compared to a simulated annealing based algorithm. Based on Verilog implementations of routers for several NoC sizes, it has been found that the hardware overhead to support minimal adaptivity for routing tables update is less than 5%. 7. "
Two-hop Free-space based optical interconnects for chip multiprocessors.,"Many resources are shared among the cores of chip-multiprocessors (CMPs), in particular on-chip caches and memory systems. Efficient intra-chip communication is necessary for efficient resource sharing and the performance of such systems, especially in future CMPs with hundreds or thousands of cores. Current Free-space optical networks-on-chip (NoCs) provide the potential to avoid the reduced wire performance and degraded signal integrity facing electronic networks. However, current proposals utilize fixed direction lasers and mirrors to realize one-hop all-to-all connectivity, which results in difficulties scaling to larger numbers of processors. In this paper we present two-hop optical strategies that provide better performance over the one-hop strategy while improving on both the required resources and scalability for future large scale CMPs.","Two-Hop Free-Space Based Optical Interconnects for Chip Multiprocessors∗ Ahmed Abousamra, Rami Melhem University of Pittsburgh Computer Science Depar tment {abousamra, melhem}@cs.pitt.edu Alex Jones University of Pittsburgh Electrical and Computer Engineering Depar tment akjones@ece.pitt.edu ABSTRACT Many resources are shared among the cores of chip-multiprocessors (CMPs), in particular on-chip caches and memory systems. Efﬁcient intra-chip communication is necessary for efﬁcient resource sharing and the performance of such systems, especially in future CMPs with hundreds or thousands of cores. Current Free-space optical networks-on-chip (NoCs) provide the potential to avoid the reduced wire performance and degraded signal integrity facing electronic networks. However, current proposals utilize ﬁxed direction lasers and mirrors to realize one-hop all-to-all connectivity, which results in difﬁculties scaling to larger numbers of processors. In this paper we present two-hop optical strategies that provide better performance over the one-hop strategy while improving on both the required resources and scalability for future large scale CMPs. Categories and Subject Descriptors C.2.1 [Computer-communication Networks]: Network Architecture and Design; C.1.4 [Processor Architectures]: Parallel Architectures General Terms Design, Performance, Scalability Keywords Network-on-chip, NoC, free-space, optical interconnect, multicolor 1. INTRODUCTION A crucial factor to improving the performance and scalability of chip-multiprocessors (CMPs) is reducing data access latency. Efﬁcient networks-on-chip are essential for the operation of CMPs to achieve good performance since they carry the cache coherence and data trafﬁc. In the ideal case, any two communicating nodes should be one-hop away from each other. In a system of N nodes, this one-hop all-to-all connectivity requires the realization of O(N 2 ) links, which is prohibitively expensive if done using electronics. ∗ This work is supported, in part, by NSF award CCF-0702452. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS’11, May 01-04 2011, Pittsburgh, PA, USA Copyright 2011 ACM 978-1-4503-0720-8/11/05 ...$10.00. However, newly developed or emerging devices, circuits, and optics technology can be used to realize one-hop links through freespace optical links [20]: VCSELs (vertical cavity surface emitting lasers), PDs (Photodetectors), and collimating microlenses can be implemented in GaAs technologies and combined with a silicon chip, in which the transmitter and receive electronics reside, using 3D integration. The light, guided by micro-mirrors, propagates through free-space to achieve end-to-end direct communication. Realizing all-to-all one-hop links may be achieved through dedicated ﬁxed direction lasers for each destination, or in the future, by utilizing the beamsteering capability of an optical phase array (OPA) of lasers [20]. OPAs have the advantage of keeping the number of lasers constant per node, which would be crucial for scalability to large systems. However, for CMPs, OPAs are still many years from adoption even beyond free-space optics for networkson-chip (NoCs). One of the challenges facing OPA is variations in the fabrication process, which would require a post fabrication step to adjust the phase modulators of each fabricated chip - a costly process. Fixed direction lasers, on the other hand, are a more feasible alternative, but pose an interesting challenge when considering scalability to large CMPs. In this paper we propose 2-hop freespace optical based NoCs to address this scalability challenge. We extensively evaluate the performance, energy consumption, and required resources of our proposed NoCs and compare them with the 1-hop free-space optical NoC [20] using benchmark simulations and synthetic traces, for a variety of CMP sizes, link bandwidths, network packet sizes, and network loads. The rest of the paper is organized as follows: Section 2 describes the baseline one-hop all-to-all optical NoC. Sections 3 and 4 introduce our proposed two 2-hop optical NoCs. Evaluation is presented in Section 6. Related work is included in Section 5. Section 7 concludes the paper. 2. BACKGROUND: A 1-HOP FREE-SPACE OPTICAL NOC We describe brieﬂy in this section the baseline 1-hop free-space optical NoC proposed in [20]. VCSELs and PDs operate at 40 GHz such that during a 3.3 GHz processor cycle, one VCSEL can transmit 12 bits of data. Network packets are classiﬁed into meta and data packets. Meta packets represent coherence messages such as data requests, invalidations, and acknowledgments, while data packets correspond to the actual data of cache lines. Assuming a ﬂit size of at least 64 bits, meta packets are one ﬂit long, while the size of data packets varies based on the cache line size or the amount of data that is transmitted. For example, in [20], data packets are 5 ﬂits, each carrying 32 bytes of data (a cache line). Assuming a system of N nodes and a link width of k VCSELs, each node needs k(N − 1) VCSELs to have a 1-hop communication with all nodes, thus the number of VCSELs per node scales linearly with the number of nodes in the system. However, shared rather than dedicated receivers are used. Consequently, each node has only a constant k PDs to receive from all other nodes regardless of N . Unfortunately, this results in collisions at the receiver that must be detected and, if possible, avoided. Reducing Probability of Collisions: For packets that need multiple cycles to be transmitted, slotting [14] and lane separation are two techniques for reducing collision probability. For example, transmission of a 5-ﬂit packet can only start at the beginning of a 5-cycle slot. Since meta and data packets differ in length, packet transmission is separated into a meta lane of width kM VCSELs and a data lane of width kD VCSELs, respectively. Collision probability is further reduced through doubling the receiving bandwidth per lane such that half the N − 1 nodes send packets to one receiver and the other half sends to the other. Detecting and avoiding collisions: The id of the sending node, denoted by P I D, along with its complement P I D are encoded into the ﬁrst ﬂit of each packet. Due to slotting, the ﬁrst ﬂit of a packet arrives on the ﬁrst cycle of the transmission slot. The receiver sends a conﬁrmation signal (one bit) to the sender in the case of successful transmission, i.e., no collisions, at cycle c + 2, where c is the start cycle of the slot. If a collision occurs, this is detected as at least one of the P I D or P I D bits will be ﬂipped (light from one ﬂit will override darkness from a colliding ﬂit) and they would not match and no conﬁrmation is sent. The absence of a conﬁrmation signal indicates that a collision occurred at the receiver and the senders must each retransmit the packet. An exponential back-off heuristic is used in [20]. 3. A 2-HOP FREE-SPACE OPTICAL NOC We describe in detail our proposed 2-hop free-space optical NoC. This interconnect connects the data-lane links for any two nodes using at most 2 optical links. To offset the delay introduced by the extra optical link—compared to the 1-hop optical NoC in section 2—we need to at least double the link bandwidth. Increasing the link bandwidth beneﬁts data packets, which consist of multiple ﬂits. For single-ﬂit meta packets, we retain the 1-hop all-to-all optical connectivity. √ Topology and Routing: Assuming the N nodes are laid out in a rectangular n × m grid, each node ix,y that lies on row y and column x of the grid, is directly connected though an optical link to each other node located on the same row, y , or the same column, x, as depicted in Fig. 1(a). Thus, we trade-off the number of links with increased link bandwidth. For square grids the number of data N −1, compared packet transmitters per node scales linearly with to N − 1 per-node for the all-to-all network, leading to savings in the number of transmitters as the number of nodes increases. In general, the savings depend on the values of n and m, where N = n × m as evident by the formulas for the number of data VCSELs per node shown in Table 1. Data packets are routed using the X-Y routing algorithm where a packet is ﬁrst transmitted in the X-direction (i.e., horizontally) and then the Y-direction (i.e., vertically). For example, to send a packet from node ix,y to node ix(cid:2) ,y (cid:2) , where x (cid:3)= x(cid:3) and y (cid:3)= y (cid:3) , the packet is ﬁrst transmitted to the intermediate node ix(cid:2) ,y and then to the ﬁnal destination, ix(cid:2) ,y (cid:2) . Of course if the source and destination nodes are located on either the same row or the same column, then transmission occurs only in the X- or the Y-direction, respectively. Collisions are possible in this interconnect and we use the same mechanisms employed by the 1-hop optical NoC to reduce collision probability, detect collisions and handle retransmissions of packets. Table 1: Comparing the data VCSELS of the 1-hop and 2-hop optical NoCs n × m NoC Number of VCSELs per node 12 × 12 NoC 143 kD 44 kD 55 kD 1-hop optical NoC (baseline) 2-hop optical NoC with 2x the baseline link bandwidth 2-hop optical NoC with 2.5x the baseline link bandwidth kD (n × m − 1) 2kD (n + m − 2) 2.5kD (n + m − 2) (a) Optical links connect each node to other nodes in the same row and column. (b) Deadlock example with careless receiver allocation. Figure 1: The 2-hop optical NoC. Preventing Deadlock: Consider the example in Fig. 1(b): node A wants to send packet m1 to node C , and node D wants to send packet m2 to node B . If B and C lie on the same column, while A lies on the same row as B , and D lies on the same row as C then according to the X-Y routing policy, A should send m1 to B which then relays it to C . Similarly D should send m2 to C which relays it to B . If A and D successfully send to B and C , respectively and the receiver buffers RB and RC storing m1 and m2 in B and C , respectively, do not have free space to receive other packets a deadlock forms. RC cannot accept m1 which remains in RB , thus making RB unable to accept m2 , which in turn remains in RC . To prevent deadlock, we take advantage of the two sets of PD receivers from [20] dedicating the ﬁrst for X-direction trafﬁc and the second for Y-direction trafﬁc. Speciﬁcally, let RDX and RDY denote the two sets of kD PDs for receiving X and Y-direction trafﬁc, respectively. Thus, only the RDX receiver buffer may send a packet to an RDY receiver, but not vice versa since packets received by RDY are always delivered to the local node, which prevents deadlock. Router Design: The router design is very similar to that of the baseline 1-hop optical NoC described in section 2, with the additional support for relaying packets received by intermediate nodes to their intended recipients. The router has two input buffers, one for data and one for meta packets, and 4 output (receiver) buffers, where 2 are for data and 2 are for meta packets. We will focus on the data packets since the meta lane component is unchanged from [20]. In the data lane an arbiter is added between the local node data input buffer and the RDX output buffer used to buffer data ﬂits for retransmission in the Y-direction. Consider the diagram of the router shown in Fig. 2. The ﬂits of data packets injected by the local node are ﬁrst stored in the input buffer then transmitted as per the X-Y routing policy. The output buffers store the ﬂits Local  Node Data  i/p buffer Meta  i/p buffer Arbiter  Logic R DX R DY s t e k c a P a t a D Arbiter  Logic R ME R MO f o s r e t t i m s n a r T f o s r e t t i m s n a r T s t e k c a P a t e M Figure 2: Diagram showing the components of the router of the 2-hop optical NoC. received by the PDs of RDX and RDY . All packets received by RDY are delivered to the local node, while packets received by RDX may either be intended for the local node or require retransmission to its ﬁnal destination. Regarding the receivers of meta packets, we refer to the two sets of kM PDs as RM E and RM O (Fig. 2), where even numbered nodes send to RM E and odd numbered nodes send to RM O . Reducing Serialization Delay: Shifting the slot of data transmission in the Y-direction: Consider the common case where the data packet is transmitted in both the X and Y directions. When the head (ﬁrst) ﬂit of a data packet is received by RDX , the router examines it to decide whether to deliver the packet to the local node or transmit it to the ﬁnal destination node. If the transmission slots for sending in both the X and Y directions coincide in time, a packet sent in the X-direction at slot sj , cannot be sent in the Y-direction during the same slot since the router cannot simultaneously receive and re-transmit the same ﬂit. As the example in Fig. 3 shows, an L-ﬂit packet needs 2L cycles to reach its destination (assuming no queuing, contention or collision delays) as follows: 1 (head ﬂit traverses optical link to intermediate node) + (L − 1) (the remaining ﬂits of the packet traverse the ﬁrst optical link while the head ﬂit remains at the intermediate node waiting for the next transmission slot) + 1 (head ﬂit is transmitted to the destination node) + (L − 1) (the remaining ﬂits follow the head ﬂit) = 2L cycles. Figure 3: Effect of shifting the Y-direction slot on the latency of a 5-ﬂit packet. However, if the Y-direction transmission slots lag the X-direction slots by 2 cycles—to allow the router to examine the ﬂit and copy it to the transmission buffer if there is a need - the packet can reach its destination in only L + 2 cycles (see the example in Fig. 3) as follows: 1 (head ﬂit traverses optical link to intermediate node) + 2 (head ﬂit is examined at the intermediate node and transmitted to the destination node) + (L − 1) (the remaining packet ﬂits follow the head ﬂit) = L + 2 cycles, also assuming no queuing, contention or collision delays. Table 2 lists the formulas for the least (best) packet latencies in each NoC as a function of packet size in ﬂits, L. Table 2: Comparing best data packet latency of the 1-hop and 2-hop optical NoCs 1-hop optical NoC (baseline) 2-hop optical NoC with 2x the baseline link bandwidth 2-hop optical NoC with 2.5x the baseline link bandwidth Packet Latency L L 2 + 2 L = 10 10 cycles 7 cycles L 2.5 + 2 6 cycles 4. A MULTICOLOR 2-HOP NOC We now describe our proposed multicolor 2-hop NoC. Nodes are partitioned into C groups, where each group of nodes is conceptually considered to have a distinct color. We classify links into intracolor and inter-color links. As their names suggest, an intra-color link connects two nodes of the same color, while an inter-color link connects two nodes of two different colors. We achieve a maximum of 2-hop communication between any two nodes by having: 1) allto-all intra-color links connecting the nodes of the same color, and 2) each node of color c is connected by an inter-color link to exactly one node of each color c(cid:3) (cid:3)= c. Thus, any two nodes of the same color and any two nodes that are connected by an inter-color link can communicate in 1-hop. Consider the communication between any two nodes ic of color c, and node jc(cid:2) of color c(cid:3) (cid:3)= c, which are not directly connected by an inter-color link. Assume ic wants to send a data packet, m, to jc(cid:2) . Node ic ﬁrst sends m over an intra-color link to the node kc , which is also of color c, and which is connected to jc(cid:2) through an inter-color link. Second, kc sends m to jc(cid:2) over their connecting inter-color link. In the absence of delays due to queuing or collisions, an L-ﬂit packet traveling on a 2-hop path needs L + 2 cycles to reach its destination as follows: 1 (head ﬂit traverses intra-color link to intermediate node) + 2 (head ﬂit is examined at the intermediate node and transmitted to the destination node over the inter-color link) + (L − 1) (the remaining packet ﬂits follow the head ﬂit) = L + 2 cycles. Similar to the 2-hop optical NoC described in section 3, we use at least double the link bandwidth of the baseline 1-hop NoC. With free-space optical communication collisions can occur. Similar to the 1-hop NoC, we use slotting and lane separation between data and meta packets (for meta packets we retain the all-to-all 1-hop connectivity among all the N nodes). We also double the receiving bandwidth to reduce collisions, and use the same mechanisms of collision detection and exponential back-off for retransmitting collided packets. We now compute the number of data transmitters (VCSELs) required per node for a C-color 2-hop optical NoC. Let Ni be the set of nodes of color i, where i ∈ S = {0, ..., C − 1} and S where |Ni | is the cardinality of Ni . If N is divisible by C , then is the set of C colors. By design, we have ∀i ∈ S (the formulas in Table 3 assume that N is divisible by C ). In general the number of required VCSELs for a node |Ni | = N ∈ Ni is: xkD (Ni − 1 + C − 1) = xkD (Ni + C − 2), where kD (cid:2)C−1 i=0 |Ni | = N , C             is the number of VCSELs per link in the baseline 1-hop NoC, and xkD is the number of VCSELs per link in this C-color 2-hop NoC. Table 3: Comparing data VCSELs of C-color 2-hop NoCs n × m NoC Number of VCSELs per node 2kD ( n×m + C − 2) 2.5kD ( n×m + C − 2) C C-color 2-hop NoC with 2x the baseline link bandwidth C-color 2-hop NoC with 2.5x the baseline link bandwidth Example: 12 × 12 NoC kD = 6 3-color 2-hop NoC with 2x the baseline link bandwidth 4-color 2-hop NoC with 2x the baseline link bandwidth 4-color 2-hop NoC with 2.5x the baseline link bandwidth C Number of VCSELs per node 588 456 570 Deadlock-Free Routing: Because this is a 2-hop scheme like the X-Y scheme, deadlock lock can also occur unless we separate the receivers as described above in Section 3. Table 4: Power Parameters Dynamic Power Optical Transmitter (laser driver and VCSEL) [20] Photodetector [20] 1-hop electronic wire (1-bit) [19] 7.2 mW 4.2 mW 0.7 mW Standby Power 0.43 mW 0.43 mW 2.89 µW 4.1 Example 3-Color 2-hop NoC: As an example, we describe the topology and router design for a 3-color 2-hop NoC. We assume the N nodes are laid out in a rectangular n × m grid. We partition the nodes into 3 subsets of c ∈ S = {0, 1, 2} is directly connected through an intra-color almost equal size (subset sizes may differ by 1). Each node of color optical link to each other node of the same color, c. We can color the nodes such that the inter-color links run only between adjacent nodes. Although we have described the architecture in terms of optical inter-color links, the locality of these links actually allow them to be more efﬁciently implemented in electronic, rather than optics. Such an implementation would reduce power as shown by Table 4. In the rest of this paper, we will only discuss multicolor architectures with electronic inter-color links. Fig. 4(a) shows an example of the node coloring scheme: starting with the top left node, visit the nodes in a snake like fashion, i.e., visit the nodes of the top row from left to right, then visit the nodes of the second top row from right to left, and so on. Color the ﬁrst visited node 0. Set the color of the next visited node, ci+1 , to (ci + 1) mod 3, where ci is the color of the last visited node. Electronic links are laid in a snake like fashion as in Fig. 4(a). We add two extra nodes, ih , it , one adjacent to the head node of the snake (top left node) and the other adjacent to the tail node of the snake, respectively. These extra nodes have optical receivers but no transmitters, and each is connected through an electronic link to the adjacent node at the snake’s end (see Fig. 4(a)). ih is colored 2, while it is colored, (cl + 1) mod 3, where cl is the color of the tail node of the snake. ih and it only act as intermediate nodes in 2-hop paths; they ensure the head and tail nodes of the snake are each connected to two nodes of the other two colors. The ih and it nodes are not needed in the special case where there is an even number of rows and the number of columns is divisible by 3, e.g., a 6x6 CMP (see Fig. 4(b)). Instead of the snake topology, we consider the rows of nodes in pairs starting from the top, and use an electronic ring to connect the nodes of each pair of rows. i h i t (a) Electronic links form a snake. (b) Electronic links form rings. Figure 4: 3-Color 2-hop NoC. Local  Node Data  i/p buffer Meta  i/p buffer Transmitters of  Data Packets R Left electronic  input port ME Arbiter  Logic Transmitters of  Meta Packets R MO Right electronic  input port R DE R DO Left electronic  output port p p Arbiter Arbiter  Logic Right electronic  output port p p Figure 5: Diagram showing the components of the router of the 3-color 2-hop NoC. Router Design: This router has two additional buffers compared to the 2-hop NoC (section 3) or the baseline 1-hop NoC for storing the ﬂits received over the two electronic links. Because the buffers used for the electronic links are different from the buffers used for the optical links deadlock cannot occur. Since we have 2 sets of optical data receivers, we partition them into RDE and RDO , where even numbered nodes send to RDE and odd numbered nodes send to RDO . For each packet, m, received by either RDE or RDO , the router decides whether to deliver m to the local node or transmit it over an electronic link to one of the two neighboring nodes. Note that the router arbitrates between the packets in the buffers of RDE and RDO and the data input buffer, as more than one data packet may need to go over the same electronic link. The delivery to the local node considers the packets received through the two electronic links in addition to the RDE , RDO , RM E , and RM O receivers. Fig. 5 shows a diagram of the router design. 5. RELATED WORK Optical interconnections have been extensively studied for multiprocessor systems (see for example [4, 9, 12, 13]) and advances in nanophotonics [10, 15] have motivated many recent optical NoC proposals. Shacham et al. [16] propose a hybrid waveguide-based optical NoC architecture with a complementary electronic NoC for control messages. Kurian et al. [8] propose a hierarchical interconnection with a global optical (bus-like) network that utilizes wavelength division multiplexing (WDM) to avoid contention. In [11], Morris Jr. and Kodi propose a multilevel nanophotonic that combines WDM, space division multiplexing (SDM), optical tokens, and nanophotonic crossbars to develop a two-hop network for CMPs. Zhang and Louri [22] propose a multilayer photonic network-onchip, which leverages 3D integration to provide a global crossbarlike connectivity using layers of waveguides. 6. EVALUATION In this section we compare our proposed 2-hop optical NoC (2hop) and multicolor 2-hop NoC (3-color) from sections 3 and 4, respectively, with the baseline 1-hop optical NoC (1-hop). 6.1 Evaluation Methodology To explore the NoC design space we consider several parameters including: a. Size of CMP ranging from 8x8 to 12x12. b. The ratio of the link bandwidth of the 2-hop and 3-color NoCs relative to the link bandwidth of the 1-hop NoC, denoted BW ; we consider BW = 2, 2.5 and 3.33. c. Different sizes of data packets which correspond to cache line sizes of 32, 64, and 128 bytes. d. Various communication-to-computation (C2C) ratios. Due to this relatively large design space, we take a 2-step approach to evaluation. First, we explore the design space with synthetic traces. Synthetic traces allow us to both keep the simulation time manageable, and easily vary the C2C ratio. In our traces each node injects 20,000 data requests to random destinations. The receiver of a data request replies with a data packet to the requester. We evaluate with the following request injection rates, r = 0.5, 1, 3, 5, 10 requests per 100 cycles. Second, we use benchmarks from the SPLASH2 [18] and PARSEC [1] suites to more rigorously evaluate a smaller subset of the design space. We use the functional simulator Simics [17] to simulate running these benchmarks. To keep the simulation time manageable, we simulate a CMP of size 8x8. Each core has a private L1 I/D cache and a slice of the distributed shared L2 cache [6]. The CMP and cache parameters are listed in Table 5. Table 5: System Conﬁguration CMP conﬁguration Cores 64, 2-way in-order. 45 nm, Freq.: 3.3 GHz L1 I/D cache (private) 32KB, 2-way, 64B line, 2 cycles, 2 ports L2 cache (shared) 1MB per core, 16-way, 64B line, 8 cycles Memory Latency 250 cycles Buffers for the 1-hop, 2-hop, and 3-color NoCs Input port buffer 4 packets Receiver buffer 2 packets Electronic port buffer 1 packet We base our network parameters on the ones used for the evaluation of the baseline 1-hop NoC [20]. In [20], a meta packet is 72 bits and a data packet is 360 bits. The width of the meta lane is kM = 3 VCSELs, which transmits a meta packet in 2-cycles. The width of the data lane is kD = 6 VCSELs, which transmits a 5-ﬂit data packet in 5-cycles. Since a 5-ﬂit data packet carries the data of a 32-byte cache line [20], we use 10- and 20-ﬂit data packets for transferring the contents of 64-byte and 128-byte cache lines, respectively. For the 2-hop and 3-color NoCs, the size of data ﬂits depends on the link bandwidth. For example, consider a 5-ﬂit packet in the baseline 1-hop NoC. With BW = 2.5 for the 2-hop and 3-color NoCs, we have kD = 15 VCSELs and data ﬂits become 180 bits; making data packets 2-ﬂits long and requiring 2-cycles to transmit. Table 5 lists the buffer sizes of the network routers. We evaluate the average data packet latency, execution time, and overall energy consumed in buffers and data transfer across links (i.e., link energy) (We use the power parameters in table 4 for computing link-energy). Our proposed network conﬁgurations impact link and buffer energy in interesting ways: 1. Compared to the baseline 1-hop NoC, our networks reduce the overall number of VCSELs (Table 1) which contributes to reducing total link energy. 2. However, wider links require more PDs, thus potentially having the opposite effect of increasing total link energy. 3. Execution time also affects energy consumption; in most cases, as we show below, we observe reduced execution time, which contributes to reducing the overall link energy. 4. Similarly, the network topology impacts the utilization of router buffers: the more available network links, the more one-hop paths in the network, the less time packets spend in router buffers, and vice versa. Note that we use the optimization of shifting the Y-direction transmission slot by 2-cycles with the 2-hop NoC throughout the results—provided the transmission slot is more than 2 cycles1— since we found that it consistently improves this NoC’s performance. 6.2 Impact of Different Cache Line Sizes and Different Request Injection Rates In this evaluation we ﬁxed the CMP size to 10x10. However, in the following subsection we present a scalability study on a 8x8, 10x10, and 12x12 CMPs. Note that for BW = 2 and 3.33, we do not simulate 2-hop and 3-color systems with 32-byte cache lines since in these cases the data packet is not divisible into an integral number of ﬂits. Network Latency: As can be seen in Fig. 6, with increased injection rates, the average packet latency increases more quickly with the 2-hop NoC compared to the 3-color NoC. Increased injection rates are accompanied by increased packet collisions and retransmissions. However, the additional links connecting pairs of nodes directly in the 3-color NoC, compared to the 2-hop NoC, provide two advantages: 1) Reduced probability of collision, and 2) Higher probability of packets traversing only one link to reach their destinations. Speedup: Speedup is computed based on execution time or the time until the delivery of the last packet. Lower network latency contributes to faster execution. The network latency’s contribution to the execution time is limited by the C2C ratio. Low injection rates mean low C2C, resulting in only modest speedups, as can be seen in Fig. 7. Conversely, high C2C ratios increase the likelihood of collisions, causing retransmissions and possibly increasing network delays. This is why the speedup of the 2-hop NoC peaks with r < 10 for BW = 2 and 2.5, while the wider links of BW = 3.33 allow the speedup of the 2-hop to continue to improve even with r = 10. On the other hand, the 3-color NoC with its more direct links is able to continue to show greater speedups with even larger cache lines and injection rates for all considered link bandwidths. Link and Buffer Energy: In almost all conﬁgurations, the 2hop network consumes less total link-energy than both the 3-color and the 1-hop NoCs. In Fig. 8 we show the normalized link and 1For 32 byte cache lines and BW = 2.5, the data slot size is 2 cycles on the 2-hop and 3-color NoCs. 120% 120% 100% 100% 80% 80% 60% 60% 40% 40% 20% 20% 0% 0% 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r BW = 2 BW = 2 BW = 2.5 BW = 2.5 BW = 3.33 BW = 3.33 z z A A d d e e e e v v r r e e k k c c a a P P e e g g a a t t a a L L t t n n e e y y c c a a e e R l l t t i i e e v v t t 1 1 o o p p o o h h N N C C o o r = 0.5 r = 1 r = 3 r = 5 r = 10 r = 0.5 r = 1 r = 3 r = 5 r = 10 r = 0.5 r = 1 r = 3 r = 5 r = 10 Line size = 32 bytes Line size = 64 bytes Line size = 128 bytes N o r m a i l Figure 6: Normalized average data packet latency relative to the 1-hop NoC on a 10x10 CMP for different cache line sizes, link bandwidths, and request injection rates 2 2 1.8 1.8 1.6 1.6 1.4 1.4 1.2 1.2 1 1 0.8 0.8 0.6 0.6 0.4 0.4 0.2 0.2 0 0 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r BW = 2 BW = 2 BW = 2.5 BW = 2.5 BW = 3.33 BW = 3.33 a a e e R R p p u u d e e p S l l t t i i e e v v t t 1 1 o o p p o o h h N N C C o o r = 0.5 r = 1 r = 3 r = 5 r = 10 r = 0.5 r = 1 r = 3 r = 5 r = 10 r = 0.5 r = 1 r = 3 r = 5 r = 10 Line size = 32 bytes Line size = 64 bytes Line size = 128 bytes Figure 7: Speedup relative to the 1-hop NoC on a 10x10 CMP for different cache line sizes, link bandwidths, and request injection rates buffer energy consumption for B = 2.5. The 2-hop NoC achieves link-energy savings of 19%-28%, over the 1-hop NoC. The 3-color NoC consumes more link-energy than the the 2-hop NoC due to having more links. However, for r = 10 and a 128-bit cache line, the situation is reversed: the 3-color NoC consumes the least linkenergy. This conﬁguration results in a high C2C ratio that causes more collisions and retransmissions on the 2-color NoC. For low values of r , the 3-hop NoC consumes similar or a little more energy than the baseline 1-hop NoC. With respect to buffer energy, the 3-color NoC consistently consumes the least energy, since packets spend little time in the network buffers thanks to the availability of many wide direct links. Conversely, buffer utilization is higher in the 2-hop NoC. We note in Fig. 8, that for 32-byte cache lines, the 2-hop NoC consumes even more buffer-energy than the 1-hop NoC. In the conﬁgurations using 32-byte cache lines, the reduction in average packet latency (Fig. 6) is not enough to compensate for the energy consumed by the wider buffer slots (compared to the 1-hop NoC). 6.3 Scalability Area Requirements: We start by comparing the area requirements of the 2-hop and 3-color NoCs to that of the 1-hop NoC. Table 6 lists the area savings of the optical transmitters and receivers of the 2-hop and 3-color NoCs for the CMP sizes and link bandwidths we consider. For each pair of BW and CMP size, the table lists three values of area savings for three different PD sizes: 1x if constructed with a modiﬁed VCSEL with similar size [7] or a photodiode with areas stated as around 5-10x larger than a VCSEL [2, 21]. The area savings increase with the number of cores, since the network resources of the 2-hop and 3-color NoCs scale at a lower rate than the resources of the 1-hop NoC. Notice that for BW = 3.33, the 3-color NoC needs a signiﬁcant amount of additional, and thus is excluded from further evaluations. Table 6: Percentage of network area savings relative to 1-hop NoC assuming the area of a PD is 1x, 5x, and 10x the area of a VCSEL, according to technology. Area savings for a 2-hop NoC BW 8x8 CMP 10x10 CMP 12x12 CMP 1x 5x 10x 1x 5x 10x 1x 5x 10x 2 35% 29% 23% 41% 37% 32% 45% 42% 39% 2.5 28% 20% 12% 35% 30% 24% 40% 36% 32% 3.33 15% 5% -6% 24% 18% 10% 31% 26% 20% Area savings for a 3-color NoC 2 22% 17% 11% 22% 19% 15% 22% 20% 17% 2.5 11% 5% -3% 11% 7% 2% 11% 8% 5% 3.33 -7% -16% -26% -7% -13% -20% -7% -11% -16% For the remainder of the scalability study we choose the following subset of parameters: We use conﬁgurations with 64-byte cache lines since it is the most common on modern processors, for example [3, 5], BW = 2, 2.5, and r = 1, 5 to represent both low and high request injection rates. Network Latency: The 3 NoCs experience small increases in average data packet latency with larger CMPs. The 2-hop NoC experiences a greater increase in packet latency than the 3-color NoC as can be seen in Fig. 9, due to its fewer overall links. Comparing the normalized average packet latency on the 8x8 and 12x12 CMPs, we observed an increase of about 3.5% and 0.5% with the 2-hop and 3-color NoCs, respectively, for both BW = 2, 2.5. Speedup: For BW = 2.5, we observed that the speedups of the different systems remain almost the same for the 3 CMP sizes.                                                                                                                                                                     140% 140% 120% 120% 100% 100% 80% 80% 60% 60% 40% 40% 20% 20% 0% 0% 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r 2 2 p p o o h h 3 3 c c o o o o l l r r Normalized Link Energy Normalized Link Energy Normalized Dynamic Buffer Energy Normalized Dynamic Buffer Energy r = 0.5 r = 1 r = 3 r = 5 r = 10 r = 0.5 r = 1 r = 3 r = 5 r = 10 r = 0.5 r = 1 r = 3 r = 5 r = 10 Line size = 32 bytes Line size = 64 bytes Line size = 128 bytes Figure 8: Normalized overall link and dynamic buffer energy relative to the 1-hop NoC on a 10x10 CMP for different cache line sizes, link bandwidths, and request injection rates With r = 1, we observed speedups of 6.3% and 6.9% on the systems with the the 2-hop and 3-color NoCs, respectively. While for r = 5, we observed speedups of 16% and 24% on the systems with the the 2-hop and 3-color NoCs, respectively; the higher speedups are due to the higher C2C ratio. A similar observation is true for BW = 2 and r = 1. For BW = 2 and r = 5, the speedup of the 2-hop NoC decreases with larger CMPs, while the speedup of the 3-color NoC remains unchanged for all 3 CMPs. 0% 0% 20% 20% 40% 40% 60% 60% 80% 80% 100% 100% p p o o h h o o o o c c l l r r p p o o h h o o o o c c l l r r p p o o h h o o o o c c l l r r p p o o h h o o o o c c l l r r p p o o h h o o o o c c l l r r p p o o h h o o o o c c l l r r BW = 2 BW = 2 BW = 2.5 BW = 2.5 g g v v . . e e k k c c a a P P t t a a L L t t a a e e R R y y c c n n e e l l i i t t e e v v t t o o 1 1 p p o o h h N N C C o o 2 3 2 3 2 3 2 3 2 3 2 3 r = 1 r = 5 r = 1 r = 5 r = 1 r = 5 8x8 10x10 12x12 N o r m a i l A d e z Figure 9: Scalability Study: Normalized average packet latency relative to the 1-hop NoC Link and Buffer Energy: With larger numbers of cores, more energy savings are achieved by the 2-hop and 3-color NoCs relative to the corresponding 1-hop NoC. In Fig. 11 we plot the normalized link and buffer energy consumption for BW = 2.5. We observed a 20% and 15.6% additional savings in link-energy by the 2-hop and 3-color NoCs on the 12x12 CMP compared to their energy savings on the 8x8 CMP, while their respective buffer-energy consumption increased by 5.2% and 0.9% relative to the 1-hop NoC. As mentioned above, the network resources of the 2-hop and 3-color NoCs scale with increased number of cores at a lower rate than the resources of the 1-hop NoC, which allowed more link-energy savings, but increased the utilization of the network buffers. 6.4 Evaluating with benchmarks After exploring the impact of the different design parameters above, we evaluate the 2-hop and 3-color networks on a 8x8 CMP whose parameters are given in Table 5. We evaluate the 2-hop and 3-color NoCs with BW = 2 and 2.5. Speedup: For BW = 2, the geometric mean of the speedup of the 2-hop and 3-color NoCs is 7.6% and 8.4%, respectively, compared to the 1-hop NoC, while with BW = 2.5, their respective speedups are 8.4% and 9.2%. Link and Buffer Energy: As shown in Figs. 12 and 13, for BW = 2, the 2-hop and 3-color NoCs consume on average 63.3% and 75.2% of the total link-energy of the 1-hop NoC, respectively, and consume on average 85.7% and 86.1% of total buffer-energy of the 1-hop NoC, respectively. While for BW = 2.5, the 2-hop and 3-color NoCs consume on average 68.6% and 83.4% of the total link energy of the 1-hop NoC, respectively, and consume on average 72.4% and 54% of total buffer-energy of the 1-hop NoC, respectively. These results show that for BW = 2, there is little difference between the 2-hop and 3-color conﬁgurations with respect to performance and the network energy, thus making the 2-hop color more attractive since it is simpler and requires less area to realize. For BW = 2.5, the performance of both the 2-hop and 3-color systems improved by 0.8%. However the 3-color system reduced bufferenergy consumption considerably, but at the expense of additional area. 0.0 0.0 0.2 0.2 0.4 0.4 0.6 0.6 0.8 0.8 1.0 1.0 1.2 1.2 1.4 1.4 p p o o h h o o o o c c l l r r p p o o h h o o o o c c l l r r p p o o h h o o o o c c l l r r p p o o h h o o o o c c l l r r p p o o h h o o o o c c l l r r p p o o h h o o o o c c l l r r BW = 2 BW = 2 BW = 2.5 BW = 2.5 p p u d e e p S a a e e R R l l i i t t e e v v t t 1 1 o o p p o o h h N N C C o o 2 3 2 3 2 3 2 3 2 3 2 3 r = 1 r = 5 r = 1 r = 5 r = 1 r = 5 8x8 10x10 12x12 Figure 10: Scalability Study: Speedup relative to the 1-hop NoC 7. CONCLUSION With scaling of CMPs to hundreds and thousands of cores, scalable efﬁcient intra-chip communication becomes more critical to performance. The ideal one-hop, all-to-all connectivity that may be achieved through free-space optics comes at a high cost of hardware and energy resources. Through careful allocation of resources, we propose alternate 2-hop strategies. We explore the tradeoffs of performance, network latency, and resource usage through extensive evaluation with a variety of cache line sizes, link bandwidths, network loads, and different CMP sizes. We ﬁnd that our proposed 2-hop strategies are not only better scalable but also better performing compared to the 1-hop network. In our future work we plan to study and improve the scalability of both the data and narrow meta lanes.                                                                                                                                                   0% 0% 20% 20% 40% 40% 60% 60% 80% 80% 100% 100% 120% 120% p p o o h h o o o o c c l l r r p p o o h h o o o o c c l l r r p p o o h h o o o o c c l l r r p p o o h h o o o o c c l l r r p p o o h h o o o o c c l l r r p p o o h h o o o o c c l l r r Normalized Link Energy Normalized Link Energy Normalized Dynamic Buffer Energy Normalized Dynamic Buffer Energy 2 3 2 3 2 3 2 3 2 3 2 3 r = 1 r = 5 r = 1 r = 5 r = 1 r = 5 8x8 10x10 12x12 Figure 11: Scalability Study: Normalized link and dynamic buffer energy consumption relative to the 1-hop NoC (BW = 2.5) 0.0 0.0 0.2 0.2 0.4 0.4 0.6 0.6 0.8 0.8 2-hop, BW = 2 2-hop, BW = 2 1.0 1.0 a a b b r r e e n n s s l l u u o o n n a a e e c c o o d d a a i i s s i i t t y y y y a a r r t t e e c c a a a a t t e e r r _ _ s s p p y y d d o b r r t t k k c c a a n n a a d u i i m m a a t t e e p p a a t t i i n n o o s s o o e m m e e r r t t i i c c M M n n a a e e 2-hop, BW = 2.5 2-hop, BW = 2.5 3-color, BW = 2 3-color, BW = 2 3-color, BW = 2.5 3-color, BW = 2.5 m m a a i i l l z z d d e e L L i i e e n n E E k k n n r r n n o o C C y y g g s s u u m m p p t t i i n n o o a a e e R R l l t t i i e e v v t t 1 1 o o p p o o h h N N C C o o r r w l f i s w G N o r Figure 12: Normalized overall link energy consumption relative to the 1-hop NoC 8. "
HOPE - Hotspot congestion control for Clos network on chip.,"Hotspot congestion control is one of the most challenging issues when designing a high-throughput low-latency network on the chip (NOC). When a destination node is overloaded, it starts pushing back the packets destined for it, which in turns blocks the packets destined for other nodes. How to detect the occurrence(s) of hotspot and notify all source nodes to regulate their traffic to the hotspot node(s) can be quite complex because of potentially high volume of information to be collected and the non-negligible latency between the detection point of congestion and the source nodes. In this paper, we propose an effective end-to-end flow control scheme, called HOPE (HOtspot PrEvention), to resolve the hotspot congestion problem for the Clos network on the chip (CNOC). Specifically, HOPE regulates the injected traffic rate proactively by estimating the number of packets inside the switch network destined for each destination and applying a simple stop-and-go protocol to prevent hotspot traffic from jamming the internal links of the network. We evaluate HOPE's overall performance and the required hardware. Extensive simulation results based on both static and dynamic hotspot traffic patterns confirm that HOPE can effectively regulate hotspot flows and improve system performance. Our hardware analysis shows that HOPE has very small logic overhead.","HOPE: Hotspot Congestion Control for Clos Network On Chip Najla Alfaraj, Junjie Zhang, Yang Xu, and H. Jonathan Chao Department of Electrical and Computer Engineering Polytechnic Institute of New York University Brooklyn, NY 11201 {nalfar01,jzhang10}@students.poly.edu , {yangxu, chao}@poly.edu ABSTRACT Hotspot congestion control is one of the most challenging issues when designing a high-throughput low-latency network on the chip (NOC). When a destination node is overloaded, it starts pushing back the packets destined for it, which in turns blocks the packets destined for other nodes. How to detect the occurrence (s) of hotspot and notify all source nodes to regulate their traffic to the hotspot node(s) can be quite complex because of potentially high volume of information to be collected and the non-negligible latency between the detection point of congestion and the source nodes. In this paper, we propose an effective end-to-end flow control scheme, called HOPE (HOtspot PrEvention), to resolve the hotspot congestion problem for the Clos network on the chip (CNOC). Specifically, HOPE regulates the injected traffic rate proactively by estimating the number of packets inside the switch network destined for each destination and applying a simple stopand-go protocol to prevent hotspot traffic from jamming the internal links of the network. We evaluate HOPE’s overall performance and the required hardware. Extensive simulation results based on both static and dynamic hotspot traffic patterns confirm that HOPE can effectively regulate hotspot flows and improve system performance. Our hardware analysis shows that HOPE has very small logic overhead. Categories and Subject Descriptors C.1.2 [Computer Systems Organization]: Multiple Data Stream Architectures (Multiprocessors) – Interconnection architectures. General Terms Algorithms, Design. Keywords Saturation-tree congestion, Congestion Control, Clos Network onChip, Hotspot traffic. 1. INTRODUCTION Continuing Moore’s law to 32nm technology and beyond encourages researchers to design scalable architecture with multiple computation cores, also called processing elements (PE), on a single chip. As the number of PEs increases, a scalable and efficient interconnection infrastructure together with the routing Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. NOCS'11, May 1-4, 2011 Pittsburgh, PA, USA Copyright 2011 ACM 978-1-4503-0720-8… $10.00. algorithm and flow control mechanism is critically required to provide low-latency and high-throughput communications among PEs. Network-on-Chip (NOC) is an attractive and effective communication infrastructure for chip multiprocessors (CMPs). Among the NOC architectures proposed in literature, 2D mesh [6] has been most widely used because of its simple layout and short wires. In contrast, flattened butterfly [11] and Clos-network [2] have better efficiency despite their long wires. Most NOC implementations are based on lossless networks, consisting of many small buffered switch modules (SMs, or routers). Having buffers inside each SM avoids packet loss in the case of contention but introduces extra queuing delays. Small buffer limitation degrades the system performance in terms of latency and  throughput when  traffic  load  is heavy, and complicates the design of congestion control mechanisms. Congestion can occur at the SMs as called internal/link congestion, or at overloaded destinations as called hotspot1. Poor routing in the 2D mesh and torus networks can result in link congestion as illustrated in Figure 1(a). Several efforts showed that adaptive routing alleviates link congestion by steering traffic away from the congested links [8][9]. On the other hand, the high bisection bandwidth and symmetric nature of Clos NOC (CNOC, see Figure 3) allows traffic to be evenly distributed among the SMs in the central stage. With a good load-balanced routing algorithm, CNOC should experience less link congestion than the above mentioned networks. However, the end-point congestion or hotspot problem still exists. As soon as a hotspot occurs, it starts pushing back the traffic from upstream SMs, resulting in head-of-line (HOL) blocking in the SMs. This phenomenon is called saturation-tree congestion [1], as shown in Figure 1(c), where starting from the congestion point, all Figure 1. (a) Link Congestion (b) Hotspot Congestion with adaptive routing (c) Hotspot Congestion in CNOC 1 In this paper, unless specially noted, hotspot refers to the congested destinations (i.e., end-points). traffic flows destined for the choke point form a tree and cannot move forward. Packets destined to low loaded destinations could also be blocked by the jammed traffic in the internal SMs and suffer from high queuing delay. Our performance study shows that CNOC’s throughput degrades significantly under hotspot traffic as shown in Figure 2. Uniform Transpose Hotspot ) c c ( y c n e t a L e v A 100 80 60 40 20 0 1 7 13 19 25 31 37 43 49 55 61 67 73 79 85 91 97 Injection Rate Figure 2. CNOC ave. latency (in clock cycles) under different traffic patterns The objective of this paper is to design an effective scheme to detect hotspot congestion at the earliest stage and regulate traffic destined to hotspot destinations to avoid the saturation-tree from being formed. We propose an effective end-to-end flow control scheme, called HOPE (HOtspot PrEvention), for the CNOC. HOPE regulates traffic by prohibiting sources from injecting new packets to a destination when the number of backlogged flits destined for that destination exceeds a certain threshold. HOPE allows the sources to send packets again when the number of backlogged flits drops below another threshold. It is critical to choose these two thresholds carefully to prevent overflow (i.e. too much backlogged traffic in the network) and underflow (i.e. nonfully utilized output links). This paper is organized as follows: Section 2 discusses related work. Section 3 presents the CNOC architecture. Section 4 details our proposed congestion control scheme (HOPE). In Sections 5 and 6, we show first the hardware evaluation and the simulation results. Finally, we conclude our paper in Section 7. 2. RELATED WORK Several proposals have been proposed to solve the congestion problem in NOC in two directions: solving internal congestion (e.g., link congestion) or end-point congestion. 2.1 Internal Congestion Control In recent link congestion control works, Gratz [8] and Ramanujam [9] proposed minimal adaptive algorithm called Regional Congestion Awareness (RCA) and Destination–based Adaptive Routing (DAR). Both schemes propagate remote congestion states. With extra hardware cost, DAR outperforms RCA because it tracks per destination basis as opposed to per quadrant in RCA. Adaptive routing cannot be used to handle end-point congestion problems as it may enlarge the saturation tree and make the endpoint congestion even worse as shown in Figure 1(b). Baydal et al. [10] proposed three different mechanisms, Uchannel, ALO and INC, to detect the internal congestion. Uchannel scheme counts free virtual channels (VCs) on each output link of the SM and throttles traffic destined to the link when the number of free VCs is below a threshold. A simpler version of Uchannel is ALO, which declares a congestion if none of the following two conditions is satisfied (1) there is at least one VC on each physical link, or (2) there is at least one physical link on which all VCs are free. The INC scheme detects local and remote congestion based on the information of the local VC activity. One main disadvantage of the three schemes is that using local information cannot accurately measure the severity of the endpoint congestion. Moreover, since traffic throttling is applied only based on local information , the scheme may also affect the flows destined for low-loaded destinations. A self-tuned congestion control mechanism proposed in [16] detects congestion by counting the number of filled buffers inside the global network and compare it with a threshold. The scheme requires local information exchanged among all SMs to get the global number of filled buffers; thus extra hardware is required. Moreover, it doesn’t differentiate flows destined for low-loaded destinations from those destined for high-loaded destinations. Therefore, both kinds of flows will be affected when internal congestion occurs. 2.2 End-point Congestion Control Chrysos et al. [3][4] and Walter et al. [14] apply request-grant protocol to regulate traffic to the switch. Before injecting any traffic into the network, each source sends a request message corresponding to one or multiple packets to the destination and will not send packets until it receives the grant message. There are several disadvantages in the request-grant based schemes: (1) additional bandwidth is required to send the control messages; (2) an output may experience temporary underflow because the source may be busy on sending packets to other outputs while the grant from the output port arrives; (3) extra hardware is required to process the messages sequentially; (4) the request-grant based scheme cannot detect/handle the link congestion inside the NOC. Yang et al in [15] proposed a token-based network resource allocation scheme, in which a source PE needs to grab a token from a centralized controller before send ing traffic into the NOC and return the token to the controller upon the finish of the data transmission. The token-based resource allocation scheme has a potential scalability problem due to the latency involved in the token request/grant procedure. As pointed out in [15], the token assignment latency increases as the network scales up. Another approach to resolve end-point congestion is to apply a feedback control as proposed in [5][10][16][17]. In these schemes, local or global information is required to detect the end-point congestion. In [17], congestion is detected based on queue length at each destination. If the queue length exceeded a certain threshold, congestion is declared; otherwise, the congestion is relieved. The main disadvantage of this scheme is its slow congestion detection. By the time the queue at the destination reaches the threshold the network would have already been saturated. Another disadvantage is that the scheme cannot detect the internal congestion inside the network (traffic cannot reach destinations and therefore queues at the destinations are most likely empty). Our proposed HOPE scheme differs from the aforementioned endpoint congestion control schemes in several aspects: First, HOPE solves end-point congestion by monitoring each destination’s backlogged traffic inside the network. This global knowledge enables HOPE not only to resolve the end-point congestion problem but also to alleviate internal congestion in the early stage. Second, HOPE does not physically separate hotspot flows from non-hotspot flows in the SMs. Otherwise, extra hardware is required as in [5]. Third, HOPE doesn’t change the baseline NOC router design and requires minimal hardware overhead for congestion detection and notification. Fourth, as we will see in the evaluation section, HOPE is robust in accommodating varied delays between the HOPE controller and the source PEs.     3. CNOC ARCHITECTURE Figure 3 shows a 3-stage Clos network in the green shaded area. The switch modules in the first, second, and third stages are denoted as input modules (IMs), central modules (CMs), and output modules (OMs), respectively. All stages have n SMs, each with n inputs and n outputs. All links are uni-directional. Hence, every flit has to travel through three SMs between any input source and output destination. Figure 3. CNOC and HOPE Architecture.Each IM has N local counters (LC). N global monitors (GC) for N destinations. i = 1..n, j=1..N 3.1 Switch Module Design The architecture of the SM, based on input queued structure, is the same as the baseline Virtual Output Queue (VOQ) SM described in [12]. The throughput that can be achieved by a CNOC with flitbased scheduling in SMs is limited due to the Tail-of-line (TOL) blocking problem [12]. To overcome the TOL problem, [12] proposed a cut-through scheduling algorithm, which has been shown to achieve higher throughput and lower packet latency. In this paper, all SMs will use the cut-through scheduling scheme [12] to schedule packets. 3.2 Load-Balance Routing CNOC can easily avoid the link congestion problem by using a load-balance routing algorithm. The packets of each input-output pair flow will be evenly distributed among n paths each corresponding to one CM. Each source maintains N virtual destination queues (VDQs) (i.e., N equals the number of output destinations of the CNOC). To evenly distribute traffic on CMs, each IM forwards packets destined for same OM in a round-robin manner among CMs. Flits of the same packet follow the same CM selected by the head flit. In this case, we distribute packets evenly under the assumption of fixed packet size. If packet size varies, the number of flits will be considered in the routing scheme to maintain load-balancing distribution. 4. HOPE CONGESTION CONTROL In this section we describe the behavior of HOPE and its design parameters.  The hotspot congestion control scheme has to be effective to detect congested destinations in an early stage and decide when the congestion is relieved. This detection requires global knowledge about incoming rates and buffer occupancy inside SMs. One way to obtain this knowledge is to place monitors in all stages of the network in addition to a central unit to evaluate the congestion status of each output destination. The hardware complexity for this solution is proportional to the network size; thus scalability is an issue. Moreover, having monitors inside the SMs will increase each SM’s complexity and area. Also, the tight area constraint in NOC limits the numbers of counters and/or a complicated centralized controller that can be used. Hence, we want to design an effective scheme with the minimal hardware cost to cope with NOC constraints. Evaluating the effectiveness of the scheme depends on the performance metric used. Since NOC's applications are delay-sensitive, we assess our scheme by measuring the effective throughput ( TH ) of the system, which is defined as the number of flits received at their destinations within a period of time, such that their latency is no more than the required delay constraint. To improve the effectiveness of the scheme we want to maximize this TH . eff eff High TH 4.1 Hotspot Detection using Two-Thresholds The beginning of hotspot congestion is easily detected if the backlogged traffic of the congested output destination has exceeded a high threshold, . Consequently, traffic destined for that hotspot is throttled based on the traffic regulation mechanism. As a result, the backlogged traffic starts draining from the network and the congestion ends when the backlogged traffic drops below a low threshold, . TH Each destination has two possible states: hotspot or non-hotspot. Based on the two thresholds, HOPE updates the state of each destination in each clock cycle following the finite state machine (FSM) given in Figure 3. Low 4.2 Traffic Regulation using Stop-and-Go Protocol HOPE regulates incoming traffic using a lightweight stop-and-go protocol. This mechanism is tightly coupled with the two threshold mechanism discussed earlier. When congestion is detected for a specific destination, HOPE sends a stop signal to all sources stopping the traffic generation of that congested destination. During stop period ( stop t ), backlogged flits destined for the hotspot start draining out of the network buffers. When the hotspot destination changes its state to non-hotspot, HOPE announces the go period ( got ) by sending a go signal to all sources allowing them to generate traffic to the relieved hotspot. 4.3 Thresholds Selection eff It must be noted that the two-thresholds play an important role in traffic regulation. In the go period, hotspot traffic is allowed to accumulate buffer resources, and in the stop period sources stop injecting hotspot traffic and drain out backlogged flits. We must maintain certain buffer occupancy inside the network to maximize TH . Hence, we have to decide carefully which thresholds are to be used in our system since we are restricted by delay constraint. Buffer size in each SM, network size and the switching algorithm used influence the network performance in terms of overall throughput and overall latency. Given these system parameters, we set some rules to select the thresholds. We first analyze the upper and lower bounds for TH and TH , then discuss HOPE sensitivity to the thresholds to maximize TH . High Low eff 4.3.1 Threshold bounds Low High We evaluate the upper and lower bounds for TH and TH by monitoring global counter behavior as demonstrated in Figure 4. The global counter curve for destination j ( jGC ) depends on the states of destination, i.e. whether a destination is in a hotspot or non-hotspot state. If it is non-hotspot, then sources inject traffic into the network, and the backlogged monitor starts to increase with the positive slope of jGC as demonstrated in Figure 4 during the go period.  While in the stop period (i.e. when a congestion is declared), sources stop generating traffic to the hotspot after receiving a stop signal. Consequently jGC starts draining its backlogged flits, which we can see in the negative slope during stop t . To simplify our evaluation, we have assumed that there is (1) no underflow or overflow buffer; (2) a fixed flow rate; (3) a fixed packet size p; and (4) 100% injection rate . 4.3.1.1 Low threshold lower bound go max stop Let us assume in one period T that a destination changes its status from non-hotspot to hotspot and again to non-hotspot as illustrated in Figure 4. jGC increases during got and decreases during stop t (i.e. T = t + t ). It is notable that t is equal to GC divided by the average draining rate ( rD ) identified by system parameters (i.e. t = GC / D ). To avoid underflow, we must notify sources to start generating new traffic to the corresponding hotspot before totally draining all backlogged flits. As shown in Figure 4, Δ is the time required to drain remaining backlogged flits before experiencing underflow. By using geometric calculations we get: max stop stop r TH Low *Δ= D r (1) Based on our assumptions, Δ is affected by: (1) the time required for the propagation delay of the congestion notification ( prop t ) to reach all sources  and inform them about the beginning of go period, (2) the average arbitration time ( arb t ) needed at the source to inject hotspot flow into the network, (3) the propaga tion delay introduced by the aggregation and update global counter ( aggr t ), and (4) summation of average latency at each stage ( stages t ) (i.e =Δ t + t + t + t ). To avoid underflow and wasting bandwidth at hotspot destination, TH should be at least rD*Δ (i.e. TH *Δ≥ D ). stages aggr prop Low Low arb r 4.3.1.2 High threshold upper bound High Overflow in the intermediate SMs occurs when we allow injecting traffic into the network for a long period of time. To avoid this situation, we bound TH such that it does not exceed the maximum allowed buffer occupancy inside the network (i.e. maximum value of global counter GC in Figure 4). GC is evaluated based on the delay constraint. The required time (i.e.  in Figure 4) from detecting congestion until reaching GC is very critical and depends on multiple factors. Traffic rate, packet size, and propagation delay of the congestion notification (i.e. stop signal) are factors to bound TH . Each source participates with hotspot input rate ijA (i.e. flow rate from source i to hotspot destination j). In the Go period, jGC increases based High max max max Figure 4. Global Counter Behavior N i on the aggregated input rates ∑ 1= ijA . To calculate TH upper bound, we first evaluate the time  . Obviously,  ’s worst case is at most t plus the time required to inject a packet (i.e. packet size/physical link rate). By applying geometric equations based on jGC behavior,  we bound TH to avoid overflow as follow: High prop High TH High ≤ GC max ∑*τ N i 1= A ij (2) 4.3.2 Thresholds sensitivity Choosing the threshold is restricted by the objective function of maximizing the TH . To understand the sensitivity of the scheme, let us fix one of the thresholds, move the other, and observe its effect on TH . eff eff Low _ eff _ _ Low Best Best Best High High High High High High High In the first scenario, we fix TH to its lower bound and we change TH such that it varies in the range between TH and the upper bound of TH . As we increase TH , TH starts increasing but after a certain threshold let us call it TH , TH starts decreasing.  To  explain  this phenomenon, let us consider two cases, case 1 just before TH , and case 2 after TH . In case 1, having low TH will not allow many hotspot packets inside the network which means the stop signal is too early. Since hotspot rate is usually higher than non-hotspots then most of the packets supposedly received is related to hotspot destination. By stopping these packets in early stage we waste a lot of bandwidth. Moreover, Non-hotspot destinations may be considered hotspots mistakenly. This inaccuracy in detecting hotspots is also a factor of lower TH . In case 2, as we increase TH greater than TH , TH decreases because when we increase TH too much, HOL blocking occurs and non-hotspot flits experience a long queuing delay; thus less number of packets received within delay requirement. In the second scenario, we fix TH instead and test the TH effect. The same phenomena for TH occurs. As we increase TH , TH starts increasing but after a certain threshold let us call it TH , it starts decreasing.  To explain this phenomenon, let us consider two cases, case 1 just TH TH before and case 2 after . In case 1, when TH is low this means we send go signal too late. Therefore, there are few hotspots packets inside the network which is in favor for non-hotspot latency but as we mentioned earlier most of High High High High Low Low Low Best Best Best Best eff _ Low Low eff _ eff _ Low eff eff _ eff eff Low the traffic is destined for hotspot destination; thus TH is not very high and as we increase TH , TH increases until it reach its maximum at TH . Case 2 in the other side consider when the go period is too early. This means we accumulate more hotspot traffic before relieving congestion. This accumulation blocks non-hotspot flows and increases their latency. What we want is to choose our thresholds such that we accurately identify hotspots and at the same time maintain low latency for non-hotspot destination in order to maximize TH . Low Best _ eff Low High High Let us consider our objective function and test thresholds effect. Based on the delay constraint, the thresholds will change. Recall that as TH increases, latency also increases. We do not want to exhaust the network with hotspot traffic. At the same time, we don't want to waste bandwidth from the underflow that hotspot destination might experience. Therefore, we have tested CNOC under uniform traffic conditions and identified the average buffer occupancy under the restricted delay required. We found that as the delay constraint increases the best threshold selection increases too. This is because the maximum allowed buffer occupancy is increased also. This average occupancy limits the average global counter that can be reached. By setting this value, TH can be calculated from (2). The same can be done to calculate TH . We have tested many threshold combinations experimentally, and we have chosen the best combination that maximizes TH under uniform traffic. The selected threshold pair ( TH , TH ) regulates input traffic effectively and achieves maximum TH . We have proved the analytical equations using simulations and found that they are very close. From our discussion, we can formulate a set of rules to choose the two -thresholds for a given network parameters (i.e. SM size, buffer size for each input port, number of stages, draining rate, congestion propagation delay and packet size): (a) to avoid overflow, TH must be less than the upper bound in (2); (b) to avoid underflow, TH should not be less than (1); (c) based on our objective function to maximize TH , GC is limited to the average buffer occupancy for each flow under uniform traffic at the required  delay constraint. High High Low Low max eff eff eff 5. HARDWARE EVALUATION We have assessed HOPE's hardware complexity and bandwidth requirement. The VHDL code of the design is synthesized and analyzed by the Cadence Encounter RTL Compiler on the ST Microelectronics Company 65nm technology. Then we use SOC Encounter to do automatic place and route. For memory read/write operations, we use CACTI [13] to characterize the memory delay and area. 5.1 Backlogged Traffic Aggregation Logic Cost As shown in Figure 3, HOPE monitor and calculate the backlogged traffic for each destination in separate g lobal counters by collecting local injected input traffic from each IM. Each IM has N (i.e. N is network size) local counters to aggregate input traffic. During every clock cycle, each IM counts incoming flits for each destination and stores them in a local counter (LC) with a size of log2 n (i.e. maximum number of flits injected in nxn SM). Local counters in different IMs corresponding to one destination Table 1. Summary of HOPE Hardware Overhead PE Routers HOPE Total 144 3.84 0.089 148 97.3% 2.6% 0.1% 100% Area (mm2) Percentage Table 2. Summary of Hardware Evaluation Delay Adders (in-out) bits LC Levels (5-6) > (5-6) (1-3) (3-4) (4-5) LCA SCA TBA 9 6 5 8 8 8 1 2 4 1 2 2 1 2 1 5 2 1 Delay (ns) 1.45 1.07 0.85 are all aggregated and added to the previous accumulated value stored in a global counter jGC . Simultaneously, if there are flits departed from OMs, the corresponding global counters will be decremented by 1. If we consistently track the numbers of flits injected into IMs and departed from OMs we are able to estimate the number of backlogged flits inside the network. Adding all local counters together in one clock cycle is not practical because of  the complex computation required;  therefore, we have evaluated  three  aggregation methods: (1)  long-cascade aggregation (LCA), (2) short-cascade aggregation (SCA) and (3) tree-based aggregation (TBA). LCA is a simple chain such that each IM aggregates its traffic to the next IM and so on. Despite its simplicity, it has a very long aggregation path proportional to number of IMs. To avoid this long aggregation, we divided IMs into two sets in SCA and followed the same concept as in LCA for each set. The aggregation delay is reduced, but we have large adder sizes to accumulate in sequence. TBA follows a tree-based structure and accumulates two IMs at a time, and the result is accumulated again with other results from two other IMs and so on. Table 2 lists the number of required adders according to the adder size for one destination of a 64-nodes CNOC. We also show the delay introduced after aggregating all these adders, excluding wiring propagation delay. Since we want to minimize the hardware propagation delay, TBA is the best option because it has the shortest aggregation path. Moreover, TBA has more numbers of smaller adder sizes as opposed to large adders in LSA and SCA. Evaluation results are listed in Table 1 and 2. Table 1 confirms that HOPE require a very light-weight logic units, and its hardware overhead does not exceed 56% of one SM area. This proves that the physical implementation of HOPE is practical under NOC constraints. 5.2 Backlogged traffic Wiring Overhead One issue we may think of is the number of wires required to send all N counters corresponding to N destinations from each IM. Consider a 64-node CNOC as an example, the total number of wires required to send local counter information to the global counters is 1536. Let us consider CNOC layout to understand how to reduce this overhead. We have adapted the CNOC layout evaluated in [2] to illustrate HOPE implementation from the layout perspective. The layout design is illustrated in Figure 5, and it must be noted that the CNOC layout is symmetric such that each quadrant has two IMs (i.e. the upper right quadrant has i1 and i8), two CMs (i.e. the upper right quadrant has c1 and c2), and two OMs (i.e. the upper right quadrant has o1 and o2). We place the HOPE logic unit in the center of the layout. Every two adjacent IMs aggregate their local counters and send 4-bit wires to each global counter. In this case to 64 output destinations, each quadrant requires 256 wires bandwidth (i.e. 64*4 -bit) to the global counter. This bandwidth is acceptable and conforms to NOC constraints. Figure 5. HOPE is at the center of CNOC layout. Each quadrant has 256-bit wires from adjacent IMs and 16-bit wires from 2 OMs. We also have thought how we can reduce this wiring (i.e. 256 wires per quadrant) by using sampling technique. Specifically, instead of sending N backlogged traffic corresponding to N destinations every clock cycle from each IM, each IM samples its local backlogged into n samples corresponding to n OMs, such that each clock cycle each IM sends n local backlogged corresponding to n destinations out of the N destinations. Thus, local backlogged are accumulated for the last n clock cycle, let us call it round. IMs follow a round robin manner in sending their backlogged traffic corresponding to each OM (i.e. every clock cycle total backlogged traffic is N). Consequently, each local counter should track input traffic during one round; thus its size is log2 n2. As a result, only (log2(n2))*n*2 wires are required from each quadrant. To simplify this explanation, let us consider 64 nodes CNOC with 8x8 SMs (i.e. 8 IMs) as an example. Each IM sends 8 accumulated backlogged local counters corresponding to 8 destinations per round. Therefore, each local backlogged is at most 6-bits value (log2(8*8)), then the total number of wires from each quadrant is 6*8*2 which is 96 wires. Clearly, number of wires using sampling is 62.5% less than the basic calculation. The tradeoff is between exact calculations of the backlogged traffic and complexity. To assure that HOPE performance is not affected by the sampling mechanism, we have simulated HOPE with the sampling and added also extra notification delay for the stop and go signal. The result confirms HOPE robustness and the approximation introduced by sampling slightly affected the performance by only 0.04% which is very small and negligible. As a conclusion, although it seems HOPE requires many wires, we have proved by using sampling that wires are not an issue and it can be reduced significantly. Simulation results confirm HOPE effectiveness despite having approximated backlogged traffic measurement. 5.3 Congestion Detection and Traffic Regulation Low High Two registers are required to store TH and TH . The size of the two registers depends on the network size and buffer resources as explained earlier. From our analysis and simulation results, each register is 6-bits in size. The congestion detection is evaluated using FSM with two states (see Figure 3). As a result, a 1-bit signal is enough to indicate hotspot or non-hotspot status. For N network size, we need N-bit bandwidth to propagate the congestion decision for N destinations to each source. Each source reads this N-bit value and acts accordingly. The propagation delay for this signal is also evaluated using simulations as discussed later. 6. SIMULATION & DISCUSSION We evaluate HOPE performance using a 64-node, 3-stage CNOC with 8x8 VOQ input-buffered SMs. Our design is load-balancing routing and simple cut-through scheduling. Each input buffer can accommodate 64 flits. The performance metric is based on effective throughout with delay constraint D such that we count number of  packets whose latencies are less than or equal to D. All our simulation  tests, unless specially noted, use delay constraint=100 clock cycles (cc). 6.1 Experiment 1: HOPE Performance Evaluation Under Different Traffic Patterns We evaluate HOPE under 5 different traffic patterns: 1. uniform, 2. transpose (i.e. each source sends to one destination [7]), 3. static hotspot traffic with 1 hotspot (i.e. each source sends 10% of its traffic to the hotspot, and 90% is distributed uniformly among others), 4. static hotspot traffic with 4 hotspots which are connected to the same OM(i.e. each hotspot receives 10% of source input rate, and the remaining 60% is distributed uniformly among other destinations), and 5. Dynamic traffic such that each OM has 1 hotspot, 5 non-hotspot output ports and 2 dynamic output ports whose traffic loads are dynamically changing. Each source distributes 2.5% to each of eight hotspot destinations and 1% to each of forty non-hotspot destinations. The remaining 20% of the traffic is dynamically shared by those dynamic output ports according to the predetermined traffic load pattern, which changes every 5000 cc. Accordingly, those dynamic output ports may experience hotspot and non-hotspot states during the whole simulation period, and we sample the effective throughput every 1000 cc. For all these five cases, we use (30, 50) as our threshold, based on our analysis described earlier. We compare HOPE with two other models. One is a simple model without congestion control (NCC); the other is the ideal using output queue switch model (Ideal_OQ), in which we assume all the packets received are effective. We derive the ideal OQ effective throughput curve from the following mathematical function. Injection rate: ρ ; hotspot traffic percentage: α ; and hotspot number: h . Ideal OQ TH eff        ( (min( *63* *)1), h 64(* *63 * h )) .64 Figures 6-7 show the performance of three models under different traffic patterns.  As we can see in Figure 6 and 7, at the low load that effective throughputs of HOPE, NCC and the Ideal OQ model are almost overlapping. As we increase injection rate , three curves begin to separate. Due to congestion, NCC’s effective throughput remains very low when the injection rate is high. On the other hand, HOPE’s effective throughput can’t keep increasing as Ideal OQ does but still maintains high effective throughput after it reaches the highest effective throughput that the network can achieve. This means HOPE begins to regulate flows when the network can not afford that traffic load and maintains the maximum effective throughput that the network can achieve. Moreover, in Figure 8 under dynamic traffic, HOPE regulates traffic and maintains almost maximum effective throughput effectively. 1 0 .8 0 .6 0 .4 0 .2 0 0 20 40 60 80 100 Un ifo rm-NCC Un ifo rm-HOPE Un ifo rm-Ideal_OQ Transpose-NCC Transpose-HOPE Transpose-Ideal_OQ E f f c e i t e v T h r u p h g u o t Injection Rate Figure 6. HOPE under uniform and transpose pattern 1 0 .8 0 .6 0 .4 0 .2 0 0 20 40 60 80 100 Ho tspo t(1 )-NCC Ho tspo t(1 )-HOPE Ho tspo t(1 )-Theo retical Ho tspo t(4 )-NCC Ho tspo t(4 )-HOPE Ho tspo t(4 )-Theo retical E f f c e i t e v T h r u p h g u o t Injection Rate Figure 7. HOPE under different static Hotspot pattern 0.8 0.6 0.4 0.2 0 1 6 11 16 21 26 31 36 41 46 Dynamic-NCC Dynamic-HOPE Sample Index E i f f c i n e t T h r u p h g u o t Figure 8. HOPE under dynamic traffic pattern 6.2 Experiment 2:HOPE Parameters Analysis In this experiment we analyze the sensitivity of our scheme's performance to different parameters of HOPE . Threshold Effect Test: we evaluate the two-threshold effect on the effective throughput separately in two cases. Each case fixes one threshold and varies the other. We test HOPE under 100% injection rate uniform traffic. In case 1, we fix the low threshold to 30. Alternatively, in case 2, we fix the high threshold to 80. In both cases, the effective throughput keeps increasing till it reaches the maximum value and then drops down as shown in Figure 9 and 10. In case 1, as we increase the high threshold, more packets can enter the network. Therefore, the effective throughput would increase a little bit, but when the network becomes congested with too many packets, packet latency would increase and the effective throughput would drop. In case 2, as the low threshold is increased, the stop period for congested flows becomes shorter, and they resume faster, which increase effective throughput. But when the low threshold is too high, the new packets belonging to congested flows enter the network before the network digests the outstanding packets already in the network. This will cause HOL blocking and increase the packet latency; thus decreasing the effective throughput. 0 . 7 0 . 6 0 . 5 0 . 4 0 . 3 0 . 2 0 . 1 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 1 0 0 1 1 0 1 2 0 1 3 0 1 4 0 1 5 0 L o w _ T h r e s h o l d = 3 0 H i g h T h r e s h o l d E f f c e i t e v T h r u p h g u o t Figure 9. THeff vs. THHigh 0 . 6 1 0 . 6 0 . 5 9 0 . 5 8 0 . 5 7 0 . 5 6 0 . 5 5 0 . 5 4 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 H i g h _ T h r e s h o l d = 8 0 L o w T h r e s h o l d E f f c e i t e v T h r u p h g u o t Figure 10. Theff vs. THLow Packet Delay Constraint Test: Here we show how thresholds selection is affected by the delay constraint.  In this test, the delay constraint is 200cc, and we test two threshold pairs (30, 50), (80, 100). Figure 11 shows that the performance with threshold (80, 100) is better than the one with threshold (30, 50) which is the best threshold for 100cc. This is because increasing the delay constraint allows increasing the maximum buffer occupancy for each flow as discussed earlier. 1 0.8 0.6 0.4 0.2 0 0 10 20 30 40 50 60 70 80 90 100 Uniform-HOPE(30,50)-200cc Uniform-HOPE(80,100)-200cc Injection Rate E f f c e i t e v T h r u p h g u o t Figure 11. THeff with D=200 Hotspot Number and Hotspot Rate Test: In our hotspot number test, we define three traffic patterns with different hotspot numbers to evaluate HOPE with the same threshold for multiple hotspots. Each source distributes 10% of its traffic to each hotspot destination and the rest to other destinations uniformly. In hotspot rate test, we define three traffic patterns with different hotspot traffic rates to evaluate HOPE with the same threshold for multiple rates. All three traffic patterns have one hotspot destination as follow: 1. Each source distributes 10% of its traffic to hotspot destination and 90% to other destinations uniformly. 2. Each source distributes 30% its traffic to hotspot destination and 70% to other destinations uniformly. 3. Each source distributes 50% its traffic to hotspot destination and 50% to other destinations uniformly. Figures 12 and 13 show that with different hotspot numbers, or with different hotspot traffic rates, HOPE maintains high effective throughput the network can achieve. Congestion Notification Delay and Sampling Test: Our motivation in this test is to show HOPE sensitivity to congestion notification delay, aggregate delay, and the accuracy of the global counter value. Here signal notification delay means the time for the global control unit to notify sources to stop or resume the flows. Aggregation delay means the time required for aggregating local information to the global control unit. Figure 14 shows three             1 0.8 0.6 0.4 0.2 0 0 20 40 60 80 100 Hotspot(1)-HOPE Hotspot(1)-Theoretical Hotspot(2)-HOPE Hotspot(2)-Theoretical Hotspot(4)-HOPE Hotspot(4)-Theoretical Injection Rate E f f c e i t e v T h r u p h g u o t 1 0.8 0.6 0.4 0.2 0 0 20 40 60 80 100 Hotspot(1)-HOPE-0.1 Hotspot(1)-Theoretical-0.1 Hotspot(1)-HOPE-0.3 Hotspot(1)-Theoretical-0.3 Hotspot(1)-HOPE-0.5 Hotspot(1)-Theoretical-0.5 E f f c e i t e v T h r u p h g u o t Injection Rate 0.63 0.62 0.61 0.6 0.59 0.58 0.57 0.56 1 6 11 16 21 26 31 36 41 46 Dynamic-HOPE(0) Dynamic-HOPE-Sampling&Signal_delay=5 Dynamic-HOPE-Aggregation_delay=5&signal_delay=5 Sample Index E i f f c i n e t T h r u p h g u o t Figure 12. Hotspot Numbers Effect Figure 13. Hotspot Rate Effect Figure 14. Delay Overhead Effect 80 60 40 20 0 1 11 21 31 41 51 61 71 81 91 Hotspot(IM)-NCC Hotspot(IM)-HOPE Non-Hotspot(IM)-NCC Non-Hotspot(IM)-HOPE N u m e b r o f F t i l s clock cycle 100 80 60 40 20 0 1 11 21 31 41 51 61 71 81 91 Hotspot(CM)-NCC Hotspot(CM)-HOPE Non-Hotspot(CM)-NCC Non-Hotspot(CM)-HOPE N u m e b r o f F i l s clock cycle 100 80 60 40 20 0 1 11 21 31 41 51 61 71 81 91 Hotspot(OM)-NCC Hotspot(OM)-HOPE Non-Hotspot(OM)-NCC Non-Hotspot(OM)-HOPE N u m e b r o f F t i l s clock cycle Figure 15. Average IM buffer occupancy Figure 16. Average CM buffer occupancy Figure 17. Average OM buffer occupancy curves: the first is the perfect implementation of HOPE without any delay and sampling; the second is HOPE with sampling and signal notification delay=5cc; the third is HOPE with aggregation delay=5cc and signal notification delay=5cc. The second and third curves are close to the first one and show that HOPE is not very sensitive to aggregation delay and signal notification delay (i.e. wire delay) or using sampling mechanism. 6.3 Experiment 3: Buffer Occupancy Here, we show hotspot and non-hotspot buffer occupancy at each switch stage. We compare NCC with HOPE to see how HOPE affects the outstanding packets inside the network. The simulation parameters are: fixed thresholds pair (30, 50), 100% injection rate and 10% hotspot traffic rate from each source. As shown in Figure 15-17, without congestion control, many outstanding packets are saturated in each stage, while HOPE relieves the network by regulating the congested flows. 7. CONCLUSION We propose an effective and light-weight end-to-end flow control scheme, HOPE, in order to avoid the saturation-tree congestion in CNOC. HOPE regulates hotspot traffic proactively and avoids overflow and underflow using two- thresholds mechanism and stop-and-go protocol. HOPE effectiveness allows us to alleviate internal congestion. As the analytical discussion shows, we can choose the thresholds based on the constraint delay required. We demonstrated the effectiveness of HOPE on 3-stage CNOC by simulations. The results confirm the effectiveness and robustness of HOPE under different traffic patterns, different numbers of hotspots, different hotspot rates and different propagation delays required by the scheme. HOPE hardware cost is minimal and it does not exceed 0.1% of total chip size; thus its hardware implementation copes with NOC constraints. 8. ACKNOWLEDGMENTS This work was supported by a Polytechnic Institute of NYU Angel Fund grant and by the U.S. Army CERDEC . 9. "
Delay analysis of wormhole based heterogeneous NoC.,"We introduce a novel evaluation methodology to analyze the delay of a wormhole routing based NoC with variable link capacities and a variable number of virtual channels per link. This methodology can be utilized to analyze different heterogeneous NoC architectures and traffic scenarios for which no analysis framework has been developed before. In particular, it can replace computationally-extensive simulations at the inner-loop of the link capacities and virtual channels allocation steps of the NoC topology optimization process. Our analysis introduces a set of implicit equations which can be efficiently solved iteratively. We demonstrate the accuracy of our approximation by comparing the analysis results to a simulation model for several use-cases and synthetic examples. In addition, we compare the analysis with simulation results for a chip-multi-processor (CMP) using SPLASH-2 and PARSEC traces for both homogeneous and heterogeneous NoC configurations.","Delay Analysis of Wormhole Based   Heterogeneous NoC  Yaniv Ben-Itzhak1   Israel Cidon2   Avinoam Kolodny2  Electrical Engineering Department  Technion – Israel Institute of Technology  Haifa, Israel  1yanivbi@tx.technion.ac.il   2{cidon, kolodny}@ee.technion.ac.il  ABSTRACT  We introduce a novel evaluation methodology to analyze the  delay of a wormhole routing based NoC with variable link  capacities and a variable number of virtual channels per link. This  methodology can be utilized to analyze different heterogeneous  NoC architectures and traffic scenarios for which no analysis  framework has been developed before. In particular, it can replace  computationally-extensive simulations at the inner-loop of the  link capacities and virtual channels allocation steps of the NoC  topology optimization process. Our analysis introduces a set of  implicit equations which can be efficiently solved iteratively. We  demonstrate the accuracy of our approximation by comparing the  analysis results to a simulation model for several use-cases and  synthetic examples. In addition, we compare the analysis with  simulation results for a chip-multi-processor (CMP) using  SPLASH-2 and PARSEC traces for both homogeneous and  heterogeneous NoC configurations.    Categories and Subject Descriptors  G.1.2  [Numerical Analysis]: Approximation – Nonlinear  approximation.   General Terms  Performance.  Keywords  Networks-on-Chip, Heterogeneous NoC, Analysis-Methodology,  Delay Evaluation.  1. INTRODUCTION  NoCs are designed to support a variety of SoC designs with  bandwidth and latency requirements for heterogeneous module-tomodule  flows.  In many cases,  the SoC communication  characteristics are known at design time through specifications  that describe the data rates and timing restrictions of each  communicating pair. The NoC design parameters and topology  are then tailored to meet the given communication requirements,  spending minimum power and area. The NoC design process for  such systems heavily relies on extensive performance simulations,  Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  NOCS’11, May 1–4, 2011, Pittsburgh, PA, USA.  Copyright 2011 ACM 978-1-4503-0720-8…$10.00.  where each intermediate NoC configuration is tested against the  requirements in a long ‘change and test’ search sequence. The use  of detailed simulations makes the task of searching for efficient  link capacities and virtual channels allocation computationally  intensive and it does not scale well with the size of the problem.  The use of approximated analysis of the NoC behavior replacing  simulations can dramatically save time and resources during  design. Costly simulations can be left for the final verification and  fine-tuning of the system.  This paper explores a novel delay analysis methodology for  heterogeneous wormhole based NoCs, with a variable number of  virtual channels per link and variable link capacities. The average  end-to-end latency per flow is analyzed by calculating its three  components: (1) The  time  it  takes  the  head-flit to leave the  source queue (queuing time at the source); (2) The  time  it  takes   the  head-flit  to reach  the  destination  module  (path  acquisition   time)  and (3) The time it takes the rest of the packet to leave the  network (transfer  time).  SoC and CMPs are heterogeneous in terms of module-to-module  traffic requirements. Therefore, the appropriate NoC to support  such SoCs should be non-uniform in terms of link capacities and  virtual channels. However, exiting analysis methodologies [2, 5,  15, 16, 19, 10, 9, 13] are based on the assumption that NoCs are  homogeneous. Figure 1 illustrates the delay evaluation of a  heterogeneous NoC  for  two  existing  extreme analysis  methodologies: single-VC [2, 5, 15, 16] and ""infinite-VCs”, i.e.  assuming that virtual channels are always available for any flow  [6, 12, 1]. Single-VC based methodologies can result in higher  latencies due to the over-estimation of path acquisition latencies  (i.e.  acquiring  VCs  along  the  entire  path).  Figure 1. Comparison of different methodology approaches  for a heterogeneous NoC. Single-VC ([2, 5, 15, 16]), ""infinite""  number of VCs, i.e. assuming that VCs are always available  for any flow, ([6, 12, 1]) and our new proposed heterogeneous  NoC analysis methodology.          number of flows over link ܺ into the effective number of flows  their capacities. Therefore, we decompose the aggregate effective  from link ܻ 	to link ܺ, where link ܻ is any ingress link to link ܺ .  Section  3.2 describes the definitions and section  4.2 describes  how and why to use it.  We present another phenomenon  termed remaining path  acquisition time. Generally, the time for a flow`s head-flit to  acquire a VC over a link depends on the time that other interfering  flows occupy the link. Clearly, an interfering flow occupies the  VC as long as it is transferring; moreover, the interfering flow  occupies the VC also during the time it acquires VCs over  subsequent links along its path, i.e. the remaining path acquisition  time. In addition, we take into account that during the remaining  path acquisition time the interfering flow occupies a VC but is not  transmitted;  therefore,  it  is not actually disturbing other  transmitting flows. This phenomenon is explained in detail in  section  4.2.  Furthermore, we make a distinction between the packet generation  rate into the source`s queue and the packet injection rate into the  NoC. The injection rates are the values that actually affect the  end-to-end NoC latencies and not the values of the generation  rates. Therefore, for given generation rates of the sources, our  analysis determines the actual injection rates, which can be lower  than the generation rates. Section  4.4 describes the procedure for  determining the injection rates.  These phenomena also have some relevance for a NoC with  uniform number of VCs. However, previous analyses of NoCs  with a uniform number of VCs did not take them into account.  Therefore, our analysis methodology also offers better  approximation for such NoC topologies.   This paper is organized as follows: Section  2 presents the  assumptions of our analysis. In section  3 we described the  number of flows over link ܺ and effective number of flows from  notation, and formally define the terms: aggregate effective  link ܻ 	to link ܺ . In section  4 we present details of our novel delay  evaluation analysis. Section  5 presents numerical validation  results of our analysis. We evaluate several use-cases, synthetic  examples and modeling of chip-multi-processor (CMP) with a  single shared cache.   In a wormhole routing network, the end-to-end latency ሺܶ௙ ሻ	of a  packet of flow f, sent between a specific source-destination pair, is  the sum of the queuing time at the source ൫ݍ ௙ ൯, the path  acquisition time ൫ܽ ௙ ൯ and the transfer time ሺݐ ௙ ሻ. Formally,  The transfer  time, ݐ ௙ , is  affected  by  other  flows  sharing  the  VCs sharing the link. The path acquisition time, ܽ ௙ , is affected by  same links, since each link capacity  is  divided among all active  an even more complex interaction among the flows, as a packet  may wait for the evacuation of VCs by other packets sharing links  with it, which in turn wait for the evacuation of VCs in other  links.  ܶ௙ = ݍ ௙ + ܽ ௙ + ݐ ௙ .	 2. BASICS OF DELAY ANALYSIS  ሺ1ሻ	 For simplicity, in this paper we use the assumptions that each  source generates a single flow and the propagation delay of flits  through routers is negligible. However, the analysis can be easily  extended for sources with multiple flows and for routers with nonzero latency.  Figure 2. Example for a NoC with non-uniform numbers of  VCs (Unless noted, links have sufficient number of VCs).   ""Infinite-VCs” based methodologies are also imprecise for two  reasons. First, there is a wrong assumption that the path  acquisition latencies are instantaneous which under-estimate  realistic path acquisition latencies. Second, it is assumed that all  potential flows are concurrently multiplexed over each link; this  results  in higher  transfer  latencies. Therefore,  these  two  methodologies are not capable of evaluating delay properly.  Hence, an analysis methodology for heterogeneous NoCs is  required.  Previous works have presented analysis methodologies for NoCs  with a single-VC [2, 5, 15, 16], a uniform (same for all links)  number of VCs [19, 10, 9, 13] and ""infinite"" number of VCs [6,  12, 1].      Huang et al. in [8] presented the only previously existing analysis  methodology for non-uniform numbers of VCs. However, their  analysis accounts for Head-of-line blocking probability and  therefore is incapable of calculating many performance metrics  such as end-to-end latency. It analyzes the traffic and estimates  the congestion on each channel independently rather than having a  global view of all the flows by taking into account the mutual  interaction between them. While we cannot directly compare our  end-to-end latency analysis to this work, our experience and  results show that one must consider the mutual interaction of  intersecting  flows  in order  to get a  reasonable delay  approximation. Guz et al.  in [6] presented an analysis  methodology for non-uniform capacities of links. However, they  assume ""infinite"" number of VCs.  To our knowledge, no existing analysis accounts for the  combination of heterogeneous traffic patterns, non-uniform link  capacities, and variable numbers of VCs. In the sequel, we  illustrate some of the phenomena which can occur in a NoC with  non-uniform links including a variable numbers of VCs and  variable link capacities, and we address the challenges facing the  development of a sound analysis methodology in this case.   Figure 2 depicts a NoC example comprised of links with nontransmitted over link 2 ሺ݈ଶ ሻ. However, since link 7 ሺ݈଻ ሻ has only  uniform numbers of VCs. There are four flows which can be  two VCs, no more than three flows can simultaneously enter link  2. This phenomenon affects the time it takes for a flow to acquire  a VC over link 2; therefore, we should take into account only  three waiting flows and not four. We address this phenomenon by  introducing the term aggregate effective number of flows over link  ܺ; it stands for the maximum number of flows that simultaneously  can enter any link ܺ. Furthermore, the aggregate interleaving rate  experienced by a flow over link 2 depends on the effective  number of flows from each ingress link (i.e. links 1 and 7) and      Other assumptions are: all flows generate packets according to a  Poisson process; sources have  infinite packet queues and  destinations immediately consume arriving flits; routers have a  single flit input queue per VC; the wormhole back-pressure credit  signal is instantaneous; and the routing algorithm is deterministic.  3. NOTATIONS AND  DEFINITIONS  3.1 General Notations and Definitions  We denote each unidirectional link by ݈௝ (݆ ∈ ሼ1, … , |ܮ|ሽ, where  |ܮ| is the total number of links in the NoC). Link ݅ ሺ݈௜ ሻ can be  either a unidirectional link between network routers or a  unidirectional access link that connects the module to the router.  We define ݀ሺ݈௜ , ݈௝ ሻ as the minimal number of routers connecting  links ݅ and ݆.   ܫ݊൫݈௝ ൯ is defined as the group of all ingress links to ݈௝ 		(link ݆ሻ.	  The network is defined using the parameters listed in Table 1.  Table 1. Parameter Definitions  Definition: ܫ݊൫݈௝ ൯ ≜ ൛݈௜ ห݀൫݈௜ , ݈௝ ൯ = 1ൟ.  ݂݈݅ݐ  ݉௙  ߣ௙  ߣ௡௘௧௪௢௥௞ ௙ ܥ௟ೕ  ܸ௟ೕ  ߨ ௙  ߨ௟ೕ௙  ܨ௟ೕ  ܨ௟೔,௟ೕ  ܨ௟೔ ,௟ೕா௙௙  ܨ௟ೕா௙௙  Flit size [bits].  The mean packet length of flow f  [flits].  Packet generation rate of flow f  [packets/sec].  ܨ௟ೕ ≜ ൛݂ ห݈௝ ∈ ߨ ௙ ൟ.   ܨ௟೔ ,௟ೕ ≜ ൛݂ ห݈௜ ∈ ߨ ௙ , ݈௝ ∈ ߨ௟೔ ௙ , ݀൫݈௜ , ݈௝ ൯ = 1ൟ.   Packet injection rate of flow f   into the network  [packets/sec].  Capacity of link ݆ [bits/sec].  Number of VCs on link ݆.  Ordered set of consecutive links that compose the  path of flow f.      Set of subsequent links to link ݆ over the path of  flow f, i.e. a suffix of the path ߨ ௙ .   The group of flows sharing link ݆,   The group of flows sharing link ݆ from the ingress  link ݅ . i.e.:  Effective number of flows from link ݅ 	to link ݆.  Aggregate effective number of flows over link	݆ .   Defined in sub-section  3.2.  Defined in sub-section  3.2.  The source generates packets of flow f at a rate of  ߣ௙ . The  packets are queued at the source queue and injected into the  packet injection rate of packets into the NoC, ߣ௡௘௧௪௢௥௞ network in a FIFO manner. When the source queue is instable, the  than the packet generation rate, ߣ௙ , i.e. ߣ௙ > ߣ௡௘௧௪௢௥௞  is lower  	is the value that affects the network NoC latencies (i.e.  . Clearly,  path  acquisition  time plus transfer  time) and not the value of  ߣ௙ .	In section  4.4 we describe an iterative procedure to evaluate   values for given values of ߣ௙ .  A non-uniform numbers of VCs over the NoC can cause different  phenomena as illustrated by Figure 2. We  take  them into account   ߣ௡௘௧௪௢௥௞ ௙ ߣ௡௘௧௪௢௥௞ ௙ 3.2 Effective Number of Flows   ௙ ௙ by using the following notations in our analysis:  ܨ௟೔,௟ೕா௙௙ − Effective number of flows from link ݅ 	to link ݆.   ݆ from the ingress link ݅ .   The maximum number of flows that simultaneously can enter link  ܨ௟ೕா௙௙ − Aggregate effective number of flows over link ݆.  ݆ from all ingress links. It is equal to the sum of ܨ௟೔ ,௟ೕா௙௙over all  The maximum number of flows that can simultaneously enter link  ingress links.  Figure 2 presents an example for ܨ௟೔,௟ೕா௙௙ , ܨ௟ೕா௙௙ , ܨ௟೔,௟ೕ and ܨ௟ೕ . For  instance, since ݈଻ has only two VCs (i.e. ܸ௟ ଻ = 2), the maximum  number of flows that can enter simultaneously from ݈଻ to ݈ଶ , ܨ௟ళ ,௟మா௙௙ ,  is equal to two. However, there are three flows (numbered 2,3 and  4) that can be transmitted from ݈଻ to ݈ଶ , denoted by the group  ܨ௟ళ,௟మ . Similarly, since ݈ଶ has only two VCs (i.e. ܸ௟ ଶ = 2), ܨ௟ ଷா௙௙ is  equal to two, and ܨ௟ ସா௙௙ is equal to one because ݈ଷ has only a  single VC.   We formally define ܨ௟೔ ,௟ೕா௙௙ andܨ௟ೕா௙௙ using the recursive procedure  procedure finds, for each flow from link ݅ to link ݆ , ܨ௟೔ ,௟ೕ , the link  described in Appendix A. For a given link ݆ and ingress link ݅ , the  with the minimum number of VCs over its path till link ݆. Since  enter link ݆, we sum up the number of VCs of those links. Finally,  these links can limit the number of flows that simultaneously can  ܨ௟೔ ,௟ೕா௙௙ is equal to the minimum between this value, the number of  flows from link ݅ to link ݆,	ܸ௟ ௜ 	and ܨ௟೔ா௙௙ . After ܨ௟೔ ,௟ೕா௙௙ is calculated  for all ingress links to link j , ሼܫ݊ሺ݈௝ ሻሽ, ܨ௟ೕா௙௙ can also be calculated.  The analysis methodology approximates the average end-to-end  latency of flow f, ܶ௙ , which is composed of the queuing time at  source,	ݍ ௙ , the path-acquisition time, ܽ ௙ , and the transfer time, ݐ ௙  In order to calculate ܽ ௙ , we add up the acquisition time of every  (1).   link ݆ along flow f `s path, ܽ௟ೕ௙ . Therefore,   The transfer time, ݐ ௙ , is dominated by the hop with the smallest  bandwidth available to flow f along its path, and is therefore  computed as following:  4. END-TO-END LATENCY ESTIMATION  ܽ ௙ = ∑ ሺ2ሻ	 ௟ೕఢగ೑ 	 ܽ௟ೕ௙ 	.	 ݐ ௙ = ݉௙ ⋅ max ቄݐ௟ೕ௙ ቚ݈௝ ∈ ߨ ௙ ቅ.	 Where ݐ௟ೕ௙ is the flit transfer time of flow f over link ݆ .  Note: For very short packets (i.e. when the packet length is much  shorter  than  the number of buffers along  its path),  the  approximation of (3) is not sufficient. We need to calculate the  latency of the head-flit apart from the latency of the body-flits.  Thus, alternatively, the following equation can be used:  ሺ3ሻ	 ݐ ௙ = ∑ ௟ೕఢగ೑ 	 + ൫݉௙ − 1൯ ⋅ max ቄݐ௟ೕ௙ ቚ݈௝ ∈ ߨ ௙ ቅ.		 ݐ௟ೕ௙ ሺ4ሻ Finally, by substituting (2) and (3) into (1) we get the average  end-to-end latency of a packet of flow f,       	 Figure 4. Rationale for using effective number of flows (An  example).  0 ܽ௟ೕ௙ = ቐ M/M/m/ ܨ௟ೕா௙௙ ൑ 	ܸ௟ೕ ܶ௙ = ݍ ௙ + ∑ ௟ೕఢగ೑ 	 ܽ௟ೕ௙ + ݉௙ ⋅ max ቄݐ௟ೕ௙ ቚ݈௝ ∈ ߨ ௙ ቅ.		 ሺ5ሻ	 ݍ ௙ ( 4.1), ܽ௟ೕ௙ ( 4.2) and ݐ௟ೕ௙ ( 4.3). The aggregate effective number of  In the following sub-sections, implicit equations are derived for  flows over link ܺ , the effective number of flows from link ܻ 	to link  ܺ and the remaining path acquisition time phenomena result in  dependency between the equations. These equations are based on  heuristics and approximations. Finally, we solve by iterations the  set of implicit equations in order to evaluate the average end-toend latency of each flow. The variables and their corresponding  equations are listed in Table 2.   4.1 Queuing Time at the Source  We approximate the source queuing time using the M/D/1 queue  model [11]:  ݍ ௙ = ௧ ೑ା௔೑ଶ ቀ ൫௧ ೑ା௔೑൯∙ఒ೑ ଵିሺ௧ ೑ା௔೑ሻ∙ఒ೑ቁ	.	 ሺ6ሻ	 Clearly,  when  a  flow  does  not  share  any  of  its  links with  required to deliver a packet through the network (ݐ ௙ + ܽ ௙ ) is  other flows, (6) is the exact mean queuing time, since the time  deterministic. When a packet might be multiplexed with other  packets within the network, the service time is not deterministic  any more. However, thorough simulations [6] show that (6) is a  good approximation for the queuing time even for flows that are  frequently multiplexed.  4.2 Path Acquisition Time over a Link  The path-acquisition  ܽ௟ೕ௙ , is the time for the head-flit of the packet to acquire a VC over  time  of  flow  f over  link  the link. Clearly, path-acquisition is instantaneous whenever the  number of VCs of the link exceeds the number of flows that  traverse this link.  This section addresses the case when there is  possible competition among the flows for the same VCs. In such a  case, the head-flits of different packets acquire VCs in FIFO  manner.   ݆ ,	 Figure 3. M/M/m/K queue model.        destination, ∑ ௟೔ఢగ೗ೕೖ ߤ = ܽ௟೔௞ ௞∈ி೗ೕ ∑ ቌ 1 ݐ௞ + ∑ ቚܨ௟ೕ ቚ  (remaining path acquisition time).   ቍ ܽ௟೔௞ ሺ9ሻ	 ௟೔ఢగ೗ೕೖ Figure 5 illustrates the rationale for adding the remaining path  acquisition time expression. Assume that all the flows have the  same packet generation rate, and all the links have the same  capacity and a single VC (see Figure 5(a)). Figure 5(b) presents  the end-to-end latency versus the packet generation rate of the  flows for flow 2 reported by the simulation, the analysis and a  modified analysis that does not take into account the remaining  path acquisition time. It can be seen that ignoring this delay  depends on the time that flow 1 occupies ݈ଵ ; this time 	is equal to  results in gross inaccuracy. The path acquisition time of flow 2  ݈ଶ , 	݈ଷand ݈ସ : 	ܽ௟మଵ +ܽ௟యଵ +ܽ௟రଵ ) plus its transfer time (ݐଵ ).    its remaining path acquisition time (i.e. acquiring-VC over  ܽ௟ೕ௙ = ܣݒ݁ݎܽ݃݁ 	ܳݑ݁ݑ݁ 	ܵ݅ݖ݁ ܣݒ݁ݎܽ݃݁ 	ܣݎݎ݅ݒ݈ܽ 	ܴܽݐ݁ = ∑ ௠ା௄௞ୀ௠ାଵ ሺ݇ − ݉ሻܲ௞ ሺ1 − P୫ା୏ ሻ time, ܽ௟ೕ௙ (10).Where ሼܲ௞ ሽ are the equilibrium probabilities of the  Finally, using Little's law [11] we calculate the path-acquisition  M/M/m/K queue model (Figure 3).  ሺ10ሻ	 	.	 λ ۓ୫୧୬ (௏೗ೕ ିଵ,ி೗ೕಶ೑೑ିଵ) ி೗ೕಶ೑೑ିଵ ∑ ௟೔∈ ூ௡൫௟ೕ൯ ܤ௟೔,௟ೕ௙ 0 ܤ௟ೕ௙ = ە۔ ; ܨ௟ೕா௙௙ > 1 ۗ  (12)  ; ܨ௟ೕா௙௙ = 1 ۙۘ ܤ௟೔,௟ೕ௙ [flits/sec] is the effective bandwidth consumed by all flows  other than flow f on link ݆ from the ingress link ݅ .   ௔೗೔ೖ ቌ bw bw ≜ ∑ ۖۖۓ ەۖۖ۔ ܤ௟೔,௟ೕ௙ = ௞∈ቄி೗೔,೗ೕ \௙ቅ  ⋅ ݉௞ ೗೔ചഏ೗ೕೖ ቍ ߣ௡௘௧௪௢௥௞ ௧ ೖ ௞ ௧ ೖା∑ ; ܨ௟೔,௟ೕா௙௙ = ቚܨ௟೔,௟ೕ ቚ ቚி೗೔ ,೗ೕ ቚ bw ; ݈௜ ∉ ߨ ௙ & ܨ௟೔,௟ೕா௙௙ < ቚܨ௟೔,௟ೕ ቚ ቚி೗೔,೗ೕ \௙ ቚ bw ; ݈௜ ∈ ߨ ௙ & ܨ௟೔,௟ೕா௙௙ < ቚܨ௟೔,௟ೕ ቚۙۖۖۘ ி೗೔ ,೗ೕಶ೑೑ ி೗೔ ,೗ೕಶ೑೑ିଵ ۖۖۗ (13)  (14)  ൰ ൗ ܽ௟೔௞ ௟೔ఢగ೗ೕೖ multiplexed flows from the ingress link ݅ . Each such flow is not  Equation (13) addresses the effective bandwidth of all the  transmitted over link ݆ while it still acquires VCs along its path to  destination (i.e. during the remaining path acquisition time).  the ratio ݐ ௞ ൬ݐ ௞ + ∑ Therefore, the packet generation rate of the flow is multiplied by  . Furthermore, ܤ௟೔,௟ೕ௙ depends on  whether all multiplexed flows from the ingress link ݅ can be  transmitted simultaneously on link ݆ or not. For the first case,  ܤ௟೔,௟ೕ௙ is equal to the sum of the bandwidths over all the  lower since not all the flows can be transmitted over link ݆.  multiplexed flows. For the latter case, the effective bandwidth is  Therefore, ܤ௟೔,௟ೕ௙ is decreased by the ratio of ܨ௟೔ ,௟ೕா௙௙ ቚܨ௟೔ ,௟ೕ ቚ . Hence,  (14) results in the effective bandwidth of the multiplexed flows  link ݅ of the multiplexed flows. Finally, since flow f is already  while taking into account the lack of VCs over previous links to  being transmitted and consequently occupies a VC, we use (12) in  order to bound the total number of interleaved flows over link ݆ by  The total transfer time of flow f, ݐ ௙ , which is dominated by the  hop with the smallest per-flow rate, is calculated by (3). As  mentioned above, for very short packets (4) can be used instead.   ܸ௟ ௝ − 1.   ൗ Figure 5. Rationale for using remaining path acquisition time  (An example)  4.3 Flit Transfer Time over a Link  Since flits from different flows are multiplexed over a link, the flit  transfer time of flow f over link ݆,	ݐ௟ೕ௙ , should account for the flittransmission of other flows on the same link. ݐ௟ೕ௙ is calculated by  modifying the basic M/M/1 service rate [11] to account for the  flows multiplexing:   ݐ௟ೕ௙ = ଵ భ೑೗೔೟஼೗ೕି஻೗ೕ೑ .  Where ܤ௟ೕ௙ [flits/sec] is the effective bandwidth consumed by all  flows other than flow f on link ݆.   ሺ11ሻ  Figure 6. The accuracy improvement of the iterative  procedure. (For end-to-end latency of flow 5 depicted in  Figure 5(a)).   4.4 Iterative Procedure for Finding ࣅ࢔ࢋ࢚࢝࢕࢘࢑ The analysis consists of a set of implicit equations for the  ࢌ 	              variables, ݍ ௙ , ܽ௟ೕ௙ , ݐ௟ೕ௙ , that can easily be solved (see Table 2).  Thereafter, we evaluate the end-to-end latency, ܶ௙ .As mentioned  generation rate of flow f, ߣ௙ , may cause inaccurate results. In  above, the actual packet injection rate to the NoC could be lower  than the packet generation rate. Therefore, using the given packet  order to obtain the packet injection rate to the network, ߣ௡௘௧௪௢௥௞ we first solve the equation set using the ߣ௙ values (i.e. ߣ௡௘௧௪௢௥௞ ,  ߣ௙ ). Thereafter, for every instable source queue, we use a lower   value and solve the equations again. We repeat this  procedure in an iterative manner until we achieve the minimal   that still causes instability at the source queue for each  proper flow.  ߣ௡௘௧௪௢௥௞ ௙ ߣ௡௘௧௪௢௥௞ ௙ = ௙ ௙ Figure 6 illustrates the accuracy improvement of the iterative  procedure for flow 5 depicted in Figure 5(a). It presents the endto-end latency when taking into account the packet generation  rates and the packet injection rates obtained by the procedure. It  can be seen that using the packet injection rates significantly  improves the accuracy.   5. NUMERICAL RESULTS   The analysis was programmed using Matlab. For a given NoC  architecture (i.e. capacity and number of VCs of each link,  topology and routing) we generate the equations (see Table 2) and  solve them using a standard Matlab non-linear solver. Then, we  compare the analysis results to an event-driven (flit-level) NoC  simulator written in OMNeT++ [17]. The implemented simulator  supports any heterogeneous NoC configuration in terms of every  link capacity and number of VCs. The simulator executes  wormhole switching with VCs employing round-robin arbitration  and deterministic XY routing. We simulate an asynchronous and  ideal router (i.e. no internal latency for the NoC router) with a  single flit input-buffer for each VC.   5.1 Evaluation of Use Cases  In order to evaluate our analysis, we use a 4x4 2D mesh NoC with  a single VC for all links. We simulate a uniform traffic pattern  with the same packet generation rate for all the sources. Then, we  evaluate the end-to-end latency of the flow over the diagonal path  ܰଵ,ସ → ܰସ,ଵ ). Figure 7 presents the end-to-end latency of this flow  of the NoC (i.e. from the left-bottom tile to the right-top tile,  reported by simulation, our analysis and the analysis presented in  [6]. It can be seen that our analysis results in a better  approximation. The inaccuracy of our analysis is less than 8% for  offered load less than 5 Gbps; the inaccuracy of the saturation  threshold is less than 4%. Moreover, using the ""infinite-VCs”  assumption results in inaccurate analysis [6].  In addition, we compare against the simulation the results of our  analysis for a multimedia application (MMS) introduced in [7].  This multimedia application used for the evaluation of end-to-end  latency for the methods proposed in [16] and [5]. We manually  map the application into 4x4 2D mesh NoC with a single VC for  all links. Figure 8 presents the average end-to-end latency as  reported by the simulation and our analysis. The difference  between our analysis and the simulation is less than 2%;  moreover, our analysis accurately computes  the saturation  threshold. The same accuracy is also achieved for a NoC with two  VCs for all links.   5.2 Synthetic Example  In this section, we present a synthetic example of heterogeneous  NoC (see Figure 9);  all  flows  have the  same  packet  generation   Figure 7. End-to-end latency of the diagonal flow ࡺ૚,૝ → ࡺ૝,૚  for 4x4 NoC with uniform traffic pattern, reported by:  simulation, our analysis and the ""infinite-VCs"" analysis  presented in [6].    Figure 8. Average end-to-end latency of multimedia  application (MMS) [7], reported by simulation and our  analysis.  rate. Figure 10 presents the end-to-end latency of each flow. As  can be seen, our analysis offers good approximation; the  inaccuracy of the saturation threshold is less than 4% for flow 1  and less than 2% for flows 2, 3 and 4.   5.3 Application and Benchmark-Based  Comparison  We evaluate the results of our analysis with trace-based CMP  traffic. We model a chip multi-processor (CMP) with a single  shared-cache (the cache line size is 256B) which acts as hotmodule. The traces of the benchmarks are produced using Simics  simulator [14] running SPLASH-2 [18] and PARSEC [4]  benchmarks. Then, we simulate the CMP using OMNeT++, where  each module generates packets according to a given trace.   Figure 9. Synthetic example of NoC with non-uniform   numbers of VCs and non-uniform capacities of links (Unless  noted links are 16Gbps and have sufficient number of VCs).  ࡯࢒૛ = ૚૙ ࡳ࢈࢖࢙; ࡯࢒૚૛ = ૚૛ࡳ࢈࢖࢙; ࢂ࢒ૠ = ૚.          Figure 10. The end-to-end latency of the flows in Figure 9.  We evaluate the average end-to-end latency using our analysis.  Furthermore, we compare our analysis to the one presented in [6]  which assumes an ""infinite"" number of VCs.  Figure 11(a) presents the results of the simulation, our analysis  and the analysis presented in [6] for several benchmarks. First, we  consider a homogenous NoC topology (i.e. all links have the same  capacity and number of VCs). The results are averaged for several  homogenous NoC topologies. The evaluation of our analysis  provides a good end-to-end latency approximation. Moreover, it is  much more accurate than the results of [6].  Figure 11(b) presents the average end-to-end latency results for a  heterogeneous NoC topology. The results are averaged for several  heterogeneous NoC topologies. Our analysis offers more accurate  results than [6]. Moreover, the accuracy differences between our  analysis and [6] is higher in comparison with the homogenous  topologies.   In addition, we model the SoC applications presented in [3] for  homogenous and heterogeneous NoC topologies. The applications  are manually mapped into the NoC. Figure 12 presents the    average    end-to-end    latency    results    of   the simulation, our  analysis and the analysis presented in [6]. The results are averaged  for several homogenous and heterogeneous NoC topologies  respectively. As can be seen, our analysis accurately computes the  end-to-end latency.   5.4 Run-Time Comparison  In this section, we demonstrate the run-time saving gained by  using our analysis compared to the simulation. The run-time is  measured for simulation executed with 15 cores and analysis  executed with a single core (i.e. the analysis uses 6.6% of the  resources in comparison with the simulation).  Table 3 presents the simulation run-time and our analysis run-time  for several cases presented along the paper. Our analysis offers  significant time and computing-resource saving (99.9% and  93.4% respectively).  6. CONCLUSION  A novel delay evaluation analysis to calculate the average end-toend latencies per flow of a heterogeneous NoC with variable link  capacities and number of VCs per link has been presented.  observed: the aggregate effective number of flows over link ܺ, the  Several crucial phenomena in a heterogeneous NoC were  effective number of flows from link ܻ to link ܺ and the remaining  path acquisition time. The quality of our approximation was  improved by using these observations. Our analysis offers  accurate end-to-end  latency evaluation for different NoCs  architectures and traffic scenarios.  Figure 11. A single shared cache NoC-based CMP with (a)  homogenous and (b) heterogeneous topologies. Comparison  between simulation, our analysis and the analysis presented in  [6] for SPLASH-2 and PARSEC benchmarks.  Figure 12. A single shared cache NoC-based CMP with (a)  homogenous and (b) heterogeneous topologies. Comparison  between simulation, our analysis and the analysis presented in  [6] for the applications presented in [3].  Table 3. Run-Time Comparison  (For Analysis Computing-Resource Saving  of 93.4%)  Simulation   Analysis  Run-Time [Sec]  Run-Time [Sec]  3780  9615  13725  12585  7455  4.34  5.4  11.4  4.45  4.7  Figure 4  Figure 5  Uniform traffic pattern (Figure 7)  MMS (Figure 8)  Synthetic example (Figure 9)  7. "
NoCs simulation framework for OMNeT++.,"As chip density keeps doubling every process generation, the use of Network-on-Chip becomes the prevalent architecture of SoC, MPSoC and large scale CMP designs. To that end, diverse NoC solutions are developed by the industry and the research community in order to meet heterogeneous on-chip communication requirements. Consequently, there is a growing need to rely on a simulation tools in order to explore, evaluate and optimize these new NoC architectures and topologies. The simulation platform is based on OMNeT++. It provides an open-source, modular, scalable, extendible and fully parameterizable framework for modeling NoC. In this demo we describe the structure of this framework.","NoCs Simulation Framework for OMNeT++  Yaniv Ben-Itzhak1    Eitan Zahavi1   Israel Cidon2    Avinoam Kolodny2  Electrical Engineering Department  Technion – Israel Institute of Technology  Haifa, Israel  1{yanivbi,ezahavi}@tx.technion.ac.il   2{cidon, kolodny}@ee.technion.ac.il  ABSTRACT  As chip density keeps doubling every process generation, the use  of Network-on-Chip becomes the prevalent architecture of SoC,  MPSoC and large scale CMP designs. To that end, diverse NoC  solutions are developed by the industry and the research  community  in  order  to meet  heterogeneous  on-chip  communication requirements. Consequently, there is a growing  need to rely on a simulation tools in order to explore, evaluate and  optimize these new NoC architectures and topologies. The  simulation platform is based on OMNeT++. It provides an opensource, modular, scalable, extendible and fully parameterizable  framework for modeling NoC. In this demo we describe the  structure of this framework.     Categories and Subject Descriptors  I.6.0 [Computing Methodologies]: Simulation and Modeling –  General.   General Terms  Measurement, Performance, Experimentation.  Keywords  Networks-on-Chip, NoC Simulator.  1. INTRODUCTION  Network-on-Chip has emerged as a new on-chip communication  approach. It offers better scalability, throughput, overall latency  and area compared to bus-based on-chip interconnect. However,  the NoC design space is very large and high dimensional. It  includes the optimization of topology, traffic switching technique,  routing mechanism, congestion control methodologies,  link  capacities, number of buffers and virtual channels per  link, etc…  Furthermore, the NoC research area is still at its infancy and  consequently new architectures, techniques and ideas are being  proposed, developed and evaluated. Simulators are essential tools  in evaluating the performance of different NoC designs and new  proposals. Therefore, in order to be able to cover the existing and  future NoC diversities, NoC simulators should be modular,  scalable, extendible and fully parameterizable.   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  NOCS’11, May 1–4, 2011, Pittsburgh, PA, USA.  Copyright 2011 ACM 978-1-4503-0720-8…$10.00.  Table 1. NoC Simulators Comparison  Frame-  work  Availability  Parallel -ism  Topologies  OpenSource  SICOSYS [7]  C++  Noxim [2]  SystemC  NNSE [6]  SystemC  Nirgam [4]  SystemC  gpNoCsim [3]  Java  [1]  OMNeT++  DARSIM [5]  C++  Our Simulator OMNeT++  +  +  +  +  +  -  +  +  -  -  Limited  Mesh  - Mesh/Torus  -  -  +  +  +  All  All  All  All  All  +  +  +  +  +  -  +  +  In this demo we introduce an OMNeT++ based modular and  open-source NoC simulator. Several previous NoC simulators  have been presented; however, none of which satisfies all the  aforementioned requirements needed to cover the design and  research space. Table 1 presents a comparison between previous  NoC simulators and our proposed simulator. The simulator is  based on OMNeT++ [8], which is an extensible, modular, opensource component-based C++ simulation library and framework,  primarily aimed at building network simulators.   OMNeT++ provides several  important advantages  to  the  simulation framework. It offers easy traceability and debugutilities to reduce debug-time, and built-in parallelism support to  reduce simulation run-time. Moreover, it supports flexible and  efficient topology definition using NED, the OMNeT++ topology  description language. NED has a simple syntax. Yet, it is very  powerful  for defining arbitrary  topologies. Furthermore,  OMNeT++ offers free license for academic usage. All these  advantages give it the potential to become a highly beneficial and  extensible open source simulation platform for the service of the  NoC research community.  Currently,  it  supports  three  router  types:  asynchronous and virtual output queue (VoQ).   synchronous,  2. SIMULATOR STRUCTURE  The simulator architecture is optimized for extendibility and  comparative architecture evaluation. This  requirement  is  translated to several major features: the use of module interfaces,  message classes, modules directory, support for arbitrary  topologies and module selection as a simulation parameter. In this  section we focus on describing  the modules,  the  traffic  configuration and the statistics collection.   An OMNeT++ module represents a hardware or a software entity  that is capable of receiving messages (from itself or other  modules) and implements a self-contained logic. Modules are             3. NUMERICAL RESULTS   We present comparison between three routers implemented by the  simulator (i.e. synchronous, asynchronous and virtual output  queue (VoQ)) for a uniform traffic pattern. Figure 2 presents the  average throughput and end-to-end latency for a uniform traffic  pattern over 4x4 NoC with two VCs.  Figure 1. A 4x4 mesh NoC: (a) a complete mesh built from  routers and cores; (b) a hierarchical router of 5 ports; (c) a  core structure; (d) a single port structure.  declared by specifying their attributes and their “ports” – where  messages can arrive or leave. A module-interface is a skeleton  that has no real module implementation behind it. Modules may  declare their adherence to such an interface (meaning the set of  ports and properties) and thus may be used in place of the  skeletons in a dynamic manner.   Generally, a NoC is built from two main modules: Routers and  network interfaces (NI) as shown in Figure 1(a) (the NIs are  named  “core[*]”   and  routers  “router[*]"").  Internally  the NI  contains sources and sinks (see Figure 1(c)). Routers are either  flat or hierarchically built as a collection of connected ports. Flat  router has no sub-modules. Hierarchical routers consist of ports as  depicted in Figure 1(b). The ports include scheduler (sched),  input-buffer (inPort), VC-allocator (vcCalc) and output-port  selector (opCalc), as shown in Figure 1(d). The scheduler  arbitrates the VC to win the output of the port which implements  the switch allocation pipe-stage. The input port (inPort) stores the  incoming flits in a set of buffers. The VC-allocator decides about  the output VC for each packet. The output port selector  implements the routing decision. The switch is built by the cross  connections of port (Port_Ifc) outputs (driven by the input-buffer  and named “sw_in”)  to the other ports inputs (driving the  scheduler and named “sw_out”).  The traffic is configured by setting the destination and packetarrival time parameters for each source. The destination can be  either deterministic or randomly distributed. The distributions are  set using the built-in OMNeT++ random number generation  functions (e.g. intuniform). The packet inter-arrival times can be  either distributed (e.g. exponential, constant) or driven by a trace  file. The trace file includes the specific times where a packet  should be generated. Hence, it can cover a variety of traffic  patterns and schemes.   The simulator provides a rich set of statistical measurements  collected by the sink, the source and the in-port modules. The  sinks collects throughput and different latency statistics at the flit  and package levels. The source collects several source queue  indicators in order to identify the point of NoC saturation. The inport collects VC acquisition latencies (i.e. latency to acquire an  out-port VC) and transfer latencies.   Figure 2. Uniform traffic pattern. (a) Average end-to-end  latency versus offered load; (b) Average throughput versus  offered load.  4. SUMMARY  A modular NoC simulator based on OMNeT++ framework has  been presented. Several advantages of the simulator compared to  previous NoC simulators were discussed. It was demonstrated that  it offers an open-source, scalable, extendible and  fully  parameterizable framework for modeling NoC.   5. REFERNCES  [1] R. Al-Badi, M. Al-Riyami, and N. Alzeidi. A parameterized  NoC  simulator using OMNet++.  In Ultra Modern  Telecommunications & Workshops, 2009.  ICUMT’09.  International Conference on, pages 1–7. IEEE, 2009.  [2] F. Fazzino, M. Palesi, and D. Patti. Noxim: Network-on-chip  simulator, 2008.  [3] H. Hossain, M. Ahmed, A. Al-Nayeem, T. Islam,  and  M. Akbar. Gpnocsim-A General Purpose Simulator  for  Network-On-Chip.  In  Information and Communication  Technology, 2007. ICICT’07. International Conference on,  pages 254–257. IEEE, 2007.  [4] L. Jain, B. Al-Hashimi, M. Gaur, V. Laxmi, and A. Narayanan.  NIRGAM: a simulator for NoC interconnect routing and  application modeling. In Workshop on Diagnostic Services in  Network-on-Chips, Design, Automation and Test in Europe  Conference (DATE’07), pages 16–20, 2007.  [5] M. Lis, K. Shim, M. Cho, P. Ren, O. Khan, and S. Devadas.  DARSIM: a parallel cycle-level NoC simulator. In 6th Annual  Workshop on Modeling, Benchmarking and Simulation.  Citeseer, 2010.  [6] Z. Lu, R. Thid, M. Millberg, E. Nilsson, and A. Jantsch.  NNSE: Nostrum network-on-chip simulation environment. In  Swedish System-on-Chip Conference (SSoCC’03), pages 1–4.  Citeseer, 2005.  [7] V. Puente,  J. Gregorio, and R. Beivide. SICOSYS: an  integrated framework for studying interconnection network  performance  in multiprocessor  systems.  In Parallel,  Distributed  and Network-based  Processing,  2002.  Proceedings. 10th Euromicro Workshop on, pages 15–22.  IEEE, 2002.  [8] A. Varga et al. The OMNeT++ discrete event simulation  system.  In Proceedings of  the European Simulation  Multiconference (ESM’2001), pages 319–324, 2001.      "
Efficient routing implementation in complex systems-on-chip.,"In application-specific SoCs, the irregularity of the topology ends up in a complex implementation of the routing algorithm, usually relying on routing tables implemented with memory structures. As system size increases, the routing table increases in size with non-negligible impact on power, area and latency overheads. In this paper we present a routing implementation for application-specific SoCs able to implement in an efficient manner (without requiring routing tables and using a small logic block in every switch) a routing algorithm in these irregular networks. The mechanism relies on a tool that maps the initial irregular topology of the SoC system into a logical regular structure where the mechanism can be applied. We provide details on the mapping tool as well the proposed routing mechanism. Evaluation results show the effectiveness of the mapping tool as well as the low area and timing requirements of the mechanism. With the mapping tool and the routing mechanism complex irregular SoC topologies can now be supported without the use of routing tables.","Efﬁcient Routing Implementation in Complex Systems-on-Chip José Cano Parallel Architectures Group Universitat Politècnica de València, Spain José Flich Parallel Architectures Group Universitat Politècnica de València, Spain José Duato Parallel Architectures Group Universitat Politècnica de València, Spain jocare@gap.upv.es jﬂich@disca.upv.es jduato@disca.upv.es Marcello Coppola Riccardo Locatelli STMicroelectronics Grenoble, France STMicroelectronics Grenoble, France marcello.coppola@st.com riccardo.locatelli@st.com ABSTRACT In application-speciﬁc SoCs, the irregularity of the topology ends up in a complex implementation of the routing algorithm, usually relying on routing tables implemented with memory structures. As system size increases, the routing table increases in size with non-negligible impact on power, area and latency overheads. In this paper we present a routing implementation for application-speciﬁc SoCs able to implement in an eﬃcient manner (without requiring routing tables and using a small logic block in every switch) a routing algorithm in these irregular networks. The mechanism relies on a tool that maps the initial irregular topology of the SoC system into a logical regular structure where the mechanism can be applied. We provide details on the mapping tool as well the proposed routing mechanism. Evaluation results show the eﬀectiveness of the mapping tool as well as the low area and timing requirements of the mechanism. With the mapping tool and the routing mechanism complex irregular SoC topologies can now be supported without the use of routing tables. Categories and Subject Descriptors C.2.2 [Processor architectures]: Other Architecture Styles— Heterogeneous (hybrid) systems ; C.2.2 [Computer Communication Networks]: Network Protocols—Routing protocols ; D.2.8 [Software Engineering]: Metrics—Performance measures General Terms Design, Experimentation Keywords Systems-on-Chip; Networks-on-Chip; Routing Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS ’11, May 1-4, 2011 Pittsburgh, PA, USA Copyright 2010 ACM 978-1-4503-0720-8 ...$10.00. 1. INTRODUCTION As technology advances, systems-on-chip (SoC) designs become more complex with the inclusion of many IP components. Tens (and in the near future several hundreds) of elements need to be connected within the same chip, thus requiring an eﬃcient on-chip interconnect. Usually, the system is designed taking into account the future application that will be running on the system, thus, the design is customized and adapted to the application needs. Traﬃc patterns are known in advance, and the interconnect is customized. The net result of such design approach is a network within the chip ([1], [2]) with no regular shape and with varying switch complexities and link bandwidths. Figure 1 shows a possible example where IP blocks are connected by using an on-chip network with 28 switches. The target utility of this example (and similar ones) could be a high-end home multimedia entertainment subsystem or an application processor. As can be observed, the topology of the network is totally irregular and heterogeneous. Figure 1: Example of a complex irregular topology for an application-speciﬁc SoC system. P means producers and C means consumers. Two key pillars of an interconnect are the topology and the routing algorithm. The topology sets the physical connection pattern between end nodes, and as indicated previously, in application-speciﬁc SoC systems is usually irregular. However, there are other topologies with regular patterns like 2D meshes, tori and fat trees, mostly used in other environments like Chip MultiProcessor (CMP) systems. The routing algorithm, on the other hand, sets the paths that messages need to take within the network. One important issue in the routing algorithm is the prevention of deadlock. A deadlock situation occurs when messages in the network block cyclically forever as they request resources already occupied by other messages. Since message dropping (and later retransmission) is not eﬃcient in such systems, the routing algorithm needs to be designed carefully in order to prevent any potential deadlock situation. Once the topology is set, then, the routing algorithm needs to be applied and messages need to be instructed about the paths to follow. In order to implement the routing algorithm two trends can be followed: source routing [3] and distributed routing [3]. In source routing, the entire path of the message is encoded in the message header, and switches simply read the header and take the corresponding output port to forward the message. In this case, the sender node has memory blocks (tables) to encode all the possible paths for every destination node. Also, as the system size increases, paths tend to increase and the packet header increases, thus sometimes requiring more network bandwidth. In distributed routing, the message header includes the destination identiﬁer and switches are in charge of computing the appropriate output port for the message. In this situation, the length of the header is independent of the system size. However, switches need to implement the routing algorithm. Today, few MPSoC systems are using regular topologies (like picoArray technology [4], Tilera products [5] and AsAP [6]). In these cases, a simple, yet powerful, routing algorithm (e.g. DOR routing [3]) can be implemented with minimum cost (few gates) on every switch (distributed routing). Simply, the coordinates of the destination switch in the message are compared with the coordinates of the current switch. Contrary to this, the ma jority of application-speciﬁc SoC systems in current products are using irregular topologies based on well-known on-chip technologies (examples are Spidergon STNoC [7], Arteris NoC [8], Sonics MicroNetwork [9] and AMBA [10]). Those irregular solutions are mainly based on source routing and address decoding, and normally need a complex implementation of the routing algorithm (with routing tables using memory structures). Indeed, the lack of regularity in the topology prevents simpliﬁcations in the routing algorithm design. As system size increases, the routing table increases in size with non-negligible impact on power, area and latency overheads (for a comparison between logic-based routing and tables, refer to [11]). In this paper we address the implementation of the routing algorithm in application-speciﬁc SoC systems where the topology is set by the application, thus being totally irregular. The aim is to design a mechanism and an algorithm that enables the use of table-less distributed routing on every switch with a constant and reduced logic cost, regardless of system size. The proposal builds from the Logic-Based Distributed Routing (LBDR) mechanism [11], a previous proposal suited for CMPs and high-end MPSoC systems where initial regular 2D mesh topologies are used but manufacturing defects end up inducing some irregularities in the topology. LBDR is able to implement in an eﬃcient manner (without requiring routing tables) a routing algorithm in most of these topologies. In this paper we extend the LBDR approach to cover complex topologies derived from SoC designs, thus enabling the use of the LBDR approach in application-speciﬁc SoC systems. We also provide a tool able to map the initial irregular topology into a logical regular structure where the enhanced LBDR approach can be used. By doing this, the routing algorithm can be eﬃciently implemented in the SoC design with no need of routing tables and with no topology change. The rest of the paper is organized as follows. Section 2 shows the related work with some routing solutions for irregular topologies. In Section 3 we ﬁrst describe the concrete contribution of the paper in a preliminary subsection, in order to clarify and focus the description of the mechanism and the mapping tool. Then, we describe the LBDRx mechanism to cover practical topologies from SoC designs. In Section 4 we describe the mapping tool. Finally, in Section 5 we provide evaluation results and conclude the paper in Section 6. 2. RELATED WORK There has been considerable work on routing algorithms for irregular NoCs. In [12], authors use well-known principles from parallel computer architecture to develop a deadlock free highly adaptive solution for irregular mesh-based NoCs. Bolotin et al. [13] propose three hardware eﬃcient routing methods that combine a ﬁxed routing function (such as XY routing) and reduced size routing tables based on distributed and source routing techniques. For each method, they developed path selection algorithms that minimize the overall cost. Finally, in [14], authors propose a synthesis approach that, depending on the degree of routing ﬂexibility, can reduce very signiﬁcantly the area cost of the conﬁgurable source routing tables by adopting partially reprogrammable routing logic instead of fully reprogrammable tables. Conﬁgurable routing provides intelligent adaptation without impacting the consistency of traﬃc ﬂows. None of the solutions shown allow the implementation of distributed routing algorithms in irregular NoCs topologies with no routing tables and minimum logic. In general, in application-speciﬁc designs, the topology is set without accounting for the routing algorithm. Once set, routing paths are searched and their implementation is assumed to be with routing tables, due to the excessive irregularity required by the application. Our approach tries to remove tables in such environment by minimizing and condensing all the routing information in a small and bounded set of bits. 3. LBDRX DESCRIPTION The description of the proposed LBDRx mechanism will be presented as an evolution from the basic LBDR mechanism previously proposed (with low coverage for complex irregular topologies) to the most enhanced version (with full coverage to all the complex topologies analyzed). 3.1 Preliminary: Basic Idea Prior to describing the mechanism and the mapping tool, we need, however, to brieﬂy describe the basic idea. In regular networks, e.g. a 2D mesh network, the regular connectivity pattern is useful when designing the routing algorithm. Indeed, with the Dimension-Order routing (DOR), the implementation is quite straightforward as messages are forwarded with minimal paths ﬁrst in the X direction and then in the Y direction. Thus, there is no need for a routing table, only a set of gates is enough. This renders to an eﬃcient implementation in terms of area, power, and delay. If we consider small irregularities on 2D-mesh networks, for instance due to manufacturing defects, then the inherent irregularity complicates the routing implementation. For instance, DOR is no longer valid as some paths are not possible now. However, other routing algorithms are still suitable for such topologies, for instance, topology-agnostic routing algorithms like up*/down* [15]. Their implementation is usually performed with routing tables. Eﬀorts to provide As we can see in the ﬁgure, there are switches with varying connectivity patterns with other switches. For instance, switch 1, mapped at row 2 and column 2, is connected to switches 4, 2, and 21 with diﬀerent link mapped lengths. 1 In particular, mapped length of links are 2 hops for links connecting to switches 2 and 4, and 3 hops for the link connecting to switch 21. In addition, links with the same number of mapped hops have diﬀerent orientations/directions, thus, being diﬀerent. This is the case for link connecting to switch 4 which is located one hop north and one hop west from switch 1, and link connecting to switch 2 that is two hops north. As previously described, LBDR relies on switches with up to four ports, and each one connecting switches in one direction in the 2D mesh plane (N, E, W and S). Also, the maximum distance covered with a link is 1 hop in the 2D grid. Thus, the links with higher mapping lengths are not supported, and thus, the mapped topology is not supported by LBDR. In order to overcome this limitation, the new mechanism, LBDRx, allows for switches with up to 20 ports for connecting to other switches (ports used to connect end nodes are excluded). Also, any of these ports can be conﬁgured as a 1-hop port, a 2-hop port, or a 3-hop port. A X-hop port connects two switches that are at mapping distance X. In order to uniformly refer to X-hop ports, we deﬁne additional directions. In particular, 20 diﬀerent directions are supported for the ports, and each of the 20 possible ports of the switch can be conﬁgured to any of the 20 directions. The supported directions are: the initial 1-hop four directions (supported by LBDR): N, E, W, S; four 2-hop straight directions NN (two hops along the north direction), SS, EE, WW; four 2-hop diagonal directions NE (one hop north and one hop east), NW, SE, SW; and eight 3-hop directions: NNE, EEN, EES, SSE, SSW, WWS, WWN, NNW. Figure 3 shows all the possible port directions supported by LBDRx. eﬃcient implementations of such algorithms in those irregular topologies have been performed in the recent years. One important method is LBDR that collapses all the routing information required on every switch on a small set of bits, thus reducing signiﬁcantly implementation costs. LBDR relies still on the fact that the topology is the 2D mesh network but with some missing links. Adding some bits enables LBDR to successfully deal with the irregularity induced by missing links. However, LBDR still relies on the fact that every switch has at most four links connecting neighboring switches (at north, east, west, or south directions). LBDR uses, as DOR does, the coordinates of the destination switch in the message and the coordinates of the current switch, to compute the appropriate set of output ports. Thus, LBDR still beneﬁts from the original 2D-mesh layout. In this paper, what we propose is the extension and applicability of the LBDR concept to truly irregular and applicationspeciﬁc networks (as an example see Figure 1). The approach we follow is to map the topology into a 2D grid (notice, however, we do not change the initial topology). Once the topology is mapped, we need to provide coordinates to every switch in the network. Based on the coordinates of the destination switch and the current switch, the derived LBDR logic will decide the output port that needs to be used to forward the packet towards its destination. In order to correctly map the topology into a 2D grid we have developed a mapping algorithm that will search the space of combinations and will deliver the most suitable ones, always guaranteeing deadlock freedom and connectivity. Due to the mapping performed, and because of the high irregularity we will ﬁnd, some switches will require a varying number of ports to connect to other switches, and in that situation some links will connect switches not placed closely in the 2D grid. This kind of connectivity has not been provided by the original LBDR mechanism, and thus, requires modiﬁcations. In this paper, we further extend LBDR for its support in this kind of mappings. 3.2 LBDR extension: LBDRx We start the description with the mechanism required at every switch to deal with the irregular topologies. In order to be concise, we take as a reference the mapping of the initial topology (shown in Figure 1) that appears on Figure 2. This mapping is obtained with the mapping tool that will be described in the next section. The mapping is representative of all the connectivity patterns between switches that we need to address in this section. Figure 3: Possible directions in LBDRx. Simpliﬁed versions of the mechanism can be conceived by restricting the type of ports that can be supported. For instance, a LDBR mechanism is embedded in the proposed mechanism when only 1-hop ports are allowed. Another implementation is allowing 1-hop and 2-hop ports only, thus obtaining a LBDR2 mechanism. Therefore, the LBDRx proposal can be seen as a method to further extend the connectivity of switches when mapped on a 2D grid. As we will see 1 It is worth mentioning that the link length is set in the original unmapped topology (Figure 1) and the showed length in the mapped topology (Figure 2) is the same, although in the grid is set by the number of hops in each direction. We will refer to physical length for original unmapped topologies, and to mapped lengths for link lengths in the mapping layout. Figure 2: Mapping example for the initial topology. in the evaluation section, LBDR3 is enough to map all the tested topologies, thus not requiring a more complex implementation. Anyway, we will show also results for the LBDR2 approach. As an example, the mapped topology in Figure 2 requires some ports being 3-hops (link from switch 1 to switch 21), thus not being suitable for an LBDR2 approach. It is worth mentioning that, although 20 ports are allowed on every switch, not all of them need to be implemented. Indeed, only a subset of ports will be implemented, e.g. switch 1 at Figure 2 will be implemented with only 3 output ports. Figure 4: Logic of LBDRx. The logic required for LBDRx is shown in Figure 4. The mechanism relies on some conﬁguration bits grouped in two sets: routing bits and connectivity bits. Routing bits indicate which routing options can be taken, whereas connectivity bits indicate whether a switch is connected with its neighbors. As an alternative view, the connectivity bits set the mapped topology and the routing bits set the routing algorithm. As we support 20-port switches, at maximum we will have 20 connectivity bits per switch. We represent the connectivity bit for a port X as Cx , where X can be a possible direction of any mapped link (N, E, W, S, NN, NE, ..., NNE, ...). Notice these bits will be hardwired depending on the ﬁnal topology and the radix of each switch, thus not being implemented with ﬂip-ﬂop registers. The routing bits Rxy (where x and y can be n, e, w, and s) indicate whether messages routed through the x output port may take at the next switch the y port. In other words, these bits indicate whether messages are allowed to change direction at the next switch. The value of these bits is computed in accordance to the applied routing algorithm and to prevent deadlock while still guaranteeing connectivity. In order to simplify the routing logic, however, not all the possible routing bits are implemented. Indeed, no new routing bits are used except those already deﬁned in LBDR: Rne , Rnw , Ren , Res , Rwn , Rws , Rse , Rsw . Notice that routing bits are used only between 1-hop links. By default, the LBDRx mechanism will assume messages can take 2-hop and 3-hop links without restriction along their path without risk of inducing deadlock. The mapping strategy described in section 4 will guarantee in those cases the absence of deadlocks. Although allowing more routing bits would lead to greater ﬂexibility, we noticed that they are not needed in order to reach our ob jective (shown in the evaluation section). This will also help to keep a low implementation cost of the mechanism. Routing logic of LBDRx is divided into two parts (see Figure 4). The ﬁrst part of the logic computes the relative position of the message’s destination. For this, two comparators are used and coordinates of the current switch (Xcurr and Ycurr ) are compared with the coordinates of the message’s destination (Xdst and Ydst ) located in the message header. At the output of this logic one or two signals may be active (e.g. if the packet’s destination is in the NW quadrant then N’ and W’ signals are active at the same time). Note also that packets forwarded to the local port are excluded from the routing logic. Additionally, four extra signals, NN’, EE’, WW’ and SS’ are computed. These signals are set to one if the message’s destination is at least two hops away in the corresponding direction (if NN’ is active, then at least two hops must be performed in the N direction to get closer to its destination). These signals can be easily computed with additional comparators with the Xcurr and Ycurr coordinates shifted in one position. Notice that in some situations diﬀerent signals will be active at the same time, for instance signals NN’ and N’. These cases are ﬁltered in the second part of the logic. Higher priority will be given to larger hop ports. We refer to the signals produced by the comparators as intended direction signals. Once the direction signals are computed, the second part of the logic comes into play. It consists of a logic at each input port in the switch. Figure 4 shows the details. The logic is divided in three parts in order to address the logic for the diﬀerent type of output ports (1-hop, 2-hop, and 3-hop ports). Notice that 3-hop ports have the highest priority followed by 2-hop ports. This means that if a 3-hop output port is eligible for routing a message then, ports with lower mapping length will not be considered. To implement this priority scheme, two control signals (2hop and 3hop signals) are used. Besides this, the logic to compute 2-hop and 3-hop ports is quite straightforward. Indeed, a port X is eligible if the port exists in the switch (Cx bit is set), and the message’s destination is in the same direction of the output port (direction signals). As an example, output port NNE is eligible for routing if the message’s destination is in the NNE direction (both direction signals NN’ and E’ are set). The logic for 1-hop ports is, however, slightly more complex. It deals also with the routing bits. The logic, in this case (excluding the priority signals) is implemented with two inverters, four AND gates and one OR gate. The logic ﬁlters out the routing options that could lead to deadlock situations (by using the routing bits). Figure 4 shows the logic for the North 1-hop output port. As an example, the incoming message is forwarded to the North output port (signal N” is set) if either one of the following three conditions is met: (cid:15) The message’s destination is on the same column (N E ). (cid:15) The message’s destination is on the NE quadrant and the message is allowed to take the E port at the next ′ (cid:2) Rne ). switch through the N port (N (cid:15) The message’s destination is on the NW quadrant and the message can take the W port at the next switch ′ (cid:2) Rnw ). through the N port (N Obviously, connectivity bits and priority signals are used in combination with the previous logic. The logic for the 1-hop output ports is the same used in LBDR. For a deeper description of LBDR, refer to [11]. ′ (cid:2) W ′ (cid:2) ′ ′ (cid:2) E ′ (cid:2) W Figure 5: (a) Non-minimal path support for LBDRx. (b) LBDRx conﬁguration bits (with deroutes) for the mapping shown in Figure 2 (Routing bits are 1 when no restiction is applied; Connectivity bits are 1 when the switch is connected to another. Deroute bits are ”A” when the ﬂow departs from itself ). 3.3 Support for Non-minimal Mapped Paths The previous logic guarantees minimal path routing in the mapped topology. As each output port is used when the destination is located in the same direction of the output port, then every hop performed guarantees the message will get closer to its destination. Indeed, this fact renders with a very simple implementation of the routing algorithm (as seen in Figure 4). However, there are mapping cases that can not be solved with only minimal path support. As an example, in the mapped topology shown in Figure 2 a message going from switch 11 (mapped in row 4 and column 3) requires a mapped non-minimal path to reach switch 21, as it needs to be forwarded N and then WWS.2 This fact simply renders the mapped topology as unsupported by LBDRx (Figure 4). One possible solution is to discard the mapped topology and obtain one that guarantees all the paths will follow minimal paths. Indeed, this is one of the targets of the mapping tool shown later in the paper. However, in some complex topologies, this kind of mappings will simply not exist. We evaluate this issue later. To solve this problem in a smooth way, and allowing much more ﬂexibility to the mechanism, we introduce a small additional logic on every input port to allow such non-minimal path support. Indeed, going back to the non-minimal path example in the ﬁgure, if at switch 11 we force the message to go north, then the message will be able to reach its ﬁnal destination and the mapping will be valid. Figure 5 (a) shows the logic for the non-minimal support we propose. The logic is borrowed from [16]. In particular, the logic forces messages to take a non-minimal port whenever the LBDRx logic fails in routing the message. For the example provided, at switch 11 none of the output ports will be eligible by LBDRx. Thus, in this case, the logic will take the conﬁgured non-minimal port. In this case, the selected port is N. The logic requires a conﬁguration register per input port, in our case, of size 5 bits (to select an output port out of maximum 20 ports). The logic uses a demultiplexer to decode the output port. Notice that deroutes are taken only if the LBDRx logic does not provide a valid output port. As an overall example, in Figure 2 we can observe how a message being forwarded from switch 3 to switch 27 is using the conﬁguration bits shown in Figure 5 (b). At switch 3, the message could take port SSW, or port SS. However, the 3-hop port (SSW) ﬁlters out the 2-hop port (SS). The 2Notice, however, the path in the original unmapped topology is minimal and the same. message, then, moves to switch 1 and from that switch takes output port SSW again. Finally, at switch 21 the port W is selected. This example is straightforward, and as can be easily deduced the way the topology is mapped onto a 2D grid will inﬂuence on the applicability of the LBDRx mechanism. Furthermore, in Figure 2 two diﬀerent deroute options are required for two diﬀerent input ports at router 11 (see Figure 5 (b)). If going W, and the message comes from input port S, then a deroute is set to E. On the other hand, if the message is coming from SW, and the intention is to go SSW, then a deroute is set to S. It is worth mentioning that the deroute option needs to be computed in accordance to the routing algorithm, as it must not introduce cycles that could lead to deadlocks. For example, if we would have a routing restriction South-East at router 11 in Figure 2, a deroute option at input port S at router 11 can not be set to E as it would let messages crossing the routing restriction. In this example, the only restriction is located at router 12. As a ﬁnal remark for the LBDRx routing mechanism, its success depends strongly on the mapping performed for the topology. Indeed, there are mappings that allow all the links to conform to the LBDRx link structure (1-hop, 2-hop, 3-hop links) and there are mappings that have larger links. Also, there are mappings that introduce deadlocks in the routing algorithms or even prevent the connectivity between particular source-destination ﬂows. Therefore, the mapping tool, described in the next section, is a key element to guarantee applicability of the LBDRx mechanism. 4. MAPPING TOOL In this section we describe the mapping tool required by the LBDRx mechanism. The mapping tool is adapted to diﬀerent versions of LBDRx routing, e.g. with 3-hop links, 2-hop links, and with/without non-minimal path support. The mapping tool takes as an input the topology and the type of LBDR support and outputs several possible solutions, each of them able to be used with the target LBDRx version. Indeed, the mapping tool (see Figure 6) provides for any possible solution the set of conﬁguration bits together with the mapping coordinates of every switch into a 2D grid. It is worth highlighting that the mapping tool does not physically change the topology, indeed it only logically maps the topology onto a 2D grid. Figure 7 shows the diﬀerent stages of the mapping tool. For the sake of understanding, in the next subsections we describe the details of each stage along with an example. 1. Any switch has at maximum 12 outgoing ports and 12 incoming ports, possibly having less number of ports, and not necessarily the same number of input and output ports. 2. In every switch one possible direction out of 12 can be taken, in the 2D mesh mapping, through a single output port. The directions are the ones supported by LBDR2 (2-hop and 1-hop links depicted in Figure 3). Taking into account the previous restrictions, some mappings will become not valid, e.g. those with link lengths longer than the targeted LBDRx version or those that lead to unconnected networks. In any case, those mappings are excluded. Figure 9: Example of (a) connectivity pattern applied to two diﬀerent mappings, and (b) mapped topology with the routing algorithm applied. Figure 9(a) shows the connectivity pattern for the previous two mapping cases. As can be seen, the second mapping case is not compatible with LBDR2 since it contains a 3-hops link. Furthermore, we can observe how this mapping ends up in an unconnected network (once the routing algorithm is applied). The issue comes from the fact that switch 1 cannot be reached from switch 3 with the LBDR2 logic. From the point of view of switch 3, switch 1 is at the North-East quadrant but there is no North-East link nor any combination of 1-hop links (with North and East directions) leading to the destination, instead, the North-East-East exists. The ﬁrst mapping case does not suﬀer from that problem, and thus, that case is compatible with LBDR2. Mappings leading to unconnected networks are ﬁltered out at this stage. Notice however, that if we use LBDR3 or deroutes are allowed, both topologies are, then, supported. 4.3 Compute a Proper Routing Algorithm Once we have obtained a correct mapping we need to check whether the mapped topology contains cycles. In that case, in order to avoid cycles, it will be necessary to apply a routing algorithm. In the ﬁrst mapping in Figure 9(a), a cycle can be formed between switches 0, 3, and 2 in the counter clockwise direction. Applying a routing algorithm on top of the topology will remove such cycle. In our case, the routing algorithm used is the segmentbased routing (SR) [17], a technique that divides the network into segments and puts a routing restriction in each segment. A routing restriction is placed between two consecutive links and prevents any message from using both links sequentially. Drawing routing restrictions is a way of representing a routing algorithm since restrictions establish the allowed paths, those not crossing routing restrictions. In order to compute the routing restrictions, only 1-hop links in the mapped topology are assumed. As commented above, this assumption simpliﬁes the LBDRx logic and still allows to reach our ob jective of avoiding cycles. Figure 9(b) shows the valid mapped topology in Figure 9(a) with the unidirectional routing restriction applied only at switch 2. Notice that no cycles exist without crossing the routing restriction. LBDR2 computes the routing bits from the routing restrictions deﬁned by the routing algorithm. Figure 6: Mapping tool. Figure 7: Stages in the mapping tool. 4.1 Compute Mapping of Switches The ﬁrst stage provides an initial mapping of the switches into a 2D grid. Some basic assumptions are considered: 1. Only links and switches are considered for the mapping, thus not considering end nodes. 2. The mapping grid (the diameter of the 2D mesh) will be minimized and made as square as possible (diﬀerences between diameters of each dimension will be minimized). 3. Every possible mapping of switches onto the 2D mesh is explored, and the best solutions are extracted and further analyzed (in the following stages). Figure 8: Example of two initial mappings. The last assumption may lead to a large number of mapping combinations, most of them will result in mappings not supported by LBDRx. As an example, Figure 8 shows two possible mappings corresponding to the example topology provided in Figure 6. Considering LBDR2, at ﬁrst sight we cannot deduce which one can be supported. For this, we need the next step: computing directions and link connectivity (connection pattern). 4.2 Compute the Connection Pattern For each mapping in the previous step, the connection pattern needs to be performed. The connection pattern considers only links connecting switches (switch-to-switch links) and the direction of each link (unidirectional links are considered). Several restrictions are enforced in this step (considering LBDR2): 4.4 Check Deadlock and Connectivity 5.1 Mapping tool analysis The last step of the mapping tool is to verify the mapping is deadlock-free and guarantees the connectivity of the initial topology. The routing algorithm applied in the previous step ensured deadlock-freedom. However, when applying the routing algorithm and when not using deroutes, some pair of end nodes may be unconnected. The reason is because LBDRx without deroutes relies exclusively on minimal paths, those always getting closer to its destination (the comparator modules in the ﬁrst part of the logic enforce this property). Therefore, a routing restriction may lead to a path being routed non-minimally, which needs the use of deroutes to be supported. At this stage, the tool iterates on all the communicating ﬂows of the application (a ﬂow is deﬁned as the path from a producer to a consumer). For each ﬂow, the tool searches a valid LBDRx path using the connectivity and routing bits set by the mapped topology. If, for a ﬂow there is no connectivity then the mapping is not valid and we will need either to search a new mapping or use deroutes. On success of a mapping topology, the ﬁnal output is the mapping of each switch into the 2D grid and the conﬁguration (connectivity, routing, and deroute) bits. Notice that many mapping solutions exists and the mapping tool succeeds if at least one mapping solution is obtained. Also, if no mapping solution exists for a grid size, the mapping tool extends the grid by one row and/or column thus having much larger ﬂexibility. However, this will incur in larger register ﬁles at switches to store the relative position of the switch in the grid. As an example, Figure 2 shows a successful deadlock-free mapping for the initial topology depicted in Figure 1 where connectivity between switches is assured (due to its complexity, the version used in order to obtain that mapping was LBDR3 with deroutes). Now we describe how deroutes are computed. Once LBDRx bits are computed the deroute options are searched. To do this, the algorithm looks for a valid path for every source-destination pair (the algorithm is computed oﬄine before any normal operation of the chip, thus computation complexity is not a ma jor issue). As LBDRx may allow multiple paths for a given source-destination, the algorithm deeply searches all the paths in a recursive way. Two end nodes are connected by LBDRx if all the possible paths reach the destination. In the search of all paths, whenever it fails to provide a valid path, then, a deroute action is needed. At this stage and at the switch where LBDRx is not providing a valid output port, the tool tries all the possible deroute options available, one per output port but avoiding U turns. Options leading to crossing routing restrictions are also evicted. The algorithm starts with the ﬁrst deroute option and keeps following the path, thus taking the deroute, checking if the path (and all their possible alternative paths) will reach the destination. In case of success, the deroute option is set. In case of failure (destination is not reached) another deroute option is tried. In case all deroute options fail the mapping is discarded. Notice that several deroute options may be required for a single path. 5. EVALUATION In this section, we provide a comprehensive evaluation of LBDRx. Firstly, we show the results of applying the mapping tool to diﬀerent sets of topologies with increasing complexity. In second place, all the LBDRx versions are compared between them and with an example using routing tables from an implementation cost and eﬃciency viewpoint, with a glance at scalability properties (a power comparison is available in [18]). Finally, two diﬀerent mappings of the same topology are evaluated in terms of performance to check possible deviations between them. Table 1 shows the results of some initial experiments performed in order to test the mapping tool. To this end, we used several sets of sample topologies (with increasing complexity in each type) corresponding to high-end home multimedia entertainment (sub)systems. The mapping tool was run in AMD Opteron (2,8 Ghz dual core, 8Gb RAM) machines. Type 1 Type 2 Type 3 Type 4 Figure 1 Grid size 4x4 5X5 5X6 5X7 5X7 Correct >10.000 >100.000 >100.000 >100.000 578.952 Deroutes not needed not needed not needed 10-15 13-15 Time 3-5 min 10-15 min 30-45 min 1-2 hours 2 hours Table 1: Topology mappings The main purpose was to measure the number of correct mappings generated for every analyzed topology and the time required to complete the procedure. In each case, the table shows the minimum grid size needed to map the topology (4x4, 5x5, ..., 5x7) and the number of correct mappings obtained and the average number of deroutes used (per mapping), if necessary. Note that correct mappings will be those which met the restrictions imposed by the LBDRx version applied in each case. In the last column the computation time to complete the entire process is shown. This time depends mainly on the complexity of each topology, and for these examples ranges from some minutes to several hours. 5.2 LBDRx against routing tables LBDRx versions were designed and synthesized with 45nm Nangate opensource library. Also, comparisons with routing tables, by using Memaker, are provided. In Figure 10, we observe that the diﬀerences between all the LBDRx versions are slight in terms of both area and delay. When compared with a RAM memory of 256 entries, the LBDRx version are much compact (notice the logarithmic scale). For delay, although similar, the LBDRx versions have lower access latency. Figure 10: Sample topologies used. 5.3 Performance analysis Finally, the following graphs compare two diﬀerent mappings of the same topology being simulated into gNoCSim simulator (a cycle-accurate ﬂit-level simulator). We can observe that diﬀerences are small. The left graph compares the traﬃc generated against the traﬃc actually received. These values are almost equal until the network is saturated and received packets remain constant. The right graph compares the traﬃc generated against the average network ﬂit latency. Although we get small diﬀerences regarding performance, however, having diﬀerent mappings can be useful for select[4] A. Duller, D. Towner, G. Panesar, A. Gray, and W. Robbins, “Picoarray technology: The tool’s story,” in DATE ’05: Proceedings of the conference on Design, Automation and Test in Europe. IEEE Computer Society, 2005, pp. 106–111. [5] D. Wentzlaﬀ, P. Griﬃn, H. Hoﬀmann, L. Bao, B. Edwards, C. Ramey, M. Mattina, C. Miao, J. F. Brown III, and A. Agarwal, “On-chip interconnection architecture of the tile processor,” IEEE Micro, vol. 27, no. 5, pp. 15–31, September/October 2007. [6] Z. Yu, M. Meeuwsen, R. Apperson, O. Sattari, M. Lai, J. Webb, E. Work, T. Mohsenin, M. Singh, and B. M. Baas, “An asynchronous array of simple processors for dsp applications,” in IEEE International Solid-State Circuits Conference, (ISSCC ’06), Feb. 2006, pp. 428–429. [7] M. Coppola, M. D. Grammatikakis, R. Locatelli, G. Maruccia, and L. Pieralisi, Design of Cost-Eﬃcient Interconnect Processing Units: Spidergon STNoC. Boca Raton, FL, USA: CRC Press, Inc., 2008. [8] I. Arteris, “Arteris noc,” http://www.arteris.com/, 2010, [Online; accessed 31-August-2010]. [9] D. Wingard, “Micronetwork-based integration for socs,” in In Proceedings of the 38th Design Automation Conference, 2001, pp. 673–677. [10] A. Ltd., “The advanced microcontroller bus architecture (amba),” http://www.arm.com/products/system-ip/amba/, 2010, [Online; accessed 01-September-2010]. [11] J. Flich, S. Rodrigo, J. Duato, S. Medardoni, and D. Bertozzi, “Eﬃcient implementation of distributed routing algorithms for nocs,” in IET Computers and Digital Techniques. IET, 2009, pp. 460–475. [12] M. K. F. Schafer, T. Hollstein, H. Zimmer, and M. Glesner, “Deadlock-free routing and component placement for irregular mesh-based networks-on-chip,” in Proceedings of the 2005 IEEE/ACM International conference on Computer-aided design (ICCAD ’05). IEEE Computer Society, 2005, pp. 238–245. [13] Bolotin, E., Cidon, I., Ginosar, R., and Kolodny, A., “Eﬃcient routing in irregular topology nocs,” Israel Institute of Technology, Technion Department of Electrical Engineering, Tech. Rep. CCIT 554, 2005. [14] I. Loi, F. Angiolini, and L. Benini, “Synthesis of low-overhead conﬁgurable source routing tables for network interfaces.” in DATE. IEEE, 2009, pp. 262–267. [15] D. Gelernter, “A dag-based algorithm for prevention of store-and-forward deadlock in packet networks,” IEEE Trans. Comput., vol. 30, pp. 709–715, October 1981. [16] S. Rodrigo, J. Flich, A. Roca, S. Medardoni, D. Bertozzi, J. Camacho, F. Silla, and J. Duato, “Addressing manufacturing challenges with cost-eﬃcient fault tolerant routing,” in Proceedings of the 2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip, ser. NOCS ’10. IEEE Computer Society, 2010, pp. 25–32. [17] Flich, J., Mejia, A., L´opez, P., and Duato, J., “Region-based routing: an eﬃcient routing mechanism to tackle unreliable hardware in networks on chip,” in 1st ACM/IEEE Int. Symp. Networks on Chip (ISNOC), 2007. [18] S. Rodrigo, J. Flich, J. Duato, and M. Hummel, “Eﬃcient unicast and multicast support for cmps,” in Microarchitecture, 2008. MICRO-41. 2008 41st IEEE/ACM International Symposium on, 2008, pp. 364 –375. Figure 11: Simulation of diﬀerent mappings. ing a diﬀerent path between a set of alternatives. That is, depending on the nodes location in the 2D mesh and the version of LBDRx used, some routes could have preference over others. In other words, selecting diﬀerent mappings could be possible to give preference to some routes without using any additional logic or mechanism. This is left, however, to future work. 6. CONCLUSIONS In this paper we have presented LBDRx, a series of routing mechanisms (with support to non-minimal paths) for application-speciﬁc SoC systems where the topology is totally irregular. Moreover, we presented a mapping tool able to obtain diﬀerent mappings of the same topology onto a 2D mesh. The main goal of LBDRx is to enable the use of tableless distributed routing on every switch with a constant and reduced logic cost, regardless of system size. LBDRx builds from the Logic-Based Distributed Routing (LBDR) mechanism, a previous proposal suited for CMPs and high-end MPSoC systems. A tool to eﬀectively map irregular topologies onto a 2D grid has been proposed. The tool is key for the application of the LBDRx mechanism. The provided results demonstrate the applicability of the mapping tool onto a wide set of topologies. In all the cases, a valid mapping was achieved, thus routing tables were replaced by the LBDRx mechanism. Implementation costs also showed the beneﬁts of such replacement. As future work we plan to further explore the LBDRx mechanism and the mapping tool, focusing on performance issues. Diﬀerent mappings will end up in diﬀerent performance numbers. Thus, we plan to optimize the tool to provide the best mapping for the target application. Acknowledgment This work was supported by the Spanish MEC and MICINN, as well as European Comission FEDER funds, under Grant CSD2006-00046. It was also partly supported by the COMCAS pro ject (CA501), a pro ject labelled within the framework of CATRENE, the EUREKA cluster for Application and Technology Research in Europe on NanoElectronics. Finally, the authors would like to thank Antoni Roca for his assistance in the Area-Delay comparison tests. 7. "
Deadlock-free fine-grained thread migration.,"Several recent studies have proposed fine-grained, hardware-level thread migration in multicores as a solution to power, reliability, and memory coherence problems. The need for fast thread migration has been well documented, however, a fast, deadlock-free migration protocol is sorely lacking: existing solutions either deadlock or are too slow and cumbersome to ensure performance with frequent, fine-grained thread migrations.
In this study, we introduce the Exclusive Native Context (ENC) protocol, a general, provably deadlock-free migration protocol for instruction-level thread migration architectures. Simple to implement, ENC does not require additional hardware beyond common migration-based architectures. Our evaluation using synthetic migrations and the SPLASH-2 application suite shows that ENC offers performance within 11.7% of an idealized deadlock-free migration protocol with infinite resources.","Deadlock-Free Fine-Grained Thread Migration Myong Hyon Cho, Keun Sup Shim, Mieszko Lis, Omer Khan and Srinivas Devadas Massachusetts Institute of Technology ABSTRACT Several recent studies have proposed ﬁne-grained, hardware-level thread migration in multicores as a solution to power, reliability, and memory coherence problems. The need for fast thread migration has been well documented, however, a fast, deadlock-free migration protocol is sorely lacking: existing solutions either deadlock or are too slow and cumbersome to ensure performance with frequent, ﬁne-grained thread migrations. In this study, we introduce the Exclusive Native Context (ENC) protocol, a general, provably deadlock-free migration protocol for instruction-level thread migration architectures. Simple to implement, ENC does not require additional hardware beyond common migration-based architectures. Our evaluation using synthetic migrations and the SPLASH-2 application suite shows that ENC offers performance within 11.7% of an idealized deadlock-free migration protocol with inﬁnite resources. Categories and Subject Descriptors B.4 [Input/Output and Data Communications]: tions (subsystems) InterconnecGeneral Terms Algorithms, Design, Performance Keywords On-chip interconnection networks, Thread migration protocols 1. INTRODUCTION In SMP multiprocessor systems and multicore processors, process and thread migration has long been employed to provide load and thermal balancing among the processor cores. Typically, migration is a direct consequence of thread scheduling and is performed by the operating system (OS) at timeslice granularity; although this approach works well for achieving long-term goals like load balancing, the relatively long periods, expensive OS overheads, and high communication costs have generally rendered fast thread migration impractical [16]. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS’11 May 1–4, 2011 Pittsburgh, PA, USA Copyright 2011 ACM 978-1-4503-0720-8 ...$10.00. Recently, however, several proposals with various aims have centered on thread migration too ﬁne-grained to be effectively handled via the OS. In the design-for-power domain, rapid thread migration among cores in different voltage/frequency domains has allowed less demanding computation phases to execute on slower cores to improve overall power/performance ratios [12]; in the area of reliability, migrating threads among cores has allowed salvaging of cores which cannot execute some instructions because of manufacturing faults [11]; ﬁnally, fast instruction-level thread migration has been used in lieu of coherence protocols or remote accesses to provide memory coherence among per-core caches [4] [5]. The very ﬁne-grained nature of the migrations contemplated in these proposals—a thread must be able to migrate immediately if its next instruction cannot be executed on the current core because of hardware faults [11] or to access data cached in another core [4]— demands fast, hardware-level migration systems with decentralized control, where the decision to migrate can be made autonomously by each thread. The design of an efﬁcient ﬁne-grained thread migration protocol has not, however, been addressed in detail. The foremost concern is avoiding deadlock: if a thread context can be blocked by other contexts during migration, there is an additional resource dependency in the system which may cause the system to deadlock. But most studies do not even discuss this possibility: they implicitly rely on expensive, centralized migration protocols to provide deadlock freedom, with overheads that preclude frequent migrations [3], [10], or limit migrations to a core’s local neighborhood [14]. Some ﬁne-grain thread migration architectures simply give up on deadlock avoidance and rely on expensive recovery mechanisms (e.g., [8]). With this in mind, we introduce a novel thread migration protocol called Exclusive Native Context (ENC). To the best of our knowledge, ENC is the ﬁrst on-chip network solution to guarantee freedom from deadlock for general ﬁne-grain thread migration without requiring handshaking. Our scheme is simple to implement and does not require any hardware beyond that required for hardware-level migrations; at the same time, it decouples the performance considerations of on-chip network designs from deadlock analysis, freeing architects to consider a wide range of on-chip network designs. In the remainder of this paper, • we present ENC, a novel deadlock-free ﬁne-grained thread migration protocol; • we show how deadlock arises in other migration schemes, and argue that ENC is deadlock-free; • we show that ENC performance on SPLASH-2 application benchmarks [17] running under a thread-migration architec1 1 1 2 Thread T1 Thread T2 Other threads (a) Network Buffers in a Protocol-level Deadlock N1→C1 N1→N2 C1→N1 C2→N2 N2→N1 N2→C2 (b) Channel Dependency Graph Figure 1: Protocol-level Deadlock of Fine-grain, Autonomous Thread Migration ture [4] is on par with an idealized deadlock-free migration scheme that relies on inﬁnite resources. 2. DEADLOCK IN THREAD MIGRATION 2.1 Protocol-level deadlock Most studies on on-chip networks focus on the network itself and assume that a network packet dies soon after it reaches its destination core—for example, the result of a memory load request might simply be written to its destination register. This assumption simpliﬁes deadlock analysis because the dead packet no longer holds any resources that might be needed by other packets, and only live packets are involved in deadlock scenarios. With thread migration, however, the packet carries an execution context, which moves to an execution unit in the core and occupies it until it migrates again to a different core. Thus, unless migrations are centrally scheduled so that the migrating context always ﬁnds available space at its destination, execution contexts occupying a core can block contexts arriving over the network, creating additional deadlock conditions that conventional on-chip network deadlock analysis does not consider. Core architecture The size of a thread context (relative to the size of network ﬂit) Number of threads Number of hotspots Migration interval Core and Migration single-issue, two-way multithreading 4 ﬂits 64 1, 2, 3 and 4 100 cycles On-chip Network Network topology Routing algorithms Number of virtual channels The size of network buffer (relative to the size of context) The size of context queue (relative to the size of context) 8-by-8 mesh Dimension-order wormhole routing 2 and 4 4 per link or 20 per node 0, 4 and 8 per core Table 1: The simulation setup for synthetic migration benchmarks. For example, suppose a migrating thread T1 in Figure 1(a) is heading to core C1 . Although T1 arrives at routing node N1 directly attached to C1 , all the execution units of C1 are occupied by other threads (∼), and one of them must migrate to another core for T1 to make progress. But at the same time, thread T2 has the same problem at core C2 , so the contexts queued behind T2 are backed up all the way to C1 and prevent a C1 thread from leaving. So T1 cannot make progress, and the contexts queued behind it have backed up all the way to C2 , preventing any of C2 ’s threads from leaving, and completing the deadlock cycle. Figure 1(b) illustrates this deadlock using a channel dependency graph (CDG) [2] where nodes correspond to channels of the on-chip network and edges to dependencies associated with making progress on the network. We call this type of deadlock a protocol-level deadlock, because it is caused by the migration protocol itself rather than the network routing scheme. Previous studies involving rapid thread migration typically either do not discuss protocol-level deadlock, implicitly relying on a centralized deadlock-free migration scheduler [3, 10, 14], using deadlock detection and recovery [8], employing a cache coherence protocol to migrate contexts via the cache and memory hierarchy, effectively providing a very large buffer to store contexts [12], or employing slow handshake-based context swaps [11]. All of these approaches have substantial overheads, motivating the development of an efﬁcient network-level deadlock-free migration protocol. 2.2 Evaluation with synthetic migration benchmarks As a non-deadlock-free migration protocol, we consider the naturally arising SWAP scheme, implicitly assumed by several works: whenever a migrating thread T1 arrives at a core, it evicts the thread T2 currently executing there and sends it back to the core where T1 originated. Although intuitively one might expect that this scheme should not deadlock because T2 can be evicted into the slot that T1 came from, this slot is not reserved for T2 and another thread might migrate there faster, preempting T2 ; it is therefore not guaranteed that T2 will exit the network and deadlock may arise. (Although adding a handshake protocol with extra buffering can make SWAP deadlock-free [11], the resulting scheme is too slow for systems which require frequent migrations).     	                                                                                                           Figure 2: Deadlock scenarios with synthetic sequences of migrations. 2VC+4 for example corresponds to 2 virtual channels and a context queue of 4 contexts. Figure 4: The percentage of accesses to a threads native core (i.e., the core where it started and that holds its stack) in various SPLASH-2 benchmarks. In order to examine how often the migration system might deadlock in practice, we used a synthetic migration benchmark where each thread keeps migrating between the initial core where it was spawned and a hotspot core. (Since migration typically occurs to access some resource at a core, be it a functional unit or a set of memory locations, such hotspots naturally arise in multithreaded applications). We used varying numbers (one to four) of randomly assigned hotspots, and 64 randomly located threads that made a thousand migrations to destinations randomly chosen among their originating core and the various hotspots every 100 cycles. To stress the migartion framework as in a ﬁne-grain migration system, we chose the migration interval of 100 cycles. We used the cyclelevel network-on-chip simulator DARSIM [6], suitably modiﬁed with a migration system, to model a 64-core system connected by a 2D mesh interconnect. Each on-chip network router had enough network buffers to hold 4 thread contexts on each link with either 2 or 4 virtual channels; we also examined the case where each core has a context queue to hold arriving thread contexts when there are no available execution units. We assumed Intel Atom-like x86 cores with execution contexts of 2 Kbits [12] and enough network bandwidth to ﬁt each context in four or eight ﬂits. Table 1 summarizes the simulation setup. Figure 2 shows the percentage of runs (out of the 100) that end with deadlock under the SWAP scheme. Without an additional context queue, nearly all experiments end in deadlock. Further, even though context buffering can reduce deadlock, deadlock still occurs at a signiﬁcant rate for the tested conﬁgurations. The synthetic benchmark results also illustrate that susceptibility to deadlock depends on migration patterns: when there is only one hotspot, the migration patterns across threads are usually not cyclic because each thread just moves back and forth between its own private core and only one shared core; when there are two or more hotspots and threads have more destinations, on the other hand, their paths intersect in more complex ways, making the system more prone to deadlock. Although small context buffers prevent deadlock with some migration patterns, they do not ensure deadlock avoidance because there are still a few deadlock cases. 3. EXCLUSIVE NATIVE CONTEXT PROTOCOL ENC takes a network-based approach to provide deadlock freedom. Unlike coarse-grain migration protocols, ENC allows autonomous thread migrations. To enable this, the new thread context may evict one of the thread contexts executing in the destination core, and ENC provides the evicted thread context a safe path to another core on which it will never be blocked by other threads that are also in transit. To provide the all-important safe path for evicted threads, ENC uses a set of policies in core scheduling, routing, and virtual channel allocation. Each thread is set as a native context of one particular core, which reserves a register ﬁle (and other associated context state) for the thread. Other threads cannot use the reserved resource even if it is not being used by the native context. Therefore, a thread will always ﬁnd an available resource every time it arrives at the core where the thread is a native context. We will refer to this core as the thread’s native core. Dedicating resources to native contexts requires some rudimentary multithreading support in the cores. If a thread may migrate to an arbitrary core which may have a different thread as its native context, the core needs to have an additional register ﬁle (i.e., a guest context) to accept a non-native thread because the ﬁrst register ﬁle is only available to the native context. Additionally, if a core has multiple native contexts, there must be enough resources to hold all of its native contexts simultaneously so no native thread is blocked by other native threads. It is a reasonable assumption that an efﬁcient ﬁne-grain, migration-based architecture will require some level of multithreading, in order to prevent performance degradation when multiple threads compete for the resources of the same core. If an arriving thread is not a native context of the core, it may be temporarily blocked by other non-native threads currently on the same core. The new thread evicts one of the executing nonnative threads and takes the released resource. We repeat that a thread never evicts a native context of the destination core because the resource is usable only by the native context. To prevent livelock, however, a thread is not evicted unless it has executed at least     
   	 	     
 Channel for Migrating Traffic Channel for Evicted Traffic N2→N1 C2→N2 N1→C1 If C1 is full C1→N1 N1→N2 C1→N1 N1→Nn Nn→Cn This node depends on nothing. Nn→Cn N2→Nn C2→N2 N2→C2 If C2 is full This node depends on nothing. Figure 3: Acyclic Channel Dependency Graph of ENC one instruction since it arrived at the current core. That is, an existing thread will be evicted by a new thread only if it has made some progress in its current visit on the core. This is why we say the arriving thread may be temporarily blocked by other non-native threads. Where should the native core be? In the ﬁrst-touch data placement policy [7] we assume here, each thread’s stack and private data are assigned to be cached in the core where the thread originates. We reasoned, therefore, that most accesses made by a thread will be to its originating core (indeed, Figure 4 shows that in the SPLASH-2 benchmarks we used, about 60%–85% of a thread’s accesses are to its native core). We therefore select each thread’s originating core as its native core. In what follows, we ﬁrst describe a basic, straightforward version of ENC, which we term ENC0, and then describe a betterperforming optimized version. 3.1 The basic ENC algorithm (ENC0) Whenever a thread needs to move from a non-native core to a destination core, ENC0 ﬁrst sends the thread to its native core which has a dedicated resource for the thread. If the destination core is not the native core, the thread will then move from its native core to the destination core. Therefore, from a network standpoint, a thread movement either ends at its native core or begins from its native core. Since a thread arriving at its native core is guaranteed to be unloaded from the network, any migration is fully unloaded (and therefore momentarily occupies no network resources) somewhere along its path. To keep the migrations deadlock-free, however, we must also ensure that movements destined for a native core actually get there without being blocked by any other movements; otherwise the native-core movements might never arrive and be unloaded from the network. The most straightforward way of ensuring this is to use two sets of virtual channels, one for to-native-core trafﬁc and the other for from-native-core trafﬁc. If the baseline routing algorithm requires only one virtual channel to prevent network-level deadlock like dimension-order routing, ENC0 requires a minimum of two virtual channels per link to provide protocol-level deadlock avoidance. Note that ENC0 may work with any baseline routing algorithm for a given source-destination pair, such as Valiant [15] or O1TURN [13], both of which require two virtual channels to avoid deadlock. In this case, ENC0 will require four virtual channels. 3.2 The full ENC algorithm Although ENC0 is simple and straightforward, it suffers the potential overhead of introducing an intermediate destination for each thread migration: if thread T wishes to move from core A to B, it must ﬁrst go to N , the native core for T . In some cases, this overhead might be signiﬁcant: if A and B are close to each other, and N is far away, the move may take much longer than if it had been a direct move. To reduce this overhead, we can augment the ENC0 algorithm by distinguishing migrating trafﬁc and evicted trafﬁc: the former consists of threads that wish to migrate on their own because, for example, they wish to access resources in a remote core, while the latter corresponds to the threads that are evicted from a core by another arriving thread. Whenever a thread is evicted, ENC, like ENC0, sends the thread to its native core, which is guaranteed to accept the thread. We will not therefore have a chain of evictions: even if the evicted thread wishes to go to a different core to make progress (e.g., return to the core it was evicted from), it must ﬁrst visit its native core, get unloaded from the network, and then move again to its desired destination. Unlike ENC0, however, whenever a thread migrates on its own accord, it may go directly to its destination without visiting the home core. (Like ENC0, ENC must guarantee that evicted trafﬁc is never blocked by migrating trafﬁc; as before, this requires two sets of virtual channels). Based on these policies, the ENC migration algorithm can be described as follows. Note that network packets always travel within the same set of virtual channels. 1. If a native context has arrived and is waiting on the network, move it to a reserved register ﬁle and proceed to Step 3. 2. (a) If a non-native context is waiting on the network and there is an available register ﬁle for non-native contexts, move the context to the register ﬁle and proceed to Step 3. (b) If a non-native context is waiting on the network and all the register ﬁles for non-native contexts are full, choose one among the threads that have ﬁnished executing an instruction on the core1 and the threads that want to 1No instructions should be in ﬂight. migrate to other cores. Send the chosen thread to its native core on the virtual channel set for evicted trafﬁc. Then, advance to the next cycle. (No need for Step 3). 3. Among the threads that want to migrate to other cores, choose one and send it to the desired destination on the virtual channel set for migrating trafﬁc. Then, advance to the next cycle. This algorithm effectively breaks the cycle of dependency of migrating trafﬁc and evicted trafﬁc. Figure 3 illustrates how ENC breaks the cyclic dependency shown in Figure 1(b), where Cn denotes the native core of the evicted thread, and Nn its attached router node. There is a subtlety when a migrating context consists of multiple ﬂits and the core cannot send out an entire context all at once. For example, the core may ﬁnd no incoming contexts at cycle 0 and start sending out an executing context T1 to its desired destination, but before T1 completely leaves the core, a new migrating context, T2 , arrives at the core and is blocked by the remaining ﬂits of T1 . Because T1 and T2 are on the same set of virtual channels for migration trafﬁc, a cycle of dependencies may cause a deadlock. To avoid this case, the core must inject migration trafﬁc only if the whole context can be moved out from the execution unit so arriving contexts will not be blocked by incomplete migrations; this can easily be implemented by monitoring the available size of the ﬁrst buffer on the network for migration trafﬁc or by adding an additional outgoing buffer whose size is one context size. Although both ENC0 and ENC are provably deadlock-free under deadlock-free routing because they eliminate all additional dependencies due to limited context space in cores, we conﬁrmed that they are deadlock-free with the same synthetic benchmarks used in Section 2.2. We also simulated an incomplete version of ENC that does not consider the aforementioned subtlety and sends out a migrating context if it is possible to push out its ﬁrst ﬂit. While ENC0 and ENC had no deadlocks, deadlocks occurred with the incomplete version because it does not provide a safe path for evicted trafﬁc in the case when a migrating context is being sequentially injected to the network; this illustrates that ﬁne-grained migration is very susceptible to deadlock and migration protocols need to be carefully designed. 4. PERFORMANCE EVALUATION 4.1 Baseline protocols and simulated migration patterns We compared the performance overhead of ENC0 and ENC to the baseline SWAP algorithm described in Section 2.2. However, as SWAP can deadlock, in some cases the execution might not ﬁnish. Therefore, we also tested SWAPinf, a version of SWAP with an inﬁnite context queue to store migrating thread contexts that arrive at the core; since an arriving context can always be stored in the context queue, SWAPinf never deadlocks. Although impractical to implement, SWAPinf provides a useful baseline for performance comparison. We compared SWAP and SWAPinf to ENC0 and ENC with two virtual channels. The handshake version of SWAP was deemed too slow to be a good baseline for performance comparison. In order to see how ENC would perform with arbitrary migration patterns, we ﬁrst used a random sequence of migrations in which each thread may migrate to any core at a ﬁxed interval of 100 cycles. In addition, we also wished to evaluate real applications running under a ﬁne-grained thread-migration architecture. Of the three such architectures described in Section 1, we rejected core salvaging [11] and ThreadMotion [12] because the thread’s migration patterns do not depend on the application itself but rather on external sources (core restrictions due to hard faults and the chip’s thermal environment, respectively), and could conceivably be addressed with synthetic benchmarks. We therefore selected the EM2 architecture [4], which migrates threads to a given core to access memory exclusively cached in that core; migrations in EM2 depend intimately on the application’s access patterns and are difﬁcult to model using synthetic migration patterns. We used the same simulation framework as described in Section 2.2 to examine how many cycles are spent on migrating thread contexts. 4.2 Network-Independent Traces (NITs) While software simulation provides the most ﬂexibility in the development of many-core architectures, it is severely constrained by simulation time. For this reason, common simulation methods do not faithfully simulate every detail of target systems, to achieve reasonably accurate results in an affordable time. For example, Graphite [9] provides very efﬁcient simulation of a many-core system based on the x86 architecture. However, it has yet to provide faithful simulation of network buffers. Therefore, Graphite simulation does not model the performance degradation due to head-ofline blocking, and moreover, deadlock cannot be observed even if the application being simulated may actually end up in deadlock. On the other hand, most on-chip network studies use a detailed simulator that accurately emulates the effect of network buffers. However, they use simple trafﬁc generators rather than simulating actual cores in detail. The trafﬁc generator often replays network traces captured from application proﬁling, in order to mimic the trafﬁc pattern of real-world applications. It, however, fails to mimic complex dependency between operations, because most communication in many-core systems depends on the previous communication. For example, a core may need to ﬁrst receive data from a producer, before it processes the data and sends it to a consumer. Obviously, if the data from the producer arrives later than in proﬁling due to network congestion, sending processed data to the consumer is also delayed. However, network traces typically only give the absolute time when packets are sent, so the core may send processed data to the consumer prior to it even receiving the data from its producer! In other words, the network-trace approach fails to realistically evaluate application performance, because the timing of packet generation, which depends on on-chip network conditions, is assumed before the actual simulation of the network. It is very important to reﬂect the behavior of network conditions, because it is critical not only for performance, but also to verify that network conditions don’t cause deadlock. Therefore, we use DARSIM [6], a highly conﬁgurable, cycle-accurate on-chip network simulator. Instead of using network traces, however, we generate network-independent traces (NITs) from application proﬁling. Unlike standard application traces, NITs keep inter-thread dependency information and relative timings instead of absolute packet injection times; the dependencies and relative timings are replayed by an interpreter module added to the network simulator. By replacing absolute timestamps with dependencies and relative timings, NITs allow cores to “respond” to messages from other cores once they have arrived, and solve the consumer-before-producer problem that occurs with network traces. The NITs we use for EM2 migration trafﬁc record memory instruction traces of all threads, which indicate the home core of each memory instruction and the number of cycles it takes to execute all                ' (()  
                                       	               !!""              #$              %&               	              "" Figure 5: Total migration cost (4 ﬂits per context) non-memory instructions between two successive memory instructions. With these traces and the current location of threads, a simple interpreter can determine whether each memory instruction is accessing memory cached on the current core or on a remote core; on an access to memory cached in a remote core, the interpreter initiates a migration of the corresponding thread. After the thread arrives at the home core and spends the number of cycles speciﬁed in the traces for non-memory operations, the interpreter does the same check for the next memory instruction. The interpreter does not, of course, behave exactly the same as a real core does. For one, it does not consider lock/barrier synchronization among threads; secondly, it ignores possible dependencies of the actual memory addresses accessed on network performance (consider, for example, a multithreaded work-queue implemented via message passing: the memory access patterns of the program will clearly depend on the order in which the various tasks arrive in the work queue, which in turn depends on network performance). Nevertheless, NITs allow the system to be simulated in a much more realistic way by using memory traces rather than network traces. 4.3 Simulation details For the evaluation under arbitrary migration patterns, we used a synthetic sequence of migrations for each number of hotspots as in Section 2.2. We also chose ﬁve applications from the SPLASH2 benchmark suite to examine application-speciﬁc migration patterns, namely FFT, RADIX, LU (contiguous), WATER (nsquared), and OCEAN (contiguous), which we conﬁgured to spawn 64 threads in parallel. Then we ran those applications using Pin [1] and Graphite [9], to generate memory instruction traces. Using the traces and the interpreter as described in the previous section, we executed the sequences of memory instructions on DARSIM. As in Section 2.2, we ﬁrst assumed the context size is 4 ﬂits. However, we also used the context size of 8 ﬂits, to examine how ENC’s performance overhead would change if used with an on-chip network with less bandwidth, or a baseline architecture which has very large thread context size. The remaining simulation setup is similar to Section 2.2. Table 2 summarizes the simulation setup used for the performance evaluation. 4.4 Simulation results Protocols and Migration Patterns SWAP, SWAPinf Migration Protocols ENC0 and ENC a random sequence & 5 applications from SPLASH-2: FFT, RADIX, LU (contiguous) WATER (nsquared) OCEAN (contiguous) Migration Patterns Core Core architecture The size of a thread context (relative to the size of ﬂit) Number of threads single-issue, two-way multithreading EM2 4, 8 ﬂits 64 On-chip Network Network topology Routing algorithms Number of virtual channels The size of network buffer (relative to the size of context) The size of context queue 8-by-8 mesh Dimension-order wormhole routing 2 4 per link (20 per node) ∞ for SWAPinf, 0 otherwise Table 2: The simulation setup for synthetic sequences of migrations. RANDOM FFT RADIX LU OCEAN WATER 8 61 60 61 61 61 Table 3: The maximum size of context queues in SWAPinf relative to the size of a thread context Figure 5 shows the total migration cost in each migration pattern normalized to the cost in SWAPinf when the context size is equivalent to four network ﬂits. Total migration cost is the sum of the number of cycles that each thread spends between when it moves out of a core and when it enters another. First of all, the SWAP algorithm causes deadlock in FFT and RADIX, as well as in RANDOM, when each thread context migrates in 4 network ﬂits. As we will see in Figure 8, LU and OCEAN also end up with deadlock with the context size of 8 ﬂits. Our results illustrate that real applications are also prone to deadlock if they are not supported by a deadlock-free migration protocol, as mentioned in Section 2.2. Deadlock does not occur when SWAPinf is used due to the inﬁnite context queue. The maximum number of contexts at any moment in a context queue is smaller in RANDOM than in the application benchmarks because the random migration evenly distributes threads across the cores so there is no heavily congested core (cf. Table 3). However, the maximum number of contexts is over 60 for all application benchmarks, which is more than 95% of all threads on the system. This discourages the use of context buffers to avoid deadlock.2 Despite the potential overhead of ENC described earlier in this section, both ENC and ENC0 have comparable performance, and 2Note that, however, the maximum size of context buffers from the simulation results is not a necessary condition, but a sufﬁcient condition to prevent deadlock. 	    	    	    	    	    	    	   	   	   	   	   	    
                                                                                         
                         !        !        ! ""	 #$ %%& "" '( )* # 	 &"" Figure 6: Total migration distance in hop counts for various SPLASH-2 benchmarks. are overall 11.7% and 15.5% worse than SWAPinf, respectively. Although ENC0 has relatively large overhead of 30% in total migration cost under the random migration pattern, ENC reduces the overhead to only 0.8%. Under application-speciﬁc migration patterns, the performance largely depends on the characteristics of the patterns; while ENC and ENC0 have signiﬁcantly greater migration costs than SWAPinf under RADIX, they perform much more comptetitively in most applications, sometimes better as in applications such as WATER and OCEAN. This is because each thread in these applications mostly works on its private data; provided a thread’s private data is assigned to its native core, the thread will mostly migrate to the native core (cf. Figure 4). Therefore, the native core is not only a safe place to move a context, but also the place where the context most likely makes progress. This is why ENC0 usually has less cost for autonomous migration, but higher eviction costs. Whenever a thread migrates, it needs to be “evicted” to its native core. After eviction, however, the thread need not migrate again if its native core was its migration destination. The effect of the portion of native cores in total migration destinations can be seen in Figure 6, showing total migration distances in hop counts normalized to the SWAPinf case. When the destinations of most migrations are native cores, such as in FFT, ENC has not much different total migration distance from SWAPinf. When the ratio is lower, such as in LU, the migration distance for ENC is longer because it is more likely for a thread to migrate to nonnative cores after it is evicted to its native core. This also explains why ENC has the most overhead in total migration distance under random migrations because the least number of migrations are going to native cores. Even in the case where the destination of the migration is often not the native core of a migrating thread, ENC may have an overall migration cost similar to SWAPinf as shown in LU, because it is less affected by network congestion than SWAPinf. This is because ENC effectively distributes network trafﬁc over the entire network, by sending out threads to their native cores. Figure 7 shows how many cycles are spent on migration due to congestion, normalized to the SWAPinf case. ENC and ENC0 have less congestion costs under RANDOM, LU, OCEAN, and WATER. This is analogous to the motivation behind the Valiant algorithm [15]. One very distinguishable exception is RADIX; while the migration distances of                                                                                
  
                                        	  !!"" #$ %&  	 "" Figure 7: Part of migration cost due to congestion                                                                                   
                                                         	!"" ##$  %& '( ! 	 $  ) **+ Figure 8: Total migration cost (8 ﬂits per context) ENC/ENC0 are similar to SWAPinf because the native-core ratio is relatively high in RADIX, they are penalized to a greater degree by congestion than SWAPinf. This is because other applications either do not cause migrations as frequently as RADIX, or their migration trafﬁc is well distributed because threads usually migrate to nearby destinations only. If the baseline architecture has a large thread context or an onchip network has limited bandwidth to support thread migration, each context migrates in more network ﬂits which may affect the network behavior. Figure 8 shows the total migration costs when a thread context is the size of eight network ﬂits. As the number of ﬂits for a single migration increases, the system sees more congestion. As a result, the migration costs increase by 39.2% across the migration patterns and migration protocols. While the relative performance of ENC/ENC0 to SWAPinf does not change much for most migration patterns, the increase in the total migration cost under RADIX is greater with SWAPinf than with ENC/ENC0 as the network becomes saturated with SWAPinf too. Consequently, the overall overhead of ENC and ENC0 with the context size of 8 ﬂits is 6% and 11.1%, respectively. The trends shown in Figure 6 and 	    	    	    	    	    	    	   	   	   	   	   	    
              	    	    	    	    	    	    	   	   	   	   	   	    
       	    	    	    	    	    	    	   	   	   	   	   	    
             Proceedings of NP2: Workshop on Network Processors, 2003. [9] Jason E. Miller, Harshad Kasture, George Kurian, Charles Gruenwald, Nathan Beckmann, Christopher Celio, Jonathan Eastep, and Anant Agarwal. Graphite: A distributed parallel simulator for multicores. In Proceedings of HPCA 2010, pages 1–12, 2010. [10] Matthew Misler and Natalie Enright Jerger. Moths: Mobile threads for on-chip networks. In Proceedings of PACT 2010, pages 541–542, 2010. [11] Michael D. Powell, Arijit Biswas, Shantanu Gupta, and Shubhendu S. Mukherjee. Architectural core salvaging in a multi-core processor for hard-error tolerance. In Proceedings of ISCA 2009, pages 93–104, 2009. [12] Krishna K. Rangan, Gu-Yeon Wei, and David Brooks. Thread motion: Fine-grained power management for multi-core systems. In Proceedings of ISCA 2009, pages 302–313, 2009. [13] Daeho Seo, Akif Ali, Won-Taek Lim, Nauman Raﬁque, and Mithuna Thottethodi. Near-Optimal Worst-Case Throughput Routing for Two-Dimensional Mesh Networks. In Proceedings of the 32nd Annual International Symposium on Computer Architecture (ISCA 2005), pages 432–443, 2005. [14] Kelly A. Shaw and William J. Dally. Migration in single chip multiprocessor. In Computer Architecture Letters, pages 12–12, 2002. [15] L. G. Valiant and G. J. Brebner. Universal schemes for parallel communication. In STOC ’81: Proceedings of the thirteenth annual ACM symposium on Theory of computing, pages 263–277, 1981. [16] Boris Weissman, Benedict Gomes, Jürgen W. Quittek, and Michael Holtkamp. Efﬁcient ﬁne-grain thread migration with active threads. In Proceedings of IPPS/SPDP 1998, pages 410–414, 1998. [17] S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. The SPLASH-2 programs: characterization and methodological considerations. In Proceedings of the 22nd Annual International Symposium on Computer Architecture, pages 24–36, 1995. Figure 7 also hold with the increased size of thread context. 5. CONCLUSIONS ENC is a deadlock-free migration protocol for general ﬁne-grain thread migration. Using ENC, threads can make autonomous decisions on when and where to migrate; a thread may just start traveling when it needs to migrate, without being scheduled by any global or local arbiter. Therefore, the migration cost is only due to the network latencies in moving thread contexts to destination cores, possibly via native cores. Compared to a baseline SWAPinf protocol which assumes inﬁnite queues, ENC has an average of 11.7% overhead for overall migration costs under various types of migration patterns. The performance overhead depends on migration patterns, and under most of the synthetic and application-speciﬁc migration patterns used in our evaluation ENC shows negligible overhead, or even performs better; although ENC may potentially increase the total distance that threads migrate by evicting threads to their native cores, it did not result in higher migration cost in many cases because evicted threads often need to go to the native core anyway, and intermediate destinations can reduce network congestion. While the performance overhead of ENC remains low in most migration patterns, a baseline SWAP protocol actually ends up with deadlock, not only for synthetic migration sequences but also for real applications. Considering this, ENC is a very compelling mechanism for any architecture that exploits very ﬁne-grain thread migrations and which cannot afford conventional, expensive migration protocols. Finally, ENC is a ﬂexible protocol that can work with various onchip networks with different routing algorithms and virtual channel allocation schemes. One can imagine developing various ENCbased on-chip networks optimized for performance under a speciﬁc thread migration architecture. 6. "
Reconfiguration of a 3GPP-LTE telecommunication application on a 22-core NoC-based system-on-chip.,"The MAGALI chip is a 65nm digital baseband dedicated to advanced telecommunication applications. Based on a 15-router mesh Network-on-Chip - NoC, it embeds 22 Processing Elements - PE performing the different functions of a complex baseband in a programmable manner. The NoC framework is based on asynchronous routers which provide a complete Globally Asynchronous Locally Synchronous -- GALS framework. Thanks to this structure, each PE is a synchronous island with a programmable frequency. On the architectural side, the NoC supports fast reconfiguration thanks to a distributed scheme. In this demonstration, we propose to show the reconfiguration capabilities of the MAGALI chip on three modes of a 3GPP-LTE receiver part, by switching on-the-fly between these three modes. The resulting transmission quality with different levels of upcoming signal noises is shown.","Reconfiguration of a 3GPP-LTE  telecommunication  application on a 22-core NoC-based System-on-Chip F. Clermidy, N. Cassiau, N. Coste , D. Dutoit, M. Fantini, D. Ktenas, R. Lemaire, L. Stefanizzi  CEA/LETI – MINATEC center   Grenoble, France  +33 438 782 233  fabien.clermidy@cea.fr  ABSTRACT  The MAGALI chip is a 65nm digital baseband dedicated to  advanced telecommunication applications. Based on a 15-router  mesh Network-on-Chip - NoC, it embeds 22 Processing Elements  - PE performing the different functions of a complex baseband in  a programmable manner. The NoC framework is based on  asynchronous routers which provide a complete Globally  Asynchronous Locally Synchronous – GALS framework. Thanks  to this structure, each PE is a synchronous island with a  programmable frequency. On the architectural side, the NoC  supports fast reconfiguration thanks to a distributed scheme. In  this demonstration, we propose to show the reconfiguration  capabilities of the MAGALI chip on three modes of a 3GPP-LTE  receiver part, by switching on-the-fly between these three modes.  The resulting transmission quality with different levels of  upcoming signal noises is shown.  Categories and Subject Descriptors  C.1.3 [Processor Architectures]: Other Architecture Styles –  Data-flow architectures, Heterogeneous (hybrid) systems.   General Terms  Design, Experimentation.  Keywords  3GPP-LTE, reconfiguration, Asynchronous Network-on-Chip.  1. INTRODUCTION  Recent telecom applications are requiring high performance  computing part in a very constrained real-time and power  consumption environment. For example, 3GPP-LTE application  (Figure 1) increases the complexity compared to previous 3G  UMTS applications in two ways: augmented throughput to more  than 100 Mbits/s targeted; improved quality of service by using  high-level coding schemes in Multiple Input Multiple Output  (MIMO) antennas such as directional beam forming. This  typically leads to 10 times increase of computing complexity for  the same power consumption of around 500 mW [1].  Due to the requirements in terms of flexibility, purely dedicated  hardware is no more an option. On the other side, using general  purpose DSP leads to large power consumptions not compatible  with embedded systems constraints. A new way must be found.   One solution is to mix dedicated hardware with programmable  tiles. In such a scheme, communications between tiles became the  main challenge. In the rest of this paper, we present a proposal  based on the use of a dedicated asynchronous Network-on-Chip  (NoC) for high throughput and flexibility purpose. Based on this  NoC framework, a specialized streaming programming model has  been deployed as well as a very efficient dynamic reconfiguration  mechanism [4]. The demonstration is aimed at showing this  reconfiguration by switching between 3 modes of a 3GPP-LTE  receiver.  460Mb/s R F F r o n t E n d O F D M O F D M D e f r a m i n g D e f r a m i n g C F m O e s t i . C C o F O r r . C C o F O r r . E C s h a n . . t i m P R B P R B M M O I d e c o d n g i S o f t d e m a p p n g i B i t d e i n e t r l e a v n g i H A R Q R X C h a n n e l d e c o d n g i L 1 L 2 / i n t e r f a c e t o M A C l a y e r 76Mb/s 192Mb/s 54 to 77Mb/s Figure 1. 3GPP-LTE application  2. DEMONSTRATION SET-UP  The MAGALI chip [2] is a 3x5 mesh NoC with 22 different units  (Figure 2). Two main kinds of resources are implemented. The  first ones have no predetermined position in telecommunication  chain; it includes MEPHISTO resources [3] which are coarsegrain reconfigurable cores for complex matrix computations; and  SMEs (Smart Memory Engines) which are reconfigurable  memory resources  including data re-arrangement functions  through virtual buffers and configuration server. The second type  of resources is specific reconfigurable IP cores that directly  support functions of the reception chain: OFDM, RX_BIT (bit  operations such as demapping, deinterleaving…) and channel  decoding units (ASIP, UWB_LDPC and WIFLEX). Finally, a  general purpose CPU (ARM1176 core) acts as a general  controller for the platform, and is used for interacting with the  external world.  The board designed for the demonstrator (Figure 3) is based on  the MAGALI chip and a virtex5 FPGA LX110T. The FPGA and  the telecom chip are connected through two NoC links of 32 bits  width running at 150 MHz, which corresponds to a bi-directional  9.6 Gbits/s data rate. This extension of the NoC of the telecom  Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee.  NOCS'11, May 1-4, 2011 Pittsburgh, PA, USA  Copyright 2011 ACM 978-1-4503-0720-8… $10.00.                              chip to the FPGA produces a global network which can be easily  adjusted by reconfiguring the FPGA.  NOC_PERF NOC_PERF noc_perf_00n noc_perf_00n TX_BIT TX_BIT tx_bit_00 tx_bit_00 SME SME sme_01 sme_01 MEPHISTO MEPHISTO mephisto_01 mephisto_01 TRX_OFDM TRX_OFDM trx_ofdm_03 trx_ofdm_03 TRX_OFDM TRX_OFDM trx_ofdm_04 trx_ofdm_04 00 10 MEPHISTO MEPHISTO mep_10 mep_10 SME_EXT SME_EXT sme_10w sme_10w TRX_OFDM TRX_OFDM trx_ofdm_20 trx_ofdm_20 nocif2 20 01 11 21 ARM CPU ARM CPU arm11_11 arm11_11 SME SME sme_21 sme_21 02 12 22 MC8051 MC8051 mc8051_12 mc8051_12 MEPHISTO MEPHISTO mep_22 mep_22 03 13 23 04 nocif1 SME SME SME_13 SME_13 UWB_LDPC UWB_LDPC uwb_ldpc_14 uwb_ldpc_14 MEPHISTO MEPHISTO mep_23 mep_23 14 24 ASIP ASIP asip_24 asip_24 TRX_OFDM TRX_OFDM trx_ofdm_20s trx_ofdm_20s MEPHISTO MEPHISTO mep_21s mep_21s SME SME sme_22s sme_22s RX_BIT RX_BIT rx_bit_23s rx_bit_23s WIFLEX WIFLEX wiflex_24s wiflex_24s Figure 2. MAGALI Chip synoptique  (b)  (c)  Figure 4. 3GPP-LTE modes computed by MAGALI. (a)  QPSK (b) 16-QAM (c) 64-QAM  4. CONCLUSION  This demonstration shows some of the MAGALI chip concepts.  Firstly, the asynchronous NoC is used and its efficiency and  robustness are proved on a real application. Secondly, the  reconfiguration architecture is demonstrated on a real-time  application.  The chip is currently in used for porting new algorithms for two  European projects on telecommunication called ARTIST4G and  BEFEMTO.  5. "
Dynamic power management of voltage-frequency island partitioned Networks-on-Chip using Intel's Single-chip Cloud Computer.,"Continuous technology scaling has enabled the integration of multiple cores on the same chip. To overcome the disadvantages of buses, the Network-on-Chip (NoC) architecture has been proposed as a new communication paradigm. To further mitigate the tradeoff between performance and power consumption, dynamic voltage and frequency scaling (DVFS) became the de facto approach in multi-core design. DVFS-based NoC communication was implemented in Intel's most recent Singlechip Cloud Computer (SCC). Using the SCC we demonstrate a power management algorithm that runs in real time and dynamically adjusts the performance of the islands to reduce power consumption while maintaining the same level of performance.","Dynamic Power Management of Voltage-Frequency Island  Partitioned Networks-on-Chip using Intel’s Single-chip  Cloud Computer General Terms  Keywords  Performance, Design, Verification, Experimentation   Radu David, Paul Bogdan, Radu Marculescu  Carnegie Mellon University   5000 Forbes Avenue, Pittsburgh, PA 15206  {radud,pbogdan,radum}@ece.cmu.edu  ABSTRACT  Continuous technology scaling has enabled the integration of  multiple cores on the same chip. To overcome the disadvantages  of buses, the Network-on-Chip (NoC) architecture has been  proposed as a new communication paradigm. To further mitigate  the tradeoff between performance and power consumption,  dynamic voltage and frequency scaling (DVFS) became the de  facto approach  in multi-core design. DVFS-based NoC  communication was implemented in Intel’s most recent Singlechip Cloud Computer (SCC). Using the SCC we demonstrate a  power management algorithm  that runs  in real time and  dynamically adjusts the performance of the islands to reduce  power consumption while maintaining  the same  level of  performance.   Categories and Subject Descriptors  J.6 [Computer Applications]: Computer-Aided Design (CAD)  Network-on-chip, DVFS, Power management, multicore  1.  INTRODUCTION  The Single-chip Cloud Computer (SCC) is a research multi-core  platform built by Intel. The chip contains 24 tiles, each with two  processing cores, interconnected by a 4x6 mesh NoC. There are  six voltage islands, organized in groups of four tiles, where  frequency is adjusted on each tile individually (see Figure 1).   In this demo, we show the effectiveness of an on-line power  management algorithm proposed in [2], using the SCC platform.  The algorithm solves both the static VFI assignment problem and  the dynamic feedback control problem. The static voltage/speed  assignment problem is based on allocating tasks to the smallest  possible voltage/frequency domain that meets the latency and  bandwidth constraints. The dynamic control  is based on  monitoring the queue occupancy of each tile of the SCC platform.  The queue occupancy depends on application workload variation,  so we implement a state-based feedback control to the network  (based on monitoring the queues occupancy) to save energy at  Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  NOCS'11, May 1-4, 2011 Pittsburgh, PA, USA  Copyright 2011 ACM 978-1-4503-0720-8… $10.00  Umit Ogras  Strategic CAD Laboratories  Intel Corporation  Hillsboro, OR 97006 USA  umit.y.ogras@intel.com  runtime.   To demonstrate this power management scheme on a real-life  application, we choose an image detection algorithm. More  precisely, we use multiple images or video frames that contain the  target object, and then detect the target object using a parallel  application that runs on the SCC cores. The algorithm is  computationally dependent on  the  image contents,  thus  introducing significant workload variation at run time. The  feedback controller of the power management strategy described  above exploits precisely this workload variation to adjust the  voltage and frequency of the tiles and thus save power.  The  experimental results show significant savings when using the  algorithm and taking advantage of the VFIs compared to the case  in which all the voltages and frequencies are held at the initial  values.   2.  IMPLEMENTATION  The 48 cores run a special Linux distribution that uses the RCCE  library for inter-core communication. The power management  policy is implemented in software, and runs on all the cores  simultaneously. Each core runs its own C application stored in the  system shared DDR3 memory, while information about the queue  states is transferred among cores in separate islands via a  message-passing interface. Each tile has its own message passing  buffer (MPB), consisting of 16KB of SRAM, for a total of 384KB  (24 tiles * 16KB).   Our system contains n VFIs and n MPBs representing the target  queues, where n=1..6. The control algorithm gathers information  about the MPB queue occupancy (Q), the arrival rate () and  service rate () of the messages over two consecutive time  increments to compute the feedback values, given a queue  occupancy target (see Figure 1).   The feedback-based algorithm selects the best voltage and  frequency such that the reference is followed. The algorithm runs  over a predefined control interval. A step-by-step description of  the algorithm is shown below.   1. Monitor queue occupancy, Q   Update average arrival rate  and service rate  3. Compute state feedback values based on previous states  and target occupancy  4. Select best frequency divider  5. Update frequency/voltage   The occupancy of the MPB is monitored by keeping track of the  writes and reads between the cores; this is done via the functions  RCCE_send() and RCCE_receive(). These functions take the size  of the data as a parameter; we use this size to compute what the  occupancy of the MPB is at any given point in time.        a) b) c) r y g e n E d e z i l a m r o N ) s ( e m i T n o i t u c e x E d) n o i t c u d e s J R / ( ) / y e g m r i e n T E n n o i i t e u s c a e e x E c n n r I i 1.2 1 0.8 0.6 0.4 0.2 0 12 10 8 6 4 2 0 40 30 20 10 0 Total energy when perform ing object detection on 1024x1024 images (includes communication and power adjustment overhead) 100% 107% 117% Lowest Power Dynamic Power Management Highest Power Execution times for perform ing image detection on 1024x1024 images 11s 8s 7s Lowest Power Dynam ic Power Management Highest Power Rate of increase in energy as a function of increase in execution time Dynamic Power  Management High voltage  and frequency 4.  Figure 1 - a) SCC Platform overview showing the tiles connected by the mesh network and a schematic representation of one tile  showing monitored parameters for our control approach (Figures from Intel [3]). b, c) energy and execution time for analyzing  1024x1024 images. d) energy delay product showing the great savings of dynamic power management  We determine  and  by computing the ratio of received/sent  (measured by Energy/Exe_time) is almost 2 better than  data packets and observation time intervals which are obtained by  running at highest voltage and frequency.   using the RCCE primitives (RCCE_send() and RCCE_receive() ).  The default operating mode of the tiles is 800 MHz at 1.1 V. The  voltage controller unit (VRC) allows the selection among seven  voltage levels between 0.7V and 1.3V with 0.1V increments. Tile  frequencies are obtained from a 1.6 GHz reference clock that can  be divided by integer values from 2 to 16, obtaining a range of  800 MHz to 100 MHz respectively.  To change the operating mode, the function RCCE_iset_power ()  takes a frequency divider as a parameter and determines the  accepted voltage level. It also returns the voltage and frequency  dividers, such that we can keep track of the current state of the  system. Frequency can be changed without the voltage, using the  function RCCE_set_frequency_divider(). The given frequency  divider is only applied if it doesn’t result in a damaging voltagefrequency combination; otherwise, the highest safe frequency  divider is selected.   This demo shows the practical implementation and impact of a  dynamic power management algorithm running on a state-of-theart NoC-based multi-core system. Power management is a crucial  aspect in today computer systems, and our implementation  successfully quantifies the trade-offs between performance and  power consumption while running a multimedia application.  The authors acknowledge support from Semiconductor Research  Corporation via grant 2008-HJ-1800, National Science  Foundation (NSF) via grant CCF-0916752, and Intel's MARC  program. Paul Bogdan also acknowledges the financial support  from the Roberto Rocca Educational Program.  ACKNOWLEDGEMENTS  CONCLUSION  5.  6.  "
Online task remapping strategies for fault-tolerant Network-on-Chip multiprocessors.,"As CMOS technology scales down into the deep-submicron domain, the aspects of fault tolerance in complex Networks-on-Chip (NoCs) architectures are assuming an increasing relevance. Task remapping is a software based solution for dealing with permanent failures in processing elements in the NoC. In this work, we formulate the optimal task mapping problem for mesh-based NoC multiprocessors with deterministic routing as an integer linear programming (ILP) problem with the objective of minimizing the communication traffic in the system and the total execution time of the application. We find the optimal mappings at design time for all scenarios where single-faults occur in the processing nodes. We propose heuristics for the online task remapping problem and compare their performance with the optimal solutions.","Online Task Remapping Strategies for Fault-tolerant Network-on-Chip Multiprocessors Onur Derin Deniz Kabakci Leandro Fiorin ALaRI - Faculty of Informatics - University of Lugano Via Bufﬁ 13, 6904, Lugano, Switzerland name.surname@usi.ch ABSTRACT As CMOS technology scales down into the deep-submicron domain, the aspects of fault tolerance in complex Networkson-Chip (NoCs) architectures are assuming an increasing relevance. Task remapping is a software based solution for dealing with permanent failures in processing elements in the NoC. In this work, we formulate the optimal task mapping problem for mesh-based NoC multiprocessors with deterministic routing as an integer linear programming (ILP) problem with the ob jective of minimizing the communication traﬃc in the system and the total execution time of the application. We ﬁnd the optimal mappings at design time for all scenarios where single-faults occur in the processing nodes. We propose heuristics for the online task remapping problem and compare their performance with the optimal solutions. Categories and Subject Descriptors B.8.1 [Performance and Reliability]: Reliability, Testing, and Fault-Tolerance; B.8.2 [Performance and Reliability]: Performance Analysis and Design Aids General Terms Algorithms, Performance, Reliability, Theory Keywords adaptivity, fault-tolerance, Kahn Process Networks, Networkson-Chip, mapping 1. INTRODUCTION As CMOS technology scales down into the deep-submicron domain, the aspects of fault tolerance in complex Networkson-Chip (NoCs) architectures are assuming an increasing relevance. In fact, devices and interconnect are sub jected to new types of malfunctions and failures that are often hardly Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS ’11, May 1-4, 2011 Pittsburgh, PA, USA Copyright 2011 ACM 978-1-4503-0720-8 ...$10.00. predictable and avoidable with current design methodologies [16]. Fault tolerant approaches are therefore necessary to overcome these limitations, and new methodologies based either on architectural or software solutions should be explored. Faults in NoCs may aﬀect both the communication system and the cores, and diﬀerent solutions in the literature have been proposed to address these two speciﬁc classes of malfunctioning. In this work, we focus on the use of software based solutions for tolerating permanent failures in processing elements, and allowing a graceful degradation of the system performance by remapping online tasks running on faulty processors. Tasks reallocation represents an alternative solution to the classical use of resource redundancy, which exploits the intrinsic availability of spare computation resources available in modern Multiprocessor Systems-on-Chip platforms. Moreover, it represents an obliged choice in particular in embedded systems, constrained in terms of number of computation resources available. Our paper presents two main contributions. The ﬁrst one is a method for ﬁnding an optimal solution to mapping tasks onto heterogeneous NoC multiprocessor systems. In most of related works [14, 11], the problem of optimal task mapping has been addressed in two phases. The ﬁrst phase addresses the partitioning problem, which deals with the selection of the IP for implementing the tasks of the application. Core mapping constitutes the second phase, where the selected IPs are mapped on the tiles of the NoC. The partitioning deals with optimization of the computation, whereas core mapping deals with optimizing communication. Partitioning problem starts with the task graph and a list of IPs; and results in a core communication graph (CCG). Core mapping problem starts with the CCG and results in a mapping of cores to tiles. Our formulation combines the two steps, and starting from a task graph provides as output the mapping of tasks onto tiles. Therefore both computation and communication is optimized in a single step. We adopt the platform-based design paradigm, where the NoC architectural platform with its pre-selected and pre-placed IPs is already given. Goal of the methodology is to execute a given application on this platform optimizing both computation and communication. To achieve this goal, we propose an ILP formulation for the problem. The same methodology is also applicable to the core mapping problem, by interpreting the task graph as a CCG and by considering an additional constraint which imposes to map at most one task onto each tile. To the best of our knowledge, the use of ILP formulation for ﬁnding an optimal solution to the task mapping problem onto heterogeneous NoC multiprocessors has not been presented in previous works. There have been several works that proposed as possible solution to the optimization problem the use of heuristics [8, 18, 9, 15, 23], evolutionary algorithms [2, 25, 11, 3, 22, 7] and a mix of both [19, 17]. However, the usual approach followed is to compare the performance of the proposed solutions with each other, or with a solution found by applying simulated annealing. An ILP formulation guarantees to ﬁnd an optimal solution to the problem; however, since the ILP solution is not scalable, and the needed execution time will increase signiﬁcantly with the dimension of the NoC, we acknowledge that heuristics and evolutionary algorithms are of signiﬁcant value. Nevertheless, an optimal solution to the problem will also serve a good deed in comparing the performances of heuristics-based solutions. In fact, results of approaches based on evolutionary algorithms are not guaranteed to be optimal whereas ILP results are. ILP-based solutions to similar problems have been proposed in the case of contention-aware application mapping on NoC [5]; ﬂoor-planning and topology generation for NoC [20]; task mapping and scheduling on multi-core architectures [24]; task mapping on shared memory bus-based heterogeneous MPSoCs [6]. It is possible to classify the previous works in terms of the tackled problem (core mapping, task mapping, partitioning, allocation, scheduling, routing, topology generation); optimization goals (execution time, delay, communication, power, robustness, contention, ﬂexibility); optimization techniques (heuristics, evolutionary algorithms, exact solutions); architectural platform (ﬁxed/free NoC topology, ﬁxed/free routing). Following such criteria, our solution can be classiﬁed as an ILP solution to the task mapping problem with minimization of total execution time, and minimization of total communication, on mesh-based NoC architectures with ﬁxed routing algorithms. Two of the most related works that target the application mapping problem are [21] and [12]. They both apply an optimization based on the use of genetic algorithms; the former optimizes the total execution time and communication load; the latter optimizes the throughput, area and ﬂexibility. With respect to these works, our approach focuses on platforms in which each NoC tile has one computational core (either programmable or non-programmable). In this work, we refer to a mesh-based NoC multiprocessor architecture. Our analytical model for calculating the total execution time of the application is valid for Kahn Process Networks (KPN). Thus our task graphs are restricted to KPN task graphs making our solution applicable to mostly streaming applications. Our formulation is valid for any deterministic routing scheme. Deterministic routing implies that communication binding is implied by the task mapping and that the task mapping is the only degree of freedom. Our second main contribution is an online solution to the task remapping problem in presence of run-time faults. We propose heuristics and make a comparison of performance degradations between heuristics and the optimal solutions found by using the ILP formulation for various fault scenarios. Regarding related work in this research direction, in [1] authors address the core remapping problem in NoCs.In that case, the only concern for remapping is the minimization of the communication load. Whereas, in our case, we move individual tasks on a core to, possibly, diﬀerent cores. Their remapping strategy can be imported to our context as the center of gravity (CoG) technique described in section 4. Our results reveal that CoG does not perform well for the computation ob jective. The remainder of this paper is organized as follows. Section 2 presents the ILP formulation for the mapping problem. In Section 3, we apply the proposed methodology to an MPEG2 decoder case study, showing the obtained results. In Section 4, we propose and evaluate a set of heuristics for online task remapping using the MPEG2 decoder case study. Finally, in Section 5, we discuss conclusions and future work. 2. ILP FORMULATION OF THE PROBLEM • A task graph gt = (Vt , Et ) is composed of tasks t ∈ Vt and data dependencies e ∈ Et ⊆ Vt × Vt . • An architecture graph ga = (Va , Ea ) is composed of processing nodes n ∈ Va and bidirectional communication links l ∈ Ea ⊆ Va × Va . • A task mapping βt : Vt → Va is an assignment of tasks t ∈ Vt to nodes n ∈ Va . • A communication binding βc : Et → E i a is an assignment of data dependencies e ∈ Et to paths of length i in the architecture graph ga . A path p of length i is given by i-tuple p = (l1 , l2 , ..., li ). • path : (Ea , Ea ) → E i a is a function that implements a deterministic routing algorithm and returns a path between two given nodes. Path set P is the set of paths between all node pairs: P = {pk : pk = path(ni , nj ), ∀ni , nj ∈ Va ∧ ni 6= nj } Initial and ﬁnal nodes of a path can be obtained by source and sink functions. pk = path(ni , nj ) ⇒ source(pk ) = ni ∧ sink(pk ) = nj • The task graph can be annotated with demand values where demand di on a data dependency ei ∈ Et , denotes the required bandwidth between the two tasks. Demand values are application speciﬁc and can be calculated by proﬁling the application with a test input. • The architecture graph can be annotated with capacity values where capacity on an architectural link li ∈ Ea , ci , denotes the maximum bandwidth of the communication link between two architectural nodes. • Core type set C consists of core types Ci and lists the types of cores available in a given NoC platform. We would like to minimize the total network traﬃc and the computation time. 2.1 Minimization of the communication cost In order to formulate the problem, we deﬁne several incidence matrices, namely decision variables X N T , Y P E ; and parameters M T E , M N P and M P L . X N T is an incidence matrix of size |Va | × |Vt | that denotes the mapping of tasks onto the nodes and it consists of the main decision variables of the problem. ij =  1, if tj ∈ Vt is bound onto node ni ∈ Va 0, otherwise X N T Y P E M T E if ∃tk , ej = (ti , tk ) ∈ Et if ∃tk , ej = (tk , ti ) ∈ Et 1, −1, 0, otherwise Y P E is an incidence matrix of size |P | × |Et | that denotes which path realizes which data dependency. Y P E depends on the task mapping, hence it constitutes the second set of our decision variables. ij =  1, if ej ∈ Et is mapped to pi ∈ P 0, otherwise M T E is an oriented incidence matrix of size |Vt | × |Et | that relates the tasks to the data dependencies. For a given task graph, M T E is known. ij = 8< M N P is an oriented incidence matrix of size |Va | × |P | that denotes the relation between the paths and the nodes that the path connects. For a given routing algorithm and architecture graph, M N P is known. ij = 8< M P L is an incidence matrix of size |P | × |Ea | that denotes the relation between all paths resulting from a given deterministic routing algorithm and the links that make up the path. For a given routing algorithm and architecture graph, M P L is known. 1, −1, 0, otherwise if source(pj ) = ni if sink(pj ) = ni : : M N P M P L if lj ∈ pi ij =  1, 0, otherwise Constraint 1 (routing) : We have derived the following linear equation that constrains the task mapping and the communication binding with each other. Such a constraint arises from the routing algorithm implemented in the NoC. X N T M T E = M N P Y P E (1) Constraint 2 (task mapping) : A task can be mapped exactly on one node. X T N 1|Va | = 1|Vt | (2) where 1m is a matrix of size m × 1 with all elements equal to 1. It is to be noted that X T N = (X N T )T . Similar relation holds for all deﬁned matrices. Constraint 3 (communication mapping) : A data dependency can be mapped at most on one path. Y EP 1|P | ≤ 1|Et | (3) Constraint 4 (capacity) : Total bandwidth demand on a link lj should not exceed the capacity of the link cj . M LP Y P E d ≤ c (4) Ob jective 1 (communication cost) : The total traﬃc on the links can be calculated as the sum of all demands di on the links of the paths that arise according to a given mapping with the following equation. min: dT Y EP M P L 1|Ea | (5) This is a static model that has also been used in [18] and disregards the congestion on the links. However at low load conditions, it is argued that it is a good approximation. Note that the communication cost takes into account the inter-tile communication done over the NoC between tasks and not the intra-tile communication when communicating tasks are mapped onto the same node. The latter is usually much faster compared to the former. Therefore the ob jective for communication is the minimization of the total traﬃc (Equation 5) sub ject to routing algorithm constraints (Equation 1), mapping constraints (Equation 2, 3) and capacity constraints (Equation 4). Since the equations are linear, this problem can be solved with an integer linear programming (ILP) solver. Given our analytical cost model, it is obvious that when communication cost is taken as the only ob jective, the resulting mapping will always be that all tasks are mapped on a single node. However this will reﬂect badly on the computation time due to non-parallelism. Therefore we introduce a conﬂicting second ob jective that favors tasks to be placed on separate nodes. 2.2 Minimization of the total computation time M T C In order to formulate the problem we deﬁne additional parameters in matrix form, namely M T C cap is an incidence matrix of size |Vt | × |C | that denotes which core types are capable of realizing which tasks. Programmable cores would be capable of realizing diﬀerent kinds of task functionalities, whereas non-programmable cores would have dedicated functions. cap and M N C . cap , T T C if Cj ∈ C is capable of realizing task ti ∈ Vt 0, otherwise cap ij =  1, M T C T T C T T C cap is a matrix of size |Vt | × |C | that denotes the completion times of all tasks on all core types for a test input. This value is obtained multiplying the number of times the task body is executed by the time it takes to process at each ﬁring. Given an application and architecture, this matrix can be obtained by oﬄine proﬁling. cap ij = ( completion time of ti on Cj , 0, M N C is an incidence matrix of size |Va | × |C | that denotes the core type of the architectural nodes. Given an architecture, M N C is known. ij =  1, if ni ∈ Va is of core type Cj ∈ C 0, otherwise M T C is an incidence matrix of size |Vt | × |C | that denotes the actual mapping of tasks on core types. Given a task to node mapping matrix, X T N , it can be calculated as cap ij = 1 cap ij = 0 if M T C if M T C M N C M T C = X T N M N C T T is a matrix of size |Vt | × 1 that denotes the completion time of the task on the core type that it is mapped onto. It can be calculated as T T = (M T C . T T C cap ) 1|C | where the ’.’ operator represents element-wise matrix multiplication. T N is a vector of size |Va | × 1. T N i denotes the sum of execution times of tasks that are mapped on the same node, ni . It can be calculated as T N = X N T T T Constraint 5 (capability) : All tasks should be mapped on cores that are capable of implementing those tasks. M T C = X T N M N C ≤ M T C cap (6) Ob jective 2 (total execution time) : We calculate the total computation time of the application by ﬁnding the maximum of the sum of the execution times of tasks mapped on the same core. min: max(T N ) = max(X N T (((X T N M N C ) . T T C cap ) 1|C | )) (7) where max is a function that returns the maximum value in a given vector. This is a static model that has also been used in [21] and disregards the context switching times. However it is argued that this model has a reasonable accuracy for typical streaming applications. The ob jective for computation is the minimization of the total execution time (Equation 7) sub ject to capability constraints (Equation 6). The ob jective function in Equation 7 is not linear due to the multiplication X N T X T N and the max function. However, there are linearization techniques that transform these equations to linear counterparts by introducing new variables. Linearization of the product of binary decision variables in X N T X T N by introducing new binary decision variables: xijkl = xij ∗ xkl ⇒ 8< xijkl ≤ xij xijkl ≤ xkl xijkl ≥ xij + xkl − 1 : Linearization of max() can be done by introducing a new variable: min : max(x, y , z ) ⇒ min : t sub ject to t ≥ x, t ≥ y , t ≥ z 2.3 Multi-objective ILP optimization We have deﬁned two ILP problems that optimize two objectives separately. What we actually need is the multiob jective optimization of the combined problem that should result in a Pareto curve representing optimal solutions with diﬀerent trade-oﬀs for the two ob jectives. This is done by employing the ε-constraint method [4]. This method relies on adding one of the ob jectives as a constraint by requiring it to be smaller than a chosen threshold. By solving the ILP problem several times for diﬀerent values of the threshold and for a single ob jective, we obtain a Pareto curve. It is also possible to adjust the density of Pareto curve by adjusting the intervals of the threshold range. It is worth-noting that the solutions found by the multiob jective ILP optimization are absolute optima unlike what would be obtained by evolutionary algorithms. However we acknowledge the fact that the time required to solve the ILP problem will be multiplied by the desired number of Pareto points. 3. CASE STUDY We use the MPEG-2 decoder task graph shown in Figure 1 and oﬄine proﬁling taken from [21]; and the XY-routingbased NoC architecture in Figure 2 to illustrate the problem formulation better. The throughput of the links of the NoC is 100 MBps. The video is 15 seconds long with a resolution of 704×576 pixels and the frame rate is 25 frames per second. M T E can be obtained from the task graph. M N P and M P L can be obtained from the given architecture. We avoid e 1 t1 t2 2e 8e e 3 4e 9e t3 t7 5e 6e 11e t4 t5 t8 t6 7e 13e t10 10e t9 12e 14e t11 t12 Figure 1: An MPEG-2 encoder task graph with 12 tasks n1 RISC n 4 DSP n7 RISC l3 l 8 n 2 DSP l1 n5 RISC l6 n8 l4 l9 n 3 RISC l 2 n6 DSP l7 n9 l5 l10 DSP RISC l11 l12 Figure 2: A 3x3 mesh-based NoC with RISC and DSP processors writing them here due to space constraints. T CT cap and d are given in Table 1 and 2 respectively. Other parameters are listed as follows: ci = 100 MBps, 1 ≤ i ≤ |Ea | C = {C1 , C2 } = {RISC, DSP} M T C cap = 112×2 3.1 Results The case study described in section 3 has been solved using the IBM ILOG CPLEX optimizer. Figure 3 shows the Pareto curve obtained. Considering that it was a 15 seconds long video clip, the solutions that satisfy the frame rate requirement are those that have computation time less than 15 seconds. One can choose the mapping among the rest by trading oﬀ computation time and communication load. In order to have case studies with more number of tasks, we have solved the optimal mapping problem of 2 and 3 MPEG2 decoders giving us two more task graphs with 24 and 36 tasks. The Pareto curves for these cases are also plotted in Figure 3. The time for the ILP solver to obtain these solutions was in average 0.78 sec, 27.87 sec and 1740 sec per Pareto point for the cases with 12, 24 and 36 tasks, respectively. Table 1: Execution times (in seconds) of tasks on the available core types (T CT Tasks cap ) Core type RISC DSP t1 0.13 0.20 t2 6.68 8.52 t3 0.06 0.04 t4 2.00 1.25 t5 2.00 1.25 t6 0.05 0.04 t7 0.06 0.04 t8 2.00 1.25 t9 2.00 1.25 t10 0.05 0.04 t11 12.33 8.51 t12 0.18 0.30 Table 2: Bandwidth demands (in MBps) of edges (d) d1 1.0 d2 d3 d4 d5 d6 d7 d8 d9 34.6 28.1 28.1 28.1 28.1 65.0 34.6 28.1 d10 28.1 d11 28.1 d12 28.1 d13 65.0 d14 15.2 unlimited task migration cases, and ﬁve diﬀerent heuristics for the limited task migration case. Obviously the ILP solution cannot be applied at run-time. However, it makes it possible to measure the quality of the heuristic methods. 4.1 Optimal Task Remapping In the case of unlimited task migration, we are able to obtain the Pareto curves for all single fault scenarios by adding the faulty core constraint below to the original ILP formulation. Constraint (faulty core) : Given a faulty node nf , a new constraint is added to the ILP formulation that forbids mapping of tasks on the faulty node nf . X N T f j = 0 |Vt | Xj=1 (8) In the case of limited task migration, the below constraint should be added as well. Constraint (migrate only tasks on the faulty core) : Given a faulty node nf and an initial task mapping M N T , a new constraint is added to limit the reconﬁguration just to the tasks that are running on faulty node nf . X N T ij = M N T ij , 1 ≤ i ≤ |Va |, 1 ≤ j ≤ |Vt |, i 6= f (9) |Va | N = For an heterogeneous NoC with all IP cores being diﬀerent from each other, there can be |Va | successive single faults, eventually leading to no remaining non-faulty cores. The total number of diﬀerent conﬁgurations N , from all healthy cores to one healthy core is i ! − 1 = 2|Va | − 1 It means that we will need to calculate N Pareto curves for all these diﬀerent scenarios. It is a heavy task even if it is done oﬄine. One way of implementing the task remapping algorithm is by means of a look-up table where we keep the resulting optimal mappings for all Pareto curves of the N diﬀerent conﬁgurations. Assuming we have p points in a Pareto curve, encoding such information would cost B bits calculated as Xi=1 |Va | B = (2|Va | − 1) p |Vt | ⌈log(|Va |)⌉ For a case with |Va | = 9, |Vt | = 12, p = 5, we have B = 14.97 Kbytes. As the number of cores with same type increases and also depending on their placements in the NoC, this number decreases due to the occurrences of symmetrical conﬁgurations. In case the local memory in the Figure 3: Pareto curves for optimal mapping of three applications with 12, 24 and 36 tasks 4. ONLINE TASK REMAPPING Our overall goal is enabling the execution of KPN applications on NoC platforms in a fault tolerant manner. Selftesting and self-checking are two techniques to detect and, in the case of the latter, mask faults. After the faulty unit is detected, it is followed by a reconﬁguration phase. This phase enables isolation of the fault and continuity of operation, possibly with a degraded performance. The problem we are considering here is the reconﬁguration problem in the case of a fault in processing cores. Reconﬁguration in this particular case implies remapping of tasks onto the remaining healthy cores. Upon receiving a fault signal via the fault detection mechanisms, an algorithm will decide on the new task assignment conﬁguration. This has to be a very fast algorithm, in order not to disrupt operation for long. The study of migration cost in the context of real-time systems is not the focus of this paper. We are currently more focused on minimizing performance degradation rather than minimizing migration costs. The task remapping algorithm can work in two ways: limited task migration where only the tasks on the faulty core are migrated to other cores; and unlimited task migration where any task can be migrated. The former will have a shorter reconﬁguration time due to less number of tasks being migrated. However it will most likely result in a more degraded performance. The latter will certainly require a long reconﬁguration time while having higher chance of less/non- degraded performance. We propose an optimal solution to online task remapping problem based on our ILP formulation for both limited and tiles is restricted and/or the size of the NoC and the problem increase, this memory requirement may be prohibitive to apply the look-up table technique. However, it has been argued that such an oﬄine technique can provide more predictable, faster reconﬁguration times, and optimal degradations [13]. 4.2 Center of Gravity method (CoG) This heuristic places the task to be migrated in a core that resides in between the other tasks it communicates with considering the amount of communication. This heuristic takes into account only communication cost. More formally, let ti ∈ Lf be the tasks that reside on the faulty core nf . Let peers be a function that returns the list of tasks that a given task communicates with. Let weight be a function that returns the bandwidth demand between two given tasks. Let coord be a function that returns the (x,y) coordinates of a given node in mesh-based NoC. Let map be a function that returns the node of the given task. We transform the problem to ﬁnding the center of gravity of masses by considering the weights of communication as the masses of the peer tasks. Then the new node ni for task ti ∈ Lf will have coordinates coord(ni ) coordi = Ptj ∈peers(ti ) coord( map(tj ) ) weight(tj , ti ) Ptj ∈peers(ti ) weight(tj , ti ) It is most likely that coordi will not have integer values, so we round it to obtain actual coordinates. , ti ∈ Lf coord(ni ) = ⌊coordi + (0.5, 0.5)⌋ If ni is not a core type that can realize the task ti or it is a faulty core itself, then we look at the node in the close vicinity with minimum computational load. If there are tasks that communicate with each other and reside in the faulty node, the resulting mapping will depend on which order those tasks are being migrated. A logical decision is to sort the tasks with respect to their total bandwidth demands on their edges connected to the tasks on the non-faulty nodes and then migrate them in descending order. 4.3 Nonidentical Multiprocessor Scheduling The ob jective regarding computation is equivalent to the scheduling of independent tasks on nonidentical processors in order to minimize the makespan. We adopt three heuristics that has been proposed in [10] for this problem. They are slightly diﬀerent from each other and it has been shown that there are examples in which each of them is superior to others. However they have diﬀerent orders of complexity, O(n), O(n log n), O(n2 ). These heuristics consider only computation cost when remapping the tasks. It may be the case that the resulting remapping does not satisfy the capacity constraints (Equation 4). NMS-A: Lf is the set of tasks to be migrated from the faulty node nf . T N is the sum of the execution times of tasks assigned to node nj . Let Lj be the set of tasks assigned to core nj . The execution time T T N cap ij of task ti if assigned to node nj can be calculated in matrix form as j T T N cap = T T C cap M CN The task ti ∈ Lf is scheduled on the core that minimizes its ﬁnishing time. Inputs to the NMS-A algorithm is the initial mapping L and T N before the fault occurrence. The output is the new mapping L. Algorithm 1 NMS-A Algorithm 1: for all ti ∈ Lf do j + T T N cap ij ≤ T N l + 2: ﬁnd the smallest j such that T N cap il for all 1 ≤ l ≤ |Va |, l 6= f T T N 3: 4: Lj ← Lj ∪ {ti }, Lf ← Lf \ {ti } T N j ← T N 5: end for 6: return j + T T N cap ij NMS-B : For each task ti ∈ Lf , Algorithm NMS-B ﬁrst orders the tasks in Lf according to decreasing min{T T N 1 ≤ j ≤ |Va |}, and then calls Algorithm NMS-A. NMS-C : This algorithm iteratively schedules the tasks by choosing a task from Lf that gives the least ﬁnishing time. cap ij : Algorithm 2 NMS-C Algorithm 1: while Lf 6= ∅ do 2: ﬁnd a task ti ∈ Lf and nj ∈ |Va | ∧ nj 6= nf such that T N cap kj for all tk ∈ Lf and cap ij is minimum. j + T T N cap ij ≤ T N j + T T N j + T T N 3: T N Lj ← Lj ∪ {ti }, Lf ← Lf \ {ti } 4: end while 5: return 4.4 Localized NMS Heuristic (LNMS) The heuristics proposed above take into account either communication or computation. In order to propose a heuristic that performs well for both ob jectives, we limit the region of nodes where we employ the NMS heuristics. This algorithm is called the Localized NMS (LNMS) where communication cost is bounded by selecting a remapping region for each task that falls in between the peer tasks. We deﬁne a reg ion function that returns a list of nodes for a given task. All three NMS heuristics can have localized versions such as LNMS-A, LNMS-B and LNMS-C. Rather than rewriting the whole pseudocode, we highlight the diﬀerences in each case: LNMS-A and LNMS-B : In line 2, instead of 1 ≤ j ≤ |Va |, we have nj ∈ region(ti ). instead of nj ∈ Va , we have nj ∈ LNMS-C : In line 2, region(ti ). We deﬁne a parametrized reg ion(ti , s) function that takes an integer s and returns a set of s nodes making up a region centered around the center of gravity of the peer tasks of the given task ti . For s = 1, LNMS reduces to CoG (LN M S (1) ≡ C oG) and for s = |Va |, it reduces to NMS (LN M S (|Va |) ≡ N M S ). Therefore we should be able to obtain a sub-optimal Pareto curve for the range 1 ≤ s ≤ |Va | that represents diﬀerent trade-oﬀ points between communication and computation. 4.5 Results For the case study described in section 3, we obtained Pareto curves for 3 diﬀerent fault scenarios: one with faulty core n1 , one with faulty n2 and one with faulty n5 . There are only 3 unique single fault scenarios for the 3x3 NoC in Figure 2 because of symmetry relations (faulty n1 ≡ faulty n3 ≡ faulty n7 ≡ faulty n9 etc). Figure 4a shows the Paretooptimal remappings in the case of faulty n5 and unlimited task migration along with the Pareto-optimal mappings (a) Pareto curves for optimal mappings and optimal remappings (b) Comparison of results between heuristics, optimal remap(unlimited task migration) pings and the initial mapping (limited task migration) Figure 4: Remapping results for the MPEG2 decoder case study on a 3x3 mesh NoC-based heterogeneous multi-processor architecture with a faulty node n5 when all nodes are working. The performance degradation is clearly visible especially for more constrained values of total execution time. This is simply because the required parallelism, thus number of nodes, increases for such values and n5 plays a key role as the central node of the mesh. The limited task migration case has also been considered. Starting with an initial optimal mapping (t7 , t8 , t9 , t10 → n1 ; t11 → n2 ; t3 , t4 , t5 , t6 → n3 ; t1 , t2 , t12 → n5 ), the Paretooptimal remappings have been calculated for 3 diﬀerent cases: faulty n1 , faulty n2 and faulty n5 . Table 3 lists Paretooptimal remappings in limited case for the 3 fault scenarios along with the performance degradation ratios with respect to the initial mapping situation prior to the fault. We have also calculated the remappings for the three fault scenarios by using the CoG, NMS-A, NMS-B, NMS-C and LNMS-C(4) heuristics. Table 4 lists these results providing also the degradation with respect to the initial mapping. The results for limited task remapping in the fault scenario of n5 is visualized in Figure 4b. It shows the proximity of the remapping results of 5 heuristics to the metrics of the initial mapping and the Pareto-optimal remappings. The results reveal that LNMS-C(4) gives a remapping that is the closest to the optimal remapping values for all fault scenarios incurring at most 6% more degradation than Pareto-optimal remappings for both of the ob jectives. 5. CONCLUSION AND FUTURE WORK We formulated the optimal task mapping problem for meshbased NoC multiprocessors with deterministic routing as an integer linear programming (ILP) problem with the ob jective of minimizing the communication traﬃc in the system and the total execution time of the application. We used it to obtain optimal task remappings in presence of faults in processing cores. Several heuristics has been proposed and compared with respect to the optimal remappings. We have found out that LNMS-C has performed the best (within 6% Table 3: Degradation achieved by Pareto-optimal limited remappings for all single fault scenarios faulty exe.time com.cost node (ob j. 1) (ob j. 2) degradation mapping none n1 n2 n5 initial Pareto1 Pareto2 Pareto3 Pareto4 Pareto5 Pareto1 Pareto2 Pareto1 Pareto2 Pareto3 Pareto4 Pareto5 Pareto6 8.51 8.51 8.55 9.05 9.80 11.09 8.51 16.44 8.51 8.52 8.72 8.81 10.79 10.92 ob j. 1 1.00 1.00 1.06 1.15 1.30 1.00 1.93 1.00 1.00 1.02 1.04 1.27 1.28 ob j. 2 1.08 1.05 0.92 0.73 0.65 1.46 1.05 1.24 1.00 1.00 0.95 0.71 0.70 283.6 305.2 296.4 261.8 205.6 184.0 413.6 298.8 352.8 284.6 283.6 268.4 200.2 199.2 proximity to optimum) in the MPEG2 decoder case study with three fault scenarios. We wish to continue this work by evaluating our heuristics on more applications, larger NoC dimensions and more fault scenarios. Although our examples involved single fault scenarios, the proposed technique can be applied in presence of successive faults. Acknowledgment This work was funded by the European Commission under the Pro ject MADNESS (No. FP7-ICT-2009-4-248424). The authors would like to thank Mariagiovanna Sami and Z. Caner Taskin for their valuable comments on this work. Table 4: Degradation achieved by proposed heuristics for all single fault scenarios faulty node com.cost (ob j. 2) degradation Approach exe.time (ob j. 1) none n1 n2 n5 initial CoG NMS-A NMS-B NMS-C LNMS-C(4) CoG NMS-A NMS-B NMS-C LNMS-C(4) CoG NMS-A NMS-B NMS-C LNMS-C(4) 8.51 10.99 8.51 8.51 8.51 8.51 16.44 8.51 8.51 8.51 8.51 17.53 8.51 8.51 8.51 9.02 ob j. 1 1.29 1.00 1.00 1.00 1.00 1.93 1.00 1.00 1.00 1.00 2.06 1.00 1.00 1.00 1.06 ob j. 2 1.05 2.13 2.13 1.70 1.11 1.05 1.46 1.46 1.46 1.46 0.70 1.30 1.35 1.35 1.05 283.6 296.4 603.8 603.8 482.6 314 298.8 413.6 413.6 413.6 413.6 199.2 370.0 384.2 383.2 298.8 6. "
Spidergon STNoC design flow.,"In this demonstration we present an enhanced version of the usual Spidergon STNoC design flow. In addition, we show the automatic generation of a simulation platform that can be used to perform early architecture exploration.","Spidergon STNoC Design Flow Florentine Dubois ST Microelectronics TIMA laboratory CNRS/Grenoble INP/UJF Grenoble, France Jose Cano Universitat Politècnica de València València, Spain jocare@gap.upv.es Marcello Coppola ST Microelectronics Grenoble, France marcello.coppola@st.com ﬂorentine.dubois@imag.fr Jose Flich Universitat Politècnica de València València, Spain jﬂich@disca.upv.es ABSTRACT In this demonstration we present an enhanced version of the usual Spidergon STNoC design ﬂow. In addition, we show the automatic generation of a simulation platform that can be used to perform early architecture exploration. Categories and Sub ject Descriptors B.8.2 [Performance and Reliability]: Performance Analysis and Design Aids General Terms Design, Performance Keywords Architecture, Design ﬂow, Design space exploration, Network on chips, Performance estimation, Spidergon STNoC, Synthesis 1 Introduction The term System-on-Chip (SoC) refers to the integration of all the necessary electronic circuits with diverse functionality onto a single chip, in order to obtain a complete electronic system which performs a complex and useful ﬁnal product functionality. Some advantages derived of using SoC technology are: a) higher performance and system reliability (since all the circuits are on a single chip) and b) lower bill of materials (BOM). SoC development is a complex task as it includes lots of disciplines ranging from circuit design to test engineering. In addition, a modular, component-based approach is required to both hardware and software design. As technology advances, tens (and in the near future several hundreds) of elements need to be connected within the same chip, thus requiring an eﬃcient on-chip interconnect. The Network-on-Chip (NoC) paradigm is emerging as the solution for interconnecting multiple cores into a single SoC [3]. NoCs are normally characterized by a vast number of architectural parameters, like topology, routing or trafﬁc repartition; ﬁnding the optimum solution in term of latency, power or area in this huge set of possibilities is usually called design space exploration [12]. As it is impossible to test every alternative, the platform is modiﬁed in an optimization loop, where the designer improves iteratively the system conﬁguration thanks to a set of evaluation methods [11]. However, the later the performance evaluation is performed in the design ﬂow, the more complex the evaluation methods and design modiﬁcations become. In order to reduce the optimization loop, the designer can then use Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS’11 May 1-4, 2011 Pittsburgh, PA, USA Copyright 2011 ACM 978-1-4503-0720-8 ...$10.00. Frederic Petrot TIMA laboratory CNRS/Grenoble INP/UJF Grenoble, France frederic.petrot@imag.fr high-level network evaluation methods to early explore the design space at low-cost, thus decreasing the overall SoC’s time-to-market. This demonstration presents an enhanced version of the usual ST Microelectronics Spidergon STNoC hardware design methodology, which can be directly included in a more general SoC design ﬂow. This ﬂow guides in a fast and efﬁcient way designer’s choices in the design space thanks to several performance evaluations at diﬀerent levels of abstraction. It is inspired of the well-known Y-chart methodology, which is an iterative design strategy based on the separation of the application from the architecture [8, 9]. The organization of the paper is the following: after presenting the NoC design ﬂow’s state of the art, we will give an overview of the Spidergon STNoC technology, before describing step by step our ﬂow proposal and concluding. 2 State of the art Whereas mature SoC design tools already exist, NoCs design ﬂow and their integration in SoC synthesis is still an open research area. An example of tool is the commercial one proposed by the company Arteris [1], which use a ﬂow similar to our proposal. They explore the architectural possibilities and validate the network functionalities with both abstract network models and low-level simulations. The main difference with our work is that their validation steps do not use pure mathematical models to provide early performance estimations at very low modeling cost. Goossens et al. [7] and Bolotin et al. [4] propose similar design ﬂows based on three main steps: NoC generation (topology, mapping), NoC conﬁguration (routing, QoS, etc) and ﬁnally NoC veriﬁcation, based on mathematical models and/or simulations. However, neither of those works use intermediate abstract models to further explore the design space at lower cost than simulations and with more accuracy than pure theoretical models. Others Network-on-Chip generation ﬂows can be found in [5, 10]. 3 Spidergon STNoC technology overview Figure 1: Spidergon STNoC example The Spidergon STNoC interconnect is a ﬂexible Networkon-Chip technology developed by ST Microelectronics [6]; a system example is given in ﬁgure 1. It is composed of three diﬀerent types of building blocks, appropriately interconnected to each other, and provides a set of customizable low-level platform services along with a set of communication primitives. The key building blocks are a) The network interface (NI), which interfaces IP blocks and the network, b) The router, which is responsible for determining the next network point to which a packet/ﬂit should be forwarded toward its destination and c) The physical link, which is a logical component responsible for the physical communications. 4 Spidergon STNoC design ﬂow The enhanced Spidergon STNoC design ﬂow proposed in this demonstration is schematized in Figure 2. It is a direct application of the Y-chart design methodology ([8, 9]) to the ﬁeld of NoCs and can be easily included in a general SoC design strategy. The system is optimized iteratively; designer’s choices are based on a set of evaluation steps, going in ascending modeling complexity order, thus lowering the speed of the evaluation but improving its accuracy. In highlevel steps, the designer has a great freedom in the design space exploration, whereas low-level steps limits the optimization possibilities to neighbourhood conﬁgurations, due to design modiﬁcation costs. The designer can then explore and greatly reﬁne the possibilities during ﬁrst steps, reducing the number of time-consuming low-level validations, resulting in a fast and eﬀective design space exploration. For an overview of existing NoCs evaluation metrics, see [14]. The remainder of this section is devoted to the description of the diﬀerent steps, which are all included in the Spidergon STNoC design tool. The demonstration will focus on the high-level steps (steps 1,2,3) and their associated model generations. to explore quickly the design space. Moreover, those methods can also detect conﬁguration errors to decrease the number of bugs at low-level steps, and thus • Cycle approximate simulation: This second method greatly reduce the time-to-market. is based on the free simulation library OMNeT++ [2], and uses an abstract model to compute cycle-approximate simulations of the network. The designer can here evaluate very quickly network latencies and bandwidths with more precision than with static analysis and a small modeling eﬀort. 4.4 Fourth step: STNoC generation The ﬂow’s ﬁnal step is low-level explorations and veriﬁcations that provides highly accurate but computationally expensive network performance estimations. Few simulations are required here thanks to previous steps and the designer can thus quickly choose the ﬁnal conﬁguration inside the reduced design space. This step is composed of two parts described below and its output is an implementation model of the platform (RTL). • Functional veriﬁcation: This step is responsible for checking functional properties of the diﬀerent actors in the system. It is based on a set of general predeﬁned tests which validates the components behaviour • Performance veriﬁcation: Platform performance is at RTL level. evaluated in an IPTG environment [13], where the SoC architecture is modeled either at the transactional or at RTL level. The designer can then optimize the design until it meets the initial constraints with correct performance, before generating the ﬁnal NoC’s model. 5 Conclusion An enhanced Spidergon STNoC design ﬂow was presented. This ﬂow is based on the Y-chart methodology and can be included in a general SoC design method. It is based on two evaluation steps which guide the designer in the optimization loop; those evaluations are ﬁrst located at a high level of abstraction so the exploration areas can be greatly reﬁned before computing time-consuming simulations to choose the ﬁnal conﬁguration. 6 Acknowledgements Figure 2: Spidergon STNoC design ﬂow 4.1 First step: Speciﬁcations capture This step is responsible for the end-to-end characterization: only the IP properties are deﬁned and the network is considered as a black-box. Properties chosen in this phase are a) the IP communication properties, (protocols , working frequencies or data widths) and b) The connectivity matrix, which deﬁnes which pairs of IPs communicate together and the associated ﬂow properties. 4.2 Second Step: Architectural model This step is responsible for capturing the SoC requirements which have direct implications on diﬀerent network features (network topology, routing, power, frequency domains, etc). 4.3 Architecture Exploration This evaluation step follows the mapping of a set of usecases, based on the diﬀerent target applications, on the current NoC (mapping step of ﬁgure 2), according to the Ychart methodology main idea. The architecture exploration step is composed of two highlevel evaluation methods described below. They provide early estimations of network performances and give a good overview of the diﬀerent conﬁguration possibilities with a minimum amount of eﬀort. All models required here are automatically generated by the Spidergon STNoC design tool. • Static analysis: Static analysis is based on analytical models of the network and provide pseudo-realistic performance evaluations at very low modeling cost. Even though the precision of those metrics depends on the mathematical methods used, they are very useful This work was supported by the Spanish MEC and MICINN, as well as European Comission FEDER funds, under Grant CSD2006-00046. It was also partly supported by the COMCAS pro ject (CA501), a pro ject labelled within the framework of CATRENE, the EUREKA cluster for Application and Technology Research in Europe on NanoElectronics. 7 "
Exploring partitioning methods for 3D Networks-on-Chip utilizing adaptive routing model.,"Three-Dimensional (3D) integration is a solution to the interconnect bottleneck in Two-Dimensional (2D) MultiProcessor System on Chip (MPSoC). 3D IC design improves performance and decreases power consumption by replacing long horizontal interconnects with shorter vertical ones. As the multicast communication is utilized commonly in various parallel applications, the performance can be significantly improved by supporting of multicast operations at the hardware level. In this paper, we propose a set of partitioning approaches each with a different level of efficiency. In addition, we present an advantageous method named Recursive Partitioning (RP) in which the network is recursively partitioned until all partitions contain comparable number of nodes. By this approach, the multicast traffic is distributed among several subsets and the network latency is considerably decreased. We also present Minimal Adaptive Routing (MAR) algorithm for the unicast and multicast traffic in 3D-mesh Networks-on-Chip (NoCs). The idea behind the MAR algorithm is utilizing the Hamiltonian path to provide a set of alternative paths.","Exploring Partitioning Methods for 3D Networks-on-Chip  Utilizing Adaptive Routing Model Masoumeh Ebrahimi, Masoud Daneshtalab, Pasi Liljeberg, Juha Plosila, Hannu Tenhunen   Department of Information Technology, University of Turku  Joukahaisenkatu 3-5 B, 20520 Turku, Finland  {masebr,masdan,pakrli,juplos,hanten}@utu.fi ABSTRACT  Three-Dimensional (3D)  integration  is a solution  to  the  interconnect bottleneck  in Two-Dimensional  (2D) MultiProcessor System on Chip (MPSoC). 3D IC design improves  performance and decreases power consumption by replacing long  horizontal interconnects with shorter vertical ones. As the  multicast communication is utilized commonly in various parallel  applications, the performance can be significantly improved by  supporting of multicast operations at the hardware level. In this  paper, we propose a set of partitioning approaches each with a  different  level of efficiency. In addition, we present an  advantageous method named Recursive Partitioning (RP) in  which the network is recursively partitioned until all partitions  contain comparable number of nodes. By this approach, the  multicast traffic is distributed among several subsets and the  network latency is considerably decreased. We also present  Minimal Adaptive Routing (MAR) algorithm for the unicast and  multicast traffic in 3D-mesh Networks-on-Chip (NoCs). The idea  behind the MAR algorithm is utilizing the Hamiltonian path to  provide a set of alternative paths.   Categories and Subject Descriptors  C.2.1  [COMPUTER-COMMUNICATION NETWORKS]:  Network Architecture and Design - Packet-switching networks.  General Terms  Algorithms, Performance, Design.  1. INTRODUCTION  The technology trends toward the increased number of processing  elements with higher levels of integration and higher performance  will require scalable and efficient communication infrastructure.  The Network-on-Chip (NoC) architecture paradigm, based on a  modular packet-switched mechanism, can address many of the onchip communication design issues such as wiring complexity and  integration of a large number of Intellectual Property (IP) cores  into a 2D chip  [1] [2] [3]. The 3D technology can overcome the  limited floor-planning choices of 2D designs and allows each  layer to have a specific technology  [6]. The major advantages of  3D NoCs are the considerable reduction in the average wire length  and wire delay, resulting in lower power consumption and higher  performance  [5] [7] [8] [9]. The routing protocols in NoCs and  MPSoCs can be unicast or multicast  [10]. In the unicast Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  NOCS'11, May 1-4, 2011 Pittsburgh, PA, USA  Copyright 2011 ACM 978-1-4503-0720-8… $10.00  communication a message is sent from a source node to a single  destination node, while in the multicast communication a message  is delivered from one source node to an arbitrary number of  destination nodes. Multicast is a special case of unicast while the  unicast routing cannot support multicast messages efficiently  [11].  This inefficiency arises for several reasons such as 1-sending  multiple copies of the same message into the network not only  imposes a significant amount of traffic in the network but also  increases the overall power consumption. 2-multiple unicast  messages required to access the local link connected to the router  sequentially, thus introducing additional latency. As the vast  majority of traffic in MPSoCs consists of unicast traffic and most  of studies have assumed only unicast traffic, the concept of  unicast communication has been studied extensively in the  literature. The proposed unicast protocols are efficient when all  injected messages are unicast. However,  if only a small  percentage of the total traffic is multicast, the efficiency of the  overall  system  is considerably  reduced. The multicast  communication has a large impact on CMP systems performance  and is frequently employed in many coherence protocols such as  directory-based protocols, token coherence protocols, Intel QPI  protocol  [12] [13]. For instance, in a SGI-Origin protocol, i.e.  directory based protocol, around 5% of the total traffic is  generated by multicast messages. In this protocol, the network  latency can be reduced by 50%, if multicast is supported in  hardware, thus highlighting the importance of hardware-level  multicast support. In order to determine the percentage of  multicast messages in cache coherence protocols, we have used  synthetic benchmark (TPC-H  [14]) and analyzed application  traces (i.e. SPLASH-2  [15], PARSEC  [16] [17]) in two popular  cache coherence protocols, MESI  [18] and token-based MOESI   [19] [20]. On account of our analysis, on average, 6% of MESI  traffic and 99% of token-based MOESI traffic are multicast.   A 3D NoC can have different topologies for each layer of it, such  as mesh  [7] [21], torus  [7] [21], and ring  [7]. In this work we limit  our considerations to 3D-mesh NoCs, in which each layer consists  of a 2D mesh. Routing algorithms can be classified into  deterministic and adaptive. A deterministic routing algorithm uses  a fixed path for each pair of nodes resulting in increased network  latency especially in congested networks  [22] [23]. In contrast, in  adaptive routing algorithms, a packet is not restricted to a single  path when traveling from a source node to its destination(s).  Therefore, adaptive routing algorithms could obtain better  performance at the congested network utilizing alternative routing  paths  [22] [23].  The rest of this paper is organized as follows: Section 2 reviews  related work. The proposed partitioning methods are discussed in  Section 3. The minimal adaptive routing is presented in Section 4.  The results are given in Section 5 while we summarize and  conclude in the last section.      2. RELATED WORK  Some research has been conducted to evaluate the performance  metrics of 3D NoCs. The authors in  [5] [21] demonstrate that  besides reducing the footprint in a fabricated design, 3D designs  provide a better performance compared to traditional 2D designs.  They have also demonstrated that both mesh and tree topologies  for 3D systems achieve better performance compared  to  traditional 2D systems. However, the mesh topology shows  significant performance gains in terms of throughput, average  latency, and energy dissipation with a small area overhead  [5]. In   [9] different 3D-mesh based architectures have been compared in  the zero-load  latency  to compare  the speed and power  consumption of 3D NoC with 2D NoC.   Due to the fact that the multicast communication is used  commonly in various parallel applications, there have been  several attempts to improve the performance of multicast  communication in 2D NoCs. “Virtual Circuit Tree Multicasting”  (VCTM)  [12], “Recursive Partitioning Multicast” (RPM)  [24] and  “Hamiltonian path multicast algorithm for NoCs”  [25] are three  recent works in the realm of 2D NoC in which RPM and VCTM  are based on the tree-based method and the proposed algorithm in   [25] is based on the path-based method. In VCTM, a set up  message is sent from a source node to all destinations in order to  build a virtual circuit tree, then the multicast message is send  down the tree. RPM supposes the network is divided into several  partitions. This method minimizes the message replication time by  defining priority rules to reach each partition. The authors in [25]  presented a deadlock free adaptation of the dual-path multicast  algorithm for 2D mesh NoCs and then evaluated the impact of the  proposed method on  the performance of  the network,  demonstrating the efficiency of the proposed multicast algorithm.  An adaptive multicast communication in 3D-mesh networks is  discussed in  [26]. The algorithm is based on an extension of a  theory defined in  [27] from 2D to 3D-mesh network. The  algorithm utilizes the Hamiltonian path and prevents deadlocks by  using virtual channels. However, adding virtual channels is costly  in NoCs due to increased arbitration complexity and buffering  requirements  [28]. Two another methods of unicast/multicast  communication in 3D-mesh NoCs are presented in  [29]. The  proposed methods are guaranteed to be deadlock free because of  using the Hamiltonian path. However, the presented algorithms  are suffering from the low performance and inability to partition  the network efficiently. In this paper, we present several  partitioning methods in 3D-mesh NoCs in order to improve the  performance of unicast/multicast communication. In addition, we  propose an advantageous partitioning method named recursive  partitioning method which outperforms the other presented  methods, and finally we propose an adaptive routing algorithm for  all proposed partitioning methods.  3. PARTITIONING METHODS  The performance of multicast communication is measured in  terms of its latency in delivering a message to all destinations.  Multicast latency consists of two parts: startup latency and  network latency. The startup latency is the time required to break  a message into several packets (each with different destinations),  prepare packets, and deliver them completely to the network. The  network latency is defined as the time between the first flit is  injected to the network until the tail flits of all packets has reached  corresponding destinations. Partitioning methods reduce network  latency by dividing the network into several partitions and  reducing the overall path length. Nevertheless, breaking the  network into partitions has differing constraints as follows: 1Increasing the number of network partitions leads to additional  startup latency due to the preparation time of more packets at the  source node. 2-Breaking the network into unbalanced partitions  create long paths in the network. Therefore, they increase the  latency to reach the last destination which increases network  latency for multicast messages. We call this factor “fairness”.  The Hamiltonian path strategy  [11] guaranties that the network  will be free of deadlocks for the unicast and multicast traffic. The  Hamiltonian path visits each node exactly once along the path. As  shown in Fig. 1(a), for each node a label is assigned from 0 to N-1  in which N is the number of nodes in the network. Several  Hamiltonian paths can be considered in the mesh topology. In 3D  a×b×c mesh, each node is presented by the ordered triple (x,y,z)  where x is the X-coordinate, y is the Y-coordinate and z is the Zcoordinate. The following equations show one possibility of  assigning the labels which we utilize in this paper:  { } { { { } As exhibited in Fig. 1, two directed Hamiltonian paths (or two  subnetworks) are constructed by the labeling. The high channel  subnetwork (Fig. 1(b)) starts at node 0, and the low channel  subnetwork (Fig. 1(c)) ends at node 0.  zyxL ,( , zyxL ,( , zyxL ,( , zyxL ,( , even , y even , y odd , y : odd , y : where where where where : even : odd even odd + + −− −− )( x ( a y y x ))1 ))1 −− + + )1 ( a )( x ×× ×× ×× ×× b b b b z z z z : : : : ( a ( a ( a ( a ( ( ( ( a a a a × × × × y ) y ) ( b ( b z z z z ) ) ) ) −− x )1 } } ) ) ) ) = = = = + + + + 3.1 Two-Block Partitioning (TBP)  The TBP is a base scheme in which the network is partitioned into  high and low channel subnetworks. The high channel subnetwork  contains all directional channels with nodes labeled in ascending  order, and the low channel subnetwork contains all directional  channels with nodes labeled in descending order. In this method,  all destination nodes are split at most into two disjoint groups: a  high group and a low group. The high group consists of all  destination nodes with the higher labels than the source node and  the low group contains all destination nodes with the lower labels.  When considering label assignment described in the Hamiltonian  path strategy, all destination nodes located in the same layer as the  source node are divided at most into high and low groups while all  destinations in higher (lower) layers are put in the high (low)  group (see  Fig. 3). In addition, one packet is created for each  group and the destinations within each packet should be sorted in  the correct order in which they are visited in the path. Therefore,  destinations in the high group should be sorted in ascending order  and other destinations in descending order. The created packets  are routed via high and low channel subnetworks. The pseudo  code for the TBP method is illustrated in Fig. 2.    Fig. 3(a) shows an example of the partitioning policy and the  portions of each partition that depends on the source node  position. As illustrated in  Fig. 3(a), if the source node is located  at the middle layer, two partitions cover comparable number of  nodes but with a large number of nodes in both partitions.  However in  Fig. 3(b), one partition contains considerably more  nodes than the other. Now, suppose that the multicast message  m=(6,{1,2,19,25,44}) is generated by the core where the source  node is 6. The destinations are split into two groups according to  their labels: GH={19,25,44} and GL={1,2}. The packet created for  GH  uses  the  Hamiltonian  path  as  follows:  {6,9,10,11,12,19,20,21,22,25,38,41,42,43,44} where 14 hops are  needed to reach the last destination. The packet path for the GL  is:{6,5,2,1} where 3 hops are required for delivering the packet to  all destinations.     Fig. 1. (a) A 3×3×3 mesh physical network with the label assignment (b) high channel and (c) low channel subnetworks. The solid lines indicate the  Hamiltonian path and dashed lines indicate the links that could be used to reduce path length in routing.  Algorithm: Two-Block Partitioning (TBP)  Inputs:   a×b×c network; destinations labels; source label;  Begin      For “i: 0 to number of destinations” loop         If (destinationLabel(i) > sourceLabel) then             GH <= destinationAddress;       -- sort GH in ascending order         Else            GL <= destinationAddress;        -- sort GL in descending order         End if;      End loop;        --Construct a message for each group  End TBP;  Fig. 2. The pseudo code of the TBP method.   Fig. 3. The TBP method (a) balanced partitions (b) unbalanced partitions.  3.2 Vertical Block Partitioning (VBP)  In this method, similar to the TBP method, the network is  partitioned into high and low subnetworks; destination nodes are  divided into high and low groups and then sorted in each group. In  the next step, each subnetwork is vertically partitioned in which  the nodes with the same x value will be put in the same group.  The algorithm is shown in Fig. 4. This scheme has several  advantages over the TBP method as it achieves a high degree of  parallelism; avoids the creation of long paths and reduces the  network latency. However the VBP method increases the startup  latency due to using up to 2×a packets in a×b×c network. As  shown in  Fig. 5, this scheme does not guarantee the fairness  among partitions as it is fair when the source node is located at 25  while it is not when the source node is at 6. Moreover, the time  required to prepare and deliver at most 2×4 packets is considered  as  the  startup  latency. For  the multicast message  m=(6,{1,2,19,25,44}),  four  groups  are  formed:  GH2={25},GH4={19,44},GL2={1} and GL3={2}. One packet is  generated  for  each  group  and  packet  paths  are  {6,25},{6,9,10,11,12,19,44},{6,1} and {6,5,2}  in which  the  maximum hop is 6.  3.3 Recursive Partitioning (RP)  The objective of the recursive partitioning method is to optimize  the number of nodes that can be included in a partition. In this  method, the network is recursively partitioned until each partition  contains less than n nodes. In the worst case, the network is  partitioned into 2×a vertical partitions like in the VBP method.  So, we have considered the value n as the maximum number of  nodes in a partition of the VBP method, i.e. (n=bc) in a a×b×c  network. The pseudo code of the RP method is shown in Fig. 6.  An example of the RP method is illustrated in  Fig. 7(a) where a  multicast message is generated at the source node 25. The  required steps of the RP method can be expressed as follows:  Step1: The value n is set to 12 in a 4×4×3 network.  Step2: The network is divided into two partitions using the TBP  method. The  Fig. 3(a) shows two formed partitions when the  source node is located at the node 25.   Step3: If the number of nodes in a partition exceeds the  predefined value n, the partition is divided into two new  partitions. This step is repeated until all partitions in the network  cover at most n nodes. Following the example of  Fig. 3(a), 22  nodes are covered by the high channel subnetwork which is  greater than n=12. So, the high channel subnetwork needs to be  divided further into two new partitions (GH1 and GH2 as shown in   Fig. 7(a)). The GH1 and GH2 partitions contain 10 and 12 nodes,  respectively. Since both numbers are less than or equal to n=12,  no further partitioning is needed for the high channel subnetwork.   The same partitioning technique is applied to the low channel  subnetwork.  Fig. 7(b) shows another example of the RP method  where the multicast message is m=(6,{1,2,19,25,44}). In this  example  three packets are  formed and  their paths are  {6,9,10,11,12,19,44},{6,25} and {6,5,2,1} with 6 hops as the  maximum latency. As a result, the number of nodes is comparable  among partitions while the startup latency is less than in the VBP  method. By considering the RP method, the creation of balanced  partitions is less susceptible to the source node position, and thus  it avoids long paths in the network and increases parallelism while  keeping the startup latency relatively small.   4. MINIMAL ADAPTIVE ROUTING  A Minimal Adaptive Routing (MAR) algorithm is presented for  the proposed partitioning methods utilizing the Hamiltonian path.  A network can be represented by a connected graph G = (V,E),  where V denotes a set of vertices (routers or node) and E a set of  edges (communication links). A pair (u,v) Є E form an edge of the  graph, if u is physically connected to v via a communication link.  A path is a sequence of non-repeated nodes such that for a given i,  0≤i<n-1 there exists a communication link from vi to vi+1, i.e. (vi,  vi+1) Є E. The set of neighboring nodes of node u is defined as:        Algorithm: Vertical-Block Partitioning (VBP)  Inputs:     a×b×c network; destinations labels; source label;                   The X value of destinations (Xd)  Begin       For “i: 0 to number of destinations” loop           If (destinationLabel(i) > sourceLabel)  then                Case “Xd(i)” is                     When 1 =>     GH1 <= destinationAddress;                     When 2 =>     GH2 <= destinationAddress;                     …                     When a =>     GHa <= destinationAddress;               End Case;--Meanwhile sort GH1,.. GHa in ascending order            Else                Case “Xd(i)” is                     When 1 =>     GL1 <= destinationAddress;                     When 2 =>     GL2 <= destinationAddress;                     …                     When a =>     GLa <= destinationAddress;               End Case;--Meanwhile sort GL1,..GLain descending order            End if;       End loop;              --Construct a msg. for each group  End VBP;           Algorithm: Recursive Partitioning (RP)  Definitions: Num_P: Number of nodes in the partition;                          (x_p,y_p,z_p): (x,y,z) coordinates of the given partition                      n value: (b×c) in a×b×c network;  Begin       Function Partitioning (G,Num_P) is                If (Num_P = a×b×c) then                                  --Partition the network using TBP method                  G => GH,GL;                 Partitioning(GH,Num_PH);                     Partitioning(GL,Num_PL);                        Elsif (MaxHopCount > n)  then                  --Divide the given P into two new partitions(Gi,Gi+1)                  G=>Gi(cid:198)((0:[(x_p)/2]),y_p,z_p),                           Gi+1(cid:198)(([(x_p)/2]:x_p-1),y_p,z_p);                   Partitioning(Gi,Num_Pi);                      Partitioning(Gi+1,Num_Pi+1);                       Else                   Return (G,Num_P);             End if;       End Partitioning;         --Construct a msg. for each group   End RP;          Fig. 4. The pseudo code of the VBP method.  Fig. 6. The pseudo code of the RP method.   Fig. 5. The VBP method (a) balanced partitions (b)unbalanced partitions   Fig. 7. The RP method when the source node is at (a) node 25 (b) node 6  P(u) = {{v} | v Є V and (u, v) Є E and v ≠ u }. The multicast  message can be represented by m=(u,D), where u Є V is the  source node, D = {d1,d2, . . . ,dx} is the set of ordered destination  nodes, and x is the number of destination nodes. Each node in the  graph has a label (L) determined by the Hamiltonian path labeling  mechanism. For a given node u and a destination d, the MAR  algorithm finds possible neighbors of the current node that can be  selected to deliver a packet, so:  If L(u)<L(d), then MAR(u,d)={{p}| pЄP and L(u)<L(p)≤L(d) and  ((xu,yu,zu)≤(xp,yp,zp)≤(xd,yd,zd) or (xu,yu,zu)≥(xp,yp,zp)≥(xd,yd,zd))};  If L(u)>L(d), then MAR(u,d)={{p}| pЄP and L(d)≤L(p)<L(u) and  ((xu,yu,zu)≤(xp,yp,zp)≤(xd,yd,zd) or (xu,yu,zu)≥(xp,yp,zp)≥(xd,yd,zd))};  The Minimal Adaptive Routing (MAR) algorithm can be  described in three steps as follows:  Step1: it determines the neighbors of node u that can be used to  move a packet closer to the destination d (pseudo code in Fig. 9).  Step2: due to the fact that in the Hamiltonian path all nodes are  visited in ascending order (in the high channel subnetwork) or  descending order (in the low channel subnetwork), all of the  selected neighbors in Step1 do not necessarily satisfy the ordering  constraint. Therefore, if the labels of the selected neighbors (in  Step1) are between the label of node u and destination d, it/they  can be selected as the next hop (pseudo code in Fig. 9).  Step3: Since the MAR algorithm provides several choices at each  node, the goal of Step3 is to route a packet through the less  congested neighboring nodes. So, in the case where a packet can  be forwarded through multiple neighboring nodes, the stress  values of the selected neighbors are checked and then the packet  is sent to the neighbor with the smallest stress value.   Note that we have described the MAR algorithm to support  minimal paths. However the algorithm can be easily modified (by  skipping step1) to follow non-minimal paths as well as minimal  paths. An example of the MAR algorithm is illustrated in Fig.  8(a). According to the algorithm, in the first step the neighbors are  chosen that get the packet closer to the destination, i.e. p={6, 10,  26}. At the second step, the selected neighbors (in Step1) are  checked to determine whether they are in the Hamiltonian path or  not. Since the labels of the three selected neighbors are between  the labels of the current node (u=5) and destination node (d=47),  the packet can be routed via all of them. Suppose that the  neighbor p=10 has the less stress value than the other neighbors,  so the algorithm chooses this neighbor to forward the packet. If  we continue with the node u=10, this node has three neighbors  belonging to the minimal paths, i.e. p={9, 13, 21}. However, two  of them (p={13, 21}) have the labels greater than the label of the  current node (u=10) and lower than the label of the destination  node (d=47). Finally, one of these neighbors is selected as the                Fig. 8. (a) MAR algorithm for unicast message (b) showing all possible paths between source 5 to destination 47 (c) MAR algorithm for multicast messages.  next hop according to their stress values. The algorithm is  repeated for the rest of the nodes until the packet reaches to the  final destination. Another example is shown in Fig. 8(c) where the  source node (u=5) forwards a multicast packet towards its  destination nodes (D={16, 47}). The MAR algorithm provides a  set of alternative paths to send a packet from the source node to  the first destination (d1=16). However, it can suggest only one  path between the first destination (d1=16) and the second  destination (d2=47). The MAR algorithm is compatible with all  methods supporting the Hamiltonian path in 2D or 3D NoCs.  Therefore, the TBP, VBP and RP methods can utilize MAR  algorithm for both unicast and multicast messages.  Algorithm:Minimal Adaptive Routing (MAR_3D)  Inputs:      current node label, destination label and Neighbors Labels    Begin     X_dir = East when (x_s<x_d) else West;     Y_dir = North when (y_s<y_d) else South;     Z_dir = Up when (z_s<z_d) else Down;     Process     Begin          If ((Label(CurrentNode) = Label(DestNode)) then                 Select Local;          Elsif ((Label(CurrentNode) < Label(DestNode)) then  --High--                If (Label(CurrentNode) < Label(Neighbor(X_dir))) and                    (Label(Neighbor(X_dir)) < Label(DestNode)) then                        First Choice -> Neighbor(X_dir)               End if;               If (Label(CurrentNode) < Label(Neighbor(Y_dir))) and                    (Label(Neighbor(Y_dir)) < Label(DestNode)) then                        Second Choice -> Neighbor(Y_dir)               End if;               If (Label(CurrentNode) < Label(Neighbor(Z_dir))) and                    (Label(Neighbor(Z_dir)) < Label(DestNode)) then                        Third Choice -> Neighbor(Z_dir)               End if;                   Elsif ((Label(CurrentNode) >Label(DestNode)) then  --Low--             --is similar to high channel subnetwork by changing “<” to “>”          End If;      End Process;  End MAR_3D;          Fig. 9. The pseudo code of the MAR algorithm.  4.1 Deadlock Avoidance  Deadlock is a situation where network resources continuously  wait for each other to be released. To show that the proposed  algorithms are deadlock free, it is required to prove that there is  no cyclic dependency between channels.   4.1.1 The partitioning methods are deadlock free   If we could prove that the message routing algorithm in the high  channel subnetwork is deadlock free, then it is obvious that the  low channel subnetwork is also deadlock free, and since  GH∩GL=Φ, the whole network will be free of deadlocks. So, we  take the high channel subnetwork into consideration. In order to  reduce the overall path length, a multicast message can be split  into several multicast packets each containing different  destinations.  If a multicast message  is supposed  to be  m=(u,{d1,d2,d3,d4}), then it can be broken into m1=(u,{d1,d4})  and m2=(u,{d2,d3}). According to the Hamiltonian path, the  intermediate nodes are selected in a way that:   For packet m1:  L(v0)≤L(u)<L(a1)≤L(a2)≤…≤L(ax)≤L(d1)<L(ax+1)≤L(ax+2)≤…≤L(a y)≤L(d4)≤L(vn-1).  For packet m2:  L(v0)≤L(u)<L(b1)≤L(b2)≤…≤L(bx)≤L(d2)<L(bx+1)≤L(bx+2)≤…≤L(b y)≤L(d3)≤L(vn-1).   According to the Hamiltonian path strategy, there cannot exist any  link like (ai,ai+1) or (bi,bi+1) where L(ai)>L(ai+1) or L(bi)>L(bi+1).  Some common resources (e.g. a1=b1, a2=b2) might be used by  both m1 and m2 packets. However, since m1 and m2 and the other  possible created packets are routed only in ascending order,  regardless of the underlying partitioning method, splitting a  multicast message into several packets can be done without  introducing any additional dependency. The similar proof can be  applied to the low channel subnetwork.  4.1.2 MAR algorithm is deadlock free:  Given a source node u and a destination node d, p(u, d) is a set of  all possible neighboring nodes of node u that can be chosen to  deliver a packet. In the MAR algorithm, the labels of these  neighbors are between the label of the current node and the label  of the destination node, so that L(u)<L(p(u,d))≤L(d). Therefore,  for a multicast message m=(u,{d1,d2}),  L(v0)≤L(u)<L(p1(u,d1))≤L(p2(p1,d1))≤…≤L(px(px_1,d1))≤L(d1)<L( px+1(d1,d2))≤L(px+2(px+1,d2))≤…≤L(py(py-1,d2))<L(d2)≤L(vn-1).   Since the MAR algorithm always follows the path in ascending  order, no cyclic dependency can be formed among channels, and  thus the MAR algorithm is deadlock free. The similar proof can  be applied to the low channel subnetwork.  5. RESULTS AND DISCUSSION  To assess the efficiency of the proposed partitioning method, we  have developed a cycle-accurate NoC simulator based on  wormhole switching in 3D-mesh configuration. The simulator  calculates the average delay and the power consumption for the  message transmission. The simulator inputs include the array size,  the routing algorithm, the link width, buffer size, and the traffic  type.      To estimate the power consumption of routers, we have used  Orion library functions  [30]. The power and delay of both  horizontal and vertical links are modeled based on the equation in   [9]. Finally, we have compared the proposed partitioning methods  with each other. The on-chip network, considered for experiment  is formed by a typical wormhole router structure including input  buffers, a routing unit, a switch allocator and a crossbar. Each  router has 7 input/output ports, a natural extension from a 5-port  2D router by adding two ports to make connections to the upper  and lower layers  [26] [31]. There are some other types of 3D  routers such as the hybrid router  [5] [31] [32] and MIRA  [33],  however, since router efficiency is out of the concept of this  paper, we have chosen simple 7-port router in our simulation. The  arbitration scheme of the switch allocator in the typical router  structure is round-robin. The data width and the frequency were  set to 32 bits and 1 GHz , respectively, and each input channel has  a buffer size of 6 flits. The packet size was assumed to be 12 flits.  We also assume that the 3D-mesh topology is regular and the  delays on wires will not exceed the clock period. For the  performance metric, we use the multicast latency defined as the  number of cycles between the initiation of a multicast packet  operation and the time when the tail of the multicast packet  reaches all the destinations.  5.1 Multicast Traffic Profile  The first set of simulations was performed for a random traffic  profile. The array size was considered to be 4×4×4. In the  multicast traffic profile, each core sends a message to a set of  destinations. A uniform distribution is used to construct the  destination set of each multicast message  [11]. The number of  destinations has been set to 8 and 16. The average communication  delay as a function of the average message injection rate has been  shown in Fig. 10. As observed from the results, the RP method  meets lower delay than the TBP and VBP methods. As mentioned  earlier, adaptive routing algorithms obtain better performance in  congested networks due to using alternative routing paths   [22] [23]. This can be seen in Fig. 11 where ARP and AVBP are  the adaptive models of the RP and VBP, respectively. As it is  illustrated, adaptive routings become more advantageous when the  injection rate increases.  5.2 Unicast and Multicast (Mixed) Traffic  Profile  In this set of simulation, we have employed a mixture of unicast  and multicast traffic, where 80% of injected messages are unicast  messages and the remaining 20% are multicast messages. Hotspot  traffic model profile  [34] has been taken into account for unicast  traffic generation. Under the hotspot traffic pattern, one or more  nodes are chosen as hotspots receiving an extra portion of the  traffic in addition to the regular uniform traffic. In the hotspot  traffic model, given a hotspot percentage of h, a newly generated  message is directed to each hotspot node with an additional h  percent probability. We simulate hotspot traffic with a single  hotspot node. The hotspot node is chosen to be node (2,2,2) in the  4×4×4 mesh network. Fig. 12 shows the performance with h =  10%. As the figure shows, the RP method outperforms the other  two partitioning methods. Also, Fig. 13 reveals that when utilizing  adaptive routing to route the messages based on the presented  partitioning methods, the adaptive routing reduces the average  latency in comparison with the deterministic routing.   5.3 Application Traffic Profile  In order to show the real impact of the presented methods, we  used traces from some application benchmark suites selected from  SPLASH-2  [15], and PARSEC  [16] [17] using the GEMS  [35]  simulator which is based on a cycle-accurate 3D NoC. We have  used a 4×4×4 mesh network in which out of 64 nodes, 16 nodes  are processors in the first layer and the other 48 nodes are shared  cache memories. The simulations are run on the Solaris 9  operating system based on SPARC instruction set.   Each  processor has a private write-back L1 cache (16KB, 4-way  associative and 64-bit line) along with the shared L2 cache (1MB,  and 64-bit line). The L2 cache shared by all processors is split into  banks. The size of each cache bank node is 1MB; hence, the total  size of shared L2 cache is 48MB. The simulated memory/cache  architecture mimics static non-uniform cache architecture  (SNUCA) where the main memory is a 4GB DRAM. As shown in  Fig. 14, MAR diminishes the average delay of each method  significantly under all benchmarks. That is, adaptive routing has  an opportunity to improve performance. For instance, under the fft  application, the performance gain of ARP over TBP, ATBP, RP,  VBP, and AVBP is about 42%, 37%, 7%, 26%, and 16%.   5.4 Hardware Overhead  To evaluate the area overhead of the proposed methods, the  routers were synthesized with Synopsys D.C. using UMC 90nm  standard cell  library. Depending on  the  technology and  manufacturing process, the pitches of TSVs can range from 1μm  to 10μm square  [36]. In this work, the pad size for TSVs is  assumed to be 5μm square with pitch of around 8µm. All the  schemes used the same routing unit implementation (MAR), but  their partitioning mechanisms use different computation modules.  Comparing the area cost of the base router with the TBP, VBP,  and RP schemes indicates 4%, 8%, and 11% additional overhead,  respectively.   5.5 Power Dissipation  The power dissipation of the TBP, VBP, and RP methods were  calculated and compared under the multicast traffic model with 16  destinations using the simulator based on the Orion and the  equation in  [9]. The typical clock of 1 GHz is applied in the 4×4×4  3D-mesh network. The results for the average power under  multicast traffic are shown in Fig. 15. The average power values  are computed near the saturation point, 0.16 (messages/cycle),  under multicast traffic. As the results, the average power  consumption of the RP scheme is 16% and 8% less than that of  the TBP and VBP  schemes,  respectively, when using  deterministic routing. In fact, this is achieved by smoothly  balancing the traffic over the network using efficient balancing  scheme which reduces the number of the hotspots and, hence,  lowering the average power.  6. SUMMARY AND CONCLUSION  This paper presented a novel idea of balanced partitioning to  partition the network effectively. We first presented a set of  partitioning methods for 3D-mesh NoCs each with a different  level of efficiency. In order to achieve higher performance with  minimum startup latency, the recursive partitioning method was  introduced. This method partitions the network recursively until  all partitions contain comparable number of nodes. Experimental  results show that the presented idea in the recursive partitioning  method reduces the transmission delay and provides a high degree  of parallelism compared with the other proposed methods, twoblock partitioning and vertical block partitioning. The paper  continued by presenting an adaptive routing algorithm for both  unicast and multicast traffic in 3D-mesh NoCs. The presented  algorithm can add adaptivity to the network by taking advantage  of the Hamiltonian path strategy without using virtual channels.  Fig. 10. Performance with different loads in 4×4×4 3D-mesh using deterministic routing with (a) 8 destinations, (b) 16 destinations.  Fig. 11. Performance with different loads in 4×4×4 3D-mesh using adaptive routing with (a) 8 destinations, (b) 16 destinations.  Fig. 12. Performance with different loads in 4×4×4 3D-mesh using deterministic routing with (a) 8 destinations, (b) 16 destinations under mixed traffic  (20% multicast and 80% unicast). Unicast traffic is based on the hotspot traffic model with a single hotspot node (2,2,2), and h=10%.  Fig. 13. Performance with different loads in 4×4×4 3D-mesh using adaptive routing with (a) 8 destinations, (b) 16 destinations under mixed traffic (20%  multicast and 80% unicast). Unicast traffic is based on the hotspot traffic model with a single hotspot node (2,2,2), and h=10%.  0 50 100 150 200 250 300 350 0 0.05 0.1 0.15 0.2 0.25 0.3 A e v r a L e g a t y c n e ( c y c l e ) Message Injection Rate (messages/cycle) (a) TBP RP VBP 0 50 100 150 200 250 300 350 0 0.05 0.1 0.15 0.2 0.25 0.3 A e v r a L e g a t y c n e ( c y c l e ) Message Injection Rate (messages/cycle) (b) TBP RP VBP 0 50 100 150 200 250 300 350 0 0.1 0.2 0.3 A e v r e g a a L t y c n e ( c y c l e ) Message Injection Rate (messages/cycle) (a) AVBP VBP RP ARP 0 50 100 150 200 250 300 350 0 0.1 0.2 0.3 A e v r e g a a L t y c n e ( c y c l e ) Message Injection Rate (messages/cycle) (b) AVBP VBP RP ARP 0 50 100 150 200 250 300 350 0 0.05 0.1 0.15 0.2 0.25 A e v r e g a a L t y c n e ( c y c l e ) Message Injection Rate (messages/cycle) (a) TBP RP VBP 0 50 100 150 200 250 300 350 0 0.05 0.1 0.15 0.2 0.25 A e v r a L e g a t y c n e ( c y c l e ) Message Injection Rate (messages/cycle) (b) TBP RP VBP 0 50 100 150 200 250 300 350 0 0.05 0.1 0.15 0.2 0.25 A e v r e g a a L t y c n e ( c y c l e ) Message Injection Rate (messages/cycle) (a) AVBP VBP RP ARP 0 50 100 150 200 250 300 350 0 0.05 0.1 0.15 0.2 0.25 A e v r e g a a L t y c n e ( c y c l e ) Message Injection Rate (messages/cycle) (b) AVBP VBP RP ARP                                                                                                           y c n e t a l e g a r e v a d e z i l a m r o N 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 TBP ATBP RP ARP VBP AVBP ) W ( r e w o P e g a r e v A Adaptive deterministic 0.188 0.181 0.169 0.174 0.161 0.155 0.2 0.18 0.16 0.14 0.12 0.1 x264 Radix Ocean fft Fig. 14. Performance under different application benchmarks.  "
Interconnect Physical Analyser (IPAA) applied to the design of scalable Network-on-Chip interconnect for Cryptographic accelerators.,"This paper introduces Interconnect Physical Analyser (IPAA) - a tool for the analysis and optimisation of SoC/NoC toplevel interconnect. IPAA extracts information from the IC router and power analysis tool after implementation. Combining information from both sources, the tool performs a wirelength-driven power analysis of toplevel interconnect in the design. A range of statistics for toplevel interconnect is reported and a set of plots is produced. Multiple designs can be analysed simultaneously, enabling comparison and optimisation. The tool is applied to the design of scalable, efficient bus-replacement Network-on-Chip interconnect for Public Key Cryptographic accelerators. IPAA analyses the physical effects of scaling the cryptographic accelerator's floorplan, the number of cores and the cryptographic wordlength. The tool's plots and reports highlight the efficiency and scalability of the bus replacement Network-on-Chip and help guide the optimisation of the design.","Interconnect Physical Analyser (IPAA) applied to the design of scalable Network-on-Chip Interconnect for Cryptographic Accelerators Tom English and Emanuel Popovici Dept of Electrical and Electronic Engineering University College Cork Ireland tom.english@ue.ucc.ie, e.popovici@ucc.ie ABSTRACT This paper introduces Interconnect Physical Analyser (IPAA) - a tool for the analysis and optimisation of SoC/NoC toplevel interconnect. IPAA extracts information from the IC router and power analysis tool after implementation. Combining information from both sources, the tool performs a wirelength-driven power analysis of toplevel interconnect in the design. A range of statistics for toplevel interconnect is reported and a set of plots is produced. Multiple designs can be analysed simultaneously, enabling comparison and optimisation. The tool is applied to the design of scalable, eﬃcient bus-replacement Network-on-Chip interconnect for Public Key Cryptographic accelerators. IPAA analyses the physical eﬀects of scaling the cryptographic accelerator’s ﬂoorplan, the number of cores and the cryptographic wordlength. The tool’s plots and reports highlight the efﬁciency and scalability of the bus replacement Network-onChip and help guide the optimisation of the design. Categories and Subject Descriptors B.7.2 [Hardware]: Integrated Circuits—design aids General Terms Design, Measurement, Performance Keywords Interconnect, Physical Design, Networks-on-Chip 1. INTRODUCTION The analysis of power consumption in a 130nm Intel microprocessor presented in [1] is based on the observation that many wire physical and power consumption metrics are strongly correlated with wire length. The analysis yields several interesting results with relevance to designers of onPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS’11 May 1-4, 2011 Pittsburgh, PA, USA Copyright 2011 ACM 978-1-4503-0720-8 ...$10.00. chip networks and other forms of global on-chip interconnect. Firstly, interconnect power consumption accounts for over 50% of total power consumption in the microprocessor. This is broken down roughly evenly between local and global power consumption. The contribution of global interconnect tends to increase as feature size decreases due to the undesirable eﬀects of physical scaling on long wires [2]. Secondly, a small number of high-capacitance, high-activity global nets account for a large proportion of the overall switching power budget. These nets clearly merit the most optimisation effort. Accordingly, the authors of [1] modiﬁed their routing methodology to reduce the lengths and capacitances of the most power-consuming nets. This reduced interconnect dynamic power dissipation signiﬁcantly. Commercial IC implementation and power analysis tools do not readily facilitate this kind of wirelength-driven analysis of power consumption in toplevel interconnect. IC routers provide physical metrics, while power analysis tools report wire switching and power consumption. The approach in [1] calls for these physical and power statistics to be brought together. Therefore, there is a need for a tool which combines physical information from the router with power and activity information from the power analysis tool. Doing this facilitates a detailed, wirelength-driven analysis of interconnect power consumption in SoCs and - in particular Networks-on-Chip. Interconnect Physical Analyser (IPAA) is a new CAD tool which aims to achieve this. It is designed to drive the analysis and optimisation of global interconnect. It extracts physical attributes for toplevel wires from the router and switching activity and power consumption data from the power analysis tool. By merging these, IPAA obtains the wirelength and wire switching power distributions, along with a variety of other statistics. Multiple designs can be analysed at once, facilitating automated comparisons between architectures, between iterations of the same architecture or even between various software scenarios running on a single physical design. As a case study, IPAA is applied to the design of scalable, eﬃcient bus-replacement Network-on-Chip (NoC) interconnect for Public Key Cryptographic accelerators. IPAA analyses the physical eﬀects of scaling the cryptographic accelerator’s ﬂoorplan, the number of cores and the cryptographic wordlength. The tool’s statistical comparisons, wirelength and switching power histograms and other plots illustrate the eﬃciency and scalability of the bus replacement NoC design. 2. RELATED WORK 2.1 Interconnect Physical Analysis At the circuit level, the INTACTE tool [3] facilitates design space exploration for point-to-point interconnect, while NoCIC [4] focuses on power, area and frequency prediction in 2D mesh NoC links. At the architecture level, ORION [5] uses post-placement models of building blocks such as crossbars, FIFOs and links to estimate overall NoC power and area based on a range of user-supplied parameters. These front-end estimation tools facilitate NoC exploration before physical design. IPAA analyses physical designs after implementation. It gathers information from the back-end tools, reporting detailed and accurate wirelength and switching power statistics. To the authors’ knowledge, no similar academic or commercial tools have appeared. 2.2 Bus Replacement NoCs Angiolini [6] carried out a comparison of multi-layer AMBA AHB with the ×pipes NoC technology on 130nm CMOS. The scalability of ×pipes on 65nm CMOS was explored in [7]. These works highlighted not only predictable physical beneﬁts to the NoC but also the excessive energy, area and latency cost of buﬀers. In this paper, the NoC designs we describe are buﬀerless and focus on low latency rather than high throughput. They are eﬃcient, scalable and exhibit excellent physical characteristics. The only other known proposal for bus-replacement NoCs in Public Key cryptographic hardware is the authors’ earlier work [8]. 3. IPAA TOOL ARCHITECTURE Interconnect Physical Analyser (IPAA) merges physical and power metrics to produce the graphs and statistics for a wirelength-driven global interconnect power optimisation methodology. After a design has been placed and routed, the tool extracts a range of statistics from both the router and the power analysis tool. It merges the information in these separate silos to produce plots and reports for wirelengthdriven global interconnect power optimisation. IPAA is implemented in MATLAB and augments the Synopsys Galaxy IC implementation ﬂow1 . Wire physical information is extracted from IC Compiler post place-and-route. Wire power information is extracted from PrimeTime-PX. 3.1 Analysis & Optimisation Flow A design ﬂow incorporating IPAA is shown in Figure 1. The tool is run at the end of the physical implementation, when accurate physical and power information is available. IPAA ﬁrst extracts the wire physical attributes from IC Compiler. Using ICC’s report_net -physical command, IPAA automatically extracts the physical attributes of all toplevel wires to a text report. Clock, power and ground nets are excluded using ﬁlter terms. Along with each wire’s name, the physical attributes extracted are: • Length (l, µm) • Number of fanouts (nfo) and fanins (nfi) • Number of pins (np) • Number of vias (nv) • Load capacitance (Cload , F ) 1http://www.synopsys.com/galaxy Figure 1: Analysis ﬂow using IPAA • Resistance (r, Ω) The tool then performs a power analysis using PrimeTimePX. After switching activity is annotated onto the design, wire switching power information is extracted for all toplevel nets using report_power -net. Along with each net’s name, the power metrics extracted are: • Switching Power (Psw W ) • Static Probability (sp) • Toggle Rate (tr) • Swing voltage (Vdd , V ) • Load capacitance (Cload , F ) IPAA can compare multiple physical designs in a single run. Each design is treated as a separate ob ject called a PDI (Physical DesIgn ). To create a PDI, the raw physical and power reports from ICC and PrimeTime-PX go through a PDI Preprocessor which removes extraneous information. Creating multiple PDIs enables physical designs with different architectures, successive iterations of a particular architecture or multiple use cases of a single physical design to be compared. In this way, IPAA serves both as a comparative analysis tool and as an iterative optimisation driver. After all PDIs have been read into IPAA, the tool matches nets from ICC and PrimeTime, merging physical and power metrics for each. User-deﬁned name ﬁlters are then applied to each PDI. This facilitates regular-expression-based control over the nets included in the analysis. 3.2 IPAA Statistics and Reports For each PDI, the tool identiﬁes the total wirelength (Ltot = Σ(l)), the maximum and minimum wirelength (lmin , lmax ) and the mean and standard deviation (µ(l), σ(l)). The total wire switching power (Psw tot = Σ(Psw )), minimum and maximum switching power (Psw min , Psw max ), mean (µ(Psw )) and standard deviation (σ(Psw )) are also calculated. Maxima, minima, mean and standard deviation are calculated for Wire Resistance (r), capacitive load (Cload ), fanout (nfo) and via count (nv). All of these statistics for given by (cid:112)(µP .Psw tot )2 + (µL .Ltot )2 . The programmable each PDI are listed in a single report ﬁle for comparative analysis. PDIs are awarded a physical design quality metric, Q, scaling coeﬃcients µP and µL have units of 1 W and 1 m respectively, making Q dimensionless. The Q metric allows functionally-equivalent designs or iterations of the same design to be ranked according to their relative physical eﬃciency. Smaller Q values represent lower overall toplevel wire and power costs. The scaling coeﬃcients allow the relative weights of power and wirelength in the Quality metric to be adjusted. Both coeﬃcients are set to 1 here. The tool produces reports listing the nets in each PDI sorted according to various criteria - wirelength (l), number of vias (nv), toggle rate (tr), switching power (Psw ) and number of fanouts (nfo). A ranking report sorts nets by length and then by switching power, identifying the longest, most power-hungry toplevel nets. 3.3 IPAA Plots IPAA’s plots contain overlaid datapoints from all PDIs, facilitating visual comparisons when more than one PDI is analysed. There are nine plots: 1. wirelength (l) histogram 2. wire switching power (Psw ) histogram 3. Psw vs l scatter plot 4. fanout (nfo) vs wirelength (l) scatter plot 5. toggle rate (tr) vs wirelength (l) scatter plot 6. load Cload vs wirelength (l) scatter plot 7. resistance (r) vs wirelength (l) scatter plot 8. via count (nv) vs wirelength (l) scatter plot 9. Quality scatter plot, (x, y) = (µL .Ltot , µP .Psw tot ) for each PDI 4. IPAA CASE STUDY - INTERCONNECT FOR PUBLIC KEY CRYPTOGRAPHIC ACCELERATORS Public Key cryptographic algorithms such as RSA/DSA and their Elliptic Curve Cryptography (ECC) equivalent ECDSA have a shared reliance on modular multiplication. Consequently, hardware acceleration is beneﬁcial to both. However, the eﬃciency of a hardware implementation often comes at the cost of ﬂexibility and scalability [9]. A few proposals for scalable Public Key Cryptography hardware accelerators have appeared [10, 11]. In both of these designs, throughput is sacriﬁced for scalability; cryptographic operands are broken down and processed serially. This reduces the complexity of both the cryptographic functional units and the bus interconnect between them. To preserve throughput, it is preferable to process cryptographic operands without serialisation. ECC operands typically have wordlengths between 160 and 512 bits, while RSA operands reach 2048 bits. Transporting these over a scalable, eﬃcient and high-performance interconnect is challenging. This case study explores the scalability of bus interconnect in Public Key Cryptographic hardware. It aims to develop a bus replacement interconnect using a Network-on-Chip. The NoC must overcome the physical scalability limits of the bus without introducing excessive latency or area overhead. A bus-based test platform for Public Key Cryptography has been developed. The ﬂoorplan size, number of cryptographic functional units and cryptographic wordlength of the test platform are parameterised to facilitate scalability experiments. A Network-on-Chip-style replacement interconnect is developed which aims to oﬀer improved scalability, better energy eﬃciency and reduced routing congestion. Bus and NoC versions of the test platform expressed in SystemVerilog RTL are synthesised, placed and routed on 65nm CMOS. Using IPAA’s wirelength-driven analysis of toplevel interconnect, we compare the scalability of bus and NoC interconnects as we scale the designs in three separate domains. Firstly, the ﬂoorplan area (FA) is scaled, modelling the eﬀect of using larger, more complex functional units for improved performance. Secondly, the number of functional units (N ) is scaled, modelling the eﬀect of adding new hardware for improved ﬂexibility. Finally, the cryptographic data wordlength (DW ) is scaled, modelling the effect of increased security requirements on the interconnect. 4.1 Scalability Test Platforms 4.1.1 Bus-based Test Platform To explore the scalability of bus interconnect, a parameterised Public Key Cryptography test platform was created. The reference for our test platform is the ﬂattened design of a hardware cryptographic accelerator for an ECC function called the Tate Pairing [12]. In this reference design, there are four GF (2m ) Galois ﬁeld functional units - a modular multiplier, divider, adder and squarer. The Pairing operation is sequenced by a microcode ROM and a state machine. There is no operational parallelism and data is communicated point-to-point over an m-bit shared bus. This type of architecture is representative of many accelerators for ECC and for Public Key Cryptography generally. The test platform (referred to here as Quadra ) reﬂects the architecture of the reference design. Its parameters allow us to explore the scalability of such a design in three domains: N , the number of Functional Units, DW - the data wordlength and FA, the ﬂoorplan area. Dummy functional units communicate point-to-point over a DW -bit shared bus which interconnects all FUs. There is no parallelism; control is centralised and operations are statically scheduled. A single system clock runs at fclk = 100MHz . A block diagram of the Quadra test platform appears in Figure 2. Figure 2: Bus-based test platform (Quadra ) 4.1.2 NoC-based Test Platform For comparison with the bus-based Quadra platform, a version using a NoC as a Bus Replacement Interconnect was created. The NoC is very simple and oﬀers low latency. The aim is to overcome the physical scalability problems of long, wide buses, such as skew, wire congestion and excessive switching power. Figure 3: NoC-based test platform (Nevada ) The NoC-based test platform (referred to as Nevada ) integrates a NoC router into the FU Hub to connect four FUs, forming a cluster. The cluster’s star topology oﬀers low latency and low cost, but limited throughput. The router quired, using a 5×5 1-way switch, illustrated in Figure 4. creates circuits between source and destination FUs as reIncoming packets cannot collide due to the lack of parallelism in the design, eliminating arbitration logic and costly buﬀers. A single clock-gated pipeline register is placed at each switch output. Figure 5: NoC P W -bit link stances of the 4-FU cluster via Hierarchy Links with a cluster hub. The cluster hub’s operation is similar to the FU Hub. Figure 6: Nevada N scaling methodology 4.2 Scaling Floorplan Area (FA) The square ﬂoorplan area of the test platforms, FA µm2 , is calculated as the square of the ﬂoorplan side length, F SL µm. Quadra and Nevada were implemented initially with N = 4 and DW = 256 using a congestion-driven square ﬂoorplan with F SL ≈ 430µm. F SL was increased in steps to a maximum of 4000µm. This models the use of higherperformance cryptographic functional units consuming greater silicon area. The Functional Units were pinned to the four corners of the ﬂoorplan, with the hub in the centre, stretching the interconnect as shown in Figure 7. The P W parameter was used to create two variants of the Nevada test platform - the ﬁrst with P W = DW = 256 and the second with P W = 128. Figure 4: 5×5 1-way Switch in FU Hub DW -bit words are transmitted serially through the network as a sequence of P W -bit phits. The Phit Width (P W ) and clock frequency (fclk ) parameters facilitate a tradeoﬀ between power and routing congestion while maintaining the same runtime and throughput as the bus-based test platform. Links between FUs and routers are not pipelined, to keep latency to a minimum. The NoC link is illustrated in Figure 5. The Nevada test platform scales in the same domains as Quadra - N , DW and FA. As illustrated in Figure 6, scaling in the N domain is accomplished by interconnecting inFigure 7: F SL scaling methodology IPAA’s wirelength histograms illustrate how toplevel wirelength is aﬀected in the bus (Figure 8) and NoC (Figure 9) as F SL is increased. The eﬀect of P W on NoC toplevel wirelength is evident. IPAA’s statistics report that at F SL = 4000µm, total toplevel wirelength and switching power in the P W = 128 NoC are 59% and 65% lower, respectively, than the bus version, indicating it is the most scalable. Figure 8: Bus Wirelength Histogram vs F SL Figure 9: NoC Wirelength Histogram vs F SL IPAA’s scatter plot (Figure 10) of wire switching power vs wirelength highlights the switching power impact of scaling the ﬂoorplan area. Each point (x, y) = (l, Psw ) on this plot indicates the length and switching power consumption of an individual toplevel net. The bus nets’ length and switching power grow faster than those of the NoCs as F SL is scaled. IPAA’s tr-vs-l scatter plot identiﬁes that the multicast bus nets toggle more frequently than the point-to-point NoC links, explaining the increased switching power consumption. While the NoC provides the most eﬃcient toplevel interconnect as F SL is increased, the energy consumption of the entire design must determine which is the better solution overall. Figure 11 shows overall energy consumption for the bus and NoC test platforms as F SL is scaled. Bus interconnect is most energy-eﬃcient when the ﬂoorplan is small and wires are short. As the ﬂoorplan area increases, however, the eﬃciency of the bus degrades rapidly Figure 10: Wire Switching Power versus Wirelength as F SL is scaled due to the increased contribution of switching power. The energy cost of all of the NoC variants grows much more slowly, highlighting the poor scalability of wired buses compared to the NoC. test platform at F SL ≈ 1000µm and exceed it thereafter. The NoC variants match the eﬃciency of the bus-based Accordingly, the F SL parameter is ﬁxed at 2000µm for all further explorations. Small ﬂoorplans for which the NoC overheads outweigh the beneﬁts are not investigated. 4.3 Scaling Number of Functional Units (N) To explore interconnect scalability versus the number of functional units (N ), three variants of the Quadra and Nevada test platforms having N = 4, N = 8 and N = 16 were implemented. F SL, DW and P W were ﬁxed at 2000µm, 256 and 128, respectively. Functional Units were distributed around the edge of the ﬂoorplan, with the Hub in the centre. IPAA’s wire switching power histogram (Figure 12) illustrates the eﬀect of N scaling on the bus- and NoC-based test platforms. Despite the overall amount of communication traﬃc remaining the same regardless of N , the Quality (Q) scatter plot (Figure 13) highlights rapid degradation in eﬃciency of the bus interconnect as N is scaled. The statistics report indicates that Quadra ’s Ltot and Psw tot grow by factors of 4 and 2.4 respectively as N scales from 4 to 16. The report also identiﬁes that the total number of toplevel bus wire segments grows 570%. While the mean load (µ(Cload )), resistance (µ(r)) and via count (µ(nv)) of individual segments is comparable in bus and NoC versions, the sheer number of bus wire segments requires three times the number of vias of the equivalent NoC version. Overall energy consumption ﬁgures versus N for the bus and NoC test platforms are shown in Figure 14. The NoC versions represent not just the most eﬃcient solution for all values of N but also the more scalable interconnect. 4.4 Scaling Cryptographic Wordlength (DW) The DW parameter determines the data wordlength of each test platform. DW values are chosen in powers of two from 256 to 2048, spanning the range found in modern ECC and RSA. All variants have N = 4 and F SL = 2000µm. Figure 11: Overall Energy Consumption of the test platforms as F SL is scaled Figure 12: Toplevel Wire Switching Power vs N Figure 13: Quality (Q) Metric vs N IPAA’s wirelength histograms (Figures 15(a) and 15(b)) show how the bus and NoC wirelengths are aﬀected by the data wordlength DW . Total toplevel wirelength grows much faster in the P W = DW NoC than the bus version. The P W = DW 2 NoC scales better, reaching DW = 2048 with 47% lower total toplevel wirelength than the 2048-bit bus version. The wire switching power histogram (Figure 15(c)) highlights the diﬀerence between the switching power distributions of the bus and NoC interconnects. The bus distributions have a markedly diﬀerent shape with more powerhungry nets than their NoC equivalents. IPAA’s Quality (Q) scatter plot (Figure 15(d)) identiﬁes the diﬀering eﬀects of scaling DW on the bus and NoC test platforms. The total toplevel switching power Psw tot of the Quadra test platform increases rapidly as DW is scaled. The P W = DW Nevada NoC exhibits a greater increase in total wirelength Ltot than in Psw tot . Clearly, the P W = 2 NoC scales best overall and oﬀers good predictability. At DW = 2048, it consumes 59% less switching power for toplevel interconnect than the 2048-bit bus equivalent. The statistics report shows that the P W = DW 2 NoC has 50% fewer toplevel wire segments and roughly the same mean via count per wire as the bus equivalent - µ(nv) ≈ 25. This results in a reduction in overall toplevel via count. On DW Figure 14: Overall Energy Consumption of the test platforms as N is scaled the other hand the P W = DW NoC has on average 39 vias/wire and 65% more wire segments than the bus, resulting in a much larger via count. The P W = DW NoC also exhibits roughly 50% greater total toplevel wirelength than its bus equivalent. However, despite increased via count and total wirelength, the P W = DW NoC still consumes up to 22% less toplevel switching power than the bus equivalent. Overall energy consumption results appear in Figure 16. The contribution of wire switching energy is indicated in green, while all other energy is indicated in blue. The NoCs prove to be up to 24% more energy-eﬃcient than the bus for values of DW less than 2048. At DW = 2048, however, the increasing contribution of clock tree switching power and (a) Bus Wirelength Histogram vs DW (b) NoC Wirelength Histogram vs DW (c) Wire Switching Power vs DW (d) Quality (Q) Metric vs DW Figure 15: Bus and NoC metrics as DW is scaled register internal power means that the P W = DW 2 NoC consumes slightly more energy than the bus version. This is due to the greater number of registers in the design, the larger clocktree and the increased fclk required to maintain throughput. Despite this, the NoCs still oﬀer reduced routing congestion and consume less cell area compared to the bus versions. The cell area of the bus versions is dominated by buﬀer cells, which constitute as much as 56% of the total cellcount. IC Compiler’s implementation runtimes are reduced by as much as 65% by the use of a NoC at DW = 2048, underscoring the better physical scalability and reduced routing complexity of the NoC design compared to a bus. 4.5 Summary The NoC scales more easily than the bus in the FA and N domains. In the DW domain, the toplevel results reported by IPAA indicate that the P W = DW 2 NoC is the most scalable interconnect. However, increasing power consumption by synchronous parts of the design hierarchy causes the overall energy consumption of the NoC test platform to exceed that of the bus equivalent as DW reaches 2048. IPAA results highlight the link between toplevel wirelength and overall scalability. Reducing toplevel wirelength contributes to a better-behaved physical design, with reduced congestion and switching power consumption. IPAA’s results have shown the eﬀect of reducing the Phit Width (P W ) on toplevel wirelength. However, reduced phit width requires a compensating increase in fclk to preserve throughput. Experiments reveal that reducing P W below DW fails to improve energy eﬃciency. IPAA’s toggle rate (tr) plots and reports identify the lower switching activity on the NoC links compared to the bus wires. We have taken advantage of this with a modiﬁed Nevada NoC having Double DataRate (DDR) P W = DW 4 NoC links. Initial experiments suggest that DDR delivers both improved energy eﬃciency and physical scalability. The DDR technique is implemented at RTL and is entirely synthesis-friendly, requiring no special cells or timing constraints. 2 Figure 16: Overall Energy Consumption of the the test platforms as DW is scaled 5. CONCLUSIONS A new CAD tool called Interconnect Physical Analyser (IPAA) has been presented. The tool reads physical information about top-level wires from Synopsys IC Compiler, along with power information from Synopsys PrimeTimePX. After merging these, the tool extracts the toplevel wirelength and switching power distributions along with an array of other statistics, producing a set of reports and graphs. IPAA was applied to the design of a bus replacement Network-on-Chip for a Public Key Cryptography accelerator. The NoC is based on a Star topology and employs circuit switching for the lowest possible latency and energy costs. Test platforms featuring functionally-equivalent bus and NoC interconnects were implemented fully on 65nm CMOS. The test platforms were scaled in three domains - ﬂoorplan area, number of connected Functional Units and data wordlength. The NoC demonstrates better physical scalability than the bus interconnect in all three domains with no loss of throughput. Along with better physical properties, the NoC oﬀers reduced energy consumption in nearly all cases compared to the bus. We have applied IPAA to other case studies, including the iterative optimisation of a high-performance Ring NoC for a cryptographic application. Over ten iterations, the tool guided us towards ma jor reductions in both toplevel wirelength and switching power, resulting in a more energyeﬃcient, reliable ﬁnal design. As a proof of concept, the tool has also been applied successfully to the comparison of toplevel interconnect power consumption in a microcontroller design across a range of software use cases. 6. ACKNOWLEDGEMENTS The authors acknowledge the support of IDA Ireland and Synopsys in funding this research. Thanks also to W.P. Marnane and Maurice Keller for many useful discussions. 7. "
Curbing energy cravings in networks - A cross-sectional view across the micro-macro boundary.,"The soaring power dissipation of computing infrastructures has effectively become a key performance bottleneck. At the same time, thermal issues arising from power hungry devices result in compromising reliability of such systems. At present, the processor cores within a chip multiprocessor (CMP) are interconnected using on-chip networks. At the same time, the emerging era of cloud computing and the associated large-scale distributed computing model are stretching the limits of computer networking. Hence, as an essential and pervasive fabric that binds all computing machinery, there is an underlying network that appears with varying scale and characteristics. In this paper, we take a look at methodologies for better management of the power budget across the cross-section of computing systems traversing the boundary between micro-level on-chip networks to macro-level cloud computing. We discuss methodologies for power and thermal management both at micro and macro scales and explore the role of one in reducing the power budget of the other.","Curbing Energy Cravings in Networks: a Cross-Sectional  View Across the Micro-Macro Boundary Amlan Ganguly  Department of Computer Engineering  Rochester Institute of Technology  Rochester, NY, USA  amlan.ganguly@rit.edu  Partha Kundu  Corporate CTO Office  Juniper Networks  Sunnyvale, CA, USA  partha.kundu@juniper.net  Pradip Bose  Thomas J. Watson Research Center  IBM  Yorktown Heights, NY, USA  pbose@us.ibm.com  ABSTRACT  The soaring power dissipation of computing infrastructures has  effectively become a key performance bottleneck. At the same  time, thermal issues arising from power hungry devices result in  compromising reliability of such systems. At present, the  processor cores within a chip multiprocessor (CMP) are  interconnected using on-chip networks. At the same time, the  emerging era of cloud computing and the associated large-scale  distributed computing model are stretching the limits of  computer networking. Hence, as an essential and pervasive  fabric that binds all computing machinery, there is an underlying  network that appears with varying scale and characteristics. In  this paper, we take a look at methodologies for better  management of the power budget across the cross-section of  computing systems traversing the boundary between micro-level  on-chip networks to macro-level cloud computing. We discuss  methodologies for power and thermal management both at micro  and macro scales and explore the role of one in reducing the  power budget of the other.  Categories and Subject Descriptors  B.7.1 [Integrated Circuits]: Types and Design Styles –  advanced technologies, microprocessors and microcomputers.  General Terms  Performance, Design.  Keywords  Network-on-Chip, Interconnect, Data Center, Cloud Computing  1. INTRODUCTION  The increasing power consumption of Integrated Circuits has  plagued the semiconductor industry over the past decade. Soaring  power dissipation has proved to be the limiting factor in the quest  for maintaining historical performance growth at the chip and  system level. The problem appears to be escalating at an  astounding rate with each passing year. This has necessitated  paradigm shifts in the IT and semiconductor industries at various  levels. For example, power density and  instruction-level  parallelism (ILP) limits caused an end to the processor frequency  escalation game. Shrinking feature size and increase in transistor  density helped usher in the era of multi-core chips. This paradigm  leverages  task-level parallelism  to keep  the  throughput  performance curve on an upward trajectory, despite the power  crunch. However, the multi-core paradigm did not eliminate the  power wall; it was only able to push it a bit to the future.  Permission to make digital or hard copies of all or part of this work  for personal or classroom use is granted without fee provided that  copies are not made or distributed for profit or commercial  advantage and that copies bear this notice and the full citation on the  first page. To copy otherwise, or republish, to post on servers or to  redistribute to lists, requires prior specific permission and/or a fee.  NOCS'11, May 1-4, 2011 Pittsburgh, PA, USA  Copyright 2011 ACM 978-1-4503-0720-8… $10.00.  Increasing core counts have stressed the off-chip memory  bandwidth; and, the on-chip network along with the I/O support  facilities have contributed to an escalating area and power  challenge yet again. Power management in multi-core chips has  demanded a fresh look and has hence become a significant  research direction both in industry and academia. Effective chiplevel power management techniques can mitigate lifetime  reliability issues resulting from thermal hot spots. Processor cores  are generally the hottest regions on a chip in terms of power  density or actual temperature. On the other hand, on-chip  interconnection networks or Networks-on-Chip (NoCs) form the  backbone of multi-core chips [1]. Furthermore, a major fraction  of all chip power dissipation is from the metal-dielectric based  interconnects; and this fraction is steadily on the rise with the  increase in core count. This trend has resulted in extensive  research and innovations in novel interconnect technologies.  Photonic, RF and wireless interconnects coupled with threedimensional (3D) integration bears the promise of reducing  interconnect power dissipation by several orders of magnitude [25].   Turning our attention to the networks in the macro world, it is  important to note that the world-wide server power consumption  and cooling costs amounted to over twenty-six billion dollars in  2005 and was predicted to increase to over forty four billion in  the next five years. These figures clearly indicate the need to  rethink the way software and hardware are currently organized in  the IT industry. Collective efforts of researchers and practitioners  in recent years have resulted in the new paradigm of Cloud  Computing. Cloud computing is transforming the IT industry by  making the use of software and associated computing a utilitybased service and requiring significant redesign of hardware  systems. Data centers and servers located “up in the clouds” have  very different needs for controlling power dissipation and  maintaining reliability [6]. Hence, in both micro and macro  networks spanning the IT world, we are forced to look at  performance with a vigilant eye on power consumption levels.  In this paper, we strive to provide a holistic view of power  management techniques for two new paradigms namely, multicore chips and cloud computing and their impact on cost and  reliability. We address the issue at different scales and from three  different perspectives. In particular, we discuss the power and  thermal management techniques for multi-core chips, possibility  of interconnect-architecture co-design for power optimization of  on-chip networks and the role of NoCs in Data Centers or Cloud  Computing.   In the following sections we elaborate upon each of the three  methodologies and their promises and challenges.   2. POWER AND THERMAL  MANAGEMENT OF MULTI-CORE  PROCESSORS  In this section we discuss the techniques developed for power  management of multi-core chips with a side reference to the role  of the on-chip interconnection fabric. A brief comment on the  implications of such power management technology in the      emerging context of three-dimensional integrated circuits is also  included.  At the processor chip-level, the initial design focus is often on  static efficiency: i.e. trying to ensure that the design is as “lean”  and “simple” as possible, without compromising system  performance targets. For this purpose, accurate, cycle-accurate  power-performance models  (e.g.  the detailed, core-level  reference model, Mref, alluded to in Jacobson et al. [7]) are used  to track processor power on a weekly basis during the concept  definition and RTL  implementation phase of  the chip  development project. For such power tracking exercises, the  primary focus is usually on maximum power, in the context of  the worst-case “power virus”  test case. Another key  consideration, of course, is the “idle power” level of the  processor core, chip and overall system; i.e. the no-load power  consumed. The design goal is to minimize the peak and no-load  power-levels, without compromising market-driven performance  targets.   Once the peak power level is known or fixed, the attention shifts  to dynamic power management. In this case, the changing  workload dynamics are exploited by the power manager to meet  system-dictated goals like: (a) maximize a specified index of  performance without violating the stipulated maximum power  draw (and dissipation) limits; (b) minimize the average power  consumption over long durations, while adhering to committed  system-level performance or quality of service (QoS) levels.   The power manager could be built into the chip hardware  functionality (as in Intel’s Nehalem chip offering [8]) or  architected as an off-chip controller, backed by the system  firmware/software stack (as in IBM’s POWER7-based systems  [9]). In abstraction, the multi-core power management solution  approach can be depicted as a hierarchical sense-and-control  system of its own (see Figure 1, quoted from [10]).   In this view, we consider each core to have multiple operating  modes that we may call power modes. (In a more generalized  view, non-core elements of the chip or system – e.g. storage or  interconnect resources – would also have multiple operational  power modes). These modes can be set independently by the  hierarchical controller, depending on workload behavior, overall  budget constraints and mode-shift constraints. Within each core  (or other controllable resource), we may assume local (open- or  closed-loop) control actions, without interference from the  global controller. For example, baseline dynamic power control  techniques  like clock-gating, fetch-gating, adaptive cache  resizing or even adaptive control of the on-chip network  reconfiguration may be applied via such resource-local controls.  The local monitors do, however, provide periodic per-core  power-performance data to the global manager. The global  management layer provides per-core mode-setting directives, in  coordination with higher-level scheduling and load-balancing  directives provided by system software, as indicated in Figure 1.  As reported in Isci et al. [10], using per-core dynamic voltage  and frequency scaling (DVFS) as the basic mode-setting  mechanism, one could devise dynamic power management  algorithms that maximize the chip throughput performance, for  any specified power budget.   Bus-based interconnect fabrics are still the prevalent on-chip  communication medium  in most server-class multi-core  processor  chips. Specifically, pipelined bus or  ring  interconnection protocols are commonly used in such systems.  This interconnect resource still consumes a relatively small  percentage of the total chip power in today’s systems; although,  if one factors in the on-chip I/O related power as well then the  net “communication-related” power fraction can be as high as  15-20%. This fraction is of course expected to grow in the  future. At present this component of chip power tends to be  Figure 1. Hierarchical Sense-and-Control Power Manager  pretty constant in absolute terms (i.e. watts). In other words, the  workload-dependent variance is not visible, because of lack of  dynamic power management  facilities  that are directed  specifically at this component of the power pie. Adding in the  clock-grid power and considering latches that are not clockgateable due to cycle-time constraints, one can easily see that a  large percentage of the chip power (perhaps in the range of 2540%, depending on the particular chip) may remain constant,  regardless of the workload that is running – i.e. even for the  “processor idle” scenario. High levels of idle power at the chip  level are not conducive to realization of the vision of  proportional computing [11] at the system or data center level.  Recent directions in chip- and system-level power management  have been directed aggressively at reduction of idle power, via  use of techniques like power-gating [8], in which both active and  leakage components of power consumption are eliminated, when  the targeted resource is not in use.   As on-chip power, performance,  temperature and aging  (reliability) sensors become pervasive in future multi-core chips  and associated systems, a legitimate area of research is in  figuring out how the on/off-chip sensor network needs to be  architected, in conjunction with the actual data interconnect.  This architecture must be such that it is essentially “hidden” or  totally non-intrusive in terms of added power, off-chip data  bandwidth and overall system performance.  In managing peak power consumption within a multi-core  system, with built-in data and sensor interconnect networks,  future algorithms are likely to adopt an application-driven  policy, in which so-called power-shifting protocols are applied  to dynamically re-allocate resource-specific power budgets  within the system. Thus, if the application phase is CPUintensive, then certain storage and communication resources  may be gated off to conserve power; while, if the phase is  storage- or communication-intensive, then a significant number  of cores can be turned off. Both intra-core, unit-level power  gating (e.g. [12]) and inter-core power gating heuristics (e.g.  [13]) have been examined in recent prior work. However,  pervasive,  coordinated gating  techniques  across  core,  cache/memory and  interconnect resources have not been  adequately explored in current R&D.  In managing thermal hot-spots across a system of cores within a  chip, or a system of chips within a server (and so on),  temperature aware task scheduling is an interesting area of  research. Initial work in that area (e.g. [14])  points to promising  levels of temperature (and hence leakage power) reduction,  without giving up any performance. The emerging promise of  3D-integrated chip technology, brings with it an even more  challenging power delivery and heat dissipation problem than  posed by current systems. Dynamic power and temperature  management solutions are going to be key in terms of reaping  the potential performance benefits of this new technology.             3. INTERCONNECT-ARCHITECTURE  CO-DESIGN FOR POWER  MANAGEMENT OF ON-CHIP  NETWORKS  The design of multi-core chips in the future technologies will  pose unprecedented challenges to the designer related to an  exponential increase in device densities and soaring power  dissipation issues. It is well understood that traditional on-chip  interconnects are going to be the major bottleneck in the road to  improved performance at low energy costs. Hence, several  alternative interconnect technologies like photonic, RF and  wireless interconnects along with three-dimensional integration  are being explored. However, mere augmenting and replacing  metal interconnects with such novel technologies is not sufficient  to leverage their full potential. Nontraditional, nature-inspired  interconnection architectures coupled with  the  judicious  deployment of such novel interconnects can make the powerperformance hurdle more negotiable given the manufacturing and  integration issues of these technologies using CMOS processes.  Reliability-aware interconnect design with emerging technologies  overcome the challenges of integration and further improve the  trade-offs between power and performance in NoCs. In this  section we will discuss the potential benefits and design  challenges of adopting non-classical on-chip  interconnect  technologies like photonic, RF and wireless along with threedimensional interconnects in conjunction with efficient natureinspired architectures.  3.1  Emerging On-Chip  Technologies  Interconnect  The Network-on-Chip (NoC) paradigm proposed in current  research, has  the potential of emerging as a scalable  interconnection infrastructure for future multi-core chips that  integrate hundreds of cores on the same die. The mesh-connected  topology is a commonly advocated vision of such many-core  architectures. However, the advantage of adopting such a NoC as  the communication backbone is severely limited by its inherent  multi-hop mode of data transfer. Transfer of data packets through  several intermediate stages result in both high energy dissipation  and high latency. According to the International Technology  Roadmap  for Semiconductors  (ITRS),  improvements  in  metal/dielectric based interconnects through material innovation  alone will not be able to sustain the power-performance trend  needed to support the multi-core chips. In order to alleviate this  problem several alternative interconnect technologies have been  envisioned.  3.1.1. Three-Dimensional Integration  3D IC structures that contain multiple layers of active devices  have been proposed in the literature. Since, the distances between  cores or devices are much shorter in the vertical dimension, the  bottlenecks arising out of data transfer along long wireline paths  are mitigated to a large extent. Hence, as shown in [5] there are  huge gains in performance and energy dissipation profiles of  multi-core 3D ICs enabled by 3D NoCs.  The enabling technology that makes 3D chips possible are  vertical interconnects. Among various technologies proposed to  create these vertical interconnects, Through Silicon Vias (TSVs)  provide the highest interconnect densities and are hence more  widely adopted. However, despite advances in research and  development of 3D IC technology in both academia and industry,  TSVs suffer from manufacturability issues and yields are shown  to drop drastically with increase in the number of TSVs beyond  certain thresholds. Besides causing a sharp increase in testing cost,  this problem also limits the advantages of 3D chips as the number  of TSVs in reality is not very scalable. Hence, 3D NoCs designed  with TSVs need to be aware of this inherent limitation in 3D IC  technologies.  3.1.2. Photonic Interconnects  Photonic waveguides were envisioned as an alternative to metal  wires as the means of data transfer in multi-core NoCs [2]. The  fundamental premise is that data transfer through photonic  signaling would completely avoid power dissipation caused by  the parasitics of metal interconnects. It is shown that photonic  NoCs dissipate orders of magnitude less energy in data transfer  compared to wireline NoCs. Miniature nano-photonic electrooptic signal converters like Mach Zehnder Modulators and  Demodulators and micro-ring resonators are possible to design in  such a way as to make on-chip integration a possibility. However,  photonic devices and waveguides would require a separate active  layer on the die resulting in 3D integration.   The area overheads of photonic devices and waveguides are  shown to be significant. The overhead required to sustain  significant gains over traditional counterparts in the presence of  scaling system size would become prohibitively high [15].  Irrespective of the topology, abundant use of photonic devices in  the NoC can result in issues related to reliable manufacturability.  Research is yet to address the manufacturing issues of NoC  architectures with dense photonic waveguides.  3.1.3. RF-Interconnects  NoCs overlaid with a microstrip waveguide was proposed in [3].  Predictions indicate that NoC architectures with RF interconnects  serving as traffic “freeways” would result in significant gains in  power dissipation profiles of multi-core chips.  Optimum benefits can be leveraged from the RF interconnect  only when multiple users can use it together. Hence a collisionfree routing scheme depending on multiple non-overlapping  frequency bands has been proposed. However, in reality this  requires high precision frequency synthesizers and band-select  filters for the transceivers. Design of such analog components is  non-trivial and can often result in loss of reliability. Hence, more  efficient communication protocols become necessary for optimal  usage of the RF interconnect.  3.1.4. Wireless Interconnects  Recently proposed miniature on-chip antennas make on-chip  wireless interconnects a possibility. Wireless NoCs designed with  both metal antennas operating in sub-THz frequency range [16]  and carbon nanotube (CNT) based antennas operating in THz  frequency range [4] are shown to improve the performance and  power dissipation characteristics.  Mm-wave metal antennas have transceiver overheads and limited  wireless bandwidth which is much less than the bisection  bandwidth of wireline NoCs. Hence, the deployment of mmwave wireless links is a critical aspect of wireless NoC design.  CNT based antennas in the THz band have manufacturing issues  related to controlled fabrication of CNT elements.   Consequently, all NoC architectures designed with the emerging  interconnects geared towards lower energy dissipation and  simultaneous gains in performance need to be aware of the  challenges in reliability associated with each of them. Efficient  fault-tolerant architecture design is the key to leveraging the  benefits of these technologies.  3.2 Interconnect-Architecture Co-Design  In this subsection we outline methodologies to design NoCs using  any emerging interconnect technology while being aware of their  advantages as well as inherent unreliability. Here we look at the  problem of an optimal architecture design using one or multiple  emerging interconnects. In particular, we focus on ways to  minimize power dissipation  in data  transfer along with  performance and area overhead while maintaining scalability.  Based on state-of-the-art research it can be assumed that  judicious use of the emerging interconnects can help us better  negotiate the power-performance bottleneck but only if they are  adoptable in NoC designs. Hence, the key to leverage the  advantages of these technologies is to design reliability aware  take  the nontraditional  full advantage of  systems  that  interconnects.  The problem of designing the optimal NoC architecture with the  novel interconnects is therefore a multi-variable optimization  problem taking into account several metrics to optimize as well as  several factors on which those metrics depend. The problem can  be therefore stated as   .   (1)  o  o  Here, it is desirable to optimize the architecture, A with respect to  the following metrics:  •  energy dissipation, E,   •  performance, P,   •  area overhead, A  •  reliability, R.   The architecture, A can be considered to be dependent upon the  following aspects:  • Connectivity, C, which in turn depends upon:  Floorplan of cores, Co  Floorplan of switches, S  o The set of interconnects to be used, I  • Routing strategy, ρ, which depends on C  • Communication paradigm, γ, which depends on I  •  Traffic pattern, τ, which depends on:   Mapped application, Ap  A systematic mathematical approach to solve the optimization  problem quickly becomes  intractable because of  the  interdependence of the factors and scalability. Hence, the solution  space of this problem can be derived from evolutionary or genetic  algorithms. Millennia of evolution in the domain of biological  networks of living organisms have generated tested solutions to  such complex optimization problems.  o  3.2.1 Topology  Biological networks like food webs, neuromorphic networks and  biochemical nets in molecular biology have evolved into  topologies which have very efficient interconnection architectures  coupled with a high degree of resilience to failures. For example,  fast mutations of bacteria to attain resilience to antibiotics prove  the robustness of networks in terms of the sexual interactions  between the bacteria. Quick recollections of visions experienced  long back in time by the human brain is also proof of the efficient  connectivity of neurons and fast recovery times. All these  complex systems can be analyzed by complex network theory. It  is observed that naturally occurring complex networks called  Small-World networks have evolved to be highly resilient to  targeted attacks whereas, Scale-Free networks are resilient to  random failures [17]. Figure 2 shows small-world and scale-free  Figure 2. (a) Small-World and (b) Scale-Free Network  topologies.  network topologies. All nodes of small-world networks have  similar degrees of connectivity. Hence, no particular node is more  critical to the overall performance of the network than the others.  Consequently, attacks on any node affect the network marginally.  By contrast, nodes of scale-free networks are widely distinct in  their connectivity where only a few nodes are connected to many  other nodes and most nodes have very few connections. Thus  random failures often result in failures of nodes or links which  are not critical to the overall performance of the network.  Consequently, interconnect fault models indicating random  failures of interconnects can be used to design NoCs with scalefree topologies. This will ensure fault-tolerance of the NoCs  which are designed with emerging interconnects failing randomly  and sustain the advantages of the respective interconnect  technologies. On the other hand, fault models for interconnects  which indicate a pattern of attacks can be best addressed by  small-world  topologies. Following  these design rules  the  interconnection topology of the NoC inherently solves the issues  of fault-tolerance with emerging interconnects. Hence, the key  element in the design of NoCs with novel interconnects can be  addressed.  Food webs of living organisms like slime mold are observed to  evolve into efficient networks depending upon the location of  food particles. Similar evolutionary approaches can be adopted to  establish network topologies based on traffic interactions between  cores on the chip. For example, traffic hotspots can be modeled  as location of food particles and the network topology designed to  evolve around those hotspots.  3.2.2 Routing Strategy  All these emerging interconnects need to be used in conjunction  with efficient communication protocols. In NoCs, data is  typically transferred via a flit-based wormhole routing [1]. Due to  the irregular topology of the SWNoC, the adopted routing  algorithm needs to be aware of the established topology. A  decentralized, shortest-path routing strategy needs to be adopted  where at each switch the shortest path to the destination for a  particular packet is determined and the flits are advanced to the  next switch. This is done by an exhaustive search of all possible  paths between the source and the destination for that packet.  However, exhaustive search shortest path algorithms can be very  expensive to implement in on-chip routers. Hence, sophisticated  yet  lower complexity routing algorithms or heuristics  to  determine the shortest path need to be evaluated.   3.2.3 Communication Protocols  Design of efficient communication protocols is needed to  maximize the benefit of each of the emerging interconnects. A  communication protocol can be viewed as a means of reducing  overheads. In 3D NoCs, communication in the vertical dimension  through TSVs can be implemented through a bus-based protocol.  This reduces the overheads of multi-hop data transfer in the short  vertical dimension. But collisions on the common vertical bus can  lead to degradation of performance. Consequently, collision  avoidance bus arbitration mechanisms need to be implemented to  improve performance.   In order to reduce overheads from abundant use of optical  waveguides in photonic NoCs and make efficient use of shared  wireless medium, medium access control (MAC) protocols can  be  implemented. To reduce area overheads of photonic  waveguides,  multiple cores may be using the same links and  hence the need for a MAC arises. Similarly due to limited  bandwidth of wireless channels efficient MAC protocols become  necessary to enable multiple cores communicate simultaneously  using the wireless medium. Study of channel characteristics of  photonic or RF waveguides and on-chip wireless channels can be  used to design appropriate MAC protocols for use with respective  interconnect technologies.  Through reliable architecture design, the emerging interconnect  technologies can be made viable and adoptable in the face of      integration and manufacturing issues to deliver better powerperformance trade-offs than traditional NoCs. In the next section  we discuss how multi-core NoCs can contribute to a sustainable  and energy efficient computing infrastructure which involves  networks of computing resources.  4. ROLE OF NOC IN DATA CENTER  AND CLOUD COMPUTING  The era of ubiquitous computing seems to be dawning near: with  the increased proliferation of thin and portable devices connected  through relatively high bandwidth pipes to the internet, it is  finally becoming possible to deploy services far from the actual  consumer with a utility “pay as you need” style of computing.  This usage model insulates the common user from having to do  complex system maintenance on software and hardware. At the  same time, enterprises are realizing significant capital and  operating cost reductions through usage of a shared data center  infrastructure: one that is outsourced to a third party. Enterprises  may now retain a very small footprint of IT resources (if at all)  and defer on a demand basis to this data center in the “clouds” for  most compute and storage intensive services.  4.1 Data Center Architecture  Typical data centers hold tens to hundreds of thousands of servers  and concurrently support a large number of distinct services e.g.  search, email, map-reduce computations etc. The motivations for  building such data centers are both economic and technical: to  leverage the economies of scale available to bulk deployments  and to benefit from the ability to dynamically re-allocate servers  among services as workload changes or equipment fails. The cost  is also large: upwards of $12 million per month for a 100,000  server data center – with the servers and network fabric  interconnecting  these servers, comprising  the  largest cost  component. To understand the investment on the network:  consider that each server hosts 32 virtual machines (VMs). This  translates to a total of 3 million IP and MAC addresses in the data  center. For a typical data center installation of 100,000 nodes,  assuming one switch is required for every 25 physical hosts and  accounting for interior nodes, the fabric topology would consist  of 8000 switches.   4.2 Power Requirements  A typical server node has a power supply rating of about 500  watts. This rating covers the power delivery (or supply)  requirements. In addition, for every watt of power consumed  roughly 0.5-1 watt of power is expended in extracting the heat  generated. A typical data center installation consumes 10-30 MW  of power to operate [6]. Furthermore, [18] shows that the power  consumption of a server has increased by nearly a factor of 10  over the last 10 years.   While the nominal power consumption rating per server is high,  much of the time the server lies underutilized [19]. Furthermore,  there is wide disparity within component utilization when  running different workloads running in the data center [20].  Without careful application tuning and load balancing, a server  may not be well utilized.  4.3 Network Requirements  The key to energy efficiency (and thus overall cost reduction) is  high server utilization. High utilization requires agility: without  agility, each service must pre-allocate enough servers to meet  difficult to predict demand spikes, or risk failure at the brink of  success. With agility, the data center operator can meet the  fluctuating demands of individual services from a large shared  server pool, resulting in higher server utilization and lower costs  directly leading to lower energy usage.  Thus we require that VMs migrate ideally based on resource  footprint requirements; and that any VM may migrate to any  physical machine. Migrating VMs should not have to change  their IP addresses to avoid breaking any pre-existing TCP  connections and application level state. We also require that  routing, forwarding and management protocols that we run in  data centers evolve out of the same building blocks used in LANs  viz. Ethernet.  4.4 How can the NOC help?    As multi-cores become the norm both in the generalized  computing world, application specific processing nodes, fabric  controllers and the like, the NOC becomes an extension of the  interconnection backbone and thereby can realize synergies for a  more stable system design. Much of data center traffic is IP based  and packets are forwarded over Ethernet-based fabrics.  4.5 Virtual Machine Migration  VM migrations make packet forwarding difficult, having to  resolve IP address with the physical MAC address. Furthermore,  reconfiguration (in the presence of failure and other management  tasks) is difficult and error prone. Researchers have proposed  several schemes recently that attempt to fuse layers 2 & 3 to  improve network manageability and network performance. [21]  & [22] use a flat addressing to allow service instances to be  placed anywhere in the network. It is envisioned that the NOC of  the future will be required to do Layer 2 & layer 3 packet  forwarding within a single socket to reduce latency, conserve  interconnection bandwidth and thereby conserve energy.  4.6 Energy conservation through TCP layer  acceleration  A consistent theme in data center design has been to build highly  available, high performance computing and storage infrastructure  using low cost, commodity components. Correspondingly, there  is a trend to provide low cost switches at the top-of-rack,  providing up to 48 ports at 1/10/100 Gbps at cost points  approximating a single data center server. While attractive, this  exposes certain risks: many of the applications running in the data  center require near-real time deadlines for end results [23].  Queries exceeding 10-100ms are cancelled thereby impacting  revenue. Reducing network latency for these flows is greatly  beneficial in these circumstances so that application developers  may use the cycles gained. At the same time, large flows –  carrying media, text etc. – need to achieve high utilization. Thus  the challenge is one where the network needs to maintain low  latency as well as high network throughput (to enable fat flows to  be transported) particularly as small (often latency sensitive)  traffic is queued behind large flows. Researchers have proposed  modifying TCP particularly with the view of data center usage:  Data Center TCP [23] obtains congestion experienced in the path  of the packet. It then reacts by reducing the window by a factor  that is proportional to the congestion experienced. The latency  sensitive Query traffic (2KB – 20KB sizes) meanwhile is subject  to another TCP phenomenon: Incast collapse [24] whereby highly  bursty fast data transmissions overfill Ethernet switch buffers  causing intense packet loss that leads to TCP timeouts. These  timeouts last hundreds of milliseconds on a network whose  round-trip-time (RTT) is in the 10-100 micro-seconds.  These  timeouts can reduce application throughput by 90% or more [25].   [26] proposes micro-timeout that is substantially lower than the  network RTT.  In conjunction with  their proposal  to  desynchronize re-transmissions, the authors show close to  optimal network throughput. Ideally, under Ethernet flow control  we would want to stall some large flows and enable some smaller  ones – in order to make optimal overall system progress. It is  envisioned  that a NOC can rapidly react  to congestion  information collected downstream and create a smoother  interleaving of flows.  5. CONCLUSION  Power and thermal management of computing systems is an everincreasing challenge. The issue of power dissipation exists in  computing systems at all scales from on-chip networks to largescale data centers or the “cloud”. In this paper we have discussed  some emerging methodologies for power management at both  micro and macro levels.  6. "
Complex network inspired fault-tolerant NoC architectures with wireless links.,"The Network-on-Chip (NoC) paradigm has emerged as a scalable interconnection infrastructure for modern multi-core chips. However, with growing levels of integration, the traditional NoCs suffer from high latency and energy dissipation in on-chip data transfer due to conventional metal/dielectric based interconnects. Three-dimensional integration, on-chip photonic, RF and wireless links have been proposed as radical low-power and low-latency alternatives to the conventional planar wire-based designs. Wireless NoCs with Carbon Nanotube (CNT) antennas are shown to outperform traditional wire based NoCs by several orders of magnitude in power dissipation and latency. However such transformative technologies will be prone to high levels of faults and failures due to various issues related to manufacturing and integration. On the other hand, several naturally occurring complex networks such as colonies of microbes and the internet are known to be inherently fault-tolerant against high rates of failures and harsh environments. This paper proposes to adopt such complex network based architectures to minimize the effect of wireless link failures on the performance of the NoC. Through cycle accurate simulations it is shown that the wireless NoC architectures inspired by natural complex networks perform better than their conventional wired counterparts even in the presence of a high degree of faults.","Amlan Ganguly  Department of Computer Engineering  Rochester Institute of Technology  Rochester, NY, USA  +1-585-475-4082  amlan.ganguly@rit.edu  Paul Wettin, Kevin Chang  School of Electrical Engineering and  Computer Science  Washington State University  Pullman, WA, USA  +1-509-335-6249  Partha Pande  School of Electrical Engineering and  Computer Science  Washington State University  Pullman, WA, USA  +1-509-335-5223  Complex Network Inspired Fault-Tolerant NoC  Architectures with Wireless Links  {pwettin,jchang}@eecs.wsu.edu  pande@eecs.wsu.edu  ABSTRACT  The Network-on-Chip (NoC) paradigm has emerged as a scalable  interconnection  infrastructure  for modern multi-core chips.  However, with growing levels of integration, the traditional NoCs  suffer from high latency and energy dissipation in on-chip data  transfer due to conventional metal/dielectric based interconnects.  Three-dimensional integration, on-chip photonic, RF and wireless  links have been proposed as radical low-power and low-latency  alternatives  to  the conventional planar wire-based designs.  Wireless NoCs with Carbon Nanotube (CNT) antennas are shown  to outperform traditional wire based NoCs by several orders of  magnitude in power dissipation and latency. However such  transformative technologies will be prone to high levels of faults  and failures due to various issues related to manufacturing and  integration. On the other hand, several naturally occurring complex  networks such as colonies of microbes and the internet are known  to be inherently fault-tolerant against high rates of failures and  harsh environments. This paper proposes to adopt such complex  network based architectures to minimize the effect of wireless link  failures on the performance of the NoC. Through cycle accurate  simulations it is shown that the wireless NoC architectures inspired  by natural complex networks perform better  than  their  conventional wired counterparts even in the presence of a high  degree of faults.   Categories and Subject Descriptors  B.8.1 [Performance and Reliability]: Reliability, Testing and  Fault-Tolerance  General Terms  Performance, Reliability.   Keywords  Wireless Links, Small-World, Fault-Tolerance.  1. INTRODUCTION  Massive levels of integration are making modern multi-core chips  all pervasive  in several domains  ranging  from scientific  applications like weather forecasting, astronomical data analysis,  and bioinformatics to even consumer electronics like graphics and  animation. Design of multi-core integrated systems beyond the  current CMOS era will present unprecedented advantages and  challenges, the former being related to very high device densities  Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  NOCS'11, May 1-4, 2011 Pittsburgh, PA, USA  Copyright 2011 ACM 978-1-4503-0720-8… $10.00.  and the latter to soaring power dissipation issues. According to the  International Technology Roadmap for Semiconductors (ITRS) in  2007, the contribution of interconnects to chip power dissipation is  expected to increase from 51% in the 0.13µm technology  generation to up to 80% in the next five-year period. This clearly  indicates the challenges facing future chip designers associated  with traditional scaling of metal interconnects and material  innovation. Conventionally, the Network-on-Chip (NoC) paradigm  is used as a scalable interconnection infrastructure for multi-core  chips [1]. Advances in NoC research along several dimensions  spanning architectural explorations, development of routing  protocols, and reliability have made it the choice for the  communication backbone of complex multi-core chips. However,  despite its widespread adoption, traditional NoCs suffer from the  limitations arising out of planar metal interconnect-based multihop communication, which in turn gives rise to high power and  latency. Several attempts have been made to improve the  limitations of conventional NoC architectures by innovations in  routing and/or supplementary  interconnect deployments [2].  However, due to the basic interconnect technology being the  metal/dielectric combination the performance improvements were  only incremental. Hence, advantages of radically new interconnect  technologies need to be harnessed in NoCs. To enhance the  performance of conventional metal interconnect-based multi-core  chips a few radically different interconnect technologies are being  currently explored such as, 3D integration, Photonic interconnects  and multi-band RF or wireless interconnects [3-5]. All these new  technologies have been predicted to be capable of enabling multicore designs that improve the speed and power dissipation in intercore data  transfer significantly. However,  these alternative  interconnect paradigms are in their formative stage and need to  overcome significant challenges pertaining to integration and  reliability. So far, all these interconnect technologies have been  used  in existing multi-core platforms without significant  architectural innovations, which undermines their adoptability in  the face of potentially high levels of malfunctions.  Many naturally occurring large irregular networks like the internet,  microbial colonies, cortical interconnections or social groups are  found to display a striking resilience to faults either in the form of  random failures or targeted attacks. Theoretical studies in complex  networks reveal that certain types of network connectivity are  inherently more resilient to failures [6]. A particular class of  complex networks called small-world networks has a fairly  homogeneous structure where all nodes have almost the same  number of links. Both attacks and random failures of such links  marginally affect the overall connectivity of this network. The  reason for this behavior is the fact that it has a low average  distance between cores and that no particular link contributes more  to the interconnectedness of the system than the others due to the      homogeneity of the network. Adopting novel architectures inspired  by such natural complex networks in conjunction with the  emerging interconnection technologies will enable design of highperformance and robust multi-core chips for the future.  Wireless NoC is one of the promising alternatives to alleviate the  performance limitations of traditional multi-hop counterparts.  However, wireless interconnects with either Carbon Nanotube  (CNT) based antennas or mm-wave metal antennas may encounter  significant failure rates pertaining to issues of integration and  transceiver design respectively. NoCs using these emerging  interconnects demand high performance from inherently unreliable  technology. Small-world networks have very short average  distance between any pair of nodes and are known for their  robustness and efficient communication mechanisms [7-8]. Hence,  adopting a small-world connectivity with on-chip wireless links  will not only significantly improve NoC performance; it will also  enhance  the robustness of  the system. In  this paper, we  demonstrate how small-world NoC architectures are capable of  handling a high-degree of possible faults  in  the wireless  communication  channels without  significant  impact  on  performance.  2. RELATED WORK  Conventional NoCs  use multi-hop  packet  switched  communication. To improve performance, the concept of express  virtual channels is introduced in [9]. It is shown that by using  virtual express lanes to connect distant cores in the network, it is  possible to avoid the router overhead at intermediate nodes, and  thereby greatly improve NoC performance. NoCs have been shown  to perform better by inserting long range wired links following  principles of small world graphs [2]. The design principles of  three-dimensional NoCs and photonic NoCs are elaborated in  various recent publications [3-4]. It is estimated that 3D and  photonic NoCs will dissipate significantly less power than their  electronic counterpart. Another alternative for low power is NoCs  with multi-band RF interconnects [5]. Recently, the design of a  wireless NoC based on CMOS Ultra Wideband (UWB) technology  was proposed [10]. In [11] the feasibility of designing on-chip  wireless communication network with miniature antennas and  simple transceivers that operate at the sub-THz range of 100-500  GHz has been demonstrated. If the transmission frequencies can be  increased to THz range then the corresponding antenna sizes  decrease, occupying much less chip real estate. One possibility is  to use nanoscale antennas based on CNTs operating in the THz  frequency range [12]. Consequently building an on-chip wireless  interconnection network using THz frequencies for inter-core  communications becomes feasible. Design of a wireless NoC  operating in the THz frequency range using CNT antennas is  elaborated in [13]. All these emerging interconnect technologies  are shown to improve the performance and power dissipation of  NoCs. However due to the fact that these technologies are still in  their formative stages their fabrication and integration with  standard CMOS processes are unreliable. Moreover, system level  techniques for sustaining gains in performance in presence of these  inherently unreliable  technologies have not received much  attention from researchers. In this paper we propose an architecture  that will sustain the advantages of one such technology even in the  presence of inherent unreliability. In particular we propose the  design of a wireless NoC using THz wireless links with CNT  based antennas which will enable high gains in performance and  power dissipation despite high rates of failures of the wireless  links.   3. COMPLEX NETWORK INSPIRED  ARCHITECTURE  Naturally occurring complex networks with a power-law based  connectivity model [6] are shown to exhibit robustness against  high rates of failures of nodes or links. One such type of network is  commonly known as small-world network. In this work we  propose the design of fault-tolerant small-world wireless NoCs  (SWNoCs) with a power-law based interconnection architecture  and investigate its performance in presence of failures of the  wireless links. In this section we discuss the various aspects of the  design methodology for the SWNoC architecture like topology,  routing and physical layer.  3.1 SWNoC Topology  In the proposed SWNoC topology, each core is connected to a  NoC switch and the switches are interconnected using wireline and  wireless links. The topology of the SWNoC is a small-world  network where  the  links between switches are established  following a power law distribution as shown in (1).   (1)  Where, the probability of establishing a link, between two  switches, i and j, P(i,j), separated by an Euclidean distance of lij is  proportional to the distance raised to a finite power [7]. The  frequency of traffic interaction between the cores, fij, is also  factored into (1) so that more frequently communicating cores  have a higher probability of having a direct link. This frequency is  expressed as the percentage of traffic generated from i that is  addressed to j. This frequency distribution is based on the  particular application mapped to the overall SWNoC and is hence  set prior  to wireless  link  insertion. Therefore,  the apriori  knowledge of the traffic pattern is used to establish the topology  with a correlation between traffic distribution across the NoC and  network configuration as in [14]. This optimizes the network  architecture for non-uniform traffic scenarios. The parameters, α  and β govern the nature of connectivity and the significance of the  traffic pattern on the topology respectively. In particular, a bigger  α would mean a very locally connected network with few or no  long range links similar to that of a cellular automata based  topology. Whereas, a zero value of α would generate an ideal  small-world network following the Watts-Strogatz model [8] with  long range shortcuts virtually independent of the distance between  the cores. A higher value of β implies a higher probability of  establishing direct links between switches with higher traffic  exchange. Both of these parameters can be considered as design  knobs that maybe tuned for a particular application to generate  optimized network architecture depending on floorplan and traffic.  In this work we look at the performance of a wireless NoC with  small-world  topology by taking into consideration physical  parameters  like  interconnect characteristics. As  long wired  interconnects are extremely costly both in terms of power and  latency we use wireless links to connect switches that are separated  by a long distance. Depending upon the available wireless  resources as explained in section IIID, we have a constraint on the  maximum number of possible wireless links in the SWNoC, say, n.  Hence, we make the n longest links in the SWNoC wireless while  the others still remain wireline. Figure 1 represents such a SWNoC  with 16 cores where each core is associated with a NoC switch (not  shown for clarity). It has many short range local links as well as  few long range shortcuts schematically represented by the arching         the spectrum of up to 750 THz have been studied [16]. Antenna  characteristics of carbon nanotubes (CNTs) in the THz frequency  range have also been  investigated both  theoretically and  experimentally [17, 12]. Bundles of CNTs are predicted to enhance  performance of antenna modules by up to 40dB in radiation  efficiency and provide excellent directional properties in far-field  patterns [18]. Moreover these antennas can achieve a bandwidth of  around 500 GHz, whereas the antennas operating in the millimeter  wave range achieve bandwidths of tens of GHz. Thus, antennas  operating in the THz/optical frequency range can support much  higher data rates. CNTs have numerous characteristics that make  them suitable as on-chip antenna elements for THz frequencies.  Given wavelengths of hundreds of nanometers  to several  micrometers, there is a need for virtually one-dimensional antenna  structures for efficient transmission and reception. With diameters  of a few nanometers and any length up to a few millimeters  possible, CNTs are the perfect candidates. Such thin structures are  almost impossible to achieve with traditional microfabrication  techniques for metals. Radiation characteristics of multi-walled  carbon nanotube (MWCNT) antennas are observed to be in  excellent quantitative agreement with traditional radio antenna  theory [12], although at much higher frequencies of hundreds of  THz. Using various lengths of the antenna elements corresponding  to different multiples of the wavelengths of the external lasers,  scattering and radiation patterns are shown to be improved. Such  nanotube antennas are good candidates for establishing on-chip  wireless communications links and are henceforth considered in  this work.  As mentioned above in section IIIA, a subset of all the switches in  the SWNoC has wireless links. These wireless links are connected  to the switches using a wireless port, WP. Consequently, each WP  will require wireless transceivers to be able to establish the bidirectional wireless links. The THz antennas can be excited using  external laser sources [13]. As mentioned in [4], the laser sources  can be located off-chip or bonded to the silicon die. Hence their  power dissipation does not contribute to the chip power density.  To achieve line of sight communication between WPs using CNT  antennas working at THz frequencies, the chip packaging material  has to be elevated from the substrate surface to create a vacuum for  transmission of the high frequency electromagnetic (EM) waves.  Techniques for creating such vacuum packaging are already  utilized for MEMS applications [19], and can be adopted to make  creation of line of sight communication between CNT antennas  viable. In classical antenna theory it is known that the received  power degrades inversely with the 4th power of the separation  between source and destination due to ground reflections beyond a  certain distance. This threshold separation, r0 between source and  destination antennas assuming a perfectly reflecting surface, is  given by (2).  (2) Here H is the height of the antenna above the reflecting surface and  λ is the wavelength of the carrier. Thus, if the antenna elements are  at a distance of H from the reflective surfaces like the packaging  walls and the top of the die substrate, the received power degrades  inversely with the square of the distance until it is r0. Thus H can  be adjusted to make the maximum possible separation smaller than  the threshold separation r0 for a particular frequency of radiation  used. Considering the hundreds of THz frequency ranges of CNT  antennas and depending on the separation between the source and  destination pairs in a single chip, the required elevation is only a  few tens of microns.  Figure 1. SWNoC architecture with short and long range  links.  interconnects. The longest of these links would be wireless  whereas the rest will be wireline.   Most naturally occurring small-world networks are created by selfassembly and does not guarantee a fully connected topology as it is  fundamentally stochastic in nature. So, we adopted a simple  iterative algorithm to ensure that the SWNoC is fully connected.  To establish the network connectivity each pair of switch in the  NoC is selected and a link is established between them with the  probability given in (1). The network setup is repeated until a fully  connected system is formed. We have assumed an average number  of connections from each switch to other switches, <k>. Also, an  upper bound, kmax is imposed on the number of links attached to a  particular  switch  so  that no particular  switch becomes  unrealistically large in the SWNoC. Also, it reduces the skew in  the distribution of the links among the switches. The parameters α  and β can also be chosen to aid in ensuring a fully connected  network. For instance, a lower value of α or β means a more even  distribution of the links and hence less chance of some switches  left being isolated.  Thus, we propose a hybrid small-world wireless NoC architecture  where the longest shortcuts are wireless and the rest of the  interconnections are standard metal wires.  3.2 Hierarchical Wireless NoC  There is another recently proposed wireless NoC architecture with  CNT-based wireless links called WiNoC [13]. This architecture is  a hierarchical hybrid wireless NoC. The whole NoC is subdivided  into smaller networks called subnets. Cores within a subnet are  connected with each other in a regular wireline network like a  mesh, ring or star. The cores are connected to a central hub in each  subnet through wireline links. These hubs are then connected  among themselves using wireline links in a ring forming the upper  level of the network. The wireless links are optimally distributed as  long range shortcuts between a few hubs. We will evaluate the  performance of the SWNoC architecture with respect to WiNoC.   3.3 On-chip Antennas for the Wireless Links  Suitable on-chip antennas are necessary to establish the wireless  links. In [15] the authors demonstrated the performance of silicon  integrated  on-chip  antennas  for  intra-  and  inter-chip  communication. They have primarily used metal zig-zag antennas  operating in the range of tens of GHz. An ultra-wideband (UWB)  antenna was used in the design of a wireless NoC [10] mentioned  earlier in section II. The above mentioned antennas principally  operate in the millimeter wave (tens of GHz) range and  consequently their sizes are on the order of a few millimeters.  If the transmission frequencies and bandwidths are to be increased  to THz/optical range then the corresponding antenna sizes  decrease, occupying much less chip real estate. Characteristics of  metal antennas operating in the optical and near-infrared region of            Chemical vapor deposition (CVD) is the traditional method for  growing nanotubes in specific locations by using lithographically  patterned catalyst islands. The application of an electric field  during growth or the direction of gas flow during CVD can help  align nanotubes. However, the high-temperature CVD could  potentially damage some of the pre-existing CMOS layers. To  alleviate this, localized heaters in the CMOS fabrication process to  enable localized CVD of nanotubes without exposing the entire  chip to high temperatures are used [20]. Many of these processes  lead to stochastically varying CNT characteristics with high  standard deviations from the intended properties. These can cause  high rates of defects in the systems designed using these structures.  In particular, unreliable manufacturing techniques can result in  high degrees of faults in the WPs used in the SWNoC. Hence, we  propose to adopt a complex network based NoC architecture using  these antennas where even extremely high rates of failure of the  antennas have marginal effect on the performance of the system.  3.4 Adopted Routing and Communication  Protocols   In the proposed SWNoC, data is transferred via a flit-based  wormhole routing [1]. Due to the irregular topology of the  SWNoC, the adopted routing algorithm needs to be aware of the  established topology. We adopted a decentralized, shortest-path  routing strategy where at each switch the shortest path to the  destination for a particular packet is determined and the flits are  advanced to the next switch. This is done by an exhaustive search  of all possible paths between the source and the destination for that  packet. This shortest path computation is done only for the header  flit, the body and tail flits simply follow the path established by the  header. It is worth mentioning that many wired interconnects in the  SWNoC are sufficiently long such that they cannot be traversed in  a single clock cycle. While determining the shortest path for each  packet, the actual number of cycles required for the flits to traverse  those links has been considered. The routing algorithm is shown in  Figure 2.  By using multiband laser sources to excite CNT antennas, different  frequency channels can be assigned to pairs of communicating  source and destination nodes. This will require using antenna  elements tuned to different frequencies for each pair, thus creating  a form of frequency division multiplexing (FDM) creating  dedicated channels between a source and destination pair. This is  possible by using CNTs of different lengths, which are multiples of  the wavelengths of the respective carrier frequencies. High  directional gains of these antennas, demonstrated in [18], aid in  creating directed channels between source and destination pairs. In  [21], 24 continuous wave laser sources of different frequencies are  used. Thus, these 24 different frequencies can be assigned to  multiple wireless links in the SWNoC in such a way that a single  frequency channel is used only once to avoid signal interference on  the same frequencies.  This enables concurrent use of multi-band  channels over the chip. The number of wireless links in the  network can therefore vary from 24 links, each with a single  frequency channel, to a single link with all 24 channels. It is shown  in [13] that 24 links each with a single channel provides the best  overall network performance as all the channels are distributed  over the NoC resulting in a better connectivity. Hence, in this work  we assume a maximum of 24 wireless links each with a single  channel for both the SWNoC and the WiNoC architectures.  Currently, high-speed silicon integrated Mach-Zehnder optical  modulators and demodulators, which convert electrical signals to  optical signals and vice versa are commercially available [22]. The  optical modulators can provide 10Gbps data rate per channel on  these links. This rate is also expected to increase in the near future.  At the receiver a low noise amplifier (LNA) can be used to boost  Figure 2.  Distributed routing algorithm for the SWNoC.  the power of the received electrical signal, which will then be  routed to the destination core. The modulation scheme adopted is  non-coherent on-off keying (OOK), and therefore does not require  complex clock recovery and synchronization circuits. Due to  limitations in the number of distinct frequency channels that can be  created through the CNT antennas, the flit width in NoCs is  generally higher than the number of possible channels per link.  Thus, to send a whole flit through the wireless link using a single  channel a proper channelization scheme needs to be adopted. In  this work we assume a flit width of 32 bits. Hence, to send the  whole flit, time division multiplexing (TDM) is adopted. The  various components of the THz wireless transceiver viz., the  electro-optic modulators, the TDM modulator/demodulator, the  LNA and the router for routing data on the network of hubs are  implemented as a part of the WP.  4. EXPERIMENTAL RESULTS  In this section we characterize the performance of the proposed  SWNoC in presence of wireless link failures through detailed  system level simulations. We show that the performance of the  proposed architecture is better than that of a conventional wireline  mesh-based NoC even in the presence of very high rates of failure  of the wireless links. The faults are injected into the system by  randomly disabling a certain percentage of the wireless links. We  also compare the performance of the SWNoC in presence of faults  with  the already proposed WiNoC architecture [13]. The  effectiveness of the SWNoC in presence of non-uniform and  application based traffic pattern is also evaluated. Lastly, we  discuss the area overhead required for designing the SWNoC.  We evaluate the architectures considered in this work using a cycle  accurate simulator which models the progress of data flits  accurately per clock cycle accounting for flits  that reach  destination as well as those that are dropped. One hundred  thousand iterations were performed to reach stable results in each  experiment, eliminating the effect of transients in the first few  thousand cycles. Two different system sizes with 64 and 256 cores  arranged in equal-area tiles over a 20mmx20mm die are considered  in this work. For the 64 core SWNoC <k> and kmax is considered to  be 4 and 8 respectively. For the 256 core system the value of those      parameters are chosen to be 6 and 10 respectively. The width of all  wired links is considered to be same as the flit size, which is 32 in  this paper. The particular NoC switch architecture has three  functional stages, namely,  input arbitration,  routing/switch  traversal, and output arbitration. The input and output ports  including the WPs have four virtual channels per port, each having  a buffer depth of 2 flits. Each packet consists of 64 flits. Similar to  the wired links, we have adopted wormhole routing in the wireless  links too. A simple flow control mechanism is adopted uniformly  for wireless links in which, the sender WP stops transmitting flits  only when a full signal is asserted from the receiver WP. This full  signal is embedded in a control flit sent from the receiver to the  sender only when the receiver buffer is filled above a predefined  threshold. The network switches are synthesized from a RTL level  design using 65nm  standard cell  libraries  from CMP  (http://cmp.imag.fr), using Synopsys Design Vision.  The NoC  switches are driven with a clock of frequency 2.5 GHz. The delays  and energy dissipation on the wired links were obtained through  HSPICE simulations taking into account the specific lengths of  each link based on the established connections in the 20mmx20mm  die following the logical connectivity of the small-world topology.  The energy dissipation on the wireless links were obtained through  analytical and experimental findings in [12] as outlined later in  section IVC.  The next subsection describes the performance metrics that we use  to evaluate the proposed SWNoC architecture. This is followed by  the detailed analysis of its performance compared to conventional  counterparts as well as architectures existing in literature using the  same technology.  4.1 Performance Metrics  To characterize  the performance of  the proposed SWNoC  architectures, we consider two network parameters: throughput and  energy dissipation. Throughput is defined as the average number of  flits successfully received per embedded core per clock cycle. The  throughput, t is calculated according to (3).   (3)  Where, µC is the total number of messages successfully routed, φ is  the size of a single message in number of flits, N is the total  number of cores in the NoC and Tsim is the simulation duration in  number of cycles.  As a measurement of energy, we use the average energy  dissipation per packet. Energy dissipation per packet is the average  energy dissipated by a single packet when routed from the source  to destination node through multiple switches, wired, and wireless  links. For the wireless links, the main contribution to energy  dissipation comes from  the WPs, which  include antennas,  transceiver circuits and other communication modules like the  TDM block and the LNA. Energy dissipation per packet, Epkt, can  be calculated according to (4) below.  (4)  In (4), ηwire,ij, ηwireless,ij and ηbuf,ij are the numbers of hops the ith  flit of the jth message makes on wireline links, wireless links and  the number of cycles it waits in the switch buffers due to  congestions respectively. Ewire, Ewireless, Ebuf are the energy  dissipated by a flit traversing a single hop on the wired link  including the switch, wireless link including the switch and WP  and while waiting in the buffers in case of a congestion  respectively.  4.2 Throughput of SWNoC with Wireless  Link Failures  In this subsection we evaluate the performance of the proposed  SWNoC architecture in terms of throughput with special emphasis  on the behavior in presence of high rates of failure of the wireless  links. First, we consider a uniform random spatial distribution of  traffic.  Figure 3 shows the variation in throughput with injection load for  the SWNoC for two different system sizes of 64 and 256 cores.  The throughput is demonstrated for four situations, namely with no  wireless link failure and with 25%, 50% and 75% of the links  failing. For comparison we present  the  throughput of a  conventional wireline mesh and the SWNoC architecture with all  wireline links (SWNoC-wireline).  From Figure 3 we find that the throughput of the SWNoC is much  higher compared to that of a simple wireline mesh of the same  size. This is because of two reasons: the SWNoC, being connected  following the principles of a small-world network has a much  lower number of hops separating the cores along the shortest paths  and the wireless links connecting the cores separated by long  (a)  (b)  Figure 3.  Throughput characteristic of SWNoC for (a) 64 core and (b) 256 core systems                            Figure 4.  Saturation throughput of mesh, WiNoC and SWNoC for (a) 64 core and (b) 256 core systems in presence of faults  (a)  (b)  physical distances through single-hop low-latency links. The more  interesting observation is that though we inject a high number of  wireless link failures by randomly disabling a certain percentage of  the wireless links, the throughput does not degrade considerably,  especially for the larger system size. This is because the network  connectivity is established following a small-world topology. It has  been observed in [6] that small-world networks display amazing  resilience to high rates of link failures. It has been shown that the  average distance between nodes in small-world network increases  very marginally with even extreme high rates of faults. Hence, the  effect on the connectivity of the network is minimal. This is why  the throughput does not degrade appreciably due to the failure of  the wireless links. The slight degradation occurs due to the  reduction in the total available wireless bandwidth for the SWNoC.  Next we compare the performance in terms of throughput at  network saturation of the SWNoC and the hierarchical WiNoC in  presence of the same degree of faults in Figure 4. The wireless link  failures do not impact the performance of the flat wireline mesh  architecture. Hence,  its performance  remains unchanged.  Following the guidelines for best performance in [13] we  considered the WiNoC to have 8 subnets each with 8 cores for a  system size of 64 and 16 subnets each with 16 cores for the system  size of 256. All the subnets were considered to have mesh-based  connectivity among the cores within the subnets. We can see that  the maximum achievable throughput of the SWNoC is better than  the WiNoC in the absence of faults. This is because the whole  SWNoC is a small-world network with low average distance  between cores whereas; the subnets of the WiNoC are regular  wireline networks, only the upper level of it has the small-world  property. It is interesting to note that the performance of the  WiNoC degrades comparatively more rapidly than that of the  SWNoC with increasing degree of failure. The percentage  reduction in throughput between the fault-free and 75% fault cases  for the SWNoC is 8.6% for the 64 core system and 6.7% for the  256 core system. For the WiNoC the reduction in throughput is  11.1% and 33.9% for the 64 and 256 core systems respectively.  This correlates to a very insignificant rise in the average distance  between cores in the SWNoC as expected. The average distance  between cores in the WiNoC however increases more with failure  of the wireless links. This is because the WiNoC does not have a  perfect small-world topology. The lower level of the hierarchy  consists of wireline subnets which are established following a  mesh topology. The degradation in performance for the WiNoC  becomes more apparent for larger system size as the larger wireline  subnets become the limiting factor in performance. The increase in  average distance between cores is less with faults for larger system  size of the SWNoC and hence the percentage degradation in  performance is even less for larger system sizes.  4.3 Energy Dissipation  In this subsection we evaluate the energy dissipation characteristics  of the SWNoC and compare that with the conventional mesh. To  determine the energy dissipation characteristics of the wireless  architecture, we first estimated the energy dissipated by the  antenna elements. As noted in [12], the directional gain of  MWCNT antennas that we propose to use is very high. The ratio of  emitted power to incident power is around -5dB along the direction  of maximum gain. Assuming an ideal line-of-sight channel over a  few millimeters,  transmitted power degrades with distance  following the inverse square law. Therefore the received power PR  can be related to the transmitted power PT as   .  (5)  In (5), GT is the transmitter antenna gain, which can be assumed to  be -5dB [12]. AR is the area of the receiving antenna and R is the  distance between  the  transmitter and receiver. The energy  dissipation of the transmitting antennas therefore depends on the  range of communication. The area of the receiving antenna can be  found by using the antenna configuration used in [12]. It uses a  MWCNT of diameter 200nm and length 7λ, where λ is the optical  wavelength. The length 7λ was chosen as it was shown to produce  the highest directional gain, GT, at the transmitter. In one of the  setups in [12], the wavelength of the laser used was 543.5nm, and  hence the length of the antenna is around 3.8µm. The area of the  receiving antenna, AT is calculated using these dimensions.   The noise floor of the LNA [23] is -101dBm. Considering the  MZM demodulators cause an additional loss of up to 3dB over the  operational bandwidth, the receiver sensitivity turns out to be 98dBm in the worst case. The length of the longest possible  wireless  link considered among all SWNoC and WiNoC  configurations is 23mm. For this length and receiver sensitivity, a  transmitted power of 1.3mW is required. Considering the energy  dissipation at the transmitting and receiving antennas, and the  components of the transmitter and receiver circuitry such as the  MZM, TDM block and the LNA, the energy dissipation of the  longest wireless link on the chip is 0.33 pJ/bit.   As mentioned earlier the values of energy dissipation on the wired  links are obtained through HSPICE simulations taking into account  the specific lengths of each link based on the established topology.  The energy dissipation on the NoC switches are obtained by  feeding a large set of data patterns into the gate-level netlists and                    by running SynopsysTM Prime Power. The energy dissipations of  the silicon-photonic components like the MZM modulators and  demodulators are obtained from [22].  In Figure 5 we show the packet energy dissipation of the proposed  SWNoC architecture in presence of various rates of wireless link  failure for the two different system sizes considered in this work.  For the sake of comparison we present the packet energy  dissipation of the flat wireline mesh, a small-world NoC where all  the links are wireline (SWNoC-wireline) as well the SWNoC with  the long wires replaced by the wireless links in presence of various  rates of failures. It can be seen that the SWNoC has significantly  lower energy dissipation per packet compared to that of the mesh.  This is because of better connectivity of the small-world based  architecture compared to the regular mesh topology. In addition,  the low power long range wireless links also contribute to a huge  savings in energy dissipation signified by the difference in packet  energy between the completely wireline SWNoC and the SWNoC  with the wireless links. Due to the characteristics of the smallworld topology of the SWNoC the energy dissipation increases  slightly even in the presence of large number of wireless link  failures. This demonstrates the resilience of the SWNoC towards  random  failures of  the wireless  interconnections without  dissipating significant additional energy.  4.4 Performance of SWNoC in the Presence of  Non-Uniform Traffic  So far we have considered only uniform random spatial  distribution of traffic on the NoC to characterize its performance.  In reality however, there could be various different types of traffic  distributions on the chip depending on the applications considered.  In this section we present the performance of the SWNoC in  presence of several non-uniform traffic patterns. We consider both  synthetic as well as application specific traffic scenarios on a  SWNoC with 64 cores in this part. We considered two types of  synthetic traffic to evaluate the performance of the proposed  SWNoC architecture. First, a transpose traffic pattern [2] was  considered where a certain number of cores were considered to  communicate more frequently with each other. We considered 3  such pairs and targeted 50% of the traffic originating from those  Figure 5.  Packet Energy dissipation of the SWNoC  architecture with different rates of wireless link failure  Figure 6.  Saturation throughput of mesh and SWNoC  for a 64 core system with different traffic patterns and  wireless link failure rates  cores to the other core of the pair. The other synthetic traffic  pattern considered was the hotspot [2], where all the cores in the  SWNoC communicate with a particular core more frequently than  with the others. We have assumed that all cores send 20% of the  traffic originating in them to a particular core. To model  application based traffic, a 256-point fast Fourier transform (FFT)  application was considered, wherein each core performs a 4-point  radix-2 FFT computation.  Figure 6 shows the maximum throughput at network saturation of  the SWNoC in presence of the same rates of wireless link with  various types of traffic. We see that for non-uniform traffic  patterns there is relatively higher degradation in performance with  increasing rates of failure than that in the case of uniform traffic.  This is because, due to the adopted shortest-path routing strategy  where packets are routed following only the shortest path from  source to destination, failure of wireless links cause extra load on a  particular alternative link, leading to congestion. However, it  should be noted that the performance degradation with faults is still  not significant.  4.5 Area and Wiring Overhead  Here we compare the area and wiring overhead of the SWNoC  compared to a wireline mesh in order to deliver the high gains in  performance. For the sake of comparison we also present the  overheads required by the hierarchical wireless NoC, WiNoC. The  silicon area overhead in the SWNoC arises due to the transceivers  and the CNT antennas. However, the CNT antennas have very  small diameters and hence, have negligible area overheads. Figure  7(a) quantifies total area overheads of the SWNoC compared to a  flat wireline mesh for a 64 core system. It may be noted that  WiNoC has much higher overheads as the hubs in each subnet of  the WiNoC consume a lot of area as they have a large number of  ports to all the cores in the subnet. Figure 7(b) shows the additional  wiring requirements of the SWNoC with 64 cores. Due to the  power-law distribution of the interconnects there are a few long  wires in the SWNoC architecture compared to the mesh however,  most of the interconnects are very short in length. Moreover, the  total number of wires is the same as the average number of links  attached to a switch was assumed to be the same as that of a mesh  for the 64 core system. The WiNoC has more wires because each  core in the subnets has direct connections to the hubs.  Figure 7 shows that due to the small-world network architecture  adopted in the SWNoC, it achieves considerable improvements in  performance even with very minimal silicon real estate and wiring      (a)  (b)  Figure 7.  (a) Area and (b) wiring requirements of 64 core wireline mesh, WiNoC and SWNoC  overheads in excess of that already required by a flat wireline  mesh. This indicates that the small-world network is a very  efficient network topology with very small average distance  between cores.  5. CONCLUSION  A natural complex network inspired small-world architecture  where the long-range links are implemented through single-hop  wireless channels improves the performance and energy efficiency  of multi-core chips. Due to the inherent robustness of the small  world based connectivity the proposed wireless NoC architecture  outperforms its wireline counterparts even in presence of very high  rates of wireless link failures without a significant penalty in  energy dissipation. The principal contribution of this work is the  demonstration of how architectural innovations can make NoC  architectures robust and energy efficient even when designed with  an emerging technology like on-chip wireless communication.  6. ACKNOWLEDGEMENT  This work was supported in part by the US National Science  Foundation (NSF) CAREER grant CCF-0845504.  7. "
Link pipelining strategies for an application-specific asynchronous NoC.,"Wire latency across the links of a NoC can limit throughput, especially in deep submicron technology. Stateful pipeline buffers added to long links allow a higher clock rate, but this wastes resources on links needing only low bandwidth. In asynchronous (clockless) NoCs, link pipelining can be done to only those that will benefit from both increased throughput and buffering capacity, and is especially useful in heterogeneous embedded SoCs. We evaluate two strategies that determine where link pipeline buffers should be placed in the topology. The first compares available link bandwidth, based on physical wirelength, to the throughput needed by each source-to-destination path, for each link. The second adds buffers to a link such that its bandwidth is at least equal to the throughput of a core's network adapter. These strategies were integrated into our network optimization tool for an application-specific SoC. Simulations were based on its expected traffic patterns, floorplan-derived wirelength, and uses self-similar traffic generation for more realistic behavior. Results show improved large-message network latency and output buffer delay of the network adapter. There was a slight power increase with the addition of pipeline buffers, but our proposal is a complexity-effective improvement by the power*latency product metric. The results indicate the strategy of pipelining certain links provides more efficiency opposed to a ubiquitous addition of buffers.","Link Pipelining Strategies for an Application-Speciﬁc Asynchronous NoC Daniel Gebhardt University of Utah School of Computing gebhardt@cs.utah.edu Junbok You University of Utah Dept. of Electrical and Computer Engineering jyou@ece.utah.edu Kenneth S. Stevens University of Utah Dept. of Electrical and Computer Engineering kstevens@ece.utah.edu link pipelining for an asynchronous (clockless) NoC that has already been optimized for link-speciﬁc bandwidth requirements. Our work focuses on a class of system-on-chips (SoC) termed application-speciﬁc in which the NoC is designed and optimized for performance and energy qualities with knowledge of the speciﬁc tasks done by the SoC. These ﬁxedfunction SoCs can use many specialized cores and are less ﬂexible than general-purpose chip-multiprocessors or even platform-SoCs. However, knowledge of the traﬃc, such as bandwidth between particular cores, provides an opportunity for optimization. Many globally-asynchronous locally-synchronous (GALS) interconnect solutions rely on a clock, either with standard synchronous clock distribution, or a mesochronous method. However, an asynchronous network has a number of potential advantages over a clocked network in a GALS environment. Standard arguments for asynchronous (async) circuit design include robustness to process/voltage/temperature variation, and average-case instead of worst-case performance. However, there are also NoC-speciﬁc considerations. In a synchronous NoC, the clock tree for all routers and pipeline buﬀers can consume signiﬁcant power as shown in a heterogeneous network [1], and in a large CMP (chip multiprocessor) 33% of router power [2]. Many SoC designs have ABSTRACT Wire latency across the links of a NoC can limit throughput, especially in deep submicron technology. Stateful pipeline buﬀers added to long links allow a higher clock rate, but this wastes resources on links needing only low bandwidth. In asynchronous (clockless) NoCs, link pipelining can be done to only those that will beneﬁt from both increased throughput and buﬀering capacity, and is especially useful in heterogeneous embedded SoCs. We evaluate two strategies that determine where link pipeline buﬀers should be placed in the topology. The ﬁrst compares available link bandwidth, based on physical wirelength, to the throughput needed by each source-to-destination path, for each link. The second adds buﬀers to a link such that its bandwidth is at least equal to the throughput of a core’s network adapter. These strategies were integrated into our network optimization tool for an application-speciﬁc SoC. Simulations were based on its expected traﬃc patterns, ﬂoorplan-derived wirelength, and uses self-similar traﬃc generation for more realistic behavior. Results show improved large-message network latency and output buﬀer delay of the network adapter. There was a slight power increase with the addition of pipeline buﬀers, but our proposal is a complexity-eﬀective improvement by the power*latency product metric. The results indicate the strategy of pipelining certain links provides more eﬃciency opposed to a ubiquitous addition of buﬀers. Categories and Subject Descriptors B.4.3 [Input/Output and Data Communications]: Interconnections—Asynchronous/synchronous operation 1. INTRODUCTION A link of a network-on-chip (NoC) can be pipelined using latch or register-based buﬀers when its wire delay is a limiting factor in throughput. This often occurs with long links, small process technology, and relatively fast clock speeds. Another beneﬁt to link pipelining comes from the additional buﬀering space it provides, assuming a compatible link-level ﬂow-control. This paper explores the system-level eﬀects of Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS ’11 May 1-4, 2011 Pittsburgh, PA, USA Copyright 2011 ACM 978-1-4503-0720-8 ...$10.00. synchronous (GALS) design. The routers are entirely async and do not use a clock to communicate to each other, or to the network adapters. An example implementation of this is presented in [3]. sync clock domain async domain (no clock signals) IP c ore N et w ork A da pter R o uter network link packet-level communication bus-to-network protocol conversion (transactionlevel communication) synchronizer circuit Figure 1: Component diagram of an asynchronous NoC. S e n d e r Data Request Acknowledge R e c e v e i r Figure 2: Signals of an asynchronous channel handshake. An asynchronous channel, which transfers data from a sender to a receiver, is shown in Figure 2. Instead of a synchronous clock signal determining when data should be read, a handshaking protocol performs this function. Typically a request and acknow ledge signal are used to accomplish this. The sender generates the request signal to indicate new data is available. The receiver responds with the acknowledge signal, indicating data is stored. There have been a number of NoC proposals for incorporating storage and/or control logic within inter-router links. The work of iDEAL [4] showed that the performance penalty of reduced-complexity routers with few input/output buﬀers can be improved by using links containing storage elements. For a traditional synchronous NoC and mesh topology, moving storage to the links signiﬁcantly reduced network power at a very slight performance reduction. Link pipelining is described for the Xpipes network components [5]. These are placed primarily to meet clock timing requirements. Error detecting link pipeline circuits were designed to achieve greater NoC robustness while maintaining high throughput [6]. Elastic buﬀers, similar to the asynchronous buﬀers used here, were used to reduce router complexity by using the link as a distributed FIFO buﬀer [7]. Throughput per energy was improved compared to the baseline architecture. Elastic and asynchronous link pipelining was explored in [8], but with an ad hoc approach in determining where and when a buﬀer should be inserted on a link. It also did not evaluate eﬀects on large-message latency. A number of energyeﬃcient proposals, including pipelined links shared between multiple sources, is given by [9]. It uses a standard mesh topology and homogeneous SoC for evaluation and does not address the optimization problem of determining the the number of buﬀers on each link. Link pipelining for a delayinsensitive asynchronous NoC are described in [10], where multiple virtual channels can overlap packet transmissions at the ﬂit level to maintain high link utilization. The paper did not describe the conditions or depth of the pipeline, nor was a system-level evaluation of the proposal given. NoC optimization for a particular SoC is a rich topic to draw from. The COSI framework generates an applicationspeciﬁc NoC and ﬂoorplan, taking as input such constraints as core areas and average bandwidth between cores [11]. A heuristic search determines the topology and router conﬁguration [12]. It uses ﬂoorplan information, router energy models, and core communication requirements. The results indicate a signiﬁcantly reduced power and hop-count versus the best mesh topologies for a variety of SoC designs. For the QNoC routers, application-speciﬁc optimization is discussed in [13], but it focuses on mapping logical resources of a mesh-style topology rather than physical concerns. It presents a link capacity allocation method, but does not explain mechanisms for implementing such a method. A large variety of async networks have been developed, but these have not provided details of link pipelining, even when it assumed. Fulcrum Microsystems created a large asynchronous crossbar to interconnect cores of a SoC [14]. The commercial startup Silistix, based on earlier academic research, sells EDA software and circuits that provide a customized asynchronous NoC, but has no published methods for the optimization process [15]. The MANGO router [16] provides both best-eﬀort and guaranteed-service traﬃc. Faust [17] is a platform and fabricated chip used in 4G telephony development, and uses an asynchronous mesh-based NoC [18]. The QNoC group has developed an asynchronous router that provides multiple service levels and dynamically allocated virtual channels per level [19]. 2. LINK PIPELINING The goal of pipelining a link is to reduce the cycle-time, created by wire delay, by inserting a buﬀer between sender and receiver. Throughput along a link is increased, at the expense of single-ﬂit latency and a slight power increase over only wire repeaters. This organization is illustrated in Figure 3, where a buﬀer is placed between a router and network adapter. Our asynchronous pipeline buﬀer is composed of a bank of latches (rather than ﬂip ﬂops) and a handshake controller. This arrangement is called a linear control ler or link buﬀer. The use of latches saves almost half the area, and potentially power, compared to a traditional synchronous ﬂip-ﬂop design. Note, the pipeline buﬀer does not negate the need for wire repeaters (large inverters). In this work, we assume the buﬀers for the separate input and output channels are located in the same proximity, but this is not required. From this point forward, when we use the term buﬀer in the context of describing the network conﬁguration we refer to the collection of latches and controllers for both channel directions. Link Buffer latches wire repeaters data Router req. ack. controller L.B. input channel output channel N e t w o r k A d a p t e r Figure 3: Organization of NoC link components showing detail for a router’s output channel, and the equivalent link buﬀer (L.B.) for its input channel.   The value of k is a user-parameter, and is explored in Section 4. In other words, this is an insertion threshold based on the percentage of link utilization. For the application-speciﬁc SoCs we consider in this work, a common representation of core-to-core traﬃc requirements is a core communication graph (CCG) as shown in Figure 4. The expected average throughput required between cores is shown with an annotated edge. For this particular CCG, traﬃc is assumed to be equal in both directions, but a CCG is often shown with directed edges. Given a topology and CCG, each link in the topology is attributed a required throughput. Available link bandwidth (ABW) is based on link wirelength, as shorter links have faster cycle-time. VU 64 1 AU 20 14 SDRAM 3 304 DSP 11 SRAM 1 UP SAMP MED CPU 200 40 84 SRAM 2 224 58 167 RAST IDCT BAB RISC Figure 4: MPEG4 decoder core communication graph, with edges in MBytes/second. The second strategy adds pipeline buﬀers to links with an available bandwidth (ABW) less than the throughput of the network adapters. We call these core-throughput matching buﬀers (CTMBs). For example, if a network adapter had a maximum throughput of 2 Gﬂits/s (yielding 64 Gbits/s with 32-bit ﬂits), and a long link had a handshake delay of 700 ps (yielding 46 Gbits/s), the link would need one CTMB. This is analogous to wire pipelining for clocked networks when a link fails to meet the timing requirement derived from the clock period. For async systems, however, this is optional; links can be slower than the sending or receiving component Our asynchronous router is designed for eﬃciency and simplicity, similar to other recent work [20]. Each switch directs a ﬂit to one of two output ports. With bi-directional channels, this results in a three-ported router. Possible topologies include a ring, tree, or an irregular arrangement, but this work considers trees, as they have the minimal number of routers. The packet format consists of a single ﬂit containing source-routing bits in parallel, on separate wires, with the data bits. We assume a transaction layer protocol implemented in the network adapter will use the packet’s data ﬁeld to transmit control information, such as addresses and command-type. This is not fundamentally diﬀerent to packet formats that serialize a ﬁxed packet size into multiple ﬂits. The packet is switched through a demultiplexer controlled by the most-signiﬁcant routing bit. The bits are simply rotated for the output packet. The number of required routing bits is determined by the maximum hop count of a network, and this study required eight and twelve bits for the two benchmark designs. This format has the overhead of requiring routing bits with every ﬂit, but does not require an extra header-ﬂit carrying routing information common to other packet formats. Its energy overhead is further reduced in that series of packets following the same source-todestination path do not cause switching in the routing bit wires. This design uses an Artisan cell library on the IBM 65 nm 10sf process. We used post-layout back-annotation to evaluate the router/link buﬀer’s latency, HSPICE simulation for energy measurement, and leakage power from SOC Encounter. Latency and energy results are shown in Table 1, and assume 25% of bits switch each ﬂit. Forward latency is the time between when an input channel receives a request signal to the time the router/buﬀer asserts the outgoing channel’s request signal. Throughput is the maximum cycle-time of the component, assuming zero wire delay. Table 1: Circuit-level properties of router and link buﬀer. Energy/ﬂit Leakage Latency Throughput (pJ) ( µW) (ps) Gﬂits/sec 1.03 9.76 460 2.1 0.45 1.21 130 4.1 Router Link Buf. Insertion Strategies We propose two strategies to determine under what conditions a link pipeline buﬀer should be inserted. The ﬁrst, path-speciﬁc buﬀer insertion, pipelines links that require a throughput greater than a fraction, k, of the link’s available bandwidth (ABW). The intuition behind this is that hightraﬃc links will beneﬁt from additional buﬀering to reduce contention in the preceding routers, and also to decrease latency from a receiving router’s ack signal (indicating the next ﬂit may be sent) to a sending buﬀer req signal (indicating the next ﬂit is ready). This strategy is termed path-speciﬁc because the required throughput (Treq ) is derived from the source-to-destination paths that utilize the link; it is the sum of average throughput of each of these paths (Tp ). The number of buﬀers, ℵ, to insert on link ℓ is ℵℓ = (cid:22) (ABWℓ × k) (cid:23) where Treq = Xpath p using ℓ Treq Tp The network adapter in the MPEG4 design had a throughput of 2 Gﬂits/s with 32 bits/ﬂit, and for the TI-SoC, it had a throughput of 4 Gﬂits/s with 64 bits/ﬂit. Our network optimization tool, ANetGen, was conﬁgured to produce a topology and placement of the routers and pipeline buﬀers for various values of the path-speciﬁc k parameter. This was done for two sets of link pipeline conﬁgurations; with path-speciﬁc buﬀers only, and with both pathspeciﬁc and CTMBs. The tool already optimizes link bandwidth on high-traﬃc paths by reducing their wirelength. risccpu au pb1 upsamp pb0 vu pb3 9 3 pb2 babcalc sdram 2 1 6 7 0 5 pb5 rast mediacpu pb7 pb8 idct pb4 dsp pb9 4 8 pb6 sram2 sram1 Figure 7: SoC ﬂoorplan showing cores, routers, buﬀers, and topology, cropped and edited for legibility. term is the sum of dynamic and leakage power of the routers, wires, and wire pipeline buﬀers. The delay term is the mean packet latency through a network adapter’s output buﬀer, added to the mean message (256 bytes) latency through the network. Delays were normalized to give equal weight to network and output buﬀer latencies. Figure 8 shows this metric on the Y-axis, with the X-axis showing the total number of buﬀers inserted on all links, including both CTMBs and path-speciﬁc. Data is given in two series, with and without additional core-throughput matching buﬀers. For the MPEG4 benchmark, the addition of one pathspeciﬁc buﬀer greatly lowers (improves) PLP. A slight additional improvement is seen with the addition of a few more path-speciﬁc buﬀers, but PLP gets worse after this. This trend is similar when CTMBs are also used, indicated by the darker line data. This chart is also useful in comparing the beneﬁt of using the CTMBs. With just CTMBs and no path-speciﬁc buﬀers, the PLP is improved, but only very slightly from the initial solution. The best solution shown in the data is with three path-speciﬁc buﬀers as well as CTMBs (10 total network buﬀers). PLP worsens with more path-speciﬁc buﬀers (14 total buﬀers), but is better than if CTMBs were not used (7 total buﬀers). For the TI-SoC design the results are quite diﬀerent. This stems from the much larger number of IP blocks and paths, as well as a more uniform distribution of the communicating blocks. Generally, the addition of buﬀers helps greatly, but CTMBs are a more eﬃcient strategy from the standpoint of mean latency over all paths. The addition of a low number of path-spec buﬀers only (totals of 10–20) reduces PLP a modest 20%. If the power budget can allow 30–50 total buﬀers, it is better to add CTMBs, which show a large reduction of over 50% in PLP. Power consumption of various network conﬁgurations is shown in Table 2. For these experiments the topology and placement are left constant, with only the number of buﬀers varying. Therefore, the power of the routers and wires is constant because the same traﬃc is sent in each trial and the total wirelength is the same. The dynamic and leakage power of routers and wires, common to all conﬁgurations, is shown at the top of the table. Conﬁgurations are organized in sets, separated by benchmark (MPEG4 and TISoC) and by buﬀer insertion type (only path-speciﬁc buﬀers and both path-speciﬁc and CTM buﬀers). A lower value of k-threshold represents an increased number of path-speciﬁc Figure 8: Power-latency product for various numbers of buﬀers inserted based on link utilization percentage. Results shown with and without core-throughput matching buﬀers. buﬀers, with a value of 1 having none. For each of these, the buﬀer power (sum of dynamic and leakage) is shown, along with the total network power. For clariﬁcation, the wire power is due to the drivers/repeaters (large inverters) needed along a link’s length. The much greater power consumption of the TI-SoC is due to it sending far more aggregate traﬃc. The total power consumption rises slightly in both benchmarks with the addition of more link buﬀers, as expected. Link buﬀer power is a small portion of the total, less than 10% in most conﬁgurations. Table 2: Power (mW) of various buﬀer conﬁgurations. MPEG4 TI-SoC Power values common to all conﬁgs Rtrs dyn Rtrs leak Wire dyn Wire leak 1.34 0.29 3.28 0.84 47.8 1.63 34.6 16.70 path-speciﬁc k threshold 0.5 0.3 0.2 1 MPEG4 Without CTMBs Link Buﬀers Total 0 5.76 0.06 5.82 1 With CTMBs Link Buﬀers Total TI-SoC Without CTMBs Link Buﬀers Total 0 100.7 With CTMBs Link Buﬀers Total 3.37 104.1 0.12 5.88 0.18 5.94 0.1 2.61 103.3 5.27 106.0 0.36 6.13 0.42 6.18 0.5 9.01 109.7 9.78 110.5 0.85 6.61 0.90 6.66 0.3 20.72 121.5 21.67 122.4 Mean latencies are shown in Table 3 for packets through the network adapter’s output buﬀer for messages through the network. Both of these metrics generally improve when do see an improvement, but it does not greatly impact mean network latency because they carry less traﬃc. Maximum latency improvements were mixed, with some drastically worse paths, and many slightly improved ones. The paths showing worse maximum latency are the topologically longest, and thus have the highest probability for contention and delay. The addition of CTMBs increases the rate messages can enter the network, but not necessarily Table 3: Mean latency (ns) of buﬀer conﬁgurations. Measurement Location MPEG4 Without CTMBs Core’s Output Buﬀer Network With CTMBs Core’s Output Buﬀer Network TI-SoC Without CTMBs Core’s Output Buﬀer Network With CTMBs Core’s Output Buﬀer Network path-speciﬁc k threshold 1 0.5 0.3 0.2 15281 55.74 15332 54.92 1 11.8 21.9 10.1 19.1 12257 54.58 12400 53.78 0.1 9.7 19.4 8.6 17.9 11430 52.92 11248 51.72 0.5 8.4 17.8 8.2 17.3 11644 53.58 10693 51.59 0.3 6.5 16.2 6.2 15.7 more path-speciﬁc buﬀers are used. The large output buﬀer latency in the MPEG4 benchmark is an eﬀect of lack of backpressure to the traﬃc generator; it is a useful relative comparison, but does not reﬂect latencies expected in an actual system. The network message latency is more representative of actual system behavior. Message Latency per Path Besides overall mean message latency, another picture of performance is seen by looking at the message latencies for each source-to-destination path in the SoC. For example, the MPEG4 path from source core sram2 to destination core risccpu has a median latency of 65 ns with no link buﬀering, as seen in Figure 9 with the path-ID 21. These measurements are from the time a 256-byte message has begun its exit of a core until it completely enters a receiving core. This is a measure of available bandwidth on a path, while considering dynamic eﬀects of contention with other paths. The addition of link buﬀering improves median latency signiﬁcantly on some paths, notably 20,21,22 which carry high traﬃc from sram2. Latency rises slightly with the addition of more buﬀers. Buﬀers were added to the link connecting the sram2 core to a router, which explains the beneﬁt on those paths. Other paths do not beneﬁt from these added buﬀers. The eﬀects on maximum message latency is not conclusive, although a few paths seem to beneﬁt slightly, such as 0 (au→sdram ) and 18 (sram1→rast ). This is from reduced contention, a side-eﬀect of the improved connection to sram2. The eﬀects on per-path message latency by adding CTMBs is shown in Figure 10, for the MPEG4 benchmark. All values on the chart are normalized to the conﬁguration with only path-speciﬁc buﬀers (no CTMBs) inserted with the same k-threshold. The data series represent various k-thresholds and thus diﬀerent numbers of path-speciﬁc buﬀers. Paths that showed little diﬀerence with the addition of CTMBs were removed from the ﬁgure. Median latency is improved on many paths, and is an indication of increased throughput when the network is uncongested. Note that the paths improved with CTMBs are diﬀerent than those improved with path-speciﬁc buﬀers; paths 20,21,22 did not show much change. Also interesting is that even though many paths have a median latency reduction, the mean latency considering all paths (shown in Table 3) was not improved much with the addition of CTMBs. This is due to the fact that the paths carrying the greatest traﬃc already have buﬀers assigned to them as the path-speciﬁc type. The other paths Figure 9: Median and maximum message latencies for the MPEG4 network for each source-to-destination path. Data series represent diﬀerent numbers of path-speciﬁc buﬀers (more buﬀers for a lower k-threshold) and no CTMBs. requirements. A SoC design may be more sensitive to one insertion strategy or the other. Generally, a design with a low number of critical, high throughput paths will see the most gain from path-speciﬁc buﬀers, while a design with a large number of lower throughput paths will beneﬁt most from CTMBs. The given strategies oﬀer the NoC engineer another knob to turn for setting the power–performance point. Acknowledgments This research is supported by the U.S. National Science Foundation under grant CCF-0810408, CCF-0702539, and Semiconductor Research Corporation under task 1817.001. We would like to thank ARM and IBM for providing the 65 nm library cells and process technology. 6. "
A low-latency adaptive asynchronous interconnection network using bi-modal router nodes.,"A new bi-modal asynchronous arbitration node is introduced for use as a building block in an adaptive asynchronous interconnection network. The target network topology is a variant Mesh-of-Trees (MoT), combining a binary fan-out network (i.e. routing) and a binary fan-in network (i.e. arbitration) for each source-sink pair. The key feature of the new arbitration node is that it dynamically reconfigures based on the traffic it receives, entering a special ""single-channel-bias"" mode when the other channel has no recent activity. Arbitration is totally bypassed on the critical path, resulting in significantly lower node latency and, in high-traffic scenarios, improved throughput.
The router nodes were implemented in IBM 90nm technology using ARM standard cells. SPICE simulations indicate that the bi-modal arbitration node provided significant reductions in latency (41.6%), and increased throughput (19.8%, in high-traffic single-channel scenarios), when in biased mode. Node reconfiguration required at most 338 ps. Simulations were then performed on two distinct MoT indirect networks, ""baseline"" and ""adaptive"" (the latter incorporating the new bi-modal node), on eight diverse synthetic benchmarks, using mixes of random and deterministic traffic. Improvements in system latency up to 19.8% and throughput up to 27.8% were obtained using the adaptive network. Overall end-to-end latencies, through 6 router nodes and 5 hops, of 1.8-2.8 ns (at 25% load) and throughputs of 0.27-1.8 Gigaflits/s (at saturation rate) were also observed.","A Low-Latency Adaptive Asynchronous Interconnection Network Using Bi-Modal Router Nodes Gennette Gill, Sumedh S. Attarde, Geoffray Lacourba and Steven M. Nowick Department of Computer Science, Columbia University ggill@cs.columbia.edu, ssa2141@columbia.edu, gml2144@columbia.edu, nowick@cs.columbia.edu Abstract A new bi-modal asynchronous arbitration node is introduced for use as a building block in an adaptive asynchronous interconnection network. The target network topology is a variant Mesh-of-Trees (MoT), combining a binary fan-out network (i.e. routing) and a binary fan-in network (i.e. arbitration) for each source-sink pair. The key feature of the new arbitration node is that it dynamically reconﬁgures based on the trafﬁc it receives, entering a special “single-channelbias” mode when the other channel has no recent activity. Arbitration is totally bypassed on the critical path, resulting in signiﬁcantly lower node latency and, in high-trafﬁc scenarios, improved throughput. The router nodes were implemented in IBM 90nm technology using ARM standard cells. SPICE simulations indicate that the bi-modal arbitration node provided signiﬁcant reductions in latency (41.6%), and increased throughput (19.8%, in high-trafﬁc single-channel scenarios), when in biased mode. Node reconﬁguration required at most 338 ps. Simulations were then performed on two distinct MoT indirect networks, ""baseline"" and ""adaptive"" (the latter incorporating the new bi-modal node), on eight diverse synthetic benchmarks, using mixes of random and deterministic trafﬁc. Improvements in system latency up to 19.8% and throughput up to 27.8% were obtained using the adaptive network. Overall end-to-end latencies, through 6 router nodes and 5 hops, of 1.8-2.8 ns (at 25% load) and throughputs of 0.27-1.8 Gigaﬂits/s (at saturation rate) were also observed. Categories and Subject Descriptors B.4.3 [Hardware]: I/O and Data Communications—Interconnections (Subsystems)—Asynchronous operation 1. INTRODUCTION Developments in networks-on-chip (NOCs) in the last decade have demonstrated great promise in handling several of the key challenges facing digital system designers in the deep submicron era [5, 7], including design time, scalability, reliability and ease-of-integration. However, recent studies still predict signiﬁcant challenges, and shortcomings, under extrapolated technology trends, in terms of system latency, throughput and power [19, 18]. One promising direction that has been explored is to eliminate the global clock, either using entirely asynchronous systems [23, 10], or integrating synchronous cores, nodes and memories through asynchronous communication in a a globally-asynchronous locallysynchronous (GALS) system [6, 4, 12, 20, 22]. Such systems offer This work was supported in part by NSF award CCF-0811504. The system simulation infrastructure used for this research was supported by resources of Prof. Simha Sethumadhavan’s Computer Architecture and Security Technology Lab (CASTL) at Columbia University, which is partially funded through grants FA 99500910389, FA87501020253, an NSF CAREER award and gifts from Microsoft Research, WindRiver Corp, Xilinx and Synopsys Inc. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS ’11 May 1-4, 2011 Pittsburgh, PA, USA Copyright 2011 ACM 978-1-4503-0720-8 ...$10.00. the potential of more ﬂexible integration of heterogeneous components, such as heterochronous systems (i.e., with arbitrary unrelated clock domains) [12, 20, 8, 1]; lower system latency [12, 20]; reduced overall power consumption [10]; and higher performance [23]. The focus of the current work is on designing a ﬂexible, highperformance and fully-asynchronous network, suitable for sharedmemory chip multiprocessors (CMP’s). Our work builds as a starting point on a recent approach introduced by Horak et al. [12], which developed two high-performance NOCs: one fully-asynchronous and the other GALS-style. Each used new lightweight asynchronous router node designs. These NOCs showed signiﬁcant beneﬁts under metrics of system latency, area and power. In particular the fully-asynchronous network, in 90nm technology, provided much lower system latency (by 1.7x) than a comparable synchronous network operating at 800MHz, with identical throughput over the latter’s entire operating range. The mixed-timing network, which can integrate arbitrary unrelated clock domains, when operating at 800MHz, provided lower system latency up to 29% of synchronous maximum input trafﬁc rate, and comparable throughput up to 52% of this maximum rate. This work observed several constraints, in terms of network topology (a variant Mesh-of-Trees [MoT]), router design (low-radix), communication style (an indirect network, suitable for processor-tomemory interconnection) and routing policy (deterministic). The MoT style includes two classes of nodes: binary fan-out (i.e. routing) and binary fan-in (i.e. arbitration). The goals of our proposed work are two-fold. First, using this asynchronous NOC as a starting point, we introduce a novel design approach to ﬁne-grain dynamic adaptivity. The main target is further reduce system latency, but throughput is often also increased. In particular, router nodes observe their local recent trafﬁc, and rapidly enter — or leave — a special “biased” mode which allows an uncontended cut-through path. The optimization is targeted to one of the two classes of routers: arbitration nodes. Effectively, arbitration is bypassed for transient (or persistent) periods where only one input channel is active. The key contribution is the design of a bi-modal arbitration node, with very low overhead, that can safely support this capability. A number of related approaches for dynamic adaptivity have recently been explored, in both synchronous and asynchronous domains, and several have been highly effective. These include asynchronous dynamic leakage management [24, 13] and synchronization reduction in multi-synchronous NOCs [14]. Other synchronous approaches, such as express virtual channels [15], similarly aim at lowering latency (by skipping nodes). The proposed method is complementary to these approaches, and can be used in tandem with some of them. What distinguishes it is the ﬁne granularity of its adaptivity. For the new bi-modal arbitration nodes, in IBM 90nm technology, using ARM standard cells, node reconﬁguration required at most 338 ps, and biased node latency was only 214 ps. Second, this new arbitration node was incorporated into an asynchronous network and extensive simulations were performed on eight diverse synthetic benchmarks, using mixes of random and deterministic trafﬁc. Two distinct MoT networks were designed: “baseline” (a re-implementation of the original network from [12]) and “adaptive” (incorporating the new bi-modal arbtitration node). All cells were technology-mapped, and inter-cell wire delays extrapolated from a recent synchronous MoT layout. Signiﬁcant improvements in system latency and throughput were observed over the baseline network. In addition, the absolute results for performance were also encouraging: end-to-end latencies, through 6 router nodes and 5 hops, of 1.8-2.8 2. BACKGROUND 2.1 Mesh-of-trees network Figure 1. Mesh-of-trees network (N=4) ns (at 25% load) and throughputs of 0.27-1.8 Gigaﬂits/s (at saturation rate). We expect the new approach can be migrated in the future to other network topologies. It also can be dropped into a GALS network, following the approach outlined in [12]. Our target network topology is a variant mesh-of-trees (MoT) [2], which is designed to provide the needed bandwidth for a highperformance parallel processor with globally uniform memory access. It has been proven effective in recent detailed evaluations on a range of trafﬁc for chip multiprocessors (CMPs). In particular, for the CMP platform introduced by Vishkin et al. [16], the mesh serves as a highspeed parallel interconnect between cores and partitioned shared L1 data cache. Each of the 8 cores, or processing clusters, itself contains 16 separate processing units, or thread control units (TCUs), for a total of 128 TCUs. Recent extensions have been proposed to reduce area overhead through a hybrid MoT/butterﬂy topology, which maintains the throughput and latency beneﬁts of MoT with the area advantages of butterﬂy [3]. The MoT network consists of two main structures: a set of fan-out trees and a set of fan-in trees. Figure 1(b) shows the fan-out trees, in which each source is the root of a binary tree. The 16 leaf nodes represent connections to the leaves of the binary fan-in trees, which have destinations as their roots (Figure 1(c)). An MoT network that connects N sources and M destinations has logM levels of fan-out and logN levels of fan-in trees. Network routing is deterministic and packets are source routed. During operation, a ﬂit travels from the source root to one of the leaves of the corresponding fan-out tree. It passes to the leaf of a corresponding fan-in tree, and then travels to the root of that fan-in tree to reach its destination (Figure 1(d)). To minimize contention, fan-out trees eliminate competition between packets from different sources, and fan-in trees eliminate competition between packets to different destinations. This separation guarantees that, unless the memory access trafﬁc is extremely unbalanced, packets between different sources and destinations will not interfere. Therefore, the MoT network provides high average throughput that is very close to its peak throughput. There are two switching primitives in a MoT network: (a) routing and (b) arbitration. 2.2 Asynchronous Primitives This section describes the two asynchronous components that were used to create the original asynchronous mesh-of-trees network by Horak et al. [12]: routing and arbitration network primitives. A basic overview is provided below, and further details can be found in [12, 11, 17]. These components are based on an existing linear asynchronous pipeline called Mousetrap [21]. In particular, they use single-rail bundled data encoding, in which a synchronous-style data channel is augmented with an extra req wire, and a single transition on the req accompanying the data “bundle” indicates the data is valid. 2.2.1 Routing primitive The routing primitive, shown in Figure 2, receives one incoming stream and conditionally passes it to one of two outgoing streams. Figure 2(a) shows its structure. When the stage is empty the data regFigure 2. Original routing node [12]: (a) top-level, (b) latch control Figure 3. Original arbitration node [12] ister is opaque. Following the bundled-data encoding, the basic operation begins with new data arriving, including the routing signal B. After the data inputs are stable and valid, a request transition on Req occurs at the input. Then, the latch controller selected by the routing signal, B, enables its latches (i.e. makes them transparent), thereby allowing the data to ﬂow to the successor stage. To complete the asynchronous handshaking, three operations are preformed in parallel: 1) a request transition is sent to the successor stage that is selected by the routing signal B; 2) the data latches are made opaque, to protect the recently-received data; 3) an acknowledgement transition is sent to the predecessor stage. This completes one full cycle of operation for the routing primitive. Because it has separate latch control modules and data registers for each output, this node is able to decouple processing between the two output routing channels. Even if one of the output channels is stalled awaiting acknowledgment, the other output channel can successively process multiple full transactions. This concurrency feature provides the capability of a limited virtual input channel, thereby providing signiﬁcant system-level performance beneﬁts. 2.2.2 Arbitration primitive The arbitration primitive mediates between two incoming streams of ﬂits–enforcing mutual exclusion–and merges the result into a single outgoing stream. Thus, it provides complimentary functionality to the routing primitive. Figure 3 shows the design of the basic arbitration primitive. When the arbitration primitive is empty, the control latches L1 and L2 are opaque while the all other latches, including the data register, are transparent. Following the bundled-data encoding, operation begins when data appears at the input of an empty primitive followed by a request transition from the previous stage. To resolve contention between potentially concurrent incoming requests, the incoming request Figure 4. New bi-modal arbitration node must ﬁrst trigger a lock on the mutex. Next, two operations take place channel 0) are processed by the node, change to biased (e.g. biasconcurrently: 1) the multiplexer select signal (mux_select) chooses to-0); and (ii) in biased mode, if one ﬂit arrives on the opposite input the correct data input; and 2) either L1 or L2 is enabled, thereby forchannel, revert to default. warding the winning request. The latch controller will then concurOne subtlety of the new design is that a safe time window must rently protect the new data by making the data latches opaque, genbe selected for one of the two mode changes: from biased to default. erate a request to the next stage, and acknowledge to the ﬂow control Since there is no reference clock, new ﬂits may arrive at arbitrary unit that data has been safely stored. To complete the cycle, the ﬂow times, hence care must be taken that reconﬁguration is applied in a control unit will reset the mutex and then send an acknowledge to the suitable interval. A lightweight asynchronous monitoring network is previous stage. At the end of the cycle, as a power optimization, the added for this purpose; its design and use will be discussed below. SR latch maintains the value of the mux select, thereby preventing unnecessary switching on the datapath. 3.2 Node Microarchitecture 3. A NEW ASYNCHRONOUS BI-MODAL ARBITRATION NODE 3.1 Overview of the Approach The new bi-modal arbitration node builds on the earlier nonreconﬁgurable design by Horak et al. [12] by adding dynamic-modechange capability. The node can be in either in normal mode or a “single-channel-bias” mode. Since there are two input channels, channel 0 and channel 1, it therefore has three distinct modes or “states”: default, bias-to-0 and bias-to-1. The node enters bias-to-0 mode when recent trafﬁc has been observed only on input channel 0, and input channel 1 is inactive (similarly, for entering bias-to-1 mode). In this case, it effectively operates as a fast-forward “single input channel” node: its arbitration unit is entirely bypassed (operating vestigially in parallel, but is not observed) and some of the latches are forced transparent, resulting in lower node latency. In addition, when single-input channel trafﬁc is at moderate or high rate, higher node throughput is also obtained. The node exits biased mode, reverting to default mode, when any ﬂit arrives on the inactive input channel (e.g. input channel 1). Note that, once the node is in biased mode (e.g. bias-to-0), it always passes through default mode before changing to the other biased mode (e.g. bias-to-1). Hence, all state changes are between default and biased mode. In summary, a node’s mode changes are determined entirely locally — at the node itself — based solely on its recent observed trafﬁc history. Hence, adjacent nodes may be in entirely different states, at any time. The current design uses the following mode change policy: (i) in default mode, if two successive ﬂits from one input channel (e.g. The design of the new bi-modal arbitration node is shown in Fig. 4. The external interfaces are largely identical to the earlier arbitration node of Horak et al. [12], but with additional channels for monitoring (discussed in Section 3.5). There are four major additions: (i) a central policy unit, which initiates all mode changes; (ii) two safety modules, which implement the mode changes; (iii) a reconﬁgurable req-latch control, and (iv) a reconﬁgurable ack-latch control. The policy module indicates the current mode and initiates all mode changes. It is effectively a local monitor, observing the recent history of ﬂits that have arrived on the input channels that have won arbitration. Its inputs are the mutex outputs, zerowins and onewins. Its outputs are two state bits, initbias0 and initbias1, which encode the current mode: 00 = default, 10 = bias-to-0, 01 = bias-to-1. The unit initiates all mode changes according to the policy outlined above: two successive ﬂits on one input channel (and winning arbitration) initiate a bias to that input channel; any ﬂit on the other input channel (and winning arbitration) returns the mode to default. The unit is also modular: other policies can be easily implemented by upgrading the module. It is implemented as burst-mode asynchronous controller, a widely-used Mealy-type asynchronous state machine. The speciﬁcation is shown in Fig. 5, and it is synthesized to a small hazard-free standard cell implementation (with 35 2- and 3-input gates) using the Minimalist CAD package [9]. The safety modules implement the mode changes initiated by the policy module, as shown in Fig. 6. The outputs setbias0 and setbias1 reconﬁgure the req-latch and ack-latch control for the corresponding mode changes (along with a wedgeopen0 and wedgeopen1 signals). For a mode change from default to bias-to-0, for example, the unit immediately asserts setbias0 high and reconﬁgures the node. For a mode change from bias-to-0 to default, however, its behavior is more complex, as it is used to determine a safe time window before it can assert setbias0 high. Details are provided in Sections 3.4 and 3.5. Example. A ﬂit arriving on input channel 0 results in a much shorter forward path: reqin0 toggled on the input channel, passing through latch L1 (transparent), through another XOR2 gate, through latch L5, to a toggle on Req on the output channel. Hence, the critical forward latency of the node is now only 2 latch delays + 1 XOR2 delay. This latency reduction also shortens the node’s cycle time to generating a left acknowledgment (ackout0), compared to the earlier node design, since the setting and resetting of the mutex has been eliminated from this path. 3.4 Mode Change Operation The two design goals in implementing mode change are to ensure very low-latency and also safe operation. A sketch of the basic operation is outlined below. A key challenge is in changing from biased to default modes. In this case, the req-latch (L1) is currently forced open (i.e. transparent) and will be closed. No change can be made if a new ﬂit is about to arrive on input channel 0, otherwise a malfunction may occur. Hence, a “safe window” must be identiﬁed. Default-to-Biased (bias-to-0). The policy module ﬁrst initiates the mode change (bias0 initbias1 from 00 to 10), and the rising transition on initbias0 is received by Safety 0 (see Fig. 6). The bottom input of both mutexes is then released (after inversion) and the setbias0 output is rapidly asserted high, which then reconﬁgures the req-latch and ack-latch control. Since the TypeB mutex is released, it then allows the ack-latch enable signal (acklatchen_enbiased0) to be generated as described above for bias-to-0 mode (i.e. from reqin0 and ackout0 events). This is a “no-overhead” mode change: the ﬂit arriving on input channel 0 that triggers the mode change is not delayed. Reconﬁguration occurs concurrently while the ﬂit is forwarded to the output channel. Biased(bias-to-0)-to-Default. The policy module again initiates the mode change (initbias0 signal de-asserted low). Eventually, the Safety 0 module will de-assert its setbias0 output low, thereby reconﬁguring the node to default mode. A safe time window must now be obtained before reconﬁguration can be applied (i.e. setbias0 de-asserted). In particular, no ﬂit may arrive on input channel 0 after reconﬁguration has started, since it may cause malfunction during the closing of the L1 latch. A simple solution would be to wait until no trafﬁc on input channel 0, to change mode. However, if there is continuous trafﬁc, starvation on input channel 1 would result. The ﬁnal solution is a hybrid one: (i) if no trafﬁc is coming to input channel 0, initiate the mode change; and (ii) if some trafﬁc is coming to input channel 0, wait till the next ﬂit arrives, and piggyback the initiation of the mode change once the node has completed processing it. Case (i) is called Type A, and case (ii) is called Type B. Sketch of Operation. Modules to handle each case are shown in Fig. 6. A new monitoring input (something-coming-in-0) is also provided on the input channel, indicating if a new ﬂit is arriving soon. When the mode change request arrives from the policy module (i.e. initbias0 de-asserted to 0), it is arbitrated with this monitoring signal. If the request arrives when nothing is yet coming, a “Type A” scenario is initiated. The initbias0 signal (after inversion) wins the mutex, the L1 latch rapidly becomes opaque (wedgeopen0 goes to 0), and the mode change is safely implemented (resettypeA to 1, then setbias0 to 0). If the mode change request arrives when something is coming, a “Type B” scenario is initiated. The TypeA mutex is locked (by its top input something-coming-in-0), and the TypeB mutex will be locked (by its bottom input initbias0 inverted). Once the ﬂit passes through the node (preackout0 toggles), the mode change is safely implemented (resettypeB to 1, then setbias0 to 0).1 Figure 5. Policy module: burst-mode controller speciﬁcation Figure 6. Safety module The req-latch control is used to enable the two input channel latches, L1 and L2. In default mode, it operates exactly as in the old arbitration node (each latch is opaque until mutex is won). In biased mode, it enables fast-forwarding of ﬂits through the node on the biased channel: it “wedges open” (i.e. holds transparent) the corresponding latch (e.g. L1 in bias-to-0) and “wedges shut” (i.e. holds opaque) the other latch (e.g. L2 in bias-to-0). The ack-latch control is also likewise reconﬁgured in biased mode: the ack latches are initially opaque, and open on demand when a new ﬂit arrives. 3.3 Steady-State Operation The operation of the new bi-modal arbitration mode is now illustrated for the two modes in steady-state operation. Default Mode. In this mode, the node operates identically to the old arbitration node of Fig. 3. In particular, in Fig. 4, the policy module is in the default state (initbias0 initbias1 = 00), and the safety module outputs setbias0 and setbias1 are both 0. Hence, the req-latches L1 and L2 are normally opaque, and exactly one of them passes data when a corresponding input ﬂit has won arbitration (i.e.when zerowins or onewins, respectively, is asserted high). Likewise, the ack-latches L3 and L4 are normally transparent and become opaque as soon as a corresponding input ﬂit has won arbitration (thus blocking the left acknowledgment); the left acknowledgment is ﬁnally propagated (i.e. L3 or L4 becomes transparent) when the mutex is again reset. Example. A ﬂit arriving on input channel 0, if uncontested, results in a forward path of: reqin0 toggled on the input channel, passing through an XOR2 gate through the mutex, through the req-latch control, to enable latch L1, through another XOR2 gate, through latch L5, to a toggle on Req on the output channel. Biased Mode (Bias-to-0). A bias-to-0 mode is assumed for this example (bias-to-1 is handled similarly). The policy module is in the bias-to-0 state (initbias0 initbias1 = 10), and only Safety 0 module has asserted its reconﬁguration output (setbias0 setbias1 = 10). The wedgeopen0 signal is also asserted high. Hence, req-latch L1 is held transparent and req-latch L2 is held opaque. Any arriving ﬂit on input channel 0 can pass directly through L1, while any ﬂit on input channel 1 is blocked by L2. Ack-latch L3 is now by default opaque, and becomes transparent as soon as a new ﬂit arrives on input channel 0 (it becomes opaque again after the left ack, ackout0, makes a transition), while ack-latch L4 is now transparent (as in the old arbitration new), but with no activity. 3.5 The Monitoring Network To support ﬁnding a safe window for mode change from biased to default, a lightweight monitoring network is used. A block diagram is shown in Fig. 7. This network provides continuous information to each arbitration node if any ﬂit entering the network is heading towards it. For modularity, it is combined into the existing MoT network. Each node—routing and arbitration—has an added small monitor control unit, as well as new input and output ports for monitoring 1There is an additional special case, where a ﬂit is currently in the node while arbitration is performed; it is omitted due to space limitations. The node has been simulated with SPICE, and shown to operate correctly in all cases. Figure 7. Block structure of MoT network with monitoring Figure 9. New non-root routing node with monitoring: (a) top-level, (b) latch control Table 1. Area comparison in square microns for pre-layout mapped cells with a packing factor of 0.8. Figure 8. New root routing node with monitoring: (a) top-level, (b) latch control signals. Monitoring channels and existing MoT data channels are also combined, so no new inter-cell channels are needed. System-Level Operation. Whenever a ﬂit enters the network, through a root routing node (left of ﬁgure), it initiates a monitoring signal transition. The signal is rapidly forwarded, i.e. demultiplexed, through the fan-out network until it reaches the leaves (mid-ﬁgure), with a path determined by the ﬂit’s source-routed address bits. It then is forwarded through the fan-in network (i.e. arbitration nodes) to the target root (right of ﬁgure). Effectively, this monitoring signal is an advance notiﬁcation of the data ﬂit, traversing the same path. Every bi-modal arbitration node along its path uses it as a something-coming-in signal. Node-Level Implementation. Two variants of the routing node are used to support monitoring: a root node (Fig. 8(a)) and a non-root node (Fig. 9(a)). Their latch/monitor control units are shown in Fig. 8(b) and Fig. 9(b), respectively. In the former, the control directs generates a monitoring signal, and routes it to 1 of 4 distinct monitoring signals (something-coming-out), on 1 of the 2 output channels, depending on the ﬂit’s target address. In the latter, monitoring input signals are demultiplexed, based on an address bit, to one of two output channels.2 The arbitration node then combines input monitoring signals onto one output monitoring signal, as shown in Fig. 4.3 The asynchronous monitoring network can be entirely implemented using standard cells.4 Taken as a whole, the new monitoring provides rapid forwarding of information on new ﬂits entering the network to all relevant nodes in the fan-in network. 4. EXPERIMENTAL EVALUATION This section provides evaluations of the new asynchronous router nodes, including detailed simulations of technology-mapped network 2This cell is shown for the middle level of fan-out nodes in Fig. 7(a); analogous ones are deﬁned for each other level. 3Details of the pipelined operation of this network are omitted due to space limitations. 4The asynchronous C-elements and aC-elements are sequential components that can be implemented using 3-4 combinational gates. primitives and also network-level simulations of 8-terminal mesh-oftrees networks. Earlier asynchronous primitives that do not feature bi-modal functionality serve as a baseline for comparison. In particular, Horak et al. [12] presented asynchronous routing and arbitration primitives, and showed that these asynchronous primitives have performance advantages over synchronous counterparts that were described in [2]. Both the baseline and new primitives are implemented using ARM/IBM 90nm SAGE-X standard cells. 4.1 Asynchronous Primitives We evaluate the new asynchronous primitives in terms of area, latency, and maximum throughput. As a baseline for comparison, we also re-implement and evaluate existing asynchronous primitives [12]. Both the new and baseline primitives were implemented using ARM 90nm SAGE-X standard cells with a 32-bit wide datapath. Simulation results were obtained at nominal temperature and voltage (1.0V, 25◦C) using Hspice in a Cadence Virtuoso environment. Area Comparison Implementing bi-modal and monitoring capability adds some area overhead to the asynchronous primitives. In order to obtain area results, we estimate the ﬁnal layout area by summing the cell areas of the pre-layout cells and then dividing by a typical packing factor of 0.8. Table 1 shows the total node area, which includes both control and data path, and the control area alone. For the routing primitive, the control overhead is less than 9% for both the typical and root nodes. For the arbitration primitive the control area is 2x higher than the baseline area. Because the 32-bit datapath area dominates the control area, the percent area increase is much less when the datapath is included: 2% for the arbitration primitive an 43% for the arbitration primitive. Our area estimates of the baseline primitives with an 8-bit datapath to layout-level areas found by [12] are within 16% of each other, which indicates that our estimates give a reasonable picture of area overheads. Latency and throughput The performance of the new asynchronous primitives has some small overheads compared to the baseline primitives [12]. Latency is the time from a request transition arriving on an input channel of an empty primitive to the time that the primitive produces a request on its output. Maximum throughput in Giga-ﬂits per Table 2. Routing Primitive: Performance Table 3. Arbitration Primitive: Performance Table 4. Monitor Latency: Cell Level Table 5. Mode Change Evaluation second (GFPS) is evaluated under different trafﬁc patterns. In order to capture the interactions between neighboring nodes, throughput is measured at the root primitive of a 3-level fan-out or fan-in tree. Table 2 shows results of latency and throughput experiments for the new routing primitive under two routing patterns: single and alternating. The single trafﬁc pattern routes data along only one of the routing primitive’s output ports while the alternating trafﬁc pattern routes data along each of the routing primitive’s output ports in a strictly alternating pattern. No latency overhead was observed for the new routing nodes, in either root and typical versions. Only small throughput overhead was observed for the new routing primitive under both the single and alternating trafﬁc patterns, due to the added load of the monitor control. Table 3 shows the results of performance experiments for the new arbitration primitive under two steady-state trafﬁc patterns: single (i.e. packets arrive at a single port) and all (i.e. packets arrive at both ports.) The table also compares the performance of the arbitration primitive in its two modes: default and biased. When the new primitive is in default mode, it has a small latency and throughput overhead, due to the added req-latch control logic. However, in biased mode, it shows signiﬁcant improvement over the baseline node: 41.6% lower latency and 19.8% higher throughput. Since the new arbitration primitive switches out of biased mode under trafﬁc on both ports, the all trafﬁc pattern is not reported for biased mode. Table 4 explores the performance of the monitoring signals, as they advance through a single node. For the root routing primitive, the latency is measured from the time that a new request arrives until the time that something-coming-out is asserted, while for all other primitives it is the time between something-coming-in and somethingcoming-out. The deassertion delay for the monitoring signal for all primitives is measured from the time that an acknowledge from the successor stage arrives until the time that something-coming-out deasserts. Overall, except for the routing root node, the latencies are quite fast: under 173 ps. Table 5 evaluates the mode changes for the bi-modal arbitration primitive. The latency is the delay between the arrival of a request that triggers the mode change and the production of a new request at the output of the primitive. Reconﬁguration time is measured from the arrival of a request until all control modules have been reconﬁgured. In each case, node reconﬁguration required at most 338 ps. Interestingly, a mode change from default to biased modes has no latency overhead, compared to the biased mode latency, because reconﬁguration takes place in parallel to processing the ﬂit. In contrast, mode changes from biased to default mode does incur overhead: the incoming ﬂit is stalled while reconﬁguration takes place. 4.2 Asynchronous Network This section presents the system-level performance evaluation of a new 8-terminal asynchronous network with dynamic adaptability and compares it to an implementation of an 8-terminal asynchronous network using existing asynchronous primitives [12]. For each network, we run experiments for eight benchmarks and report latency and throughput. Experimental Setup Two 8-terminal mesh-of-trees networks were modeled in structural Verilog using ARM 90nm standard cells. Each network consists eight 3-level fan-out trees connected to eight 3-level fan-in trees, for a total of 112 asynchronous routers. Wire delays between nodes are based on the layout ﬂoorplan presented by Horak et al. [12]. Network throughput and latency are evaluated for both networks following the standard approach by Dally [7]. A PLI (Programming Language Interface) trace generator and test harness used in this work was originally developed by Prof. Simha Sethumadhavan (Columbia University), and enhanced by us to support our asynchronous NoC framework. The test environment was written in the C programming language and invoked using a customized Verilog PLI setup, which provides inputs and records outputs in three phases: warmup, measurement and drain. The input environment generates packets at random intervals that follow an exponential distribution such that the mean Giga-ﬂits per second (GFPS) equals to the desired input trafﬁc rate. Packet source queues are installed at network inputs ports for accurate recording of latency [7]. To ensure accuracy of results, we ran simulations with a long enough warmup and measurement phase that the system reaches steady state. To determine the proper simulation time, we followed the standard procedure [7] of comparing a simulation of some warmup and measurement phase to another simulation with these periods doubled and checking that the results are comparable. Since different benchmarks inherently require different amounts of time to stabilize to steady state, benchmarks simulation times vary between 1000ns to 5000ns. Benchmarks Experiments are conducted for eight synthetic benchmarks chosen to represent a wide range of network conditions: 1) Bit permutation benchmark that uses a “shufﬂe” to choose sourcedestination pairs [7]; 2) Digit permutation benchmark that uses a “tornado” pattern to choose source-destination pairs [7]; 3) Uniform random trafﬁc in which each source is equally likely to send a ﬂit to every destination [7]; 4) Simple alternation with overlap in which each source alternates between exactly two destinations, and each destination has contention between exactly two sources; 5) Random restricted broadcast with partial overlap, in which four sources randomly send to three destinations each, where four of the destinations experience contention four have no contention; 6) Hotspot8 benchmark [7] in which one destination receives contending packets from every source; 7) Random single source broadcast sends uniform random data from one source to all destinations; 8) Partial streaming in which six sources each send statically to a unique distinct destination, where each of these six destinations is randomly interrupted from one of two remaining sources. Since the new bi-modal arbitration primitive lowers latency in contention-free scenarios, the benchmarks which offer the most contention are the most adversarial. Benchmarks 3 and 6 were picked to be extremely adversarial, benchmarks 4, 5, and 8 are designed to be moderately adversarial, and benchmarks 1, 2, and 7 have the least contention and are the least adversarial. In order to both highlight the strengths and point out the overheads of the new adaptive network, we test it over a wide range of benchmarks and report both throughput and latency. Overview. Overall, we see latency beneﬁts in six of the eight benchmarks, ranging up to 19.81%. For the remaining two adversarial benchmarks (3 and 6) there are latency overheads up to 13.20%. For saturation throughput, we see beneﬁts in seven of the eight benchmarks, ranging up to 27.84%. For the remaining single adversarial benchmark (3), there are throughput overheads up to 5.88%. The main cause for these differences is the high level of contention in benchmarks 3 and 6. In particular, detailed simulations of benchmark 6 show that changes from default to biased mode are often soon followed by mode changes back to default.We expect that a change in the policy mode to detect such thrashing scenarios could mitigate these overheads. Latency Results. Figs. 10 and 11 show the latency results for baseline (Network #1) and new adaptive (Network #2) networks respectively. Results are plotted as the network-level latency for each ﬂit vs. Network-level simulation results Figure 10. Latency for the baseline network. [12] Figure 13. Throughput for the baseline network. [12] Figure 11. Latency for the new adaptive network. Figure 14. Throughput for the new adaptive network. Figure 12. Latency comparison for 25% network load. the mean offered throughput rate. Under lightly-loaded trafﬁc conditions, network-level latency in the baseline network is near 2200ps for every benchmark. For the new adaptive network, latency varies from about 1800ps under low-contention benchmarks to 2800ps for adversarial benchmarks. To highlight these latency differences, Fig. 12 shows the latencies of both networks at identical throughput rates. Speciﬁcally, we chose an offered trafﬁc rate that is 25% of the saturation throughput when run on Network #1 and then ran experiments on Network #2 at that same rate. We chose 25% because it is high enough to show the characteristics of the benchmarks while still allowing ﬂits to travel through an uncongested network, as is standard when measuring latency. Evaluation was also performed at an offered trafﬁc rate that is 50% of the saturation throughput, and latency results were not signiﬁcantly different. Only benchmarks 3 and 6, which were chosen to be as adversarial as possible to our technique, display latency overheads, of 10.86% and 13.20% respectively. The remaining six benchmarks all show signiﬁcant latency improvements, ranging from 11.05% to 19.81%. Throughput Results. Figs. 13 and 14 show the network-level throughput results for baseline (Network #1) and new 8-terminal networks respectively. Results are plotted as the output rate normalized to the number of active input sources vs. the mean offered throughput rate for each active source. When lightly loaded, the performance of Network #2 tracks the performance of Network #1 for all benchmarks. At some offered throughput rate, the throughput results for every benchmark levels off to a value known as the saturation throughFigure 15. Saturation throughput comparison. put. Fig. 15 highlights the differences in saturation throughput values between the two networks. Of the two adversarial benchmarks, benchmark 6 still showed slight improvements, while benchmark 3 shows only a 5.88% throughput overhead. The remaining six benchmarks show throughput improvements from 0.39% to 27.84%. Monitoring Network Evaluation. The monitoring network is used to ﬁnd a safe time window for a mode change from biased to default. As an example, if an arbitration node is in bias-to-0 mode, its latch L1 is transparent (refer to Fig. 4). During the mode change to default, latch L1 becomes opaque. If L1 becomes opaque just as a new ﬂit is arriving on input channel 0, a malfunction may occur. The monitoring network solves this problem by enabling the node to decide whether it is safe to immediately perform a mode change if nothing is coming on channel 0 (“Type A” scenario), or if it must wait to piggy-back the change on the next arriving ﬂit on channel 0 (“Type B” scenario). A race condition occurs when nothing appears to be coming on channel 0, thereby initiating a Type A mode change, and then a new ﬂit is immediately detected as coming on channel 0. In this case, there are two critical global paths to the target arbitration node: (i) the new ﬂit, traversing from the entrance to network (routing root node) to the node’s input channel, and (ii) the monitoring signal, traversing the same route. If the monitoring signal, something-coming-in-0, is asserted high just after the ﬂit on channel 0 has initiated the mode change to default, initbias0 deasserted low, then a Type-A change will occur. The timing constraint is between (a) the differential between the arrivals of the global monitoring signal and the corresponding ﬂit Table 6. Monitoring Network Latency on channel 0, which must always be longer than (b) the local path to cut off channel 0, i.e. from initbias0 low to request latch L1 becoming opaque. Table 6 shows the timing margins for (a), which constitute the ﬁrst part of the timing constraint. This is relevant only for Type-A mode changes from biased to default mode; all other mode change types have local timing constraints only. The smallest timing margin, 404ps, is to a target arbitration leaf node, which is closest to the entrance to the network. The greatest timing margin is to a target arbitration root node, which is farthest from the entrance to the network. The margin depends on whether the intermediate arbitration nodes on the path are in default mode, 1122ps, or biased mode, 754ps. For (b), which forms the second part of the constraint, the local path to make L1 opaque includes a mutex element (see safety module’s TypeA Mode Change block in Fig. 6). The entire timing constraint between (a) and (b) is easily met with a 235ps margin in almost all cases, when competing input arrivals to the Type-A mutex are not extremely close. If the mutex inputs arrive within 2ps, the margin becomes 136ps. However, as the differential between the input arrivals at the Type-A mutex become less than 1ps, the mutex delay may lead to an unsatisﬁed constraint in extremely rare cases. These cases not only require this narrow differential, but can only occur with the precise scenario above of activation of a Type A mode change. We plan to evaluate the mean time between failure (MTBF) for this rare case, which we expect to be negligible. Further, we also anticipate that small changes to the safety module design and protocol will entirely eliminate this issue. 5. CONCLUSIONS AND FUTURE WORK In this paper, we presented a method for creating a dynamicallyadaptive asynchronous interconnection network that rapidly reconﬁgures based on local trafﬁc conditions. In particular, the network uses a new bi-modal arbitration node, which supports a “single-channelbias” mode when one of its two input channels is inactive. In this case, arbitration within the node is totally bypassed, resulting in improved node latency and cycle time. When evaluated as part of the adaptive network, system latency improvements up to 19.8% and throughput up to 27.8% were obtained. Overall end-to-end latencies were promising, at 1.8-2.8 ns (at 25% load) through 6 router nodes and 5 hops. Throughputs of 0.27-1.8 Gigaﬂits/s (at saturation rate) were also observed. As future work, we intend to explore alternative policy modules to reduce thrashing and enable better performance in adversarial scenarios. In addition, we plan to complete layouts of the design and fabricate a chip. Acknowledgments. The authors gratefully acknowledge Prof. Simha Sethumadhavan (Columbia CS Dept.) for providing the simulation framework which we built on for system-level simulation and for his valuable input. They also thank Young-Jin Yoon for simulation support (Columbia CS Dept.), Ryan Field and Prof. Ken Shepard (Columbia EE Dept.) for providing design tools and support, and Mike Horak (Advanced Simulation Technology, Inc.) for providing reference code and consultation on the simulation setup. 6. "
Optimal network architectures for minimizing average distance in k-ary n-dimensional mesh networks.,"A general expression for the average distance for meshes of any dimension and radix, including unequal radices in different dimensions, valid for any traffic pattern under zero-load condition is formulated rigorously to allow its calculation without network-level simulations. The average distance expression is solved analytically for uniform random traffic and for a set of local random traffic patterns. Hot spot traffic patterns are also considered and the formula is empirically validated by cycle true simulations for uniform random, local, and hot spot traffic. Moreover, a methodology to attain closed-form solutions for other traffic patterns is detailed. Furthermore, the model is applied to guide design decisions. Specifically, we show that the model can predict the optimal 3-D topology for uniform and local traffic patterns. It can also predict the optimal placement of hot spots in the network. The fidelity of the approach in suggesting the correct design choices even for loaded and congested networks is surprising. For those cases we studied empirically it is 100%.","Optimal Network Architectures for Minimizing Average Distance in k-ary n-dimensional Mesh Networks Matt Grange, Roshan Weerasekera, Dinesh Pamunuwa Centre for Microsystems, Engineering Depar tment, Faculty of Science and Technology Lancaster University, Lancaster, United Kingdom, LA1 4YR {m.grange,r.weerasekera,d.pamunuwa}@lancaster.ac.uk Axel Jantsch and Awet Yemane Weldezion Depar tment of Electronic Systems, KTH Royal Institute of Technology 120 Forum, 16440 Kista, Sweden {axel,awye}@kth.se ABSTRACT A general expression for the average distance for meshes of any dimension and radix, including unequal radices in diﬀerent dimensions, valid for any traﬃc pattern under zero-load condition is formulated rigorously to allow its calculation without network-level simulations. The average distance expression is solved analytically for uniform random traﬃc and for a set of local random traﬃc patterns. Hot spot traﬃc patterns are also considered and the formula is empirically validated by cycle true simulations for uniform random, local, and hot spot traﬃc. Moreover, a methodology to attain closed-form solutions for other traﬃc patterns is detailed. Furthermore, the model is applied to guide design decisions. Speciﬁcally, we show that the model can predict the optimal 3-D topology for uniform and local traﬃc patterns. It can also predict the optimal placement of hot spots in the network. The ﬁdelity of the approach in suggesting the correct design choices even for loaded and congested networks is surprising. For those cases we studied empirically it is 100%. Categories and Subject Descriptors C.4 [Performance of Systems]: Design studies, Modeling techniques, Performance attributes; C.2.1 [Network Architecture and Design]: Packet-switching networks—3-D IC optimization, Average distance, Hot spot placement 1. INTRODUCTION Two important metrics of performance for NoCs are latency and throughput, generally functions of network characteristics such as topology, interconnect characteristics, routing scheme and switch architecture, as well as application Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS’11, May 1-4, 2011, Pittsburgh, PA, USA Copyright 2011 ACM 978-1-4503-0720-8 ...$10.00. characteristics primarily deﬁned by traﬃc pattern. The average distance is a NoC performance metric that depends on the topology and the traﬃc pattern only, under the assumption that the network operates well below its saturation point. We express it as a closed-formula comprising a sum over all source-destination node pairs for arbitrary ndimensional radix-k mesh networks and for arbitrary spatial traﬃc patterns. A traﬃc pattern is deﬁned as the packet exchange probability for each source-destination pair. Average distance is an upper bound on the performance for all possible routing, switching, and ﬂow control algorithms. For instance, a routing algorithm is optimal for a given traﬃc pattern if packets on average do not travel more than the average distance through the network. We deﬁne the distance of communication as the minimum number of switch-points or nodes that a packet has to traverse from a source node to a destination node. It is measured in hops, where a hop is deﬁned as the traversal of a node. The average distance ¯H is the average distance of all packets in the network under a given traﬃc pattern, and a function of the dimension and radix in a mesh topology. It is a useful basic metric providing insight into the performance of the overall network. The starting point in our analysis is the rigorous formulation of an expression for the average distance in k−ary n−dimensional meshes, including unequal radices in the different dimensions, that is valid for any traﬃc pattern. We proceed to evaluate this expression for Uniform Random Traﬃc (URT), Local Random Traﬃc (LRT), and Hot Spot Traﬃc (HST) and verify through network simulations that the resulting formula accurately predicts the average latency for unloaded1 networks. The upshot of this is that the upper bound on performance given by the average latency of a k−ary n−mesh unsaturated network for any traﬃc pattern can be estimated from the general model we propose, without running network simulations, which saves both model development time and computation time. In the case of URT, the general expression can be solved to yield a closed-form expression for the delay, which is proven 1We use the terms unloaded, uncongested and unsaturated interchangeably to essentially mean the same thing: the injection rate of packets into the network is low enough to ensure the network is stable. 3 (3) n( k 3 − 1 ¯H = ( nk k even 3k ) k odd for k + 1 nodes in each dimension. This expression is in contradiction to both Liu’s and Agarwal’s formulæ. However, closer inspection shows that, under the corrected assumption of radix k (rather than k + 1), the odd case formula is identical to Agarwal’s formula and the even case is an upper bound of ¯H , approaching the true value asymptotically for large k . Holsmark [7] devotes appendix I of his Licentiate thesis to the clariﬁcation of the average distance. He concludes with k1 + k2 ¯H = (4) 3 for a k1 × k2 mesh assuming no self-traﬃc. Agarwal and Dally et al. include the self-traﬃc case. Taking this into account, it turns out that Holsmark’s formula is consistent with Agarwal’s expressions, but he covers only 1-D and 2-D meshes. We provide a derivation for the average distance in ndimensional meshes with the general case of unequal radices along the diﬀerent dimensions. This is important as many practical on-chip networks serving a few tens or hundreds of cores in multiprocessor systems are often irregular, and the number of nodes along the x, y , and z dimensions in a 3-D system for example, are seldom equal. We show how this formula is exactly correct, and is a generalization of all the models mentioned above. Koohi et al. [11] present abstract performance models in a spirit similar to ours. They propose power and throughput models for uniform, local, hot spot, and ﬁrst matrix transpose traﬃc models. Their approach is more empirical since they use simulation results as starting point and analyze the eﬀect of combinations of diﬀerent traﬃc models to derive comprehensive throughput and power models for mixed traﬃc patterns. In contrast, our approach focuses on distance and latency, deriving an analytical formula which is further validated through simulation. Primarily we focus on the unloaded case but we show that design decisions based on minimum latency provided by our model also hold true in loaded networks. While Koohi et al. consider only 2-D networks, we have special interest in 3-D topologies. In a seminal paper from 1990 Dally [4] studied the communication performance of k-ary n-dimensional tori, a similar class of topologies that we cover (meshes rather than tori). The paper analyzes average latency in networks with diﬀerent dimensions (mostly between 2-20) under diﬀerent cost to be more exact and more general than other expressions available in the literature. We also propose empirical closedform solutions for three speciﬁc LRT patterns based on the Response Surface Method (RSM) and illustrate the general methodology to obtain closed-form solutions for an arbitrary traﬃc pattern. Due to the simplicity of the model, it is particularly useful in ﬁnding optimal network architectures to minimize average communication delay under any traﬃc pattern, including empirically validated traﬃc models for irregularly sized networks with multiple constraints. For relatively simple traﬃc patterns such as URT, LRT or empirically-validated models where a closed-form expression can be obtained for the delay, the optimization problem takes a few seconds of computation time. In our results we show solutions for optimal architectures under diﬀerent traﬃc patterns and boundary conditions that can be obtained from a few iterations at most with minimal computation time using any general purpose programming language such as Matlab or C, rather than computationally expensive cycle-accurate packet-level simulations. However even for those cases where the traﬃc model does not allow such a formulation, the general model for average distance we propose has a computational complexity O(N2 ) in the worst-case where N is the number of nodes in the network, meaning that a brute-force search to ﬁnd the optimal network conﬁguration for network sizes up to say a thousand nodes is feasible on a desktop. Notably our model exhibits 100% ﬁdelity for all simulations carried out for three types of traﬃc (namely URT, LRT, and HST), where the optimum architecture to minimize latency for unloaded networks remains optimal even under congestion up to the point of saturation. The main contribution of this paper is in providing a generally valid model for the average distance and accompanying analysis that provides insight into network behavior under common traﬃc loads and constraints. 2. RELATED WORK The analytic formulæ for average distance provided in the literature either cover only a special case, or the assumptions made are not fully explained leading to misunderstanding. Agarwal [1] gives 1 n ¯H = 3 (cid:18)k − as the average distance in an n-dimensional mesh with radix k . The formula assumes that k is the same in all dimensions and no derivation or further motivation is provided. Liu et al. [12] provide a formula for a k1 × k2 2-D mesh: k (cid:19) (1) 1 (2) ¯H = k2 (cid:19) 1 3 (cid:18)k1 − k1 (cid:19) + 1 1 3 (cid:18)k2 − Here the case with diﬀerent radices is covered but only for two dimensions. The derivation of the formula is somewhat unclear because it contains an approximation step, that replaces k1 k2 − 1 by k1 k2 without motivation or explanation. It seems that the authors intended this formula only to be an approximation. They exclude the self-traﬃc case (when a node is allowed to send packets to itself ) which is in contradiction to Agarwal’s assumptions. Dally and Towles [3] oﬀer k−1 1 0 S1 S2 S3 S4 Sk 2 3 Figure 1: A 1 × k Mesh Network (dimension=1 and radix=k). The distances from switch S1 to all other switches including itself is shown. other node, and each such path has an associated distance. Shown in the ﬁgure are the distances hi,j from the ﬁrst node (S1 ) to the other nodes in the array. The average distance for a k-ary 1-D array is: k Xi=1 ¯H1×k = k Xj=1 k Xi=1 pi,j × |i − j | . (8) pi,j k Xj=1 3.1 Uniform Random Trafﬁc For Uniform Random Traﬃc, each node generates the same number of packets uniformly distributed over time, and all destination nodes are equally likely. Hence the probabilities are the same for all source-destination pairs, leading to pi,j = purt where purt is a constant. Therefore, (8) can be simpliﬁed thus: purt ¯Hurt1×k = |i − j | k k Xi=1 Xj=1 k k Xi=1 Xi=1 purt 1 = k3 3 − k 3 k2 (9) the published literature on this topic is substantial. Most of the work (e.g. [13, 5, 14, 10, 9, 6, 16, 15]) make very speciﬁc assumptions about routing (mostly deterministic, dimension order routing) and switching (mostly wormhole switching). The ma jority focuses on average delay [5, 14, 10, 9, 6] while some work targets worst-case delay [16, 15]. Sometimes even more speciﬁc assumptions are made such as single ﬂit buﬀers [6] or one dedicated virtual channel for each ﬂow [8]. In contrast with all this and similar work, we do not offer a delay model under congestion or for a speciﬁc switch architecture, but we start with an analytical formula for average distance which is valid for all k-ary n-dimensional meshes and for any traﬃc pattern, which abstracts entirely from routing and switching techniques. We then derive average distance formulas for speciﬁc traﬃc patterns and we illustrate the usefulness of this abstract, ideal formulae. In particular, we use the expression for average distance to investigate optimal architectures for minimizing delay when the link delays are not necessarily equal. 3. CALCULATING AVERAGE DISTANCE First, we derive a general average distance formula for k-ary n-dimensional networks and for arbitrary traﬃc patterns. Then, in section 3.1, we derive a closed-form solution for Uniform Random Traﬃc. This is straightforward and the result is consistent with earlier published formulas. In section 3.2 we derive a closed-form formula for a speciﬁc type of Local Random Traﬃc, which is a more complicated derivation. In section 3.3 we introduce Hot Spot Traﬃc but do not provide a closed formula due to the intractable dependency on the precise location of the hot spots. But later on in section 4 we show how an optimal placement of hot spot nodes can be found by minimizing the average distance. The average distance of a network is the ratio between the total distance that the packets emitted by all switches travel and the total number of packets emitted. In a network with N nodes, the total distance traveled by the packets emitted by all switches is: Dt = XA∈N XB∈N pA,B × dA,B , (5) leading to where A and B are any given nodes, dA,B is the Manhattan distance between the node A and node B , and pA,B is the probability that a packet is sent from node A to node B . pA,B deﬁnes the traﬃc pattern and can be an arbitrary function. E.g. for local random traﬃc pA,B is a function of the distance dA,B . This equation is a general formulation for any k-ary n-dimensional network. The total number of paths Np traversed by the emitted packets considered here is: Np = XA∈N XB∈N pA,B Hence the Average Distance Davg is given by: Davg = Dt Np = XA∈N XB∈N pA,B × dA,B XA∈N XB∈N pA,B (6) (7) In order to demonstrate the formula (7), we calculate the average distance over a single dimension by considering a 1-D array shown in Fig. 1. Each node connects to every k − . ¯Hurt1×k = 1 3 3k In the case of URT, since the probability is not a function of distance that the packets travel, the average dimension for an n-D network can be calculated by adding the average distance for each dimension. Therefore for an n-D array, the average distance for URT can be expressed using the following closed-form equation: (10) ¯Hurt = = n Xi=1 k1 3 ¯Hurt1×ki − 1 3k1 + k2 3 − 1 3k2 + · · · + kn 3 − 1 3kn (11) When the radix along each dimension is identical, it is true that k1 = k2 = k3 · · · = k , and (11) reverts to (1) and the odd case of (3). For n = 2 equation (11) simpliﬁes to (2). Although Liu et al. [12] intended an approximation, they ended up providing an exact formula for the case that includes a node sending traﬃc to itself. Even though in their setup they excluded the self-traﬃc case, the approximation they included as a corrective measure resulted in exactly the right formula including self-traﬃc. Finally it can be seen that taking the diﬀerent assumptions into account (i.e. with and without self-traﬃc) equation (11) reverts to equation (4) for n = 2 as follows. Since the average distance is computed by dividing the sum of all distances by the number of paths, we correct for these diﬀerent assumptions by multiplying with the number of paths in the self-traﬃc case ((k1 k2 )2 ) and dividing by the number of paths in the case with no self-traﬃc ((k1 k2 )2 − k1 k2 ). Since the distance for self-traﬃc is 0, the sum of all distances does not diﬀer in the two cases. Setting n = 2 and applying this correction to (11) results in: 1 + k2 − 1 k1 1 3 (cid:18)k1 − which turns out to be identical to Holsmark’s equation (4). (k1 k2 )2 − k1 k2 (cid:19) = (k1 k2 )2 k2 (cid:19) (cid:18) (k1 + k2 ) 1 3 %Error Table 1: Comparison of Average Distance Calculated by the formula (7) and Simulations Netw. Prob. Avg. Distance Size Model Formula Simulation 5x5x5 URT 4.8300 4.813 6x6x6 URT 5.8600 5.888 7x7x7 URT 6.8772 6.971 8x8x8 URT 7.8900 7.931 9x9x9 URT 8.9000 8.976 10x10x10 URT 9.9090 9.894 4x8x16 URT 9.9090 10.008 5x5x5 LRT:α=1.0 3.7900 3.81 6x6x6 LRT:α=1.0 4.5900 4.555 7x7x7 LRT:α=1.0 5.3900 5.418 8x8x8 LRT:α=1.0 6.1900 6.146 9x9x9 LRT:α=1.0 7.0000 6.969 10x10x10 LRT:α=1.0 7.8060 7.855 5x5x5 LRT:α=1.5 3.1800 3.163 7x7x7 LRT:α=1.5 4.4781 4.498 4x8x16 LRT:α=1.5 5.3757 5.301 0.35 0.47 1.36 0.52 0.85 0.15 0.99 0.53 0.94 0.52 0.71 0.44 0.62 0.53 0.44 1.38 3.2 Local Random Trafﬁc 1 For local traﬃc, a variety of probabilistic models can be considered, with the probability that a given node being the destination is some inverse function of the distance from the source: pA,B = λA f (dA,B ) where λA is the normalization constant deﬁned by XB∈N pA,B = 1, and dA,B is the distance between nodes A and B . A,B Table 1 compares the average distance estimated by (7) and RTL network simulations, for different traﬃc models and network sizes. The RTL simulator employs buﬀerless 5-port (2-D) and 7-port (3-D) switches, where one port serves the independent packet generating resources. The switches are buﬀerless and the routing deciFor f (dA,B ) = dα Table 2: Coeﬃcients for the closed-form equation for average distance for a network of size kx × ky × kz , where kx , ky ∈ 2, . . . 10 and kz ∈ 2, . . . 30. b b0 bx by bz bxx byy bzz bxy bxz byz %Avg. Error LRT:α = 0.5 LRT:α = 1.0 LRT:α = 1.5 γ=1 γ=0.5 γ=0.25 γ=1 γ=0.5 γ=0.25 γ=1 γ=0.5 γ=0.25 -0.4915 -0.4455 -0.4224 -0.4272 -0.3841 -0.3626 -0.1569 -0.1716 -0.1789 0.3556 0.3472 0.3430 0.3281 0.3087 0.2990 0.2680 0.2497 0.2406 0.3556 0.3472 0.3430 0.3281 0.3087 0.2990 0.2680 0.2497 0.2406 0.2804 0.1423 0.0732 0.2136 0.1123 0.0617 0.1493 0.0801 0.0455 -0.0061 -0.0054 -0.0051 -0.0101 -0.0084 -0.0076 -0.0122 -0.0101 -0.0091 -0.0061 -0.0054 -0.0051 -0.0101 -0.0084 -0.0076 -0.0122 -0.0101 -0.0091 -0.0013 -0.0008 -0.0005 -0.0022 -0.0014 -0.0010 -0.0027 -0.0016 -0.0011 0.0022 0.0027 0.0029 0.0055 0.0061 0.0064 0.0099 0.0096 0.0095 0.0030 0.0019 0.0014 0.0055 0.0036 0.0026 0.0066 0.0044 0.0032 0.0030 0.0019 0.0014 0.0055 0.0036 0.0026 0.0066 0.0044 0.0032 0.49 0.44 0.44 0.91 0.82 0.81 1.35 1.21 1.19 Table 3: Optimum Network Sizes for diﬀerent traﬃc models under planar link clock speeds, γ , of 0.5 and 0.25 times the 3-D link clock. Uniform vertical and horizontal clock speeds through a network intuitively perform at their optimum latency in symmetrical conﬁgurations of N×N×N. δ is the ratio between the average distance for optimum network size and the average distance for cubic solution. URT LRT:α = 0.5 γ = 0.5 γ = 0.25 LRT:α = 1.0 γ = 0.5 γ = 0.5 N γ = 0.5 γ = 0.25 Nopt 2x2x7 2x4x8 4x4x8 4x5x11 5x5x14 5x7x15 7x7x15 7x8x18 δ 0.96 0.98 0.95 0.96 0.97 0.97 0.95 0.95 Nopt 2x2x7 2x3x11 3x3x14 3x4x18 4x4x22 5x5x21 5x6x25 6x6x28 δ 0.78 0.82 0.82 0.83 0.84 0.84 0.84 0.84 27 64 125 216 343 512 729 1000 Nopt 2x2x7 2x4x8 4x4x8 4x5x11 5x5x14 5x7x15 7x7x15 7x8x18 δ 0.94 0.97 0.95 0.95 0.96 0.96 0.95 0.95 Nopt 2x2x7 2x2x16 3x3x14 3x4x18 4x4x22 4x5x26 5x5x30 6x6x28 δ 0.78 0.79 0.80 0.82 0.82 0.83 0.83 0.83 Nopt 2x2x7 2x3x11 3x3x14 4x5x11 5x5x14 5x7x15 7x7x15 7x8x18 δ 0.92 0.95 0.94 0.94 0.94 0.95 0.95 0.94 Nopt 2x2x7 2x2x16 3x2x21 3x3x24 3x4x29 4x5x26 5x5x30 6x6x28 δ 0.78 0.74 0.77 0.78 0.79 0.80 0.81 0.81 model to demonstrate optimum placements, which can be a signiﬁcant factor in reducing congestion and communication bottlenecks and improving overall system performance. parasitics of through silicon vias (TSV) used to connect vertical die layers as compared to long planar wires used on a 2-D IC. 4. APPLICATIONS OF THE MODEL In design space exploration it is often of interest to ﬁnd the topology that minimizes delay under various constraints imposed by technological, physical and system-level requirements. For the unsaturated case, delay is a straightforward function of the average distance, and such constrained optimization problems can be solved accurately with the proposed analytical model by treating it as the ob jective cost function. In this section we demonstrate how our model for average distance can be used to provide performance comparisons between networks and optimize the topological conﬁguration of nodes in 2-D and 3-D meshes for any traﬃc pattern. A 3-D mesh is an increasingly common topology with the advent of 3-D Integrated Circuits (IC). Equal radices in each dimension, translating to a cube with the same number of nodes or switches however, is an unrealistic arrangement; rather, the number of nodes in each dimension are likely to be diﬀerent, especially in the vertical direction. For example, recent work [17] has shown that the lower physical delay associated with the vertical interconnects in a 3-D stacked IC can enable higher data rates in the vertical dimension by clocking die-to-die links at greater frequencies than the horizontal dimensions. This is largely due to the relatively lower 4.1 Optimization of Network Topology The most eﬃcient network topology for minimizing latency under diﬀerent vertical and horizontal clocking constraints can be found by solving the constrained optimization problem of minimizing pA,B × (|xA − xB | + |yA − yB | + γ |zA − zB |) D3D = XA∈N XB∈N pA,B XA∈N XB∈N (16) sub ject to kx × ky × kz = N where N is the total number of nodes. Here dA,B has been obtained by multiplying the distance in each dimension by the corresponding clock period, under the assumption that the periods in the x and y dimensions are equal and normalized to 1, and that the period in the z dimension is 1 γ shorter, where 0 < γ ≤ 1. The optimum network size under the given constraint is found using an extensive brute-force search algorithm, and the solutions for diﬀerent N are shown in Table 3. As described in section 3.2 closed-form equations for diﬀerent traﬃc models were obtained considering three diﬀerent γ values. These equations are used for to ﬁnd the optimal network sizes which considerably reduces the computational load when compared to the general expression in equation (7) In order to assess the validity of the topological solution given by our model for the uncongested and congested cases, we use RTL cycle-accurate simulations to measure average distance for networks under varying loads. Figure 2(a) plots the average distance in hops as the packet injection rate per cycle increases. Intuitively, in a network with uniformly clocked horizontal and vertical links, the optimum conﬁguration for minimum communication distance under uniform traﬃc is always a symmetrical network. The unloaded average distance for three 64-node network topologies from our model is shown as a dashed base-line. This is the absolute minimum unloaded average latency achievable in that particular architecture. In this case a 64-node 3-D mesh will be best organized as a 4×4×4 network for minimum average latency. Our model matches the baseline simulation result to within a 1% error when the injection rate is suﬃciently low as to negate any contention issues. As we increase the injection rate, contention becomes more prevalent and the average latency increases as a result of non-ideal routing conditions and link bandwidth limitations. Despite loading the networks to the point of saturation (where the network bandwidth is exceeded by the packet injection rate), Figure 2(a) demonstrates that the topology predicted as optimum by our traﬃc pattern-based average distance model is still valid for loaded networks as a network topology deemed as optimal in the unloaded case will be suboptimal under congestion only if the lines intersect or cross. Figure 2(b) plots the same result for a local traﬃc pattern in a larger 256-node network. 4.2 Hot spot Node Placement Optimization In a 3-D IC with TSVs providing the vertical switch-toswitch links, it is likely that the oﬀ-chip I/O will only be able to serve the outermost dies. Ball-Grid Array (BGA) or other area-array packages allow the I/O pads to be dispersed across the entire die surface, meaning that any outer node in the network could interface with oﬀ-chip devices. In a high-throughput multi-tier 3-D NoC, it is further likely that mesh nodes on the bottom layer, which have direct access to the BGA I/O will face higher traﬃc due to the on/oﬀ-chip communication. To model this eﬀect we place nodes which attract high-traﬃc from the network (called hot spot nodes) on the bottom layer of a 3-D NoC in several conﬁgurations. The placement of the hot spot network nodes is crucial to the overall performance of the system (for example, placing hot spot nodes on the edge of a network will limit the surrounding link bandwidth due to the unused switch links). The model we developed in section 3 can be used to estimate the average unloaded latency and the optimum placement (giving minimum average distance) of hot spots in a network. We place two hot spots on the bottom layer in three conﬁgurations as depicted in Figure 3(b): (HS1) hot spots are placed on the edge of the network in opposing corners, Figure 3(c) (HS2) hot spots are placed in opposing corners one node removed from the edge, and Figure 3(d) (HS3) the hot spots are diagonally adjacent at the center of the network. The probability that any node in the network will send a packet to either of the hot spots is 80%, where the remaining 20% of the generated packets have a uniform probability to the other non-hot spot network nodes. Similar to Figure 2, we have used our analytical model to predict the optimum placement of hot spot nodes, given by 15 10 5 ) s p o H ( e c n a t s i D l a u t c A e g a r e v A 0 0 15 10 5 0 0 ) s p o H ( e c n a t s i D l a u t c A e g a r e v A 4x4x4 2x4x8 8x8x1 Avg D Model 4x4x4 Avg D Model 2x4x8 Avg D Model 8x8x1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Injection Rate (packets per node per cycle) (a) URT 2x8x16 8x8x4 16x16x1 Avg D Model 2x8x16 Avg D Model 8x8x4 Avg D Model 16x16x1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Injection Rate (packets per node per cycle) (b) LRT Figure 2: Average actual distance for increasing injection rates under (a) Uniform Random Traﬃc for a 64-node network and (b) Local traﬃc for a 256-node network. The model prediction for optimal network conﬁguration (given the minimum average distance) is consistent under increasing injection rate. the minimum average latency for several network conﬁgurations and found that in the RTL simulations even under heavily congested networks with hot spot traﬃc, the optimum node placement to achieve the lowest average packet latency is consistent with the model prediction at zero load. Figure 4 plots the growth of latency with injection rate for the three hot spot placement schemes in a 7×7×7 network. Despite the minimal diﬀerence in unloaded latency provided by our model between the HS2 and HS3 placement schemes, the simulation results indicate no crossover points even at heavy loads. These results demonstrate the model’s usefulness in predicting network optimal topologies and node placement without computationally expensive packet-level simulations. Further to this, the model can quickly predict the optimum placement and conﬁguration for hot spot trafﬁc under diﬀerent vertical and horizontal clocking schemes. This eﬃcient design space exploration is enabled by the availability of a closed-form comparison metric.             (a) Hot spots on bottom layer (b) HS1 ) s p o H ( e c n a t s i D l a u t c A e g a r e v A 18 16 14 12 10 8 6 1 HS1 HS2 HS3 Model HS1 Model HS2 Model HS3 2 3 4 5 6 7 8 9 10 Injection Rate (packets per node per cycle) −3 x 10 (c) HS2 (d) HS3 Figure 3: Example of hot spot node placements HS1, HS2 and HS3 for the bottom layer of a 6×6×6 network To assess the ﬁdelity of our model, we have conducted RTL simulation sweeps for unloaded and loaded networks for twelve diﬀerent mesh conﬁgurations of sizes ranging from 8 to 1000 nodes each with the three hot spot placement schemes shown by Figure 3 and six packet injection rates of between 0.0001 and 0.8 packets-per-node-per-cycle (see Table 4 for a range of results). We consider the model to correctly predict the optimal network topology under loading if the latency curve with increasing injection rate corresponding to the zero-load optimal conﬁguration is always below the latency curve corresponding to any other topology. Formally, ﬁdelity can be deﬁned as meeting the following condition: ¯HNW1 (I R) < ¯HNWi (IR) for any injection rate (IR) where ¯HNW1 (0) < ¯HNWi (0), ¯HNWi being the average distance for the network conﬁguration nwi at zero load. The latter condition essentially deﬁnes ¯HNW1 as the optimal network under zero load. If the data points cross-over at any point with a stable network or ¯HNW1 (IR) > ¯HNWi (I R), the model prediction is deemed to have failed. For the simulations we have conducted under local, uniform and hot spot traﬃc, we found that the optimum placement predicted by the model is valid throughout all the network conﬁgurations when the injection rate is below the threshold of saturation2 for the network. These exhaustive ﬁndings report that although the model cannot predict the exact latency of a topology under congestion (which depends on low-level architectural features such as switch design, buﬀering and routing algorithms among others) it can accurately and repeatedly determine the optimum network topology and placement of hot spots under 2We consider a network saturated when the packet injection rate exceeds the available link bandwidth. Figure 4: Hot spot simulation with increasing injection rate. The model prediction for optimal hot spot placement holds true for loaded and unloaded networks even for a relatively minor diﬀerence in minimum average latency. any network load. This assumption holds true for the simulations we have conducted with our traﬃc models, however we have yet to test it under other traﬃc scenarios such as burst mode or worm-hole routing which may reduce the ﬁdelity of the model. 5. CONCLUSIONS The average traveling distance of packets in a NoC is an abstract performance metric of the network topology for a given traﬃc pattern and does not consider routing and switching schemes. We have rigorously formulated and presented an analytic expression for the average distance in k-ary n-dimensional meshes for any traﬃc pattern as a summation over the nodes in the network. It is shown to be a generalization of previously presented models and is over 99% accurate when compared against cycle-accurate RTL simulations for uniform-random, local and hot spot traﬃc, with any errors being traceable to stochastic deviations and rounding errors. The formula deals with the unloaded case and does not constitute a delay model for congested networks. However it can be used as a metric for choosing between network topologies for optimal performance, with solutions that are valid even under loading. To demonstrate its usage we have applied formulæ derived for URT and LRT to guide high level design decisions. In particular, we demonstrate our model’s ability to optimize the conﬁguration of 2-D and 3-D symmetric and asymmetric networks given the constraints of latency, traﬃc pattern and diﬀerent clock speeds over the on-chip planar and 3-D interconnect. Also, we predict the optimal placement of hot spot nodes in a network with HST based on the zero-load average distance formula and show that the placement remains optimal for loaded networks. The predictive power of our model (its ﬁdelity) is a surprising 100% for the cases we have studied empirically. However further studies are necessary to understand the scope and limitations of this method. In particular we have       Table 4: Simulation sweeps to assess the ﬁdelity of the model. The optimum hot spot placement calculated by our model is shown to be valid for any traﬃc congestion in stable networks for over 100 data points. HSP 4×4×4 6×6×6 7×7×7 8×8×8 10×10×10 HS1 HS1 HS2 HS3 HS1 HS2 HS3 HS1 HS2 HS3 HS1 HS2 HS3 Model 4.396 7.1877 6.1285 5.606 7.4485 6.8289 6.7692 9.987 8.7897 7.598 12.79 11.509 9.5958 0.0003 4.467 7.29 6.23 5.66 7.48 6.85 6.75 9.99 8.82 7.68 12.89 11.64 9.61 0.001 4.49 7.34 6.26 5.98 7.57 7.13 7.06 10.41 9.2 8.06 13.46 12.28 10.42 0.003 4.63 7.61 6.62 6.04 8.27 7.69 7.57 11.70 10.69 9.51 115.5 117.93 120.96 0.005 4.67 8.14 7.17 6.6 9.63 9.25 8.99 38.31 36.09 38.7 301.2 309.3 308.6 0.007 4.72 8.73 8.05 7.53 25.46 20.73 19.29 194.36 202.74 209.6 388.5 391.03 385.2 0.009 4.82 10.29 10.02 9.68 177.42 180.84 178.82 288.55 290.42 299.3 414.2 417.78 409.3 not studied time variant traﬃc patterns and deterministic routing. First, the ﬁdelity may be lower for bursty traﬃc because bursts may temporarily clog parts of the network without overloading the network as a whole in the long term. Such eﬀects are not predicted by our model but may aﬀect diﬀerent network topologies very diﬀerently. The second limitation in our empirical evidence is the fact that we use adaptive routing in our simulations. Deterministic routing has much less capacity to balance load over the entire network. Individual links can easily become bottlenecks even though the network is only modestly loaded. Again, such local, temporary overload may not be predicted well by our zero-load model. Further experiments are required to study these eﬀects. In summary, although further studies are need to understand the full scope of the model, its power in predicting network performance and its usefulness as a metric in high-level topology exploration is very promising given the simplicity of the basic formula, being simply the average geometric traveling distance of packets. 6. "
The XMOS XK-XMP-64 development board.,"The XMOS XK-XMP-64 is an experimental multi-processor board that demonstrates the scalability of the XS1 architecture; it connects 64 XCore processors, providing 512 hardware threads and 25.6 GIPS aggregate performance. This paper briefly overviews the XK-XMP-64, giving a simple example program and presenting the results of a set of experiments designed to benchmark its performance. These include message latencies and timings for barrier synchronisations and well-known static traffic permutations.","The XMOS XK-XMP-64 development board [Demo Abstract] James Hanlon XMOS Ltd. Venturers House, King Street, Bristol, BS1 4PB, UK www.xmos.com ABSTRACT The XMOS XK-XMP-64 is an experimental multi-processor board that demonstrates the scalability of the XS1 architecture; it connects 64 XCore processors, providing 512 hardware threads and 25.6 GIPS aggregate performance. This paper brieﬂy overviews the XK-XMP-64, giving a simple example program and presenting the results of a set of experiments designed to benchmark its performance. These include message latencies and timings for barrier synchronisations and well-known static traﬃc permutations. Categories and Subject Descriptors C.1.4 [Computer Systems Organization]: PROCESSOR ARCHITECTURES—Paral lel Architectures Keywords XMOS, distributed-memory, multi-processor 1. INTRODUCTION The XMOS XS1 processor architecture [2] is general-purpose, multi-threaded, scalable and provides low-level support for concurrency. It allows systems to be constructed from multiple XCore processors which communicate with each other through fast communication links. The XK-XMP-64 [4] is an experimental multi-processor board that demonstrates the scalability of the XS1 architecture. It connects together 64 XCore processors in 16 XS1-G4 devices. These are arranged as a 4-dimensional hypercube using 5b XMOS links on a single PCB. The system delivers 25.6 GIPS aggregate performance. It also allows external interfaces to be connected with IDC connectors which provide access to IOs on the XCores, and by a dual Ethernet interface. 2. A COMPLETE EXAMPLE PROGRAM The XC language [3] provides explicit control of concurrency, communication, timing and IO along with the sequenPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS’11 May 1-4, 2011 Pittsburgh, PA, USA Copyright 2011 ACM ACM 978-1-4503-0720-8... $10.00 ...$10.00. #include <platform.h> #include <xs1.h> void node(chanend i, chanend o, int n) { if (n == 0) i :> n; while (1) { n = n + 1; o <: n; i :> n; } } int main() { chan c[512]; par (int i = 0; i < 512; i++) on stdcore[i/8] : node(c[i], c[(i+1) & 511], i); return 0; } Figure 1: A complete example program to create a 512thread pipeline. tial capabilities of C. The following complete example program demonstrates the programming approach taken by XC. It creates 512 node processes connected in a pipeline communication topology. A token is then passed from node 0 to node 511, each time incremented by one. 3. PERFORMANCE MEASUREMENTS 3.1 Message latency Point-to-point message latency varies depending on the relative position of the source and destination cores in the network and presence of other traﬃc. Table 1 gives the measured latency timings in an empty network for the diﬀerent possible distances, from intra-chip to inter-chip across the full diameter of the network. The eﬀect of congestion is captured in the results of Section 3.3. Core-to-core journey Time (ns) On-chip 70 Oﬀ-chip (1 hop) 200 Oﬀ-chip (2 hops) 290 Oﬀ-chip (3 hops) 390 Oﬀ-chip (4 hops) 480 Table 1: Latency of core-to-core journeys, both on and oﬀchip. Note 4 is the diameter of the network. 100 1000 10000 100000 1e+06 1 10 100 1000 10000 A v r e e g a L a e t n c y ( µ s ) Message size (KB) Shuﬄe Transpose Bit-comp Bit-rev Random Figure 2: Average latency as a function of message size for 64 cores. 3.2 Barrier synchronisation Barrier synchronisation is a key operation in many parallel algorithms where threads of execution may enter the barrier at any time, but will leave only when all other threads have entered. The hypercube topology of the XK-XMP-64 can be exploited to implement this operation in O(log n) communication steps where n is the number of nodes in the network. This was measured to complete in 990ns, potentially allowing over one million to be completed per second. 3.3 Trafﬁc permutations We are interested in the temporal characteristics of different traﬃc patterns and the congestion that they induce over the network. These can be simulated with synthetic traﬃc patterns which are commonly considered as a permutation which provides a one-to-one mapping of source to destination addresses. Because permutation traﬃc concentrates load on individual source-destination pairs, they provide good stress-testing. Figure 2 gives average latency for varying message sizes for well known permutation patterns. Figure 11 give latency distributions for each of these patterns. The recorded measurements are of the total message delivery time for simultaneous transmission by all cores of a single 32-bit message. The latency distribution for random permutations given in Figure 11e clearly shows distributions around 1, 2, 3 and 4 hop journeys. Note, each distribution is asymmetric as messages may be delayed for some period of time. 4. CONCLUSION This paper gives details of the programming approach and performance experiments for the XMOS XK-XMP-64. For more details of the experiments and results presented please refer to [1]. 5. "
Prevention flow-control for low latency torus networks-on-chip.,"The challenge for on-chip networks is to provide low latency communication in a very low power budget. To reduce the latency and keep the simplicity of a mesh network, torus network is proposed. As torus networks have inherent circular dependency, additional effort is needed to prevent deadlock, even if deadlock free routing algorithms are used.
We describe a novel flow-control mechanism to address cost/performance constraints in torus networks and ensure freedom from deadlock. Flow-control is achieved using a prevention mechanism which uses virtual cut-through switching, and deadlock freedom is achieved by considering only a single packet buffer per input port. We can simplify the router design by having a simple switch allocator, which prioritizes in-flight packets, and a single packet buffer per input port, which eliminates the need for virtual channels. Experimental validation reveals that our design achieves significant improvement in throughput, as compared to the traditional design, using significantly fewer buffers.","Prevention Flow-Control for Low Latency Torus Networks-on-Chip Arpit Joshi Madhu Mutyam Computer Architecture and Systems Lab Depar tment of Computer Science & Engineering Indian Institute of Technology, Madras Computer Architecture and Systems Lab Depar tment of Computer Science & Engineering Indian Institute of Technology, Madras arpitj@cse.iitm.ac.in madhu@cse.iitm.ac.in ABSTRACT The challenge for on-chip networks is to provide low latency communication in a very low power budget. To reduce the latency and keep the simplicity of a mesh network, torus network is proposed. As torus networks have inherent circular dependency, additional eﬀort is needed to prevent deadlock, even if deadlock free routing algorithms are used. We describe a novel ﬂow-control mechanism to address cost/performance constraints in torus networks and ensure freedom from deadlock. Flow-control is achieved using a prevention mechanism which uses virtual cut-through switching, and deadlock freedom is achieved by considering only a single packet buﬀer per input port. We can simplify the router design by having a simple switch allocator, which prioritizes in-ﬂight packets, and a single packet buﬀer per input port, which eliminates the need for virtual channels. Experimental validation reveals that our design achieves signiﬁcant improvement in throughput, as compared to the traditional design, using signiﬁcantly fewer buﬀers. Categories and Subject Descriptors C.1.2 [Computer Systems Organization]: Multiprocessors–Interconnection architectures General Terms Design, Performance. Keywords Networks-on-Chip, Torus, Deadlock, Flow-control. 1. INTRODUCTION With the continuous scaling of transistors, the number of transistors available on a single chip is increasing signiﬁcantly. To utilize these available transistors eﬀectively, larger multicore processors are being proposed with Networkson-Chip (NoCs) as the de facto solution for providing communication among these large number of cores. NoCs are Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS ’11, May 1-4, 2011 Pittsburgh, PA, USA Copyright 2011 ACM 978-1-4503-0720-8 ...$10.00. (a) Mesh (b) Torus (c) Folded Torus Figure 1: Various Network Topologies. one of the most critically shared resources in many-core systems, and the performance of such systems will heavily depend on the eﬃciency of NoC designs. Power consumption is another ma jor concern in NoC design. For instance, power consumed by NoC itself is 28% of the tile power in Intel Teraﬂop chip [11] and 36% of the total chip power in MIT RAW chip [22]. Thus any competitive NoC design should reduce both latency and power consumption. Several topologies, such as Torus [5], Mesh [10], Star [1], have been proposed for NoCs in literature so far. Among these topologies, two-dimensional (2D) mesh topology (as shown in Figure 1a) has gained a lot of attention as it allows for simple routing algorithms and low-radix router architectures. But, implementing packet-switching techniques on 2D mesh networks increases the router complexity, which in turn increases packet latency and power consumption. Furthermore, the long diameter of the mesh topology can negatively impact the communication latency. Optimizations have been proposed to reduce latency, but at the cost of added complexity of the router [16] [17]. To reduce the latency and keep the simplicity of mesh topology, torus topology (as shown in Figure 1b) is proposed, wherein the switches on the opposite edges are connected to each other through wrap-around channels. Although torus topology reduces the network diameter, the long wrap-around connections may result in excessive delay. To avoid this problem, folded torus (as shown in Figure 1c) is proposed [6]. Torus networks have inherent circular dependency, which demands additional effort to prevent deadlocks even if deadlock free routing algorithms like dimension ordered routing [6] are used. In this paper, by considering 2D folded torus networks, we try to achieve a deadlock free operation keeping in view the cost/performance constraints. In speciﬁc, the key contributions of this work are: • A novel prevention ﬂow-control mechanism, based on virtual cut-through switching, which ensures deadlock free operation while requiring only one packet buﬀer per input port. (a) High-level block diagram of a baseline wormhole router showing ma jor modules. (b) Router pipeline diagram. (BW: Buﬀer Write, VA: VC Allocation, SA: Switch Allocation, ST: Switch Traversal, LT: Link Traversal) Figure 2: Baseline wormhole router. • As a complement to the above mechanism, a simplistic router design to eliminate the need for virtual channels and prioritizing the in-ﬂight packets. Experimental results show that the proposed design achieves as high as 15% improvement in throughput and as high as 36% reduction in zero-load latency, with reduced buﬀer requirement (4× less number of buﬀers) when compared to the traditional design. Using closed loop simulations, we also show that for synthetic workloads our proposed design achieves as high as 54% reduction in execution time. The rest of the paper is organized as follows: Section 2 discusses the conventional designs for deadlock free torus networks and Section 3 provides the motivation for proposed design. We describe our proposed technique in Section 4 and discuss ﬂow-control and fairness issues. Section 5 deals with the experimental validation and Section 6 provides the related work. We conclude the paper in Section 7. 2. CONVENTIONAL DESIGNS There have been many proposals for deadlock free routing in torus interconnection networks [4] [7] [8] [21]. Wormhole switching with virtual channels is the most commonly used technique for deadlock free routing. Wormhole routers are proposed to reduce the number of buﬀers required in the routers. They require only a few ﬂit buﬀers as opposed to packet buﬀers for cut-through networks because when a packet is blocked it is stored across routers on its path in the network. But a complex ﬂow control mechanism is needed to avoid overﬂowing the buﬀers, because ﬂow control is performed at ﬂit level. Due to the additional complexity of virtual channels, which are needed to avoid deadlock in torus networks, complexity of switch arbitration increases and it also adds an extra stage in the router pipeline for virtual channel allocation. Addition of virtual channels and increased complexity of arbitration increases the router pipeline stages, which in turn increases lower limit on the number of buﬀers per input port to cover the credit round trip latency. In folded torus topology, since the link lengths are longer, it takes two cycles to traverse a link, which further increases the minimum required buﬀering. A conventional wormhole router along with its pipeline diagram is shown in Figure 2. Since per-hop router (a) Architecture of the proposed router showing ma jor modules. (b) Router pipeline diagram will look like R1 when injecting or turning a packet and like R2 when forwarding in the same dimension. (BW: Buﬀer Write, SA: Switch Allocation, ST: Switch Traversal, LT: Link Traversal) Figure 3: Proposed router latency is 5 cycles, in order to enable eﬃcient utilization of network bandwidth, the router requires at least 10 ﬂit buﬀers per input port to cover credit round trip latency. 3. MOTIVATION In torus networks with wormhole switching, due to the inherent cyclic dependency, mechanisms like virtual channels are necessary to avoid deadlocks. These mechanisms are implemented at the cost of increased router complexity. But, latency of a network strongly depends on complexity of routers used. Simple and fast routers can reduce the latency and at the same time provide area and power eﬃciency. In this work we try to meet these contradictory demands of avoiding deadlocks and low latency communication by proposing a novel ﬂow-control mechanism which inherently enables us to simplify the router. Proposed ﬂow-control mechanism is based on virtual cut-through switching, which has a simpler ﬂow control mechanism than wormhole switching. Also, usage of virtual cut-through switching simpliﬁes deadlock avoidance as given in [3]. Our mechanism gives an opportunity to use a simple, low-cost router as shown in Figure 3a. The router pipeline, shown in Figure 3b, can be achieved by elimination of virtual channels, having small write-through buﬀers and a simpliﬁed priority based arbitration mechanism coupled with single bit lookahead signal. Our ﬂow-control mechanism and the router design are presented in detail in the sections to follow. 4. PREVENTION FLOW-CONTROL ROUTER DESIGN In this section, we describe the proposed prevention slot ﬂow control (PFC) mechanism for 2D torus networks. We simplify the router by using virtual cut-through switching, prioritized arbitration, a single packet buﬀer per input port, and by splitting the network into two separate and independent sub-networks based on message classes. We put forth the deadlock free design initially by considering constant ﬂit size packets and then extend it to packets with variable number of ﬂits. 4.1 Buffering We use virtual cut-through switching [14], in which ﬂow control is done at packet level. A router can forward or inject a packet only when it ﬁnds enough buﬀers at the next hop router to hold the entire packet. So, we need buﬀers in multiples of packet size at each input port of the router. In the proposed design, only one packet buﬀer is suﬃcient to avoid deadlock as will be shown in Section 4.3. We consider write-through buﬀer [22] to allow bypassing when the input buﬀer is empty. 4.2 Prioritized Arbitration We use a priority mechanism to simplify the complexity of switch arbitration [15]. Packets traveling in the same dimension are given priority over other packets which are trying to change dimension. For example, if a packet from North input port is requesting South output port, it is given priority over packets from East, West or local input ports requesting South output port. Similarly, if a packet from West input port requests East output port, it will be given priority over packets from other input ports that are requesting East output port. When a packet is to be injected or has to switch dimension, it has to go through a stage in the router pipeline, where it requests for switch arbitration, because packets moving in the same dimension have priority over packets changing dimension. Whereas for the packet traveling in the same dimension, a single bit lookahead signal is sent one cycle in advance. This lookahead signal requests for switch allocation one cycle before the packet arrives at the router enabling a single cycle router traversal (shown in Figure 3b). 4.3 Prevention Flow-Control for Unidirectional Ring For the sake of simplicity, consider a N-node unidirectional ring with single cycle per-hop latency. Consider that we have single ﬂit packets and single ﬂit buﬀer per input port of router, and each node wants to send a packet to a node which is two hops away. Suppose initially at time t, the network is empty as shown in Figure 4a, and all the nodes try to inject a packet. All nodes are able to inject a packet as they have buﬀers available at the next stage router and there is no packet in their input buﬀer requesting for the same output port. But eventually at time t + 1, all the buﬀers in the network are ﬁlled up as shown in Figure 4b, leading to a deadlock. The network ended up in a deadlock because all nodes were able to inject packets in the same cycle. We can avoid this deadlock scenario if we prevent any one node from injecting a packet into the network. If we prevent N0 from injecting a packet at time t, it leaves one buﬀer free in the network (as shown in Figure 4c). Also, because of prioritized arbitration, Node N0 cannot inject a packet at time t+ 1, as there is a packet in its input buﬀer which has a higher priority than the packets to be injected. So the network will stay free from deadlock. In summary, if we prevent one node in the network from injecting a packet in every cycle, it ensures that there is at least one free packet buﬀer allowing the network to be free from deadlock. To prevent a node from injecting, we use prevention slots. A prevention slot prevents a selected node from injecting a packet into the network. To ensure fairness across nodes, prevention slot should be moved in a round robin fashion across nodes. Prevention slot mechanism can be implemented in a distributed manner. For a N -node ring, each node has a modulo N counter, which initially is reset to 0. (a) At time t (b) At time t + 1 (without prevention slot) (c) At time t+1 (with prevention slot) Figure 4: Illustration of (a) a network with single packet buﬀer per input port, (b) occurrence of deadlock and (c) deadlock avoidance by preventing N0 from injecting at time t = 0 The counter is incremented every cycle. When the counter value equals the node number for a node, it implies the occurrence of a prevention slot for that node and hence the node cannot inject any packet in that cycle. For example, at time t, N0 cannot inject a packet into the ring, at t + 1, N1 cannot inject a packet into the ring and so on. This ensures that in each cycle there is at least one node which is not injecting a packet and hence deadlock is avoided. Claim. A unidirectional ring with single packet buﬀer per input port, prioritized arbitration, and prevention slot mechanism is free from dead lock. Proof. Assume to the contrary that the ring is deadlocked. So all the buﬀers in the ring are full, with packet in each buﬀer waiting for the buﬀer in the next router to be free. Before reaching this conﬁguration, the network could have been in two possible conﬁgurations: either none of the packet buﬀers were occupied; or k packet buﬀers were occupied, for 0 < k < N . In the former case, all nodes will have to start injecting a packet in the same cycle for the ring to be deadlocked, which is not possible as prevention slot ﬂow control will prevent some node from injecting. In the latter case, if k packet buﬀers were occupied, N − k nodes have to start injecting a packet in the same clock cycle for the ring to be deadlocked. This is also not possible as at least one node from N − k nodes which are trying to inject a packet will have a packet in the input port of its router. Let Nt be the node which has a packet in the input port of prioritized arbitration. Hence, not all the N − k nodes can its router. Then node Nt cannot inject a packet as we use inject a packet. Thus, the deadlock conﬁguration can never be reached. So the ring is deadlock free. 4.4 Prevention Flow-Control for Torus Network We consider dimension ordered (X-Y) routing in N ×N 2D torus networks. Dimension ordered routing does not introduce any cycles across dimensions, because there is a strict ordering in which dimensions are traversed. So, deadlock can occur only within a dimension. A deadlock can occur either when a node is trying to inject a packet, or when a packet is switching dimension. To simplify deadlock avoidance mechanism, we can consider switching of dimension as injection into a new dimension. Prevention slot ﬂow control can be applied in these scenarios in order to ensure deadlock freedom. One direction in one dimension of the torus network is considered here for the sake of simplicity. This work (a) At time t (b) At time t + 1 (a) At time t (b) At time t + 1 (c) At time t + 2 (d) At time t + n + 1 (c) At time t + n (d) At time t + 2n Figure 5: Illustration of deadlock in one dimension of a torus network when using prevention slot of single cycle. (a) All nodes start injecting a packet except N1 which has a prevention slot. (b) N1 starts injecting a packet as prevention slot has moved to N2 . (c) Prevention slot moved to N3 and packets advance in the network. (d) Network is deadlocked because all buﬀers in the network are occupied. Figure 6: Illustration of deadlock in one dimension of a torus network when using prevention slot for n cycles. (a) All nodes start injecting a packet except N1 which has a prevention slot and N0 which does not have a packet to inject. (b) N0 gets a packet to send and starts injecting. (c) Packet from N0 has not reached router of N1 and prevention slot moved to N2 , so N1 starts injecting. (d) Network is deadlocked because all buﬀers in the network are occupied. can be easily extended to all dimensions. We also consider that each node sends a packet to nodes which are at least 2 hops away. In torus networks, per-hop latency, which includes router latency and link traversal latency, will be n cycles, where n > 1. So, it will take n cycles for the next hop node to see the data being injected from the previous node. Hence, if we have prevention slot of one cycle, it can still lead to deadlock as shown in Figure 5. As shown in Figure 5a, at time t the network is empty and all nodes want to inject a packet. Node N1 does not inject a packet because of the prevention slot. But since prevention slot lasts for one cycle, N1 tries to inject a packet in the next cycle at time t + 1 (Figure 5b). Since the packet injected by N0 has still not reached N1 , node N1 is able to inject a packet (Figure 5c), which will eventually lead to a deadlock, as shown in Figure 5d. Even when we apply the prevention slot for n cycles at each node, we cannot prevent deadlock, as shown in Figure 6. For example, as shown in Figure 6a, at time t the network is empty and all nodes except node N0 have a packet to inject. Assume that the prevention slot starts at node N1 at time t. So, all nodes except N0 and N1 start injecting a packet at time t. At time t + 1, N0 gets a packet to inject (Figure 6b) and it immediately starts injecting, since there is no packet in its input buﬀer requesting for the same output port. At time t + n, the packet from N0 has not yet reached N1 , and since N1 has no other packet in its input port requesting the same output port, it starts injecting a packet (Figure 6c). This situation also eventually leads to a deadlock, as shown in Figure 6d. Hence, if we allow nodes to inject packet at any time and use prevention slot it can still lead to deadlock. We propose a simple approach where nodes are allowed to inject packets only at the start of a slot, i.e., at every nth clock cycle, and prevention slot round robins across all nodes. So, if the prevention slot is at N0 at time t = 0, it will be at N1 at time t = n, will be at N2 at time t = 2n and so on. This mechanism can also be implemented in a distributed manner. Each node can maintain two counters. A modulo-n counter, incremented every clock cycle, identiﬁes the start of slots (start of slot detection counter) and a modulo-N counter, incremented at every start of a slot, identiﬁes the occurrence of prevention slot (prevention slot detection counter). This nth cycle injection mechanism and the prevention slot has to be maintained separately for each output port of the router. 4.5 Sub-Networks Until now we assumed constant ﬂit size packets. But the actual traﬃc in chip multiprocessors has a mix of two packet sizes. Short protocol packets and large data packets. Our design assumes constant ﬂit size packets. To accommodate packets of diﬀerent sizes we could use extra virtual channels. But adding virtual channels increases the router pipeline stages, which in turn increases the cost of the design. We propose to use separate networks for protocol and data packets [12]. Bandwidth is not equally partitioned between sub-networks, because the packets traversing the two sub-networks are of unequal size. Moreover, both the subnetworks carry similar load, considering a read write traﬃc, which consists of remote cache read/write requests and responses. Wide channels constitute the data network and narrow channels constitute the protocol network. Protocol packets of 16 bytes and data packets of 64 bytes with an additional 16 byte protocol header are considered. A 129-bit wide channel is considered which is partitioned into two sub channels. Channel used by protocol network is 22-bit wide and the channel used by data network is 107-bit wide. So, both the protocol packet and the data packet are split across 6 ﬂits. Since, the per hop router-to-router communication latency is 3 cycles, a 6 ﬂit packet ensures that the credit round trip latency is covered with a single packet buﬀer per input port. Figure 7: Example of starvation. N0 is continuously sending packets to N2 and because of prioritized arbitration N1 does not get a chance to send packets. 4.6 Fairness By using prioritized arbitration in the proposed design, fairness can become an issue. Since in-network packets have higher priority than the packets to be injected, packets from some nodes might experience more latency than others. One example of starvation is shown in Figure 7. Here, N0 is continuously injecting packets destined to N2 and hence N1 is starved. We could send an explicit signal from N1 to N0 requesting it to stall, until N1 can inject. But we already have a prevention slot mechanism which prevents nodes from injecting in some cycles. Prevention slots ensure that a node will not inject packets continuously for a very long time. Each node will be prevented from injecting once every n ∗ N cycles, where n is per-hop latency and N is the number of nodes in the ring. By properly tuning the prevention slot mechanism, we can avoid this scenario. If the prevention slot is moving in the direction of the ring, node N1 can be starved for indeﬁnite time. Let the prevention slot occurs at N0 at time t, then N0 cannot inject a packet in that slot. But a packet injected by N0 at time t − n can be in the input buﬀer of N1 , and so it cannot inject in the slot at time t. The slot moves in the direction of ring and it occurs at N1 at time t + n. Now, the input buﬀer of N1 will be empty, as N0 has not injected a packet in the previous slot, but the prevention slot prevents N1 from injecting. This can go on indeﬁnitely, starving N1 for a long time. Instead, if we have the prevention slot moving in the direction opposite to that of the ring, this problem can be solved. Now, the prevention slot ﬁrst occurs at N1 at time t. At time t + n, it occurs at N0 , preventing N0 from injecting a packet in that slot. So, at time t + 2n, input buﬀer of N1 is empty as N0 has not injected any packet in the previous slot, thus allowing it to inject a packet. 5. EXPERIMENTAL ANALYSIS In this section, we analyze the performance of the proposed design against a conventional input queued design for a 64 node (8 × 8) 2D torus network. We use a cycle accurate interconnection network simulator wherein all the components of a router are modeled in suﬃcient detail [6]. We ﬁrst compare the latency-throughput curves, for which the simulator is warmed up under load without taking any measurements until a steady state is reached. Then, the next set of packets are labeled and injected. Measurements are taken during the simulation, which runs until these labeled packets exit the system. We also compare the performance of the designs on synthetic workloads using closed loop simulations [2]. These workloads are used to model memory coherence traﬃc, where each node initiates 1000 transactions. A transaction comprises of a request from the source node and a response from the destination node on receiving this request. The overall workload completion time for the network is measured. In order to mimic the eﬀect of MSHRs, r outstanding requests per router node are allowed. Hence, when r outstanding requests are injected into the network for a node, it is restricted from injecting further requests until the response packets are received. Here the results are presented considering r = 4. As stated earlier in Section 4.5, we consider a data path of 22 bits for protocol network and of 107 bits for data network. Read request and write response are considered as protocol packets, on the other hand read response and write request are considered as data packets. A baseline router having 3 cycle latency and two virtual channels and using the routing algorithm described in [4] is considered. For a fair comparison, we consider the same implementation of sub-networks for the baseline design also. For latency-throughput comparison, we consider a bimodal distribution with equal number of protocol and data packets. 5.1 Latency and Throughput Comparison The load vs. latency curves for the baseline (BASE) design and the prevention ﬂow-control (PFC) design are shown in Figure 8. We vary the number of buﬀers for the BASE design in multiples of packet size. For the BASE router with 6 buﬀers per input port (b = 6), there is severe throughput degradation because of insuﬃcient number of buﬀers to cover the credit round trip latency. The BASE design needs as high as 4× buﬀering to match the throughput of the PFC design for tornado traﬃc. The BASE design with b = 24 achieves slightly higher throughput for uniform random traﬃc, but is not able to match the throughput of the PFC design for any other traﬃc pattern. As compared to the BASE, the PFC design also achieves as high as 36% reduction in zero-load latency at the same time giving up to 15% improvement in throughput. We also compare the PFC design with bubble ﬂow control (BUBBLE) mechanism [3], considering the same router pipeline as for the PFC router (Figure 9). The BUBBLE mechanism achieves high throughput under uniform traﬃc when having twice the number of buﬀers as compared to the PFC design. When both the designs have equal number of buﬀers, the diﬀerence in throughput is as low as 5%. It should be noted that, the BUBBLE mechanism has a lower bound of two packet buﬀers per input port and for injecting a packet it requires at least two free packet buﬀers in the dimension in which packet has to be injected. 5.2 Impact of Prevention Slots In order to evaluate the impact of prevention slot mechanism, the PFC and the BASE designs are compared, assuming that the BASE design also has single cycle router latency. Both the designs have equal number of buﬀers (b = 6). As shown in Figure 10, the overhead of prevention slots is very low and causes a slight increase in the zero-load latency. As the number of nodes in each dimension is large enough, the prevention slot occurs less frequently at each node. The BASE design achieves slightly higher throughput for uniform random traﬃc, whereas the PFC design achieves slightly higher throughput for tornado traﬃc. Hence, high throughput can be achieved using cut-through switching even with the overhead of prevention slots. In an 8 × 8 network, every node is prevented from starting to inject a packet into the network once every eight slots. So in the PFC design with the slot size of 3 cycles, injec(a) Uniform (b) Tornado (c) Bitcomp Figure 8: Load vs. Latency comparison of the BASE router to the PFC router for various traﬃc patterns. The number of buﬀers for the PFC router is kept constant at 6 ﬂit buﬀers per input port and the number of buﬀers for the BASE router is varied. (a) Uniform (b) Tornado (c) Bitcomp Figure 9: Load vs. Latency comparison of the PFC router to the BUBBLE router for various traﬃc patterns. The number of buﬀers for the BUBBLE router is kept constant at 12 ﬂit buﬀers per input port and the number of buﬀers for the PFC router is varied. (a) Uniform (b) Tornado (a) (b) Figure 10: Impact of prevention slots for uniform random and tornado traﬃc considering similar pipelines and constant buﬀering for both designs. Figure 11: Impact on (a) throughput of network and (b) standard deviation of latencies across all nodes, of change in direction of prevention slot mechanism under tornado traﬃc. tion bandwidth available to all nodes is reduced by only 4%, which leads to a negligible impact on performance. The effect of reduction in injection bandwidth can be more prominent in networks with smaller dimensions, leading to a signiﬁcant performance degradation. 5.3 Fairness Fairness can become an issue due to the prioritized arbitration. In Section 4.6, we proposed moving prevention slot in direction opposite to that of the ﬂow of traﬃc (OPDIR), rather than in the direction of traﬃc (SM-DIR), to improve fairness. We compare both the methods for tornado traﬃc as it is considered to be adversarial traﬃc for torus networks. Comparison of both methods is shown in Figure 11a. We can see that OP-DIR gives a slight improvement in throughput and average latency over SM-DIR at saturation. This improvement in throughput is because of the improved fairness across all nodes, which reduces maximum latency experienced by packets. Moreover, standard deviation of latencies across all nodes is reduced in OP-DIR and is comparable to that of the BASE design (Figure 11b). Figure 12: Synthetic workload comparison using closed loop simulation. Runtime of the BASE design is normalized to the runtime of the PFC design. The number of buﬀers per input port is kept constant at 6 for PFC, whereas it is varied from 6 to 24 for BASE. 5.4 Execution Time To emulate real world traﬃc, closed loop simulations are used to compare the two designs. Results are shown in Figure 12. For the PFC router, the number of ﬂit buﬀers per input port is held constant at 6 and the number of buﬀers for the BASE design is varied from 6 to 24. When using the same number of buﬀers, PFC achieves up to 54% reduction in execution time. Even with the BASE router having b = 24, which is 4× that of PFC, PFC achieves up to 13% reduction in execution time. 5.5 Power and Area For the network under consideration, we need a log 8-bit prevention slot detection counter and a log 3-bit start of slot detection counter for four output ports connecting to the adjacent routers. So the PFC design adds an overhead of 20 ﬂip-ﬂops per router. We compare the area and power consumption of the BASE and the PFC router using Orion2.0 [13]. We assume a 65nm technology at 2GHz and a data path width of 22 bits for protocol network and of 107-bits for data network. Our estimates using Predictive Technology Model [23] show that a signal takes two cycles to traverse the channel at 2 GHz. We assume 24 buﬀers per input port for the BASE router and 6 buﬀers per input port for the PFC router to approximately match the throughput of the two designs. The area comparison, normalized to the BASE router, is shown in Figure 13a. Since the BASE router uses 4× more buﬀering than the PFC router, the area consumed by the buﬀers is reduced by 75% and we get an overall reduction of 25% in the router area. The power consumption, normalized to the BASE router is shown in Figure 13b. We can see that the PFC router consumes 22% less power as compared to the BASE router, by reducing power consumption of buﬀers, clock and allocator. 6. RELATED WORK There have been many proposals for low latency router designs. Mullins et. al [19] propose a single cycle virtual channel router, based on speculation and precomputation. Kumar et. al [18] also propose a single cycle router based on advanced bundle signals, sent one cycle ahead of the packet. In these two designs, cost is paid in terms of increased complexity of the router by doing speculative allocation and adding advanced bundle setup and conﬂict detection logic. A low cost single cycle router, including link traversal, is proposed (a) Area (b) Power Figure 13: Area and power comparison of the BASE router with the PFC router. for 2D mesh networks [15]. It uses a partitioned crossbar and prioritized arbitration to achieve single cycle router, but cannot be used for torus networks as there is no provision for deadlock avoidance. Balfour et. al [2] show that splitting a network into two sub-networks improves performance as well as area and power eﬃciency. They divide bandwidth equally among two sub-networks and show that this achieves better performance than dividing traﬃc based on traﬃc class. In the proposed design, separate sub-networks are used for separate traﬃc classes and the bandwidth is divided proportionately based on the requirements of each traﬃc class. Various techniques have been proposed for deadlock avoidance and recovery in torus networks. Dally et. al [4] propose usage two virtual channels to break the cyclic dependency and hence avoid deadlock. Duato [8] provides the necessary and suﬃcient conditions for deadlock freedom in cutthrough and, store and forward networks. A ﬂow control mechanism has been proposed in [3] for deadlock avoidance which does not require any virtual channels. Their design requires a minimum of two packet buﬀers per input port and explicit control signals to avoid starvation of nodes because of prioritized arbitration. Puente et. al [20] propose an adaptive router based on bubble ﬂow control. They use two virtual channels, one for adaptive routing and one escape virtual channel based on bubble ﬂow control. Duato et. al [9] show that virtual cut-through routers perform better than wormhole routers when packet sizes are small and buﬀering requirements are low. 7. CONCLUSION AND FUTURE WORK We presented a novel ﬂow control mechanism to avoid deadlocks for torus networks. Injection restriction constraints allow nodes to inject packets only at the start of slot durations. Along with these constraints, prevention slots are used which prevent a node from injecting packets into the network to ensure deadlock freedom. These slots move in a round robin fashion across nodes and ensure that there is at least one node, in every circular dependency set of nodes, not injecting any packet in an injection cycle. By using the proposed prevention ﬂow-control (PFC) mechanism with cut-through switching, the need for virtual channels to avoid deadlock is eliminated. Simple priority based arbitration, coupled with the elimination of virtual channels simpliﬁes the design of switch allocator for the PFC router. Also, reducing minimum buﬀering requirements to a single packet buﬀer per input port allows simpliﬁcation of input port of the router. All the above enhancements allowed us to reduce the pipeline stages of the router from three in the baseline down to one in the PFC router when a packet is traveling in the same dimension. Since priority in switch allocation is given to in-ﬂight packets moving in the same dimension, it can cause starvation of certain nodes. We showed that by moving the prevention slot in the opposite direction to that of the data ﬂow, fairness can be improved and starvation can be avoided. Experimental analysis showed signiﬁcant improvement in performance and power consumption over the baseline design. We also compared the PFC design with bubble ﬂow control and showed that the diﬀerence in performance is small. Our current design does not address some issues. There is a uniform reduction in injection bandwidth of all nodes due to the prevention slots. This uniformity in reduction of injection bandwidth is not necessary to avoid deadlocks and can lead to an avoidable degradation in performance when only certain nodes in the network are injecting packets. Our design assumes a globally synchronous network which might not necessarily be the case with designs that employ DVFS or GALS architecture. Moreover, message level deadlocks have not been addresses in this design. We plan to address these issues, without adding much complexity in our design, as part of our future work. 8. ACKNOWLEDGMENTS This work is supported in part by grant from Defence Research and Development Organization (DRDO) under IITMDRDO MoC. 9. "
A distributed and topology-agnostic approach for on-line NoC testing.,"A new distributed on-line test mechanism for NoCs is proposed which scales to large-scale networks with general topologies and routing algorithms. Each router and its links are tested using neighbors in different phases. Only the router under test is in test mode and all other parts of the NoC are in functional mode.
Experimental results show that our on-line test approach can detect stuck-at and short-wire faults in the routers and links. Our approach achieves 100% fault coverage for the data-path and 85% for the control paths including routing logic, FIFO's control path and the arbiter of a 5x5 router. Synthesis results show that the hardware overhead of our test components with TMR (Triple Module Redundancy) support is 20% for covering both stuck-at and short-wire faults and 7% for covering only stuck-at faults in the 5x5 router. Simulation results show that our on-line testing approach has an average latency overhead of 20% and 3% in synthetic traffic and PARSEC traffic benchmarks on an 8x8 NoC, respectively.","A Distributed and Topology-Agnostic Approach for On-line NoC Testing Mohammad Reza Kakoee DEIS University of Bologna Bologna, Italy m.kakoee@unibo.it Valeria Ber tacco CSE University of Michigan Ann Arbor, USA valeria@umich.edu Luca Benini DEIS University of Bologna Bologna, Italy luca.benini@unibo.it ABSTRACT A new distributed on-line test mechanism for NoCs is proposed which scales to large-scale networks with general topologies and routing algorithms. Each router and its links are tested using neighbors in different phases. Only the router under test is in test mode and all other parts of the NoC are in functional mode. Experimental results show that our on-line test approach can detect stuck-at and short-wire faults in the routers and links. Our approach achieves 100% fault coverage for the data-path and 85% for the control paths including routing logic, FIFO’s control path and the arbiter of a 5x5 router. Synthesis results show that the hardware overhead of our test components with TMR (Triple Module Redundancy) support is 20% for covering both stuck-at and short-wire faults and 7% for covering only stuck-at faults in the 5x5 router. Simulation results show that our on-line testing approach has an average latency overhead of 20% and 3% in synthetic trafﬁc and PARSEC trafﬁc benchmarks on an 8x8 NoC, respectively. Categories and Subject Descriptors B.7.3 [Hardware]: Integrated circuits - Reliability and Testing [ Built-in tests, Error-checking] General Terms Design, Reliability 1. INTRODUCTION WORK AND RELATED Embedded systems have been shifting to multi-core solutions (Multiprocessor System-on-Chips; MPSoCs). A clear example of high-end MPSoCs are the products offered by Tilera [22] where multi-core chips provide support to a wide range of computing applications, including high-end digital multimedia, advanced networking, wireless infrastructure and cloud computing. Main microprocessor manufacturers are also shifting to chip multiprocessors (CMPs) for their latest products. In CMPs many cores are put together in the same chip and, as technology advances, more cores are being included. Recently, Intel has announced a research chip with 48 cores, under the Tera-scale Computing Research Program [20]. Previously, Intel also developed a chip proPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS’11, May 1-4, 2011 Pittsburgh, PA, USA. Copyright 2011 ACM 978-1-4503-0720-8 $10.00 . totype [21] that included 80 cores (known as TeraFlops Research chip). Current trends indicate that Multi-core architectures will be used in most application domains with energy efﬁciency requirements exceeding 10GOPS/Watt. However, aggressive CMOS scaling accelerates transistor and interconnect wearout, resulting in shorter and less predictable lifespans for CMPs and MPSoCs [15]. It has been predicted that future designs will consist of hundreds of billions of transistors, with upwards of 10% of them being defective due to wearout and process variation [13]. Consequently, in order to support the current technology trends we must develop solutions to design reliable systems from unreliable components, managing both design complexity and process uncertainty [14]. Network on Chip (NoC), a high performance and scalable communication mechanism, is being increasingly investigated by researchers and designers to address the issues of interconnect complexity in both CMPs and MPSoCs [16]. The reliability of NoC designs is threatened by transistor wearout in aggressively scaled technology nodes. Wear-out mechanisms, such as oxide breakdown and electromigration, become more prominent in these nodes as oxides and wires are thinned to their physical limits. These breakdown mechanisms occur over time, so traditional post burn-in testing will not capture them. NoCs provide inherent structural redundancy and interesting opportunities for fault diagnosis and reconﬁguration to address transistor or interconnect wearout. A Network on Chip consists of routers, links, network interfaces (NIs), and cores. Failures can occur in any of these components. In this work we focus on failures in routers and links, and propose an on-line functional testing solution for the NoC infrastructure. A number of works has evolved NoC reconﬁguration to counteract faults in both routers and links [4, 5, 6, 7, 8, 9, 10]. For instance, routing algorithms for fault-tolerant networks have been extensively explored for network-level reconﬁguration. These algorithms direct trafﬁc around failed network components to avoid network deadlock. However, in order to take countermeasures against NoC faults, we must ﬁrst detect and diagnose them. Most of the works proposed for fault-tolerant NoCs do not address the testing mechanism and how they ﬁnd faulty components. Some of the them rely on off-line testing mechanisms, such as BIST and scan chains, which require external test sources, are not scalable and cannot be applied while the system is in operation. Others rely on traditional error detection and correction mechanisms, such as CRC and parity checkers, to detect data-path faults in both links and routers [3, 11, 12]. However, their fault coverage is very limited and, more importantly, they cannot be used for fault detection in control paths of the router. Recently, Lubaszewski, et al. in [1, 2] proposed a new post burnin testing for NoCs, which is based on the at-speed functional testing of several 2x2 meshes in an NxN NoC. They test each 2x2 mesh using a set of test sequences and patterns. However, as they showed in [2], the 2x2 mesh can give good fault coverage for the links but not for the routers, especially for the routing logics, FIFO’s control paths and arbiters. To obtain higher fault coverage for the routers they added more than 12 other mesh conﬁgurations including 3x1, 1x3, 2x2, and etc. However, having those additional conﬁgurations is not efﬁcient for on-line testing and keeps several switches busy in testing procedure, impacting the NoC’s performance and packet latency. Moreover, their approach is suitable for manufacturing testing using ATE (Automatic Test Equipment), and implementing it in hardware leads to overhead especially with its enhanced version. Motivation and Contribution. The motivation behind this work is having a distributed and scalable on-line testing mechanism for NoC that can detect faults in both data-path and control-path of the routers as well as the links with a high fault coverage and with minimum hardware overhead. In this paper we propose a scheme for on-line detection and diagnosis of faults in NoC’s routers and links. First, we propose a new test architecture, which can be implemented in routers and NIs with small hardware overhead. Similar to the work proposed in [2] we use Test Pattern Generator (TPG) and Test Response Analyzer (TRA). However, to have minimum performance overhead on the NoC in on-line mode we take advantage of TPG and TRA in both router and NI. We also propose a hardware-efﬁcient Fault Diagnosis Module (FDM) in each router, which can diagnose the location of the fault. Second, we propose a new test sequence to test each router. Unlike the work proposed in [2], we do not use 2x2 meshes to test the entire NoC, but we test each router separately and using its neighbors in on-line and at-speed functional mode. We use logic level fault models and the corresponding system-level failure modes to better tune the test pattern sequences. Note that, in this work, we are seeking the integration of the test of interconnects and routers, at the lowest possible cost, but considering logic level fault models. We try to cover most of the logic faults in the entire router and in all data and control wires that build the NoC infrastructure. Experimental results show that our functional test approach can detect all the stuck-at and short-wire faults in both inter-router and intra-router wires. The results also show that, our approach has a 100% fault coverage for the data-path and 85% for the control paths including routing logic, FIFO’s control path and the arbiter of a 5x5 switch. Synthesis results show that the hardware overhead of our test components with TMR (Triple Module Redundancy) support is 20% for covering both stuck-at and short-wire faults and only 7% for covering stuck-at faults. Simulation results show that our on-line testing approach has an average latency overhead of 20% and 3% in the synthetic trafﬁc and real trafﬁc benchmarks on an 8x8 NoC, respectively. 2. NOC CASE STUDY In this work, we base our analysis on a packet-switching network model, called Xpipes, introduced in [18]. Each router in the Xpipes network is composed of ﬁve input and ﬁve output ports as shown in Figure 1. The switch contains control-paths and data-paths as shown in the ﬁgure. One pair of input/output ports is dedicated to the connection between the router and the core, while the remaining four pairs connect the router with the four adjacent routers. Without lose of generality we consider a 2-D mesh as our topology with XY routing where a packet is ﬁrst routed on the X direction and then on the Y direction before reaching its destination. However, our testing methodology can work with any topology and routing algorithm. Switching is based on the wormhole approach, where a packet is partitioned into multiple ﬂits (ﬂow control units): a header ﬂit, a set of payload ﬂits followed by a tail ﬂit. Flit is the smallest unit Figure 1: Xpipes switch used in this work. (RC: Routing Computation) over which the ﬂow control is performed and its size equals the channel width. For the case-study NoC topology, the communication channels between routers are deﬁned to be 32-bit wide. 3. FAULT SPACE MODEL In this section we describe the fault models we used for this work. We deﬁne fault a hardware failure in any location of the NoC. Any fault in the logic may have a system failure which shows itself in the NoC’s operation. We also use system level failure modes to propagate the effect of any logic level fault to a higher level of abstraction. These system level failures help us to better generate test patterns for improving the logic level fault coverage. 3.1 Fault Location Considering a NoC as a network of routers and NIs, a hardware fault can occur in different places including: routers, links, and NIs. In this work we focus on the routers and links. We assume that NIs are already tested or can be tested together with cores. Faults in the links can be detected using a small number of test patterns. However, to fully test the routers several complicated test patterns are needed. Faults in router can occur in two main parts: data-path and control-path. Data-path contains ﬂip-ﬂops in FIFOs, router wires, and output MUXes; control-path includes routing logics, FIFO’s control part, and arbiters. In this work we cover faults in both control and data paths. 3.2 Gate-Level Fault Model To see the effectiveness of our testing mechanism, we consider realistic gate level fault models. We consider two types of gate level faults: stuck-at faults, and bridging faults. Stuck-at Fault: Stuck at faults are the most common faults at the gate level. They are actually shorts to VDD (stuck-at-1) and shorts to Ground (stuck-at-0). Both routers and links are subject to these types of faults. Any wire in the links is susceptible to stuckat faults. Stuck-at faults can occur in any component in the router including FIFOs, routing logics, MUXes, arbiters, and wires. In this work we cover stuck-at faults in both routers and links. Bridging Fault: Bridging faults can occur mostly on the interrouter links as well as the internal wires of the router. They are shorts between wires of a link. These types of faults are different from stuck-at faults and need different test patterns to be detected. In this work, we also cover bridging faults in links. 3.3 System-Level Failure Modes As our approach is at-speed functional testing, we need to deﬁne some system level failure modes to better tune the test patterns which can give us higher gate level fault coverage. Note that, these system level failure modes help us only to generate better test patterns, but the quality of testing approach is measured considering gate level fault coverage. Gate level faults may have different effects on the functional behavior of the system. They can put the system into a failure mode. In case of routers and links, as proposed in [17], we categorize the system level failures into four modes: dropped data, corrupted data, misrouting, and multiple copies. We will describe each failure mode in more details in the following. Dropped Data: In this failure mode the switch receives the data, but never sends it to the intended output port. Dropped data failure can cause by any fault in the control-path of FIFOs, arbiters, or multiplexers of the switch. For instance, consider a case where the head counter of the FIFO is faulty and its current value is increased by 2 or more each time a buffered ﬂit is being removed from the head of FIFO to be sent to its appropriate destination. In this situation, some of the ﬂits in the FIFO will be lost and never exit the switch. Faults in arbiters may cause the select signals of the output multiplexers to not be activated and, therefore, the ﬂit will be dropped. If the output multiplexer is faulty, e.g., one or more bits of its select signal are stuck at one or zero, one of its unintended inputs is selected and consequently the original ﬂit will be dropped. Detecting faults related to this failure mode is not easy and requires several test patterns probing all possible combinational paths. Corrupted Data: In this failure mode the switch receives the data, but it corrupts the data and sends it to the intended output port. This failure mode can also happen on the links. Any fault in data-path of FIFOs (ﬂipﬂops), multiplexers, links, and internal wires may lead to this failure. As this failure mode can only be created by faults in the data-path of the switch and links, faults related to this failure are easier to detect compare to those related to the dropped data. Misrouting: Misrouting is the result of the faulty behavior of the switch router. Assume that the router has a faulty behavior and makes wrong decisions while routing its incoming data. As these faults happen in the control path of the switch, several test patterns are needed to cover all possible routing paths. Faults in the header bits of the FIFOs which are related to the destination address can also create this failure as they may change the intended output port. However, these faults are already covered using the patterns related to the corrupted data failure. Multiple Copies: Multiple copies in time failure are originated from faults in FIFOs control-path. Consider a FIFO with a faulty “empty” signal. The false empty signal may cause the FIFO send its old data out of the switches port. This old data is usually an earlier packet’s ﬂits, and sending the same sequence of ﬂits out of the switch’s ports leads to repetition of a packet, i.e., multiple copies in time. The faults related to this mode can be detected easily using Test Response Analyzer (TRA) which checks the order of received ﬂits in a test packet. 4. THE PROPOSED NOC ON-LINE TESTING We propose a technique that is capable of detecting stuck-at and bridging faults in routers of any network with minimum overhead on NoC performance. Figure 2 shows the overall architecture of our on-line NoC testing on a Mesh. As can be seen, in our approach only the Router Under Test (RUT) and its links will put in the test mode, while other parts of the network are in operational mode. All the packets that want to traverse through RUT will be held in the RUT’s neighbors until the test is ﬁnished. This may have a little latency degradation for Figure 2: Overall architecture of our on-line NoC testing on Mesh. Each router which gets token can start testing itself with the help of its neighbors. Only one router and its links are in the test mode and others are in the operational mode. some packets. However, we use a token-based technique to make sure that only one router is under test at any given time. In other words, two or more routers cannot be put in test mode simultaneously. Token is simply one bit traversing in the network. When a router receives the token, it decides whether it wants to test itself or not; this can be done using a counter in the switch or a request from the core connecting to it. After ﬁnishing the testing, the current switch passes the token to the next switch. As token is only one bit, we can use TMR for its reliability to make sure all the routers receive the token correctly. When a router is in test mode, it should be tested by its neighbors. Each router contains three main test blocks: Test Pattern Generator (TPG), Test Response Analyzer (TRA), and Fault Diagnosis Module (FDM). TPG is responsible for generating test patterns and sending them to the neighbor router which is under test (RUT). TPG of the local port which is inside the router under test’s NI sends test packets to the neighbors. Test response analyzer receives test patterns from the neighbor that is under test and analyzes them to detect any fault in the corresponding router and links. TRA of the local port which is inside router under test’s NI, receives test packets sent by neighbors and veriﬁes them. Fault diagnosis module gets the status signals from the neighbors’ TRAs as well as the local TRA and diagnoses the faulty channel. There is only one fault diagnosis module in each router. Since only one router can be tested at any given time, TPG and TRA can be shared between all the ports (except the local port). Therefore, for our testing mechanism, we need one pair of TPG and TRA inside each router and each NI. Also, we need one FDM for each router. 4.1 Test Architecture for Fault Detection As mentioned before, in our testing approach each router is tested with the help of its neighbors. Thus, this mechanism is scalable to any size of the network with any topology. The architecture of testing one router with four neighbors and one NI is shown in Figure 3. When a router is under test, all its neighbors send the test packet generated by their TPG to it. Also, RUT sends test packet to the neighbors using the TPG inside its NI. This enables us to test the local channels connected to the NI as well. At all output ports (except the local) of the routers, we add a 2x1 multiplexer which selects data from either crossbar or TPG. This additional multiplexer is quite small and we use TMR for its reliability. TPG generates a sequence of predeﬁned packets based on the test phases. We will describe test phases in details in Subsection 4.3. The packets generated in different phases are the same in terms of data ﬂit. The header ﬂit of the packet depends on the test phase. Therefore, the same data is sent to different destinations in different phases. The rationale behind this is that the data ﬂits in each packet are responsible to cover the faults in data-paths and different phases to decrease the latency overhead as well. We will show this tradeoff in more details in experimental results section. Figure 4 shows the packet format which is generated at each phase of testing in TPG. Note that, in TRA the same packet format is veriﬁed at each phase of the testing. Since all packets are sent with headers and tails, faults may affect those ﬂits, thus modifying the packet routing. When this occurs, the packet can be routed to any other node of the NoC or a tail ﬂit may not be received. In both cases, TRA at the target node will not receive the packet or the tail within a predeﬁned time interval, thus reporting a time-out error. Since the network is in the normal mode of operation during the test, one might consider the possibility of other errors caused by the logic fault, such as a deadlock, in addition to packet losses and payload errors. We note that a fault cannot cause a deadlock within our test conﬁguration because of the packet format and paths established during testing. Figure 4: Format of test packet generated by TPG and veriﬁed by TRA at each phase of testing. This format is ﬁxed and independent from network topology. 4.3 Test Phases As described earlier with the speciﬁed packet format we are able to detect all stuck-at and bridging faults in the data-path traversed by the packet. However, to provide coverage for faults in the control-path we need different test rounds or phases. Each phase is responsible to cover some of the combinational paths inside the router while the data path is being tested multiple times. This phases depend on the network topology and the RUT’s number of connected ports. In the following, we describe the suitable phases for a router with four neighbors and one NI. In Subsection 4.5 we discuss how we can modify them for other topologies. For a router with four neighbors and one NI, we consider 9 different phases to cover all the control paths including different routing, arbitration and FIFO’s control. Figure 5 shows these phases. Arrows in the ﬁgure show the source and destination of the test packet. At each phase, one test packet is sent from source to the destination through RUT (R0 in this ﬁgure) and its links. Those test packets that come from the neighbors are generated at the output port of the neighbors using their TPGs; while, packets whose sources is RUT are generated in the TPG of the RUT’s NI. Therefore, packets originated from RUT also cover the local input channel. Similarly, those packets targeting the neighbors are veriﬁed immediately at the input port of the neighbor and before the FIFO. While, packets whose destinations is RUT are veriﬁed in the RUT’s NI. Thus, packets targeting RUT cover the local output channel as well. As can be seen in Figure 5, test phases are chosen in such a way that they cover all functional failures of the router related to the Misrouted Data, Dropped Data, and Multiple Copies failure modes which are the results of logic faults in the RC, FIFO’s control path, or the arbiter. Since for each input port there could be 4 different destinations, we need at-least 4 test phases to fully cover all the routing logics. Test phases 1 to 4 cover all different routing possibilities inside the switch without any arbitration policy. These test phases (1 to 4) are chosen so that we can test all RC blocks inside the router with minimum number of tests. However, in phases 1 to 4 the packet’s destination (stored in header ﬂit) is chosen so that for each output port of the RUT there is only one request. Thus, although some parts of the Figure 3: Each router and its links are tested using neighbors. (TPG:Test Pattern Generator, TRA: Test Response Analyzer, FDM: Fault Diagnosis Module) of testing cover control paths’ faults. We will describe the format of test packet in more details in next subsection. TRA veriﬁes whether the incoming data corresponds to the predeﬁned sequence generated by TPG or not. TRA sends a 2-bit signal (TR) to the RUT which shows the result of pattern checking. RUT receives TR signal from all neighbors and its NI , and diagnoses the faulty channel using Fault Diagnosis Module (FDM) which is described in Section 4.4. In addition to the corrupt data checking, TRA is able to check the dropped packet failure. In each phase of the test, TRA waits for a speciﬁc pattern to arrive. If it receives the packet within a limited number of cycles, it checks the data and sends the results to the RUT. If TRA does not receive any packet, it generates a time-out error. For time-out checking we use a 5-bit counter inside TRA. If the counter reaches the limit, TRA generates a time-out signal (one bit of TR) and sends it to the RUT. Table 1 shows the meaning of different bits of TR signal generated by TRA. Note that, since TR is only 2 bits, parity checks can be used for its reliability with a small area overhead. Table 1: 2-bit TR signal generated by Test Response Analyzer TR (bits) 00 01 10 11 Meaning No Error - Data is received correctly Error - Data is received but is corrupted Error - Data is not received (Dropped) No Info - (reserved for unconnected ports) 4.2 Test Packet Format As mentioned earlier, TPG generates the same packet targeting different destinations at different phases during test of the router. The format of the data ﬂit in the packet is chosen so that it covers all stuck-at and bridging faults in the data-path which includes links, FIFO’s ﬂipﬂops, switch internal wires, and output MUXes. This format is independent from the topology of the network and from the RUT’s number of ports. Two data ﬂits of All-One and All-Zero are enough to cover all stuck-at faults in the data-path. However, to detect all bridging faults, we need a set of Walking-One data ﬂits 1 . Therefore, the number of data ﬂits required for bridging faults is equal to the ﬂitwidth i.e. 32 in our switch. As a result, to cover all stuck-at and bridging faults in the data-path we need at least 34 data ﬂits. If we target only stuck-at faults, the number of data ﬂits will decrease to 2. This can help to reduce the overhead of both TPG and TRA and 1Walking-One refers to a set of data patterns where, in each data word, a single bit is set to one and the rest to zero. ing the input link and the FIFO, and the output channel as the path covering internal link, output MUX, and the output link. Figure 6 shows input and output channels for West and East ports. Using 2bit TR signals, FDM is able to ﬁnd out which channel (either input or output) is not faulty. Figure 6: Channels in which Fault Diagnosis Module is able to diagnosis data-path faults. For a 5x5 router, we have a 10-bit register which shows the status of each channel; we name this register CSR (Channel Status Register). Each bit of CSR corresponds to one channel as shown in Figure 7. FDM receives TR signals and in each phase of the test updates CSR. FDM is based on ﬁnding non-faulty channels. As seen in Figure 5, each test packet covers 2 channels considering local channels. Thus, if a TR signal shows a corrupt error (01) FDM can not diagnose which of the two channels is faulty. However, if a TR signal shows no error (00), FDM can diagnose that both channels are non-faulty. At the beginning of the test, FDM sets all the bits in CSR to ’0’ meaning all channels are faulty. During the test, when FDM receives a TR signal (from either any neighbor or its NI) which shows no-error (00) it sets the corresponding bits of the CSR to ’1’ which are related to the channels in which the packet has traversed meaning they are non-faulty. If the bit is already set to ’1’, it does not change it. At the end of the testing, those bits of CSR which are ’1’ show the non-faulty channels. FDM is a simple combinational block which decides based on the TRs and the testing phase. FDM is also able to diagnose which RC or arbiter is faulty. We have two 5-bit registers showing the status of RCs and arbiters in a 5x5 router; we call these registers RSR (Routing Status Register) and ASR (Arbiter Status Register) respectively. At the beginning of the test FDM sets all bits of both registers to ’1’ meaning all the RCs and arbiters are non-faulty. During the ﬁrst 4 phases, if the FDM receives a TR signal of dropped error (10) it sets the corresponding bit in RSR to ’0’ which is decided based on the test phase. During phases 5 to 9, if FDM receives a TR of dropped error it sets the corresponding bit in ASR to ’0’. As mentioned earlier, if TRA detects a time-out error in the ﬁrst 4 phases, it does not check it again;therefore, if there is a fault in one RC, it does not have any effect in the results of phases 5 to 9. Thus, if TR shows a dropped error (10) in phases 5 to 9 it is related to the arbiter but not to the RC. 4.5 Testing of Custom Topologies Our testing methodology is based on testing each router using its neighbors and the NI connected to it. Each router receives TRs from the neighbors as well as its own NI and updates the related status registers. Considering the switches at the boundary of the mesh, there are one or two ports which are not connected to any neighbor. We connect TRs of those ports to “11” meaning no information is available on those speciﬁc ports. FDM module does not update anything based on the TR of those unconnected ports. On the other hand, neighbors that are connected to the boundary switches expect test packet from those ports of the RUT which are Figure 5: Nine test phases for a router with 4 neighbors and one NI. Phases 1 to 4 cover data-path and all routings without any arbitration. Phases 5 to 9 cover arbiters as well as FIFO’s control logic. arbiter are being tested during these phases, these phases (1 to 4) are not sufﬁcient to test the functionality of the arbiters. Also, they cannot fully test the control logic of the FIFO. The FIFO’s control logic decides based on the arbiter grants. During these test phases the arbiter grant is always given to the requester immediately and, therefore, the output’s busy signal which goes to the FIFO’s control logic is always zero. Thus, in these phases FIFO’s control logic always pop the data out of the buffer without waiting. We need more test vectors to put the FIFO in the waiting state. Both arbiter an FIFO’s control block can be covered by a set of test packets targeting the same destination. As we have ﬁve output and input ports, we need at-least 5 test phases to cover all arbiters and FIFOs. Test phases 5 to 9 are created to perform this task. As can be seen in Figure 5, at each phase of this set (phases 5 to 9) one arbiter together with 4 FIFOs are tested. At the end of these phase, each arbiter is tested once and each FIFO (both control and data paths) is tested four times. These test phases (5 to 9) are mainly responsible to cover the Dropped Data, Misrouting and Multiple Copies failure modes. In the TRA, at each phase we check whether all the packets are arrived or not. In these phase of the test, TRA should receive 4 consecutive packets from four different sources provided that there were no time-outs in the ﬁrst 4 phases. If it does not receive the related packets it means that the arbiter has not given the grant to one of the requests and TRA generates time-out error. We note that, if there are time-outs in the ﬁrst 4 phases, TRA does not check it again in phases 5 to 9 and only validates those packets that suppose to arrive. 4.4 Fault Diagnosis Using TPG we generate test packets in different test phases and in TRA we verify the incoming packet and detect if there is any error in it (either corrupted data or misrouted). TRA sends the results of checking to RUT using 2-bit TR signal. However, we still need a mechanism to diagnose the location of the fault. We perform this using Fault Diagnosis Module (FDM) inside RUT. For data-path faults which are related to the Corrupted Data failure, FDM is able to diagnose fault location in the input or output channels of RUT. We deﬁne input channel as the data-path cover5. EXPERIMENTAL RESULTS To evaluate our on-line testing methodology, we used Xpipes [18] which is a packet-switching NoC model implemented in Verilog. The switch is conﬁgured to have 5 input and 5 output ports with the ﬂit width of 32 bits and the buffer depth of 2. We used Synopsys design compiler for hardware synthesis and Synopsys Tetramax for logic fault simulation. All designs are mapped on CMOS 65nm technology from ST-Microelectronics. 5.1 Hardware Overhead First, we implemented both TPG and TRA in Verilog to see their hardware overhead on a 5x5 router. As mentioned earlier, we can have one TPG and one TRA for the whole switch. We also need one pair of TPG and TRA for each NI. However, TPG and TRA of the NI are simpler than those of the switch because the sequence of test generated/veriﬁed by TPG/TRA inside the switch depends on the port while the sequence of those inside NI is ﬁxed. As we mentioned earlier, TPG and TRA are the critical components and we need to use robust techniques to make them reliable. Since these module are quite small with respect to the switch and NI, we can use TMR for this purpose. We implemented two kinds of TPG and TRA. One pair with allone, all-zero, and walking-one sequence and another pair without walking-one sequence to cover only stuck-at faults. The synthesis results are shown in Tables 2 and 3. As can be seen, if we want to cover both stuck-at and bridging faults in the 5x5 router the area overhead of all TPGs and TRAs with TMR is about 58% which is pretty high. This is due to the long walking-one sequence which needs a shift register. However, if we target only stuck-at faults the area overhead with TMR is 16% which is acceptable considering TMR support for reliability. Table 2: Synthesis results of TPG and TRA with walking-one sequence (Area of 5x5 Switch + NI = 8766) Module Area Switch TPG 413 NI TPG 363 Switch TRA 472 NI TRA 410 All TPG + TRA 1658 Area Area (with TMR) with TMR Overhead on 5x5 Switch + NI 1280 14% 1119 12% 1463 16% 1270 14% 5132 58% Table 3: Synthesis results of TPG and TRA without walkingone sequence (Area of 5x5 Switch + NI = 8766) Module Area Switch TPG NI TPG Switch TRA NI TRA All TPG + TRA 121 73 140 117 451 Area Area (with TMR) with TMR Overhead on 5x5 Switch + NI 375 4% 240 2.5% 462 5% 366 4% 1443 16% We also implemented the Fault Diagnosis Module (FDM) together with status registers in Verilog and synthesized them. Based on our synthesis results, the area overhead of the FDM module with TMR is 10% on the Xpipes switch. Considering NI and switch together, the area overhead of FDM with TMR is only 7%. Therefore, the area overhead of all modules including TPGs, TRAs, and FDM with TMR support is 65% to cover both stuck-at and bridging faults and 23% to cover only stuck-at faults. Note that as the area of switch is much less than that of the IP cores,the overhead on the whole NoC including IPs is very small. For instance, compare to Plasma [27], a small synthesizable 32-bit Figure 7: FDM gets TR signals and updates CSR, RSR, and ASR. not connected to any other router. Therefore, they generate timeout error for those speciﬁc ports and RUT sets the related bits of RSR to ’0’ meaning those ports that are not connected to anywhere are faulty. This does not have any effect on the functionality of the router as those ports are not used. Based on this mechanism, our testing technique can be used for any topology and any routing. TPG and TRA modules should be modiﬁed based on the routing algorithm. TPG should send test packet to all output ports of the RUT in different phases. If the routing algorithm and the maximum number of ports in the switch are known, TPG and TRA are the same for all routers and independent from the topology. However, they can be optimized based on the NoC topology to reduce their hardware overhead. For example, TPG does not need to send the test packet to those ports of the RUT which are not connected to any other router. Note that, sending test packet to those ports does not have any effect on the related bits of status registers because the TRs of those port are already connected to “11”. Also, TRA does not need to check the time-out error for those input ports of the RUT which are not connected to any other routers. Moreover, CSR, RSR and ASR can be optimized based on the number of connected ports. As mentioned earlier, test phases are chosen so that they can cover all the functional paths of the router. They depend on the number of ports in each router. Phases shown in Figure 5 are suitable for maximum 5 input and 5 output ports. However, if a router inside the NoC has more ports, we need to deﬁne more test phases to cover those additional ports. Note that, for switches with less than 5 ports, the phases shown in Figure 5 are more than enough and may be reduced to a smaller number for optimizing test time and hardware cost. For instance, Figure 8 shows the optimized test phases for a 2x2 switch. Figure 8: Test phases for a router with 2 neighbors and one NI. Phases 1 and 2 cover data-path and all routings without any arbitration. Phases 3 to 5 cover arbiters as well as FIFO’s control logic. RISC microprocessor, the switch area is 1/5 of the processor area. Therefore, considering switch, NI and a small sized IP core the area overhead of all modules with TMR is 21% to cover stuck-at and bridging faults and 8% to cover only stuck-at faults. 5.2 Fault Coverage We used Synopsys Tetramax to evaluate the fault coverage of our testing approach. We synthesized the Xpipes switch and simulated our 9 testing phases on it. Then we collected the corresponding VCD ﬁle to perform fault simulation. Table 4 shows the number of stuck-at faults related to each component in the Xpipes switch. As it was expected, the number of faults inside each component is proportional to the area of that component. Table 4: Number of faults of different components inside Xpipes switch extracted from Tetramax Component Routing Logics (RCs) Arbiters Buffers Output MUXes other gates Entire Switch % of the Area 9% 20.5% 51.5% 19.5% 0.5% 100% # of Faults % of faults 1980 11.5% 4342 25.4% 5820 34% 4722 27.6% 80 0.1% 17080 100% We applied the VCD vectors obtained from the simulation of 9 phases on the synthesized design in Tetramax to see the fault coverage of the vectors. Figure 9 shows the related results. These results are for stuck-at faults. Note that, for bridging faults, the test vectors give us 100% coverage on the links in the ﬁrst 4 phases. As can be seen, the fault coverage of the entire switch for the ﬁrst four phases of the test is 68% which is not quite good. After applying the next 5 phases (total 9 phases) we could improve it to 85% which is an acceptable coverage for a functional test without any traditional test technique like scan. Figure 9: Stuck-at fault coverage of our on-line functional test approach. Note that, the achieved fault coverage can still be increased by adding to the test phases other network functional conﬁgurations (arbitration possibilities that were not applied, for example). However, insisting in a completely functional test approach will require an increasing number of test conﬁgurations and phases, for a decreasing number of detected faults. The trade-off seems to be not in favor of pushing further a functional approach. From the point of view of a functional test, about 10% of the remaining faults are undetectable because they are related to unreachable control states. Then, it is worthy looking at the implementation of short scan chains to further improve the router fault coverage. Since FIFOs and MUXes are almost fully covered by the functional test proposed here, the scan costs related to silicon overhead, test data volume and test application time will be greatly reduced. 5.3 On-line Testing Evaluation in Simulation To see the effect of our on-line testing mechanism on the latency of the packets, we performed system simulation. We used Noxim [19], a cycle accurate NoC simulator implemented in SystemC. The switch model in this simulator has a 2-stage pipeline and therefore has 2 cycles minimum latency. We extended it for performing our on-line testing approach. We periodically place a switch under test and hold all the packets traversing it. This is performed by arbiters in the neighbors. They do not give grant to the packets traversing the switch under test, until the test is ﬁnished. We compared the average latency of the packets in different synthetic and real trafﬁcs with and without on-line testing. We implemented a token-based mechanism in the simulator to avoid having multiple routers under test at the same time. We have a counter inside each router which counts the number of ﬂits forwarded by the switch. When the counter reaches the threshold value if the router has the token it can start testing. We swept the threshold value from 1,000 to 20,000. The simulation has been performed on an 8x8 Mesh with XY routing algorithm. Figure 10 shows the average packet latency of different synthetic trafﬁcs in the NoC while using our on-line testing mechanism. We performed simulation with and without walking-one sequence which is for covering bridging faults. As can be seen, when the threshold value of the test counter is low (less than 10,000 ﬂits) the latency overhead is quite high, especially when the test packet includes walking-one sequence. This is due to the fact that with lower threshold values the switches go to the test mode more frequently. However, for threshold values greater than 10,000 ﬂits the latency overhead of our on-line testing is almost negligible. In addition to the synthetic traces, we also performed simulation to see the effect of on-line testing on real trafﬁc traces from the PARSEC benchmarks, a suite of next-generation shared-memory programs for CMPs [23]. The traces used are for a 64-node shared memory CMP arranged as a 8x8 mesh. Each processor node has a private L1 cache of 32KB and 1MB L2 cache (64MB shared distributed L2 for the entire system). There are 4 memory controllers at the corners. To obtain the traces, we used Virtutech Simics [24] with the GEMS toolset [25], augmented with GARNET [26], simulating a 64-core NoC. Like for the previous experiments we swept the test counter threshold from 1,000 to 20,000 ﬂits. We performed the simulation for two types of test packets: with walking-one sequence and without that. The results are shown in Figure 11. As can be seen, for all benchmarks with a threshold value of 10,000 the latency overhead is almost zero even with walking-one sequence. Based on our experiments, 10,000 ﬂits is an appropriate value for the test counter’s threshold for both synthetic and real trafﬁc. 6. CONCLUSIONS A technique for on-line detection and diagnosis of faults in NoC’s routers and links has been proposed. This method can be applied on any topology and routing algorithm. Each router in the NoC is tested using Test Pattern Generators and Test Response Analyzers in the neighbors and its own NI. During the testing only the router under test is in test mode while other parts of the network are operational. The proposed testing mechanism covers all the short-wire, and stuck-at faults on the links. It also covers 100% of the data-path faults and 85% of the control-path faults inside the router. 7. ACKNOWLEDGMENTS This work is supported by EU FP7 project Pro3D (GA n. 248776) and ENIAC project Modern (GA 120003). Figure 10: Simulation results for our on-line testing approach on different synthetic trafﬁcs with various test counter thresholds. Figure 11: Simulation results on PARSEC benchmarks with various test counter thresholds. 8. "
BLOCON - A Bufferless Photonic Clos network-on-chip architecture.,"On-chip photonic waveguides have been proposed as a feasible replacement for the long interconnects that cause speed and power bottlenecks. Along with recent advancements in nanophotonic technologies, we believe that combining on-chip waveguides with high-radix Network on Chip (NoC) topologies is a promising way to improve NoC performance. In this paper, we propose the BLOCON (BufferLess phOtonic ClOs Network) to exploit silicon photonics. We propose a scheduling algorithm named Sustained and Informed Dual Round-Robin Matching (SIDRRM) to solve the output contention problem, and a path allocation scheme named Distributed and Informed Path Allocation (DIPA) to solve the Clos network routing problem. In the simulation results, we show that with SIDRRM and DIPA, BLOCON improves the delay and power performance of the compared electrical and photonic NoC architectures over synthetic traffic patterns and SPLASH-2 traces.","BLOCON: A Bufferless Photonic Clos Network-on-Chip  Architecture  Yu-Hsiang Kao and H. Jonathan Chao  Department of Electrical and Computer Engineering  Polytechnic Institute of New York University, Brooklyn, NY 11201  ykao01@students.poly.edu, chao@poly.edu  ABSTRACT  On-chip photonic waveguides have been proposed as a feasible  replacement for the long interconnects that cause speed and power  bottlenecks. Along with recent advancements in nanophotonic  technologies, we believe that combining on-chip waveguides with  high-radix Network on Chip (NoC) topologies is a promising way  to improve NoC performance. In this paper, we propose the  BLOCON (BufferLess phOtonic ClOs Network) to exploit silicon  photonics. We propose a scheduling algorithm named Sustained  and Informed Dual Round-Robin Matching (SIDRRM) to solve  the output contention problem, and a path allocation scheme  named Distributed and Informed Path Allocation (DIPA) to solve  the Clos network routing problem. In the simulation results, we  show that with SIDRRM and DIPA, BLOCON improves the  delay and power performance of the compared electrical and  photonic NoC architectures over synthetic traffic patterns and  SPLASH-2 traces.    Categories and Subject Descriptors  B.4.3 [Hardware]: Input/Output and Data Communications  Interconnections (Subsystems).   Keywords  Network-on-chip, silicon photonics, Clos network.  1. INTRODUCTION  To exploit the ever-growing number of transistors on a single die,  Chip Multiprocessors  (CMP) have become  favored over  traditional superscalar processors because of  their power  efficiency and modularity. A CMP system comprises duplicate  processing elements (PE), and each PE contains the core logic and  L1/L2 caches. The chip area in a CMP system is usually divided  into a number of tiles, with each tile containing a PE. Tiles are  interconnected through an interconnection network, which greatly  influences the CMP performance.  The power consumption of the on-chip long interconnects  increases radically as CMOS technology continues to scale. Onchip photonic waveguides combined with high-radix NoC  architecture designs have been proposed as a feasible solution to  the power performance problem  [2][3][19]. Along with  wavelength division multiplexing (WDM) and necessary on-chip  photonic devices such as holographic lenses, microring resonators,  waveguides, and photodetectors [22], silicon photonics offers a  Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  NOCS’11, May 1-4, 2011 Pittsburgh, PA, USA.  Copyright 2011 ACM 978-1-4503-0720-8…$10.00.  competitive option for  low-power,  bandwidth on-chip communications.  low-latency, and highIn this paper, we propose the BLOCON (BufferLess phOtonic  ClOs Network) to exploit silicon photonics. BLOCON is a  bufferless Clos network, which applies wormhole routing, and  could be viewed as an input-queued switch without reassembly  queues or virtual channels (VCs) in the output ports. In BLOCON,  the buffers only exist in the PEs and the first stage of the switch  modules (SMs). The absence of multi-stage buffered SMs makes  BLOCON enjoy high throughput with a proper scheduling  algorithm. The zero-load latency of BLOCON is low, compared  to other NoC architectures, because a packet only needs to travel a  buffered SM, a waveguide, a crossbar, and two electrical links  between any source and destination PE. Since the Clos network  has a high-bisection bandwidth and large number of routes  between any source and destination, BLOCON has a very stable  delay and power performance under different kinds of traffic  patterns. There are two major challenges in designing a bufferless  Clos network – the output contention resolution scheme and the  path allocation scheme. We proposed Sustained and Informed  Dual Round-Robin Matching (SIDRRM) and Distributed and  Informed Path Allocation (DIPA) to tackle these two problems.  In the simulation results, we show that both SIDRRM and DIPA  perform better than existing scheduling algorithms for bufferless  Clos networks. With SIDRRM and DIPA, we show that  BLOCON outperforms the buffered photonic Clos network and  other high-radix electrical NoC architectures in terms of delay,  throughput, and power efficiency under different synthetic traffic  patterns and SPASH-2 traces.  2. RELATED WORKS  As a counterpart of the bufferless Clos network, the on-chip  Buffered Photonic Clos Network (BPCN), in which a packet has  to pass through three “buffered” VC routers between any source  and destination, was proposed in [3] with the motivation to reduce  the total ring number of the photonic crossbar (PXBar) [2]. For  the ring resonators to function correctly, the temperature of the  rings has to be controlled by the integrated heaters. In [3] and [4],  it is assumed that two adjacent rings share an integrated heater  that requires 1 ߤW per K, and the temperature range is assumed to  be 20K. A 64 ൈ 64 PXBar requires about 262k microrings, which  consume a static thermal-tuning power of 2.6W. The 64 ൈ 64  BPCN requires only 14k rings, which consume 0.14W. Although  the ring number and the thermal-tuning power are reduced  dramatically in BPCN, each packet has to travel two waveguides  between any source and destination PE, unless the source and the  destination are in the same cluster. This causes the zero-load  latency of BPCN to be larger than those of some electrical highradix NoCs, because each waveguide costs a flit three clock  cycles (CCs) for E/O, light-wave propagation, and O/E. Also, the  throughput of BPCN is limited by the individual throughput of the            VC routers. The head-of-line (HOL) blockings encountered by a  packet in multiple routers along its path to the destination also  degrade the throughput of BPCN.  Besides BPCN and PXBar, many high-radix NoC topologies have  been proposed to improve delay and power performance in  electrical and photonic realms. The Concentrated Mesh (CMesh)  with express channels [12] adapts the 2D Mesh by connecting  several PEs to the same router, and adding express channels on  the border of the CMesh. The Flattened Butterfly (FBfly) [5] has a  regular floorplan, as does CMesh, and applies a higher router  radix than CMesh to achieve better performance. In FBfly, each  router is fully connected with all the routers in the same row and  the same column. In [18], a circuit-switching photonic torus  network was proposed, in which an electrical network is used to  control the photonic data path. Because a circuit-switching  network needs a larger packet size to amortize the path-setup  latency, [18] has different target applications from the BLOCON.  In [19], a 64 ൈ 64 hybrid NoC architecture named T-PROPEL,  which assumes that four PEs share the same physical L2 cache,  was proposed. In the 64 ൈ 64 BLOCON, it is assumed that each  PE has its own physical L1/L2 caches.  3. SILICON PHOTONICS TECHNOLOGY  In this section, we present a high-level introduction to the  photonic components that are mentioned in this paper.  3.1 Point-to-point Photonic Link  Fig. 1 shows a point-to-point photonic link. The off-chip laser  source injects ߣ different wavelengths of light into the power  waveguide. The sender, which has a set of modulation rings in  between the power waveguide and the data waveguide, selectively  activates a subset of its ߣ modulation rings, and introduces the  selected wavelengths of light into the data waveguide. The  different wavelengths of light travel through the data waveguide  until they drop into the filter rings, which are always activated by  the receiver. In this paper, ߣ is always assumed to be 64, which is  the size of a flit.  Figure 1. Point-to-point photonic link.  3.2 N-to-1 MUX Waveguide and ۼ ൈ ۼ  Photonic XBar  The N-to-1 MUX waveguide is very similar to the point-to-point  photonic link. The difference is that there are multiple senders in a  MUX waveguide. In an N-to-1 MUX waveguide, there are N  senders and a receiver attached to the data waveguide. At any  given time in this many-writer-one-reader MUX waveguide, at  most one sender can modulate the signal and send data to the  receiver.  An ܰ ൈ ܰ photonic crossbar (PXBar) can be constructed with N  N-to-1 MUX waveguides. In a PXBar, every output port is  associated with a distinctive MUX waveguide, and all the input  ports can access all the waveguides. In other words, each input  port has to connect to N different waveguides to send data to the  different output ports.  3.3 Photonic Broadcast Bus and Photonic  Token Ring Bus  The photonic broadcast bus utilizes a many-writer-many-reader  waveguide and couplers/splitters. At any given time, there can be  at most one writer for each wavelength. The filter rings of the  receivers are always activated and can capture the messages from  the senders at any time. In [1] a photonic broadcast bus is  employed to facilitate the snoopy cache coherence protocol. In [2]  the photonic broadcast bus is used for the MOESI coherence  protocol. One concern about the photonic broadcast bus is its laser  power loss. Therefore, we limit the receiver number to 16 in this  paper.  The photonic token ring bus also uses a many-writer-many-reader  waveguide [2][19]. But unlike the broadcast bus in which every  reader receives the signal from the writers, in the token ring bus  only one reader can capture the token signal released by a certain  writer. To ensure each receiver has a chance to capture the token  signal, the waveguide should form a ring shape. A token  waveguide can convey ߣ tokens, each represented by a distinct  wavelength.  Figure 2. (a) A ૟૝ ൈ ૟૝ 3-stage Clos network. (b) The  floorplan of a 64-node CMP and the photonic Clos network.  4. CLOS NETWORK ARCHITECTURE  Fig. 2(a) shows a 64 ൈ 64 3-stage Clos network. The SMs in the  first, second, and third stages are denoted as input modules (IMs),  center modules (CMs), and output modules (OMs). One thing to  notice is that all the links shown in Fig. 2(a) are uni-directional  rather than bi-directional. In [3], the SMs of the BPCN are  implemented as VC routers that employ wormhole routing. The  interconnects between the IMs/CMs and the CMs/OMs are  implemented as point-to-point photonic links. The floorplan of the  64 ൈ 64 BPCN is illustrated in Fig. 2(b), in which there are eight  clusters of PEs, and each cluster is associated with an IM, a CM,  and an OM.  Figure 3. (a) A ૝ ൈ ૝ Clos network with its CMs represented  by MUXes and DEMUXes. (b) A ૝ ൈ ૝ Clos network with its  CMs replaced by four 2-to-1 MUX waveguides.  BLOCON can lower the zero-load latency of BPCN, because the  CMs in BPCN are replaced by the MUX waveguides, and the OM      VC routers in BPCN are replaced by XBars without input buffers.  Fig. 3 shows a 4 ൈ 4 Clos network with four 2-to-1 MUX  waveguides. In this example, each input port of the OMs is  connected to a distinct waveguide, and each output port of the IMs  can access two different waveguides. Compared to PXBar,  BLOCON still has a much lower ring number, which is about 32k  for the data path waveguides. The idea of the BLOCON has been  mentioned in [3]. However, there are two major challenges that  have not been identified and tackled: the output contention  resolution scheme and the path allocation scheme.  There has been extensive research into the packet-scheduling  algorithms of the input-queued switches with the VOQ buffer  structure. Among the existing scheduling algorithms, we are  interested in Dual Round-Robin Matching (DRRM) [6] and iSLIP  [7], since both schemes use round-robin (RR) arbiters that could  be implemented by the photonic token bus. In DRRM, an input  port first chooses one of its VOQs as the local winner using an RR  input arbiter (IA). Based on the IA’s decision, the input port  makes a request to the associated output port. The output port uses  an RR output arbiter (OA) to decide which input port to grant.  iSLIP can be considered an output-first algorithm. In an outputfirst algorithm, an input port sends all the requests to different  output ports. After receiving the grants from different output ports,  the input port decides which grant to accept.  Figure 4. The buffer structure of a ૝ ൈ ૝ BLOCON with  VOQs in the PEs and FIFOs in the IMs.  5. OUTPUT CONTENTION RESOLUTION  SCHEME  BLOCON can be viewed as an input-queued switch with the input  buffers residing in the PEs and IMs. When a packet leaves the  input port of an IM, the packet will not experience any queuing  delay. We designed BLOCON based on the virtual output queue  (VOQ) buffer structure. Fig. 4 shows the buffer structure of a  4 ൈ 4 BLOCON, in which there are three VOQs in each PE, and a  FIFO in each input port of the IMs. In BLOCON, there are no  reassembly queues or VCs on the output port side. A packet has to  be transferred continually without interruption between an IM  FIFO and the destination PE. To send a packet, a PE has to solve  the output contention, making sure that the packet has the  exclusive right to enter its destination PE. After winning the  output contention, the input/output ports are locked by this packet.  The packet then leaves the PE and enters the input port FIFO in an  IM. The IM will allocate a MUX waveguide for the packet before  the packet leaves the FIFO. After the packet leaves the FIFO, the  input/output ports are unlocked. In this section, we describe the  output contention resolution scheme in the PEs.  We first explain why BLOCON applies the VOQ buffer structure.  It is a well-known phenomenon that the throughput of an inputqueued switch under uniform traffic is 58.6% with a single FIFO  queue in each input port. To cure the HOL blocking problem,  VOQs or VCs are employed in the switch design. In BLOCON,  the VOQ buffer structure is applied for four reasons:  4)  1) VOQ can eradicate the HOL blocking problem.   2) VOQ can eradicate the packet out-of-order problem.  3) Multi-stage networks suffer from hotspot traffic, because  hotspot traffic forms a congestion tree in the upstream  buffers of the hotspot and block non-hotspot (benign) traffic.  With VOQs, benign traffic will not be blocked by hotspot  traffic.  In the 64 ൈ 64 BLOCON, each PE contains 63 VOQs, which  are implemented as linked lists. The packets are stored in the  shared memory. For each linked list, a pair of head/tail  registers is needed. The total VOQ number for the entire chip  is 4032, which makes the amount of the linked list state  registers about 8kB, if we assume the size of the head/tail  register to be 1B, which is sufficient for a shared memory  with a size up to 256 flits. 8kB is small considering the  abundant transistor budget in today’s technology.  Figure 5. The OA and the broadcast waveguide for SIDRRM.  In the 64 ൈ 64 BLOCON, the 64 RR OAs are implemented by a  token waveguide with 64ߣ passing through all the PEs as  illustrated in Fig. 5. Each token represents the right to access  different output ports. When making a request to an OA, a PE  activates the associated filter ring and tries to grab the token. A  PE wins the output arbitration when obtaining a token. By  keeping the token, the PE has the exclusive right to enter the  associated output port. When a PE finishes transferring data to an  output port, it has to keep releasing and absorbing the token in  every CC, until another PE grabs this token. With each PE having  64 filter rings, the large number of pass-by rings on the token  waveguide results in at least 20-dB laser power loss in the critical  path, assuming that each pass-by ring causes 0.005-dB loss. Thus  we make 4 PEs share 64 filter rings, and the loss from pass-by  rings drops to about 5dB. This leads us to design the BLOCON  with an input-first scheduling algorithm to avoid sequentially  allocating the grabbed tokens to the four PEs required by the  output-first scheduling algorithm.  DRRM is a good candidate for the scheduling algorithm in  BLOCON since the token waveguide can function as N RR  arbiters with ܰߣ. An IA of a PE can be implemented by a local  token waveguide, where there is only one token representing the  right to be the local winner, ݊ filter rings for ݊ VOQs. For the  64 ൈ 64 BLOCON, there will be 64 IA waveguides and an OA  waveguide.  In this paper, we define 25-ns throughput as the traffic injection  rate that causes the average packet latency to be 25-ns under a  certain traffic pattern. Although DRRM is proven to achieve  100% throughput under uniform traffic with one-flit packet size,  we found that its 25-ns throughput is only 70%. When the packet  size is 8 flits under uniform traffic, the 25-ns throughput drops to  60%, and the overall throughput is 95%, which is still high.  From our observation, the low 25-ns throughput of DRRM is  caused by the ignorance of the IAs and the lack of multiple  iterations in the NoC environment. In DRRM, the IAs choose  their local winner without knowing the availability of an output  port. In the case of packet length 8, it is likely that an IA chooses  a local winner that is destined for an output port locked by another  packet. Without multiple iterations in DRRM, in the case of  packet length 1, it is possible that multiple IAs choose the same      output port to send requests in the same CC. Both phenomena  increase the average latency, resulting in low 25-ns throughput,  though the overall throughput is still high.  We propose SIDRRM to resolve the low 25-ns throughput  problem in DRRM. In SIDRRM, an IA is informed of the  availability of the output ports. In this way, the VOQs that are  destined for a locked output port will not be considered by any IA  in BLOCON. To implement the informing mechanism, we need  the photonic broadcast bus described in section 3. There are 16  groups of token and broadcast logics, each shared by 4 adjacent  PEs, and 64 bits that need to be broadcast as illustrated in Fig. 5.  A source PE should keep broadcasting that an output port is not  available during the period when the PE is locked with the output  port.  Besides the informing mechanism, BLOCON applies a technique  called “sustained mode” to emulate the effect of multiple  iterations. In DRRM, a matched source PE has to unlock the  current input/output pair by releasing the token, when the current  packet is completely served. In the sustained mode, a locked  input/output pair tries to sustain their matching until a timeout  period (TP) has been reached or the VOQ is empty. In other  words, upon finishing serving a packet, the source PE will keep  locking and serving the same VOQ, until the VOQ becomes  empty, or a TP has been reached. In this way, regardless of the  packet size, the locked pairs can try to last for TP CCs, and other  IAs/OAs can keep improving the current matching. The packets of  the same sustained period are transferred between a locked  input/output pair without interruption.  In the description of SIDRRM, the flit types are abbreviated as  follows: Single flit (S), Head flit (H), Body flit (B), and Tail flit  (T). A VOQ is not available if the broadcast bus shows that the  associated output port is locked. An IA is available if it is not  locked.  Step 1: Request. Each available IA sends an output request  corresponding to the first nonempty and available VOQ in a fixed  RR order, starting from the current position of the pointer in Step  2. The pointer of the IA is advanced by one location beyond its  current position if (1) this VOQ has been served for at least TP  CCs and the granted flit is a T or S, or (2) after the granted flit is  served, this VOQ becomes empty and the granted flit is a T or S.  Otherwise, the pointer remains unchanged.  Step 2: Grant. If an OA receives one or more requests, it chooses  the one that appears next in a fixed RR order starting from the  current position of the pointer. The pointer is moved to this  position. The pointer of the OA is incremented to one location  beyond the granted input if the granted input port updates its IA  pointer. If there are no requests, the pointer remains unchanged. If  an IA is granted by an OA, both the IA and the OA are locked.  The IA and OA pair is unlocked only when the IA updates its  pointer.  6. PATH ALLOCATION SCHEME  After a flit wins the output contention and becomes eligible to  access an output port, the PE can send the flit to the IM. In  BLOCON, there is a credit-based flow control mechanism  between the IMs and the PEs to prevent buffer overflow in the  IMs. When the head flit of the first packet of a sustained period  enters the IM, the IM needs to find an available MUX waveguide  for the group of packets to enter a specific OM. This group of  packets has locked the input/output ports during SIDRRM. If the  IM finds an available MUX waveguide, the IM will lock this  MUX waveguide as well as the IM’s associated output port for  this group of packets, until a tail flit marked with an unlock signal  passes. The unlock signal was marked by the PE, when its IA  stopped the sustained period and updated its pointer. We term this  problem the “path allocation problem.”  To describe the path allocation problem, we define ܯ௜௝ as the  availability of the MUX waveguide connected to output port ݆ of  ܱܯ௜ . We define ܣ௜௝ as the availability of output port ݆ of ܫܯ௜ .  ܯ௜௝ ൌ 0 if the associated MUX waveguide is locked. ܣ௜௝ ൌ 0 if  output port ݆ of ܫܯ௜ is locked.  Figure 6. (a) A ૝ ൈ ૝ BLOCON with ࡹ࢏࢐ and ࡭࢏࢐ labeled on  the associated ports. (b) An example of the path allocation  problem.  Path allocation problem - When there is an input port of ܫܯ௞  containing packets destined for ܱܯ௜ , ܫܯ௞ needs to find ݆ , such  that ܯ௜௝ · ܣ௞௝ ൌ 1, and lock ܣ௜௝ and ܯ௜௝ for the group of packets  of the same sustained period in the input port. In other words, the  group of packets needs an available output port of the IM, and an  available MUX. In addition, the MUX has to be connected to this  output port. Fig. 6 shows an example of the path allocation  problem in a 4 ൈ 4 BLOCON. In Fig. 6, there is a packet in ܫܯଵ  requesting a path to ܱܯଵ . ܫܯଵ should compare ܣଵ with ܯଵ , find a  ݆ such that ܣଵ௝ ൌ 1 and ܯଵ௝ ൌ 1 , and lock ܣଵ௝ and ܯଵ௝ for this  packet. In this example ݆ could only be 1. A path allocation  algorithm should prevent a certain ܯ௜௝ from being locked by  multiple IMs at any time. For example, if there is another packet  in ܫܯ଴ requesting a path to ܱܯଵ , the path allocation algorithm  should allow only one IM to be able to lock ܯଵଵ .  The path allocation problem is also known as the Clos network  routing problem. Many algorithms have been proposed, such as  distributed pipeline routing (DPR) [8], m-matching [9], and  Karol’s algorithm [10]. Among these algorithms, only the DPR  considered the hardware implementation issues and can be applied  to BLOCON with small changes. In DPR, a CC is divided into ݇  micro CCs, where ݇ is the number of IMs or OMs. There is no  centralized scheduler. Each IM does path allocation in a  distributed fashion. At micro CC ݆ , ܫܯ௜ grasps a distinct k-bit  array ሼܯ௫,ሺ௜ା௝ା௫ሻ ௠௢ௗ ௞ | 1 ൑ ݔ ൑ ݇ ሽ and has the right to lock any  path within this set of ܯ௜௝ . For example, in Fig. 6, at micro CC 0,  ܫܯ଴ has the right to lock ܯ଴଴ and ܯଵଵ as long as they are not  locked. ܫܯଵ has the right to lock ܯ଴ଵ and ܯଵ଴ as long as they are  not locked. At micro CC 1, ܫܯ଴ and ܫܯଵ have the right to lock the  remaining ܯ௜௝ ’s that they did not grasp at micro CC 0, if the ܯ௜௝ ’s  are not locked. The difficulty in applying DPR to BLOCON is  that we cannot afford passing these k-bit arrays among the IMs  within a CC. Thus, we modified DPR by extending the micro CC  to CC. In this way, for every ݇ CC the modified DPR can finish  the operation that used to be done within a CC. The average  packet latency of BLOCON with the modified DPR is not  satisfactory, because sometimes an IM has to wait ݇ െ 1 CCs  before grabbing the desired ܯ௜௝ , and this ܯ௜௝ might be available  since the first CC. We designed a heuristic algorithm – DIPA – to  solve the path allocation problem.    In the 64 ൈ 64 BLOCON, there is a token waveguide that  conveys 64 ܯ௜௝ tokens passing through all the IMs. Each token  represents the right to lock ܯ௜௝ for accessing output port ݆ of ܱܯ௜ .  There is a photonic broadcast bus that advertises the availability  of each ܯ௜௝ passing through all the IMs. All the ܯ௜௝ tokens are  controlled by the DIPA logic inside the IMs. That is, only the IMs  can release or grab the ܯ௜௝ tokens. If an IM grabs and locks a  certain ܯ௜௝ , it has to keep broadcasting a 1-bit signal notifying  other IMs that the ܯ௜௝ is not available, until the IM unlocks this  ܯ௜௝ . After an IM unlocks an ܯ௜௝ , the IM should keep releasing  and absorbing the ܯ௜௝ token in every CC, until the token is  grabbed by some other IM. In this way, there is always an IM  responsible for releasing an ܯ௜௝ token. In total, the DIPA scheme  in the 64 ൈ 64 BLOCON requires one token waveguide and one  broadcast waveguide as shown in Fig. 7(a). There will be 64  wavelengths on each waveguide.  Figure 7. (a) The token and the broadcast waveguides for  DIPA. (b) An example of the path allocation problem.  The IMs work in parallel in DIPA. An IM has to execute two  steps of operations when there are head flits requesting a path –  target path assignment and ܯ௜௝ token grabbing. We explain the  operation of DIPA through an example. In Fig. 7(b), there are  three packets requesting path allocation in a 4 ൈ 4 BLOCON. ܫܯ଴  learns that ܲଵ is destined for ܱܯ଴ , and from the broadcast  information, ܫܯ଴ knows that both ܯ଴଴ and ܯ଴ଵ are available at  this moment. Since only ܣଵଵ is available, ܫܯ଴ assigns ܯ଴ଵ as the  target ܯ௜௝ for ܲଵ . At the same time, ܫܯଵ assigns ܯ଴ଵ for ܲଶ and  ܯଵଵ for ܲଷ . The detail of target path assignment will be explained  in the rest of this section. After assigning the target ܯ௜௝ ’s to the  packets, the IMs activate their filter rings on the ܯ௜௝ token bus,  trying to lock the ܯ௜௝ ’s for the packets. An IM releases a captured  ܯ௜௝ token after a tail flit marked with the unlock signal passes.  When locking an ܯ௜௝ , the IM keeps broadcasting that ܯ௜௝ is not  available.  In target path assignment, an IM chooses a distinct available  target ܯ௜௝ for each of the head flits. An available target ܯ௜௝ means:  1) The broadcast bus shows that ܯ௜௝ is available.   2) The packet is destined for ܱܯ௜ .  3) ܣ௣௝ is not locked, where ݌ is the IM number.  Figure 8. An example showing how to perform target path  assignment.  An IM should not assign the same target ܯ௜௝ to multiple head flits.  Fig. 8 illustrates how ܫܯଵ performs target path assignment  following the example of Fig. 7(b). There are three steps of  operation for ܫܯଵ . In the first step, ܫܯଵ generates matrix 1 by  putting the ܯ௜ arrays of ܲଶ and ܲଷ ’s destination OMs in the first  and second rows to form ܤ଴ and ܤଵ . An ܯ௜ array represents the  set of MUXes to enter ܱܯ௜ . The positions of ܯ଴ and ܯଵ could be  exchanged according to the priority of ܫܯଵ ’s input ports (ܤଵ can  be assigned the values of ܯ଴ in this case). To ensure fairness  among all the input ports of an IM, we assume the priority  changes in an RR fashion in every CC. In the second step, we  update matrix 1 to be matrix 2. This step masks out the ܯ௜௝ ’s that  are not available for ܫܯଵ according to ܣଵ , by making ܤԢ௫௬ ൌ ܤ௫௬ · ܣଵ௬ . In the third step, ܫܯଵ fills out matrix 3, where ܴ௫௬ ൌ 1 if  ܤԢ௫௬ ൌ 1 and ܴ௫௕ ൌ ܴ௔௬ ൌ 0, ܽ ൏ ݔ , ܾ ൏ ݕ . This means that a  head flit can only request a path at a time, and the path will be  distinct. ܴ௫௬ ൌ 1 makes the associated ܯ௜௝ in matrix 1 a target  path for the associated input port. Step 3 can be implemented by  the combinational logic “simple two-dimensional ripple carry  arbiter” proposed in [11]. In DIPA, we always give the top left bit  of matrix 3 the highest priority. Fairness is not a concern since  each input port’s request has a chance to be on the top row. After  the third step, ܫܯଵ will request ܯ଴ଵ for ܲଶ and ܯଵଵ for ܲଷ .  7. COST EVALUATION OF BLOCON  Table I shows the total ring numbers and the thermal-tuning  power for the 64 ൈ 64 PXBar, BPCN, and BLOCON. The BPCN  and BLOCON both apply the floorplan in Fig. 2(b), in which the  IM/(CM)/OM of the same PE cluster are placed together. In  estimating the thermal-tuning power, we follow the assumption in  [3] and [4] that two adjacent rings share an integrated heater that  requires 1 ߤܹ/ܭ with a temperature range 20K. The PXBar  consists 64 63-to-1 MUX waveguides ( 64ߣሻ and a token  waveguide (64ߣሻ, resulting in about 262k rings. The BPCN uses  112 point-to-point photonic links (64ߣሻ, resulting in about 14k  rings. The BLOCON uses 64 7-to-1 MUX waveguides (64ߣሻ, 64  token waveguides (1ߣሻ for the IAs, 1 token waveguide for the OA  (64ߣሻ, 1 16-to-16 photonic broadcast bus (64ߣሻ for SIDRRM, 1  8-to-8 photonic broadcast bus for DIPA, and 1 token waveguide  for DIPA (64ߣሻ . This results in about 39k rings. The thermaltuning power for the BLOCON is 0.39W, which is larger than  BPCN’s 0.14W. In the experimental results, we show that the  BLOCON has better power-efficiency since  it has better  throughput and delay performance. In comparing the BPCN and  BLOCON, the PXBar has a very high thermal-tuning power  caused by the large number of rings.  Table I. Thermal-tuning power of different architectures  PXBar  Ring Number  262k  Thermal-Tuning Power  2.6W  BPCN  Ring Number  Thermal-Tuning Power  14k  0.14W  BLOCON  Ring Number  Thermal-Tuning Power  39k  0.39W  Table II shows the laser power loss parameters that we applied to  estimate the off-chip laser power budget. The maximum laser loss  in the 64 ൈ 64 BLOCON is estimated to be 39dB, which results  in a 2.77W off-chip laser power, if we assume 30% laser  efficiency and െ9.8dBm sensitive receivers. Because the off-chip  laser power varies according to the detail layout of the photonic  components, we only compare the on-chip power values among  different NoC architectures in the experimental results.                      Table II. Laser power loss parameters    Photonic Device  Laser Loss (dB)  Waveguide  Pass by ring  Drop into ring  Waveguide bend  1.5 dB per cm  0.005 dB  0.5 dB  0.005 dB per 90˚  8. PERFORMANCE EVALUATION OF  BLOCON  In this section, we compare the delay and power performance  among the 64 ൈ 64 BLOCON, BPCN [3], 2D Mesh, CMeshX2  [12], and FBfly [5] with our in-house cycle-accurate simulator.  The VC router follows the SA+VA design in [13] by performing  virtual channel allocation (VA) and switch allocation (SA) in the  same CC. In SA+VA, the VCs are assigned non-speculatively  after the input-first SA, so that VA simply involves finding a free  VC for each output port from a pool of free VCs. We set the VC  router pipeline stage number to 2 for 2D Mesh or 3 for other highradix topologies. The VC number on each physical link is set to 4  for the 2D Mesh, 8 for other topologies, and each VC has 4-flit  buffers in the input ports of the routers. For the BLOCON, each  IM’s input port has an 8-flit FIFO, which is large enough to cover  the round trip delay of the credit-based flow control between the  IMs and the PEs. XY routing is applied to the 2D Mesh,  CMeshX2, and FBfly to prevent deadlock. In the BPCN, an IM  sends packets to different CMs in an RR fashion. The BLOCON  has ten pipeline stages, where two are for SIDRRM (one for IA  and broadcast receiving, one for OA), two are for DIPA (one for  broadcast receiving, one for target path assignment and token  grabbing), three are for the data path waveguide, two for the  traveling between PE/IM and OM/PE, and one for self-routing  and XBar traversal in the OMs.  We used Orion2 [14] for the power parameters of different VC  routers and different lengths of metal links, assuming 32-nm  technology, 4-GHz operating frequency, 2-mm long PE tile edge,  and 64-bit width for all links. According to the simulation results,  the metal links consume 36.8pJ per 2mm per flit. For the photonic  links, we estimate the per flit energy consumption by the  modulator and detectors to be 37.76pJ according to [15] and [16],  assuming a 50% activity factor.  8.1 SIDRRM VS. DRRM  We tested SIDRRM and DRRM on a 64 ൈ 64 XBar with VOQs  to exclude the influence of the path allocation scheme in  BLOCON. Fig. 9 shows the delay under different traffic injection  rates of a 64 ൈ 64 XBar with different output contention  resolution schemes under uniform traffic. In the remainder of this  paper, the TP of the SIDRRM is always set to 8. As stated in  section 5, although the DRRM can achieve 100% throughput  under uniform traffic with 1-flit packet size, the 25-ns throughput  is just 70%. If the packet size increases to 8 flits, the overall  throughput can still achieve 95.31%, but the 25-ns throughput  drops to 60%. With SIDRRM and a packet size of 1, the 25-ns  throughput is 86%, and the throughput is 98.85%. With SIDRRM  and a packet size of 8, the 25-ns throughput is 89%, and the  throughput is 98.54%.  DRRM (pkt1) SIDRRM (pkt1) DRRM (pkt8) SIDRRM (pkt8) 4 0 3 5 3 0 2 5 2 0 1 5 1 0 5 0 ) s n ( y c n e t a L t e k c a P . e v A 1 6 1 1 1 6 2 1 26 31 3 6 4 1 4 6 51 56 61 6 6 7 1 7 6 81 86 9 1 9 6 Injection Rate (% ) Figure 9. Delay vs. Injection rate under uniform traffic for a  ૟૝ ൈ ૟૝ XBar with DRRM or SIDRRM (packet size 1 or 8  flits) (TP=8).  ) s n ( y c n e t a L t e k c a P . e v A The modified DPR DIPA 40 35 30 25 20 15 10 5 0 1 6 1 1 1 6 21 26 31 3 6 4 1 4 6 51 56 61 6 6 7 1 7 6 8 1 86 91 96 Injection Rate (%) Figure 10. Delay vs. Injection rate under uniform traffic for a  ૟૝ ൈ ૟૝ BLOCON with DIPA and the modified DPR (packet  size 8 flits).  8.2 DIPA VS. The Modified DPR  Fig. 10 shows the delay performance of the 64 ൈ 64 BLOCON  with SIDRRM and different path allocation schemes under  uniform traffic. The throughput with DIPA is 86.7%, and with the  modified DPR it is 70.76%. The 25-ns throughput with DIPA is  80%, and with the modified DPR it is 62%. The delay and  throughput performance of DIPA are better than the modified  DPR, because in the modified DPR, an IM sometimes needs to  wait multiple CCs to lock an available ܯ௜௝ . This ܯ௜௝ could be  available since the first CC, but an IM has to wait until the proper  timing before grabbing the token. On the other hand, with DIPA,  an IM can try to lock any target ܯ௜௝ as long as the target ܯ௜௝  satisfies certain conditions as described in section 6.  8.3 Delay and Power Performance  Comparison  In Fig. 11, we compare the 25-ns throughput and the power  performance among different 64 ൈ 64 NoC architectures. The  definitions of the permutation traffic patterns could be found in  [21]. In non-uniform traffic, a source PE has a 50% probability of  generating uniform traffic, and 50% probability of generating  transpose traffic.  Fig. 11(a) shows the 25-ns throughput performance, from which  we know that the BLOCON can deliver 100% of the traffic under  25-ns for all the permutation traffic. The 25-ns throughput of the  BPCN is around 60% under all kinds of traffic, showing that the  BPCN has a good load-balance property under RR routing. In all  the traffic patterns, both photonic NoC architectures show better  25-ns throughput than the electrical ones.                    (a)  (b)  (c)  Figure 11. (a) The 25-ns throughput of different NoC  architectures under different kinds of traffic patterns. (b) The  normalized energy consumption per packet under 25-ns traffic  load. (c) The power consumption under 10% traffic load.  Fig. 11(b) shows the normalized energy spent by a packet under  the 25-ns traffic load. The 25-ns traffic load is the traffic load that  causes the average packet latency to reach 25-ns under a certain  traffic pattern. The link energy in Fig. 11(b) includes the energy  consumed on the metal and photonic links. Fig. 11(b) tries to  characterize the power performance under high traffic loads for  different architectures. Fig. 11(b) shows that the photonic NoC  architectures are more power-efficient under high traffic loads  compared to the electrical ones, and the BLOCON consumes the  least energy when sending a packet under the 25-ns traffic load.  Fig. 11(c) shows the power values of different NoC architectures  under 10% traffic load of different traffic patterns. In Fig. 11(c),  we try to characterize the power performance of different  architectures under the same low traffic load. In Fig. 11(c), the  power values of the BLOCON and BPCN do not vary with the  traffic patterns. This is because in a Clos network, from any  source to any destination the hop count for a packet to travel is  always the same. Fig. 11(c) also shows that under low traffic load,  the BLOCON has the best power performance.  8.4 SPLASH-2 Simulation Results  For the benchmark simulation, we use Graphite [17], an x86  multicore simulator, to generate SPLASH-2 traffic traces. Then  we injected the traffic traces into our in-house cycle-accurate  simulator to obtain the delay and power performance. In Graphite,  it is assumed that there are 64 PEs, each with its own private  L1/L2 (32kB/512kB) caches. The caches are coherent with MSI  protocol, and the packet sizes are either 8B or 64B (1 flit or 8  flits). In [20], it is identified that the SPLASH-2 benchmarks have  a higher traffic load during the starting phase, which is about 2  million CCs. We follow the methodology in [20] and compare the  delay and power performance of different NoC architectures  under the traces generated in the first 2 million CCs. Fig. 12 and  Fig. 13 illustrate the normalized average packet latency and the  normalized delay-power products of different NoC architectures  under SPLASH-2 traces, in which BLOCON shows the best delay  and delay-power product performance. FBfly outperforms BPCN  in both Fig. 12 and Fig. 13, because of the low traffic load of the  SPLASH-2 traces obtained from Graphite. The 64 ൈ 64 FBfly has  lower zero-load latency than that of the BPCN. Therefore the  FBfly has a better delay and delay-power product performance.  9. CONCLUSION  In this paper, we propose BLOCON, a bufferless Clos network  architecture to exploit silicon photonics. A 64 ൈ 64 BLOCON  reduces the thermal-tuning power of the PXBar from 2.6W to  0.39W, and has a 100% bi-section bandwidth under uniform  traffic. BLOCON applies wormhole routing and could be viewed  as an input-queued switch without reassembly queues or VCs in  the output port. The absence of multi-stage routers allows  BLOCON to enjoy high throughput with a proper scheduling  algorithm. In this paper, we proposed two scheduling algorithms -  Sustained and Informed Dual Round-Robin Matching (SIDRRM)  and Distributed and Informed Path Allocation (DIPA) to solve the  output contention and Clos network routing problem. The  simulation results show that both SIDRRM and DIPA outperform  the existing scheduling algorithms on BLOCON. The simulation  results also show that BLOCON with SIDRRM and DIPA  outperforms  the compared photonic and electrical NoC  architectures in terms of 25-ns throughput and power efficiency  under synthetic traffic patterns and SPLASH-2 traces.  10. ACKNOWLEDGMENTS  This work was supported by a Polytechnic Institute of NYU  Angel Fund grant and by U.S. Army CERDEC.  11. "
Reducing network-on-chip energy consumption through spatial locality speculation.,"As processor chips become increasingly parallel, an efficient communication substrate is critical for meeting performance and energy targets. In this work, we target the root cause of network energy consumption through techniques that reduce link and router-level switching activity. We specifically focus on memory subsystem traffic, as it comprises the bulk of NoC load in a CMP. By transmitting only the flits that contain words predicted useful using a novel spatial locality predictor, our scheme seeks to reduce network activity. We aim to further lower NoC energy through microarchitectural mechanisms that inhibit datapath switching activity for unused words in individual flits. Using simulation-based performance studies and detailed energy models based on synthesized router designs and different link wire types, we show that (a) the prediction mechanism achieves very high accuracy, with an average misprediction rate of just 2.5%; (b) the combined NoC energy savings enabled by the predictor and microarchitectural support are 35% on average and up to 60% in the best case; and (c) the performance impact of these energy optimizations is negligible.","Reducing Network-on-Chip Energy Consumption Through Spatial Locality Speculation Pritha Ghoshal∗ Boris Grot† Paul V. Gratz∗ Daniel A. Jiménez‡ Hyungjun Kim∗ ∗Depar tment of Electrical and Computer Engineering, Texas A&M University †Depar tment of Computer Science, The University of Texas at Austin ‡Depar tment of Computer Science, The University of Texas at San Antonio {hyungjun, pritha9987, pgratz}@tamu.edu, bgrot@cs.utexas.edu, dj@cs.utsa.edu ABSTRACT As processor chips become increasingly parallel, an eﬃcient communication substrate is critical for meeting performance and energy targets. In this work, we target the root cause of network energy consumption through techniques that reduce link and router-level switching activity. We speciﬁcally focus on memory subsystem traﬃc, as it comprises the bulk of NoC load in a CMP. By transmitting only the ﬂits that contain words predicted useful using a novel spatial locality predictor, our scheme seeks to reduce network activity. We aim to further lower NoC energy through microarchitectural mechanisms that inhibit datapath switching activity for unused words in individual ﬂits. Using simulation-based performance studies and detailed energy models based on synthesized router designs and diﬀerent link wire types, we show that (a) the prediction mechanism achieves very high accuracy, with an average misprediction rate of just 2.5%; (b) the combined NoC energy savings enabled by the predictor and microarchitectural support are 35% on average and up to 60% in the best case; and (c) the performance impact of these energy optimizations is negligible. Categories and Subject Descriptors B.3.0 [MEMORY STRUCTURES]: General General Terms Design, Measurement Keywords Cache Design, Flit Encoding 1. INTRODUCTION While process technology scaling continues providing more transistors, the transistor performance and power gains that accompany process scaling have largely ceased [1]. Chipmultiprocessor (CMP) designs achieve greater eﬃciency than traditional monolithic processors through concurrent parallel execution of multiple programs or threads. As the core count in chip-multiprocessor (CMP) systems increases, networks-on-chip (NoCs) present a scalable alternative to Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS ’11 May 1-4, 2011 Pittsburgh, PA, USA Copyright 2011 ACM 978-1-4503-0720-8 ...$10.00. Figure 1: Percentage of cache block words utilized by applications in the PARSEC multithreaded benchmark suite. traditional, bus-based designs for interconnection between processor cores [2]. As in most current VLSI designs, power eﬃciency has also become a ﬁrst-order constraint in NoC design. The energy consumed by the NoC itself is 28% of the per-tile power in the Intel Teraﬂop chip [3] and 36% of the total chip power in MIT RAW chip [4]. In this paper we present a novel technique to reduce energy consumption for CMP core interconnect leveraging spatial locality speculation to identify unused cache block words. In particular, we propose to predict which words in each cache block fetch will be used and leverage that prediction to reduce dynamic energy consumption in the NoC channels and routers through diminished switching activity. 1.1 Motivation Current CMPs employ cache hierarchies of multiple levels prior to main memory [5,6]. Caches organize data into blocks containing multiple contiguous words in an eﬀort to capture some degree of spatial locality and reduce the likelihood of subsequent misses. Unfortunately, applications often do not fully utilize all the words fetched for a given cache block, as recently noted by Pujara et al. [7]. Figure 1 shows the percentage of words utilized in applications from the PARSEC multithreaded benchmark suite [8]. On average, 61% of cache block words in the PARSEC suite benchmarks will never be referenced and represent energy wasted in transference through the memory hierarchy. In this work we focus on the waste associated with traditional approaches to spatial locality, in particular the wasted energy and power caused by large cache blocks containing data that ultimately is not used. 1.2 Proposed Technique The goal of the proposed technique is to reduce dynamic energy consumption in CMP interconnect by leveraging spatial locality speculation on the expected used words in fetched cache blocks in CMP processor memory systems. The paper makes the following contributions: • A novel intra-cache-block spatial locality predictor, to identify words unlikely to be used before the block is evicted. • A static packet encoding technique which leverages spatial locality prediction to reduce the network activity factor, and hence dynamic energy, in the NoC routers and links. The static encoding requires no modiﬁcation to the NoC and minimal additions to the processor caches to achieve signiﬁcant energy savings with negligible performance overhead. • A complementary dynamic packet encoding technique which facilitates additional energy savings in transmitted ﬂits, reducing switching activity in NoC links and routers via light-weight microarchitectural support. In a 16-core CMP implemented in a 45-nm process technology, the proposed technique achieves an average of ∼35% savings in total dynamic interconnect energy at the cost of less than 1% increase in memory system latency. 2. BACKGROUND AND RELATED WORK 2.1 Dynamic Power Consumption When a bit is transmitted over interconnect wire or stored in an SRAM cell, dynamic power is consumed as a result of a capacitive load being charged up and also due to transient currents during the momentary short from Vdd to Gnd while transistors are switching. Dynamic power is not consumed in the absence of switching activity. Equation 1 shows the dynamic and short-circuit components of power consumption in a CMOS circuit. P = α · C · V 2 · f + t · α · V · Ishort · f (1) In the equation, P is the power consumed, C is the switched capacitance, V is the supplied voltage, and F is the clock frequency. α represents the activity factor, which is the probability that the capacitive load C is charged in a given cycle. C, V, and F are a function of technology and design parameters. In systems that support dynamic voltage-frequency scaling (DVFS), V and F might be tunable at run time; however, dynamic voltage and frequency adjustments typically cannot be done at a ﬁne spatial or temporal granularity [9]. In this work, we target the activity factor, α, as it enables dynamic energy reduction at a very ﬁne granularity. 2.2 NoC Power and Energy Researchers have recently begun focusing on the energy and power in NoCs, which have been shown to be signiﬁcant contributors to overall chip power and energy consumption [3, 4, 10, 11]. One eﬀective way to reduce NoC power consumption is to reduce the amount of data sent over the network. To that extent, recent work has focused on compression at the cache and network levels [12, 13] as an eﬀective power-reduction technique. Compression is complementary to our approach. While our work seeks to reduce the amount of data transmitted through identiﬁcation of useless words, compression could be used to more densely pack the remaining data. Researchers have also proposed a variety of techniques to reduce interconnect energy consumption through reduced voltage swing [14]. Schinkel et al. propose a scheme which uses a capacitative transmitter to lower the signal swing to 125 mV without the use of an additional low-voltage power supply [15]. In this work we evaluate our prediction and packet encoding techniques for links composed of both fullsignal swing as well as low-signal swing wires. Finally, static power consumption due to leakage currents is also a signiﬁcant contributor to total system power. However, researchers have shown that power-gating techniques can be comprehensively applied at the NoC level and are highly eﬀective at reducing leakage power at periods of low network activity [16]. 2.3 Spatial Locality and Cache Block Utilization Spatial and temporal locality have been studied extensively since caches came into wide use in the early 1980’s [17]. Several works in the 1990’s and early 2000’s focused on indirectly improving spatial locality through compile and runtime program and data transformations which improve the utilization of cache lines [18–21]. While these techniques are promising, they either require compiler transformations or program changes and cannot be retroﬁtted onto existing code. Our proposed approach relies on low-overhead hardware mechanisms and is completely transparent to software. Hardware techniques to minimize data transfer among caches and main memory have also been explored in the literature. Sectored caches were proposed to reduce the data transmitted for large cache block sizes while keeping overhead minimal [22]. With the sectored cache, only a portion of the block (a sector) is fetched, signiﬁcantly reducing both the miss time and the bus traﬃc. The proposed technique builds upon this idea by speculatively fetching, not just the missing sector but sectors (words in this case) which have predicted spatial locality with the miss. Pujara et al. examined the utilization of cache lines and showed that only 57% of words are actually used by the processor and the usage pattern is quite predictable [7]. They leverage this information to lower power in the cache itself through power gating of portions of the cache array. This mechanism is orthogonal and potentially complementary to our technique, as we focus primarily on achieving lower energy consumption in the interconnect. Qureshi et al. suggested a method to pack the used words in a part of the cache after evicting it from the normal cache, thus increasing performance and reducing misses [23]. Their work thus focuses on performance rather than energy-eﬃciency and targets the eﬀectiveness of the second-level cache. Spatial locality prediction is similar to dead block prediction [24]. A dead block predictor predicts whether a cache block will be used again before it is evicted. The spatial locality predictor introduced in this paper can be thought of as a similar device at a ﬁner granularity. The spatial locality predictor, however, takes into account locality relative to the critical word oﬀset, unlike dead block predictors. Chen et al. predicted a spatial pattern using a pattern history table which can be referenced by the pc appended with the critical oﬀset [25]. The number of entries in the pattern history table, as well as the number of indexes increase the memory requirement of the technique. Unlike these schemes, our predictor uses a diﬀerent mechanism for managing prediction thresholds in the face of mispredictions. 3. DESCRIPTION Our goal is to save dynamic energy in the memory system interconnect NoC. To this end we developed a simple, low complexity, spatial locality predictor, which identiﬁes the words expected to be used in each cache block. A used word prediction is made on a L1 cache miss, before the request packet to the L2 is generated. This spatial locality predicFigure 3: Prediction Example threshold (conﬁgured at design time), then the word is predicted to be used; otherwise, it is predicted not used. Figure 3 shows the steps to the prediction. In this example, the number of words in a block is assumed to be 4 and the threshold is conﬁgured as 1. The cache miss occurs because an instruction is accessing the second word in the missing block (critical word oﬀset = 1). The lower-order bits of the ﬁll PC select a row in the prediction table. Among the counters in the row, the selection window of 4 counters, which initially includes the four rightmost counters, moves to the left by the number of the critical word oﬀset. Thus, the number of counters per row must be twice the number of words per block, less one to account for all possible critical word oﬀsets. Then, those selected counters are translated into the used-vector based on the threshold value. The usedvector 1100 indicates that the ﬁrst and the second words in this block will be used. The L1 cache keeps track of the actual used vector while the block is live, as well as the lower-order bits of the ﬁll PC, and the critical word oﬀset. When the block is evicted from the L1, the predictor is updated with the actual used vector; if a word was used, then the corresponding counter is incremented; otherwise, it is decremented. While updating, it ﬁnds the corresponding counters with the ﬁll-PC and the critical word oﬀset as it does for prediction. In the event a tion is used to reduce dynamic energy consumption in the NoC within the cache hierarchy. Figure 2: General CMP Architecture Figure 2 depicts the general, baseline architecture, representing a 16-node NoC-connected CMP. A tile consists of a processor, a portion of the cache hierarchy and a Network Interface Controller (NIC), and is bound to a router in the interconnection network. Each processor tile contains private L1 instruction and data caches. We assume the L2 is organized as a shared S-NUCA cache [26], each tile containing one bank of the L2. The chip integrates two memory controllers, accessed via the east port of node 7 and west port of node 8. Caches have a 64-byte block size. The NoC link width is 16 bytes, discounting ﬂow-control overheads. Thus, cache-block-bearing packets are ﬁve ﬂits long, with one header ﬂit and four data ﬂits. Each data ﬂit contains four 32-bit words, as shown in Figure 5b. 3.1 Spatial Locality Prediction When a block must be ﬁlled in the L1 cache, a spatial locality predictor is consulted to generate a prediction of the words likely to be used. We deﬁne used-vector as a bit string which identiﬁes the words predicted used by the predictor in the cache block to be ﬁlled. A used-vector of 0xFFFF represents a prediction that all sixteen words will be used while a used-vector of 0xFF00 signiﬁes that only the ﬁrst eight words will be used. 3.1.1 Prediction Overview The principle of our predictor stems from the intuition that a set of instructions may access several memory blocks in a similar manner. In fact, we have observed that patterns of spatial locality are highly correlated to the address of the instruction responsible for ﬁlling the cache (the ﬁl l PC). We have also observed that in order to avoid aliasing and to achieve better accuracy in the prediction, the oﬀset of the word which causes the cache miss (the critical word oﬀset) is also necessary for the predictor. Thus, our predictor takes those two inputs to make the prediction. The prediction is based on the history of patterns in which cache blocks brought by a certain instruction has been accessed. The history of patterns is represented by a row of four-bit saturating counters where each counter corresponds to a word in the cache block. Our prediction table is composed of rows of these counters. The value of the saturating counter is proportional to the probability that a corresponding word is used. The lower the counter is, the higher conﬁdence that the word will not be used. Initially, all counters are set to their max value, representing a prediction where all words are used. As cache lines are evicted with unused words, counters associated with those unused words are decremented while counters associated with used words are incremented. If a given word counter exceeds a ﬁxed more energy eﬃcient approach is to begin the prediction as soon as the tag mismatch is discovered and simultaneously with victim selection in the L1 cache. While this approach would add a cycle to the L1 miss time, no time would be added to the more performance critical L1 hit time. On eviction of a L1 cache block, the used-vector and ﬁll PC collected for that block are used to update the predictor. This process is less latency sensitive than prediction since the predictor does not need to be updated immediately to provide good accuracy. 3.2 Packet Composition Once we have predicted the application’s expected spatial locality to determine the unused words in a missing cache block, we employ a ﬂit encoding technique which leverages unused words to reduce dynamic link and router energy in the interconnect between the L1, directory, L2 cache banks and memory controllers. We propose two complementary means to leverage spatial locality prediction to reduce α, the activity factor, in the NoC, thereby directly reducing dynamic energy: 1) Remove ﬂits from NoC packets (ﬂit-drop ); 2) Keep interconnect wires at ﬁxed polarity during packet traversal (word-repeat ). For example, if two ﬂits must be transmitted and all the words in the second ﬂit are predicted unused, our ﬂit-drop scheme would discard the unused ﬂit to reduce the number of ﬂits transmitted over the wire. In contrast, our word-repeat scheme would re-transmit the ﬁrst ﬂit, keeping the wires at ﬁxed polarity to reduce gate switching. The packet compositioning may be implemented either ously fetched by the NIC is placed between the cache and the NIC and helps the NIC in repeating words. An extra mux and logic gates are also necessary in the NIC to encode repeated words. Figure 5d depicts the response packet for the request in Figure 5a using the static-word-repeat scheme. In body1 , word6 and word7 are unused and, thus, replaced with word2 and word3 which are at the same location in the previous ﬂit. All of the words in body2 are repeated by the words in body1 , thus it carries virtually nothing but ﬂow-control overhead. We also encode the unused header words, if possible. 3.2.2 Dynamic Packet Composition Figure 6: Dynamic Packet Compositioning Router The eﬀectiveness of static packet compositioning schemes is reduced in two commonly-occurring scenarios: (a) when single-ﬂit, atomic packets are being transmitted, and (b) when ﬂits from multiple packets are interleaved in the channel. In both cases, repeated words in the ﬂits cannot be statically leveraged to eliminate switching activity in the corresponding parts of the datapath. In response, we propose dynamic packet composition to reduce NoC switching activity by taking advantage of invalid words on a ﬂit-by-ﬂit basis. The diﬀerence between dynamic and static composition schemes resides primarily in how word-repeat treats unused words. In static composition, the unused portion of a ﬂit is statically set at packet injection by the NIC to minimize inter-ﬂit switching activity, requiring no changes to the router datapath. In dynamic composition, portions of the router datapath are dynamically enabled and disabled based on the validity of each word in the ﬂit. In eﬀect, an invalid word causes the corresponding portion of the datapath to hold its previous value, creating the illusion of word repeat. Table 1: Area and Power baseline static dynamic Area (mm2 ) Static Power (mW) 0.073 0.71 Router with full-swing link Dynamic Power (mW) 1.47 1.02 Total Power (mW) 2.18 1.73 Router with low-swing link Dynamic Power (mW) 0.54 0.38 Total Power (mW) 1.25 1.09 0.078 0.75 0.78 1.53 0.28 1.03 is examined with the threshold value of 1 unless stated otherwise. The NoC link width is assumed to be 128 bits wide, discounting ﬂow-control overheads. NoC Link Wires: NoC links require repeaters to improve delay in the presence of the growing wire RC delays due to diminishing interconnect dimensions [1]. These repeaters are ma jor sources of channel power and area overhead. Equally problematic is their disruptive eﬀect on ﬂoorplanning, as large swaths of space must be allocated for each repeater stage. Our analysis shows that a single, energyoptimized 6 mm link in 45 nm technology requires 13 repeater stages and dissipates over 42 mW of power for 128 bits of data at 1 Ghz. In this work, we consider both full-swing repeated interconnects (ful l-swing links ) and an alternative design that lowers the voltage swing to reduce link power consumption (low-swing links ). We adopt a scheme by Schinkel et al. [15] which uses a capacitative transmitter to lower the signal swing to 125 mV without the use of an additional lowvoltage power supply. The scheme requires diﬀerential wires, doubling the NoC wire requirements. Our analysis shows a 3.5× energy reduction with low swing links. However, lowswing links are not as static-word-repeat friendly as much as full-swing links are. There is link energy dissipation on lowswing links, even when a bit repeats the bit ahead because of leakage currents and high sense amp power consumption on the receiver side. Thus, the repeated unused-words consume ∼ 20% of what used-words do. The dynamic encoding technique fully shuts down those portions of link by power (a) full-signal swing link (b) low-signal swing link Figure 8: Dynamic energy breakdown for reads We also show the average energy consumption with pure ﬂit-drop (ﬂitdrop ), static-word-repeat (s-wr ) and dynamicword-repeat (d-wr ). The bars are normalized against the energy consumed by baseline. Each bar is subdivided into energy reduction. The miss bars includes all predictions which falsely predict a word is not used. These mispredictions result in additional memory system packets, potentially increasing both energy and latency. In general, as the threshold value increases, the portions of correct and miss increase while over decreases. This implies that the higher the threshold we choose, the lower the energy consumption but the higher the latency. Experimentally we determined that Energy × Delay2 is approximately equal across the thresholds between 1 and 12. Both latency and energy consumption become worse with threshold of 15. We conservatively chose 1 for the threshold value in our experiments to reduce the incidence of redundant L1 misses, which may eﬀect performance. We ﬁnd the performance impact with this bias is negligible. On average, with this threshold, the additional latency of each operation is ∼0.8%. These results show that further energy savings could be achieved through improved predictor accuracy, which we leave to future work. Figure 9: Breakdown of Predictions Outcomes 5. CONCLUSIONS In this paper, we introduce a simple, yet powerful mechanism using spatial locality speculation to identify unused cache block words. We also propose a set of static and dynamic methods of packet composition, leveraging spatial locality speculation to reduce energy consumption in CMP interconnect. These techniques combine to reduce the dynamic energy of the NoC datapath through a reduction in the number of bit transitions, reducing α the activity factor of the network. Our results show that with only simple static packet encoding, requiring no change to typical NoC routers and very little overhead in the cache hierarchy, we achieve an average of 20% reduction in the dynamic energy of the network if full-signal swing links are used. Our dynamic compositioning technique, requiring a small amount of logic overhead in the routers, enables deeper energy savings of 35%, on average, for both full-swing and low-swing links. 6. "
All-optical wavelength-routed NoC based on a novel hierarchical topology.,"This paper proposes a novel topology for optical Network on Chip (NoC) architectures with the key advantages of regularity, vertex symmetry, scalability to large scale networks, constant node degree, and simplicity. Moreover, we propose a minimal deterministic routing algorithm for the proposed topology which leads to small and simple photonic routers. Built upon our novel network topology, we present a scalable all-optical NoC, referred to as 2D-HERT, which offers passive routing of optical data streams based on their wavelengths. Utilizing wavelength routing method along with Wavelength Division Multiplexing technique, our proposed optical NoC eliminates the need for electrical resource reservation. We compare performance of the proposed architecture against electrical NoCs and alternative all-optical on-chip architectures under various synthetic traffic patterns. Averaging through different traffic patterns, achieves average perpacket power reduction of 53%, 45%, and 95% over optical crossbar, λ-router, and electrical Torus, respectively.","All-Optical Wavelength-Routed NoC based on a  Novel Hierarchical Topology  Somayyeh Koohi   Meisam Abdollahi  Shaahin Hessabi  Sharif University of Technology  Computer Engineering Department  Tehran, Iran  koohi@ce.sharif.edu  Sharif University of Technology  Computer Engineering Department  Tehran, Iran  m_abdollahi@ce.sharif.edu  Sharif University of Technology  Computer Engineering Department  Tehran, Iran  hessabi@sharif.edu  ABSTRACT  This paper proposes a novel topology for optical Network on Chip  (NoC) architectures with the key advantages of regularity, vertex  symmetry, scalability to large scale networks, constant node  degree, and simplicity. Moreover, we propose a minimal  deterministic routing algorithm for the proposed topology which  leads to small and simple photonic routers. Built upon our novel  network topology, we present a scalable all-optical NoC, referred  to as 2D-HERT, which offers passive routing of optical data  streams based on their wavelengths. Utilizing wavelength routing  method along with Wavelength Division Multiplexing technique,  our proposed optical NoC eliminates the need for electrical  resource reservation. We compare performance of the proposed  architecture against electrical NoCs and alternative all-optical onchip architectures under various synthetic  traffic patterns.  Averaging through different traffic patterns, achieves average perpacket power reduction of 53%, 45%, and 95% over optical  crossbar, λ-router, and electrical Torus, respectively.   Categories and Subject Descriptors  B.4.3  [Input/Output  and  Data  Communications]:  Interconnections – Optics, B.4.4 [Input/Output and Data  Communications]: Performance Analysis – Simulation, B.7.1  [Integrated Circuits]:  Types and Design Styles – Advanced  technologies  General Terms  Performance, Design  Keyword   Photonic; NoC; Topology; Architecture  1. INTRODUCTION  Optics is a very different physical approach that can address most of  the problems associated with electrical interconnects such as  bandwidth, latency, power consumption, and crosstalk. Several onchip interconnect architectures have been proposed that leverage  silicon photonics for future multicore microprocessors, such as  Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  NOCS'11, May 1-4, 2011 Pittsburgh, PA, USA  Copyright 2011 ACM 978-1-4503-0720-8… $10.00  Cornell hybrid electrical/optical interconnect architecture [1],  Firefly [2] which proposes the implementation of reservationassisted single-write-multi-read buses, and HP Corona crossbar  architecture [3] which is in fact several multiple writer, single  reader buses. The Columbia optical network [4] is one of the few  that proposes on-chip optical switches. The proposed architecture  takes advantage of an electrical sub-network to set up the switches  in advance of data transmission. Hence, the network must transmit a  large amount of data to amortize the relatively high setup latency.  Optical NoC proposed by Koohi et al. [5] improves the hybrid  architecture proposed by Shacham et al. [6]. However, it cannot  eliminate the role of electrical transactions.   Almost all of the previously proposed hybrid architectures in [1],  [2], [4], [5], and [6] suffer from high latency and power overheads  for electrically resolving optical contentions. Therefore, an alloptical NoC can overcome the limitations of electrically-assisted  ONoCs. Briere et al. [7] have developed an all-optical contentionfree NoC (λ-router). However, the proposed structure is obtained at  the cost of large arrays of fixed-wavelength light sources and fast  switches which increase power consumption and area overheads.  Although recent studies on the design of optical NoCs have  addressed various network topologies, almost all of these optical  architectures [2] [4] [5] [8] [9] are built upon traditional topologies  initially introduced for electrical NoCs, such as Mesh, Torus,  Spidergon, Fat tree, and crossbar. Moreover, previously developed  optical architectures adopt routing algorithms of the corresponding  electrical topologies for optical data routing through the network.  Due to the inherently different properties of light transmission  through optical waveguides and switching elements, novel  topologies specifically developed for optical infrastructure are  inevitable to fully utilize advantages of the photonic technology.   This paper proposes a novel topology for ONoC architectures which  concerns for physical properties of light transmission, and examines  the advantages and limitations of routing data streams through  photonic switching elements. Some of  the most interesting  characteristics of the proposed topology are: i) regularity, ii) vertex  symmetry, iii) scalability, iv) constant node degree, and v)  simplicity. Built upon our novel network topology, we propose a  scalable all-optical NoC which offers all-optical on-chip routing of  data streams. Passive routing is adopted in the proposed optical  architecture which is performed by routing optical data streams  based on their wavelengths. Utilizing wavelength routing method,  our proposed optical NoC eliminates the need for electrical resource  reservation and the corresponding latency and area overheads.  Taking advantage of Wavelength Division Multiplexing (WDM)  technique, the proposed architecture avoids packet congestion  scenarios (discussed in [4], [5], and [6]). Moreover, combining  wavelength routing method with WDM technique enables the  network to simultaneously transmit multicast and unicast packets.                 • Constant node degree of four, similar to Spidergon, for  arbitrary numbers of processing cores   • High degree of connectivity, similar to Mesh (Torus)  topology, which leads to small network diameter  • Vertex symmetry   • Scalability for large scale optical networks  • Planner layout  • Local data transmission between neighbor nodes through  local optical networks   2-D layout of the proposed topology is depicted in Fig. 1.b. As  shown in this figure, small number of waveguide intersections  reduces optical insertion loss, fabrication complexity, and total cost.  3. PROPOSED ALL-OPTICAL ROUTER  ARCHITECTURE   3.1 Routing Algorithm  As shown in Fig. 1.a, the proposed topology schematically consists  of k diagonals each interconnecting m (even) local clusters of four  processing cores. For brevity, we will refer to the local clusters as  super-nodes  (SN). Hence,  for  2D-HERT  architecture  interconnecting N processing cores, we have ˚  Ÿ 0 ˫ 0 ˭ .  Based on this notation, each IP (optical router) is uniquely  determined with triplet (i,j,l) Ŵ 3 ˩  ˫  Ŵ 3 ˪  ˭ Ŵ 3 ˬ  Ÿ ,  where i and j refer to the index of the corresponding diagonal and  super-node, respectively, and l represents the index of the  processing core within the super-node.   {˩  ˪  ˬ { This paper proposes Circular-first Routing (CR) as a novel  deterministic  routing algorithm  for 2D-HERT architecture.  According to this routing scheme, for routing optical data streams  from  source  node   to  the  destination  node  {˩  ˪  ˬ {circular links are traversed first to reach the destination  diagonal (˖(cid:20)(cid:21) ). Then, radial links are taken though the destination  diagonal to reach the target super-node (˟˚(cid:23)(cid:21) ). For minimal data  routing, the proposed routing scheme may take advantage of wraparound links, depending on the target super-node’s position, to  achieve ˟˚(cid:23)(cid:21) . Finally, within the destination cluster, intra-cluster  links are utilized to reach the target node (˚(cid:24)(cid:21) ). Fig. 2 illustrates an  example of the path taken by circular-first routing in a 144-node  2D-HERT architecture.   It is straightforward to show that CR routing scheme takes the  minimal path through 2D-HERT architecture. Considering the  proposed algorithm, distance from the source node {˩  ˪  ˬ {  to the  destination node {˩  ˪  ˬ { in terms of long hops, interconnecting  super-nodes, can be computed as follows:   ˖  ˭˩JIIJ{˩ . ˩ { ˫ . IIJ{˩ . ˩ {   -˭˩J{IIJ{˪ . ˪ { ˭ . IIJ{˪ . ˪ {{  (1)  The rest of the paper is organized as follows: Section II introduces a  novel topology for optical NoCs. In Section III, we suggest a  deterministic algorithm and an all-optical router which guarantees  contention-free operation of the photonic architecture. Section IV  describes multicast and unicast data transmission scenarios through  the network. Section V explores  the developed simulation  environment. In Section VI, we present our simulation-based  experiments and discuss the results. Section VII compares the  performance of the proposed architecture with alternative all-optical  on-chip architectures. Finally, Section VIII concludes the paper.  2. PROPOSED TOPOLOGY  2.1 Topology Exploration for ONoCs  Prior to presenting our novel topology, we discuss main issues in  the design of optical network on-chip in details.   Node degree. Maintaining a small node degree in an optical NoC  facilitates router design and fabrication. Moreover, due to the tradeoff between the degree of connectivity and the network diameter,  the proposed topology should address small node degree while  retaining small network diameter.    Planner layout. Planner layout of the proposed topology enables  single layer construction of optical transmission network above the  metal stack [4], thus reducing fabrication complexity, the chip  dimensions, and the total cost.    Scalability. A scalable topology for ONoCs should address the  following properties:  • Constant node degree with increases in the network size  • Extensible: capable of inserting new routers with no or  slight modification to the existing nodes  • Simple 2-D layout for large numbers of processing cores  Symmetry. Symmetrical and regular optical on-chip architecture  facilitates the design of photonic routers and routing algorithm, and  reduces fabrication complexity.  2.2 Two Dimensional Hierarchical Expansion  of Ring Topology  Most recent ONoC architectures have been implemented on top of  Mesh or Torus topologies.  Despite their efficiency for electrical  NoCs, node degree of five (including connection to the local IP)  complicates photonic  router design  in  these architectures.  Implementing Torus topology, Shacham et al. [4] solved the  problem by introducing extra injection and ejection switches which  lead to power and area overheads. ONoC architecture proposed by  Koohi et al. [5] is built upon Spidergon topology. Despite constant  node degree of four, network diameter of the Spidergon topology  limits efficiency of large scale ONoCs.   To overcome the drawbacks of traditional topologies in optical  NoCs, this paper addresses a novel topology for optical on-chip  interconnects. The proposed topology, which is referred to as 2DHERT (Two Dimensional Hierarchical Expansion of Ring  Topology), benefits from high degree of connectivity along with  small node degree. 2D-HERT is built upon clusters of processing  cores locally connected by optical 1-D rings. Different clusters in  the proposed topology are hierarchically interconnected by global  optical 2-D rings. A sample 2D-HERT topology interconnecting 64  optical routers is shown in Fig. 1.a. As depicted in this figure, 2DHERT can be viewed as a two dimensional hierarchical expansion  of Ring topology. The main advantages of the proposed topology  are listed as follows:      where, the first term represents number of circular links, and the  second term stands for the number of radial links taken by the CR  scheme through the destination diagonal to reach the target supernode. Based on this equation, in an N-node 2D-HERT NoC  implementing CR routing scheme, network diameter equals to  ˫ Ŷ   - ˭ Ŷ  in terms of long hops.   3.2 All-optical Data Routing  SOI-based microring resonators have been explored as passive  Optical Add/Drop (OAD) filters [10] which insert (add) or extract  (drop) optical wavelengths to or from the optical transmission  stream without any electronic processing. Utilizing wavelengthselective switches, we propose an all-optical router, named as  WSOR (Wavelength-Switched Optical Router), which eliminates  optical resource reservation required in most of the previously  introduced ONoCs [4] [5] [8] [9].   3.3 WSOR Architecture  Various studies have focused on the design of efficient optical  router architectures customized for existing topologies. However,  almost all of the previously proposed optical routers benefit from  electrically controlled microring resonators [4] [5], while the  proposed wavelength-switched optical router is a passive optical  building block, and routes optical data streams according to their  wavelengths.   3.3.1 Data routing through WSOR  2D-HERT associates one (a set of) wavelength(s) to each router in  the network. For data injection, optical data streams targeted to a  specific node are modulated on its dedicated wavelength(s). Hence,  destination address is not contained in the packet but rather in the  wavelength(s) of the optical signal.   Minimum number of wavelength channels which guarantees  contention-free routing of optical packets considerably impacts  performance metrics of the optical NoC. Hence, WSOR, as a basic  building block of the proposed ONoC, should be developed to  minimize required number of wavelength channels in the network.  Regarding WDM technique, the number of required channels  depends on the maximum number of multiplexed optical flows on a  waveguide, referred to as maximum degree of multiplexing (MDM).  Assuming that only one data stream can be targeted to a specific  node at each time, MDM metric for radial and circular links can be  derived as follows.   According to circular-first routing scheme, optical data streams  passing through a radial link from SNi to SNi+1 on a diagonal,  consist of optical packets targeted to jth super-node which is located  on the same diagonal at the distance ˤ# of SNi, where ˤ# 3 ˭ Ŷ9 .  Hence, MDM for radial links equals to:  (2)  H˯˲˖˥˧-(cid:20)(cid:24)  Ÿ   ˭ Ŷ  Ŷ˭   On the other hand, optical data streams passing through a circular  link from ith diagonal (Di) to i+1th diagonal (Di+1) consist of optical  packets targeted to any super-node located on the diagonals at the  distance ˤ$ of Di, where ˤ$ 3  ˫ Ŷ . Hence, MDM for circular links  can be calculated as follows:  H˯˲˖˥˧1(cid:20)(cid:24)  Ÿ  ˭  ˫ Ŷ   ˚ Ŷ   (3)  Based on above discussion, maximum degree of multiplexing in  2D-HERT ONoC is computed as the maximum of MuxDegCircular  and MuxDegRadial metrics, which equals to ˚ Ŷ . Hence, at least  ˚ Ŷ distinct wavelengths are required in an N-node 2D-HERT  architecture to guarantee contention-free operation of the network  under CR routing scheme. For this purpose, each wavelength is  associated to two different optical routers represented by {˩#  ˪#  ˬ# {  and {˩$  ˪$  ˬ${ in the case of˩# ɬ ˩$ {˭Jˤ  ˫ Ŷ {,  j1 = j2, and l1 =  l2. In other words, k diagonals of an N-node 2D-HERT architecture  are partitioned to two groups, named as wavelength groups, each  containing ˫ Ŷ consequent diagonals and ˚ Ŷ distinct wavelength  channels. Hence, it is easy to show that λk is assigned to node (i,j,l),  where˫  Ÿ 0 ˭ 0 ˩ {˭Jˤ ˫ Ŷ { - Ÿ 0 ˪ - ˬ .     Although the proposed wavelength assignment approach devotes  each wavelength channel to two different nodes in the network, the  proposed routing algorithm avoids interference of different optical  data streams, targeted to different optical routers with the same  associated wavelength channels, on inter super-node links, i.e.  radial or circular links. However, according to the multiple choices  for intra super-node data routing, the proposed routing algorithm  may lead to contention scenarios inside the local clusters. As an  example, Fig. 3.a depicts optical routing of two different data  streams shown with red and green paths in the figure. Regarding the  proposed wavelength assignment approach, the same wavelength  channel is associated to the corresponding destination nodes, and  hence, two different paths are modulated on the same wavelength  which leads to an interfere scenario on a local waveguide inside a  cluster, shown with the dotted line.   Generally, an optical flow initiated by an optical router inside a  specific super-node may interfere with another optical flow passing  through the same super-node in the case that two data streams are  targeted to different destination nodes, N1  and N2, with the same  associated wavelength channel. Assuming only one data stream can  be targeted to a specific node at each time, it is easy to show that  optical interference is possible if N1 and N2 are located at different  wavelength groups. Based on this fact, to prevent interference  scenarios, the proposed ONoC architecture utilizes path direction to  discriminate optical data streams targeted to different wavelength  groups. Specifically, clockwise direction within a super node is  taken for routing data streams to the destination nodes located at the  same wavelength group, while counter-clockwise direction is  chosen for routing data packets targeted to the other wavelength  group. Therefore, in the case of different wavelength groups for  source and destination nodes, path direction changes from counterclockwise to clockwise at the boundary of wavelength groups,  where data streams enter the target group (e.i. wavelength group at  which target node is located). As shown in the next section, the  proposed optical router is developed to passively decide proper path  direction for each optical data passing through the router on the  basis of its modulated wavelength(s). While this property may lead  to non-minimal paths inside the clusters, the proposed routing  algorithm guarantees minimal  routing  through  long hops,  interconnecting super nodes. Fig. 3.b illustrates routing paths for  two pairs of source and destination nodes depicted in Fig. 3.a.   3.3.2 WSOR Micro-Architecture  According to the proposed topology, a 4×4 optical router,  represented by (i,j,l), interconnects the corresponding local IP to  two local neighbor nodes and an adjacent super node. As shown in  Fig. 1.a, for odd values of l (either one or three), optical router is  connected to the neighbor cluster on the same diagonal through  radial links, while for even values of l (either zero or two), it is  connected to the neighbor cluster on the same circle through  circular links. In this regard, we partition optical routers in 2DHERT architecture into two groups; radial and circular routers.  According to the CR routing scheme, routing role differs for  different  types of optical router which  leads  to different  architectures for radial and circular routers.          Source S D Destination Figure 2.  Sample routing path  Figure 3.  a) Intra-cluster interference       b) Resolving interference based on path direction  3.3.2.1 Injection ports   Due to low optical loss and ultra-high bandwidth of the optical  waveguides, implementation of multicast communication with  multiple unicast transmissions turns into an efficient scheme in the  optical networks. In this regard, each optical router can inject  simultaneous packets to a set of destinations, where for n  destination nodes we have ŵ 3 J 3 ˚ . ŵ . However, since each  wavelength channel is devoted to two different nodes in the  network, wavelength-switched optical router benefits from two  injection ports to enable multicast data communication, while the  appropriate port for data injection is chosen by the processing core.  Assuming Ni as the source node, first (I1) and second (I2) injection  links of the optical router are utilized for optical data transmission  to the same and the other wavelength groups, respectively.  Considering multicasting capability, each of these channels (i.e. I1  and I2) may be responsible for transmitting simultaneous messages.   3.3.2.2 Ejection switches:   Utilizing OAD element, we augment WSOR with Ejection  Microring Resonators (EMRs) to extract (demultiplex) optical data  streams targeted to this IP from those passing through the router.   3.3.2.3 Routing switches  Considering two wavelength groups, referred to as λ1 and λ2,  routing path for a pair of source and destination nodes depends on  the relative position of target optical router with respect to the  transmitter one. Assuming source node {˩  ˪  ˬ { , destination  nodes are partitioned into seven different groups, represented by  D1,α , D1,β , Dl , Dd1 , Dd2 , D2,α , and D2,β in Fig. 4, which are  defined as follows for Ŵ 3 ˩  ˫ Ŷ . Definitions for ˫ Ŷ 3 ˩  ˫ can be simply derived with a shift of diagonal indices by ˫ Ŷ .  (4.a)  ˖#  {˞{˩  ˪  ˬ {ÉŴ 3 ˩  ˩  Ŵ 3 ˪  ˭ Ŵ 3 ˬ  Ÿ{ ˖#	  Ӝ˞{˩  ˪  ˬ {}˩  ˩  ˫ Ŷ9  Ŵ 3 ˪  ˭  Ŵ 3 ˬ  Ÿӝ ˖(cid:24)  {˞{˩  ˪  ˬ {É˩  ˩  ˪  ˪  ˬ  ˬ {   ˖#  ˞{˩  ˪  ˬ {˩  ˩  ˪ . ˭ Ŷ9 ˭Jˤ˭  ˪  ˪  Ŵ 3 ˬ  Ÿ  ˖$  ˞{˩  ˪  ˬ {˩  ˩  ˪  ˪ 3 ˪ - ˭ Ŷ9 ˭Jˤ˭ Ŵ 3 ˬ  Ÿ  (4.d)  (4.b)  (4.c)  (4.e)  ˖$  Ӝ˞{˩  ˪  ˬ {}˫ Ŷ9 3 ˩ 3 ˩ - ˫ Ŷ9  Ŵ 3 ˪  ˭ Ŵ 3 ˬ  Ÿӝ (4.f)  ˖$	  Ӝ˞{˩  ˪  ˬ {}˩ - ˫ Ŷ9  ˩  ˫  Ŵ 3 ˪  ˭ Ŵ 3 ˬ  Ÿӝ       {4.g{ WSOR architecture allows for multiplexing different optical data  streams, targeted to different destinations, on the same optical  waveguide and then demultiplexing them according to their  wavelengths. For appropriately demultiplexing optical data streams  targeted to different destination groups, defined above, wavelengthswitched optical router utilizes optically controlled microring  resonators. These switches, referred to as Routing Microring  Resonators (RMRs), also multiplex optical data streams transmitted  from the source node with those passing through the optical router.   Proposed architectures of WSOR are shown in Fig. 5 for radial and  circular routers for different values of local indices (l) depicted in  Fig. 1. While EMR of a router only drops its associated wavelength,  RMRs are labeled with their respective resonant wavelength modes.  Specifically, RMRs of router (i,j,l) labeled with λ1,α , λ1,β , λl , λd1 ,  λd2 , λ2,α , and λ2,β drop wavelength channels associated to the  destination groups D1,α , D1,β , Dl , Dd1 , Dd2 , D2,α , and D2,β ,  respectively.    4. DATA TRANSMISSION APPROACH  While most of the previously proposed ONoCs, like those proposed  in [4] and [9], implicitly prevent simultaneous optical data reception  by an optical router, 2D-HERT ONoC only limits the number of  concurrent data received by WSOR to the number of wavelength  sets dedicated to the router. The later property necessitates  implementation of control transactions to manage optical data  transmission.   4.1 Optical Destination checking  4.1.1 Control Packet Transmission  According to the trade-off between the number of wavelength sets  associated to each of unicast and multicast traffic patterns and the  probability of fully occupied channels at the target node, before  initiating optical data transmission at the source node, an optical  destination-checking packet is routed to the target node to check the  status of the respective (i.e. unicast or multicast) ejection channels.  In the case of free ejection channel, an optical acknowledgement  packet is sent back to the source node to initiate its optical  transmission phase.                                                                                                    In the case of fully occupied ejection channels at the destination  node, an optical packet is sent back to the transmitter IP and informs  it to attempt request transmission again. 2D-HERT ONoC utilizes  the request retransmission scheme proposed in [5] to postpone  optical data transmission.   4.1.2 Optical Control Infrastructure  Processing cores in each local network (i.e. super node) are  electrically connected to a control unit monitoring status of the  ejection channels in the super node. Moreover, optical control  packets (destination-checking and acknowledgement packets) are  transmitted and received by the control unit in each super node,  respectively. Considering 2D-HERT architecture, each control unit  (CU) is uniquely determined with a pair of indices (i,j) Ŵ 3 ˩  ˫  Ŵ 3 ˪  ˭ and is connected to the four optical routers indexed with  (i,j,l) Ŵ 3 ˬ  Ÿ . Hence, in an N-node 2D-HERT architecture,  number of control units is limited to ˚ Ÿ9 .   As a simple version of the proposed control architecture, various  control units are interconnected through a Single-Write MultipleRead (SWMR) optical bus [2]. As a key point, the proposed control  architecture associates one wavelength channel to each control unit.  Therefore, for control packet injection to the network, optical  stream initiated by a specific control unit is modulated on its  dedicated wavelength channel and is transmitted through the control  waveguide. Hence, utilizing WDM technique, concurrent control  packets can be targeted to the same control unit, since they are  modulated on different wavelength channels.   Each control packets is organized as follows. A one-bit identifier  specifies type of the control packet, i.e. destination-checking or  acknowledgement packets. Address field contains destination  address of the control packet which is determined with the address  of the destination control unit and a two-bit index of the target IP  inside the cluster. For acknowledgement packets, a one-bit flag  specifies status of the respective ejection channel of the target node.   Considering SWMR architecture, data transmitted through the  control bus is detected and received by all control units, where it is  converted to electrical signals and examined to determine whether  the control packet is targeted to the corresponding control unit. In  this regard, taking advantage of a single waveguide for control  packet transmission leads to a simple and low cost implementation.  However,  this simplicity comes at  the price of  increased  transmission power for control packets, since packets should be  properly received by all control units. Hence, there is a trade-off  between the number of control waveguides and the optical power  dissipated for control packet transmission.   As a power-efficient low-cost approach, our proposed control  architecture, referred  to as TOC (Transmitter-based Optical  Control) architecture, partitions ˚ Ÿ  control units to multiple  disjoint sets, while control packets targeted to each set are  transmitted through a dedicated control waveguide. In this regard,  optical transmission power reduces compared to the singlewaveguide approach, at the price of higher, yet reasonable,  implementation cost. As a case study, we considered one optical  waveguide for all control units located on the same diagonal, which  is shown in Fig. 6. As shown in this figure, k different SWMR buses  in TOC architecture are built upon Ring topology with network  diameter equal to ˚ % .   Finally, since data transmission network does not share optical  waveguides with the control buses, wavelength sets devoted for data  transmission through 2D-HERT architecture can be reused in the  proposed control architecture. Hence, TOC architecture does not  increase minimum number of wavelength channels required for alloptical operation of the network.   4.2 Optical Data Transmission   Once the existence of a free ejection channel at the destination has  been verified, optical message can be transmitted through the  optical resources without buffering.    5. SIMULATION ENVIRONMENT  5.1 All-Optical Network Simulator  We have implemented a parameterized 2D-HERT ONoC based on  OMNeT++ simulation framework [11], in which optical routers are  organized in a two-dimensional layout, as shown in Fig. 1. As a  case study, we analyze a 64-node 2D-HERT architecture which  consists of four diagonals, each interconnecting four local clusters  of four processing cores. Assuming a chip size of 1.8cm×1.8cm  [12], the proposed architecture is arranged in an 8×8 planner layout  and will be built in a future 22nm CMOS process technology.  According to chip dimensions and 2D layout for a 64-node 2DHERT from Fig. 1.b, optical data links between adjacent super  nodes have an approximate length of 1.5mm. Assuming shorter  intra-cluster links between adjacent routers within a super node  against those interconnecting adjacent local networks, intra-super  node optical links have an approximate length of 0.5 mm. We  assume a propagation velocity of 15.4ps/mm in a silicon waveguide  for the optical signals [5]. Consequently, optical delays for intersuper node and intra-super node links are calculated as 23.1ps and  7.7ps, respectively. Moreover, since for each router, local link to the  IP block is shorter than the three other links, we assume length of               0.2mm for local links, which leads to optical delay of 3ps between  each router and its associated processing core.   According to the proposed control architecture from Fig. 6, optical  links between adjacent control units on a control waveguide have an  approximate length of 2mm, which leads to optical delay value of  30.8ps between adjacent CUs. Finally, 5GHz clock speed [4] is  assumed for data modulation, transmission, and demodulation in the  network interface.   5.2 Traffic Generation   Traffic pattern in an on-chip network is defined by packet size  distribution, packet injection process, and distribution of the packet  destination. Packet sizes are uniformly distributed in the range of  [1KB, 4KB] [5]. We model inter-message gap as a Poisson random  variable with the parameter µ. Based on this parameter, the load  offered to the network (α) is defined as [4]:     I J(cid:20) I J(cid:20)
   (5)  Finally, traffic pattern in the network highly depends on the  distribution of the packet destination. Investigating efficiency of  2D-HERT ONoC, we perform various simulations under different  synthetic workloads including uniform, hotspot, local, and first  matrix transpose (FMT). In the case of local traffic, neighbor nodes  for each processing element are assumed to be located within the  same local cluster (super-node). Although the proposed architecture  is capable of data multicasting, for brevity, we only discuss unicast  data transmission in this paper.   5.3 Wavelength Assignment  Without loss of generality, we dedicate one wavelength set to each  processing core for optical data transmission. In a 64-node 2DHERT architecture, maximum degree of multiplexing is computed  as ˚ Ŷ  ŷŶ . Hence, assuming 4Tbps peak bandwidth per port,  each optical data stream should be modulated on three distinct  wavelengths at the rate of 40 Gbps. Moreover, regarding the  proposed control architecture, ˚ Ÿ  ŵź distinct wavelengths are  required for control packet transmission, which are selected among  those provided for optical data transmission. Since each control unit  is dedicated to four locally-connected IP cores, each optical control  packet is modulated on four distinct channels.  6. EXPERIMENTAL RESULTS AND  ANALYSIS  In this section, we investigate the efficiency of 2D-HERT ONoC  and compare it with an Electrical NoC (ENoC) built upon the same  topology for the same number of processing cores. Analyzing the  impact of traffic pattern in Section VII, destination distribution is  assumed to be uniform in this section. Based on the predictions  made by Shacham et al. [4], future ENoCs will route packets by  168-bit flits between adjacent routers under 5-GHz clock frequency.  Router processing delay is assumed to be 600ps [4]. Moreover, we  suppose propagation velocity of 131ps/mm in an optimally repeated  wire at 22nm technology [5]. The estimated power consumption per  unit length for electrical wires is of the order of 1 mW/mm [13].  Shacham et al. [4] have reported energy values in header processing  operations. Assuming 5-GHz clock frequency, power consumption  of buffer and crossbar units and static power consumption are equal  to 0.6mW/bit, 1.8mW/bit, and 1.75mW/bit, respectively.  6.1 Delay Analysis  Table I lists main contributions to the optical latency for data and  control packet transmission at the future 22nm CMOS process  technology and 5GHz clock frequency [5]. Based on these  parameters, Fig. 7 depicts the average optical latencies for control  and data packet  transmission  through TOC and 2D-HERT  architectures, respectively, for varying values of α. Simulation  results of average total latency for data transmission through the  proposed optical architecture, computed as the sum of control and  data transmission latencies are also shown in Fig. 7. As depicted in  this figure, for all traffic conditions, total latency for data  transmission is dominated by optical data routing through the  waveguides. Specifically, in the case of light traffic conditions,  optical latency for control packet transmission is negligible  compared to the total latency, and this ratio retains small values  even for high traffics.   Fig. 8 compares average total data transmission latency through 2DHERT ONoC with that of ENoC for varying values of α. As  depicted in this figure, traditional NoC, compared to 2D-HERT  ONoC, leads to about 2.5 and 13 times bigger average latency in the  case of low (or moderate) and high traffics imposed to the network,  respectively. Moreover, as depicted in this figure, unlike ENoC,  total latency in the proposed optical architecture does not approach  infinity in a fully loaded network. The reason is stated as follows.  The only delay component which depends on the traffic load is the  waiting time interval, computed in the destination-checking phase.  Although this component increases with traffic load, it is a limited  variable due to the ultra-high bandwidth and small propagation  delay of optical waveguides and blocking-free operation of the data  network.  6.2 Power analysis  Total loss in any optical link is computed as follows:   ˜(cid:20)T   ˜1U -  ˜  - ˜W - ˜ 1 -  ˜ - ˜1-  (6)   where, PCV is the coupling coefficient between the photonic source  and optical waveguide, PW is the waveguide propagation loss per  unit distance, PB is the bending loss, PWC represents waveguide  crossing insertion loss, PY is the Y-coupler loss, and finally, PCR is  the coupling loss from the waveguide to the optical receiver. In this  analysis, we assumed 50% coupling efficiency from the source to a  single mode waveguide [14] which leads to PCV = 3 dB.  Propagation loss of the waveguides and Y-coupler loss are set to  0.51dB/cm [15] and 3 dB, respectively. We assume PB less than 1%  per bend which leads to negligible bending loss [5]. PWC is set to  0.12 dB [6] per waveguide crossing. Finally, coupling efficiency is  assumed to be 0.6 dB [16]. In addition to optical losses in the  waveguides, optical switching elements of the 2D-HERT ONoC  impact total power dissipated in the network. As reported by [17],  an optically activated microring resonator consumes average dropport insertion loss (IL) (PMR, Drop) of approximately 0.5 dB, while its  through-port IL (PMR, Through) is only about 0.01 dB.   Electrical power is consumed in electro-optical converters. Chen et  al. [18] have predicted that power consumed by the transmitter and  receiver circuits for 1cm optical path at the future 22nm technology  are 5 mW and 0.3 mW, respectively. Utilizing these parameter  TABLE I DELAY CONTRIBUTIONS IN  2E+5 2D-HERT ONOC   Parameter  Value  Modulator driver  9.5 (ps)  Modulator  14.3 (ps)  Photo-detector  0.2 (ps)  Amplifier  Waveguide delay  4 (ps)  15.4  (ps/mm)  2E+5 1E+5 5E+4 0E+0 Data Delay Total Delay Control Delay 0.0 0.5 1.0 Offered Load           Figure 7. Delay values (ps) of 2D-HERT                                                                         values, we calculate power dissipated for transmitting an optical  data message through 2D-HERT ONoC as follows:  ˜(cid:20)(cid:24)J  ˜- 0 ˚ - ˜- [I[ 0 ˚ 5 0.5 1 1 1 1 30 3 0.3 1 1 1 1 100 10 1 0.1 ONoC OXbar LR ETorus Uniform HotSpot Local FMT Uniform HotSpot Local FMT Uniform HotSpot Local FMT             a)          b)      c)                        Figure 10.  Normalized system-level metrics (with respect to 2D-HERT) for various synthetic workloads    a) Delay    b) Power   c) Energy  8. CONCLUSIONS  This paper proposes a novel topology for ONoC architectures with  the key advantages of regularity, vertex symmetry, scalability,  constant node degree, and simplicity. Moreover, we introduced a  general all-optical routing algorithm with the key advantages of  minimalism and simplicity, which lead to simple optical routers.  Built upon our novel network topology, we proposed a scalable alloptical NoC, referred to as 2D-HERT, which offers passive routing  of optical data streams based on their wavelengths. Utilizing  wavelength routing method along with WDM technique, our  proposed ONoC eliminates the need for electrical resource  reservation and the corresponding latency and area overheads.  Moreover,  2D-HERT  architecture  enables  simultaneous  transmission of multicast and unicast packets.   We compare performance of the proposed architecture against  traditional electrical NoCs and alternative all-optical on-chip  architectures under various synthetic traffic patterns. In general, 2DHERT outperforms various optical architectures due to its simple  all-optical data network and high-speed  low-power control  architecture.   9. "
Cross clock-domain TDM virtual circuits for networks on chips.,"We propose cross clock-domain time-division-multiplexing (TDM) Virtual Circuit (VC), in short, VC, to provide delay and bandwidth guaranteed communication for NoCs with multiple clock domains. The cross-domain VC extends the synchronous VC in a single clock domain to multiple clock domains. The synchronous VCs reserve cyclic time slots at each node from source to destination for a traffic flow to use shared links without contention based on the assumption that all nodes share the same notion of time. However, when VCs pass multiple clock domains with different phases and frequencies, the assumption of global synchrony is not valid any more and consequently they cannot function correctly. This paper addresses this problem based on a typical FIFO clock domain interface. We give the conditions and a realization scheme to ensure correct packet delivery with QoS for VCs crossing multiple clock domains. We apply network calculus to analyze and derive the bounds of the packet delay and the FIFO size.","Cross Clock-Domain TDM Virtual Circuits for Networks on Chips Dept. of Electronic Systems, School of ICT, KTH - Royal Institute of Technology, Sweden Zhonghai Lu ABSTRACT We propose cross clock-domain time-division-multiplexing (TDM) Virtual Circuit (VC), in short, VC, to provide delay and bandwidth guaranteed communication for NoCs with multiple clock domains. The cross-domain VC extends the synchronous VC in a single clock domain to multiple clock domains. The synchronous VCs reserve cyclic time slots at each node from source to destination for a trafﬁc ﬂow to use shared links without contention based on the assumption that all nodes share the same notion of time. However, when VCs pass multiple clock domains with different phases and frequencies, the assumption of global synchrony is not valid any more and consequently they cannot function correctly. This paper addresses this problem based on a typical FIFO clock domain interface. We give the conditions and a realization scheme to ensure correct packet delivery with QoS for VCs crossing multiple clock domains. We apply network calculus to analyze and derive the bounds of the packet delay and the FIFO size. Categories and Subject Descriptors B.4.4 [Input/output and data communications]: Performance Analysis and Design Aids General Terms Design, Performance Keywords Quality-of-Service (QoS), Network Calculus, Network-on-Chip (NoC) 1. INTRODUCTION Network-on-Chip (NoC) is becoming an essential scalable architecture as System-on-Chip (SoC) advances to the many-core terascale computing era [15][19]. Since the birth of NoC, communication predictability, known as Quality-of-Service (QoS), has been a critical concern [2][9][11][12]. In packet-switched NoCs, when packet ﬂows traverse hop by hop, there are no guarantees in delay and bandwidth because of interferences from other ﬂows contending for shared buffers, links and crossbars. This is problematic for Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS ’11 , May 1-4, 2011 Pittsburgh, PA, USA Copyright 2011 ACM 978-1-4503-0720-8 ...$10.00. implementing embedded realtime applications, which require strict QoS even under worst-case conditions. For instance, video decoding requires that a new frame must be computed every 33 ms at a frame rate of 30 frames per second (fps) while maintaining audio and video synchronization [8]. The QoS problem is aggravated as advanced SoCs routinely encompass multiple clock domains to manage the clocking complexity as well as to increase power efﬁciency [14][17][18]. C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 C11 C12 f2 f5 f4 f3 f1 Figure 1: Cross-domain packet transmission on a 4 × 3 NoC As an example, Figure 1 shows a NoC chip ﬂoorplan where the entire network is partitioned into 12 clock domains C1 ,C2 , · · · ,C12 . Each clock domain is a network region. Four packet ﬂows, f1 , f2 , f3 , f4 , are transmitted over the NoC crossing multiple clock domains. For example, ﬂow f1 passes from domain C1 , C5 , C6 , C7 , C11 to C12 , sharing routers and links with ﬂows f2 , f3 , f4 and f5 . Dynamic contention among the ﬂows may occur not only within regions but also crossing regions. The problem we address in the paper is how to promise delay and throughput guarantees for the ﬂows crossing multiple clock domains? Our approach is to reuse the concept of Time-Division-Multiplexing (TDM) Virtual Circuits (VCs) proposed for synchronous NoCs, but extends it for multiple clock domains. For brevity, we shall use VC to represent TDM VC whenever appropriate throughout the paper. A VC is an end-to-end connection that reserves exclusive cyclic time slots in switches from a source to a destination. A packet ﬂow delivered on the VC enjoys contention-free transmission along the pre-conﬁgured route. Since there is no contention with other trafﬁc, both delay and throughput are guaranteed for the packet ﬂow. VC is a simple yet cost-effective power-efﬁcient QoS solution. As there is no blocking during the packet transmission, there is no need of queueing. It requires only one register per outport per hop in order to pipelining the packets. As the buffer is minimized, there is no queuing-resulted power consumption. Hence VC is also power efﬁcient. Though the VC provides strict guarantees with minimal buffering cost, it requires global synchronization, thus well suits for implementation in a synchronous clock domain. However, if a NoC has multiple clock regions, the global synchrony assumption becomes invalid, since each region may run at a different frequency and phase. As a consequence, VCs can only be correctly conﬁgured within clock domains, leaving the establishment of cross-domain VCs an open problem. In this work, we propose cross-domain VCs to guarantee perﬂow end-to-end communication delay and bandwidth. We focus on the issues when VCs cross clock domains, based on a general FIFO domain interface. We ﬁnd that crossing clock domains completely challenges the VC operation principle, since there exists no strict timing between different clocks. This raises not only a bandwidth reservation issue but also, more seriously, a correctness problem. We discuss all these issues in detail and propose solutions. Furthermore, we apply network calculus [3][5][6][16] to analyze the delay bound for ﬂows delivered on cross-domain VCs and determine the size of the cross-domain buffers. The rest of the paper is organized as follows. In Section 2, we discuss related work. We introduce the synchronous VCs and the FIFO domain interface in Section 3. In Section 4, we present the methods for bandwidth reservation and ensuring correct operation when VCs cross from one clock domain to another, as well as an implementation scheme. Section 5 is dedicated to the analysis of delay and backlog bounds. Finally we conclude and comment on future directions in Section 6. 2. RELATED WORK Over a decade of NoC research, various QoS methods have been proposed to realize predictable communication. In general, they can be mainly categorized as resource reservation and prioritybased scheduling mechanisms. Resource reservation is a service oriented strategy whereby shared resources (buffers and link bandwidth) are reserved for exclusive use so as to isolate interferences. TDM VCs are such a technique but requiring global orchestration. The Mango NoC [1] realizes guarantees in a clockless network by preserving virtual channels for end-to-end connections and using priority-based scheduling in favor of connections. The scheduling approach is more trafﬁc oriented such that trafﬁc is prioritized to resolve contention over shared resources. For example, QNoC [2] categorizes trafﬁc into four priority classes, and switches make priority-based switching decisions. Using resource reservation strategies can achieve hard performance guarantees, but resource utilization may be lower because reserved resources may be under-utilized. By contrast, priority-based schemes can achieve better resource utilization because resources are used on demand in a best-effort fashion. However, they can only achieve soft performance guarantees. The proposals for TDM VCs can be found in the Æthereal NoC [9] and the Nostrum NoC [12]. The Æthereal VC conﬁgures slot tables in each node to reserve cyclic time slots to use shared link bandwidth. It is open-ended, since the VC path does not form a loop. The Nostrum VC was suggested speciﬁcally for hot potato (deﬂection) routing networks where routing is fully adaptive and there is no buffering queue in switches. It is closed-loop, as it uses a special packet as a container running on a predeﬁned looped path to transport packets along the loop. The special container packet always has higher priority than normal packets whenever they contend for shared links. The looped VCs must be set up in a way such that there is no two special packets requesting for the same link at the same time. Though with a different ﬂavor, both types of VCs reserve exclusive cyclic time slots to share link bandwidth, thus both delay and throughput for ﬂows delivered on VCs are promised. Establishing contention-free VCs requires securing exclusive time slots. This is done in the VC conﬁguration phase via global orchestration by employing ad hoc or formal slot conﬁguration algorithms [11]. While both types of VCs provide strict QoS, there is a strong assumption on the global notion of time. Consequently, the VCs must be restricted to local regions in NoCs with multiple clock domains. This paper lifts this limitation and enables synchronous VCs to cross clock domains while maintaining their nice properties in delay and bandwidth guarantees. In [7], a NoC design ﬂow integrating the method of designing clock domain interfaces with traditional TDM synchronous NoC design procedure. Like in our approach, the synchronous domains guarantee QoS using TDM VC. The domain interfaces smooth the timing disruption using queuing and scheduling techniques. To avoid buffer overﬂow in the domain interfaces, they use end-to-end ﬂow control. This approach achieves smooth transition for VCs to operate crossing clock boundaries, but the paper does not give detailed discussions on the cross-domain issues. The aelite NoC [10] improves the TDM services of the Æthereal NoC by introducing ﬂit synchronicity, employing mesochronous links and asynchronous wrappers to achieve physical-level and function-level scalability. However, both works lack in formal analysis and has no results on delay bound and optimal bounded buffer size. Nevertheless, in terms of the techniques for achieving cross-domain timing and function correctness, both the aelite and our work can be founded in the latency-insensitive theory [4]. We apply network calculus for analyzing the performance of cross-domain VCs and dimensioning the cross-domain buffers. Network calculus is a theory of deterministic queueing systems based on the min-plus and max-plus algebra [6][3]. Network calculus has been very successful when applied to various networks such as ATM [3], Internet with differentiated and integrated services [3][16], and embedded realtime systems [20] etc. Very recently, it is being applied to performance analysis of on-chip networks [13]. 3. VC AND DOMAIN INTERFACE We ﬁrst describe synchronous VC and its properties [11], then introduce the FIFO interface bridging two clock domains [18]. 3.1 VC in synchronous clock domains sw1 v1 in out t 2k W E v1 b1 W N S E v2 sw3 v1 b0 0 1 2 4 5 7 3 6 8 9 w=6 w=6 v1 b1 t 0 1 2 4 5 7 3 6 8 9 w=6 w=6 v2 b3 0 1 2 4 5 7 3 6 8 9 w=6 w=6 b2 v1 v2 b2 b3 t t sw2 in out t 2k S E 2k+1 W E v2 v1 in t 2k+1 W N out v2 0 1 2 4 5 7 3 6 8 w=6 t 9 11 w=6 Figure 2: VCs, slot tables and contention-free operation VC-based communication involves two phases: conﬁguration and communication. The conﬁguration is to establish VCs before they are used for ﬂow delivery. Figure 2 shows two VCs, v1 and v2 , and the respective routing tables for the switches. There is exactly one buffer or register per outport per switch. A routing table (t , in, out ) is equivalent to a routing or slot allocation function R (t , in) = out , where t is time slot, in an input link, and out an output link. v1 passes switches sw1 and sw2 through {b1 → b2 }; v2 passes switches sw3 and sw2 through {b3 → b2 }. v1 and v2 overlap in buffer b2 , denoted v1 ∩ v2 = {b2 }. To avoid contention between v1 and v2 , packets cannot use b2 simultaneously. This requires global harmonization in reserving time slots during the VC setup phase. After the VC setup, packets are admitted to the network at their reserved time slots. As illustrated in Figure 2, v1 and v2 reserve one every two slots in each passing link, thus a bandwidth of 1/2 packets/cycle. While passing switch sw1 , v1 packets use even slots, {2, 4, 8, 10, · · · }, of b1 . In sw1 , (2k,W , E ) means that sw1 reserves its E (East) output link at slots 2k (∀k ∈ N) for its W (West) inport (R (2k,W ) = E ). As we can also see, v2 packets are admitted on odd slots 2k + 1 of b3 , and sw3 conﬁgures its odd slots for v2 to use the north output link. Since a v1 packet reaches sw2 one slot after reaching sw1 , sw2 assigns its odd slots of b2 to v1 . Similarly, sw2 allocates its even slots of b2 to v2 . As v1 and v2 interleave on the use of the shared buffer b2 and thus its associated output link, v1 and v2 do not conﬂict by conﬁguration. The contention-free conﬁguration enables ideal ﬂow composition. As we can observe, the packet stream passed over b2 is an ideal composition of ﬂows on v1 and v2 . The resulting merged ﬂow is a summation of the two packet ﬂows. We shall utilize this attribute when building the cross-domain analytical model in Section 5. Within a synchronous clock domain, VCs have the following characteristics: • VCs are conﬁgured by reserving time slots in slot tables to use shared links. Each node is equipped with a slot table. The size of the slot table determines the granularity of bandwidth reservation. The larger the table, the ﬁner-grained the link bandwidth. This enables ﬁner-grained bandwidth reservation, but with higher cost in storage. • VCs are contention-free. Once VC packets are launched into the network, they will experience no contention. Contention avoidance is guaranteed by globally orchestrating the slot tables during the conﬁguration phase. • Since there is no contention, pipelined in-order delivery of packets is performed without stall on the entire VC path, taking exactly one slot or one cycle per hop. Suppose that a VC, v, is conﬁgured, passing H hops in a synchronous clock domain C. Let N be the size of slot tables and S the number of slots that v reserves in associated slot tables. Let F and T be the clock frequency in MHz and period in μs, respectively. = 1 packet/cycle, thus F Mp/s Assume that the link bandwidth Bl (Mega packets/second). The VC guarantees maximum delay Dmax in μs and exact bandwidth Θ in Mp/s as follows: Dmax = Dinit + Dt rans = ((N − S) +H ) · T Θ = S N F (1) (2) where Dinit is the worst initial delay that a VC packet may experience when catching an earliest available reserved time slot, and Dt rans the contention-free delivery time. 3.2 Clock domain interface When packets travel from one clock domain to another, an interface is needed in order to prevent data loss and wrong sampling due to phase and frequency differences between the two clock domains. v1 Domain C1 φ1 sw1 din write inok b1 in out t 2k ES 2k+1 W E v2 φ1 φ2 dout read outok sw2 b2 b3 fifo interface v2 v1 Domain C2 φ2 t 2k+1 in out W E v1 t 2k in out W S v2 Figure 3: A FIFO clock domain interface We consider a general interface which uses shared buffers between the sender and the receiver to decouple the direct interaction between them. Figure 3 shows such a FIFO interface, which is robust and metastability-free 1 [18]. The protocol works as follows: To send a packet, the sender checks if inok is enabled. If so, it asserts write and drives din (data in). The interface accepts the packet into the buffer and de-asserts inok if it is full. At the receiver side, the interface asserts outok when data are available in the buffer. To consume a packet, the receiver asserts read and latches dout (data out). The inok and outok are used to signify whether the buffer is full or empty in order to prevent the sender from writing to a full buffer and the reader from reading an empty buffer, respectively. As illustrated in Figure 3, two VCs, v1 and v2 , pass from clock domain C1 to clock domain C2 via the FIFO interface. Since packets must traverse on the VCs without stalling, writes to the FIFO must be nonblocking, if no additional buffer is inserted between the sender and the FIFO. This means that the FIFO must always be available for writes, i.e., of conceptually inﬁnite size. As in reality, all buffers are bounded. We have to determine the FIFO size according to the upper bound of backlog in the FIFO, as we will analyze in Section 5. 4. CROSS-DOMAIN VC We address bandwidth reservation and correctness issues to enable smooth transitions for VCs to cross clock domains. We assume ﬁxed-frequency clocks with bounded skew and jitter. The link bandwidth Bl is 1 packet/cycle in all clock domains. 4.1 Bandwidth reservation Suppose that a packet ﬂow f with rate r passes clock domains Ci , C j , · · · , Ck , whose frequency is Fi , Fj , · · · , Fk , respectively. A VC, v, is conﬁgured to deliver the ﬂow. The slot table size for each domain is Ni , N j and Nk , respectively. The number of slots that v reserves is Si , S j , andS k , respectively. To reserve adequate bandwidth in all the domains, the following inequality must be satisﬁed: r ≤ Si Ni Fi ≤ S j N j Fj ≤ · · · ≤ Sk Fk Nk where {Si , S j , · · · Sk } ∈N, {Ni , N j , · · · Nk } ∈N and {Si ≤ Ni , S j ≤ N j , · · · Sk ≤ Nk }. The inequality says that the reserved bandwidth in one domain must be not less than that in the upstream domain. Given the clock frequencies a priori, Fi , Fj , · · · , Fk , there are various ways to ensure the inequality true. In principle, we can adapt either or both of the slot table size N and the number of reserved (3) 1This interface is simpliﬁed, showing only relevant signals here. slots S. In practice, we should limit the options by tuning only one parameter in order to minimize re-design, either keeping N tuning S or keeping S tuning N . To make a further choice, we have to understand the implication for tuning a parameter. If we keep N but tune S to adjust bandwidth granularity, this implies that, to transmit the same trafﬁc stream, we reserve more or less number of slots for transmission. Since the link bandwidth is still 1 packet/cycle, this means that the packet size should be changed, which further implies a packetization and depacketization procedure. This makes the cross-domain transfer unnecessarily complicated. Apparently we should and can avoid such extra overhead by simply preserving the packet size. Thus we prefer Si = S j = · · · = Sk to keep the packet size uniform as S, and limit our adaption to the slot table size N . Consequently, Inequality 3 is simpliﬁed as r ≤ S Ni Fi ≤ S N j Fj ≤ · · · ≤ S Nk Fk (4) 4.2 Write-Read synchronization While reserving bandwidth is the ﬁrst concern, there are correctness issues for crossing domain VCs. Speciﬁcally, if no special measures are taken, packets on a VC may be mistakenly transmitted to another VC while passing the FIFO domain interface due to timing mismatch. 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 8 9 t t In FIFO Out FIFO v1 v1 v1 v1 v1 v1 v1 v1 v1 v2 v2 v2 v2 v2 v2 v2 v2 p1,1 p1,1 p1,2 p1,2 p1,3 p1,3 p1,4 p1,4 p2,1 p2,1 p2,2 p2,2 p2,3 p2,3 p2,4 Figure 4: Wrong packet delivery due to timing mismatch We use the example in Figure 3 to illustrate such a situation in Figure 4, where v1 and v2 alternatively send packets to the FIFO according to their scheduled or reserved time slots, supporting a ﬂow with rate r = F1 /2 each. In the C1 domain, the slot table size N1 = 2. Thus the number of slots, S, that both VCs reserve is 1. To be concrete, we assume F1 = 0.75F2 , i.e., 3T1 = 4T2 . To satisfy Inequality (4), the slot table size of the C2 domain, N2 ≤ N1F2 /F1 = 8/3. Hence we set N2 = 2. We use pi,j to denote the jth packet on VC i. v1 sends p1,1 , p1,2 , p1,3 , p1,4 · · · , and v2 p2,1 , p2,2 , p2,3 , p2,4 · · · . In the ﬁgure, the time instants when they are sent to and read out from the FIFO are illustrated. In domain C2 , the FIFO is read out only when it is not empty. As we can see, the ﬁrst four packets p1,1 , p2,1 , p1,2 , p2,2 are read out and transmitted correctly to v1 and v2 in domain C2 , i.e., p1,1 and p1,2 to v1 , and p2,1 and p2,2 to v2 . But p1,3 is wrongly delivered to v2 , because at that particular time instant, the 5th instant in domain C2 , which is served for v1 , p1,3 is not ready (the FIFO is empty). When p1,3 is ready, it is actually v2 ’s turn (the 6th instant in domain C2 ) to read the FIFO. As a consequence, this unaligned timing results in wrong delivery. The same happens for packets p2,3 and p1,4 , which are wrongly delivered to v1 and v2 , respectively. The reason for the above misdelivery is essentially due to the mismatch in timing, since the notion of time is distinct for the two different clock domains. To resolve this, we can employ a blocking read technique. It works following three rules: • The reading-FIFO order follows exactly the writing-FIFO order, which is nonblocking. This strict mapping is guaranteed by slot conﬁgurations. For example, if the slot table at the sender side is conﬁgured with the VC access pattern, {v1 , v2 , −, v3 } (v1 , v2 and v3 reserve one every four slots, with one slot empty in each round), the FIFO will have the writing pattern, {v1 , v2 , v3 }. If the receiver follows the same pattern to blocking read the FIFO, packets will be delivered correctly to their corresponding VCs. • Whenever it is a VC’s turn to read the FIFO, it will run to completion. This means, if there is no packet in the current scheduled instant, it will continue to read at the next scheduled instant. • During the blocking read period, any other reads to the FIFO are forbidden. This means that blocking read repeats until successfully reading out and thus consuming a packet. 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 8 9 t t In FIFO Out FIFO v1 v1 v1 v1 v1 v1 v1 v1 v1 v2 v2 v2 v2 v2 v2 v2 p1,1 p1,1 p1,2 p1,2 p1,3 p1,3 p1,4 p1,4 p2,1 p2,1 p2,2 p2,2 p2,3 p2,3 p2,4 Figure 5: Correct packet delivery using blocking read For the example, if blocking read is applied, at the 5th instant, v1 reads the FIFO, but data not available. v1 continues to read at the 7th instant and gets p1,3 . During the blocking interval between 5th and 7th instant, v2 is disabled from reading the FIFO. This enables correct packet delivery, as shown in Figure 5. 4.3 Event preservation Another cross-domain correctness problem is mis-delivery due to event loss, the loss of timing information when packets are not available at reserved time slots. 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 8 9 t t In FIFO Out FIFO v1 v1 v1 v1 v1 v1 v1 v1 v2 v2 v2 v2 v2 v2 v2 v2 p1,1 p1,1 p1,2 p1,2 p1,3 p1,3 p1,4 p1,4 p2,1 p2,1 p2,2 p2,2 p2,3 Figure 6: Wrong packet delivery due to event loss Still with the example in Figure 3, we illustrate this problem in Figure 6, where reading the FIFO has an initial delay. At the 2nd time instant, v2 is supposed to write the FIFO according to the conﬁguration. But due to packet absence, nothing is written to the FIFO. As a consequence, in the receiver domain, v1 will read out p1,1 correctly, but p1,2 will be wrongly read out by v2 , and p2,1 wrongly by v1 . Event loss may occur due to the following: • Mismatch in trafﬁc source timing and reserved time slots, even if the bandwidth reservation is exact. For example, if a VC reserves S slots every N slots, a packet may miss up to N − S slots before it can be admitted on the VC. During the reserved S slots in the previous round, it may happen that no packets are available for transmission, resulting in event loss. • Distance between a trafﬁc source and the domain boundary. If a VC has H hops in the sender domain, VC packets will experience the initial H cycles to reach the domain boundary. During the initial delay, there will be no packets written from the VC to the FIFO interface. • Mismatch in bandwidth reservation and utilization. Ideally reserved bandwidth should be fully utilized. However, since the trafﬁc rate may not exactly match the reserved bandwidth, the reserved bandwidth may be under-utilized. v1 p1,1 v2 pd v1 p1,2 v2 p2,1 v1 p1,3 v2 v1 p2,2 p1,4 v2 p2,3 In FIFO Out FIFO 0 1 2 3 4 5 6 7 v1 p1,1 v2 pd v1 v2 v1 v2 v1 v2 p1,2 p2,1 p1,3 p2,2 p1,4 0 1 2 3 4 5 6 7 8 9 t t Figure 7: Correct packet delivery using dummy packet Event loss disrupts the strict event order enforced by the VC writing order. It is a severe problem because the receiver is totally unaware of its existence. This uncovers a necessary condition for correct transmission that all scheduled events must be preserved and transmitted to the FIFO even if they contain no useful data. To achieve this, one way is to introduce dummy packets. If no packets are to be transmitted on reserved slots, dummy packets, pd , are sent to the FIFO instead. Such events are meaningful because they carry the reserved timing information and the strict event order. For the example, if the dummy packet approach is employed, the packets are delivered correctly and in order, as shown in Figure 7. The requirement of preserving all events has another important implication: A VC must reserve bandwidth in a domain according to the conﬁgured bandwidth in its upstream domain rather than the rate of the ﬂow it carries. This is reﬂected in Inequality (3), (4). For Inequality (3), it will be insufﬁcient if it is changed to r ≤ Si Fi , r ≤ S j Fj , · · · r ≤ Sk Fk , as these inequalities can not ensure the preservation of all upstream events at the domain interface. Nk N j Ni 4.4 Implementation scheme To implement the above proposal, more functions are required in the boundary switches at both the sender side and the receiver side, assuming no other interface components are introduced. At the sender side, the switch outputs packets to the shared FIFO. The additional function is to send dummy packets to the FIFO whenever there are scheduled slots but no packets available. Since the switch has no idea whether a packet will in fact arrive or not, the dummy packet should always be ready to be transmitted. This function can be expressed as a boolean function X : X = P + ¯P · Pd ummy where P means a packet available (True), ¯P packet not available (False), and Pd ummy the dummy packet (always True). The receiver may consume a dummy packet through only peeking the FIFO. At the receiver side, the switch gets more complicated. The addition is a FSM controller for reading the FIFO. This controller uses the reservation information located in the slot table of the receiver, logically forming a new slot table. In this derived new slot table, all VCs using the input port (the outport of the FIFO) are listed. Thus each entry in the table has two columns, one is t , the other out . For example, an entry (4k + 1, East) (k ∈ N) means that a packet should be read out at time instants 4k + 1 to the East outport of the receiver switch. The controller will act according to this slot table using blocking read. The blocking read can be realized with the idea of passing the token. With this scheme, consuming one event requires one token, and one token allows to consume exactly one event. Initially, the token is initialized for the ﬁrst entry of the slot table. The controller starts with the ﬁrst entry to read the FIFO. If success, it will pass the token to the next entry. Otherwise, it will hold the token until reading out a packet or a dummy packet at next scheduled time slot(s) according to the entry. Note that, when a read fails (in reading out an event), the controller will continue reading only at the next scheduled time slot(s), which may not be the next slot. The token can be realized as an enable signal in hardware. read success entry 1 entry 2 entry i entry N read success read success read success receive token read failure WT RF RI read success /pass token scheduled slots Figure 8: The state diagram for reading the FIFO Figure 8 illustrates the operation of the controller using a simple state machine. At each entry, the controller stays at a super state, starting from entry 1. Each super state has three states: Wait Token (WT), Read FIFO (RF) and Read state but Idle (RI). At the WT state, it moves to the RF state whenever it gets the token. At the RF state, if success, it passes the token and goes back to the WT state; if failure, it goes to the RI state. At the RI state, this means that it is in read state but the read failed and has to wait for the next scheduled time slot. Whenever the scheduled time slot arrives according to the reservation, it resumes reading, going back to the RF state. A read success will pass the token and drive the controller to enter the next super state for the next entry. This continues cyclically throughout the system execution. 5. DELAY AND BUFFER BOUNDS Based on network calculus, we analyze the delay bound for packets transmitted over cross-domain VCs and the buffer bound for the FIFO. We also give an example. 5.1 Network calculus basics [3] In network calculus, the accumulated amount of trafﬁc is bounded by an arrival curve. A ﬂow f (t ) is the accumulated number of bits in the time interval [0, t ]. An arrival curve α(Δ) is a monotonous non-decreasing function. Flow f is constrained by α if and only if f (t2 ) − f (t1 ) ≤ α(t2 − t1 ) ∀t1 , t2 : 0 ≤ t1 ≤ t2 where t1 , t2 denote time and [t1 , t2 ] is an arbitrary time interval. Volume L + pt A A” σ + ρt β A’ B E θ E’ B’ Tmax σ L apply Eq. (5) and (6) to compute the delay bound and buffer bound, respectively. The calculated delay bound is the worst-case delay for the composed event stream. It is also the delay bound for events on each VC, since events from each of the VCs may experience the worst-case delay. t 5.3 Derive arrival curve Figure 9: Bounds for TSPEC ﬂow served by a latency-rate server. To capture a ﬂow’s average and peak characteristics, network calculus uses TSPEC (Trafﬁc SPECiﬁcation). With TSPEC, a ﬂow f is constrained by an arrival curve α(t ), which is the minimum of two linear arrival curves, α(t ) =min(L + pt , σ + ρt ) in which L is the maximum packet size, p the peak rate ( p ≥ ρ), σ the burstiness (σ ≥ L), and ρ the average (sustainable) rate. We denote this f ∼ α(L, p,σ,ρ) . When a TSPEC ﬂow, f ∼ α(L, p,σ,ρ) , is served by a node guaranteeing a latency-rate server βR,Tmax [16], where R is the minimum service rate and Tmax the maximum processing latency, the maximum delay for the ﬂow is bounded by Eq. (5) and the buffer required for the ﬂow is bounded by Eq. (6): ¯D = L + θ( p − R)+ ¯B = σ + ρTmax + (θ − Tmax )+ [( p − R)+ − p + ρ] where θ = (σ − L)/( p − ρ). The output ﬂow f is bounded by another afﬁne arrival curve α∗ (t ) = (σ + ρTmax ) +ρ t , θ ≤ Tmax ; α∗ (t ) = min{(Tmax + t )(min( p, R)) + L + θ( p − R)+ , (σ + ρTmax ) + ρt }, θ > Tmax . A TSPEC ﬂow served by a latency-rate server is shown in Figure 9. We can observe that the delay bound ¯D is the maximum horizontal distance and the backlog bound ¯B the maximum vertical distance between the arrival curve and the service curve. In the ﬁgure, ¯D is either the distance marked as AA’ or BB’; ¯B is either the distance marked as AE or A”E’. + Tmax R (5) (6) ∗ 5.2 Analytical model sw1 + α(L, p,σ,ρ) v1 v2 sw2 βR,T v1 v2 Figure 10: An analytic model for cross-domain VCs Suppose that two VCs, v1 and v2 , pass from clock domain C1 with frequency F1 to clock domain C2 with frequency F2 , as Figure 3 shows. We ﬁrst build a network calculus model for the crossdomain interface, as shown in Figure 10. The buffering space naturally models the FIFO. The input stream to the FIFO is a composed event stream coming from cross-domain VCs. The composition is a sum (+) operation implemented by the sender (sw1 ). Referring to Figure 2, we can see that the event stream passed over buffer b2 and thus the eastern outport of switch sw2 is a composed event stream from both VCs, v1 and v2 . The receiver (sw2 ) reads the FIFO, distributing the event stream to different VCs according to their scheduled slots in the slot conﬁguration table. We use a linear arrival curve α(L, p,σ,ρ) to upper bound the composed event stream and a latency-rate service curve βR,Tmax to lower bound the service that sw2 provides to the FIFO. Next, we shall determine the parameters for the arrival and service curves and then Volume (packet) 16 p = 1 ρ = 0.25 f 8 σ = 6.25 L=1 0 5 S2 S1 10 15 20 N1 = 32 25 30 35 S1 S2 40 45 t (cycle) Figure 11: Bursty event arrival process for writing the FIFO At the sender switch (sw1 ) in domain C1 , v1 and v2 reserve S1 and S2 slots, respectively, with the slot table size being N1 , where S1 + S2 ≤ N1 . After composition in switch sw1 , the combined event stream has an average rate of ρ = (S1 + S2 ) · F1 /N1 . Its peak rate p = 1 packet/cycle since VC packets may arrive continuously during a certain time interval. As the clock frequency is F1 MHz, and we assume each link can transmit one packet per cycle, perlink bandwidth is F1 Mp/s, i.e., p = F1 Mp/s. The peak burstiness equals to the packet size, i.e., L = 1 packet. Now we are to determine the average burstiness σ. To do this, we need to know exactly which slots are reserved by each VC in each round. Here we consider the worst case that events in each round are injected in sequence resulting in highest burstiness, as shown in Figure 11. From such an event arrival ﬁgure, we can easily determine the average burstiness σ by picking up two points to form two linear equations and then solving the equations. In this way, we obtain σ = (S1 + S2 )(N1 − (S1 + S2 ) +1)/N 1 . In general, if n VCs, which cross the domain boundary over a FIFO interface, reserve S1 , S2 , · · · , Sn slots in domain C j , where the slot table size N j ≥ ∑n i=1 Si , then for the combined event stream, its average burstiness σ and average rate ρ can be computed using Eq. (7) and (8), respectively, σ = (N j − n ∑ i=1 Si + 1) n ∑ i=1 Si /N j ρ = Fj n ∑ i=1 Si /N j (7) (8) 5.4 Derive service curve After deriving the arrival curve, we move on to derive the service curve. At the receiver switch (sw2 ) in domain C2 , v1 and v2 reserve the same number of slots, S1 and S2 , as in the sender, respectively, with the slot table size being N2 , where S1 + S2 ≤ N2 . This means that the service rate R = (S1 + S2 )F2 /N2 . The problem is to determine the maximum processing delay Tmax in reading the FIFO. This includes two parts, τ0 + τ, where τ0 is the clock skew of domain C2 with respect to domain C1 , and τ is the initial phase in the slot table and thus τ = (N2 − S1 − S2 )T2 . If the network is skew aligned, τ0 = 0; If not, τ0 maybe a constant or variable. τ0 can be a constant if a special circuitry is used to maintain the phase shift [14]. If the skew is a variable, worst-case analysis on the phase difference of the two clocks should be conducted. Note that, whatever value τ0 is, it only affects the FIFO size not the correctness of the VC cross-domain operations. In general, if n cross-domain VCs reserve S1 , S2 , · · · , Sn slots in their destination domain Ck , where the clock period is Tk and the ≥ ∑n slot table size Nk i=1 Si , the service rate R and maximum processing delay Tmax to the combined event stream can be calculated using Eq. (9) and (10), respectively, R = Fk n ∑ i=1 Si /Nk Tmax = τ0 + τ = τ0 + (Nk − n ∑ i=1 Si )Tk (9) (10) where R ≥ ρ and τ0 is the clock skew, which is dependent on the clocking implementation. 5.5 Analytical bounds Let us summarize the delay bound and throughput guarantee for an individual cross-domain VC. Suppose that a VC v passes from domain Ci with frequency Fi to domain C j with frequency Fj . It traverses Hi hops in domain Ci , a FIFO interface and H j hops in domain C j . Let Ni and N j be the size of slot tables, and Si and S j the number of slots that v reserves in the associated slot tables in domain Ci and C j , respectively. Then the VC provides an end-toend delay bound ¯Di, j and bandwidth Θi, j as follows: ¯Di, j = Dinit ,i + Dt rans,i + ¯D f i f o + Dt rans, j (11) Fj (12) Θi, j = Si Fi ≤ S j Ni N j where Dinit ,i is the worst-case initial delay that a VC packet may experience when catching a reserved time slot in domain Ci , Dt rans,i the contention-free transport time in domain Ci , ¯D f i f o the delay bound passing the FIFO interface and Dt rans, j the contention-free transport time in domain C j . ¯D f i f o can be computed using Eq. (5). 5.6 Analytical steps We also summarize the cross-domain delay and buffer bound analysis into three steps as follows: 1. Conﬁguration of slot tables. Given the clock frequencies of different synchronous domains and the requirements of VCs, determine the slot table size and the number of slots to be reserved per round in each clock domain. This involves both conﬁgurations in the sender and receiver domains. The conﬁguration must ensure adequate bandwidth reservation for all VCs. One may refer to Inequality (3), (4) and Eq. (8) and (9), ensuring R ≥ ρ. 2. Analysis of cross-domain arrival and service curves. For each cross-domain interface, derive an arrival curve for the combined event stream and a service curve provided to the combined stream. For the arrival curve, it can be typically expressed as α(1,1,σ,ρ) , where σ and ρ can be computed using Eq. (7) and (8), respectively. For the service curve, it can be expressed as βR,Tmax , where the service rate R can be computed using Eq. (9) and the maximum processing latency Tmax using Eq. (10). 3. Calculate delay and backlog bounds. After determining the parameters for the arrival and service curves, we can directly apply Eq. (5) and (6) to compute the delay bound and buffer bound, respectively. In the following, we give a cross-domain analysis example. 5.7 A cross-domain analysis example With the example illustrated in Figure 11 of Section 5.3, we follow the analytical steps in Section 5.6 to show how to conﬁgure slot tables, obtain the arrival and service curves and ﬁnally use the formula to calculate the bounds. To make the example more interesting, we consider three cases. Case 1: the destination domain runs faster, F1 < F2 , F1 = 200 MHz and F2 = 420 MHz; Case 2: the destination domain runs slower, F1 > F2 , F1 = 200 MHz and F2 = 90 MHz; Case 3: both domains operate with the same frequency, F1 = F2 = 200 MHz. Suppose that v1 and v2 have to support a ﬂow with rate, r, of 22.5 Mp/s each. To reﬂect the storage constraint in switches, we also assume that the maximum slot table size for any domains is 64 (Nmax = 64), i.e, at maximum 64 entries in a slot table. 1. Conﬁguration of slot tables. Assume that the slot table size in domain C1 , N1 , is set to be 32 in order to satisfy the bandwidth granularity requirement for VC reservation. Starting from N1 = 32, we determine the number of slots to be reserved in domain C1 and C2 , i.e., S1 for v1 and S2 for v2 , as well as the slot table size, N2 , for domain C2 . To reserve sufﬁcient bandwidth, we must ensure S1 = S2 ≥ rN1 /F1 . Then, S1 = S2 ≥ 3.6. We have S1 = S2 = (cid:10)3.6(cid:11) = 4. Here (cid:10)x(cid:11) is the ceiling function, which returns the smallest integer not less than x. To promise R ≥ ρ, N2 ≤ F2N1 /F1 . For Case 1, N2 ≤ 67.2. Since Nmax is 64, we choose N2 = 64; For Case 2, N2 ≤ 14.4. we choose N2 = 14. For Case 3, N2 = N1 = 32. 2. Analysis of cross-domain arrival and service curves. We ﬁrst derive the arrival curve. In our cases, S1 = S2 = 4, N1 = 32. Using Eq. (7), we have σ = 6.25 packets. Using Eq. (8), we get ρ = 0.25 packet/cycle or 0.25F1 Mp/s. Therefore the combined event ﬂow f is constrained by an arrival curve α(1,1,6.25,0.25) , as illustrated in Figure 11. Then we derive the service curve for the three cases. For Case 1, using Eq. (9), we have R = 52.5 Mp/s. With Eq. (10), we get τ = 1.333 × 10 −7 seconds. Together with the clocking condition on skew τ0 , we have Tmax = τ0 + 1.333 × 10 Therefore we have the service curve β Similarly, we can get β and β −7 . 5.25×107 ,τ0+1.333×10−7 . 5.1429×107 ,τ0+0.667×10−7 for Case 2, 5×107 ,τ0+1.2×10−7 for Case 3. 3. Calculate delay and backlog bounds. This is a straightforward step applying Eq. (5) and (6). But we have to know the clock skew τ0 before applying the formula. We assume two skew conditions for all the three cases as follows. • Skew 0: There is no skew, i.e., τ0 = 0. • Skew 1: Skew τ0 is up to 200 ns, i.e., τ0 = 0.2 μs. This assumption is not aggressive considering that a state-ofthe-art clocking scheme applied on the Itanium-2 processor with 6 MB on-die L3 cache operating at 1.5 GHz can reduce the clock skew to 24 ps [17]. Table 1: Delay and backlog bounds Case 1 (F1 < F2 ) ¯B (packets) ¯¯D (μs) Case 2 (F1 > F2 ) ¯B (packets) ¯D (μs) Case 3 (F1 = F2 ) ¯B (packets) ¯D (μs) Bound Skew 0 Skew 1 0.251 0.451 12.92 22.92 0.187 0.387 9.58 19.58 0.245 0.445 12.25 22.25 The cross-domain delay and backlog bounds are recorded in Table 1. As can be observed, given the same skew condition, the corresponding bounds are close to each other even though the clocking frequencies are quite different. This is as we expected since it is the rates of trafﬁc ﬂows rather than the domain frequencies that determine the required bandwidth, and thus the delay and backlog bounds (Refer to Eq. (5) and (6)). We can also see that the clocking skew will directly contribute to the delay bounds and increase backlogs. 6. CONCLUSION In this work, we address the problem of providing QoS for cross clock-domain communication. To achieve this goal, we reuse TDM VCs as a local solution for synchronous clock domains, and focus on the arising issues when they cross domains. Through our study, we ﬁnd that, for cross-domain VC conﬁgurations, the complication lies in bandwidth reservation but more in correctness. Due to lack of a common timing reference, events must be fully preserved over the boundary and the reading of the FIFO interface must be blocking in order to ensure that packets on different VCs are delivered correctly to their own respective VCs. Beyond these insights, we conduct solid analysis on the delay bound for cross domain ﬂows and determine the buffer size of the domain interface, based on network calculus. Examples are used throughout the paper whenever necessary. Our investigation suggests that, though sharing the same notion of time is no longer possible when VCs cross different clock domains, correctness can still be achieved and bounds on delay and interface buffer can be computed. While discussing the cross-domain guarantees, we have implicitly assumed that there is only guaranteed trafﬁc crossing clock domain boundaries in the network. However, a network typically has mixed best effort and guaranteed trafﬁc. In such cases, our recommendation is that the cross-domain best effort trafﬁc should not share the FIFO interface with VC trafﬁc so as to isolate interferences. We have focused on discussing interactions between two clock domains. We will extend our work to address the interactions among multiple clock domains. Further, the conﬁgurations at different clock domains have presumably been done statically. In the future, we shall investigate smooth transitions when the VC conﬁgurations must be done dynamically when power saving techniques such as DVFS, low power modes etc. are applied to the NoCs with multiple clock domains. Acknowledgments This work is funded in part by Swedish Research Council and Intel Corporation. 7. "
A vertical bubble flow network using inductive-coupling for 3-D CMPs.,"A wireless 3-D NoC architecture for CMPs, in which the number of processor and cache chips stacked in a package can be changed after the chip fabrication, is proposed by using the inductive coupling technology that can connect more than two known-good-dies without wire connections. Each chip has data transceivers for uplink and downlink in order to communicate with its neighboring chips in the package. These chips form a single vertical ring network so as to fully exploit the flexibility of the wireless approach that enables us to add, remove, and swap the chips in the ring. To avoid protocol and structural deadlocks in the ring network, we use the bubble flow control which is more flexible and efficient compared to the conventional VC-based deadlock avoidance. We implemented a real 3-D chip that has on-chip routers and inductive-coupling data transceivers using a 65nm process in order to show the feasibility of our proposal. The vertical bubble flow control is compared with the conventional VC-based approach and vertical bus in terms of the throughput, hardware amount, and application performance using a full system CMP simulator. The results show that the proposed vertical bubble flow network outperforms the VC-based approach by 7.9%-12.5% with a 33.5% smaller router area.","A Vertical Bubble Flow Network using Inductive-Coupling for 3-D CMPs ∗ Hiroki Matsutani1 , Yasuhiro Take2 , Daisuke Sasaki2 , Masayuki Kimura2 , Yuki Ono2 , Yukinori Nishiyama2 , Michihiro Koibuchi3 , Tadahiro Kuroda2 , and Hideharu Amano2 1The University of Tokyo 2Keio University 7-3-1 Hongo, Bunkyo-ku, Tokyo, Japan 3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Japan matutani@hal.ipc.i.u-tokyo.ac.jp cube@am.ics.keio.ac.jp 3National Institute of Informatics 2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan koibuchi@nii.ac.jp ABSTRACT A wireless 3-D NoC architecture for CMPs, in which the number of processor and cache chips stacked in a package can be changed after the chip fabrication, is proposed by using the inductive coupling technology that can connect more than two known-good-dies without wire connections. Each chip has data transceivers for uplink and downlink in order to communicate with its neighboring chips in the package. These chips form a single vertical ring network so as to fully exploit the ﬂexibility of the wireless approach that enables us to add, remove, and swap the chips in the ring. To avoid protocol and structural deadlocks in the ring network, we use the bubble ﬂow control which is more ﬂexible and eﬃcient compared to the conventional VC-based deadlock avoidance. We implemented a real 3-D chip that has onchip routers and inductive-coupling data transceivers using a 65nm process in order to show the feasibility of our proposal. The vertical bubble ﬂow control is compared with the conventional VC-based approach and vertical bus in terms of the throughput, hardware amount, and application performance using a full system CMP simulator. The results show that the proposed vertical bubble ﬂow network outperforms the VC-based approach by 7.9%-12.5% with a 33.5% smaller router area. ∗ This research was performed by the authors for STARC as part of the Japanese Ministry of Economy, Trade and Industry sponsored “Next-Generation Circuit Architecture Technical Development” program. The authors thank to VLSI Design and Education Center (VDEC) and Japan Science and Technology Agency (JST) CREST for their support. This work was also supported by Grant-in-Aid for JSPS Fellows. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS ’11, May 1-4, 2011 Pittsburgh, PA, USA Copyright 2011 ACM 978-1-4503-0720-8 ...$10.00. Categories and Subject Descriptors C.1.2 [Computer Systems Organization]: Multiprocessors—Interconnection architectures General Terms Design, Performance Keywords On-chip networks, many-core, 3-D ICs, inductive-coupling 1. INTRODUCTION The three-dimensional Network-on-Chip (3-D NoC) [26] is an emerging research topic exploring the network architecture of 3-D ICs that stack several smaller wafers or dies in order to reduce the wire length and wire delay. Regarding the 3-D NoC architecture, the network topology [17, 22], router architecture [11, 13, 21], and routing strategy [24] have already been extensively studied. Various interconnection technologies have been developed to be used as a medium for the 3-D NoCs: wire-bonding, micro-bump [4, 12], wireless (e.g., capacitive or inductive coupling) [7, 9, 19, 20] between stacked dies, and throughsilicon via (TSV) [5, 7] between stacked wafers. The comparisons of these 3-D IC technologies are discussed in [7]. Many recent studies on 3-D NoC architecture focus on the TSV that oﬀers the largest interconnect density. Unlike them, we focus on the inductive coupling that can connect more than two known-good-dies without wire connections, because it oﬀers a large degree of ﬂexibility to build the desired 3-D chip multiprocessors (3-D CMPs). In this paper, we propose a wireless NoC architecture for 3-D CMPs, in which the number of processor and cache chips stacked in a package can be changed after the chip fabrication by using the inductive coupling. Each chip has data transceivers for uplink and downlink in order to communicate with its neighboring chips in the package. These chips form a single vertical ring network so as to fully exploit the ﬂexibility of the wireless stacking that enables us to add, remove, and swap the chips in the ring. To avoid deadlocks in the ring network, we use the bubble ﬂow control [1, 23] that does not require any virtual channels (VCs), because the conventional VC-based deadlock avoidance techniques limit the ﬂexibility of the wireless 3-D CMPs, depending on the number of VCs available. We implemented a real 3-D chip that has on-chip routers and inductive-coupling data transceivers using a 65nm process. The vertical bubble ﬂow control is compared with the conventional VC-based approach and vertical bus in terms of the throughput, hardware amount, and application performance using a full system CMP simulator. The rest of this paper is organized as follows. Section 2 surveys existing 3-D IC technologies. Section 3 proposes the wireless 3-D NoC architecture and Section 4 illustrates the test chip implementation. Section 5 shows the evaluation results and Section 6 concludes this paper. 2. 3-D IC TECHNOLOGY Interconnection technologies for 3-D ICs are classiﬁed into the wired and wireless approaches. In this section, we ﬁrst survey both approaches, and then advantages of the inductive coupling for our purpose are shown. 2.1 Wired 3-D Interconnection (cid:129) Wire-bonding is a die-to-die interconnection using bonding wires. It is a popularly utilized technique for the current System-in-Package. However, since only edges of a chip can be used for the bonding, the number of wires and their density are limited. Also, the long bonding wires (cid:129) Micro-bump [4, 12] is a die-to-die interconnection through cause a considerable delay for communication. solder balls. This approach is limited to the face-to-face connection of only two dies. (cid:129) Through-silicon via (TSV) [5, 7] is a wafer-level interconnection that uses via-holes formed through multiple wafers. The footprint of the TSV is small, and highdensity implementation is possible. However, the fabrication cost is increased due to the extra process steps for forming the TSVs. Although more than two wafers can be connected, the number and order of the wafers cannot be changed after the fabrication, since it is a wired approach. Although these wired approaches have been utilized in real products, they are not enough ﬂexible for customizing the chips in a package in response to application requirements. 2.2 Wireless 3-D Interconnection (cid:129) Capacitive coupling [9] connects the known-good-dies without wire connections. However, only the face-to-face connection is allowed; so the number of stacked dies is limited to only two. (cid:129) Inductive coupling [7, 19, 20, 25] also supports a wireless interconnection between the known-good-dies, and more than two dies can be stacked. As an interconnection technology for custom 3-D CMPs, the inductive coupling is hopeful, since it can stack a number of known-good-dies wirelessly. That is, addition, removal, and swapping of processor or cache chips are possible after the chip fabrication in response to the application. Recently, the inductive coupling techniques have been improved, and the contact-less interface without an electrostatic-discharge (ESD) protection device achieves high speed more than 1GHz with a low energy dissipation (0.14pJ per bit) and a low bit−12 ) [19]. error rate (BER< 10 CPU 0 CPU 1 CPU 2 CPU 3 CPU 4 CPU 5 CPU 6 CPU 7 L1 I/D cache L2 cache bank Memory controller On-chip router Figure 1: Baseline 2-D CMP In this approach, an inductor is implemented as a square coil with metal in common CMOS layout (see Figure 6 for the layout). The data modulated by a driver are transferred between two coils placed at exactly the same position of two stacked dies, and it is received at the other chip by the receiver. Here, a set of a driver and a coil for sending data is called TX channel, while one with a receiver and a coil is RX channel. If a TX channel is placed at the same location of multiple RX channels in diﬀerent chips, multicast of the data can be done. On the other hand, stacked multiple TX channels at the same location cannot send the data simultaneously in order to avoid the interference. The footprint of the channel ranges from 30µm × 30µm [19, 20] to 150µm × 150µm [7], depending on the process technology and communication distance (i.e., chip thickness). In addition, a number of TX and RX channels can be implemented for the parallel data transfer, depending on the required vertical bandwidth. A 1-Tbit/sec inductivecoupling inter-chip clock and data link has been developed by using 1024 transceivers arranged with a pitch of 30µm [20]. The inductive coupling has been applied to various purposes, such as multi-core processors and dynamically reconﬁgurable processors [25]. The next section proposes the wireless 3-D NoC architecture for CMPs, in which processor and cache chips are stacked wirelessly using the inductive coupling links. 3. WIRELESS 3-D CMP ARCHITECTURE We apply the inductive coupling to build the wireless 3-D CMP, in which the numbers of processors and cache banks can be customized in response to the application performance and cost requirements. In this section, ﬁrst the baseline 2-D CMP architecture is illustrated. Then, the wireless 3-D CMP architecture is proposed. 3.1 Baseline 2-D CMP Architecture Figure 1 illustrates an example of CMP inspired by [3], in which eight processors (or CPUs) and 32 shared L2 cache banks are interconnected by sixteen on-chip routers. These L2 cache banks are shared by all processors; so the cache architecture is SNUCA [10] and a cache coherence protocol is running on it. We extend this CMP to 3-D. Type A (CPU & Cache) CPU Type B (CPU & CPU) CPU CPU Type C (Cache & Cache) Plane #7 Plane #6 Plane #5 Plane #4 Plane #3 Plane #2 Plane #1 Plane #0 L1 I/D cache L2 cache bank Memory controller On-chip router Figure 2: Proposed wireless 3-D CMP. Type B for computation-bound applications while Type C for memory-bound ones. 3.2 Wireless 3-D CMP Overview Figure 2 illustrates the proposed wireless 3-D CMP architecture. The baseline 2-D CMP is divided into a number of planes, each of which has on-chip routers, processors, and/or cache banks. These planes are stacked vertically and connected by using the inductive coupling links. Memory controllers and external I/Os are connected to the bottom chip. We assume this simple 3-D SNUCA as a baseline for simplicity, although various optimization techniques for 3-D SNUCA are discussed in [14]. Three types of planes are illustrated in this ﬁgure: Type A (CPU and cache), Type B (CPU and CPU), and Type C (cache and cache). Typically, application performance is limited by either memory bandwidth or computation power; thus the applications can be classiﬁed into memory-bound and computation-bound ones [18]. Depending on the set of target applications, the wireless 3-D CMP using the inductive coupling enables us to customize the number and types of planes stacked in a package after the chip fabrication, such as more Type B planes for computation-bound applications while more Type C planes for memory-bound ones. This ﬂexibility is attractive, because designing a new mask pattern for each set of applications is too costly in recent advanced process technologies. This is the ﬁrm advantage of the wireless 3-D CMPs over the conventional ones. 3.3 Network Design Here, we consider the intra-plane and inter-plane networks separately. Their requirements are summarized as follows. (cid:129) Intra-plane network connects processors and cache banks on a single chip by using on-chip wires. Various types of chips will be stacked depending on the application requirements in the case of wireless 3-D CMPs; thus we should not expect any pre-determined network topology on it as long as each chip has data transceivers for uplink and downlink in pre-speciﬁed locations. That is, some chips may have 2-D mesh based intra-plane network, while the others may not have any intra-plane network except for the data transceivers for vertical communication. Needless to say, each intra-plane network itself must be deadlockfree if it exists, because adding “deadlocked chips” will kill the whole CMP. (cid:129) Inter-plane network connects the data transceivers of all intra-plane networks in a package wirelessly by using the inductive coupling and forms a single vertical network. It should be simple and highly tolerable for adding, removing, and swapping the chips in a package. Of course, it must be deadlock-free. To meet the above mentioned requirements, in this paper, we employ a uni-directional ring network for the inter-plane network as shown in Figure 2, because it is simple and easy to add, remove, and swap the nodes in a ring network without updating any routing information. Typically, the downside of uni-directional ring is poor scalability on the throughput and communication latency. However, the inductive coupling link can oﬀer ample throughput (e.g., 1-Tbit/sec using 1024 transceivers [20]). Also, communication latency is still reasonable for up to eight chips in a package, or we can just duplicate another uni-directional ring to form a single bi-directional ring. We will conﬁrm this using a full system simulator assuming latency-sensitive shared-memory CMPs in Section 5. Note that a ring network inherently contains a cyclic dependency, which introduces structural deadlocks. The router and ﬂow control design for deadlock-freedom will be discussed in Section 3.4. To sum up the network design for the wireless 3-D CMP, (cid:129) Network design rule 1: Each chip has a pair of data we summarize the rules each chip must comply as follows. transceivers for uplink and downlink in pre-speciﬁed loca(cid:129) Network design rule 2: tions for enabling the vertical communication. All processors and cache banks are connected to the data transceivers on the same chip via the intra-plane deadlock-free network. (cid:129) Network design rule 3: Only the intra-plane networks for the top and bottom chips must have a horizontal link that connects the uplink data transceiver and downlink data transceiver. Network design rule 3 is required to form a single unidirectional ring (see Planes 0 and 7 in Figure 2). Otherwise, no such connection is needed. 3.4 Router and Flow Control Design This section discusses the router and ﬂow control design for the deadlock-free inter-plane ring network. Various deadlock-free strategies have been used for ring networks, and they are summarized as follows. (cid:129) VC-based approach is the most conventional deadlock avoidance technique for rings. Two VCs (e.g., VC-0 and VC-1) are at least required for each message class. Packets are ﬁrstly transferred with VC-0 while VC-1 is used after they go over a wrap-around channel or dateline in the ring. Thus, the cyclic dependency of a ring is cut at the dateline. (cid:129) Bubble ﬂow approach [1, 23] is also a deadlock avoidance technique for rings with virtual cut-through (VCT) switching. It does not require any VCs but requires a single buﬀer with capacity of at least two packets for each input port. By limiting the packet injection not to consume all the buﬀer resources in a ring, packets on the ring continuously move without any deadlocks. Plane #7 Plane #6 Plane #5 Plane #4 Plane #3 Plane #2 Plane #1 Plane #0 To upper From lower To Plane #6 From To Plane #5 From To Plane #4 From 1-packet buffer space (       Occupied        Empty) Figure 3: Vertical bubble ﬂow network (cid:129) Detection and recovery approach [6] detects deadlocks when they really occur. Then, it recovers from the deadlock situation by discarding related packets or moving them to another network resource, such as escape VCs. The detection and recovery approach requires a deadlock detection mechanism for routers. It also imposes a certain performance overhead for the recovery. Since a simple deadlock-free mechanism is preferable for on-chip purposes, we focus on the deadlock avoidance in this paper. 3.4.1 VC-Based Approach Considering the avoidance approaches, we believe that the bubble ﬂow approach is more reasonable compared to the VC-based one in shared-memory CMPs that rely on cache coherence protocols, in which packets with multiple message classes (e.g., cache request and reply) are transferred [27]. The downslide of the conventional VC-based approach is the hardware complexity of additional VCs required for removing the dependencies between request and reply messages of a protocol. That is, it requires a separate virtual network for each message class to avoid the protocol deadlocks. Since it requires two VCs for each message class to avoid the structural deadlocks, for example, six VCs are required for MOESI directory-based protocol that deﬁnes three messages classes, while ten VCs are required for MSI/MOSI directory-based protocol with ﬁve message classes. Router hardware complexity increases as the number of VCs increases. However, the most serious problem of the VC-based approach is that the communication protocol to be used is limited, depending on the number of VCs available. For example, if the network equips only four VCs, it never uses the MOESI directory-based protocol that requires six VCs. Such limitation will harm the ﬂexibility of the wireless 3D CMPs that can customize their structure after the chip fabrication. On the other hand, the bubble ﬂow approach stated below does not rely on any VCs for avoiding the protocol and structural deadlocks. 3.4.2 Vertical Bubble Flow The bubble ﬂow control [1, 23] is applied to the inter-plane vertical ring network of the wireless 3-D CMP. Here, we illustrate its behavior using Figure 3. In this ﬁgure, each input port of the routers has a buﬀer with capacity of three packets. Each white box indicates an empty buﬀer with capacity of a single packet while gray box is an occupied one. The routers must follow the following ﬂow control rules. (cid:129) Flow control rule 1: A packet on a ring can move to the next hop along the ring when the input buﬀer of the next hop has an empty space of at least one packet. (cid:129) Flow control rule 2: An intra-plane network can inject a packet to a ring when the vertical input buﬀer of the ingress router has an empty space of at least two packets. For example, Planes 4 and 5 can inject a packet to the ring, while Plane 6 cannot. (cid:129) Flow control rule 3: A packet can exit from a ring only when the horizontal output buﬀer of the egress router has an empty space of at least one packet. Otherwise, the packet must go around the ring again (i.e., miss routing). For example, packets destined to Planes 4 and 5 can exit from the ring, while that to Plane 6 cannot exit and must go around the ring until an empty space appears in the horizontal output buﬀer. Based on the above mentioned rules, the packet injection is limited so as not to consume all the buﬀer resources in a ring, which guarantees that packets on the ring can continuously move. As long as the packets are continuously moving on the ring, no deadlocks occur even though multiple message classes coexist in the same virtual network. For more details about the bubble ﬂow control, refer to [1] and [23]. Adding these rules to the conventional VCT router is simple. To conﬁrm this, we implemented test chips to build the vertical bubble ﬂow network using the inductive coupling links in Section 4. Note that the miss routing, in which packets go around the ring again, signiﬁcantly degrades the network performance; so we use routers with deep input buﬀers that can absorb three packets in order to suppress the miss routing. We compare the VC-based approach and vertical bubble ﬂow control in terms of the application performance on CMPs in Section 5. 4. TEST CHIP IMPLEMENTATION To show the feasibility of our proposal, we implemented a real test chip that has on-chip routers and inductive-coupling data transceivers using a 65nm process. Figure 4 shows the top view of the test chip architecture, and Figure 5 shows the side view when four test chips are stacked. This test chip supports the following two communication schemes. a) Vertical bubble ﬂow: Each chip has a pair of data transceivers for uplink and downlink. The data transceiver for uplink receives data from a neighboring lower chip and transmits the data to the neighboring upper chip. The downlink is used for the opposite direction of the uplink. b) Vertical broadcast bus: Each chip uses a single data transceiver that can switch its communication modes (i.e., TX and RX modes) in a system clock cycle. All chips share the same clock counter, and an 8-cycle time-slot is assigned for each chip periodically. A chip transmits data (if any) with TX mode when a time-slot is assigned to it. Otherwise, it must listen with RX mode. Table 1: Design parameters of test chip Process technology Fujitsu CS202SZ 65nm 2.1mm×2.1mm Chip size System clock 200MHz Router input buﬀer 16-ﬂit FIFO Flit size 32-bit data + 3-bit control Packet size 150µm×150µm 5-ﬂit Inductor for bubble 250µm×250µm Inductor for bus Inductor bandwidth 35 [bit/cycle/channel] (2) Downlink (1) Test core (3) Uplink RX TX CK CK TX RX Core    0 Core    1 TX RX CK CK RX TX Bonding wires for power, clock,  chip ID, etc... TX RX CK TX RX CK TX RX CK TX RX CK (4) Vertical bus On-chip router 35-bit data 2-bit credit Figure 4: Test chip architecture (top view) RX TX TX RX Plane #3 RX TX TX RX Plane #2 RX TX TX RX Plane #1 RX TX TX RX Plane #0 RX TX RX RX a) Vertical bubble flow b) Vertical broadcast bus Figure 5: Test chip architecture (side view) We chose the conservative design parameters for this chip. Table 1 lists the design parameters, and Figure 6 shows the layout of the test chip. We can duplicate the inductors for parallel transfer if a higher vertical bandwidth is required. The test chip can be divided into the following four parts, as shown in Figures 4 and 6. 1) Test core consists of two cores and two routers. The core has a packet generator and a packet receiver with a 45-bit packet counter. One router is connected to the downlink data transceiver and another for the uplink. Although these two routers are connected via bi-directional on-chip wired link, only the top and bottom chips use one unidirectional link to form an inter-plane vertical ring. 2) Downlink and 3) uplink inductors consist of two pairs of TX and RX channels. One TX and RX pair is used for 35-bit ﬂit transfer, while another pair for 2-bit credit back for ﬂow control. The wireless data are transferred serially using the doubled communication rate of a 4GHz local clock shared by neighboring two chips. Thus, a TX channel can transmit a 35-bit ﬂit in each 200MHz system clock. 4) Vertical bus inductors consist of four TX/RX channels and four clock channels. The TX/RX channel works as TX mode when a time-slot is assigned to the chip, while it is in RX mode otherwise. In the vertical broadcast bus, only one TX/RX channel among four is used, depending on the chip ID, as shown in Figure 5 (b). For example, only the leftmost channel is used in Plane 0. Although this is ineﬃcient for real use since the other three channels are not used, in this test chip, we chose this architecture in order to implement and test both Figure 6: Layout of test chip vertical bubble ﬂow and vertical bus by using a single mask pattern. The test chip is still under fabrication as of December 2010. Its test and evaluations are our future work. 5. EVALUATIONS The vertical bubble ﬂow control is compared with the conventional VC-based approach and vertical bus in terms of the throughput, application performance, and hardware amount. 5.1 Zero-Load Latency Zero-load latency for the uni-directional ring, T0,ring , is calculated as T0,ring = (H + 1)Trouter + H Tlink + L/BW, (1) where H is average hop count and L is packet length. Trouter and Tlink are latencies for transferring a header ﬂit on a router and a link, respectively. Zero-load latency for the vertical bus, T0,bus , is calculated as T0,bus = Tlink + L/BW + Tslot/N N −1(cid:2) i=0 i, (2) where N is number of chips stacked and Tslot is length of each time-slot. The rightmost term indicates the average waiting time to be assigned a time-slot. Here, we assume the following three traﬃc patterns. Table 2: Zero-load latency (N = 4, 6, 8) [cycle] Traﬃc Topology N = 4 N = 6 N = 8 Uniform Vertical ring 19 25 31 Neighbor Vertical ring 10 10 10 Adversary Vertical ring 28 40 52 Any Vertical bus 18 26 34 ] e r o c / l e c y c / s t i l f [ t u p h g u o r h t k r o w t e N  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0 ] e r o c / l e c y c / s t i l f [ t u p h g u o r h t k r o w t e N  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0 Bubble (15-flit) 2-VC (15-flit) Vertical bus 2-VC (10-flit) 2-VC (15-flit) 2-VC (20-flit) 2-VC (30-flit) Bubble (15-flit) Uniform Neighbor Traffic patterns Adversary Figure 7: Network throughput (N = 4) Vertical bus 2-VC (10-flit) 2-VC (15-flit) 2-VC (20-flit) 2-VC (30-flit) Bubble (15-flit) Bubble (15-flit) 2-VC (15-flit) Uniform Neighbor Traffic patterns Adversary Figure 8: Network throughput (N = 8) (cid:129) Uniform traﬃc: A source node sends packets to randomly selected destinations. Assuming each chip has two nodes, H = N . (cid:129) Neighbor traﬃc: A source node sends packets to the nearest destination. Thus, H = 1. (cid:129) Adversary traﬃc: A source node sends packets to the farthest destination. Thus, H = 2N − 1. Table 2 shows the zero-load latencies for these traﬃc patterns, assuming L = 5, Trouter = 2, Tlink = 1, and Tslot = 8. Zero-load latency for the vertical bus is constant regardless of the traﬃc patterns applied. In the case of uniform traﬃc, zero-load latencies for the vertical ring and vertical bus are comparable. 5.2 Network Throughput RTL simulations of the vertical bubble ﬂow, conventional VC-based approach, and vertical bus are performed to measure their network throughputs for uniform, neighbor, and adversary traﬃcs. In the vertical bubble ﬂow control, we implemented a 15ﬂit FIFO buﬀer for each input port. In the VC-based approach, two VCs are required for deadlock-freedom, assuming a single message class. Here, 2-VC (n-ﬂit) means each input port has two VCs, each of which has an (n/2)-ﬂit FIFO buﬀer. The exception is 2-VC (15-ﬂit). Because VCT Table 3: Processor parameters (N = 4, 8) Processor UltraSPARC-III L1 I/D cache size 64 KB (line:64B) # of processors 4/8 L1 cache latency 1 cycle L2 cache bank size 256 KB (assoc:4) # of L2 cache banks 16/32 L2 cache latency 6 cycle Memory size 160 (± 2) cycle 4 GB Memory latency # of memory controllers 2 Table 4: Network parameters (N = 4, 8) Topology Uni-directional ring # of routers 8/16 Router pipeline [RC/VSA][ST][LT] Flit size 128 bit Protocol MOESI directory # of message classes 3 Control packet size 1 ﬂit Data packet size 5 ﬂit switching is used for this router, 2-VC (15-ﬂit) indicates the average throughput of the following two sub-conﬁgurations: 1) a 5-ﬂit buﬀer for VC-0 and a 10-ﬂit buﬀer for VC-1, and 2) a 10-ﬂit buﬀer for VC-0 and a 5-ﬂit buﬀer for VC-1. Notice that the buﬀer requirements of Bubble (15-ﬂit) and 2-VC (15-ﬂit) are the same. Figures 7 and 8 show their network throughputs for 4-chip and 8-chip networks, respectively. As shown, Bubble (15ﬂit) outperforms 2-VC (15-ﬂit) and is almost comparable to 2-VC (30-ﬂit). Notice the throughput of Vertical bus is quite low compared to the ring-based networks and is constant regardless of the traﬃc patterns. 5.3 Application Performance Full system simulations of the wireless 3-D CMPs that stack four and eight Type A chips (see Figure 2) are performed to measure the real application performance. As their communication schemes, the vertical bubble ﬂow, conventional VC-based approach, and vertical bus are compared. Tables 3 and 4 list the processor, memory, and network parameters. Each Type A chip has one processor, four shared L2 cache banks, and two on-chip routers. Two memory controllers are connected to the bottom chip, as shown in Figure 2. The cache architecture is SNUCA [10]. To simulate the wireless 3-D CMPs, we use a full-system multi-processor simulator: GEMS [16] and Wind River Simics [15]. We modiﬁed a detailed network model of GEMS, called Garnet [2], to accurately simulate the vertical bubble ﬂow, conventional VC-based approach, and vertical bus. A directory-based MOESI coherence protocol that deﬁnes three message classes is used. Thus, the VC-based approach requires six VCs for each input port, because each message class requires two VCs for avoiding structural deadlocks. We used default VC assignments of GEMS for this protocol. Here, 6-VC (n-ﬂit) means each input port has six VCs, each of which has an (n/6)-ﬂit FIFO buﬀer. We compare 6-VC (18-ﬂit), 6-VC (30-ﬂit), and Bubble (15-ﬂit). Because          0  20  40  60  80  100  120  140 BT CG DC EP FT IS LU MG Benchmark programs SP UA Average E u c e x i t n o i t m e ( o n r m a i l d e z ) [ % ] Vertical bus (100%) -12.5% 6-VC (18-flit) 6-VC (30-flit) Bubble (15-flit) Figure 9: Application performance (N = 4)  0  20  40  60  80  100  120  140 BT CG DC EP FT IS LU MG Benchmark programs SP UA Average E u c e x i t n o i t m e ( o n r m a i l d e z ) [ % ] Vertical bus (100%) -7.9% 6-VC (18-flit) 6-VC (30-flit) Bubble (15-flit) Figure 10: Application performance (N = 8)  0  20  40  60  80  100  120 6-VC (18-flit) 6-VC (30-flit) Bubble (15-flit) R e u o t r a h r d w a r a e m n u o t [ k l i o a g t s e ] -33.5% VC state control and MUXes for 6 VCs Crossbar+Others Input ports Input buffers Figure 11: Router hardware amount (3 ports) the packet length is up to 5-ﬂit, 6-VC (30-ﬂit) and Bubble (15-ﬂit) use VCT switching, while 6-VC (18-ﬂit) uses wormhole. The buﬀer requirements of Bubble (15-ﬂit) is less than that of 6-VC (18-ﬂit). To evaluate the application performance of these communication schemes on the wireless 3-D CMPs, we use ten parallel programs from the OpenMP implementation of NAS Parallel Benchmarks [8]. Sun Solaris 9 operating system is running on the 4-chip and 8-chip CMPs. These benchmark programs were compiled by Sun Studio 12 and are executed on Solaris 9. The number of threads was set to four or eight, depending on the number of Type A chips stacked. Figures 9 and 10 show the application execution cycles of ten benchmark programs (BT, CG, DC, EP, FT, IS, LU, MG, SP, and UA) for 4-chip and 8-chip networks, respectively. The application execution time (Y-axis) is normalized so that the execution time using the vertical bus indicates 100%. As shown, Bubble (15-ﬂit) outperforms 6-VC (18-ﬂit) by 7.9%-12.5%, because it uses a deep 15-ﬂit FIFO buﬀer while 6-VC (18-ﬂit) and 6-VC (30-ﬂit) use shallow 3-ﬂit and 5-ﬂit FIFO buﬀers, respectively. Also, Vertical bus still outperforms 6-VC (18-ﬂit) only for the 4-chip network. However, its performance is degraded as the number of chips stacked increases, such as the 8-chip network. 5.4 Router Hardware Amount Using the RTL model of the test chip, 6-VC (18-ﬂit), 6VC (30-ﬂit), and Bubble (15-ﬂit) are compared in terms of the router hardware amount. As shown in Figure 4, the router has three bi-directional ports: two bi-directional ports for a local core and another router on the same chip, and two uni-directional ports for routers on upper and lower chips, respectively. The ﬂit size was set to 128-bit in this design, although the test chip conservatively employed 32-bit data width (Table 1). These routers were synthesized with Synopsys Design Compiler, and then placed and routed with Synopsys IC Compiler. We used a 65nm process for these designs. Figure 11 shows their gate counts. As shown, input ports consume the most of the router area while the crossbar area is quite small, because the number of crossbar ports is only three in these routers. The input port area is divided into FIFO buﬀers and the other control circuits. Bubble (15ﬂit) and 6-VC (18-ﬂit) have almost the same buﬀer areas. However, 6-VC (18-ﬂit) requires more control circuits for supporting six VCs, such as the VC state controllers and VC multiplexers. As a result, Bubble (15-ﬂit) requires a 33.5% smaller router area compared to 6-VC (18-ﬂit). 6. SUMMARY AND FUTURE WORK In this paper, we proposed a wireless 3-D NoC architecture for CMPs, in which the number and types of CMP chips stacked in a package can be changed after the chip fabrication, by using the inductive coupling. These chips form a single vertical ring network so as to fully exploit the ﬂexibility of the wireless that enables us to add, remove, and swap the chips in a package without updating any routing information. As the communication schemes, we compared the vertical bubble ﬂow control, conventional VC-based approach, and vertical bus, in terms of the latency, throughput, hardware amount, and application performance. Below are the observations from the evaluation results. (cid:129) Vertical bus is simple and still low-latency (Table 2). However, its network throughput is quite low (Figures 7 and 8) and its application performance is also lower than the other approaches in the 8-chip network (Figure 10). (cid:129) VC-based approach is the most conventional deadlock avoidance technique for rings. However, the required number of VCs increases depending on the number of message classes on the network. Also, network protocols to be used are limited by the number of VCs already equipped. This will harm the ﬂexibility of the wireless 3-D CMPs that can customize their structure for given purposes. (cid:129) Vertical bubble ﬂow does not rely on any VCs; thus the communication protocols to be used are not limited by the number of VCs available. It outperforms the VCbased approach by 7.9%-12.5% (Figures 9 and 10) with a 33.5% smaller router area (Figure 11). However, the performance improvement decreases slightly as the number of chips increase, since the negative impacts of the                     miss routing on performance increases as the ring size is enlarged. As future work, we will focus on the following issues. (cid:129) More scalable wireless 3-D NoC architecture: We employed the ring network with bubble ﬂow control, since the network sizes we assumed were eight chips at most in this paper. As more scalable approach, we are implementing a plug-and-play protocol that detects whole network structure at boot time and then conﬁgures the routing tables based on multiple spanning trees. (cid:129) Evaluations using the test 3-D chip: The test 3-D chip presented in Section 4 is still under fabrication as of December 2010. We will evaluate it as our future work. Also, we are planning to produce a more practical test chip, in which the packet generator/counter cores of the current chip will be replaced with MIPS R3000 processors. 7. "
Analysis of application-aware on-chip routing under traffic uncertainty.,"Application-aware routing exploits static knowledge of an application's traffic pattern to improve performance compared to generalpurpose routing algorithms. Unfortunately, traditional approaches to application-aware routing cannot efficiently handle dynamic changes in the traffic pattern limiting its usefulness in practice. In this paper, we study application-aware routing under traffic uncertainty. Our problem formulation allows an application to statically specify an uncertainty set of traffic patterns that each occur with a given probability, and our goal is to find a single set of combined routes that will enable high-performance across all of these traffic patterns. We show how efficient combined routes can be found for this problem using convex optimization. These combined routes are optimal when the performance for every traffic pattern using the combined routes is the same as the performance using routes that are specialized for just that traffic pattern. We derive necessary and sufficient conditions for when our optimization framework will find optimal combined routes. We use theoretical and numerical analysis for the important class of permutation traffic patterns to quantify how often optimal combined routes exist and to determine the performance loss when optimal combined routes are infeasible. Finally, we use a cycle-level simulator that includes realistic pipeline latencies, arbitration, and buffered flow-control to study the latency and throughput of combined routes compared to specialized routes and routes generated using general-purpose routing algorithms. The theoretical analysis, numerical analysis, and simulation results in this paper provide a first step towards more flexible application-aware routing.","Analysis of Application-Aware On-Chip Routing under Trafﬁc Uncertainty Nithin Michael, Milen Nikolov, Ao Tang, G. Edward Suh, Christopher Batten School of Electrical and Computer Engineering, Cornell University, Ithaca, NY {nm373,mvn22,at422,gs272,cbatten}@cornell.edu ABSTRACT 1. INTRODUCTION Application-aware routing exploits static knowledge of an application’s trafﬁc pattern to improve performance compared to generalpurpose routing algorithms. Unfortunately, traditional approaches to application-aware routing cannot efﬁciently handle dynamic changes in the trafﬁc pattern limiting its usefulness in practice. In this paper, we study application-aware routing under trafﬁc uncertainty. Our problem formulation allows an application to statically specify an uncertainty set of trafﬁc patterns that each occur with a given probability, and our goal is to ﬁnd a single set of combined routes that will enable high-performance across all of these trafﬁc patterns. We show how efﬁcient combined routes can be found for this problem using convex optimization. These combined routes are optimal when the performance for every trafﬁc pattern using the combined routes is the same as the performance using routes that are specialized for just that trafﬁc pattern. We derive necessary and sufﬁcient conditions for when our optimization framework will ﬁnd optimal combined routes. We use theoretical and numerical analysis for the important class of permutation trafﬁc patterns to quantify how often optimal combined routes exist and to determine the performance loss when optimal combined routes are infeasible. Finally, we use a cycle-level simulator that includes realistic pipeline latencies, arbitration, and buffered ﬂow-control to study the latency and throughput of combined routes compared to specialized routes and routes generated using general-purpose routing algorithms. The theoretical analysis, numerical analysis, and simulation results in this paper provide a ﬁrst step towards more ﬂexible application-aware routing. Categories and Subject Descriptors C.3 [Peformance of Systems]; C.2.2 [Computer-Communication Networks]: Network Protocols—routing protocols General Terms Algorithms, Theory Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS’11, May 1–4, 2011, Pittsburgh, PA, USA. Copyright 2011 ACM 978-1-4503-0720-8 ...$10.00 Most routing algorithms proposed for on-chip networks are general-purpose algorithms, since they are designed to perform well over a wide-range of applications. They are either completely oblivious to the application’s trafﬁc pattern (e.g., dimension ordered routing, ROMM [11], O1TURN [13]) or they dynamically adapt to an application’s trafﬁc pattern through indirect local information about the network’s global performance (e.g., minimal adaptive routing [7], GOAL [17]). If the application’s trafﬁc pattern is known statically, then application-aware routing can potentially achieve better performance compared to general-purpose algorithms. In on-chip networks, application-aware routes are usually determined by either solving a mixed-integer linear program to generate single-path routes [9], or by solving the optimal-routing multicommodity ﬂow problem [18] following earlier work done for general networks [2, 5]. We call this the optimal specialized routing problem since the goal is to ﬁnd the optimal routes for a speciﬁc trafﬁc pattern. These routes are then used to conﬁgure the on-chip network before executing the application. Single-path routes can use source-routing or table-based routing, while multi-path routes require table-based routing and per-router split-ﬂow management. Multi-path routes offer increased network performance but also require more complicated hardware. Although application-aware multi-path routes are more common in wide-area-networks, they are an interesting research direction for on-chip networks and are the focus of this paper. Unfortunately, application-aware routing cannot efﬁciently handle dynamic changes in the trafﬁc pattern, and this is a serious limitation for modern workloads which often include many application phases [14, 15]. Each application phase exhibits signiﬁcantly different behavior than other phases, and thus each application phase is characterized by a unique network trafﬁc pattern. Application phases can last for thousands to millions of cycles. In this work, we assume that the phase trafﬁc patterns are known statically, and that the sequence of application phases is either known statically or is determined dynamically at runtime. In other words, we have an uncertainty set of trafﬁc patterns (one per application phase) each of which can occur with a given probability. The trafﬁc patterns in the uncertainty set and the corresponding probabilities of occurrence are usually obtained through static analysis or by proﬁling the application of interest. Given the uncertainty set, one approach to application-aware routing is to ﬁnd the optimal network routes for each application phase, and then reconﬁgure the network at runtime before each phase. However, the cost of detecting application phases and reconﬁguring the network can be high. We deﬁne the combined optimal routing problem as ﬁnding a single set of routes to be used across all application phases that results in the same performance or close to the same performance as if we used specialized routes for each application phase. Naive approaches to this problem include heuristically combining the optimal specialized routes for each application phase into a single set of routes, or combining the trafﬁc patterns for all application phases into a single trafﬁc pattern and solving a uniﬁed optimal-routing multi-commodity ﬂow problem. We will show that neither of these naive approaches is optimal. Instead, we formulate the problem as a convex optimization problem (Section 2), and we use theoretical analysis (Section 3), numerical analysis (Section 4), and simulation results (Section 5) to illustrate that this formulation produces optimal solutions when possible and produces nearly-optimal solutions when the optimal solutions are infeasible. 2. OPTIMAL ROUTING UNDER TRAFFIC UNCERTAINTY To put the problem that we are trying to solve in context and to introduce the notation used in the paper, we ﬁrst describe the optimal routing problem for a single trafﬁc pattern and its well-understood formulation as a convex optimization problem. We then discuss the more general combined optimal routing problem and illustrate how it can also be formulated as a similar convex optimization problem. 2.1 Specialized Optimal Routing Problem An on-chip network interconnects terminals through a set of routers and unidirectional point-to-point channels (links). For this work, we focus on direct networks where there is one router per terminal, and we call the combination of a router and a terminal a node. Trafﬁc patterns can be modeled by the communication between the different nodes. We denote the number of nodes in the network as N and the number of links as L. The capacities of the links are represented by C ∈ RL . First, we deﬁne a few terms that will help us with the mathematical formulation of the problem. Trafﬁc Matrix/Pattern (D) – The trafﬁc matrix D ∈ RN×N speciﬁes the trafﬁc requirements of the application. Each entry D(s, d ) represents the desired rate of data transfer from node s to node d and each such source-destination pair is said to constitute a network ﬂow. We suppose that there are F non-zero ﬂows in each trafﬁc matrix, and we label the ﬂow from s to d as the tuple (cid:104)s, d (cid:105). Incidence Matrix (A) – The ﬂow constraints imposed by the topology of the network are captured by its incidence matrix A ∈ RN×L which is deﬁned as follows,  +1, A(i, j) = if link j is directed to node i −1, if link j is directed away from node i 0, otherwise. Link Rates (Y ) – Y ∈ RL×F represents the rate on each link due to each ﬂow in the trafﬁc matrix. It is easy to see that solving for the link rates for each ﬂow speciﬁes the route the ﬂow takes through the network. We also deﬁne γ = ∑F j=1 Y j as the vector of the total rate on each link required by the trafﬁc matrix where Y j represents the link rates corresponding to the ﬂow j . Cost Function ( f ) – We will use the following cost function. f (γ ) = L ∑ l=1 γ (l ) C(l ) − γ (l ) (1) With this formula, the cost function becomes the average number of packets in the system based on the hypothesis that each queue behaves as an M/M/1 queue of packets. Although this assumption is violated in real networks, the cost function described above provides a useful measure of performance in practice, because it expresses qualitatively the idea that congestion arises when the total rate on a link approaches its capacity as pointed out in [1]. Other measures of congestion include the maximum link utilization, but we do not consider them since a computational study has shown that the choice of the objective function between maximum link utilization and average number of packets in the network does not signiﬁcantly impact the performance when used for routing optimization [19]. As noted earlier, we call the optimal routing problem for a single trafﬁc matrix the specialized optimal routing problem, and it can be formulated as follows, minimize Y f (γ ) subject to AY = D, Y j ≤ C, (2) F ∑ j=1 Y ≥ 0. where the matrix D ∈ RN×F is obtained from the trafﬁc matrix D  +D(s, d ), as follows, −D(s, d ), 0, The above formulation is a classic convex optimization problem and therefore can be solved efﬁciently to ﬁnd the specialized optimal routes for the trafﬁc matrix D. if l = d for the ﬂow (cid:104)s, d (cid:105) if l = s for the ﬂow (cid:104)s, d (cid:105) otherwise. D(l , sd ) = 2.2 Combined Optimal Routing Problem Specialized routes are tuned for a single trafﬁc pattern, but real applications often include a sequence of application phases each with their own trafﬁc pattern. In other words, we have an uncertainty set D = {D1 , . . . , DM } of M trafﬁc patterns that occur with probabilities P1 , . . . , PM . As noted earlier, we deﬁne the combined optimal routing problem as follows: ﬁnd a single set of routes that enables the same performance or close to the same performance on each trafﬁc pattern as if we used specialized routes for each trafﬁc pattern. If the combined routes achieve the same performance as the specialized routes on each trafﬁc pattern we call them optimal combined routes. Combined routes will enable us to conﬁgure the on-chip network once, and achieve optimal or near-optimal performance during all application phases. Fig. 1 illustrates the combined (a) (b) (c) Figure 1: Specialized and Combined Routes for for Trafﬁc Matrices DA and DB – DA (i, j) = DB (i, j) = 0 except for DA (0, 3) = DA (0, 2) = DB (0, 1) = DB (1, 2) = 1. (a) (b) (c) (d) Figure 2: Specialized and Combined Routes for Trafﬁc Matrices DA and DC – DA (i, j) = DC (i, j) = 0 except for DA (0, 3) = DA (0, 2) = DC (0, 2) = DC (2, 1) = 1. (a) (b) (c) (d) Figure 3: Specialized and Combined Routes for Trafﬁc Matrices DA and DD – DA (i, j) = DC (i, j) = 0 except for DA (0, 3) = DA (0, 2) = DD (1, 3) = DD (1, 2) = 1. optimal routing problem for a four-node ring network and two trafﬁc patterns, DA and DB , each with two ﬂows. Note that the example (and the other examples in this section) use single-path routing to simplify the discussion, but the illustrated concepts are common across both optimal single-path and multi-path routing. Fig. 1a and Fig. 1b illustrate optimal specialized routes for DA and DB respectively, and Fig. 1c illustrates combined routes for the uncertainty set containing DA and DB assuming each trafﬁc pattern occurs with equal probability. Note that since the two trafﬁc patterns have no ﬂows in common, the optimal combined routes are simply the combination of the specialized routes for each trafﬁc pattern. We can conﬁgure the four routers once with the routes show in Fig. 1c and we will achieve optimal performance during both application phases. Given this example, a naive approach to the combined optimal routing problem is to simply solve the specialized optimal routing problem for each trafﬁc matrix in the uncertainty set and then merge the resulting routes to create the combined routes. Unfortunately, this approach does not robustly handle ﬂows that are shared across multiple trafﬁc matrices. For example, Fig. 2a and Fig. 2b show the optimal specialized routes for two trafﬁc patterns, DA and DC . The specialized route for ﬂow (cid:104)0, 2(cid:105) is different for the two trafﬁc patterns, and thus it is unclear which one to choose when combining the specialized routes. In Fig. 2c, we choose the specialized route from the solution to DA , while in Fig. 2d, we choose the specialized route from the solution to DC . The former results in optimal combined routes, while the latter will result in reduced performance when executing trafﬁc pattern DA since the link between node 0 to 3 is more heavily loaded. In larger network topologies with more complicated trafﬁc patterns, it is not clear how to effectively combine optimal specialized routes. The key problem with this approach is that it considers the trafﬁc matrices in the uncertainty set in isolation without considering their interaction when combining routes. Another naive approach is to weight each element in each trafﬁc matrix in the uncertainty set by the probability of that trafﬁc matrix occurring and sum the corresponding elements across all trafﬁc matrices. We can then solve the specialized optimal routing problem 2 1 1 2 1 /D1 (s, d ) = Theorem 1. Suppose that D1 and D2 both have a ﬂow (cid:104)s, d (cid:105) and that Y (cid:104)s,d (cid:105) and Y (cid:104)s,d (cid:105) are the corresponding link rates. Then the route taken by the ﬂow (cid:104)s, d (cid:105), as speciﬁed by the link rates, is the same for both trafﬁc patterns if and only if Y (cid:104)s,d (cid:105) Y (cid:104)s,d (cid:105) /D2 (s, d ) where Di (s, d ) represents the demand from s to d for trafﬁc pattern Di . Proof. Intuitively, the route for a ﬂow (cid:104)s, d (cid:105) can be uniquely represented by how the ﬂow splits at the intermediate nodes between the source and the destination. These split ratios indicate the route that one unit of trafﬁc will take through the network from source to destination. We know that link rates can be uniquely determined from these node-based split ratios [5]. Since a route can be uniquely speciﬁed by node-based split ratios, it is easy to see that if the routes are the same between s and d for both D1 and D2 then the normalized rates on each link of the network will also be the same, i.e., Y (cid:104)s,d (cid:105) Now suppose that Y (cid:104)s,d (cid:105) /D2 (s, d ). This implies that the normalized inﬂow into each node of the network and the normalized outgoing rates from each node are the same for both trafﬁc patterns. So the split ratios at each node are the same for the ﬂow between s and d for both D1 and D2 . Since at each node, the split ratios corresponding to a ﬂow completely deﬁne the route taken by the ﬂow through the network, we can conclude that the route taken by ﬂow (cid:104)s, d (cid:105) is the same for both trafﬁc patterns. /D2 (s, d ). /D1 (s, d ) = Y (cid:104)s,d (cid:105) /D1 (s, d ) = Y (cid:104)s,d (cid:105) 2 1 2 for this combined trafﬁc matrix. Unfortunately, some solutions for the combined trafﬁc matrix result in non-optimal combined routes. For example, Fig. 3a and Fig. 3b show the specialized routes for two trafﬁc patterns, DA and DD . There are no shared ﬂows, so combining DA and DD into a single trafﬁc matrix is straightforward. Fig. 3c and Fig. 3d show two possible solutions to the specialized optimal routing problem for the combined trafﬁc matrix. The former results in optimal combined routes, while the latter will result in reduced performance when executing trafﬁc pattern DA as well as DD since the links connecting node 0 to node 3 and node 1 to node 2 respectively will be more heavily loaded. The key problem with this approach is that a single combined trafﬁc matrix implies that ﬂows from all trafﬁc patterns in an uncertainty set happen simultaneously, but our problem formalation only uses a single trafﬁc pattern during each application phase. These examples illustrate that the key challenge in solving the combined optimal routing problem is creating a uniﬁed optimization framework that can determine both the specialized routes for each trafﬁc pattern and the way these specialized routes interact to determine optimal combined routes. This is true for both singlepath or multi-path routes. Our approach is to design an optimization problem that minimizes the expected cost function across all trafﬁc matrices in the uncertainty set. In addition, the combined routes have to satisfy the requirements of every trafﬁc matrix in the uncertainty set simultaneously. This means that if there is a ﬂow that is shared across multiple trafﬁc matrices, then the route computed for it should be the same for each of those trafﬁc matrices. It is not immediately clear whether we can capture this intuitive constraint in terms of a convex constraint that will let us set up a convex optimization problem. Fortunately, the following theorem says that we can do precisely that. Now we can deﬁne the optimal routing problem when there is (a) (b) (c) (d) Figure 4: Specialized and Combined Routes for Trafﬁc Matrices DA and DE – DA (i, j) = DE (i, j) = 0 except for DA (0, 3) = DA (0, 2) = DE (0, 2) = DE (1, 2) = 1. uncertainty in the trafﬁc pattern as follows, minimize {Y1 ,Y2 ,...,YM } M ∑ i=1 pi f (γi ) subject to FYi = Di , i = 1, 2, . . . , M Fi∑ j=1 Y j i ≤ C, i = 1, 2, . . . , M Yi ≥ 0, i = 1, 2, . . . , M /Di (s, d ) = Y (cid:104)s,d (cid:105) (3) j i /D j (s, d ) Y (cid:104)s,d (cid:105) if ﬂow (cid:104)s, d (cid:105) is in both Di and D j . Solving the above optimization problem is guaranteed to ﬁnd the optimal combined routes if they exist. Suppose that there exists optimal combined routes for an uncertainty set D = {D1 , . . . , DM } and that the solution to Eq. 3 does not correspond to these routes. We can then show that a contradiction results, since selecting the optimal combined routes will further decrease the cost function in Eq. 3 as the f (γi ) corresponding to each Di is minimized by the optimal combined routes by deﬁnition. But there are still cases where it is simply not possible to ﬁnd optimal combined routes. Fig. 4a and Fig. 4b illustrate the optimal specialized routes for two trafﬁc matrices, DA and DE . Fig. 4c and Fig. 4d illustrate two solutions that are similar in spirit to what might be found using Eq. 3 (remember that for simplicity the examples in this section are for single-path routing, but our analysis is for general multi-path routing). Unfortunately, neither solution is optimal since both solutions result in more heavily loaded links compared to the optimal specialized routes. In this case, reconﬁguring the network before each application phase would result in higher performance than using a single set of combined routes but perhaps this will not be necessary if the combined routes are close to optimal. The rest of the paper uses theoretical analysis, numerical analysis, and simulations to answer three key questions: (1) When are optimal combined routes feasible? (2) How often is this condition satisﬁed for different uncertainty set sizes? and (3) When optimal combined routes are not feasible, what is the performance loss compared to the optimal specialized routes? 3. THEORETICAL ANALYSIS The effectiveness of the combined optimal routing framework is best characterized by how much performance we have to sacriﬁce for the additional ﬂexibility that the framework offers. In this section, we ﬁrst derive when we can ﬁnd optimal combined routes, and we then study permutation trafﬁc in more detail. 3.1 Necessary and Sufﬁcient Conditions for Optimality i i ) /D j (s, d ) (i, j = 2 , . . . , Y ∗ /Di (s, d ) = Y (cid:104)s,d (cid:105)∗ We present the necessary and sufﬁcient condition obtained from the equilibrium conditions of Eq. 3 in the next theorem albeit with a simpler proof for ease of exposition. Identifying the particular cases when can achieve optimal combined routes and identifying the probabilities of their occurrence is an area that we are continuing to explore. Theorem 2. Suppose that Y 1 , . . . , Y M are the combined routes obtained from Eq. 3 and Y ∗ 1 , Y ∗ M are the optimal routes corresponding to the trafﬁc patterns D1 , D2 , . . . , DM . Then f (γ i ) = f (γ ∗ (i = 1, . . . , M ) if and only if Y (cid:104)s,d (cid:105) 1, . . . , M ) where Di (s, d ) > 0 and D j (s, d ) > 0 represents the demand from s to d for trafﬁc patterns Di and D j respectively. Proof. Clearly if Y (cid:104)s,d (cid:105)∗ 1, . . . , M ) where Di (s, d ) > 0 and D j (s, d ) > 0 then the condition required to be satisﬁed by the shared ﬂows in Eq. 3 is satisﬁed and consequently Y ∗ i (i = 1, . . . , M ) is a solution to Eq. 3 indicating that f (γ i ) = f (γ ∗ Next suppose that f (γ i ) = f (γ ∗ i ) (i = 1, . . . , M ). Clearly Y i (i = 1, . . . , M ) are optimal for the trafﬁc matrices D1 , D2 , . . . , DM respectively, i.e., Y i = Y ∗ i (i = 1, . . . , M ). Also since Y i (i = 1, . . . , M ) is a solution to Eq. 3 we know that the condition Y (cid:104)s,d (cid:105) Y (cid:104)s,d (cid:105) 0 is satisﬁed for the shared ﬂows between the pairs of trafﬁc matrices Di and D j and so we conclude that Y (cid:104)s,d (cid:105)∗ Y (cid:104)s,d (cid:105)∗ /Di (s, d ) = /D j (s, d ) (i, j = 1, . . . , M ) where Di (s, d ) > 0 and D j (s, d ) > /Di (s, d ) = Y (cid:104)s,d (cid:105)∗ /D j (s, d ) (i, j = 1, . . . , M ). i ) (i = 1, . . . , M ). /Di (s, d ) = /D j (s, d ) (i, j = i j j j j i i In words, the above result states that we can ﬁnd optimal combined routes for a given uncertainty set, if and only if for any shared ﬂow there exists optimal routes obtained by solving Eq. 2 that are the same for all the trafﬁc matrices in the uncertainty set that contain that ﬂow. But this condition as stated is difﬁcult to verify in practice and we would like to ﬁnd veriﬁable conditions. For instance, from the structure of Eq. 3, it is clear that if there are no shared ﬂows between the Di s then the problem decouples into M independent optimal routing problems and the routes obtained by solving Eq. 3 will be individually optimal for each element of D. This gives us an easy to check sufﬁcient condition for when solutions to Eq. 3 are optimal combined routes. 3.2 Permutation Trafﬁc Matrices The analysis in the previous section applies to all trafﬁc patterns, but in this section we narrow our focus to just permutation trafﬁc matrices. In these trafﬁc patterns, each row and each column has only one non-zero entry. For a network with N nodes there are N ! possible permutation trafﬁc matrices. First, for the sake of simplicity, suppose that the uncertainty set D consists of two permutation trafﬁc matrices. The next theorem tells us how likely it is that these two trafﬁc matrices do not have any shared ﬂows. Proposition 1. The number of trafﬁc patterns that do not share a (cid:1)Pi where P0 = 1. Furﬂow with a given permutation trafﬁc pattern for a network that has N nodes is given by PN = N ! − ∑N−1 thermore, we see that limN→∞ PN /N ! = 1/e or that as N → ∞ the probability of selecting a pair of permutation trafﬁc matrices which do not share ﬂows tends to 1/e. (cid:0)N i=0 i Proof. We ﬁrst note that for the purpose of determining if there are shared ﬂows between two trafﬁc matrices, the rates required by the ﬂows do not matter. So we index ﬂows simply by a 1 if a ﬂow exists and a 0 otherwise. Any permutation matrix can be converted to any other permutation matrix by left multiplying it with a suitable permutation matrix. We note that a permutation trafﬁc pattern A will share a ﬂow with another permutation trafﬁc pattern B only if the permutation matrix that transforms A’s permutation matrix into the permutation matrix of B has non-zero diagonal entries. By eliminating all permutation matrices with non-zero diagonal entries from the set of permutation matrices leaves us with (cid:1)Pi . Here we set P0 = 1 for brevity in notation. the transformations that will yield trafﬁc patterns that do not share a ﬂow with a given permutation trafﬁc pattern. Consequently we have PN = N ! − ∑N−1 The second part of the theorem is similar to the famous ""Hat Check Problem"" studied by Bernoulli and Montmort although we provide a different solution. We have, (cid:0)N i=0 i (cid:18)N (cid:19) (cid:18)N (cid:19) i N ∑ i=0 ⇒ N ∑ i=0 ⇒ N ∑ i=0 Pi = N ! ki i! = N ! i ki /(N − i)! = 1 Using induction we can show that ki = ∑i j=0 (−1) j / j!. First we note that for N = 0, k0 = 1. Then applying the induction hypothesis to ∑N i=0 ki /(N − i)! yields N ∑ i=0 N ∑ k=0 = i ∑ j=0 k ∑ j=0 (−1) j j!(N − i)! (−1) j j!(k − j)! = N ∑ k=0 0k /k! = 1 j=0 (−1) j / j!, we have limi→∞ ki = 1/e completing Since ki = ∑i the proof. As one might expect this probability decreases as the number of trafﬁc matrices in D increases. Another interesting restriction is obtained when we study what the maximum size of the set D can be if we consider only permutation trafﬁc patterns that do not share ﬂows. Proposition 2. The cardinality of D is N if we restrict attention to permutation trafﬁc patterns that do not share ﬂows. Proof. If the trafﬁc patterns do not share ﬂows, by the pigeon hole principle we can conclude that the cardinality of D can be at most N . To see that it is indeed N , we observe that row rotating an N×N identity matrix yields a set of N permutation matrices with corresponding trafﬁc patterns that do not share ﬂows. Multiplying any given permutation matrix by this set yields a set of permutation matrices corresponding to trafﬁc patterns that do not share ﬂows. % of all with Γ = 1 65 20 10 3 0 % of opt with shared 62 72 100 100 − % of non-opt with Γ < 1.05 100 100 87 82 78 M 2 3 4 5 6 max Γ 1.004 1.009 1.073 1.090 1.150 % of non-opt with given num shared ﬂows 1 2 3 4 5+ 29 57 14 0 29 18 12 18 0 0 16 5 0 0 0 0 0 0 0 0 0 23 79 100 100 Table 1: Results of Numerical Analysis – Columns list the size of the uncertainty sets (M ); percentage of all uncertainty sets with loss factor (l ) of one (i.e., optimal combined routes are feasible); percentage of optimal combined routes for which there is at least one shared ﬂow; percentage of non-optimal combined routes with a loss factor less than 1.05; maximum loss factor over all uncertainty sets; percentage of non-optimal combined routes with the given number of shared ﬂows. For uncertainty sets comprising well-structured permutation trafﬁc patterns, with the above results we are able to characterize to some extent when we have optimal combined routes. However, as pointed out earlier, even in this case it is challenging to determine every situation in which we can ﬁnd optimal combined routes and if there is loss in optimality to quantify the loss. Consequently, we rely on numerical experiments to help us further characterize the performance of the combined optimal routing framework. 4. NUMERICAL ANALYSIS i=1 pi f (γi )/ ∑M i=1 pi f (γ ∗ i ). In this section, we empirically answer, as the size of the uncertainty set increases, how often we can ﬁnd optimal combined routes and if optimal combined routes do not exist, what is the loss factor. The loss factor (Γ) is the factor by which the average number of packets with combined optimal routes differ from that with optimal specialized routes. In other words, Γ = ∑M Once again, we restrict attention to permutation trafﬁc patterns in order to obtain a more complete characterization of the performance of the combined optimal routing framework on this important class of trafﬁc patterns. We performed our evaluations over uncertainty sets with two to six trafﬁc matrices on a 6×6 two-dimensional mesh. For each set size, we randomly generated 500 uncertainty sets and solved the corresponding specialized and combined optimal routing problems with the objective being to minimize the average number of packets in the network. Note that these numerical experiments involve solving multiple convex optimization problems with several hundred thousand variables. Even though the optimization problems were solved efﬁciently using cvx [6], the calculations for each uncertainty set took on the order of hours to complete and the complete numerical analysis required many thousands of hours of computation. The results from the numerical analysis are summarized in Table 1. The ﬁrst metric that we studied was the empirical probability of being able to ﬁnd optimal combined routes. In order to go beyond the analytical results of the previous section in quantifying the performance of the combined optimal routing framework, we also studied the probability of ﬁnding optimal combined routes when the trafﬁc matrices in the uncertainty set shared ﬂows. But as the size of the uncertainty set was increased, the empirical probability (a) (b) (a) (b) Figure 5: Latency vs. Offered Bandwidth for Optimal Combined Routes – Theoretical and numerical analysis predicts the combined routes should be able to acehive the same throughput as the specialized routes. Figure 6: Latency vs. Offered Bandwidth for Sub-Optimal Combined Routes – Theoretical analysis shows that optimal combined routes are infeasible, but numerical analysis predicts that the combined routes should still perform close to the specialized routes. of ﬁnding optimal combined routes decreased as we would expect. Encouragingly, for most of the uncertainty sets that were generated, combined optimal routes performed to within 5% of the optimal specialized routes as can be seen. This observation naturally led to the next question which was when we did lose optimality, how bad was the loss? For uncertainty set sizes 2 and 3, at least in our sample sets, very low loss in optimality was observed. However, for larger uncertainty set sizes, fairly high values of Γ were observed in the worst case. But even in these cases, we expect that the combined optimal routes will perform better than the general-purpose routing algorithms as the next section will illustrate. For the uncertainty sets with non-optimal combined routes, we also studied the percentages of occurrence of different numbers of shared ﬂows. The idea was to study the correlation between the number of shared ﬂows and the probability of an uncertainty set having an undesirably high value for Γ. 5. SIMULATION RESULTS The simulator that we used was DARSIM [10], a cycle-level onchip network simulator. All the simulations were performed on a 6×6 two-dimensional mesh network. The simulator was given a warm-up period of 20,000 cycles after which performance statistics were collected over 100,000 cycles in order to ensure the accuracy of the results. The primary performance criteria that we measured were throughput and latency. The data rates are expressed in ﬂits/cycle and each packet is divided into 8 ﬂits. Also the simulator was conﬁgured so that each physical channel was divided into 6 virtual channels with 8 ﬂits of buffering each. The capacity of the physical channel was set to be 1 ﬂit/cycle. In the simulator, virtual channels are pre-allocated to the different ﬂows once the routes are computed so that deadlock is avoided according to the static virtual channel allocation scheme described in [16]. The aim of the simulations was to get an idea of how factors like buffering and ﬂow control inﬂuenced the performance of the optimal routes. We conducted our study with two uncertainty sets of size two where one had optimal combined routes and the other did not. In order to compare the performance of the optimal routing scheme with the general-purpose routing algorithms, we also studied how ROMM, O1TURN, and DOR performed for the trafﬁc matrices in the uncertainty sets. As expected, from the latencythroughput curves in Fig. 5, the optimal combined routes match the peak throughput achieved by the optimal specialized routes for both trafﬁc matrices in the uncertainty set while outperforming the general-purpose routing algorithms. More interesting results can be observed from the uncertainty set with non-optimal combined routes. From the previous section we expect that for uncertainty set size two, the combined routes should perform very close to the optimal specialized routes. Even factoring in the affects of non-idealities introduced by the simulator, we see from Fig. 6 that the specialized and combined routes are very close to each other in performance. Once again, it can be observed that the application-aware schemes yield better performance than the general-purpose algorithms. Of course it would be interesting to continue exploring the space of permutation matrices and study how the non-ideal characteristics of on-chip networks affect the performance of combined optimal routing on larger uncertainty sets where adversarial trafﬁc matrices can result in larger loss factors. The simulation and numerical results suggest this and many other directions of continued research to give us a better understanding of the properties of the combined optimal routing framework which appears to be a promising ﬁrst step towards introducing optimal routing with a certain degree of ﬂexibility to networks on-chip. 6. RELATED WORK In the context of on-chip networks, application-aware optimal single-path routing for a single trafﬁc pattern was explored in [3, 9]. But the focus on single path routes made the optimal routing problem NP-hard and consequently inefﬁcient to solve. On the other hand, optimal multi-path routing for a single trafﬁc pattern was explored even earlier [18]. Unlike the optimal single-path routing problem, the optimal multi-path routing problem is convex and therefore can be solved efﬁciently to determine optimal routes. Our work computes optimal multi-path routes as well, but differs from the previous work in that we are computing the routes for an uncertainty set of trafﬁc patterns. Another approach to application-aware routing can be found in [12] where the idea is to map the application’s communication graph to the network in such a way as to avoid cycles in the channel dependency graph. Then minimal adaptive deadlock-free routing is performed on the acyclic application-aware channel dependency graph. However, here knowledge of the application is just used to avoid deadlock by a suitable mapping of the trafﬁc requirements to the network. It is not used to try and optimize the performance of the routing algorithm with respect to any metric. The problem of dealing with trafﬁc uncertainty when formulating the optimal routing problem has only begun to receive attention over the last few years. Algorithms like COPE [20] approach the problem by trying to minimize the worst case performance of the routing scheme within an uncertainty set. The problem of ﬁnding optimal routes by minimizing the expected cost over a set of trafﬁc patterns has been studied previously in the context of intra-domain routing in the Internet [21]. However, the focus was on setting up the problem and extending the results of [5] to develop a distributed solution method. Our work goes further by providing conditions for the existence of optimal combined routes, empirically studying the probability that these conditions are met, and quantifying the loss in optimality when optimal combined routes do not exist. 7. CONCLUSIONS The paper presents a ﬁrst step towards more ﬂexible applicationaware routing in on-chip networks. In order to get around the fact that specialized optimal routing is not viable for trafﬁc patterns other than the one that it was designed for, we introduce combined optimal routing that can handle uncertainties in the trafﬁc patterns that might be produced in the on-chip network. The performance of the schemes and their advantages are characterized analytically, numerically, and through simulations. Importantly, we derive necessary and sufﬁcient conditions for the combined routes to have no loss in optimality. Numerical experiments with randomly generated uncertainty sets of permutation trafﬁc matrices provide empirical evidence that combined optimal routing can perform very close to specialized optimal routing. The simulation results obtained using sample points from these randomly generated uncertainty sets show that this observation from numerical experiments holds even with realistic pipeline latencies, arbitration, and buffered ﬂow-control. Overall, the initial results indicate that the combined optimal routing framework is a promising technique that adds ﬂexibility to application-aware optimal routing. There are a number of future directions that we plan to pursue in order to further our understanding of the structure of the combined routing problem and address practical implementation issues. For instance, we would like to better quantify how likely optimal combined routes exist for more complex trafﬁc patterns. We are also currently exploring how to bound the loss in optimality in the combined routing problem. The simulation results imply that the loss is typically small, but analytical results will provide a better understanding given that simulations alone cannot the sweep the entire parameter space. There also exist practical implementation issues that need further investigation. For example, multi-path routes can potentially cause out-or-order delivery of ﬂits and require buffers to re-order them. It is also necessary to have a good deadlock scheme in order to fully exploit the performance advantages of optimal routing. In the paper, we used static allocation of virtual channels in order to avoid deadlock [16]. We plan to further study different deadlock avoidance schemes such as resource ordering [8] or escape channels [4], and their performance implications. Also, we believe that it is important to couple the optimal routing schemes with a suitable ﬂow control scheme in order to fully exploit their performance advantages. Consequently, identifying a good ﬂow control scheme is of particular interest to us. Finally, in order to be able to use the routes generated by the techniques described in the paper, we need a router that is capable of splitting ﬂows. Designing and evaluating the router and ways to encode the routes efﬁciently are two other directions for future research. ACKNOWLEDGMENTS This work was funded in part by the National Science Foundation (Awards CNS-0708788, CCF-0835706 and CCF-0905208), a Jacobs Graduate Research Fellowship (Michael), and an equipment donation from Intel Corporation. The authors acknowledge and thank Francis Ciaramello for his help running simulations on the Cornell Teragrid. "
Inferring packet dependencies to improve trace based simulation of on-chip networks.,"With the advent of large scale chip-level multiprocessors, there is a growing interest in the design and analysis of on-chip networks. The use of full system simulation is the most accurate way to perform such an analysis, but unfortunately it is very slow and thus limits design space exploration. In order to overcome this problem researchers frequently use trace based simulation to study different network topologies and properties, which can be done much faster. Unfortunately, unless the traces that are used include information about dependencies between messages (packets), trace based simulation can lead one to draw incorrect conclusions about network performance metrics such as latency and overall execution time. In this paper we will demonstrate the importance of including dependency information in traces, as well as present an inference-based technique for identifying and including dependencies, and show that using these augmented traces results in much better simulation accuracy without excessively extending simulation time.","Inferring Packet Dependencies to Improve Trace Based Simulation of On-Chip Networks Christopher Nitta cjnitta@ucdavis.edu University of California, Davis Davis, CA 95616 Matthew Farrens farrens@cs.ucdavis.edu University of California, Davis Davis, CA 95616 Kevin Macdonald klmacdonald@ucdavis.edu University of California, Davis Davis, CA 95616 Venkatesh Akella akella@ucdavis.edu University of California, Davis Davis, CA 95616 ABSTRACT With the advent of large scale chip-level multiprocessors, there is a growing interest in the design and analysis of on-chip networks. The use of full system simulation is the most accurate way to perform such an analysis, but unfortunately it is very slow and thus limits design space exploration. In order to overcome this problem researchers frequently use trace based simulation to study different network topologies and properties, which can be done much faster. Unfortunately, unless the traces that are used include information about dependencies between messages (packets), trace based simulation can lead one to draw incorrect conclusions about network performance metrics such as latency and overall execution time. In this paper we will demonstrate the importance of including dependency information in traces, as well as present an inference-based technique for identifying and including dependencies, and show that using these augmented traces results in much better simulation accuracy without excessively extending simulation time. Categories and Subject Descriptors C.2.m [Computer-Communication Networks]: Miscellaneous; I.6.5 [Simulation And Modeling]: Model Development—Modeling methodologies General Terms Design, Performance 1. INTRODUCTION There is a general consensus in the architecture community that the best way (in terms of performance per watt) to harness the beneﬁts of Moore’s law is through parallelism. Consequently, from servers to mobile phones [1], future chips will contain dozens (if not hundreds) of processors, memories, and/or hardware accelerators connected by on-chip networks. The on-chip network is a critPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS ’11, May 1-4, 2011 Pittsburgh, PA, USA Copyright 2011 ACM 978-1-4503-0720-8 ...$10.00. ical component of the chip, as it constitutes a signiﬁcant fraction of the area and power consumed. As a result, a “one-size-ﬁts-all” approach is not appropriate for designing on-chip networks - a thorough exploration of the design space is required. For example, the amount of heterogeneity, buffer sizes, number of virtual channels, topology of the network, arbitration and ﬂow control schemes can all be optimized for a given application or market segment. The most accurate way to evaluate potential on-chip network designs is through the use of full system simulation, using a real operating system running real applications. In order to compare two different network designs, for example, a set of full system simulations should be run for each conﬁguration - doing so will give the best measure of how the designs compare. Unfortunately, full system simulation is very slow - the execution time can grow quadratically with increased node counts, preventing researchers from doing full system simulations with a large number of nodes. In our review of the current literature we found that only one study featured full-system simulations with as many as 32 nodes [2], one used 24 nodes [3], and the majority restricted their simulations to 16 nodes or less [4–8]. One commonly used method for circumventing this problem is to use a full system simulator to extract a record of network activity (a trace), and feed it into a trace-based simulator in order to evaluate various network conﬁgurations. Trace based simulations run much faster than full system simulations, and are widely used [9–17]. Unfortunately, these traces only include information about the order of and time between packet transmissions - in real systems there are dependencies between packets as well (some outgoing packets cannot be generated until incoming data has been received, for example). These dependencies are analogous to data dependencies in pipelined processors, and we will refer to them as reception dependencies. While a trace from a given full-system simulation will implicitly include the reception dependencies for that particular network conﬁguration, the whole purpose of network simulation is to be able to vary network parameters and evaluate the results - to look at different topologies, arbitration schemes, ﬂow control mechanisms, buffer sizes, etc. The absence of explicit information about reception dependencies means that packets are often injected into the network by the simulator at a higher rate than would occur in a real system, because the simulator does not know it needs to wait for certain events to occur. The ramiﬁcation of this unrealistically high packet injection rate is that measured latencies can climb dramatically for the network being analyzed, since many messages are spending an artiﬁcially large amount of time sitting in network buffers. Thus, drawing any meaningful conclusion about system parameters (such as overall speedup or ideal buffer size) based on these results is exceedingly difﬁcult, if not impossible.1 Unfortunately, many studies that use dependency-free traces include results relating to network speedup [9, 10, 12], normalized execution time [11], or network latency [13–16]. In this paper we will show that one has to be very careful when making predictions about the performance of on-chip networks based on the results of trace-driven simulations. In order to increase the accuracy of trace-based simulation we have developed a technique that allows us to add dependency information to traces, which we accomplish by inferring dependency information based on a series of full-system runs using different latencies. Using these augmented traces allows one to gain further insight into the true behavior of on-chip networks, provides more accurate results, and can help guide real on-chip network system design. The rest of the paper is organized as follows. In Section 2 we provide motivation for this work by providing a simple demonstration of the disparities that can occur between full system simulations and trace based simulations. We discuss related works from the literature in Section 3. In Section 4 we present a formal model for representing dependency information in a trace, and describe our algorithm for automated dependency inference, called PDG_GEN. Section 5 presents the validation methodology and experimental results. 2. MOTIVATION We will illustrate the pitfalls of using trace-based-simulation without packet dependency information by comparing the full-system simulation results (which is the true indicator of performance) with the output of a trace-based simulation, where the traces are generated from a network topology different from the one being simulated. We used Simics 3.0 [18] and GEMS 2.1.1 [19] for the experiment. First we modiﬁed Garnet [20] (which is the network simulator inside GEMS) to generate a trace consisting of all messages that enter and exit the network. We then conﬁgured Simics to model a 16 core processor with a fully connected single cycle network, and ran a 1 million point FFT benchmark. Once we had the trace generated by the Simics FFT simulation, we used Garnet to run a network-only simulation in which each message was injected into the network at the timestamp speciﬁed in the trace. The results of this simulation matched the Simics results, which is what we would expect; given the same network conﬁguration and the same messages injected at the same times, the network’s behavior should match the actual Simics simulation. We then used the same trace to re-run the network-only simulation on three different topologies: a torus, a mesh, and a ring. The results of these trace-based simulations are shown as the lighter bars in Figure 1. As Figure 1(a) shows, the execution times for the trace-based simulations hardly change for different topologies, because each message is always injected into the network at a ﬁxed time. Figure 1(b) shows that network latency is not affected until the trace is run on a network with a low enough bandwidth that congestion begins to occur. Basically, a trace by itself represents the packet injection distribution for the speciﬁc network conﬁguration on which the trace was collected. When it is used on a different network con1Simulating a trace taken from a slower network on a higher performing network is also a problem, because it may not show the simulated networks true potential since packets are injected at a lower rate than they would be in a real system. (a) Execution Time (b) Average Network Latency Figure 1: 1M point FFT execution time (a) and average network latency (b) for Simics full system simulation and tracebased network simulation ﬁguration, it no longer represents the actual packet injection distribution for the application and hence could yield erroneous results. If the trace had knowledge of the inherent packet dependencies in the application, then the packet injection would be closer to what actually happens on a given network conﬁguration, and the simulation results would be more meaningful and reliable. 3. RELATED WORK The relevant related work can be classiﬁed into three broad categories - software based functional/timing simulation, FPGA based emulation of either the functional or the timing model (or both), and high-level workload modeling using statistical techniques. BookSim 2.0 [21] is one of the ﬁrst and most basic network-on-chip simulators. It does not use traces from real applications, but instead uses synthetic trafﬁc to predict the average latency of a network. Garnet [20] is the successor to BookSim, and it incorporates detailed timing and power models. In its stand-alone conﬁguration it also uses traces without any dependency information, so it suffers from the pitfalls described above. Simics [18] is a commercial tool (recently acquired by Intel) that allows full-system functional simulation. However, as the benchmarking data in [22] shows, it is very slow and does not scale beyond approximately 16 cores. Furthermore, it does not have any support for modeling on-chip networks and lacks a timing model for the underlying architecture of the network. GEMS [19] provides a timing model and network model on top of Simics, making it one of the most widely used simulators in the architecture community today. However, since it runs on top of Simics, it is (obviously) even slower and less scalable, and unsuitable for fast design space exploration. Graphite [23] is a recent effort from MIT to take advantage of multiple machines to accelerate functional simulation of as many as 1024 cores. However, Graphite does not maintain strict ordering of events in the system, and as a result it is unsuitable for evaluating on-chip networks (a point the authors themselves mention in their paper.) Hestness et. al [30] recognize the necessity for dependency information within traces, and propose a technique for inferring dependencies based upon the ordering of memory references from a single full system simulation. In parallel with these developments, in the design automation community (where fast design space exploration and applicationspeciﬁc customization of networks is important) researchers are exploring the possibilities of high-level network trafﬁc models [24]. Marculescu [25] was the ﬁrst to propose a mathematical characterization of node to node trafﬁc for the MPEG-2 application. Soteriou et. al [26] generalized this to a comprehensive network trafﬁc model based on hop-count, burstiness and packet injection distributions. Gratz and Keckler [27] provide a detailed analysis of why existing approaches to simulation are not appropriate, and make a case for realistic workload characterization that includes the temporal and spatial imbalances in network trafﬁc distribution. Their work supports the central argument in this paper, which is that neglecting to properly model packet injection rate leads to substantial inaccuracies. The solution advocated by Gratz and Keckler is to provide synthetic trafﬁc models enhanced by the network trafﬁc characteristics, while we propose the creation of traces from full-system simulation that are augmented with a model for packet injection that is application speciﬁc. 4. MODELS AND ALGORITHM There are two ways of capturing packet dependencies - a topdown approach which instruments the source code of the application to record dependency information during a full-system run, or a bottom-up approach which infers the dependencies. We feel the latter is the better approach for a number of reasons. The source code of applications is frequently unavailable, for example, especially if a system has hard IP blocks. In addition, a detailed knowledge of the source code is required in order to instrument it correctly. An inference based approach is more practical, although it will not be quite as accurate as the top-down approach. Problem Formulation Our goal is to generate a packet dependency graph (PDG) for an application. To do this, we assume a computing system with N nodes (processors), and that the application has already been partitioned to ﬁt on the N nodes. The generation of the PDG is a onetime activity for a given application and partition, and these PDGs will be used to design and optimize on-chip networks just like SPEC, PARSEC, and EEMBC benchmarks are used today to explore processor and memory conﬁgurations. Thus, the generation of the PDG will require an investment of time and computing resources equal to that required by full-system simulation, but the use of the PDGs will be on trace-based simulators, and will therefore be much less resource-intensive. We use a two-step approach, which consists of a sampling step followed by the use of an inference heuristic to infer the PDG from the samples generated in the ﬁrst step. Sampling involves running a number (m) of full system simulations, where m ≥ 1. There are several important questions that must be addressed for example, what network topology and parameters/conditions are used to generate the samples? How many samples are needed? How does one validate that the inferred PDG is correct? We start by describing a formal model for a PDG, then describe the heuristics we use with an example. In the next section, detailed results and metrics are discussed. Abstract Model A t race is deﬁned as a time-ordered list of event s. An event Ei is a 4-tuple < Ti , Li , Ri , Pi >, where i is the entry number in the list, Ti is the time stamp of the global clock, Li is the local node, Ri is the remote node, and Pi is the unique packet ID. If Li is the sender of the packet and Ri is the receiver of the packet, then Ei is a transmit event. Each transmit event results in one or more receive events. For example, the transmit event, Ei =< Ti , Li , Ri , Pi > results in a receive event at node E j =< T j , Ri , Li , Pi >, where T j is the clock cycle at which the packet is received by node Ri . Note that T j − Ti is the network latency for the packet Pi . Each transmit event Ei can be modeled as a complex gate C 2 with t ≥ 1 inputs. This set of t inputs is deﬁned as the dependency set for Ei , which implies event Ei happens after all the t inputs have arrived, and some delay Di has elapsed. (The dependency set is primarily the set of reception dependencies, although a transmit can depend upon the previous transmit or the start of the simulation.) A transmit event is modeled by the ﬁring of the complex gate C. The intrinsic delay Di is used to model the computation or processing delay required before producing the data being transmitted. We will exploit this property when developing the algorithm PDG_GEN (described below) to infer dependencies. A PDG which models the on-chip network is an interconnection of these gates. Network latency is modeled as the propagation (or wiring) delay between the output of one gate and the input of another gate. Transmit event Ei in node Ni generally depends on ""some"" transmit events in other nodes N j , where j (cid:54)= i. These events form the reception dependency set for the event Ei . Thus the task of inferring the PDG boils down to determining the reception dependency set, and an estimate of the intrinsic delay Di , for each transmit event Ei in the trace. To determine the reception dependency set for each event, we ﬁrst need to ensure that causality is not violated, since a given transmit event cannot depend on a transmit event that is going to happen in the future (in other words, Ei cannot depend on any event E j where T j ≥ Ti .) Hence, we need at least one trace that is generated on a network that exposes causality of events. We can accomplish this by modeling it as a network of gates with zero wire delay. Such a network is called speed-independent in asynchronous logic literature. A speed-independent network can be emulated using a fully connected topology with single cycle latency and inﬁnite receive buffers, so there is no delay due to routing, arbitration, switching, or ﬂow control. Thus any delay observed can be attributed solely to the computation or processing delay. We use a trace generated on such a network as the initial base trace. PDG_GEN Algorithm In general, for a given transmit event Ei , any event at that node that has occurred before Ti could be in its dependency set. This is a problem, since there are usually millions of packets in a trace. In order to deal with this we make two simplifying assumptions - that transmit events in a given node are ordered, and that a given transmit can only depend on the packets received in a window of time since the k previous transmits at that particular node. For example, if k=1, it means that a transmit can only depend on packets that have arrived since the previous transmit event at the same node; if k=2, a transmit can only depend on packets that have arrived since the transmit event before the most recent one. In addition to the base trace, m other traces are created by partitioning the nodes into m sets such that pairs of nodes with the most communication between them are put in different sets. The m additional traces are generated using networks with skewed link latencies, such that the outgoing links of the nodes in one of the m partitions has high latency, while the remaining links have a latency of one. The highly skewed latencies in the partitioned network conﬁgurations serve to expose the dependencies between packets, since some packets will be delayed while waiting for dependent packets arriving on slower links, while others with no dependencies on packets from slower links will not. The PDG_GEN algorithm has 3 steps: 2The analogy with a Muller C element in asynchronous logic is intentional as it helps us deﬁne parameters such as network latency more intuitively and accurately Table 1: Trace Fragment - TX denotes transmit event and RX denotes a receive event. For simplicity only the time is shown, the rest of the details of the packet are omitted TX RX RX RX RX Sample # 1 2 3 T13 1000 1050 1100 T6 900 1020 1045 T7 950 1000 1050 T8 980 1030 1075 T9 990 1100 1095 STEP 1: For each transmit event Ei , add all the receive events within the window of W to the set of potential receive dependencies, Si . STEP 2: Remove all receive events from Si that violate causality, i.e. arrive after the transmit event Ei , in any of the m traces. STEP 3: Find the computation time (Di ) associated with the transmit event Ei . The computation time associated with a transmit event is calculated as follows: The initial computation delay for each event is computed using the base trace. Let Ei =< Ti , Li , Ri , Pi > be a transmit event in node (or processor) Li . Recall that this event will happen in cycle Ti after all the packets in its reception dependency set have arrived. Let T j be the clock cycle at which the last member of the reception dependency set arrives. The initial computation delay D j is then calculated as Ti − T j . We deﬁne two properties: Property 1: If node N transmits a packet Pi at time Ti and if the initial computation delay as computed above is Di , then any packet received by node N at time T j > Ti − Di cannot be a reception dependency for Pi . Property 2: If node N transmits packet Pi at time Ti , and Pi ’s computation time is Di , then any packet received by n at time T j < Ti − Di cannot be Pi ’s last reception dependency. These properties are used to prune the set of possible dependencies as follows. The last reception dependency for each packet is found in each of the m traces, and if it violates Property 1 in any trace, it is removed from the set Si . If any of the elements in Si violate Property 2, it means that the estimated computation time Di was too small, so the corresponding reception dependency is removed from Si . This process continues until no pruning occurs for an entire iteration of all the traces. Note that the choice of m is up to the developer of the PDGs. A small value of m will require a lower upfront investment in terms of simulation time, because m + 1 is the number of full system simulations that are needed. However, it will also lead to less accurate results, since it might not be possible to expose many packet dependencies. PDG_GEN Example The following is an example of how we determine the computational delay and reception dependency set for a given packet. Table 1 gives the transmission and reception times for a packet (P13 ) in three different traces. STEP 1 of the algorithm would result in adding events E6 , E7 , E8 , and E9 as potential reception dependencies for packet P13 . In STEP 2 of the algorithm any events that violate causality will be removed - in this example, E9 will be removed from the reception dependency set since it arrives after the transmission of P13 in trace sample 2. In STEP 3, the computational delay is estimated ﬁrst, which in this case is 20 (E8 is the last in the set to be received before transmission of P13 in the ﬁrst trace). Next, Properties 1 and 2 are used to iteratively prune the set of initial dependencies generated in STEP 1. At this point Property 1 holds - however, trace sample Figure 2: dependencies Space-Time Diagram to Illustrate Quasi3 violates Property 2, because the computational delay exhibited in trace sample 3 is 25 (E8 is received 25 before transmission of P13 ). Therefore, E8 is removed from the reception dependency set. The third step repeats by calculating the new computational delay, now estimated to be 50 (E7 is received at 950). Property 1 does not hold now, since event E6 occurs within the window of the transmission of P13 and 50 before that in trace sample 2. E6 is removed from the dependency set, and at this point all Properties hold for the third step for all traces. Thus the PDG_GEN algorithm will indicate P13 is dependent upon only E7 , with a computational delay of 50. 5. METHODOLOGY AND RESULTS 5.1 Validation (cid:48) (cid:48) In the previous section, we described how a PDG is generated for a given application mapped to N processors. In this section we describe a methodology to validate the proposed approach. We validate our approach by comparing a reference PDG to the PDG inferred by the PDG_GEN heuristic. Since reference PDGs do not exist for benchmarks such as SPLASH or PARSEC, we created our own set of reference PDGs using a trafﬁc generator which takes a wide range of user selectable parameters as inputs, and generates trafﬁc based on well-known synthetic trafﬁc models for onchip networks. Next we created m trace samples by simulating the PDG on a modiﬁed version of Garnet that tracks dependencies, and fed them into PDG_GEN to create PDG . We then compared PDG to PDG to see how many dependencies were missed, as well as the number of additional ""quasi-dependencies"" that were added (we deﬁne quasi-dependencies as packets that are classiﬁed as dependencies in PDG , but are not explicitly stated as dependencies in PDG.) We also compared the execution time for PDG and PDG as well as the average network latency predicted from PDG(cid:48) to the , average network latency in PDG, when the two PDGs were simulated on the modiﬁed Garnet. Figure 2 illustrates the two types of quasi-dependencies. In this ﬁgure packet 7 is actually dependent on packet 6, while packets 2 and 5 are quasi-dependencies. The PDG_GEN algorithm will identify packets 2 and 5 as dependencies since the partitioning is unlikely to be able to make either packet violate causality. For example, a trace generated with Node A using slow outbound links will also slow delivery of packet 4 and hence delay packet 6, and slowing Node B in a partitioning will further exacerbate the problem. Fortunately, regardless of topology, packet 2 is highly unlikely to ever be the last dependency met since it is sent before packet 4. However, quasi-dependencies of the form of packet 5 are of greater concern, since it is possible that for some topology packet 5 will be (cid:48) (cid:48) Figure 3: Comparison of true and quasi-dependencies for the various trafﬁc patterns (normalized to number of depencies present in the reference PDGs) (a) 8x8 Mesh (b) 3 Level FatTree Figure 5: Normalized Latency for different trafﬁc patterns for Stripped PDG, PDG and Inferred PDG on 8x8 mesh (a) and 3 level FatTree (b) networks (a) 8x8 Mesh (b) 3 Level FatTree Figure 4: Normalized Execution Time for different trafﬁc patterns for Stripped PDG, PDG and Inferred PDG on 8x8 mesh (a) and 3 level FatTree (b) networks received after packet 6 (the true dependency). Property 1 and Property 2 used in the PDG_GEN algorithm are more likely to prune packet 5 from the set of reception dependencies than packet 2, thus reducing the potential impact on predicted execution time. The reference PDGs were created for the following trafﬁc patterns - uniform random (rand), nearest neighbor (nn), tornado (tor), transpose (trans), bit inverse (inv), hotspot (hot), and NED [28]. We also used three additional trafﬁc patterns - ball, central and tree for further stress testing. The ball pattern simulates a selectable number of tokens that are randomly sent to the next node based on NED (modeling passing a beach ball in a crowd). The central pattern simulates a central or hotspot node that receives requests and responds to the source node (designed to model a central memory controller or a master node). The tree pattern models a barrier synchronization, whose performance is critical in many parallel applications. Figure 3 shows that PDG_GEN discovers almost all of the true dependencies for most of the trafﬁc patterns. With the exception of ball, PDG_GEN also ﬁnds a large number of quasi-dependencies, but we will next show that they have little impact on execution time and network latency. Execution time and average network latency are other important metrics to evaluate the accuracy of the inferred PDGs. To perform this analysis, we created reference PDGs of 1M packets for a 64 Figure 6: Accuracy vs. Number of Partitions node system. We ran our algorithm with k=1 and m=4 to generate the inferred PDGs. The reference and inferred PDGs were then simulated on an 8x8 mesh with two virtual channels. We compared the total execution time (shown in Figure 4(a)) and average packet latency (shown in 5(a)) of the inferred PDG and the reference PDG. We also compared the inferred PDG performance to that of traces with all dependencies removed, which are referred to as Stripped PDGs. In the ﬁgures, the execution times and average packet latencies are normalized to the original PDG, and they show that on average the execution times of the inferred PDGs were within 3% of the reference PDGs. Figures 4(b) and 5(b) show the same PDGs running on a 3 level FatTree network. The average difference of execution time between the inferred and reference PDGs in this case was 1.5%, while the execution time of the stripped PDGs varied widely between different trafﬁc patterns (for both the mesh and the FatTree) - a further demonstration of the importance of augmenting the traces with dependency information. The largest discrepancy in execution time for the inferred PDG was 13.1%, seen in the tree trafﬁc pattern. This may seem large, but it pales in comparison to the Stripped PDG, which exhibited a 94.8% difference. 5.2 Sensitivity Analysis Figure 6 shows the performance of PDG_GEN for different values of m (the number of sample traces used to generate the inferred (a) Static (b) Dynamic Figure 7: Effect of Window Size on Accuracy for NED trafﬁc pattern on 128 nodes with Static ((a)) and Dynamic ((b)) Window Sizes. Data is normalized to the number of true dependencies in the reference PDG for the NED trafﬁc pattern PDG.) Notice that even for relatively small values of m, there are very few missed true dependencies. The biggest impact of using a larger value of m is that it lowers the number of quasi-dependencies detected, and the quasi-dependencies do not appear to have a significant impact on execution time or average latency. This is encouraging, because it means that a small number of full system simulations will be necessary to generate the PDGs, even for a large number of nodes. This ensures that the proposed approach is scalable, which is one of the goals of the work. Figure 7(a) shows the performance of PDG_GEN on a system with 128 nodes using a NED trafﬁc pattern and m=8. In this ﬁgure only w previous packets are examined when inferring dependencies. The results show that as the window size grows the number of true dependencies detected increases, but the number of quasidependencies increases even faster. Figure 7(b) shows the results for the same setup using various dynamic window sizes, where all received packets since the k previous transmits are considered. The results are normalized to the number of true dependencies in the reference PDG, and they show that using a dynamic window of k=1 tends to perform well due to its adaptive nature. The ﬁgure also shows that increasing k does not dramatically improve the detection of true dependencies, but does dramatically increase the number of quasi-dependencies detected. 5.3 PDG Results for Shared Memory Benchmarks By using the simulation setup described in Section 2, we were able to acquire traces from real shared memory benchmarks running on Simics for use with the PDG algorithm. We ran Simics simulations in a 16 core conﬁguration for FFT, LU, and CHOLESKY from the SPLASH 2 benchmark suite (other benchmarks from the suite were omitted due to lack of adequate simulation resources). We chose to use m=4 and k=1, which Figures 6 and 7 indicate will yield a good tradeoff between accuracy and required simulation overhead. The traces obtained from the Simics simulations cannot be directly used by the PDG_GEN algorithm because the packets (or messages) in each of the m runs of the simulator are not identical. This is due to the fact that the trafﬁc in shared memory systems consists of coherence messages generated by cache misses, and con(a) Execution Time (b) Average Network Latency Figure 8: 64K point FFT execution time (a) and Average Network Latency (b) for Simics full system simulation and PDG trace-based network simulation ﬂicts resulting from each processor making requests to read/write memory locations in the shared address space. Each trace will be of a different length due to a slightly different sequence of coherence events occurring when a given processor-to-directory latency is higher or lower than it was originally. We addressed this issue by developing a matching algorithm that correlates the messages that correspond to each other across all traces. Most of the matching can be done by using only the message contents (source, destination, memory address reference, coherence message type, etc.). Any messages that are not matched across all the traces cannot remain in the trace as-is, because the PDG_GEN algorithm requires each trace to consist of identical messages. However, if only the successfully matched messages are used, the overall trafﬁc volume will be lower, thus changing the trace’s impact on the network it is injected into. In order to keep the volume of trafﬁc consistent, we ""ﬁll out"" the traces with messages that are in the base trace but unmatched, by inserting the messages into the traces that are missing them. The base trace was chosen as the comparison since it represents the theoretical ideal (the base trace should be closest to the true data dependencies). After running the matching algorithm on the Simics traces, we ran the PDG_GEN algorithm on them and generated PDG traces for each benchmark. The PDG traces corresponding to each benchmark were simulated on a modiﬁed version of Garnet, using various network topologies; these results were then compared with the results of Simics simulations on the same topologies. Figure 8 shows the execution times and average network latencies for Simics, PDG, and basic timestamp simulations for a 64K point FFT on various topologies. Figures 9 and 10 show results for 256x256 LU and CHOLESKY with TK14.O input, respectively. The PDG trace averaged 11.4%, 23.4%, and 5.2% error in execution time and 0.16%, 0.34%, and 1.3% error in average network latency for FFT, LU, and CHOLESKY, respectively. On average across all the benchmarks, the PDG was 2.5 times more accurate than the basic timestamp for execution time and 5.7 times more accurate for latency. The PDG was the least accurate on the LU benchmark, a problem that most likely could be remedied through increasing the number of samples (m) used by PDG_GEN. These results show that the PDG_GEN algorithm can be applied to real world benchmarks that are commonly employed by researchers today. 5.4 Discussion Several questions may have come to the reader’s mind regarding this approach. For example, isn’t running m full system simulations increasing the amount of work? Is a gate level model necessary? Are there other approaches to solving this problem, and if so, why was this one chosen? Can taint analysis techniques from the security ﬁeld be used to infer packet dependencies? Is the static (a) Execution Time (b) Average Network Latency Figure 9: 256x256 LU execution time (a) and Average Network Latency (b) for Simics full system simulation and PDG tracebased network simulation (a) Execution Time (b) Average Network Latency Figure 10: TK14.O CHOLESKY execution time (a) and Average Network Latency (b) for Simics full system simulation and PDG trace-based network simulation. (Note: Simics simulation errors forced us to use lower link latencies for fattree and ring for the CHOLESKY experiments) application partitioning in the full system simulation a limitation due to this approach? Why does the heuristic perform poorly on the hotspot trafﬁc based reference PDG? Here we will attempt to address these questions. It is likely that the m full system simulations will require a substantial amount of work on useful (long) benchmarks, but the simulations only have to be performed once - the inferred PDG derived from the full system runs will be used for future NoC exploration. In addition, the required full system simulations will be run on fully connected networks, and therefore will execute somewhat faster than more complex multi-hop networks. There are other approaches to developing the PDG from full system simulations. While modifying the full system simulator to produce the packet dependencies may seem to be the most intuitive approach, there are many pitfalls that would need to be addressed. Tracking dependencies within an application would require not only modiﬁcation of the full system simulator, but also the application source code as well. This fact alone may make the modiﬁed simulator approach infeasible, since source code is not always available for both the applications and the full system simulators. Furthermore the tracking of the dependencies within the simulator becomes so cumbersome that this approach is unrealistic. Unlike taint tracking, commonly used in security [29] which requires only a logical or of the taint tag bit, dependency tracking requires creating list unions for each operation. Consequently, a taint tracking approach would be unpractical for any application that executes for a reasonable length of time. In addition ""taint explosion"" would result in unwieldy dependency lists for most memory locations. Recall that the PDG is generated for a given application mapping. Clearly, if the application mapping changes, a new PDG has to be generated. However, this limitation is shared by all trace based simulation techniques, and we consider it a fair tradeoff for the potential gains in simulation speed and accuracy. The PDG_GEN algorithm performs poorly on the hotspot and central trafﬁc patterns since many packets are sent to a hotspot node, and few are transmitted from it to the other nodes. This results in the high rate of quasi-dependencies (for hotspot), as well as the low detection rate of the true dependencies. The way we partition the m runs (all outgoing links from a node are slowed, for example) does not help to expose the true dependencies in this case, and a method that marks only some outgoing links of a particular node as slow may provide for more accurate results. 6. CONCLUSION AND FUTURE WORK In this paper we have shown that using network traces which have only total ordering information to predict system level performance (such as execution time or network latency) can lead to erroneous conclusions, because in a real system there are reception dependencies between packets - i.e., some packets cannot be sent until some other packets have been received. We have presented a methodology to infer PDGs from traces that can be used for future design space exploration. The proposed PDG_GEN algorithm has been shown to accurately detect the true dependencies over a wide range of trafﬁc patterns. The number of quasi-dependencies detected by the current implementation of the PDG_GEN algorithm has limited impact on the execution time or average latency of the inferred PDG, but they do increase the time necessary to perform the simulation. An investigation into techniques to reduce the number of quasi-dependencies should be completed in the future. Different partitioning schemes, as discussed in the previous section, may also be useful in reducing the number of quasi-dependencies and may increase the detection of true dependencies. In addition, the matching algorithm that we use to correlate messages between shared memory traces has room for improvement. It may be possible to identify sections of each trace that correlate well, and in some way account for the sections of each trace in which memory access and coherence events played out differently. We plan to address these and other issues in the future. Despite the shortcomings of the matching algorithm, our experiments show that inferred PDGs can predict execution time for applications running on shared memory systems with an average error of 13.4% (2.5 times more accurate than trace based simulation without dependencies). We expect our approach to yield even more accurate results for message passing systems, where there is no layer of indirection caused by cache coherence protocols. We plan to investigate the application of this approach to message passing systems in the future. 7. "
"FIST - A fast, lightweight, FPGA-friendly packet latency estimator for NoC modeling in full-system simulations.","FIST (Fast Interconnect Simulation Techniques) is a fast and simple packet latency estimator to replace time-consuming detailed Network-on-Chip (NoC) models in full-system performance simulators. FIST combines ideas from analytical network modeling and execution-driven simulation models. The main idea is to abstractly model each router as a load-delay curve and sum load-dependent delay at each visited router to obtain a packet's latency by tracking each router's load at runtime. The resulting latency estimator can accurately capture subtle load-dependent behaviors of a NoC but is much simpler than a full-blown execution-driven model. We study two variations of FIST in the context of a software-based, cycle-level simulation of a tiled chip-multiprocessor (CMP). We evaluate FIST's accuracy and performance relative to the CMP simulator's original execution-driven 2D-mesh NoC model. A static FIST approach (trained offline using uniform random synthetic traffic) achieves less than 6% average error in packet latency and up to 43x average speedup for a 16x16 mesh. A dynamic FIST approach that adds periodic online training reduces the average packet latency error to less than 2% and still maintains an average speedup of up to 18x for a 16x16 mesh. Moreover, an FPGA-based realization of FIST can simulate 2D-mesh networks up to 24x24 nodes, at 3 to 4 orders of magnitude speedup over software-based simulators.","FIST: A Fast, Lightweight, FPGA-Friendly Packet Latency Estimator for NoC Modeling in Full-System Simulations Michael K. Papamichael∗ , James C. Hoe+ and Onur Mutlu+ ∗Computer Science Depar tment, Carnegie Mellon University, Pittsburgh, PA, USA +Electrical and Computer Engineering Depar tment, Carnegie Mellon University, Pittsburgh, PA, USA papamix@cs.cmu.edu, jhoe@ece.cmu.edu, onur@cmu.edu ABSTRACT FIST (Fast Interconnect Simulation Techniques) is a fast and simple packet latency estimator to replace timeconsuming detailed Network-on-Chip (NoC) models in fullsystem performance simulators. FIST combines ideas from analytical network modeling and execution-driven simulation models. The main idea is to abstractly model each router as a load-delay curve and sum load-dependent delay at each visited router to obtain a packet’s latency by tracking each router’s load at runtime. The resulting latency estimator can accurately capture subtle load-dependent behaviors of a NoC but is much simpler than a full-blown execution-driven model. We study two variations of FIST in the context of a software-based, cycle-level simulation of a tiled chip-multiprocessor (CMP). We evaluate FIST’s accuracy and performance relative to the CMP simulator’s original execution-driven 2D-mesh NoC model. A static FIST approach (trained oﬄine using uniform random synthetic traﬃc) achieves less than 6% average error in packet latency and up to 43x average speedup for a 16x16 mesh. A dynamic FIST approach that adds periodic online training reduces the average packet latency error to less than 2% and still maintains an average speedup of up to 18x for a 16x16 mesh. Moreover, an FPGA-based realization of FIST can simulate 2D-mesh networks up to 24x24 nodes, at 3 to 4 orders of magnitude speedup over software-based simulators. Categories and Subject Descriptors I.6.5 [Simulation and Modeling]: Model Development— Modeling methodologies General Terms Design, Performance, Measurement Keywords Modeling, Network-on-Chip, Full-System Simulation, Performance Models, FPGA Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS ’11, May 1-4, 2011, Pittsburgh, PA, USA Copyright 2011 ACM 978-1-4503-0720-8 ...$10.00. 1. INTRODUCTION The increasing number of cores in a chip-multiprocessor (CMP) and the demand for full-system modeling has cast simulation speed and complexity as a ma jor obstacle to simulation-based architectural research. As on-chip interconnects become a vital component of future CMPs, there is increasing demand for fast, complexity-eﬀective NoC models. Typical stand-alone network simulators, such as Booksim [8] or Orion [28] employ high-ﬁdelity, cycle-accurate models to faithfully capture the detailed behavior of the on-chip interconnect — such as the eﬀects of ﬁne-grain packet interactions. While such a high level of ﬁdelity is desirable when studying a new interconnect in isolation, it becomes prohibitively complex and slow — consuming a large fraction of the total simulation time — in the context of fullsystem simulations. As a result, it is not uncommon for full-system simulators to compromise accuracy for improvements in simulation speed and reduced complexity [4, 15]. In general, within a full-system simulation framework, it is desirable to have fast, low-complexity interconnect models that closely mimic the behavior of their cycle-accurate counterparts. Furthermore, given the recent increased interest in FPGA-accelerated full-system simulation [6, 7], it is also desirable that these interconnect models can be eﬃciently implemented as part of a broader FPGA-based simulation framework. In this paper we present FIST (Fast Interconnect Simulation Techniques), a set of fast, complexity-eﬀective techniques to accurately estimate NoC packet latencies in the context of full-system CMP simulations. To avoid the cost and complexity of simulating the NoC in detail, FIST borrows an idea from analytical network modeling where each router in the network is abstractly modeled only as a set of load-delay curves. These per-router load-delay curves can be obtained via oﬄine or online training for a given network conﬁguration and traﬃc pattern. At runtime, instead of a cycle-level detailed simulation of the NoC’s operation, FIST only tracks the load experienced by each router based on the observed traﬃc. In turn, the latency of a packet is estimated by determining which routers are traversed by the packet and then summing the load-dependent latencies at each visited router. As a proof-of-concept, we present two variants of a simple FIST-based 2D-mesh model and evaluate them against a cycle-accurate interconnect model used within an existing CMP simulator. In addition, to demonstrate the low hardware complexity and scalability of FIST, we include implementation results for an FPGA-based FIST model that achieves 3 to 4 orders of magnitude speedup over softwarebased cycle-accurate NoC models. The rest of this paper is organized as follows. Section 2 provides background on full-system simulation and Section 3 introduces the FIST NoC modeling approach. In Section 4 we describe our evaluation methodology including the FIST model variants we use in our experiments. Section 5 presents accuracy, performance and hardware speedup results. Finally we discuss related work in Section 6 and conclude in Section 7. 2. BACKGROUND Full-system simulation has become an indispensable tool for studying modern multi-core computing systems with sophisticated caching, interconnect, memory, and I/O subsystems, that run rich software stacks and complex commercial workloads. To avoid complexity and detrimental slowdowns, full-system simulators must strike a balance between accuracy and performance. On the one hand, they need to achieve high simulation speeds to be able to simulate modern workloads consisting of tens of billions of instructions. On the other hand, they need to model the system at suﬃcient ﬁdelity to not unacceptably compromise timing accuracy and skew overall system performance results (e.g., aggregate system instruction throughput). The required functionality and simulation ﬁdelity of a network model within a full-system simulation framework is typically much simpler compared to a dedicated network simulator. For example, in its simplest form, the network model of a full-system simulator only needs to ”tag” packets with their estimated delay. Moreover, full-system simulators can in many cases tolerate some degree of inaccuracy at submodules, as long as the overall performance trends are not signiﬁcantly aﬀected. This set of goals and requirements gives a diﬀerent twist to the problem of NoC modeling and forms the guidelines around which FIST was developed. 3. THE FIST APPROACH At an abstract level, any interconnection network, regardless of topology, can be decomposed into a set of routers that are connected by links. At this abstraction level, all buﬀering and logic within the network is lumped inside the router, which can be treated as a black box device with a set of input and output ports. Our goal in FIST is to leverage this idea and model the network while still retaining this ”black box” abstraction, i.e., without having to deal with and model the complex internal structure, logic and state of a router. To achieve this, FIST borrows an idea from analytical network modeling, where for a given network conﬁguration and traﬃc pattern each router is represented as a set of load-delay curves. These load-delay curves relate the load at the input ports of a network router to the average latency of a packet going through this router. During runtime, FIST tracks the load at each router and uses these curves for packet latency estimation. 3.1 How FIST Works Routers as Curves. As an example, consider a 2D-mesh network that consists of horizontal and vertical links connecting a grid of routers. Figure 1 shows a few possible ways to decompose this mesh network into ”black box” components that can be represented by one or more load-delay curves. In the simplest case each router is mapped to a single Figure 1: Representing routers as load-delay curves. load-delay curve (as in the top-row routers in the ﬁgure), but a more elaborate mapping can be used to adjust the level of abstraction. For example, to raise the level of abstraction, multiple routers can be lumped together and represented by a single curve (as in the four bottom-right routers). Alternatively, if a more detailed representation of the network is desired, then multiple curves can be used for a single router; for instance there can be a separate curve characterizing the traﬃc for each packet priority class or each output router port (as in the bottom-left router). To construct these load-delay curves, FIST relies either on an analytical network model or on training that is performed by an existing cycle-accurate network simulator. In the latter case the network simulator can be used for oﬄine or online training. In oﬄine training, the network simulator is used before the actual experiment to pre-generate the curves based on synthetic or actual workload traﬃc patterns. In online training, the cycle-accurate network simulator occasionally runs on the side and, if needed, can dynamically update the curves while the experiment is running. Online training can still beneﬁt from the existence of a pre-generated set of curves obtained through oﬄine training (these pre-generated curves would be used while the online training warms up). FIST in Action. In the context of full-system simulation, FIST’s role is to estimate packet latencies that would be experienced by the same packet in a detailed execution-driven NoC model and deliver packets to their destinations at the appropriate time. To do this, FIST dynamically tracks the load at each router and consults the current set of load-delay curves to estimate packet latencies. For each packet that is injected into the network, FIST follows these four steps: 1. Routing: Identify the set of routers that this packet traverses. 2. Delay query: Acquire delay estimates at aﬀected routers by indexing their load-delay curves with their current load. 3. Load update: Update the load at aﬀected routers to reﬂect the load increase caused by this packet. 4. Report total latency: Sum the partial latency estimates for the aﬀected routers to obtain the total packet latency. NoC Dimensions Datapath 4x4 6x6 8x8 10x10 12x12 14x14 16x16 32-bit 64-bit 128-bit 256-bit 28% 64% 114% 178% 256% 348% 455% 51% 114% 202% 316% 456% 620% 810% 95% 214% 380% 594% 856% 1156% 1521% 184% 414% 735% 1149% 1655% 2252% 2942% Table 1: FPGA resource usage (LUT utilization) for diﬀerent NoC conﬁgurations on a Xilinx Virtex5 LX330T. Contrary to cycle-accurate network simulators, in FIST, injected packets are never actually routed through or stored inside the network model. Instead, they are ”instantaneously” processed and tagged with their estimated latency. The only state maintained and updated over time in FIST is the load at each router. By relying on analytical models or high ﬁdelity network models for training, FIST is able to ofﬂoad a large fraction of the modeling eﬀort and signiﬁcantly reduce its implementation complexity. The lightweight nature of FIST allows for very eﬃcient software-based or low cost and scalable FPGA-based implementations. 3.2 FIST-based Network Models The ”black box” approach of FIST-based network models allows them to be used as building blocks within larger software-based or hardware-based full-system simulation environments. Depending on the implemented routing logic and the selected load-delay curves, FIST can model a variety of network topologies for a range of diﬀerent network conﬁguration parameters (e.g., number of VCs, buﬀer sizes, arbitration logic). Software-based Implementations. In the case of software-based full-system simulators, FIST presents a fast and lightweight alternative to existing high-ﬁdelity network models. FIST can either completely replace existing network models or can run on the side in tandem with the existing network models to accelerate less critical regions of a workload. In this latter case, existing network models can interact with FIST to provide feedback on the observed latency estimation error and dynamically update the loaddelay curves for online training. Hardware-based Implementations. The simple, lightweight nature of FIST-based network models makes them ideal candidates for hardware implementation on an FPGA. Although FPGAs have traditionally been used as prototyping platforms for networks [26, 25], such approaches are complex and entail high implementation and veriﬁcation eﬀort, both for the initial design as well as for any subsequent modiﬁcations, because they accurately model all components of a router. More importantly, they suﬀer from limited scalability, as current FPGAs can only emulate small designs due to capacity constraints [29]. To illustrate this point, Table 1 shows the fraction of FPGA area needed for a 2D-mesh NoC that uses a state-of-the-art virtual channel (VC) router [19] for various sizes and datapath widths, obtained using FPGA synthesis. Faded cells represent design points that would not ﬁt on the FPGA. Even on one of the largest Xilinx FPGA parts, the designs that ﬁt on the FPGA are only limited to 4x4 or 6x6 meshes with a narrow datapath. A 16x16 design with a wide datapath would require 29X of the area of such an FPGA. Figure 2: FIST Hardware Block Diagram. FIST Hardware Architecture. Figure 2 shows the architectural block diagram for our hardware implementation of FIST targeting FPGAs. Our hardware implementation accepts packet descriptors through a FIFO-based input interface that contain packet information, such as source, destination and size. After being processed, packets are tagged with a latency estimate and are placed in a FIFO-based output interface. The design is pipelined and can process a new packet descriptor every clock cycle. For each packet, the hardware version of FIST follows the same four processing steps mentioned earlier in Section 2. Once a new packet descriptor is received through the FIFO input, routing logic determines which routers are aﬀected by this packet. The aﬀected routers then all simultaneously perform a parallel lookup in their load-delay curve based on their current load and subsequently update the load to reﬂect this packet’s contribution to additional load. Finally, a tree of adders sums up the partial router latencies to tag the packet with the ﬁnal latency estimate, at which point it is ready to be placed in the output FIFO interface. Compared to existing approaches [25, 27, 26] that use FPGAs to implement the target simulated network at cyclelevel accuracy, FIST provides an attractive lower ﬁdelity alternative that not only avoids the high implementation complexity, but also vastly increases the scalability of the model, in terms of the number of simulated routers. As shown later in the results section, we were able to ﬁt up to a 24x24 (576-router) 2D-mesh FIST network model on an FPGA that perfectly replicates the software-based FIST models presented in Section 4. 3.3 FIST Limitations and Requirements FIST-based network models are meant to be used within full-system simulation environments that can tolerate or simply accept some degree of simulation inaccuracies. They are not meant to replace dedicated cycle-accurate models built to study networks in detail. In fact, FIST relies on the existence of such detailed network models for the purposes of training. However, even under such conditions, the abstract modeling approach employed by FIST limits its applicability to certain types of networks and speciﬁc traﬃc patterns. FIST-friendly Networks. When used with a static set of load-delay curves obtained through oﬄine training, FIST models can successfully capture the behavior of networks that a) exhibit stable and predictable behavior as loads at individual routers ﬂuctuate and b) are used with traﬃc patterns that do not signiﬁcantly deviate from the patterns used during oﬄine training. As a counterexample, networks that react to changes in load or in traﬃc patterns by drastically altering their routing decisions or arbitration policies are not good candidates for oﬄine-trained FIST models. For instance, FIST is likely to fail when trying to model an adaptive network that employs GOAL routing [23] or a network where the traﬃc pattern rapidly oscillates between ”uniform random” and ”neighbor” [8]. To overcome some of the above limitations, FIST can be used with online training. In this case a cycle-accurate network model is occasionally run in tandem with the FIST model and ”re-trains” the load-delay curves of FIST on the ﬂy. Such a hybrid simulation model is able to adapt to transient changes in network behavior and traﬃc patterns, as long as these changes occur in suﬃciently long stable intervals to be captured by the occasional ”re-training”. Even with online training, however, a network that encounters rapid and vast changes in traﬃc patterns at a faster rate than the training rate will likely not be faithfully modeled using FIST. In summary, FIST’s limitations stem from its abstract representation of the network using load-delay curves. FIST can be viewed as a predictor that is only as good as the representativeness of its training data. The accuracy of packet latency estimates depends heavily on the ﬁdelity and representativeness of the models used for training. Moreover, since FIST relies on training data from an existing analytical or simulation model, it is obviously not useful for performing exploratory studies that study new types of networks. Accuracy Sensitivity to Load and Buﬀering. Regardless of the type of training, the accuracy of FIST is also aﬀected by the amount of buﬀering and the average load in the network. Intuitively, the amount of buﬀering determines the range of possible packet latencies and the load determines the amount of variance in observed packet latencies. As network load increases, variance is ampliﬁed, because of more ﬁne-grain packet interactions (e.g., output contention) and because of congestion that builds up at some routers and eventually starts aﬀecting other neighboring routers. FIST for NoC Modeling. In a modern CMP, NoCs have to share chip resources with other components such as cores, caches and memory controllers. Given the above limitations and the scarcity of on-chip resources, such as area, power and on-chip memory, FIST would be a good candidate for modeling NoCs that are part of a broader CMP system for the following reasons: • Due to resource contention among diﬀerent components on a CMP, NoCs are usually relatively simple designs. As an example, two recent industry-developed NoCs [10, 21] employ dimension-ordered routing, one of the simplest deterministic routing algorithms. Such algorithms are easy to model using FIST. • To compensate for their simple design and provide worst-case guarantees, NoCs are often over-provisioned and, as a result, many on-chip interconnection networks are observed to be operating at low packet injection rates relative to peak. This in turn results in low router loads [11, 13, 17], which reduce packet latency variance and create good conditions for FIST-based latency predictions. • Given the abundance of wires in on-chip environments which allows for very wide datapaths, buﬀering in NoCs is usually limited or in some cases completely eliminated [17]. As was the case with low loads, limited buﬀering also helps FIST in accurate latency estimation, because it limits the range and variance of latency values, especially when combined with low loads. 4. EVALUATION We examine two variants of a simple FIST model for 4x4, 8x8 and 16x16 2D-mesh NoCs that employ DO (Dimension Ordered) wormhole routing. We use 4VCs/channel and use 8, 16 and 32-ﬂit VC buﬀers. To measure the eﬀectiveness of FIST in a broader simulation environment, we use our FIST models to replace an existing cycle-accurate NoC simulator used within an in-house tiled CMP simulator the front-end of which is based on Pin [5]. Table 2 shows the ma jor system and network parameters. We run multiprogrammed and multithreaded workloads and observe how FIST aﬀects salient simulation results, such as average packet latency and average aggregate instruction throughput. Traﬃc is generated due to cache misses and consists of single-ﬂit control packets for data requests or coherence control messages and 8-ﬂit data packets for cache block transfers. Workloads. We use 26 SPEC CPU2006 [24] benchmarks for multiprogrammed workloads, as well as 8 SPLASH-2 [22] and 2 PARSEC [1] multithreaded workloads, which are listed in Table 3. To form multiprogrammed workloads, we classiﬁed the SPEC CPU2006 workloads in terms of network intensity and formed multiprogrammed workloads of varying intensity. Each benchmark was compiled using gcc 4.1.2 with -O3 optimizations and a representative simulation phase was chosen using PinPoints [20]. For the 4x4, 8x8 and 16x16 mesh conﬁgurations we run workloads for 50, 20 and 5 million cycles respectively, or to completion if they ﬁnish earlier than their allotted cycles. For multiprogrammed workloads, we classify benchmarks based on the amount of L1 MPKI (misses per kilo instructions), which is tightly coupled to their network intensity; benchmarks with an average MPKI greater than ten are labeled as network-intensive, while all other benchmarks are labeled as network-non-intensive. Depending on the fraction of network-intensive workloads, multiprogrammed workloads (MP) are classiﬁed as low (0%), medium (50%) or high (100%) intensity. For each memory intensity category (0%, 50% and 100%), we simulate 16 multiprogrammed workloads, for a total of 48 workloads in the case of 4x4 or 8x8 meshes. For 16x16 mesh conﬁgurations we only simulate 24 multiprogrammed workloads, 8 from each network intensity class. Model Parameters. In our FIST models we represent each router as a set of two load-delay curves. The ﬁrst curve relates the load at a router to the average network latency of a packet traversing this router on its way to the destination. This network latency is deﬁned to be the time interval from the moment a packet’s tail ﬂit enters a router’s VC buﬀers until the moment it reaches the next router on its path. The second curve relates the load at a router against the injection latency that a packet observes when it is injected into the network from the node attached to that router. Load, which is used to index the load-delay curves, is deﬁned as the number of ﬂits observed on the ﬁve input ports of a router in the last HL (HistoryLength) cycles. We set the HL parameter according to the size of the VC buﬀer size of the NoC. For a VC buﬀer size of 8, 16 and 32 ﬂits we set the HL parameter to 64, 128 and 256 cycles respectively. System topologies 4x4, 8x8, 16x16 mesh (core + coherent L1 + slice of the distributed L2 at each node) CPU model Out-of-order x86, 128-entry instruction window, 16 MSHRs Private L1 cache 64 KB, 2-way associative, 64-byte block size Shared L2 cache Perfect (always hits), distributed (S-NUCA [12]) with randomized static mapping of cache blocks to L2 slices Coherence protocol Simple directory-based, based on SGI Origin [14], perfect directory Interconnect links 1-cycle latency, 64-bit ﬂit width (8 ﬂits per cache block) Router conﬁgurations 4-cycle latency, 4VCs/channel, 8 and16 ﬂits/VC for 4x4 and 8x8, 16 ﬂits/VC for 16x16 Table 2: System parameters used in our evaluation. SPEC2006 Benchmarks for multiprogrammed workloads Low network intensity: gromacs, tonto, calculix, namd, dealII, h264, gobmk, sjeng, hmmer, perlbench High network intensity: xalancbmk, gcc, libquantum, sphinx3, bzip2, povray, mcf, lbm, soplex, leslie3d, astar, omnetpp, gems, wrf, milc, cactus Multithreaded workloads SPLASH-2: cholesky, lu n, lu c, ocean n, barnes, fmm, water n, water s PARSEC: streamcluster, ﬂuidanimate Table 3: Workload Information. Oﬄine and Online FIST. We examine two variants of the presented NoC model. The ﬁrst (FIST oﬄine) relies on oﬄine training with synthetic traﬃc using the original cycleaccurate NoC model. Given the static random mapping of cache blocks to L2 slices, we train using a uniform random synthetic traﬃc pattern, which we expect to resemble actual workload traﬃc patterns. The second variant (FIST online ) relies on online training that is occasionally performed by the original cycleaccurate NoC model to obtain and dynamically update the FIST load-delay curves. In this scheme, routers are initially loaded with the same load-delay curves used in the F I ST of f line model. Once simulation starts, the cycleaccurate NoC model is enabled every Quantumcycles cycles and is then used to observe the packet latency error and train FIST for T raincycles cycles. In the beginning of each training period, the cycle-accurate NoC simulator is run for W armupcycles to warm up its structures. Training continues until the average packet latency drops below E rrorthreshold . Unless stated otherwise we set E rrorthreshold to 5%, which means that training will continue until the average packet latency error during the last T raincycles cycles falls below 5%. We empirically set the default Quantumcycles , T raincycles and W armupcycles to 100000, 10000 and 1000 cycles respectively. In the results section we also experiment with more aggressive values that reduce the amount of training to observe the eﬀects on error and performance. To avoid abrupt changes in predicted latencies during training, updated latency values aﬀect the existing latency values in a decaying fashion, where each new latency sample only contributes by a factor of Decayf actor to the existing average latency for a speciﬁc load. Given a new latency sample Latsample observed at some load, the updated latency Latupdated is calculated based on the existing latency Latprev as follows: (Decayf actor − 1) ∗ Latprev + Latsample Latupdated = Decayf actor We empirically set the Decayf actor to 100, a value that appeared to be high enough to tolerate noise in the latency updates, but also low enough to quickly adapt to changes in workload and traﬃc behavior. Accuracy. To assess the simulation inﬁdelity introduced by FIST, we present the observed error in average packet latency and average aggregate instruction throughput. Average packet latency is obtained by summing the latencies of all packets and dividing this sum by the number of packets. Average aggregate instruction throughput (IPCΣ ) is deﬁned as the sum of the average IPC (Instructions Per Cycle) for each core in the simulated system. Finally for any metric M we deﬁne the error E between the actual value Mactual and the estimated value Mestimate as follows: E = Mestimate − Mactual Mactual To gain deeper insight than what average values can provide, we also present a case study where we continuously run both the actual cycle-accurate and the FIST NoC models side-by-side and present a timeline of the actual and the estimated latencies. Hardware Evaluation. To evaluate the cost and performance of an FPGA-based FIST implementation, we developed an RTL model written in Bluespec System Verilog [2] that realizes the architecture discussed in Section 3 and shown in Figure 2. Our implementation precisely replicates the presented software-based model. We show results for FPGA resource usage and post-synthesis clock frequency estimates for a moderately sized (LX155T) and a large (LX760) Xilinx FPGA. Performance. To evaluate performance for the softwarebased models, we present the approximate speedups observed for the various FIST variants. For the software-based models, we report the network simulation speedup, which was measured based on average wall clock time of hundreds of experiments run on a uniform 256-core cluster based on 2.5GHz Intel Xeon processors. For the FIST models that rely on online training we also report FIST utilization, which is the fraction of cycles that the FIST model was running alone, without the aid of the cycle-accurate model. For the FPGA-based FIST models we report performance based on post-synthesis clock frequency results. 5. RESULTS We ﬁrst present accuracy results for our software-based FIST models for 4x4, 8x8 and 16x16 mesh conﬁgurations. We extend our accuracy results with a case study for a sample multithreaded workload and highlight speciﬁc regions of interest. We then present FPGA implementation results and ﬁnally report speedup for both the software and hardware FIST models. 5.1 Accuracy and Performance Results Figure 3 plots the error in average packet latency and aggregate IPC across all workloads using the FIST oﬄine Figure 3: Average Latency and Aggregate IPC Error for 8x8 mesh using FIST oﬄine model. model for an 8x8 network. The error for average latency is in all cases below 10%, whereas IPCΣ error is signiﬁcantly lower, always below 5%. There is a clear bias towards underestimation of latency (i.e., negative error), which can be attributed to using uniform random traﬃc for training; in our experiments we observed that even though traﬃc patterns generally resemble uniform random over long periods of execution, workloads tend to go through phases of varying intensity, that create transient hot-spots. In such cases, under the same load, the aﬀected routers will observe higher latencies compared to uniform random traﬃc. The same error results, but this time using the FIST online model that leverages dynamic training information, are shown in Figure 4. The beneﬁts of online training are clear; error for both latency and IPCΣ are greatly reduced and lie in all cases well within the 5% range. In addition, the underestimation bias is now eliminated; transient workload behavior changes are now captured by the cycle-accurate simulator that occasionally runs on the side and the FIST model is able to successfully adapt. Table 4 presents Latency and IPCΣ error for all experimental conﬁgurations, as well as observed network simulation speedup and the fraction of time spent in network simulation when using the cycle accurate NoC model. For experiments that use the FIST online model we also report FIST utilization. Finally, we also report results for a more aggressive variant of FIST online (denoted by Onlaggr in the table) where we set the Quantumcycles , T raincycles and W armupcycles parameters to 500000, 5000 and 1000 cycles respectively. Such a conﬁguration introduces slightly higher error but achieves much better performance, since it is only forced to train 1% of the time, instead of 10%, which is the case for our baseline results with FIST online. Eﬀect of Latency Error on IPC Error. It is interesting to note that for both FIST models the aggregate instrucFigure 4: Average Latency and Aggregate IPC Error for 8x8 mesh using FIST online model. Figure 5: Average Latency and Aggregate IPC Error for 8x8 mesh using hop-based model. tion throughput (IPCΣ ) error is signiﬁcantly lower than the latency error. This indicates that IPC, one of the most commonly reported results in full-system simulations, is stable under small network modeling errors. However, to show that this does not hold when larger network errors are introduced, we also ran experiments with a very simplistic hop-based model that estimates packet latencies based on the number of hops between two nodes and does not consider network load at all. The results for the simple hop-based model are shown in Figure 5. As expected, this model introduces much higher latency error, especially for the more network-intensive workloads, where it reaches up to 60%. Contrary to what was shown in the two previous ﬁgures, the severe error in latency does indeed translate into IPC error, which in some cases approaches 80%. These results indicate that fullsystem simulation might be able to tolerate some amount of inﬁdelity in the network model, but beyond some error threshold, this inﬁdelity can start introducing signiﬁcant errors in higher-level simulation results. Case Study. To get a glimpse of how FIST works at a ﬁnegrain level and to demonstrate the eﬀects of online training, Figure 6 (a) shows a timeline of average actual and estimated latency sampled every 3000 cycles for a total of 3 million cycles. Due to space limitations, we condense a 1.5M stable phase that would otherwise span the middle of the timeline. To produce the timeline, we used the ”LU” SPLASH-2 workload, which exhibits very distinct phases due to barrier synchronizations. For this experiment, we use the FIST online model, but to better highlight the eﬀects of training, we intentionally load it with initial curves that report a latency of zero for all loads. The solid line corresponds to the actual latency reported by the cycle-accurate simulator and the dashed line corresponds to latencies estimated by FIST. Having no previous training data, FIST initially is unable to immediately capture the sudden latency changes in this workload. Eventually, though, with the help of online training, it approaches the actual latencies reported by the cycle-accurate model. Notice for instance that FIST is initially unable to properly estimate the latency spike around 300K cycles, but can do so the second time it encounters a similar spike around 2300K cycles. The reaction delay exists because the cycle-accurate model is invoked only every Quantumcycles . The presence of an initial set of curves used by the FIST online model can alleviate such problems as is shown in Figure 6(b). Mesh size 4x4 8x8 16x16 (for 24 MP workloads) Flits/VC Buf 8 16 32 8 16 16 % in Net. Sim. 47.4% 46.1% 46.4% 53.2% 51.7% 58.2% FIST model Oﬄ Onl Oﬄ Onl Onlaggr Oﬄ 1.35 1.46 5.81 Onl Oﬄ Onl Oﬄ Onl Onlaggr Oﬄ 0.6 0.82 4.77 Onl Onlaggr Lat. Error % 3.7 1.39 5.08 1.37 1.54 0.92 3.88 1.34 1.63 IPCΣ Error % 1.65 Speedup 13.36 0.70 1.44 0.58 0.73 1.89 0.53 3.39 1.19 1.66 1.07 1.25 4.64 0.84 1.18 3.88 14.31 4.30 7.31 12.93 4.41 22.22 5.79 26.00 6.68 16.10 43.13 7.85 18.42 FIST Util. % 81.79 83.30 97.13 84.91 84.95 87.23 98.13 85.66 96.50 Table 4: Accuracy, performance and other results for all experiments across 4x4, 8x8 and 16x16 mesh conﬁgurations. (a) Without initial training data (b) With initial training data Figure 6: Actual and Estimated Latency for ”LU” SPLASH-2 benchmark over a window of 3 million cycles. 5.2 Hardware Implementation Results Our hardware implementation of FIST precisely replicates the presented software-based model. On a relatively small Xilinx FPGA (Virtex-5 LX155T) we were able to ﬁt models for up to 20x20 mesh networks running at 200MHz, which corresponds to a packet processing rate of 200M packets/sec. On a larger, more recent Xilinx FPGA (Virtex-6 LX760) we were able to successfully synthesize designs up to 24x24 running at estimated frequencies beyond 300MHz, which corresponds to 300M packets/sec. To put these numbers in perspective, for a small 4x4 network, the cycle-accurate software model used in this paper can process packets at an approximate rate of 30K packets/sec, which makes our FIST hardware implementation three to four orders of magnitude faster. Detailed resource usage and post-synthesis clock frequency results are shown in Table 5. The design uses only a small amount of LUTs. As a result, its scalability is only limited by the number of BRAMs required to store the load-delay curves. Virtex-5 LX155T Virtex-6 LX760 Size BRAMs LUT % Freq. BRAMs LUT % Freq. 4x4 8 1% 380MHz 8 0% 448MHz 8x8 32 5% 263MHz 32 1% 443MHz 12x12 72 11% 250MHz 72 2% 375MHz 16x16 128 20% 214MHz 129 5% 375MHz 20x20 200 32% 200MHz 201 8% 319MHz 24x24 289 12% 312MHz Table 5: FPGA resource usage (LUT utilization, #BRAMs) and post-synthesis clock frequency. 6. RELATED WORK There has been a vast body of work in the area of network modeling. The recent emergence of NoCs and increased demand for full-system simulation has posed new constraints and introduced new opportunities that even further expanded this research area; approaches range all the way from analytical modeling to hardware prototyping. In the area of NoC analytical modeling there has been a signiﬁcant amount of work examining generalized models that capture a variety of diﬀerent NoC router architectures [18, 3, 9] as well as analytical models that target speciﬁc NoC instances [16]. Compared to FIST, the static nature of analytical modeling does not make it suitable for use within execution-driven simulations. However, such models can be used to construct the load-delay curves used by FIST. In the context of abstract network modeling for full-system simulations, prior work has looked at the performance-accuracy trade-oﬀ for pure software approaches. In [4], the accuracy and performance of ﬁve simple network models within a full-system simulator is studied. The ”approximate” model shares ideas with FIST, such as the parallel processing of a packet by all aﬀected routers; however, contrary to FIST, packets are still routed through the network. The concept of load-delay curve representation has also been used in the past [15], but in a more complex manner and for entire segments of a high-speed cluster interconnect. Finally there have been a number of previous attempts at using FPGAs for NoC modeling, such as [25, 27, 26]. However, compared to FIST, these approaches oﬀer cycleaccurate ﬁdelity at the cost of very limited scalability combined with high complexity. Previous work has looked at time-multiplexing multiple simulated routers on a single virtualized FPGA-resident router [29]. While such an approach can overcome FPGA capacity limitations, it still suffers from implementation complexity, both for building the cycle-accurate router model as well as for handling timemultiplexing. 7. CONCLUSION We presented FIST, a fast, lightweight FPGA-friendly approach to packet latency estimation for full-system simulations. We evaluated two FIST-based models against multiprogrammed and multithreaded workloads on 4x4, 8x8 and 16x16 mesh conﬁgurations. Our simpler model that relies only on oﬄine training can approximate latency and aggregate throughput values within an error of 6% and 2% respectively and consistently provides simulation time speedups ranging up to 43x for 16x16 mesh networks. We also present a hybrid model that employs online training, by periodically running a cycle-accurate NoC simulator on the side. This hybrid model reduces both latency and aggregate throughput error to less than 2% and can still achieve a simulation time speedup up to 18x for 16x16 mesh networks. Finally, we present results for an FPGA-based hardware implementation of FIST that can simulate up to 24x24 mesh conﬁgurations and can achieve 3 to 4 orders of magnitude speedup over software-based cycle-accurate NoC models. 8. ACKNOWLEDGMENTS Funding for this work has been provided by NSF CCF0811702. We thank the anonymous reviewers and members of the Computer Architecture Lab at Carnegie Mellon (CALCM) for their comments and feedback. We thank Xilinx for their FPGA and tool donations. We thank Bluespec for their tool donations and support. 9. "
VLSI micro-architectures for high-radix crossbar schedulers.,"We study the scaling of parallel-matching crossbar schedulers to radices above 100. First, we examine a traditional microarchitecture that implements the matching decision of each input and each output of the crossbar in a separate arbiter block and communicates the matching decisions between the input and the output arbiters through global point-to-point links. Using simple models and experimentation with 90nm CMOS layouts, we show that this architecture is expensive because the global point-to-point links take up O(N4) area, where N the radix of the crossbar. Next, by observing that the wiring of an arbiter fits in a minimal O(NlogN) area, we propose a novel microarchitecture that inverts the locality of wires by orthogonally interleaving the input with the output arbiters, thus lowering the wiring area of the scheduler down to O(N2log2N). Using this architecture, the scheduler for a radix-128 FIFO, VOQ, or 2-VC crossbar becomes gate limited, fitting in 3.6, 7.2, and 7.2mm2 respectively, which is a 40, 50, and 70% improvement compared to the traditional. Moreover, the proposed schedulers find a new match in less than 10ns, thus allowing a minimum packet below 30Bytes at 24Gb/s line rate. Based on these findings, we conclude that crossbar schedulers are feasible even for radices above 100.","VLSI Micro-Architectures for High-Radix Crossbar Schedulers Giorgos Passas Manolis Katevenis Dionisios Pnevmatikatos Institute of Computer Science (ICS) Foundation for Research and Technology – Hellas (FORTH) Heraklion, Crete, Greece {passas, kateveni, pnevmati}@ics.for th.gr Abstract We study the scaling of parallel-matching crossbar schedulers to radices above 100. First, we examine a traditional microarchitecture that implements the matching decision of each input and each output of the crossbar in a separate arbiter block and communicates the matching decisions between the input and the output arbiters through global point-to-point links. Using simple models and experimentation with 90nm CMOS layouts, we show that this architecture is expensive because the global point-to-point links take up O(N 4 ) area, where N the radix of the crossbar. Next, by observing that the wiring of an arbiter ﬁts in a minimal O(N logN ) area, we propose a novel microarchitecture that inverts the locality of wires by orthogonally interleaving the input with the output arbiters, thus lowering the wiring area of the scheduler down to O(N 2 log 2N ). Using this architecture, the scheduler for a radix-128 FIFO, VOQ, or 2-VC crossbar becomes gate limited, ﬁtting in 3.6, 7.2, and 7.2mm2 respectively, which is a 40, 50, and 70% improvement compared to the traditional. Moreover, the proposed schedulers ﬁnd a new match in less than 10ns, thus allowing a minimum packet below 30Bytes at 24Gb/s line rate. Based on these ﬁndings, we conclude that crossbar schedulers are feasible even for radices above 100. General Terms Algorithms, Design Keywords Crossbar, Parallel Iterative Matching, ASICs Giorgos Passas and Manolis Katevenis are also with the University of Crete, Computer Science Department (CSD). Dionisios Pnevmatikatos is also with the Technical University of Crete, Department of Electronic & Computer Engineering, Microprocessor & Hardware Lab. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS’11 May 1-4, 2011 Pittsburgh, PA, USA Copyright 2011 ACM 978-1-4503-0720-8 ...$10.00. 1. INTRODUCTION Designers generally believe that many-core chips need a multi-hop Network on Chip (NoC) topology to interconnect a few tens of cores because a crossbar would be prohibitively expensive [4][13]. In contrast, we showed that the datapath of a crossbar interconnecting 128 cores in a single hop, with 24Gb/s (unidirectionally) per core, ﬁts in just 7.6mm2 of silicon in 90nm CMOS [12]. Our approach is also applicable in router chips for processormemory or I/O interconnection networks and in packetswitching fabrics for the Internet. It shows that crossbar speedup is not overly expensive, even for radices above 100. Thus, combined input-output queueing (CIOQ) [12] is advantageous over crosspoint queueing (bufXbar) [8][9][11], as it signiﬁcantly reduces the costly partitioning of buﬀer memories [12]. In this paper, we contribute the control part of the crossbar, focusing on the crossbar scheduler, which is the most intricate component thereof. We show that the scheduler of a radix-128 crossbar with single-FIFO, Virtual Output Queues (VOQs), or two Virtual Channels (VCs) per input ﬁts in less than 8mm2 of silicon. Moreover, the scheduler makes a new decision in less than 10ns, thus allowing a minimum packet as small as 30Bytes at 24Gb/s line rate. Hence, combined with our previous work, we prove that a radix-128 6Tb/s Crossbar Network on Chip is feasible, ﬁtting in less than 16mm2 of silicon, even in a conservative 90nm process. In this paper, we are concerned with the popular parallelmatching crossbar schedulers, such as the PIM [1] and the iSLIP [10] scheduler. Running these schedulers, the inputs and the outputs of the crossbar consent on a bipartite match by exchanging matching decisions, which they make locally and in parallel. Decisions are implemented using one arbiter circuit per input and one per output, and communication using point-to-point links between the input and the output arbiters [1][10]. A traditional microarchitecture [1][10][6] places each arbiter in a separate block, thus localizing the wires of the arbiters and globalizing the point-to-point links. We observe that the point-to-point links require O(N 4 ) area, where N the crossbar radix. On the other hand, the area of the arbiters is much smaller: Their aggregate area is O(N 2 ) for gates and O(N 2 logN ) for wiring. Thus, the traditional microarchitecture has a handicap at high radices. We ﬁx this handicap by inverting the locality of wires: By orthogonal ly interleaving the input with the output arbiters, we localize the point-to-point links, while only globalizing the wiring inside the arbiters. Thus, we lower the wiring area of the scheduler down to O(N 2 log 2N ). We call the proposed microarchitecture cross and the traditional microarchitecture block. We compare the above two microarchitectures on the iSLIP scheduler. Using simple models and experimentation with 90nm CMOS layouts, we show that for N above 64, a block iSLIP scheduler is limited by the area of the point-to-point links: For N =128, it takes up 14mm2 –twice our crossbar datapath, 70% of which is due to the point-to-point links. On the other hand, a cross iSLIP scheduler remains gate limited even for radices above 128: For N =128, it ﬁts in just 7mm2 , which is a 50% improvement. Next, we adapt the two microarchitectures to schedulers for FIFO and VC crossbars. By extrapolation, we ﬁnd that for N =128, a block FIFO or 2-VC scheduler is again wire limited, occupying 6 and 25mm2 respectively. On the other hand, the cross schedulers ﬁt in 3.5 and 7mm2 , which is a 40% and 70% improvement respectively. Below, (1.1) we summarize the results of our previous work on the datapath of the crossbar, (1.2) elaborate on the position of the scheduler on the die, and (1.3) outline our paper. 1.1 Synopsis of Previous Work In [12], we designed and laid out a radix-128 750MHz 32bit bit-sliced crossbar in a 90nm CMOS process [5]. The bit slice was an array of 128 128:1 multiplexor trees, placed in an area of 154µm × 1.1mm = 0.169mm2 (with 95% density), and routed in the lower four metal layers. The crossbar was two stacks of bit slices, placed in an area of (16×180µm) × (2×1.15mm) = 6.6mm2 , and routed in the upper ﬁve metal layers, on top of the bit slices. We clocked the crossbar at 750MHz using three pipeline stages: (i) one from the inputs of the crossbar to the inputs of the bit slices, (ii) one inside the bit slices, and (iii) one from the outputs of the bit slices to the outputs of the crossbar. the scheduler comprises 2N arbiters and 3N 2 point-to-point links. The arbiters can be instances of the same design. This design takes as input N request signals, ﬁnds the winning request in a round-robin schedule, and outputs the one-hot encoding of the winning request on N grant signals. Moreover, it decides whether to advance the round-robin pointer, or maintain its current value, by ORing together N update signals –for an output arbiter, these are the accepts it gets from the input arbiters, while for an input arbiter they are its own grant signals1 . The arbiter uses two strict priority-encoder blocks, as in ﬁgure 1(b). A priority-encoder block takes as input N request signals and outputs: (i) N grant signals equal to the one-hot encoding of the ﬁrst non-zero request in a strictpriority schedule, or zero when all inputs are zero; (ii) an any signal indicating whether there is at least one non-zero grant signal; and (iii) N pointer signals equal to the thermometer encoding of the winning request. The arbiter prioritizes the grant and the pointer outputs of the lower over the upper block whenever the any signal of the lower block is non zero. In the next arbiter decision, the lower block sees the requests masked by the pointer. Thus, the arbiter keeps granting the requests round robin until the granted request is the one with the lowest priority. Then, the lower block sees no request active, and the arbiter selects the upper block. On the other hand, the upper block sees the requests as they are, thus the round-robin pointer wraps around (modulo operation). The above design improves the vintage circular-token circuits, which are non-synthesizable. Observe that it still contains a cycle, which is broken by the register though. Finally, the output (input) arbiters encode the id of the winning input (output) on 1+logN signals, which connect to the homonym crossbar output (input). The encoders introduce small area overhead –actually, they can be removed from the critical path by registering their inputs, thus we will be ignoring them until section 5. 3. iSLIP AREA COST ANALYSIS Figure 1(a) describes the traditional architecture for parallel matching schedulers. For example, Anderson et al. [1] implemented PIM using a separate FPGA for each arbiter and running the point-to-point links oﬀ chip. On chip, the FPGAs become blocks, while the oﬀ-chip links become global links. Using this architecture, the total area of the scheduler is 2N × the area of the arbiters, plus the area of the point-topoint links. Below, (3.1) we describe simple models for the area of the arbiters and (3.2) the point-to-point links, and (3.3) we validate our models by experimenting with CMOS layouts. 3.1 Arbiter Area An arbiter (see ﬁgure 1(b)) uses N two-input AND gates for the pointer mask, 3N two-input AND/OR gates for each of the two multiplexors, N two-input OR gates for the OR of updates, plus the gates of the two priority-encoder blocks. The simplest implementation of a priority-encoder block uses the linear circuit of ﬁgure 2(a) –a chain of ORs computes the thermometer encoding of the winning request, followed by 1Or equivalently, its own request signals. 0 update req grnt 1 2 3 0 1 2 3 requests per input arbiters per output arbiters per input update grnt 0 1 1 1 req 1 1 0 1 p . e n c o d e r 0 1 0 1 0 1 0 0 0 0 1 1 any = 1 pointer 0 1 0 1 (a) top level (b) arbiter (round−robin) Figure 1: The iSLIP circuit, illustrated for an example N = 4. In (a), each downward-pointing arrow encodes the winning input (output) and connects to the corresponding output (input); the encoder is omitted in (b) for brevity. an AND operation between the (inverted) thermometer code and the requests, which gives the grants; this accounts for 2N two-input AND/OR gates. Summing up, the arbiter uses a total of 12N two-input gates. The corresponding area can be estimated by multiplying the gate count with the average area of a two-input gate. Adding the area of the N pointer ﬂip ﬂops, we get a model of the total gate area of the arbiter. We give numbers in section 3.3. On the other hand, the wiring of the arbiter is due to few linear structures –for symmetry, we assume that the OR of updates is implemented using a chain of two-input ORs. Thus, the corresponding metal-track area is k(N R/L×R/L), where k the small number of linear structures, R the average routing pitch, and L the number of metal layers per dimension. We call the above an area-optimized arbiter. Though too slow, it gives a lower bound on both gate and wiring area. To speed up the design, (i) we replace the chain in each priority-encoder block with a tree structure, as in ﬁgure 2(b) –an upwards tree of OR gates computes parts of the thermometer code, followed by a downwards tree that combines them, thus reducing delay to 2logN stages; (ii) we implement the OR of updates using a tree; (iii) we amplify the fanout of the any signal using two trees of N buﬀers each (the any signal fans out to 2N 2:1 multiplexors); (iv) we amplify the fanout of the OR of updates using one tree of N buﬀers (the OR of updates fans out to N ﬂip ﬂops). Thus, a speed-optimized arbiter uses a total of 14N twoinput gates, 3N buﬀers, and N ﬂip-ﬂops. Moreover, it uses eight trees of N nodes, thus requiring a metal-track area of 8(N R/L × (logN )R/L). Finally, the tree of ﬁgure 2(b) was proposed by Brent and Kung [2]. Faster structures exist, but in our case, they oﬀer only a minimal speedup, at signiﬁcant area cost [7]. Besides, we observe that the delay overhead of the Brent-Kung structure is due to the delay of the upwards tree only –the traversal of the downwards tree can be overlapped with the traversal of the buﬀer trees of the any signal.   (any) 0.1=0 0.2=0 0.3=0 0.4=1 0.5=1 0.6=1 0.7=1 0.8=1 thermometer−code chain in[0]=0 in[1]=0 in[2]=0 in[3]=1 in[4]=0 in[5]=0 in[6]=1 in[7]=1 0  OR gate p t r g r a n t s 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 r e q u e s t s c h a i n 1 any 1 1 metal track n x 1 1 1 1 0 0 0 0 t priority−encoder block in[0]=0 in[1]=0 in[2]=0 in[3]=1 in[4]=0 in[5]=0 in[7]=1 in[6]=1 1=4.0 1=5.0 1=6.0 1=7.0 0 . 2 = 0 2.4=1 0.4 = 1 1 = 4.0 1=4.0 4 . 6 = 0 6.0 = 1 0=2.0 1 = 6.8 4.8=1 k ill k i l l kill (any) 0.8=1 0=1.0 0=3.0 0=2.0 0 upwards OR gate downwards OR gate thermometer−code tree r e t e p t r r e q u e s t s g r a n t s 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 any 1 2logN tracks n x t 0 0 0 0 1 1 1 1 priority−encoder block (a) (b) Figure 2: The priority-encoder block of a round-robin arbiter, optimized for (a) area or (b) speed, illustrated for an example N = 8; The symbol i.j denotes the preﬁx in[i] OR in[i + 1] . . . OR in[j − 2] OR in[j − 1], where i ∈ [0 : N − 1] and j ∈ [1 : N − 1]. 3.2 Point-to-Point Link Area In ASICs, diagonal connections are implemented using                                                                                                                     out. arbs in. arbs out. arbs in. arbs iSLIP  (VLSI layout)  100  10  1   0.1   0.01   ) m m q s ( a e r A 100  10  1   0.1   0.01   ) m m q s ( a e r A all arbiters    (model)   p2p links (model)  14 sq-mm  3.7 sq-mm  grant links N = 4 1/2 horiz. + 1/2 vert. metal track / link on average 4   8 16  32 64 128 256  4   8 16  32 64 128 256  N N (a) ﬂoorplan & global routing (b) area-optimized layouts (c) speed-optimized layouts Figure 3: iSLIP area as a function of N; iSLIP is wire limited for N above 64. 4. CROSS MICROARCHITECTURE In this section, we describe the proposed cross microarchitecture. First (4.1, 4.2), we slice and interleave orthogonally the input with the output arbiters, thus removing the overhead of both the grant and the accept links. Then (4.3, 4.4), we show how to remove the overhead of the request links by maintaining state at the crosspoints of the arbiters. 4.1 Arbiter Slicing The ﬁrst step is the slicing of the arbiter. An area-optimized arbiter can be sliced into N bits as in ﬁgure 4; each bit implements: (i) one of the request inputs, (ii) one of the update inputs, and (iii) one of the grant outputs; and contains the corresponding: (i) one of the pointer-mask gates, (ii) one of the pointer ﬂip ﬂops, (iii) one of the bits of each of the two priority-encoder blocks, (iv) one of the bits of each of the two multiplexors, and (v) one of the bits of the OR of updates. Thus, ﬁve metal tracks are needed vertically (independent of N ): One track for each of the three OR chains, plus one track for each of the load enable and the any signals. The slicing of a speed-optimized arbiter is analogous. However, the chain inside each of the priority encoders is replaced by a Brent-Kung tree. Thus, each bit of the priority encoders contains one of the nodes of the Brent-Kung tree, i.e. two instead of one OR gate. Moreover, the OR of updates is a tree instead of a chain. Finally, each arbiter bit additionally contains three buﬀers, implementing one of the nodes of each of the three fanout-ampliﬁcation trees (one tree for the load-enable signal, plus two trees for the any signal). Thus, 8logN metal tracks are needed vertically, to route eight trees4 . Because the area-optimized design is impractical, in the rest of this paper, we will be considering only the speedoptimized design, for brevity. 4.2 Crossed Arbiters Now, we can invert the locality of wires by orthogonally interleaving the input with the output arbiters, as shown in ﬁgure 5. 4We consider all trees are binary, thus each tree contains N − 1 nodes, while the arbiter contains N bits. This asymmetry can be resolved by using N nodes per tree, where one node is spare. bit 0 bit 1 u p d a t e s bit 2 bit 3 g r a n t s priority encoder bit 0 priority encoder bit 0 r e q u e s t s priority encoder bit 2 priority encoder bit 2 priority encoder bit 3 priority encoder bit 3 Figure 4: Arbiter slicing, illustrated for an example N = 4. Let us index the arbiters and the bits of an arbiter with the integers i, j ; i, j in [0:N -1]. As shown in ﬁgure 5(a), the grant output of bit i of output arbiter j connects via a grant link to the request input of bit j of input arbiter i, for all i, j in [0:N -1]. Moreover, the grant output of bit i of input arbiter j connects via an accept link to the update input of bit j of output arbiter i, for all i, j in [0:N -1]. As shown in ﬁgure 5(b), by orthogonally interleaving the input with the output arbiters, we manage to place the bit i of output (input) arbiter j in the proximity of the bit j of input (output) arbiter i, for all i, j in [0:N -1]. Notice that           0 1 2 3 1 3 2 0 1 2 3 0 1 2 3 0 1 2 3 0 0 0 1 1 1 2 2 0 1 2 3 0 1 2 3 0 1 3 2 i n p u t a r b i t e r 3 i n p u t a r b i t e r 2 i n p u t a r b i t e r 1 i n p u t a r b i t e r 0 0 2 1 0 3 3 2 1 1 0 2 3 2 1 2 0 3 3 3 2 1 3 0 3 input arbiter 0 input arbiter 1 input arbiter 2 input arbiter 3 o u p u t t a r b i t e r 0 o u p u t t a r b i t e r 1 o u p u t t a r b i t e r 2 o u p u t t a r b i t e r 3 o u t p u t a r b i t e r 3 o u t p u t a r b i t e r 2 o u p u t t a r b i t e r 1 o u t p u t a r b i t e r 0 bit 0 grant accept arbiter wires Figure 5: Orthogonal interleaving of the input with the output arbiters to localize the grant and accept links, illustrated for an example N = 4. the gates of the input and the output arbiters are arranged as obeli, their wires cross. In this way, we need to provide metal tracks only for the arbiter wires. Since, there are eight global trees per output arbiter vertically and eight global trees per input arbiter horizontally, the corresponding metal-track area is N (8logN )R/L × N (8logN )R/L. Finally, crossed arbiters were also used in the Wavefront Scheduler [14]. However, in this scheduler, which pair of input and output arbiter bits has the top priority is determined by a single, global pointer, and priority shifts diagonally, in a wavefront. In contrast, iSLIP uses one separate pointer per arbiter, thus priority shifts in two phases: First vertically, in parallel across the output arbiters, and then horizontally, in parallel across the input arbiters. The stronger fairness properties of iSLIP should be attributed to this feature. 4.3 Encoded Requests Extending N request links from the scheduler to each input (see sections 1.1, 1.2) introduces too much wiring overhead. Instead, we consider that each input sends only 1+logN request signals to the scheduler, encoding the destination of each newly injected packet. For each input i, the scheduler maintains one counter for each output j , counting the packets queued for output j at input i. Additionally, for each input i, it maintains a decoder j next to the counter j , incrementing the counter whenever the newly injected packet at input i is destined to output j . Similarly, the decoder decodes the grant output of input arbiter i, and decrements the counter whenever this output is j . Thus, the counters have a consistent view of backlogs at the input queues. Note that the aggregate throughput of the encoded request links is O(N logN ), as O(N ) packets arrive at the net0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 1 3 2 0 1 2 3 0 0 1 0 0 0 0 2 0 0 0 3 3 1 1 3 2 2 3 3 3 3 3 2 2 2 2 2 1 1 1 1 1 1 2 2 1 3 3 0 3 3 0 2 2 0 1 1 [i] ++ −− [i] ++/−− request input      state 0 input      state 1 input      state 2 input      state 3 i n p u t s t a t e 0 i n p u t s t a t e 1 i n p u t s t a t e 2 i n p u t s t a t e 3 o u t p u t a r b i t e r 3 o u t p u t a r b i t e r 2 o u t p u t a r b i t e r 1 o u t p u t a r b i t e r 0 counter 0 decoder 0 Figure 6: Orthogonal interleaving of the input state with the output arbiters to localize the request links, illustrated for an example N = 4. work in this interval. On the other hand, the aggregate throughput of the point-to-point request links is O(N 2 ), as all request links may be hot during periods of traﬃc stress. Finally, instead of counting whole queue backlogs, the counters suﬃces to cover the round-trip time between the inputs and the scheduler, in analogy to ﬂow-control led buﬀers –in this paper, we consider 3-bit counters. 4.4 Crosspoint State We call a state bit the decoder and counter pair corresponding to an input-output pair. We can shorten the request links by orthogonally interleaving the state bits with the output arbiters, as shown in ﬁgure 6. Notice that the decrement of the counters is enabled by the point-to-point accept links, which are already local. In conclusion, the requests add only a small overhead of N (1+ logN ) horizontal wires, in total. Adding this overhead to the wiring area of the arbiters, the total wiring area of the scheduler is N (9logN )R/L × N (8logN )R/L. 5. CMOS LAYOUT We laid out a radix-128 cross iSLIP scheduler using some popular EDA tools and a 90nm CMOS technology with 9 layers of interconnect [5]. First, we synthesized one arbiter and one state bit. Then, by replicating and interconnecting bit cel ls, we synthesized the scheduler. Next, we placed the scheduler by generating the placement of one bit cell and shifting the coordinates of the rest5 . Figure 7 shows the ﬂoorplan of the scheduler. For 5Notably, implementing the bit cells as hard blocks was inefﬁcient because the blocks are rather small, thus introducing signiﬁcant peripheral overhead.                                                                                                                                                                                                                                                                                                                                                                    out. arb. 0 bit 0 out. arb. 1 bit 0 out. arb. 1 bit 1 out. arb. 0 bit 1 in. arb. 0 bit 0 in. arb. 0 bit 1 in. arb. 1 bit 1 in. arb. 1 bit 0 1 1 2 u . m 10.64um i n p u t s t a e t 1 i n p u t s a t t e 0 i n p u t s t a e t 1 b i t 1 b i t 0 b i t 1 b i t 0 i n p u t s a t t e 0 1 4 u m 1 4 u m x 1 2 8 = 1 8 . m m 31.92um 31.92um x 128 = 4mm 10.64um / 0.56um(pitch) x 4m. layers = 76m. tracks available 8trees/out.arb x 7m. tracks/tree = 56m. tracks utilized 10.64um 10.64um 2 . 8 u m 5 6 m . t r a c k s ( f o r i n . a r b ) + 8 ( f o r r e q u e s t s ) = 6 4 1 4 / 0 5 6 . x 4 = 1 0 0 m . t r a c k s ncode out0, bit0 ncode out1, bit1 ncode out1, bit0 ncode in0, bit0 ncode in1, bit0 ncode in0, bit1 ncode out0, bit1 ncode in1, bit1 Figure 7: Floorplan of a radix-128 cross iSLIP scheduler in 90nm CMOS. completeness, we also designed one encoder for each arbiter as seven trees of OR gates, distributed at the grant outputs of the arbiter. Total area is 7.2mm2 and area utilization nears 100%. This suggests a savings of 50% compared to the block architecture (see ﬁgure 3(c)). The area breakdown is as follows: 52% is due to arbiters (30% priority encoders, 22% multiplexors, 21% pointer ﬂip ﬂops, 12% buﬀers, and 15% others), 34% due to state keeping (50% counters and 50% decoders), and 14% due to encoders –see table 1. In section 3, for simplicity, we assumed the block architecture is stateless. While the block architecture needs state keeping as well, at high radices, where wiring dominates, the                                                           00 01 10 11 20 21 30 31 Figure 8: 2-VC crossbar scheduler, illustrated for an example N = 4. Not shown is a second-level arbiter per output and per input, prioritizing the VCs. reply messages. Moreover, we consider VCs are scheduled at the ﬂit level, thus we merge the VC and the switch allocator into one VC scheduler. In particular, we consider the VC scheduler of ﬁgure 8. In this scheduler, each input may request up to V outputs, where V the number of VCs. Thus, each input has one request link per output per VC. In turn, each output has one arbiter (identical to an iSLIP arbiter) per VC and a second-level arbiter (not shown in the ﬁgure) to prioritize the VCs. Each input ORs together the output grants per VC, and a second-level arbiter (not shown in the ﬁgure) prioritizes the VCs. To ensure fairness, this scheduler needs feedback from the second-level input arbiters to the output arbiters, similarly to iSLIP. However, we omit it here for simplicity. Moreover, when the number of VCs is small, the extra multiplexors and second-level arbiters introduce small area overhead. Thus, we have a direct comparison between a VC and a FIFO scheduler. In particular, a block VC scheduler uses V × the gate area and V 2 × the wiring area of a block FIFO scheduler. Hence, for N =128 and V =2, we estimate the area of a block VC scheduler to 25mm2 . On the other hand, a cross VC scheduler emerges by replicating each bit of a cross FIFO scheduler V times. Thus, for N =128 and V =2, a cross VC scheduler takes up 7.2mm2 , which is a 70% improvement compared to the block architecture. Finally, compared to a cross FIFO scheduler, delay increases only by the delay of the second-level arbiters. For small V , we can ignore this overhead. Then, for N =128 and V =2, delay should be well below 10ns. 7. CONCLUSION High gate densities, abundant wiring resources, and studious layout techniques all make it possible to build highradix crossbar NoC, using less than 16mm2 of silicon even in 90nm CMOS. Before replacing the moderate-scale multihop NoC topologies with crossbar NoC, there are several more issues to consider, such as energy eﬃciency and fault tolerance. However, the realization of how small a crossbar NoC is, is the ﬁrst and perhaps the most important step in this direction. Acknowledgments This work was supported by the European Commission in the context of the HiPEAC Network of Excellence (FP7 #217068). We thank the reviewers for their helpful comments. "
"Congestion aware, fault tolerant, and thermally efficient inter-layer communication scheme for hybrid NoC-bus 3D architectures.","Three-dimensional IC technology offers greater device integration and shorter interlayer interconnects. In order to take advantage of these attributes, 3D stacked mesh architecture was proposed which is a hybrid between packet-switched network and a bus. Stacked mesh is a feasible architecture which provides both performance and area benefits, while suffering from inefficient intermediate buffers. In this paper, an efficient architecture to optimize system performance, power consumption, and reliability of stacked mesh 3D NoC is proposed. The mechanism benefits from a congestion-aware and bus failure tolerant routing algorithm called AdaptiveZ for vertical communication. In addition, we hybridize the proposed adaptive routing with available algorithms to mitigate the thermal issues by herding most of the switching activities closer to the heat sink. Our extensive simulations with synthetic and real benchmarks, including the one with an integrated videoconference application, demonstrate significant power, performance, and peak temperature improvements compared to a typical stacked mesh 3D NoC.","Congestion Aware, Fault Tolerant, and Thermally Efficient Inter-Layer Communication Scheme for Hybrid NoC-Bus 3D Architectures Amir-Mohammad Rahmani Turku Centre for Computer Science (TUCS), Turku, Finland University of Turku, Finland Khalid Latif Kameswar Rao Vaddina Turku Centre for Computer Science (TUCS), Turku, Finland University of Turku, Finland Turku Centre for Computer Science (TUCS), Turku, Finland University of Turku, Finland amir.rahmani@utu.fi khalid.latif@utu.fi vaddina.rao@utu.fi Pasi Liljeberg Juha Plosila Hannu Tenhunen University of Turku, Finland University of Turku, Finland University of Turku, Finland pasi.liljeberg@utu.fi juha.plosila@utu.fi hannu.tenhunen@utu.fi ABSTRACT Three-dimensional IC technology offers greater device integration and shorter interlayer interconnects. In order to take advantage of these attributes, 3D stacked mesh architecture was proposed which is a hybrid between packet-switched network and a bus. Stacked mesh is a feasible architecture which provides both performance and area benefits, while suffering from inefficient intermediate buffers. In this paper, an efficient architecture to optimize system performance, power consumption, and reliability of stacked mesh 3D NoC is proposed. The mechanism benefits from a congestionaware and bus failure tolerant routing algorithm called AdaptiveZ for vertical communication. In addition, we hybridize the proposed adaptive routing with available algorithms to mitigate the thermal issues by herding most of the switching activities closer to the heat sink. Our extensive simulations with synthetic and  real benchmarks, including the one with an integrated videoconference application, demonstrate significant power, performance, and peak temperature improvements compared to a typical stacked mesh 3D NoC. Categories and Subject Descriptors B.4.3  [Input/Output  Data  Communication]: Interconnections; B.8.1 [PERFORMANCE  AND RELIABILITY]: Reliability, Testing, and Fault-Tolerance. and  General Terms Algorithms, Performance, Design, Reliability. Keywords 3D NoC-Bus Hybrid Architecture, Routing Algorithm, Fault Tolerance, Thermal Management 3D ICs. 1. INTRODUCTION Global interconnect is one of the major concerns in current and future high-performance System-on-Chip (SoC) design. These long interconnects are quickly becoming a performance impediment Permission to make digital or hard copies of all or part of this work for personal or cla ssroom u se is granted without fee provided that copies are not made or distr ibuted for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires pr ior specific permission and/or a fee. NOCS’11, May 1–4, 2011, Pittsburgh, PA, USA. Copyr ight 2011 ACM 978-1 -4503-0720-8...$10.00. from both communication  latency and power perspectives. Networks-on-Chip (NoCs) are proposed to be used in complex SoCs for inter-core communication because of scalability, better throughput and reduced power consumption [1]. On the other hand, increasing the number of cores over a 2D plane is not efficient due to long interconnects. With the emergence of viable 3D integration technologies opportunities exist for chip architecture innovations to enhance system power/performance characteristics [2]. In 3D integration technologies, multiple layers of active devices are stacked above each other and vertically interconnected using Through-Silicon Vias (TSVs) [3][4]. Compared to 2D designs, 3D ICs allow for performance enhancements even in the absence of scaling because of the reduced interconnect lengths [5]. In addition to this clear benefit, package density is increased significantly, power is reduced due to shorter wires, and system is more immune to noise [6]. However, the length of heat conduction path and power density per unit area increase as more dies stack vertically [7]. One of the well-known 2D NoC architectures is the 2D Mesh. This architecture consists of an m× n mesh of switches interconnecting IP blocks placed along with them. The straightforward extension of this popular planar structure is 3D Symmetric NoC by simply adding two additional physical ports to each router; one for Up and one for Down [8]. Despite simplicity, this architecture has two major inherent drawbacks. Firstly, it does not exploit the beneficial feature of a negligible inter-wafer distance in 3D chips, because in this  architecture,  inter-layer  and  intra-layer  hops  are indistinguishable. Secondly, a considerably larger crossbar is required as a result of two extra ports [9]. The Stacked (Hybrid NoC-Bus) mesh architecture presented in [10] is a hybrid architecture between the packet switched network and the bus architecture to overcome the mentioned 3D Symmetric NoC challenges. It takes advantage of the short inter-layer distances, around 20µm, that are characteristics of 3D ICs [2]. It integrates the multiple layers of 2D mesh networks by connecting them with a bus spanning the entire vertical distance of the chip. As the inter-layer distance for 3D ICs is small, the bus length will also be smaller; approximately (n-1) * 20µm, where n is the number of layers. This makes the bus suitable for inter-layer communication in vertical direction. By using the stacked mesh architecture, six-port router is required instead of seven ports for typical 3D NoC router and vertical communication is just one hop away to any destination layer. The issue with this architecture is that each packet is traversed through two buffers: the source output to transfer heat from the silicon die to the ambient. Unlike the traditional wire-bonding technology, the electrical connection of a face-down (or flipped) integrated circuit onto the substrate is done with the help of conductive bumps on the chip bond pads. The conductive bumps are initially deposited on the top-side of the die during the fabrication process. It is then flipped over so that its top side faces down, and aligned with the marching pads on the substrate. The solder is then flown to complete the interconnection. The advantages of flip-chip interconnect include reduced signal inductance, power/ground inductance, and package footprint, along with higher signal density [12]. In a stacked mesh 3D architecture, the thermal coupling of vertically aligned tiles is larger than the horizontally aligned tiles [13]. This is because the thickness of the silicon dies is much smaller than the lateral dimensions, the lateral heat flow is usually lower than the vertical heat flow. Also, having interface materials with lower thermal conductivities does contribute to this issue. The thermal impact of on-chip 3D NoCs are governed by various nondesign issues like the ambient temperature, cooling solutions and the package solutions. In this paper we assume that the size of the heat sink is fixed, the ambient temperature around the chip is constant and the velocity of air-flow is set [14]. We also assume that the application mapping is fixed and just focus on the routing based approach. 5.1 Junction temperature and thermal resistance for a 3D system The two most important thermal parameters for any semiconductor device are the junction temperature (TJ) and thermal resistance (RJX). The junction temperature is usually the highest temperature on a silicon die, whereas the thermal resistance is quantified as the rate of heat transfer between two layers in a package. The junctionto-ambient thermal resistance (RJA) which is a measure to evaluate the thermal performance of a flip-chip package is determined from equation (1). R JA T T J A Q                                     (1) The single-valued junction-to-ambient thermal resistance which has been used traditionally to describe the thermal characteristics of a silicon die is not sufficient enough to describe the thermal performance of a 3D system, due to the presence of multiple heat sources and multiple thermal resistances. Hence, Jain et a l. [15] have suggested a matrix representation for the junction-to-ambient thermal resistance. In this regard, Ri j represents the temperature rise in the ith layer per unit heat dissipation in the jth layer. This is represented in the equation (2). R ij i                                                 (2) Q j Where, i is the temperature rise above ambient of the ith node and Qj is the heat generated at the jth node. The equation (2) can be rewritten as follows. R ij T T i A Q j                                      (3) Where, Ti is the junction temperature of the ith layer. So, for a simple two-die stack, where one layer is the processing layer (denoted by subscript ‘p’) and the other a memory layer (denoted by subscript ‘m’), we have 4 different thermal resistance values namely Rpp, Rpm, Rmp and Rmm and the junction-to-ambient thermal resistance can be represented as shown below. Figure 5. Example of modifications to the proposed routing to guarantee single bus-link failure tolerance not able to provide services because of thermal problem or any other faults. When the bus arbiter receives any of such signals, which indicate there is a fault on the bus link and bus is not able to provide the communication temporarily or permanently, it asserts ‘0’ on the wa it signal accordingly. When a router checks the wa it signal, it will re-route packets within the layer and in future, will not derive any traffic for inter-layer communication to the corresponding dTDMA bus. Normally, the minimal path routing will be used even in case of faulty links without any modification in the proposed routing. However there are few cases, when routing mechanism needs to be modified. Consider the node ‘000’ in Fig. 5 needs to send a packet to node ‘112’. The vertical links on two paths are faulty as shown in Fig. 5. According to the proposed routing algorithm in Section IV, the vertical dTDMA bus connecting the nodes ‘000’, ‘001’ and ‘002’ should be tried but that bus link is faulty in current situation. So, the packet will be routed to the either of nodes ‘010’ or ‘100’ according to the routing algorithm. Then the packet will be routed normally according to the proposed algorithm. Now consider another situation, if the packet is exactly below or above the destination node. In this case, it cannot be delivered to the destination if the bus link is faulty. So, the packet will be rerouted to the neighboring node within the layer, where the packet is currently residing. Then the packet will be delivered to the destination. This is the situation, when the node ‘120’ contains a packet for node ‘122’. For the proposed routing algorithm, the packet can only be routed through the vertical dTDMA bus connecting the nodes ‘120’, ‘121’ and ‘122’, which is faulty. In this situation, the packet will be routed to the nodes ‘110’ or ‘020’ following a non-minimal path. We only forward a packet through a non-minimal path in case of a vertical link failure and only for source–destination pairs connected to the same pillar, so the lack of livelock  in  the  routing algorithm  is guaranteed. These modifications enable the proposed routing algorithm to tolerate single bus link. This is especially important in future 3D integration in which the failure rate is high. If the interlayer channels are fabricated correctly, the added wires used to enhance network latency, can guarantee the packet transmission in the vertical dimension in the case of a bus link failure. 5. THERMALLY EFFICIENT ROUTING Recent advances in packaging technology have lead to Flip-Chip Ball Grid Array (FCBGA) packages being extensively used. In an FCBGA the die is mounted upside-down (flipped) and connects to the package balls (lead-free solder bumps) via a package substrate. The cross-sectional view of a modern 3D flip-chip package is shown in the Fig. 6 whose primary consideration will be its ability Z e v i t p a d A Ada ptiveZ Z X Y / Z Y X Hybr id /XYZ YXZ Figure 6. Cross-Sectional view of a modern 3D Flip-Chip package Figure 7. The proposed thermally optimal routing algorithm R JA R pp R mp R pm R mm                                (4) We have previously published a paper [16] in which we have modeled a 3D multicore system in a flip-chip package using a commercial finite element based multiphysics modeling and simulation software called COMSOL. In there, we have shown how the thermal resistance and maximum temperature values of silicon layers is greatly improved by optimally placing them in a flip-chip package. In this part of the paper, we focus on a congestion  aware  and  thermally  efficient  inter-layer communication scheme for hybrid NoC-bus 3D architectures. 5.2 Thermal Modeling for 3D NoC Although 3D NoC systems take advantage of dimensional scaling and are seen as a natural progression towards future large and complex systems, they suffer from significant thermal challenges. These thermal issues caused by device power density and ambient conditions are an important performance and reliability concern. Instantaneous high temperature rises in the devices can possibly cause catastrophic failure, as well as longterm degradation in the chip and package materials, both of which may eventually lead to system failure [12]. We have developed a thermal model for 3D NoC using HotSpot v.5.0. [17]. Hotspot is an accurate and fast thermal model suitable for use in architectural studies and is based on an equivalent circuit of thermal resistances and capacitances that correspond to micro-architecture blocks and essential aspects of the thermal package. We have exploited Hotspot’s grid model which is capable of modeling stacked 3D chips for our thermal simulations. Hotspot takes a power trace file and floorplan file as inputs apart from a lot of other model configurations and parameters which can be modified. We obtained the power trace file from our in-house cycle accurate NoC simulator which was implemented in HDL. Like [18], the tile geometry and power model has been adopted from Intel’s 65nm based 80-core processor [19]. We have modeled a 3×3×3 NoC using Hotspot. The sizes of the silicon die’s 1, 2 and 3 are 4.5 mm × 6.0 mm × 0.15 mm. The convection capacitance and convection resistance of the heat sink are 140.4 J/K and 0.1 K/W respectively. The layers of silicon die are separated by an interlayer material (ILM) whose thickness is around 0.02 mm. The cup lid which acts as the heat spreader and whose thermal conductivity is very high, is placed on top of the silicon die. The thermal interface material (TIM) which is some sort of thermal grease and has very good adhesive properties is being used as the filler material in between the heat spreader and the silicon die. The resistivity of the interlayer material is around 0.249 mK/W (i.e. thermal conductivity = 4.016 W/mK) [20]. Other parameters are left unchanged from Hotspot’s configuration file. Three effective thermal conductivities are used for the lead solder bumps/underfill layer, substrate layer and the interlayer material respectively. The interlayer material in between the silicon dies is modeled as a homogeneous layer in our thermal model. We assumed a uniform through-silicon via (TSV) distribution on the die and obtained the effective interlayer material resistivity based on the TSV density (dTSV) values from [20], where dTSV is the ratio of total TSV’s area overhead to the total layer area. Coskun et a l. [20] have observed that even when the TSV density reaches 12%, the temperature profile of the silicon die is only limited by a few degrees, thus justifying the use of homogeneous TSV density in our thermal model. According to the current TSV technology [13], the diameter of each via is 10 m, and the spacing required around the TSV’s is assumed to be 10 m [20]. For our experiments we have assumed around 8 via’s/mm2, that is around 216 vias spread across the 27 mm2 area of the silicon die. Hence the TSV density is around 0.062% and the resistivity of the interlayer material is around 0.249 mK/W (i.e. thermal conductivity = 4.016 W/mK) [20]. 5.3 Thermally Efficient Routing Strategy In a typical stacked 3D NoC, the maximum thermal conduction usually takes place from the die which is closer to the heat sink. The die closer to the heat sink also has lower junction temperature and thermal resistance. We hybridize the proposed adaptive routing (Ada ptiveZ) with available algorithms to mitigate the thermal issues by herding most of the switching activities closer to the heat sink. Assuming that the heat sink is at the bottom, the proposed routing algorithm shown in the Fig. 7 is described as follows: 2. 1. Ada ptiveZ routing: When the current node is located at the topmost layer (farthest from heat sink), the packet that needs to be sent, first traverses adaptively downwards along the ‘Z’ direction. It is then routed using a 2D routing algorithm (e.g. XY, YX). Static XYZ/YXZ routing: When the current node is located at the bottom layer (closest to the heat sink), the packet that needs to be sent, first traverses in the current layer using a 2D routing algorithm (e.g. XY, YX) and then moves upwards along the ‘Z’ direction. 3. Hybrid routing: When the current node is other than the top or bottom  layer,  then depending on  the  location of  the destination node relative to the current node, the routing is performed. That is, if the destination node is above the current node then the static XYZ/YXZ routing is performed. If the destination is below the current node then Ada ptiveZ routing is performed. Since our algorithm is adaptive, it takes care of possible congestion that might arise due to excessive routing of packets onto the layer closer to the heat sink. The proposed routing algorithm also offers negligible area overhead being at the same time thermally efficient. It is noteworthy that the routers located in the topmost and the bottom layers do not need the hybrid routing, hence they do not adversely affect the area. It should be noted that although the communication is adaptive, it is deadlock free because of the usage of the available virtual channels. In [18], Chao et a l. proposed a traffic- and thermal-aware run-time thermal management scheme using a proactive downward routing to ensure thermal safety. Although their technique has potential to enhance the runtime thermal safety, there are some important drawbacks. Firstly, to migrate the communication power toward heatsink, they use a non-minimal path routing. They showed, even if source and destination of a packet are located in adjacent layers, it may take to many vertical hops for the packet to reach the destination. But the fact is, non-minimal path routing naturally increases the zero load latency and has power overhead. Despite driving power of a vertical transfer is small, intermediate large 3D routers consume a considerable part of power budget. Secondly, the prediction-based routing algorithm imposes a large area overhead and extra TSVs because of required logic for traffic estimation, decision logic, information passing through layers, etc. Further, the window-based prediction mechanisms potentially have inefficiency due to the presence of a probability of misprediction. Our adaptive minimal routing mechanism which benefits from onehop bus-based vertical communication, overcomes these issues using run-time congestion checking before sending a packet to bottom layers. 6. EXPERIMENTAL RESULTS To demonstrate the better performance characteristic of the proposed high-speed inter-layer communication scheme, a cycleaccurate NoC simulation environment was implemented in HDL. Symmetric 3D-mesh NoC [8], Hybrid Bus-NoC 3D-mesh [10], and the proposed architecture using Ada ptiveZ routing algorithm were analyzed for synthetic and realistic traffic patterns. With the same parameters and traffic patterns, Symmetric 3D-mesh NoC and Hybrid Bus-NoC 3D-mesh architectures were analyzed. Static ZXY wormhole routing was used for Symmetric and Hybrid NoCBus 3D mesh. For proposed architecture, AdaptiveZ( ) routing was used. The proposed architecture using the thermally optimal hybrid routing algorithm was also analyzed for a realistic application which will be presented at the end of this section. In synthetic traffic analysis, we use a 27-node (3×3×3) network which models a single-chip CMP for our experiments. For each simulation, the packet latencies were averaged over 50,000 packets. The performance of the network was evaluated using latency curves as a function of the packet injection rate. It was assumed the buffer size of each FIFO was eight  its, and the data width was set to 64 bits. To perform the simulations, we used uniform, hotspot 10% and Negative Exponential Distribution (NED) [21] traffic patterns. In the uniform traffic pattern, a node sends a packet to other nodes with an equal probability. In the hotspot traffic pattern, messages are destined to a specific node (the node at (2, 2, 2)) with a certain (10% higher than average) probability and are otherwise uniformly distributed. The NED is a synthetic traffic model based on Negative Exponential Distribution where the likelihood that a node sends a packet to another node exponentially decreases with the hop distance between the two cores. This synthetic traffic profile accurately captures key statistical behavior of realistic traces of communication among the nodes. The APL curves for uniform, hotspot 10% and NED traffic patterns with varying average packet arrival rates are shown in Fig. 8, 9, and 10 respectively. It can be observed for all the traffic patterns, the network with proposed architecture saturates at higher injection rates. The reason is the Ada ptiveZ routing for inter-layer communication increases the bus utilization and makes the load balanced. In the case of static ZXY routing, the Hybrid NoC-Bus architecture cannot deliver the desired throughput because of bandwidth limitations. For the proposed architecture, bandwidth limitations are managed by proper buffer utilization without increasing the communication resources. For NED traffic, the throughput curves show a significant difference in performance as shown in Fig. 10. For realistic traffic analysis, the encoding part of videoconference application with sub-applications of H.264 encoder, MP3 encoder and OFDM transmitter was used. The video stream used for simulation purposes was of size 300×225 pixels and each pixel consists of 24 bits. Thus each video frame is composed of 1.62 Mbits and can be broken into 8400 data packets each of size 7 flits including the header flit. The data width was set to 64 bits. The application graph with 26 nodes is shown in Fig.11. In this application, the Mem_In_Video component generates 8400 packets for one application cycle equivalent to the one video frame. The frame rate for the video stream was 30 frames /second and the data rate for the video stream was 49336 kbps. The application graph consists of processes and data flows; data is, however, organized in packets. Processes transform input data packets into output ones, whereas packet flows carry data from one process to another. A transaction represents sending one data packet by one source process to another, target process, or towards the system output. A packet flow is a tuple of two values (P, T). The first value ‘P’ represents the number of successive, same size transactions emitted by the same source, towards the same destination. The second value ‘T’ is a relative ordering number among the (packet) flows in one given system. For simulation purposes, all possible software procedures are already mapped within the hardware devices. The application mapped to 3×3×3 3D-mesh NoC is shown in Fig. 12. The central node (1, 1, 1) was used as a platform agent for monitoring purposes. On the assumption that the power supply voltage is 1V, a 200 MHz clock frequency was applied to the system. The buffer sizes were set to eight its. To estimate the power consumption, we extended the high-level NoC power simulator presented in [22] to support the 3D NoC architectures. The power estimation is for the interconnection ) s e l c y c ( L P A Symmetric NoC 3D Mesh Hybrid NoC-Bus 3D Mesh Proposed Hybrid NoC-Bus 3D Mesh 450 350 250 150 50 ) s e l c y c ( L P A Symmetric NoC 3D Mesh Hybrid NoC-Bus 3D Mesh Proposed Hybrid NoC-Bus 3D Mesh 450 350 250 150 50 ) s e l c y c ( L P A Symmetric NoC 3D Mesh Hybrid NoC-Bus 3D Mesh Proposed Hybrid NoC-Bus 3D Mesh 450 350 250 150 50 0.05 0.1 0.15 0.2 Average Packet Arrival Rate (packets/cycle) 0.05 0.1 0.15 Average Packet Arrival Rate (packets/cycle) 0.2 0.05 0.1 0.15 0.2 Average Packet Arrival Rate (packets/cycle) Figure 8. Latency versus average packet arrival rate under uniform traffic Figure 9. Latency versus average packet arrival rate under hotspot 10% traffic Figure 10. Latency versus average packet arrival rate under NED traffic       network including NoC switches, bus arbiters, intermediate buffers, and interconnects. For the realistic application, the proposed architecture which benefits from the thermally efficient hybrid routing was also analyzed. The simulation results for power and performance of the videoconference encoding application are shown in Table. 1. The proposed architecture using the AdaptiveZ routing algorithm showed 13% and 4% drop in power consumption over the Symmetric 3D-mesh NoC and Hybrid NoC-Bus 3D mesh architectures, respectively. Similarly, 18% and 8% reduction in APL over the Symmetric 3D-mesh NoC and Hybrid Bus-NoC 3Dmesh architectures was observed for the proposed architecture. Note that the proposed technique only deals with inter-layer communication, but these values are calculated for the whole system. It could be deduced that the proposed architecture has considerably optimized  the  inter-layer communication with negligible area overhead. In addition,  the average power consumption and APL of the proposed architecture using the thermally efficient hybrid routing are shown in Table 1. As predicted, although we are herding more traffic loads to the Layer 2 (closest to the heatsink) to mitigate the peak temperature, there is a negligible APL rise due to the routing adaptivity. Figure 11. Communication trace of encoding part of a H.264 video conference Figure 12. Partition and core mapping of the video conference encoding application Table 1. Power Consumption and Average Packet Latency 3D NoC Architecture Power Cons. (W) Ave. Pkt. Latency (cyc les) Symmetr ic NoC 3D Mesh 1.587 186 Hybr id Bus-NoC 3D Mesh 1.439 166 Proposed Hybrid Bus -NoC 3D Mesh (AdaptiveZ routing) 1.382 153 Proposed Hybrid Bus -NoC 3D Mesh (hybrid rout ing) 1.428 168 Table 2. Layer temperature pro le of the Hybrid Bus-NoC 3D Mesh-based system running the video conference application Layer ID  Peak Temperature (°C) Min Temperature (°C) Layer 0 114.0 80.8 Layer 1 113.5 77.0 Layer 2 93.5 69.6 Table 3. Layer temperature pro le of the proposed Hybrid Bus-NoC 3D Mesh-based system running the video conference application (thermally efficient hybrid routing) Layer ID  Peak Temperature (°C) Min Temperature (°C) Layer 0 110.0 80.4 Layer 1 110.3 76.7 Layer 2 94.8 69.5 We next study the impact of the proposed thermally efficient hybrid routing algorithm on the chip temperature of a 3×3×3 NoCbased system using the thermal model presented in Section V. To this end, we have imported the physical floorplan and the obtained power trace file to the thermal simulator, and estimate the temperature profile for each layer. The results of the thermal simulations on normal Hybrid Bus-NoC 3D Mesh-based and the proposed Hybrid Bus-NoC 3D Mesh-based (hybrid routing) systems running the videoconference encoding application are shown in Table 2 and Table 3, respectively. In these tables, we show the steady state minimum and peak temperatures of each layer. La yer 0 is considered to be the one which is farther from the heatsink in our thermal model. The comparison between these tables shows the effectiveness of temperature optimization of the proposed thermally optimal hybrid routing. As expected, moving from the traditional 3D NoC to the proposed 3D NoC causes the peak temperature of the chip to decrease. The significant importance of the proposed hybrid routing algorithm is the mitigation of hotspots. Hotspots can noticeably exacerbate performance and reduce the lifetime of the chip. The figures given in Table 2 and Table 3 show that the proposed technique improves the peak chip  temperature with negligible performance degradation. The improvement is up to 4°C for the realistic application. For such a system without too much interlayer communication, this achievement would be interesting. We expect more improvement for an application with more interlayer communication. Assuming that the mapping of task is predefined and the computation power cannot be migrated, the proposed hybrid routing offers a significant peak temperature improvement by only migrating the communication power. Fig. 13 shows the steady state grid level thermal maps of the die 1 (La yer 0) for both the normal and our proposed adaptive routing approach. In the figure, each tile is comprised of a router (R) including a network interface, its attached PE (P) and memory (M), and the corresponding links. On this layer the efficacy of our proposed adaptive routing approach is seen quite clearly. In this figure, the temperature values are in the Kelvin scale. It can be observed that the drop in the maximum temperature in this layer with our proposed routing approach is around 4K. Other layers are not shown here because the maximum temperature differences in those layers with normal and adaptive routing is not much. This is due to the presence of a very efficient heatsink which takes most of the heat generated in the system and transfers it to the ambient. Finally, the area of the different routers was computed once synthesized on CMOS 65nm LPLVT STMicroelectronics standard cells using Synopsys Design Compiler. The typical and the 2000 ,2 5600 ,1 1400,2 2800,1 4200 ,5 2800 ,1 8400,0 2800,1 2100 ,6 240 ,8 240,9 4200 ,4 2280,1 2210 ,10 2280 ,11 660 ,7 30 ,3 660 ,7 30,3 600 ,8 YUV Gene ra tor Ch rom m a Res am ple r Padding for MV Com pu ta tion Mo tion Es tim a tion Motion Com pensa tion Trans fo rm (DCT) Quan tization (Q) IQ Entropy Encoder IDCT Predic to r De -Blocking Fi lte r 90 ,1 30,3 90,1 Fi l te r Bnk MDCT FFT Quan tize r 90,2 90,0 20 ,5 Trans fo rm (D CT) 4200,5 2100 ,6 Stream Mux Mem Mem in Audio Mem in Video PS/TS Mux 620 ,9 640,10 SRAM 4200 ,4 Hu ffm an Enc. 20 ,4 IFFT 640 ,11 Modula to r (OFDM) Sam p le H old De-Block i ng Fi l ter Sampl e H ol d Chr omma Resampler Pr edict or M ot i on Copensat i on YUV Gener at or I DCT DCT M E I FFT DCT M em . I n V i deo SRAM Plat for m A gent Padding f or M V Comp. I Q Q Ent r opy Encoder M em . I n A udio Quant i zer H uf fman Encoder Fi lt er Bank M DCT Str eam M ux M em FFT M oduler (OFDM ) TS M ux I P Bl ock Router Interconnect Figure13. Steady-state grid level thermal maps for the die 1 (layer  0) for both the normal routing (a) and our hybrid routing (b) (a) (b) proposed bus arbiter were also synthesized to illustrate the area overhead of the controllers.  The layout area of a typical 6-port NoC router, the proposed Ada ptiveZ routing-based router, the proposed hybrid routing-based router, and the bus controllers are listed in Table 4. Note that for all the routers, the data width and buffer depth were set to 32 and 8, respectively. The figures given in the table reveal that the area overheads of the proposed routing units and the bus control module are negligible. Table 4. Hardware implementation details Component Typ ical 6 -Port Router Proposed 6-Port Router  (Ada ptiveZ Routing) Proposed 6-Port Router (Hybr id Routing) Typ ical Bus Arbiter for a 3-layer NoC Proposed Bus Arb iter for a 3-layer NoC Area (µm2) 40127 41256 41924 267 694 7. CONCLUSION In this paper, an efficient architecture for 3D stacked mesh NoC was proposed to enhance system performance, reduce power consumption, improve the system reliability, and mitigate thermal issues. To this end, a congestion aware adaptive inter-layer communication algorithm was introduced. The congestion signal triggered by the bus arbiter was used to deal with fault tolerance. Moreover, we hybridized the proposed adaptive routing with available algorithms  to mitigate  the  thermal  issues. The effectiveness of the proposed architecture regarding average packet latency, power consumption, and peak temperature has been demonstrated by experimental results using the synthetic as well as the realistic traffic loads. The results showed lower latencies for the proposed Ada ptiveZ algorithm for high packet injection rates at places where the congestion in the NoC occurs. Further, our simulations for the proposed Hybrid routing with an integrated videoconference application demonstrate peak  temperature improvements compared to a typical stacked mesh 3D NoC. 8. ACKNOWLEDGMENTS The authors wish to acknowledge the financial support by the Academy of Finland and Nokia Foundation during the course of this project. 9.  [1] A. Jantsch, and H. Tenhunen, Networ k on Chip, Kluwer Academic Publishers, 2003. [2] A. W. Topol et al., “Three-Dimensional Integrated Circuits,” IBM J . Resea r ch a nd Development, Vol. 50, No. 4/5, 2006, pp. 491-506. [3] A-M Rahmani et al., “An Efficient 3D NoC Architecture Using Bidirectional Bi synchronous Ver tical Channels,” in P r oc. of ISVLSI 2010, pp. 452-453. [4] V. F. Pa vlidis and E. G. Friedman, “3-D Topologies for Network s-onChip,” IEEE Tr a ns. on VLSI Systems., Vol. 15, No. 10, 2007, pp.1081-1090. [5] A.-M. Rahmani et al., “A Stacked Mesh 3D NoC Architecture Enabling  Congestion-Aware  and  Reliable  Inter-Layer Communication,” in P r oc. of PDP 2011, pp. 423-430. [6] R. S. Patti, “Three-D imensiona l Integrated Circuits and the Future of System-on-Chip Designs”, in Pr oc of IEEE, Vol. 94, No. 6, 2006, pp. 1214-1224. [7] K. Putta swamy and G. H. Loh, “Thermal herding: microarchitecture techniqu es for controll ing hotspots in high-performance 3D-integrated processors,” in P r oc. of HPCA 2007, pp. 193–204. [8] L. P. Carloni et al., “Networks-on-Chip in Emerging Interconnect Paradigms: Advantages and Challenges,” in Pr oc. of NOCS 2009, pp. 93-109. [9] J. Kim et al., “A novel dimensionaly-decomposed router for on-chip communication in 3D architectures,” in P r oc. of ISCA 2007. pp. 138149. [10] F. Li et al., “Design and Management of 3D Chip Mult iprocessors Using Network -in-Memory ,” in P r oc. of ISCA 2006 , pp. 130-141. [11] A.-C. Hsieh et al., “TSV Redundancy: Architecture and Design Issues in 3D IC,” in P r oc. of DATE 2010, pp. 166-171. [12] Texa s Instruments, “Flip Chip Ball Grid Array Package "
Challenges and promises of nano and bio communication networks.,"In recent years, the importance of interconnects on top-down engineered lithography-based electronic chips has outrun the importance of transistors as a dominant factor of performance. The major challenges in traditional chips are related to delays of non-scalable global interconnects and reliability in general, which leads to the observation that simple scaling will no longer satisfy performance requirements as feature sizes continue to shrink. In addition, the advent of massive-scale multicore architectures, novel silicon and non-silicon manufacturing techniques (such as self-assembly), and an increasing interest in biological components for computing force us to rethink, re-evaluate, and re-design the communication infrastructure and the communication paradigms in the era of nano- and biotechnology.
In this paper we present three showcase applications at the forefront of research of bio and nano communication networks. We focus on (1) the signaling and reliability in synthetic bio-circuits, (2) the pattern formation in distributed synthetic bio-networks, and on unstructured nanowire NOC (3). We provide an interdisciplinary and holistic view of such novel communication systems and highlight future challenges and promises.","Challenges and Promises of Nano and Bio Communication Networks Christof Teuscher Depar tment of Electrical and Computer Engineering Por tland State University Por tland, OR teuscher@pdx.edu Cristian Grecu CSAIL, MIT Cambridge, MA cgrecu@mit.edu Ting Lu Wyss Institute for Biologically Inspired Engineering Harvard University Boston, MA ting.lu@wyss.harvard.edu Ron Weiss Ron Weiss CSAIL, MIT Cambridge, MA rweiss@mit.edu ABSTRACT Keywords In recent years, the importance of interconnects on top-down engineered lithography-based electronic chips has outrun the importance of transistors as a dominant factor of performance. The ma jor challenges in traditional chips are related to delays of non-scalable global interconnects and reliability in general, which leads to the observation that simple scaling will no longer satisfy performance requirements as feature sizes continue to shrink. In addition, the advent of massivescale multicore architectures, novel silicon and non-silicon manufacturing techniques (such as self-assembly), and an increasing interest in biological components for computing force us to rethink, re-evaluate, and re-design the communication infrastructure and the communication paradigms in the era of nano- and biotechnology. In this paper we present three showcase applications at the forefront of research of bio and nano communication networks. We focus on (1) the signaling and reliability in synthetic bio-circuits, (2) the pattern formation in distributed synthetic bio-networks, and on unstructured nanowire NOC (3). We provide an interdisciplinary and holistic view of such novel communication systems and highlight future challenges and promises. Categories and Subject Descriptors C.2.1 [Computer and Communication Networks]: Network Architecture and Design; J.3 [Life and Medical Sciences]: Biology and genetics General Terms Theory, Experimentation, Reliability, Performance Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS ’11, May 1-4, 2011 Pittsburgh, PA, USA Copyright 2011 ACM 978-1-4503-0720-8 ...$10.00. networks-on-chip, transcription network, reliability, pattern formation, small-world 1. INTRODUCTION In recent years, the importance of interconnects on electronic chips has outrun the importance of transistors as a dominant factor of performance. The reasons are twofold: (1) the transistor switching speed for traditional silicon is much faster than the average wire delays and (2) the required chip area for interconnects has dramatically increased. The ITRS roadmap listed a number of critical challenges for interconnects and states that “[i]t is now widely conceded that technology alone cannot solve the on-chip global interconnect problem with current design methodologies.” The ma jor challenges are related to delays of non-scalable global interconnects and reliability in general, which leads to the observation that simple scaling will no longer satisfy performance requirements as feature sizes continue to shrink [13]. The motivation for investigating alternative and more biologically-inspired interconnects, that can for example be selfassembled easily and cheaply, or that are based on biological components themselves, can be summarized by the following observations: (1) long-range and global connections are costly (in terms of wire delay and of the chip area used) and limit system performance [13]; (2) it is unclear whether a precisely regular and homogeneous arrangement of components is needed and possible on a multi-billioncomponent or even Avogadro-scale assembly of bio and nano components [24]; (3) “[s]elf-assembly makes it relatively easy to form a random array of wires with randomly attached switches” [29]; (4) building a reliable system based on unreliable nano or bio components can be very challenging; and (5) diﬃculty to program the unreliable and potentially unstructured device a global and coherent behavior. 2. COMPLEX NETWORKS AND DESIGN TRADE-OFFS Most real networks, such as brain networks, electronic circuits, the Internet, and social networks share the so-called smal l-world (SW) property [26]. Compared to purely locally and regularly interconnected networks (such as for example a mesh interconnect), small-world networks have a very short average distance between any pair of nodes, which makes them particularly interesting for eﬃcient communication. The classical Watts-Strogatz small-world network [26] is built from a regular lattice with only nearest neighbor connections. Every link is then rewired with a rewiring probability p to a randomly chosen node. Thus, by varying p, one can obtain a fully regular (p = 0) and a fully random (p = 1) network topology. The rewiring procedure establishes “shortcuts” in the network, which signiﬁcantly lower the average distance (i.e., the number of edges to traverse) between any pair of nodes. In the original model, the length distribution of the shortcuts is uniform since a node is chosen randomly. If the rewiring of the connections is done proportional to a power law, l−α , where l is the wire length, then we obtain a smal l-world power-law network. The exponent α aﬀects the network’s communication characteristics and navigability, which is better than in the uniformly generated small-world network. One can think of other distance-proportional distributions for the rewiring, such as for example a Gaussian distribution, which has been found between certain layers of the rat’s neocortical pyramidal neurons. Studying the connection probabilities and the average number of connections in biological systems, especially in neural systems, can give us important insights on how nearly optimal systems evolved in Nature under limited resources and various other physical constraints. In a real network, it is fair to assume that local connections have a lower cost (in terms of the associated wire-delay and the area required) than long-distance connections. Physically realizing small-world networks with uniformly distributed long-distance connections is thus not realistic and distance, i.e., the wiring cost, needs to be taken into account, a perspective that recently gained increasing attention [20]. On the other hand, a network’s topology also directly aﬀects how eﬃcient the communication is, and thus how eﬃcient problems can be solved. In summary: there is trade-oﬀ between (1) the physical realizability and (2) the communication characteristics for a network topology. A locally and regularly interconnected topology is in general easy to build and only involves minimal wire and area cost, but it oﬀers poor global communication characteristics and scales-up poorly with system size. On the other hand, a random Erd¨os-R´enyi topology scalesup well and has a very short-average path length, but it is not physically plausible because it involves costly longdistance connections established independently of the Euclidean distance between the nodes. These trade-oﬀs generally apply to all types of communication networks, and in particular to the ones we focus on in this paper: bio and nano communication networks. 3. COMMUNICATION IN SYNTHETIC BIOCIRCUITS Traditionally, NoC researchers looked for inspiration in closely related ﬁelds (computer networking, supercomputing, etc.) to ﬁnd solutions for issues related to performance, scalability, and robustness of large-scale NoC designs. More recently, other areas such as social networks theory [18], chaos theory [2] and quantum mechanics [6] provided valuable tools for designing and analyzing on-chip networks. It is natural to inquire whether biological systems can inspire the design of their man-made, silicon counterparts. All organisms, either uni- or multi-cellular, use networking concepts at the very core of their existence: transcription/translation networks govern the mechanics of protein production and regulation; metabolic networks control their biochemical processes; biological neural networks support the ﬂow of information in higher level organisms. Conversely, when designing synthetic biological systems [3], bioengineers look for inspiration to electrical engineering for addressing issues like composability, reusability, robustness, etc. As synthetic bio-circuits become more complex, networking concepts can be used to ease their design and improve their predictability. Fueled by similar needs for predictable performance, large scale integration, and improved robustness, synthetic biology and NoC design can develop synergies that beneﬁt each other in unexpected ways. Here, we present a network-inspired solution for improving the robustness of a synthetic biological construct (the genetic toggle switch) using global signaling and voting mechanisms. 3.1 Components The fundamental component of a transcription network 1 is the transcriptional device, with two basic parts: the promoter and the transcribed region. The promoter allows the transcription process to be turned ON or OFF by controlling the access of DNA polymerase to the corresponding DNA region. Some promoters are always ON, in which case they are said to be constitutive. Special proteins called repressors can bind to speciﬁc DNA sequences and prevent the access of RNA polymerase. If the RNA transcript needs to be translated into a protein product, a ribosome binding site (RBS) must be present before the start codon (ATG) of the translated region. The ribosome translates the protein coding region beginning with the start codon ATG and ending with the stop codon TAA. (a) (b) RNA polymerase Rybosome DNA RNA protein pR (1) G T A A A T RBS (2) P (3) T (4) Figure 1: Components of transcriptional circuits: (a) Protein production mechanism (simpliﬁed); (b) Building parts for transcriptional devices: 1. Promoter; 2. Ribosome binding site; 3. Protein coding region with start (ATG) and stop (TAA) codons; 4. Transcriptional terminator. Additionally, transcriptional terminators may be used to force RNA polymerase to detach from the double-stranded DNA and cease RNA production; degradation tags attached to the end of the protein coding region (preceding the stop codon) may also be employed, modulating the rate at which the protein degradation occurs. These components can be combined together to build more complex DNA circuitry, whose behavior is dictated by the interactions among the diﬀerent parts. DNA circuit assembly is an active area of research [9], with strong emphasis on modularity and standardization. Libraries of biological parts are available [1] to bioengineers, enabling rapid circuit assembly through the use of pre-designed, pre-characterized modules. 3.2 Signaling In their natural environment, biological cells act as populations of individuals, with speciﬁc communication functions. Among such mechanisms we can enumerate small molecule signaling, protein transfer, and DNA transfer. In this work, communication channels are implemented using a particular class of small molecules called acyl-homoseryne lactone (AHL) produced enzymatically by the LuxI synthase. These small molecules can diﬀuse through the membrane of cells and bind to the receptor protein LuxR. The interaction between AHL and LuxR determines the latter to activate a speciﬁc promoter (pLux) and initiate transcription of the subsequent DNA region. In eﬀect, this is a uni-directional communication channel, with AHL acting as a global communication signal [17]. R pR AHL LuxI sender T LuxR T pIQ receiver pLux bidirectional link plasmid DNA signaling molecules (AHL) [AHL] (a) (b) Figure 2: Global cellular communication based on small molecule (AHL) signaling. (a) sender and receiver circuits; (b) Inter-cellular signaling by AHL diﬀusion. 3.3 Reliability We studied experimentally the long-term behavior of the Collins genetic toggle switch [11], modiﬁed to allow observability of both its states. The loss of function was observed after a time-window of 40 hours, suggesting that the Green state is not robust. In the experiment, it is noticed that the cell population starts to shift slowly from the Green state to the Red state; gradually, more cells transition from the intended state (Green) to a “fault” state (Red). We hypothesize that by stopping the proliferation of cells that transition more rapidly toward the “fault” state and encouraging the replication of cells that hold to the intended state, the circuit can be maintained in a “fault-free” state for a signiﬁcantly longer amount of time, eﬀectively error-correcting the original circuit. The mechanism that detects the ”faulty” cells is based on a global communication signal as described above that is coupled with the individual toggle switch circuit in each cell. When a cell fails, its toggle circuit becomes decoupled from the global communication signal; this situation is detected by dedicated logic circuits in the faulty cells, which disable faulty cell’s ability to be resistant to a particular antibiotic (chloramphenicol). Error correction is achieved by adding chloramphenicol to the circuit’s nutrient media: the IPTG aTc RFP LacI LacI pTet pLac TetR GFP T (a) T (b) Figure 3: (a) Genetic toggle switch; (b) loss of function by ﬂuorescence shift from Green to Red. fault-free cell survive, while the faulty cells die due to their chloramphenicol resistance being switched oﬀ by the error correction circuitry. (cid:2)(cid:3)(cid:5)(cid:1)(cid:12)(cid:1)(cid:5)(cid:8)(cid:9)(cid:4) (cid:7)(cid:10)(cid:11)(cid:6) (cid:2) (cid:3) (cid:13)(cid:13) (cid:13)(cid:14) (cid:2) (cid:2) (cid:2) (cid:3) ‘1’ – circuit lives ‘0’ – circuit dies (cid:14)(cid:14) (cid:1) (cid:1) (cid:14)(cid:13) (cid:3) (cid:2) Figure 4: K-map for designing error correcting transcriptional circuitry. 3.4 Open Problems This work is only an initial foray into addressing the robustness issues of synthetic biological circuits from the perspective of error detection and correction techniques; we ﬁnd that global communication in cell populations is an eﬀective mechanism that allows detecting and correcting faults in a genetic toggle switch. Bioengineers are facing multiple challenges: insuﬃcient understanding of failure modes and fault models, stochastic behavior of circuits within cells and of cells within populations, susceptibility to environmental conditions, etc. These are not fundamentally diﬀerent from the issues that the semiconductor industry in general, and the NoC community in particular, are facing in the light of increasingly large scale integration of devices on silicon systems. We believe there are lessons to be learned and strategies to be shared by the two communities which will greatly beneﬁt the state of the art. 4. PATTERN FORMATION IN DISTRIBUTED SYNTHETIC BIONETWORKS Biological organisms function under the regulation of various networks for gene expression, signal transduction, and metabolism. These networks consist of genes, proteins, and other small molecules and are assembled in a hierarchical structure. Such a structure ensures a cell, collection of integrated molecular circuitry, to sense environments, process information, and actuate upon stimuli. In addition to the regulatory networks involved in every individual cells, cellular communication is ubiquitous in nature and is of great importance to biological functions. Populations of cells often coordinate through intercellular signaling to collectively accomplish complicated tasks. Quorum sensing is probably the most well-known bacterial interaction system [16]. Coordinated cellular populations form a distributed biological network with every individuals as functional “nodes” and cellular communication as “wires” connecting the nodes. Synthetic biologists are exploiting the distributed nature of cellular populations and honing their ability to program the behavior of individual microbial populations to implement speciﬁc goals. 4.1 Distributed Synthetic Bionetworks in Wellmixed Systems Over the years, bioengineers have successfully constructed synthetic distributed bionetworks by employing the tools from molecular biology and knowledge from mathematical modeling. For instance, by coupling gene expression to cell survival and death using cellular communication, researchers have demonstrated the ability to program the dynamics of a population despite variability in the behavior of individual cells [28]: Speciﬁcally, a “population control” circuit was engineered to autonomously regulate the density of an Escherichia coli population. Another example is a consensus in a microbial bioﬁlm consortium mediated by engineered bidirectional communication [7]: Two colocalized populations of E. coli converse bidirectionally by exchanging acylhomoserine lactone (AHL) signals to form a consortium in which the microbial members communicate with each other and exhibit a “consensus” gene expression response. The consortium generates the gene-expression response if and only if both populations are present at suﬃcient cell densities. The third example is a synthetic “predator-prey” ecosystem consisting of two populations, which communicate bi-directionally through quorum sensing and regulate each other’s gene expression and survival via engineered gene circuits [4]. Microbial communities in the above examples are all primarily in liquid culture where cells are well mixed and thereby there is no eﬀect from spatial heterogeneity. 4.2 Spatiotemporal Pattern Formation in Distributed Synthetic Bionetworks Most cellular communities are actually sub ject to their microenvironments both in time and in space. The ﬁrst synthetic example is a synthetic multicellular “bullseye” system [5] in which genetically engineered “receiver” cells are programmed to form ring-like patterns of diﬀerentiation based on chemical gradients of an AHL signal that is synthesized by “sender” cells. In receiver cells, “band-detect” gene networks respond to user-deﬁned ranges of AHL concentrations. By fusing diﬀerent ﬂuorescent proteins as outputs of network variants, an initially undiﬀerentiated “lawn” of receivers is engineered to form a bullseye pattern around a sender colony. Another elegant example is a synchronized genetic oscillator engineered by Danino et al. [8]. An engineered gene network was constructed with global intercellular coupling that is capable of generating synchronized oscillations in a growing population of cells. Collective synchronization properties along with spatiotemporal waves occurring at millimetre scales were investigated using microﬂuidic devices tailored for cellular populations at diﬀering length scales. In addition to the programming of spatial structures, Synthetic cellular interaction in distributed bionetworks can be further employed to modulate the biodiversity of an ecosystem consisting of engineered Escherichia coli populations [21]. In the rest of this section, we will use our recently engineered bacterial patterning system as an example to illustrate spatial self-organization of distributed synthetic bionetworks [15]. A fundamental challenge in distributed bionetworks is understanding how global spatial patterns emerge from local interactions in isogenic cell populations. Although studies have analyzed emergent pattern formation in contexts ranging from bacterial colonies to animal coats, the complexity of natural biological systems makes it diﬃcult to decipher the underlying molecular circuitry and mechanisms. Synthetic systems, in contrast, are forward engineered to include relatively simple circuits that are only loosely coupled to the larger natural system into which they are embedded, making it easier to understand the molecular underpinnings of a biological phenomenon. (cid:18) (cid:19) (cid:6)(cid:13)(cid:8)(cid:4)(cid:5)(cid:10)(cid:16)(cid:12) (cid:8)(cid:7)(cid:10)(cid:16)(cid:12) (cid:6)(cid:13)(cid:8)(cid:4)(cid:5)(cid:10)(cid:16)(cid:12) (cid:23)(cid:1)(cid:12)(cid:18)(cid:25)(cid:2)(cid:3)(cid:13)(cid:15)(cid:4) (cid:5)(cid:8)(cid:14)(cid:7) (cid:23)(cid:1)(cid:12)(cid:18)(cid:25)(cid:2)(cid:3)(cid:13)(cid:15)(cid:4) (cid:5)(cid:8)(cid:14)(cid:4) (cid:7)(cid:12)(cid:13)(cid:4) (cid:10)(cid:14)(cid:7)(cid:11)(cid:10) (cid:23)(cid:1)(cid:12)(cid:18)(cid:20)(cid:24)(cid:2) (cid:5)(cid:8)(cid:9)(cid:4) l (cid:14)(cid:1)(cid:15)(cid:3)(cid:13)(cid:4)(cid:2) (cid:7)(cid:12)(cid:13)(cid:7) (cid:23)(cid:1)(cid:15)(cid:21)(cid:22)(cid:2)(cid:3)(cid:12)(cid:18)(cid:20)(cid:13) (cid:1)(cid:4) (cid:3)(cid:2)(cid:6) (cid:8)(cid:7)(cid:10)(cid:16)(cid:12) (cid:11)(cid:14)(cid:17)(cid:9) Figure 5: Design of a synthetic multicellular system for emergent pattern formation. a, Abstractly, the system consists of two signaling species 3OC12HSL and C4HSL: 3OC12HSL is an activator catalyzing synthesis of both species while C4HSL is an inhibitor repressing their synthesis. b, Genetic circuit implementation. Promoter regions are indicated by white boxes, while protein coding sequences are indicated by colored boxes. IPTG is an external inducer modulating system dynamics. Based on a classical theoretical study by Turing [25], we created an emergent patterning system in E. coli using two artiﬁcial diﬀusible morphogens, 3OC12HSL and C4HSL, from the las and rhl quorum sensing pathways in P. aeruginosa. In our system, 3OC12HSL serves as an activator of its own synthesis and that of C4HSL, while C4HSL is an inhibitor of both signals (Figure 5a). In our genetic implementation (Figure 5b), 3OC12HSL activates its own synthesis by binding regulatory protein LasR to form an activator complex of our hybrid promoter PLas−OR1 . This promoter regulates expression of LasI, a 3OC12HSL synthase, and rhlI, a C4HSL synthase. To increase cooperativity of 3OC12HSL self-activation, LasR is regulated by a second copy of PLas−OR1 . C4HSL inhibits the syntheses of 3OC12HSL and itself by forming an activator complex with regulatory protein RhlR. This complex activates expression of lambda repressor CI which, in turn, represses transcription of LasI, RhlI, LasR and RhlR. In our system, pattern formation can be modulated by altering the concentration of isopropyl β -D-1-thiogalactopyranoside (IPTG), a small molecule inducer that binds LacI and alleviates repression of PRhl−lacO . Green and red ﬂuorescent proteins (GFP and RFP) are expressed from the rhl and las hybrid promoters respectively to aide in experimental observation. The original Turing design requires that the two morphogen diﬀusion rates be distinct in order to generate spatial instability for symmetry breaking and pattern formation [12]. 3OC12HSL diﬀuses more slowly than C4HSL in our experimental setup. The slower diﬀusion rate of 3OC12HSL, coupled with positive feedback regulating its synthesis, allows 3OC12HSL to aggregate in local domains, leading to the formation of visible red ﬂuorescent spots. The competitive binding of 3OC12HSL and C4HSL to RhlR leads to green regions between the red spots. Shown in Figure 6, red ﬂuorescent spots emerge with sizes much larger than that of a single cell. Simultaneously, green ﬂuorescence develops in a honeycomb-like pattern with dark voids positioned precisely in the locations of the bright red ﬂuorescence. Figure 6: Experimental observations of emergent pattern formation. Representative microscope images of a typical ﬁeld of view showing a ﬂuorescent pattern formed by an initially homogeneous isogenic lawn of cells harboring the Turing circuit. Spots and voids appear in the red and green ﬂuorescence channels, respectively. Scale bar, 100µm. To assess whether cell growth by itself is responsible for the emerging patterns, we ﬁrst assayed the phenotypic behavior of lawns of cells that express ﬂuorescent proteins constitutively. Our experiments demonstrated that in our experimental setup, neither cell growth nor initial spatial heterogeneity of cell density give rise to the large scale spatial patterns observed with the Turing cells. Our engineered system provides a platform for understanding general features and characteristics of pattern formation and for revealing design principles underlying biological patterning systems. In the context of biomedical applications, our demonstration of large-scale global patterning using a small gene circuit is an important step towards realizing programmed spatial structures for tissue engineering. 4.3 Open Problems Despite considerable advancements in engineering distributed bionetworks in living organisms, there are several remaining challenges that hamper reliable construction and quick modulation of pattern formation in these synthetic systems. One ma jor challenge comes from the limited sets of well-characterized and orthogonal cellular communication cassettes. Programming of sophisticated spatial patterns often multiple signaling channels with minimal cross-talk. Orthogonality of cellular communication pathways will greatly increase our ability to program multiscale and sequential events in distributed systems. A second issue is the robustness of cellular patterns. Synthetic biologists have demonstrated the ability to program spatiotemporal patterns, however, these patterns are noisy and fragile due to stochastic events involved in the system, from gene expression, molecular diﬀusion, and population growth. A better designing strategy and tuning method is thereby required for constructing robust patterns. In addition, we need a mathematical framework that has ability to predict biological functions from basic components and thus guide the engineering of distributed bionetworks. 5. UNSTRUCTURED NANOSCALE INTERCONNECT NETWORKS Future nano-scale electronics built up from vast number of components need eﬃcient, highly scalable, and robust means of communication in order to ever become competitive with traditional silicon approaches. As the device dimensions shrink further, it is increasingly hard to provide structured and reliable components. In this section we explore two radically new approach of interconnecting processing elements by an unstructured network-on-chip-like interconnect: the ﬁrst approach is inspired by growing nanowires directly on a surface while the second approach is inspired by dropping nanowires onto a surface. The controlled synthesis of nanostructured interconnects represents a key challenge for future and emerging electronics. Carbon nanotubes and metallic nanowires are often mentioned as promising alternatives. Many techniques have been reported to grow both nanowires and carbon nanotubes in bulk, but they are generally hard to place in a structured way. We question the traditional approach of mesh-based interconnects and investigate a new interconnect paradigms that oﬀers both better performance and lower cost while being more appropriate for self-assembly processes. Our models are based based on physically realistic small-world network approach as explored by [20]. However, our approach is more applied, has an experimental part [27], and addresses the question of how much and what type of interconnect emerging electronics need. The primary goal is to investigate and understand the characteristics of such selfassembled networks and to ultimately use these insights to tune the chemical parameters of the self-assembly process. Others have proposed novel approaches to improve network-on-chip (NoC) performance. For example, Oshida and Ihara [19] investigated the packet traﬃc of scale-free and large-scale NOC and concluded that scale-free topologies achieve short latencies and low packet loss ratios. However, they have not considered the wiring cost. Ogras et al. [18] showed that a signiﬁcant reduction in the average packet latency can be achieved by superposing a few long-range links to a standard mesh network. In their approach, the links are not inserted at random but where they are most useful for increasing the critical traﬃc workload. Neither [19] nor [18] have considered distance-dependent connections in order to minimize the network wiring cost. More recently Ganguly et al. used wireless NoC to establish “short cuts” in the network [10]. 5.1 Finding Optimal Small-World Power-Law Network Topologies To evaluate and compare the performance metrics of different networks, we used a simple network-on-chip (NoC) framework that was ﬁrst described in [22]. The network is composed of programmable processing nodes (PNs), switch nodes (SNs), and an interconnect fabric. The connections between the SNs are assumed to be bi-directional point-topoint interconnects. We introduce two new design control parameters, p and α, which allow to explore the design space of a large class network topologies. Starting from a regular mesh with local connections only, we add R additional connections randomly to already existing links. This results in double, triple, or more connections for some nodes with some neighbors. We then apply a rewiring algorithm that goes through all the nodes and rewires each existing (locally connected) wire with probability p to a node chosen proEuclidean distance l between nodes: l−α . In simple terms, portional to a power-law distribution as a function of the α determines the proportion of non-local connections. If α = 0, the connections are established independent of the distance between them, for higher α, local connections are more likely. Thus p determines the how much irregularity and α how many non-local links the network has. Depending on p, such graphs have the small-world property, i.e., a very short average path length that scales logarithmically with the system size. Petermann and De Los Rios have [20] shown that small-world networks with power-law distributions have a lower wiring cost for the same performance. Note that the rewiring is only used “virtually” and as a model to obtain the type of self-assembled networks as explore in [27]. lowing aggregate ob jective function: y = a × hops + (1 − a) × In order to evaluate such interconnects, we deﬁne the folwirelength, where hops is deﬁned as the average number of hops of all shortest paths in the network and wirelength is the network. a ∈ [0..1] is a weight factor that allows to the sum of all wire lengths (measured in distance units) in vary the importance of hops versus wirelength in the system metric. Figure 7 shows the individual hops and cost surfaces as a function of p and α and the combined system metric for a = 0.6 and R = 200 wires. Most prominently, one can see the sharp drop in y as p increases. This illustrates the small-world property and is typical of the Watts-Strogatz rewiring model that we apply here. The lowest y value in ﬁgure 7 represents the optimal system conﬁguration (“sweet spot”) in the design space and is determined by the two parameters p and α. We observe that the optimal network is a small-world power-law network because of the values of p and α. We then further explore the design space by varying a and the number of additional wires R. Figure 8 shows the minimum system metric y , i.e. which represent the “sweet spots,” as a function of the weight a and diﬀerent number of wires R. The R0 line corresponds to a regular mesh. Each data point represents the minimum y value determined in each hops-cost surface plot (as shown in ﬁgure 7). For example, a designer can determine what the minimum number of additional connectivity for an optimal design and a given a is. From there, one can determine the corresponding p and α in the surface plots. Figure 8 further tells us that for a <0.3, a mesh-like network is beneﬁcial because it minimizes the wirelength. s p o h f o r e b m u N e g a r e v A 1 0.8 0.6 0.4 0.2 0 2 ) s t i n u e c n a s d t i ( t h g n e e l r i W 1 0.8 0.6 0.4 0.2 0 2 1 α 0 0 0.5  p 1  α 0 0 0.5  p 1 1 t h g n e e l r i w * ) a − 1 ( + s p o h * a = y 1 0.8 0.6 0.4 0.2 0 2 1.5 1 1  α 0.5 0 0 0.5  p Figure 7: Top: hops and cost surfaces as a function of p and α. Bottom: combined hops and wirelength plot for a = 0.6 and R = 200. The optimal network is a small-world power-law network. Data averaged over 10 networks. Unstructured interconnect topologies may seem intimidating and useless at ﬁrst, but as we have shown here an previously [22, 23], they oﬀer several beneﬁts over mesh-like topologies. In particular, by choosing particular values for the level of unstructuredness (p), the level of non-locality (α), and the total number of wires to be used, such networks allow a designer to explore parts of the design space that are not accessible with structured mesh-like interconnects, to minimize the wiring cost, and to maximize the performance. 5.2 Optimal Connectivity of Self-Assembled Nano Networks In this section we present an alternative model for unstructured nano-scale networks. We assume that the wires are grown in a solution, then harvested, and dropped on a surface. In 2007, Levitan [14] presented a simple model that used a uniform length distribution for the wires, which were then dropped on a 2D grid. As opposed to Levitan’s approach [14] of depositing wires with a uniform length distribution, we deposit wires that follow a power law distribution, l−α , where l is the wire length and α the power law exponent. The higher α, the more short wires the distribution will have, i.e., the more local the network will be. For α = 0, we obtain a uniform distribution. For each experiment, T wires with a length distribution following a power law are randomly deposited on a N × N grid. If two or more link ends fall in the same grid cell, the wires are linked together, and thus a network is formed. No short circuits are considered in this model, i.e., intersecting wires do not connect. Figure 9 shows the number of unconnected nodes and the size of the maximum spanning tree as a function the number of wires and of α. α = 0 corresponds to Levitan’s case [14]. As one can see, higher values of α do not signiﬁcantly aﬀect           ) , p α ( y m u m i n i m 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 R0 R25 R50 R75 R100 R125 R150 R175 R200 0.2 0.4 a 0.6 0.8 1 Figure 8: Minimum system metric y as a function of the weight a. Lower a favor networks with low wiring lengths. The datapoints represent “sweet spots” (i.e., minima) in the design space spanned by p and α. R = 200, data averaged over 10 networks. the number of unconnected nodes. However, for higher α, the maximum spanning tree is more slowly approaching the maximum number of nodes. Figure 10 shows the total network wiring cost (measured as the sum of all wire lengths) and the average shortest path length (measured in distance units) as a function of the power law exponent α. The wiring cost for higher α is lower because the network contains less longer and more shorter links. However, as one can see, for networks with enough wires, the average number of hops only starts to get higher for higher α values. For example a network with 150 wires has about the same average number of hops at α = 1.5 and at α = 0 (uniform wire length distribution), but it has a much lower wiring cost at α = 1.5. 5.3 Open Problems Several challenges need to be addressed before such radical approaches can become more mainstream. For example, there are essentially no methodologies and tools that would allow (1) to map an arbitrary computing architecture or a logical system on a randomly assembled physical substrate, (2) to do arbitrary computations with such an assembly, and (3) to systematically analyze performance and robustness within a rigorous mathematical framework. There are also several open questions regarding the self-assembling fabrication techniques, which will need to be further explored in the future. For example, a speciﬁc wire length-distribution is hard to control in a direct way. However, it is easy to grow nanowires of diﬀerent lengths in multiple steps. This would allow to approximate a speciﬁc distribution by having several wire length classes. Despite all the challenges, we believe that computation in unstructured self-assemblies of components and interconnections is a highly appealing paradigm, both from the perspective of fabrication as well as performance and robustness. 6. CONCLUSION In this paper, we have showcased three applications at the forefront of research of bio and nano communication networks. We focused on (1) the signaling and reliability in synthetic bio-circuits, (2) the pattern formation in distributed synthetic bio-networks, and on unstructured nanowire NOCS s e d o N 100 90 80 70 60 50 40 30 20 10 0 0 Unconnected Nodes (α=0) Unconnected Nodes (α=1) Unconnected Nodes (α=2) Unconnected Nodes (α=3) Maximum Spanning Tree (α=0) Maximum Spanning Tree (α=1) Maximum Spanning Tree (α=2) Maximum Spanning Tree (α=3) 50 100 150 200 250 300 350 400 450 500 550 Number of Links Figure 9: Unconnected nodes and maximum spanning tree as a function the number of wires and of α. Results are averaged over 10 networks. Grid size: 10 × 10. (3). Each of these applications has unique challenges and opportunities. In the area of (distributed) bionetworks, insuﬃcient understanding of failure modes and fault models, stochastic behavior of circuits within cells and of cells within populations, susceptibility to environmental conditions, etc., represent ma jor challenges. In the area of self-assembled nanowire interconnect, the lack of control over the fabrication process makes it hard to obtain both reliable and structured networks. However, there are two challenges in common of all the approaches: (1) the diﬃculty to program the unreliable and potentially unstructured devices and (2) the general issue of robustness. Most of the challenges highlighted in this paper are not fundamentally diﬀerent from the issues that the semiconductor industry in general, and the NoC community in particular, are facing in the light of increasingly large scale integration of devices on silicon systems. We believe there are lessons to be learned and strategies to be shared by the two communities which will greatly beneﬁt the state of the art. 7. "
Automatic verification for deadlock in networks-on-chips with adaptive routing and wormhole switching.,"Wormhole switching is a switching technique nowadays commonly used in networks-on-chips (NoCs). It is efficient but prone to deadlock. The design of a deadlock-free adaptive routing function constitutes an important challenge. We present a novel algorithm for the automatic verification that a routing function is deadlock-free in wormhole networks. A sufficient condition for deadlock-free routing and an associated algorithm are defined. The algorithm is proven complete for the condition. The condition, the algorithm, and the correctness theorem have been formalized and checked in the logic of the ACL2 interactive theorem proving system. The algorithm has a time complexity in O(N3), where N denotes the number of nodes in the network. This outperforms the previous solution of Taktak et al. by one degree. Experimental results confirm the high efficiency of our algorithm. This paper presents a formally proven correct algorithm that detects deadlocks in a 2D-mesh with about 4000 nodes and 15000 channels within seconds.","Automatic veriﬁcation for deadlock in networks-on-chips with adaptive routing and wormhole switching Freek Verbeek Institute for Computing and Information Sciences Radboud University Nijmegen, The Netherlands f.verbeek@cs.ru.nl Julien Schmaltz School of Computer Science Open University of The Netherlands Heerlen, The Netherlands julien.schmaltz@ou.nl ABSTRACT Wormhole switching is a switching technique nowadays commonly used in networks-on-chips (NoCs). It is eﬃcient but prone to deadlock. The design of a deadlock-free adaptive routing function constitutes an important challenge. We present a novel algorithm for the automatic veriﬁcation that a routing function is deadlock-free in wormhole networks. A suﬃcient condition for deadlock-free routing and an associated algorithm are deﬁned. The algorithm is proven complete for the condition. The condition, the algorithm, and the correctness theorem have been formalized and checked in the logic of the ACL2 interactive theorem proving system. The algorithm has a time complexity in O(N3 ), where N denotes the number of nodes in the network. This outperforms the previous solution of Taktak et al. by one degree. Experimental results conﬁrm the high eﬃciency of our algorithm. This paper presents a formally proven correct algorithm that detects deadlocks in a 2D-mesh with about 4000 nodes and 15000 channels within seconds. Categories and Subject Descriptors C.2.1 [Computer Systems Organization]: Network Architecture and Design—Network communications ; B.4.4 [Hardware]: Performance Analysis and Design Aids— Formal models; Veriﬁcation ; C.2.2 [Computer Systems Organization]: Network Protocols—Routing protocols General Terms Algorithms, Veriﬁcation Keywords Deadlock Avoidance, wormhole switching, NoCs, formal methods 1. INTRODUCTION Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS ’11, May 1-4, 2011 Pittsburgh, PA, USA Copyright 2011 ACM 978-1-4503-0720-8 ...$10.00. Networks-on-Chips (NoCs) constitute critical components of modern Multi-Processor Systems-on-Chips (MPSoCs) [5, 1]. Wormhole switching is a switching technique where packets are decomposed into smaller units, called ﬂits. A header ﬂit contains routing information and the tail ﬂits follow the header ﬂit in a pipeline fashion. Messages travel like worms and will be denoted as such in the reminder of this paper. Wormhole switching is nowadays commonly used in NoC designs, e.g., The Spiderdgon STNoC [4], Aethereal [10], Hermes [14], QNoC [2]. Wormhole switching reduces network latency and buﬀer sizes. The tradeoﬀ is that wormhole switching is prone to deadlock. Freedom of deadlocks is one of the most important properties of NoCs architectures [13]. Among the properties that are required for packet delivery, e.g., livelock and starvation freedom, deadlock is by far the hardest problem to solve [7]. In this paper, we present an algorithm that automatically veriﬁes deadlock freedom of wormhole networks. The most popular techniques to determine if a routing function is deadlock-free are based one the analysis of diﬀerent kinds of dependency graphs [6, 16, 8]. These proposed methodologies are built on necessary and suﬃcient conditions for deadlock-free routing. All these conditions state that some dependency graph must be acyclic. The seminal one is Duato’s condition that allows for deadlock-free adaptive routing with cyclic dependencies. This condition is complex. It involves four diﬀerent kinds of dependencies and checking that it holds for a realistic routing function is often a manual, non-trivial, and error prone task. Recently, Taktak et al. [18] have been the ﬁrst to propose a polynomial algorithm to detect deadlocks in wormhole networks. Their algorithm checks a variation of Duato’s condition and has a time complexity in O(N4 ), where N is the number of nodes in the network. Due to the complexity of wormhole switching and many subtleties in related theorems and algorithms, discrepancies have been found in the previously mentioned works [20]. These works omitted in their deﬁnitions and proofs the fact that in wormhole networks the worms cannot intersect. This fact causes Duato’s condition to be suﬃcient, but not necessary. Due to this fact a counter example can be found for Taktak’s polynomial algorithm. Most notably, this fact has been used to prove that deciding deadlock-freedom is coNP-complete [21]. In this paper we propose a novel condition and a novel associated algorithm with a time complexity in O(N3 ). This is one degree lower than the current state-of-the-art and consitutes a major improvement. In practice, a complexity in O(N4 ) or higher is ""intractable"" whereas a complexity in O(N3) is ""tractable"". This empirical observation is conﬁrmed by Figure 6b in Section 6. The condition, the algorithm and its correctness proof have been formalized and checked in the logic of the ACL2 interactive theorem proving system [12]. This provides a high degree of conﬁdence in the correctness of our algorithm, in spite of the complexities of wormhole switching. Experimental results show that routing functions for networks with thousands of nodes and channels can be analyzed within seconds1 . 2. A NOVEL CONDITION We ﬁrst introduce our network model and our condition. For space reason, we merely provide an informal introduction. For more details and a proof of correctness we refer to [21]. 2.1 Network model An interconnection network consists of processing nodes connected by channels. All nodes can generate messages of any size destined for any other node. Each node consists of ports and a central switch (see Figure 1). The switch contains the routing function. Each node has a local in- and out-port for injecting and removing messages from the network. We assume that if a message arrives at its destination node, it will eventually be consumed. This implies that a node never sends messages destined for itself. Figure 1: Processing node In wormhole networks, the atomic unit handled by channels is a ﬂit. When a message is injected in the network, it is decomposed into ﬂits. Typically, there is a header ﬂit followed by a sequel of data ﬂits. The end of a message is marked by a tail ﬂit. For simplicity, we do not distinguish between data ﬂits and the tail ﬂit. We refer to all of them as the tail or tail ﬂits. Only the header ﬂit contains information on the destination of a message. The header ﬂit advances along the route speciﬁed by the routing function, while the tail follows in a pipe-line fashion. Each channel has a bounded capacity to store ﬂits. A channel can store ﬂits belonging to at most one message. Only an empty channel can accept a header ﬂit. The header ﬂit and the tail block header ﬂits of other messages. An adaptive routing function determines which routes a message can take to its destination. At each intermediate node, the information in the header ﬂit is analyzed and used to compute a set of next hops. We consider memoryless routing, where the set of next hops only depends on the current location of the header ﬂit and the destination of the message. When multiple next hops are available, a selection function determines which channel will be taken. The selection function 1 Proof scripts and source code can be found at: http://www.cs.ru.nl/∼freekver/dl_ic.html. has no inﬂuence on deadlock-freedom [6]. It is not mentioned any further. A conﬁguration is an assignment of ﬂits to channels. Definition 1. A legal conﬁguration is a conﬁguration that satisﬁes the following properties: 1. The capacity of each channel is not exceeded. 2. If there is a ﬂit in a channel, this channel is supplied as next hop from the source of the channel for the destination of that ﬂit. 3. Each channel contains ﬂits belonging to one message only. 4. All tails form a connected simple path in the network supplied for one destination. Properties (3) and (4) reﬂect the fact that only an empty channel can accept a header ﬂit. Note that a channel containing ﬂits of message m cannot even accept the header ﬂit of m. This ensures that the ﬂits occupied by a worm constitute a simple path. Definition 2. A deadlock conﬁguration is a non-empty conﬁguration that is legal and satisﬁes the following properties: 5. There is no header ﬂit that has arrived at its destination. 6. For all header ﬂits, all next hops are unavailable. 2.2 Intuition behind our condition Figures 2 shows a legal and an illegal deadlock-conﬁguration. The legal conﬁguration contains three messages. Message 1 is destined for node A. It is blocked by Message 2 that holds channel c1 . This channel is the only channel leading from channel c4 to node A. Message 2, destined for node A as well, is blocked by Message 3. Message 3 is blocked by Message 2. The illegal conﬁguration violates the property that channels contain ﬂits of at most one message. Channel c3 contains ﬂits of Messages 1 and 2. We call a path that can be ﬁlled with a worm a dependency path. A deadlock can only be constructed using pairwise disjoint dependency paths. Our condition requires a pairwise disjoint set of paths to form a deadlock. Figure 3 shows a legal conﬁguration that is not in deadlock. Messages 2 and 3 are blocked. Message 1 is not blocked, as it can now use the new channel c8 to advance towards its destination. It can escape the congested area. Our condition requires the absence of escapes to form a deadlock. An empty channel leading out of the congested area is not necessarily an escape. First, the channel must be reachable from the header of the worm. Secondly, the escape must be supplied by the routing function for the destination of the worm. The tail ﬂits follow the header ﬂit and cannot use an escape channel. Figure 2a shows a deadlock-conﬁguration, even though there is an escape for the tail ﬂits in channel c1 . Consider the network in Figure 3. If channel c8 is not supplied for destination A, it cannot be used as an escape for Message 1. Our condition states that the absence of escapes for some pairwise disjoint set of dependency paths is necessary and suﬃcient to create a deadlock, or contrapositively: Theorem 1. A wormhole network is deadlock-free if and only if there exists an escape for all pairwise disjoint sets of dependency paths. A c3 c0 B A c2 c7 c1 A c4 Message 1 Message 2 Message 3 Header ﬂit c5 B c6 (a) Legal deadlock-conﬁguration A c0 c7 c1 A c2 A c4 Message 1 Message 2 Message 3 Header ﬂit c5 B c6 (b) Illegal deadlock-conﬁguration Figure 2: Deadlock-conﬁgurations. A c3 c0 B A c2 c7 c1 c8 A c4 c5 B c6 Figure 3: A legal conﬁguration that is not in deadlock. Theorem 1 states a necessary and suﬃcient condition for deadlock-free routing in wormhole networks. Deciding whether such a condition holds for a given network and routing function is co-NP-complete [21]. This is due to the fact that worms cannot intersect, i.e., Property 3 of Deﬁnition 1. As we want a polynomial algorithm, we provide a variation of Theorem 1. This new condition can be checked in polynomial time. Let a semi-deadlock-conﬁguration be a non-empty conﬁguration that satisﬁes properties (1), (2), (4), (5) and (6) of Deﬁnitions 1 and 2. marked as deadlock-immune and the network is deadlockfree or a deadlock can be created by ﬁlling all deadlocksensitive channels with header ﬂits and all deadlock-attainable channels with tail ﬂits. Our algorithm is an extension of our algorithm for packet networks [19].We ﬁrst demonstrate the three markings with an example. We then provide an example trace of the algorithm, showing how it determines the markings. Markings A channel is deadlock-immune if any ﬂit – belonging to any message for any destination – can never be permanently blocked. A channel is deadlock-sensitive if for some destination d there is no deadlock-immune neighbor. If this channel is ﬁlled with a header ﬂit destined for d, the message can be permanently blocked, as all neighbors can be permanently blocked (otherwise there would be a deadlockimmune neighbor). A channel is deadlock-attainable if it is not deadlock-sensitive but there is a path to a deadlocksensitive channel r. Tail ﬂits stored in this channel can be permanently blocked, if the corresponding header ﬂit is in r. n2 c1 c2 n1 s2 c0 s1 d0 n0 s0 d1 n n0 n0 n1 n1 n2 n2 d d0 d1 d0 d1 d0 d1 R(n, d) c0 s0 , c0 s2 s1 , c1 c2 c2 s2 d0 d1 d0 c1 d1 d1 c0 s1 c2 d1 s0 Figure 4: Example network, routing function and dependency graph Consider the network in Figure 4. Nodes d0 and d1 are the only possible destinations. Figure 4 also shows the routing function and the dependency graph. An edge (c0 , c1 ) is labeled d if destination d is the cause of the dependency from c0 to c1 . There exists exactly one possible deadlock-conﬁguration: channels c0 and c1 are ﬁlled with a worm with destination d1 and channel c2 is ﬁlled with a header ﬂit destined for d0 . Destinations d0 and d1 are sinks. They can never be blocked. Channels s0 , s1 , and s2 are marked as deadlock-immune. Channel c0 seems deadlock-immune for both destinations, as for both destinations a header ﬂit in c0 can escape directly to a deadlock-immune channel. No header ﬂit in c0 can ever be permanently blocked. However, channel c1 is marked deadlock-sensitive, as for destination d1 there is no deadlock-immune neighbor. As there exists a path from c0 to a deadlock-sensitive channel, c0 is marked deadlock-attainable. Lastly, channel c2 is deadlock-sensitive, as for destination d0 there is no deadlock-immune neighbor. Theorem 2. A wormhole network is semi-deadlock-free if and only if there exists an escape for all sets of dependency paths. Example trace In the remainder of this paper, we use the term “deadlock” for “semi-deadlock” if not speciﬁed otherwise. 3. ALGORITHM The basic objective of our algorithm is to mark each channel as deadlock-immune, deadlock-sensitive, or deadlock-attainable. After termination of the algorithm, either all channels are The algorithm consists of two steps. The ﬁrst step expands a spanning tree. After expanding the tree forwards, information is propagated backwards. The second step performs some post-processing. Let the algorithm start in c0 . The diﬀerent steps of the run of the algorithm are shown in Figure 5. A marking S | I stands for the destinations S leading to deadlock-sensitive neighbors and I leading to deadlock-immune neighbors. | | | | | d0d1 | d0d1 | | | (a) Step 1 (b) Step 2 (c) Step 3 (d) Step 4 | d0d1 | d0d1 d1 | d0d1 | d0 | d1 d1 | d0 | d1 d1 | d0 | d1 (e) Step 5 (f) Step 6 (g) Step 7 Deadlock-sensitive d1 | d0d1 Deadlock-attainable Deadlock-immune Unmarked: visited d0 | d1 Figure 5: Algorithm trace The algorithm expands a tree spanning over the reach of c0 . It starts by marking channel c0 as visited (Step 1). Destination d0 leads to channel s2 . Thus the next step is to expand s2 , i.e., mark it as visited (Step 2). Channel s2 is a sink and is marked deadlock-immune. Similarly, s1 is marked deadlock-immune (Step 3). From c0 , destination d1 leads to c1 . Channel c1 is expanded. Subsequently channel c2 is expanded (Step 4). In c2 , destination d1 leads to sink s0 . However, destination d0 leads to visited channel c0 . The algorithm assumes at this point that visited channel c0 is not deadlock-immune. Thus d0 does not lead to a deadlock-immune channel. Channel c2 is marked deadlock-sensitive (Step 5). As all neighbors have been visited, no further expansion is possible. The algorithm starts propagating information backwards. Destination d1 leads from c1 to a deadlock-sensitive channel. Thus c1 is marked deadlock-sensitive (Step 6). This information is again propagated backwards. Channel c0 now has a deadlock-immune neighbor for all destinations. However, as there exists a path leading to a deadlock-sensitive channel, c0 is marked deadlock-attainable. All channels have been marked. As there are channels that are not deadlock-immune, there are channels in which ﬂits can be blocked permanently. The network is not deadlockfree. Post processing In Step 5 of the previous trace, the algorithm assumed that channel c0 was not deadlock-immune. In hindsight, this assumption was correct as c0 becomes marked deadlockattainable. In general, the assumption is that visited (but unmarked) channels are not deadlock-immune. This assumption is not always true. The algorithm needs a postprocessing step, which checks for each time the algorithm had to make this assumption, whether it made a mistake. If so, then it corrects these. During the post-processing step, channels that are marked deadlock-sensitive or -attainable may become marked deadlock-immune. 3.1 Formal description We assume a dependency graph with each edge (c0 , c1 ) labeled with the destinations that route from c0 to c1 . Let Edep (c) return the set of neighbors of channel c and let ∆(c0 , c1 ) return the labels of edge (c0 , c1 ). We ﬁrst introduce the concept of dependency path. Definition 3. Let d be a destination in the network. A d-path πd is a set of channels c0 c1 . . . ck , with k > 0 such that: ∀0 ≤ i < k · ci+1 ∈ Edep (ci ) ∧ ∀0 ≤ i < k · d ∈ ∆(ci , ci+1 ) ∧ ∀0 ≤ i, j ≤ k · i , j =⇒ ci , c j A d-path is a simple path in the dependency graph where each edge is labeled with d. A d-path can be ﬁlled with a worm destined for d. The head of the path, denoted hd(πd ), contains the header ﬂit. A channel can get ﬁve possible markings: 0 A channel is unmarked, i.e., it is unvisited; 1 A channel is under investigation, i.e., not all neighbors have been marked; 2 All neighbors have been marked, the channel is deadlockimmune; 3 All neighbors have been marked, the channel is deadlocksensitive; 4 All neighbors have been marked, the channel is deadlockattainable. The algorithm marks sinks and acyclic channels with 2. Such channels cannot be in deadlock. A channel for which there exists a destination d such that all neighbors are not marked 2 is marked 3. The channel can be in deadlock if it is ﬁlled with a header ﬂit with destination d. A channel that cannot be marked 3, i.e., for which for all destinations there exists a 2-marked neighbor, but for which there exists a d-path to a 3-marked neighbor h is marked 4. The channel can be in deadlock if it is ﬁlled with a tail ﬂit of a worm whose header ﬂit is channel h and which is destined for d. Let us consider step 1 (Algorithm 1), named CreateTree. Let CI be a set of unmarked channels. The algorithm keeps expanding new neighbors until no unmarked neighbors exist. The current channel under investigation is c0 . The algorithm keeps track of the parent of c0 in parameter f . This enables the backwards propagation. For each channel, the algorithm stores the deadlock-immune destinations in array imm and the deadlock-sensitive destinations in array sens. If c0 is either a sink or deadlock-immune (line 6), then all destinations ∆( f , c0 ) are deadlock-immune for f . Thus these destinations are added to imm( f ). If c0 is marked otherwise (line 3) three cases arise: either c0 has already been shown to be deadlock-sensitive, or it has been shown to be deadlock-attainable, or it is unknown at this point how c0 is to be marked. In all cases, all destinations ∆( f , c0 ) are considered deadlock-sensitive for channel f and these are added to sens( f ). If c0 is neither a sink or marked, the algorithm continues its forwards expansion by expanding the neighbors of c0 (line 12). When this terminates, the gathered information is propagated backwards through the graph as follows: if there Algorithm 1 CreateTree(CI , f ) CreateTree(CI − c0 , f ) CreateTree(CI − c0 , f ) Require: CI = {c0 , c1 . . . ck }, with k ≥ 0 AND CI ⊆ Edep ( f ) 1: if CI = ∅ then 2: return 3: else if marks(c0 ) ∈ {1, 3, 4} then 4: sens( f ) ≔ sens( f ) ∪ ∆( f , c0 ) 5: 6: else if marks(c0 ) = 2 ∨ Edep (c0 ) = ∅ then 7: marks(c0 ) = 2 8: imm( f ) ≔ imm( f ) ∪ ∆( f , c0 ) 9: 10: else 11: marks(c0 ) = 1 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: end if if sens(c0 ) * imm(c0 ) then sens( f ) ≔ sens( f ) ∪ ∆( f , c0 ) marks(c0 ) = 3 else if sens(p0 ) , ∅ then sens( f ) ≔ sens( f ) ∪ ∆( f , c0 ) marks(c0 ) = 4 else imm( f ) ≔ imm( f ) ∪ ∆( f , c0 ) marks(c0 ) = 2 end if CreateTree(Edep (c0 ), c0 ) CreateTree(CI − c0 , f ) exists a destination in sens(c0 ) that is not in imm(c0 ), channel c0 is deadlock-sensitive. Thus ∆( f , c0 ) is added to sens( f ) and c0 is marked with 3 (lines 14-15). If the set sens(c0 ) is non-empty, then there is at least one neighbor that is not deadlock-immune. As all destinations lead to a deadlockimmune neighbor, but at least one destination has a path to a neighbor that is not deadlock-immune, channel c0 is marked 4 (lines 17-18). If all destinations in sens(c0 ) are included in imm(c0 ), and if all neighbors are deadlock-immune, channel c0 is deadlock-immune and ∆( f , c0 ) is added to imm( f ). Channel c0 is marked with 2 (lines 20-21). As for the post-processing step (Algorithm 2), this step initially considers all 3- and 4-marked channels with 2-marked neighbors. For all 3-marked channels it adds the destinations leading to 2-marked neighbors to the imm array, which have not been added already (line 4). If, as a result of this, the deadlock-sensitive destinations become a subset of the deadlock-immune destinations, then the channel no longer must be marked 3. If there exists a d-path from the current channel leading to a 3-marked channel then the channel is marked 4 (line 6). Two subtleties arise: ﬁrst, the destination d for which the path is supplied must be deadlock-immune for the head of the path. Only in this case the path can be ﬁlled with a worm destined for a deadlock-sensitive destination. Second, the path must at least be of length two. As the current channel is marked 3, trivially there is a path of length one. However, the algorithm has just concluded that the 3-mark is to be changed, thus this path is not valid. If the marking of a channel changes from 3 to 4, then all 4-marked parents must be reconsidered (line 8). Similarly, if the marking of a channel changes from 3 to 2, then all 4- and 2-marked parents must be reconsidered (line 11). If the post-processing considers a 4-marked channel, then it checks whether there still exists a d-path to some 3-marked channel (line 15). If not, it is marked 2 and reconsiders all 4-marked parents (lines 16-17). Algorithm 2 Post-Processing (CI ) Require: CI = {c0 , c1 . . . ck }, with k ≥ 0 1: if C = ∅ then 2: return 3: else if marks(c0 ) = 3 then 4: imm(c0 ) ≔ imm(c0 ) ∪ {d ∈ ∆(c0 , c1 ) | marks(c1 ) = 2} 5: if sens(c0 ) ⊆ imm(c0 ) then ﬁrst(πd ) = c0 marks(hd(πd )) = 3 ∧ d ∈ sens(hd(πd )) if ∃πd · then 6: ∧ ∧    |πd | > 1 7: marks(c0 ) = 4 8: CI ≔ CI ∪ {c ∈ parents(c0 ) | marks(c) = 4} 9: else 10: marks(c0 ) = 2 11: CI ≔ CI ∪ {c ∈ parents(c0 ) | marks(c) ∈ {3, 4}} 12: end if 13: end if 14: else if marks(c0 ) = 4 then ﬁrst(πd ) = c0 marks(hd(πd )) = 3 ∧ d ∈ sens(hd(πd )) if ∃πd · then 15: ∧ ∧    |πd | > 1 16: marks(c0 ) = 2 17: CI ≔ CI ∪ {c ∈ parents(c0 ) | marks(c) = 3} 18: end if 19: end if 20: Post-Processing (CI − c0 ) Algorithm 3 Main Require: C = {c0 , c1 . . . ck }, with k > 0 1: for i = 0 to k do 2: if marks(ci ) = 0 then 3: 4: end if 5: end for CreateTree(Edep (ci ), ci ) 6: Post-Processing ({c ∈ C | marks(c) ∈ {3, 4} ∧ ∃c′ ∈ Edep (c) · marks(c′ ) = 2}) 7: return {c ∈ C | marks(c) ∈ {3, 4}} = ∅ Main wraps up the two steps. It executes CreateTree for all unmarked channels c, with CI = {Edep (c)} and f = c. After this, it executes Post-Processing . It returns true if there exists 3- or 4-marked channels. In this proof, we create a set of paths without an escape. We do not – and cannot – show that this set of paths is pairwise disjoint. Our algorithm returns true if and only if the network is semi-deadlock-free. 4. ANALYSIS 4.1 Running time Let N be the number of processing nodes in the network. The algorithm consists of two steps. The ﬁrst step, explained in the trace in Section 3, is basically a depth-ﬁrst search with backwards propagation with O(N) recursions. The time complexity of one recursive call is O(N) in the worst-case, as in order to determine whether a channel must be marked deadlock-attainable the algorithm has to traverse the dependency graph once in search for a deadlock-sensitive channel. Thus the time complexity of the ﬁrst step is O(N2 ). During the second step, a check is performed for each deadlock-sensitive channel whether the channel is marked correctly. If this is not the case, then the marking is changed, invalidating in the worst-case O(N) other channels. The number of recursive steps of the second step is O(N2 ). Similarly to the ﬁrst step, the complexity of one recursive call is O(N), resulting in a total running time of O(N3 ). 4.2 Correctness We formally proved termination and correctness of the algorithm using the ACL2 interactive theorem proving system [12]. We only give some insight in the correctness proof. Our algorithm is correct if it returns true if and only if Theorem 2 holds. First, we prove suﬃciency, i.e., if our algorithm returns true then the network is deadlock-free. Theorem 3. The network is deadlock-free, if after termination of the algorithm all channels are marked deadlock-immune. Proof. By Theorem 2, a deadlock implies there exists a set of paths without an escape. We call such a set of paths a deadlock-subgraph. We prove an invariant: if a channel is marked deadlock-immune it cannot be member of any deadlock-subgraph. This invariant implies that if after termination of the algorithm all channels are deadlock-immune, there cannot exist a non-empty deadlock-subgraph, i.e., the network is deadlock-free. The proof shows that a channel can only be marked deadlockimmune if it is not part of any possible deadlock-conﬁguration. If all channels are marked deadlock-immune, there exists no deadlock-conﬁguration. We now prove necessity, i.e., if our algorithm returns false then there exists a deadlock-conﬁguration. Theorem 4. The network has a deadlock-conﬁguration, if after termination of the algorithm not all channels are marked deadlockimmune. Proof. This proof completely follows the intuition of Figure 5g: after termination of the algorithm, a deadlock can be created from all deadlock-sensitive and deadlock-attainable channels. By Theorem 2, a deadlock is a set of paths without an escape. We build the deadlock-subgraph from the deadlocksensitive and deadlock-attainable channels. 5. EXPERIMENTAL RESULTS This Section provides experimental results based on an implementation of our algorithm in C. The ACL2 description can be considered a speciﬁcation of this algorithm. The implementation closely follows this speciﬁcation. In particular, it uses the same data structures to manipulate the dependency graph. Figure 6a shows the results. All experiments have been performed on a 2.93 GHz Intel Core 2 Duo computer, with 2 GB memory. We have deﬁned four diﬀerent topologies, with six diﬀerent routing functions. 2D Mesh – XY [15] XY routing is a deterministic routing function for two-dimensional meshes used in, e.g., the HERMES chip [14]. Messages are routed ﬁrst along the X-axis and then along the Y-axis. It is deadlock-free, as there are no cyclic dependencies. We have veriﬁed this for a mesh with 4225 nodes and 16900 channels in 0.31 seconds. 2D Mesh – West-First [9] West-ﬁrst routing is an adaptive routing function for twodimensional meshes. It is based on the turn-model, which states that a routing function is deadlock-free as long as there are not enough turns allowed to create a cycle. Our algorithm needed 1.49 seconds for a 65 by 65 mesh with 16900 channels. 2D Mesh – Shortest Path [18] Shortest path routing is an adaptive routing function, which routes messages along the shortest path. It is not deadlockfree. Our algorithm returns a deadlock in 0.87 seconds for a 65 by 65 mesh with 16900 channels. Double 2D Mesh – Shortest path with XY [18] There are two channels in each direction between the processing nodes. One of the channels is used for the adaptive shortest path routing function. The other channel is used for the deterministic XY routing function. The network is deadlock-free, even though there are cyclic dependencies. Our algorithm returns true in 110,07 seconds for a 45 by 45 mesh with 16200 channels. Spidergon – Shortest path [3, 4] Spidergon STNoC is an architecture developed by STMicroelectronics [3, 4]. The Spidergon topology is a ring where each node has a channel going clockwise, counter-clockwise, and across. Its shortest path routing function is deterministic. It is not deadlock-free2. We checked this for the Spidergon chip with eight processing nodes, i.e., the Octagon chip [11]. Double ring – AAD We have designed a new adaptive routing function for Spidergon. There are two channels in the counter- and clockwisedirection between the processing nodes on the ring. AAD routing – for Adaptive-Across-Deterministic – routes a message ﬁrst adaptively in any direction. Once it has taken an across-channel, it is deterministically routed towards its destination. Our algorithm proves that it is deadlock-free in 65,12 seconds for a network with 2048 nodes and 12288 channels. As Figure 6a shows, our algorithm performs signiﬁcantly better on XY and West-First routing than on ""Shortest Path 2The ST implementation actually is deadlock-free thanks to extra virtual channels used to break cycles. 100 10 Time (s) 1 (cid:4) (cid:3) (cid:3) ∗ × + (cid:4) 0.1 (cid:3) (cid:4) × ∗ 0.01 0 (cid:4) (cid:3) 2D-XY 2D-WF 2D-SP 2D-XY+SP RING-AAD + × ∗ (cid:3) (cid:4) × ∗ + × ∗ + (cid:3) × ∗ + (cid:4) (cid:3) × ∗ + 2000 1500 1000 500 Taktak et al. Verbeek and Schmaltz + × + 5000 10000 15000 20000 +++ ××× 0 0 + 5000 + × × 10000 × × 15000 Number of channels (a) Benchmark of our algorithm Number of channels (b) Comparison to Taktak et al. Figure 6: Experimental results with XY"" and AAD routing. This is due to the fact that the ﬁrst two routing functions are deadlock-free because there are no cyclic dependencies. In such cases, our algorithm performs exactly like a regular cycle detection algorithm and terminates in linear time. 6. RELATED WORK AND DISCUSSION To the best of our knowledge, there exists only one previous algorithm for proving routing functions deadlock-free for wormhole networks [18, 17]. This algorithm is based on a channel tagging that inspired our own marking. Taktak’s algorithm ﬁrst extracts strongly connected components of the dependency graph and tries to tag channels according to a deadlock-free condition. A network is deadlock-free if all channels have been tagged. The time complexity of their procedure is in O(N4 ). Figure 6b shows a comparison with our solution for the ""Shortest Path with XY” routing functions. This Figure clearly illustrates the empirical observation that a complexity in O(N3 ) or less is considered ""tractable"" whereas a complexity in O(N4 ) or higher is not. In addition of being faster our algorithm has also been formally veriﬁed. This has played a signiﬁcant part in getting all subtleties of the algorithm exactly right. As an example, consider the criterion on which a channel is marked deadlockattainable. A path to a deadlock-sensitive channel seems suﬃcient, but it is not. First, all channels in the path must be supplied for some destination d. Otherwise, the path cannot be ﬁlled with a worm. Secondly, at the head of the path, destination d must lead to deadlock-sensitive neighbors only. Due to the complexity of wormhole switching, the algorithm contains many of these subtleties. We are convinced that we would not have obtained a correct algorithm without formal veriﬁcation. Theorem 2 is a suﬃcient condition for deadlock-free routing. Our algorithm is a semi-decision procedure. If it returns ""no deadlock"", the routing function is surely deadlock-free. In contrast, it It might output deadlocks which do not satisfy condition 3 in Deﬁnition 1. This happens only if the network is deadlock-free, but there exists a deadlock-conﬁguration where tails of worms intersect. To the best of our knowledge there are currently no real-life examples of such networks. A purely academic example is provided in [20]. The algorithm outputs the exact semi-deadlock-conﬁguration, i.e., which channels contain header ﬂits and which contain tail ﬂits. This output can be used for further analysis. Our condition is nevertheless logically equivalent to that of Duato [6] and its variations, in particular the condition of Taktak [18]. The issue is that these conditions omit property (3) of Deﬁnition 1. 7. CONCLUSION AND FUTURE WORK Designing adaptive routing functions for wormhole networks is a diﬃcult task. It involves the proof that a routing function satisﬁes a complex condition for deadlock-free routing. We have presented a simple condition equivalent to Duato’s seminal one. We provided an algorithm to automatically check this condition. The algorithm has a time complexity in O(N3 ), where N denotes the number of nodes in the network. Wormhole switching is a complex technique with many subtleties. The correctness of our condition and algorithm has been formally checked using the ACL2 interactive theorem proving system. The proof eﬀort involves about 88 deﬁnitions and 632 theorems for a total of 7263 lines of code. We implemented our algorithm in C and experimental results conﬁrm the theoretical complexity and the high eﬃciency of our algorithm. Complex routing functions for networks with about 4000 nodes and 15000 channels can be analyzed within 100 seconds. Our C implementation follows closely the ACL2 speciﬁcation but they are both diﬀerent and the C code has not been formally veriﬁed. The ACL2 system provides many features to support the eﬃcient execution of programs. A possible extension of our work would use these features to develop a veriﬁed implementation. Running this algorithm would then constitute a formal proof of deadlock-free routing. As discussed earlier, our condition is suﬃcient and the algorithm is therefore a semi-decision procedure for deadlock veriﬁcation. Checking a necessary and suﬃcient condition is co-NP-complete [20].Our algorithm checks the strongest condition that is checkable in polynomial time. Future research is focusing on using SAT solvers to complete our decision procedure. Finally, the most signiﬁcant and actual application of our fast algorithm is probably the veriﬁcation of fault-tolerant routing functions [18]. Such a function should be deadlockpacket-switching networks on chip. Integration, the VLSI Journal, 38(1):69 – 93, 2004. [15] L. Ni and P. McKinley. A survey of wormhole routing techniques in direct networks. IEEE Computer, 26:62–76, 1993. [16] L. Schwiebert and D. N. Jayasimha. A universal proof technique for deadlock-free routing in interconnection networks. In In 7th Annual ACM Symposium on Parallel Algorithms and Architectures, pages 175–184, 1995. [17] S. Taktak, J.-L. Desbarbieux, and E. Encrenaz. A tool for automatic detection of deadlock in wormhole networks on chip. ACM Transactions on Design Automation of Electronic Systems, 13(1), January 2008. [18] S. Taktak, E. Encrenaz, and J.-L. Desbarbieux. A polynomial algorithm to prove deadlock-freeness of wormhole networks. In 18th Euromicro International Conference on Parallel, Distributed and Network-Based Computing (PDP’10), February 2010. [19] F. Verbeek. A fast and veriﬁed algorithm for proving store-and-forward networks deadlock-free. In Proceedings of the 19th International Euromicro Conference on Parallel, Distributed and Network-based Processing (PDP), 2011. [20] F. Verbeek and J. Schmaltz. A comment on ""a necessary and suﬃcient condition for deadlock-free adaptive routing in wormhole networks"". IEEE Transactions on Parallel and Distributed Systems, 99(PrePrints), 2011. [21] F. Verbeek and J. Schmaltz. On necessary and suﬃcient conditions for deadlock-free routing in wormhole networks. IEEE Transactions on Parallel and Distributed Systems, 99(PrePrints), 2011. free for all possible faults it supports, e.g., a number of defective routers. An interesting future work would study how - given a routing function - our algorithm performs in the deadlock analysis for diﬀerent positions of faulty routers. Acknowledgments We would like to thank the anonymous reviewers for their apposite, constructive, and detailed comments. We would like to thank Sami Taktak and Emmanuelle Encrenaz for providing us access to the source code of their implementation and their benchmarks. This research is supported by NWO/EW project Formal Validation of Deadlock Avoidance Mechanisms (FVDAM) under grant no. 612.064.811. 8. "
DART - A programmable architecture for NoC simulation on FPGAs.,"The increased demand for on-chip communication bandwidth as a result of the multi-core trend has made networks on-chip (NoCs) a compelling choice for the communication backbone in next-generation systems [3]. However, NoC designs have many power, area, and performance trade-offs in topology, buffer sizes, routing algorithms and flow control mechanisms---hence the study of new NoC designs can be very time-intensive. To address this challenge we propose DART, a fast and flexible FPGA-based NoC simulation architecture. Rather than laying the NoC out in hardware on the FPGA like previous approaches [8, 6], our design virtualizes the NoC by mapping its components to a generic NoC simulation engine, composed of a fully-connected collection of fundamental components (e.g., routers and flit queues). This approach has two main advantages: (i) since FPGA implementation is decoupled it can simulate any NoC; and (ii) any NoC can be mapped to the engine without resynthe-sizing it, which can take time for a large FPGA design. We demonstrate that an implementation of DART can achieve over 100x speedup relative to a cycle-based software simulator, while maintaining the same level of simulation accuracy.","DART: A Programmable Architecture for NoC Simulation on FPGAs Danyao Wang, Natalie Enright Jerger, J. Gregory Steffan Depar tment of Electrical and Computer Engineering University of Toronto Toronto, ON, Canada {wangda,enright,steffan}@eecg.toronto.edu ABSTRACT The increased demand for on-chip communication bandwidth as a result of the multi-core trend has made networks on-chip (NoCs) a compelling choice for the communication backbone in next-generation systems [3]. However, NoC designs have many power, area, and performance trade-oﬀs in topology, buﬀer sizes, routing algorithms and ﬂow control mechanisms—hence the study of new NoC designs can be very time-intensive. To address this challenge we propose DART, a fast and ﬂexible FPGA-based NoC simulation architecture. Rather than laying the NoC out in hardware on the FPGA like previous approaches [8, 6], our design virtualizes the NoC by mapping its components to a generic NoC simulation engine, composed of a fully-connected collection of fundamental components (e.g., routers and ﬂit queues). This approach has two main advantages: (i) since FPGA implementation is decoupled it can simulate any NoC; and (ii) any NoC can be mapped to the engine without resynthesizing it, which can take time for a large FPGA design. We demonstrate that an implementation of DART can achieve over 100× speedup relative to a cycle-based software simulator, while maintaining the same level of simulation accuracy. Categories and Subject Descriptors C.1.2 [Computer Systems Organization]: Multiprocessors; Interconnection Architectures General Terms Design, Performance, Measurement Keywords Network-on-chip, simulation, FPGA 1. INTRODUCTION Modern multi-cores and systems-on-chip increasingly use packet-switched networks-on-chip (NoCs) to meet the growing demand for on-chip communication bandwidth, as more cores are incorporated into each chip. NoC designs are sensitive to many parameters such as topology, buﬀer sizes, routPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS ’11, May 1-4, 2011 Pittsburgh, PA, USA Copyright 2011 ACM 978-1-4503-0720-8 ...$10.00. ing algorithms, and ﬂow control mechanisms. Detailed NoC simulation is essential to accurate full-system evaluation. Software simulation is used widely, both as stand-alone NoC simulators [15, 4] and as the interconnect component of large full-system simulators [7, 1]. These tools have the advantages of being very ﬂexible, easy to program, fast to compile, and deterministic (making them amenable to debugging). However, simulation of large NoCs in software is slow, which adds to the already burdensome computation required to perform detailed full-system simulation. To maintain reasonable simulation times, the user is often forced to reduce simulation detail. The increased on-chip logic and memory capacities of recent FPGAs allow an entire on-chip system to be implemented on a single device. FPGA-based NoC emulators [6, 16, 20, 8] can reduce simulation time by several orders of magnitude compared to software. These dramatic speedups are possible because the emulator is constructed by laying out the entire NoC on the FPGA, allowing the hardware to exploit all available ﬁne and coarse grain parallelism between the emulated events in the NoC. However, this directmapped approach has three key drawbacks relative to software simulation: (i) any change in the simulated NoC requires manual redesign of the emulator HDL, (ii) redesign in turn requires complete compilation/synthesis of the FPGA design, which can take hours, or up to a day for a large design, and (iii) the maximum simulatable NoC size is determined by the FPGA capacity. 1.1 A Flexible NoC Simulation Acceleration Engine We ﬁrst proposed the DART architecture for FPGA-based NoC simulation to bridge the gap between pure software and hardware approaches [19]. The DART architecture is parametrized, and the parameters can be set by software at run-time to simulate diﬀerent NoCs without modifying the hardware simulator on the FPGA. In this paper, we describe in more detail the design decisions of DART and investigate the performance and scalability of the DART architecture. Fig. 1 shows the organization of a DART simulator, which consists of a fully-connected collection of ﬁxed-function components that model the building blocks of an NoC: traﬃc generators, routers and queues. Conﬁgurable parameters within each node allows behaviors of individual nodes to be altered to match nodes in the simulated NoC. The global interconnect provides all-to-all communication between DART nodes, thus allowing simulation of diﬀerent topologies without resynthesizing the design. Furthermore, simulated cycles are decoupled from FPGA cycles through the use of a global time counter: this counter is incremented once evcommunication overhead. DARSIM [9] is a parallel NoC simulator that achieves good scalability for up to 4 threads in cycle-accurate mode, which requires two global synchronizations per simulated cycle. Relaxing this constraint allows good scaling to 8 threads at the cost of lower accuracy. 2.2 FPGA-based Emulation Genko et al. [6] describe an emulation platform that consists of programmable traﬃc generators and receptors that drive a 6-switch NoC and is 2600× faster than a SystemC simulation of the same network. While this platform supports programmable traﬃc patterns and statistics counters, changing the router conﬁguration requires resynthesis of the emulator. DRNoC [8] circumvents this requirement by leveraging the partial reconﬁgurability of Xilinx FPGAs. The DRNoC host FPGA is divided into grids; each grid slot can be dynamically reconﬁgured to implement a diﬀerent router model. However, partial reconﬁguration requires a special design ﬂow and incurs area overheads; it is also only available for select devices. In contrast, DART’s conﬁguration interface is based on a generic shift register and is portable to any FPGA. NoCem [16] improves emulation density over Genko et al.’s design [6] and implements a 9-node mesh network on a single FPGA by eliding the router pipeline details and virtual channels. Instead of sacriﬁcing these important details, we employ a simple design for each DART Router: each has multiple input ports but only one output port, and models the all-to-all switching in a simulated router by routing one input port per DART cycle. Wolkotte et al. [20] virtualize a single router on an FPGA, allowing the simulation of an NoC with multiple routers. An oﬀ-chip ARM processor stores N contexts for the router model and orchestrates the emulation of the N -node network. This approach allows the router model to be much more detailed. However, the oﬀ-chip ARM/FPGA communication link is a performance bottleneck. DART’s simulation components are implemented completely on-chip and DART does not suﬀer from this bottleneck. AcENoCs [11] provides a novel HW/SW design for NoC emulation. It leverages a soft-processor for traﬃc generation and source queue functionality, freeing FPGA resources to allow for more detailed routers, and enabling a network size comparable to DART. DART is similar in spirit to the RAMP Gold [18] and ProtoFLEX [2] processor simulators that decouple the simulator architecture on the FPGA from that of the simulated system. Both RAMP Gold and ProtoFlex use host multi-threading to simulate large-scale multi-cores on top of a single processor pipeline. Compared to a processor, nodes in an NoC have simpler pipelines but carry out more ﬁnegrained communication. To eﬃciently model these characteristics, DART uses multiple functional components that are synchronized to exploit the ﬁne-grained parallelism in NoC simulation. A-Ports [13] separates the timing and functional models of a processor simulator using components that communicate asynchronously. DART uses a globally synchronous approach to avoid the need of large buﬀers in intermediate nodes. To overcome FPGA size constraints, HAsim [14] uses a novel time-division multiplexing scheme to simulate a larger number of cores and large NoC design. 3. DART ARCHITECTURE The basis of the DART architecture is to provide programmability by decoupling (i) the simulator architecture Figure 1: DART Simulator architecture on the FPGA ery simulated-cycle after all network transfers for that cycle are simulated (which may take a variable number of FPGA cycles). Virtualizing simulation time allows us to optimize the DART components for area eﬃciency. This partially addresses the limitation on simulatable NoC size due to FPGA capacity constraints, allowing us to easily add support for more simulated nodes than physical DART nodes to future versions of DART. Contributions We make two main contributions: First, we design and evaluate a novel overlay architecture that enables software run-time conﬁgurable NoC simulation on FPGAs. The novelty of DART lies not in the NoC features implemented but in its approach to simulation: fast and ﬂexible FPGA simulation without resynthesis. Second, we demonstrate a complete implementation of the DART simulation toolset that achieves over 100× speedup relative to the cyclebased software simulator Booksim[4], while maintaining the same level of simulation accuracy. 2. RELATED WORK 2.1 Software Simulation Cycle-accurate software NoC simulators incorporate detailed models of network components, and evaluate network performance by capturing the timing of messages as they traverse the simulated network. Software simulators exist both as stand-alone interconnect simulators such as Booksim [4] and SICOSYS [15], and as the network component in full-system simulators such as Garnet [1] of GEMS [10] and SimFlex [7]. However, detailed full-system simulation is slow. Recent work including ProtoFLEX [2] and RAMP Gold [17] speed up processor simulation using FPGAs. The NoC component can become the bottleneck in the simulation of large systems. As multi-core processors become widely available, one way to improve software simulation speed is to leverage multithreading. The main challenge in parallelizing NoC simulators using threads is that NoC simulation is communication intensive. The ﬁne-grained synchronizations can incur high from the architecture of the simulated NoC, and (ii) DART cycles from simulated cycles. To provide a conﬁgurable functional model for NoC simulation, we abstract common NoC functionalities into three basic components: Traﬃc Generators (TGs), Flit Queues (FQs) and Routers. Components can be mixed and matched to model more complex NoC nodes. A given topology is then simulated by conﬁguring the Routers to only forward to their simulated neighbors via the global interconnect; this conﬁguration does not require resynthesis. Traﬃc Model A typical NoC carries two types of trafﬁc:ﬂits (ﬂow control units) that carry data messages and credits that are exchanged between neighboring routers to enforce ﬂow control [4]. DART models ﬂits and credits using descriptors that contain only the information necessary to forward them from source to destination. A 36-bit ﬂit descriptor encodes the injection and next-transfer timestamps, source and destination addresses, and boolean ﬂags for the ﬂit type (head, tail, and warmup). Warmup ﬂits are used to bring the network to steady state and hence do not have their latencies recorded. Not encoding the data payload saves area as fewer bits are stored and passed between DART nodes. We choose 36 bits to match the port width of embedded RAM blocks on the FPGA, which are used to implement the ﬂit buﬀers. Anything wider doubles the RAM usage as two RAM blocks must be used in parallel to support the data width. A credit descriptor encodes only a timestamp and a virtual channel ID. Timing Model To capture the timing of ﬂit transfers, we use a global time counter to synchronize all network events. Each ﬂit contains a timestamp that indicates when the next transfer of this ﬂit should happen. As a ﬂit traverses the network, its timestamp is updated by intermediate DART nodes to reﬂect the delay due to pipeline latency and simulated contention. Credit transfers are timed similarly. Upon arrival at the destination TG, a ﬂit’s latency is computed by subtracting the injection timestamp from the arrival timestamp. The 10-bit timestamps allow DART to correctly compute latency provided a ﬂit’s latency does not exceed 1024 simulated cycles. We believe this is a reasonable compromise to keep the ﬂit descriptors within 36 bits as most on-chip communication takes no more than a few hundred cycles. However, the maximum simulation length DART supports is not limited to 1024 cycles. By using signed subtractions to compare timestamps, we can correctly determine the chronological order of timestamps within 512 simulated cycles even when the global time counter wraps around. Since the DART design guarantees that timestamps of all ﬂits traversing the global interconnect fall within a N cycle window, where N is the simulated latency of the router pipeline and is smaller than 512, ﬂits will always be delivered in correct simulation order. Design Space Coverage The bit widths of the other descriptor ﬁelds are also chosen to be minimum size while still providing suﬃcient functionality coverage. The 8-bit node addresses, 3-bit port ID and 2-bit virtual channel (VC) ID allow DART to scale to 256 nodes, 8 ports per node and 4 virtual channels per port. Conﬁgurations that ﬁt within these ﬂit widths can be setup in software at run-time. These widths do not fundamentally limit the size a NoC that DART can simulate; larger sizes can be accommodated through re-synthesis with only minor HDL changes. Figure 2: Data path of canonical wormhole VC router 3.1 Flit Queue (FQ) The Flit Queue component models the VC buﬀers at a router’s input ports and the bandwidth/latency constraints of the link feeding the port. The buﬀers are independent FIFO (ﬁrst-in-ﬁrst-out) queues that are implemented by statically partitioning a single block-RAM among the VCs. A Verilog parameter controls the number of VCs to incorporate. Each incoming ﬂit is queued according to its VC after its timestamp is updated to reﬂect the delay it experiences traversing the link due to latency and bandwidth constraints – both parameters are conﬁgurable per FQ. A ﬂit is forwarded to the next-hop Router when it gets to the front of the FIFO and the global simulation time is equal to its timestamp. This ensures all ﬂits arrive at a Router in chronological order, which is required for correct simulation of resource contention. A separate FIFO is used for the credit channel. Similar to the ﬂits, a credit can leave an FQ only during its scheduled dequeue time. 3.2 Trafﬁc Generator (TG) When enabled, the Traﬃc Generator component injects traﬃc in one of two modes: synthetic or dynamic. The former is useful for stress testing the simulated network. The latter provides an interface to incorporate DART into a fullsystem simulator. The mode is conﬁgurable per TG. In synthetic mode, a TG injects ﬂits in bursts of ﬁxed-sized packets using a Bernoulli process. Packet size (minimum 2 ﬂits), destination node address, and injection interval are conﬁgurable per TG. In dynamic mode, a TG receives packet descriptors from the host PC and injects packets according to the descriptors. Packet size can be varied from 2 to 256 ﬂits in powers of 2. Packet descriptors can be generated from either a memory access trace or a processor simulator running concurrently with DART. In addition to the injection state machines, each TG also contains two FQs: the input buﬀer models the last-hop delay to the TG, and the output buﬀer models the source queue. We use the same technique from Dally and Towles [4] and allow the injection state machine to lag behind the current simulation time when the output buﬀer is full, to model an inﬁnite source queue. TGs also serve as traﬃc sinks and record the number of packets received and the cumulative packet latency. More statistics counters can be easily added. 3.3 Router State-of-the-art NoCs use the classic wormhole VC router (Fig. 2), which is composed of per-VC ﬂit buﬀers, routing logic, VC and switch allocators and a crossbar. Since the FQs model the ﬂit buﬀers, the Router component only encapsulates the routing and allocation logic. Fig. 3 shows the Router datapath. A Verilog parameter controls the number We choose this organization over a full crossbar to conserve area. A separate, narrower, but otherwise identical interconnect carries the credit traﬃc. Both intra- and inter-partition arbitration use round-robin arbiters, with priority given to ﬂits with timestamps equal to the current simulation time. These ﬂits must be forwarded ﬁrst before ticking the global time counter to prevent late ﬂits, which may cause out-oforder ﬂits at the next Router. Flits with timestamp ahead of the current simulation time can be forwarded out-of-order across the global interconnect because ﬂits destined for each FQ remain in order. The priority is implemented by having two separate sets of arbiters. Because it takes a cycle to detect that all ﬂits with the current timestamp have crossed the interconnect, each simulated cycle takes at least 2 DART cycles. The partitions are the throughput bottleneck because only one ﬂit can be sent and received by a partition per DART cycle. For a ﬁxed number of DART nodes, varying the size of the partition trades oﬀ the global interconnect throughput for implementation area. For our current 9-node implementation, we use 8 partitions connected by an 8 × 8 crossbar. In general, the largest crossbar that ﬁts in the device once the nodes are implemented should be chosen. 3.5 Conﬁguration and Data Collection DART nodes are conﬁgured by connecting the conﬁgurable ﬁelds in a 16-bit shift register chain. A small ﬁnite state machine for each routing table is also connected to the chain. It captures the data that is shifted in and populates the routing table. The conﬁguration byte-stream is generated on a host PC and sent to the FPGA via the serial port. We use a custom 16-bit command protocol, which can be implemented on any physical interface. Similarly, performance counters are read back by shifting them through a 16-bit-wide chain. Currently three counters are incorporated per TG to record the number of injected and received packets (32 bits) and the cumulative packet latency (64 bits). More counters can be easily added to this shift register chain. 3.6 Software Tools The DART software tools run on a host PC connected to the FPGA where the hardware simulator resides. They allow the dynamic reprogramming of the hardware simulator after it is implemented on the FPGA. The DARTgen tool creates the conﬁguration byte-stream from two input ﬁles that specify the on-chip DART architecture and the user network to be simulated respectively. Nodes and links in the user network are mapped to Routers and Flit Queues. We use a round-robin scheme to balance the number of used DART nodes across diﬀerent partitions. This provides sufﬁcient load balancing because the on-chip communication bottleneck is within each partition, where the nodes contend for the shared access to the inter-partition crossbar. Each DART node is annotated with the properties of the corresponding user network node. The conﬁguration bytestream is generated by writing the contents of the conﬁguration registers to a ﬁle. It is used by the DARTportal tool, which provides a command-based interactive interface to conﬁgure, run and collect data from the simulator. 4. IMPLEMENTATION We design the DART components in Verilog HDL. Device speciﬁc constructs are avoided whenever possible so the simulator core can be implemented on diﬀerent FPGA systems with minimal changes. To demonstrate the functionality of Figure 3: DART Router datapath Figure 4: DART’s global interconnect. Nodes are grouped into partitions so the crossbar needed is small of ports. Table-based routing is used to allow diﬀerent routing algorithms and facilitate the simulation of a wide range of topologies. The table contents are conﬁgurable after synthesis by a host PC. A 4-bit counter for each output VC is used to implement credit-based ﬂow control. Initial credit values represent the number of entries in the input buﬀer at the downstream router. The counter is decremented when a ﬂit is routed, and incremented when a credit is received. The values are conﬁgurable for each VC and Router. Area-Speed Trade-oﬀ The allocators and the crossbar in the classic router are complex structures [12]. The virtualized simulation time in DART allows us to implement the same functionality in the DART Router using simple arbiters and a multiplexer by trading oﬀ simulation speed. The Router component routes one ﬂit per DART cycle. By routing the input VCs one at a time while holding the global time counter, the DART router can model any multi-ported classic router. A round-robin scheme selects an input VC to route in each DART cycle. Delay due to failed VC allocation or credits is modeled by incrementing the timestamp of the ﬂit while it remains in the FQ. When a ﬂit is ﬁnally routed, its timestamp is incremented by a ﬁxed pipeline latency. This pipeline latency is conﬁgurable per Router. 3.4 Global Interconnect The global interconnect provides uniform-latency communication between all DART nodes. By conﬁguring the routing tables appropriately, DART can simulate any topology. The maximum node radix is limited by the number of ports conﬁgured in the Router components. Fig. 4 shows the interconnect organization, where nodes are grouped into partitions and the partitions are connected by a small crossbar. Table 1: Resource utilization breakdown of a 9-node DART on a XC2VP30 FPGA Per-Module Resource Util. % of Total Module 4-LUTs FFs BRAMs 4-LUTs Traﬃc Gen. Flit Queue Router Global Inter. Control Unit UART interface 691 305 612 2144 152 208 500 145 201 104 70 171 2 1 0.5 0 0 1 24.7% 43.5% 21.8% 8.5% 0.6% 0.8% Total % of Available 26,38 96% 13,192 48% 99 72% 100% Table 2: 3×3 mesh conﬁguration parameters Topology 3 × 3 mesh Link latency 1 ﬂit cycle Router architecture Input queue Routing algorithm Dimension-order (XY) # of VCs per port 2 VC Allocation Round-robin Input VC buﬀer size 5 Router pipeline latency 5 ﬂit cycle Traﬃc pattern Permutation traﬃc Packet size 2 ﬂits DART and to obtain real measurement of simulation speed, we implement a 9-node DART on a Xilinx University Program Virtex-II Pro Development System (XUPV2P) [21]. We use the Xilinx ISE 10.1 software suite for synthesis and implementation. Note that the global interconnect size and ﬂit descriptor size can be trivially extended to implement a larger DART system. Table 1 shows the resource breakdown of DART components as implemented on the XUPV2P platform. Because every two Routers share a dual-ported routing table to make eﬃcient use of the dual-ported block-RAM, each Router uses 0.5 block-RAMs on average. The maximum number of DART nodes that ﬁt on this FPGA is nine. Each node consists of one TG, one 5-ported Router, and four FQs with 2 VCs each. We use 8 partitions in the global interconnect. 5. ANALYSIS In this section we validate DART’s simulation results using Booksim as a reference. Because Booksim is widely used among NoC researchers, we hope this choice of baseline provides more conﬁdence in DART’s correctness and performance potential. We measure DART’s speedup over Booksim using our Virtex-II Pro implementation. We also investigate the performance cost of a programmable simulator architecture and DART’s scalability. 5.1 Correctness We developed a cycle-accurate DART architecture simulator in C++ prior to building the FPGA architecture to explore diﬀerent design options and to verify the correctness against Booksim. The architecture simulator also serves as the design speciﬁcation to verify the hardware design. Results shown in this section are obtained from this architecture simulator. We simulate a 9-node network and compare the average packet latency of Booksim and DART (Fig. 5). Table 2 shows the parameters of the simulated network. To investigate the accuracy loss DART incurs by not modeling the  0  10  20  30  40  50  60  0  0.1  0.2  0.3  0.4  0.5  0.6 A e v r e g a P e k c a t a L t y c n e t i l f ( s e c y c l ) Flit Injection Rate (flits / node / cycle) booksim booksim2 DART Figure 5: Average packet latency for Booksim and DART for a 3×3 mesh  0  10000  20000  30000  40000  50000  60000  70000  0  0.1  0.2  0.3  0.4  0.5  0.6 s k o o B i m d e e p S ( s e c y c l / s ) Flit Injection Rate (flits / node / cycle) (a)  0  50  100  150  200  0  0.1  0.2  0.3  0.4  0.5  0.6 d e e p S T R A D / s k o o B i m d e e p S Flit Injection Rate (flits / node / cycle) (b) Figure 7: DART performance: (a) Booksim simulation speed, (b) Speedup achieved by DART vs. Booksim delay through each stage separately, we simulate two router conﬁgurations in Booksim: booksim has a 5-cycle routing delay and zero switch and VC allocation delay, while booksim2 has a 4-cycle routing delay, 1-cycle switch allocation delay and zero VC allocation delay. We simulate 15,000 warmup cycles, 30,000 measurement cycles and a draining phase. The ﬂit injection rate is varied from 0.01 until saturation. DART tracks Booksim closely at low injection rates. At higher injection rates, the one-stage pipeline in the Router results in a less accurate latency measurement. This is evident in that DART latency is enveloped by the two Booksim conﬁgurations that have the same overall router latency but diﬀerent latencies at each stage. To further investigate the mismatch, Fig. 6 shows the distribution of packet latencies at 0.4 ﬂits per cycle. The peaks at 8, 14, 20, 26, and 32 correspond to the zero-load latencies for 0, 1, 2, 3, and 4-hop paths. The lower peaks reﬂect the queuing delay and resource contention packets experience at the routers. Booksim has a much longer tail than DART. Because all contentions (buﬀer, VC, and switch) are modeled in one stage in the DART Router, DART may under predict the latency for a ﬂit to acquire all resources. However, the similar overall shapes of the two distributions increases our conﬁdence that DART produces useful predictions of network performance trends. 5.2 Speedup vs. Software Simulation In Fig. 7 we evaluate the 3×3 mesh benchmark described in Table 2 on the XUPV2P DART implementation and com                        ) s t e k c a p f o # ( y c n e u q e r F  4500  4000  3500  3000  2500  2000  1500  1000  500  0 booksim1 DART <7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49>=50 Average Packet Latency (flit cycles) Figure 6: 3 × 3 Mesh. Packet latency distribution measured by Booksim and DART (ﬂit injection rate = 0.4) l e c y C d e t a u l m i S / s e c y l C T R A D  7  6  5  4  3  2  1  0 dedicated+5port global+5port dedicated+1port global+1port l e c y C d e t a u l m i S / s e c y l C T R A D dedicated+5port global+5port dedicated+1port global+1port  30  25  20  15  10  5  0  0.1  0.2  0.3  0.4  0.5  0.6  0  0  0.05  0.1  0.15  0.2  0.25 Flit Injection Rate (flits / node / cycle) Flit Injection Rate (flits / node / cycle) (a) (b) Figure 8: Overhead of the DART interconnect and simpliﬁed Router model for a (a) 9-node and (b) 64node DART. 3×3 and 8×8 mesh with random permutation traﬃc simulated pare the simulation speed to Booksim. For the Booksim baseline, we measure the execution time of the main loop, excluding network setup, on a 2.66 GHz Core 2 Quad Linux workstation. Each data point is an average over 20 runs. We measure DART’s execution time in DARTportal from                        0  10  20  30  40  50  60  70  0  5  10 15 20 25 30 35 40 45 s k o o B i m d e e p S ( s e c y c 0 0 0 1 l / s ) Aggregated Transfers (flit hops / cycle) 3x3 mesh 4x4 mesh 6x6 mesh 8x8 mesh (a)  5000  10000  15000  20000  25000  0  5  10 15 20 25 30 35 40 45 d e e p S T R A D ( 0 0 0 1 s e c y c l / s ) Aggregated Transfers (flit hops / cycle) 3x3 mesh 4x4 mesh 6x6 mesh 8x8 mesh (b) Figure 9: Booksim (a) and DART (b) simulation speed for various networks 0 1 4 2 5 3 6 7 (a) 0 1 2 3 4 5 6 7 8 (b) Figure 10: Two microbenchmarks: (a) 4×2 mesh with express links, and (b) 2-level tree and destination pairs and number of nodes. It measures the overall amount of in-ﬂight traﬃc that traverses the network every cycle. Booksim’s simulation speed depends on both the size of the simulated network and the amount of network activity because it must simulate every cycle including those when the simulated network is idle. This overhead dominates simulation time for large networks and the network activity becomes an insigniﬁcant factor for performance. DART’s simulated time advances faster when the simulated network is idle. Its simulation speed thus depends only on the amount of network activity. As a result, DART’s speedup over Booksim varies from 300× for the 3×3 mesh to 2000× for the 8×8 mesh. These estimates are higher than the measured speedup from Section 5.2 due to the overhead of sending commands to the FPGA. In long-running simulations, this overhead can be amortized. The design focus for DART is on improving area eﬃciency so more simulator nodes can be implemented on a given FPGA. 6. CASE STUDIES We choose two examples (Fig. 10) to demonstrate DART’s ability to simulate diﬀerent network conﬁgurations without resynthesis. The results presented in this section are simulated using a randomly generated permutation traﬃc pattern. All conﬁgurations are implemented on the same 9-node DART described in Section 4. 6.1 Mesh with Express Links Fig. 10a illustrates a simpliﬁed version of express cube [5]. The solid lines represent local links and the dashed lines represent express links that allow non-local traﬃc to bypass intermediate nodes. Fig. 11 shows the average packet latency for the following conﬁgurations: • NOEX BUF5: No express link, 5 ﬂits/VC input buﬀer • EX1 BUF5: With express links, 5 ﬂits/VC input buﬀer  10  15  20  25  30  35  40  0  0.1  0.2  0.3  0.4  0.5 A e v r e k c a P e g a t y c n e a L t t i l f ( s e c y c l ) Flit Injection Rate (flits / node / cycle) NOEX_BUF5 EX1_BUF5 EX1_BUF4 EX2_BUF4 (a)  50  60  70  80  90  100  0  0.1  0.2  0.3  0.4  0.5  0.6 A e v r e k c a P e g a t y c n e a L t t i l f ( s e c y c l ) Flit Injection Rate (flits / node / cycle) NOEX_BUF5 EX1_BUF5 EX1_BUF4 EX2_BUF4 (b) Figure 11: Express links performance: packets, and (b) 16-ﬂit packets (a) 2-ﬂit  27  28  29  30  31  32  33  34  0  0.05  0.1  0.15  0.2 A e v r e k c a P e g a t a L t y c n e t i l f ( s e c y c l ) Flit Injection Rate (flits / node / cycle) BUF5_BW1 BUF5_BW2 BUF10_BW1 BUF10_BW2 (a)  40  50  60  70  80  90  100  0  0.05  0.1  0.15  0.2  0.25  0.3 A e v r e k c a P e g a t a L t y c n e t i l f ( s e c y c l ) Flit Injection Rate (flits / node / cycle) BUF5_BW1 BUF5_BW2 BUF10_BW1 BUF10_BW2 (b) Figure 12: Tree performance: (a) 2-ﬂit packets, and (b) 16-ﬂit packets • EX1 BUF4: With express links, 4 ﬂits/VC input buﬀer • EX2 BUF4: EX1 BUF4 with 2-cycle express links For both small (2-ﬂit) and large (16-ﬂit) packets, the express links reduce packet latency because ﬂits traverse fewer hops. The increased bisection bandwidth aﬀorded by the added links also allows the network to accept higher load before saturating. To compensate for the additional area the added express link port incurs in each router, we reduce the input buﬀer size from 5 ﬂits to 4 ﬂits (EX1 BUF4 vs. EX1 BUF5). The resulting performance degradation is more pronounced for large packets because they are more bursty, hence more sensitive to buﬀer space. Because the express links span two hops, we increase their latency to 2 cycles while keeping the latency of other links at 1 cycle (EX2 BUF4). This conﬁguration causes higher latency for large packets because credits take longer to replenish on the slower express links. However, the network still saturates later than the NOEX BUF5 baseline. We also experimented with 4-ﬂit and 8-ﬂit packets, and the results are in line with the trend shown here. 6.2 Tree Fig. 10b illustrates a tree where two sub-trees are linked by a root router, where the bold lines represent global links. This organization captures the essence of building blocks in a hierarchical on-chip network. Only the leaf nodes generate and receive traﬃc, and 50% of the generated traﬃc cross the root router. Fig. 12 shows the average packet latency for the following conﬁgurations:                                                     • BUF5 BW1: Unit latency and bandwidth for all links, 5 ﬂits/VC input buﬀer 9. "
A software framework for trace analysis targeting multicore platforms design.,"This demonstration presents a complete software framework for dynamically mapping multi-threaded applications on a cycle accurate Network-on-Chip (NoC) architecture, analyzing the statistics of network workloads and drawing general guidelines regarding the design and optimization of NoCs.","A Software Framework for Trace Analysis Targeting  Multicore Platforms Design  Guopeng Wei, Paul Bogdan, Radu Marculescu  Department of Electrical and Computer Engineering  Carnegie Mellon University  Pittsburgh, PA 15213, USA  {guopengw, pbogdan, radum}@ece.cmu.edu  ABSTRACT  This demonstration presents a complete software framework for  dynamically mapping multi-threaded applications on a cycle  accurate Network-on-Chip (NoC) architecture, analyzing the  statistics of network workloads and drawing general guidelines  regarding the design and optimization of NoCs.  Categories and Subject Descriptors  J.6 [Computer Applications]: Computer-Aided Design –  computer-aided design (CAD).  General Terms  Performance, Design, Tools  Keywords  Network on chip, trace analysis, multicore.  1. INTRODUCTION  Synthetic traffic patterns such as uniform random or transpose  traffic are traditionally used to predict the performance of the  system in the design of NoCs. However, recent work has shown  that the statically distributed traffic patterns and the average  injection rate of synthetic traffic do not properly characterize the  dynamics of realistic workloads which directly influence the NoC  performance [1]. Consequently, accurate traffic modeling is of  crucial importance for NoC performance analysis and platform  optimization [2]. Towards this end, in this demo, we propose a  software framework showing the dynamics of real applications  through trace analysis, and drawing general guidelines regarding  the design and optimization of NoCs.  2. APPLICATION MAPPING  The proposed software framework dynamically maps realistic  multi-threaded applications on NoC via a branch-and-bound  algorithm [4] (see Figure 1). The multi-threaded applications are  represented as timed application task graphs, where the packet  injection pattern at each node consists of a time series of  Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  NOCS'11, May 1-4, 2011 Pittsburgh, PA, USA  Copyright 2011 ACM 978-1-4503-0720-8… $10.00  destination IDs and communication volumes measured in bits.   The proposed framework has two application mapping options.  One is to specify the IP location for each task in the application  task graph, so that the system under simulation can mimic the  exact traffic conditions while running realistic applications.  Alternatively,  the proposed framework can randomly map  application task graphs to the NoC. Moreover, the runtime of each  application can be easily specified using this framework.  Figure 1. Mapping of application task graphs  3. CYCLE ACCURATE SIMULATION  The proposed framework allows user to specify various NoC  architecture parameters like, network topology, network size,  input or output buffer sizes, flits size, number of flits per packet,  routing scheme, routing service time, arbitration delays,  etc.  Figure 2. Overview of proposed software framework                              By integrating the cycle-accurate NoC simulator Worm_sim [3]  within this software framework, the advantages of various  architectural configurations can be easily quantified (as shown in  Figure 2).  4. NETWORK TRAFFIC ANALYSIS  The proposed framework allows analyzing and visualizing  general performance metrics such as average packet latency,  network throughput and detailed energy consumption values  (link/router energy consumption), as well as various high-order  statistics of the network workload. The following paragraphs give  a brief introduction of these metrics.  Distribution of packet inter-arrival times  Packet inter-arrival times are recorded for every input and output  buffer and their empirical cumulative distribution is obtained via  the Kaplan-Meier algorithm [5].  Distribution of flit inter-arrival times  Flit inter-arrival times are recorded for every input and output  buffer and their empirical cumulative distribution is obtained via  the Kaplan-Meier algorithm.  Distribution of waiting times of packets  Waiting times of packets are recorded for every input and output  buffer and their empirical cumulative distribution is obtained via  the Kaplan-Meier algorithm.   Source-to-destination latencies  Source-to-destination latencies are recorded for each source-todestination pair showing the average packet latency between a  source node and a sink node as a function of time.  Contention probability  Contention probability figures for every input output channel pair  of a router showing the probability as a function of time the  arbiter fails to grant a connection from the input channel to the  output channel due to resources contention.  Buffer utilization  Buffer utilization figures for every input and output buffer  showing the buffer occupancy as a function of time.   Link and router utilization  Link and router utilization figures for every link and router  showing the number of clock cycles the link or router is utilized  against the total simulation cycles as a function of time.  Packet velocity distribution  The software measures the speed of packet passing through a  buffer as being inversely proportional to the waiting time of the  packet in that buffer. By averaging over all packets in a buffer, it  computes the distribution of packet velocity over all buffers in the  network.  Fitness of buffers  The software computes  the distribution of fitness values  corresponding to a buffer in the network. The fitness value [6]  reflects the likelihood of receiving a new packet in a buffer.  Source  to destination path  Visualization  identification and  The proposed framework will help user identify the paths of the  traffic from any source node to any destination node. The red  nodes in Figure 3 show the paths recorded from node 15 to node  59 under adaptive routing algorithm. The number below each red  node shows the number of times the packets go through that node.  Figure 3. Path identification for node15 to node59  5. CONLCUSION  In this demo, we propose a software framework for trace analysis  targeting multicore platforms design. In order to show the  dynamics of realistic NoC traffic, the framework analyses the  high-order statistics of the network workload like the distribution  of packet inter-arrival times, contention probability, etc. All these  metrics can be used for designing better NoC-based multicore  platforms. The demonstration will be shown using a laptop and no  special requirements need to be accommodated.  6. "
Dynamic decentralized mapping of tree-structured applications on NoC architectures.,"This paper presents a novel application-driven and resource-aware mapping methodology for tree-structured streaming applications onto NoCs. This includes strategies for mapping the source of streaming applications (seed point selection), as well as embedding strategies so that each process autonomously embeds its own succeeding tasks. The proposed embedding strategies only consider the local view of neighboring cells on the NoC which allows to significantly reduce computation and monitoring overhead. Our vision is that this approach facilitates self-organizing embedded systems that provide the flexibility and fault-tolerance required in future silicon technologies. The results provided in this paper show that our local and decentralized algorithms can compete with previously presented global and centralized algorithms.","Dynamic Decentralized Mapping of Tree-Structured Applications on NoC Architectures Andreas Weichslgar tner, Stefan Wildermann, Jürgen Teich University of Erlangen-Nuremberg, Germany {andreas.weichslgar tner,stefan.wildermann,teich}@cs.fau.de one chip. This requires the need to provide new communication interfaces that scale with the technology. A comparison between (segmented) on-chip buses, which are traditionally applied in heterogeneous multi-processor platforms, and networks-on-chips (NoCs) clearly reveal the scalability advantages of the NoC-approach. [1] analytically derives that with growing number of communicating elements, the clock can be kept constant on a NoC. Area and power dissipation only scale linearly, which is far better than for bus-based communication. Besides the scalability issue, the increase of technology brings further design challenges: due to miniaturization, the number of defects may increase. It is predictable that there will be chips on the market that are not free of defects. This again means that it is nearly impossible to apply oﬄine optimization algorithms that result in a static mapping of functional blocks onto the NoC resources. The second reason is that embedded applications are getting more and more dynamic. This can especially be observed when looking at the boom of embedded devices which ABSTRACT This paper presents a novel application-driven and resourceaware mapping methodology for tree-structured streaming applications onto NoCs. This includes strategies for mapping the source of streaming applications (seed point selection), as well as embedding strategies so that each process autonomously embeds its own succeeding tasks. The proposed embedding strategies only consider the local view of neighboring cells on the NoC which allows to signiﬁcantly reduce computation and monitoring overhead. Our vision is that this approach facilitates self-organizing embedded systems that provide the ﬂexibility and fault-tolerance required in future silicon technologies. The results provided in this paper show that our local and decentralized algorithms can compete with previously presented global and centralized algorithms. Categories and Subject Descriptors C.3 [Special-Purpose and Application-Based Systems]: Real-time and embedded systems General Terms Algorithms Keywords Networks-on-Chip, graph embedding, decentralized mapping 1. INTRODUCTION Embedded systems used to be devices with limited and often static functionality. For systems with such characteristics, oﬄine design ﬂows are able to explore and synthesize design options that are optimized for speciﬁed design goals. It is, however, necessary to rethink this approach due to two main reasons. The ﬁrst reason is that technology complexity rapidly increases. Soon there will be billions of transistors available on Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS’11, May 1-4, 2011 Pittsburgh, PA, USA Copyright 2011 ACM 978-1-4503-0720-8 ...$10.00. in the NoC. This can be done in parallel in contrast to a centralized approach which has to map the tasks sequentially. Especially for streaming applications and applications with a huge amount of periodical communication data, not only the mapping of the functionality, but also of the communication onto the NoC channels is important. Congestions has to be avoided and short paths between the communicating tasks lead to a reduced delay. In this concept, we present several incarnations of an autonomous embedding algorithm, each tailored for the optimization of such objectives. Our vision is that this approach facilitates selforganizing embedded systems that provide the ﬂexibility and fault-tolerance required in future nanoelectronic NoC architecture. The results provided in this paper show that our local algorithms can compete with previously presented algorithms with global knowledge. 2. RELATED WORK There are several known techniques for dynamic task mapping onto NoCs. Carvalho et al. [3] present several heuristics for dynamic mapping that avoid congestion and reduce the overall network load. They compare simple heuristics without any cost evaluation, such as First Free (FF) and Nearest Neighbor (NN), with other heuristics that take the link occupation in account, such as Minimum Average Channel Load (MAC) or Path Load (PL). The tasks are mapped and scheduled by a central manager, which doesn’t know the actual topology of the application graphs and maps the occurring tasks dynamically. The application model contains nodes for software and hardware tasks, but there is no multitasking supported and the algorithms run on a global view. In [5], Chou et al. introduce near convex regions for incremental run-time application mapping. With these areas, the inter-node communication could be minimized and the mapping is divided into two steps: ﬁrst ﬁnd a suitable near convex region, and then place the tasks within it. This approach possesses a better scalability than any heuristic that does not consider regions for application mapping. Nevertheless, there is still a global manager which stores the complete system utilization. In [4], this work is extended by comprising the user behavior. By doing this and splitting up the pattern in four subproblems the contention and communication costs could be minimized. Faruque et al. present an agent-based distributed application mapping which reduces the monitoring traﬃc comparing to centralized approaches [9]. Agents are small tasks which can be executed on any node in the NoC and perform resource management. They act and negotiate with each other to ﬁnd processing elements suitable for mapping a task. There are two types of agents to accomplish this: Global Agents (GA) and Cluster Agents (CA). The CAs have knowledge about their cluster. When they get a new task request, they negotiate with the GAs, which have global information about all clusters. In contrast to above works, our self-embedding approach as proposed in this paper is fully decentralized and autonomous. There is no central manager like in [2], and no agents have to run on several tiles like in [9]. Every NoC unit is considered equal and contributes to the embedding. Besides the central approaches there is some previous work on dynamic and decentralized mapping: In [12], Li presents a random-walk-based dynamic tree (a) (b) Figure 1: (a) NoC architecture and (b) application tree. evolution algorithm. Tasks dynamically spawn new tasks. The tree size and further information about the evolution of the tree are not know a priori. The new tasks are embedded totally distributed by a random-walk algorithm. The optimization goal in this approach is to minimize the communication dilation and the ratio between maximal and average resource utilization, which expresses the processor load. They furthermore prove that this ratio converges to 1 if the tree size is inﬁnite. The work is based on theoretical assumptions that assume that there are no resource boundaries and all processing elements are homogeneous. Hosseinabady and Nunez-Yanez [11] propose a decentralized stochastic and fault-tolerant mapping and routing algorithm. Their methodology incrementally maps the tasks and, concurrently, routes their communication by applying a random walk. This random walk is performed by sending so-called task request ﬂits to randomly chosen neighboring routers. They have the purpose to ﬁnd a tile which can host the task. To improve this random-walk-based algorithm, they propose additional metrics which rely on information that are distributed by broadcast messages. The fault tolerance is achieved by ignoring faulty ports when performing this stochastic routing scheme. In our approach we also take the link utilization in account and develop a general scheme for decentralized mapping algorithms. It is also possible to handle dynamically created tasks. 3. PRELIMINARIES Application mapping onto NoCs involves assigning tasks onto processing elements and communication onto communication links. In the following, we give some formal deﬁnitions necessary to describe our mapping approach. 3.1 NoC Architecture In this paper, we will focus on regular NoCs with a meshbased structure where units u ∈ U are connected by links l ∈ L. The NoC can be described formally as follows: Definition 1. A NoC of width N and the height M is a directed graph GN oC (U, L) with U = {(x, y) | 0 ≤ x < N , 0 ≤ y < M } L = {(u1 , u2 ) | u1 , u2 ∈ U ∧ dm (u1 , u2 ) = 1}, where dm deﬁnes the Manhattan distance between two units. (1) (2) The distance between two units u1 = (x1 , y1 ), u2 = (x2 , y2 ) ∈ U is calculated according to: dm (u1 , u2 ) = |x1 − x2 | + |y1 − y2 |. (3) Each unit consists of one processing element (PE) and one router that is connected to the local PE (through a Network-Interface (NI)) and the four routers in the cardinal directions. Tasks can be loaded and executed on the PEs. Each unit u ∈ U provides a limited amount of consumable resources that can be occupied by tasks, e.g., memory for storing data and program code. This limit is denoted as res(u). The maximal available bandwidth of a link l ∈ L is limited to band(l). An example of a NoC can be seen in Fig. 1(a). 3.2 Application Graphs The methodology proposed in this paper is tailored for dataﬂow-dominated streaming applications, with characteristics typically found in multimedia, telecommunication, and signal processing. For the analysis presented in this work, we make following assumptions: A1: An application i is executed periodically with period Pi . A2: Data-dependencies between functional blocks result in a tree-shaped dataﬂow. A3: Applications are characterized by bandwidth-oriented one-to-one communications between tasks. A graph-based, formal model of each applications as it is illustrated in Fig. 1(b) can be given as follows: Definition 2. An tree-structured application i is modeled as an acyclic, bipartite graph GA (Vi , Ei ). The set Vi = Ti ∪ Ci is a union of task vertices Ti and communication vertices Ci . Each edge e ∈ Ei connects a task vertex t ∈ Ti to a communication vertex c ∈ Ci or vice versa. Let pred(v) denote the set of predecessors and succ(v) the set of successors of vertex v ∈ Vi . Then it holds that |pred(t)| ≤ 1, ∀t ∈ Ti (A2), and that |pred(c)| = |succ(c)| = 1, ∀c ∈ Ci (A3). An example of such a tree can be found in Fig. 1(b) where circles describe task vertices and rectangles communication vertices. Considering the heterogeneity of the NoC, each task t ∈ Ti is characterized by its execution time exec(t, u) on PE u ∈ U and the resources required for successfully executing the task on the PE req(t, u). Each message c ∈ C is designated with its payload size(c). With the application’s execution period Pi , it is possible to furthermore calculate the bandwidth requirement of c : bw(c) = size(c) Pi and the load imposed by task t to unit u: load(t, u) = exec(t, u) Pi 3.3 Mapping (4) (5) Application mapping can now be described as embedding the application graph onto the NoC graph. This involves a) task mapping and b) communication routing. Task mapping is denoted as m : Ti → U , i.e., assigning tasks to processing i=1 r : Ci → {R | R ∈ n(cid:89) elements. Routing is a function L, 0 ≤ n ≤ N + M − 2 ∧ ∀ i = 1, ..., n − 1; li = (u, u ∧ li+1 = (u )}, , u i.e., each communication node is assigned with a route through communication links. For example, a route with n hops can be described as a sequence r(ci ) = (l1 , l2 ...ln ). A feasible mapping is give, if the following constraints hold: , u) ≤ res(u) (cid:88) req(t, u) + req(t (6) (cid:48)(cid:48) (cid:48) (cid:48) ) (cid:48) t(cid:48)∈T :m(t(cid:48) )=u (cid:88) load(t (cid:48) , u) ≤ loadmax (u) (7) load(t, u) + t(cid:48) ∈T :m(t(cid:48) )=u where 6 ensures that resource restrictions are respected, and 7 is the schedulability test of resource u. Moreover, a feasible routing has to consider the available bandwidth. , l) ≤ band(l) (cid:88) bw(c) + bw(c (8) (cid:48) c(cid:48)∈C :l∈m(c(cid:48) ) In the following, we propose decentralized algorithms for application-driven mapping and routing, which we call selfembedding. 4. SELF-EMBEDDING ALGORITHM As our inspected application topologies are treestructured, tasks can be embedded incrementally by an already mapped predecessor task. Only the root nodes have to be placed differently, since they have no predecessor tasks. In Section 6, we propose algorithms for this seed point selection. Distributing the mapping calculation to the task nodes helps to parallelize the workload and to prevent a single point of failure. Our algorithm is described as follows. Definition 3. Given an already placed task t(cid:48) with mapping m(t(cid:48) ) = un . Then, the incremental self-embedding algorithm embAlg searches for a feasible mapping m(t) and r(c) is described as: (cid:40) embAlg(un , t, h, c, z ) = 1, if Eqs. 6, 7, 8 are fulﬁl led 0, else (9) • un : current unit (m(t(cid:48) ) = un ) that calculates/initiates • t: the task that has to be mapped (t ∈ succ(c)) the embedding algorithm • h: max. search distance in hops deﬁning the size of the search space • c: communication node to map (c ∈ succ(t(cid:48) )) • z (u, t, R, c): cost function which determines the optimization goal by evaluating a mapping option u and routing option R. • m(t): the resulting mapping of t, and • r(c): the route chosen for c. This generic formulation allows the use of arbitrary cost functions z (u, t, R, c), even weighted combinations of diﬀerent optimization goals are possible there. The cost function is applied onto units and links within a search space of a size that is determined by parameter h. It is a matter of (a) (b) Figure 2: An example of a self-embedding algorithm with the initial search space of 1 hop. In the upper half the search space is shown and in the lower half the mapping. The root node t0 is embedded initial with the seedpoint selection in mapping step S0 . In mapping step S1 it starts the embedding algorithm for t2 and c1 . In S2 , it starts the embedding for t3 and c2 . In the same mapping step, t2 itself can start the embedding of t4 and c3 , and so on. While embedding the right successor, the already mapped left successor can embed its own successors in parallel. the implementation of our algorithm-scheme to compute a solution out of the values of the cost function (see Section 5). By means of parameter h, the search space can be limited to units in the neighborhood of the task. This allows to reduce the overhead when a task embeds its successors. Thus, search space can be restricted to a certain area that can range from the direct tile neighbors to all tiles within the NoC. This provides a ﬂexible interface where the diﬀerence between a local embedding and a global version is only the matter of the size of the search space. In case no feasible mapping can be found in the local search space, its size can be increased iteratively. An example for this search space adaption can be found in Algorithm 1. Algorithm 1 Pseudocode of search space adaption of the self-embedding algorithm for all t ∈ succ(t(cid:48) ) do h ← hinit repeat map ← embAlg(un , t, h, c, z ); h ← h + 1 until map = 1 ∧ h ≤ hmax end for Besides the optimization goal, basic constraints have to be considered to provide a feasible mapping. This includes that, for being considered as a mapping candidate, a unit must provide suﬃcient free resources and the additional work load that would be imposed by mapping the task must not exceed the unit’s schedulable utilization. Moreover, units and the links with faults must be ignored. A motivational example for a decentralized embedding following our scheme is shown in Fig. 2. The ﬁgure also illustrates the degree of parallelization achievable with the selfembedding approach. The embedding is performed within 4 mapping steps. In contrast to a sequential embedding which would take 11 mapping steps. The achievable degree of parallelization is topology-dependent, but we can theoretically derive bounds on how many mapping steps it requires to embed an application. Note that mapping steps may however have varying runtimes. This is due to the fact that processes and monitoring messages might be delayed because of preemption, and the search space adaption might result in diﬀerent numbers of iterations. Theorem 1. For an application with |Ti | tasks, the lower bound on the required mapping steps is Ω((cid:100)ld(|Ti |)(cid:101) + 1), and the upper bound is O(|Ti |). Proof. The upper bound (worst-case) for the parallelization would be a chain of tasks, what means |pred(t)| ≤ 1, |succ(t)| ≤ 1, ∀t ∈ Ti . This is depicted in Fig. 3(a). In this case, every task can only embed one successor and no degree of parallelization is given. The embedding is still distributed, but performed sequentially. For the lower bound (best-case), the application topology would allow that all tasks mapped until step s are able to map a successor in step s + 1, as illustrated in Fig. 3(b). The bound can be proven by means of induction. The theorem states the relation between number of tasks |Ti | and number of steps s as (10) (cid:100)log2 (|Ti |)(cid:101) + 1 = s 2s−2 < |Ti | ≤ 2s−1 . For s = 1, we can only map the root node, i.e., (cid:100)log2 (1)(cid:101) + 1 = 1. (12) For s → s + 1, if the lower bound holds for s, we can map up to n = 2s−1 tasks in s steps, as given in Eq. 11. Each of these n tasks can spawn a new successor. This would result in a number of tasks being mapped after s + 1 steps that (11) S0 S1 S2 S3 S4 (a) (b) Figure 3: Examples of application topologies leading to worst-case and best-case number of mapping steps. (a) worst-case: tasks are placed sequentially. (b) best-case: all tasks mapped until step Sk map a successor in step Sk+1 . lies between n + 1 and n + n, if only one or if all previously mapped tasks spawn a successor, respectively. This leads to (cid:100)log2 (n + 1)(cid:101) + 1 = (cid:100)log2 (2s−1 + 1)(cid:101) + 1 = s + 1 and, respectively, (cid:100)log2 (n + n)(cid:101) + 1 = (cid:100)log2 (2) + log2 (n)(cid:101) + 1 = s + 1 (14) (13) 5. INCARNATIONS OF EMBEDDING ALGORITHMS According to the generic interface of the embedding algorithm described in Section 4, there are various alternatives to implement self-embedding. This mainly aﬀects how to select a unit from the search space to map the task, and how to perform the routing of the communication between the mapper task and its successor. The implementations range from deterministic to probabilistic approaches. Algorithms can work with a predeﬁned ﬁxed routing scheme, like xy-routing, and calculate the cost function according to this scheme. On the other hand, the routing scheme can be part of the embedding itself. In the following, we present one algorithm with a ﬁxed routing scheme and one with an adaptive routing scheme. 5.1 Path Load and Best Neighbor In [3], the Path Load (PL) is proposed as a goal function pation. Given the mapper task t(cid:48) , a successor t, and the to avoid congestion and to minimize the average link occucommunication c, (t(cid:48) , c), (c, t) ∈ E . The cost function for a pathLoad(cid:0)m (t) , t, r (c) , c(cid:1) = mapping m(t) with routing r(t) is given as: where the already consumed bandwidth of a link l is derived by considering the bandwidth imposed by all messages being routed over link l: (cid:88) l∈r(c) ratelink (l) (15) ratelink (l) = bw(c). (16) (cid:88) c∈C ∃i:l=r(c)i The algorithm in [3] works by considering each mapping option m(t) ∈ U with routes calculated by a ﬁxed routing scheme, e.g., xy-routing. Evaluating every single tile in the NoC results in a search space of h = (N − 1) + (M − 1) hops. The number of inspected units is N × M and the number of inspected links is (N × M ) × 2 − (M + N ). It is obvious that for large NoCs, the amount of monitoring overhead would be enormous. If a suitable mapping option is already found, then it is not necessarily required to continue the search on the remaining units. Equation 15 makes clear that the path load will only get worse when adding links to a route. The observation also holds for other network metrics, such as Mininmal Average Channel Load (MAC) and Minimum Maximum Channel Load (MMC) as proposed in [3]. This lead to our proposal of restricting the search to a locality and applying bounds on the search area. This restriction limits the units considered to the set Uh , containing all units with a distance of h hops to the current unit un , and to the set of links Lh . Uh = {u | dm (un , u) ≤ h} (17) Lh = {(u1 , u2 ) | u1 , u2 ∈ Uh ∧ dm (u1 , u2 ) = 1} (18) If the algorithm ran without success, the search range can be increased by applying Algorithm 1. Independent of the applied goal function, this incarnation can be generalized as best-neighbor (BN) algorithm: All units and links in a certain area are inspected and the best unit and routing is chosen according to the cost function. In our path load example, the cost function is communication-centric and the sum of the link occupations. But also a task-oriented goal function, e.g., considering the resource occupation, could be used in the same manner. 5.2 Random Walk One ma jor disadvantage of the path load algorithm is the ﬁxed routing scheme (here xy-routing). Even though xyrouting establishes routes with minimal hops and is easy to implement, it can’t deal with faulty links or routers. The random walk-based approach for task embedding in [12] can help here. The random walk (RW) can be performed on each unit. When considering to map a task, a unit either keeps a newly spawned task or sends it to a neighbor randomly chosen with uniform probability. We modify this theoretical investigation to comply with our self-embedding approach. Here, each unit that holds an unmapped task sends a request to a non-faulty neighbor which is randomly selected. This neighbor does the same, however, the unit from which it gets the request is excluded to prevent cycles. This procedure is repeated until the random walk has been performed h times. The found tiles and links are then evaluated with the cost function. This can be implemented by rolling back the route taken by the random walk, evaluating the cost function and keeping the best unit found so far. When returning to the initial unit, the best ﬁtting unit of the random walk is available. Moreover, the route taken by the random walk is used for the communication between the tasks, thus enabling fault-tolerant routing. A modiﬁcation of this algorithm is to apply two cost functions: besides the cost function that evaluates a mapping option m(t), we weight the probabilities to chose the next neighbor for the random walk. We call this algorithm weighted random walk (RWW). To calculate these weights, a cost function is needed. For example, one goal could be a low channel load and the other goal a low resource occupation. So, a neighboring cell which is connected via a link with less occupation should be chosen with a higher probability than a cell with a high congestion. Especially if knowledge about neighboring links is available in the routers, this approach is promising. If random walk was run without success and no suitable unit was found, then the algorithm can be started again. Because of the non-deterministic behavior, it can return diﬀerent units. 5.3 Discussion The two presented incarnations of embedding algorithms have diﬀerent behavior. While the best-neighbor algorithm searches every unit in a certain area within it choses the best mapping option. The random walk approach choses a random set of units as search space with only one unit per hop distance. Pro jecting this into the search domain, BN performs a breadth-ﬁrst search while the random-walk represents a depth-ﬁrst search. The BN algorithm with the ﬁxed xy-routing scheme can’t provide fault tolerance. In contrast, the random walk delivers an adaptive route and guarantees fault-tolerance. Another aspect are the costs of an hardware support. While BN compares only values of cost functions and maybe sorts them, the RWW algorithm needs at least pseudo-random numbers and arithmetical support to calculate the weighted probabilities. 6. SEED POINT SELECTION As every task places its succeeding tasks and communication, the initial task or root task has to be placed by a diﬀerent algorithm. For these tasks, it is necessary to ﬁnd those units on the NoC on which it is suitable to load the root nodes. We call these units seed points (Us ) and several methodologies are available to determine this set. As we don’t want to loose the decentralized characteristics of our approach, the seed point determination should not have a global view of the entire NoC system. Nevertheless, some global information, like the size of NoC, previous seed points and cluster information have to be stored centrally. However, these algorithms are only executed once per application and the saved information is linear in the NoC size. So the scalability is kept. An example of the three proposed seed point selections strategies can be seen in Fig. 4. If the number of applications is know a priori or they are mapped concurrently, a clustering algorithm like k-means can be suitable given that the applications and their tasks have roughly the same characteristics. With an eﬀective implementation like in [10], centers of regions with similar sizes can be found. Global knowledge is needed for this procedure, but not the status of every unit and link. If the applications spawn dynamically, another approach from the clustering area can be used. The Hochbaum-Shmoys or Farthest-Away algorithm [6] searches for the farthest point away from the already found seed points (Us ). The next seed point u1 is calculated as below: u1 = argmax u∈U ( min us ∈Us (dm (u, us ))) (19) If the applications are dynamically spawned and have different requirements for resources the approach of near convex regions introduced by Chou and Marculescu [5] oﬀers a viable approach. The regions are calculated with the help of a dispersion factor and a centrifugal factor. The ﬁrst one describes how many idle neighbors a unit within a region should have, the second how far a unit should be away from the current region border. Both together help to prevent isolated units and to keep the regions compact. To use this algorithm for determining the seed point for an application, the near convex region of the application is calculated and the center is chosen as seed point. This calculation has to be executed by an central manager or agents. In our implementation, this calculation only has to save which units were already used for previous near convex regions. A knowledge about the entire system status is not needed if this algorithm is only used to determine the seed. 7. EXPERIMENTAL RESULTS In this section, we present the test environment and show the experimental results in comparison to already existing task embedding algorithms. 7.1 Simulation Setup To evaluate the proposed algorithms, we implemented a Java NoC simulator. There the units and links are modeled to match the deﬁnitions in Section 3, see also Fig. 4 for some screenshots. As seed point selection the Farthest Away algorithm is used. To test our algorithms, we use three diﬀerent classes of trees.• Chains : These trees can typically be found in signal processing, where a stream of signals is processed se• Binary trees : These kind of applications are often quentially in a ﬁlter chain. • Quadtrees : Quadtrees are often used to decompose a used to solve decision-problems. picture in separate regions to de- and encode with different granularity. Such a coder is e.g. the QSDPCM codec [13]. We use TGFF [8] format and the e3s Benchmark-Suite [7] for our test applications. Several of these applications are embedded iteratively to simulated the dynamically occurrence of new applications. 500 runs of each testcase are made. 7.2 Evaluation Metrics avgnet = |L| ∀ l ∈ L For the remainder of this paper, additional evaluation metrics are required to measure the success of an embedding and to compare diﬀerent algorithms. An important metric is the average network load. It directly inﬂuences the energy consumption of the system and indirectly reﬂects the (cid:80) ratelink (l) length of found communication paths. Another important measurement is the number of unsuccessful runs of the embedding algorithm. If the algorithm returns not a valid unit or a valid route, it has to be re-run again with adapted parameters. It might even be necessary to execute a diﬀerent algorithm. The number of fails (#F ails) quantiﬁes the practicability of an embedding algorithm as well as the parameter set. To measure the overhead which is caused by the embedding, those monitoring messages are counted that are exchanged between the PEs to accomplish the mapping. If a unit needs information about the neighboring units and links, then we assume that this information is acquired by (20) (a) (b) (c) Figure 4: Seed point selection according to (a) k-means, (b) farthest away, (c) near convex region ·104 150 % n i t e n g v a 20 15 10 5 0 BestNeighbour 10 20 4 s e g a s s e BestNeighbour BestNeighbour 3 2 1 0 s l i a F # 100 50 0 10 20 10 20 M g n i r o t i n o m # search space (# hops) search space (# hops) search space (# hops) Figure 5: Best Neighbor with diﬀerent search space exchanging messages. Also loading new tasks onto processing units requires sending the conﬁguration via messages. Since they don’t belong to the application-speciﬁc traﬃc, they need also to be counted as monitoring messages. 7.3 Scalability In Fig. 5, our best neighbor algorithm is evaluated with diﬀerent size of the search space. Here the search space ranges from 1 (only the four cardinal neighbors) to 24 (the whole NoC). The algorithm with 24 hops search space is equal to the global path load algorithm from [3]. It points out that avgnet doesn’t improve with a bigger search space while the monitoring overhead rises rapidly. The number of fails drops with a further search space increase, but converges to zero before the whole search space is reached, for h=13. In Fig. 6, the experiments is repeated for NoCs of diﬀerent sizes (here N=M=NoC Width). To keep the resource occupation similar the number of application is also raised (7, 8, 9, 10, 11). It is obvious that the BN Algorithm with global search scales worse as can be seen from the huge amount of monitoringM essages while both produce similar avgnet . 7.4 Random-Walk with weighted Probability In Fig. 7, we compare the average network load between a random-walk (RW) that uses equal probabilities to perform the random walk and a random walk with weighted probabilities (RWW) as we introduced in Section 5.2. We use a 13 × 13 NoC conﬁguration with 5 faulty units in the center (see Fig. 4). The seed point selection is for both test cases the same It shows that RWW provides a better overall network utilization and also reduces the number of failed runs. It is obvious, that the number of fails increases comparing quadtrees with binary tree and binary trees with chains, because in a chain only one successor has to be embedded and so one feasible unit is needed. On the opposite for a quadtree four units are needed. The probability that four available units are in the neighborhood is less than only one available unit. 8. CONCLUSION In this paper we have described a novel approach to map tree-structured streaming applications onto NoCs. To prove the power of decentralized self-embedding, we have presented a model and interface for this kind of algorithm and also have introduced two incarnations of these algorithms. The local Best Neighbor algorithm can compete with the global Path Load algorithm by oﬀering a magnitude less monitor overhead. Also we have developed an improved randomwalk based algorithm that oﬀers several advantages against a pure random walk algorithm. In our future work, we want to extend our algorithms to diﬀerent structured applications so that it is possible to map graphs where a task has more than one predecessor. We also want to implement the selfembedding as a part of hardware router into a NoC. Acknowledgement This work was partly supported by the German Research Foundation (DFG) as part of the Transregional Collaborative Research Centre ”Invasive Computing” (SFB/TR 89). 13 14 15 NoC Width 16 17 0 5 10 ·104 m # o n t i r o i n g M s s e s e g a Local Global 13 14 15 NoC Width 16 17 10 12 n g v a e t i n % Local Global Figure 6: Increasing of the NoC Width 3 4 5 6 7 0 5 10 15 20 Number of Applications n g v a e t i n % RW RWW (a) chain 3 4 5 6 7 0 5 10 15 20 Number of Applications n g v a e t i n % RW RWW (b) binary tree 3 4 5 6 7 0 5 10 15 20 Number of Applications n g v a e t i n % RW RWW (c) quadtree 3 4 5 6 7 50 45 40 35 30 25 20 15 10 5 0 Number of Applications # F a l i s RW RWW (d) chain 3 4 5 6 7 50 45 40 35 30 25 20 15 10 5 0 Number of Applications # F a l i s RW RWW (e) binary tree 3 4 5 6 7 50 45 40 35 30 25 20 15 10 5 0 Number of Applications # F a l i s RW RWW (f ) quadtree Figure 7: Comparison between random walk and weighted random walk 9. "
Exploiting inherent information redundancy to manage transient errors in NoC routing arbitration.,"We exploit the inherent information redundancy in the control path of Networks-on-Chip (NoCs) routers to manage transient errors, preventing packet loss and misrouting. Unlike fault-tolerant routing, our method does not drop packets when faults occur in routers and thus does not increase the burden on neighboring routers. Unlike the NoC interconnect links, the routing operation is nonlinear and standard error control coding methods cannot be used. Instead, our method exploits existing information redundancy in the router, significantly reducing the area overhead and power consumption compared to triple-modular redundancy (TMR). An analytical reliability model of our method is provided, including parameters such as circuit size, different error rates for logic gates and registers, and the location of a faulty element. Compared to TMR, the proposed method improves the arbiter reliability by two orders of magnitude while reducing the total power and area by 43% and 64%, respectively. Simulations performed on a 4x4 NoC show that our method reduces the average latency by up to 90% and 12% over no-protection and TMR methods, respectively.","Exploiting Inherent Information Redundancy to  Manage Transient Errors in NoC Routing Arbitration  Qiaoyan Yu, Meilin Zhang and Paul Ampadu  Department of Electrical and Computer Engineering  University of Rochester  Rochester, NY 14620, USA  <qiaoyan, mezhang, ampadu>@ece.rochester.edu  Abstract—We exploit the inherent information redundancy in the  control path of Networks-on-Chip (NoCs) routers to manage  transient errors, preventing packet loss and misrouting. Unlike  fault-tolerant routing, our method does not drop packets when  faults occur in routers and thus does not increase the burden on  neighboring routers. Unlike the NoC interconnect links, the  routing operation is nonlinear and standard error control coding  methods cannot be used. Instead, our method exploits existing  information redundancy in the router, significantly reducing the  area overhead and power consumption compared to triplemodular redundancy (TMR). An analytical reliability model of  our method is provided, including parameters such as circuit  size, different error rates for logic gates and registers, and the  location of a faulty element. Compared to TMR, the proposed  method improves the arbiter reliability by two orders of  magnitude while reducing the total power and area by 43% and  64%, respectively. Simulations performed on a 4x4 NoC show  that our method reduces the average latency by up to 90% and  12% over no-protection and TMR methods, respectively.  Keywords-Networks-on-chip; on-chip interconnect; transient  error; information redundancy; arbiter; reliability; fault tolerant;  triple-modular redundancy   INTRODUCTION  I.  Reliability is one of the most critical challenges caused by  increasing chip densities [1-4]. Nanoscale fabrication processes  inevitably result in defective components, which may lead to  permanent errors [2]. As the critical charge of a capacitive node  decreases with technology scaling, the probability that a highenergy particle strike will flip the logic value in a storage  element increases [3]. Moreover, transient error rates in logic  gates are expected to increase because of higher frequencies  and lower supply voltages [4]. Another side effect of increasing  chip densities is the need for long interconnections, creating a  This work was supported in part by the U.S. National Science Foundation  (NSF) under Grant ECCS-0925993 and Career Award ECCS-0954999.  Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are not  made or distributed for profit or commercial advantage and that copies bear  this notice and the full citation on the first page. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior specific  permission and/or a fee.  NOCS'11, May 01-04 2011, Pittsburgh, PA, USA   Copyright 2011 ACM 978-1-4503-0720-8/11/05…$10.00.  well-known communication bottleneck. Networks-on-Chip  (NoCs) have demonstrated potential  to manage  this  communication problem in many-core systems [5]. Thus, error  management for NoCs is imperative to achieve error resilience.  Error control in NoCs is handled more differently in  interconnect links than in routers. Permanent errors in NoC  links can be managed by adding spare wires [6, 7]. While  transient errors in NoC links can be managed using error  control coding, because on-chip interconnect can be treated as  a linear system [12-15]. Coding methods have also been  employed to manage transient errors in the data path of NoC  routers [16, 17]; unfortunately, the router control path is not a  linear system, requiring alternative approaches to manage  general routing errors. One common approach to handle  permanent errors in router is fault tolerant routing. These  methods involve isolating the entire router [8-10] or a few ports  of a router [11] if permanent errors are detected. Methods for  permanent error management can either add spare components  to replace the defective elements or increase the burden on the  remaining usable elements in the system, increasing energy and  degrading performance. Thus, those methods are not suitable  for transient error management.   Triple-modular redundancy (TMR) can be used to improve  reliability by duplicating the unit under protection and selecting  the output through majority voting. Because of its simplicity,  the TMR approach has been applied to the router control paths  [18, 19]. TMR can theoretically function when up to one-third  of the components are erroneous, but in reality the presence of  multiple errors will likely occur in multiple separate units,  limiting the overall reliability. The potential for errors in the  majority voter further reduces the effectiveness of the TMR  approach, particularly when the number of units being  protected is small [20]. Consequently, TMR is not an ideal  solution for the control paths in NoC routers.  In  this work, we exploit  the  inherent  information  redundancy in the router to manage transient errors in the  nonlinear routing arbitration stage. The inherent information  redundancy is extracted from the presence of forbidden signal  patterns and inconsistent request-response pairs during the  arbitration phase. Here, we examine a router using XY routing,  which does not need routing tables and complicated update  mechanisms.        or misrouting. As shown in Fig. 2, the RC unit is protected by  error detection unit (1). If RC computation errors are detected,  a warning signal is activated to stop the input FIFO popping  out the next flit and to request RC re-computation. This recomputation process results in additional one-cycle latency, but  it is faster than multi-cycle rerouting The Req_v vector after  error detection unit (1) informs output ports whether a new  packet header arrives. The CReq_v vector indicates to release  the reserved input-output port connection in the next cycle. The  ValidFlit vector indicates if the current flit is a valid flit (i.e.,  header, payload or tail flit). The RRA unit in Fig. 1 is  composed of round-robin computation (RRC) unit and  registers shown in Fig. 2. The round-robin (RR) registers store  the grant vector used in the last cycle. Priority registers save  the priority vector, which will instruct the RRC unit to select  the next highest-priority input port. By exploiting inherent  information  redundancy, we use  four separated error  management components⎯error detection unit (2), error  correction units (3) and (4) ⎯to prevent spatial and temporal  error propagation.   B. Inherent Information Redundancy Abstraction  The inherent information redundancy is obtained through the  occurrence of forbidden signal patterns or inconsistent requestresponse pairs in the system. Note that failure detection in this  work is applied to dimensional XY routing, but the principles  introduced here can be applied to other routing algorithms, as  well. In this work, we use a practical packet format [22]. Each  packet has one header flit, one tail flit and several payload flits.  The first two bits of a header flit are ‘10’. The remaining bits in  the header flit contain information such as source identifier,  destination coordinator and routing protocol. The first two bits  of a tail flit are ‘01’. A flit with high logic on both the first and  second bits is a payload flit.   1) Failures in Route Computation (RC) Unit  The function of the RC unit is to determine which output  port the current input port should connect to. This means no  more than one output port is requested each time, and this  exclusive feature  is regarded as one of  the  inherent  information  redundancies. According  to  the RC unit  functionality, three request failures may happen.  a) Mute-request: No output port request when a header  flit arrives at the input port. This failure does not result in  packet lose, but delays the release of the current input  port, increasing latency.   b) Multiple-request: One header flit requests multiple  output ports at one cycle (we assume no flooding protocol  is being used here). This type failure will result in multicopy transmission, which consumes extra energy, and  potentially increases the network traffic congestion.   c) Request-switch: A non-header flit arrives but there is  a request to build a new input-output port connection; or  the request of the intended output port is muted while  there is a simultaneous request for another output port.  Although one request is produced by the RC unit, this  erroneous request results in packet misrouting and even  deadlock.   Fig. 1 Generic pipelined router architecture.  Fig. 2 Block diagram of proposed routing arbitration unit. We propose the  addition of four shaded units to the conventional arbitration unit. RC: route  computation, RR: round-robin.  The remainder of this paper is organized as follows: an  overview of the proposed architecture and inherent information  redundancy are explained in Section II. The proposed methods  for protecting route computation unit and round-robin  arbitration unit are introduced in Section III and Section IV,  respectively. Case studies showing the performance of the  proposed system is presented in Section V. Conclusions and  future work are provided in Section VI.  II. EXPLOITING INHERENT INFORMATION REDUNDANCY   A. Architecture Overview  In this work, we focus on transient error management for  the NoCs using popular mesh topology and five-port routers.  To obtain high clock frequency, we assume each hop has four  pipeline stages: three for the router and one for links between  two neighboring routers, as shown in Fig. 1. Note that each  dash-dotted box is duplicated by five times in each router,  because five-port router is interested here. In the first stage of  the router, ECC decoder is used to manage interconnect link  errors. Error-free packets are stored in the input FIFO. In the  second stage, the route computation (RC) block extracts the  type of the incoming flit, determines the desired output port  from the given destination address, and requests access to the  appropriate output port. The round-robin arbitration (RRA)  unit is composed of round-robin computation unit and registers  for priority vectors and port reservation information. Each  RRA unit grants the input port with the highest-priority to  access the output port that follows that RRA unit. In the third  stage, flits popped out from the output FIFO are encoded  before transmission. In this work, we concentrate on the error  resilience of Stage 2, the router control path, to complementary  to error control methods for link and router data path [12-17].  Error correction typically results in more overhead than  error detection, so our method provides error correction to the  unit only if errors in that unit will directly result in packet loss      Detecting and correcting these three failures can effectively  prevent error propagation to the next hop, saving energy on  unnecessary network fabrics and logic gates switching.  2) Failures in Round-Robin Arbitration (RRA) Unit   Each packet experiences three steps—create an input portoutput port connection, maintain the port reservation and  release the connection—to transfer a packet over the RRA  unit. The RRA unit in each port grants a single connection  between that port and one of the input ports. The connection  remains until the CReq is high. The RRA unit may experience  four types of failures.  a) Valid header flit with already active grant: the RRC  unit changes the priority vector after each packet  transmission. If one or more bits in the RR and priority  registers are flipped by particle strikes, an input port may  be given the grant before a new header flit arrives.  b) Valid payload flit without active grant: the header flit  has reserved one output port; however, the reservation  information is corrupted. Consequently, the payload flits  cannot be continually transferred and network congestion  occurs.   c) Valid tail flit without active grant: the tail flit cannot  be forwarded to the output port and the input-output port  connection cannot be released. This also causes flit loss  and network congestion.   d)  Invalid incoming flit with active grant: the output  port provides a grant to an input port, in which no valid  flit exists. This grant cannot be cleared unless another  failure happens in the RRA unit.   These inconsistent request-response pairs are used to  provide error resilience in the RRA unit, preventing packet  corruption and loss.  III. PROPOSED SIGMA AND BRANCH ERROR DETECTION  METHOD FOR ROUTE COMPUTATION UNIT   A. Method Description  The sigma and branch detection method is proposed to  detect mute-request, multiple-request and request-switch  failures. For XY routing, the route computation (RC) unit  compares  the destination address,  indicating by  the  coordinator (X_D, Y_D) in the 2D mesh network, with the  current node ID represented by (X_C, Y_C). Fig. 3(a) shows  the diagram of the request generation circuit in the RC unit.  The number of active outputs (i.e. Req_E/S/W/N/L) cannot be  more than ‘1’s at each computation. There exists two pairs of  branch points—(A0, A1) and (B0, B1), which have forbidden  pattern ‘11’. We propose to warn the system by examining the  number of active Req signals and checking the forbidden  status of the branch points. If the number of requests is more  than 1 or the branch points have the same logic value, a  warning signal is turned into high. The proposed architecture  is shown in Fig. 3(b). A sigma function (Σ > 1) is applied to  detect the multiple-request events. The validation of the  incoming flit is examined to detect the muted request. The  complementary status of the branch points is checked to  capture the failure behavior.   (a)  (b)  Fig. 3 Proposed sigma & branch detection circuit. (a) Requests generation  circuit (ReqGC). (b) Error detection circuit.  B. Analysis and Evaluation   Given NRC is the number of errors detected in the RC unit, T  is the time period for examination, Ngate is the number of gates  in the RC unit, and ε is the gate failure rate. For simplicity, we  assume each logic gate has the same gate failure rate. The  closed-form expression for NRC is provided below.  N NT         (1)  ⋅= ε⋅ RC gate ) ( ' = RC RC N Nf , Error Management Method         (2)  in which, NRC′ is the number of residual errors detected in the  RC unit after using error detection method.   Nf Error Management Method , T RC RC = ( ) γ α ⋅ = ( ⋅ NT T ⋅ ε RC )          (3)  ⋅ ⋅ ε = α RC N in which, the parameter α (in the range of [0,1]) is the factor  that an error management method can reduce the unit failure,  and γRC is the RC unit failure rate. As shown in equation (3),  γRC depends on the error control method, the circuit complexity  and the logic gate failure rate.   The influence of the number of failed gates on the RC unit  reliability is examined in Fig. 4. We randomly inject errors to  the RC unit netlist. The output of each gate is possible to be  flipped. Since the number of logic gates in the RC unit is less  than fifty, each data point is obtained by averaging 50,000                    Fig. 4 RC unit failure rate versus  the number of gate errors.  Fig. 5 Impact of gate failure rate on RC unit failure rate.  Fig. 6 RC unit failure rate reduction.  random simulations. As shown in Fig. 4, examining the number  of requests (i.e. Σ detection) can reduce the system failure rate  to 0.2. Combining Σ detection with branch point detection can  further reduce the RC failure rate. Compared to the TMR  method, our method achieves smaller system failure rate. This  is valid even if only single gate fails in the RC unit.   Fig. 5 shows the relationship of error management method  and gate failure rate with the system failure rate γRC. TMR  cannot reduce the system failure rate by several orders of  magnitude as expected. This is because the duplicated RC unit  has the same probability of experiencing logic gate failure.  Moreover, TMR has large area overhead, thus it is more likely  to have errors. Our method considers both the error detection  and correction capability (i.e. α) and the area cost (i.e. smaller  NRC), providing a smaller γRC than TMR. Fig. 6 clarifies the RC  unit failure rate reduction over the no-protection case. As can  be seen, our method achieves three times system failure rate  reduction than the TMR, and yet, the overhead of TMR is 2.6X  over our method, as shown in Fig. 7. The limitation of our  method is 10% more latency on the critical path, compared  with TMR.  Fig. 7. Area, power and delay comparison of error management in RC unit.  IV. PROPOSED SELF-CORRECTING ROUND-ROBIN  ARBITRATION (RRA) UNIT  Round-robin arbitration (RRA) unit is used to reserve an  output port for one input port each time and release that  reservation after each packet transmission. In the RC unit,  transient errors can be managed with re-computation and the  residual errors do not result in packet loss. However, the  undetected errors in the RRA unit will lead to packet loss and  network congestion  increase. Consequently,  the error  management for RRA needs error correction capability.   A. Method Description  Generic flowchart for the error-free arbitration unit is  shown in Fig. 8(a). The presence of CReq has the highest  priority, because CReq releases the reserved resource to  transfer new packets over the router. If one of the reservation  registers is not zero, no new request can be granted until the  current  packet  transmission  completes. Round-robin  arbitration is executed only when the output port is available.  However, the arbitration unit composed of logic gates and  storage elements may be affected by the voltage fluctuation  and particle strikes, experiencing mute reservation, switching  reservation and multiple reservations similar to destination  computation unit. We propose a new RRA unit, which exploits  the  inherent  information redundancy  to perform error  detection or/and correction. Assume Req, CReq and flit type  information are error free because of the error management in  the RC unit. We examine the errors in reservation registers  (i.e. round-robin registers), by checking the consistency  between the register contents and the Req, CReq and flit type  signals. The proposed flowchart is shown in Fig. 8(b). Three  error managements are added to branches shown in Fig. 8(a).   1) Conflict between Output Port Reservation to CReq  We examine whether the content in the RR registers  matches to the CReqs or not before updating the registers (i.e.  releasing reservation). If no error, only the input port that  reserves the current output port issues a valid CReq; any  incident termination on the output port reservation is caused  by errors. This constraint is the inherent information that can  be used to correct the corrupted round-robin registers. The  incorrect registers are reset when the reservation does not  match to the CReq.           TABLE I. VALID AND FORBIDDEN SIGNAL PATTERNS IN RRA UNIT  Valid  Flit  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  Req CReq RR  0  0  0  0  1  1  1  1  0  0  0  0  1  1  1  1  0  0  1  1  0  0  1  1  0  0  1  1  0  0  1  1  0  1  0  1  0  1  0  1  0  1  0  1  0  1  0  1  Operation  Validity   (cid:68)  (cid:85)  ZZ  ZZ  ZZ  ZZ  ZZ  ZZ  (cid:85)  (cid:68)  (cid:85)  (cid:68)  (cid:68)  (cid:85)  ZZ  ZZ  (a)  Conflict between output port reservation and CReq/flit type  is detected in the error correction unit (3) shown in Fig. 2.  Illegal single reservation is detected in the error detection unit  (2) in Fig. 2.   B. Implementation  All possible signal patterns are shown in the Table I, where  the combination marked as “(cid:68)” mean possible cases; the ones  marked as “(cid:85)” indicate the inconsistency between new input  information and previous output port reservation; “ZZ” cases  never occur if ValidFlit, Req and CReq are error free when  they arrives in the RRA unit. As we can see in Table I, the  case‘1 0 1 0’ indicates a packet loss situation, because the  value of the RR register is zero and thus no grant is available  for the tail flit. Our method detects the inconsistency and flips  the register logic value if necessary.   In the generic round-robin arbitration, the grant gi, t and  priority vector pi, t are given by (4) and (5), respectively.  g ( eqR & eqCR & p (|) eqR & eqCR & g )      (4)  = ti , − 1 ti , − 1 ti , p ti , = g mod( i + ),4,3 t &(| gc t p ti , − 1 )                                       (5)  in which, i is the port ID, mod(i+3,4) is the function to shift the  priority for a five port router without 180 degree transmission, t  is time, gc is the bit-OR signal of all grant signals in each  output port.   Our proposed method detects and overwrites the erroneous  round-robin register, based on Table II and the appropriate  operation for the given Req, CReq and ValidFlit information.  The round-robin register obtains error resilience by using our  new gi,t computation expressed in (6).  g eqCR & ValidFlit (& p | eqR                           (6)  = ) ti 1, − ti , Equation (5) applies to our method, as well. As can be seen  from equations (5) and (6), the grant signal has error  propagation problem: failure on gi,t will cause failure on pi,t,  vice versa. Thus, we propose an error termination technique,  for the priority register. Priority registers are reset either when  (b)  Fig. 8 (a) Generic and (b) proposed flowcharts for the RRA unit.  2) Conflict Output Port Reservation to Flit Type  Corruption of the round-robin register results in packet  loss. The input port does not have active CReq and Req either  in the middle of transferring a packet or during no data  transmission period. To differentiate these two cases, we  check whether the reservation matches to the flit type. If the  current flit type is payload, there exists one valid reservation  in one of the output ports. Missing or switched reservation in  the round-robin register can be detected and then be repaired  by rewriting the register and stopping current data flow.   3) Illegal Single Reservation  Because each arbiter issues a single grant each cycle, the  presence of multiple reservation signals after the RRA unit  indicates miscomputing. This error is caused by logic gate  error in round-robin computing unit. We examine the number  of active grants and reset the reservation registers if more  grants exist. We assume permanent errors on logic and storage  elements have been examined and repaired in the testing  process. Stopping the grant process for one cycle, we can  obtain the correct grant signal and reservation register content  in the next cycle.                          the number of ‘1’ of priority bits greater than one or when zero  priority vector occurs. The reset operation may result in packet  loss. In the future work, we will investigate using inherent  information redundancy to increase the error resilience of  priority registers.   ⎧ ⎪ ⎨ ⎪ ⎩ == ≥ = ∑ ∑ = = otherwise . ,1 0 2 ,0 _ _ 3 0 ki , 3 0 ki , p or p if n reset PR k k       (7)  C. Analysis and Evaluation  In this subsection, we compare the proposed method with  TMR using quasi-simulation method. Because purely random  simulation cannot capture each error case, we firstly verify  whether the system will yield a wrong output under the  different error injection conditions, to obtain γRRA,i.j (i is the  number of failed registers and j is the number of failed logic  gates). The average RRA failure rate γRRA is computed by  equation (8).  ∑ ∑ = = = Logic DFF N j N i ji , RRA ji , RRA p 0 0 ) * ( γ γ          (8)  where, pi,j is a function of the total number of registers NDFF,  the total number of logic gates NLogic in the RRA unit, each D  flip-flop (DFF) failure rate ε, and the failure rate ratio of logic  gate over DFF β The closed-form expression for pi,j is given by  (9).  p ( ) ( ) ( ) ( ) j N j Logic i N i DFF Logic DFF ji , Logic DFF j N Nf i N j ,, i N − − − ⎞ ⎟⎟ ⎠ ⎛ ⎜⎜ ⎝ ⋅ − ⎞ ⎟⎟ ⎠ ⎛ ⎜⎜ ⎝ = = βε βε ε ε βε 1 1 , , , (9)  in which, 0 ≤ i ≤ NDFF, 0 ≤ j ≤ NLogic, and β≤1.   The RRA reliability is evaluated in Fig. 9. Each data point  for γRRA i,j is obtained from the random simulation executed by  50,000 times. The values of each logic gate and D flip-flop can  be flipped with the probabilities of βε and ε, respectively. It has  been observed that storage element has higher failure rate than  logic gate [4]. However, as the circuit frequency increases, it is  expected to have higher logic error rate than before [3].  Consequently, we assume that the ratio β is no more than 1. As  shown in Fig. 9(a) and Fig. 9(d), our method reduces the  system failure rate by two orders of magnitude over the noprotection and TMR approaches for β= 10-3. As β increases to  10-1, our method still achieves up to 4X higher system error  rate reduction than TMR, as shown in Fig. 9(b) and Fig. 9(e).  Our method is superior to TMR in a wide range of ε and β.  When the logic gate failure rate is equal to register failure rate,  the advantage of our method does not maintain, as shown in  Fig. 9(c) and Fig. 9(f).   Results shown in Fig. 9 are obtained by injecting errors to  the RRA unit in the process of transferring payload flits. In  Fig. 10, we compare the failure rate reduction achieved by  different error management methods when payload and tail flits  are being transferred from input port to output port. The metric  γRRA reduction is defined as the ratio of TMR failure rate over  the failure rate of our method. As shown in Fig. 10 (a), our  method achieves up to 380X more failure rate reduction than  TMR, in the case of transferring payload flits. As shown in  Fig. 10(b), our method can tolerate comparable error rate to  TMR method in low ε region, and the failure reduction  increases to 14 times as ε increases to 10-2, in the case of  transferring tail flits.  (a)                                                                        (b)                                                                            (c)  (d)                                                                        (e)                                                                            (f)  Fig. 9 Failure rate of round-robin arbitration (RRA) unit for (a) β=10-3 (b) β=10-1 (c) β=1and failure rate reduction for (d) β=10-3 (e) β=10-1 (f) β=1                                                                                                                                 (a)                                                                (b)  Fig. 10 Impact of failed flit type on the system failure rate(a) Payload flit (b)  Tail flit for β=10-3.  Fig. 11 Overhead comparison for one arbitration unit  The RRA units without protection, using TMR and using  our method are implemented in a 65 nm technology. The cost  and performance for the RRA unit using TMR and our  methods are normalized to that of the no-protection RRA unit.  As shown in Fig. 11, TMR requires more than three times area  cost over the no-protection method. The area and power  overhead of our method is less than 3% over the no-protection  approach. Although the delay of our method is 79% larger than  the no protection, the absolution delay (0.24 ns) is acceptable  for a pipeline router running at ~10 GHz.  Fig. 12 Error injection model in experiments.  TABLE II. TOTAL AREA, POWER AND DELAY OF UNITS IN ROUTER STAGE 2  Area (μm2)  Dyn. Power (μW)  Leak. Power (μW)  No  Protection  1982  (100%)  859.7  (100%)  7.2  (100%)  TMR  3822  (202%)  2462.6  (286%)  13.9  (193%)  Proposed  2160  (109%)  884.3  (103%)  8.0  (111%)  V. CASE STUDY  A. Experimental Setup  We evaluate the performance and overhead of the routers  without protection, using TMR, and using proposed methods.  Following experiments were performed on a 4x4 mesh NoC,  which uses wormhole switching technique and the XY  deterministic routing algorithm. Each flit is 32-bit (the most  common flit size [23]). Three bits are used to represent the  coordinates X and Y for the destination address. Area, power  and delay are obtained based on synthesized results from  Synopsys Design Compiler using a Taiwan Semiconductor  Manufacturing Company (TSMC) 65 nm typical technology  and 2 GHz clock frequency. In the experiments to evaluate  reliability, we control the error injection enable signal (shown  in Fig. 12) to simulate different error rates and failure gate  locations. For simplicity, we assume that the gate failure rates  for different logic types are same. In future work, we will  consider the failure rate difference between logic types.  B. Area, Power and Delay Comparison  Reliability improvement of TMR and our methods has been  demonstrated in previous sections. Here, we compare the total  area and power of three different implementations for the  routing arbitration stage (shown in Fig. 1). TMR and proposed  approaches are employed to protect the route computation unit  and the round-robin arbitration unit. Error control overhead for  protecting multiplex in the data path and pipeline stage  registers are not included here. As shown Table II, the  proposed method only increases area by 9%, compared to the  no-protection approach; while TMR results in two times area  over the no-protection approach. Compared to TMR, our  method reduces area by 43%. Because of fewer logic gates and  D flip-flops are used in the arbitration units, the proposed  method reduces the dynamic power by 64% and the leakage  power by 42%, compared to TMR. The increased total power  overhead of our method over no-protection design is only 3%.             [1]  "
A comprehensive Networks-on-Chip simulator for error control explorations.,"Error control is imperative for reliable Networks-on-Chip (NoCs) design. In this demo session, we will present a CAD tool---a flexible and parallel NoC simulator. Our simulator evaluates the impact of different error control mechanisms on NoC performance and energy consumption in various noise and traffic injection scenarios. Our message passing interface language-based simulator can be executed on multiprocessors or server clusters. Multiple built-in blocks provide flexibility to evaluate different error control methods.","A Comprehensive Networks-on-Chip Simulator for  Error Control Explorations  Qiaoyan Yu, Meilin Zhang and Paul Ampadu  Department of Electrical and Computer Engineering  University of Rochester  Rochester, NY 14627, USA  <qiaoyan, mezhang, ampadu>@ece.rochester.edu  Abstract— Error control is imperative for reliable Networks-onChip (NoCs) design. In this demo session, we will present a CAD  tool⎯a flexible and parallel NoC simulator. Our simulator  evaluates the impact of different error control mechanisms on  NoC performance and energy consumption in various noise and  traffic  injection scenarios. Our message passing  interface  language-based simulator can be executed on multiprocessors or  server clusters. Multiple built-in blocks provide flexibility to  evaluate different error control methods.   Keywords-Networks-on-chip; fault tolerant; transient error;  permanent error; simulator; PARSEC.   INTRODUCTION  I.  Immature nanoscale fabrication processes inevitably result  in defective components that cause permanent errors in  integrated circuits [1]. The decreasing critical charge and  increasing clock frequency in deeply scaled circuits increase  transient error rate. Permanent and transient errors need be  managed not only in memory and microprocessors, but also in  the on-chip interconnect network. Networks-on-Chip (NoCs) is  a promising paradigm to handle the increasing on-chip  communication complexity [2, 3] and reliability issues [4, 5, 6].  However, the error characteristics have not yet been fully  understood and modeled. New hardware-efficient error control  methods will be investigated as new error models are created.  Current NoC simulators provide enormous capabilities to  assess the impact of regular NoC design parameters using  synthetic traffic patterns; however, insufficient effort has been  put on evaluating the impact of error management methods on  NoC performance and energy consumption. Our previous work  [7] has contributed to provide a simulation platform for NoC  with error control mechanisms.   II.  Based on our previous work [7], we have newly added the  function of evaluating the NoC with benchmark traffic traces  (e.g. PARSEC [8]) and the improved dependent error model, as  well as the enhanced friendly user interface. Using our  simulator, one can select (1) regular NoC design parameters,  such as NoC size, topology, packet, flit and buffer size, error  control code and scheme, (2) error injection characteristics, (3)  INNOVATIONS OF PROPOSED SIMULATOR  Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are not  made or distributed for profit or commercial advantage and that copies bear  this notice and the full citation on the first page. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior specific  permission and/or a fee.  NOCS'11, May 01-04 2011, Pittsburgh, PA, USA   Copyright 2011 ACM 978-1-4503-0720-8/11/05…$10.00.  traffic patterns—either synthetic traffic or traffic traces from  the popular benchmark. This simulator is implemented in C  and the message passing interface (MPI) language, which  allows parallel simulation and speeds up evaluation.  Commonly-used error control codes and schemes have been  built in, saving user development time. The data flow in our  simulator is shown in Fig. 1. A Java-based interface fetches  user inputs and generates three configuration files⎯traffic  injection, NoC configuration and error injection profiles. The  core of our simulator⎯C and MPI-based NoC behavior  description⎯uses a GNU scientific library (GSL) to generate  random numbers if necessary. The compiled binary codes can  be executed in a single machine with a multiprocessor or a  server cluster (e.g. Blue Gene and IA-64 [9]).  Fig. 1 Simulation flow  III. DEMONSTRATION OVERVIEW  In this demo, we will briefly introduce the details of our  simulator to ensure that the users can appropriately provide  input to describe their NoC design. Then, we will walk through  the most important features of the proposed simulator and give  examples of configuring the NoC with/without error control, as  well as mention our limitations and future plan. Some  simulation results have been published in our previous work  [7], so we do not repeat them here because of the limited space.   A. System Requirements  The proposed simulator has been successfully tested on  Linux/i386 and i686 system with four processors and Linux  cluster consisted of IBM cluster nodes (each with dual Intel  Itanium two processors), GCC version 3.4.6 and java version  1.4.2 are required to compile the source codes (in standard C  and MPI) and interface source codes (in Java), respectively.  GNU scientific library [10] is used to generate random  numbers in the simulator core if necessary.           Fig. 2 Simulator interface overview of setting NoC configuration   Fig. 3 Simulator interface overview of setting fault injection  B. Execution steps  The entire simulation takes four steps as follows:  (1) Specify NoC Settings in the NoC Configuration tab shown  in Fig. 2. Dimension of the NoC can be defined in Network  Size. Currently, mesh and torus topologies are supported in this  simulator, but supports for more  topologies are under  development. Packet, flit and buffer size can be specified in  this tab, as well. As a unique feature of the proposed simulator,  typical error control schemes, such as forward error correction  (FEC), Automatic Repeat Request (ARQ) and hybrid ARQ, are  built in the simulator. Common error control codes are  provided for the corresponding schemes. The selection of all  above is saved to a file, which is understood by the simulator.  To be convenient, default setup is available to test the simulator  and the operation system.    (2) Select synthetic or benchmark traffics in the Traffic  Injection tab. Synthetic traffic patterns in the proposed  simulator include such as uniform random, transpose, and selfsimilar traffics. Benchmark traffic patterns are consisted of  traffic traces generated based on PARSEC benchmark suites.  In this tab, users can specify the total simulation time, warm-up  period and recording period.   (3) Determine fault injection features in the Fault Injection  tab shown in Fig. 3. Both transient and permanent error  injections are available in the proposed simulator. For transient  error injection, bit error rate for dependent or independent error  models is selected in the typical examine region [10-12-10-1]; or,  the noise deviation voltage over the nominal supply voltage of  1V can be manually selected. Users are also allowed to inject  errors to the specified links, router and network interface or  roughly defined region in the network. The proposed simulator  provides flexibility to inject errors to different flits⎯header,  payload and tail⎯to clarify the cost and benefit of the error  control method that protects the specific flit type. Similar  options are provided for the permanent error injection.   (4) Post-process: simulations can be run in a multiprocessor  server or server cluster. Before simulation starts, the root  processor typically should be determined in Post-Process tab to  gather the specific simulation outputs, which will be further   processed to obtain performance metrics. In this tab, the  evaluation metrics, such as the worst-case and average latency,  average energy consumption and residual error rate. Other  customized intermediate output interfaces are allowed, as well.  IV. CONCLUSION  We propose a parallel and flexible NoC simulator that is  capable of evaluating the error control methods for NoC with  various traffic patterns and error models. This comprehensive  simulator provides  typical error control schemes and  commonly-used error control codes for user convenience.  Another feature that we included in the simulator is synthetic  traffics and traffic traces obtained from recent benchmark  suites. Audiences who are working on error control, NoC  architectures,  error modeling, benchmark  suites  and  multiprocessor server designs will be interested to attend our  demonstration.   "
Design of multi-channel wireless NoC to improve on-chip communication capacity!,"Many-core chip design has become a popular means to sustain the exponential growth of chip-level computing performance. The main advantage lies in the exploitation of parallelism, distributively and massively. Consequently, the on-chip communication fabric becomes the performance determinant. In the meantime, the introduction of Ultra-Wideband (UWB) interconnect brings in the new opportunity for giga-bps communication bandwidth, milliwatts communication power, and low cost implementation for millimeter range on-chip communication for future chip generations. In this paper, we study multi-channel wireless Network-on-Chip (McWiNoC) with ultra-short RF/wireless links for multi-hop communication. We first present the benefit of high bandwidth, low latency and flexible topology configurations provided by this new on-chip interconnection network. We then propose a distributed and deadlock-free location based routing scheme. We further design an efficient channel arbitration scheme to grant multi-channel access. With a few representative synthetic traffic patterns and SPLASH-II benchmarks, we demonstrate that McWiNoC can achieve 23.3% average performance improvement and 65.3% average end-to-end latency reduction over a baseline NoC of 8 x 8 metal wired mesh.","Design of Multi-Channel Wireless NoC to Improve On-Chip Communication Capacity∗ Dan Zhao and Yi Wang The Center for Advanced Computer Studies Univ. of Louisiana at Lafayette Lafayette, LA 70503 dzhao@cacs.louisiana.edu Jian Li IBM Research - Austin 11501 Burnet Road Austin, TX 78758 jianli@us.ibm.com Takamaro Kikkawa Research Center for Nanodevices Hiroshima University Hiroshima 739-8527, Japan kikkawa@hiroshima-u.jp ABSTRACT Many-core chip design has become a popular means to sustain the exponential growth of chip-level computing performance. The main advantage lies in the exploitation of parallelism, distributively and massively. Consequently, the on-chip communication fabric becomes the performance determinant. In the meantime, the introduction of Ultra-Wideband (UWB) interconnect brings in the new opportunity for giga-bps communication bandwidth, milliwatts communication power, and low cost implementation for millimeter range on-chip communication for future chip generations. In this paper, we study multi-channel wireless Network-on-Chip (McWiNoC) with ultra-short RF/wireless links for multi-hop communication. We ﬁrst present the beneﬁt of high bandwidth, low latency and ﬂexible topology conﬁgurations provided by this new on-chip interconnection network. We then propose a distributed and deadlockfree location based routing scheme. We further design an efﬁcient channel arbitration scheme to grant multi-channel access. With a few representative synthetic trafﬁc patterns and SPLASH-II benchmarks, we demonstrate that McWiNoC can achieve 23.3% average performance improvement and 65.3% average end-to-end latency reduction over a baseline NoC of 8 × 8 metal wired mesh. Categories and Subject Descriptors C.1.2 [Processor Architectures]: Multiple Data Stream Architectures (Multiprocessors)—Interconnection architectures General Terms Design 1. INTRODUCTION As single-thread performance improvement is reaching a diminishing return, investing multiple processing cores on the same die has become a popular means to sustain the exponential growth 9∗This research was supported in part by the National Science Foundation under grant numbers CNS-0821702 and CCF-0845983. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. NOCS ’11, May 1-4, 2011 Pittsburgh, PA, USA Copyright 2011 ACM 978-1-4503-0720-8 ...$10.00. of chip-level computing performance. Indeed, the chip industry has been exploring many-core chip designs [1, 2]. Future manycore chips will consist of hundreds or even thousands of energyefﬁcient processor cores with sophisticated memory hierarchies [3]. A scalable, cost-efﬁcient, ﬂexible and reusable on-chip communication infrastructure will become an enabling technology for this paradigm. The state-of-the-art shared-bus and point-to-point connections have been shown unable to provide both sufﬁcient bandwidth and low latency under a stringent power consumption limitation [4]. Therefore, Network-on-Chips (NoCs) has emerged as a promising communication platform for future processors. At the same time, impulse-radio Ultra-Wideband interconnect (UWB-I) has the unique features of high bandwidth and low-duty cycle pulse, which make it an attractive solution for high data rate multi-processor (multi-user) communication [5]. More speciﬁcally, the UWB-I is based on transverse electromagnetic wave propagation with the use of on-chip antennas. Since its signal has very short pulse duration, high data rate (C ) with constant signal-to-noise ratio (S/N ) is achieved by increasing the bandwidth (B ), following Shannon’s capacity equation (C = B log2 (1 + S/N )). It consumes low power in the range of a few milliwatts due to its very low duty cycle (typically < 0.1%) [6]. RF circuits can also be simpler since UWB signals are carrier free. Furthermore, pulse position modulation is typically used to modulate a sequence of very sharp Gaussian monocycle pulses. Each individual pulse is delayed in time depending on the data signal and the pseudo-random code (time-hopping sequence) assigned to the transmitter. Therefore, with the mechanism to hop the ultra-short pulses from different users at different time slots, we are able to divide the entire spectrum into several UWB “channels” where the data bit sequence for a user is transmitted at a particular channel. The received signal and the template signal is automatically correlated during the demodulation of multiple access such as time hopping. Meanwhile, CMOS-integrated dipole antennas have been proposed for on-chip implementation. Recent breakthroughs in silicon integrated circuits enable integration of tiny and low-cost antennae, transmitters and receivers onto a single chip, forming the basis of RF/wireless on-chip interconnect technology [7–9]. One of the recently-reported implementations of on-chip UWB-I has achieved 1.16Gbps data rate for single band at central frequency of 3.6GH z in 0.18µm CMOS technology [10, 11]. A Si-integrated meander type dipole antenna has been implemented for 1mm range data transmission at antenna area of 2.98×0.45mm2 . The area overhead for the transceiver design is 0.64mm2 . A 56GH z architecture is described in [12] to enable inter-chip communications at greater than 10Gbps. The system employs a self-locking receive section which attains 6.4pJ/bit efﬁciencies in 40nm CMOS. As the technology scales, UWB-I shows prominent scaling capability and its scaling trend is summarized in Table 1. Table 1: UWB Interconnect Scaling Technology (nm) 90 65 45 Cut-off frequency (GH z ) 105 170 280 Data rate per band (Gbps) 5.25 8.5 14 Dipole antenna length (mm) 8.28 5.12 3.11 Meander type antenna area (mm2 ) 0.738 0.459 0.279 Power (mW ) 33 40 44 Energy per bit (pJ ) 6 4.7 3.1 32 400 20 2.17 0.194 54 2.7 22 550 27.5 1.58 0.14 58 2.1 With the high data rate, low power and short-range communication provided by UWB-I, we propose to substitute wires with wireless radios for increased accessibility, improved bandwidth utilization, and minimized delay and crosstalk noise as in conventional wired interconnects. While data is radiated from an on-chip antenna in interposer layer without loss, a major design challenge is the sharing of ultra-wide bandwidth among multiple communicating nodes. Enabled by the multi-channeling capability, multiple nodes using different channels may communicate simultaneously on the same frequency band to signiﬁcantly improve the communication capacity while reducing the communication latency. Therefore, we propose a multi-channel wireless NoC architecture dubbed McWiNoC, where a number of multi-channeled RF nodes are dispersed on-chip to form a wireless “supertrain” network. Based on the prominent features and scaling capability of onchip RF interconnect technology, we propose a multihop wireless NoC, where the RF nodes are dispersed on-chip as wireless routers to forward data in their radio transmission range. We utilize the “free-of-wiring” ﬂexibility of RF to conﬁgure topology with varied RF ranges (Sec. 3). We propose a low-cost routing scheme for distributed and cyclic-free routing. Restricting turns based on a modiﬁed turn model further guarantees deadlock-free communication (Sec. 4). Facilitated with multiple channels, different RF nodes can transmit in parallel on distinct channels. To ensure successful concurrent transmissions, we further propose a channel arbitration scheme to grant multi-access channels and to resolve channel contention (Sec. 5). In addition, we study the hardware overhead in Sec. 6. 2. ARCHITECTURAL OVERVIEW We consider tiled multi-core design where each tile contains a processor with primary caches (L1 instruction and data caches), L2 cache slices and an interface to the on-chip interconnection network. A multi-channel wireless NoC (McWiNoC) is established for the communication among these highly integrated processor tiles. Figure 1 illustrates such a McWiNoC organized in a 4×4 2D mesh. A McWiNoC consists of a number of radio frequency (RF) nodes, each associated with a processor tile. The RF node has a radio-frequency interface (i.e., tiny low-power and low-cost UWB transceivers and antenna) with predetermined transmission range. Two nearby nodes within the transmission range are connected by a high bandwidth RF link. Note that only one physical input/output port is needed for an RF node, since only one band exists in McWiNoC. The processor tiles access the network via RF nodes, and their packets are delivered to destinations through one or multiple “hops” across the network. A collection of RF nodes connected wirelessly forms the McWiNoC topology. R F  n o d e  R F  n o d e  R F  n o d e  R F  n o d e  P  P  P  P  R F  n o d e  R F  n o d e  R F  n o d e  R F  n o d e  P  P  P  P  R F  n o d e  R F  n o d e  R F  n o d e  R F  n o d e  P  P  P  P  R F  n o d e  R F  n o d e  R F  n o d e  R F  n o d e  P  P  P  P  W i r e l e s s   L i n k  C o n t r o l   w i r e s  Figure 1: A McWiNoC-based many-core chip. Each RF node serves as a wireless router which supports routing, multi-channel arbitration, and buffering mechanisms for highspeed cost-efﬁcient on-chip communication. With multi-access capability, multiple nodes may communicate simultaneously on different channels. As a result, the communication capacity can be greatly improved. Besides the “wireless data channel”, we implenel arbitration control. The right to access the wireless channel is granted by the negotiation on the control channel. By interleaving wireless data transmission at a node, 100% wireless channel efﬁciency may be achieved. ment a separate “wired control channel” for fast and simple chan3. RF TOPOLOGY CONFIGURATION In contrast to many NoCs relying on regular topology, RF infrastructure is very ﬂexible in building any domain-speciﬁc topology without any “wiring” concerns. If each tile has its dedicated RF node, the RF nodes placement is ﬁxed with tile location. We may generate different topologies by changing the RF transmission range. For example, if the transmission range (T ) is set as the same as the RF node placement distance (L), a mesh architecture is formed in Figure 2(a). When we increase T from L to √2L or √5L respectively, we arrive at two other topologies as illustrated in Figure 2(b) or (c). By increasing T , RF node radix (i.e., number of RF links of a node) increases (i.e., more RF nodes are connected to a node via RF links) and thus the wireless connectivity is improved. The extreme case is to form a fully-connected network where each node has a long direct RF link to any other nodes. With the increase of T , the average path length for data delivery decreases which consequently reduces end-to-end communication delay. But at the same time, increased node radix may induce more channel arbitration which in turn may increase the latency of single link transmission. In addition, larger chip area overhead may be reserved for more powerful RF nodes. Therefore, we set to carefully study these two design parameters and investigate the trade-off between performance and cost. Design Impact of Node Radix and Hop Count. In McWiNoC, the overall communication time tc for a given application can be modeled as: tc = Npkt X i=1 Nhopi X j=1 (tarbij + trij + tcontij ), (1) where Npkt is the number of packets on the critical execution path D S 4 hops! (a) 8x8 mesh (c) Topology at transmission range   5 L (b) Topology at transmission range   2 L D S 5 L 8 hops! L D S 2 L 3 hops! Figure 2: Topology formation with varied RF range. 1.2 1 0.8 0.6 0.4 0.2 0 0 2 4 6 Transmission range 8 10 N o r m a i l d e z o c m m a c n u i i t n o i t m e rAr=0 rAr=1 rAr=2 rAr=3 9(a) Effect of node arbitration. 9(b) Effect of trafﬁc contention. 9(c) Effect of transmission power. Figure 3: Performance impact of transmission range on McWiNoC topologies.                                                600 500 400 300 200 100 0 0 2 4 6 8 10 Transmission range(mm) 12 N o r m a i l d e z r t s n a m i n o s s i o p w e r of the application, Nhopi is the number of hops needed by the ith packet to traverse from its source to destination, tarbij is the arbitration delay of the ith packet at its jth hop along its transmission path, which is proportional to RF node radix, trij is the transmission time for the ith packet at its jth hop, which is a function of packet size (P Si ) and wireless bandwidth (BW ), i.e., trij = P Si/BW , and tcontij is the time waiting for available communication resource (wireless channel) or buffering resource, which is linearly increased with node radix and is also closely related to trafﬁc injection. As both tarb and tcont will be affected by topology conﬁguration, we analyze the impact of hop count and node radix on them. tarb increases with the node radix which contradicts the time saved from the reduced hop count when the transmission range (T ) increases. For example, changing T from L to √2L reduces hop count by 32% while the arbitration level is doubled. For simple due to more levels of arbitration, e.g., rAr = 1 means 1 clock cycle will be added for each additional arbitration level. We consider the worst case arbitration delay where the arbitration time is determined by the node with the largest radix in the network. Figure 3(a) shows tc (normalized to the communication time obtained in the mesh network) changing with the transmission range at different rAr . In general, the hop count plays a more important role than the node radix. Although tarb tends to go up with increased radix, tr drops off rapidly due to reduced hop count. As a result, tc will go down as tr is dropping much faster than the increase in tarb . Note that, when rAr is rather high (e.g., rAr = 3), tarb increment will be more dominant than the reduction in tr . So tc will go up initially. But eventually, as the hop count keeps on decreasing, illustration, we deﬁne rAr as the arbitration overhead increment tc can drop rapidly because tr reduces more apparently than tarb . Increasing node radix will also affect the contention delay as a node needs to compete with more neighbors to gain access to the wireless channel. Such contention is also closely related to the trafﬁc injection in a node’s vicinity. We simply use a parameter, contention level (lCont ), to represent these effects. Thus Ri × lCont additional contention delay will be introduced to a node with radix Ri . Figure 3(b) illustrates the impact of node radix on tcont when rAr =1. When lCont is low, the reduction on tr due to reduced hop count is dominant and tc is decreasing in general. However, for higher lCont (e.g., lCont=0.3), the increase in tcont will dominate once passing certain point and tc actually goes up. It’s noticeable that the UWB-I transmission gain using on-chip antennas decreased inversely with transmission range. Thus the transmitter needs to use higher transmission power to reach larger transmission range. Figure 3(c) shows that the transmission power (normalized to the transmission power at T = 1mm) increases at an exponential rate. Therefore we should properly tradeoff the communication performance with the transmission power and restrict the transmission range at certain range. 4. LOCATION-BASED ROUTING After the RF nodes are placed on chip and their transmission uniﬁed range is determined, a McWiNoC is static and its topology is ﬁxed with medium scale and density, which are known prior to the routing scheme implementation. It becomes feasible to apply geographical aided routing (GAR) schemes based on the relative coordinates of nodes [13]. We develop an algorithm to ensure distributed, cyclic-free, shortest-path and deterministic rout                ing, namely, Location-Based Routing (LBR). Our LBR differs from GARs in two folds. First, its distributed logic-based routing renders low-latency, low-power and low-area communication. Second, its restricted-turn routing at the same time, guarantees deadlock freedom. 4.1 Basic LBR Design In LBR, each node is aware of the locations (coordinates) of its direct neighbors. For example in Fig. 4, when a packet is generated at a source RF node (S ), the coordinates of its destination RF node (D) are carried in the packet header. An intermediate RF node A, currently holding the packet, makes forwarding decision based solely on the locations of its neighbors and the destination node. The basic idea is that if an RF node A wants to send packet m to its destination node D (xD , yD ), it forwards m to its neighbor B (xB , yB ) whose geographic distance dBD = p(xB − xD )2 + (yB − yD )2 is shortest to D among all neighbors of A. This process iterates for the next intermediate RF nodes, for example, B , until D is eventually reached. In this example, three hops are needed to send packet m from S to D, forming a path of S → A → B → D. IV X 0< Y < 0 S I X > 0 < 0Y X <0 Y>0 III A B' X > 0 0Y > B D II Figure 4: Illustration of LBR under the topology T = √5L. To improve the routing efﬁciency, a fast process to ﬁnd the next hop among all the neighbors of a node S is to divide the entire chip area from a node’s perspective into regions, for example, four quadrants as shown in Fig. 4. Upon receiving a packet, the node checks the destination address contained in the packet header and determine in which quadrant the destination is located. Assuming the left top node has the smallest XY-coordinates. We may quickly ﬁgure out the destination’s quadrant by looking at the relative position of the destination from the node itself, i.e., ∆X = xD − xS and ∆Y = yD − yS . If ∆X ≥ 0 and ∆Y < 0, the destination is located in quadrant I, otherwise, ∆X > 0 and ∆Y ≥ 0, Quadrant II; ∆X ≤ 0 and ∆Y > 0, Quadrant III; or ∆X < 0 and ∆Y ≤ 0, Quadrant IV. For example in Fig. 4, ∆X = 3 and ∆Y = 4, so the destination is located in quadrant II. Then, we may calculate which neighbor in this quadrant advances furthest to the destination, i.e., dND is the shortest. The one with the most advance (e.g., A) is selected as the next-hop to forward the packet towards D. For each node, thereafter, we can ﬁnd its path to a destination by applying such a greedy localized routing algorithm on McWiNoC. Indeed, XY-routing with the speciﬁed RF range can also be used for our wireless topology which however results in longer routing paths. For example as shown in Fig. 4, with XY-routing, packet m can take two hops east from the source node S and another two hops south to reach the destination node D. Thus, a naive XY-route takes four hops. However, our proposed LBR saves one hop out of four, a 25% hop count reduction, by utilizing the ﬂexible wireless communication between the nodes. Note that, we may simply apply XY-routing for mesh network and no routing is required for fully-connected network, under the two extreme RF ranges. 4.2 Deadlock Avoidance Applying the basic routing scheme on McWiNoC however may induce multiple shortest paths with equal length. For example, Fig. 4 shows another path of three hops, S → A → B ! → D. The existence of such multiple paths may lead to potential deadlock. For deadlock free routing, we propose a modiﬁed turn model with the two abstract cycles as, clockwise cycle N W →N , N→N E , SE→S , S→SW and counterclockwise cycle N E→N , N→N W , SW →S , S→SE . Two clockwise turns SE→S , N W →N and two counterclockwise turns N E→N , SW →S are selected from each abstract cycle of the modiﬁed turn model without violating the diagonal position rule. As a result, LBR becomes deterministic, and chooses a path S → A → B → D as the routing path from S to D. Restricted-turn oriented LBR ensures that any routing path allocated to the same physical link will not have dependency on its own resource and thus no deadlock occurs. Note that, in the aforementioned case that speeds up the ﬁnding of the next hop by dividing the plane into four regions, one can argue that the principle of deadlock-free LBR is analogous to the one of deadlock-free XYrouting. 5. MULTI-CHANNELING & ARBITRATION Facilitated with multiple channels, different RF nodes can transmit in parallel on distinct channels. Such parallelism increases the network throughput and potentially reduces the end-to-end delay. The multi-access capability however is restricted in order to satisfy the bit-error-rate (BER) performance [14]. In other words, a limited number of channels are shared among a large number of communicating nodes. To ensure successful concurrent transmissions, we develop an efﬁcient channel arbitration scheme to determine (1) how a pair of sender and receiver agrees on the channel to be used for transmission; and (2) how to resolve potential contention for a channel. 5.1 Receiver Channel Coding & Assignment With the establishment of multiple channels, a channel coding scheme is applied to determine the time hopping sequences used for trafﬁc transmission and for monitoring trafﬁc arrival over a channel. To simplify multi-requesting arbitration and reduce handshaking latency, we apply a receiver-based coding scheme [15] where each node is assigned a unique receiving code and the code of the destination is used for any peer-to-peer transmission. Combined with coding, a channel assignment scheme alleviates the inter-node interference when the number of channels is bounded. For collision avoidance assignment, any two RF nodes, say B and C, may use different channels if they are within node A’s transmission range. An interference graph can thus be modeled based on the given WiNoC topology. Consequently, the channel assignment problem is deduced to a vertex coloring [16] of the interference graph by assigning different colors to the adjacent vertices. A greedy ﬁrst-ﬁt coloring algorithm is implemented to solve it. 5.2 Multi-Channel Arbitration   An efﬁcient channel arbitration scheme is further developed to resolve the contention due to multi-requesting of a receiving channel or competition of limited channel processing resources. For a WiNoC with static topology, distributed channel controlling and management can be carried out by setting up a dedicated wired control channel. A transmitter sends the request to the receiver through the wired control channel, and the receiver grants authorization to exactly one among multiple requesting transmitters at a time so as to prevent packet collision at the receiver. The implementation of a separate wired control channel ensures that the contention control and data transmission can be interleaved to achieve 100% channel efﬁciency. The basic idea is to handle multiple requests from different trafﬁc ﬂows simultaneously at each receiving node for more efﬁcient multi-channel contention. It involves two steps, channel arbitration and channel adaptation. In the ﬁrst step, each RF node which has packets waiting at the virtual queues sends multiple transmission requests to different neighboring receivers to request for accessing their channels. In the meantime, each node may receive multiple requests from neighboring senders simultaneously. It arbitrates the requests in round-robin and grants the channel access authorization to exactly one among multiple requesting senders at a time so as to prevent packet collision at the receiver. Consequently, the channel authorization will be acknowledged back to the winning sender. In the next step, as each sender may receive multiple channel authorizations granted from multiple receivers, such authorization signals will be competed at the sender and only one authorization will be accepted in a fair manner. The node then adapts its channel to the winning receiver’s channel so that the packet can be sent out. Note that each node maintains two arbiters to perform ‘request’ arbitration and ‘authorization’ arbitration respectively. 6. HARDWARE OVERHEAD STUDY Arbitration circuit cost (logic gates) due to higher node radix. The circuit cost follows the same trend of the arbitration delay. Assume the gate count of a 4-port arbiter is G0 , the number of logic gates at ith RF node with radix Ri is: Clogic (i) = G0 · ""log4 Ri # X j=1 lRi/4j m (2) For a McWiNoC with N RF nodes, the overall logic cost is estimated at: Clogic = G0 · N ""log4 Ri # X i=1 X j=1 lRi/4j m (3) Buffering cost due to higher node radix. In our proposed buffer management scheme, the buffering requirement at each RF node is linearly increased with the node radix, i.e., 2(Ri − 1). Assume the cost for a buffer unit to store one network packet is B0 , the overall buffering required by a McWiNoC network is thus: Cbuf f er = B0 · X i=1 N 2(Ri − 1) (4) Wiring resources (NoC wiring vs. McWiNoC control wiring). For a “wired” NoC, the interconnection between two routers includes the data bus and the control bus. Assume the width of the data bus and the control bus are BWdata and BWctr l respectively. Given the manhattan distance between the ith router and its jth neighbor, manDist(i, j ), the interconnect length between the two nodes is 2 × (BWdata + BWctr l) × manDist(i, j ). Thus, the overall interconnection length is determined by: LN oC = N X i=1 Ri X j=1 (BWdata + BWctr l) × manDist(i, j ) (5) While McWiNoC is free of wiring, it implements a control channel for fast control handshaking which requires wiring resources between neighboring nodes. The overall control wiring cost is computed by: LM cW iN oC = N X i=1 Ri X j=1 BWctr l × manDist(i, j ) (6) In McWiNoC, the increase of transmission range can increase the wiring requirement for two reasons. First, the increased node radix requires more control wires. Second, longer control wires are required for neighbors with larger distance. 7. PERFORMANCE EVALUATION 7.1 Simulation Setup We develop a simulator to evaluate the performance of the proposed McWiNoC platform under various network conﬁgurations, trafﬁc patterns and application domains. A McWiNoC with identical and omnidirectional radio range is built to cover the communication among the processor tiles. With a system of 8 × 8 square tiles, we conﬁgure 7 different topologies with increasing transmission range, as listed in Table 2, e.g., from mesh network to fullyconnected network. We then study the performance impact from the RF link length and the corresponding hardware overhead. We also conﬁgure the mesh network under different scales and wireless bandwidth. For mesh networks, we simply apply XY-routing to deliver packets. For all other topologies except for fully-connected network, we employ LBR as proposed in Sec. 4. The wireless bandwidth is properly allocated and arbitrated among multiple users in the network by implementing the receiver coded arbitration scheme in Sec. 5. The essential parameters used in our simulator are listed in Table 3. Table 2: Topology characteristics with transmission range Topology number 1 2 3 4 5 6 RF range normalized to unit 1 1.42 2.24 4.13 6.41 8.25 tile/node distance Avg. node radix 3.5 7.6 15.8 34.2 56.3 63.1 Max. node radix 4 9 21 51 64 64 Avg. hop count 5.33 3.75 2.27 1.51 1.12 1.02 7 9.9 64 64 1 We run experiments under three synthetic network trafﬁcs: uniform, one-hot spot and transpose. We introduce a parameter called normalized injection rate, or simply injection rate, which is deﬁned as a fraction of the total number of packets injected in a certain trafﬁc pattern. The lower the injection rate, the longer the packet inter-arrival time. For synthesis trafﬁc simulation, we set wireless bandwidth of 10 Gbps and the packet inter-arrival time is ﬁve cycles. We have extracted the trafﬁc traces from a few applications in the SPLASH-II suite [18] using RSIM simulator [19], which we extended to work with our WiNoC simulator. We use the default processor conﬁguration in RSIM for network trace generation. Two types of packets are generated, the control packets with size of 16 Bytes and the data packets with size of 80 Bytes, which consists of                                           9(a) Aggregate bandwidth. 9(b) Network throughput. 9(c) End-to-end delay. Figure 5: Single channeling vs. multi-channeling under different trafﬁc patterns.                                                                                                                                        9(a) Normalized execution time. 9(b) Normalized communication time. 9(c) End-to-end delay. Figure 6: Performance of SPLASH-II Benchmarks on 8 × 8 mesh McWiNoC with various wireless bandwidth. Table 3: McWiNoC network conﬁguration one-hot (1-hot) and transpose (trans.), as shown in Fig. 5. The performance improvement is apparent when shifting from single chanNetwork size up to 8 × 8 neling to multi-channeling. The network throughput of McWiNoC RF link length 1.43mm ∼ 14.2mm is approximately twice as its single channel counterpart under uniRF per-link BW 16/40/80 Bytes/cycle form trafﬁc. Under one-hot spot trafﬁc, the capacity advantage of Routing LBR multi-channeling tends to shrink due to higher channel contention Channel arbitration receiver coded arbitration caused by the hot spot. Nonetheless, an average of 11.8% improveBuffer organization Dynamically sized VOQ [17] ment is observed. McWiNoC achieves 33% reduction in latency Buffer size per node under uniform trafﬁc. A 25.4% reduction is shown under one-hot Num of channels 5 ∼ 32 trafﬁc, where contention becomes higher. However, only 4.4% rePacket size 16 & 80 Bytes duction is observed for transpose trafﬁc. It’s because under such Simulation warmup cycles 10000 trafﬁc pattern, single channeling achieves competitively high wireNum of packets analyzed 150000 less efﬁciency, low contention level and high trafﬁc locality before Die area 100mm2 saturation. When contention becomes even higher after saturation, McWiNoC will eventually outperform single-channeling and the latency reduction may even go up by 21.7%. Note that the latency implemented in McWiNoC. As such, a throttle signal is sent back via the upstream nodes all the way to the source, if the number of packets received by a node exceeds a predeﬁned threshold. Once receiving the throttle signal, the source node slows down the packet injection into the network. The performance of SPLASH-II benchmarks on the 8 × 8 mesh McWiNoC is shown in Fig. 6. The results are normalized to the baseline RSIM wired-NoC and are compared under three conﬁgurations of wireless bandwidth. The application execution time is deﬁned as the time spanning from when the ﬁrst trace was generated and sent to when the last packet was received and processed. As seen from Fig. 6(a), McWiNoC outperforms wired-NoC (i.e., normalized execution time < 1). When the wireless channel is conﬁgured at the same bidirectional per-link bandwidth (16 Bytes/cycle) as the wired baseline NoC, McWiNoC achieves up to 38% (23.2% in average) improvement in the overall execution time of the given application. It achieves around 60% performance gain in the communication time than the wired baseline for most of the applica                                                                              2(Nnei − 1) control header (16 Bytes) and data payload. The RSIM interconnection network, i.e., 8 × 8 mesh NoC, is used as the baseline for performance and overhead comparison. In order to compare the performance of the proposed McWiNoC with single channeling alternative, we also implement a single-channel WNoC as proposed in [20]. 7.2 Latency & Bandwidth Characterization We evaluate McWiNoC system performance in terms of throughput and end-to-end delay on a 8 × 8 mesh network. The Aggregate Bandwidth is deﬁned as the aggregate rate of successful message transmission through all the nodes in the network, which represents the transmission concurrency in the network as a result of multichanneling and channel access arbitration. Network Throughput is, as usual, the average rate of successful message delivery over the network. The End-to-End Delay is deﬁned as the time needed to deliver a packet successfully from a source to a destination node. We ﬁrst compare McWiNoC (labeled M ) against the single channel WNoC (labeled S ) using three trafﬁc patterns: uniform (Uni.), eventually saturates, thanks to the congestion control mechanism fft lu radix barnes water−spwater−nsq 0.4 0.5 0.6 0.7 0.8 0.9 1 N o r m a i l u c e x e d e z i t n o i t m e 9(a) Normalized execution time. 9(b) Normalized communication time. 9(c) End-to-end delay. Figure 7: Performance of SPLASH-II benchmarks on 7 different McWiNoC topologies. fft lu radix barnes water−spwater−nsq 0 0.2 0.4 0.6 0.8 1 N o r m a i l d e z o c m m u . e v o r d a e h fft lu radix barnes water−spwater−nsq 0 10 20 30 40 50 60 A e v r e g a E 2 E ( e c y c l ) 0 0 0.1 0.2 Injection rate 0.3 0.4 50 100 150 A g g r d n a b e a g e t w i d t h ( G s p b ) 4 × 4 6 × 6 8 × 8 9(a) Aggregate bandwidth. 9(b) Network throughput. 9(c) End-to-end delay. Figure 8: McWiNoC Performance under uniform trafﬁc with different network sizes. tions. McWiNoC achieves 76.8% (averagely 65.3%) reduction in For the application that has higher spacial locality such as radix, delay. the packet hop count is small (e.g., 2.4633). Increasing the transMore performance gain is obtained as the per-hop packet transmission range may not help reduce the hop count for these applimission time reduces with the increase in wireless bandwidth. For cations. The performance gain from the increment in transmission example, given the largest data packet size of 80 Bytes (16 Bytes range tends to decline when the trafﬁc becomes heavy. command head + 64 Bytes cache line), it requires 5 cycles, 2 cycles and 1 cycle respectively to transmit one data packet at each hop under the three bandwidth conﬁgurations (bd=16, 40 or 80 Bytes/cycle). But certain workload communication patterns with low ratio of Num_of_Ctrl_packet to Num_of_Data_packet may prevent McWiNoC from further performance improvement even with large bandwidth, e.g., f f t and lu. Considering the per-hop arbitration delay, the larger bandwidth conﬁguration leads to around 40.8% improvement in the end-to-end latency. Consequently, higher wireless bandwidth results in 15.7% speed up of the overall execution time. 0 0 0.1 0.2 Injection Rate 0.3 0.4 10 20 30 A g v . e n t w o r k t h r u p h g u o t ( G s p b ) 4 × 4 6 × 6 8 × 8 250 200 150 100 50 0 0 0.1 0.2 Injection rate 0.3 0.4 A g v . y a e d d n e − o − d n e t l ( e c y c l ) 4 × 4 6 × 6 8 × 8 7.3 RF Link Conﬁguration We run the SPLASH-II applications on an 8 × 8 tiled processor under 7 topology conﬁgurations as listed in Table 2. The performance is evaluated under the bandwidth conﬁguration of 16 Bytes per cycle and the results are given in Fig. 7. We assume that the link bandwidth is ﬁxed under various topologies by properly controlling the transmission power. In other words, the higher the transmission range, the higher the transmission power. Recall the analysis in Sec. 3, increasing transmission range reduces hop count while increasing node radix. As a result, the transmission time is significantly reduced with the decreased hop count while the arbitration time and the channel contention time go up with increased radix. In general, the impact from hop count reduction is more dominant in reducing the overall communication time. But the impact from high radix will become signiﬁcant if the trafﬁc injection is heavy, which may increase the communication time. Since the trafﬁc injection is relatively low for SPLASH-II benchmarks, increasing the transmission range generally speeds up the application execution time. 7.4 Scalability We evaluate the scalability of McWiNoC using the uniform trafﬁc under 3 network scales: 4 × 4, 6 × 6 and 8 × 8. As we can see from Fig. 8, the network throughput scales up while the latency scales down, when the network size increases. Note that in a larger network, the packet hop count becomes larger. The aggregated packet injection is higher for larger network scale. As reﬂected in the result, a larger network reaches the saturation point earlier. With larger trafﬁc injection, the network throughput improves until the network saturates. 7.5 Hardware Overhead We study the hardware overhead of McWiNoC according to Sec. 6 and compare it with the baseline NoC. Fig. 9 illustrates both the logic and the buffering resources increase with the transmission range (with the values normalized to the mesh McWiNoC at the given minimum transmission range). The logic overhead grows much faster than the buffering overhead. The estimation of the wiring overhead of both McWiNoC and the baseline NoC is given in Fig. 10. For instance, a baseline 8 × 8 mesh NoC with 8 Bytes of data bus and 4 bits of control bus consumes 21.76m interconnect length on a 100mm2 die area. While the McWiNoC alternative only needs 1.28m for control channel interconnection. As we can see, the control wiring cost increases with the increase of the transmission. When the transmission range reaches 3.17 (with average radix of 24), the wiring cost is nearly the same as the baseline NoC. Recall in Fig. 7, McWiNoC outperforms the baseline NoC, while the improvement starts to saturate with the increased transmission range. Therefore, the McWiNoC achieves the best cost                                        performance efﬁciency at the transmission range between L and 3L for the network and workload under study. d a e h r e v O i c g o L d e z i l a m r o N Logic Buffer 40 20 0 2 2 4 4 6 6 Transmission range 8 8 3 d a e h r e v O r e f f 2 u B d e z i l a m r o N 101 10                  Figure 9: Arbitration circuit overhead.               Figure 10: Wiring cost. 8. CONCLUSION In this paper, we have proposed a multi-channel WiNoC platform to improve the on-chip communication of future many-core chips. With the multi-access feature provided by the UWB wireless onchip interconnect, we have proposed to improve the WiNoC performance with a series of design and analysis, such as varying RF topology conﬁgurations, location based routing, multi-channeling and arbitration. We show that McWiNoC can signiﬁcantly outperform wired-NoC at low cost. An efﬁcient and low-cost RF node architecture was proposed, aiming to establish McWiNoC under limited chip resources. The best cost-performance efﬁciency is achieved at the medium RF range for the networks and workloads under study. 9. "
