title,abstract,full_text
On Characterizing Performance of the Cell Broadband Engine Element Interconnect Bus.,"With the rise of multicore computing, the design of on-chip networks (or networks on chip) has become an increasingly important component of computer architecture. The cell broadband engine's element interconnect bus (EIB), with its four data rings and shared command bus for end-to-end control, supports twelve nodes - more than most mainstream on-chip networks, which makes it an interesting case study. As a first step toward understanding the design and performance of on-chip networks implemented within the context of a commercial multicore chip, this paper analytically evaluates the EIB network using conventional latency and throughput characterization methods as well as using a recently proposed 5-tuple latency characterization model for on-chip networks. These are used to identify the end-to-end control component of the EIB (i.e., the shared command bus) as being the main bottleneck to achieving minimal, single-cycle latency and maximal 307.2 GB/sec raw effective bandwidth provided natively by the EIB. This can be exacerbated by poorly designed cell software, which can have significant impact on the utilization of the EIB. The main findings from this study are that the end-to-end control of the EIB influenced by software running on the cell has inherent scaling problems and serves as the main limiter to overall network performance. Thus, end-to-end effects must not be overlooked when designing efficient networks on chip","On Characterizing Performance of the Cell Broadband Engine               Element Interconnect Bus  Thomas William Ainsworth and Timothy Mark Pinkston  Ming Hsieh Department of Electrical Engineering, University of Southern California  Los Angeles, CA 90089-2562, USA  tainsworth@alumni.nd.edu, tpink@usc.edu  Abstract  With the rise of multicore computing, the design of  on-chip networks (or networks on chip) has become an  increasingly  important component of computer  architecture.  The Cell Broadband Engine’s Element  Interconnect Bus (EIB), with its four data rings and  shared command bus for end-to-end control, supports  twelve nodes—more than most mainstream on-chip  networks, which makes it an interesting case study.  As  a first step toward understanding the design and  performance of on-chip networks implemented within  the context of a commercial multicore chip, this paper  analytically evaluates  the EIB network using  conventional latency and throughput characterization  methods as well as using a recently proposed 5-tuple  latency characterization model for on-chip networks.   These are used to identify the end-to-end control  component of the EIB (i.e., the shared command bus)  as being the main bottleneck to achieving minimal,  single-cycle latency and maximal 307.2 GB/sec raw  effective bandwidth provided natively by the EIB.  This  can be exacerbated by poorly designed Cell software,  which can have significant impact on the utilization of  the EIB.  The main findings from this study are that the  end-to-end control of the EIB influenced by software  running on the Cell has inherent scaling problems and  serves as  the main  limiter  to overall network  performance.   Thus, end-to-end effects must not be  overlooked when designing efficient networks on chip.   Index  terms: Cell Broadband Engine, Element  Interconnect Bus, interconnection networks, on-chip  network, network-on-chip, heterogeneous multicore,  network characterization, performance bottleneck  1. Introduction  With the rise of multicore computing, the design of  on-chip interconnection networks has become an  increasingly  important component of computer  architecture.  On-chip networks (OCNs)—also known  as networks-on-chip (NOC)—interconnect various  functional and computational resources on a chip.   These days, researchers are actively pursuing the  design of efficient OCNs after becoming aware, in  recent years, that wires are now a dominant factor in  determining system performance, power dissipation,  reliability, cost, and other important characteristics  [1]–[5]. On-chip network architectures are being  proposed that are no longer flat shared bus structures  [6, 7] but, instead, are point-to-point and switched  networks taking the form of hierarchical buses, rings,  meshes, crossbars, and even irregular topologies [8]– [15]. As summarized in [16], on-chip networks are  being proposed and finding use in research and  commercial processor chips whose microarchitecture  are partitioned into multiple (and soon to be many)  clusters [17]–[20], tiles [21]–[24], and cores [25]–[31].  On-chip networks may be used for different  purposes and  take on different  forms by  the  microarchitecture to transfer instructions and operand  values, cache/memory blocks and coherency traffic,  status and control information, etc.  For example, of  the eight on-chip networks implemented in the TRIPS  EDGE  research prototype architecture  [23]  for  interconnecting its 106 heterogeneous tiles composing  two processor cores and L2 cache integrated on-chip,  two are 140-bit and 128-bit bidirectional mesh  networks used for transporting operand and memory  data, respectively, sent broadside; the remaining six are  dedicated fan-out trees or recombination networks with  unidirectional channels as wide as 168-bits for  instruction dispatch and as narrow as 10-bits for global  status and control. Alternatively, the Cell Broadband  Engine (Cell BE) [32]—a commercial heterogeneous  multicore processor—uses the Element Interconnect  Bus (EIB) as its on-chip network for transporting  instructions and data between its twelve elements  interconnected across the chip.                 The goal  in designing wire-efficient on-chip  networks, typically, is to achieve the highest possible  effective bandwidth (i.e., throughput) and the lowest  possible end-to-end latency for a given cost. In  designing OCNs for specific or general purpose  functions, it is important to understand the impact and  limitations of various design choices in achieving this  goal.  As a first step toward understanding OCN  design-performance tradeoffs in the context of a  commercial heterogeneous multicore processor chip,  we perform a case study on the Cell BE EIB.  The EIB  is an interesting OCN to study as it provides higher  raw network bandwidth and interconnects more end  nodes than most mainstream commercial multicore  processors. In this paper, we analytically evaluate the  EIB using conventional  latency and  throughput  characterization methods as well as using a recently  proposed 5-tuple latency characterization model for  on-chip networks.  Collectively, these are used to  identify the end-to-end control component of the EIB  (i.e., the shared command bus) as being the main  bottleneck to achieving minimal single-cycle latency  and maximal 307.2 GB/sec raw effective bandwidth  provided natively by the EIB.  This bottleneck can be  exacerbated further by poorly designed Cell software  which can significantly impact EIB utilization.  The  main findings from this study are that the end-to-end  control of the EIB—which is influenced by software  running on the Cell—has inherent scaling problems  and serves as the main limiter to overall network  performance.  This points to the importance of not  overlooking control and other end-to-end effects when  designing multicore networks on chip.  The rest of the paper is organized as follows.   Section 2 gives an overview of  the Cell BE  architecture and the requirements placed on its OCN.   Section 3 provides background on  the network  characterization methods used in this study.  Section 4  presents EIB latency and throughput characterization  results gathered principally from analytical cyclecounting and calculations based on published data  from IBM.  Section 5 presents an alternative view of  EIB latency results gathered by application of the 5tuple delay model proposed  in [22]. Section 6  describes how the Cell BE software can affect the  performance of the EIB.  Finally, Section 7 briefly  discusses future research directions, followed by  Section 8 which concludes the paper.  2. Cell Broadband Engine: An Overview  The Cell Broadband Engine (Cell BE), or Cell, is  the result of over four years of development by the STI  consortium made up of Sony, Toshiba, and IBM to  develop a high-performance media multiprocessor  chip.  For Sony, Cell is the central processing unit for  the Sony PlayStation 3 gaming console.  Toshiba plans  to use the Cell for media processing in its highdefinition television products.  IBM provides a Cellbased  server product  for  scientific computing  applications and has a number of future Cell-based  machine designs in development.  In general, STI  developed Cell to rapidly process large amounts of  parallel data.  The processing capabilities of Cell are  made possible by its heterogeneous processors and its  on-chip network architecture.  The Cell BE architecture [32] is a heterogeneous  multiprocessor on a chip  (i.e., heterogeneous  multicore) consisting of 12 core elements connected  together through the EIB.  It integrates one Power  Processing Element  (PPE),  eight Synergistic  Processing Elements (SPEs), one Memory Interface  Controller (MIC), and one bus controller split into two  separate elements (IOIF0 and IOIF1).  The PPE is a  complete  64-bit  IBM  Power  architecture  microprocessor featuring a 32KB L1 and a 512KB L2  cache that operates at the PPE core clock frequency of  3.2 GHz.  The PPE is dual-threaded with in-order  execution.  It contains a VMX execution unit that  supports SIMD and floating-point processing.  Direct  memory  access  (DMA)  and memory-mapped  input/output (MMIO) are both supported by the PPE.  The SPEs are 128-bit SIMD processing units that use a  new VMX-like instruction set architecture (ISA).   Each SPE contains 256KB of Local Store (LS)  memory and operate at 3.2 GHz.  The memory flow  controller (MFC), found in the SPEs, controls the  memory management unit and DMA engine.  DMA is  the only method supported for moving data between  LS and system memory.  The MIC provides memory  access of up to 512MB of RAMBUS XDR RAM.   Finally, the bus controller is a RAMBUS FlexIO  interface with twelve 1-byte lanes operating at 5 GHz.   Seven lanes are dedicated to outgoing data; five lanes  handle incoming data.  The lanes are divided between  the non-coherent IOIF0 and IOIF1 interfaces.  IOIF0  can also be used as a coherent interface by using the  Broadband Interface (BIF) protocol.  The BIF protocol  supports access  to other Cell processors  in  multiprocessor Cell system configurations.  The Cell BE architecture places a number of  requirements on its on-chip network.  The OCN must  support the twelve elements in the architecture, which  is greater  than  the  typical  two-  to eight-node  interconnect  support  needed  by most  other  commercially available multicore processors to date. In  addition, each Cell node is capable of 51.2 GB/s of     Figure 1. EIB Diagram – the Command Bus  component (center) and four data rings.  Figure 2. EIB Diagram – the Data Arbiter  component (center) and four data rings.  aggregate throughput, so the OCN must be able to  support very high data rates.  In contrast, mainstream  multicore processors such as those from Intel and  AMD support far less: typically, less than 15 GB/s per  core.  The Cell design also requires the OCN to  support memory coherent and non-coherent data  transfers.  Finally, the OCN must efficiently support  the network traffic created by typical Cell software  applications.  The Element Interconnect Bus is STI’s solution to  an OCN required by the Cell BE architecture.  Each  element contains a Bus Interface Unit (BIU) which  provides the interface from the element components to  the EIB.  The EIB consists of four 16 byte-wide data  rings (two in each direction), a shared command bus,  and a central data arbiter to connect the twelve Cell BE  elements [33].  The command bus, shown in the center  of Fig. 1, distributes commands, sets up end-to-end  transactions, and handles coherency.  It is composed of  five distributed address concentrators arranged in a  tree-like structure to allow the possibility of multiple  commands to be outstanding at a time across the  network.  The address concentrators handle collision  detection/prevention and provide fair access to the  command bus [39].  Commands propagate up the tree  to AC0 which then provides serial command reflection  back to all elements at a rate of one command per  cycle.  When coherent commands are sent back-toback, AC0 is able to reflect commands at a rate of only  one command every two bus cycles.  The address  concentrators and  the command bus are  fully  pipelined.  The command bus operates at 1.6 GHz,  which is one-half the PPE core clock frequency of 3.2  GHz.  The data rings also operate at the bus clock  frequency of 1.6 GHz.  Each ring is capable of  handling up to three concurrent non-overlapping  transfers, allowing the network to support up to twelve  data transfers at a time across the EIB.  The central  data arbiter is shown in the center of Fig. 2,  implemented in a star-like structure.  It controls access  to the EIB data rings on a per transaction basis.    In the following sections, performance capabilities  of the EIB are evaluated using conventional and  recently proposed network characterization methods.  3. Network Performance Characterization  With the increasing importance of on-chip networks  comes the need for useful methods of characterizing  network performance.  By characterizing network  performance in a consistent manner, comparisons can  be made  between  various  network  designs.   Comparisons facilitate network architecture design  tradeoffs by allowing the designer a clear view of the  advantages and disadvantages of particular network  design choices and, in particular, identification of  potential performance bottlenecks.  In this paper,  conventional and a recently proposed method are used  for characterizing the performance of the Cell BE EIB.  The conventional method of charactering network  performance applicable to OCNs as well as other  network classes is described in [34].  The two basic  performance factors are latency and throughput, i.e.,  effective bandwidth.  Latency is the amount of time  (e.g., cycles or nsec) required for a network to perform  a given action, such as a packet to traveling from one  node to the next (i.e., hop latency) or from its source to  its destination (i.e., end-to-end latency).  The total  latency of a network transaction is the summation of  the latencies of each step of the transaction from  beginning to end.  Throughput is the amount of  information transferred per unit of time (e.g., GB/sec).   Throughput is calculated by dividing network packet                  size by either the transmission time or the overhead  associated with sending or receiving the packet,  whichever is the larger [34].  There are different values  of throughput for various parts of the network end-toend.  For example, each node has a certain amount of  sending (alternatively, receiving) throughput, called  injection (alternatively, reception) bandwidth, which  defines the rate at which it can inject (alternatively,  receive) data into/from the network.  Likewise, the  network core fabric has a certain amount of bandwidth  it can support across its bisection, called the bisection  bandwidth. Following  after  [34],  the overall  throughput of the network is limited by the smaller of  these accumulated bandwidths across  the entire  network.  Network design aspects such as topology,  routing,  switching,  arbitration,  flow  control,  microarchitecture, etc., influence the latency and  throughput performance of the network.  In Section 4,  we analytically evaluate the EIB using the above  conventional methods.  An alternative method of characterizing network  performance, as proposed by Taylor, et al., in [22],  consists of describing the network in terms of a 5-tuple  delay model.  Although originally targeted to scalar  operand networks (SONs), this method can be applied  to OCNs as well as other network classes.  A SON is  an on-chip network designed to transport instructions  and their operands across elements. One example of a  SON is the traditional operand bypass network found  in most  superscalar  pipelined  commercial  microprocessors, while another example is the static  mesh OCN employed by the MIT RAW research  prototype multicore processor [21].    The 5-tuple delay model comprises five costs  presented as <SO, SL, NHL, RL, RO>.  Send  Occupancy (SO) is the number of cycles that a  processing element spends occupied in doing the work  needed to send a packet.  Send Latency (SL) is the  number of cycles a packet spends on the sending side  of the network waiting to be injected into the network  without occupying the processing element.  Network  Hop Latency (NHL) is the number of cycles needed to  transport a message from one node to an adjacent node  in the network.  Receive Latency (RL) is the number  of cycles an instruction needing the data on the  receiving end spends waiting in an issue-ready state  waiting for the remote data to be received and ready  for use.  Receiver Occupancy (RO) is the number of  cycles  the  receiving processing element spends  occupied in making the remote data ready for use.  The  5-tuple delay model is meant to allow for comparisons  of the operation-to-operand matching aspects of a  SON.  For more generalized OCNs, the 5-tuple delay  model can be used as a tool for comparing different  components of network latency from end to end.  In  Section 5, we apply the 5-tuple delay model to the EIB  as an alternative means of evaluation.    4. Conventional Characterization: Latency  and Throughput Analysis  The EIB is a pipelined circuit-switched network  that employs lossless flow control in the form of endto-end command credits.  An element on the EIB must  have a free command credit in order to make a request  on the command bus.  This allows the EIB to have a  simple, deterministic flow control design.  The EIB  allows up to 64 outstanding requests for each element  on the bus.  In order not to bottleneck network  performance, the MIC also supports the maximum 64  requests.  The SPEs, on the other hand, support only  16 requests.   The EIB’s data arbiter implements round-robin bus  arbitration with two levels of priority.  This scheme  works the same for the data arbiter as it does for the  shared command bus.  The MIC has highest priority  while all other elements have lower priority, the same  for each of them.  The data arbiter does not allow a  packet to be transferred along hops totaling more than  half the ring diameter.  That is, a data ring is granted to  a requester only so long as the circuit path requested is  no more than six hops.  If the request is greater than  six hops, the requester must wait until a data ring  operating in the opposite direction becomes free.  As  briefly mentioned above, the command bus uses endto-end command credits, or tokens, for control of the  bus.  Elements use tokens to post commands to the  command bus.  An element may send only as many  commands as it has tokens.  Each token is held by the  command bus until  the associated command  is  complete, at which point the token is returned to the  element.    The Cell BE has a software-controlled Resource  Allocation Manager that allows applications to manage  EIB resources in software.  The Resource Allocation  Manager is an optional feature that allocates the usage  of resources to prevent individual elements from  becoming overwhelmed.  It divides memory and IO  resources into resource banks and separates requestors  into groups.  Software controls access to each group  again using tokens.   4.1. Latency Estimates  Given the above mechanics, the EIB network  performance can be characterized in terms of packet  latency and effective bandwidth (throughput).  The          EIB supports two types of data transfer modes: DMA  and MMIO.   The overall network  latency  is,  essentially, the same for both transfer modes.  The  latency of a packet through the network end-to-end can  be calculated using the following equation:   Latency = Sending Overhead + Time of Flight +  Transmission Time + Receiver Overhead  (1)       EIB transactions can be divided into four phases:  the Sending Phase, the Command Phase, the Data  Phase, and the Receiving Phase.  The Sending Phase is  responsible for initiating a transaction.  It consists of  all processor and DMA controller activities needed  before transactions are injected into any components  within the EIB.  At the end of the Sending Phase, a  command is issued to the command bus to begin the  Command Phase.  The Command Phase coordinates  end-to-end transfers across the EIB.  During the  command phase, the EIB informs the read or write  target element of the impending transaction to allow  the target to set up the transaction (i.e., data fetch or  buffer reservation).  The Command Phase is also  responsible for coherency checking, synchronization,  and inter-element communication (i.e., injection into  BIUs).  The Data Phase begins at the conclusion of the  Command Phase.  The Data Phase handles data ring  arbitration and actual data transfers across the rings.   Since end-point setup was completed during the  Command Phase, the Data Phase is left with the task of  granting access to one of the four data rings to packets  when a ring becomes free and no more than six hops  are needed along the ring.  Pipelined circuit-switched  transport of packets occurs end-to-end over the granted  EIB ring.   The Receiving Phase concludes the  transaction by transferring received data from the  receiving node’s BIU to its final destination at that  receiver, such as LS memory or system memory.       With these four phases, the latency of packets across  the EIB can be calculated using a revised version of  the network latency equation above, where Sending  Overhead is composed of the Sending Phase and  Command Phase, and Time of Flight and Transmission  Time combine to form the Data Phase:  Latency = Sending Phase + Command Phase +  Data Phase + Receiving Phase  (2)  4.1.1. Best-case Latency  In the following, we calculate the number of  processor cycles needed to execute the communication  phases above for DMA transfers based on the required  number and type of instructions needing to be executed  and the microarchitecture of the elements executing  them.  Ideal best-case scenarios are assumed for  simplicity, leading to lower-bound latency estimates.   For example, the EIB is assumed to be unloaded— meaning that all queues are empty and no additional  waiting due to resource contention or otherwise is  incurred during arbitration.  Also, no processor stalls  are assumed to occur in executing instructions, i.e., due  to cache misses or any other type of anomaly.    The Sending Phase of an EIB transfer is made up of  the following: processor pipeline latency, DMA setup,  and DMA controller processing.  Both the PPE and  SPE have 23-stage processor pipelines, yielding a  pipeline latency of 23 processor clocks.  The next step,  DMA setup, requires 5 instructions to begin a DMA:  1) Write SPE Local Store Address, 2) Write Effective  Address High (upper 32-bits), 3) Write Effective  Address Low (lower 32-bits), 4) Write DMA size, and  5) Write DMA command (send, receive, etc).  Since  the DMA controller operates at the bus clock rate,  these five instructions can complete in 10 processor  clocks.  Once the DMA is issued, the DMA controller  takes an additional 20 processor clocks to select the  command, unroll  the bus  transaction, send  the  command to the BIU, and then issue it to the command  bus [35], [36].  Thus, the Sending Phase takes a total  of 53 processor clock cycles, which is 26.5 bus cycles.  The Command Phase consists of five steps [37, 38]:  command issue, command reflection, snoop response,  snoop combining, and final snoop response.  The  length of the command phase depends on whether the  transaction is coherent or non-coherent.  The EIB  master initiates a transaction during the command issue  setup, which  takes 11 bus cycles for coherent  commands and 3 bus cycles  for non-coherent  commands.  Commands are issued from a BIU and  sent through the address concentrators to the command  reflection point at AC0 [39].  The command reflection  step depends only on the distance an element is from  AC0.  To AC1, command reflection takes 3 bus cycles.   One bus cycle is added for elements connected to AC2  for 4 cycles of command reflection.   Elements  connected to AC3 add 2 bus cycles for a total of 5  cycles of command reflection to AC3.  Responses to  the reflected commands from each of the BIUs are sent  to AC0 during the snoop response step, which takes 13  bus cycles, regardless of coherency.  In the snoop  combining step, AC0 processes the responses into a  combined message which takes 9 bus cycles for  coherent transactions and 5 bus cycles for non-  coherent transactions.  The final snoop response  distributes the combined message to all elements and  ends the Command Phase.  The final step requires 3  bus cycles for AC1 elements, 4 for AC2 elements,           Table 1.  EIB zero-load packet latency estimates: best-case, lower-bound results.   Sending Phase  (29% of C)   (33% of NC)  Command Phase  (47% of Coherent Total)  (39% of Non-coherent Total)  Data Phase  (22% of Coherent Total)  (25% of Non-coherent Total)  Receiving  Phase  (2% of C)  (3% of NC)   xPE  Pipe.  Latency  C C N 11.5  11.5  xPE  Issue  to  Queu e  5  5  Issu e to  Cmd  Bus  10  10  Cmd  Issu e  11  3  Cmd  Refl.  to  AC3  5  5  Snoop  Resp.  Snoop  Comb  Final  Snoop  to AC3  Data  Req.  to  Arbiter  Data  Arb.  Data  Grant  Time  of  Flight  Trans . Time  Overhead  13  13  9  5  5  5  2  2  2  2  2  2  6  6  8  8  2  2  Totals  Bus  Clock  Cycles  91.5  79.5  and 5 for AC3 elements.  In total, the Command Phase  minimally requires 43 bus cycles for coherent  transactions and 31 bus cycles for non-coherent  transactions.   The Data Phase consists of data arbitration steps,  data propagation, and transmission time [38].  The  Data Phase latency is independent of the coherency of  the data.  A transaction takes 2 bus cycles to make a  request to the data arbiter, another 2 cycles for data  arbitration and, assuming no contention, 2 more cycles  to be granted access to a data ring by the data arbiter.   Time of flight of a DMA transfer after arbitration is  limited to a maximum of six hops by design (half the  network diameter).  Each hop takes one bus cycle.   Therefore, the worst-case time of flight is 6 bus cycles  after arbitration.  To make the EIB more deterministic,  all packets are designed to be 128 bytes in size.  Since  each data ring of the EIB is 16 bytes wide, every  transaction takes 8 bus cycles to complete.  Therefore,  transmission time is 8 bus cycles.  All together,  arbitration, worst-case time of flight, and transmission  time take at total of 20 bus cycles to complete.  The Receiving Phase takes only 2 bus cycles for the  SPEs and PPE.  The first cycle is used to move data  from the BIU to the MFC.  Conservatively, the DMA  engine in the MFC can write the 128 byte data packet  into LS memory in the second bus cycle.    The end-to-end network latency for packets is  obtained by summing each of the phases together. A  non-coherent transfer minimally takes 79.5 bus cycles  or 49.6875 nsec to complete.  Coherent transfers  minimally require 91.5 bus cycles or 57.1875 nsec to  complete.  Table 1 summarizes the EIB latency  calculations described above.  Again, we emphasize  that the latency calculations should be considered as  best-case, lower-bound estimates.  Any bus arbitration  time, network contention, stalls, or other delays will  directly impact the packet latency across the EIB.  4.1.2. Worst-case Latency  Using the maximum request queue size of the EIB  elements as an upper bound on outstanding  transactions, we determine a worst-case  latency  estimate for an EIB transaction.  The latency for each  phase from Section 4.1.1 does not change, but there is  an additional delay step before the Command and Data  phases, as well as a final delay step before completing  the transaction.  As above, the latency count begins  once the command is issued by a processor element.   Additional delays such as cache misses and pipeline  stalls are not included in this worst-case estimate.  The number of outstanding requests determines the  additional delay before entering the command phase.   There are a total of 240 request slots available across  all other elements: 64 for the MIC and 16 for the  eleven other elements.  Since each request uses two  slots for both the sender and the receiver, there are a  total of 120 outstanding transactions that could be in  the queue.  Using round-robin arbitration and a rate of  one command every two clock cycles [39], the  maximum delay before the Command Phase is invoked  is 240 bus clock cycles.  A transaction in the Data Phase may encounter  additional delay due to resource contention on the EIB  as discussed in Section 4.2.  For example, if all of the  SPEs are trying to write to the same element then the  data arbiter can process only two commands at once  because each element has only two input ports.  With a  worst-case data arbitration of 1 command every 4  clock cycles, the delay before the Data Phase is  invoked is 480 bus clock cycles.  Finally, in the Receiving Phase, there is another  delay.  The MFC can complete only one DMA request  every 8 clock cycles due to conflicts with the processor  fetching data and instructions.  Since there may be as  many as 16 requests waiting to complete in the MFC,  the final Receiving Phase delay is 128 bus clock  cycles.  The worst-case estimates are combined in                  Table 2.  The total worst-case EIB latency is 939.5 bus  clock cycles or 587 nsec.    Table 2.  EIB maximum-load packet latency  estimates: worst-case, upper-bound results.   Sending  Phase  Command  Phase  Delay  Data  Phase  Delay  Receiving  Phase    Delay  Totals  s e c y l C s u B 26.5  240  43  480  20  2  128  939.5  4.1.3. Other Latency Considerations  MMIO transfers essentially have the same latency  as DMA transfers as they require the full Command  and Data Phases mentioned above.  All MMIO  transfers also are coherent, meaning that some DMA  transfers will actually have a  lower  latency.   Regardless of their latency, MMIO transfers are far  less efficient than DMA transfers as every EIB transfer  assumes a packet size of 128 bytes, whether or not the  full width is needed.  DMA transfers use all 128 bytes,  but MMIO transfers only use 4 or 8 bytes, resulting in  significant under-utilization of the network bandwidth.  One major advantage of the EIB is its ability to  pipeline DMA data transfers.  The data arbiter can start  a transfer on every bus cycle and each data ring  supports a new operation every three cycles.  This  allows a single element to continuously pipeline  transmissions on the EIB assuming there is low  network contention, i.e., no conflicting overlaps of  source-destination paths over a ring.  The overlap  provided by pipelined data  transfers  reduces  sender/receiver overhead, and increases the effective  bandwidth when considering the command phase and  the data arbitration and propagation portions of the  data phase (as we show below).  Through pipelining,  there are only 8 bus cycles or 5 nsec of latency  between DMA transactions, equating to the 8 bus  cycles of transmission time. Thus, since there are no  wasted cycles between transmissions, the data rings  can be considered as running at 100% efficiency in this  case.     4.2. Effective Bandwidth Estimates  The maximum effective bandwidth (throughput)  required by a network design is determined by  summing across all  interconnected elements  the  maximum data rate at which packets can be injected  (alternatively, received) at each element. Each element  in the EIB is theoretically capable of sending and  receiving 16 bytes per cycle, which equates to 25.6b  GB/sec  of  injection  (alternatively,  reception)  bandwidth.  The total bidirectional effective bandwidth  of an element is, therefore, 51.2 GB/sec.  In order to  accommodate all twelve elements, the EIB should be  capable of providing a total throughput of 307.2  GB/sec, based on the effective bandwidths of each  element.  The EIB, as designed, indeed provides this in  terms of raw network bandwidth.  It supports a  maximum of three concurrent transfers on each of its  four 16-byte rings.  Each ring can begin a new transfer  every three cycles, and all transfers are eight cycles or  128 bytes long, which means that when the data rings  are fully utilized, the EIB supplies a peak effective  bandwidth of 307.2 GB/sec.  Thus, theoretically, the  EIB fully supports the bandwidth requirements of the  twelve  elements  in  the Cell BE processor.   Unfortunately, as we show below, the EIB suffers from  a number of limitations that, realistically, result in  much lower effective bandwidth in the best case.  The EIB faces the same throughput limitations as  all networks.  For example, if a single element is in  high demand, network throughput will be limited by  the response of the most heavily-loaded element or set  of links along the path to that element.  In the Cell  processor, the MIC serves as such a heavily utilized  element as most SPE/PPE threads require regular  access to system memory.   In addition to this  challenge, the design of the EIB poses additional  limitations on effective bandwidth, as described below.  The first major limitation on EIB throughput is that  which is imposed by the command bus.  The command  bus is severely limited in the number of bus requests it  can process concurrently. It is limited to one request  per bus cycle for non-coherent transactions; for  memory coherent transactions, the command arbiter  can handle only one request every two bus cycles [39].   Thus, the non-coherent data rate is limited to 204.8  GB/sec, and the coherent data rate is limited to 102.4  GB/sec due to the shared command bus posing as a  major bottleneck. All system memory commands such  as MIC and PPE cache  transfers are coherent  commands.  Some I/O commands, i.e., those typically  for multi-Cell applications, are also coherent.  Thus, a  large fraction of EIB transactions (depending on the  application) make use of only 33% of the raw effective  bandwidth provided by the EIB network. The effect of  coherency on EIB throughput need not be linear,  however.  As long as coherent transactions are limited  to one command every other clock cycle, there is no  loss due to coherency.  However, if two or more  coherent  transactions enter  the command queue  consecutively,  there will be bandwidth  losses.                               Figure 3.  Non-conflicting traffic pattern for the  high-efficiency transfer test.  Figure 4.  Conflicting traffic pattern for the lowefficiency transfer test.  Fortunately, SPE to SPE transfers are non-coherent,  but even for these EIB transactions, only 66% of the  raw effective bandwidth provided by the EIB network  is usable.  Thus, regardless of the additional limitations  caused by coherent transfers, the design of the  command bus prevents applications from being able to  exploit the 307.2 GB/sec effective bandwidth provided  natively by the four EIB data rings, even under bestcase operational scenarios. That is, in its current  implementation, the EIB is unable to handle the full  injection bandwidth of all twelve of the Cell elements.  The choice of a ring topology for the EIB serves as  another possible design limitation in achieving full  injection bandwidth.  In order to support three packet  transfers per ring, the transfers must occur over nonoverlapping source-destination paths.  If a transaction  traverses an element, that element is unable to perform  another transfer on the same data ring until the first  transaction completes.  A maximum transfer distance  of six hops potentially prevents up to five elements  from being able to access a data ring.  In [5], IBM  conducted a series of throughput tests using noncoherent transfers between SPEs to measure effective  bandwidth under various conflicting and nonconflicting traffic scenarios over the data rings.   Depending on the location of sources and destinations,  those measurements show that throughput can range  from 96% of the command bus-limited 204.8 GB/sec  (realistic) maximum down to 38% of that maximum.  Fig. 3 shows the traffic pattern used for the highefficiency test that resulted in a throughput of 197  GB/sec.  As illustrated, since no ring contention exists  and all transfers are non-coherent, the command bus  maximum of eight concurrent ring transfers can be  achieved (note: this is less than the twelve concurrent  ring transfers provided natively by EIB without the  command bus limitation).  Fig. 4 illustrates how  conflicted overlapping  transfers  (i.e.,  transfers  requiring the same links of a ring) limit the EIB to only  one transfer per ring, thus further reducing the  effective bandwidth down to 78 GB/sec.  Coherent  transfers would further throttle the network bandwidth.  Finally, the EIB effective bandwidth can be further  reduced due to under-utilization resulting from less  than full packet transfer size.  The choice of optimizing  the design for wide packets of 128 bytes can greatly  diminish the utilization of allocated bandwidth when  only a fraction of that packet size is actually used.  For  example, MMIO with its 4 or 8 byte transfers is the  greatest culprit of bandwidth under-utilization.    Fortunately, most transfers in the Cell BE are in  units of 128 bytes, since the PPE uses 128 byte cache  lines and the SPEs use 128 byte memory blocks.   Under-utilization is a challenge for most network  architecture designs.   However,  this  issue  is  exacerbated in the EIB as poor packet efficiency also  increases data ring contention, which reduces the  achievable effective bandwidth of the network.  5. An Alternative Characterization: The 5Tuple Delay Model  The 5-tuple delay model proposed in [22] serves as  an alternative method of characterizing network  latency performance.  Building from the best-case  latency components given in Section 4.1, we assign  values to the 5-tuple parameters as described below.    Both the SPEs and the PPE take 5 bus cycles to  start a DMA transaction, thus the Send Occupancy of  the EIB is 5 cycles.  Send Latency includes the time it  takes the DMA controller to process the DMA, the  entire command phase, and the data phase up to the  point at which data is ready to be sent over the EIB  data ring.  This gives a total Send Latency of 59 bus  cycles for coherent transfers and 47 bus cycles for noncoherent transfers. The EIB requires only a single  cycle per hop between EIB elements, yielding a  Network Hop Latency of 1 bus cycle.  There is some  difficulty in applying the 5-tuple delay model at the  receiving side of the EIB.  Receive Latency on the EIB  can be 1 bus cycle if the DMA transaction has  completed before the processing element accesses the  data.  That clock cycle is used to load the data from the  Local Store into the processing element’s register file.                  Table 3. The 5-tuple delay model for the EIB in comparison to other networks.  Cell BE  EIB  Coherent  Cell BE  EIB Noncoherent  Typical  Super-scalar  bypass  network [22]  5  59  1  >= 0  >= 0  5  47  1  >= 0  >= 0  0  0  0  0  0  Distributed  Shared  Memory [22]  1  14+commit  latency  2  14  1  Software  Message  Passing  [22]  3  3+commit  latency  1  1  12  M.I.T.  Raw  [22]  M.I.T.  RawDynamic  [22]  Power4  bypass  network  [40]  0  1  1  1  0  0  2  1  2  0  2  14  0  14  4  Tuple Costs  Send  Occupancy  Send Latency  Network hop  latency  Receive  latency  Receive  occupancy  U.T.  Austin  TRIPS  EDGE  [23]  0  0  1  0  0  However, in the worst-case, the processing element  may need to wait for an entire DMA to start, which can  take over 90 bus cycles.   Similarly, Receive  Occupancy is zero cycles if the DMA has completed  and the data is in memory since there is no additional  penalty for using remote data found in the LS.  If the  data is not yet in the LS, then the Receive Occupancy  cost would be significantly higher.  The high degree of abstraction of the EIB in  matching operations to operands makes it difficult to  compare directly to true SONs.  The EIB is designed to  handle data at the system memory level, whereas  SONs are designed to handle data at the operand level.   Due to its higher level of abstraction, the EIB naturally  will have higher 5-tuple costs than a typical SON.   Nevertheless, Table 3 shows how the EIB compares  with other types of networks.  In the table, commit  latency is the cycle time it takes for an instruction to  commit as  there may be many cycles between  execution and the commit point of an instruction in  modern out-of-order, speculative processors.  Not  surprisingly, the EIB has a much larger 5-tuple delay  cost than the superscalar bypass networks and SONs  used to transport data operands.  However, the EIB  compares reasonably well with distributed shared  memory and message passing networks, both of which  operate at the system memory level of abstraction.  The high send latency cost of the EIB reveals a  scalability issue with the EIB.  While the data rings  have an efficient 1 cycle per hop latency, the shared  command bus takes a long time to synchronize all EIB  interfaces.  Given the longer latency, the EIB will have  problems scaling beyond twelve nodes.  In fact, it is  likely that more nodes would require even more time  during the command phase for synchronization, which  would lead to an even higher Send Latency.   The 5-tuple delay model analysis serves the useful  purpose of highlighting the impact that software has on  EIB utilization.  If a Cell programmer carefully plans  out memory transfers, Receive Latency and Receive  Occupancy can be much lower and more in line with  those of  the SONs.   However, poor software  programming could cause very large latency costs for  the EIB if not tuned correctly, as discussed in the next  section.  6. Software Effects on the EIB  Estimating  the packet  latency and effective  bandwidth values provide only part of the story when  evaluating network architectures.  Eventually, the  applications that are to run on the network must be  considered.   Sections 4 and 5 characterize  the  performance of the EIB only analytically as cycleaccurate simulation over the EIB cannot be performed  using the publicly available Cell BE simulator [41].   Without considering the applications running on the  Cell BE system, including its software optimizations,  the EIB network cannot be fully evaluated.    Developing software for the Cell processor is the  greatest challenge for Cell-based system design.  This  heterogeneous multicore  processor  requires  a  substantially different programming paradigm.  IBM  has written a five-part guide to provide an introduction  to programming and compiling  for  the Cell  architecture [42].  From the network perspective, the  challenge for software is to balance the workload on  the Cell processor with the strengths and limitations of  the EIB.  In general, EIB transactions have significant  latency.  To complicate matters further, the latency is  not deterministic and varies based on contention and  the transfer paths of the traffic.  Thus, software  development for the processor needs to take EIB  latency and command bus bandwidth limitations into  account.  While software teams would like to treat all  SPEs the same, Section 4 shows that SPE transfer  paths can drastically affect EIB performance.  There are three main application models for the Cell  BE [43], [44].  The job queue model uses the PPE as  the central processor which maintains a job queue and          sends tasks to the SPEs.  This model has lower EIB  utilization than the other models since latency is of less  concern.  However, this model achieves only moderate  performance as the SPEs are not fully utilized.  The  self-multitasking model is a distributed application  model that operates across the SPEs.  It is a highperformance application model, but it has increased  synchronization requirements which place additional  stress on the EIB.  The stream processing model is also  SPE based.  In this case, each SPE runs a single  application and processes one part of the data stream.   This model achieves high-performance as well, but it  also highly utilizes the EIB.  Of the three models, we  believe analysis would show the stream processing  model  to be  the best at balancing the latency  limitations and bandwidth strengths of the EIB.  As mentioned previously, EIB transactions have  significant latency which has to be handled by  software.  By using the enormous bandwidth capacity  of the EIB, Cell software would be able to greatly  reduce average EIB latency.  For example, by prefetching data, bandwidth can be traded off to avoid the  latency of unnecessary stalls.  Software pipelining can  also be helpful, but unpredictable access patterns may  limit its effectiveness.  Double buffering is used to  stream data through the SPE LS, bypassing latency  effects.  Since the SPEs lack caches, a software cache  can be used instead to handle workloads with temporal  and spatial memory locality.  By using latency hiding techniques, software may  be better able to utilize the abundant bandwidth  provided by the EIB.  However, there are some pitfalls  that software should avoid to achieve optimal EIB  performance, such as under-utilization of packets.  Cell  uses a mailbox communication model with special  registers that allow for communication between the  PPE and SPEs [45].  Some of these mailbox channels  use MMIO which limit bandwidth utilization.  In  general, MMIO should be avoided as much as  possible.  Another problem area is SPE management:   SPEs are capable of context switches when necessary,  but context switches  take approximately  twenty  microseconds to complete, which is more than 33,000  bus cycles.  The SPEs should always finish program  execution to avoid this severe context switch penalty.  7. Future Research  One area of future research is to characterize typical  Cell application code and its related EIB utilization  using a cycle-accurate Cell simulator or actual Cell  hardware.  The Sony Playstation 3 and various Cellbased Linux distributions would be possible test  platforms for such research.  With such a platform, it  would be possible to quantify the impact of software  alluded to in Section 6.  Without simulation or  hardware testing, the effects of software can only be  estimated, as is done here.  Such tasks would require  Cell code development which implies another set of  issues, but with the use of a sound test platform, the  effectiveness of the EIB could be better understood.  Another area of research (possibly already in the  works at IBM) is to consider design improvements for  a second generation EIB.  In [46], it is pointed out that  Cell is architected to allow the EIB to be easily  replaced with a different network, such as a crossbar  switch.  It also mentioned that a crossbar switch would  take too much die space to be practical in the first Cell  processor.  When IBM moves Cell to the next process  generation, from 90 nm to 65 nm, it may be possible to  include a richer network, such as a crossbar or higherdimensional switched network.  Improvements to endto-end control of the network would certainly help, no  matter what the network.  Again, such design upgrades  could be tested using a cycle-accurate simulator.  8. Conclusion  The Element Interconnect Bus  is an on-chip  network design that faces a number of interesting  challenges. This paper attempts to identify a number of  these challenges by characterizing  the network  performance using conventional and recently proposed  methods based on analytical analysis.   Network  performance characterization in traditional terms of  packet latency and effective bandwidth is a good  starting point to understanding OCN design choices,  but deeper analysis can be performed in order not to  evaluate the network in a vacuum.  The entire system,  including application characteristics and software  optimizations, should be considered in future work to  characterize the network more accurately.  Our findings reveal that the EIB suffers from  latency and bandwidth limitations imposed by the  mechanism used  for end-to-end control. The  complexity of  the EIB design necessitates  long  Sending and Command Phases implemented over a  shared  command  bus  that  provides  limited  concurrency.  This greatly increases the packet latency  and reduces the achievable effective bandwidth of the  EIB.  Although the data rings provide sufficient  effective bandwidth, the command bus severely limits  the use of that bandwidth.  This result highlights the  importance of not overlooking control and other endto-end effects when designing on-chip networks.           Despite these limitations, the EIB is still capable of  an effective bandwidth in excess of 100 GB/s, which  compares  favorably  against  current multicore  processors.  The 5-tuple delay model shows that the  EIB is similar to other OCNs that transport memory  data but that the EIB will not scale well beyond its  twelve element design due to the command bus  architecture. It also highlights the design tradeoffs  made by EIB designers requiring  that software  programmers strike a delicate balance when writing  applications on Cell systems.  In order to achieve the  best performance, Cell software applications should  make use of the relatively large bandwidth provided by  the EIB to overcome the long latencies of the network,  while being careful not to underutilize nor over  subscribe that bandwidth.  If properly utilized by Cell  software, the EIB can achieve near superscalar bypass  network latencies at the receive end of the network.   10. Acknowledgements  We acknowledge the help and advice of Scott  Clark, Mike Kistler, and Fabrizio Petrini in clarifying  various aspects of EIB design features and operation.   This work was supported, in part, by NSF grants,  CCR-0311742 and CCF-0541200.  11. "
An Analytical Approach for Dimensioning Mixed Traffic Networks.,"We present an analytical method for analyzing and dimensioning a network based communication architecture. The method is based on the classic (sigma, p) network calculus. We use a TDMA approach for creating logically separated networks which makes statistical methods possible for calculations on best effort traffic, and supports implementation of guaranteed bandwidth services by using virtual circuits with looped containers","An analytical approach for dimensioning mixed trafﬁc networks Per Badlund Royal Institute of Technology Electrum 229, 16440 Stockholm, Sweden pbadlund@kth.se Axel Jantsch Royal Institute of Technology Electrum 229, 16440 Stockholm, Sweden axel@kth.se apply statistical methods for calculation of Best Effort performance. Acknowledgment I would like to thank the sponsors of my project: Saab AB, Saab Avitronics AB FMV (Swedish Defence Material Administration) and the industrial graduate school SAVEIT. "
A Power and Energy Exploration of Network-on-Chip Architectures.,"In this study, we analyse the move towards networks-on-chips from an energy perspective by accurately modelling a circuit-switched router, a wormhole router and a speculative virtual-channel router in a 90nm CMOS process. All the routers are shown to dissipate significant idle state power. The additional energy required to route a packet through the router is then shown to be dominated by the data-path. This leads to the key result that, if this trend continues, the energy cost of more elaborate control would not be vast, making it easier to justify. Given effective clock-gating, this additional energy is also shown to be more or less independent of network congestion. Accurate speed and area metrics are also reported for the networks, which would allow a more complete comparison to be made across the NoC architectural space considered","A Power and Energy Exploration of Network-on-Chip Architectures Arnab Banerjee, Robert Mullins and Simon Moore Computer Laboratory, University of Cambridge Arnab.Banerjee@cl.cam.ac.uk Abstract In this study, we analyse the move towards Networks-onChips from an energy perspective by accurately modelling a Circuit-Switched router, a Wormhole router and a speculative Virtual-Channel router in a 90nm CMOS process. All the routers are shown to dissipate signiﬁcant idle state power. The additional energy required to route a packet through the router is then shown to be dominated by the data-path. This leads to the key result that, if this trend continues, the energy cost of more elaborate control will not be vast, making it easier to justify. Given effective clockgating, this additional energy is also shown to be more or less independent of network congestion. Accurate speed and area metrics are also reported for the networks, which will allow a more complete comparison to be made across the NoC architectural space considered. 1. Introduction It has been suggested that Networks-on-Chips (NoCs) form the only efﬁcient and scalable solution to providing global on-chip communications in any large VLSI design. Simultaneously, power dissipation issues have grown to such importance that they now constrain attainable performance. If NoCs are to be accepted, their full power implications need to be known. Moreover, these power characteristics must be accurately understood across the large possible design space of NoCs. As Section 2 shows, only high-level models aiming for fast NoC power evaluations are currently available. The contribution of this study is not therefore to provide a quick, high-level modelling tool but instead to provide detailed and accurate results on a range of NoC architectures. As outlined in Sections 3 and 4, three different networks, spanning a large range of NoC architectural families — a circuit-switched network, a wormhole network and a speculative, single cycle virtual channel network — were selected for this study. Complete Hardware Description Language (HDL) models of these networks were then synthesised, placed and routed using a standard ASIC tool ﬂow, with a 90nm, high-performance TSMC process. Thereafter, extracted parasitics allowed accurate power and energy ﬁgures to be obtained for a variety of experiments, outlined in Section 6. The deeper understanding of power dissipation mechanisms provided by these results then had strong implications for low power NoC design. These data were complemented by reporting area and critical path delays of each router to give a more complete characterisation of each design. Finally, several future research directions are clearly visible and are outlined in Section 7. 2. Related Work 2.1. Network on Chip Architectures A thorough review of a range of NoC architectures has already been presented by Kavaldjiev and Smit [5] and Mello et al. [6]. Instead of repeating these works, only a brief summary of the reported ﬁndings is presented below. Dielissen et al. have presented the proven technology of Philips’ Æthereal NoC [4]. This work stresses the need for both ﬂexibility and performance guarantees in across-chip communications. Two trafﬁc classes are proposed: guaranteed throughput (GT) and best efforts (BE). The GT trafﬁc is forwarded using a connection-oriented methodology, whereas wormhole routing deals with the BE trafﬁc. Mullins et al. have presented a virtual-channel NoC router design which concentrates on efﬁcient utilisation of the global links while providing very low latency communications [8]. A mesh architecture with 5-input × 5-output port routers was designed. To enable low latencies, both virtual-channel and crossbar allocation are performed speculatively. Combining this with the lookahead routing technique1 , allowed a single cycle ﬂit2 forwarding delay per router to be achieved. Wolkotte et al. advocate a more static allocation approach, which is more suited to the highly streaming ap1 The routing calculation for each router is performed in the previous hop router. 2A sub-unit of a communications packet, a ﬂit forms the smallest unit of ﬂow-control. plications they consider [10]. The complexity of a dynamic packet-switched network seemed unnecessary and a circuitswitched network was developed instead. A 5×5 port router was used, along with a Space Division Multiple Access (SDMA) scheme, where each inter-router link is divided into a number of lanes. A 16 × 20 crossbar, conﬁgured using a local conﬁguration memory provided full connectivity between all the channels. Beyond this small selection, the interested reader is pointed to the tutorial by Dally and Towles [3] which presents a detailed study of many more NoC architectures. 2.2. Existing NoC Characterisations Peh et al. developed insightful high-level power models for a set of NoC router components and used these to estimate the power consumption of various wormhole and virtual-channel NoC architectures [9, 2] . Although such high-level models may provide valuable power estimates early in the design cycle, the accuracy limitations of the high-level results and the number of components currently modelled restrict the usefulness of these data in network design. A similar power measurement methodology is provided by Xi and Zhong [11]. High-level power models were again derived and then embedded into a Transaction Level Modelling (TLM) based simulation framework. The authors also extended the work into future technology nodes using the Berkeley Predictive Technology Models. This allowed key predictions to be made for future technologies, but the questionable accuracy of the high-level models still remains a problem. Banerjee et al. attempted to increase the accuracy of power results by ﬁrst extracting a SPICE level netlist from a synthesised design [1]. This was then used to develop power models for NoC router components which then provided power results under random trafﬁc simulations. However, only a limited range of architectures were used which limited the results that could be obtained. Mullins built on the accuracy front by fully synthesising and performing place and route on a speciﬁc router design before utilising the extracted parasitics to analyse power consumption [7]. This was the ﬁrst paper to provide power ﬁgures at this level of accuracy and was thus clearly able to demonstrate the importance of low-level ideas such as clock gating. The current limitation of this work is the single architecture utilised. A comparative study is clearly needed to be able to judge the quality of the design. ables determine the ultimate results. Ideally, to fulﬁl the aims of this wide-ranging study, all such parameters should be varied to determine their impact on the power results. However, a large time investment was necessary to perform the low-level simulations aimed at providing accurate results. It was, therefore, important to select representative values for all parameters to allow useful and comparable results to be extracted. This logic was used throughout this work to select the values of several parameters and these arguments are, therefore, not repeated every time. There are, however, some parameters which do not have just a single representative value. The large number of network architectures highlighted in Section 2.1 clearly indicates how the network architecture represents one such parameter. The challenge was therefore to select a small number of test cases that were representative of a large family of network architectures. The three networks listed below were ﬁnally selected, for the indicated reasons. 1 The circuit switched network (from now on referred to as the CS network) presented by Wolkotte et al. [10] presents an extreme case, with a very simple and static router data-path and no inherent control. Given the correct trafﬁc conditions (highly predictable, streaming trafﬁc), it is intuitively expected to be a very low power design. This design is hoped to represent the set of networks that advocate static allocation of resources and simplicity against highly dynamic techniques. 2 To compare against the other end of architectures, the speculative, single cycle, virtual-channel design presented by Mullins et al. [8] (referred to from now on as the SpecVC network), was selected as the second test case. Each router contains a large amount of allocation logic, which attempts to provide good resource sharing, while minimising latencies. 3 Having considered the two extremes of the architecture scale, a mid-way design was desired to provide comparisons to the opposing extremes. The currently widely advocated wormhole ﬂow control based router architecture was selected for this purpose (referred to from now on as the WH network). This design does perform dynamic allocation of resources, but not at the cost of highly complex allocation methods. Taken together, we believe that these networks will fulﬁl the aim of gathering power results across a wide range of network architectures. 3. Selection of NoC Test Cases 4. Design of Selected NoCs As with any characterisation study, it is not trivial to obtain meaningful power ﬁgures, as a large number of variThis Section provides a brief overview of the network architectures used. All the networks were based on a mesh topology with 5-input×5-output port routers, with 4 ports connecting to the neighbouring routers and the ﬁfth one connecting to a local computation tile. All ﬂits provided a 64-bit data payload size, with additional control bits necessary for the WH and SpecVC designs. The WH and SpecVC networks also utilised a static, dimension-ordered, XY routing scheme. 4.1. Circuit Switched Router Design The CS router provided a very simple data-path, being composed only of a crossbar with registered outputs. Each output port is 64-bits wide, since no control data is necessary. To provide more ﬂexibility, each 64-bit output port is split into four, 16-bit wide, lanes. Given the 5-port design, 20 input and output lanes therefore exist. Each input lane can then be connected to any output lane apart from the ones on the same side of the router (i.e. no ﬂit U-turns are allowed), using the 16×20 crossbar. The conﬁguration memory therefore provides a 20 entry capacity (1 for each output lane), with 5-bits per entry (4 address bits to identify an input lane and 1 valid bit). The splitting of a 64-bit ﬂit into 16-bit units for transport over the network means that a serialising and de-serialising unit is necessary at the tile interface of the router. The completely static nature of the CS network means that a separate control network is necessary to provide all circuit set-up and tear-down functions. To model a scalable solution for this, a very simple packet switched network, roughly based on wormhole ﬂow control was provided. All experiments then considered both the circuit-switched and packet-switched routers, to account for the necessary overhead of the packet-switched network. Figure 1 shows the complete structure of the CS router. Further details of this design have been reported by Wolkotte et al. [10]. 4.2. Virtual Channel Router Design The SpecVC router provides for single cycle ﬂit forwarding by utilising lookahead routing and speculative VC and crossbar allocation. A conventional input-queued architecture with 4 VCs per port and 4 ﬂit deep buffers for each VC were also used. Associated head pointer and tail pointer registers then referred to the ﬁrst and last item in the FIFO respectively. Each ﬂit identiﬁes its VC by using a one hot encoded 4bit VC identiﬁer. The lookahead routing scheme means that each ﬂit additionally carries a 5-bit, one hot encoded nextport identiﬁer used by the downstream router. Moreover, the architecture does not provide for a separate head ﬂit and every ﬂit therefore identiﬁes its destination X and Y address (4-bits each) and carries an additional single bit to indicate whether it’s a tail ﬂit or not. Combining with the 64-bit data-path results in a total ﬂit size of 82-bits. Both the VC and switch allocators are based on matrix arbiters and can allocate VCs and crossbar ports speculatively for the next clock cycle if necessary. Since both crossbar and link traversal are performed in a single clock cycle, in the best case, an incoming ﬂit ﬁnds pre-allocated resources and can thus be forwarded to the next hop in a single clock cycle. State-holding elements indicate which VCs are currently being used, to ensure they are not re-considered for allocation. A passing tail ﬂit then de-allocates this resource. Additionally a simple stop-go ﬂow control method is utilised to prevent buffer overﬂow, where a single buffer nearly full signal is output by each input VC to the corresponding upstream VC. Figure 1 shows the structure of this router, with further details having been provided by Mullins et al. [8]. 4.3. Wormhole Router Design The WH router can be considered to be a simpliﬁed version of the SpecVC router. To represent a comparable design to the SpecVC router the techniques of lookahead routing and combining the crossbar and inter-router link traversal into a single stage were again utilised. Removing speculation from any allocation then naturally resulted in a two stage pipeline where the switch allocator, again based on matrix arbiters, grants an output port in the ﬁrst pipeline stage, with crossbar and link traversal occurring in the second stage. As in the SpecVC router, no separate head ﬂit was included and thus, besides the data payload, each ﬂit carries a 5-bit, one hot encoded next-port identiﬁer, X and Y destination addresses (2-bits each) and 1-bit to identify tail ﬂits. Combining with the 64-bit data-path results in a total ﬂit size of 74-bits. In this simple implementation, an additional pipeline stage separation register is provided between the input FIFOs and the crossbar. For the crossbar traversal stage the ﬂit at the head of the FIFO is loaded into this register, which drives it across the rest of the data-path. Since in a wormhole ﬂow control mechanism allocated output ports are not shared amongst separate input ports, state holding elements identify which output ports are currently being used by which input port. Passing tail ﬂits then de-allocate this resource. A similar stop-go ﬂow control, as in the SpecVC design, is also used. Figure 1 shows the structure of this router. 5. Power Measurement Framework The ﬁrst step in the methodology was to describe the full network designs in a Hardware Description Language (a) CS router architecture (b) SpecVC router architecture (c) WH router architecture Figure 1. Router architectures used (HDL). The CS and SpecVC descriptions were provided by their respective authors, while the WH network was newly created. For the test system architecture a simple, sixteen node System-on-Chip (SoC) conﬁguration, as shown in Figure 2, was used. A 90nm, high-performance process with a core voltage of 1.2V and nominal threshold voltage was selected and a standard ASIC tool ﬂow utilised to synthesise, place and route one instance of each of the three routers in this technology. Due to the signiﬁcant beneﬁts of clock gating, shown by Mullins [7], automatic clock-gating was enabled Figure 2. Tile based SoC layout during synthesis, which meant that the tool automatically inserted low-level clock-gating cells whenever appropriate enabling conditions were detected. Parasitic extraction was then performed and the results back annotated into the designs to allow accurate power measurements on the routers. The inter-router link characterisation was performed separately from that of the routers. To tie in with the SoC layout shown in Figure 2, 1.5mm long links, based on intermediate metal layers (M3-M6), were selected, with the Quickcap ﬁeld-solver tool from Magma used to extract link capacitance values with an 8-wire model. The energy/delay tradeoffs of various link repeater conﬁgurations was then analysed with SPICE simulations. Ultimately, instead of using a delay-optimal repeater conﬁguration a lower energy conﬁguration with 9.7 FO43 delay with an associated 0.36pJ/transition/mm for the links was selected for this study. The trafﬁc sources were deﬁned entirely in C and the Verilog Programming Language Interface (PLI) was utilised to link the C trafﬁc source with the HDL network descriptions. This provided a highly ﬂexible framework, where each tile could be modelled by a separate set of C routines, at any desired level of complexity. All simulations were performed at 200MHz at the nominal Process, Voltage and Temperature corner (PVT). 6. Results and Discussion 6.1. Power at Fixed Throughput An initial power characterisation of the designs was obtained by streaming data at a ﬁxed set of rates through a single router and measuring the dissipated power. Four ﬁxed 3One FO4 delay is the delay of a single inverter driving four identical inverters. trafﬁc streams were deﬁned, one originating at each of the North, South, East and West ports of the router, with each one transmitting a stream of packets to the opposite router port. Four ﬂit packets (i.e. each packet transporting 256-bits), with each ﬂit containing a random payload were selected for these experiments. The data rate of each stream was set to an expected realistic 30% of the maximum bandwidth of a single router link, with ﬂits being sent at randomised intervals. Power analysis was performed for 5000 clock cycles for each experiment. Such results can provide useful insight into the expected communication power for different trafﬁc rates. The results for a single router can also be used to identify the power requirements of the entire communications infrastructure and be important from a thermal perspective as power densities determine local heating effects. For the CS net, any experimentation requires more detailed consideration, as the allocation has to be performed separately to utilising the data-path. For simplicity, all following experiments only considered a one-off circuit setup at the start of any experiments. Clearly, this represents a best-case for the CS network which needs to be accounted for while interpreting results. Figure 3 shows the total, router and link power results of these experiments for all the routers. As expected, the complex SpecVC router dissipates a lot more power than the other two designs. However, the power dissipation of the WH and CS routers is seen to be very similar. This was contrary to initial expectations, given the additional complexity of the dynamic allocation techniques present in the WH router. The explanations for these results are left until Section 6.2, where a breakdown of energy across the different router components is considered. An important result to note is that the router power is more than the link power for all the designs and is signiﬁcantly so for the SpecVC design. That is, the beneﬁts provided by NoCs come at a high energy cost, at least at the 90nm technology node. The work presented by Xi and Zhong [11] suggests that the routers may represent less of an overhead at future technologies at which point NoCs can become truly useful. The link power results are otherwise as expected with the widest width SpecVC ﬂits dissipating the most power and the narrowest width CS ﬂits dissipating the least. All the routers dissipated a signiﬁcant amount of power even in the 0-stream (i.e. no trafﬁc) condition (from now on referred to as standby power). Given the lack of any leakage minimisation techniques, one component of this is leakage power. Secondly, there is also some clock related dynamic power. This is caused purely by the activity in the clock tree (since it is only gated at a low-level) and on the clock pins of any non-clock gated synchronous elements. It (a) Link power (b) Router power (c) Total power Figure 3. Link and router power at ﬁxed throughput is of interest to see how each component contributes to the standby power and hence Table 1 provides a breakdown of the standby power across the main power dissipating components. Even without quantitative analysis, the ﬁrst observation is that the leakage power is roughly proportional to the complexity (and hence gate count) of the components, as was expected. The dynamic power can also be seen to vary roughly proportionally to the expected number of synchronous elements in each design, i.e. it is proportional to the expected clock tree size and number of un-gated synchronous elements, again as anticipated. Comparing between the three routers, a similar distribution is seen for the top level clock tree power — the CS router with its single ﬂit register per input port required the least power, whereas the SpecVC router with its four registers per VC per input port required the most power. The top level clock tree was also the largest single dynamic power dissipating unit in all the designs. An anomaly was observed in the WH router input FIFOs as its dynamic power was seen to be much greater than expected. This was traced to a unique case of poor automatic clock-gating. It is important to realise that when packets are being routed, the standby power forms the ﬁxed base, beyond which the power demands are increased by the routing activity. The large value of this standby power, relative to the active power, can therefore have serious implications for the feasibility of deploying NoCs. This in turn implies that it is important to try to reduce this. To this end, it can be seen that each router utilises a particular architecture to provide a certain functionality. The standby power can therefore be considered to be the power overhead required by a particular architecture to achieve it’s intended functionality. Clearly it is reasonable to expect that some architectures will be more efﬁcient than others. An example of a power efﬁcient architecture would be one that achieves its functionality with a minimum number of synchronous elements, resulting in a minimum clock tree size. The need to reduce the standby power then means that it is important to strive towards such efﬁcient architectural designs. Secondly, once an efﬁcient architecture has been selected, it is then possible to further reduce its standby power by using several standby power reduction techniques, such as the clock tree gating facility demonstrated by Mullins [7]. Leakage power reduction techniques would also be especially important considering the expected increase in leakage power for future technologies. 6.2. Packet Energy under no Congestion A more fundamental metric than the power at a given throughput is the energy required to perform a certain amount of communication in each of the three NoCs. To obtain an initial metric, the energy required to route a packet through a single router under no congestion was measured. The same streaming trafﬁc experimental setup as in Section 6.1 was utilised, i.e. each packet is composed of 4 ﬂits, with each ﬂit containing a random data payload. The same 5000 cycle measuring period was also used. As discussed in Section 6.1, the activity of routing a packet means that the router dissipates some ﬁxed standby power as well as some additional energy speciﬁc to the computation performed for each packet. Considering the standby power to be the overhead power of a particular architecture means that the increase in energy demands rep(a) Constant power components for CS router Component Top level clock tree Crossbar Flit buffers BE router Conﬁg. memory Tile interface Others Total Dynamic Power (mW) Leakage Power (mW) 0.64 0 0 0.22 0.11 0.44 0 1.41 0.02 1.0 0.1 0.15 0.06 0.33 0.10 1.77 (b) Constant power components for WH router Component Dynamic Power (mW) Leakage Power (mW) Top level clock tree Crossbar Input FIFOs + logic Crossbar allocator Others Total 1.65 0.09 1.27 0.05 0 3.06 0.04 0.28 0.93 0.09 0.06 1.41 (c) Constant power components for SpecVC router Component Dynamic Power (mW) Leakage Power (mW) Top level clock tree VC muxes and crossbar Input FIFOs + logic VC allocator Crossbar allocator Output ports Others Total 3.61 0 1.04 1.05 0.67 0.2 0.02 6.6 0.11 0.01 2.3 0.23 0.18 0.2 0.12 3.15 Table 1. Standby power components resents the dynamic energy cost of the particular computation performed for each packet. It represents the energy cost of purely the functionality provided by each architecture. Therefore, a functionally equivalent, but more standby power efﬁcient architecture, would reduce its standby power but since the functionality would remain the same, so would this dynamic energy cost. Measuring the increase in router power under the 4stream trafﬁc condition, multiplying by the simulation time and dividing by the number of packets processed then allowed this dynamic energy cost per packet to be calculated. This methodology will only work assuming effective low-level clock gating and the results obtained showed this to be generally true. If leakage power minimisation techniques had been used, the equally important ﬁgure of leakto compare functionalities across different network types. For example, for the particular case of the buffers, the WH router buffers ﬂits twice, taking 66.0pJ, which is consistent with the single buffering operation performed by the CS router taking 28.1pJ. As the SpecVC router also buffers ﬂits just once, the extra energy for the SpecVC buffers therefore comes from the wider buffers and the more complex data-path around the buffers providing added functionality (as each ﬂit needs to fan out to each register of each VC, unlike the other two designs). Similarly, the reason for the high CS energy can be clearly seen to be the higher order crossbar used for that design. The CS crossbar takes signiﬁcantly more energy than even the more complex input port multiplexer and crossbar structure used by the SpecVC design. These results again highlight the energy similarities between the particular CS and WH network architectures used. The higher order crossbar energy in the CS router is matched by the two buffer writes in the WH router. Clearly there can be many different energy-performance tradeoff points, such as accepting a lower performance by utilising a lower order crossbar for the CS router or not providing a separate pipeline separation register for the WH router. However, it is beyond the scope of this paper to consider the different tradeoffs presented by all such possible designs. The results of this work instead aim to highlight the more fundamental values, which can allow other conﬁgurations to be quickly judged. If the energy needs of the routers continue to be dominated by the data-path, as observed here, it will result in a large implication for NoC design. Namely, from an energy perspective, this can justify the use of complex, dynamic allocation techniques with regards to achieving other aims, such as increasing the efﬁciency of global link usage. These arguments would be further backed up when considered in the context of reducing transistor cost, given continued scaling. However given the non-power optimised designs considered here, it is difﬁcult to accurately judge how the data-path to control-path energy ratios might change. On one hand, several data-path optimisations such as the use of SRAMs as FIFOs can clearly reduce the data-path energies. Conversely the control-path energy might be increased by varying parameters not considered in this study, such as by the use of a higher dimension topology. On the other hand, it is questionable whether roughly the order of magnitude data-path to control-path energy difference observed in this work can be eliminated, especially in the context of even wider data-path widths expected for future technologies. 6.3. Packet Energy under Congestion The packet dynamic energy cost reported in Section 6.1 is only calculated for a single trafﬁc pattern. Clearly it is Figure 4. Packet energy for streaming trafﬁc age power cost of computation could also have been evaluated. With this methodology, some inter-packet dependencies will inevitably exist (ex. if two packets affect the same clock gating enable signal for any register), but these are reduced in Section 6.3 by utilising more random trafﬁc. Figure 4 shows the dynamic energy cost required to route a 4-ﬂit, random payload packet through each of the three routers. It is important to note that, to judge the power characteristics of the complete architecture, these results can only be used in the context of the standby power components shown in Table 1. The router energy is seen to be a similar amount to the link energy for all the designs. Again it is surprising to see that the WH and CS router energies are similar and even the SpecVC energy is not vastly different. To gain a better understanding of these results, Table 2 provides a breakdown of the energy consumption of the packet across the major router components. The anomalous clock-gating of the WH router input ports meant that the 45.72pJ value calculated with the above methodology is not directly representative of the buffer computational energy and it was manually determined to be 66.0pJ (which is then consistent with the other router buffer energy results) with a separate experiment. The key result to note here is that the data-path components dominate over the control elements in all designs, especially in the CS and WH nets, with the ﬂit buffers consuming a large proportion of the energy. It is interesting to see that the buffer energy is not directly proportional to the amount of buffering in the designs. This can be explained by realising that, with effective clock-gating, the energy needs are only proportional to the computation activity. So, to take the case of the buffers, energy is only required when data is written into or out of a buffer. The rest of the time, clock-gating ensures that very little energy is dissipated. The same explanation holds true for the rest of the router’s computation activity as well. Besides using the dynamic energy cost to compare architectures with the same functionality, it can also be used (a) CS router dynamic energy breakdown Component Energy (pJ) Top level clock tree Crossbar Flit buffers BE router Conﬁg. memory Tile interface Others Total 0 37.9 28.1 0 4.65 0 8.77 79.6 Percentage of total 0% 48% 35% 0% 6% 0% 11% 100% (b) WH router dynamic energy breakdown Component Energy (pJ) Top level clock tree Crossbar Input FIFOs + logic Crossbar allocator Others Total 0 6.62 45.72 2.94 2.33 57.61 Percentage of total 0% 11.5% 79.4% 5.1% 4.05% 100% (c) SpecVC router dynamic energy breakdown Component Energy (pJ) Percentage of total Top level clock tree VC muxes and crossbar Input FIFOs + logic VC allocator Crossbar allocator Output ports Others Total 5.98 15.5 58.2 1.92 7.27 8.99 20.5 118.4 5.1% 13.1% 49.2% 1.62% 6.1% 7.6% 17.3% 100% to reach their destinations. A single router, at co-ordinates x = 2, y = 2 was considered and the energy of any packets going through it was calculated in the same fashion as in Section 6.2. For the CS net, the current lack of dynamic reconﬁguration support means that this form of congestion energy experiment cannot yet be performed. Figure 5 shows the packet energies for various injection rates. Running several experiments showed that the link energy per packet varied widely across experiments. This appears to be because the link energy is not determined simply by the contents of a ﬂit but also by the contents of the previous ﬂit or any idle periods before the ﬂit. Since in these experiments these are also random variables, the overall variance is further increased. For both the routers the energy per packet was seen to vary very little as network trafﬁc increased. In light of the discussions about this calculated value representing the energy cost of computation this result is intuitively meaningful. This is because effective clock-gating ensures that the amount of data-path computation (the main energy consumer) does not change with congestion. Indeed, a breakdown of packet energies across all data-path components conﬁrmed that their energy demands do not signiﬁcantly change. The main reason for the SpecVC router’s energy increase was seen to be due to an increase in allocation and ﬂow control activity, given the increased resource contention. This is a novel result, showing that although performance (such as packet latency) might be seriously degraded at high congestion, there is no considerable direct impact on packet dynamic energy, given effective clock-gating. Any signiﬁcant energy impact would come from second-order effects, for example by reducing the time available to effect standby power minimisation techniques, due to increased packet traversal time. Table 2. Streaming trafﬁc packet energy breakdown across components 6.4. Area and Timing Parameters of interest to see how this computation energy will change with network congestion. For the WH and SpecVC networks, this was achieved by instantiating 4 × 4 mesh networks for each design. A trafﬁc source connected to each router then injected random trafﬁc at varying injection rates into the network, destined for random destinations (excluding itself). Four-ﬂit packets, with each ﬂit containing a random data payload, were again used. An initial 500 clock cycles were used as a warm-up time, with the next 5000 clock cycles forming the sampling time, any packets transmitted during which were the only ones considered in the analysis. A further 300 clock cycles of drain time were used to allow any packets transmitted near the end of the sampling time The ﬁgures reported so far highlight only a single aspect of the networks, that of their energy characteristics. The area and critical path delay of each router represents another set of important parameters. The area of the designs was obtained post place and route and is reported in Table 3. In the current designs the crossbar and inter-router link traversal for each ﬂit is performed in a single clock cycle. If longer links or the negative impact of scaling mean that the router crossbar traversal would need to be performed in a separate pipeline stage from link-traversal, the router’s internal critical path delay would also be of interest. Therefore both of these ﬁgures are reported in Table 3. The delay (obtained post synthesis from Synopsys Design Compiler) is reported in units of Fan-Out-4 delays to represent a technology independent ﬁgure. from the conﬁguration memory) and data-paths had similar delays in the separated crossbar and link traversal case. Having realised the fairly low impact of control complexity on the packet routing energy, the results of Table 3 now give opposing results from a timing perspective. The high complexity of the SpecVC router signiﬁcantly degrades its critical path delay, compared to the other two networks. The full implications of this can only be evaluated from a complete system level perspective, which is beyond the scope of this work. (a) Link energy 7. Future Work All the results presented here have been based on synthetic trafﬁc sources. Ultimately, real application trafﬁc would need to be added to enable a complete power analysis to be performed. NoC modelling with real applications is a complex ﬁeld and no adequate full system modelling tool, as would ideally be used, is currently available. Evaluating the feasibility of different such system modelling techniques is precisely the direction currently being pursued in this research group. Although the power and energy ﬁgures reported here present important results, a complete evaluation of each NoC can only be made by evaluating the performance provided by it to consider the particular performance-energy tradeoff it represents. Again, the performance should ideally be obtained with a full system level modelling tool, as mentioned above. However, in the absence of this the use of synthetic trafﬁc to obtain initial estimates is a possible initial direction. Many of the beneﬁts of NoCs will become truly apparent only at future technologies. This work has demonstrated the high energy cost introduced by the routers by the use of an on-chip network at the 90nm node. Understanding the impact of scaling on the router to link energy ratios will therefore form an important result in determining the feasibility of deploying NoCs and this work direction is therefore currently being pursued. Finally, there is always scope to add more NoCs to this evaluation framework to build up a library of NoC characterisations. 8. Conclusions This study has presented an accurate power characterisation of a range of NoC architectures by considering a highly static circuit-switched (CS) network, an opposing, highly dynamic speculative virtual channel (SpecVC) network and a mid-way architecture of a wormhole (WH) router. All designs were synthesised, placed and routed in a TSMC 90nm, high performance technology. Utilising the extracted parasitics then allowed accurate power results to be obtained. (b) Router energy (c) Total energy Figure 5. Packet energy under congestion The breakdown of the area for the CS router showed that the higher order crossbar is its largest component, being approximately 3.5× larger than the WH crossbar. This, along with the serialisation logic, the packet switched network and the conﬁguration memory areas of the CS router together outweigh the saving of area from reduced buffering compared to the WH router. For the SpecVC router the complex control-path determines the critical path for both cases. For the WH router, the simpler control-path means that the data-path forms the critical path in the combined crossbar and link traversal case. Finally, the highly simpliﬁed CS router was clearly limited by the data-path in the combined crossbar and link traversal case while the control (the path reconﬁguring the crossbar CS router WH router SpecVC router Area 0.108mm2 0.078mm2 0.246mm2 Combined crossbar and link traversal critical path (FO4) Separate crossbar and link traversal critical path (FO4) 23 31 35 14 23 28 Table 3. Router area and timing parameters A set of streaming trafﬁc conditions was ﬁrst used to characterise the power dissipation rates of the routers, to understand the expected power dissipation rates of the networks under different trafﬁc conditions. The router power was seen to be a signiﬁcant overhead beyond the link power. The WH and CS routers were seen to dissipate similar amounts of power, whereas the more complex SpecVC router dissipated signiﬁcantly more power. All the designs dissipated signiﬁcant standby power produced by a combination of leakage power, clock tree power and a few nonclock-gated synchronous elements. This standby power can be considered to the overhead required by a particular architecture and highlights the need to use efﬁcient architectures combined with standby power reduction techniques, to obtain power efﬁcient designs. Calculating the additional power dissipation while routing trafﬁc allowed a dynamic energy cost per packet to be calculated. The much wider data-path compared to the control path meant it dominated the energy needs, with the buffer energy forming a signiﬁcant proportion of this ﬁgure. The particular tradeoffs used in the WH and CS routers meant that they had very similar energy needs. If the energy needs continue to be dominated by the data-path it will result in important implications for NoC design, justifying the use of complex control. Calculating the packet dynamic energy cost under congestion for the WH and SpecVC routers showed no signiﬁcant change in this value. This was again seen to be because this energy cost of computation was dominated by the datapath. Since effective clock-gating ensured that the data-path computation did not signiﬁcantly change with congestion, the energy cost of this did not change either. Finally, the area reported for the three routers means that the area impact on the full system of using these NoCs can be easily evaluated. A future direction of the work is to report performance-energy tradeoffs made by the three networks, and to this end the critical paths of the networks were also calculated. Accounting for the impact of scaling forms another future direction for this work. The completion of this will then provide accurate power and performance results, for a large range of NoC architectural families with a clear understanding of how they are likely to change in the future. "
The Power of Priority - NoC Based Distributed Cache Coherency.,"The paper introduces network-on-chip (NoC) design methodology and low cost mechanisms for supporting efficient cache access and cache coherency in future high-performance chip multi processors (CMPs). We address previously proposed CMP architectures based on non uniform cache architecture (NUCA) over NoC, analyze basic memory transactions and translate them into a set of network transactions. We first show how a simple, generic NoC which is equipped with needed module interface functionalities can provide infrastructure for the coherent access of both static and dynamic NUCA. Then we show how several low cost mechanisms incorporated into such a vanilla NoC can facilitate CMP and boost performance of a cache coherent NUCA CMP. The basic mechanism is based on priority support embedded in the NoC, which differentiates between short control signals and long data messages to achieve a major reduction in cache access delay. The low cost priority-based NoC is extremely useful for increasing performance of almost any other CMP transaction. Priority-based NoC along with the discussed NoC interfaces are evaluated in detail using CMP-NoC simulations across several SPLASH-2 benchmarks and static Web content serving benchmarks showing substantial L2 cache access delay reduction and overall program speedup","The Power of Priority: NoC based  Distributed Cache Coherency  Evgeny Bolotin, Zvika Guz, Israel Cidon, Ran Ginosar and Avinoam Kolodny  Electrical Engineering Department,Technion - Israel Institute of Technology, Haifa 32000, Israel ABSTRACT  The  paper  introduces Network-on-Chip  (NoC)  design  methodology and low cost mechanisms for supporting efficient  cache access and cache coherency in future high-performance  Chip Multi Processors (CMPs). We address previously proposed  CMP architectures based on Non Uniform Cache Architecture  (NUCA) over NoC, analyze basic memory transactions and  translate them into a set of network transactions. We first show  how a simple, generic NoC which is equipped with needed  module interface functionalities can provide infrastructure for the  coherent access of both static and dynamic NUCA. Then we show  how several low cost mechanisms incorporated into such a  Vanilla NoC can facilitate CMP and boost performance of a cache  coherent NUCA CMP. The basic mechanism is based on priority  support embedded in the NoC, which differentiates between short  control signals and long data messages to achieve a major  reduction in cache access delay. The low cost Priority-based NoC  is extremely useful for increasing performance of almost any other  CMP transaction (i.e. uncached and cache-coherent R/W, search  in DNUCA, isolating low priority traffic, synchronization and  mutual exclusion support). Priority-based NoC along with the  discussed NoC interfaces are evaluated in detail using CMP-NoC  simulations across several SPLASH-2 benchmarks and static web  content serving benchmarks showing substantial L2 cache access  delay reduction and overall program speedup. For further system  improvements, we introduce additional low cost NoC mechanisms  that include: virtual invalidation rings, efficient store-and-forward  multicast for short messages which is embedded within a  wormhole NoC, and a cache-line search mechanism for the  efficient operation of dynamic NUCA. These mechanisms can  also expedite not only cache coherency but also other basic CMP  transactions such as search and serialization primitives support.   1. Introduction  Microprocessor architecture is in transition towards multicore architectures that exploit thread-level parallelism, and  provide performance improvements as well as powerefficiency. Such chip multi-processors (CMPs)[1-7] need to  employ large shared on-chip cache memory (typically a L2  cache). The cache must support parallel transactions with  multiple cores. Hence, a distributed cache, comprised of  multiple memory banks interconnected by a network on  chip (NoC)[9,31-43], as illustrated in  Figure 1, is an  accepted and likely approach. In such a structure, the  effective access time to the shared cache will become a  major performance bottleneck, as both the number of cores  and the number of clock cycles required for signal  propagation across the die will increase with technology  scaling.   depicted in  Figure 1 needs to efficiently discover the cached  location of each physical memory address and maintain  multiple data copies, while ensuring data coherency of  shared data among all the cores. Traditional snooping  protocols for cache coherency [25] are not suitable for  implementation over a NoC, and are not scalable with the  number of cores. Directory-based coherence protocols  require multiple network traversals (e.g. to search the  cached location, determine the sharing status, update or  invalidate etc.). Consequently, a CMP equipped with a  standard NoC and standard processor and cache network  interfaces may incur large delays in cache transactions.  P0 0 7 P P1 7 2 P Distributed L2  6 P 56 P5 3 P 63 P4 Figure 1. Modern CMP System interconnected by NoC :8 CPUs along  with L2 Cache distributed in 64 Banks   Previous CMP research mainly addressed the principal  architectural issues of distributed shared CMP cache over a  NoC abstraction [1-4, 6-7]. In the evaluation of SNUCA  and DNUCA  [1,2]  the authors make  simplifying  assumptions regarding network delays and behavior. They  do not evaluate any detailed NoC design or optimize the  NoC for supporting typical cache operations.   There has been substantial prior work in the area of cache  coherency optimization in the context of multiprocessors  [11-18]. The majority focused on in-protocol optimizations,  releasing consistency model and speculation [10-13]. Some  approaches combined  snooping and directory-based  protocols [12]. Several studies looked into broadcast and  multicast snooping, and ring optimizations [14-17]. In  [18]  the authors tried to efficiently map a coherency protocol  onto physical wires in several metal layers, with different  widths and thicknesses. The token coherence method  [19]  suggests to exchange and count tokens to control coherence  permissions.  This architecture raises many challenges because the system  Several recent papers focused on NoC–based CMP cache        coherency [21-23].  [23] explores the performance vs.  energy tradeoffs for hardware and software snoop-based  protocols in MPSoC. Their evaluation assumes a sharedmedium interconnect among processors and shows that  hardware solutions based on snooping are power inefficient.  It does not address directory based coherence schemes  which are the topic of our work.  Eisley et al.  [21] addressed the cache coherency problem in  CMP and proposed to alter the standard directory-based  system by directories implemented inside NoC routers (innetwork approach). In the proposed architecture, routers  may steer requests towards nearby data copies. This  approach enables to reduce memory access delay but  requires additional storage and a more sophisticated router  architecture to perform directory-related manipulations on  every packet at every hop. An alternate solution,  [22],  proposes a software solution for memory coherency in  MPSoCs. The approach  relies on  the programmer  intervention for mapping local and shared variables and  segments. Using uncached accesses  for  the shared  segments, the cache coherency and memory consistency can  be maintained at no hardware cost.   The main contribution of this paper is the introduction of  hardware based NoC priority mechanism for efficient  distributed directory-based cache-coherent access in both  static and dynamic NUCA systems, termed Priority-based  NoC. The main power of Priority-based NoC is its  simplicity, low hardware cost and its generic nature that can  extend and accelerate many other operations. The major  cache-access delay reduction is achieved by differentiating  short control messages  that constitute  the coherency  protocol and allowing these short messages to bypass long  data packets while preserving the coherence protocol  correctness. The same idea can apply to other common  CMP memory and synchronization transactions such as  regular (uncached) R/W, DMA and common variable  synchronizations [26]. Our solution has a very  low  hardware cost and it does not impact the router architecture  as in  [21]. Moreover, our approach is a complementary  solution, and can be applied to provide additional speedup  to the in-network directory of  [21], or to the software  messages of  [22], as well as to other cache coherence  protocols.  In addition, we describe other new NoC mechanisms and  services for a further improvement of efficient coherent  cache-access in NUCA, termed an Advanced-Services  NoC. These include virtual wormhole invalidation rings for  well-organized invalidation procedure, efficient store-andforward multicast for short messages which is embedded  within a wormhole NoC, and support for serialization and  mutual exclusion primitives such as locks and Test&Set.  For DNUCA based CMPs we introduce a cache-line search  optimization mechanism that is required for efficient  operation. Note that these additional mechanisms are also  generic and can be used beyond the context of cache  coherency.  The rest of the paper is structured as follows. In Section 2,  we give a short background of CMP NUCA. Then, we  analyze cache coherency memory transactions and translate  them into a set of network transactions. Then, we show how  the basic-functionality Vanilla NoC needs to be equipped  with special NoC module interfaces in order to support  cache-coherent communication of both static and dynamic  NUCA. In Section 3, we turn to introduce Priority-based  NoC  and Advanced-Services NoC  for  boosting  performance of coherent distributed cache access and  reducing its latencies. In order to illustrate the generic  nature of our simple hardware solutions we also explain  their usage beyond the cache coherency paradigm, for  normal (uncached) R/W and for serialization primitives  support such as locks and Test&Set. In Section 4, our  Priority-based NoC is evaluated in detail using cycle-  accurate CMP-NoC simulations across several benchmarks.  Section 5 summarizes the paper.  2. CMP NUCA Background   CMPs are shifting towards a NUCA  [5], where the cache is  divided into multiple banks, and accesses to closer banks  result in shorter access times. In NUCA, performance  depends on the average (rather than worst-case) latency. To  further reduce the average access time, the authors of   [5]  have suggested the use of dynamic block migration, called  Dynamic NUCA (DNUCA). In DNUCA, every access to a  block moves the block one step closer to the processor, thus  gradually reducing distances and access times to frequentlyused data. This differs from the basic Static NUCA  (SNUCA) design, where block placement  is static,  determined by address.  Several works have dealt with NUCA based CMP systems  [1,2,4,7]. [1-2] have studied both SNUCA and DNUCA  implementations for CMP. [3-4] suggested replicating  shared blocks within the cache in order to improve their  proximity to all sharing cores.  [7] achieved vicinity for  shared data by changing the common cache-in-the-middle  CMP layout and by using a designated part of the cache  capacity for shared blocks only. Using DNUCA, blocks  may reside in different locations within the cache and hence  a mechanism for locating blocks is needed. Search policies  may vary from sequentially inquiring every bank to  flooding of the interconnect in parallel, or use a hybrid  intermediate policy such as phase-multicasting [1,5,6].  Since the search introduces significant delays to the cache  access time and overload the interconnect, some search  hints are required to direct the search to specific banks and  accelerate detection of cache miss.  [6] has leveraged bloom  filters  to devise such a complexity-effective search  mechanism for CMPs.  3. NoC support for CMP NUCA   In  this section we first describe a straightforward  architecture of NUCA CMP over a Vanilla NoC. We  briefly describe the CMP memory architecture, basic CMP  communication infrastructure and details of directory-based      NoC. A basic read transaction by P0 is depicted in  Figure 2.  It is first translated into a read request packet and sent over  the NoC towards a L2 node according to the address of the  block for SNUCA, or after a search procedure for DNUCA.  If the block is missing at the home node (L2-miss) then the  block is fetched from the external memory, also via the  NoC. Otherwise, if the block exists in the L2 cache, several  scenarios are possible according to the state of the block  which is stored in the directory. If the block is not in  modified state, then L2 responds with read response packet  which carries the desired cache-line (indicated by a bold  arrow) back to the requestor P0 and sets the bit in the status  vector indicating that the block is shared by P0.  distributed coherency protocol over NoC. We also outline  common L2 cache access transactions which are translated  into multiple network transactions. We sketch the basics of  the necessary NoC communication interfaces that support  coherent cache accesses over NoC.  Then we show how by using a Priority-based NoC which is  equipped with a simple priority mechanism we can  drastically decrease cache access latency and speed up the  total program running time. Finally, we outline further  possible Advanced-Services NoC mechanism for efficient  cache-access and overall CMP performance improvement  such as: special broadcast and multicast mechanisms, ways  for faster search in DNUCA, supporting synchronization  primitives in CMP and others.  3.1 Cache-Access in NUCA over Vanilla NoC   In a CMP system such as in  Figure 1 there are two levels of  cache hierarchy: shared L2-cache and private L1-caches  (within the processor cores) that maintain copies of L2 data  and may need to modify it. Therefore, we need to provide  mechanisms to keep this non-uniform memory system in a  coherent state and support sequential consistency. In other  words, we need to support total ordering among memory  transactions in the parallel system as it would be executed  on a sequential system  [8].  Common snooping-based cache-coherency approaches [8]  are not suitable for NoC and are not scalable with the  growing number of nodes. Therefore, we focus on a  directory based approach. This approach eliminates the  need for using a slow and expensive shared bus or NoC  broadcast. The directory serves as a serialization point for  maintaining coherency.  We focus on a distributed directory scheme which is more  scalable than a central directory approach. In practice, the  distributed directory is implemented by extending each L2cache-line (block) with the directory information, which  tracks the state of this block. The directory information  contains a status vector containing  the  identity of  processors that store this cache-line in their L1 caches. It  also contains a modified bit to indicate that this cache-line  is in modified state in one of the L1 caches. In this work we  target  the  four-state  (MESI) write-back  invalidation  protocol  [8].   L2-caches and directory deal with incoming transactions inorder for maintaining transaction consistency  [8]. Since out  of-order mechanism require a considerable additional  hardware costs and protocol verification we do not address  such systems in this paper.  We assume that the network maintains the ordering of  messages for each source-destination pair. Therefore a  Vanilla NoC would be equipped with a single service level  (SL)  [9] and virtual channel (VC) , and would perform  static order-preserving packet routing.  When a processor performs a L2 cache transaction (upon  L1 miss) it is translated into multiple transactions over the      mark the directory status of this block as modified.      a possible failure. A distant processor P0 requests a cacheline from directory (1) using high priority request, as a  result the directory responds with a long and low priority  message (2). Meanwhile, a nearby processor P1 performs  an exclusive read (3) of the same cache-line, resulting in a  high priority invalidation message (4) which may reach  processor P0 before the low priority read response. A  Vanilla processor interface would invalidate the cash-line,  and reply with invalidation acknowledge towards L2-bank.  Later, when read response (2) finally reaches P0, its  processor stores this cache-line in the local L1 and P0  consumes it despite the fact that P1 already modified it.   delay components: the queuing time at the processor  interface, the round-trip delay of request and response  messages between processor and L2-cache bank over the  NoC, the queuing time in L2 incoming requests and  outgoing response queue and the round trip delay of write  back and invalidation procedures.  Observation B:  All NoC transactions which constitute the L2-cache access  procedure are of an equally very high importance since they  directly contributed to the delay period which separates  between the processor and its desired data (L1 cache-miss).  Observation C:  L2-cache accesses consist of two types of messages: first a  short control messages (either request or acknowledge), and  second, long messages that carry the cache line (64 bytes  and additional overhead – in our example).  From analyzing observations A-C, we propose  to  differentiate between short control messages and long data  messages by equipping the NoC with multiple priorities  similar to QNOC [9], and by giving a higher priority to the  (short) control messages over the (longer) data messages.  Although according to observation B all messages are of  the same importance, giving priority to short messages  significantly decreases their delay without a large impact on  the delay of long data packets. This is especially true in the  case of a wormhole NoCs where short messages can be  blocked behind long worms that are not even destined to the  same destination nodes.  There are three types of short messages in our system: read  or read exclusive requests, write back request and  invalidation request or acknowledges. By giving priority to  each type of messages we can speed up different phases in  the coherent cache-access protocol. For example, a read  request message with higher priority will reach L2 earlier  and cause an earlier response, either a data response, a write  back or an invalidation procedure. Thus, leading to a total  shorter transaction round trip delay in the NoC, that is  translated into a shorter stall time at the processor  interfaces, leading to shorter queueing delays in the  processor transmitting queues.  Similarly, by giving a higher priority to write back and  invalidation request messages and acknowledge messages,  we can not only reduce the round trip delays of write back  and invalidation procedures which are components of total  transaction delay, but also reduce the stalling time of the  L2-cache. This allows the L2-bank to start serving the next  pending request earlier, leading to a minimal overall L2  cache-access delays and an overall system speedup.  Interface Support for Coherence Protocol Correctness:  A Priority-based NoC with multiple priorities can no longer  provide  in-order delivery among packets  that are  transmitted with different priorities. As a result, a system  which uses Vanilla NoC interfaces becomes vulnerable to  coherency protocol failures.  Figure 5 shows an example of      to use for boosting performance and saving power in CMP.  search in DNUCA, synchronization messages and more.  3.3.1 Special Broadcast for Short Messages  Observation D:  Invalidation procedure is unique since it might require  sending invalidation messages to multiple processors that  share the specific cache-line and gathering invalidation  acknowledge messages from them. It becomes an important  L2 cache-access delay component in programs having a  large amount of write-sharing among the processors.   Therefore, in addition to the proposed priority mechanism  which leads to a substantial speedup, one might also want to  use a more efficient multicast or broadcast schemes instead  of multiple unicast messages (which are the only type  supported by the Vanilla NoC). However, broadcast-based  invalidation is an undesired solution for a wormhole NoC.  Wormhole broadcast is deadlock-sensitive and extremely  slow especially when several broadcast trees coexist in the  network. The wormhole broadcast tree traversal time in the  network is dominated by the speed of the slowest leaf. In  our CMP system each memory bank is a potential source of  invalidation broadcast. Therefore, there is a need to provide  deadlock-freedom (by adding additional VCs). In addition,  multiple broadcast trees may also slow each other.   Store & Forward Broadcast Embedded in Wormhole NoC:  Because of the complications described above, we propose  to enhance the wormhole NoC router with a message  replication mechanism for short control messages only. In  this way, we achieve a performance of store and forward  (S&F) broadcast by a small hardware investment, as we do  not increase router buffering.  The sketch of the enhanced router architecture is as  following. A generic input-queued wormhole router [9] [27]  consists of several flit buffers for better performance of the  wormhole pipeline [27]. The length of control messages is  few flits only, which would fit into such a queue. In a  wormhole router the flow control is performed on a flit by  flit basis, i.e. flit-based flow control. For implementing S&F  broadcast the output port of the router schedules the packet  only when there are enough buffers to contain the whole  packet, i.e. packet-based flow control. The input buffer  management logic does not remove the packet from the  input buffer before it is transmitted over all scheduled  output ports.  The broadcast routing is very simple in mesh topology. It  can be performed in XY-like manner, (see  Figure 6) while  the routers along X-axis would have to replicate packets up  and down and forward it further in the X-direction. The  routers along the Y-direction of packet propagation only  forward the packet without replication. On the other hand,  the long packets carrying data are transmitted using the  regular wormhole mechanism.   This broadcast service for short control messages can be  enhanced with a priority mechanism and used for  invalidations as well as for other kinds of system traffic, i.e.  Figure 6. XY Based S&F Broadcast in wormhole NoC  Another mechanism that is useful in CMP is message  consolidation. Consider  an  example of gathering  invalidation acknowledge messages from all invalidated  processors in a more efficient way than unicast. A router  that supports message consolidation (a.k.a. gather function)  maintains a state for each active broadcast at each output  port. The acknowledge messages return from the broadcast  tree leaves via the same route as the original broadcast tree.  Once all acknowledge messages from all ports that have  transmitted the original broadcast are received, the router  invokes a single acknowledge message towards the root.  This approach minimizes the amount of messages during  invalidation cycle and reduces invalidation delay and total  power dissipation.  Broadcast on a Virtual Ring:  P0 0 7 P 6 P 56 P5 P1 7 2 P 3 P 63 P4 Figure 7. Virtual Invalidation ring implemented on top of existing NoC  Another approach  for efficient  invalidation can be  implemented using a special multicast solution, termed  virtual-ring. It is formed among the former broadcast tree  leaves (the processors in our case). A virtual ring can be  built on top of the existing network at almost no additional  hardware cost. The basic idea is shown in  Figure 7. Upon  invalidation, L2 bank sends a single invalidation request to  the nearest processor marked as ring invalidation message.  When a processor network  interface  receives  ring  invalidation message it invalidates its local L1-cache and  immediately forwards  the message  towards  the next  processor in the ring. The invalidation message can carry a  counter of the already visited processors and then to visit all          processors in the ring. Otherwise, it can carry the sharing  status vector which will indicate which processors really  need invalidation, and the invalidation ring would be  dynamic according to the vector value. When the ring  invalidation message reaches the last processor in the ring it  is forwarded to the L2-bank that originated the invalidation  procedure. This last message also serves as invalidation  acknowledge consolidation for all processors in the ring.  Again, this technique can be combined with our priority  mechanism and the ring invalidation messages can traverse  the network at higher priority than the long data messages.  The virtual ring approach minimizes the number of  messages and consequently reduces power consumption.  However, the virtual ring can become a longer path than  sending small number of invalidation messages and its  acknowledges using unicast. Therefore the efficiency of this  approach is a function of the number of sharing processors  that need to be invalidated. In such cases, one can use a  hybrid approach. For instance, a L2-interface can decide to  send invalidation using broadcast or unicast as a function of  the number of processors that are about to be invalidated. If  the number of processors is below some threshold number it  will send several unicast messages. Otherwise it will use the  virtual ring invalidation.  3.3.2 Optimized Search over NoC in D-NUCA  Observation E:  Using DNUCA, blocks may reside in different locations  within the cache and hence a mechanism for locating blocks  is needed. Therefore the delay of search procedure in DNUCA can become an important factor which constitutes  the cache- transaction delay   In the Vanilla NoC, broadcast search can be implementing  only by sending multiple unicast search request messages to  all memory banks. We propose two approaches for  improving upon the Vanilla NoC.  Priority Search:  Since without knowing the placement of the desired block  in DNUCA the processor is stalled, it is of very high  importance that the search would be executed as fast as  possible. Therefore we can allocate a special priority for  short search messages to allow them to bypass long data  messages.  S&F Broadcast Search  The S&F broadcast which was described above, can be  useful for flooding the short search messages along the grid  of L2 banks. It would drastically decrease the number of  search messages in the system which would lead to a faster  search procedure along with less interference and delays for  other messages in the system. In addition it will reduce the  power consumption of the search procedure.  3.3.3 Synchronization and Mutual Exclusion support  In NoC-based CMPs, similarly to regular multiprocessor  machines, the issues of point-to-point, global (barrier)  synchronization and mutual exclusion among distributed  processes is very important. Mutual exclusion operations  are usually supported by hardware lock mechanisms and  atomic primitives such as Test&Set and its sophisticated  derivatives.  Synchronization  algorithms  are  also  implemented using locks.   When a processor is spinning on Lock (testing a lock), the  processor gets L1 cache miss, brings the current value of  the lock to its local L1 cache from L2-home node and it  keeps spinning on its value locally until the value of the  lock is changed and the processor gets a notification about  this via the standard invalidation of the lock in the local L1  cache. Therefore, at first glance it seems that there is not  much space for optimization for locks in CMP, since the  spinning is performed locally. However, whenever a lock  which is tested by many processors is released, the waiting  processors compete to take hold of it. In the case of  barriers, many processors must be synchronized and then  released. This is a typical case for barrier synchronization  where each processor busy-waits on a locked counter to see  if it reached a certain value. Such activity often causes hot  spots in the memory system interconnect.   It can be seen that synchronization algorithms will perform  well under low-contention periods over NoC, but under  high-contention periods there is a need to incorporate  additional mechanisms into NoC and NOC interfaces for  more efficient synchronization. There has been already a  substantial work performed in this field of software and  hardware  support  for Locks  in  the  context of  multiprocessors and CMPs [24-26].  Most of the hardware methods mainly focus on maintaining  a list of nodes waiting on a lock. It is maintained entirely in  hardware and the releaser grants the lock to one of the  waiting nodes on a  list without affecting others.  Implementing such subscription mechanism for lock at NoC  interface over Priority-based NoC will eliminate busy wait  over network. It may reduce the lock handover time as well  as the interference of lock traffic with data access and  coherence traffic.  4. Numerical Evaluation  We evaluated the schemes proposed in Section 3 through  simulation. In Subsection 4.1 we present the simulation  environment used, which  includes CMP and NoC  simulators and a description of  the evaluated CMP  benchmarks. In Subsection 4.2 we present the simulation  results that demonstrate the advantages of the priority-based  NoC approach over the Vanilla NoC in terms of cacheaccess delay and overall program speedup.  4.1 Simulation Environment  4.1.1 CMP and NoC Simulators  The NoC-based CMP system illustrated in  Figure 1 is fully  modeled by combining cycle-accurate NoC and cycleapproximate multi-processor system simulator. Simics  [28]  is used for simulating parallel programs execution in CMP      and producing L2 access traces which are fed into an  OPNET-based NoC simulator  [9]. NoC simulator is used  for accurate measurement of cache-access transactions  delay  including  coherency  protocol,  network  communication delays and delays due to contentions on  shared network and CMP resources.  The system that is modeled in Simics comprises a 8processor CMP design, using the x86 in-order processor  model as a building block. Each processor has a private, 2way set associative, 64KB L1 data cache, and all processors  share a 16MB, 16-way set associative L2 data cache.  (Instruction caching is not implemented, but instructions are  assumed to be available with no delay). Cache block size is  64 bytes for both L1 and L2.  The L2-access traces from Simics include L2-transactions  and the inter-transaction delays, which represent the times  at which the processor uses its L1-cache without accessing  L2. These traces are fed into the OPNET-based tool which  simulates the full directory-based coherent cache-access  mechanism over NoC which was described in Section 3.  The developed OPNET models include the Vanilla and the  Priority-based NoCs and the NoC interfaces for coherent  CMP communication (see Section 3). Both the Vanilla and  Priority-based NoCs are grid-based wormhole NoCs,  similar to  [9] with static XY routing mechanism and equal  capacity for all links. Flit size is 16 bit and the buffer size at  routers is 4 flits. The clock frequency of the processors is  assumed to be 10 GHz for simplicity and according to ITRS  projections.  4.1.2 Evaluated Benchmarks  We run Mandrake 10.1 Linux with SMP kernel version 2.6,  custom-compiled to interface with Simics, on top of which  we run Linux programs as benchmarks. Our benchmarks  include three SPLASH-2 benchmarks [29]: (Fft, Ocean and  Radix), and two static web content serving (Apache HTTP  server and Zeus web server). For both web benchmarks, we  use the SURGE [30] toolkit to generate a workload of web  requests from a 30,000-file, 700MB repository with a zero  back-off time. This toolkit generates a Zipf distribution of  file accesses, which was shown to match real-world web  workloads  (used by SpecWeb2005).Typical parallel  benchmarks begin with some setup code, which is run  serially, and only then embark on parallel processing. Since  we are interested in the parallel part of the applications, we  fast forward through the serial part and performed the  measurements only in the parallel part of the code.   4.2 Simulation Results  We simulated about a million instructions per each  benchmark. The measured delay of transaction is calculated  from the time that the processor writes the transaction into  its NoC interface queue until the processor receives the  requested cache-line from L2. In other words, the overall  delay consists of the queueing time at the processor  interface, the end-to-end (ETE) NoC delay towards the L2bank, the queuing delay at the L2 interface, the delay of  possible invalidation or write back procedures, and finally  the ETE NoC delay from L2 to the requesting processor.   We measure the total L2-access delay of both read and read  exclusive commands. These commands are extremely  performance critical since they stall the processor. We also  measure the total program throughput which is the number  of executed commands. Although we fully simulate writes  resulting from eviction from L1-cache, we do not include  them in the average delay calculation since they do not  affect the performance of the processors (these writes do  not cause processor stalls).  ] l s e c y c [ y a e l D 1400 1200 1000 800 600 400 200 0 Av. Delay of L2-Read in Apache 1301 994 Vanilla NoC Priority-based NoC 286 234 1 Link Capacity[gbps] 4 62 57 16 Figure 8. Average delay of L2 Read Transaction using Vanilla and  Priority-based NoCs in Apache for different link capacities   Figure 8 demonstrates  the distribution of L2 read  transactions average delays in the Apache server benchmark  for Vanilla and Priority-based NoCs as a function of link  capacity in the network. The graph shows that the delay of  L2 transactions is affected by the NoC link capacity or  alternatively by the network load. When the NoC is lightly  loaded then the read delays are relatively small. During  high network loads the L2 access delays can become  extremely large. A similar behavior is observed in read  exclusive delays.   Av. Delay Reduction of L2-T ransaction in Apache Read Read Exclusive ] % [ n o i t c u d e R y a l e D 30.00 25.00 20.00 15.00 10.00 5.00 0.00 1 4 16 Link Capacity [gbps] Figure 9. Read and Read Exclusive average delay reduction in Apache  as a function of different link capacity  One can also observe from the graph that the delay  improvement achieved using a Priority-based NoC is  growing with the network load.  Figure 9 quantifies the                delay reduction that can be achieved by using a Prioritybased NoC in both read and read exclusive transactions in  the Apache benchmark. As expected, the major delay  reduction is achieved in a relatively highly loaded network  with a large contention over the network resources. In such  cases, the priority based mechanism allows for short control  messages to bypass long worms that are blocked in the  network. It achieves a delay reduction of 26% and 24% for  regular and exclusive read respectively in a heavily loaded  NoC, and a 10% delay reduction for both read and read  exclusive transactions in a lightly loaded NoC.  We also explore the potential delay reduction of using a  Priority-based NoC compared to the Vanilla-approach  across several benchmarks over a medium loaded NoC. The  results summarized in  Figure 10 clearly show the advantage  of Priority-based NoC over Vanilla approaches.   L2 Access Delay Reduction by Priority-based NoC 32.9 28.4 25.3 Read Read Exclusive 31.8 28.0 19.6 18.3 22.3 22.6 13.5 35.0 30.0 25.0 20.0 15.0 10.0 5.0 0.0 ] % [ n o i t c u d e R y a e l D apache zeus fft ocean radix Figure 10. Average L2 access delay reduction in read and read exclusive  transactions by using Priority-based NoC over several benchmarks  The actual L2-transaction delay as well as the delay  reduction of the Priority based approach is a function of  many factors  that depend on  the system and  the  benchmarks. These include the frequency of L2-access (or  inter-transaction delay), the load of the specific routing path  in the network, the amount of read-write sharing in the  benchmark and as a result the amount of invalidations and  write backs, the number of sharers that need to be  invalidated during each invalidation, the distance from the  desired L2 bank and finally the contention on that bank.  Despite this complication, using simple and low-cost  Priority-based NoC approach we succeed to achieve a  substantial delay reduction across a variety of benchmarks.  The maximal delay reduction reached 33% in FFT read and  28% in Zeus read exclusive transactions.   Finally, we examine the total application speedup using a  Priority-based NoC. The results are depicted in  Figure 11.  We observe that the priority-based NoC improves the  overall system performance in all benchmarks. The best  improvement is achieved in Zeus, which boosts overall  system performance by 9.4%. The system performance  improvement strongly depends on the amount of L2-delay  reduction and the frequency of L2-access in the benchmark.  Total Program Speedup by Priority -based NoC 8 .7 9.0 8.6 10.0 9.4 5.0 ] % [ p u d e e p S 9.0 8.0 7.0 6.0 5.0 4.0 3.0 2.0 1.0 0.0 apache zeus fft ocean radix Figure 11. Overall Program Speedup by using priorities over several  benchmarks  5. SUMMARY  We explored the NoC communication paradigm associated  with cache coherency of static and dynamic NUCA CMP.  We described its basic mapping over a Vanilla-NoC  system, including the details of processor and L2 network  interfaces. The main contribution of the paper is the  extension of the vanilla NoC and processor interface with a  simple and low cost priority mechanism. Such Prioritybased NoC is capable of differentiating and prioritizing  short control messages from long data packets. This generic  approach  fits  and  expedites  almost  any CMP  communication task (i.e uncached and cache-coherent R/W,  search  in DNUCA,  isolating  low priority  traffic,  synchronization and mutual exclusion support) for any  proposed coherency-protocol modifications. In particular,  we showed how to boost performance of directory-based  coherency protocol using priority-based NoC, while  maintaining coherency correctness. We show how to further  enrich the unicast-based communication services of such a  Vanilla NoC by Advanced-Services NoC mechanisms such  as: virtual invalidation rings, efficient store-and-forward  multicast for short messages which is embedded within a  wormhole NoC, and a cache-line search mechanism for the  efficient operation of dynamic NUCA. In addition to cache  coherency operations, these mechanisms can also improve  other basic CMP  transactions such as search and  synchronization support. Detailed CMP-NoC simulations  show an impressive (up to 33%) reduction in L2-access  delay by using priority-based NoC instead of the simplistic  Vanilla-NoC across a variety of benchmarks. The  simulations also demonstrate a substantial total system  speedup in all simulated benchmarks (up to 9.4% speedup).  6. "
A Generic Model for Formally Verifying NoC Communication Architectures - A Case Study.,"Networks on chip are emerging as a promising solution for the design of complex systems on a chip, to interconnect manufactured IP cores, and the need to formally guarantee their correctness is crucial. In a NoC centered design, the individual IP's are considered already validated. This paper addresses the validation of the communication infrastructure. A generic formal model for NoC's has been developed and implemented in the ACL2 theorem prover. As an application, the HERMES network has been formalized in this model, and we show that both formal proofs and simulation experiments can be performed in ACL2","A Generic Model for Formally Verifying NoC Communication Architectures: A Case Study Dominique Borrione, Amr Helmy, Laurence Pierre TIMA Laboratory - INPG 46 Av. F ´elix Viallet 38031 Grenoble cedex - France Email: name.surname@imag.fr Julien Schmaltz Saarland University Computer Science Department 66041 Saarbr ¨ucken - Germany Email: julien@cs.uni-sb.de Abstract Networks on Chip are emerging as a promising solution for the design of complex Systems on a Chip, to interconnect manufactured IP cores, and the need to formally guarantee their correctness is crucial. In a NoC centered design, the individual IP’s are considered already validated. This paper addresses the validation of the communication infrastructure. A generic formal model for NoC’s has been developed and implemented in the ACL2 theorem prover. As an application, the HERMES network has been formalized in this model, and we show that both formal proofs and simulation experiments can be performed in ACL2. 1 Introduction The Network-on-Chip paradigm is emerging as a promising solution for the design of complex Systems on a Chip (SoC). Recent papers [3], [24] give an overview of the key problems in the design of NoC’s, pinpointing related research domains: topology and sizing, optimization, and all the topics related to communications (routing, switching, . . . ), [29], [19]. These papers put an emphasis on the ﬂow control optimization, while guaranteeing ﬂawless communications. Widespread current techniques to analyse the behaviour of NoC’s are based on simulation or, more recently, on emulation ([11], [26],...). Some approaches yield solutions to fault detection [13] or error recovery [23]. Formal veriﬁcation methodologies provide a reliable alternative for the validation of the communication infrastructure: mathematical techniques are used to prove desired properties. In this context, we propose a method for verifying the correctness of the communication operations, which is based on a generic formal model for the representation of the communications on a chip, called GeNoC [30, 31]. To our knowledge, it is the ﬁrst attempt to deﬁne a general framework for proving correctness statements about communications in parameterized NoC’s. This model has been used to formally prove properties of various communication functions, such as the routing algorithm of the Octagon NoC from STMicroelectronics [14]. This approach is generic, a major advantage: while performing the proof, parameters such as the size of the network or the length of messages need not be instantiated. The model has been encoded in the ACL2 logic [16]. In this paper, we describe the formalization of the HERMES NoC [22]. An extension of the GeN oC model was needed. We show the complementarity of test case simulation and formal proof to validate the correctness of the HERMES high-level functional speciﬁcation. 2 Formal veriﬁcation techniques 2.1 Overview Through the use of mathematical techniques, formal veriﬁcation provides reliable methods for the validation of functional or temporal aspects of hardware components [18, 9, 17]. Formal reasoning requires a formal speciﬁcation of the system under consideration and of the properties to be veriﬁed. Such a formal speciﬁcation is made within a logic that depends on the characteristics of the system and properties: ﬁrst-order logic, higher-order logic, temporal logics like LTL or CTL, . . . Depending on the veriﬁcation to be done, different tools can be involved: • To compare two versions of the design at different abstraction levels, typically a gate netlist compared to an RTL model, equivalence checkers are to be used (for instance the tools Formality of Synopsys or FormalPro of Mentor Graphics). • To check temporal properties expressed using a temporal logic or a speciﬁcation language like PSL [1] or SVA [33], the appropriate tools are model-checkers (e.g. RuleBase of IBM or Solidify of Averant). • When the speciﬁcation involves parameters or complex data types and operations (for instance unbounded integers or real numbers), formal proofs require a theorem prover or proof assistant like ACL2 [16], PVS [25] or HOL [12]. Model-checkers implement algorithmic techniques and provide fully automated solutions to verify properties over ﬁnite-state systems. For inﬁnite-state systems, such as designs with data paths, and for the validation of properties on abstract speciﬁcations, these tools can no longer be applied fully automatically: abstractions (such as data path width reduction) are necessary [6]. Theorem provers and proof assistants implement deduction techniques, under the form of mechanized inference rules or interactive tactics, and often require user-guidance. The counterpart is their applicability to high-level and/or parameterized speciﬁcations. Deﬁning a high-level generic formal model for Networks on Chip, and encoding it into a theorem prover, we can perform high-level reasoning on a large variety of potentially inﬁnite-state systems. To our knowledge, this is the ﬁrst application of theorem proving techniques to the formalization of a generic model for NoC architectures. Our utmost objective is to build a formal theory for communication networks. The results reported here lay the groundwork for such a theory, and some simplifying hypotheses have been made. Many results have been proposed in the ﬁelds of protocol or network veriﬁcation, but none of them have actually tried to provide a generic formalization. Most of them are based on model-checking techniques. For instance, [5] uses the notion of regular languages and involves abstraction functions to verify temporal properties of families of systems represented by context-free network grammars. Applications such as the Dijkstra’s token ring and a network of binary trees are reported. In [7], the focus is on the inductive structure of branching tree networks. The authors put the emphasis on data independence: data are abstracted in order to use the FDR model checker to prove properties of CSP speciﬁcations. Theorem provers, or combinations of theorem provers and model-checkers, have also been exploited, but most results concern one speciﬁc architecture. The HOL theorem prover is used in [8] to verify a speciﬁc network component, the Fairisle ATM switching fabric. In this proof effort, a structural description of the fabric is compared to a behavioural speciﬁcation. The proof of a broadcasting protocol in a binary tree network is reported in [2]. It makes use of the combination of the theorem prover Coq and the model-checker Spin. In [10], the Æthereal protocol of Philips has been speciﬁed in the PVS logic. The main property that has been veriﬁed is the absence of deadlock for an arbitrary number of masters and slaves. 2.2 ACL2 The ACL2 theorem prover [16] has been developed at the University of Texas at Austin. It supports ﬁrst-order logic without quantiﬁers and is based on powerful principles, among which: • the deﬁnition principle, which allows to deﬁne recursive functions, with a strong veriﬁcation of the correctness of the recursive form by the system, • the induction principle, on which the induction heuristics of the proof mechanism is based. An induction variable is automatically chosen and an induction scheme is automatically generated. The speciﬁcation language is the functional programming language Common Lisp, hence ACL2 allows to perform formal reasoning as well as execution on the same speciﬁcations. Predeﬁned data types are: Booleans, characters and strings, rational numbers, complex numbers, and lists. The language is not typed i.e., the types of the function parameters are not explicitly declared. Predicate functions are used instead, in the function bodies (for instance (intergerp x) checks whether x is an integer). This prover has already been used to formally verify various complex hardware architectures, such as microprocessors [4, 28], ﬂoating point operators [21], and many other structures [15]. 3 A generic model The generic model GeNoC is deﬁned as the composition of key components: routing, scheduling and interfaces. Essential properties inherent to each one of them have been identiﬁed, and the overall correctness of GeNoC is directly deduced from these constraints. 3.1 Communication abstraction GeNoC is based on an abstract view of the communications, see Figure 1. An arbitrary, but ﬁnite, number of nodes are connected to some communication architecture that represents the interconnection structure, e.g. bus or network. Our model includes topologies, routing algorithms and scheduling policies, but makes no assumption on these components. As proposed by Rowson and SangiovanniVincentelli [27], each node is made of an application and an interface. The interface is connected to the communication messages Application Interface frames messages Application Interface frames Application Interface frames Communication Architecture Application Interface messages frames messages 3.2.3 Switching technique architecture. Interfaces allow applications to communicate using protocols. An interface and an application communicate using messages but two interfaces communicate using frames (messages that are sent from one application to another one are encapsulated into frames). Applications are either active or passive. Typically, active applications are processors and passive applications are memories. We consider that each node contains one passive and one active application, i.e. each node is capable of sending and receiving frames. As we want a general model, applications are not considered explicitly: passive applications are not actually modeled, and active applications are reduced to the list of their pending communication operations. Figure 1. Communication Model 3.2 The model The model represents a generic communication architecture. It encompasses a set of functions that specify the characteristics of an arbitrary topology, routing algorithm and switching technique [30, 31, 32]. Function GeNoC represents the transfer of messages from their source to their destination, a pictured on Figure 2. Its main argument is the list of emitted messages. It returns the list of the results received at destination nodes. Its deﬁnition mainly relies on the following functions: • interfaces are represented by two functions: send to inject frames on the network, and recv to receive frames, • the routing algorithm and the topology are represented by function Routing , • the switching technique is represented by function Scheduling . These functions are not given explicit deﬁnitions. Rather, they are characterized by the set of properties they should satisfy, called proof obligations or constraints. 3.2.1 Interfaces Function send represents the encapsulation of a message into a frame, and recv represents the decoding of this frame to recover the emitted message. The main constraint associated to these functions exinformation, i.e. the composition (recv ◦ send ) of function presses that a receiver should be able to extract the encoded recv with function send is the identity function. 3.2.2 Routing algorithm The routing algorithm is represented by the successive application of unitary moves (e.g. routing hops). For each source-destination pair, the routing function computes all possible routes allowed by the unitary moves. The main constraint associated to the routing function expresses that each route from a source s to a destination d actually starts in s, uses only existing nodes, and ends in d. The scheduling policy participates in the management of conﬂicts. It deﬁnes the set of communications that can be performed at the same time: formally these communications satisfy an invariant that is speciﬁc to the scheduling policy. Scheduling a communication, i.e. adding it to the current set of authorized communications, must preserve the invariant, in any admissible state of the network. In our formalization, the existence of this invariant is assumed but not explicitly represented. From a list of requested communications, the scheduling function returns a sub-list of communications that satisfy the invariant. The other communications are delayed. 3.2.4 Function GeNoC The model considers a list T of transactions of the form t = (id A msgt B). Transaction t represents the intention of the application A to send a message msgt to the application B . A is the origin and B the destination. Both A and B are members of the set of nodes, NodeSet . Each transaction is uniquely identiﬁed by a natural id. For every message in the initial list of transactions, the GeNoC model computes the corresponding frame using send . Each frame together with its id, origin and destination constitutes a missive. A missive is valid if the ids are naturals (with no duplicate) and if the origin and the destination are members of NodeSet . The GeNoC function computes the routes of the missives and schedules them using functions Routing and Scheduling . It takes as parameters: • M : a set of missives in the network, • N odeS et: the characterization of the network, • att: attempts allowed to send messages. To make sure that GeNoC terminates, we associate to every node a ﬁnite number of attempts. At every recursive call, Scheduling Node A Interface Node B Interface Node A Application Messages recv send Frames Frames recv send Messages Node B Application Node A Node B Routing Figure 2. Generic model every node with a pending transaction will consume one attempt. Function SumOfAtt (att ) computes the sum of the remaining attempts of the nodes and is used as the decreasing measure of parameter att . GeNoC halts if every attempt has been consumed, • T : the set of missives that can actually be completed (originally empty). Its deﬁnition is: GeNoC(M, NodeSet, att, T) = if SumOfAttempts(att)=0 then list(T, M) else let (S D att’) = Scheduling(Routing(M, NodeSet), att) in GeNoC(D, NodeSet, att’, S U T) This deﬁnition can be understood as follows: • if no attempt is left, it stops and returns a pair composed of the scheduled (T) missives and the aborted (M) missives, • otherwise the composition of functions S cheduling and Routing computes two lists: the missives that can be scheduled simultaneously (S cheduled, or S ) and the temporarily delayed missives (Delayed, or D). Then the recursive call to GeNoC tries to schedule the formerly delayed missives (in D), once the scheduled missives have reached their destination and released the nodes on their route. Function Routing computes a list of routes for every missive. Once routes are computed, a travel denotes the list composed of a frame, its id and its list of routes. The role of function S cheduling is to try to schedule the corresponding communications while avoiding conﬂicts. Function GeNoC is considered correct if every non aborted transaction t = (id A msg B) is completed in such a way that B actually receives msg: ∀r ∈ S cheduled, ∃ ! m ∈ M, (cid:1) Id (r ) = Id (m) ∧ Msg (r ) = Msg (m) ∧ Dest (r ) = Dest (m) This theorem states that every scheduled missive actually corresponds to a missive issued at a node of the network (no production of new messages), and its message reaches the intended destination. This general proof is done once and for all. Proving that a particular NoC is a valid instance of GeNoC amounts to: • deﬁning the functions that describe the network topology, • deﬁning all the functions associated with the communications, in particular Routing and S cheduling , • discharging the proof obligations for these functions. 4 A veriﬁcation-oriented description of the HERMES NoC This section describes the original speciﬁcation of the HERMES NoC, and the architecture we have actually veriﬁed. Some simplifying hypotheses have been made to fulﬁl the constraints of GeNoC . Future work will aim at improving the model to relax these constraints. 4.1 Speciﬁcation of HERMES HERMES [22] is the result of a cooperation between the Catholic University of Rio Grande do Sul (Porto Alegre, Brazil) and LIRMM (Montpellier, France). It is based on a regular 2D mesh architecture (Figure 3). Each node is made of an IP core and a switch. Each switch has routing control logic and ﬁve bi-directional ports (Figure 4): E ast, W est, N orth, S outh for the communications with the neighbor switches, and Local to communicate with the IP core. Figure 3. Mesh architecture [22] The routing policy is based on a deterministic, minimal algorithm: the XY routing algorithm. Each packet is routed on one dimension at a time, it travels ﬁrst along the X axis until the ﬁrst coordinate of the destination is reached, and then travels along the Y axis. The wormhole scheduling technique has been chosen: packets are decomposed into smaller units called ﬂits. The ﬂits may be: a header ﬂit (worm’s head), a data ﬂit (worm’s body), or a control ﬂit. Only the header ﬂit (the ﬁrst one) contains information necessary to route the packet through the network. The route is determined for this header ﬂit and the rest of the ﬂits (the “body” of the worm) follows the path of this ﬂit. The second ﬂit is a NOF (Number Of Flits) control ﬂit. Figure 4. HERMES switch [22] Each port of the switch has an input buffer queue. A Round Robin priority policy is used to access the output ports. If the requested port is busy, the ﬂits are blocked in this node and the request signal remains active. 4.2 The formally proven architecture The VHDL sources of HERMES are publicly available1 and we have used them to generate our ACL2 representa1 http://toledo.inf.pucrs.br/˜gaph/Projects/Hermes/Hermes.html tion of HERMES. To that goal we have partly extended the original GeNoC model which overly simpliﬁed the representation of a network: • it assumes that messages are atomic, • each node has only one port, and two or more messages cannot occupy a node simultaneously, • there is no explicit notion of time, and a communication action is not considered to be the move from one node to its neighbour, but the move from the source to the destination, • as a consequence, the scheduling policy is simpliﬁed: a centralized control is assumed, each recursive call to the GeNoC function computes the set of messages that can be “scheduled” together (i.e., without conﬂict) and delays the other ones, to be processed by the next recursive call. Hence, messages for which a conﬂict is predictable do not leave their source, and are blocked here instead of being blocked in the node where the conﬂict occurs. These simpliﬁcations have been instrumental in performing the proof of the main theorem, and in elaborating the general modeling method. We are currently reﬁning the model, and relaxing some constraints in order to more realistically portray actual NoC behaviour. The ﬁrst reﬁnement allows to process non-atomic messages, thus enabling the wormhole switching technique. Successive ﬂits that follow each other using the same route can now be modeled. We have also extended the original model to express the fact that a node can have several ports, and that it can be occupied simultaneously by several messages provided that they do not use the same ports in the same direction. With this extension, the wormhole and XY routing algorithm can be considered faithfully. Some restrictions still apply at this stage, which slightly impact the GeN oC functional speciﬁcation of HERMES. For instance, the Round Robin priority policy has been simpliﬁed and transformed into a centralized scheduling control. As a consequence, the model used in ACL2 translates the original speciﬁcation, except that messages that are blocked in a node of their route in case of conﬂict in the original speciﬁcation, are blocked in their source node in the formal model. This can be veriﬁed by simulation, as we will show in section 5.2: the messages in both descriptions go through the same nodes, using the same ports, and reach the right destination, but are not blocked in the same place. As a result, the state of each message separately is the same in both models, but the global state is slightly different. We discuss below two simulations in a 3x3 and in a 4x5 VHDL implementations of the original HERMES network. The ﬁrst one is illustrated on Figure 5. Three messages are considered, as shown in Table 1. Messages 1 and 2 are split into 3 ﬂits (hence their total number of ﬂits is 5), and message 3 is split into 5 ﬂits (its total number of ﬂits is 7). Figure 5. Example 1 ID Source Destination 1 (1 1) (2 0) 2 (1 2) (2 0) 3 (0 1) (1 2) Contents 11 21 31 12 22 32 13 23 33 43 53 Nb of ﬂits 5 5 7 Table 1. Messages of example 1 The result of the VHDL simulation of this example is displayed on Figure 6. For the sake of clarity, only some output ports appear on this snapshot. Each node is represented by a signal data showXY, where X and Y are the node coordinates. Each signal is made of ﬁve 8-bit vectors, one for each port: 0 for East, 1 for West, 2 for North, 3 for South, and 4 for Local. In this simulation, we see the evolution of the three messages. Each message is made of a ﬁrst ﬂit that contains its destination (for instance 12 for message 3), a second ﬂit that gives the total number of ﬂits (for instance 7 for message 3), and the remaining ﬂits are made of the actual contents of the message. We can see that message 3 goes successively through data show01(0) i.e., the East port of node (0,1), data show11(3) i.e., the South port of node (1,1), and data show12(4) i.e., the Local port of node (1,2), to get to its destination. Similarly, message 1 goes successively through data show11(0) i.e., the East port of node (1,1), data show21(2) i.e., the North port of node (2,1), and data show20(4) i.e., the Local port of node (2,0). As for message 2, its path must be: data show12(0), data show22(2), data show21(2), and data show20(4). Since the North port of node (2,1) is already occupied by message 1 when message 2 arrives, this message is blocked in this node as long as message 1 gets through. Then message 2 completes its travel. Figure 7. Example 2 The second example is described by Figure 7 and Table 2. It involves a larger mesh and more messages, but remains simple for readability. In this example, four messages are sent in the network. None of them is blocked, because they do not use the same ports in the same direction. However this example shows that messages can go across the same node at the same time using different ports (messages 2 and 3 occupy the node (1,3) at the same time), or even using the same port provided that they do not go in the same direction (messages 3 and 4 use simultaneously the nodes of the fourth column). The travels of these messages in the NoC can be observed by simulation, as shown on Figure 8. ID Source Destination 1 (0 0) (2 1) 2 (1 1) (1 4) 3 (0 3) (3 1) 4 (4 0) (3 4) Contents 11 21 31 12 22 13 23 33 43 14 24 34 44 54 Nb of ﬂits 5 4 6 7 Table 2. Messages of example 2 5 HERMES in GeNoC This section summarizes the formalization of HERMES in our extended version of GeNoC , and its encoding in ACL2. We describe the formal veriﬁcation process, and then discuss simulations in ACL2. 5.1 Formal proof First, all the primitive data types have been redeﬁned to take into account the number of ﬂits; for instance a transaction is now made of: a message identiﬁer id, a source s, a message m, a destination d, and a number of ﬂits n. Figure 6. VHDL simulation of example 1 Figure 8. VHDL simulation of example 2 5.1.1 Network topology A node is deﬁned as a 4-tuple (X Y P D), where X and Y are the coordinates of the node, P ∈ {E , W , S , N , L} is the port name and D ∈ {input, output} is the direction. The encoding of the network topology in ACL2 required the deﬁnition of 10 functions and the veriﬁcation of 6 proof obligations (essentially typing properties). 5.1.2 Routing function In GeNoC , the Routing function computes the list of all possible routes between each source and destination (in case of a deterministic algorithm, only one route is feasible). It takes as parameters a list of missives and returns a list of routes. Here is our formalization of the XY routing algorithm in HERMES: XYRouting(M) = if empty(M) then nil else let miss=car(M) /* first missive */ and from=Org(miss) /* origin */ and to=Dest(miss) /* destination */ and id=Id(miss) /* identifier */ and frm=Frame(miss) /* frame */ and flits=Flit(miss) /* # flits */ in cons(list(id,frm,XY-routing(from, to),flits), XYRouting(cdr(M))) /* recursive call */ where the XY-routing function is a recursive ACL2 encoding of the following algorithm: XYRouting_algorithm(from,to) : if from=to /* destination reached */ then take the Local port else if Xfrom != Xto then /* change X */ if Xfrom < Xto then take port East else take port West /* change Y */ if Yfrom < Yto then take port South else take port North else Four proof obligations and 44 intermediate lemmas have been proven. Most of them ensure that routes are wellformed (see section 3.2.2). 5.1.3 Scheduling function In GeNoC , the S cheduling function takes as parameters a list of routes (computed by Routing ), and selects the feasible ones. It returns a list S of scheduled missives and a list D of temporarily delayed missives. Our function W ormH oleS ched is the realization of this function for the wormhole scheduling in the 2D mesh. It calls on the recursive function W ormH S ched below, that is the heart of this algorithm. We recall that a travel means the association of a missive with the set of all its possible routes. Function W ormH S ched takes as parameters a list of travels L, the current set of simultaneously scheduled missives S , the current set of temporarily delayed travels D , and a list P that describes the current state of the network (nodes occupied by the missives in S ). It puts recursively all the elements of L either in S or in D depending on route compatibilities with the elements of S . Its deﬁnition is as follows: WormHSched(L, S, D, P) = if empty(L) then list(S, D, P) else let tr=first(L) /* first travel */ and n=nbflits(tr) /* number of flits */ and r=routesOf(tr) /* set of routes */ and c=check_routes(n,r,P) in if c=true then WormHSched(rest(L), S U update(tr), D, updateP(n,P)) else WormHSched(rest(L), S, D U tr, P) More precisely, for each travel in L, let n be the number of message ﬂits, and r be the set of routes. Function check routes checks whether there exists a route in r that is compatible with the current state of the network P : every ﬂit will be able to travel along this route i.e., will not cross the route of another message in a same node (with the same ports) at the same time. If this check succeeds, the missive with this route is added to S and the state of the network is updated using function updateP . Otherwise, the travel is added to D . The deﬁnitions of functions check routes and updateP require great care because they must take into account the travel of all the ﬂits along the same route, in the right order. Twelve proof obligations have been proven using 20 intermediate lemmas. An essential one states that, after each recursive call of W ormH S ched, the intersection between S and D remains empty. 5.1.4 Instantiation of the GeNoC function Once the generic model has been instantiated for the HERMES network, as explained above, the main function GeNoC remains to be instantiated. It is similar to the generic one given in section 3.2, where Routing and S cheduling are instantiated by X Y Routing and W ormH oleS ched respectively: GeNoC(M, NodeSet, att, T) = if SumOfAttempts(att)=0 then list(T, M) else let (S D att’) = WormHoleSched(XYRouting(M, NodeSet), att) in GeNoC(D, NodeSet, att’, S U T) As a ﬁnal result we obtain the correctness theorem for HERMES by instantiating the generic formula of section 3.2.4. It is worth mentioning that this proof is generic on the size of the 2D-mesh and on the number of ﬂits of messages. We give below the main ﬁgures of the complete proof (CPU times are measured on a Intel Core Duo T2400). Proof Generic model HERMES topology HERMES routing HERMES scheduling HERMES correctness # functions 71 10 6 7 2 # theorems 119 6 48 32 1 CPU time (s) 29.75 6.25 54.87 10.72 5.56 5.2 ACL2 simulations ACL2 provides both a theorem prover and an execution engine in the same environment. Speciﬁcations are written in Common Lisp. Theorems that express properties of these functions can be proven, and the same function definitions can be executed efﬁciently [20]. Thus we can get conﬁdence in the formal model by comparing the ACL2 and VHDL simulations. Let us describe the ACL2 simulations of the examples of section 4.2. The ACL2 simulation of example 1 (Figure 5) ﬁrst shows the progression of messages 1 and 3 in the network, and then the travel of message 2. The only difference between the original simulation and this one is here: with the GeN oC model, the fact that message 2 cannot travel simultaneously with message 1 is expressed by a situation where message 2 is blocked in its source node i.e., node (1,2), and starts its travel afterwards. The results obtained for messages 1 and 3 are presented step by step in Table 3 below. Flits are numbered in decreasing order, from nbﬂits-1 downto 0. In the ﬁrst step, the ﬂit number 6 of message 3 takes the port L of node (0 1), and the ﬂit number 4 of message 1 takes the port L of node (1 1). In the second step, two ﬂits of each scheduled message are visible: the ﬂit 6 of message 3 takes the port E of node (0 1), and its ﬂit 5 takes the port L. Similarly, the ﬂit 4 of message 1 takes the port E of node (1 1) while its ﬂit 3 follows by the port L. After 10 and 12 steps respectively, the last ﬂits (number 0) of these messages reach the local ports of nodes (2 0) and (1 2). Each message separately has exactly the same behaviour as in the VHDL simulation (Figure 6). Step 1st step 2nd step 3rd step 4th step 5th step ... 10th step 11th step 12th step Evolution of the messages ( ((3 6)(0 1 L i)) ((1 4)(1 1 L i)) ) ( ((3 5)(0 1 L i)) ((3 6)(0 1 E o)) ((1 3)(1 1 L i)) ((1 4)(1 1 E o)) ) ( ((3 4)(0 1 L i)) ((3 5)(0 1 E o)) ((3 6)(1 1 W i)) ((1 2)(1 1 L i)) ((1 3)(1 1 E o)) ((1 4)(2 1 W i)) ) ( ((3 3)(0 1 L i)) ((3 4)(0 1 E o)) ((3 5)(1 1 W i)) ((3 6)(1 1 S o)) ((1 1)(1 1 L i)) ((1 2)(1 1 E o)) ((1 3)(2 1 W i)) ((1 4)(2 1 N o)) ) ( ((3 2)(0 1 L i) ((3 3)(0 1 E o)) ((3 4)(1 1 W i)) ((3 5)(1 1 S o)) ((3 6)(1 2 N i)) ((1 0)(1 1 L i)) ((1 1)(1 1 E o)) ((1 2)(2 1 W i)) ((1 3)(2 1 N o)) ((1 4)(2 0 S i)) ) .... ( ((3 0)(1 1 S o)) ((3 1)(1 2 N i)) ((3 2)(1 2 L o)) ((1 0)(2 0 L o)) ) ( ((3 0)(1 2 N i)) ((3 1)(1 2 L o)) ) ( ((3 0)(1 2 L o)) ) Table 3. ACL2 simulation of example 1 Similarly, the ACL2 simulation of example 2 (Figure 7) gives results that are comparable to the ones of Figure 8. These results are summarized in Table 4. We see that, in the ﬁrst step, the ﬂits number 6 of message 4, number 5 of message 3, number 3 of message 2, and number 4 of message 1 take the local ports of their origin nodes. This table displays the progression of these four messages in the network, up to the 17th step, in which the last ﬂit of message 3 reaches its destination node. The ACL2 simulations yield such textual results, made of lists. Since these informations are not easily readable, a tool has been developed in Java for the animated visualization of these results. It takes as input the ACL2 results and performs an animation of the evolution of the messages in the network. Figure 9 gives a screenshot of the 7th iteration for example 2 above (see Table 4). Figure 9. Animated ACL2 simulation Step 1st 2nd 3rd 4th . . . 7th . . . 15th 16th 17th Evolution of the messages ( ((4 6)(3 0 L i)) ((3 5)(0 3 L i)) ((2 3)(0 1 L i)) ((1 4)(0 0 L i)) ) ( ((4 5)(3 0 L i)) ((4 6)(3 0 S o)) ((3 4)(0 3 L i)) ((3 5)(0 3 E o)) ((2 2)(0 1 L i)) ((2 3)(0 1 E o)) ((1 3)(0 0 L i)) ((1 4)(0 0 E i)) ) ( ((4 4)(3 0 L i)) ((4 5)(3 0 S o)) ((4 6)(3 1 N i)) ((3 3)(0 3 L i)) ((3 4)(0 3 E o)) ((3 5)(1 3 W i)) ((2 1)(0 1 L i)) ((2 2)(0 1 E o)) ((2 3)(1 1 W i)) ((1 2)(0 0 L i)) ((1 3)(0 0 E o)) ((1 4)(1 0 W i)) ) ( ((4 3)(3 0 L i)) ((4 4)(3 0 S o)) ((4 5)(3 1 N i)) ((4 6)(3 1 S o)) ((3 2)(0 3 L i)) ((3 3)(0 3 E o)) ((3 4)(1 3 W i)) ((3 5)(1 3 E o)) ((2 0)(0 1 L i)) ((2 1)(0 1 E o)) ((2 2)(1 1 W i)) ((2 3)(1 1 S o)) ((1 1)(0 0 L i)) ((1 2)(0 0 E o)) ((1 3)(1 0 W i)) ((1 4)(1 0 E o)) ) . . . ( ((4 0)(3 0 L i)) ((4 1)(3 0 S o)) ((4 2)(3 1 N i)) ((4 3)(3 1 S o)) ((4 4)(3 2 N i)) ((4 5)(3 2 S o)) ((4 6)(3 3 N i)) ((3 0)(0 3 E o)) ((3 1)(1 3 W i)) ((3 2)(1 3 E o)) ((3 3)(2 3 W i)) ((3 4)(2 3 E o)) ((3 5)(3 3 W i)) ((2 0)(1 1 S o)) ((2 1)(1 2 N i)) ((2 2)(1 2 S o)) ((2 3)(1 3 N i)) ((1 0)(1 0 W i)) ((1 1)(1 0 E o)) ((1 2)(2 0 W i)) ((1 3)(2 0 S o)) ((1 4)(2 1 N i)) ) . . . ( ((4 0)(3 4 N i)) ((4 1)(3 4 L o)) ((3 0)(3 2 N o)) ((3 1)(3 1 S i)) ((3 2)(3 1 L o)) ) ( ((4 0)(3 4 L o)) ((3 0)(3 1 S i)) ((3 1)(3 1 L o)) ) ( ((3 0)(3 1 L o)) ) Table 4. ACL2 simulation of example 2 6 Conclusion As circuits become more complex, and design increasingly relies on a platform-based approach and the availability of generic IP’s, the early and high-level functional validation of the communications gains importance. In this context, GeNoC is a ﬁrst generic functional model for interconnection networks; it is written in a logic that is both executable and mathematically rigorous, allowing both the simulation of test cases, and the abstract reasoning about the properties of the same model description. Like most initial formalizations, the original GeNoC model made simplifying assumptions, in order to concentrate on the most essential features, and exhibit a methodology. Previous works had shown that GeNoC was applicable to a variety of topologies, routing and scheduling policies, taken in isolation. The work reported in this paper is the ﬁrst attempt to completely cover all the essential functions of a same NoC. We took HERMES as case study because it is a realistic design, the RTL code of which is publicly available. In selecting HERMES, we knew that the switching hypothesis made in the original GeNoC was too crude, and we performed a ﬁrst extension of the formal model, that covers the characteristics of message splitting and wormhole scheduling. This paper describes how these features have been added to GeNoC , and the necessary proof obligations discharged on the extended model. This last point is an essential principle of the modeling approach: these proof obligations are a necessary condition for the overall correctness theorem, irrespective of the network size parameters (number of nodes, ﬂit size, etc.). The application of the extended GeNoC model to HERMES took about 3 man-months. We estimate that the time development for similar case studies would be counted in weeks. The current modeling level has no timing information. We are investigating extensions of this model to deal with an explicit notion of time, and a characterization of properties like deadlocks and livelocks. Many more reﬁnement steps are needed before a cycle accurate modeling of the RTL design can be reached. At each step, a proof of compliance between the more detailed model and the more abstract one is needed. Our current efforts aim at providing a systematic method to help the designer and the proof engineer show that design decisions along the reﬁnement path preserve the correct behavior of the network. "
A Hybrid Ring/Mesh Interconnect for Network-on-Chip Using Hierarchical Rings for Global Routing.,"A popular network topology for network-on-chip (NoC) implementations is the two-dimensional mesh. A disadvantage of the mesh topology is in its large communication radius. By partitioning a two-dimensional mesh into several sub-meshes and connecting them using a global interconnect, we can reduce the average number of hops for global traffic. This paper presents a hybrid architecture that partitions a large 2D-mesh into several smaller sub-meshes which are globally connected using a hierarchical ring interconnect. Hierarchical rings have been selected for study because of their simplicity, speed and efficiency in embedding onto a circuit layout, as well as for their suitability for efficient cache coherent protocols. An original SystemC modeling platform was implemented in order to compare the traditional 2D-mesh with the hybrid ring architectures and the simulation results will show that our hybrid architecture does indeed have a positive effect on the average hop count","A Hybrid Ring/Mesh Interconnect for Network-on-Chip Using Hierarchical Rings for Global Routing S. Bourduas, Z. Zilic Department of Electrical and Computer Engineering McGill University Montreal, Quebec, Canada {stephan, zeljko}@macs.ece.mcgill.ca Abstract— A popular network topology for Network-on-Chip (NoC) implementations is the two-dimensional mesh. A disadvantage of the mesh topology is in its large communication radius. By partitioning a two-dimensional mesh into several sub-meshes and connecting them using a global interconnect, we can reduce the average number of hops for global trafﬁc. This paper presents a hybrid architecture that partitions a large 2D-mesh into several smaller sub-meshes which are globally connected using a hierarchical ring interconnect. Hierarchical rings have been selected for study because of their simplicity, speed and efﬁciency in embedding onto a circuit layout, as well as for their suitability for efﬁcient cache coherent protocols. An original SystemC modeling platform was implemented in order to compare the traditional 2D-mesh with the hybrid ring/mesh architectures and the simulation results will show that our hybrid architecture does indeed have a positive effect on the average hop count. I . IN TRODUCT ION The ﬁrst system-on-chip (SoC) implementations used buses to connect components together. As technology scaling enabled more cores to be integrated onto a single chip, it became apparent that bus-based approaches could not adequately handle the communication demands of multi-processor SoC (MPSoC) implementations. The main problem with shared medium approaches is that as the number of connected components increases, so does the parasitic capacitance, propagation delay, power consumption and arbitration times. There is therefore a practical limit to the number of cores which can be connected using shared medium approaches. Systems consisting of tens or even hundreds of cores are not feasible using shared medium architectures. A scalable alternative is presented by a network-centric approach, whereby packets are routed through an interconnect network [1], [2]. A network-on-chip (NoC) [1], [3], [4] which routes packets in a way similar to traditional networks would replace the bus and allow for systems with many more components to be efﬁciently connected together. A NoC interconnect consist of multiple switches connected together to form a suitable network topology [1]. In contrast to bus-based approaches, NoCs are more energy-efﬁcient, can support higher aggregated bandwidth, and most importantly offer greater scalability. The 2D-mesh [2] topology is a popular topology used for NoC interconnects. It consists of M × N number of tiles arranged in a grid where each tile is connected to its four neighbors (with the exception of the edge tiles). Because only neighboring nodes are connected, packets that need to travel long distances suffer from large hop counts. In this paper, we propose a hybrid topology that breaks a large mesh into smaller meshes connected by a hierarchical ring interconnect for routing global trafﬁc. We have selected hierarchical rings for study because of their simplicity, speed and the ease with which such structures can be laid out in 2D IC layouts. Recent industrial designs, such as the Cell processor and some high-end graphical processor from ATI employ multiple rings as their interconnect networks. To study the proposed topology in detail, we have developed and implemented a modular simulation platform in SystemC which enables us to compare the performance characteristics of our hybrid interconnect with that of the traditional 2D-mesh. I I . R E LAT ED WORK There are several topologies that have been proposed for use in NoCs which can be classiﬁed as either ﬂat or hierarchical. A detailed comparison of several architectures is presented in [5], [6]. As previously discussed, the 2D-mesh proposed in [2] is the most popular. The torus and folded torus are similar to the mesh but the edge switches are connected to the opposite switches in order to form a ring. The torus interconnect has a higher bisection bandwidth than the mesh, but also exhibits higher energy consumption [2]. An example of a hierarchical topology is the fat tree architecture presented in [7] which can support low latencies and high bandwidths depending on the chosen conﬁguration. The architecture in [7] uses dedicated feedback wires between pairs of receivers and senders for ﬂow control which we feel is not suitable for large scale NoCs with a large number of nodes. A second example of a hierarchical topology is the butterﬂy fat-tree (BFT) [8] where the number of switches converges to a constant depending on the number of levels. Unlike the mesh architecture proposed in [2], [9] where each cell is composed of a PE and a switch, the fat-tree and butterﬂy fat-tree place their processing elements at the leaves and the switches at the vertices of the tree [7], [8]. The Proteo [10] NoC is a hierarchical network topology which uses a global bidirectional ring to connect several subnets together. The topology of each subnet is chosen to suit local trafﬁc requirements. The architecture presented in citesiguenca-tortosa:csn02 has been built using the Scalable Coherent Interface (SCI) standardized by IEEE [11]. This standard has garnered some industrial acceptance as a ringbased network topology with its own distributed directory cache coherency protocol that is cache based and requires a linked list of cache locations to be maintained to keep the shared memory data coherent. Practical problems with SCI arose exactly because of the need to traverse the network following the linked list for each coherence operation. Somewhat similar ideas in the topology were also reused with the emergence of the Inﬁniband storage area network, which to our knowledge has not been used in any SoC implementations. Similarly, the Ring Road [12] topology was proposed with the idea of using ring switching elements in a manner that provides more bisectional bandwith and eliminates hotspots in the center. There are two types of ring interfaces, just like with the hierarchical rings: those that are on local rings, and those on intersection of a pair of rings. The motivation for this kind of interconnect is in avoiding congestion in the center of the area, similar to the rings of roads outside large cities. In [13], a reconﬁgurable system which uses a hierarchical mesh interconnection network consisting of nearest neighbor connectivity at the lowest level of the hierarchy and horizontal and vertical buses for global connectivity. The architecture presented in [14] takes the opposite approach and uses a hierarchical interconnect to link multiple bus-based SoCs together. A parameterizable library of components called Xpipes which can be used to generate domain-speciﬁc heterogeneous architectures is described in [15]. The architectures discussed in [15] are not hierarchical in nature and the problems of growing hop-counts and latencies associated with increasing network size are not addressed, however the authors state that arbitrary topologies can be achieved by their tool. In [16], the problem of large hop counts associated with routing packets over long distances in 2D-meshes is addressed through the use of express channels which span multiple hops. The drawback of using express channels is that they require long wires in each dimension and they also increase the router complexity. In [17], a hierarchical ring topology was introduced which made use of a two-tier hierarchical conﬁguration of unidirectional rings. The use of this topology was inspired by a NUMAchine multiprocessor designed at University in Toronto. As shown in [18], good speedups were observed for virtually all multiprocessor benchmarks, in spite of its apparent bisectional bandwidth limitations. The architecture was shown amenable to efﬁcient implementations, and the cache coherence protocol incorporated in NUMAchine exploited well the given topology resulting in a feasible and correct implementation. I I I . MOT IVAT ION A. Hybrid Topology It was shown in [16] that express channels can help reduce the latencies associated with routing global packets. The major disadvantage of this approach is that express channels are needed in both the x and y directions and so the routing complexity of this topology increases with the number of express channels; a problem which is further exacerbated as the mesh size gets larger and longer express channels are needed. In Section II, several tree-like hierarchies were brieﬂy discussed. When comparing hierarchical architectures to a 2Dmesh architecture, we can see that on average, the hop count for global packets is smaller, but the aggregate bandwidth is also reduced because of the bottlenecks associated with routing global packets through a small number of switches. On the other hand, less resources are required and thus the energy requirement for global interconnect will be reduced which is of critical importance as the interconnect has already been reported to account for a large portion of the total system energy requirement [19]. The challenge then becomes ﬁnding a balance between resource requirements and system performance. Since both the mesh and hierarchical topologies exhibit desirable characteristics with respect to embedding them onto the 2D layout of SoCs, combining them would enable us to draw on the strengths of each and result in reduced average hopcounts and latencies for global trafﬁc while still maintaining the high throughput that meshes exhibit for local trafﬁc. B. Suitability of Hierarchical Rings Our topology of unidirectional rings connected in a hierarchical manner exhibits several characteristics which are of importance to NoC implementations. The simplicity of the rings reduces the complexity at each node which results in reduced buffer, area and energy requirements. Furthermore, the topology discussed in this paper has no global routing so place-and-route will also be more efﬁcient than using global channels as was shown in [20]. The unidirectional nature of the rings reduces the overhead associated with routing and thus results in low latencies and high throughput. One limitation of the NUMAchine architecture that we try to address here is in the scalability of the hierarchical ring. Because the buses are used at the lowest level of the hierarchy in [18], the total number of nodes will be kept modest if the number of hierarchy levels is kept low. By replacing local buses with meshes, we can accommodate more processors for the same hierarchical ring. Similarly to [18], we aim to use the concept of the network cache placed at the gateways to rings, to facilitate efﬁcient cache coherency primitives, including the use of low-cost multicast/broadcast invalidations with guaranteed serialization property, which is in turn critical for both cache coherence and the useful consistency models. Also of interest is the fact that the hierarchical ring interconnect can be easily partitioned into multiple clock domains, giving designers increased ﬂexibility when tuning design parameters for individual applications. As discussed in [17], distinct clock domains enable the application of dynamic frequency and/or voltage scaling (DVS) techniques for energy optimization. Taking the ideas from [17] one step further, we can also envision the application of clock throttling to an entire sub-mesh which can be easily achieved since each mesh can exist in a separate clock domain. In the case of a heterogeneous architecture where certain types of computational units are assigned to a speciﬁc mesh, the energy savings has the potential to be signiﬁcant if entire sub-meshes could be powered down during idle periods. Sub-Mesh Local Mesh IV. ARCH I T ECTURE Fig. 1. Hybrid mesh architecture using hierarchical rings for global interconnect (N = 8) Our hybrid architecture a combination of a hierarchical ring interconnect with meshes, used in conjunction with a speciﬁc addressing and routing scheme. The hierarchical ring is used for routing global trafﬁc between several meshes. A. Hybrid Interconnect In contrast to other approaches which add complexity to the interconnect through the use of global global wiring [7], [16], we leave the 2D-mesh architecture unmodiﬁed and simply replace a processing element in one of the cells by a bridge component which enables sending global trafﬁc through the hierarchical interconnect transparently. Figure 1 shows the hybrid-mesh architecture where a 8 × 8 mesh has been split into 16 4 × 4 sub-meshes which are globally connected using a 2-level hierarchical ring interconnect. From Figure 1, we can deduce that if the width of the mesh being partitioned is of width N , then the width of a sub-mesh and the width of a local mesh is described by Eqs. 1, 2, where a sub-mesh is the smallest mesh in the system a local mesh is deﬁned as the theoretical mesh obtained by combining the 4 meshes connected to a local ring. Wsub = Wlocal = N 4 N 2 (1) (2) In a normal mesh network, the maximum number of hops that a packet will travel is when a packet is sent from a corner node to its diagonal opposite. For example, if node (0,0) send a packet to node (M-1, N-1), then the total number of hops will be M + N = 2N for when M = N . For some large enough value of N , the latencies incurred by the network will be too large for application software to support. The worst-case hop count for our hybrid interconnect depends on which tile in the mesh is used to connect to the hierarchical ring interconnect. If a corner tile is used, the worst case hop count can be described by Eq. 3 whereas if a tile in the middle of the sub-mesh is used, the worst case hop count can be described by Eq. 4. Note that the maximum number of hops a packet can take to travel through the hierarchical ring interconnect is 12. 4 8 (4) N + N + (3) 4 (cid:21) + 12 8 (cid:21) + 12 Hworst = 2 (cid:20) M Hbest = 2 (cid:20) M Figure 2 shows how the hop counts increase as N increases for the mesh and hybrid architecture, but it does not necessarily imply that the latencies will increase proportionately. There are many parameters that can affect the latency resulting in a large design space. A property of the hybrid interconnect that we have implemented is that the xy-routing algorithm tends to route global trafﬁc away from the center of sub-mesh towards the edges when the bridge tile is located in a corner. As will be seen in Section VI-A, the overall effect is that the resources in the center of the mesh end up processing less global trafﬁc and local trafﬁc is handled more efﬁciently. When global trafﬁc is routed through a bridge component, that component can quickly become ﬂooded causing the ﬂow control mechanisms to assert themselves having a negative impact on latency. Increasing the input buffer sizes of the bridge tile solved the problem and enabled trafﬁc to ﬂow much more smoothly. The required buffer size is very sensitive to the ratio of global to local trafﬁc as well as the size of the sub-mesh since the amount of trafﬁc increases quadratically with N . B. Hierarchical Rings The hierarchical-ring architecture used for the global interconnect in our hybrid-architecture is an adaptation of the design presented in [21] which was used in shared memory multiprocessor systems. As shown in Figure 3, our two-level hierarchy consists of four local rings connected to a global ring. In order to keep buffering and latencies to a minimum, each packet is actually a ﬂit/phit and can be forwarded in 1 clock cycle. As shown in Figure 3, packets are routed onto a local ring via a ring-interface (RI). Once on the local ring, a packet can be forwarded to another RI on the same local ring or it can be routed upwards to the global ring via the inter-ring interface (IRI). 200 150 s p o H 100 50 0 0 Mesh Hybrid Mesh (Corner) Hybrid Mesh (Center) 20 40 60 80 100 Mesh size N Fig. 2. Worst case hop counts for the mesh and hybrid-mesh topologies Local Ring Inter−Ring Interface Global Ring Ring Interface Fig. 3. Hierarchical Ring Interconnect. 1) Routing and Flow Control: The hierarchical rings provide lossless communications through the the use of a backpressure mechanism for handling network congestion which prevents packets from being dropped. At each hierarchical level, a backpressure signal can be propagated in order to prevent injection of new packets until outstanding packets have been drained from the interconnect and the backpressure signal has been de-asserted. A Stop Up signal is asserted by the global ring in order to stop the local rings from injecting new packets because of congestion on the global ring. A Stop Down signal is asserted by a local ring in order to stop the global ring from putting new packets on a local ring. The hierarchical ring interconnect was originally designed to be realistically used on an FPGA [20]. The interconnect had to be area efﬁcient yet support both point-to-point addressing, multicasting and broadcasting. This was solved by virtue of the hierarchical conﬁguration of the rings coupled with a onehot encoding of the addresses. The destination address in each packet header is of the form {Global Routing Mask, Local Routing Mask}. For example, if a PE has to send data to all stations on the local ring, the destination address would look like {G=0001,L=1110}. If the same data had to be broadcast to all stations on Local Ring 3, the destination address would be {G=0100, L=1111}. C. Enhanced Hybrid Interconnect The hierarchical ring interconnect described in [17], [20] is prone to congestion when backpressure signals are asserted resulting in the network being underutilized. The hierarchical nature of the interconnect and its backpressure signals can be exploited in order to boost performance. It can be seen from Figure 3 that the system bottleneck is the global ring. If the global ring asserts a backpressure signal, all 4 local rings must stop sending data, which can result in the local rings being under-utilized. In the work presented in [17], [20], the hierarchical ring interconnect was used to connect PEs together, so the situation was acceptable, especially considering that the resources usage of the interconnect is efﬁcient [20]. In this paper, we propose to use the same structure to connect multiple NoCs together instead of PEs, so the bandwidth requirement on the hierarchical ring interconnect will be much greater. Trafﬁc on the hierarchical ring interconnect can be classiﬁed as local and global. Local trafﬁc does not need to travel to the global ring, which is a fact that can be exploited and enable the implementation of an enhanced version of our hybrid interconnect. The Stop Up and Stop Down signals described in Section IV-B.1 can be used to determine if data can still be injected into the interconnect. If the global ring asserts a Stop Down signal because of congestion, this does not necessarily mean that the local ring is also congested. We can thus decouple the sending of local and global data in order to reduce the overall latencies of trafﬁc in our hybrid architecture. By modifying the implementation of the ring-interface component of the hierarchical rings, we can achieve two virtual channels over the same hardware for every local ring: • The inner channel is used for sending data locally on a ring. • The outer channel is used for sending global data through the global ring. The splitting of the network into two logical channels requires that we sacriﬁce a second tile on the mesh in order to keep from having to multiplex the data at the level of the mesh. Figure 4 shows how the enhanced architecture uses two tiles to connect the mesh and hierarchical rings together. Since a corner tile has only two input and output ports, we prefer to route trafﬁc to the inner ring through the interface that is not on an edge. D. Mesh The switch used in the mesh portion of our NoC simulation model has 5 input ports which have input FIFOs to store incoming packets. The output ports are unbuffered and are connected directly to their neighbors. Flow control between nodes on the mesh is achieved through the use of ‘on-off ’ ﬂow control [3]. To facilitate the interfacing of the mesh interconnect with the hierarchical ring interconnect, we chose to route ﬂits in the mesh instead of a more complicated scheme such as wormhole routing. We use simple xy-routing to route packets through the mesh. Outer Ring PROBAB I L I T I E S U S ED TO G EN ERAT E TRA FFIC DUR ING S IMU LAT ION S . TABLE I Inner Ring Trafﬁc Type Probability L0 L1 L2 0.7 0.2 0.1 categories that correspond to the layers in the hierarchy as illustrated in Figure 1: 1) level 0 (L0): local trafﬁc that is sent between stations at the lowest level of the hierarchy. 2) level 1 (L1): trafﬁc that goes through a single local ring to reach a station on another sub-mesh (e.g. L0 up to L1 and back down to L0). 3) level 2 (L2): trafﬁc that is routed between local rings needs to pass through the global ring. The packet types relate to the levels that need to be traversed in the hierarchy. In order to compare the mesh and hybrid architectures, a similar classiﬁcation of trafﬁc types was devised for the mesh architecture. From the description of the hybrid architecture in IV-A, we see that the maximum distance a packet can travel locally is equal to Wsub in either direction, so the maximum hop count for local packets (L0) can be expressed as 2 × Wsub . Similarly, we we can deﬁne the maximum hop count for L1 trafﬁc to be 2 × Wlocal . Lastly, trafﬁc of type L2 has a maximum hop count equal to N . For the hybrid interconnect, a local packet is physically constrained to be destined to a station belonging to the same sub-mesh as the sender. In the normal mesh topology, there is no such physical constraint, so we deﬁne a local packet on the normal mesh to be any packet that does not need to travel farther than the maximum allowed by L0. Types L1 and L2 are similarly constrained. B. Trafﬁc Generation Trafﬁc was generated using two methods. The ﬁrst method consisted of using random variables to generate trafﬁc and the second consisted of mapping a task graph to the interconnect. 1) Pseudo-random Trafﬁc Generation: Packets were injected into the interconnect using random variables which were generated using the Boost random number library [22]. Several random generators were used to generate trafﬁc of the three types described in Section V-A. The ﬁrst 3 variables, P0, P1, P2 relate to the probability that a packet will be of type L0, L1 or L2 respectively. The probabilities use to generate each type of trafﬁc are shown in Table I. 2) Task Graphs: A separate application called gengraph was implemented in C++ which randomly generates tasks graphs and maps them to a mesh. Each vertex in the graph corresponds to a task and each directed edge between two vertices indicates that the source vertex will send data to the sink vertex during simulation. The complexity of the task graph is controlled by several input parameters. The process of generating a task graph and mapping consists of the following steps: Fig. 4. Enhanced architecture which uses two tiles to send data through the hierarchical ring interconnect. Some mesh implementations use more complicated ﬂow control schemes such as stop-and-wait or go-back-n in order to recover from bit errors. We decided to make the network interconnect as simple as possible so that the resource usage and latencies were reduced. We felt that if needed, error detection and resolution can be done at the next level up in the processing stage. Since our interconnect routes ﬂits instead of multi-ﬂit packets, we can easily implement a variety of protocols on top of the interconnect depending on QoS constraints. E. Globally Asynchronous, Locally Synchronous As discussed in [17], [20], the hierarchical-ring interconnect can be partitioned into separate clock domains. The fact that the clock rate of the different rings can be independent allows for increased ﬂexibility when tuning the interconnect for speciﬁc applications. For example, the clock rate of the global ring can be higher than that of the local rings in order to reduce the latency of global trafﬁc [21]. Furthermore, multiple clock domains provide the facility for the eventual introduction of dynamic clock throttling [17] which can allow rings to be slowed down or sped up as needed to accommodate changing bandwidth requirements while reducing energy consumption. V. S IMU LAT ION P LAT FORM As mentioned previously, we implemented our model in SystemC using an object-oriented style which enabled the modeling of different architectures by plugging modules from a library of components together. We incorporated into the model a module which tracks packet latencies and hop-counts during simulation and and calculates the average of each at the end of a run. A. Trafﬁc Types As we are primarily interested in discovering the effects of using the hierarchical ring model in order to route global trafﬁc, we have partitioned our trafﬁc into three possible LAT ENC I E S (CYC L E S ) FOR TH E HYBR ID AND M E SH ARCH I T EC TUR E S FOR TABLE II D I FF ER EN T VA LU E S O F N Hybrid Mesh N 36 40 44 48 L0 150 162 176 190 L1 458 483 526 572 L2 Avg. L0 517 532 576 627 215 229 249 269 210 228 247 266 L1 359 392 428 464 L2 Avg. 512 567 619 673 248 270 293 316 HO P COUN T S FOR TH E HYBR ID AND M E SH ARCH I T EC TUR E S FOR TABLE III D I FF ER EN T VA LU E S O F N Hybrid Mesh N 36 40 44 48 L0 L1 L2 Avg. L0 L1 L2 Avg. 5 6 7 7 18 20 22 24 23 25 27 29 8 9 10 11 8 9 10 11 16 18 19 21 23 26 29 32 10 11 13 14 B. Exploiting Locality One of the most important things to consider when measuring system performance is the fact that trafﬁc patterns are not completely random. If systems designers simply assigned communicating tasks to nodes on the network randomly, the resulting system performance would be far from optimal. Exploiting locality has a signiﬁcant impact on system performance. In our hybrid interconnect, there are two major ways to exploit locality. The ﬁrst way is try to assign communicating tasks to the same sub-mesh so that the amount of local trafﬁc (L0) dominates the amount of global trafﬁc (L1, L2). This was modeled in VI-A by setting the probability of sending global trafﬁc to be much smaller than that of sending local trafﬁc. While effective at reducing global trafﬁc, we still have not taken advantage of the placement of the bridge to the hierarchical ring interconnect in relation to stations which need to send/receive global trafﬁc. If the bridge component is placed at the corner of a sub-mesh, the system performance is improved by placing components that need to send global trafﬁc closer to the bridge tile, thereby reducing the average number of hops required for global trafﬁc to reach the bridge component. Conversely, components which send predominantly local trafﬁc are placed as far as possible from the bridge tile. This will have the effect of decoupling global from local trafﬁc and will have an overall positive effect on trafﬁc latency. In order to model this behavior, we modiﬁed the probability that a station would send global trafﬁc based on the radial distance from the bridge tile. Table IV shows the simulation results for a 36 × 36 size mesh where the latencies and hop counts decreased when stations closer to the bridge tile had a higher probability of sending global trafﬁc as compared to stations farther away. The net effect is that the total amount of trafﬁc being routed through the center of the mesh is reduced, leaving more resources free Fig. 5. Example of a task graph being mapped onto a 3 × 3 mesh. 1) A task graph is generated based on input parameters. 2) The task graph is mapped to a mesh. 3) The ﬁnal mapping is written to a conﬁguration ﬁle. 4) The conﬁguration ﬁle is read by the simulator. As we are only interested in generating application-like trafﬁc on the interconnect, we have made some simplifying assumptions in our models: • Only a single task is mapped to a node on the interconnect. • Each task can send to (out-degree) and receive from (indegree) zero or more tasks. • The maximum in/out-degrees of a task can be constrained. The simulator constructs a send schedule for each node based on the out-degree of the task that has been mapped to it. An simple example of a task graph and subsequent mapping is shown in Figure 5. When the conﬁguration ﬁle is read by the simulator, the node which has been assigned task A will construct a send schedule consisting of a single entry, namely B. During the simulation, task A will send data to task B at randomly spaced intervals. V I . S IMU LAT ION R E SU LT S A. Comparison of the Hybrid and Mesh Architectures In order to afﬁrm the suitability of the hierarchical ring interconnect for global routing, we ran our simulator for increasing sizes of N and collected the results for both the hybrid and mesh architectures. Table II shows how the latencies varied for both architectures. What is quite interesting is the fact that the contrary to expectation, the latencies for local trafﬁc (type L0) passing through the hybrid interconnect is actually less than that observed in the normal mesh topology. This phenomenon can be explained by the fact that for the case where the mesh-ring bridge component is located on a corner tile, the xy-routing algorithm causes global trafﬁc (types L1 and L2) to be routed away from the center of the sub-mesh and towards the edges. The net effect of the xy-routing of global trafﬁc towards the edges is that there is less congestion in the center of the sub-mesh thereby resulting in lower latencies for local trafﬁc. Since local trafﬁc will make up the majority of trafﬁc in the system (assuming locality has been exploited), the net effect on the average latency on all trafﬁc is positive, and in fact we can see from Table II that the average latency for hybrid architecture is less than that of the mesh. TABLE IV LAT ENCY (CYC L E S ) AND HO P COUN T S FOR UN I FORM AND NON -UN I FORM D I S TR IBU T ION O F TRA FFIC TY P E PROBAB I L I T I E S AT TH E PE S FOR TH E HYBR ID ARCH I T EC TUR E O F S I Z E N = 36 Uniform Non-Uniform Latency. (cycles) Hops Latency (cycles) Hops L0 L1 L2 150 523 674 5 18 23 148 410 449 5 16 20 Avg. 278 10 211 8 L0 L1 L2 Avg 0 100 200 300 400 500 a L t n e y c ( c y c l s e ) Bridge (6,6) Bridge (8,8) L0 L1 L2 Avg 0 5 10 15 20 25 H p o C n u o t Fig. 6. Performance improvement of the hybrid architecture when the bridge component is moved away from the absolute corner location of the sub-mesh for local trafﬁc. In our experiments, we have chosen to place the bridge to the hierarchical ring interconnect at a corner tile of the mesh. While this has the effect that global trafﬁc gets routed away from the center of the mesh, the number of input ports for a corner tile is limited to 2. It is obvious that a bridge component will have to handle more packets than a normal PE. If we move the bridge component away from the absolute edge of the sub-mesh while still keeping the bridge in the relative corner of the mesh, we would expect to see an improvement in the performance of the system. Figure 6 shows the improvement in performance for a 36x36 mesh when the bridge is moved from the absolute corner position of (8, 8) to (6, 6). The latencies for local trafﬁc stay relatively unchanged while the L1 and L2 latencies have decreased. These results make intuitive sense since stations which are more likely to send global data have been placed near the bridge location, which is located in the upper-right quadrant of the sub-mesh. C. Enhanced Architecture We implemented the enhanced architecture described in Section IV-C using our SystemC library of components. We mentioned previously that it is possible to implement the hybrid interconnect by making some modiﬁcations to the architecture from [20], but for simulation purposes, it was L0 L1 L2 Avg 0 100 200 300 400 500 600 700 a L t n e y c ( c y c l s e ) Normal Enhanced L0 L1 L2 Avg 0 5 10 15 20 25 H p o C n u o t Fig. 7. Performance improvement of the enhanced hybrid over the normal architecture for N = 32. simpler to simply instantiate an extra ring-interface component for every sub-mesh and to connect them together to form the inner ring as shown in Figure 4. We performed simulations for a mesh size of 32 and the results obtained are shown in Figure 7. It can be seen that the latency for the hybrid interconnect has improved for types L1 and L2 trafﬁc. What is interesting is that hybrid interconnect has had a signiﬁcant impact on type L2 trafﬁc. Since trafﬁc types L1 and L2 are unaffected by each other’s stop signals, we see an overall decrease in their latencies. D. Task Graphs The gengraph program described in Section V-B.2 was used to generate a task graph with 1000 vertices and 700 edges. Figure 8 shows the results for the generated task graph when simulating the normal mesh and hybrid architectures. The latencies and hop counts observed for the hybrid topology are lower than that for the mesh. Furthermore, type L2 trafﬁc is the most improved trafﬁc type, which is consistent with the trend shown in Figure 2. Interestingly, Figure 8 shows that approximately 70% of trafﬁc for the hybrid mesh was of type L2, indicating that the mapping did not exploit locality and resulted in a large amount of global trafﬁc relative to the amount of local trafﬁc. Irrespective of the poor mapping, the hybrid architecture still performed well. V I I . CONCLU S ION We have presented a hybrid architecture which uses a mesh topology for local routing and a hierarchical ring interconnect for global routing. A SystemC simulation model was used to simulate our hybrid topology and to compare the performance to the mesh architecture. The partitioning of a large mesh into smaller sub-meshes leads to some interesting results. A well known problem with the mesh topology is the fact that trafﬁc hotspots develop in the center of the mesh. In the hybrid topology, the global trafﬁc is routed towards the edges of the sub-mesh thereby reducing the congestion in the center of the mesh and resulting         [5] T. Ye, L. Benini, and G. Micheli. Packetization and routing analysis of on-chip multiprocessor networks. Journal of Systems Architecture, 50(2-3):81–104, 2004. [6] C. Grecu, M. Jones, A. Ivanov, and R. Saleh. Performance evaluation and design trade-offs for Network-on-Chip interconnect architectures. IEEE Trans. Comput., 54(8):1025–1040, 2005. Student Member-Partha Pratim Pande and Senior Member-Andre Ivanov and Senior MemberResve Saleh. [7] P. Guerrier and A. Greiner. A generic architecture for on-chip packet switched interconnections. 2000. [8] P. Pratim et al. Design of a switch for network on chip applications. In ISCAS (5), pages 217–220, 2003. [9] S. Kumar et al. A Network on Chip architecture and design methodology. isvlsi, 00:0117, 2002. [10] D. Sig ¨uenca-Tortosa and J. Nurmi. Proteo: A new approach to networkon-chip. In Proceedings of IASTED International Conference of Communication Systems and Networks, CSN’02, 2002. [11] Scalable Coherent Interfafce. IEEE Standard 1596-1992, 1992. [12] H. Samuelsson and S. Kumar. Ring road network on chip architecture. In Proceedings of the IEEE Norchip Conference, 2004. [13] H. Singh, M.-H. Lee, G. Lu, N. Bagherzadeh, F.-J. Kurdahi, and E.M. Chaves Filho. MorphoSys: An integrated reconﬁgurable system for data-parallel and computation-intensive applications. IEEE Trans. Comput., 49(5):465–481, 2000. [14] A. Brinkmann, J-C. Niemann, I. Hehemann, D. Langen, M. Porrmann, and U. Ruckert. On-chip interconnects for next generation System-onChips. In ASIC/SOC Conference, pages 211–215, 2002. [15] M. Dall’Osso, G. Biccari, L. Giovannini, D. Bertozzi, and L. Benini. xpipes: a latency insensitive parameterized Network-on-Chip architecture for multi-processor SoCs. In ICCD ’03: Proceedings of the 21st International Conference on Computer Design, page 536, Washington, DC, USA, 2003. IEEE Computer Society. [16] W.J. Dally. Express cubes: Improving the performance of k-ary ncube interconnection networks. IEEE Transactions on Computers, 40(9):1016–1023, 1991. [17] S. Bourduas, B. Kuo, Z. Zilic, and N. Manjikian. Modeling and evaluation of an energy-efﬁcient hierarchical ring interconnect for Systemon-Chip multiprocessors. In NEWCAS, 2006. [18] R. Grindley et al. The NUMAchine multiprocessor. In Proc. 29th Int’l Conf. on Parallel Processing, pages 487–496, Toronto, Ontario, 2000. [19] C.A. Zeferino, M.E. Kreutz, L.C., and A. Susin. A study on communication issues for Systems-on-Chip. In Proceedings of the 15 th Symposium on Integrated Circuits and Systems Design (SBCCI 02), 2002. [20] S. Bourduas J-S. Chenard and Z. Zilic. A RTL-level analysis of a hierarchical ring interconnect for Network-on-Chip multi-processors. In Proceedings of International System-on-a-Chip Design Conference (ISOCC), October 2006. [21] G. Ravindran and M. Stumm. A performance comparison of hierarchical ring and mesh-connected multiprocessor networks. In 3rd IEEE Symposium on High-Performance Computer Architecture, pages 58–69, 1997. [22] Boost random number library. Available at: http://www.boost.org/libs/ random/index.html. ) s e l c y c ( y c n e t a L 800 700 600 500 400 300 200 100 0 t n u o C p o H 40 35 30 25 20 15 10 5 0 c i f f a r t f o % 1.0 0.8 0.6 0.4 0.2 0.0 Mesh Hybrid L0 L1 L2 Avg L0 L1 L2 Avg L0 L1 L2 Fig. 8. Performance characteristics of a task graph mapped to the mesh and hybrid architectures for N = 32. in reduced latencies for local trafﬁc. By placing resources which need to send global trafﬁc closer to the bridge station and taking advantage of locality, latencies and hop counts can be decreased. Furthermore, we showed that placing the bridge location away from the absolute corner of a sub-mesh, a further gain in performance could be achieved for global trafﬁc without negatively affecting the latencies of local trafﬁc. Lastly, we presented an enhanced version of our architecture which decreased latencies by providing separate virtual paths through the hierarchical-ring interconnect. As the size of the mesh is increased, the sub-meshes also get larger, which results in quadratic increase in trafﬁc that can go through the hierarchical ring interconnect. We can therefore conclude that there is some optimal size for the sub-meshes before performance will degrade to unacceptable levels due to congestion. The problem could be alleviated by simply scaling up the hierarchical ring, but we suggest that since a mesh size of about 36 resulted in similar performance characteristics for both the mesh and hybrid networks, it would be better to keep the sub-mesh sizes ﬁxed while increasing the size of the hierarchical ring interconnect by adding an extra level to the hierarchy. In future, we plan to investigate further the use of routing protocols that deal with path congestion for the mesh part of the topology. "
A Low-Latency and Low-Power Hybrid Insertion Methodology for Global Interconnects in VDSM Designs.,"Current VLSI designs face a serious performance bottleneck due to reverse scaling of global interconnects as CMOS technology scales into VDSM regime. Interconnections techniques which decrease delay, power, and ensure signal integrity, play an important role in the growth of semiconductor industry into future generations. In this paper we present a novel hybrid insertion methodology for on-chip global interconnects. It takes advantage of repeaters and low-swing differential-signaling transceivers on driving long wires in different length, and optimally inserts them along the wires in order to decrease delay, power and gate area cost of interconnects. Simulation results using HSPICE for 0.18mum process showed that delay, power, delay-energy-product (EDP) and gate area cost were considerably decreased compared with other approaches available. Moreover, its computational technique is relatively easy and not limited to a specific low-swing differential-signaling transceiver. Therefore the methodology is very suitable for integration in EDA tool flow and beneficial for the reuse of low-swing differential-signaling transceivers","A Low-Latency and Low-Power Hybrid Insertion Methodology for Global  Interconnects in VDSM Designs  Shuming Chen and Xiangyuan Liu  School of Computer Science, National University of Defense Technology  smchen@nudt.edu.cn and xiangyuan.liu@gmail.com  Abstract  Current VLSI designs face a serious performance  bottleneck due to reverse scaling of global interconnects  as CMOS  technology scales  into VDSM regime.  Interconnections techniques which decrease delay, power,  and ensure signal integrity, play an important role in the  growth of semiconductor industry into future generations.  In this paper we present a novel hybrid insertion  methodology for on-chip global interconnects. It takes  advantage of repeaters and  low-swing differentialsignaling transceivers on driving long wires in different  length, and optimally inserts them along the wires in  order to decrease delay, power and gate area cost of  interconnects. Simulation results using HSPICE for  0.18µm process showed that delay, power, delay-energyproduct (EDP) and gate area cost were considerably  decreased compared with other approaches available.  Moreover, its computational technique is relatively easy  and not limited to a specific low-swing differentialsignaling transceiver. Therefore the methodology is very  suitable for integration in EDA tool flow and beneficial  for  the  reuse of  low-swing differential-signaling  transceivers.  Keywords: on-chip interconnects, low-swing, differentialsignaling, insertion methodology.  1. Introduction  As CMOS technology scales down to VDSM realms,  designing on-chip global interconnects is facing a few  serious challenges to delay, power, and signal-integrity  with clock rates and die sizes of VLSI circuits continuing  to increase. Now it is generally recognized that on-chip  global  interconnects eventually be a performance  bottleneck of the future designs [1].  The most popular design approach to reducing the  propagation delay of global interconnects is to insert  uniform repeaters at regular intervals along the wires, i.e.  the optimal repeater insertion (ORI) technique [2][3],  which make the delay linear with respect to its length.  Due to ease of design and their robust nature, the ORI  approaches have been widely supported by most EDA  tools. But it has been shown that the size and the number  of repeaters required increase rapidly with feature size  decreasing and chip size increasing. Usually these  repeaters consume precious gate area and significant  power while complicating placement and  routing.  Therefore the approaches become more infeasible as  integration levels continue to rise. Some previous work  can be found in the literature [4]-[6], which attempt to  optimize power, area or throughout of interconnects with  a given delay penalty. These solutions are suitable for the  optimization of non-timing-critical interconnects.  Alternatives to the repeater insertion paradigms are  low-swing signaling interconnect approaches, which use a  smaller swing in a limited range between ground and the  nominal VDD to reduce power consumption linearly [2].  In general, the low-swing signaling fall into two major  categories, single-ended signaling [7] and differentialsignaling [8][9] techniques. While the latter approach  requires substantially more wiring resource, it has the  advantage of being more robust in the presence of noise,  which is of prime importance in the VDSM designs. Also,  the swing of differential signals can be aggressively lower  resulting in more improvement in performance and power.  From a signaling point of view, low-swing differentialsignaling (LSDIFF) can be classified further as voltagemode [10][11] and current-mode [12][13]. Voltage-mode  approach assumes that the data to be transmitted over the  wire is represented by a set of voltage levels referenced to  the supply rails, while current-mode approach represented  by the direction of current injected into the wire. Currentmode approach can operate at a quicker speed and  consume  lower dynamic power  than voltage-mode  approach; but additional circuits usually have to be  employed to restrain the static power consumption. Both  approaches can be  turned  to accomplish higher  performance levels and lower power consumption than  repeater insertion approaches; therefore they are suitable  for the area of high-speed on-chip interconnects.                        The most researches on the LSDIFF approaches are  focused on designing transceivers structures, while less  focused on the insertion approaches of those transceivers.  Since the practical applications of low-swing differentialsignaling transceivers usually employ full-custom design  method, the transceivers have to be resized with the wire  length and parasitic parameters varying when using  LSDIFF interconnects. Therefore these methods are not  very suitable for directly integration in an EDA tool flow,  resulting in poor design efficiency. In order to readily  reuse the low-swing differential-signaling transceivers  and automatically cope with interconnects on a large scale,  our work emphasize particularly on  the  insertion  methodology of those low-swing differential-signaling  transceivers.  In [14] a hybrid circuit based on the well known delayoptimal repeaters and the proposed differential currentsensing is presented, and the simulation results show that  driving 25% of the wire with repeaters and remaining  with current-sensing is the best solution from delay  standpoint. Since the used current-sensing circuit has to  be resized with length increasing, the proposed hybrid  technique is not regarded as an insertion methodology  and is unhelpful to the reuse of the current-sensing circuit.  For the purpose of reducing interconnect delay, power  dissipation and reusing the pre-designed low-swing  differential-signaling transceivers, we propose a novel  hybrid insertion methodology based on repeaters and  transceivers. In this paper, the optimization methodology  has been analyzed reasonably, and simulation results  indicate that delay, power, EDP and gate area cost are all  considerably decreased when compared  to other  techniques.  2. Hybrid insertion methodology  In this paper, we use the following notations of  parameters.  r: unit length wire resistance (Ω/mm)  c: unit length wire capacitance (pF/mm)  R0: driving resistance (kΩ)  C0: load capacitance (fF)  re: effective output resistance of minimum size inverter (kΩ)  cg: gate capacitance of minimum size inverter (fF)  cd: drain capacitance of minimum size inverter (fF)  VDD: power supply voltage (V)  Ioffn: leakage current per unit NMOS transistor width (µA/µm)  Wnmin(Wpmin): width of the NMOS(PMOS) transistor in  minimum size inverter (µm)  fclk: clock frequency (GHz)  α: signal activity factor  2.1. Overview of the Methodology  Considering a uniform interconnect, if the distributed  RC line segment is of length L, the delay of that segment  which is defined as the time difference between the input  and output waveforms crossing 50% of their full-swing  value can be written as  pT L ( ) = k R C 1 0 0 + ( k R c k rC L k rcL + + 2 2 0 4 ) 0 3            (1)  where k1, k2, k3 and k4 are non-dimensional constants,  which varied with different transmission techniques. In  general, for the repeater insertion approach, k1 equals to 1,  k2 and k3 equals to 0.69, and k4 equals to 0.38 [2].  Using the ORI approach in the interconnect line,  shown in Figure 1-(a), reduces its propagation delay  quadratically, then the delay can be given by the  following expression [4].  T ORI ( L ) = 2 1 ⎛ ⎜ ⎜ ⎝ + 1 2 1 ⎛ +⎜ ⎜ ⎝ c c d g ⎞ ⎟ ⎟ ⎠ ⎞ ⎟ ⎟ ⎠ rcr c L e g               (2)  A pair of low-swing differential-signaling transceiver  usually includes a driver and a receiver. Therefore, when  using LSDIFF approach, shown in Figure 1-(b), the  interconnect delay can be given by   T LSDIFF ( L ) = T driver + T receiver + T L ( p )                 (3)  where, Tdriver and Treceiver are intrinsic delay of the  driver and the receiver respectively.  IN predriver repeater IN (a) (b) OUT 10fF OUT 10fF driver receiver Figure 1. Two Common Approaches for Interconnects:  (a) ORI Approach and (b) LSDIFF Approach  As shown in Figure2, the delay TORI(L) is linear with  the wire length L, while TLSDIFF(L) is quadratic with L.  Assume that for the curves TORI(L) and TLSDIFF(L), lcrit1 is  the small horizontal coordinate of  two points of  intersection, and lcrit2 is horizontal coordinate of the point  of tangency, at which the slope of TLSDIFF(L) equals to that  of TORI(L). Therefore, their values can be derived from the  following equations respectively.  T ORI ( ) L = T LSDIFF ( ) L                              (4)                            ( ) L L ( ) L ORI LSDIFF T T L ∂ ∂ = ∂ ∂                            (5)  When there is no solution to (4), the values of lcrit1 and  lcrit2 are all assigned to the infinite ∞.  T L 1crit l 2 crit l ( ) ORIT L ( ) LSDIFF T L 0 Figure 2. Delay Curves Sketch  It can be seen from Figure 2 that, if L∈(0, lcrit1),  TORI(L)<TLSDIFF(L); if L∈(lcrit1, lcrit2], TLSDIFF(L)<TORI(L);  and if L∈(lcrit2, ∞), both TORI(L) and TLSDIFF(L) are  relatively large, moreover the growth rate of the latter  exceed that of the former.  xL ( ) 1 x L ORI Approach − IN OUT 10fF 1 1 2 M LSDIFF Approach Figure 3. Hybrid Insertion Methodology for  Interconnects  In order to reduce the delay of wires whose length  exceeds lcrit2, a hybrid insertion (HI) methodology is  presented in this paper. The HI methodology takes  advantages of repeaters and low-swing differentialsignaling transceivers on driving long wires in different  length, and optimally inserts these circuits along the wire.  As shown in Figure 3, the wire is divided into two parts:  one part of length xL using the LSDIFF approach, and the  other part of length (1-x)L using the ORI approach. The  LSDIFF part needs to insert M pairs of low-swing  differential-signaling transceivers. Moreover, the wire  segment between each pair of transceivers is limited to be  not longer than lcrit2. Hence, the nonnegative integer M,  the percentage x and the optimal insertion length lopt_lsdiff  can be expressed as (6), (7) and (8) respectively.  1 1 2 1 2 0, 1, , crit crit crit crit crit L l L l M l L l l o therw ise      0 < < ⎧ ⎪= ⎪ ⎨ ⎪ ⎡ ⎢ ⎪ ⎢ ⎩ ≤ ≤ ⎤ ⎥ ⎥ −                 (6)  ( ) 1 1 2 2 2 0, 1, lM 1 , crit crit crit crit crit L l M x l l L Ml otherwise L ⎧ ⎪⎪ ⎨ ⎪ ⎪ ⎩ 0 < < + ⎧ ⎪= ⎪     ⎨ ⎪ ⎪⎩ − ≤ ≤          (7)  ( ) 1 _ 1 2 2 2 0, L M l 0 , 1 , crit opt lsdiff crit crit crit crit L l xL M l l M l L Ml otherwise < < = = + − ≤ ≤   (8)  And for the ORI part of length (1-x)L, the optimal  insertion length lopt_ori, the repeater size h, and the repeater  number k can be computed from following expressions,  respectively [4].  ( ) opt ori _ 2 e r c rc                                            (10)  g d c l + =                          (9)  e g r c rc x L h = ( ) opt ori _ 1 l k − =                                         (11)  2.2. Delay Analysis  Since the HI methodology partitions the wire into M  sections using the LSDIFF approach and k sections using  the ORI approach, the total propagation delay of the wire  is the summation of the propagation delays of the each of  the individual sections. Therefore, the total delay of HI  interconnect can be written as  ( ) ( ) ( ) opt ori _ _ HI ORI LSDIFF opt lsdiff T L kT l MT l = +     (12)  It is not difficult to theoretically derive a conclusion  that, when L (∈ lcrit1, ∞), THI(L)<TORI(L), as described in  the appendix. That is to say, the HI approach performs  better than the ORI approach for a long wire in terms of  delay.  2.3. Power Analysis  Neglecting the short-circuit power due to its small  contribution, the total power for a single repeater of size h  driving a wire segment of length l can be written as [4],  ( ) ( ) 2 3 2 repeater DD clk g d DD offn nmin P l V f h c c cl V I W h α ⎡ ⎣ ⎤ ⎦ = + + +  (13)  And when a pair of low-swing differential-signaling  transceiver drives a wire segment of length l, the total  power is given by                                                                                                               P transceiver ( ) l = α f V V clk DD Swing 2 ⎡ ⎣ ( C cl + 0 ) ⎤ ⎦ + α f V clk DD 2 C P + i static n ∑ i = 1  (14)  where, Ci and Pstatic are the internal nodes capacitances  and the static power of low-swing differential transceiver,  respectively.  Therefore, the power for HI interconnects can be  written as  P L kP l MP l    (15)  Since the signal swing VSwing is much lower than the  full swing voltage VDD and interconnect capacitances  dominate gate capacitances,  the power of global  interconnects can be reduced considerably due to greater  part using LSDIFF interconnects.  transceiver repeater opt ori _ lsdiff = + ) ( ) ( opt ( ) HI _ 2.4. Signal Integrality Analysis  Using LSDIFF technique already offers less capacitive  crosstalk susceptibility and reduced electro-magnetic  interference (EMI) due to its differential signaling mode.  However, the inductance effect becomes important when  the interconnect lines are sufficiently long and the rise  time of the signal waveforms are comparable to the time  of flight across the line. And the offset twisted differential  line arrangement (as shown in Figure 4) presented in [9]  has been shown to offer a perfect cancellation of  inductive crosstalk between lines.   Therefore, the HI methodology combining LSDIFF  techniques with the TDL layout strategy can ensure well  signal integrity.  TDL Interconnect Figure 4. Offset Twisted Differential Line  Arrangement for LSDIFF Interconnects  3. Simulation  3.1. Experiments Setting  For our simulation, we use a major foundry’s 1.8V  0.18µm process. The metal lines are implemented in  metal5 with all lines having a metal width of 1µm and a  metal to metal spacing of 1µm consistent with typical  high level metal implementations of high performance  global busses. As wires inside chips are presently  dominated by RC behavior when using ORI techniques,  we use distributed RC model for full-swing interconnects.  However, inductance of long wire can not be ignored  when using LSDIFF techniques, so we use distributed  RLC model for low-swing interconnects. FastCap [15] is  employed to extract the capacitance and FastHenry [16] is  employed to extract the resistance and the inductance.   The experiments parameters, as shown in Table 1, are  obtained  from HSPICE, FastCap and FastHenry,  respectively. Extensive simulations for interconnects of  lengths range from 1mm to 20mm have been performed  to evaluate different approaches in terms of wire delay,  power, EDP and area cost. These simulations consist of  the following cases, which represent ORI [3], LP-ORI [4],  LSDIFF [11], INC-LSDIFF [11], 25%-75%-HI [14] and  HI approaches, respectively.  Table 1. Experiments Parameters  r   (Ω/mm)  38.76  Ioffn   (µA/µm) 0.2  c  (pF/mm) 0.271  fclk   (GHz)  1  re   (kΩ)  7.39  Wnmin   (µm)  0.36  cg   (fF)  1.997  Wpmin   (µm)  0.72  cd   (fF)  3.923  α  0.5  Table 2. The Number of Repeaters Required  L(mm) kORI  kLP-ORI 2 1 1 4 2 2 6 3 2 8 3 3 10  4  3  12  5  4  14  5  4  16 6 5 18 7 5 20 7 6 ORI: The ORI interconnects [4], as shown in Figure  1-(a). The optimal insertion length lopt_ori is 2.87mm, the  size of repeaters required h is 161 times than that of  minimum size inverter, and the number kORI increases  with wire length increasing, as shown in Table 2.  LP-ORI: The low power repeater insertion approach  presented in [4], which employs smaller repeater to  driving longer wires, decreasing 24% power consumption  with only 5% delay penalty compared to the ORI  approach. According to the conclusion of [4], here the  insertion length and repeater size are 3.74mm and 111,  respectively. And the number of repeaters kLP-ORI is listed  in Table 2.  LSIDFF: Traditional low-swing differential-signaling  transmission, as shown in Figure 1-(b). An overdrived  low-swing differential-signaling transceiver in [11] (as  Figure 5) is used. The stabile signal swing is 200mv, and  the overdrived swing is 600mv when the input signal  transition occurs. The overdrived NMOS transistors are  set to be 45 times than NMOS transistor of minimum  inverter in size. Moreover, the size of the transceiver is  fixed through the simulation.   INC-LSDIFF: The setting in this case is the same  with LSDIFF case given above, except that the driver is  resized with wire length increasing in order to further  reduce the interconnect delay. As shown in Table 3, the  overdrived NMOS transistors are set to be 225 times than                       NMOS transistor of minimum inverter in size when the  wire length extends to 20mm.  10.6mm respectively, and  then M, x and  corresponding to different wire length, as shown in Table  4, can be calculated by (6), (7) and (8).  lopt_lsdiff  VDDL IN GND VDD Rising Edge Detection Falling Edge Detection GNDL VDDH OUT0 OUT1 IN0 IN1 RLC Interconnect GNDH VDD GND OUT receiver driver Figure 5. The Overdrived Low-Swing DifferentialSignaling Transceiver  Table.3 The Size of Overdrived NMOS Transistors  L  (mm)  2  4  6  8  10  Size (NMOS of  min. inverter)  45x  45x  60x  80x  100x  L  (mm)  12  14  16  18  20  Size (NMOS of  min. inverter)  125x  145x  170x  200x  225x  25%-75%-HI: A hybrid insertion approach proposed  in [14], which driving 25% of the wire with repeaters and  remaining with current-sensing circuits. For comparison,  we replace the current-sensing circuit by the low-swing  differential transceiver shown in Figure 5, whose size is  the same with that in INC-LSDIFF case, and keep to the  constant proportion of 25%-75%.   Table 4. The Values of M, x and lopt_lsdiff in HI Case  L  (mm) M  2  0  4  1  6  1  8  1  10  1  x  0  1  1  1  1  lopt_lsdiff  (mm)  0  4  6  8  10  L  (mm) M  12  1  14  1  16  2  18  2  20  2  x  0.88 0.76 1  1  1  lopt_lsdiff  (mm)  10.6  10.6  8  9  10  HI: The HI methodology we proposed, as shown in  Figure 3. The repeater and low-swing differentialsignaling transceiver are the same with ORI case and  LSDIFF case, respectively. For overdrived low-swing  differential transmission approach, it is too hard to obtain  precise values of parameters k1, k2, k3 and k4 in (1).  Therefore, the parameters lcrit1 and lcrit2 required in this  case are approximately computed from the simulation  results of ORI case and LSDIFF case by curve fitting  method. Their approximate values are 3.55mm and  3.2. Simulation Results  Delay, power, EDP, gate area cost and wire area cost  in each of these cases, not including the pre-drivers, have  been compared and plotted in Figure 6, 7, 8, 9, and 10,  respectively.   Compared with the ORI approach, when wire length  being longer than 4mm, the results of the HI methodology  are better not only from delay standpoint but also from  power, EDP and gate area cost standpoint. Especially,  when wire length is 20mm, HI methodology reduces  delay, power, EDP and gate area cost by 17.8%, 47.2%,  64.4% and 29.4%, respectively.  Figure 6. Delay of Different Approaches  Figure 7. Power of Different Approaches                        Compared with the LP-ORI approach, though the HI  methodology consumes more power when wire length  being range from 1mm to 7mm, it benefits smaller delay  and less EDP. Moreover, the delay, power, and EDP of  the methodology are all considerably improved when  wire length exceed 7mm.  Compared with the INC-LSDIFF approach, the delay  of the HI methodology is a little slower (not more than  8%) when wire length ranging from 6mm to 14mm, but  the power, EDP and gate area cost are much smaller. And  it has to be pointed out that, though the delay of the INCLSDIFF case is smaller than that of the ORI case, the gate  area cost is comparable with that of the ORI case, and it is  more and more difficult to reduce the delay at the cost of  the gate area with wire length increasing.   Compared with  the 25%-75%-HI approach,  the  metrics of the HI methodology are better in most  instances except  that slower (not more  than 5%)  interconnect of length ranging from 12mm to 14mm.  However, the HI methodology spends wire area about  twice over  single-ended  interconnect  approaches.  Fortunately, the wire resource is sufficient by far for  differential global interconnects due to available routing  layers increasing with CMOS technology developing.  Therefore, from the tradeoff point of view, the HI  methodology offers more advantages  than other  approaches available.  4. Discussions  In the HI methodology, the insertion length lopt_lsdiff is  restricted within a wide limit between lcrit1 and lcrit2, which  are obtained from the performance comparing between  repeaters  and  low-swing  differential-signaling  transceivers. This insertion length may be not the delayoptimal one for a given low-swing differential-signaling  transceiver; however it can be regarded as the nearoptimal one of all the feasible insertion length, which  could get more benefits from the reduction of power and  gate area cost. The computational technique of the HI  methodology is relatively easy and fit for all kinds of lowswing differential-signaling transceivers; therefore it is  very suitable for integration in an EDA tool flow and  helpful to the reuse of low-swing differential-signaling  transceivers.  There are some differences of the parameters lcrit1 and  lcrit2 between different low-swing differential-signaling  transceivers. Considering lcrit1, if it is very small, the HI  methodology degenerates into the insertion approach of  low-swing differential-signaling transceivers; and if it is  very large, the methodology degenerates into the ORI  approach. When considering lcrit2, if it is large in the  extreme, the methodology degenerates into the traditional  LSDIFF approach.  Figure 8. EDP of Different Approaches  Figure 9. Gate Area Cost of Different Approaches  Figure 10. Wire Area Cost of Different Approaches              When wire length exceeds lcrit2, the signal swing may  change several times between low swing and full swing  for  the HI methodology, resulting  in  the design  complexity increasing. But the methodology can improve  considerably interconnect performance, and reduce the  power consumption and area cost in effect at the same  time. Therefore, the complexity cost is acceptable for a  high-performance, low-power design.  Due to fewer cells required and longer wire segment  uninterrupted, the complexity of the placement can be  reduced considerably. On  the other hand,  the HI  methodology has a few drawbacks  inherited from  LSDIFF techniques, such as more routing area cost and  more static power consumption.  5. Conclusions  In this paper we present a low-latency and low-power  approach as a candidate solution to the interconnect  design problem. The HI methodology based on repeaters  and low-swing differential-signaling transceivers takes  advantages of them on driving long wires in different  length. Moreover, it can perform well with wire length  varying and not be limited to a specific low-swing  differential-signaling  transceiver. Simulations  results  have shown that, all of delay, power, EDP and gate area  cost are considerably decreased compared with other  approaches available.   Therefore,  the methodology  is  an  efficient  optimization approach, and  suitable  for on-chip  interconnects, especially for global interconnects, in  VDSM VLSI designs. With the development of lowswing differential-signaling transceivers design approach,  the HI methodology will be made more applicable and  attractive.  6. Acknowledgments  This work was supported partly by National Natural  Science Foundation of China (No. 60473079) and Ph.D.  Program Foundation of Ministry of Education of China  (No. 20059998026).  7. Appendix  Proposition: if L (∈ lcrit1, ∞), then THI(L)<TORI(L)  Proof: As shown in section 2, the propagation delay of  interconnects using the HI methodology has the form  given by (12), and interconnect structure parameters: M, x,  lopt_lsdiff, lopt_ori, h, and k can be computed from (6), (7), (8),  (9), (10), and (11), respectively.   (i) If lcrit1<L≤lcrit2, we can figure out that, M=1, x=1,  lopt_lsdiff=L and k=0. Thus,   ( ) L ( ) L ( ) L HI LSDIFF ORI T T T = <                 (16)  (ii) If L>lcrit2, then M≥1, x>0, and lopt_lsdiff>0. Note that  when lcrit1+(M-1)lcrit2≤ L≤Mlcrit2, there is  ( ) 1 2 1 2 1 crit crit crit crit l M M l L M l l + − ≤ ≤ ≤          (17)  Therefore, we can derive the following inequation  from (8) and (17).   1 _ 2 crit opt lsdiff crit l l l < ≤                            (18)  And then  _ _ ( l ) ( l ) LSDIFF opt lsdiff ORI opt lsdiff T T <               (19)  Furthermore, it can be seen from (2) that, the delay  TORI(L) is in direct proportion to wire length L, therefore,   ( ( ) ) ( ( ( ) ) ( ) opt ori _ _ HI ORI LSDIFF opt lsdiff T T L L kT kT l l MT MT Ml x L xL L l = < + + ( ) opt ori _ _ HI ORI ORI opt lsdiff l ) ( ( ) opt ori _ _ HI ORI opt lsdiff T L T kl < + ) ( ) ( 1 T HI ORI T L T T L < − + ⎡ ⎣ < ⎤ ⎦  ( ) ) HI ORI                        (20)  (iii) In summary, we have derived the conclusion that  when L (∈ lcrit1, ∞), there is THI(L)<TORI(L).  8. "
NoC - Network or Chip?,"Summary form only given. The concept of a communication network emerged, many times in the past, for connecting a large number of systems, replacing dedicated point-to-point connection and other small-scale interconnection mechanisms. Each network needs to provide a cost effective solution for a large number of possibly conflicting requirements such as flexibility, scalability, reliability and performance. Therefore, the task of network architects and designers is to solve multiple instances of a complex constrained optimization problem resulting in numerous and diverse network solutions. For example, there are different standards and architectures associated with interconnection networks, home networks, LANs, MANs, WANs and wireless networks. In this paper, we map the common lessons and concepts from the networking research to the emerging NoC field. We argue that the NoC optimization problem consists of several distinguished types that should lead to multiple diverse solutions. NoC network layer architectures pose new challenges in exploring solutions to traditional networking problems such as routing, quality-of-service, flow and congestion control and reliability. The unique characteristics of silicon chips require new solutions to these classical problems, and define a new set of NoC specific problems, such as automatic network design process, power and area optimization and specialized system functionalities. We speculate which class of solutions is likely to fit the different NoC types","NOCS 2007  Keynote 3  NoC: Network or Chip?  Israel Cidon  Tark Professor and Dean of Electrical Engineering Department, Technion,   Israel Institute of Technology  Abstract:      The concept of a communication network emerged, many times in the past, for connecting a large  number of systems,  replacing dedicated point-to-point connection and other small-scale  interconnection mechanisms. Each network needs to provide a cost effective solution for a large  number of possibly conflicting requirements such as flexibility, scalability, reliability and performance.  Therefore, the task of network architects and designers is to solve multiple instances of a complex  constrained optimization problem resulting in numerous and diverse network solutions. For example,  there are different standards and architectures associated with interconnection networks, home  networks, LANs, MANs, WANs and wireless networks.      In this talk we will map common lessons and concepts from the networking research to the emerging  NoC field.  We will also argue that the NoC optimization problem consists of several distinguished types  that should lead to multiple diverse solutions.     NoC network layer architectures pose new challenges in exploring solutions to traditional networking  problems such as routing, quality-of-service, flow and congestion control and reliability. The unique  characteristics of silicon chips require new solutions to these classical problems, and define a new set of  NoC specific problems, such as automatic network design process, power and area optimization and  specialized system functionalities. We will speculate which class of solutions is likely to fit the different  NoC types.  Bio:     Israel Cidon is a Tark Professor and the dean of the Electrical Engineering Department at the Technion,  Israel Institute of Technology. He holds a B.Sc. and D.Sc. degrees in Electrical Engineering from the  Technion (1980 and 1984 respectively). Between 1985 and 1994, he was with IBM T. J. Watson Research  Center NY, where he was the manager of the Network Architecture and Algorithms group, leading the  research and implementation of the world first converged multi-media WAN and MAN packet switched  networks. In 1994 and 1995, he was the manager and founder of the High-Speed Networking group at  Sun Microsystems Labs, CA. He is a co-founder of Micronet Ltd. (1981) an early vendor of networked  mobile data entry hand-held computers, Viola Networks (1998) a provider of VoIP monitoring and  diagnosis software and Actona Technologies (2000, acquired by Cisco in 2004) the first vendor of wide  area file system (WAFS) for remote office storage centralization. He was a founding editor of the  IEEE/ACM Transactions on Networking and Editor for Network Algorithms for the IEEE Transactions on  Communications. He received the IBM Outstanding Innovation Awards for his work on the PARIS project  and topology update algorithms (1989 and 1993 respectively). He has authored over 140 journal and  conference papers and holds 21 US patents.                  "
NoC Communication Strategies Using Time-to-Digital Conversion.,"A radical approach to high-speed on-chip communication between computational modules is proposed. Data communication is performed over multiple serial buses, where the time difference between events is used to encode and decode data on a number of wires. We present results obtained through a proof-of-concept implementation on FPGA and simulations on a 0.18mum technology","(cid:2) (cid:2)	(cid:6)(cid:7)(cid:8)(cid:6) (cid:10)(cid:8)(cid:12)(cid:13)(cid:6)(cid:12) 	(cid:6)(cid:13) (cid:15)(cid:6)(cid:12)		(cid:17)(cid:6)(cid:13)(cid:6)(cid:8)  (cid:2)(cid:19)(cid:12)(cid:6) (cid:0)(cid:2)(cid:4)(cid:2)(cid:6) (cid:8)(cid:10) (cid:2)(cid:12)(cid:13) (cid:16)(cid:17) (cid:12) (cid:16)(cid:12) (cid:2)(cid:16)(cid:21)  (cid:2) (cid:8)(cid:12)(cid:23)(cid:16)(cid:13) (cid:16)(cid:16)(cid:2) (cid:10) (cid:2)(cid:25) (cid:26)(cid:12)(cid:17)(cid:23) (cid:2)(cid:23) (cid:16)(cid:4)(cid:2) (cid:2)(cid:4)(cid:16)(cid:4) (cid:27)(cid:28)(cid:2) (cid:8)(cid:2)(cid:16)(cid:29) (cid:30)	 (cid:2)!(cid:4)(cid:12) (cid:2) ""(cid:16)(cid:23)(cid:2)(cid:16)(cid:28) "" #(cid:4)(cid:2)(cid:4)(cid:2)(cid:6)$(cid:13)(cid:12) (cid:2)(cid:12)(cid:13) (cid:16)(cid:17) (cid:12)$(cid:16)(cid:12) (cid:17)(cid:2)(cid:16)(cid:21)$(cid:21)(cid:2) (cid:13)(cid:12)(cid:23)(cid:16)(cid:13)$(cid:17)(cid:16)(cid:16)(cid:2) (cid:12) (cid:2)(cid:25)$(cid:28)(cid:12)(cid:17)(cid:23) (cid:2)(cid:23)%&(cid:4) $(cid:12)(cid:4)$	(cid:17) (cid:0)(cid:1)(cid:5)(cid:6)(cid:0) (cid:1) (cid:3)(cid:4)(cid:5)(cid:6)(cid:3)  (cid:3)(cid:3)(cid:6)(cid:10)  (cid:10)(cid:5)(cid:12)(cid:10)	(cid:15)(cid:15)(cid:4) 	(cid:6)(cid:10)(cid:5) (cid:6)	 	(cid:5)(cid:6)(cid:3)(cid:5) (cid:19)(cid:15)(cid:20)(cid:15)(cid:15) (cid:6)	(cid:3)(cid:5)(cid:3)  (cid:4)	 (cid:15) (cid:5) (cid:15)(cid:4)(cid:21) (cid:22)(cid:3)(cid:3) (cid:6)	(cid:5)(cid:6)(cid:3)(cid:5) (cid:5) (cid:15)(cid:23)(cid:15)(cid:4) (cid:24)(cid:15) 	 (cid:5) (cid:15) (cid:15)(cid:5)(cid:3)  (cid:19)	(cid:15) (cid:20)(cid:10)(cid:15)(cid:15) (cid:10)(cid:15) (cid:5)(cid:15) (cid:4)(cid:5)(cid:26)(cid:15)(cid:15)(cid:6)(cid:15) (cid:19)(cid:15)(cid:20)(cid:15)(cid:15) (cid:15)(cid:24)(cid:15) (cid:5) 	(cid:15)(cid:4)  (cid:15)(cid:6)(cid:4)(cid:15) (cid:3)(cid:4) (cid:4)(cid:15)(cid:6)(cid:4)(cid:15) (cid:4)(cid:3)(cid:3)  (cid:3) 	(cid:19)(cid:15) (cid:23) (cid:20)(cid:5)(cid:15)(cid:21) (cid:27)(cid:15) (cid:15)(cid:15) (cid:15)	 	  (cid:19)(cid:3)(cid:5)(cid:15)(cid:4) (cid:10)	(cid:12)(cid:10) (cid:3) (cid:23)	(cid:23)	(cid:6)(cid:6)(cid:15) (cid:5) (cid:15)(cid:15)(cid:3)(cid:5)  (cid:28)(cid:30)(cid:1) (cid:3)(cid:4) (cid:5)	 (cid:3)(cid:5)  (cid:3) (cid:31)(cid:21) !µm (cid:15)(cid:6)(cid:10) (cid:12)""(cid:21) (cid:1) (cid:6)	(cid:8)(cid:9) (cid:0)(cid:1)(cid:2) (cid:3)	(cid:2) (cid:7) (cid:2)(cid:8)(cid:9)(cid:3)(cid:2) (cid:3)(cid:2)(cid:9)(cid:2)(cid:9) (cid:3) (cid:2) (cid:7) (cid:16)(cid:2) (cid:9)	(cid:3) (cid:17)(cid:2)(cid:17) (cid:17)(cid:18) (cid:2)(cid:2)(cid:18) (cid:3) (cid:7)(cid:17) (cid:19)(cid:2)(cid:9)(cid:3)(cid:20) (cid:17) (cid:9)	(cid:9)(cid:3)(cid:17)  (cid:17)(cid:2)(cid:9) (cid:3) (cid:1)(cid:2) (cid:18)(cid:2)(cid:3)(cid:20) (cid:7) (cid:22)(cid:2)  (cid:24) (cid:17)(cid:9)(cid:1)(cid:3)(cid:2)(cid:9)	(cid:2)(cid:25) (cid:0)(cid:1)(cid:3) (cid:3) (cid:18)	(cid:2)  (cid:1)(cid:2) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2) (cid:9)(cid:17) (cid:3)(cid:20) (cid:7) (cid:17)(cid:20)(cid:17)(cid:3) (cid:3)(cid:2) (cid:7) (cid:20)(cid:17)(cid:2) (cid:17)(cid:18) (cid:16)(cid:3)(cid:2) (cid:17) (cid:18)(cid:2)(cid:9)(cid:3)(cid:19)(cid:2)(cid:18) (cid:19)(cid:27) (cid:22)(cid:17)(cid:3)	 (cid:17)	(cid:1) (cid:2)(cid:2) (cid:7) (cid:2)(cid:29)(cid:17) (cid:2) (cid:30)(cid:31)  (cid:17)(cid:18) (cid:2)(cid:2)(cid:18) (cid:3) (cid:1)(cid:2)  (cid:17)(cid:2) (cid:2)(cid:18)(cid:3)(cid:3) (cid:7) (cid:1)(cid:2) (cid:2)(cid:17)(cid:3)(cid:17)  (cid:0)(cid:2)(cid:9)(cid:1)	  (cid:20)(cid:27) #(cid:17)(cid:18)(cid:17) (cid:7) $(cid:2)(cid:3)(cid:9)(cid:18)	(cid:9) (cid:0)#$ (cid:3) (cid:1)(cid:2) (cid:2)	 (cid:9)(cid:2)(cid:9) (cid:2)(cid:9)(cid:3) (cid:30)% (cid:25) (cid:0)(cid:1)(cid:3) (cid:9)(cid:17) (cid:1)(cid:17)(cid:22)(cid:2) (cid:18)(cid:2) (cid:2)(cid:2)(cid:3)	 (cid:2)(cid:26)(cid:2)(cid:9)  (cid:1)(cid:2) (cid:18)(cid:2)(cid:3)(cid:20) &(cid:16)’ (cid:3) (cid:7)(cid:17)(cid:9)  	(cid:16)(cid:17)(cid:2)(cid:18) (cid:2)(cid:26)(cid:2)(cid:9) (cid:18)	(cid:2)   (cid:3)(cid:2)(cid:9)(cid:2)(cid:9)  (cid:17)(cid:27)	 (cid:16)(cid:3)   (cid:19)(cid:2)(cid:9)(cid:2) (cid:2)(cid:22)(cid:3)(cid:18)(cid:2)  (cid:27) (cid:17)(cid:7)(cid:2)  (cid:17)(cid:9)(cid:2)	 (cid:17)(cid:18)		(cid:2) (cid:3)(cid:3)(cid:20)  (cid:17)(cid:20)(cid:2) (cid:3)(cid:2) (cid:2)(cid:17) (cid:3)(cid:2)  (cid:1)(cid:2) (cid:18)(cid:2)(cid:22)(cid:2) (cid:2)(cid:25) (cid:0)(cid:1)(cid:2) (cid:9)(cid:9)(cid:2) (cid:7) (cid:3)(cid:2)(cid:9)(cid:2)(cid:9)	(cid:9)(cid:2)(cid:3)(cid:9) (cid:18)(cid:2)(cid:3)(cid:20) (cid:7) (cid:3)(cid:17)(cid:9)(cid:2) (cid:30)(  (cid:17)(cid:2)  (cid:2)(cid:17)(cid:2) (cid:1)(cid:3) (cid:19) (cid:2) (cid:19)(cid:27) (cid:27)(cid:2)(cid:17)(cid:3)(cid:9)(cid:17)  (cid:27) )	(cid:3)(cid:20)* (cid:1)(cid:2)  (cid:17)(cid:9)(cid:2) (cid:17)(cid:18) 	(cid:2) (cid:17)	(cid:18) (cid:1)(cid:2) (cid:3)(cid:2)(cid:9)(cid:2)(cid:9) (cid:7)(cid:17)(cid:19)(cid:3)(cid:9) (cid:3)(cid:19) (cid:27) (cid:17) (cid:1)(cid:2) (cid:2)(cid:29)(cid:2)(cid:2) (cid:7) (cid:20)(cid:17)(cid:2)  (cid:17)(cid:9)(cid:2)(cid:2)(cid:25) (cid:0)(cid:1)(cid:2) (cid:17)(cid:17)(cid:9)(cid:1) (cid:2)(cid:18) (cid:3) (cid:1)(cid:2) (cid:2)(cid:2) (cid:16)+ (cid:9)(cid:3) (cid:3) (cid:2)(cid:9)(cid:18)(cid:3)(cid:20) (cid:1)(cid:2) (cid:18)(cid:17)(cid:17) (cid:3)(cid:2) (cid:3) (cid:1)(cid:2) (cid:3)(cid:2) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2)(cid:9)(cid:2) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:17) 	(cid:19)(cid:2) (cid:7) (cid:2)(cid:22)(cid:2)(cid:25) (cid:1)(cid:17)(cid:2)	(cid:2)(cid:9)(cid:18)(cid:3)(cid:20) (cid:16)(cid:17) (cid:2)(cid:18) (cid:3) (cid:30)- (cid:7) (cid:1)(cid:2) (cid:18)	(cid:17) 	(cid:17)(cid:3)  (cid:9)(cid:17)(cid:2) (cid:17)(cid:18) (cid:3) (cid:30). (cid:7) (cid:1)(cid:2) (cid:20)(cid:2)(cid:2)(cid:17)  	 (cid:3) (cid:2)	(cid:17)(cid:3)  (cid:9)(cid:17)(cid:2)(cid:25)  (cid:1)(cid:3) (cid:9)(cid:1)(cid:2)(cid:2) (cid:1)(cid:2) (cid:18)(cid:17)(cid:17) (cid:3)(cid:2) (cid:17)(cid:2) (cid:2)(cid:9)(cid:18)(cid:2)(cid:18) (cid:3) (cid:1)(cid:2) (cid:2)	(cid:2)(cid:9)(cid:2) (cid:7) (cid:2)(cid:22)(cid:2)  (cid:17) 	(cid:19)(cid:2) (cid:7)  (cid:3)(cid:2) n (cid:16)(cid:1)(cid:2)(cid:2) n > 1(cid:25)  (cid:1)(cid:2) (cid:16)+ (cid:1)(cid:16)(cid:2)(cid:22)(cid:2) (cid:1)(cid:2) (cid:17)	(cid:1)  (cid:27) (cid:9)(cid:3)(cid:18)(cid:2) (cid:1)(cid:2) (cid:9)(cid:17)(cid:2) (cid:16)(cid:1)(cid:2)(cid:2) (cid:1)(cid:2) (cid:2)(cid:22)(cid:2) (cid:17)   (cid:9)(cid:9)	 (cid:17) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2) (cid:3)(cid:2)  (cid:1)(cid:17)  (cid:16) (cid:2)(cid:22)(cid:2) (cid:9)(cid:17) (cid:9)(cid:9)	 (cid:3)	 (cid:17)(cid:2)	 (cid:27)(cid:25) (cid:0)(cid:1)(cid:3) (cid:17)  (cid:16) (cid:1)(cid:2) (cid:2)	 (cid:9)(cid:18)(cid:3)(cid:20)  n! (cid:17)(cid:2) (cid:17)(cid:18) (cid:1)	 log2 n! (cid:19)(cid:3)’ (cid:1)(cid:2) (cid:18)(cid:2)(cid:9)(cid:18)(cid:3)(cid:20) (cid:3) (cid:3) (cid:27) (cid:2)(cid:7)(cid:2)(cid:18) 	(cid:3)(cid:20) (cid:19)(cid:17)+ (cid:7) (cid:17)(cid:19)(cid:3)(cid:2) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:20)(cid:2)(cid:2)(cid:17)(cid:2) 	(cid:3)	(cid:2) )(cid:3)(cid:20)(cid:17)	(cid:2)* (cid:7) (cid:1)(cid:2) (cid:27)(cid:19) (cid:25) (cid:0)(cid:1)(cid:2)(cid:2) (cid:17)(cid:2) (cid:1)(cid:2) (cid:7)	(cid:1)(cid:2) (cid:18)(cid:2)(cid:9)(cid:18)(cid:2)(cid:18) (cid:3) (cid:19)(cid:3)(cid:17)(cid:27) (cid:18)(cid:17)(cid:17)(cid:25) (cid:0)(cid:1)(cid:2) (cid:2)(cid:2) (cid:16)+ (cid:2)(cid:29)(cid:17)(cid:18)  (cid:1)(cid:2) (cid:2)(cid:22)(cid:3)	 (cid:16)+ (cid:19)(cid:27) (cid:2)(cid:17)	 	(cid:3)(cid:20) (cid:1)(cid:2) (cid:1)(cid:17)(cid:2) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2)(cid:9)(cid:2) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:1)(cid:2) (cid:2)(cid:22)(cid:2)  (cid:1)(cid:2)  (cid:3)(cid:2) (cid:1)	 (cid:3)(cid:9)(cid:2)(cid:17)(cid:3)(cid:20) (cid:1)(cid:2) (cid:19)(cid:3)(cid:17)(cid:2)(cid:25) (cid:0)(cid:1)(cid:3) (cid:3) (cid:17)(cid:9)(cid:1)(cid:3)(cid:2)(cid:22)(cid:2)(cid:18) 	(cid:3)(cid:20) (cid:0)(cid:3)(cid:2)	 	0(cid:3)(cid:20)(cid:3)(cid:17)  (cid:24)(cid:22)(cid:2)(cid:2) (cid:0)0(cid:24) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:20)(cid:2)(cid:2)(cid:17)(cid:2) (cid:17) (cid:22)(cid:17) 	(cid:2) (cid:17)(cid:9)	 (cid:9)(cid:18)(cid:3)(cid:20)  (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:16) (cid:3)	 (cid:2)(cid:22)(cid:2) (cid:17)(cid:18)(cid:3)(cid:3)(cid:17)  (cid:27) (cid:17)(cid:2)(cid:18) )(cid:17)* (cid:17)(cid:18) )*(cid:25)  (cid:1)(cid:2) (cid:3) (cid:2) (cid:9)(cid:17)(cid:2) (cid:1)(cid:2)(cid:2)(cid:7)(cid:2) (cid:1)(cid:2) (cid:2)(cid:18)(cid:2) (cid:1)(cid:2)(cid:2)(cid:7)(cid:2) (cid:2)(cid:9)(cid:18)(cid:2) (cid:17) (cid:3)(cid:2) (cid:7) (cid:18)(cid:17)(cid:17) (cid:17) (cid:1)(cid:2) (cid:3)(cid:2) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2)(cid:9)(cid:2) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:16) (cid:2)(cid:18)(cid:20)(cid:2)’ (cid:1)(cid:3) (cid:3)(cid:2) (cid:0)(cid:1)(cid:2) (cid:4)(cid:7) (cid:2) 	(cid:11)(cid:12) (cid:13)(cid:14) (cid:15)(cid:17)(cid:18)(cid:19) (cid:4)(cid:2)(cid:1) (cid:20)(cid:21) (cid:15)(cid:23)(cid:19)(cid:24)(cid:24)(cid:25)(cid:26)(cid:27)(cid:28)(cid:23)(cid:29) (cid:21)(cid:12) (cid:15)(cid:23)(cid:19)(cid:30)(cid:29)(cid:26)(cid:28)(cid:29)(cid:26)(cid:23)(cid:29)(cid:31) (cid:2)(cid:17)(cid:17)(cid:3) (cid:3) (cid:2)(cid:17)	(cid:2)(cid:18) (cid:19)(cid:27) (cid:1)(cid:2) (cid:0)0(cid:24) (cid:17)(cid:18) (cid:17) (cid:9)(cid:2)(cid:18)(cid:3)(cid:20) (cid:19)(cid:3)(cid:17)(cid:27) 		 (cid:3) (cid:20)(cid:2)(cid:2)(cid:17)(cid:2)(cid:18)(cid:25) (cid:7) (cid:1)(cid:2) (cid:2)(cid:18)(cid:2) (cid:17)(cid:18) (cid:1)(cid:2) (cid:2)(cid:9)(cid:2)(cid:3)(cid:22)(cid:2) (cid:17)(cid:2) (cid:9)(cid:17) (cid:3)(cid:19)(cid:17)(cid:2)(cid:18)  (cid:1)(cid:2) (cid:17)(cid:2) (cid:17)(cid:20)(cid:2) (cid:1)(cid:2) 		 (cid:7) (cid:1)(cid:2) (cid:2)	 (cid:9)(cid:2)(cid:3)(cid:22)(cid:2)  (cid:20)(cid:3)(cid:9) (cid:16)(cid:3)   (cid:19)(cid:2) (cid:3)(cid:18)(cid:2)(cid:3)(cid:9)(cid:17)   (cid:1)(cid:2) (cid:3)	 (cid:22)(cid:17) 	(cid:2)(cid:25) 1 (cid:1)(cid:2) (cid:3)(cid:2) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2)(cid:9)(cid:2) (cid:3) (cid:2)(cid:17)	(cid:2)(cid:18) (cid:16)(cid:2) (cid:18)(cid:2)(cid:9)(cid:3)(cid:19)(cid:2) (cid:1)(cid:3) (cid:2)(cid:1)(cid:18) (cid:17) (cid:1)(cid:3)	(cid:3)(cid:6)(cid:8)(cid:1)(cid:9)(cid:25) (cid:0)(cid:1)(cid:3) (cid:9)(cid:1)(cid:2)(cid:2) (cid:3) (cid:3)(cid:1)(cid:2)(cid:2) (cid:27) (cid:2) (cid:7)	(cid:3)(cid:2)(cid:18) (cid:17) (cid:1)(cid:2) (cid:2)(cid:2)(cid:9)(cid:2) (cid:7) (cid:18)(cid:17)(cid:17) (cid:3) (cid:3)(cid:18)(cid:3)(cid:9)(cid:17)(cid:2)(cid:18) 	(cid:2)	(cid:3)(cid:22)(cid:9)(cid:17)  (cid:27) (cid:16)(cid:3)(cid:1)	 (cid:1)(cid:2) (cid:2)(cid:2)(cid:18) (cid:7) (cid:17) (cid:20) (cid:19)(cid:17)  (cid:9) (cid:9)+ (cid:17)(cid:18) (cid:3) (cid:1)(cid:2)(cid:2)(cid:7)(cid:2) 	(cid:3)(cid:2)(cid:18)  2 (cid:19)(cid:17)  (cid:27) 1(cid:27)(cid:9)(cid:1)	 (cid:9)(cid:17)  (cid:27) $(cid:27)(cid:9)(cid:1)	 21$ (cid:18)(cid:2)(cid:3)(cid:20)(cid:25) (cid:0)(cid:1)(cid:2) (cid:2)(cid:18)	(cid:9)(cid:2)(cid:18) (cid:2)(cid:2)(cid:18) (cid:7) (cid:17) (cid:20) (cid:19)(cid:17)  (cid:9)	(cid:3)(cid:9)(cid:17)(cid:3) (cid:9) (cid:9)+ (cid:2)(cid:18)	(cid:9)(cid:2) (cid:1)(cid:2) (cid:2)(cid:26)(cid:2)(cid:9) (cid:7) +(cid:2)(cid:16) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:1)(cid:2) (cid:16)(cid:3)(cid:2) (cid:3) (cid:17) (cid:17)(cid:17)  (cid:2)  (cid:19)	’ (cid:3)(cid:2)(cid:17)(cid:18) (cid:17) 	(cid:19)(cid:2) (cid:7) (cid:2)(cid:18)	(cid:9)(cid:2)(cid:18)	(cid:16)(cid:3)(cid:2) (cid:19)	(cid:2) (cid:9)(cid:17) (cid:19)(cid:2) 	(cid:2)(cid:18) (cid:3) (cid:18)(cid:2)  (cid:3)(cid:22)(cid:2) (cid:17)(cid:9)(cid:1)(cid:3)(cid:20) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:16)(cid:3)(cid:2)(cid:25) (cid:0)(cid:1)(cid:2) (cid:17)(cid:2) (cid:3) (cid:20)(cid:17)(cid:3)(cid:2)(cid:18) (cid:17) (cid:7)  (cid:16)(cid:25) $(cid:2)(cid:9)(cid:3)  (cid:18)(cid:2)4(cid:2) (cid:1)(cid:2) (cid:19)(cid:17)(cid:9)+(cid:20)	(cid:18)  (cid:1)(cid:2) (cid:19) (cid:2) (cid:18)(cid:3)(cid:9)	(cid:3)(cid:20) (cid:1)(cid:2) (cid:17)(cid:2) (cid:7) (cid:1)(cid:2) (cid:17) (cid:3) (cid:2) (cid:7) 	(cid:9)(cid:1)(cid:3) (cid:9)	(cid:3)(cid:9)(cid:17)(cid:3) (cid:17)(cid:18) (cid:0)0(cid:24) (cid:18)(cid:2)(cid:3)(cid:20)(cid:25) $(cid:2)(cid:9)(cid:3)  (cid:18)(cid:2)(cid:9)(cid:3)(cid:19)(cid:2) (cid:1)(cid:2) (cid:9)(cid:9)(cid:2) (cid:7) (cid:3)(cid:2)	(cid:2)(cid:9)(cid:18)(cid:3)(cid:20) (cid:7) (cid:16) (cid:16)(cid:3)(cid:2) (cid:7)  (cid:16)(cid:2)(cid:18) (cid:19)(cid:27) $(cid:2)(cid:9)(cid:3) 5 (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:17)(cid:17) (cid:27)(cid:2) (cid:1)(cid:2) 	 (cid:3) (cid:2)	 (cid:17)(cid:3)  (cid:9)(cid:17)(cid:2)(cid:25) $(cid:2)(cid:9)(cid:3) 5 (cid:3)  	(cid:17)(cid:2) (cid:1)(cid:2) (cid:7)(cid:2)	(cid:2)(cid:9)(cid:27) (cid:17)(cid:18) (cid:19)(cid:3)(cid:17)(cid:2) (cid:22)(cid:17) 	(cid:2) (cid:3)(cid:19) (cid:2) (cid:16)(cid:3)(cid:1) (cid:1)(cid:2)(cid:2) (cid:9)(cid:1)(cid:2)(cid:2)(cid:25) $(cid:2)(cid:9)(cid:3) 5 (cid:22)(cid:3)(cid:18)(cid:2) (cid:2) (cid:2)  (cid:1)(cid:2) (cid:9)(cid:17) (cid:3)(cid:19)(cid:17)(cid:3) (cid:7) (cid:0)0(cid:24)(cid:25) $(cid:2)(cid:9)(cid:3) 5 (cid:18)(cid:3)	 (cid:9)	(cid:2) (cid:1)(cid:2) (cid:2)	  (cid:19)(cid:17)(cid:3)(cid:2)(cid:18) (cid:1)	(cid:20)(cid:1) 	(cid:9)(cid:1)(cid:3) (cid:3)	 (cid:17)(cid:3) (cid:7) 6(cid:25)(cid:31)7µm (cid:2)(cid:9)(cid:1) (cid:20)(cid:27) (cid:17)(cid:18) (cid:17) (cid:7)	(cid:7)	(cid:9)(cid:9)(cid:2) (cid:3) (cid:2)(cid:2)(cid:17)(cid:3)  821(cid:25) 8(cid:3)(cid:17)  (cid:27) $(cid:2)(cid:9)(cid:3) 5 (cid:9)(cid:9) 	(cid:18)(cid:2) (cid:1)(cid:2) (cid:17)(cid:2)(cid:25) (cid:1) (cid:10)(cid:11)(cid:8)(cid:12)(cid:13)	(cid:6) (cid:10)(cid:11) (cid:12)(cid:13)(cid:3) (cid:14) (cid:15)(cid:3) (cid:13) (cid:1) 	(cid:6)(cid:15)(cid:1) (cid:6)	(cid:1)(cid:6)(cid:13)(cid:1) $(cid:2)(cid:22)(cid:2)(cid:17)  (cid:1)(cid:3)(cid:20)(cid:1)	(cid:2)(cid:2)(cid:18) (cid:9)	(cid:3)(cid:9)(cid:17)(cid:3) (cid:9)(cid:1)(cid:2)(cid:2) (cid:1)(cid:17)(cid:22)(cid:2) (cid:19)(cid:2)(cid:2) (cid:2)(cid:18) (cid:3) (cid:2)(cid:9)(cid:2) (cid:27)(cid:2)(cid:17)  (cid:17)(cid:2)  (cid:22)(cid:3)(cid:18)(cid:2) (cid:17)  	(cid:3)  (cid:1)(cid:2) (cid:9)	(cid:3)(cid:9)(cid:17)(cid:3) (cid:19) (cid:2)(cid:2)(cid:9)+(cid:25) (cid:2) 	(cid:9)(cid:1) (cid:2)(cid:9)(cid:1)(cid:3)	(cid:2) (cid:3) (cid:1)(cid:2) (cid:15)(cid:16)(cid:17) (cid:3)(cid:3)(cid:3)(cid:17)  (cid:27) (cid:2)(cid:18) (cid:3) (cid:30): (cid:17)(cid:18) (cid:1)(cid:2) (cid:18)(cid:2)(cid:22)(cid:2) (cid:2)(cid:18) (cid:17) (cid:0)(cid:2)(cid:9)(cid:1)(cid:3) (cid:2)(cid:2) (cid:7) (cid:2)(cid:29)(cid:17) (cid:2) (cid:30); (cid:25)  (cid:1)(cid:3) (cid:2)(cid:9)(cid:1)(cid:3)	(cid:2) (cid:16) (cid:16)(cid:3)(cid:2) (cid:9)(cid:17)(cid:27) (cid:2) (cid:19)(cid:3) (cid:7) (cid:18)(cid:17)(cid:17)< (cid:1)(cid:2) $  (cid:3)(cid:2) (cid:3)(cid:18)(cid:3)(cid:9)(cid:17)(cid:2) (cid:1)(cid:2) )(cid:17)(cid:2)* (cid:7) (cid:1)(cid:2)  (cid:3)(cid:2) (cid:17)(cid:18) (cid:1)(cid:2)   (cid:3)(cid:2) (cid:3)(cid:18)(cid:3)(cid:9)(cid:17)(cid:2) (cid:1)(cid:2) )(cid:1)(cid:17)(cid:2)*(cid:25) (cid:0)(cid:1)(cid:2) $  (cid:3)(cid:2) (cid:3) (cid:17) (cid:16)(cid:17)(cid:27) (cid:1)(cid:2) (cid:18)(cid:17)(cid:17)  (cid:3)(cid:2)’ (cid:1)(cid:2)   (cid:3)(cid:2) (cid:3)(cid:2)(cid:17)(cid:18) (cid:9)(cid:1)(cid:17)(cid:20)(cid:2) (cid:17)(cid:2) (cid:3)(cid:7) (cid:1)(cid:2) $  (cid:3)(cid:2) (cid:18)(cid:2) (cid:25) (cid:0)(cid:1)	 (cid:3)(cid:7) (cid:1)(cid:2) (cid:17)(cid:9)+(cid:16) (cid:2)(cid:18)(cid:20)(cid:2)(cid:2) (cid:3)(cid:20)(cid:17)  (cid:3) (cid:3)(cid:20)(cid:2)(cid:18) (cid:17) (cid:3)(cid:20) (cid:2) (cid:17)(cid:3)(cid:3) (cid:2) (cid:19)(cid:3) (cid:3) (cid:2)	(cid:3)(cid:2)(cid:18) (cid:19)(cid:17)(cid:3)	 (cid:3)(cid:20)  (cid:16) (cid:16)(cid:2) (cid:7) (cid:1)(cid:2) (cid:18)(cid:17)(cid:17) (cid:9)	(cid:3)(cid:9)(cid:17)(cid:3)(cid:25)  (cid:17)(cid:9)(cid:3)(cid:9)(cid:2) (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:17)(cid:3)(cid:3) (cid:3)  (cid:3)(cid:20)(cid:1) (cid:27) (cid:1)(cid:3)(cid:20)(cid:1)(cid:2) (cid:1)(cid:17) (cid:2) (cid:17) (cid:17) (cid:16)(cid:18)	(cid:17)(cid:9)+(cid:16) (cid:2)(cid:18)(cid:20)(cid:2)(cid:2) (cid:3) (cid:2) (cid:27)(cid:2)(cid:18)(cid:25) (cid:0)(cid:1)(cid:3) (cid:2)(cid:9)(cid:1)(cid:3)	(cid:2) (cid:2)	 	(cid:3)(cid:2) (cid:1)(cid:3)(cid:2) (cid:18)(cid:3)(cid:20) (cid:7) (cid:1)(cid:2) (cid:16)(cid:3)(cid:2)  (cid:17)(cid:22)(cid:3)(cid:18) (cid:9)	(cid:17) + (cid:19)(cid:1) (cid:7) (cid:1)(cid:2) (cid:2)(cid:22)(cid:3)(cid:2) (cid:17)(cid:18) (cid:7) (cid:1)(cid:2) (cid:1)(cid:27)(cid:3)(cid:9)(cid:17)  (cid:16)(cid:3)(cid:2) (cid:3)(cid:22) (cid:22)(cid:2)(cid:18) (cid:3) (cid:1)(cid:2) (cid:9)	(cid:3)(cid:9)(cid:17)(cid:3)< (cid:1)(cid:2) (cid:9)(cid:1)(cid:2)(cid:2) (cid:2)(cid:18) (cid:19)(cid:27) 0(cid:19)+(cid:3) (cid:2) (cid:17) (cid:25) (cid:2) (cid:27) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2)(cid:3)(cid:17)  (cid:3)(cid:20)(cid:17)  (cid:3)(cid:20)  (cid:3)(cid:9)(cid:2)(cid:17)(cid:2) (cid:3)(cid:20)(cid:17)  (cid:3)(cid:2)(cid:20)(cid:3)(cid:27) (cid:17)(cid:18) (cid:20)	(cid:18)  (cid:3)(cid:2) (cid:7) (cid:1)(cid:3)(cid:2) (cid:18)(cid:3)(cid:20)(cid:25) (cid:0)(cid:1)(cid:3) (cid:1)(cid:2)(cid:2)(cid:7)(cid:2) (cid:2)	  (cid:3) : (cid:16)(cid:3)(cid:2) (cid:17)(cid:18) - (cid:17)(cid:3)(cid:3) (cid:2) (cid:19)(cid:3) (cid:2)  (cid:3)+(cid:25) 1 (cid:18)(cid:3)(cid:26)(cid:2)(cid:2) (cid:17)(cid:17)(cid:9)(cid:1) (cid:3) (cid:2)(cid:18) (cid:19)(cid:27) (cid:2)(cid:2) (cid:2) (cid:17) (cid:25) (cid:3) (cid:30)7  (cid:16)(cid:1)(cid:2)(cid:2) (cid:1)(cid:2) (cid:2)(cid:3)(cid:17) (cid:3)(cid:17)(cid:3) (cid:3) (cid:18)(cid:2) 	(cid:3)(cid:20) (cid:16)(cid:17)(cid:22)(cid:2)	(cid:3)(cid:2) (cid:3)(cid:3)(cid:20) (cid:17) (cid:9)	 (cid:9)(cid:2) 4 (cid:2)(cid:18) (cid:19)(cid:27) (cid:24)(cid:2) (cid:3) (cid:30)= (cid:25) (cid:0)(cid:1)(cid:3) (cid:9)(cid:3) (cid:3) 		 (cid:3)(cid:20) (cid:1)(cid:2) (cid:17)(cid:20)(cid:17)(cid:3) (cid:18)(cid:2) (cid:17)(cid:27) (cid:7) (cid:1)(cid:2) (cid:16)(cid:3)(cid:2)  (cid:9)(cid:2)(cid:17)(cid:2) )(cid:3) (cid:3)(cid:9)(cid:3)* (cid:3)(cid:2) (cid:3)(cid:2)< (cid:1)(cid:2) (cid:18)(cid:17)(cid:17) (cid:17)(cid:22)(cid:2)   (cid:16)(cid:17)(cid:22)(cid:2)	(cid:7) (cid:17)(cid:9) (cid:1)(cid:2) (cid:16)(cid:3)(cid:2)(cid:25)  (cid:1)(cid:2) (cid:18)(cid:19)(cid:20)(cid:21) (cid:9)(cid:1)(cid:2)(cid:2) (cid:30)7 (cid:7)(cid:17) (cid:1)(cid:3)(cid:7) (cid:2)(cid:20)(cid:3)(cid:2) (cid:17)(cid:2) 	(cid:2)(cid:18) (cid:17) (cid:1)(cid:2) (cid:2)(cid:18)(cid:2) (cid:17)(cid:18) (cid:17) (cid:1)(cid:2) (cid:2)(cid:9)(cid:2)(cid:3)(cid:22)(cid:2)(cid:25) (cid:0)(cid:1)(cid:2) (cid:16) (cid:1)(cid:3)(cid:7) (cid:2)(cid:20)(cid:3)(cid:2) (cid:17)(cid:2) )	(cid:2)(cid:18)* (cid:19)(cid:27) 	(cid:3)(cid:20) (cid:17)(cid:9)(cid:1)(cid:3)(cid:20) (cid:18)(cid:2) (cid:17)(cid:27) (cid:2) (cid:2)(cid:2)(cid:25) 1 (cid:3)  	 (cid:2) (cid:3) 	(cid:2)(cid:18)  (cid:3)(cid:18)(cid:3)(cid:9)(cid:17)(cid:2) (cid:1)(cid:2) (cid:2)(cid:2)(cid:9)(cid:2) (cid:7) (cid:18)(cid:17)(cid:17)(cid:25) (cid:0)(cid:1)(cid:2) (cid:17)(cid:17)(cid:9)(cid:3)(cid:22)(cid:2)(cid:2) (cid:7) (cid:1)(cid:3) (cid:9)(cid:1)(cid:2)(cid:2) (cid:3) (cid:1)(cid:2) (cid:3)(cid:20) (cid:2)	(cid:16)(cid:3)(cid:2) (cid:17)	(cid:2) (cid:7) (cid:1)(cid:2)  (cid:3)+(cid:25)  (cid:16)	 (cid:2)(cid:22)(cid:2) (cid:2)(cid:17)(cid:17)(cid:19)(cid:3) (cid:3)(cid:27) (cid:17) (cid:1)(cid:2) (cid:2)(cid:9)(cid:2)(cid:3)(cid:22)(cid:2) (cid:2)(cid:18) (cid:3)  (cid:18)(cid:3)(cid:9)	(cid:2)(cid:18) (cid:3) (cid:1)(cid:2) 	(cid:19) (cid:3)(cid:1)(cid:2)(cid:18) (cid:16)+  (cid:1)(cid:2) 	(cid:19) ?(cid:2)(cid:9)(cid:25) 1 (cid:18)(cid:2) (cid:17)(cid:27)	(cid:3)(cid:2)(cid:3)(cid:3)(cid:22)(cid:2) (cid:9)(cid:1)(cid:2)(cid:2) (cid:3) (cid:2)(cid:18) (cid:19)(cid:27) @(cid:17)(cid:3)(cid:19)(cid:3)(cid:18)(cid:20)(cid:2) (cid:30)(cid:31)6  (cid:19)(cid:17)(cid:2)(cid:18)  (cid:2)(cid:17) (cid:3)(cid:2) (cid:16)+  	(cid:14)	 (cid:2)(cid:9)(cid:18)(cid:3)(cid:20) (cid:30)(cid:31)(cid:31) (cid:25) (cid:0)(cid:1)(cid:2) (cid:3)(cid:18)(cid:2)(cid:17) (cid:3)  	(cid:2) (cid:17) (cid:31)	(cid:7)	-	(cid:2)(cid:9)(cid:18)(cid:2)(cid:18)  (cid:3)+ (cid:17) (cid:1)(cid:2) (cid:1)(cid:27)(cid:3)(cid:9)(cid:17)   (cid:17)(cid:27)(cid:2) (cid:7) (cid:17) (cid:2)(cid:16)+		(cid:24)(cid:1)(cid:3)(cid:25) 0(cid:2) (cid:17)(cid:27)	(cid:3)(cid:2)(cid:3)(cid:3)(cid:22)(cid:2) (cid:9)(cid:3)(cid:9)	(cid:3) (cid:18)(cid:2)4(cid:2) (cid:17) (cid:9) (cid:17) (cid:7) (cid:9)(cid:3)(cid:9)	(cid:3) (cid:16)(cid:1)(cid:2)(cid:2) (cid:1)(cid:2) (cid:9)	(cid:17)(cid:3) (cid:3) 	(cid:17)(cid:26)(cid:2)(cid:9)(cid:2)(cid:18) (cid:19)(cid:27) (cid:1)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27) (cid:7) (cid:1)(cid:2) (cid:16)(cid:3)(cid:2) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:20)(cid:17)(cid:2)(cid:25)  (cid:7)(cid:17)(cid:9) (cid:3)(cid:17)(cid:9)(cid:1)(cid:2) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:1)(cid:2) (cid:16)(cid:3)(cid:2) (cid:3)(cid:18)	(cid:9)(cid:2) 	(cid:2)(cid:29)(cid:2)(cid:9)(cid:2)(cid:18) (cid:18)(cid:2) (cid:17)(cid:27) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:9)(cid:17)	(cid:2) (cid:27)(cid:3)(cid:9)(cid:17)  (cid:27)(cid:9)(cid:1)	 (cid:9)(cid:3)(cid:9)	(cid:3)  (cid:7)(cid:17)(cid:3)  (cid:3)(cid:3)(cid:20)	(cid:9) 	(cid:2) (cid:19)(cid:27) (cid:9)(cid:17)		 (cid:3)(cid:20) (cid:1)(cid:2) (cid:2)	 (cid:17)(cid:18) (cid:1) (cid:18) (cid:9)(cid:18)(cid:3)(cid:3)   (cid:19)(cid:2) (cid:2)(cid:25) 0(cid:2) (cid:17)(cid:27)	 (cid:3)(cid:2)(cid:3)(cid:3)(cid:22)(cid:3)(cid:27) (cid:3) (cid:17)  (cid:27) (cid:17)(cid:9)(cid:1)(cid:3)(cid:2)(cid:22)(cid:2)(cid:18) (cid:19)(cid:27) 	(cid:3)(cid:20) 	 (cid:3) (cid:2)	(cid:17)(cid:3)  (cid:9)(cid:18)(cid:3)(cid:20) (cid:1)(cid:2)  (cid:9) (cid:7) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:3) (cid:1)(cid:2) (cid:18)	(cid:17) 	(cid:17)(cid:3) (cid:25) (cid:31)	(cid:7)	(cid:17)  (cid:16)  (cid:2) (cid:17)(cid:3)(cid:3)B(cid:19)(cid:3) (cid:17) (cid:1)(cid:2) (cid:9) (cid:7) (cid:16)(cid:3)(cid:2) (cid:17)(cid:2)(cid:17)(cid:25) 		(cid:17)(cid:3)	(cid:18)(cid:2) (cid:17)(cid:27)	(cid:3)(cid:2)(cid:3)(cid:3)(cid:22)(cid:2) 	0 (cid:17)(cid:27)(cid:9)(cid:1)	 (cid:9)	(cid:3)(cid:9)(cid:17)	 (cid:3) (cid:9)  (cid:9)(cid:3) (cid:7) (cid:9)(cid:1)(cid:2)(cid:2) (cid:16)(cid:1)(cid:2)(cid:2) (cid:1)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27) (cid:3)	 (cid:18)	(cid:9)(cid:2)(cid:18) (cid:19)(cid:27) (cid:1)(cid:2)  (cid:3)(cid:2) (cid:1)(cid:17) (cid:17) (cid:2)(cid:26)(cid:2)(cid:9)  (cid:1)(cid:2) (cid:19)(cid:2)(cid:1)(cid:17)(cid:22)(cid:3)	 (cid:7) (cid:1)(cid:2)  (cid:3)+ (cid:19)	 (cid:1)(cid:2)(cid:2) (cid:2)(cid:26)(cid:2)(cid:9) (cid:9)(cid:17) (cid:19)(cid:2) (cid:3)(cid:3)(cid:3)(cid:2)(cid:18)(cid:25) @	(cid:18) (cid:2)(cid:18)	(cid:18)(cid:17)(cid:17) (cid:9)  (cid:17)(cid:2) (cid:1)(cid:2) (cid:27)(cid:3)(cid:9)(cid:17)  (cid:9)(cid:17)(cid:2)< (cid:1)(cid:2) (cid:18)(cid:17)(cid:17) (cid:3)(cid:2) (cid:17)(cid:2) (cid:17)	 (cid:3)(cid:2)(cid:18) (cid:22)(cid:2) (cid:17)(cid:17)  (cid:2)   (cid:3)(cid:2) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:17)(cid:2) (cid:17)(cid:9)(cid:1)(cid:2)(cid:18)  (cid:17) 	  (cid:2) (cid:3)(cid:2) (cid:3)(cid:2) (cid:3) (cid:1)(cid:2) (cid:3) (cid:2) (cid:9)(cid:17)(cid:2)(cid:25) (cid:0) (cid:3)(cid:9)(cid:2)(cid:17)(cid:2) (cid:2)(cid:2)(cid:18) (cid:17)(cid:18) (cid:2) (cid:3)	 (cid:17)(cid:19)(cid:3) (cid:3)(cid:27)  (cid:16)+ (cid:1)(cid:17) (cid:19)(cid:2)(cid:2) (cid:18)(cid:2)  (cid:1)(cid:2) (cid:3)(cid:2) (cid:3)(cid:2) (cid:9)(cid:1)(cid:2)(cid:2)(cid:25)  (cid:20)(cid:2)(cid:2)(cid:17)  -	(cid:1)(cid:17)(cid:2) (cid:3)(cid:2) (cid:3)(cid:3)(cid:20) (cid:3) (cid:1)(cid:2) (cid:3) (cid:2) (cid:9)(cid:17)(cid:2) (cid:16)(cid:1)(cid:2)(cid:2) (cid:1)(cid:2) (cid:17)(cid:2) (cid:7) (cid:1)(cid:2)  (cid:3)(cid:2) (cid:3)(cid:18)(cid:3)(cid:9)(cid:17)(cid:2) (cid:1)(cid:2) (cid:2)(cid:2)(cid:9)(cid:2)  (cid:17)(cid:19)(cid:2)(cid:9)(cid:2) (cid:7) (cid:18)(cid:17)(cid:17)(cid:25)  (cid:18)(cid:2)  (cid:3)(cid:9)(cid:2)(cid:17)(cid:2) (cid:2)(cid:2)(cid:18) %	(cid:1)(cid:17)(cid:2) (cid:3)(cid:2) (cid:3)(cid:2) (cid:9)(cid:17) (cid:19)(cid:2) 	(cid:2)(cid:18) (cid:16)(cid:1)(cid:2)(cid:2) (cid:17)(cid:3)(cid:3) (cid:17)(cid:2) 	(cid:2)(cid:18)  (cid:2)(cid:9)(cid:18)(cid:2) (cid:1)(cid:2) )(cid:2)	(cid:2)* (cid:17)(cid:18) )(cid:17)(cid:9)+(cid:16) (cid:2)(cid:18)(cid:20)(cid:2)* (cid:3)(cid:20)(cid:17) ’ (cid:1)(cid:3) (cid:9)(cid:1)(cid:2)(cid:2) (cid:16)(cid:17) (cid:3)(cid:2)(cid:2)(cid:2)(cid:18) (cid:19)(cid:27) (cid:22)(cid:17) $	(cid:1)(cid:2) (cid:17)(cid:18) (cid:30)(cid:31)% (cid:25) 2(cid:17) (cid:30)(cid:31)( (cid:3) (cid:17) (cid:2)(cid:9)(cid:1)(cid:3)	(cid:2) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:17)(cid:3)  (cid:3)(cid:9)(cid:2)(cid:17)(cid:2) (cid:2)(cid:2)(cid:18) (cid:7) (cid:1)(cid:2) (cid:3)(cid:2) (cid:3)(cid:2) 	(cid:3)(cid:20) %	(cid:1)(cid:17)(cid:2) (cid:3)(cid:2) (cid:3)(cid:3)(cid:20)(cid:25) (cid:22)	(cid:23)(cid:9)(cid:13) (cid:30)(cid:31)- (cid:1)(cid:17) (cid:19)(cid:2)(cid:2) (cid:2)(cid:18) (cid:17) (cid:17) (cid:2)(cid:17)  (cid:3)(cid:22)(cid:2) (cid:2)	  (cid:3)(cid:17)(cid:19)(cid:3) (cid:3)(cid:27) (cid:7) (cid:1)(cid:2) (cid:3)(cid:2) (cid:3)(cid:2)(cid:25)  (cid:20)(cid:2)(cid:2)(cid:17)  (cid:3)(cid:17)(cid:9)(cid:1)(cid:2)  (cid:1)(cid:2) (cid:18)(cid:17)(cid:17)  (cid:3)(cid:2) (cid:3)(cid:20)(cid:1) (cid:9)(cid:17)	(cid:2) 	(cid:17)(cid:9)(cid:9)(cid:2)(cid:17)(cid:19) (cid:2) (cid:3)(cid:20)(cid:17)  (cid:9)	(cid:3)(cid:25) (cid:22)	(cid:23)(cid:9)(cid:13) (cid:17)(cid:2)  (cid:3)(cid:22)(cid:2) (cid:2) (cid:3)(cid:17)(cid:19)(cid:3) (cid:3)(cid:27) (cid:16)(cid:1)(cid:3) (cid:2) (cid:17)(cid:3)(cid:17)(cid:3) (cid:1)(cid:3)(cid:20)(cid:1) (cid:9)	 	(cid:3)(cid:9)(cid:17)(cid:3) (cid:2)(cid:2)(cid:18) (cid:19)(cid:27) )(cid:1)(cid:2) (cid:3)(cid:20)* (cid:18)(cid:17)(cid:17)  (cid:3)(cid:2)  +(cid:2)(cid:2) 	 (cid:16)(cid:3)(cid:1) (cid:1)(cid:2) (cid:2)	(cid:2)  (cid:3)(cid:2) (cid:1)	 (cid:1)(cid:3)(cid:20) 	 (cid:1)(cid:2) (cid:18)(cid:2) (cid:2)(cid:2)(cid:3)	 (cid:2)(cid:26)(cid:2)(cid:9) (cid:7) (cid:3)(cid:17)(cid:9)(cid:1)(cid:2)(cid:25) 8 (cid:1)(cid:2)  (cid:3)(cid:2)(cid:17)	(cid:2) (cid:3) (cid:17)(cid:2)(cid:17) (cid:1)(cid:17) (cid:1)(cid:2) (cid:18)(cid:2)(cid:3)(cid:20) (cid:7)  (cid:20)	 (cid:18)(cid:3)(cid:17)(cid:9)(cid:2)  (cid:3)+ (cid:7) (cid:24) (cid:17)(cid:18) $(cid:24) (cid:3) (cid:20)(cid:2)(cid:2)(cid:17)  (cid:16)(cid:3)   (cid:2)	(cid:3)(cid:2) (cid:2) (cid:7) (cid:7) (cid:2) (cid:7)	(cid:3)(cid:2)(cid:18) (cid:17)(cid:17)(cid:9)(cid:1)(cid:2)(cid:25) (cid:7) (cid:1)(cid:2)(cid:2) 	(cid:14)	 (cid:2)	 (cid:9)(cid:18)(cid:3)(cid:20) (cid:2)	(cid:3)(cid:2) (cid:17)(cid:9)+(cid:16) (cid:2)(cid:18)(cid:20)(cid:2)(cid:2) (cid:3)(cid:20)(cid:17)   (cid:2)	(cid:2) (cid:9)(cid:2)(cid:9) (cid:2)(cid:17)(cid:3) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:1)(cid:16)(cid:2)(cid:22)(cid:2) (cid:2)(cid:18)	(cid:9)(cid:2) (cid:1)(cid:2) (cid:2)(cid:2)(cid:18) (cid:7) (cid:1)(cid:2)  (cid:3)+(cid:25) (cid:17)	 (cid:17)  (cid:2)   (cid:3)+ (cid:19)	(cid:18) (cid:2)(cid:18)	(cid:18)(cid:17)(cid:17) (cid:7) (cid:2)(cid:29)(cid:17) (cid:2) (cid:3)(cid:9)(cid:2)(cid:17)(cid:2) (cid:19)(cid:3)(cid:17)(cid:2) (cid:17) (cid:1)(cid:2) (cid:2)(cid:29)(cid:2)(cid:2) (cid:7) (cid:2) (cid:9) (cid:2)(cid:29) (cid:3)(cid:3)(cid:20)	(cid:9) 	(cid:2) (cid:19) (cid:2) (cid:18)	(cid:2)  (cid:1)(cid:2) (cid:2)	(cid:3)(cid:2)(cid:2)  (cid:17)(cid:9)(cid:1) (cid:17) 	(cid:19)(cid:2) (cid:7) (cid:18)(cid:17)(cid:17)  (cid:3)(cid:2) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:3) 	 (cid:2)(cid:18)	(cid:9)(cid:2)  (cid:3)+ (cid:2)(cid:2)(cid:18)(cid:25) 8(cid:3)(cid:17)  (cid:27) (cid:17)(cid:18)(cid:3)(cid:3)(cid:17)  (cid:2)(cid:3)(cid:17)   (cid:3)+ (cid:1)(cid:17)(cid:22)(cid:2) (cid:1)(cid:2) (cid:17)(cid:18)(cid:22)(cid:17)(cid:17)(cid:20)(cid:2) (cid:7)  (cid:3)(cid:3)(cid:3)(cid:20) (cid:1)(cid:2) (cid:3)(cid:3)(cid:20) (cid:9) 	(cid:2) (cid:19) (cid:2)  (cid:16) (cid:16)(cid:3)(cid:2) (cid:9) (cid:9)+ (cid:17)(cid:18) (cid:18)(cid:17)(cid:17) (cid:19)	 (cid:1)(cid:2) (cid:18)(cid:3)(cid:17)(cid:18)(cid:22)(cid:17)(cid:17)(cid:20)(cid:2) (cid:7) (cid:2)	 (cid:18)	(cid:9)(cid:3)(cid:20) (cid:19)(cid:3)(cid:17)(cid:2)(cid:25) (cid:0)(cid:1)(cid:2) (cid:2)(cid:1)(cid:18) (cid:16)(cid:2) (cid:2) (cid:2)(cid:1)(cid:17)(cid:9)(cid:2) (cid:1)(cid:2) (cid:2)(cid:3)(cid:17)  start stop out0 out1 out2 out3 out n δ δ δ δ  (cid:2)(cid:20)(cid:31) (cid:29)(cid:31) (cid:0)(cid:21)(cid:11)(cid:12) (cid:12)(cid:11) (cid:21)(cid:14)  (cid:2)(cid:11) (cid:0)""(cid:19) (cid:17)	(cid:2) (cid:7) (cid:1)(cid:2)  (cid:3)+ (cid:3)(cid:9)(cid:2)(cid:17)(cid:3)(cid:20) (cid:1)(cid:2) (cid:17)  (cid:16)(cid:2)(cid:18) (cid:19)(cid:3)(cid:17)(cid:2) (cid:7) (cid:17) (cid:20)(cid:3)(cid:22)(cid:2) (cid:9) (cid:9)+ (cid:2)(cid:2)(cid:18)(cid:25) 1  (cid:3) (cid:3) (cid:3)(cid:1)(cid:2)(cid:2) (cid:27) (cid:2) (cid:7)	(cid:3)(cid:2)(cid:18) (cid:17)(cid:18)  	(cid:3)	 (cid:17)(cid:19) (cid:2) (cid:7) (cid:17)(cid:27)(cid:9)(cid:1)	 (cid:3)(cid:20)(cid:17)  (cid:3)(cid:20) (cid:17) (cid:1)(cid:2) (cid:2)(cid:2)(cid:9)(cid:2) (cid:7) (cid:18)(cid:17)(cid:17) (cid:3) (cid:3)(cid:18)(cid:3)(cid:9)(cid:17)(cid:2)(cid:18) (cid:19)(cid:27) (cid:1)(cid:2) (cid:16)(cid:3)(cid:9)(cid:1)(cid:3)(cid:20) (cid:7) (cid:19)(cid:1)  (cid:3)(cid:2)(cid:25) (cid:19)(cid:11) (cid:20)(cid:1)(cid:3)		(cid:21)(cid:1)(cid:9)(cid:1)(cid:13)  (cid:23)(cid:24)(cid:3)(cid:3) (cid:0)(cid:3)(cid:2)		0(cid:3)(cid:20)(cid:3)(cid:17)  (cid:24)(cid:22)(cid:2)(cid:2) (cid:0)0(cid:24) (cid:17)(cid:2) (cid:3) (cid:20)(cid:2)(cid:2)(cid:17)  (cid:18)(cid:2)	 (cid:22)(cid:3)(cid:9)(cid:2) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:2)(cid:17)	(cid:2) (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:16) (cid:2)(cid:22)(cid:2) (cid:17)(cid:18) (cid:18)	(cid:9)(cid:2) (cid:17) 	(cid:2)(cid:3)(cid:9)(cid:17)  (cid:22)(cid:17) 	(cid:2) (cid:3)(cid:17)   (cid:1)(cid:3) (cid:2)(cid:17)	(cid:2)(cid:25) 0(cid:3)(cid:26)(cid:2)(cid:2) 	(cid:9)	(cid:2) (cid:7) (cid:0)0(cid:24) (cid:1)(cid:17)(cid:22)(cid:2) (cid:19)(cid:2)(cid:2) 	 (cid:2)(cid:18) (cid:22)(cid:2) (cid:1)(cid:2) (cid:27)(cid:2)(cid:17) (cid:30)(cid:31).  (cid:17) (cid:1)(cid:3) (cid:9) (cid:17) (cid:7) (cid:18)(cid:2)(cid:22)(cid:3)(cid:9)(cid:2) (cid:3) 	(cid:2)	 (cid:7)	  (cid:3) (cid:17)(cid:27) (cid:17) (cid:3)(cid:9)(cid:17)(cid:3) (cid:19)(cid:1) (cid:3) (cid:9)(cid:3)(cid:2)(cid:9)(cid:2) (cid:17)(cid:18) (cid:3)(cid:18)	(cid:27)(cid:25) (cid:0) (cid:3) (cid:3)(cid:7)(cid:27) (cid:1)(cid:2) (cid:18)(cid:2)(cid:9)(cid:3)(cid:3) (cid:9)(cid:3)(cid:18)(cid:2) (cid:1)(cid:2) (cid:9)(cid:17)(cid:2) (cid:16)(cid:1)(cid:2)(cid:2) (cid:16) (cid:3)(cid:3)(cid:20) (cid:2)(cid:18)(cid:20)(cid:2) (cid:2)(cid:2)(cid:2) (cid:1)(cid:2) (cid:1)(cid:27)(cid:3)(cid:9)(cid:17)  (cid:2)(cid:22)(cid:2) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:3)  (cid:19)(cid:2) (cid:2)(cid:17)	(cid:2)(cid:18)(cid:25) E(cid:2) (cid:18)(cid:2)(cid:2) (cid:1)(cid:2)(cid:2) (cid:2)(cid:22)(cid:2) (cid:17) )(cid:17)* (cid:17)(cid:18) )*(cid:25) (cid:2)(cid:3)(cid:9)  (cid:9)(cid:17)(cid:2) (cid:1)(cid:2) (cid:2)(cid:7)(cid:17)(cid:9)(cid:2) (cid:7) (cid:0)0(cid:24) (cid:3)(cid:9) 	(cid:18)(cid:2) (cid:9)(cid:17)	(cid:2) (cid:17)(cid:20)(cid:2) (cid:1)(cid:2) (cid:17)(cid:29)(cid:3)	 (cid:3)(cid:2) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2)(cid:9)(cid:2) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:2)(cid:22)(cid:2) (cid:1)(cid:17) (cid:9)(cid:17) (cid:19)(cid:2) (cid:2)(cid:17)	(cid:2)(cid:18) (cid:2) 	(cid:3) (cid:1)(cid:2) (cid:3)(cid:3)	 (cid:18)(cid:3)(cid:26)(cid:2)(cid:2)(cid:9)(cid:2) (cid:3) (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:2)(cid:22)(cid:2) (cid:1)(cid:17) (cid:9)(cid:17) (cid:19)(cid:2) 	(cid:2)(cid:18)  (cid:18)(cid:3)(cid:3)(cid:20)	(cid:3)(cid:1) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:16) (cid:2)(cid:17)	(cid:2) (cid:17)(cid:18)  (cid:17)(cid:2)(cid:9)(cid:27) (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)	(cid:3)(cid:2)(cid:18)  (cid:18)	(cid:9)(cid:2) (cid:17) 		(cid:25) (cid:0)(cid:1)(cid:2)  	(cid:2)(cid:18) (cid:0)0(cid:24) (cid:17)(cid:18) (cid:1)(cid:2) (cid:3) (cid:2)  (cid:22)(cid:3)	(cid:17) (cid:3)(cid:2) (cid:17)(cid:2) (cid:19)	(cid:3)  	(cid:3)(cid:20) (cid:17) (cid:17)(cid:2)(cid:18) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2)< (cid:1)(cid:2) )(cid:17)* (cid:2)(cid:22)(cid:2) (cid:3) (cid:9)(cid:1)(cid:17)	 (cid:2)  (cid:2)(cid:18) (cid:3) (cid:17) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2) (cid:17)(cid:18) (cid:2)(cid:22)(cid:2)(cid:27) (cid:18)(cid:2) (cid:17)(cid:27)(cid:2)(cid:18) (cid:22)(cid:2)(cid:3) (cid:7) (cid:1)(cid:2) (cid:2)(cid:22)(cid:2) (cid:1)(cid:2) )(cid:17)* (cid:3) (cid:9)(cid:17)(cid:2)(cid:18)  (cid:1)(cid:2)  (cid:2)(cid:22)(cid:2)(cid:25) 1 	(cid:19)(cid:2) (cid:7) (cid:17)(cid:19)(cid:3)(cid:17)(cid:3) (cid:2) (cid:2)(cid:2) (cid:3) 	(cid:2)(cid:18)  (cid:9)(cid:1)(cid:2)(cid:9)+ (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:2)(cid:22)(cid:2) (cid:9)(cid:9)	 4 (cid:17) (cid:2)(cid:22)(cid:2)(cid:27) (cid:17)(cid:25) (cid:0)(cid:1)(cid:2)(cid:2)(cid:7)(cid:2) (cid:17) 	(cid:19)(cid:2) (cid:7) (cid:17)(cid:19)(cid:3)(cid:17)(cid:3) (cid:2) (cid:2)	 (cid:2) (cid:16)(cid:3)   (cid:1)(cid:16) (cid:1)(cid:17) (cid:2) (cid:2)(cid:22)(cid:2) (cid:16)(cid:17) 4 (cid:16)(cid:1)(cid:3) (cid:2) (cid:1)(cid:2) (cid:2) (cid:16)(cid:3)   (cid:1)(cid:16) (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)	 (cid:25) 1 (cid:2)(cid:29)(cid:17) (cid:2) (cid:7) 	(cid:9)(cid:1) (cid:17) (cid:18)(cid:2)(cid:22)(cid:3)(cid:9)(cid:2) (cid:9)(cid:17) (cid:19)(cid:2) (cid:7)	(cid:18) (cid:3) (cid:30)(cid:31):  (cid:16)(cid:1)(cid:3) (cid:2) 8(cid:3)(cid:20)	(cid:2) (cid:31) (cid:1)(cid:16) (cid:17) (cid:17)(cid:2)(cid:18) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2) (cid:0)0(cid:24) 	(cid:3)(cid:20) 		(cid:17)  (cid:2)(cid:29)(cid:9) 	(cid:3) (cid:2) (cid:2)(cid:2) F(cid:0)GH(cid:2) (cid:30)(cid:31); (cid:25) 8(cid:3)(cid:20)	(cid:2) % (cid:1)(cid:16) (cid:1)(cid:2) (cid:3)(cid:3)(cid:20) (cid:18)(cid:3)(cid:17)(cid:20)(cid:17) (cid:7) (cid:1)(cid:2) (cid:17)(cid:2)(cid:18) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2) (cid:0)0(cid:24)(cid:25) (cid:0)(cid:1)(cid:2) )(cid:17)* (cid:17)(cid:18) (cid:1)(cid:2) )* (cid:3)(cid:20)(cid:17)  (cid:17)(cid:2) (cid:1)(cid:2) (cid:2)(cid:22)(cid:2) (cid:16)(cid:1)(cid:2) (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) ∆ (cid:3)  (cid:19)(cid:2) (cid:2)(cid:17)	(cid:2)(cid:18)(cid:25) (cid:0)(cid:1)(cid:2) )(cid:17)* (cid:3)(cid:20)(cid:17)  (cid:3) (cid:18)(cid:2) (cid:17)(cid:27)(cid:2)(cid:18) (cid:1)	(cid:20)(cid:1) (cid:1)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2) (cid:17)(cid:18) (cid:2)(cid:17)(cid:9)(cid:1) (cid:17) (cid:3) (cid:9)(cid:17)(cid:2)(cid:18)  (cid:1)(cid:2) )* (cid:3)(cid:20)(cid:17) (cid:25) (cid:0)(cid:1)(cid:2) 		 (cid:7) (cid:1)(cid:2) F	 (cid:0)GH(cid:2) (cid:3) (cid:1)(cid:3)(cid:20)(cid:1) (cid:3)(cid:7) (cid:1)(cid:2) (cid:2) (cid:17)(cid:3)(cid:22)(cid:2) (cid:17) (cid:3) (cid:2)(cid:17) (cid:3)(cid:2) (cid:1)(cid:17) (cid:1)(cid:2)  (cid:3)(cid:20)(cid:17)  (cid:17)(cid:18) (cid:3)  (cid:16) (cid:1)(cid:2)(cid:16)(cid:3)(cid:2)(cid:25) (cid:0)(cid:1)(cid:2) (cid:1)(cid:2)(cid:2)(cid:2) (cid:9)(cid:18)(cid:2) 		 ∆ t_0 before t_1 t_1 before t_0 Start Stop s p a t δ t t u p u o e d o c r e e t m o m r e h T k n a b X E T U M out0 out1 out2 out3 out4 out5 out6 1 1 1 1 1 0 0 4δ < ∆ < 5δ ref t_1 t_0 sp0 sp1 sp0 sp1 data 0 0 1  (cid:2)(cid:20)(cid:31) ’(cid:31) (cid:15)((cid:21) (cid:11) $ (cid:12)	(cid:21) 	(cid:21)(cid:2)  (cid:1)(cid:21)(cid:11)	(cid:11)*(cid:12)(cid:2)(cid:20) sp0 0 bacd bacd abdc bacd ref a b c d { { { { sp0 sp1 sp0 sp1 data 4 4 1 4  (cid:2)(cid:20)(cid:31) (cid:30)(cid:31) (cid:15)((cid:21) (cid:11) $ 	 (cid:2) (cid:11)	(cid:21)(cid:2)  (cid:1)(cid:21)(cid:11)	(cid:11)*(cid:12)(cid:2)(cid:20) (cid:18)(cid:17)(cid:17)  (cid:17) (cid:19)	’ (cid:17)   (cid:16)(cid:3)(cid:2) (cid:16)(cid:3)(cid:9)(cid:1) (cid:18)	(cid:3)(cid:20) (cid:17) (cid:17)(cid:3)(cid:3)(cid:25) (cid:7) (cid:1)(cid:2)  (cid:3)+ (cid:3) (cid:9) (cid:9)+(cid:2)(cid:18) (cid:1)(cid:2) (cid:16)(cid:3)(cid:2) (cid:16)(cid:3)   (cid:17)(cid:2)(cid:17) (cid:17) (cid:1)(cid:17)(cid:2)	(cid:1)(cid:3)(cid:7)(cid:2)(cid:18) (cid:22)(cid:2)	 (cid:3) (cid:7) (cid:1)(cid:2) (cid:9) (cid:9)+ (cid:1)(cid:2)(cid:9)(cid:2) (cid:1)(cid:2) (cid:18)(cid:2)(cid:3)(cid:17)(cid:3) (cid:7) (cid:1)(cid:2) (cid:9)(cid:1)(cid:2)(cid:2)(cid:25) 8(cid:3)(cid:20)	(cid:2) - (cid:1)(cid:16) (cid:1)(cid:2) (cid:17)(cid:3)(cid:3) (cid:16)(cid:17)(cid:22)(cid:2)(cid:7) (cid:7) (cid:17) (cid:18)	(cid:17) 	(cid:17)(cid:3)  (cid:1)(cid:17)(cid:2)	(cid:2)(cid:9)(cid:18)(cid:2)(cid:18)  (cid:3)+ (cid:17) (cid:18)(cid:2)(cid:9)(cid:3)(cid:19)(cid:2)(cid:18) (cid:3) (cid:30)- < (cid:1)(cid:2) (cid:18)(cid:2) (cid:7) (cid:2)(cid:22)(cid:2)  (cid:1)(cid:2) (cid:16)  (cid:3)(cid:2) (cid:3)(cid:18)(cid:3)(cid:9)(cid:17)(cid:2) (cid:1)(cid:2) (cid:2)(cid:2)(cid:9)(cid:2) (cid:7) (cid:17) (cid:31)  (cid:17) 6(cid:25) 8(cid:3)(cid:20)	(cid:2) . (cid:3)(cid:2)(cid:17)(cid:18) (cid:1)(cid:16) (cid:1)(cid:2) (cid:16)(cid:17)(cid:22)(cid:2)(cid:7) (cid:7) (cid:17) 	 (cid:3) (cid:2)	(cid:17)(cid:3)  (cid:1)(cid:17)(cid:2)	 (cid:2)(cid:9)(cid:18)(cid:2)(cid:18)  (cid:3)+ (cid:17) (cid:18)(cid:2)(cid:9)(cid:3)(cid:19)(cid:2)(cid:18) (cid:3) (cid:30). (cid:25)  (cid:1)(cid:3) (cid:9)(cid:17)(cid:2) (cid:1)(cid:2) (cid:18)(cid:2) (cid:7) (cid:2)(cid:22)(cid:2) (cid:3) (cid:2)(cid:29)(cid:2)(cid:18)(cid:2)(cid:18)  (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:16)(cid:3)(cid:2) 	(cid:2)(cid:18) (cid:3) (cid:1)(cid:2) (cid:3)	  (cid:2)(cid:2)(cid:17)(cid:3)(cid:25)  (cid:1)(cid:3) (cid:9)   (cid:27) (cid:2) (cid:2)(cid:22)(cid:2) (cid:9)(cid:17) (cid:9)(cid:9)	 (cid:17) (cid:17)(cid:27) (cid:3)(cid:2) (cid:17) (cid:3) (cid:3) (cid:3)(cid:3)(cid:19) (cid:2)  (cid:2)(cid:9)(cid:22)(cid:2) (cid:1)(cid:2) (cid:18)(cid:2) (cid:7) (cid:16) (cid:2)(cid:22)(cid:2) (cid:9)(cid:9)	(cid:3)(cid:20) (cid:17) (cid:1)(cid:2) (cid:17)(cid:2) (cid:3)(cid:2)(cid:25) (cid:0)(cid:1)(cid:2) (cid:3)(cid:18)(cid:2)(cid:17) (cid:19)(cid:2)(cid:1)(cid:3)(cid:18) (cid:1)(cid:2)(cid:2) (cid:9)(cid:1)(cid:2)(cid:2) (cid:3)  (cid:3)(cid:9)(cid:2)(cid:17)(cid:2) (cid:1)(cid:2) (cid:3)(cid:18) (cid:2) (cid:3)(cid:2) (cid:7) (cid:1)(cid:2)  (cid:3)+ (cid:2)(cid:18)	(cid:9)(cid:3)(cid:20) (cid:1)(cid:2) (cid:3)(cid:24)(cid:3) (cid:28)(cid:1)(cid:8)(cid:28) (cid:16)(cid:1)(cid:2)(cid:2) (cid:1)(cid:2) (cid:18)(cid:17)(cid:17) (cid:3) (cid:2)(cid:26)(cid:2)(cid:9)(cid:3)(cid:22)(cid:2) (cid:27) (cid:17)(cid:3)(cid:2)(cid:18)(cid:25) (cid:0)(cid:1)(cid:3) (cid:9)(cid:17) (cid:1)(cid:17)(cid:22)(cid:2) (cid:19)(cid:2)(cid:2)4(cid:9)(cid:3)(cid:17)  (cid:2)(cid:26)(cid:2)(cid:9)  (cid:1)(cid:2) (cid:2) (cid:3)(cid:17)(cid:19)(cid:3) (cid:3)(cid:27) (cid:7) (cid:1)(cid:2)  (cid:3)+(cid:25) (cid:0)(cid:1)(cid:2) (cid:9)(cid:1)(cid:2)(cid:2) (cid:2)(cid:18) (cid:3) (cid:1)(cid:2) (cid:2)(cid:2) (cid:16)+ (cid:2)(cid:29)(cid:17)(cid:18) (cid:1)(cid:2) (cid:3)(cid:3)(cid:3)(cid:17)  (cid:1)(cid:17)(cid:2)	(cid:2)(cid:9)(cid:18)(cid:3)(cid:20) (cid:9)(cid:9)(cid:2)< (cid:3) (cid:1)(cid:2) (cid:3)(cid:20)(cid:3)(cid:17)  (cid:17)(cid:2) (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:1)(cid:2) (cid:2)(cid:22)(cid:2) (cid:3)  (cid:2)(cid:17)	(cid:2)(cid:18)’ (cid:17)   (cid:16) (cid:2)(cid:22)(cid:2) (cid:9)(cid:17) (cid:9)(cid:9)	 (cid:17) (cid:1)(cid:2) (cid:17)(cid:2) (cid:3)(cid:2)(cid:25) (cid:0)(cid:1)(cid:3) (cid:3)(cid:2) (cid:17) 4(cid:29)(cid:2)(cid:18) (cid:27)(cid:19)  (cid:3)(cid:2) (cid:7) nδ  (cid:16)(cid:1)(cid:2)(cid:2) n (cid:3) (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:16)(cid:3)(cid:2) (cid:17)(cid:18) δ (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:1)(cid:2)(cid:25) #(cid:2) (cid:17)(cid:29)(cid:3)(cid:20) (cid:1)(cid:2)(cid:2) (cid:9)(cid:17)(cid:3) (cid:17)  (cid:16) (cid:1)(cid:2) (cid:3)(cid:9) 	(cid:3) (cid:7) (cid:2) (cid:19)(cid:3) (cid:2) (cid:27)(cid:19) (cid:25) (cid:0)(cid:1)(cid:2) (cid:9)(cid:3)(cid:19)	(cid:3) (cid:7) (cid:1)(cid:3) (cid:16)+ (cid:3)  (cid:2)(cid:29) (cid:3)   (cid:27) (cid:1)(cid:2) (cid:18)(cid:2) (cid:7) (cid:2)(cid:22)(cid:2) (cid:19)	 (cid:17)  (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:1)(cid:2)(cid:25)  (cid:2)(cid:20)(cid:31) (cid:26)(cid:31) (cid:0)(cid:2)(cid:2)(cid:20) (cid:12)(cid:2)(cid:21)(cid:20)(cid:21) $ (cid:1)(cid:11) (cid:21)(cid:11)(cid:12) (cid:12)(cid:11) (cid:21)(cid:14)  (cid:2)(cid:11) (cid:0)""(cid:19) $ (cid:2)(cid:20)	(cid:11) (cid:29) start stop δ2 δ2 δ2 δ2 out0 out1 out2 out3 out n δ1 δ1 δ1 δ1  (cid:2)(cid:20)(cid:31) %(cid:31) &(cid:11)(cid:2)(cid:11) (cid:12)(cid:11) (cid:21)(cid:14)  (cid:2)(cid:11) (cid:0)""(cid:19) 	 (cid:3)(cid:18)(cid:3)(cid:9)(cid:17)(cid:2) (cid:16)(cid:1)(cid:2)(cid:2) (cid:1)(cid:2) (cid:16) (cid:3)(cid:20)(cid:17)  (cid:22)(cid:2) (cid:17)(cid:2)(cid:18)< (cid:3) (cid:1)(cid:2) (cid:9)(cid:17)(cid:2) (cid:1)(cid:16) (cid:1)(cid:2) )(cid:17)* (cid:17)(cid:18) (cid:1)(cid:2) )* (cid:3)(cid:20)(cid:17)  (cid:16)(cid:2)(cid:2) (cid:2)(cid:17)(cid:17)(cid:2)(cid:18) (cid:19)(cid:27) (cid:17) (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) ∆ (cid:2)(cid:2)(cid:9)(cid:3)(cid:20) (cid:1)(cid:2) (cid:3)(cid:18)(cid:3)(cid:9)(cid:17)(cid:2)(cid:18) (cid:3)(cid:2)	(cid:17) 	 (cid:3)(cid:27)< 4τ < ∆ < 5τ (cid:25) (cid:0)(cid:1)(cid:2)(cid:2)(cid:7)(cid:2) (cid:1)(cid:2) (cid:2) 	(cid:3) (cid:7) (cid:1)(cid:2) (cid:0)0(cid:24) (cid:18)(cid:2)(cid:2)(cid:18)  (cid:1)(cid:2) (cid:17)(cid:20)(cid:17)(cid:3) (cid:18)(cid:2) (cid:17)(cid:27) (cid:7) (cid:2)(cid:17)(cid:9)(cid:1) (cid:18)(cid:2) (cid:17)(cid:27) (cid:2) (cid:2)(cid:2) (cid:7) (cid:1)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2) δ’ (cid:3) (cid:17)(cid:3)(cid:9)	 (cid:17) (cid:1)(cid:2) (cid:2) 	(cid:3) (cid:3) δ (cid:25) (cid:0)(cid:1)(cid:2) (cid:22)(cid:17) 	(cid:2) (cid:7) δ (cid:3) (cid:20)(cid:2)(cid:2)(cid:17)  (cid:27) (cid:17) (cid:3)(cid:20) (cid:2) (cid:20)(cid:17)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27) (cid:7) (cid:1)(cid:2) (cid:20)(cid:3)(cid:22)(cid:2) (cid:2)(cid:9)(cid:1)	  (cid:20)(cid:27)(cid:25) (cid:0)(cid:1)(cid:3) (cid:9)(cid:17) (cid:19)(cid:2) (cid:3) (cid:2) (cid:9)(cid:17)(cid:2)   (cid:17)(cid:20)(cid:2) (cid:2)	(cid:3)(cid:3)(cid:20) (cid:2) (cid:2)4(cid:2)(cid:18) 	(cid:9)	(cid:2)(cid:25) 1 (cid:18)(cid:3)(cid:26)(cid:2)(cid:2)  	(cid:3) (cid:17)  (cid:16) (cid:1)(cid:2) (cid:2)(cid:18)	(cid:9)(cid:3) (cid:7) (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)	  	(cid:3)  (cid:19)(cid:2) (cid:16) (cid:2) (cid:20)(cid:17)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27) 	(cid:3)(cid:20) (cid:17) 5(cid:2)(cid:3)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2) (cid:1)(cid:16) (cid:3) 8(cid:3)(cid:20)	(cid:2) ((cid:25) (cid:0)(cid:1)(cid:3) (cid:3) (cid:17)(cid:9)(cid:1)(cid:3)(cid:2)(cid:22)(cid:2)(cid:18) (cid:19)(cid:27) 	(cid:3)(cid:20) (cid:16) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2) (cid:2) (cid:7) (cid:2)(cid:17)(cid:9)(cid:1) (cid:2)(cid:22)(cid:2)  (cid:3)(cid:2) (cid:17)(cid:18) (cid:17)(cid:3)(cid:20) (cid:1)(cid:2) 		 (cid:7) (cid:2)(cid:17)(cid:9)(cid:1) (cid:17)(cid:20)(cid:2) (cid:7) (cid:1)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2)(cid:25) (cid:7) δ1 > δ2 (cid:1)(cid:2) (cid:2) 	(cid:3) (cid:16)(cid:3)   (cid:19)(cid:2) δ1 − δ2 (cid:17) (cid:18)(cid:2)(cid:9)(cid:3)(cid:19)(cid:2)(cid:18) (cid:3) (cid:30)(cid:31)7 (cid:25) $(cid:3)(cid:3) (cid:17) (cid:19) (cid:2) (cid:9)(cid:9)	 (cid:3) (cid:1)(cid:3) (cid:0)0(cid:24)< (cid:1)(cid:2) (cid:2)	(cid:3)(cid:2)(cid:2) (cid:7) (cid:17)(cid:9)(cid:9)	(cid:17)(cid:2) (cid:3)(cid:3)(cid:20) (cid:2) (cid:2)(cid:2) (cid:3)(cid:1)(cid:2)(cid:2) (cid:27)  (cid:3)(cid:3) (cid:1)(cid:2) 	(cid:2) (cid:7) 	(cid:9)(cid:1) (cid:17) (cid:0)0(cid:24) (cid:7) (cid:1)(cid:3)(cid:20)(cid:1)	(cid:2) 	(cid:3) (cid:17) (cid:3)(cid:9)(cid:17)(cid:3)(cid:25) (cid:23)(cid:11) (cid:15)(cid:13)(cid:3)	(cid:27)(cid:6)(cid:8)(cid:1)(cid:9) (cid:1)(cid:17)(cid:2)	(cid:2)(cid:9)(cid:18)(cid:3)(cid:20) (cid:16)(cid:17) (cid:3)(cid:18)	(cid:9)(cid:2)(cid:18) (cid:3) (cid:30)- (cid:7) (cid:1)(cid:2) (cid:18)	(cid:17) 	(cid:17)(cid:3)  (cid:9)(cid:17)(cid:2) (cid:17)(cid:18) (cid:3) (cid:30). (cid:7) (cid:1)(cid:2) (cid:20)(cid:2)(cid:2)(cid:17)  	 (cid:3) (cid:2)	(cid:17)(cid:3)  (cid:9)(cid:17)(cid:2)(cid:25) (cid:0)(cid:1)(cid:2) (cid:27)	 (cid:2) (cid:9)(cid:3) (cid:3) (cid:2) (cid:27)(cid:3)(cid:20) (cid:1)(cid:2) (cid:18)(cid:2) (cid:7) (cid:2)(cid:22)(cid:2)  (cid:2)(cid:9)(cid:18)(cid:2) (cid:1)(cid:2) (cid:1) (cid:21)(cid:24)	(cid:24)(cid:9)(cid:26) (cid:21)(cid:9)(cid:26)	(cid:15)(cid:8)(cid:6)(cid:9)(cid:13) (cid:0)(cid:1)(cid:2) (cid:3) (cid:2) (cid:9)(cid:1)(cid:2)(cid:2) (cid:2) (cid:27)  (cid:27) (cid:17) (cid:17)(cid:3) (cid:7) (cid:16)(cid:3)(cid:2)(cid:25) (cid:0)(cid:1)(cid:3) (cid:17)  (cid:16) (cid:1)(cid:2) (cid:18)(cid:2)(cid:3)(cid:20)(cid:2)  (cid:2)(cid:18)	(cid:9)(cid:2) (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:16)(cid:3)(cid:2) (cid:2)	(cid:3)(cid:2)(cid:18)       (cid:7) (cid:1)(cid:2) (cid:9)	(cid:3)(cid:9)(cid:17)(cid:3) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) 	(cid:9)(cid:1)(cid:3) (cid:19) (cid:9)+ (cid:17) (cid:1)(cid:2) (cid:2)(cid:29)	 (cid:2)(cid:2) (cid:7) (cid:1)(cid:3)(cid:20)(cid:1) (cid:17)(cid:3)(cid:3) (cid:2)(cid:2)(cid:18)(cid:25)  (cid:18)(cid:2)  (cid:2)(cid:29) (cid:2) (cid:1)(cid:2) (cid:17)(cid:18)(cid:2)	(cid:26) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:16)(cid:3)(cid:2) (cid:17)(cid:18)  (cid:3)+ (cid:7)(cid:2)	(cid:2)(cid:9)(cid:27) (cid:2)	 	(cid:3)(cid:2)(cid:18)  (cid:17)(cid:9)(cid:1)(cid:3)(cid:2)(cid:22)(cid:2) (cid:17) (cid:20)(cid:3)(cid:22)(cid:2) (cid:1)	(cid:20)(cid:1)	 (cid:3) (cid:3) (cid:2)	(cid:3)(cid:2)(cid:18)  (cid:2)(cid:22)(cid:17) 	 	(cid:17)(cid:2) (cid:1)(cid:2) (cid:19)(cid:3)B(cid:27)(cid:19)  (cid:2)(cid:3)(cid:17)  (cid:7) (cid:1)(cid:2) (cid:9)(cid:1)(cid:2)(cid:2) (cid:20)(cid:2)(cid:1)(cid:2) (cid:16)(cid:3)(cid:1) (cid:1)(cid:2) (cid:27)(cid:19)  (cid:3)(cid:2)(cid:25) (cid:0)(cid:1)(cid:2)  (cid:17)(cid:2) (cid:2)(cid:17)	(cid:2) (cid:3)  (cid:2)(cid:2) (cid:3) (cid:17) (cid:27)(cid:3)(cid:9)(cid:17)  (cid:27)(cid:9)(cid:1)	  (cid:3)+ (cid:17) (cid:1)(cid:2) (cid:27)(cid:19)  (cid:3)(cid:2) (cid:3) 4(cid:29)(cid:2)(cid:18) (cid:19)(cid:27) (cid:1)(cid:2) (cid:9) (cid:9)+ (cid:2)(cid:2)(cid:18)(cid:25)  (cid:17)(cid:27)(cid:9)(cid:1)	 (cid:27)(cid:2) (cid:1)(cid:2) (cid:27)(cid:19)  (cid:18)		 (cid:17)(cid:3) (cid:9)(cid:17) (cid:9)(cid:1)(cid:17)(cid:20)(cid:2) (cid:17)(cid:18) (cid:1)(cid:2) (cid:18)	(cid:17)(cid:3) (cid:3) )(cid:2)(cid:20)(cid:3)(cid:17)(cid:2)(cid:18)* (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:1)(cid:2) (cid:2)(cid:18)(cid:2) (cid:17)(cid:18) (cid:1)(cid:2) (cid:2)(cid:9)(cid:2)(cid:3)(cid:22)(cid:2) (cid:17) 		(cid:3)(cid:2)(cid:25) (cid:2)(cid:17)(cid:18) (cid:1)(cid:2) (cid:3)(cid:2)	 (cid:2)(cid:9)(cid:18)(cid:3)(cid:20) (cid:2)(cid:9)(cid:1)(cid:3)	(cid:2) (cid:2)(cid:18) (cid:3)(cid:2) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2) (cid:27)(cid:19)  (cid:18)		 (cid:17)(cid:3)(cid:25)  (cid:1)(cid:2) (cid:3) (cid:2) (cid:9)(cid:17)(cid:2) (cid:2) (cid:7) (cid:1)(cid:2)  (cid:3)(cid:2) (cid:16)(cid:3)   (cid:17)(cid:9) (cid:17) (cid:17) (cid:3)(cid:2) (cid:2)(cid:7)	 (cid:2)(cid:2)(cid:9)(cid:2) (cid:17) (cid:16)(cid:17)(cid:27) (cid:19)(cid:2)(cid:3)(cid:20) (cid:1)(cid:2) )(cid:17)* (cid:3)(cid:20)(cid:17)  I (cid:2)(cid:26)(cid:2)(cid:9)(cid:3)(cid:22)(cid:2) (cid:27) (cid:17) (cid:9) (cid:9)+ (cid:7) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:2)(cid:9)(cid:18)(cid:2) (cid:18)(cid:17)(cid:17) (cid:3) (cid:2)(cid:17)	 	(cid:2)(cid:18)(cid:25) (cid:0)(cid:1)(cid:2) (cid:1)(cid:2) (cid:2)(cid:22)(cid:2) (cid:16)(cid:3)   (cid:9)(cid:9)	 (cid:17) (cid:17) (cid:3)(cid:2) ∆ (cid:7) (cid:1)(cid:2) 4 (cid:16)(cid:1)(cid:2)(cid:2) ∆ = kδ, k ∈ {0, 1, 2, ...}(cid:25) E(cid:2) (cid:3)(cid:18)(cid:3)(cid:9)(cid:17)(cid:2) (cid:16)(cid:3)(cid:1) δ (cid:1)(cid:2) (cid:17)  (cid:2) (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:2)(cid:18)(cid:20)(cid:2) (cid:17) (cid:1)(cid:2) (cid:2)(cid:18)(cid:2)’ (cid:16)(cid:2) (cid:17)	(cid:2) (cid:7) (cid:1)(cid:2) (cid:2) (cid:17) (cid:3)(cid:18)(cid:2)(cid:17)   (cid:3)+  (cid:1)(cid:17) (cid:1)(cid:2) 	 (cid:3)(cid:17)  (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:3) (cid:2)(cid:2)(cid:22)(cid:2)(cid:18) (cid:17) (cid:20) (cid:1)(cid:2)  (cid:3)+(cid:25) 1  (cid:16)(cid:2) (cid:17)	(cid:2) (cid:7) (cid:1)(cid:2) (cid:2) (cid:1)(cid:17) (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)(cid:7)(cid:2)(cid:2)(cid:9)(cid:2) (cid:2)(cid:22)(cid:2) (cid:17)(cid:18) (cid:1)(cid:2) )* (cid:2)(cid:22)(cid:2) (cid:3) (cid:3)(cid:3)(cid:22)(cid:2) (cid:17)(cid:18) (cid:20)(cid:2)(cid:17)(cid:2) (cid:1)(cid:17) J(cid:2)(cid:25) (cid:0)(cid:1)(cid:2) (cid:2)(cid:18)(cid:2) (cid:16)(cid:3)   (cid:2)	(cid:3)(cid:2) (cid:17) 	(cid:19)(cid:2) (cid:7) )(cid:17)*< (cid:1)(cid:2)(cid:2) (cid:3)(cid:18)(cid:3)(cid:9)(cid:17)(cid:2) (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:7) (cid:1)(cid:2) )(cid:17)* (cid:3)(cid:20)(cid:17)  (cid:17) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:1)(cid:2) )* (cid:3)(cid:20)(cid:17)  (cid:9)(cid:17) (cid:19)(cid:2) (cid:2) (cid:3) 	(cid:3) (cid:7) δ (cid:25) E(cid:2) (cid:3)(cid:18)(cid:3)(cid:9)(cid:17)(cid:2) (cid:1)(cid:2)(cid:2) )(cid:17)* (cid:16)(cid:3)(cid:1) σ   (cid:1)(cid:17) (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:3) ∆ = kδ, 0 ≤ k < σ ’ (cid:1)(cid:2) (cid:17)(cid:29)(cid:3)	 (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:3) (cid:1)(cid:2)(cid:2)(cid:7)(cid:2) (σ − 1)δ (cid:25) (cid:16)(cid:3)(cid:2)  (cid:3)+ (cid:3) (cid:3)log2 σ(cid:4)(cid:25) (cid:0)(cid:1)	 (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:19)(cid:3) (cid:1)(cid:17) (cid:9)(cid:17) (cid:19)(cid:2) (cid:2)(cid:9)(cid:18)(cid:2)(cid:18)  (cid:1)(cid:2) (cid:16)	 (cid:2) (cid:1)(cid:17) (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) )(cid:17)* (cid:3)(cid:9) 	(cid:18)(cid:2) (cid:1)(cid:2) (cid:29)(cid:3)	(cid:13) < (cid:1)(cid:3) (cid:3) 	(cid:9)(cid:1) (cid:1)(cid:17) (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:17) (cid:1)(cid:2) (cid:2)(cid:18)(cid:2) (cid:3) 6(cid:25) (cid:0)(cid:1)(cid:2) J(cid:2)	(cid:17) (cid:3) 	(cid:2)(cid:7)	  (cid:3)(cid:7)  (cid:27) (cid:3)(cid:3)(cid:22)(cid:2) (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:17)(cid:2) (cid:3)(cid:19) (cid:2) (cid:3)(cid:25)(cid:2)(cid:25) (cid:2) (cid:16)(cid:3)(cid:2) (cid:3) (cid:17) (cid:16)(cid:17)(cid:27) (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)(cid:7)(cid:2)(cid:2)(cid:9)(cid:2)(cid:25)  (cid:1)(cid:3) (cid:9)(cid:17)(cid:2) 	(cid:3)(cid:20) (cid:1)(cid:2) J(cid:2)	 (cid:17)  (cid:16) (cid:17) (cid:17)(cid:18)(cid:18)(cid:3)(cid:3)(cid:17)  (cid:17)(cid:2)  (cid:19)(cid:2) (cid:2)(cid:9)(cid:18)(cid:2)(cid:18) (cid:2)(cid:18)	(cid:9)(cid:3)(cid:20) (cid:1)(cid:2) (cid:17)(cid:29)(cid:3)	 (cid:27)(cid:19)  (cid:3)(cid:2)(cid:25) (cid:0) (cid:3)(cid:9)(cid:2)(cid:17)(cid:2) (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:19)(cid:3)B(cid:27)(cid:19)  (cid:16) (cid:0)0(cid:24) (cid:9)(cid:17) (cid:19)(cid:2) 	(cid:2)(cid:18) (cid:2) (cid:7) (cid:2)(cid:17)(cid:9)(cid:1) (cid:16)(cid:3)(cid:2) (cid:19)	(cid:3)  (cid:3) 	(cid:9)(cid:1) (cid:17) (cid:16)(cid:17)(cid:27) (cid:1)(cid:17) (cid:1)(cid:2) )(cid:17)* (cid:17)(cid:18) (cid:1)(cid:2) )* (cid:3)(cid:20)(cid:17)  (cid:17)(cid:2) (cid:16)(cid:17)(cid:2)(cid:18) (cid:17) (cid:1)(cid:2) (cid:3)	 (cid:7) (cid:1)(cid:2) (cid:0)0(cid:24)(cid:25)  (cid:1)(cid:3) (cid:9)(cid:17)(cid:2) (cid:1)(cid:2) J(cid:2)	 (cid:19)(cid:2)(cid:9)(cid:2)  (cid:2) 	(cid:2)(cid:7)	  (cid:17) (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:17)(cid:2) (cid:16)(cid:3)   (cid:19)(cid:2)(cid:9)(cid:2) (cid:18)(cid:18)(cid:25) (cid:24)(cid:3)(cid:18)(cid:2) (cid:1)(cid:2) (cid:3) (cid:2) (cid:9)(cid:17)(cid:2) (cid:7) (cid:16)(cid:3)(cid:2) (cid:17) (cid:17)(cid:18) (cid:19) (cid:16)(cid:3)(cid:1) σ = 4 (cid:3)(cid:9) 	(cid:18)(cid:3)(cid:20) (cid:1)(cid:2) J(cid:2)	’ (cid:1)(cid:2) (cid:2)(cid:22)(cid:2)  (cid:16)(cid:3)(cid:2) (cid:17) (cid:9)(cid:17) (cid:19)(cid:2) (cid:17) (cid:3)(cid:2) 6 δ  2δ  3δ (cid:16)(cid:3)(cid:1)(cid:3) (cid:17) (cid:3)(cid:2)	(cid:7)(cid:17)(cid:2)(cid:25) (cid:7) (cid:1)(cid:3) (cid:3) (cid:1)(cid:2) (cid:17)(cid:2) (cid:7) (cid:16)(cid:3)(cid:2) (cid:19) (cid:1)(cid:2) (cid:3)(cid:2) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2)(cid:9)(cid:2) ∆ (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:1)(cid:2) (cid:9)(cid:17) (cid:19)(cid:2) (cid:2) (cid:7) (cid:1)(cid:2) (cid:7)  (cid:16)(cid:3)(cid:20) (cid:2)< {0, δ,2 δ, 3δ, −δ, −2δ, −3δ} (cid:0)(cid:1)(cid:3) (cid:3) 	(cid:2) (cid:19)(cid:2)(cid:9)(cid:17)	(cid:2) (cid:7) (cid:3)(cid:17)(cid:9)(cid:2) (cid:3)(cid:7) (cid:16)(cid:3)(cid:2) (cid:17) (cid:17)(cid:3)(cid:22)(cid:2) (cid:17) (cid:3)(cid:2) δ (cid:17)(cid:18) (cid:16)(cid:3)(cid:2) (cid:19) (cid:17)(cid:3)(cid:22)(cid:2) (cid:17) (cid:3)(cid:2) 2δ  (cid:1)(cid:2) (cid:3)(cid:2) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2)(cid:9)(cid:2) (cid:3) (cid:3)   δ  (cid:17) (cid:3) (cid:16)	 (cid:18) (cid:1)(cid:17)(cid:22)(cid:2) (cid:19)(cid:2)(cid:2) (cid:3)(cid:7) (cid:16)(cid:3)(cid:2) (cid:17) (cid:17)(cid:3)(cid:22)(cid:2)(cid:18) (cid:17) (cid:3)(cid:2) 6 (cid:17)(cid:18) (cid:16)(cid:3)(cid:2) (cid:19) (cid:17) (cid:3)(cid:2) δ (cid:25) (cid:0)(cid:1)(cid:3) (cid:2) (cid:1)(cid:17) ; (cid:2) (cid:2)(cid:2) (cid:17)  (cid:16)(cid:3)(cid:20)  (cid:27) % (cid:19)(cid:3)’ (cid:3) (cid:20)(cid:2)(cid:2)(cid:17)  	(cid:3)(cid:20) (cid:1)(cid:3) (cid:9)(cid:1)(cid:2)(cid:2) (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:19)(cid:3) (cid:2) (cid:27)(cid:19)  (cid:16)(cid:3)   (cid:19)(cid:2) (cid:3)log2 (2σ − 1)(cid:4)(cid:25) 1 (cid:16)(cid:17)(cid:27)  (cid:2)(cid:18)	(cid:9)(cid:2) (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:19)(cid:27) (cid:2)  (cid:9)(cid:17) (cid:19)(cid:2) (cid:18)(cid:2)(cid:22)(cid:3)(cid:2)(cid:18)(cid:25) (cid:24)(cid:3)(cid:18)(cid:2) (cid:1)(cid:2) (cid:9)(cid:17)(cid:2) (cid:16)(cid:1)(cid:2)(cid:2) (cid:1)(cid:2) (cid:3)(cid:2) (cid:7) (cid:17)(cid:3)(cid:22)(cid:17)  (cid:17)(cid:2) (cid:3) (cid:1)(cid:2) (cid:2) T = {0, δ1 , δ2 , δ3 , ..., δσ−1 } (cid:16)(cid:3)(cid:1) δx (cid:5)= δy ∀δx , δy ∈ T , x (cid:5)= y (cid:25)  (cid:1)(cid:3) (cid:9)(cid:17)(cid:2) (cid:1)(cid:2) (cid:3)(cid:2) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2)(cid:9)(cid:2) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:1)(cid:2) (cid:2)(cid:22)(cid:2) (cid:16)	 (cid:18) (cid:19)(cid:2) (cid:3) (cid:1)(cid:2) (cid:2) D = {δx − δy }, δx , δy ∈ T (cid:25) (cid:0)(cid:1)(cid:2) (cid:9)(cid:17)(cid:18)(cid:3)(cid:17) (cid:3)(cid:27) (cid:7) (cid:1)(cid:3) (cid:2) (cid:3) σ2 − σ + 1< (cid:1)(cid:2)(cid:2)(cid:7)(cid:2) (cid:3)(cid:7) σ = 4 (cid:31)( (cid:17)(cid:2) (cid:9)(cid:17) (cid:19)(cid:2) (cid:2)(cid:9)(cid:18)(cid:2)(cid:18) (cid:2)	 (cid:3)(cid:20) (cid:3) ( (cid:19)(cid:3) (cid:2) (cid:27)(cid:19) (cid:25) (cid:17)  (cid:2)(cid:17) min{D}  (cid:1)(cid:17) (cid:1)(cid:2) (cid:0)0(cid:24) (cid:9)(cid:17) (cid:18)(cid:3)(cid:3)(cid:20)	(cid:3)(cid:1) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:0)(cid:1)(cid:3) (cid:9)(cid:1)(cid:2)(cid:2) (cid:2)	(cid:3)(cid:2) (cid:1)(cid:2) (cid:2) 	(cid:3) (cid:7) (cid:1)(cid:2) (cid:0)0(cid:24)  (cid:19)(cid:2) (cid:1)(cid:2) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2) (cid:17)(cid:2)(cid:25) (cid:0)(cid:1)(cid:3) (cid:9)(cid:17) (cid:2)	  (cid:3)  (cid:17)(cid:20)(cid:2) (cid:22)(cid:2)(cid:1)(cid:2)(cid:17)(cid:18) (cid:17)(cid:18) (cid:9)	 (cid:18) (cid:19)(cid:2) 	(cid:7)(cid:2)(cid:17)(cid:3)(cid:19) (cid:2)(cid:25) 1(cid:18)(cid:18)(cid:3)(cid:3)(cid:17)  (cid:27) (cid:1)(cid:2) (cid:2)(cid:18)(cid:2) (cid:19)(cid:2)(cid:9)(cid:2) (cid:2) (cid:9) (cid:2)(cid:29) (cid:19)(cid:2)(cid:3)(cid:20) (cid:2)	(cid:3)(cid:2)(cid:18)  (cid:16)(cid:3)(cid:9)(cid:1) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2) (cid:9)(cid:19)(cid:3)	 (cid:17)(cid:3) (cid:7) (cid:2)(cid:22)(cid:2) (cid:9)(cid:9)	(cid:2)(cid:9)(cid:2)(cid:25) 8(cid:3)(cid:17)  (cid:27) (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:17)(cid:2) (cid:3)  (cid:17) (cid:16)(cid:2) (cid:7) % (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:3)(cid:20)(cid:1) (cid:19)(cid:2) (cid:16)(cid:17)(cid:2)(cid:7)	  (cid:7) (cid:18)(cid:17)(cid:17) (cid:9)	 	(cid:3)(cid:9)(cid:17)(cid:3)(cid:25) 8 (cid:1)(cid:2) (cid:3) (cid:2) (cid:9)(cid:17)(cid:2) (cid:16)(cid:1)(cid:2)(cid:2) σ = 4 (cid:3) (cid:3) (cid:3)(cid:2)(cid:17)(cid:18) (cid:2)(cid:17)(cid:3)(cid:2)  (cid:3) (cid:27) (cid:17)(cid:18)(cid:18) (cid:17)(cid:1)(cid:2) δ  (cid:1)(cid:2) (cid:3)(cid:19) (cid:2) )(cid:17)* (cid:3)(cid:9)(cid:2)(cid:17)(cid:3)(cid:20) σ (cid:19)(cid:27) (cid:31)(cid:25) (cid:0)(cid:1)(cid:3) (cid:16)(cid:17)(cid:27) (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:19)(cid:3)B(cid:27)(cid:19)  (cid:19)(cid:2)(cid:9)(cid:2) (cid:17)  (cid:2) (cid:2)(cid:29)(cid:2)(cid:2)  (cid:1)(cid:2) (cid:0)0(cid:24) (cid:17)(cid:18) (cid:1)(cid:2)  (cid:20)(cid:3)(cid:9)(cid:25) (cid:28)(cid:1) 	 (cid:9) (cid:26)	(cid:24)(cid:9)(cid:26) (cid:21)(cid:9)(cid:26)	(cid:15)(cid:8)(cid:6)(cid:9)(cid:13) (cid:0)(cid:1)(cid:2) 	 (cid:3) (cid:2)	(cid:16)(cid:3)(cid:2) (cid:9)(cid:17)(cid:2) (cid:2)(cid:2) (cid:3)(cid:2)(cid:2)(cid:3)(cid:20) (cid:19) (cid:2) (cid:17) (cid:1)(cid:2) (cid:9)(cid:1)(cid:3)(cid:9)(cid:2) (cid:7) (cid:17) (cid:3)(cid:2) (cid:2)(cid:7)(cid:2)(cid:2)(cid:9)(cid:2) (cid:19)(cid:2)(cid:9)(cid:2) (cid:2) (cid:18)(cid:3)(cid:8)(cid:9)	 (cid:25) (cid:0)(cid:1)(cid:2) (cid:9)(cid:1)(cid:3)(cid:9)(cid:2) (cid:9)(cid:17) (cid:19)(cid:2) 4 (cid:18)(cid:3)(cid:22)(cid:3)(cid:18)(cid:2)(cid:18) (cid:3) (cid:16) (cid:17)(cid:3) (cid:9)(cid:17)(cid:2)(cid:20)(cid:3)(cid:2)< (cid:17) )	(cid:2)* 	 (cid:3) (cid:2)	(cid:16)(cid:3)(cid:2) (cid:3) (cid:2)(cid:2)(cid:17)(cid:3)  (cid:17) (cid:20)	 (cid:7) (cid:16)	 (cid:16)(cid:3)(cid:2) (cid:3)(cid:2)	(cid:2)(cid:9)(cid:18)(cid:2)(cid:18)  (cid:3)+(cid:25) (cid:0)(cid:1)(cid:2) (cid:2)(cid:29) (cid:16) 	(cid:19)(cid:2)(cid:9)(cid:3) (cid:16)(cid:3)   (cid:18)(cid:2)(cid:9)(cid:3)(cid:19)(cid:2) (cid:1)(cid:2)(cid:2) (cid:16) (cid:17)(cid:17)(cid:9)(cid:1)(cid:2) (cid:3) (cid:2) (cid:18)(cid:2)(cid:17)(cid:3) (cid:25) (cid:10)(cid:11) (cid:12)(cid:13)(cid:1)(cid:9)(cid:15) 	 (cid:1) (cid:3)	(cid:13)(cid:1)   (cid:1)(cid:3) (cid:9)(cid:1)(cid:2)(cid:2) (cid:17) (cid:0)0(cid:24) (cid:3) 	(cid:2)(cid:18) (cid:7) (cid:2)(cid:17)(cid:9)(cid:1) (cid:16)(cid:3)(cid:2) (cid:17)(cid:18) (cid:2)(cid:17)(cid:9)(cid:1) (cid:16)(cid:3)(cid:2) (cid:16)(cid:3)   (cid:2)(cid:2)(cid:2) (cid:1)(cid:2) )* (cid:3)(cid:20)(cid:17)  (cid:7) (cid:1)(cid:2) (cid:9)(cid:2)(cid:18)(cid:3)(cid:20) (cid:0)0(cid:24)(cid:25) (cid:0)(cid:1)(cid:2) (cid:17)(cid:3) (cid:19) (cid:2) (cid:1)(cid:2)(cid:2)(cid:7)(cid:2) (cid:19)(cid:2)(cid:9)(cid:2) (cid:1)(cid:2) (cid:9)(cid:1)(cid:3)(cid:9)(cid:2) (cid:7) (cid:17) 	(cid:3)(cid:17)(cid:19) (cid:2) )(cid:17)* (cid:2)(cid:22)(cid:2)(cid:25) 1 (cid:3) (cid:1)(cid:2) (cid:9)(cid:17)(cid:2) (cid:7) (cid:16)	(cid:16)(cid:3)(cid:2) (cid:3) (cid:2)	 (cid:2)(cid:17)(cid:3) (cid:2) (cid:3) (cid:9)(cid:3) (cid:3) 	(cid:3)(cid:20) (cid:2) (cid:7) (cid:1)(cid:2) (cid:16)(cid:3)(cid:2) (cid:17) (cid:17) (cid:3)(cid:2) (cid:2)(cid:7)(cid:2)(cid:2)(cid:9)(cid:2) I (cid:1)(cid:2) )(cid:17)* (cid:7) (cid:17)   (cid:1)(cid:2) (cid:0)0(cid:24)(cid:25) (cid:1)(cid:2)(cid:16)(cid:3)(cid:2) (cid:2) (cid:7) (cid:1)(cid:2) (cid:2)(cid:22)(cid:2) (cid:16)(cid:3)   (cid:19)(cid:2)(cid:9)(cid:2) (cid:1)(cid:2) (cid:2)(cid:7)(cid:2)(cid:2)(cid:9)(cid:2) (cid:7) (cid:1)(cid:2) (cid:1)(cid:2) (cid:2)(cid:22)(cid:2)(cid:25)  (cid:1)(cid:2) (cid:2)(cid:9)(cid:18) (cid:9)(cid:17)(cid:2) (cid:1)(cid:2) (cid:0)0(cid:24) (cid:16)(cid:3)   (cid:1)(cid:17)(cid:22)(cid:2)  (cid:16)(cid:3)(cid:9)(cid:1)  (cid:1)(cid:2) (cid:9)(cid:2)(cid:9) (cid:27) (cid:3)(cid:18)(cid:2)(cid:3)4(cid:2)(cid:18) 4 (cid:2)(cid:22)(cid:2) (cid:17)(cid:18) (cid:9)(cid:3)(cid:18)(cid:2) (cid:3) (cid:1)(cid:2) )(cid:17)* (cid:3)(cid:20)(cid:17) (cid:25) 1 (cid:3) (cid:3)(cid:3)(cid:9) (cid:17)(cid:17)(cid:9)(cid:1) (cid:3)(cid:20)(cid:1) (cid:19)(cid:2)  (cid:1)(cid:17)(cid:22)(cid:2) (cid:17) 	(cid:19)(cid:2) (cid:7) (cid:0)0(cid:24) (cid:2) (cid:7) (cid:2)(cid:17)(cid:9)(cid:1) (cid:9)(cid:19)(cid:3)(cid:17)(cid:3) (cid:7) )(cid:17)* (cid:17)(cid:18) )* (cid:17)(cid:18) (cid:1)(cid:2)  (cid:20)(cid:3)(cid:9) (cid:9)(cid:17) (cid:1)(cid:2) (cid:18)(cid:2)(cid:9)(cid:18)(cid:2) (cid:1)(cid:2) 		(cid:25)  (cid:16)(cid:2)(cid:22)(cid:2) (cid:1)(cid:3)  	(cid:3) (cid:3) (cid:3)(cid:17)(cid:9)(cid:3)(cid:9)(cid:17)  (cid:7)  (cid:17)(cid:20)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:16)(cid:3)(cid:2)(cid:25) (cid:1)(cid:2)	 (cid:16)(cid:3)(cid:2) (cid:17) (cid:19)(cid:17)+ (cid:7) 	 (cid:3) (cid:2)(cid:29)(cid:2) (cid:9)(cid:17) (cid:19)(cid:2) (cid:2) (cid:27)(cid:2)(cid:18)  	(cid:2) (cid:1)(cid:2) (cid:9)(cid:2)(cid:9) )(cid:17)* (cid:3)(cid:20)(cid:17)   (cid:1)(cid:2) (cid:0)0(cid:24)(cid:25) (cid:7) (cid:2) (cid:1)(cid:17) (cid:2) (cid:3)(cid:20) (cid:2) (cid:2)(cid:22)(cid:2) (cid:9)(cid:17) (cid:9)(cid:9)	 (cid:17) (cid:1)(cid:2) (cid:17)(cid:2) (cid:3)(cid:2) (cid:1)(cid:2) (cid:18)(cid:3)(cid:8)(cid:9)	 (cid:27) (cid:9)(cid:3) (cid:3) (cid:2)(cid:9)(cid:20)(cid:3)(cid:3)(cid:20) (cid:1)(cid:2) (cid:2)(cid:2)(cid:9)(cid:2) (cid:7) 	 (cid:3) (cid:2) )(cid:2)(cid:7)(cid:2)(cid:2)(cid:9)(cid:2)* (cid:3)(cid:20)(cid:17)  (cid:17)(cid:18) (cid:17)(cid:9) (cid:17)(cid:9)(cid:9)(cid:18)(cid:3)(cid:20) (cid:27)(cid:25) (cid:0)(cid:1)(cid:2) (cid:3)(cid:7)(cid:17)(cid:3)	(cid:9)(cid:17)(cid:27)(cid:3)(cid:20) (cid:9)(cid:17)(cid:17)(cid:19)(cid:3) (cid:3)(cid:27) (cid:3) (cid:3)log2 (σw )(cid:4) (cid:19)(cid:3)B(cid:27)(cid:19)  (cid:16)(cid:1)(cid:2)(cid:2) w (cid:3) (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:16)(cid:3)(cid:2) (cid:17)(cid:18) σ (cid:3) (cid:17) (cid:18)(cid:2)(cid:9)(cid:3)(cid:19)(cid:2)(cid:18) (cid:17)(cid:19)(cid:22)(cid:2) (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:3)(cid:19) (cid:2) )* (cid:17) (cid:1)(cid:2) (cid:2)(cid:18)(cid:2)(cid:25)  (cid:1)(cid:2) (cid:9)(cid:17)(cid:2) (cid:7) (cid:3)(cid:17)(cid:9)(cid:2) (cid:7) - (cid:16)(cid:3)(cid:2) (cid:17)(cid:18) - )* (cid:1)(cid:2) (cid:27)(cid:2) (cid:16)(cid:3)   (cid:17)  (cid:16) 7 (cid:19)(cid:3)B(cid:27)(cid:19) (cid:25) 1 (cid:3) (cid:3)4(cid:9)(cid:17)(cid:3) (cid:9)(cid:3) (cid:3) (cid:7)(cid:9)(cid:3)(cid:20) (cid:1)(cid:2) 4 (cid:2)(cid:22)(cid:2)  (cid:9)(cid:9)	 (cid:17) (cid:2)  (cid:1)(cid:17) (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)(cid:7)(cid:2)(cid:2)(cid:9)(cid:2) (cid:9)(cid:17) (cid:19)(cid:2) (cid:9)(cid:2)(cid:9) (cid:27) (cid:2) 	’ (cid:1)(cid:3) (cid:3) (cid:3)4(cid:2) (cid:1)(cid:2) (cid:2)(cid:9)(cid:2)(cid:3)(cid:22)(cid:2) (cid:18)(cid:2)(cid:3)(cid:20)(cid:25) (cid:0) (cid:3) (cid:3)(cid:7)(cid:27) (cid:1)(cid:3) (cid:9)(cid:17)(cid:2) (cid:1)(cid:2) (cid:9)(cid:17)(cid:17)(cid:9)(cid:3)(cid:27) (cid:16)(cid:3)   (cid:19)(cid:2) (cid:19)(cid:3)B(cid:27)(cid:19)  (cid:1)(cid:2) (cid:22)(cid:17)(cid:3)(cid:17)(cid:19) (cid:2) (cid:1)(cid:17)(cid:22)(cid:2) (cid:1)(cid:2) (cid:17)(cid:2) (cid:2)(cid:17)(cid:3)(cid:20)(cid:25)  (cid:1)(cid:2) (cid:17)(cid:2) (cid:9)(cid:17)(cid:2) (cid:7) w = σ = 4 (cid:1)(cid:2) (cid:9)(cid:17)(cid:17)(cid:9)(cid:3)(cid:27) (cid:16)(cid:3)   (cid:19)(cid:2) : (cid:19)(cid:3)B(cid:27)(cid:19) (cid:25) 1 (cid:7)	(cid:1)(cid:2) (cid:3) (cid:3)4(cid:9)(cid:17)(cid:3) (cid:9)(cid:3) (cid:3) 	(cid:3)(cid:20) (cid:2) (cid:16)(cid:3)(cid:2)  (cid:19)(cid:2) (cid:1)(cid:2) )(cid:17)*  (cid:17)   (cid:1)(cid:2) (cid:16)(cid:3)(cid:2)(cid:25) 1 (cid:18)(cid:2)(cid:9)(cid:3)(cid:19)(cid:2)(cid:18) (cid:3) (cid:1)(cid:2) (cid:2)(cid:22)(cid:3)	 $(cid:2)(cid:9)(cid:3) (cid:1)(cid:3) (cid:16)(cid:3)(cid:2) (cid:16)(cid:3)   (cid:17)(cid:9) (cid:17) (cid:17) (cid:9) (cid:9)+ (cid:17)(cid:18) (cid:9)	 (cid:18) (cid:3) (cid:7)(cid:17)(cid:9) (cid:19)(cid:2) (cid:1)(cid:2) (cid:9) (cid:9)+ (cid:3)(cid:20)(cid:17)  (cid:3) (cid:17) (cid:27)(cid:9)(cid:1)	 (cid:2)(cid:22)(cid:3)(cid:2)(cid:25)  (cid:1)(cid:3) (cid:9)(cid:17)(cid:2) (cid:1) log2 [(σ − 1)(w−1) ∗ w] (cid:2) (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:19)(cid:3) (cid:3) (cid:3) (cid:27) (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:19)(cid:3) (cid:2)(cid:9)(cid:18)(cid:2)(cid:18) (cid:3) (cid:3)(cid:2) (cid:7) < (cid:3)log2 (σ − 1)(cid:4) ∗ (w − 1)(cid:25) 8 (cid:1)(cid:2) (cid:2)(cid:22)(cid:3)	 (cid:9)(cid:17)(cid:2) (cid:7) w = (cid:2)(cid:17)(cid:9)(cid:1) (cid:16)(cid:3)(cid:2) (cid:3)(cid:2) (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:16)(cid:3)(cid:2) (cid:3)	 (cid:2) (cid:1)(cid:2) (cid:9) (cid:9)+ σ = 4 (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:19)(cid:3) (cid:16)(cid:3)   (cid:19)(cid:2) ( (cid:1)(cid:17) (cid:7) (cid:1)(cid:2) (cid:2)(cid:22)(cid:3)	 (cid:9)(cid:1)(cid:2)(cid:2)(cid:25)  (cid:16)(cid:2)(cid:22)(cid:2) (cid:1)(cid:2) (cid:9)(cid:3)(cid:9)	(cid:3)(cid:27) (cid:3) (cid:1)(cid:3)(cid:20)(cid:1) (cid:27) (cid:3) (cid:3)4(cid:2)(cid:18)’  (cid:2)(cid:29) (cid:3) (cid:1)(cid:3) (cid:16)(cid:2) (cid:19)(cid:2)(cid:22)(cid:2) (cid:1)(cid:17) (cid:3)(cid:7) (cid:16)(cid:2) (cid:3)(cid:9)(cid:2)(cid:17)(cid:2) (cid:1)(cid:2) (cid:22)(cid:17) 	(cid:2) (cid:7) σ (cid:19)(cid:27) (cid:31) (cid:16)(cid:2) (cid:20)(cid:2) % (cid:19)(cid:3) (cid:2) (cid:16)(cid:3)(cid:2) (cid:1)	 (cid:17)(cid:9)(cid:1)(cid:3)(cid:2)(cid:22)(cid:3)(cid:20) : (cid:19)(cid:3)B(cid:27)(cid:19)  (cid:17) (cid:1)(cid:2) (cid:2)(cid:22)(cid:3)	 (cid:9)(cid:17)(cid:2) (cid:20)(cid:17)(cid:3)(cid:3)(cid:20) (cid:3) (cid:9)(cid:3)(cid:9)	(cid:3)(cid:27) (cid:19)	 (cid:17) (cid:1)(cid:2) (cid:2)(cid:29)(cid:2)(cid:2) (cid:7) (cid:17) (cid:3)(cid:20) (cid:2) (cid:17)(cid:18)(cid:18)(cid:3)(cid:3)(cid:17)  δ (cid:25) (cid:19)(cid:11) (cid:13)(cid:1)	(cid:28)(cid:1)(cid:3) 	 (cid:1) (cid:3)	(cid:13)(cid:1)  1 (cid:18)(cid:3)(cid:26)(cid:2)(cid:2) (cid:17)(cid:17)(cid:9)(cid:1) (cid:9)(cid:3) (cid:3) 	(cid:3)(cid:20) (cid:17) 	(cid:19)(cid:2) (cid:7) (cid:16)	 (cid:16)(cid:3)(cid:2)  (cid:3)+  (cid:2)(cid:17)(cid:2) (cid:3) (cid:17)(cid:17)  (cid:2) (cid:25) (cid:0)(cid:1)(cid:3) (cid:20)(cid:2)(cid:17) (cid:27) (cid:3) (cid:3)4(cid:2) (cid:1)(cid:2) (cid:18)(cid:2)(cid:3)(cid:20) (cid:17) (cid:1)(cid:2) (cid:16)	(cid:16)(cid:3)(cid:2)  (cid:3)+ (cid:9)(cid:17) (cid:19)(cid:2) (cid:18)(cid:2)(cid:3)(cid:20)(cid:2)(cid:18) (cid:17) (cid:3)(cid:18)(cid:2)(cid:2)	 (cid:18)(cid:2) (cid:16)(cid:3)(cid:1)  (cid:27) (cid:17) (cid:2)(cid:1)(cid:17)(cid:9)(cid:2)(cid:18) (cid:2)(cid:9)(cid:2)(cid:3)(cid:22)(cid:2)  (cid:20)(cid:3)(cid:9)  (cid:9)(cid:18)(cid:3)(cid:17)(cid:2) (cid:1)(cid:2) (cid:2)(cid:9)	(cid:9)(cid:3) (cid:7) (cid:1)(cid:2) (cid:17)(cid:9)+(cid:2)(cid:25) 1 (cid:17)(cid:18)(cid:18)(cid:3)(cid:3)(cid:17)  (cid:7)(cid:2)(cid:17)	(cid:2) (cid:3) (cid:1)(cid:17) (cid:1)(cid:2)  (cid:3)+ (cid:9)	 (cid:18) (cid:19)(cid:2) (cid:9)	(cid:9)(cid:2)(cid:18)  (cid:2)(cid:9)(cid:22)(cid:2) (cid:7) (cid:1)(cid:17)(cid:18) (cid:7)(cid:17)	 < (cid:2) (cid:7) (cid:1)(cid:2)  (cid:3)+ (cid:9)	 (cid:18) (cid:19)(cid:2) (cid:1)	 (cid:18)(cid:16) (cid:3)(cid:7)  (cid:16)+(cid:3)(cid:20) (cid:9)(cid:2)(cid:9) (cid:27) (cid:2)(cid:18)	(cid:9)(cid:3)(cid:20) (cid:19)(cid:3)	(cid:17)(cid:2) (cid:7) (cid:1)(cid:2) (cid:2)(cid:2)(cid:19) (cid:2) (cid:19)	 (cid:2)(cid:17)(cid:3)(cid:3)(cid:20) (cid:2) (cid:7)	(cid:9)(cid:3)(cid:17) (cid:3)(cid:27)(cid:25) (cid:2) 	(cid:9)(cid:1) (cid:9)(cid:1)(cid:2)(cid:2) (cid:3) (cid:2)(cid:18) (cid:3) (cid:30)(cid:31)= (cid:25) (cid:0)(cid:1)(cid:2)  (cid:3)+ (cid:9)(cid:17) (cid:9)(cid:17)(cid:27) w/2 · (cid:3)log2 σ(cid:4) (cid:19)(cid:3)B(cid:27)(cid:19)  (cid:3)(cid:7) (cid:2) (cid:16)(cid:3)(cid:2) (cid:7) (cid:2)(cid:17)(cid:9)(cid:1) (cid:17)(cid:3) (cid:3) 4(cid:29)(cid:2)(cid:18) (cid:17)(cid:18) w/2 · (cid:3)log2 (2σ − 1)(cid:4) (cid:3)(cid:7) (cid:1)(cid:2)(cid:27) (cid:9)(cid:17) (cid:19)(cid:1) (cid:19)(cid:2) (cid:1)(cid:2) )* (cid:3)(cid:20)(cid:17)   (cid:2)(cid:17)(cid:9)(cid:1) (cid:1)(cid:2) (cid:1)(cid:2) (cid:17)(cid:3) (cid:3) (cid:1)(cid:2) (cid:17)(cid:2) (cid:17) (cid:1)	(cid:20)(cid:1)	 (cid:1)(cid:2) (cid:17)(cid:2)(cid:25) 8 (cid:1)(cid:2) (cid:9)(cid:17)(cid:2) (cid:7) w = σ = 4 (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:19)(cid:3)B(cid:27)(cid:19)  (cid:16)(cid:3)   (cid:19)(cid:2) - (cid:3) (cid:19)(cid:1) (cid:9)(cid:17)(cid:2)’ (cid:1)(cid:16)	 (cid:2)(cid:22)(cid:2) (cid:17) (cid:18)(cid:2)(cid:9)(cid:3)(cid:19)(cid:2)(cid:18) (cid:3) $(cid:2)(cid:9)(cid:3)  (cid:3)(cid:9)(cid:2)(cid:17)(cid:3)(cid:20) σ (cid:19)(cid:27) (cid:31) (cid:17)  (cid:16) : (cid:19)(cid:3)B(cid:27)(cid:19)  (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:3) (cid:3)(cid:18)(cid:2)(cid:3)(cid:9)(cid:17)   (cid:2) (cid:7) (cid:1)(cid:2) (cid:2) (cid:9) (cid:2)(cid:29) (cid:17)(cid:17)(cid:20)(cid:2)(cid:2) (cid:18)(cid:2)(cid:9)(cid:3)(cid:19)(cid:2)(cid:18) (cid:17)(cid:19)(cid:22)(cid:2) (cid:19)	 	(cid:9)(cid:1) (cid:3) (cid:2)  (cid:3) (cid:2)	 (cid:2)< (cid:3) (cid:7)(cid:17)(cid:9)  (cid:9) (cid:2)(cid:29) (cid:9)(cid:3)(cid:9)	(cid:3)(cid:27) (cid:3) (cid:2)	(cid:3)(cid:2)(cid:18)  (cid:17)(cid:9)+ (cid:1)(cid:2) 4	(cid:9)(cid:9)	(cid:3)(cid:20) (cid:2)(cid:22)(cid:2)’ (cid:17)  (cid:1)(cid:2) (cid:2)(cid:18)(cid:2) (cid:9)(cid:17) (cid:19)(cid:2) (cid:2)(cid:17)(cid:3) (cid:27) (cid:18)(cid:2)	 (cid:9)(cid:2)(cid:18) (cid:3) (cid:2)(cid:3)	(cid:3)(cid:18)(cid:2)(cid:2)(cid:18)(cid:2) (cid:19) (cid:9)+ (cid:16)(cid:3)(cid:1) (cid:17) (cid:9) (cid:3)(cid:2) (cid:2)(cid:7)(cid:2)(cid:2)(cid:9)(cid:2) (cid:3)(cid:20)(cid:17) (cid:25) (cid:23)(cid:11) (cid:23)(cid:13)(cid:1) (cid:28)(cid:1)(cid:15) (cid:15)(cid:13)(cid:3)	(cid:3)(cid:6)(cid:8)(cid:1)(cid:9) (cid:0)(cid:1)(cid:2) 	 (cid:3) (cid:2)	(cid:17)(cid:3)  (cid:1)(cid:17)(cid:2)	(cid:2)(cid:9)(cid:18)(cid:3)(cid:20) (cid:9)(cid:1)(cid:2)(cid:2) (cid:30). (cid:17)  (cid:16)  (cid:16) (cid:2)(cid:22)(cid:2)  (cid:9)(cid:9)	 (cid:17) (cid:1)(cid:2) (cid:17)(cid:2) (cid:3)(cid:2) (cid:17) (cid:1)(cid:2) (cid:18)(cid:2) (cid:7) (cid:17)	 (cid:3)(cid:3) (cid:18)(cid:2)(cid:2)(cid:3)(cid:2) (cid:1)(cid:2) (cid:18)(cid:17)(cid:17) (cid:19)(cid:2)(cid:3)(cid:20) (cid:2)’ (cid:16)(cid:2) (cid:9)(cid:17) (cid:1)(cid:2)(cid:2)(cid:7)(cid:2) (cid:9)(cid:17)(cid:2) (cid:2)(cid:17)(cid:3) (cid:27) (cid:1)(cid:2) (cid:16) (cid:9)(cid:1)(cid:2)(cid:2)(cid:25)  (cid:1)(cid:2) 	 (cid:3) (cid:2)	(cid:17)(cid:3)  (cid:1)(cid:17)(cid:2) (cid:2)(cid:9)(cid:18)(cid:3)(cid:20) (cid:1)(cid:2) (cid:22)(cid:17) 	(cid:2) (cid:7) σ (cid:3) (cid:2)	(cid:17)   (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:16)(cid:3)(cid:2)’ (cid:1)(cid:2) (cid:19)(cid:3)B(cid:27)(cid:19)  (cid:17)(cid:3) (cid:3) (cid:3)log2 w!(cid:4)(cid:25)  (cid:1)(cid:2) (cid:9)(cid:17)(cid:2) (cid:7) (cid:1)(cid:2) (cid:9)(cid:1)(cid:2)(cid:2) (cid:2)(cid:18) (cid:3) (cid:1)(cid:3) (cid:17)(cid:2) (cid:16)(cid:2) (cid:9)(cid:17)  (cid:3)(cid:3) 	(cid:2) (cid:7)  (cid:1)(cid:2) (cid:3) (cid:16)(cid:1)(cid:2)(cid:2) (cid:2) (cid:16)(cid:3)(cid:2) (cid:17)(cid:9) (cid:17) (cid:17) (cid:9) (cid:9)+’ (cid:1)(cid:2) (cid:19)(cid:3)B(cid:27)(cid:19)  (cid:17)(cid:3) (cid:3) (cid:2)	(cid:17)   (cid:3)log2 (σ − 1)(cid:4) ∗ (w − 1)(cid:25) (cid:7) (cid:16)(cid:2) +(cid:2)(cid:2) w = σ (cid:16)(cid:2) (cid:3)(cid:9)(cid:2) (cid:1)(cid:17) (cid:1)(cid:2) (cid:2)(cid:9)(cid:18) (cid:22)(cid:17) 	(cid:2) (cid:3) (cid:20)(cid:2)(cid:17)(cid:2) (cid:1)(cid:17) (cid:1)(cid:2) 4’ (cid:7) (cid:2)(cid:17) (cid:3)(cid:3)(cid:9) (cid:22)(cid:17) 	(cid:2) (cid:7) w (cid:1)(cid:2) (cid:3)(cid:2)	(cid:2)(cid:9)(cid:18)(cid:3)(cid:20) (cid:9)(cid:1)(cid:2)(cid:2) (cid:2)(cid:18) (cid:17)  (cid:16) (6 (cid:2) (cid:19)(cid:3)B(cid:27)(cid:19)  (cid:1)(cid:17) (cid:1)(cid:2) (cid:1)(cid:17)(cid:2)	(cid:2)(cid:9)(cid:18)(cid:3)(cid:20) (cid:9)(cid:1)(cid:2)(cid:2)(cid:25)  (cid:16)(cid:2)(cid:22)(cid:2) (cid:3) (cid:3) (cid:3)(cid:17)  (cid:3)(cid:9)(cid:2) (cid:1)(cid:17) (cid:1)(cid:2) (cid:1)(cid:17)(cid:2)	(cid:2)(cid:9)(cid:18)(cid:3)(cid:20) (cid:9)(cid:1)(cid:2)(cid:2) (cid:18)(cid:2)  (cid:2) (cid:27)  (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2)  (cid:2)(cid:9)	(cid:9) (cid:1)(cid:2) (cid:18)(cid:17)(cid:17) (cid:19)	  F	 (cid:0)GH(cid:2)  (cid:2)(cid:9)(cid:22)(cid:2)  (cid:27) (cid:1)(cid:2) (cid:18)(cid:2) (cid:7) (cid:2)(cid:22)(cid:2)(cid:25) (cid:28)(cid:1) (cid:10)(cid:11)(cid:6)(cid:24)(cid:9)(cid:6) (cid:11)(cid:6) (cid:10)(cid:9)	(cid:11)(cid:26) (cid:19)(cid:11) !(cid:9) (cid:0)(cid:1)(cid:2) (cid:19)(cid:17)(cid:18)(cid:16)(cid:3)(cid:18)(cid:1) (cid:7) (cid:17) (cid:9)	(cid:3)(cid:9)(cid:17)(cid:3)  (cid:3)+ (cid:3)  (cid:3)(cid:3)(cid:2)(cid:18) (cid:19)(cid:27) (cid:17) 	(cid:19)(cid:2) (cid:7) (cid:7)(cid:17)(cid:9)(cid:25) E(cid:2) (cid:16)(cid:3)   (cid:17) (cid:19)(cid:27) (cid:17)(cid:17) (cid:27)(cid:3)(cid:20) (cid:1)(cid:2) (cid:3)(cid:3)(cid:3)(cid:9) (cid:2)(cid:3)(cid:2) (cid:7) (cid:1)(cid:2) (cid:9)(cid:1)(cid:2)(cid:2) (cid:2)(cid:18)  (cid:9)(cid:17) (cid:9)	 (cid:17)(cid:2) (cid:1)(cid:2) (cid:3)(cid:18)(cid:2)(cid:17)  (cid:19)(cid:17)(cid:18)(cid:16)(cid:3)(cid:18)(cid:1) (cid:17)(cid:18) (cid:19)(cid:3)	(cid:17)(cid:2) (cid:7) (cid:17) (cid:18)(cid:17)(cid:17) (cid:2)(cid:17) (cid:17)(cid:3)(cid:2)(cid:18)  	(cid:9)(cid:1) (cid:17)  (cid:3)+(cid:25) (cid:0)(cid:1)(cid:2) (cid:9)(cid:1)(cid:2)(cid:2) (cid:2)	(cid:3)(cid:2) (cid:2) (cid:2)(cid:18)(cid:20)(cid:2) (cid:2) (cid:16)(cid:3)(cid:2)  (cid:2)(cid:9)(cid:18)(cid:2) (cid:17) 		 (cid:19)(cid:2) (cid:7) (cid:19)(cid:3) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:18)(cid:2)(cid:2)(cid:18)  (cid:1)(cid:2) (cid:3) (cid:2)(cid:2)(cid:17)(cid:3)< (cid:1)(cid:2) (cid:3)(cid:2) Start t_setup δ t_hold Input to stage 1 of TDC Stop time  (cid:2)(cid:20)(cid:31) +(cid:31) (cid:11)(cid:1)(cid:12)  -(cid:12) (cid:2)(cid:2)	 δ (cid:2)(cid:17)(cid:17)(cid:3) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:1)(cid:2)(cid:2) (cid:2)(cid:18)(cid:20)(cid:2) (cid:16)(cid:3)   (cid:18)(cid:2)(cid:2)(cid:18) (cid:2)(cid:29)(cid:2)(cid:3)(cid:17)  (cid:27)  (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:19)(cid:3) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:9)(cid:17) (cid:19)(cid:2) (cid:17)(cid:3)(cid:2)(cid:18) (cid:16)(cid:3)(cid:1)(cid:3) (cid:17) (cid:27)(cid:19) ’ (cid:3)(cid:7) &(cid:3)	& (cid:17)(cid:2) 	(cid:2)(cid:18) (cid:3) (cid:1)(cid:2) (cid:0)0(cid:24) (cid:1)(cid:2) (cid:16)(cid:3)(cid:2) (cid:9)(cid:17) (cid:17)   (cid:16)(cid:3)(cid:9)(cid:1) (cid:19)(cid:17)(cid:9)+  (cid:20)	(cid:18) (cid:17)(cid:7)(cid:2) (cid:1)(cid:2) (cid:1) (cid:18) (cid:3)(cid:2) (cid:7) (cid:1)(cid:2) &(cid:3)	& (cid:1)(cid:17) (cid:19)(cid:2)(cid:2) (cid:2)(cid:2)(cid:9)(cid:2)(cid:18) (cid:16)(cid:1)(cid:3) (cid:2) (cid:3)(cid:7)  (cid:17)(cid:9)(cid:1)(cid:2)  		(cid:17)  G(cid:29)(cid:9) 	(cid:3) (cid:2) (cid:2)(cid:2) (cid:17)(cid:2) 	(cid:2)(cid:18) F(cid:0)GH(cid:2) (cid:1)(cid:2) (cid:16)(cid:3)(cid:2) (cid:17)(cid:2) (cid:2)	(cid:3)(cid:2)(cid:18)  (cid:17)(cid:3)(cid:17)(cid:3) (cid:1)(cid:2) (cid:17)(cid:2) 	(cid:3)  (cid:1)(cid:2) (cid:9)(cid:19)(cid:3)(cid:17)(cid:3)(cid:17)   (cid:20)(cid:3)(cid:9) (cid:1)(cid:17) (cid:9)	  (cid:2)(cid:2)(cid:18) (cid:1)(cid:2) (cid:18)(cid:2)(cid:9)(cid:18)(cid:3)(cid:20)(cid:25) E(cid:2) (cid:17) (cid:1)(cid:2) (cid:17)(cid:17) (cid:27)(cid:3)  +(cid:3)(cid:20) (cid:17) (cid:17) &(cid:3)	& (cid:19)(cid:17)(cid:2)(cid:18) (cid:0)0(cid:24)(cid:25) (cid:9)(cid:2) (cid:1)(cid:2) )(cid:17)* (cid:3)(cid:20)(cid:17)  (cid:1)(cid:17) (cid:19)(cid:2)(cid:2) (cid:17)(cid:3)(cid:2)(cid:18) (cid:3) (cid:17)(cid:22)(cid:2)  (cid:17) (cid:20) (cid:1)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2) (cid:7) (cid:1)(cid:2) (cid:0)0(cid:24) (cid:17) (cid:1)(cid:2) (cid:2)(cid:9)(cid:2)(cid:3)(cid:22)(cid:2) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:7) (σ − 1)δ (cid:25) (cid:0)(cid:1)(cid:3) (cid:3) (cid:17)  (cid:1)(cid:2) (cid:17)(cid:29)(cid:3)	 (cid:2)(cid:17)(cid:17)(cid:3) (cid:3)(cid:2) (cid:1)(cid:17) (cid:3) (cid:1)(cid:2) (cid:3) (cid:2) (cid:9)(cid:17)(cid:2) (cid:17) (cid:17)(cid:29)(cid:3)	 (cid:17)(cid:20)(cid:17)(cid:3) (cid:18)(cid:2) (cid:17)(cid:27) (cid:2)(cid:22)(cid:2) (cid:9)(cid:9)	 (cid:9)(cid:9)	(cid:2) (cid:27)  (cid:27) (cid:2) (σ − 1)δ (cid:2) (cid:3) (cid:2)	 (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:1)(cid:2) )(cid:17)* (cid:17)(cid:18) (cid:1)(cid:2) )* (cid:3)(cid:20)(cid:17) ’ (cid:17) (cid:1)(cid:2)(cid:2) (cid:16) 	(cid:3)(cid:2)(cid:18)(cid:25) (cid:0)(cid:1)(cid:2) (cid:22)(cid:17) 	(cid:2) (cid:7) δ (cid:9)(cid:17) (cid:19)(cid:2) (cid:9)(cid:17) (cid:9)	 (cid:17)(cid:2)(cid:18) (cid:9)(cid:3)(cid:18)(cid:2)(cid:3)(cid:20) (cid:1)(cid:2) (cid:3)(cid:3)(cid:20) (cid:2)	(cid:3)(cid:2)(cid:2) (cid:7) (cid:1)(cid:2) &(cid:3)	& (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:3)(cid:9) 	(cid:18)(cid:2) (cid:17) (cid:2)	 (cid:3)(cid:2) tsetup (cid:17)(cid:18) (cid:17) (cid:1) (cid:18) (cid:3)(cid:2) thold (cid:25) +(cid:3)(cid:20) (cid:17) 8(cid:3)(cid:20)	(cid:2) : (cid:3) (cid:3) (cid:9) (cid:2)(cid:17) (cid:1)(cid:17) (cid:1)(cid:2) (cid:3)(cid:3)	 (cid:22)(cid:17) 	(cid:2) (cid:7) δ 	 (cid:19)(cid:2) (cid:20)(cid:2)(cid:17)(cid:2) (cid:1)(cid:17) thold + tsetup (cid:25) (cid:0)(cid:1)(cid:3) (cid:2)	(cid:3)(cid:2)(cid:2) (cid:3) (cid:3)(cid:17)  (cid:17)(cid:22)(cid:3)(cid:18) (cid:2)(cid:17)(cid:17)(cid:19)(cid:3) (cid:3)(cid:27) (cid:3) (cid:1)(cid:2) (cid:0)0(cid:24) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:3)(cid:20)(cid:1)  (cid:2)(cid:17)(cid:18)  (cid:3)(cid:17)(cid:9)(cid:9)	(cid:17)(cid:2) (cid:2)	  (cid:17)(cid:18) (cid:2) (cid:3)(cid:17) (cid:27)  	(cid:19)	(cid:18)(cid:2)(cid:18) (cid:2) 	(cid:3) (cid:3)(cid:2) (cid:7) (cid:1)(cid:2) (cid:0)0(cid:24)(cid:25)  (cid:16)(cid:2)(cid:22)(cid:2) (cid:9)(cid:2)B(cid:22) (cid:17)(cid:20)(cid:2)B(cid:2)(cid:2)(cid:17)	(cid:2) 5(cid:0) (cid:22)(cid:17)(cid:3)(cid:17)(cid:3) (cid:16)(cid:3)   (cid:9)(cid:1)(cid:17)(cid:20)(cid:2) (cid:1)(cid:2) (cid:3)(cid:17)  (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:1)(cid:2) )(cid:17)* (cid:17)(cid:18) )* (cid:3)(cid:20)(cid:17)  (cid:16)(cid:3)(cid:1) (cid:1)(cid:2) (cid:3)(cid:19)(cid:3) (cid:2) (cid:3)(cid:18)	(cid:9)(cid:3) (cid:7) (cid:2)(cid:17)(cid:17)(cid:19) (cid:2) (cid:17)(cid:2)(cid:25) (cid:0)(cid:1)(cid:2)(cid:2)(cid:7)(cid:2) (cid:2)  (cid:2)	 (cid:17)(cid:9)(cid:2) 	 (cid:19)(cid:2) (cid:3)(cid:9) 	(cid:18)(cid:2)(cid:18) (cid:3) (cid:1)(cid:2) (cid:9)(cid:17) (cid:9)	 (cid:17)(cid:3) (cid:7) δ  (cid:17)(cid:22)(cid:3)(cid:18) (cid:1)(cid:3) (cid:19) (cid:2)(cid:25) (cid:7) (cid:1)(cid:2) (cid:16)	(cid:9)(cid:17)(cid:2) (cid:22)(cid:17)(cid:3)(cid:17)(cid:3) (cid:3) +(cid:16) (cid:18)(cid:2)(cid:2)(cid:18) (cid:16)(cid:3)(cid:1) vwc  (cid:1)(cid:2) δ (cid:1)	 (cid:18) (cid:2)(cid:29)(cid:9)(cid:2)(cid:2)(cid:18) thold + tsetup + vwc (cid:25)  (cid:1)(cid:2) (cid:7)  (cid:16)(cid:3)(cid:20) (cid:17)(cid:17) (cid:27)(cid:3) (cid:16)(cid:2) (cid:9)(cid:3)(cid:18)(cid:2) (cid:17) (cid:17)(cid:20)(cid:20)(cid:2)(cid:3)(cid:22)(cid:2) (cid:9)(cid:17)(cid:2) (cid:16)(cid:1)(cid:2)(cid:2) (cid:1)(cid:2) (cid:22)(cid:17)(cid:3)(cid:17)(cid:3) (cid:3)  (cid:17)+(cid:2) (cid:3) (cid:17)(cid:9)(cid:9)	’ (cid:1)(cid:2) (cid:9)(cid:17) (cid:9)	 (cid:17)	 (cid:3) (cid:1)(cid:16)(cid:2)(cid:22)(cid:2) (cid:2)(cid:17)(cid:3) (cid:22)(cid:17) (cid:3)(cid:18) (cid:7) (cid:2) (cid:9)(cid:2)(cid:22)(cid:17)(cid:3)(cid:22)(cid:2) (cid:9)(cid:17)(cid:2)(cid:25) 8(cid:3)(cid:20)	(cid:2) ; (cid:16)(cid:3)   (cid:1)(cid:2)   (cid:22)(cid:3)	(cid:17) (cid:3)(cid:2) (cid:1)(cid:2) (cid:2)	(cid:3)(cid:2)(cid:2) (cid:7) (cid:1)(cid:2) (cid:0)0(cid:24)  (cid:9)(cid:17) (cid:9)	 (cid:17)(cid:2) (cid:1)(cid:2) (cid:3)(cid:3)	 (cid:9)(cid:27)(cid:9) (cid:2) (cid:3)(cid:2)(cid:25) (cid:0) (cid:2)(cid:2)(cid:9) (cid:1)(cid:2) (cid:2)	 (cid:3)(cid:2) (cid:1)(cid:2) (cid:3)(cid:20)(cid:17)  (cid:1)(cid:17)  (cid:2)(cid:17)(cid:3) (cid:1)(cid:3)(cid:20)(cid:1) (cid:7) tsetup (cid:17)(cid:7)(cid:2) (σ− 1)δ   (cid:1)(cid:17) (cid:1)(cid:2)  (cid:17) &(cid:3)	& (cid:9)(cid:17) (cid:16)(cid:3)(cid:9)(cid:1) (cid:17)(cid:7)(cid:2) (cid:27)(cid:25) (cid:0)(cid:1)(cid:2) (cid:2)(cid:9)(cid:2)(cid:3)(cid:22)(cid:2) (cid:16)(cid:3)   (cid:19)(cid:2) (cid:2)(cid:17)(cid:18)(cid:27)  (cid:2)(cid:9)(cid:2)(cid:3)(cid:22)(cid:2) (cid:1)(cid:2) (cid:2)(cid:29) (cid:27)(cid:19)   (cid:27) (cid:17)(cid:7)(cid:2) (cid:1)(cid:2) (cid:18)(cid:17)(cid:17) (cid:1)(cid:17) (cid:19)(cid:2)(cid:2) 	(cid:9)(cid:9)(cid:2)(cid:7)	  (cid:27) (cid:18)(cid:2)(cid:9)(cid:18)(cid:2)(cid:18) (cid:17)(cid:18) (cid:2)(cid:18)< (cid:1)(cid:3) (cid:9)(cid:2) (cid:3) (cid:17)(cid:2)(cid:18) (cid:17)(cid:7)(cid:2) (cid:1)(cid:2)  (cid:17) &(cid:3)	& (cid:1)(cid:17) (cid:2) (cid:22)(cid:2)(cid:18)(cid:25)  (cid:17)(cid:3)(cid:9)	 (cid:17) (cid:3) (cid:3)(cid:22) (cid:22)(cid:2) (cid:1)(cid:2) &(cid:3)	& (cid:2) 	(cid:3) (cid:3)(cid:2) (cid:17)(cid:18) (cid:1)(cid:2) (cid:3)(cid:2) (cid:7) (cid:1)(cid:2)  (cid:20)(cid:3)(cid:9)  (cid:18)	(cid:9)(cid:2) (cid:17) 		(cid:25) (cid:0) (cid:2)(cid:2)(cid:18) 	 (cid:1)(cid:2)  (cid:3)+ (cid:16)(cid:2)  (cid:17)(cid:9)(cid:1) (cid:1)(cid:2) (cid:18)(cid:17)(cid:17)  (cid:1)(cid:2) (cid:7)(cid:17)  (cid:3)(cid:20) (cid:2)(cid:18)(cid:20)(cid:2) (cid:7) (cid:1)(cid:2) (cid:3)	 (cid:3)(cid:20)(cid:17)  (cid:1)	 (cid:17)  (cid:16)(cid:3)(cid:20) (cid:7)(cid:17)(cid:2) (cid:2)(cid:2)<  (cid:1)(cid:2) (cid:2)(cid:22)(cid:3)	 (cid:3)(cid:2) (cid:16)(cid:2) (cid:17)(cid:18)(cid:18) (cid:1)(cid:2) (cid:2)	 (cid:3)(cid:2) (cid:7) (cid:1)(cid:2) 		  (cid:17)(cid:9)(cid:1)(cid:2)(cid:25) (cid:0)(cid:1)(cid:2) 	 (cid:7) (cid:1)(cid:2)(cid:2) (cid:3)(cid:2) (cid:16)(cid:2) (cid:3)(cid:18)(cid:3)(cid:9)(cid:17)(cid:2) (cid:16)(cid:3)(cid:1) tlogic (cid:25) 1(cid:7)(cid:2) (cid:1)(cid:2) (cid:3)(cid:2) tlogic (cid:1)(cid:2)  (cid:3)+ (cid:1)(cid:17) t m δ Start Stop t s p a (σ−1)δ min cycle time setup time hold time idle time  (cid:2)(cid:20)(cid:31) (cid:25)(cid:31) (cid:15)( (cid:21)(cid:21)(cid:14) -(cid:20)	(cid:11)  *(cid:21) *	 (cid:21)(cid:11) (cid:1)(cid:11) (cid:2)(cid:2)	 *(cid:14)* (cid:11) (cid:2)(cid:11)(cid:31) (cid:0)(cid:1)(cid:11) -(cid:20)	(cid:11) (cid:11)$(cid:11)  (cid:21) (cid:4)	(cid:4)(cid:2)(cid:11)  (cid:2)(cid:7) (cid:4)(cid:2)(cid:1) σ = 4 (cid:21)(cid:12) (cid:1)(cid:4) (cid:1)(cid:11) (cid:4)	*(cid:21)(cid:11) *(cid:11)(cid:21)(cid:2)(cid:31)  (cid:20) (cid:3)(cid:18) (cid:2) (cid:7) (cid:17)  (cid:2)(cid:17) (cid:17) tidle (cid:3)(cid:2) (cid:19)(cid:2)(cid:7)(cid:2) (cid:17) (cid:2)(cid:16) (cid:27)(cid:19)  (cid:9)(cid:17) (cid:19)(cid:2) (cid:2)(cid:9)(cid:2)(cid:3)(cid:22)(cid:2)(cid:18) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:3)(cid:9) 	(cid:18)(cid:2) (cid:1)(cid:2) (cid:1) (cid:18) (cid:3)(cid:2) (cid:7) (cid:1)(cid:2) 		  (cid:17)(cid:9)(cid:1)(cid:2)(cid:25) (cid:2) (cid:1)(cid:17) (cid:1)(cid:2) (cid:3)(cid:7) (cid:1)(cid:2) (cid:9) (cid:9)+ (cid:7) (cid:1)(cid:2) 		  (cid:17)(cid:9)(cid:1)(cid:2) (cid:3) (cid:20)(cid:2)(cid:2)(cid:17)(cid:2)(cid:18) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2) (cid:27) (cid:1)(cid:2)  (cid:3)+ (cid:9)(cid:17) (cid:20) (cid:3)(cid:18) (cid:2) (cid:30)(cid:3)(cid:14)(cid:3) (cid:1)(cid:2)  (cid:20)(cid:3)(cid:9) (cid:1)(cid:17) (cid:9) (cid:2)(cid:2)(cid:18) (cid:3) (cid:2)(cid:17)(cid:3) (cid:17) (cid:1)(cid:2) &(cid:3)	& (cid:16)(cid:3)   (cid:2)(cid:17)(cid:3) (cid:1)(cid:2)(cid:3) (cid:22)(cid:17) 	(cid:2) 	(cid:3)  (cid:1)(cid:2) (cid:2)(cid:29) (cid:9) (cid:9)+ (cid:9)(cid:27)(cid:9) (cid:2)(cid:25) (cid:0)(cid:1)(cid:2)(cid:2)(cid:7)(cid:2) (cid:22)(cid:3)(cid:18)(cid:2)(cid:18) (cid:1)(cid:2) (cid:1)(cid:2)  (cid:3)+ (cid:2)(cid:17)(cid:3) (cid:3)(cid:18) (cid:2) 	(cid:3)  (cid:1)(cid:2)  (cid:20)(cid:3)(cid:9) (cid:1)(cid:17)  (cid:17)(cid:9)(cid:1)(cid:2)(cid:18) (cid:1)(cid:2) 		 (cid:18)(cid:17)(cid:17) (cid:1)(cid:2)  (cid:3)+ (cid:9)(cid:17) (cid:2)(cid:18) (cid:17)(cid:1)(cid:2) (cid:3)(cid:2) (cid:7) (cid:18)(cid:17)(cid:17)(cid:25) (cid:0)(cid:1)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2) (cid:16)(cid:3)   (cid:17)(cid:9) (cid:17) (cid:17) (cid:3)(cid:2) (cid:3)(cid:2) (cid:7) (cid:1)(cid:2) (cid:3)(cid:20)(cid:17) (cid:25) (cid:0)(cid:1)(cid:2) (cid:3)(cid:3)	 (cid:7)	   (cid:2)(cid:3)(cid:18) (cid:16)(cid:3)   (cid:19)(cid:2) T = (σ − 1)δ + tsetup + tlogic + tidle (cid:17)(cid:18) (cid:1)(cid:2) (cid:17)(cid:29)(cid:3)	  (cid:3)+ (cid:7)(cid:2)	(cid:2)(cid:9)(cid:27) (cid:7) 	(cid:3)(cid:7) (cid:27)	 (cid:3)(cid:2)(cid:18) (cid:27)(cid:19)  (cid:3) f = 1/(σ−1)δ+tsetup+tlogic+tidle  (cid:16)(cid:1)(cid:2)(cid:2) (cid:16)(cid:2) (cid:2)(cid:9)(cid:17)   (cid:7) $(cid:2)(cid:9)(cid:3)  σ (cid:3) (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) )(cid:17)* 	(cid:2)(cid:18) (cid:17)(cid:18) δ (cid:3) (cid:1)(cid:2) 	(cid:3) (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:2)(cid:22)(cid:2)  (cid:1)(cid:17) (σ − 1)δ (cid:2)(cid:2)(cid:2) (cid:1)(cid:2) (cid:17)(cid:29)(cid:3)	 (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:1)(cid:2) (cid:17)(cid:29)(cid:3)	 (cid:19)(cid:3)	(cid:17)(cid:2) (cid:3) BR = (cid:3)log2 σ(cid:4) /(σ − 1)δ + tsetup + (cid:2)(cid:22)(cid:2)(cid:25) @(cid:2)(cid:9)(cid:17)	(cid:2) (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:19)(cid:3) (cid:2) (cid:27)(cid:19)  (cid:3) log2 σ  tlogic + tidle (cid:25) (cid:0)(cid:1)(cid:3) (cid:19)(cid:17)(cid:3)(cid:9) (cid:7)	 (cid:17) (cid:9)(cid:17) (cid:1)(cid:2) 	(cid:2)(cid:18)  (cid:9)(cid:17) (cid:9)	 (cid:17)(cid:2) (cid:1)(cid:2) (cid:19)(cid:3)	(cid:17)(cid:2) (cid:3) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2) (cid:22)(cid:17)(cid:3)(cid:17) (cid:7) (cid:1)(cid:2) (cid:17)(cid:2) (cid:9)(cid:1)(cid:2)(cid:2)(cid:25) (cid:7) (cid:16) (cid:0)0(cid:24) (cid:17)(cid:2) 	(cid:2)(cid:18) 	(cid:3)(cid:20) (cid:3)(cid:3)(cid:22)(cid:2) (cid:17)(cid:18) (cid:2)(cid:20)(cid:17)(cid:3)(cid:22)(cid:2) (cid:3)(cid:2) (cid:2)	 (cid:17)(cid:17)(cid:3)  (cid:2)(cid:9)(cid:18)(cid:2) (cid:18)(cid:17)(cid:17) (cid:1)(cid:2) (cid:1)(cid:2) (cid:7)	 (cid:17) (cid:3) (cid:27) (cid:19)(cid:2)(cid:9)(cid:2) BR = (cid:3)log2 (2σ − 1)(cid:4) /(σ − 1)δ + tsetup + tlogic + tidle (cid:25) 	 (cid:2)(cid:17)(cid:18) (cid:3)(cid:7) (cid:19)(cid:1) (cid:3)(cid:3)(cid:22)(cid:2) (cid:17)(cid:18) (cid:2)(cid:20)(cid:17)(cid:3)(cid:22)(cid:2) (cid:17)(cid:3)(cid:3) (cid:17)(cid:2) 	(cid:2)(cid:18) (cid:1)(cid:2) (cid:7)(cid:2)	(cid:2)(cid:9)(cid:27) (cid:16)(cid:3)   (cid:19)(cid:2) (cid:1)(cid:17) (cid:22)(cid:2)(cid:18) (cid:17)(cid:3)(cid:17)(cid:3)(cid:3)(cid:20) (cid:1)(cid:2) (cid:17)(cid:2) (cid:19)(cid:3)	(cid:17)(cid:2)(cid:25) 8(cid:3)(cid:20)	(cid:2) 7 (cid:1)(cid:16) (cid:17) (cid:2)(cid:29)(cid:17) (cid:2) (cid:7) (cid:1)(cid:2) (cid:7)(cid:2)	(cid:2)(cid:9)(cid:27)B(cid:19)(cid:3)	(cid:17)(cid:2) (cid:22)(cid:17) 	 	(cid:2) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:9)(cid:17) (cid:19)(cid:2) (cid:19)(cid:17)(cid:3)(cid:2)(cid:18) 	(cid:3)(cid:20) 6(cid:25)(cid:31)7µm (cid:2)(cid:9)(cid:1) (cid:20)(cid:27)(cid:25) (cid:0)(cid:1)(cid:2) (cid:2)(cid:9)(cid:2)(cid:3)(cid:22)(cid:2) (cid:3) (cid:3) (cid:2)(cid:2)(cid:2)(cid:18) 	(cid:3)(cid:20) (cid:17) (cid:0)0(cid:24) (cid:7) (cid:2)(cid:17)(cid:9)(cid:1) (cid:16)(cid:3)(cid:2)  (cid:1)(cid:17) (cid:1)(cid:2) (cid:19)(cid:3)(cid:17)(cid:2) (cid:3) BR = (cid:3)log2 (2σ − 1)(cid:4) /(σ − 1)δ + tsetup + tlogic + tidle (cid:2) (cid:1)(cid:17) (cid:1)(cid:2) (cid:19)(cid:3)(cid:17)(cid:2) (cid:3)(cid:9)(cid:2)(cid:17)(cid:2) (cid:3) )?	* (cid:2)(cid:22)	 (cid:2)(cid:27) (cid:3)(cid:2) (cid:1)(cid:2) (cid:22)(cid:17) 	(cid:2) (cid:7) σ (cid:18)	(cid:19) (cid:2)(cid:25) (cid:0)(cid:1)(cid:3) (cid:3) (cid:3)	(cid:3)(cid:3)(cid:22)(cid:2) (cid:27) (cid:18)	(cid:2)  (cid:1)(cid:2) (cid:7)(cid:17)(cid:9) (cid:1)(cid:17) (cid:1)(cid:2) (cid:19)(cid:3)(cid:17)(cid:2) (cid:3)(cid:9)(cid:2)(cid:17)(cid:2)  (cid:20)(cid:17)(cid:3)(cid:1)(cid:3)(cid:9)(cid:17)  (cid:27) (cid:3) (cid:19)(cid:17)(cid:2) %(cid:25) (cid:2) (cid:17)  (cid:1)(cid:17) (cid:1)(cid:2) (cid:17)(cid:29)(cid:3)	 (cid:22)(cid:17) 	(cid:2) (cid:7) (cid:19)(cid:3)(cid:17)(cid:2) (cid:3) (cid:17) σ = 5 σ − 1 = 4)< (cid:17) (cid:1)(cid:3) (cid:3) ( (cid:19)(cid:3)B(cid:27)(cid:19)  (cid:17)(cid:2) (cid:17)  (cid:16)(cid:2)(cid:18)(cid:25) 1 σ (cid:3)(cid:9)(cid:2)(cid:17)(cid:2) (cid:3) (cid:16)(cid:3)   (cid:1)(cid:17)(cid:22)(cid:2)  (cid:2)(cid:17)(cid:9)(cid:1) (cid:1)(cid:2) (cid:22)(cid:17) 	(cid:2) (cid:7) =  (cid:3)(cid:9)(cid:2)(cid:17)(cid:2) (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:19)(cid:3)B(cid:27)(cid:19)  (cid:19)(cid:27) (cid:2)(cid:25) 1 (cid:1)(cid:17) (cid:3) (cid:1)(cid:2) (cid:7)(cid:2)	(cid:2)(cid:9)(cid:27) (cid:16)(cid:3)   (cid:1)(cid:17)(cid:22)(cid:2) (cid:18)(cid:2)(cid:18) (cid:3)(cid:20)(cid:3)4(cid:9)(cid:17) (cid:27) (cid:2)(cid:18)	(cid:9)(cid:3)(cid:20) (cid:1)(cid:2) (cid:2)(cid:26)(cid:2)(cid:9)(cid:3)(cid:22)(cid:2)(cid:2) (cid:7) (cid:3)(cid:9)(cid:2)(cid:17)(cid:3)(cid:20) σ (cid:25) 8 	 (cid:3) (cid:2) (cid:16)(cid:3)(cid:2) (cid:3) (cid:2)(cid:2)(cid:17)(cid:3) (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:16)(cid:3)(cid:2) (cid:3)  (cid:19)(cid:2) (cid:17)+(cid:2) (cid:3) (cid:17)(cid:9)(cid:9)	(cid:25) (cid:7) (cid:1)(cid:2) (cid:17)(cid:3)	(cid:16)(cid:3)(cid:2) (cid:9)(cid:1)(cid:2)(cid:2) (cid:3) (cid:9)(cid:3)(cid:18)(cid:2)(cid:2)(cid:18) (cid:7) (cid:3)(cid:17)(cid:9)(cid:2) (cid:1)(cid:2) (cid:17)(cid:29)(cid:3)	 (cid:7)(cid:2)	(cid:2)(cid:9)(cid:27) (cid:16)(cid:3)   (cid:2)	 (cid:17)(cid:3) (cid:1)(cid:2) (cid:17)(cid:2) (cid:16)(cid:1)(cid:3) (cid:2) (cid:1)(cid:2) (cid:19)(cid:3)	(cid:17)(cid:2) (cid:16)(cid:3)   (cid:19)(cid:2) BR = (w/2) · Frequency and Bitrate for 2−wire implementation σ  4  6  8  10  2  0  0.2  0.4  0.6  0.8  1  1.2  0  0.25  0.5  0.75  1  1.25  1.5  1.75  2  2.25  2.5 F r y c n e u q e ( H G z ) B r t i a t e ( G b / t i c e s ) Frequency Bitrate  (cid:2)(cid:20)(cid:31) (cid:28)(cid:31)  (cid:11)	(cid:11)*(cid:14) (cid:21)(cid:12) /(cid:2)	(cid:21)(cid:11) 0(cid:21) 	(cid:11) $ (cid:26)	(cid:4)(cid:2)(cid:11) (cid:2) (cid:11)(cid:11)(cid:21)(cid:2) 0(cid:21)(cid:14)(cid:2)(cid:20) σ δ = 180ps tlogic = 508ps tsetup = 90ps tidle = 200ps Frequency and Bitrate for 5−wire implementation σ  2  4  6  8  10  0  0  0  0.2  0.2  0.2  0.4  0.4  0.4  0.6  0.6  0.6  0.8  0.8  0.8  1  1  1  1.2  1.2  1.2  0  0  0 F F F r r r y y y c c c n n n e e e u u u q q q e e e ( ( ( H H H G G G z z z ) ) ) B B B r r r t t t i i i a a a t t t e e e ( ( ( G G G b b b / / / t t t i i i c c c e e e s s s ) ) )  0.75  0.75  1.5  1.5  2.25  2.25  3  3  3.75  3.75  4.5  4.5  5.25  5.25  6  6 Frequency Bitrate  (cid:2)(cid:20)(cid:31) (cid:27)(cid:31)  (cid:11)	(cid:11)*(cid:14) (cid:21)(cid:12) /(cid:2)	(cid:21)(cid:11) 0(cid:21) 	(cid:11) $ (cid:30)	(cid:4)(cid:2)(cid:11) (cid:2) (cid:11)(cid:11)(cid:21)(cid:2) 0(cid:21)(cid:14)(cid:2)(cid:20) σ δ = 180ps tlogic = 508ps tsetup = 90ps tidle = 200ps (cid:3)log2 (σ − 1) + 1(cid:4) /(σ − 1)δ + tsetup + tlogic + tidle  (cid:16)(cid:1)(cid:2)(cid:2) w (cid:3) (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:16)(cid:3)(cid:2) (cid:3) (cid:1)(cid:2) (cid:3) (cid:2)(cid:2)(cid:17)(cid:3)(cid:25) (cid:0)(cid:1)(cid:2) (cid:3) (cid:2) (cid:3) (cid:2)(cid:2)(cid:17)(cid:3) (cid:7) 	 (cid:3) (cid:2) (cid:16)(cid:3)(cid:2) (cid:1)(cid:16)	 (cid:2)(cid:22)(cid:2) (cid:9)(cid:3) (cid:3) 	(cid:3)(cid:20) (cid:17) (cid:3)(cid:20) (cid:2) (cid:16)(cid:3)(cid:2) (cid:17) (cid:1)(cid:2) )(cid:9) (cid:9)+* (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)(cid:7)(cid:2)(cid:2)(cid:9)(cid:2) (cid:7) (cid:17)   (cid:0)0(cid:24)(cid:25)  (cid:1)(cid:3) (cid:9)(cid:17)(cid:2) (cid:1)(cid:2) (cid:19)(cid:3)	(cid:17)(cid:2) (cid:16)(cid:3)   (cid:19)(cid:2) BR = (w − 1) · (cid:3)log2 (σ − 1)(cid:4) /(σ − 1)δ + tsetup + tlogic + tidle (cid:25) 8(cid:3)(cid:20)	(cid:2) = (cid:3)  	(cid:17)(cid:2) (cid:1)(cid:2) (cid:7)(cid:2)	(cid:2)(cid:9)(cid:27) (cid:17)(cid:18) (cid:19)(cid:3)	(cid:17)(cid:2) (cid:17)(cid:9)(cid:1)(cid:3)(cid:2)(cid:22)(cid:17)(cid:19) (cid:2) 	(cid:3)(cid:20) .	(cid:16)(cid:3)(cid:2) (cid:22)(cid:17)(cid:27)(cid:3)(cid:20) (cid:1)(cid:2) (cid:22)(cid:17) (cid:3)(cid:18) (cid:7) σ (cid:17)(cid:20)(cid:17)(cid:3) (cid:3) 6(cid:25)(cid:31)7µm (cid:2)(cid:9)(cid:1)	  (cid:20)(cid:27)(cid:25) (cid:0)(cid:1)(cid:2) (cid:3)	 (cid:22)(cid:17) 	(cid:2) (cid:7) σ (cid:7) (cid:1)(cid:3)(cid:20)(cid:1)(cid:2) (cid:19)(cid:3)	(cid:17)(cid:2) (cid:3) . σ − 1 = 4 (cid:17) (cid:3) (cid:1)(cid:2) (cid:9)(cid:17)(cid:2) (cid:7) (cid:1)(cid:2) (cid:16)	(cid:16)(cid:3)(cid:2)  (cid:3)+’ (cid:17)(cid:27) (cid:3)	 (cid:9)(cid:2)(cid:17)(cid:2) (cid:3) (cid:19)(cid:3)B(cid:27)(cid:19)  (cid:2)	(cid:3)(cid:2) (cid:17) (cid:18)	(cid:19) (cid:3)(cid:20) (cid:7) (cid:1)(cid:3) (cid:22)(cid:17) 	(cid:2)(cid:25)  (cid:3) (cid:3)(cid:17)  (cid:2) (cid:1)(cid:16)(cid:2)(cid:22)(cid:2) (cid:1)(cid:17) (cid:3)(cid:9)(cid:2)(cid:17)(cid:3)(cid:20) (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:16)(cid:3)(cid:2) (cid:16)(cid:3)   (cid:2)	(cid:3)(cid:2) (cid:3)(cid:20)(cid:1)(cid:2) (cid:18)(cid:2)(cid:3)(cid:20) (cid:7) (cid:1)(cid:2) )(cid:9) (cid:9)+* (cid:16)(cid:3)(cid:2)  (cid:2)	(cid:2) (cid:19)(cid:17) (cid:17)(cid:9)(cid:2)(cid:18)  (cid:17)(cid:18)(cid:25) (cid:28)(cid:1) #(cid:11) (cid:9)$(cid:11)(cid:9) (cid:24)(cid:17) (cid:3)(cid:19)(cid:17)(cid:3) (cid:3) (cid:17) (cid:3)(cid:17) (cid:2) (cid:7) (cid:0)0(cid:24) (cid:18)(cid:2)(cid:3)(cid:20)(cid:25) (cid:3)	 (cid:17)(cid:9)(cid:1)(cid:2) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)(cid:7)(cid:2)(cid:2)(cid:9)(cid:2) (cid:3)(cid:18)	(cid:9)(cid:2)(cid:18) (cid:19)(cid:27)  (cid:17)(cid:9)(cid:2)                 (cid:17)(cid:18) 	(cid:2)    5(cid:0) (cid:22)(cid:17)(cid:3)(cid:17)(cid:3) (cid:9)(cid:17) (cid:2)	  (cid:3) 	(cid:16)(cid:17)(cid:2)(cid:18) (cid:3)(cid:20)(cid:17)  (cid:18)(cid:2)(cid:20)(cid:17)(cid:18)(cid:17)(cid:3)  (cid:3)(cid:3)(cid:3)(cid:20) (cid:1)(cid:2) (cid:2)(cid:26)(cid:2)(cid:9)(cid:3)(cid:22)(cid:2)(cid:2) (cid:7) (cid:1)(cid:2) (cid:2)(cid:9)(cid:1)	 (cid:3)	(cid:2)(cid:25) (cid:0)(cid:16) (cid:27)(cid:2) (cid:7) (cid:9)(cid:17) (cid:3)(cid:19)(cid:17)(cid:3) (cid:9)(cid:17) (cid:19)(cid:2) (cid:3)(cid:18)(cid:2)(cid:3)4(cid:2)(cid:18)< (cid:17) )(cid:17) (cid:3)(cid:20)(cid:2)* (cid:7) (cid:1)(cid:2) (cid:16)(cid:3)(cid:2)  (cid:1)(cid:17) (cid:17) (cid:3)(cid:2) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2)(cid:9)(cid:2) (cid:7) J(cid:2) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:16)  (cid:3)(cid:2) (cid:17)(cid:2)(cid:17) (cid:17) J(cid:2) (cid:17) (cid:1)(cid:2) (cid:2)(cid:9)(cid:2)(cid:3)(cid:22)(cid:2)  (cid:20)(cid:3)(cid:9) (cid:1)(cid:3) (cid:3) (cid:2)	(cid:3)(cid:2)(cid:18) (cid:19)(cid:2)(cid:9)(cid:17)	(cid:2) (cid:27)(cid:2)(cid:17)(cid:3)(cid:9) (cid:26)(cid:2) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:1)(cid:2)  (cid:3)(cid:2) (cid:9)(cid:17) (cid:19)(cid:2) (cid:2)(cid:2) (cid:17)(cid:18) (cid:19) 	(cid:3)(cid:20) (cid:7) (cid:1)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27) (cid:2) (cid:2)(cid:2)  (cid:2)	(cid:2) (cid:17)(cid:9)(cid:1)(cid:3)(cid:20) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:2)(cid:18)(cid:2) (cid:17)(cid:18) (cid:2)(cid:9)(cid:2)(cid:3)(cid:22)(cid:2) (cid:3)(cid:2) (cid:2)(cid:7)(cid:2)(cid:2)(cid:9)(cid:2)(cid:25) (cid:0)(cid:1)(cid:2) (cid:9)(cid:17) (cid:3)(cid:19)(cid:17)(cid:3) (cid:7) (cid:1)(cid:2) (cid:0)0(cid:24) 	(cid:2)(cid:18) (cid:3) (cid:1)(cid:2) (cid:9)(cid:1)(cid:2)(cid:2) (cid:2)(cid:18) (cid:3) (cid:1)(cid:3) (cid:17)(cid:2) (cid:1)	 (cid:18) (cid:19)(cid:2) +(cid:2)  (cid:17) (cid:3)(cid:3)	  (cid:17)(cid:22)(cid:3)(cid:18) (cid:9) (cid:27) (cid:17)		 (cid:3)(cid:2) (cid:17)(cid:18) (cid:18)(cid:2)(cid:18)(cid:3)(cid:9)(cid:17)(cid:2)(cid:18) (cid:9)(cid:17) (cid:3)(cid:19)(cid:17)(cid:3) (cid:3)(cid:2)(cid:25) (cid:0)(cid:1)(cid:2) (cid:3)(cid:18)(cid:2)(cid:17)  (cid:9)(cid:18)(cid:3)(cid:3) (cid:3) (cid:7) (cid:9)	(cid:2) (cid:16)(cid:1)(cid:2)(cid:2)  (cid:9)(cid:17) (cid:3)(cid:19)(cid:17)(cid:3) (cid:3) (cid:2)	(cid:3)(cid:2)(cid:18) (cid:17) (cid:17)  (cid:25) (cid:0)(cid:1)(cid:3) (cid:9)(cid:17) (cid:19)(cid:2) (cid:17)(cid:9)(cid:1)(cid:3)(cid:2)(cid:22)(cid:2)(cid:18) (cid:3)(cid:7) (cid:1)(cid:3)(cid:20)(cid:1) (cid:3)(cid:2) (cid:17)(cid:20)(cid:17)(cid:3) (cid:17)(cid:9)(cid:1)(cid:3)(cid:20) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2)  (cid:16) (cid:17) (cid:7) (cid:1)(cid:2) (cid:9)(cid:1)(cid:3) (cid:3) (cid:3)(cid:19) (cid:2)  (cid:17)(cid:22)(cid:3)(cid:18) (cid:1)(cid:2) (cid:2)(cid:2)(cid:18)  (cid:9)(cid:17) (cid:3)(cid:19)(cid:17)(cid:2) (cid:1)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2) (cid:17)(cid:18) (cid:17)  (cid:3)(cid:7) (cid:1)(cid:2) (cid:3)(cid:2) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2)(cid:9)(cid:2) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:1)(cid:2)  (cid:3)+ (cid:16)(cid:3)(cid:2) (cid:9)(cid:17) (cid:19)(cid:2) (cid:17)(cid:9)(cid:1)(cid:2)(cid:18)  (cid:17)(cid:22)(cid:3)(cid:18) (cid:1)(cid:2) (cid:2)(cid:2)(cid:18)  )(cid:17) (cid:3)(cid:20)* (cid:1)(cid:2) (cid:16)(cid:3)(cid:2)(cid:25) (cid:7) (cid:9)(cid:17) (cid:3)(cid:19)(cid:17)(cid:3) (cid:3) (cid:2)	(cid:3)(cid:2)(cid:18) (cid:22)(cid:17)(cid:3)	 (cid:2)(cid:9)(cid:1)(cid:3)	(cid:2) (cid:17)(cid:2) (cid:17)(cid:22)(cid:17)(cid:3) 	 (cid:17)(cid:19) (cid:2) (cid:30)%6 (cid:25) (cid:0) (cid:9)(cid:2)(cid:9) (cid:1)(cid:2) (cid:26)(cid:2) (cid:2) (cid:17) 0(cid:2) (cid:17)(cid:27)	(cid:9)+(cid:2)(cid:18)  0 (cid:30)%(cid:31) (cid:9)(cid:17) (cid:19)(cid:2) (cid:2) (cid:27)(cid:2)(cid:18) (cid:30)%% < (cid:1)(cid:3) (cid:18)(cid:2)(cid:22)(cid:3)(cid:9)(cid:2) (cid:3)(cid:3) (cid:17)  (cid:17) (cid:1)(cid:17)(cid:2)	(cid:9)+(cid:2)(cid:18)   (cid:2)	(cid:2) (cid:1)(cid:17) (cid:16) (cid:3)	 (cid:1)(cid:17)(cid:22)(cid:2) (cid:17)(cid:9)(cid:1)(cid:2)(cid:18) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:1)(cid:17) (cid:1)(cid:2)(cid:27) (cid:17)(cid:2)(cid:17) (cid:17) (cid:1)(cid:2) (cid:3)	 (cid:7) (cid:1)(cid:2) (cid:0)0(cid:24) (cid:16)(cid:3)(cid:1)	 (cid:26)(cid:2)(cid:25) 1 (cid:27)(cid:2)  (cid:2)(cid:22)(cid:2)  (cid:1)(cid:2) (cid:19) (cid:9)+ (cid:9)	 (cid:18) (cid:1)(cid:17)(cid:22)(cid:2) (cid:17) )(cid:9)(cid:17) 	 (cid:3)(cid:19)(cid:17)(cid:2)* (cid:17)(cid:2) (cid:17) (cid:2)(cid:18) (cid:19)(cid:27) 2(cid:3)(cid:17) (cid:17)(cid:18)   (cid:3) (cid:30)%(  (cid:16)(cid:1)(cid:2)(cid:2) (cid:1)(cid:2) (cid:2)(cid:18)(cid:2) (cid:9)	 (cid:18) (cid:17)(cid:3) (cid:17) +(cid:16) (cid:2)	(cid:2)(cid:9)(cid:2) (cid:7) (cid:18)(cid:17)(cid:17) (cid:3)(cid:2)’ (cid:1)(cid:2) (cid:2)(cid:9)(cid:2)(cid:3)(cid:22)(cid:2) 0 (cid:16)(cid:3)   (cid:1)(cid:2) (cid:2)(cid:7) (cid:1)(cid:2) (cid:26)(cid:2) (cid:9)(cid:2)(cid:9)(cid:3)(cid:25) (cid:0)(cid:1)(cid:2) 	(cid:3)(cid:20) (cid:7) (cid:1)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27) (cid:2) (cid:2)(cid:2) (cid:9)	 (cid:18) (cid:17)  (cid:19)(cid:2) (cid:2)(cid:7)(cid:2)(cid:18) 	(cid:3)(cid:20) (cid:3)(cid:3) (cid:17) (cid:2)(cid:9)(cid:1)(cid:3)	(cid:2)’ (cid:1)(cid:16)(cid:2)(cid:22)(cid:2) (cid:1)(cid:3) (cid:16)	 (cid:18) (cid:2)	(cid:3)(cid:2) (cid:2) (cid:9) (cid:2)(cid:29) (cid:9)(cid:3)(cid:9)	(cid:3)(cid:27)  (cid:3)(cid:18)(cid:2)(cid:2)(cid:18)(cid:2) (cid:27) (cid:17)(cid:18)?	 (cid:1)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27) (cid:7) (cid:2)(cid:17)(cid:9)(cid:1) (cid:17)(cid:20)(cid:2)< (cid:1)(cid:3) (cid:9)(cid:17) (cid:22)(cid:2) (cid:3)(cid:17)(cid:9)(cid:3)(cid:9)(cid:17) (cid:25)  (cid:17) (cid:17)(cid:9)(cid:3)(cid:9)(cid:17)  (cid:17) (cid:3)(cid:9)(cid:17)(cid:3) (cid:9)(cid:17) (cid:3)(cid:19)(cid:17)(cid:3) (cid:2)(cid:9)(cid:1)(cid:3)	(cid:2) (cid:16)	 (cid:18)  (cid:27) (cid:19)(cid:2) (cid:2) (cid:27)(cid:2)(cid:18) (cid:3)(cid:7) (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:17)(cid:2)  (cid:17)   (cid:1)(cid:17) (cid:3)	(cid:2) (cid:3)(cid:17)(cid:9)(cid:1)(cid:2) (cid:17)(cid:27) (cid:9)(cid:17)	(cid:2) (cid:2)(cid:17)(cid:17)(cid:19)(cid:3) (cid:3)(cid:27) (cid:3) (cid:1)(cid:2) (cid:0)0(cid:24)(cid:25) $(cid:2)(cid:9)(cid:3) 5 (cid:16)(cid:3)   (cid:1)(cid:16) (cid:2) (cid:2)(cid:29)(cid:2)(cid:3)(cid:2)(cid:17)  (cid:2)	  (cid:7) (cid:3)	 (cid:17)(cid:9)(cid:1)(cid:2)(cid:18) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2)  (cid:17) 821 (cid:3) (cid:2)(cid:2)(cid:17)(cid:3)(cid:25) 8	(cid:1)(cid:2) (cid:16)+ (cid:17)(cid:3)  (cid:3)(cid:22)(cid:2)(cid:3)(cid:20)(cid:17)(cid:2) (cid:1)(cid:2) (cid:2)(cid:2)(cid:18) (cid:7) (cid:9)(cid:17) (cid:3)(cid:19)(cid:17)(cid:3) (cid:17)(cid:18) (cid:1)(cid:2)  (cid:3)(cid:3)(cid:17)(cid:3) (cid:7) (cid:17) (cid:27)(cid:2) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:3) (cid:18)(cid:2)(cid:3)(cid:20)(cid:2)(cid:18)   (cid:2)	(cid:3)(cid:2) (cid:9)(cid:17) (cid:3)(cid:19)(cid:17)(cid:3)(cid:25) (cid:28)(cid:1) (cid:17)(cid:26)	   (cid:18)(cid:2)  (cid:2)(cid:22)(cid:17) 	(cid:17)(cid:2) (cid:1)(cid:2) (cid:2)(cid:7)(cid:17)(cid:9)(cid:2) (cid:7) (cid:1)(cid:2) (cid:18)(cid:2)(cid:3)(cid:20) (cid:17)(cid:18) (cid:18)(cid:2)(cid:17)(cid:2) (cid:1)(cid:2) (cid:2)(cid:3)(cid:17)  (cid:2)(cid:26)(cid:2)(cid:9)(cid:3)(cid:22)(cid:2)(cid:2) (cid:7) (cid:1)(cid:2) (cid:17)(cid:17)(cid:9)(cid:1) (cid:17) (cid:7)	(cid:7)	(cid:9)(cid:9)(cid:2) 821 (cid:3) (cid:2)(cid:2)(cid:17)(cid:3) (cid:16)(cid:17) (cid:3) (cid:2)(cid:2)(cid:2)(cid:18) (cid:17)(cid:18) (cid:1)(cid:2) (cid:17)(cid:2) (cid:9)(cid:3)(cid:9)	(cid:3) (cid:16)(cid:17) (cid:17)(cid:18)(cid:17)(cid:2)(cid:18) (cid:7) 	(cid:9)(cid:1)(cid:3) 	(cid:2) (cid:3) 6(cid:25)(cid:31)7µm (cid:2)(cid:9)(cid:1) (cid:20)(cid:27) (cid:17) (cid:17)(cid:3)  (cid:2)(cid:22)(cid:2) (cid:25) (cid:0)(cid:1)(cid:2) (cid:18)(cid:2)(cid:3)(cid:20) (cid:9)(cid:3) (cid:3) (cid:17) %	 (cid:16)(cid:3)(cid:2) %	(cid:19)(cid:3) (cid:2) (cid:27)(cid:19)  (cid:3) (cid:2)(cid:2)(cid:17)(cid:3)(cid:25) (cid:0)(cid:1)(cid:2) (cid:2)(cid:17)(cid:3)(cid:18)(cid:2) (cid:7) (cid:1)(cid:3) (cid:9)(cid:1)(cid:17)(cid:2) (cid:16)(cid:3)   (cid:3)  	(cid:17)(cid:2) (cid:17)(cid:18) (cid:18)(cid:3)(cid:9)	 (cid:1)(cid:2) (cid:2)	  (cid:7) (cid:1)(cid:2) (cid:3)	 (cid:17)(cid:3) (cid:17)(cid:18) 821 (cid:2)(cid:25) (cid:10)(cid:11) (cid:31) (cid:10)  (cid:3)(cid:3)(cid:13)(cid:1) (cid:0)(cid:1)(cid:2) (cid:0)(cid:3)(cid:2)		0(cid:3)(cid:20)(cid:3)(cid:17)  (cid:24)(cid:22)(cid:2)(cid:2) (cid:0)0(cid:24) (cid:16)(cid:17) (cid:3) (cid:2)(cid:2)(cid:2)(cid:18) (cid:3) (cid:17) H(cid:3) (cid:3)(cid:29) 5(cid:3)(cid:2)(cid:29) (cid:31)666 821 (cid:17)	(cid:7)(cid:17)(cid:9)	(cid:2)(cid:18) (cid:3) (cid:17) 6(cid:25)(. (cid:24)$ (cid:9)(cid:2) (cid:17)(cid:18) (cid:7)(cid:2)(cid:17)	(cid:3)(cid:20) (cid:2) (cid:3)  (cid:3) (cid:2)	(cid:3)(cid:22)(cid:17) (cid:2) (cid:20)(cid:17)(cid:2)(cid:25) (cid:0)(cid:1)(cid:2) (cid:0)0(cid:24) (cid:16)(cid:17) (cid:3) (cid:2)(cid:2)(cid:2)(cid:18) 	(cid:3)(cid:20) (cid:1)(cid:2) (cid:7)(cid:16)(cid:17)(cid:2) 	 (cid:3)(cid:2)(cid:18) (cid:19)(cid:27) (cid:1)(cid:2) 821 (cid:22)(cid:2)(cid:18) (cid:17)(cid:18)  (cid:17)(cid:18)(cid:18)(cid:3)(cid:3)(cid:17)  (cid:7)(cid:16)(cid:17)(cid:2) (cid:16)(cid:17) 	(cid:2)(cid:18)  (cid:3)(cid:22)(cid:2) (cid:1)(cid:2) (cid:2)(cid:7)(cid:17)(cid:9)(cid:2)  (cid:1)(cid:2) 	(cid:3)(cid:20) (cid:7) (cid:1)(cid:2) (cid:18)(cid:2)(cid:3)(cid:20)(cid:25) (cid:0)(cid:1)(cid:2) (cid:17)(cid:3) (cid:19) (cid:2) (cid:7)	(cid:18) (cid:16)(cid:1)(cid:2) (cid:17) 821 (cid:3) (cid:2) (cid:27)(cid:2)(cid:18) (cid:7) (cid:1)(cid:2) (cid:3) (cid:2)(cid:2)(cid:17)(cid:3) (cid:7) (cid:17) (cid:0)0(cid:24) (cid:3) (cid:1)(cid:17) (cid:1)(cid:2) 	(cid:3)(cid:20) (cid:7) 	 		 	6 	(cid:31) 	% 0(cid:17)(cid:17)(cid:31) 0(cid:17)(cid:17)6 (cid:31) (cid:31) (cid:31) 6 6 6 (cid:31) (cid:31) 6 (cid:31) 6 6 (cid:31) (cid:31) 6 (cid:18)(cid:17)(cid:17)(cid:31)NQ1 (cid:18)(cid:17)(cid:17)6NQ0 · Q1 + Q2 6 6 6 (cid:31) (cid:31) (cid:0)4/(cid:15)  (cid:0)(cid:1)(cid:2)(cid:4)(cid:1) 	(cid:8) (cid:9)(cid:10) (cid:1) (cid:9)(cid:4) (cid:1)	(cid:9)(cid:14) (cid:1)(cid:2) (cid:18)(cid:2)(cid:3)(cid:20) (cid:3) (cid:17)(cid:17)(cid:20)(cid:2)(cid:18) (cid:2)(cid:3)(cid:2) (cid:27) (cid:19)(cid:27) (cid:1)(cid:2) (cid:7)(cid:16)(cid:17)(cid:2) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:9)(cid:17) (cid:17)(cid:18)(cid:18) (cid:2)(cid:29)(cid:17) (cid:17)(cid:20)(cid:17)(cid:3) (cid:18)(cid:2) (cid:17)(cid:27) (cid:3)(cid:20)(cid:3)4(cid:9)(cid:17) (cid:27)  (cid:16)(cid:2)(cid:3)(cid:20) (cid:1)(cid:2) (cid:2)	  	(cid:3) (cid:7) (cid:1)(cid:2) (cid:0)0(cid:24)(cid:25)  (cid:16)(cid:2)(cid:22)(cid:2) (cid:3)(cid:2)(cid:17)(cid:18) (cid:7) 	(cid:3)(cid:20) (cid:3)(cid:22)(cid:2)(cid:2)  (cid:19)	(cid:26)(cid:2)  	(cid:17)(cid:3)J(cid:2) (cid:17) (cid:2) (cid:3)(cid:2)(cid:22)(cid:17)  H# (cid:20)(cid:17)(cid:2) 	(cid:2)(cid:18) (cid:3) (cid:9)(cid:17)(cid:27) (cid:2)(cid:17)(cid:3) H#(cid:24)O (cid:16)(cid:2)(cid:2) (cid:2) (cid:27)(cid:2)(cid:18) (cid:3)(cid:2)(cid:17)(cid:18)(cid:25) (cid:0)(cid:1)(cid:3) (cid:17)+(cid:2) (cid:3) (cid:3)(cid:19) (cid:2)  (cid:17)	(cid:17)  (cid:27)  (cid:17)(cid:9)(cid:2) (cid:17)(cid:18) 	(cid:2) (cid:1)(cid:2) (cid:9)(cid:2) (cid:7) (cid:1)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27) (cid:9)(cid:1)(cid:17)(cid:3) (cid:17)(cid:18) (cid:17)(cid:9)(cid:9)	(cid:17)(cid:2) (cid:27) (cid:2)(cid:18)(cid:3)(cid:9) (cid:17)(cid:18) (cid:9)(cid:17) (cid:9)	 (cid:17)(cid:2) (cid:1)(cid:2) (cid:3)(cid:2)(cid:9)(cid:2)(cid:9) (cid:18)(cid:2) (cid:17)(cid:27)(cid:25) H#(cid:24)O (cid:20)(cid:17)(cid:2) (cid:7)  (cid:16) (cid:2)(cid:9)(cid:3)4(cid:9) 	(cid:3)(cid:20) (cid:17)(cid:1) (cid:3)(cid:2)(cid:2)(cid:9)(cid:3)(cid:22)(cid:2) (cid:7) (cid:1)(cid:2)  (cid:9)(cid:17) (cid:3)J(cid:17)(cid:3) (cid:7) (cid:1)(cid:2) (cid:18)(cid:2)(cid:3)(cid:20) 	 (cid:3)+(cid:2) +		 (cid:0)(cid:17)(cid:19) (cid:2) F(cid:0) (cid:16)(cid:1)(cid:2)(cid:2) (cid:1)(cid:2)  (cid:17)(cid:9)(cid:2)(cid:2) (cid:7) (cid:1)(cid:2) (cid:18)(cid:2)	  (cid:17)(cid:27) (cid:2) (cid:2)(cid:2) (cid:3) (cid:2)(cid:3)(cid:2) (cid:27) 	  (cid:1)(cid:2) (cid:7)(cid:16)(cid:17)(cid:2) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:9)(cid:17) (cid:9)(cid:2)(cid:17)(cid:2) (cid:18)(cid:3)(cid:9)(cid:3)	(cid:3)(cid:3)(cid:2) (cid:3) (cid:1)(cid:2) (cid:2)(cid:17)	(cid:2)(cid:2) P(cid:19)	(cid:19)(cid:19) (cid:2)P (cid:3) (cid:1)(cid:2) (cid:2)	 (cid:2)(cid:17)	(cid:2) (cid:9)(cid:18)(cid:2)(cid:25) (cid:0)(cid:1)(cid:2)(cid:2) (cid:2) (cid:2)(cid:2) (cid:16)(cid:2)(cid:2) (cid:1)(cid:2) 	(cid:2)(cid:18)  (cid:18)(cid:3)(cid:22)(cid:2) (cid:1)(cid:2) )* (cid:17)(cid:18) (cid:1)(cid:2) )(cid:17)* (cid:3)(cid:20)(cid:17)  (cid:7) (cid:1)(cid:2) (cid:0)0(cid:24)(cid:25)  (cid:3) (cid:3)(cid:17)  (cid:2) (cid:1)(cid:17) (cid:1)(cid:2) (cid:2)(cid:3)(cid:2)  (cid:17)(cid:9)(cid:2)(cid:2) (cid:16)(cid:17) (cid:18)(cid:2) (cid:17)	(cid:17)  (cid:27) (cid:17)(cid:18) (cid:17)   (cid:1)(cid:2) (cid:3)	 (cid:3)J(cid:17)(cid:3) (cid:26)(cid:2)(cid:2)(cid:18) (cid:19)(cid:27) (cid:1)(cid:2) (cid:7)(cid:16)(cid:17)(cid:2) 	(cid:2)(cid:18) (cid:16)(cid:17) 	(cid:2)(cid:18) (cid:26) (cid:3)(cid:9)(cid:2) (cid:2)(cid:17)(cid:9)(cid:1) (cid:9)(cid:2) (cid:16)(cid:17)  (cid:17)(cid:27)(cid:3)(cid:20) (cid:17) (cid:3)(cid:20)(cid:3)4(cid:9)(cid:17)  (cid:2) (cid:17)(cid:18) (cid:17)(cid:27) 	 (cid:3)(cid:3)J(cid:17)(cid:3) (cid:9)	 (cid:18) (cid:17)(cid:26)(cid:2)(cid:9) (cid:1)(cid:2) (cid:2)(cid:7)(cid:17)(cid:9)(cid:2) (cid:7) (cid:1)(cid:2) (cid:0)0(cid:24)(cid:25) (cid:0)(cid:1)(cid:2) (cid:0)0(cid:24) (cid:1)(cid:17) (cid:19)(cid:2)(cid:2) (cid:3) (cid:2)(cid:2)(cid:2)(cid:18) (cid:19)(cid:1) (cid:17) (cid:17) 5(cid:2)(cid:3)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2) (cid:17)(cid:18) (cid:17) (cid:17)(cid:2)(cid:18) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2)(cid:25)  (cid:1)(cid:17) (cid:17)(cid:20)(cid:2) (cid:1)(cid:2) (cid:2) 	(cid:3) (cid:7) (cid:1)(cid:2) (cid:0)0(cid:24) (cid:3)  (cid:9)	(cid:9)(cid:3)(cid:17)  (cid:16)(cid:1)(cid:17) (cid:3) (cid:3)(cid:17) (cid:3) (cid:1)(cid:2) (cid:2)(cid:17)(cid:3)(cid:17)  (cid:7)(cid:2)	(cid:2)(cid:9)(cid:27) (cid:7) (cid:1)(cid:2) (cid:0)0(cid:24) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:9)(cid:17) (cid:1)(cid:2) (cid:19)(cid:2) 	(cid:2)(cid:18)  (cid:9)(cid:17) (cid:9)	 (cid:17)(cid:2) (cid:1)(cid:2) (cid:19)(cid:3) (cid:17)(cid:2) (cid:17) (cid:18)(cid:2)(cid:9)(cid:3)(cid:19)(cid:2)(cid:18) (cid:3) $(cid:2)(cid:9)(cid:3) 5(cid:25) $(cid:3)(cid:9)(cid:2)  (cid:27) (cid:16)	(cid:16)(cid:3)(cid:2) (cid:3)(cid:2)	(cid:2)(cid:9)(cid:18)(cid:3)(cid:20) (cid:3) (cid:2)(cid:2)(cid:18)(cid:2)(cid:18) (cid:17) (cid:3) (cid:2) (cid:0)0(cid:24) (cid:16)(cid:3)(cid:1)  (cid:27) (cid:1)(cid:2)(cid:2) (cid:17)(cid:20)(cid:2) (cid:9)(cid:17) (cid:19)(cid:2) 	(cid:2)(cid:18)(cid:25) (cid:0)(cid:1)(cid:2) (cid:2)	 (cid:3)(cid:20) 		 (cid:7) (cid:1)(cid:2) 8 (cid:3)	8  (cid:3) (cid:3) (cid:1)(cid:2) (cid:7) (cid:7) (cid:17) (cid:1)(cid:2)(cid:2)(cid:2) (cid:9)(cid:18)(cid:2) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:2)(cid:2)(cid:18)  (cid:19)(cid:2) (cid:9)(cid:22)(cid:2)(cid:2)(cid:18) (cid:19)(cid:17)(cid:9)+  (cid:19)(cid:3)(cid:17)(cid:27) 	(cid:3)(cid:20) (cid:17) (cid:1)(cid:2)(cid:2)(cid:2)  (cid:19)(cid:3)(cid:17)(cid:27) (cid:18)(cid:2)(cid:9)(cid:18)(cid:2)(cid:25) (cid:0)(cid:1)(cid:2) (cid:18)(cid:2)(cid:9)(cid:18)(cid:2) (cid:1)(cid:17) (cid:16) 		 	 (cid:18)(cid:17)(cid:17)6 (cid:17)(cid:18) (cid:18)(cid:17)(cid:17)(cid:31)’ (cid:3) 	(cid:1) (cid:17)(cid:19) (cid:2) (cid:3) (cid:1)(cid:16)  (cid:0)(cid:17)(cid:19) (cid:2) (cid:31)(cid:25) (cid:0)(cid:1)(cid:2) (cid:3) (cid:2)(cid:2)(cid:17)(cid:3) (cid:7) 	(cid:9)(cid:1) (cid:17) (cid:18)(cid:2)(cid:9)(cid:18)(cid:2) (cid:3) (cid:22)(cid:2)(cid:27) (cid:3) (cid:2) (cid:17)(cid:18) (cid:2)	(cid:3)(cid:2)  (cid:27) (cid:17) (cid:7)(cid:2)(cid:16) (cid:20)(cid:17)(cid:2) (cid:18)(cid:2)(cid:9)(cid:3)(cid:19)(cid:2)(cid:18) (cid:19)(cid:27) (cid:1)(cid:2) (cid:2)	(cid:17)(cid:3) (cid:1)(cid:16) 	(cid:18)(cid:2) (cid:0)(cid:17)(cid:19) (cid:2) (cid:25) 1 (cid:16)	(cid:9)(cid:1)(cid:17)(cid:2)  1(cid:20)(cid:3) (cid:2) 	 (cid:2) (cid:20)(cid:2)(cid:2)(cid:17) (cid:16)(cid:17) 	(cid:2)(cid:18)  (cid:18)(cid:3)(cid:22)(cid:2) (cid:1)(cid:2) )(cid:17)* (cid:17)(cid:18) )* (cid:3)(cid:20)(cid:17) (cid:25) (cid:0)(cid:1)(cid:2) )(cid:17)* (cid:3)(cid:20)(cid:17)  (cid:16)(cid:17) (cid:9)	 (cid:2)(cid:9)(cid:2)(cid:18)  (cid:1)(cid:2) (cid:18)(cid:17)(cid:17) (cid:3)	 (cid:7) (cid:1)(cid:2) 8 (cid:3)	8  (cid:16)(cid:1)(cid:3) (cid:2) (cid:1)(cid:2)  (cid:3)(cid:20)(cid:17)  (cid:16)(cid:17) (cid:9)(cid:2)(cid:9)(cid:2)(cid:18)  (cid:1)(cid:2) (cid:9) (cid:9)+ (cid:3)	(cid:25) (cid:0)(cid:1)(cid:2) (cid:16) (cid:3)(cid:20)(cid:17)  (cid:17) (cid:16)(cid:3)(cid:1)  (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:17)(cid:18) (cid:1)(cid:2) (cid:20)(cid:17)(cid:18)	(cid:17)  (cid:27) (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:16)(cid:17) (cid:17) (cid:2)(cid:2)(cid:18) (cid:3) (cid:3)(cid:9)(cid:2)(cid:2) (cid:7) (cid:31)66 	(cid:3)  (cid:17)   8 (cid:3)	 8  (cid:16)(cid:3)(cid:9)(cid:1)(cid:2)(cid:18) (cid:25)  (cid:18)(cid:2)  (cid:17)(cid:22)(cid:3)(cid:18) (cid:17)(cid:27) (cid:9)(cid:7)	(cid:3) (cid:1)(cid:2) )(cid:17)* (cid:3)(cid:20)(cid:17)  (cid:16)(cid:17) (cid:17) (cid:16)(cid:17)(cid:27) (cid:9)(cid:17) (cid:16)(cid:1)(cid:3) (cid:2) (cid:1)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27) (cid:7) (cid:1)(cid:2) )* (cid:3)(cid:20)(cid:17)  (cid:16)(cid:17) (cid:3)(cid:9)(cid:2)(cid:2)(cid:2)(cid:18) 	(cid:3)  (cid:2)(cid:17)(cid:9)(cid:1) (cid:9)(cid:2)(cid:9)	(cid:3)(cid:22)(cid:2) 8 (cid:3)	 8  (cid:16)(cid:17) (cid:16)(cid:3)(cid:9)(cid:1)(cid:2)(cid:18) (cid:25) (cid:0)(cid:1)(cid:2) (cid:2) 	(cid:3) (cid:7) (cid:1)(cid:2) (cid:0)0(cid:24) (cid:18)(cid:2)(cid:2)(cid:18)  (cid:1)(cid:2) (cid:3)(cid:2) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2)(cid:9)(cid:2) (cid:7) (cid:1)(cid:2) (cid:16) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2) (cid:17) (cid:18)(cid:2)(cid:9)(cid:3)(cid:19)(cid:2)(cid:18) (cid:3)  (cid:2)(cid:20)(cid:31) (cid:29)(cid:24)(cid:31) (cid:18)(cid:11)	  $ 74 (cid:2) (cid:11)(cid:11)(cid:21)(cid:2) 	(cid:2)(cid:20) (cid:25)(cid:24)  9 (cid:2)	 (cid:11)$(cid:11)	 (cid:11)*(cid:11)  (cid:2)(cid:20)(cid:31) (cid:29)(cid:29)(cid:31) (cid:11)(cid:21)(cid:21)(cid:13)(cid:2) (cid:2)(cid:14) (cid:2) (cid:1)(cid:11) 74 (cid:0)""(cid:19) 	(cid:2)(cid:20) (cid:11)((cid:11)(cid:21)  (cid:2)	(cid:31) (cid:15)((cid:11)(cid:21)  (cid:11)$(cid:11)(cid:11)*(cid:11) * *(cid:7) : (cid:25)(cid:24) 9 $(cid:2)(cid:9)(cid:3) (cid:25)  (cid:16)(cid:2)(cid:22)(cid:2) (cid:3)(cid:9)(cid:2) (cid:1)(cid:2) (cid:16) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2) (cid:17)(cid:2) (cid:17)(cid:18)(cid:2) (cid:7) (cid:1)(cid:2) (cid:17)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27) (cid:2) (cid:2)(cid:2) (cid:1)(cid:2) (cid:2) 	(cid:3) (cid:18)(cid:2)(cid:2)(cid:18) 	 (cid:1)(cid:2) (cid:3)(cid:2) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2)(cid:9)(cid:2) (cid:7) (cid:3)(cid:2)(cid:9)(cid:2)(cid:9)(cid:25)  (cid:1)(cid:3) (cid:9)(cid:17)(cid:2) (cid:1)(cid:2) (cid:2) 	(cid:3) (cid:16)(cid:17) (cid:2)(cid:17)	(cid:2)(cid:18) (cid:17)  (cid:2) (cid:1)(cid:17) (cid:31)66 (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:3) (cid:1)	(cid:20)(cid:1)  (cid:19)(cid:2) (cid:17)(cid:3)(cid:7)(cid:17)(cid:9)(cid:27) (cid:7) (cid:17) 821	(cid:19)(cid:17)(cid:2)(cid:18) (cid:3) (cid:2)(cid:2)(cid:17)(cid:3)’ (cid:1)(cid:16)(cid:2)(cid:22)(cid:2) (cid:1)(cid:2) (cid:2)(cid:17)(cid:3)(cid:17)  (cid:7)(cid:2)	(cid:2)(cid:9)(cid:27) (cid:7) (cid:1)(cid:2) (cid:0)0(cid:24) (cid:16)(cid:17)  (cid:20)(cid:2)(cid:17)(cid:2) (cid:1)(cid:17) (cid:31). J(cid:25) $(cid:3)(cid:9)(cid:2) (cid:1)(cid:2) (cid:17)(cid:3) (cid:7) (cid:1)(cid:2)(cid:2) (cid:2)(cid:29)(cid:2)(cid:3)(cid:2) (cid:16)(cid:17)  	(cid:9)(cid:9)(cid:2)	 (cid:7)	  (cid:27) (cid:3)(cid:2)(cid:2) (cid:1)(cid:2) (cid:3)(cid:2) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2)(cid:9)(cid:2) (cid:7) (cid:16) (cid:16)(cid:3)(cid:2) (cid:17) (cid:22)(cid:2)(cid:27) (cid:1)(cid:3)(cid:20)(cid:1) (cid:7)(cid:2)	(cid:2)(cid:9)(cid:27) (cid:17) (cid:17) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2) (cid:0)0(cid:24) (cid:16)(cid:17) (cid:17)  (cid:3) (cid:2)(cid:2)(cid:2)(cid:18)(cid:25) (cid:0)(cid:1)(cid:2) (cid:17) (cid:18)(cid:2) (cid:17)(cid:27) (cid:0)0(cid:24) (cid:7)(cid:2)(cid:17)	(cid:2)  (cid:27) (cid:2) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2) (cid:3) (cid:1)(cid:3) (cid:9)(cid:17)(cid:2) (cid:3) (cid:1)(cid:2) )(cid:17)* (cid:3)(cid:20)(cid:17) ’ (cid:2)(cid:17)(cid:9)(cid:1) (cid:17) (cid:3) (cid:9)(cid:17)(cid:2)(cid:18) (cid:3) (cid:1)(cid:2) )* (cid:3)(cid:20)(cid:17) (cid:25) 8 (cid:17) (cid:17) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2) (cid:0)0(cid:24) (cid:1)(cid:2) (cid:2) 	(cid:3) (cid:19)(cid:2)(cid:22)(cid:2)(cid:18) (cid:16)(cid:17) (cid:17) (cid:1)(cid:2) (cid:18)(cid:2) (cid:7) .66(cid:25) G(cid:22)(cid:2) (cid:1)	(cid:20)(cid:1) (cid:1)(cid:2) (cid:2) 	(cid:3) (cid:16)(cid:17) (cid:16)(cid:2) (cid:1)(cid:17) (cid:1)(cid:2) (cid:2) (cid:17)(cid:9)(cid:1)(cid:3)(cid:2)(cid:22)(cid:2)(cid:18) (cid:19)(cid:27) (cid:1)(cid:2) 5(cid:2)(cid:3)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2) (cid:0)0(cid:24) (cid:3) (cid:16)(cid:17) (cid:19)(cid:2)(cid:22)(cid:2)(cid:18) (cid:1)(cid:17) (cid:1)(cid:2) (cid:17) (cid:18)(cid:2) (cid:17)(cid:27) (cid:0)0(cid:24) (cid:16)(cid:17) (cid:17)(cid:19) (cid:2) (cid:17) (cid:22)(cid:2) .6 J (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:2)(cid:17) (cid:1)(cid:17) (cid:1)(cid:2) (cid:2)(cid:17)(cid:3)(cid:17)  (cid:7)(cid:2)	(cid:2)(cid:9)(cid:27) (cid:7) (cid:1)(cid:2) (cid:17) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2) (cid:0)0(cid:24) (cid:16)(cid:17) (cid:17)  (cid:1)(cid:2)(cid:2) (cid:3)(cid:2) (cid:20)(cid:2)(cid:17)(cid:2) (cid:1)(cid:17) (cid:1)(cid:2) 5(cid:2)(cid:3)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27) (cid:0)0(cid:24)(cid:25) (cid:0)(cid:1)(cid:2)  (cid:3)(cid:3) (cid:7) (cid:2)(cid:17)(cid:3) 	(cid:3)(cid:20) (cid:1)(cid:2) 	 (cid:2) (cid:20)(cid:2)(cid:2)(cid:17) (cid:16)(cid:17) (cid:7)	(cid:18)  (cid:19)(cid:2) (cid:17)	(cid:18) ;6  J(cid:25) 8(cid:3)(cid:20)	(cid:2) (cid:31)6 (cid:1)(cid:16) (cid:1)(cid:2) (cid:16)(cid:17)(cid:22)(cid:2)	 (cid:7) (cid:19)(cid:17)(cid:3)(cid:2)(cid:18) 	(cid:3)(cid:20) (cid:1)(cid:3) (cid:2) 	(cid:25) (cid:2) (cid:1)(cid:17) (cid:3) (cid:1)(cid:3) (cid:3)(cid:9)	(cid:2) (cid:17)(cid:18) (cid:3) (cid:17)   (cid:1)(cid:2) (cid:7)  (cid:16)(cid:3)(cid:20) (cid:9)(cid:2)(cid:2)(cid:1) (cid:2)(cid:7)(cid:2)(cid:3)(cid:20)  (cid:1)(cid:2) 821 (cid:2)	  - &(cid:3)	& (cid:16)(cid:2)(cid:2) 	(cid:2)(cid:18) (cid:3)(cid:2)(cid:17)(cid:18) (cid:7) (cid:1)(cid:2) ( (cid:2)	(cid:3)(cid:2)(cid:18)’ (cid:1)(cid:2) (cid:7)	(cid:1) (cid:3) (cid:2)(cid:18)	(cid:18)(cid:17) (cid:17)(cid:18) (cid:16)(cid:17) (cid:17)(cid:18)(cid:18)(cid:2)(cid:18) (cid:7) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2) (cid:2)(cid:17)(cid:25)  (cid:3) (cid:9) (cid:2)(cid:17) (cid:7) (cid:1)(cid:2) 4(cid:20)	(cid:2) (cid:1)(cid:17) (cid:1)(cid:2) 821 (cid:3) (cid:2)(cid:2)(cid:17)(cid:3) (cid:3) (cid:19)(cid:2)(cid:3)(cid:20) (cid:18)(cid:3)(cid:22)(cid:2)  (cid:3)  (cid:3)(cid:3)< (cid:1)(cid:2) )(cid:9) +*  (cid:3)(cid:2) (cid:1)(cid:2) (cid:3)(cid:2)(cid:17)  (cid:2)(cid:9)	(cid:9)(cid:3) (cid:7) (cid:1)(cid:2) (cid:2)(cid:18)(cid:2) (cid:9) (cid:9)+ (cid:1)(cid:17) (cid:3)(cid:1)(cid:17)(cid:2)(cid:18) 	 (cid:2) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:3)(cid:20)(cid:1) (cid:3)(cid:17)(cid:9)  (cid:1)(cid:2) (cid:9)(cid:2)(cid:9) (cid:19)(cid:2)(cid:1)(cid:17)(cid:22)(cid:3)	 (cid:7) (cid:1)(cid:2) (cid:18)(cid:2)(cid:22)(cid:3)(cid:9)(cid:2)(cid:25) (cid:0)(cid:1)(cid:2) (cid:2)	  (cid:18)(cid:3)(cid:9)	(cid:2)(cid:18) 	(cid:3)  (cid:16) (cid:17)(cid:2) (cid:19)(cid:17)(cid:2)(cid:18)  (cid:17) (cid:26)	(cid:9)(cid:1)(cid:3) (cid:3)(cid:2) (cid:2)(cid:7)(cid:2)(cid:2)(cid:9)(cid:2) (cid:17)(cid:18) (cid:2)(cid:18)(cid:2)(cid:25) (cid:0)(cid:1)(cid:3) (cid:3)(cid:2)  (cid:3)(cid:3)(cid:17)(cid:3)  (cid:1)(cid:2) (cid:2)(cid:2)(cid:18) (cid:7) (cid:1)(cid:2) (cid:9)(cid:1)(cid:2)(cid:2) (cid:19)(cid:2)(cid:9)(cid:17)	(cid:2) (cid:7) (cid:1)(cid:2) (cid:9)(cid:1)(cid:17)(cid:17)(cid:9)(cid:2)(cid:3)(cid:3)(cid:9) (cid:7) (cid:1)(cid:2) (cid:3)	 (cid:17)(cid:18) (cid:7) (cid:1)(cid:2) 821(cid:25) (cid:0)(cid:1)(cid:2)(cid:2)  (cid:3)(cid:3)(cid:17)(cid:3) (cid:17)(cid:2) (cid:18)	(cid:2)  (cid:26)	 (cid:2) (cid:3) 	 (cid:18)	(cid:2)  	(cid:3)(cid:20) (cid:7) (cid:1)(cid:2) (cid:3)(cid:20)(cid:17)   (cid:1)(cid:2) 821< (cid:3) (cid:16)(cid:17) (cid:19)(cid:2)(cid:22)(cid:2)(cid:18) (cid:2)(cid:29)(cid:2)(cid:3)(cid:2)(cid:17)  (cid:27) (cid:1)(cid:17) (cid:1)(cid:2) (cid:0)0(cid:24) (cid:1)(cid:17)(cid:18) (cid:17) (cid:26)(cid:2) (cid:7) (cid:1)(cid:2) (cid:18)(cid:2) (cid:7) %66  (cid:17)(cid:18) (cid:1)(cid:17) (cid:1)(cid:2) (cid:2) (cid:7) (cid:1)(cid:2) (cid:0)0(cid:24) (cid:1)(cid:16)(cid:2)(cid:18) (cid:2) 		(cid:3)(cid:7)(cid:3)(cid:27)(cid:25) (cid:0)(cid:1)(cid:3) (cid:2)	 (cid:2)(cid:18) (cid:3) (cid:2)(cid:17)(cid:17)(cid:19)(cid:3) (cid:3)(cid:27) (cid:3) (cid:1)(cid:2) &(cid:3)	& 	(cid:2)(cid:18) (cid:3) (cid:1)(cid:2) (cid:0)0(cid:24)< (cid:17) (cid:2)(cid:29)(cid:17) (cid:2) (cid:9)(cid:17) (cid:19)(cid:2) (cid:2)(cid:2) (cid:3) 8(cid:3)(cid:20)	 	(cid:2) (cid:31)(cid:31)(cid:25)  (cid:1)(cid:2) (cid:9)(cid:2)(cid:2)(cid:1) (cid:3) (cid:3) (cid:2)(cid:22)(cid:3)(cid:18)(cid:2) (cid:1)(cid:17) (cid:1)(cid:2) (cid:2)(cid:9)(cid:18) (cid:17)(cid:20)(cid:2) (cid:1)(cid:2) &(cid:3)	& 88(cid:31) (cid:3) (cid:20)(cid:3)(cid:20) (cid:2)(cid:17)(cid:17)(cid:19) (cid:2) (cid:17) (cid:1)(cid:2) 		 (cid:9)(cid:3) 	  (cid:17)(cid:2) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:31) (cid:17)(cid:18) 6(cid:25) (cid:0)(cid:1)(cid:3) (cid:3) (cid:17) (cid:2)(cid:29)(cid:2)(cid:2) (cid:9)(cid:17)(cid:2)< (cid:22)(cid:2)(cid:17)   (cid:1)(cid:2) &(cid:3) & (cid:16)	 (cid:18) (cid:9)(cid:1)(cid:17)(cid:20)(cid:2) (cid:17)(cid:2) (cid:17) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2) (cid:3)(cid:2)(cid:25) (cid:0) (cid:17)(cid:17) (cid:27)(cid:2) (cid:1)(cid:2) (cid:19)(cid:2)(cid:1)(cid:17)(cid:22)(cid:3)	 (cid:7) (cid:1)(cid:2) (cid:27)(cid:2) (cid:16)(cid:1)(cid:2) (cid:17)   (cid:9)	 (cid:2) (cid:17)(cid:2) 	(cid:9)(cid:1)(cid:3) (cid:17) (cid:2)(cid:18)(cid:2) (cid:16)(cid:17) (cid:3) (cid:2)(cid:2)(cid:2)(cid:18)  (cid:1)(cid:2) 821 ref 00 01 10 11 b a 00 01 10 11 a b start stop start 2−bit Up−counter  (cid:2)(cid:20)(cid:31) (cid:29)(cid:26)(cid:31) (cid:17)(cid:11)(cid:12)(cid:11) *(cid:2)*	(cid:2) (cid:2) (cid:11)(cid:11)(cid:11)(cid:12) (cid:2) (cid:1)(cid:11) 74 (cid:17)(cid:18) (cid:9)(cid:2)(cid:9)(cid:2)(cid:18)  (cid:1)(cid:2) (cid:17)(cid:2)(cid:18) (cid:18)(cid:2) (cid:17)(cid:27) (cid:0)0(cid:24) (cid:18)(cid:2)(cid:9)(cid:3)(cid:19)(cid:2)(cid:18)(cid:25) (cid:0)(cid:1)(cid:2) (cid:9)(cid:1)(cid:17)  (cid:2)(cid:20)(cid:2) (cid:16)(cid:17)  (cid:9)(cid:2)(cid:17)(cid:2) (cid:17) (cid:9)(cid:3)(cid:9)	(cid:3) (cid:1)(cid:17) (cid:16)(cid:3)   (cid:17) (cid:2) (cid:1)(cid:2) (cid:3)(cid:2) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2)(cid:9)(cid:2) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) )(cid:17)* (cid:17)(cid:18) )* (cid:3)(cid:20)(cid:17)  (cid:17)	(cid:17)(cid:3)(cid:9)(cid:17)  (cid:27) (cid:17)(cid:9)(cid:9)(cid:18)(cid:3)(cid:20)  (cid:1)(cid:2) (cid:18)(cid:17)(cid:17) (cid:3)(cid:2)  (cid:19)(cid:2) (cid:2)(cid:25) 8 (cid:1)(cid:3) 	(cid:2) (cid:17) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2) (cid:17)(cid:9)(cid:1)(cid:2)(cid:18)  (cid:1)(cid:2) (cid:0)0(cid:24) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2) (cid:16)(cid:17) (cid:3) (cid:2)(cid:2)(cid:2)(cid:18) 	(cid:3)(cid:20) H#(cid:24)O (cid:20)(cid:17)(cid:2) (cid:16)(cid:1)(cid:3) (cid:2) (cid:1)(cid:2) (cid:16)(cid:3)(cid:9)(cid:1)(cid:3)(cid:20) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:1)(cid:2) (cid:17) (cid:16)(cid:17) (cid:18)(cid:2) 	(cid:3)(cid:20) (cid:16) 	 (cid:3) (cid:2)(cid:29)(cid:2) (cid:17) (cid:1)(cid:16) (cid:3) 8(cid:3)(cid:20)	(cid:2) (cid:31)%(cid:25) (cid:0)(cid:1)(cid:2) 	 (cid:3) (cid:2)(cid:29)(cid:2) (cid:7) (cid:1)(cid:2) )(cid:17)* (cid:3)(cid:20)(cid:17)  (cid:3) (cid:3) (cid:1)(cid:3) (cid:9)(cid:17)(cid:2) 4(cid:29)(cid:2)(cid:18)  (cid:17) (cid:9)(cid:17) (cid:3)(cid:2)< (cid:1)(cid:3) (cid:16)(cid:17)(cid:27) (cid:16) (cid:19)(cid:3) (cid:9)(cid:17) (cid:19)(cid:2) (cid:2)(cid:9)(cid:18)(cid:2)(cid:18)  (cid:1)(cid:2) (cid:3)(cid:2) (cid:2)(cid:17)(cid:17)(cid:3) (cid:9)(cid:1)(cid:17)(cid:20)(cid:3)(cid:20) (cid:1)(cid:2) (cid:3)(cid:2) (cid:7) (cid:1)(cid:2) )* (cid:3)(cid:20)(cid:17) (cid:25) (cid:0)(cid:1)(cid:2) (cid:2)(cid:18)	(cid:18)(cid:17) 	 (cid:3) (cid:2)(cid:29)(cid:2)  (cid:1)(cid:2) )(cid:17)* (cid:17)(cid:1) (cid:3) (cid:2)(cid:2)  (cid:17)(cid:9)(cid:1) (cid:1)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27) (cid:3)(cid:18)	(cid:9)(cid:2)(cid:18) (cid:19)(cid:27) (cid:1)(cid:2) 	 (cid:3) (cid:2)(cid:29)(cid:2)  (cid:1)(cid:2) )* (cid:17)(cid:1)(cid:25) 1 (cid:16) (cid:19)(cid:3) (cid:9)(cid:17) (cid:19)(cid:2) (cid:2)(cid:9)(cid:18)(cid:2)(cid:18)  (cid:1)(cid:2)  (cid:3)+ (cid:17) %	(cid:19)(cid:3) (cid:9)	(cid:2) (cid:16)(cid:17) 	(cid:2)(cid:18)  (cid:18)(cid:3)(cid:22)(cid:2) (cid:1)(cid:2) (cid:9)  (cid:3)	 (cid:7) (cid:1)(cid:2) 	 (cid:3) (cid:2)(cid:29)(cid:2)(cid:25) (cid:0)(cid:1)(cid:2) (cid:9)	(cid:2) (cid:16)(cid:17) (cid:9) (cid:9)+(cid:2)(cid:18)  (cid:1)(cid:2) (cid:2)(cid:20)(cid:17)(cid:3)(cid:22)(cid:2) (cid:2)(cid:18)(cid:20)(cid:2) (cid:7) (cid:1)(cid:2) 	 (cid:3) (cid:2)(cid:29)(cid:2) 		 (cid:9)(cid:2)(cid:9)(cid:2)(cid:18)  (cid:1)(cid:2) (cid:17) (cid:3)(cid:20)(cid:17)   (cid:1)(cid:17) (cid:2)	(cid:20)(cid:1) (cid:3)(cid:2) (cid:16)(cid:17) (cid:20)(cid:3)(cid:22)(cid:2)  (cid:1)(cid:2) (cid:18)(cid:2)(cid:9)(cid:18)(cid:2)  (cid:18)	(cid:9)(cid:2) (cid:17) (cid:22)(cid:17) (cid:3)(cid:18) 		 (cid:19)(cid:2)(cid:7)(cid:2) (cid:1)(cid:2) (cid:2)(cid:29) (cid:2) (cid:7) (cid:2)(cid:17)	(cid:2)(cid:2) (cid:16)(cid:2)(cid:2)  (cid:17)(cid:18)(cid:2)(cid:18)  (cid:1)(cid:2) (cid:0)0(cid:24)(cid:25) (cid:0)(cid:1)(cid:2) (cid:3)(cid:3)(cid:20) (cid:7) (cid:1)(cid:2) (cid:9)	(cid:2)  (cid:2)	(cid:2) (cid:9)(cid:2)(cid:9) (cid:19)(cid:2)(cid:1)(cid:17)(cid:22)(cid:3)	 (cid:16)(cid:17) (cid:19)(cid:17)(cid:2)(cid:18)  (cid:1)(cid:2) (cid:3)(cid:7)(cid:17)(cid:3) (cid:22)(cid:3)(cid:18)(cid:2)(cid:18) (cid:19)(cid:27) (cid:1)(cid:2) (cid:22)(cid:2)(cid:18)(cid:25) ref start stop 00 01 10 11 b a d1 d0 d2 00 01 10 11 a b d1 d0 d2  (cid:2)(cid:20)(cid:31) (cid:29)’(cid:31) (cid:18)(cid:11)	  	(cid:2)(cid:20) (cid:1)(cid:11) 74 (cid:2) (cid:11)(cid:11)(cid:21)(cid:2)(cid:31) 	 * *(cid:7) : (cid:29)(cid:24)(cid:24) 9  (cid:2)(cid:20)(cid:31) (cid:29)%(cid:31) %	(cid:13)(cid:2)(cid:23)(cid:14)(cid:13)  (cid:11)(cid:12)(cid:11) (cid:2) (cid:11)(cid:11)(cid:21)(cid:2) (cid:0)(cid:1)(cid:2) (cid:3) (cid:2)(cid:2)(cid:17)(cid:3) (cid:2)(cid:2)(cid:2)(cid:18) (cid:3) (cid:7) %	(cid:19)(cid:3)’ (cid:16)(cid:3)(cid:1) (cid:3) (cid:18)(cid:3)4(cid:9)(cid:17)(cid:3) (cid:1)(cid:2) (cid:17)(cid:2) (cid:9)(cid:3)(cid:9)	(cid:3) (cid:9)(cid:17) (cid:19)(cid:2) (cid:2)(cid:9)4(cid:20)	(cid:2)(cid:18) (cid:7) (	 (cid:19)(cid:3)B(cid:27)(cid:19) (cid:25) 1 (cid:19)(cid:17)(cid:3)(cid:9) (cid:2)(cid:29)(cid:17) (cid:2) (cid:3) (cid:1)(cid:16) (cid:3) 8(cid:3)(cid:20)	(cid:2) (cid:31)((cid:25) (cid:0)(cid:1)(cid:2) (cid:17)  (cid:17)(cid:2)(cid:17) (cid:2)	(cid:3)(cid:2)(cid:18)  (cid:1)(cid:2) 821 (cid:7) (cid:1)(cid:2) (cid:2)(cid:18)(cid:2) (cid:17)(cid:18) (cid:1)(cid:2) (cid:2)(cid:9)(cid:2)(cid:3)(cid:22)(cid:2) (cid:3) (cid:2) (cid:17)(cid:3)(cid:22)(cid:2) (cid:27) (cid:17)  < (cid:1)(cid:2) (cid:20)(cid:17)(cid:2)  (cid:3) (cid:9)(cid:3) (cid:7) ( &(cid:3)	& : (cid:20)(cid:17)(cid:2) % 	 (cid:3) (cid:2)(cid:29)(cid:2) (cid:17) %	(cid:19)(cid:3) (cid:9)	(cid:2) (cid:17)(cid:18) (cid:1)(cid:2) (cid:2)(cid:9)(cid:2)(cid:3)(cid:22)(cid:2)  (cid:20)(cid:3)(cid:9)(cid:25) (cid:22)(cid:2)(cid:17)   (cid:31)(cid:31) (cid:24)4(cid:20)	(cid:17)(cid:19) (cid:2) (cid:20)(cid:3)(cid:9) @ (cid:9)+ (cid:24)@ (cid:16)(cid:2)(cid:2) 	(cid:2)(cid:18) (cid:7) (cid:1)(cid:2) (cid:7)	   (cid:3) (cid:2)(cid:2)(cid:17)(cid:3)(cid:25) 8(cid:3)(cid:20)	(cid:2) (cid:31)- (cid:1)(cid:16) (cid:1)(cid:2) (cid:2)	  (cid:19)(cid:17)(cid:3)(cid:2)(cid:18) 	(cid:3)(cid:20) (cid:1)(cid:2) (cid:3) (cid:2)	 (cid:2)(cid:17)(cid:3)(cid:25) F(cid:3)(cid:20) 	(cid:9)(cid:1) (cid:17) (cid:9)4(cid:20)	(cid:17)(cid:3) (cid:7)(cid:2)	(cid:2)(cid:9)(cid:3)(cid:2) (cid:7) 	  (cid:31)66  J (cid:16)(cid:2)(cid:2) (cid:17)(cid:9)(cid:1)(cid:3)(cid:2)(cid:22)(cid:2)(cid:18)  %66 (cid:19)(cid:3)B(cid:2)(cid:9) (cid:16)(cid:3)(cid:1) (cid:16) (cid:16)(cid:3)(cid:2)(cid:25) $(cid:3)(cid:9)(cid:2) (cid:1)(cid:2)(cid:2) (cid:3) (cid:1)(cid:2) (cid:19)(cid:17)(cid:19)(cid:3) (cid:3)(cid:27) (cid:1)(cid:17) (cid:2) (cid:7) (cid:1)(cid:2) 8 (cid:3)	8  (cid:17)(cid:27) (cid:20) (cid:3) (cid:17) (cid:2)(cid:17)(cid:17)(cid:19) (cid:2) (cid:17)(cid:2) (cid:17)(cid:18) (cid:1)(cid:2)(cid:9)(cid:2) (cid:18)	(cid:9)(cid:2) (cid:3)(cid:17)(cid:9)(cid:9)		 (cid:17)(cid:2) (cid:2)	  	(cid:8)(cid:9)(cid:3)(cid:2) (cid:3)(cid:2) (cid:16)(cid:17) (cid:20)(cid:3)(cid:22)(cid:2)  (cid:1)(cid:2) 8 (cid:3)	8   (cid:2) (cid:22)(cid:2) (cid:19)(cid:2)(cid:7)(cid:2) (cid:1)(cid:2)(cid:3) (cid:22)(cid:17) 	(cid:2) (cid:16)(cid:2)(cid:2) (cid:17) (cid:2)(cid:18)(cid:25) (cid:0)(cid:1)(cid:2) (cid:22)(cid:17) 	(cid:2) (cid:7) τ (cid:2) 	(cid:3) (cid:3)(cid:2) (cid:9)(cid:17) (cid:7) (cid:1)(cid:2) 8 (cid:3)	8  (cid:3) (cid:7) (cid:1)(cid:2) (cid:18)(cid:2) (cid:7) .6 (cid:30)%- (cid:25)  (cid:17) 	(cid:9)(cid:1)(cid:3) (cid:3) (cid:2)(cid:2)(cid:17)(cid:3) F(cid:0)GH(cid:2) (cid:16)(cid:3)   (cid:19)(cid:2) 	(cid:2)(cid:18) (cid:3)(cid:2)(cid:17)(cid:18) (cid:7) 8 (cid:3)	8   (cid:9)(cid:17)(cid:2) (cid:1)(cid:2) (cid:3)(cid:20)(cid:17)  (cid:1)(cid:17)(cid:2) (cid:3)(cid:9)(cid:2) (cid:1)(cid:2)(cid:27) (cid:1)(cid:17)(cid:22)(cid:2) (cid:19)(cid:2)(cid:2) (cid:2)(cid:17)(cid:17)(cid:19) (cid:2) (cid:2)(cid:2)’ (cid:1)(cid:16)(cid:2)(cid:22)(cid:2) (cid:17) (cid:3) 821 F(cid:0)GH(cid:2) (cid:17)(cid:2) (cid:22)(cid:2)(cid:27) (cid:18)(cid:3)(cid:8)(cid:9)	   (cid:18)(cid:2)(cid:3)(cid:20) (cid:3) (cid:16)(cid:17) (cid:18)(cid:2)(cid:9)(cid:3)(cid:18)(cid:2)(cid:18)  (cid:2) (cid:27) 8 (cid:3)	8 (cid:25) (cid:0) (cid:9)(cid:9) 	(cid:18)(cid:2) (cid:1)(cid:2) 821	(cid:19)(cid:17)(cid:2)(cid:18) (cid:2)	  8(cid:3)(cid:20)	(cid:2) (cid:31). (cid:1)(cid:16) (cid:1)(cid:2) (cid:2)(cid:26)(cid:2)(cid:9) (cid:7) (cid:3)(cid:17)(cid:9)(cid:1)(cid:2)(cid:18) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2)  (cid:1)(cid:2) (cid:19)(cid:2)(cid:1)(cid:17)(cid:22)(cid:3)	 (cid:7) (cid:1)(cid:2) (cid:0)0(cid:24)(cid:25)  (cid:1)(cid:3) (cid:2) 	 (cid:17) (cid:3)(cid:2)(cid:17)  (cid:2)(cid:18)(cid:2) (cid:16)(cid:17) (cid:9)(cid:2)(cid:17)(cid:2)(cid:18) (cid:16)(cid:1)(cid:2)(cid:2) (cid:1)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2) (cid:18)(cid:3)(cid:18)  (cid:17)(cid:9)(cid:1) (cid:1)(cid:2) (cid:0)0(cid:24) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2)(cid:25)  (cid:9)(cid:17) (cid:19)(cid:2) (cid:19)(cid:2)(cid:22)(cid:2)(cid:18) (cid:1)(cid:17) (cid:1)(cid:2) (cid:2)	(cid:2)(cid:9)(cid:2) (cid:7) (cid:17)(cid:2) (cid:7) (cid:1)(cid:2) (cid:0)0(cid:24) (cid:2)(cid:2) (cid:2) 	(cid:16)(cid:17)(cid:2)(cid:18) (cid:17)(cid:2)< (cid:2) (cid:1)(cid:2) (cid:2)	(cid:2)(cid:9)(cid:2) (cid:7) (cid:1)(cid:2)  (cid:2)(cid:7) (cid:31)(cid:31)(cid:31)(cid:31) (cid:31)(cid:31)66 (cid:3)(cid:3)(cid:20) (cid:31)(cid:31)(cid:31)6 (cid:31)666 (cid:31)66(cid:31)  (cid:2)(cid:2)	 (cid:17)	(cid:2) (cid:9)(cid:18)(cid:2) (cid:2)(cid:9)(cid:25) (cid:0)(cid:1)(cid:3) (cid:3) (cid:18)	(cid:2)  (cid:2)(cid:17)(cid:17)(cid:19)(cid:3) (cid:3)(cid:27) (cid:9)(cid:9)	(cid:3)(cid:20) (cid:3) (cid:1)(cid:2) (cid:0)0(cid:24) (cid:2)	 (cid:3)(cid:20) (cid:3) 	(cid:2)(cid:29)(cid:2)(cid:9)(cid:2)(cid:18) (cid:17)(cid:2)(cid:25) (cid:0)(cid:1)(cid:2) (cid:2)(cid:17)(cid:3)(cid:17)  (cid:7)(cid:2)	(cid:2)(cid:9)(cid:27) (cid:16)(cid:17) (cid:31)66  J(cid:25)  (cid:2)(cid:20)(cid:31) (cid:29)(cid:30)(cid:31) (cid:11)(cid:21)(cid:21)(cid:13)(cid:2) (cid:2)(cid:14) (cid:2) 74 	(cid:2)(cid:20) (cid:2)(cid:21)*(cid:1)(cid:11)(cid:12) (cid:2)(cid:11)(cid:21)  (cid:11)(cid:12)(cid:11) (cid:21)(cid:12) (cid:11)*(cid:11)(cid:2)0(cid:11)(cid:31) (cid:11)	(cid:11)*(cid:14) : (cid:29)(cid:24)(cid:24)  9 (cid:19)(cid:11) 	(cid:6)(cid:15)(cid:1) (cid:12)(cid:1)	 (cid:13)(cid:1) #(cid:3)	  (cid:0)(cid:1)(cid:2) 	(cid:9)(cid:1)(cid:3) (cid:22)(cid:2)(cid:3) (cid:7) (cid:1)(cid:2) (cid:18)(cid:2)(cid:22)(cid:3)(cid:9)(cid:2) (cid:3) (cid:2)(cid:2)(cid:2)(cid:18)  6(cid:25)(cid:31)7µm (cid:2)(cid:9)(cid:1) (cid:20)(cid:27) (cid:3) (cid:2)(cid:17) (cid:27) (cid:3)(cid:18)(cid:2)(cid:3)(cid:9)(cid:17)   (cid:1)(cid:2) 821	(cid:19)(cid:17)(cid:2)(cid:18) (cid:3) (cid:2)(cid:2)(cid:17)(cid:3)(cid:25) (cid:18)(cid:2)(cid:2)(cid:18) (cid:1)(cid:2)  (cid:27) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2)(cid:9)(cid:2) (cid:9)(cid:3) (cid:3) (cid:1)(cid:2) (cid:2)(cid:2)(cid:18) (cid:7) (cid:1)(cid:2) (cid:9)(cid:3)(cid:9)	(cid:3) (cid:17)(cid:18) (cid:1)(cid:2) (cid:18)(cid:2) (cid:17)(cid:27)  (cid:3)(cid:2) (cid:2) (cid:27)(cid:2)(cid:18) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:2)	  (cid:3) (cid:1)(cid:3)(cid:20)(cid:1)(cid:2) (cid:2)(cid:17)(cid:3)(cid:20) (cid:2)(cid:2)(cid:18)(cid:25) 8(cid:3)(cid:20)	(cid:2) (cid:31): (cid:1)(cid:16) (cid:1)(cid:2) (cid:3)	 (cid:17)(cid:3) (cid:2)	  (cid:7) (cid:17) (cid:2)(cid:19)(cid:2)(cid:9)(cid:1) (cid:2)	 (cid:3)(cid:3) (cid:17)  (cid:1)(cid:2) (cid:2) (cid:18)(cid:2)(cid:9)(cid:3)(cid:19)(cid:2)(cid:18) (cid:17)(cid:19)(cid:22)(cid:2)(cid:25) 1 (cid:9)	(cid:2) (cid:2)(cid:18) (cid:1)(cid:2) (cid:2)(cid:17) (cid:7) (cid:18)(cid:17)(cid:17) 6(cid:29)6 6(cid:29)(cid:31) 6(cid:29)% 6(cid:29)( (cid:18)(cid:3)(cid:30)6<(cid:31)  (cid:17)(cid:18) (cid:1)(cid:2) (cid:2)	 (cid:9)(cid:2)(cid:3)(cid:22)(cid:2) 		 (cid:1)(cid:2) (cid:9)(cid:2)(cid:9) (cid:22)(cid:17) 	(cid:2) (cid:18)(cid:30)6<(cid:31) (cid:25) (cid:2) (cid:1)(cid:17) (cid:1)(cid:2) (cid:17)(cid:17) (cid:20)	(cid:2) (cid:2)	  (cid:16)(cid:2)(cid:2) (cid:18)(cid:3) (cid:17)(cid:27)(cid:2)(cid:18) 	(cid:3)(cid:20) (cid:17) (cid:18)(cid:3)(cid:20)(cid:3)(cid:17)  (cid:16)(cid:17)(cid:22)(cid:2)(cid:7)  (cid:3)(cid:9)(cid:2)(cid:17)(cid:2) (cid:2)(cid:17)(cid:18)(cid:17)(cid:19)(cid:3) (cid:3)(cid:27) (cid:19)	 (cid:17)(cid:2)  (cid:9)(cid:18)(cid:3)(cid:3)(cid:2)(cid:18) (cid:3) (cid:17)(cid:27) (cid:16)(cid:17)(cid:27)(cid:25) (cid:0)(cid:1)(cid:2) (cid:17)(cid:17) (cid:20)	(cid:2) (cid:2)(cid:7)(cid:2)(cid:2)(cid:9)(cid:2) (cid:9) (cid:9)+ (cid:3)(cid:20)(cid:17)  (cid:17) (cid:1)(cid:2) 		 (cid:7) (cid:1)(cid:2) (cid:2)(cid:9)(cid:2)(cid:3)(cid:22)(cid:2) (cid:3) (cid:1)(cid:16) (cid:2)(cid:7)	 (cid:17)(cid:18) (cid:1)(cid:2) (cid:16)(cid:3)(cid:2) (cid:3)(cid:20)(cid:17)  (cid:17)(cid:2) (cid:17)  (cid:1)(cid:16) (cid:16)(cid:3)(cid:2)Q(cid:17) (cid:17)(cid:18) (cid:16)(cid:3)(cid:2)Q(cid:19)(cid:25) (cid:0)(cid:1)(cid:2)  (cid:3)+ (cid:3) (cid:2)(cid:17)(cid:3)(cid:20) (cid:17) (cid:31)2 J (cid:2)	 (cid:3)(cid:20) (cid:3) %2(cid:19)B (cid:19)(cid:3)(cid:17)(cid:2)(cid:25) (cid:0)(cid:1)(cid:2) (cid:17)(cid:3) (cid:9)	 (cid:3) ;% (cid:7) (cid:1)(cid:2) (cid:2)(cid:18)(cid:2) (cid:17)(cid:18) =6 (cid:7) (cid:1)(cid:2) (cid:2)(cid:9)(cid:2)(cid:3)(cid:22)(cid:2)(cid:25) (cid:0)(cid:1)(cid:2) (cid:2)(cid:17) (cid:7) 	(cid:9)(cid:1) (cid:2)(cid:7)(cid:17)(cid:9)(cid:2) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2)(cid:9)(cid:2) (cid:17)(cid:3)(cid:2) (cid:7) (cid:1)(cid:2) (cid:3)(cid:19)(cid:3) (cid:3)(cid:27)  (cid:2) (cid:17)(cid:9)(cid:9)	(cid:17)(cid:2) (cid:27)  (cid:17)(cid:9)(cid:2) (cid:1)(cid:2) (cid:9)(cid:2) (cid:17)(cid:18) (cid:1)(cid:2)  (cid:3)(cid:20)(cid:1)(cid:2)  (cid:17)(cid:18)  (cid:1)(cid:2) (cid:20)(cid:17)(cid:2) (cid:9)(cid:17)(cid:2)(cid:18)  (cid:1)(cid:2) 821 (cid:18)(cid:2)(cid:3)(cid:20)(cid:25)  (cid:16)(cid:2)(cid:22)(cid:2) (cid:1)(cid:2)(cid:2) (cid:2)	  (cid:2)  (cid:1)(cid:2) (cid:3)(cid:3)(cid:3)(cid:9) (cid:3)(cid:18)(cid:2)< (cid:2) (cid:18)(cid:2)(cid:17)(cid:3) (cid:2)(cid:18) (cid:17)(cid:17) (cid:27)(cid:3) (cid:3) (cid:2)	(cid:3)(cid:2)(cid:18)  (cid:3)(cid:22)(cid:2)(cid:3)(cid:20)(cid:17)(cid:2) (cid:1)(cid:2) (cid:2)(cid:26)(cid:2)(cid:9) (cid:7) (cid:3)(cid:17)(cid:9)(cid:1) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:9)(cid:2) (cid:17)(cid:18)  (cid:17)(cid:18) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:16)(cid:3)    (cid:3)(cid:3) (cid:1)(cid:2) (cid:2)(cid:17)(cid:3)(cid:17)  (cid:2)(cid:2)(cid:18) (cid:7) (cid:1)(cid:2) (cid:9)(cid:1)(cid:2)(cid:2)(cid:25)  (cid:3) (cid:3)(cid:17) (cid:17)   (cid:19)(cid:2)(cid:22)(cid:2) (cid:1)(cid:17) (cid:1)(cid:2) (cid:2)	  (cid:1)(cid:16) (cid:1)(cid:3)(cid:20)(cid:1)(cid:2) (cid:2)(cid:2)(cid:18) (cid:9)(cid:17)(cid:2)(cid:18)  (cid:1)(cid:2) (cid:2)	  (cid:18)(cid:2)(cid:9)(cid:3)(cid:19)(cid:2)(cid:18) (cid:3) $(cid:2)(cid:9)	 (cid:3) 5(cid:25) (cid:0)(cid:1)(cid:3) (cid:3) (cid:18)	(cid:2)  (cid:17) (cid:2) (cid:17)(cid:20)(cid:20)(cid:2)(cid:3)(cid:22)(cid:2) (cid:18)(cid:2)(cid:9)(cid:18)(cid:3)(cid:20) (cid:9)(cid:1)(cid:2)(cid:2) Transient Analysis ‘tran’: time = (0 s −> 5 ns) 0 1 2 3 di[0:1] do[0:1] (cid:1)(cid:2) (cid:0)0(cid:24) (cid:17)(cid:18) (cid:2)(cid:9)(cid:2)(cid:3)(cid:22)(cid:2) (cid:17)(cid:18) (cid:17)  (cid:1)(cid:2) (cid:9)(cid:1)(cid:3)(cid:9)(cid:2) (cid:7) (cid:18)(cid:3)(cid:26)(cid:2)(cid:2) (cid:2)(cid:9)(cid:18)(cid:3)(cid:20) (cid:2)	(cid:2)(cid:18)(cid:2)  (cid:3)(cid:22)(cid:2) (cid:2) (cid:3)(cid:17)(cid:19)(cid:3) (cid:3)(cid:27)(cid:25) (cid:7) (cid:9)	(cid:2) (cid:3) 	 (cid:3)(cid:9) (cid:2)	  (cid:7) (cid:22)(cid:17) (cid:3)(cid:18)(cid:17)(cid:3) (cid:7) (cid:1)(cid:2) 	(cid:9)(cid:1)(cid:3) (cid:18)(cid:2)(cid:3)(cid:20) (cid:3) (cid:2)	(cid:3)(cid:2)(cid:18) (cid:3) (cid:1)(cid:2) (cid:1) (cid:2)(cid:25) 0 1 0 2 0 1 3 0 (cid:17)(cid:26)(cid:23)(cid:26)(cid:26)(cid:8)(cid:26) ;(cid:29)< ;%< ;(cid:26)< (cid:31) ""(cid:21)0(cid:2) (cid:21)(cid:12) (cid:31) (cid:11)(cid:2)(cid:12)  (cid:9)(cid:6)(cid:9)(cid:6) (cid:11)(cid:9)(cid:6)(cid:12) (cid:14)(cid:15) (cid:5)(cid:16) (cid:17)(cid:9)(cid:18)(cid:14) (cid:19) (cid:20)(cid:18)(cid:14)(cid:5)(cid:6)(cid:5) (cid:9) (cid:9)(cid:14)(cid:5)(cid:18)(cid:31)  	(cid:4)(cid:11) 4*(cid:21)(cid:12)(cid:11)(cid:2)* 	(cid:13) (cid:2)(cid:1)(cid:11) (cid:26)(cid:24)(cid:24)%(cid:31) ? (cid:11)(cid:21)(cid:2)(cid:21)  (cid:0)(cid:11)*(cid:1) (cid:20)(cid:14) (cid:18)(cid:21)(cid:12)(cid:21) $ (cid:17)(cid:11)(cid:2)*(cid:12)	* (cid:0)(cid:18)(cid:17)	 (cid:26)(cid:24)(cid:24)(cid:30)@ (cid:26)(cid:24)(cid:24)(cid:30)(cid:31) (cid:31) 	(cid:2)  (cid:31) (cid:0)(cid:11)(cid:1)	(cid:11) (cid:31) (cid:21)(cid:1) (cid:21)(cid:12) 4(cid:31) (cid:21)*(cid:1) (cid:11)(cid:12)(cid:31) (cid:9)(cid:6)(cid:9)(cid:6)	(cid:22)(cid:9)(cid:18)(cid:6) (cid:17)(cid:9)(cid:18)(cid:14) (cid:19) (cid:0)(cid:16)(cid:23)(cid:5)(cid:6)(cid:9)(cid:16) (cid:24)(cid:22) (cid:5)(cid:16) (cid:22)(cid:31) (cid:17)(cid:2)(cid:20)(cid:11) (cid:26)(cid:24)(cid:24)’(cid:31) ;’< (cid:19)(cid:31) ""4 (cid:11)(cid:21)(cid:12) ""(cid:31) (cid:17)(cid:1)(cid:21)(cid:20) 4(cid:31) /(cid:14)0 (cid:21)(cid:12) 4(cid:31) C(cid:21)(cid:7)0 (cid:11)0 ? (cid:17) (cid:17)(cid:2)(cid:20)(cid:21)  (cid:2)(cid:20)  (cid:17)(cid:19) /	(cid:11)@ (cid:2) (cid:6)(cid:28) (cid:0)(cid:11)(cid:24) (cid:30)(cid:31)(cid:31)  (cid:17)(cid:2)(cid:20)(cid:11) (cid:26)(cid:24)(cid:24)(cid:30)(cid:31) ;(cid:30)< (cid:19)(cid:31) ""4 (cid:11)(cid:21)(cid:12) ""(cid:31) (cid:17)(cid:1)(cid:21)(cid:20) 4(cid:31) /(cid:14)0 4(cid:31) C(cid:21)(cid:7)0 (cid:11)0 (cid:21)(cid:12) (cid:31) (cid:21)(cid:11)0(cid:7)(cid:14) ?	 (cid:2) (cid:11)	(cid:18)(cid:21)(cid:2)  (cid:1)(cid:21)(cid:11)	(cid:15)*(cid:12)(cid:2)(cid:20) $ (cid:19)@ (cid:2) (cid:6)(cid:28) !(cid:30)(cid:12) (cid:0)(cid:24)""(cid:22) (cid:31) (cid:29)(cid:24)(cid:25)E(cid:29)(cid:29)+ (cid:21)*(cid:1) (cid:26)(cid:24)(cid:24)+(cid:31) ;+< (cid:31) (cid:15)(cid:31) ""(cid:11)(cid:21) (cid:0)(cid:31) (cid:15)(cid:31) F(cid:2)  (cid:2)(cid:21) (cid:21)(cid:12) ""(cid:31) (cid:31) ""(cid:2)   ?(cid:15)G*(cid:2)(cid:11) (cid:11) $	 (cid:2)(cid:2)(cid:20) (cid:4)(cid:2)(cid:1)  (cid:11)0(cid:11) 	(cid:11)*(cid:12)(cid:11)(cid:12) (cid:26)	(cid:1)(cid:21)(cid:11) (cid:12)	(cid:21) 	(cid:21)(cid:2)  (cid:15)""(cid:18)@ (cid:2) (cid:6)(cid:28) (cid:19) (cid:12)(cid:9) !##! $(cid:18)(cid:23)(cid:9)(cid:18)(cid:15) (cid:19) (cid:22)(cid:5) (cid:18)(cid:19)(cid:18)(cid:5)%(cid:24)(cid:5)(cid:5) (cid:22)	’ (cid:6)(cid:19)(cid:9)(cid:9)(cid:6)(cid:9) (cid:0)(cid:16)(cid:23)(cid:5)(cid:6)(cid:9)(cid:16) (cid:9)(cid:9)(cid:5)(cid:6)(cid:12) (cid:18) ((cid:24) (cid:19)(cid:21)(cid:13)(cid:2)(cid:12)(cid:20)(cid:11) 4 H(cid:17)4 (cid:31) (cid:30)(cid:30)E(cid:25)(cid:24) (cid:0) (cid:11) (cid:29)(cid:27)(cid:27)(cid:29)(cid:31) ;(cid:25)< (cid:18)(cid:31) ""(cid:13)(cid:7)(cid:2) (cid:18)(cid:31) 7(cid:2)(cid:21) (cid:21)(cid:12) 4(cid:31)  (cid:12)(cid:14) ? (cid:21) (cid:21)(cid:14)*(cid:1)	 (cid:1)(cid:2)$ (cid:11)(cid:20)(cid:2)(cid:11) $ (cid:13)(cid:2)	(cid:11)(cid:2)(cid:21)  *	(cid:2)*(cid:21)(cid:2)@ (cid:2) (cid:6)(cid:28) !(cid:30)(cid:12) (cid:0)(cid:24)""(cid:22) (cid:31) (cid:29)(cid:29)(cid:25)E(cid:29)(cid:26)+ (cid:21)*(cid:1) (cid:26)(cid:24)(cid:24)+(cid:31) ;(cid:28)< (cid:17)(cid:31)	(cid:31) (cid:11)(cid:11) (cid:31) (cid:2)  (cid:31) (cid:2) (cid:31) (cid:19)(cid:1) (cid:21)(cid:12)  (cid:31)	(cid:31) C ?4(cid:12)(cid:21)(cid:2)0(cid:11) (cid:11)(cid:4)(cid:7)		*(cid:1)(cid:2) (cid:4)(cid:2)(cid:1) (cid:4)(cid:21)0(cid:11)	$ (cid:21)(cid:2) (cid:11)(cid:2)(cid:21) (cid:2)9(cid:21)(cid:2) *(cid:1)(cid:11)(cid:11)@ (cid:2) (cid:24)(cid:15)(cid:18)	  ((cid:24) (cid:22)(cid:18)(cid:6)	(cid:18) (cid:26)(cid:24)(cid:24)(cid:30)(cid:31) ;(cid:27)< (cid:31) (cid:19)(cid:11) ?(cid:21)((cid:2)	 (cid:21)(cid:11) (cid:2)(cid:11) (cid:2)(cid:11)(cid:12) (cid:14)(cid:11)@ (cid:2) (cid:6)(cid:28) (cid:0),(cid:24) (cid:24)(cid:18)(cid:14) (cid:18) (cid:22)(cid:6)	(cid:9) (cid:22)(cid:19)(cid:9)(cid:9)(cid:6)(cid:9) (cid:29)(cid:27)+(cid:27)(cid:31) ;(cid:29)(cid:24)< F(cid:31) /(cid:21)(cid:2)(cid:13)(cid:2)(cid:12)(cid:20)(cid:11) (cid:0)(cid:15)(cid:12)(cid:6)	 (cid:24)(cid:15)(cid:9)		(cid:22)(cid:12)(cid:18) (cid:9)(cid:6)(cid:9)(cid:6)(cid:31) (cid:1)"" (cid:1)(cid:11)(cid:2) H(cid:2)0(cid:11)(cid:2)(cid:14) $ (cid:21)*(cid:1)(cid:11)(cid:11) H (cid:21)*(cid:1) (cid:26)(cid:24)(cid:24)(cid:24)(cid:31) ;(cid:29)(cid:29)< (cid:31) /(cid:21)(cid:2)(cid:13)(cid:2)(cid:12)(cid:20)(cid:11) (cid:21)(cid:12) (cid:17)(cid:31) 	(cid:13)(cid:11) ?""(cid:11) (cid:21)(cid:14) (cid:2)(cid:11)(cid:2)(cid:2)0(cid:11) (cid:14)(cid:11)		*(cid:1)(cid:2) (cid:2)(cid:11)*(cid:11)* 	(cid:2)(cid:20) (cid:29)	$	’ (cid:12)(cid:21)(cid:21) (cid:11)*(cid:12)(cid:2)(cid:20)@ (cid:2) (cid:6)(cid:28) .(cid:12) (cid:0)(cid:24)""(cid:22) (cid:31) (cid:29)(cid:29)(cid:28)E(cid:29)(cid:26)+ (cid:26)(cid:24)(cid:24)(cid:29)(cid:31) ;(cid:29)(cid:26)< (cid:31) (cid:17)	(cid:1)(cid:11) (cid:21)(cid:12) ?(cid:2)*(cid:2)(cid:11) (cid:2)(cid:11)@ (cid:22)	(cid:18)(cid:6)(cid:5)(cid:18) (cid:19) (cid:12)(cid:9) (cid:0)(cid:22) 0 (cid:31) %(cid:26) (cid:31) (cid:25)(cid:26)(cid:24)E(cid:25)%(cid:28) 	(cid:11) (cid:29)(cid:27)(cid:28)(cid:27)(cid:31) ;(cid:29)%< (cid:31) (cid:17)	(cid:1)(cid:11) (cid:21)(cid:12) (cid:21)(cid:12) (cid:17)(cid:31) (cid:21)(cid:2)(cid:13)(cid:21)(cid:7) ?7(cid:21)I (cid:21) (cid:2)(cid:2)(cid:21)  -$ * @ (cid:2) (cid:6)(cid:9)(cid:9)(cid:16)(cid:18)(cid:14) (cid:19) (cid:12)(cid:9) .(cid:12) (cid:0)(cid:24)""(cid:22) (cid:26)(cid:24)(cid:24)(cid:29)(cid:31) ;(cid:29)’< (cid:31) 7(cid:11)(cid:11)(cid:11)(cid:11) (cid:21)(cid:12) (cid:31) (cid:18)(cid:11) ? (cid:17)	-(cid:20) (cid:11)*(cid:11)*@ (cid:2) (cid:6)(cid:28) !(cid:30)(cid:12) (cid:0)(cid:24)""(cid:22) (cid:31) (cid:27)(cid:28)E(cid:29)(cid:24)+ (cid:21)*(cid:1) (cid:26)(cid:24)(cid:24)+(cid:31) ;(cid:29)(cid:30)< (cid:31) (cid:31) (cid:11)0(cid:2)(cid:11) (cid:21)(cid:12) 7(cid:31) F(cid:31) (cid:18)(cid:13)(cid:11) ?4 (cid:1)(cid:2)(cid:20)(cid:1) (cid:11) 	(cid:2) J(cid:21)(cid:1) (cid:2)(cid:11)	 	(cid:12)(cid:2)(cid:20)(cid:2)(cid:21)  *0(cid:11)(cid:11) (cid:21)(cid:12) *(cid:21) (cid:2)(cid:13)(cid:21)(cid:2) $ (cid:14)(cid:11)		*(cid:1)(cid:2) (cid:11)(cid:2)(cid:20)@ // (cid:6)(cid:9)(cid:9)(cid:16)(cid:18)(cid:14) 0 (cid:22)	(cid:9) (cid:5)(cid:16) (cid:17)(cid:18)(cid:14)(cid:18)(cid:5)  (cid:11)(cid:9)(cid:6)(cid:12)(cid:18)	(cid:9) 0 (cid:31) (cid:29)(cid:30)(cid:26) (cid:31) ’(cid:29)(cid:30)E’(cid:26)+ (cid:21)(cid:14) (cid:26)(cid:24)(cid:24)(cid:30)(cid:31) ;(cid:29)+< (cid:19)(cid:31) (cid:0)(cid:31) 7(cid:21)(cid:14) F(cid:31) (cid:2)	 F(cid:31) 4(cid:31) (cid:31) &(cid:21) (cid:2)K(cid:11) (cid:0)(cid:31) 4(cid:31)  	(cid:20)(cid:1)(cid:11) (cid:31) (cid:21)(cid:12) (cid:18)(cid:31) (cid:31) 7(cid:21)0(cid:2) ?4 (cid:21) (cid:2)(cid:20) (cid:11)*(cid:1)(cid:2)	(cid:11) (cid:21)(cid:12) (cid:2) (cid:19)(cid:17) (cid:2) (cid:11)(cid:11)(cid:21)	 (cid:2) (cid:4)(cid:2)(cid:1) (cid:29) 7(cid:13)(cid:23) (cid:13)(cid:21)(cid:12)(cid:4)(cid:2)(cid:12)(cid:1) (cid:21)(cid:12) (cid:26)(cid:30) (cid:11) 	(cid:2)@ /// 	(cid:5)  (cid:19) (cid:24) (cid:18)(cid:16)	(cid:24)(cid:5)(cid:9) (cid:22)(cid:18)(cid:6)	(cid:18) 0 (cid:31) (cid:26)(cid:27) (cid:31) %’(cid:24)E%’(cid:27) (cid:21)*(cid:1) (cid:29)(cid:27)(cid:27)’(cid:31) ;(cid:29)(cid:25)< (cid:19)(cid:31)  (cid:21) (cid:21)(cid:12) (cid:31) (cid:11) ?(cid:17)(cid:2) (cid:11) *(cid:2)*	(cid:2) (cid:1)(cid:21) (cid:4)(cid:7) $ * (cid:2)	 *(cid:21)(cid:11)(cid:12) (cid:11)(cid:21)@ (cid:2) (cid:6)(cid:28) 2(cid:12) (cid:0)(cid:24)""(cid:22) 0 (cid:31) (cid:29) (cid:31) (cid:29)%(cid:28)E(cid:29)’(cid:27) (cid:15)(cid:15)(cid:15) (cid:19)(cid:17) 4(cid:2)  (cid:26)(cid:24)(cid:24)(cid:24)(cid:31) ;(cid:29)(cid:28)< (cid:31) ""	(cid:12)(cid:11)(cid:7) (cid:17)(cid:31) (cid:17)9*9(cid:11)(cid:21)L(cid:7)(cid:2) (cid:21)(cid:12) (cid:31) &(cid:31)  (cid:21)-(cid:11) (cid:12) ?4 (cid:1)(cid:2)(cid:20)(cid:1)	(cid:11) 	(cid:2) (cid:19)(cid:17) (cid:0)(cid:2)(cid:11)		""(cid:2)(cid:20)(cid:2)(cid:21)  *0(cid:11)(cid:11) 	(cid:2) (cid:2)9(cid:2)(cid:20) (cid:21) &(cid:11)(cid:2)(cid:11) (cid:12)(cid:11) (cid:21)(cid:14)  (cid:2)(cid:11)@ /// (cid:11)(cid:5)(cid:5)(cid:6)(cid:18)  (cid:24) (cid:18)(cid:16)	(cid:24)(cid:5)(cid:9) (cid:22)(cid:18)(cid:6)	(cid:18) 0 (cid:31) %(cid:30) (cid:31) (cid:26)’(cid:24)E(cid:26)’(cid:25)  (cid:11)(cid:13)	(cid:21)(cid:14) (cid:26)(cid:24)(cid:24)(cid:24)(cid:31) ;(cid:29)(cid:27)< &(cid:31) &(cid:21)(cid:1)(cid:21)0(cid:7)(cid:14) &(cid:31) (cid:21)(cid:21)(cid:7)(cid:1)0(cid:7)(cid:14) (cid:21)(cid:12) 4(cid:31) C(cid:21)(cid:7)0 (cid:11)0 ?(cid:0)(cid:4)(cid:21)(cid:12) (cid:11) $	 *(cid:1)(cid:11)*(cid:7)(cid:2)(cid:20) (cid:21)(cid:12) (cid:11) $	(cid:11)*0(cid:11)(cid:14) (cid:2) (cid:11) $	(cid:2)(cid:11)(cid:12) (cid:11)(cid:13)(cid:11)(cid:12)(cid:12)(cid:11)(cid:12) (cid:14)(cid:11)@ (cid:2) (cid:6)(cid:9)(cid:9)(cid:16)(cid:18)(cid:14) (cid:19) (cid:12)(cid:9) /// (cid:9)(cid:5)(cid:18)(cid:5)  34(cid:12)  /(cid:1)(cid:9)(cid:16)(cid:16)(cid:9)(cid:16) ,(cid:5)	 	(cid:11) (cid:9)(cid:5) (cid:24)(cid:15)(cid:9) (cid:29)(cid:27)(cid:27)+(cid:31) ;(cid:26)(cid:24)< (cid:31) (cid:21) (cid:2)9 ?(cid:18)(cid:11)0(cid:2)(cid:11)(cid:4) $ (cid:11)(cid:1)(cid:12) $ (cid:2)(cid:11) (cid:2)(cid:11)0(cid:21)  (cid:11)(cid:21)	(cid:11)(cid:11) (cid:4)(cid:2)(cid:1) (cid:2)*(cid:11)*(cid:12) (cid:11) 	(cid:2)@ (cid:9) (cid:14)(cid:18)(cid:5) 0 (cid:31) ’(cid:29) (cid:31) (cid:29)(cid:25)E%(cid:26) (cid:26)(cid:24)(cid:24)’(cid:31) ;(cid:26)(cid:29)< 4(cid:31) (cid:15)$(cid:11)(cid:12)0(cid:2)*(cid:1) C(cid:31) 4$(cid:11)(cid:7) (cid:19)(cid:31) (cid:17)(cid:11)  (cid:21) (cid:21)(cid:12) M(cid:31) /(cid:2)(cid:7)(cid:4)(cid:7)(cid:14) ?	 (cid:2)	 $(cid:11)	(cid:11)*(cid:14) 9(cid:11)	K(cid:2)(cid:11) (cid:12)(cid:11) (cid:21)(cid:14)	 *(cid:7)(cid:11)(cid:12)  @ /// $5(cid:0) , (cid:24)(cid:17)	(cid:24)(cid:11)(cid:0)(cid:11)/ (cid:22)5(cid:22)$(cid:11)(cid:24) 0 (cid:31) (cid:26)(cid:27) (cid:31) +(cid:25)E(cid:25)(cid:24) (cid:21)	(cid:21)(cid:14) (cid:29)(cid:27)(cid:27)’(cid:31) ;(cid:26)(cid:26)< (cid:0)(cid:31) (cid:18)(cid:21)(cid:1)(cid:7)(cid:11) (cid:21)(cid:12) (cid:31) (cid:21)0(cid:21)(cid:21)(cid:21) ?(cid:0)(cid:1)(cid:11) 	(cid:11) $ (cid:21)(cid:13)(cid:2) (cid:2)9(cid:11)(cid:12) * (cid:12)(cid:11) (cid:21)(cid:14)  (cid:2)(cid:11) $ (cid:1)(cid:11) (cid:12)(cid:2)(cid:20)(cid:2)(cid:2)9(cid:21)(cid:2) $ (cid:1) (cid:2)(cid:11) (cid:2)(cid:11)0(cid:21) @ /// 	(cid:5)  (cid:19) (cid:24) (cid:18)(cid:16)	(cid:24)(cid:5)(cid:9) (cid:22)(cid:18)(cid:6)	(cid:18) 0 (cid:31) (cid:26)(cid:28) (cid:31) (cid:28)(cid:28)(cid:25)E(cid:28)(cid:27)’ 4	(cid:20)	 (cid:29)(cid:27)(cid:27)%(cid:31) ;(cid:26)%< (cid:18)(cid:31) 7(cid:2)(cid:21) (cid:21)(cid:12) (cid:18)(cid:31)   ?4(cid:12)(cid:21)(cid:2)0(cid:11) (cid:14)*(cid:1)(cid:2)9(cid:21)(cid:2)@ (cid:2) (cid:6)(cid:28) (cid:0)(cid:11)(cid:30)(cid:31)(cid:31)(cid:31) (cid:31) (cid:27)%E(cid:29)(cid:24)(cid:29) 	 (cid:14) (cid:26)(cid:24)(cid:24)(cid:24)(cid:31) ;(cid:26)’< (cid:31) (cid:2)(cid:21) ""(cid:31) (cid:2)(cid:2)(cid:11) 7(cid:31) (cid:18)	(cid:11)   (cid:21)(cid:12) 4(cid:31) C(cid:21)(cid:7)0 (cid:11)0 ?(cid:11)(cid:21)(cid:21)	 (cid:13)(cid:2) (cid:2)(cid:14) (cid:2) 74 (cid:12)(cid:11)0(cid:2)*(cid:11)@ (cid:2) !7(cid:12) $ (cid:0)(cid:15)(cid:6)(cid:12)	 ,	 (cid:17)(cid:11)(cid:11)(cid:13)(cid:11) (cid:26)(cid:24)(cid:24)+(cid:31)  /refOut   /wire_b  /wire_a   /wire_b  /wire_a  ) ) V V ( ( V V ) ) V V ( ( V V 2.5 1.5 .5 −.5 2.5 1.5 .5 −.5 0 1 2 time (ns) time (ns) 3 4 5  (cid:2)(cid:20)(cid:31) (cid:29)+(cid:31) (cid:24)(cid:31)(cid:29)(cid:28)µm (cid:11)*(cid:1) (cid:20)(cid:14) 	*(cid:1)(cid:2) (cid:2) (cid:11)(cid:11)(cid:21)(cid:2) (cid:11)	  (cid:19)(cid:17)(cid:2)(cid:18)  F(cid:0)GH(cid:2) (cid:16)(cid:1)(cid:3)(cid:9)(cid:1) (cid:17)  (cid:16)(cid:2)(cid:18) (cid:1)(cid:3)(cid:20)(cid:1)(cid:2) (cid:2)(cid:2)(cid:18)(cid:25) (cid:2) (cid:1)(cid:17) (cid:1)(cid:2) (cid:2)	(cid:2)(cid:9)(cid:2) (cid:7) (cid:18)(cid:17)(cid:17) (cid:17) (cid:1)(cid:2) 		 (cid:7) (cid:1)(cid:2) (cid:2)(cid:9)(cid:2)(cid:3)(cid:22)(cid:2) (cid:9)	 (cid:17)(cid:3) (cid:2) 6 (cid:22)(cid:17) 	(cid:2)< (cid:1)(cid:3) (cid:3) (cid:18)	(cid:2)  (cid:1)(cid:2) (cid:7)(cid:17)(cid:9) (cid:1)(cid:17) (cid:17) (cid:1)(cid:2) F(cid:0)GH(cid:2)  (cid:27) (cid:20)(cid:2)(cid:2)(cid:17)(cid:2) (cid:17) 		 (cid:16)(cid:1)(cid:2) (cid:2)  (cid:3)(cid:2) (cid:3) (cid:1)(cid:3)(cid:20)(cid:1) (cid:17)  (cid:17) (cid:19)(cid:1)  (cid:3)(cid:2) (cid:20)  (cid:16) (cid:1)(cid:2) F(cid:0)GH(cid:2)  (cid:2) (cid:1)(cid:2)(cid:3) (cid:17)(cid:2)(cid:25) (cid:0)(cid:1)(cid:2)(cid:2)(cid:7)(cid:2) (cid:3) (cid:17) (cid:2) (cid:9)(cid:2)(cid:22)(cid:17)(cid:3)(cid:22)(cid:2) (cid:17)(cid:17)(cid:9)(cid:1) (cid:1)(cid:2)  (cid:3)(cid:2) (cid:16)	 (cid:18) +(cid:2)(cid:2) (cid:1)(cid:3)(cid:20)(cid:1) (cid:7) (cid:17)  (cid:20) (cid:17) (cid:3) (cid:2)(cid:2)(cid:18)(cid:2)(cid:18)  (cid:2)	(cid:2) (cid:9)(cid:2)(cid:9)(cid:2)(cid:25)  (cid:1)(cid:2) (cid:2)(cid:29)(cid:17) (cid:2) (cid:1)(cid:16) (cid:1)(cid:2)  (cid:3)+ (cid:3) 	(cid:1)(cid:2)(cid:18)  (cid:1)(cid:2) (cid:2)(cid:29)(cid:2)(cid:2)  (cid:27) (cid:17) (cid:17) (cid:7)	(cid:7)	(cid:9)(cid:9)(cid:2)(cid:25) (cid:0)(cid:1)(cid:2) (cid:17)(cid:18)(cid:22)(cid:17)(cid:17)(cid:20)(cid:2) (cid:7) 	(cid:3)(cid:20) F(cid:0)GH(cid:2) (cid:17)  (cid:2)(cid:3)(cid:18)(cid:2) (cid:3) (cid:1)(cid:2) (cid:3)(cid:19)(cid:3) (cid:3)(cid:27)  (cid:2) (cid:17)(cid:9)(cid:9)	(cid:17)(cid:2) (cid:27) (cid:3) (cid:1)(cid:2) (cid:17)	 (cid:7) (cid:1)(cid:2) (cid:0)0(cid:24) (cid:17) (cid:1)(cid:2) (cid:18)(cid:2)(cid:22)(cid:3)(cid:9)(cid:2) (cid:16)(cid:3)   		 (cid:17) (cid:22)(cid:17) 	(cid:2)  (cid:27) (cid:17)(cid:7)(cid:2) (cid:17)(cid:19)(cid:3)(cid:17)	 (cid:3) (cid:1)(cid:17) (cid:19)(cid:2)(cid:2) (cid:9) (cid:2)(cid:2)(cid:18)(cid:25) (cid:0)(cid:1)	 (cid:1)(cid:2) (cid:2)(cid:9)(cid:2)(cid:3)(cid:22)(cid:2) (cid:9)(cid:17) (cid:20)(cid:2)(cid:2)(cid:17)(cid:2) (cid:17)(cid:9)+(cid:16) (cid:2)(cid:18)(cid:20)(cid:2) (cid:3)(cid:20)(cid:17)   (cid:27) (cid:17)(cid:7)(cid:2) (cid:1)(cid:2) (cid:0)0(cid:24) (cid:3) (cid:2)(cid:17)(cid:18)(cid:27)  (cid:22)(cid:3)(cid:18)(cid:2) (cid:17) (cid:3)(cid:2) (cid:7) (cid:18)(cid:17)(cid:17)  (cid:1)(cid:2) (cid:18)(cid:2)(cid:9)(cid:18)(cid:3)(cid:20)  (cid:20)(cid:3)(cid:9)(cid:25) (cid:28)(cid:1) #(cid:8) 	(cid:9) 1 (cid:17)(cid:18)(cid:3)(cid:9)(cid:17)  (cid:17)(cid:17)(cid:9)(cid:1)  	(cid:9)(cid:1)(cid:3) (cid:9)	(cid:3)(cid:9)(cid:17)(cid:3) (cid:1)(cid:17) (cid:19)(cid:2)(cid:2) (cid:2)(cid:18)(cid:25) (cid:0)(cid:1)(cid:2) (cid:2)	  (cid:1)(cid:16) (cid:3)(cid:3)(cid:20) (cid:18)(cid:2)(cid:22)(cid:2) (cid:2) (cid:3) (cid:2) (cid:7) (cid:1)(cid:3)(cid:20)(cid:1)	(cid:2)(cid:2)(cid:18) (cid:9)	(cid:3)(cid:9)(cid:17)(cid:3) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:19) (cid:9)+ (cid:17)(cid:18) (cid:9)(cid:17) (cid:19)(cid:2) 	(cid:2)(cid:18) (cid:7) (cid:24) (cid:18)(cid:2)(cid:3)(cid:20)(cid:25) 1 (cid:7)	(cid:7)	(cid:9)(cid:9)(cid:2) 821 (cid:18)(cid:2)(cid:3)(cid:20) (cid:1)(cid:17) (cid:19)(cid:2)(cid:2) (cid:3) (cid:2)(cid:2)(cid:2)(cid:18) (cid:17)(cid:18) (cid:2)(cid:2)(cid:18) (cid:17)(cid:18) (cid:17) 	(cid:9)(cid:1)(cid:3) (cid:27)(cid:2) (cid:1)(cid:17) (cid:19)(cid:2)(cid:2) (cid:3)	 (cid:17)(cid:2)(cid:18)(cid:25) (cid:0)(cid:1)(cid:2) (cid:20) (cid:3) (cid:7) (cid:1)(cid:3) (cid:9)(cid:9)(cid:2) (cid:3) (cid:1)(cid:2) (cid:3)(cid:19)(cid:3) (cid:3)(cid:27)  (cid:2)	 (cid:18)	(cid:9)(cid:2) (cid:1)(cid:2) 	(cid:19)(cid:2) (cid:7) (cid:16)(cid:3)(cid:2) (cid:2)	(cid:3)(cid:2)(cid:18) (cid:7) (cid:17) (cid:20)(cid:3)(cid:22)(cid:2) (cid:19)(cid:3)	(cid:17)(cid:2) (cid:9)	 (cid:17)(cid:2)(cid:18)  (cid:17)(cid:27)(cid:9)(cid:1)	 (cid:2)(cid:9)(cid:1)(cid:3)	(cid:2) (cid:2)(cid:2)(cid:9)(cid:3)(cid:17)  (cid:27) (cid:3) (cid:1)(cid:2) (cid:9)(cid:17)(cid:2) (cid:7) 	 (cid:3) (cid:2) (cid:16)(cid:3)(cid:2) (cid:1)(cid:17)+  (cid:1)(cid:2) (cid:1)(cid:3)(cid:20)(cid:1)(cid:2) (cid:19)(cid:3)B(cid:16)(cid:3)(cid:2) (cid:17)(cid:3) (cid:17)  (cid:16)(cid:2)(cid:18) (cid:19)(cid:27) (cid:1)(cid:2) (cid:2)(cid:18) (cid:9)(cid:1)(cid:2)(cid:2)(cid:25) 1  (cid:3)(cid:7) (cid:2) (cid:3)(cid:17)(cid:19)(cid:3) (cid:3)(cid:27) (cid:3) (cid:17) (cid:7)	(cid:18)(cid:17)(cid:2)	 (cid:17)  (cid:3)	(cid:2) (cid:17)(cid:18) (cid:17)(cid:2)(cid:17) (cid:22)(cid:2)(cid:1)(cid:2)(cid:17)(cid:18) (cid:3)  (cid:2) (cid:3)(cid:17) (cid:1)(cid:2) (cid:0)0(cid:24) (cid:18)(cid:2)(cid:3)(cid:20) (cid:9)(cid:17) (cid:2) (cid:27) (cid:1)(cid:3)(cid:20)(cid:1)(cid:2)	(cid:2) 	(cid:3) (cid:0)0(cid:24)(cid:25) (cid:0)(cid:1)(cid:3) (cid:3) (cid:1)(cid:2) (cid:17)(cid:3) (cid:7)(cid:9)	 (cid:7) (cid:7)		(cid:2) (cid:16)+ (cid:3) (cid:18)(cid:2)  (cid:3)(cid:22)(cid:2)(cid:3)(cid:20)(cid:17)(cid:2) (cid:1)(cid:2) (cid:17)(cid:18)(cid:2)	 (cid:26) (cid:19)(cid:2)(cid:16)(cid:2)(cid:2) (cid:17)(cid:2)(cid:17) (cid:17)(cid:18) (cid:2) (cid:3)(cid:17)(cid:19)(cid:3) (cid:3)(cid:27) (cid:3) (cid:1)(cid:2) (cid:2)(cid:18)(cid:2) (cid:17)(cid:18) (cid:17)(cid:3)(cid:9)	 	 (cid:17) (cid:27) (cid:1)(cid:2) (cid:2)(cid:9)(cid:2)(cid:3)(cid:22)(cid:2)(cid:25) (cid:7) (cid:9)	(cid:2) (cid:3)(cid:7) (cid:2) (cid:3)(cid:17)(cid:19)(cid:3) (cid:3)(cid:27) (cid:3) (cid:3)(cid:17) (cid:17) (cid:2)	(cid:2)B(cid:17)(cid:9)+(cid:16) (cid:2)(cid:18)(cid:20)(cid:2)(cid:2) (cid:9)(cid:1)(cid:2)(cid:2) (cid:3) (cid:2)(cid:17)(cid:3) (cid:27) (cid:3) (cid:2)(cid:2)(cid:2)(cid:18)< (cid:17)(cid:7)(cid:2) (cid:1)(cid:2) (cid:0)0(cid:24) (cid:1)(cid:17) (cid:9) (cid:2)(cid:2)(cid:18) (cid:3) (cid:2)(cid:17)(cid:3) (cid:17)(cid:18) (cid:1)(cid:2) (cid:18)(cid:17)(cid:17) (cid:3) (cid:17)(cid:7)(cid:2) (cid:27)  (cid:17)(cid:9)(cid:1)(cid:2)(cid:18) (cid:1)(cid:2) (cid:17)(cid:9)+(cid:16) (cid:2)(cid:18)(cid:20)(cid:2)(cid:2) (cid:3)(cid:20)(cid:17)  (cid:9)(cid:17) (cid:19)(cid:2) (cid:3)	(cid:2)(cid:18)(cid:25) (cid:0)(cid:1)(cid:3) (cid:3) (cid:2)(cid:17)(cid:27) (cid:17) (cid:1)(cid:2) (cid:2)(cid:2)(cid:9)(cid:2) (cid:7) (cid:18)(cid:17)(cid:17) (cid:3) (cid:3)(cid:18)(cid:3)(cid:9)(cid:17)(cid:2)(cid:18) (cid:19)(cid:27) (cid:19)(cid:1) (cid:1)(cid:2) (cid:3)	 (cid:3)(cid:20)(cid:17)  (cid:19)(cid:1) (cid:1)(cid:3)(cid:20)(cid:1) (cid:17)(cid:18) (cid:1)(cid:2) (cid:17)	 (cid:7) (cid:1)(cid:2) (cid:0)0(cid:24) (cid:3)(cid:7) F(cid:0)GH(cid:2) (cid:17)(cid:2) 	(cid:2)(cid:18)(cid:25) 1 (cid:2) (cid:7)(cid:17)  (cid:17)(cid:18) (cid:18)(cid:2)(cid:17)(cid:3) (cid:2)(cid:18) (cid:17)(cid:17) (cid:27)(cid:3) (cid:7) (cid:1)(cid:2) (cid:17)(cid:18)(cid:2)	(cid:26) (cid:26)(cid:2)(cid:2)(cid:18) (cid:19)(cid:27) (cid:1)(cid:3) (cid:2)(cid:9)(cid:1)(cid:3)	(cid:2) (cid:16)(cid:3)   (cid:19)(cid:2) (cid:17) (cid:7) (cid:1)(cid:2) (cid:22)(cid:2)(cid:17)   (cid:2)(cid:2)(cid:17)(cid:9)(cid:1) (cid:2)(cid:26)  (cid:1)(cid:2) (cid:2) (cid:3)(cid:17)(cid:19)(cid:3) (cid:3)(cid:27) (cid:7) (cid:1)(cid:2) (cid:9)(cid:1)(cid:2)(cid:2)(cid:25) (cid:1)(cid:2) (cid:7)		(cid:2) (cid:2)(cid:2)(cid:17)(cid:9)(cid:1) (cid:3)(cid:22) (cid:22)(cid:2) (cid:26)	(cid:9)(cid:1)(cid:3) (cid:17) (cid:3)(cid:9)(cid:17)(cid:3) (cid:17)(cid:18) (cid:17) (cid:2) (cid:1)	(cid:20)(cid:1) (cid:17)(cid:17) (cid:27)(cid:3) (cid:7) (cid:1)(cid:2) (cid:3)(cid:17)(cid:9) (cid:7) (cid:2)(cid:17)(cid:17)(cid:19)(cid:3) (cid:3)(cid:27)          "
Enabling Technology for On-Chip Interconnection Networks.,"As we enter the era of many-core processors and complex SoCs, on-chip interconnection networks play a dominant role in determining the performance, power, and cost of a system. These networks are critically dependent on a number of underlying technologies: channel, buffer, and switch circuits, router microarchitecture, flow-control and routing methods, and network topology. Too often on-chip networks are built in a naive manner using a ring or mesh topology and standard cell methodology. Compared to this approach, optimized circuits can reduce power by an order of magnitude and an optimized topology can give an additional factor of two to three in area and power efficiency. This talk can explore key enabling technologies for on-chip networks giving a number of examples and identifying opportunities for future research","NOCS 2007  Keynote 1  Enabling Technology for On-Chip Interconnection Networks  Bill Dally  Bell Professor and Chair of CS, Stanford University  Abstract:     As we enter the era of many-core processors and complex SoCs, on-chip interconnection networks  play a dominant role in determining the performance, power, and cost of a system. These networks are  critically dependent on a number of underlying technologies: channel, buffer, and switch circuits, router  microarchitecture, flow-control and routing methods, and network topology. Too often on-chip  networks are built in a naive manner using a ring or mesh topology and standard cell methodology.   Compared to this approach, optimized circuits can reduce power by an order of magnitude and an  optimized topology can give an additional factor of two to three in area and power efficiency. This talk  will explore key enabling technologies for on-chip networks giving a number of examples and  identifying opportunities for future research.  Bio:     Bill Dally is the Willard R. and Inez Kerr Bell Professor of Engineering and the Chairman of the  Department of Computer Science at Stanford University. Bill and his group have developed system  architecture, network architecture, signaling, routing, and synchronization technology that can be  found in most large parallel computers today. While at Bell Telephone Laboratories Bill contributed to  the design of the BELLMAC32 microprocessor and designed the MARS hardware accelerator. At Caltech  he designed the MOSSIM Simulation Engine and the Torus Routing Chip which pioneered wormhole  routing and virtual-channel flow control. While a Professor of Electrical Engineering and Computer  Science at the Massachusetts Institute of Technology his group built the J-Machine and the M-Machine,  experimental parallel computer systems that pioneered the separation of mechanisms from  programming models and demonstrated very low overhead synchronization and communication  mechanisms. At Stanford University his group has developed the Imagine processor, which introduced  the concepts of stream processing and partitioned register organizations. Bill has worked with Cray  Research and Intel to incorporate many of these innovations in commercial parallel computers, with  Avici Systems to incorporate this technology into Internet routers, co-founded Velio Communications to  commercialize high-speed signaling technology, and co-founded Stream Processors,  Inc. to  commercialize stream processor technology. He is a Fellow of the IEEE, a Fellow of the ACM and has  received numerous honors including the IEEE Seymour Cray Award and the ACM Maurice Wilkes award.  He is chairman of Stream Processors and on the board of directors of Portal Player. He currently leads  projects on high-speed signaling, computer architecture, network architecture, and programming  systems. He has published over 170 papers in these areas and is an author of the textbooks, Digital  Systems Engineering and Principles and Practices of Interconnection Networks.              "
NOC-centric Security of Reconfigurable SoC.,"This paper presents a first solution for NoC-based communication security. Our proposal is based on simple network interfaces implementing distributed security rule checking and a separation between security and application channels. We detail a four- step security policy and show how, with usual NOC techniques, a designer can protect a reconfigurable SOC against attacks that result in abnormal communication behaviors. We introduce a new kind of relative and self-complemented street-sign routing adapted to path-based IP identification and reconfigurable architectures needs. Our approach is illustrated with a synthetic set-top box, we also show how to transform a real-life bus-based security solution to match our NOC-based architecture","NOC-centric security of reconﬁgurable SoC Jean-Philippe Diguet, Samuel Evain, Romain Vaslin, Guy Gogniat, Emmanuel Juin Université de Bretagne Sud / CNRS, LESTER lab., France Jean-Philippe.Diguet@univ-ubs.fr Abstract This paper presents a ﬁrst solution for NoC-based communication security. Our proposal is based on simple network interfaces implementing distributed security rule checking and a separation between security and application channels. We detail a fourstep security policy and show how, with usual NOC techniques, a designer can protect a reconﬁgurable SOC against attacks that result in abnormal communication behaviors. We introduce a new kind of relative and self-complemented street-sign routing adapted to path-based IP identiﬁcation and reconﬁgurable architectures needs. Our approach is illustrated with a synthetic Set-Top box, we also show how to transform a real-life bus-based security solution to match our NOC-based architecture. 1 Introduction As a key issue for multiprocessor chip design, a lot of attention is being payed to communications architectures. With properties such as scalability, bandwidth increase and high-level formalization networks on chip have emerged as interesting alternatives to bus hierarchies. Many research efforts are still necessary, such as the management of QoS in the context of chip with heterogeneous clocks implementing applications with dynamic behaviors, however NoC could introduce new security challenges in future embedded systems. A multiplication of communication links within chips also means an increase of doors to program instructions and sensitive data. In this paper we show how this drawback can be turned into opportunities to eﬃciently implement security policy associated to communication controls. First, we introduce the context of our solution in the next section. In section 3, we present the main diﬀerences between recent work in bus-based security and our solution to face considered attacks. In section 4, we describe our architecture model for Secure Network Interface (SNI) and our new routing algorithms for path-based identiﬁcation and backward path computation. Section 5 illustrates our solution with a Multimedia/Set-top box case study, we also apply our approach to a bi-processor example from [9]. Finally we conclude about security cost and perspectives in terms of monitoring. 2 Communication security 2.1 Model of threat Authors of embedded devices attacks have diﬀerent motivations and frameworks to proceed. End users may try to extract, from the ﬁrmware of their own device, information for hacking licenses or network accesses. Hackers aim to modify behaviors of remote systems for diﬀerent reasons and operators may be interested by reading personal data for marketing or licensing purposes. All these attacks can be initiated through abnormal communications. Considering SOC complexity such behaviors can also be caused by bugs (HAL errors, hazardous pointers), thus security techniques can be used also for improving reliability and bug detection. In this paper we don’t address communication conﬁdentiality that can be handled with complementary ciphering techniques [11]. We don’t either consider physical attacks such as fault injection or side channel attacks, but our solution makes possible the implementation of counter-measures based on monitored reconﬁguration. We focus our study on entity authenticity and data integrity, both attacks can be identiﬁed as abnormal communication behaviors. These attacks may be classiﬁed in the following three categories : Hijacking Hijacking is a write access in the secure area in order to modify the behavior or the conﬁguration of the system, it also includes buﬀer overﬂow and internal registers reconﬁguration. Extraction of secret information This second aspect consists in read accesses to data stored in secure targets. The stolen information can be sensitive data (e.g. encryption keys), instructions from critical programs, IP conﬁguration registers and so on. Denial of service This kind of attacks aims to bring down the system performances. The network over utilization downgrades the operability of the system. We can distinguish four kinds of attack scenarios. The last three ones are speciﬁc to NoCs and can occur if NI are programmable or in case of multi-NoC systems with secure and unsecure areas. Replay: intentional repeated requests imply wastes of bandwidth and cause higher latency transfers in the system resulting in deadline misses for instance. In the context of NoC new kinds of denial of service attacks may occur in order to overcrowd links with useless communications : Incorrect path: it consists in introducing in the network a packet with erroneous paths with the aim to trap it into a dead end. The body of the trapped packet takes some channels and makes them unavailable for the others valid packets. Dead lock: it means the use of packets with paths that intentionally disrespect deadlock-free rules of the routing technique with the intent to create deadlocks in the network. This leads to the contention of the channel and consequently of a part or the entire NoC. Livelock: this is the introduction of a packet that can’t reach its target and stay turning inﬁnitely in the network, causing a waste of bandwidth, latency and power. 2.2 Claims for reconﬁguration Security is usually considered as a software issue based on ﬁxed and so identiﬁed hardware targets. However we observe that needs for hardware reconﬁguration are rising, this evolution is mainly driven by productivity gains. The ﬁrst point is the hardware and software ﬁrmware upgrades on reconﬁgurable SoC for improving performances and implementing new standards and applications with dedicated hardware IP. The second point is relying on the optimization of design costs for hardware bug ﬁxing as already done for software. Finally, reconﬁguration is also a convenient way for chip reuse with a specialization of a single chip for diﬀerent system integrations. Thus, we have considered a solution that may be implemented on reconﬁgurable SoC, our experiences are based on Xilinx Virtex2pro series. Both hardware for IP and NoC and Software (SNI) partial reconﬁgurations can be considered in the context of NoC-based SoC. Our secure reconﬁguration protocol depicted in Fig.1 is described in section 3.2. 3 From Bus to enhanced NoCbased solution 3.1 Relative work Few work exist in the domain of communicationbased security for SoC out of speciﬁc secure processor cores such as ARM/Trustzone [2] or Stanford/XOM [5]. Design of security-aware communication architectures has been introduced by [9]. Their solution named SECA relies on both a single Security Enforcement Module (SEM) and a Security Enforcement Interface (SEI) for each slave device connected to the bus. Their solution which has been demonstrated through the AMBA bus can be used to implement features such as (a) addressbased protection, (b) data-based protection and (c) sequence-based protection. Such a solution is interesting since it can detect a large number of attacks from access control violation up to slave device conﬁguration corruptions. However there are still some limitations that should be considered when dealing with multiprocessor systems where a large number of resources will have to exchange data in parallel. Even if some security features, such as peripheral register write protection, are distributed within the SEI most of the monitoring is centralized and performed by the SEM especially the addressbased protection which is based on a ternary content addressable memory (TCAM). Such an approach leads to an exponential increase of the cost when adding new slave devices on the bus and strongly limits the scalability. When one device is added to the bus, the whole TCAM needs to be modiﬁed. A NoC based solution mitigates this cost since the address-based protection can be performed within network interfaces (NI) that are intrinsic to NoC implementations. Thus, Address-based protection can be distributed and so performed in parallel like data transfers. In the domain of NoC, ARTERIS [3] proposes the introduction of Firewall relying on ﬁxed communications schemes and software security based on a secure boot conﬁguration. In [12] we present an overview of NoC security issues. Gebotys, in [6], extends classical key exchange protocols for IP identiﬁcation within NoC. 3.2 NoC-centric proposal We consider reconﬁgurable architectures based on 2.2 motivations. Then our ob jective is not exactly to secure NoC-based communications, but rather to select a NoC in order to implement secure communications based on the range of opportunities it offers. They are mainly: spatial parallelism enabling path-based identiﬁcation and distributed and dedicated implementation of security functions. Our solution for a secured NoC regarding abnormal traﬃc detection is based on the following ﬁve points: I. Single, centralized, dedicated and secured master IP. The Security and Conﬁguration Manager (SCM) is in charge of safe hardware and communication conﬁgurations and counter-attacks monitoring. This is the most sensitive resource, so its conﬁguration must be based on an initial boot and secured network communications for online updates. The SCM can be for instance a usual processor core dedicated to this task. II. Enhanced Network Interface for access control. Secure Network Interface (SNI) handles in a distributed and dedicated way the following attack symptoms: (i) Denial of service, (ii) Unauthorized Read Access (information extraction) and Unauthorized Write Access (Hijacking). The main points are ﬁrstly the use of NoC scalability, basically the large number of I/Os that oﬀer NoC enables to personalize and dedicate access control to speciﬁc and small memory areas (memory partitions and peripheral conﬁguration registers). Moreover NI proceeding delays can be usefully exploited for implementing security control in parallel with data-ﬂow. The second point is that we don’t need costly probes in router [4] since all NoC traﬃcs can be analyzed thought Secured NI. As a consequence to scalability use, the NI cost may be minimized. III. Two distinct (virtual) networks for a separation between application and security data. Two virtual channels (VC) are implemented, the ﬁrst one is dedicated to elementary best-eﬀort communications (BE) for inter IP transactions and the second one is a priority best-eﬀort (PBE) channel dedicated to SNI / SCM communications (security monitoring and conﬁguration). For deadlock protection, the transport layer implements connexionoriented transactions with end to end ﬂow control and the network layer implements source-routing preventing from the introduction of incorrect paths. Note that security messages are very short, typically one ﬂit is enough to transmit a denial of service alert or an unauthorized access attempt. Initially our intention was to use PBE channels for End-To-End ﬂow controls, namely for credits exchanges which correspond to inter-SNI communications but this idea has been left out in order not to create potentially malicious PBE traﬃc besides SCM communications. IV. Four steps access control. The veriﬁcation of access rights and the detection of denial of service can be simpliﬁed compared to bus solution thanks to scalability NoC property. Basically a NoC provides a distributed address control that enables to separate IP identiﬁcation (i.e. path) and local short address checking while bus-based approaches are centralized and lead an exponential cost. 1. Overﬂow checking. On the emitter SNI side, the ﬁrst packet contains the message size, namely the number of words to be transmitted within the upcoming transaction. If the bound is reached and the FIFO is not empty then the FIFO is ﬂushed and the transaction ended. Moreover the packet length is limited to the input FIFO size and the network layer control bits of the last ﬂit are automatically set by the NI controller. 2. Path Based Identiﬁcation. An IP identiﬁcation can be based on tags inserted in the packet header or payload, this tag can be ﬁxed if an ASIC solution is considered. However intrinsic NOC properties provide a simpler solution, basically paths can be used to identify request and response communications. Our solution is based on this property that enables hardware reconﬁguration and does not need extra-information like the ID or the backward path to the remote IP. Moreover we implement a new routing algorithm that simpliﬁes path coding and backward path computation, this point is detailed in section 4.2. Thus the remote IP (request or response) is ﬁltered according the Read, Write rights stored in the acceding SNI and the path instruction extracted from the packet header. 3. Local Address checking. From the payload of the ﬁrst transaction packet is extracted the local base address and the message size. These values are compared to bounds stored in the SNI. The veriﬁcation is limited to a reduced number of local address bits. 4. Statistics. Alerts are transmitted to the SCM when traﬃc bounds are out of predeﬁned normal behavior bounds. Available credits allocations are accumulated and compared to upper and lower bounds over a given observation period. V. Conﬁguration protocol and SNI conﬁguration. All security policies are ineﬀective if the system can be conﬁgured by the attacker, so the (re)conﬁguration protocol and order is a key issue. In software-oriented security, this point is guaranteed by boot conﬁguration stored in an embedded ciphered memory [7]. In the case of NoC on reconﬁgurable SoC, we distinguish four phases. INIT: SOC hardware initial safe conﬁguration. At the boot time, the complete hardware initial conﬁguration is loaded from an external ciphered memory. The reset conﬁguration depends on the reconﬁgurable architecture. In case of a tile-based architecture built around an hard NoC core only IPs are conﬁgured but if we consider a FPGA, then both IP and NoC cores are conﬁgured at reset time. In such a case, a dedicated decryption core is available to run this task and a speciﬁc strategy can be elaborated to secure this ﬁrst step as described in [10]. In this paper we consider this stage as already performed. SNI: initial security conﬁguration. In the very ﬁrst conﬁguration two kinds of communications are authorized, the ﬁrst one is a BE read operation from SCM to a ciphered memory containing SNI conﬁgurations according to IP access policy and secondly PBE SCM / SNI communications. The available channels are used by SCM to manage a SW conﬁguration of SNI (an example is given on Fig.8). Note that after the SNI step is achieved, a new conﬁguration may be dynamically changed only by the SCM since reconﬁguration control resource (e.g. ICAP) is only connected to SCM. RUN: runtime monitoring based on the current hardware and software conﬁguration. If alerts are detected, diﬀerent strategies can be implemented and lead to SNI software reconﬁgurations or to hardware reconﬁguration. DPR: dynamic and partial reconﬁguration. Based on monitoring data (adaptation to traﬃc), Attack detection, or Upgrades requirements the SCM can decide at run-time partial hardware reFigure 1: Reconﬁguration protocol conﬁguration. These reconﬁgurations are available as bitstreams initially available or downloaded from a secured network connection. It has a consequence on the conﬁguration of the SNI connected to the SCM which is slightly diﬀerent to IP ones. Actually, it needs only one virtual channel, which can be connected to the BE VCs during reconﬁguration in order to access to conﬁguration memories whereas it is only connected to priority VCs at runtime. Thus they are no possible connections with other IPs. This point is described in section 5.1. Online reconﬁguration management based on network download a is not a trivial issue from a security perspective, a dedicated protocol still remains to be speciﬁed. According to FIPS 140-2 level 3 recommendations, the ultimate solution would consist in a physical separation between secure and unsecure network ports and so to a SCM with dedicated network connection capabilities. Such an implementation would be safe but costly, another one can be based on a pooling procedure that enables the SCM to get ciphered tags authenticating new hardware upgrades from a ciphered memory. This last solution requires four conditions: i) a new BE Read access from SCM to a ciphered memory, ii) a new BE Write access to network processor FIFO to launch secured bitstream downloads, iii) a safe network link (e.g. SSL) and iv) a certiﬁcation procedure [1] to authenticate upgrade requirements. 4 Architecture models 4.1 SNI Standard NI Basic design of standard NI is quite simple, since it is based on a single channel with end to end ﬂow controls based on credits exchanges between IPs (see Fig.2-a)). The design is based on a restriction, which is that transactions interleaving is not allowed at emission. Namely the API developed in C for Xilinx microblaze doesn’t permit to initiate a transaction before the end packet of the previous transaction is loaded into the FIFO. At reception, packets from diﬀerent transactions can be interleaved since message sizes are loaded at transaction initialization in SNI registers. These design decisions result in two ma jor changes compared to our previous work where different FIFOs were associated to the diﬀerent channels. First, there are only one Input FIFO and one Output FIFO, it means that credits and FIFO status are checked during the NI access protocol. Namely, a new transaction is initiated only if the input FIFO is empty and if credits are available. Moreover the read/write address is transformed into the associated path in packet header and local address stored in the ﬁrst packet. At reception, a simple address generator unit (AGU) produces the local address by summing basis address and oﬀset. SNI, IP conﬁguration SNI architecture is conﬁgured for IP as depicted in Fig.2-b). Compared to standard conﬁguration, we add a second high priority Best Eﬀort channel (PBE) for monitoring data. Security is based on a complete separation of channels, there are no connexion between IP and BPE FIFOs, which are reserved exclusively to SNI communications. It results ﬁrstly in the implementation of a decoder and arbiter for channel selection. Secondly counters are added for statistics updates. Thirdly, a couple of registers for oﬀset bounds checking are implemented for the survey a each inter-IP communication. Finally, the NI controller is enhanced with security controls for oﬀset checking, path identiﬁcation and alert message generation based on statistics. On the one hand as security control can be performed in parallel with data-ﬂows there is no impact on processing delay, on the other hand the cost increased is mainly dominated by the introduction of a second virtual channel that also strongly impacts the router costs. SNI, SCM conﬁguration SNI conﬁguration for SCM/NOC interface is speciﬁc since the SCM is the only IP allowed to communicate with SNIs as the only trusted IP. These communications can be performed sequentially, so only one couple of input/output FIFO is necessary as described in Fig.2c). The SCM can access to SNI local registers in order to modify VC ID inserted in packet headers and switch between PBE and BE channels. The SCM uses BE VC to access to conﬁguration memFigure 2: SNI conﬁgurations ories in order to get SNI conﬁguration for all IPs, the SCM uses the PBE VC to conﬁgure IP SNIs. At run time, SCM SNI is conﬁgured with PBE VC to prevent any intrusion from remote IP and to transmit security alerts to the SCM. Finally we observe that oﬀset and access conﬁguration registers are useless and can be removed. In section 5.1 we detail access along with the diﬀerent phases of conﬁguration protocols. SCM At reset time, SNIs are conﬁgured in such a way that no IPs communication are allowed except for the SCM that can access to its own program and data memories, and to conﬁguration memories. SCM is a processor dedicated to security control, it is in charge of two main tasks. The ﬁrst one is devoted to SNI conﬁgurations, the second Figure 3: The separate use of virtual channels one is active at run time and consists in listening alert messages from SNIs, the reaction to attacks is decided by software designers. It can consist in closing aﬀected SNIs and transmit an alert through a secured network communication. Another role of counter-measure management can be assigned to the SCM, against side-channels attacks. 4.2 Optimized path coding for identiﬁcation and reconﬁguration Concept We propose a simple and low cost solution to allow a receiving SNI to identify the sending SNI of received packet. This one is based on a smart use of routing instructions in packet header. Classical X-Y and street-sign routing coding techniques don’t preserve routing information, so we propose a routing technique [8] that we call selfcomplemented path coding (SCP). The choice of the output port in a router is given by the turn number in counter-clockwise from the considered input port (Fig. 4). This feature induces some key improvements detailed hereafter. The instruction is relative to both input and output ports considered. It allows preserving routing instruction information in packet header to identify the sender from the destination. Moreover this information constitutes not only a single key but also the backward path compliant with reconﬁgurable architectures where master IP location can be modiﬁed. It is a reliable identiﬁcation key because a sender can’t use the routing instructions of another one to usurp its identity because its topological position is diﬀerent and requires an other path and so other routing instructions. Architecture Packet header is made of an instruction ﬁeld constituted of instructions for crossed routers on the path of this packet to travel from the source to the destination. Moreover after proceeding instructions, routers compute backward instructions and shift instructions as indicated on Fig. 5. Note that a small ﬁeld indicates the number of remaining instructions to proceed. This one is decreased by each crossed router. It allows router to detect a live-lock path (an inﬁnite loop imprisoning packet in the network conducing to a denial of service). A zero count must occur only at destination SNI. Thus, if before router processing it is detected, it is abnormal and conducts the router to delete the entire packet. Destination SNI needs Figure 6: Inversion technique Step 3 At run time, alerts are transmitted to the SCM through the PBE VC as indicated in Fig.9. If no abnormal behavior is detected, these channels remain unused or if no explicit read access for monitoring are programed in SCM. It means, that PBE impact on BE traﬃc is null out of any trouble detection. Step 4 In case of online reconﬁgurations for ﬁrmware updates or bug ﬁxing, a secure connection can be established by the network processor under the control of the SCM. Diﬀerent possibilities may be implemented. In this example we base our implementation on the four conditions described in 3.2-V., thus the SCM BE communications are limited to a Read access to a ciphered memory and a write access to the network processor. Thus, based on a pooling, processed with a period that depends on the application characteristics (e.g. every day for a set-top box), the SCM switches to BE trafﬁc mode and reads an upgrade ﬂag from (ciphered) Data Memory 2. After an authentication procedure, if the ﬂag reveals that an upgrade is available then the SCM writes a command to the input buﬀer of the network processor to launch a safe new conﬁguration download. Otherwise, the SCM returns to the run state. When a new conﬁguration is available (after upgrade, for adaptation or for security reasons), the SCM can load new contexts and partially reconﬁgure the SOC through the ICAP controller. Fig.10 gives the resulting partial reconﬁguration with new DSP and Turbo decoder IPs. 5.1.2 Preliminary results We have estimated the cost of security while considering a NOC with standard NIs and a second one implementing SNIs. Results have been obtained with our NOC CAD tool (µSpider), the VHDL hierarchy of ﬁles is produced according to designer parameters. The selected conﬁguration is the following, Data width is 32 bits, each link between connected routers is composed of two unidirectional opposite links, Buﬀer size is 8 words in input queue of routers for the message virtual channel, and only 4 words for secure virtual channels. This last one requires minor buﬀer size because packets are short and contentions between packet on secure VC are short and rare. The security controller used is still elementary, but we have added required resources Figure 8: NoC conﬁguration, monitoring links in order to estimate quite accurately the ﬁnal cost. Synthesis has been performed with Xilinx/ISE tool. Tab. 2 shows a comparison between a NoC with and without secure features, results include (S)NI, links and routers. As also observed in processorbased work, security has a price, we note that the cost increase is around 45%. This is mainly due to the additional VC for secure transmissions. NoC FPGA slices NI based 4x3 2DMesh SNI based 4x3 2DMesh 23818 34568 Table 2: Security overhead This main overhead is due to routers since a router with two VCs is nearly two times the cost of router with a single VC. The cost observed in NI for additional FIFO and registers is not really signiﬁcant. It means that minimizing routers cost is an important point, that we address with our SCP routing technique. Our method doesn’t increase the architecture cost since it is close to the classical street sign and consists only of the replacement of the forward instruction by the reverse instruction and bit inversions by crossing wires. Moreover, it Figure 9: NoC Run-time conﬁguration Figure 10: Dynamic IP and NoC reconﬁguration enables to reduce routing instruction ﬁeld to the minimum. To travel across the network in diagonal with a shortest path, 6 routing instructions are needed. If equal size would be chosen, the width of the instruction ﬁeld would be 6x3, so 18 bits. With our reduction coding, this width is 6x2, 12 bits, or in worst case 4x2bits + 2x3bits = 14 bits. This coding reduction allows reducing instruction ﬁeld cost in packet headers, or permits longer paths. Routers with many ports require more bits for instruction coding, and so reduce path length but oﬀer also more output port choices and so opportunities to ﬁnd a path. Finally note that our VC implementation doesn’t introduce any delay overhead since VC (de)coding is performed in parallel with other header bit processing. 5.2 NEC case study To illustrate our solution we have mapped to a NoC the DRM architecture for portable playback of multimedia content application given in [9] with a bus-based security implementation. In this example two processors CPU A and CPU B (crypto processor) are required. Processors A and B have diﬀerent rights regarding accesses to the memory mapping. The resulting mapping is described in Fig.11 where N/RW/W/R respectively mean not accessible, read-write, write only and read only. We observe that the previous NoC can be reused for this example. SNIs have been connected to Flash, ROM and peripherals, which have homogeneous access rules. This is not the case for the SDRAM which is divided in ﬁve areas having distinct access rules. Diﬀerent solutions may be chosen regarding SDRAM SNIs. Based on our solution, we have used diﬀerent SNIs for each area which enables a clear security policy for each memory access. An opposite solution would consist in using a single SNI considering that SDRAM accesses can not be performed simultaneously as it is with the initial architecture. The ﬁrst solution means an increase of NoC cost mainly in terms of routers compared to a solution using a single SNI implementation. However, it also provides a very simple solution if the NoC is already available, in such a case SNI security rules are extremely basic. The availability of small distributed scratch pad memories can also justify this kind of implementation. The second choice ﬁts if a NoC architecture can be decided, however the SNI security must distinguish ﬁve areas. Actually the best solution depends on performance constraints "
QNoC Asynchronous Router with Dynamic Virtual Channel Allocation.,An asynchronous router for quality-of service NoC is presented. It combines multiple service levels (SL) with multiple equal-priority virtual channels (VC) within each level. The VCs are assigned dynamically per each link A different number of VCs may be assigned to each SL and per each link The router employs fast arbitration schemes to minimize latency,"QNoC Asynchronous Router with Dynamic Virtual Channel Allocation  Rostislav (Reuven) Dobkin, Ran Ginosar and Israel Cidon  VLSI Systems Research Center, Technion—Israel Institute of Technology, Haifa 32000, Israel  rostikd@tx.technion.ac.il  Abstract  An asynchronous router for Quality-of service NoC  is presented. It combines multiple service levels (SL)  with multiple equal-priority virtual channels (VC)  within each level. The VCs are assigned dynamically  per each link. A different number of VCs may be  assigned to each SL and per each link. The router  employs fast arbitration schemes to minimize latency.   1. Introduction  Large systems on chip (SoC) are interconnect  limited due to high area, power and delays [1].  Networks on Chip (NoC) were proposed as a solution  for the SoC interconnect problem [2]. Quality-ofService NoC (QNoC) [3], designed to enable GALS  systems with multiple clock domains,  is best  implemented  as  an  asynchronous  circuit  [4],  eliminating en-route resynchronizations.  QNoC intra-SL packet transfer preemption causes  stalls in routers on the preempted packet route.  Employing VCs for each SL will allow better  utilization of the router output ports (OPs) and links.   Previously published asynchronous routers explore  only one dimension of output port sharing, consisting  of a set of either prioritized or non-prioritized VCs. In  this paper we explore both dimensions, providing both  multiple priority and same-priority sharing.  2. QNoC Router with VCs  We employ dynamic VC allocation within each SL.  The number of VCs can change among the output and  input ports of the same router and among different SLs.  A flit entering the router through one of the MSLIPs (Figure 1) is sent to the appropriate input port (IP)  VC according to VC and SL identification (steps 1 and  2). At this point input VC and SL information are  peeled off the flit. In step 3 the IP computes the OP  address and applies to the VC Admission Control  (VCAC) for output VC assignment. In step 4, the  VCAC assigns one of the output VCs to the requesting  packet. The flits are then fed into the corresponding  VC in the OP. Once there, the flit competes with other  flits from the other VCs of the same SL. The VC  arbiter selects a flit from one of the output VCs (Step  5). The flit subsequently arrives at the last stage (Step  6), where it is arbitrated according to priority (SL).   Five-port QNoC router with four SLs, two VCs for  each SL and 8-bit flit width has cell area of 1,345mm2  and throughput of 103Mflits/s (in 0.35µm technology).  Full details are provided in [5].  "
Region-Based Routing - An Efficient Routing Mechanism to Tackle Unreliable Hardware in Network on Chips.,"The design of scalable and reliable interconnection networks for system on chips (SoCs) introduce new design constraints not present in current multicomputer systems. Although regular topologies are preferred for building NoCs, heterogeneous blocks, fabrication faults and reliability issues derived from the high integration scale may lead to irregular topologies. In this situation, efficient routing becomes a challenge. Although table-based routing allows the use of most routing algorithms on any topology, it does not scale in terms of latency and area. In this paper we propose the region-based routing mechanism that avoids the scalability problems of table-based solutions. From an initial topology and routing algorithm, the mechanism groups, at every switch, destinations into different regions based on the output ports. By doing this, redundant routing information typically found in routing tables is eliminated. Evaluation results show that the mechanism requires only four regions to support several routing algorithms in a 2D mesh with no performance degradation. Moreover, when dealing with link failures, our results indicate that the mechanism combined with the segment-based routing algorithm is able to pack all the routing information into eight regions providing high throughput. The paper provides also a simple and efficient hardware implementation of the mechanism requiring only 240 logic gates per switch to support eight regions in a 2D mesh topology","Region-Based Routing: An Efﬁcient Routing Mechanism to Tackle Unreliable Hardware in Network on Chips ∗ J. Flich, A. Mejia, P. L ´opez, J. Duato Dept. de Inform ´atica de Sistemas y Computadores Universidad Polit ´ecnica de Valencia P.O.B. 22012, 46022 - Valencia, Spain E-mail: {jﬂich,andres,plopez,jduato}@gap.upv.es Abstract The design of scalable and reliable interconnection networks for System on Chips (SoCs) introduce new design constraints not present in current multicomputer systems. Although regular topologies are preferred for building NoCs, heterogeneous blocks, fabrication faults and reliability issues derived from the high integration scale may lead to irregular topologies. In this situation, efﬁcient routing becomes a challenge. Although table-based routing allows the use of most routing algorithms on any topology, it does not scale in terms of latency and area. In this paper we propose the region-based routing mechanism that avoids the scalability problems of table-based solutions. From an initial topology and routing algorithm, the mechanism groups, at every switch, destinations into different regions based on the output ports. By doing this, redundant routing information typically found in routing tables is eliminated. Evaluation results show that the mechanism requires only four regions to support several routing algorithms in a 2D mesh with no performance degradation. Moreover, when dealing with link failures, our results indicate that the mechanism combined with the Segment-Based Routing algorithm is able to pack all the routing information into eight regions providing high throughput. The paper provides also a simple and efﬁcient hardware implementation of the mechanism requiring only 240 logic gates per switch to support eight regions in a 2D mesh topology. 1 Introduction System on Chips (SoCs) integrate into a single chip all the parts that were found on a small printed circuit board of ∗ This work was supported by the European Commission in the context of the SCALA integrated project no. 27648 (FP6), by CONSOLIDERINGENIO 2010 under Grant CSD2006-00046 and by CICYT under Grant TIN2006-15516-C04-01. the past. They are typically composed of several processor cores together with application-speciﬁc circuitry. A SoC generally reduces size and lowers power compared to less integrated solutions. The solutions for SoC communication structures have been characterized by custom designed ad hoc mixes of buses and point-to-point links [2]. However, as the size of the system increases, buses become not only a bottleneck but also an inefﬁcient solution in terms of power usage and arbitration. This has motivated a shift in the communication design paradigm from non-scalable bus-based architectures to a shared global communication structure based on the use of an on-chip data-routing network or a Network on Chip (NoC) based architecture [1]. In a SoC, each core node (end node) is attached to the NoC through a network adapter. The NoC is composed of several switches and links interconnected following some connection pattern or topology. End nodes communicate with each other by sending packets to/from the NoC. Packets are transmitted across the links and through the switches according to the switching technique. Wormhole switching is the preferred choice for NoCs, as it requires small buffers at each switch. In absence of a fully connection between all switches, packets must be forwarded through several intermediate switches until they reach their destination. Concerning topology, most NoCs implement regular forms of network topology that can be easily laid out on a chip surface (a 2-dimensional plane). Therefore 2D-torus or 2D-meshes are the preferred choices. In some designs [10], torus is discarded in favor of a mesh because in the latter wires are shorter, thus leading to a lower and constant link delay. However, some SoCs are composed of a heterogeneous collection of ALUs, memories, FPGAs, etc. Due to the heterogeneity of the SoC components, the resulting interconnection topology is not longer regular. Indeed, the high integration scale used in SoCs pushes a number of communication reliability issues. Crosstalk, power supply noise, electromagnetic and intersymbol interference are some of these issues. Moreover, fabrication faults may appear, in the form of defective core nodes, wires or switches. In these cases, while some regions of the chip are defective, the remaining chip area may be fully functional. From the NoC point of view, in presence of such fabrication defects, the initial regular topology has become an irregular one. Anyway, the fact that the original topology was regular may be exploited. Routing determines the path that each packet follows between a source–destination node pair. Routing is deterministic if only one path is provided per node pair, or adaptive, if several paths are available. Adaptive routing better balances network trafﬁc, thus allowing the network to obtain a higher throughput. Routing strategies can be also classiﬁed as source or distributed. In source routing, the source end node computes the path and stores it in the packet header. Since the header itself must be transmitted through the network, it consumes network bandwidth. Source routing has been used in some networks because switches are very simple. They only have to select the output port for a packet according to the information stored in its header. In distributed routing, each switch computes the next link that will be used while the packet travels across the network. By repeating this process at each switch, the packet reaches its destination end node. The packet header only contains the destination ID. Distributed routing has been used in most hardware switches for efﬁciency reasons, since it allows more ﬂexibility, as different ports can be available to reach a given destination at each switch (distributed adaptive routing). Distributed routing can be implemented in different ways. The approach followed in regular topologies of large multicomputers was the so called algorithmic routing, which relies on a combinational logic circuit that computes the output port to be used as a function of the current and destination nodes and the status of the output ports. The implementation is very efﬁcient in terms of both area and speed, but the algorithm is speciﬁc to the topology and to the routing strategy used on that topology. Indeed, when using a fault-tolerant routing algorithm, additional hardware must be used to properly support it. With the introduction of clusters of workstations, switches based on forwarding tables were proposed. In this case, there is a table at each switch that stores, for each destination end node, the output port that must be used. This scheme can be easily extended to support adaptive routing by storing several outputs in each table entry [7]. The main advantage of table-based routing is that any topology and any routing algorithm can be used, including fault-tolerant routing algorithms. However, routing based on forwarding tables suffers from a lack of scalability. The size of the forwarding table grows linearly with network size and, most important, the time required to access the table also depends on its size, and, thus, on network size. On the other hand, power consumption is not negligible. For these reasons, forwarding tables are not suitable for NoCs. Several NoC design frameworks for application speciﬁc NoC that address the problem of routing in irregular meshes using forwarding tables have been proposed in the literature [14, 12, 11]. In [14] Srinivasan et al. proposed a linear programming algorithm that minimizes the number of routers utilized in the topology in order to reduce the trafﬁc ﬂowing in the network. It takes into account the communication traces of the application and generates only a unique route for every communication. Unfortunately the routing method provides not adaptivity at all and does not state how deadlock freedom is guaranteed. In [12] Schafer et al. proposed a ﬂoorplanning method based on the Odd-Even routing algorithm. This method is enhanced with special rules that deﬁne routing patterns under certain types of irregularities. In order to avoid deadlocks, the IP cores are required to have special dimensions and at the same time, the connection point of a core most be situated in its south-west edge. This presents a drawback as it limitates the use of general purpose IP cores. In [11], a mechanism that compresses forwarding tables has been proposed by Palesi et al. Although it helps in improving forwarding table scalability, it is aimed to support a particular kind of routing algorithms where the actual communication pattern among end nodes is considered for deadlock-freedom (application speciﬁc routing). Even more, the mechanism is proposed for minimal path routing, thus, not allowing routing through networks with some failed links/switches. Our challenge in this paper is to develop a routing framework for NoCs that is able to implement not only routing algorithms for regular topologies but also for the irregular ones that arise when some defects or faults appear in the network (i.e., agnostic routing algorithms that can route packets on any topology). For efﬁciency reasons, distributed adaptive routing is a concern. On the other hand, the routing algorithm should be computed quickly and the silicon area and power should be as low as possible. Speciﬁcally designed algorithmic routing are not feasible, as the topology could be irregular and distributed routing based on forwarding tables is inefﬁcient due to its scalability problems. 2 Motivation When a packet arrives to a switch, the routing algorithm obtains the corresponding output port by analyzing (i) the input port used for the packet to reach the switch and (ii) is deﬁned in the domain C × N → C (input port, destinathe destination node of the packet. The routing algorithm tion node, output port). Up*/down* [13] and derivatives are examples of these routing algorithms. Sometimes, the information about the input port is not required, obtaining the output port only as a function of the destination node. In this case, the domain is N × N → C (current node, destination node, output port). Dimension-order routing algorithms for meshes and tori are examples of this class. From the domain point of view, a routing algorithm associates each input – output port combination a set that contains the destination nodes that can be reached from that port. Given a packet entering through a given input port, it must be checked whether its destination belongs to the corresponding set. If so, the output port can be used to forward the packet. Forwarding tables are a way to easily implement this check. The input port and destination identiﬁer of the packet are used to obtain the output port. In this way, the set of destinations associated to an input – output port combination is spread throughout the table. The table describes this set by enumerating all its elements. Usually, the elements of the set have some common properties that allow representing it in a more compact way. This happens in regular topologies. For instance, in XY routing, the input port is not used for routing, and the port in the X dimension can be used if the X coordinate of the current and destination nodes differ, regardless of the Y coordinate. The destinations that match this condition deﬁne a planar ﬁgure (a rectangle), and can be represented by using the identiﬁers of two opposite corners. To check whether the destination of a given packet belongs to the rectangle, all we have to do is comparing the X coordinate of the destination with the X coordinates of the nodes that deﬁne the rectangle. This is the approach followed by hardwired routing algorithms, Interval Routing [6] and derivatives [4]. The implementation is very efﬁcient in terms of area, speed and power consumption. The problem of these approaches is that they do not support faults. When some faults appear in a regular network or a true irregular network is used, again the input port is required for routing, and the set of nodes associated with a given input – output port combination are not longer represented by a single rectangle. However, we could deal with this case by representing this set by several rectangles that may intersect. The corresponding input – output port combination may be used if the destination is found inside any of them. We will refer to each rectangle as a region. As several regions will be associated to each input – output port combination, the implementation of this region-based routing will be half-way between hardwired algorithmic routing and forwarding tables. The higher the number of regions, the closer to the complexity of forwarding routing. In fact, a forwarding table is a particular case of region-based routing, where each region is composed of only one destination identiﬁer. The total amount of logic for routing using regions depends on the number of required regions. At the same time, the number of regions depends on the regularity of the topology (the number of failures and their position) and the routing algorithm used. However, in some cases (regular topologies with faults) different input ports may share the same set of destinations mapped to the same output port. We can exploit this fact by associating each region only to an output port, and also by coding in the region which input ports share the set of destinations. This trick may help in reducing the number of regions. On the other hand, the number of regions is closely related to the number of routing options. In fact, a way of reducing the number of regions is to limit the adaptiveness of the routing algorithm. In this paper, we propose the region-based routing as an alternative implementation of routing algorithms in NoCs. While providing the required ﬂexibility to support faulttolerance, is efﬁcient in terms of area, speed and power consumption, thus being appropriate for large networks. Additionally, we combine in this paper the region-based routing mechanism with an appropriate routing algorithm for regular networks with failures. The Segment-Based Routing (SR) algorithm [8] will be key to provide high network throughput while requiring an acceptable number of regions. SR routing will take beneﬁt from the regular structure of the topology even in the presence of failures. The rest of the paper is organized as follows. Section 3 describes the region-based routing mechanism, ﬁrst by providing the foundations of the mechanism, then by describing and analyzing a very simple hardware implementation. Then, we will describe the way regions are computed. Later, in Section 4, we present an analysis of the number of regions and the performance achieved by region-based routing for different trafﬁc patterns. Cost, area, and latency of the hardware proposal are also explored. Finally, in Section 5 we set the basis for future work and draw some conclusions. 3 Region-Based Routing 3.1 Overview The basic idea behind the mechanism is to group at every switch different destinations within different regions. Roughly, each region will be deﬁned by all the destinations that can be reached using the same set of output ports at the switch where the region is deﬁned. As an example, Figure 1 shows a 8 × 8 mesh network with seven link failures. Ports at every switch are labeled as N (north), E (east), W (west), S (south) and I (internal). The Figure also shows the set of bidirectional routing restrictions deﬁned by the applied routing algorithm1 . A routing restriction is deﬁned by two consecutive links in a given switch. Both links can not be used consecutively by any packet in order to guarantee deadlock freedom. In Figure 1, some switches have been grouped into regions (r1, r2, r3). Regions are deﬁned for the switch highlighted with a circle (this switch will be referred to as the 1 SR has been applied for this case; it will be described in section 3.4. r1 r2 r3 r2 N W E r1,r2,r3,r4 S r4,r5 r4 r5 routing restrictions used by the packets, a subset of the output ports that may be used and the potential destinations that can be reached. Additionally, as regions are deﬁned by rectangular boxes of switches in a 2D mesh network, we can notate a region for switch x as Rx (iport list, {swref 1 , swref 2 }, oport list) (swref 1 is the top left most switch and swref 2 is the bottom right most switch). Therefore, whenever a packet arrives into a switch, all the regions must be inspected in order to detect which are the regions suitable for routing the packet. The control routing unit will, thus, select the set of output ports provided by the matched regions and deliver all the possible output ports to the arbiter in charge of selecting the most convenient one. Figure 1. Example of region deﬁnitions. 3.2 Hardware Description reference switch). All the packets arriving to the reference switch (or being generated at its local port) addressed to any destination included in region r1 necessarily need to use the W output port at the reference switch. Notice that if they use the N port, they will need to cross a routing restriction which is forbidden or use a not minimal route which is inefﬁcient. The same happens when using the E and S ports. Therefore, at the reference switch, region r1 includes destinations that can be reached only through the W port. Also, there are other destinations that can be reached using only the W port (i.e. switches within region r3), however the mechanism relies on deﬁning rectangular regions for the shake of implementation, thus, two regions (r1 and r3) are deﬁned for the same output port. An interesting property of the mechanism is that regions can be deﬁned for more than one output port, thus allowing the adaptiveness inherent to the applied routing algorithm. For instance, region r2 is deﬁned by those switches that can be reached either using the N or W ports at the reference switch. The input port used by the packet to arrive to the reference switch must be also taken into account when deﬁning regions. For instance, at ﬁrst sight we can use ports W and S to reach region r4. However, W output port should be only used when the packet does not reach the reference switch through the N port, as it would cross a routing restriction at the reference switch and potentially would lead to deadlock. Therefore, regions must be deﬁned based on the set of destinations, the output ports and the input ports. In the ﬁgure, region r4 is deﬁned for packets using the input port E and I and region r5 is deﬁned for packets using the input port N . Notice that the set of destinations of two regions can be overlapped, but they will differ in the set of input ports. To summarize, a set of regions is computed at every switch. Each region is deﬁned by the possible input ports In this section we describe a simple implementation of the region-based routing mechanism for a N × N mesh network. Figure 2 shows the proposed implementation. The mechanism requires nearly the same functional blocks than any other NoC switch, that is: the input ports, the routing control unit, the arbiter and scheduler, the crossbar and the output ports. The input port for wormhole switches generally requires two different blocks, an input port controller (I P C ) that manages the input buffer and transmits the status information to neighboring nodes, and a header decoder (HD) that decodes the header information of every packet. The packet header should be as compact as possible in order to keep the minimum overhead. Among other information it contains the destination identiﬁer of the packet. In our proposal, an absolute addressing is performed, and the packet ID identiﬁes the X and Y coordinates of the destination (the MSBs indicate the row (RowDst) and the LSBs indicate the column (C olDst)). Once decoded, the coordinates of the destination are compared against the coordinates of the current switch. If equal, the packet is delivered to the internal port, otherwise, the coordinates of the destination are sent to the routing control unit. The routing control (RC ) unit is made of different logic modules, one for each possible region deﬁned in the switch for the region-based routing. Each module has six registers, the input port (I P ) register with one bit per input port (NEWSI; North, East, West, South, and Internal), the ROW1 , COL1 , ROW2 , COL2 registers with Log(N ) bits each (these registers contain the coordinates of the top left most switch and the bottom right most switch of the region), and the output port (OP ) register with one bit per output port (NEWS) (this register does not take into account the internal port as the packets going to the current switch have previously been delivered to the internal port). These registers must be programmed before routing any packet at network boot time. The way register values are computed will be described in Section 3.3. North Port IPC & HD East Port . . . . Region Trigger Region matcher . . . . Output Port Selector . . . . Region Trigger Region matcher Output Port Selector Routing Region n . . . . . . Routing Region 2 . . Xbar Switch Output Ports Header Decoder (HD) Region Trigger (RT) Routing Region 1 RowDst ColDst MSB LSB IPC N E W S I N E W S I IP register West Port IPC & HD South Port IPC & HD Local Port IPC & HD Switch Input Ports RowDst Row1 RowDst Row2 ColDst Col1 ColDst Col2 − + + − − + + − RowDst >= Row1 Row1 >= RowDest ColDst >= Col1 Col2 >= ColDst Output Port Selector N E W S OP register Region Matcher (RM) Routing Control Unit (RC) ... ... ... ... N E W S Arbiter North Port Status East Port Status West Port Status South Port Status Figure 2. Proposed hardware implementation of the region-based routing mechanism. Once the packet header has been decoded it is passed to the RC unit together with the input port identiﬁer where the packet arrived to the switch. At the RC unit this information is compared against the pre-deﬁned regions. In order to save power consumption a pre-selection of regions is made according to the I P registers. That is, we discard checking the regions whose input port registers do not match with the input port of the packet being routed i.e, if the packet has arrived to the switch through the south port we discard regions deﬁned at the south of the switch. To achieve this in hardware, the RT (region trigger) unit matches the input port of the packet with the input port register of every region. The RT unit consists of ﬁve AND gates (one per input port) and a single OR gate. The output of the RT unit is a signal that triggers the rest of logic for the region. After checking the input port, and only on success, the rest of the logic for a region is activated. In particular, the logic determines if the destination is within the boundaries deﬁned by the region. To achieve this, the row and column of the packet’s destination are compared with the contents of ROW1 , COL1 , ROW2 , and COL2 registers. For this, four magnitude comparators are used at the region matcher (RM ) unit. If all the comparisons are true, the destination of the packet is within the region and therefore, the output ports deﬁned for the region must be considered for routing purposes. Thus, the logic selects the output ports previously introduced in the OP register. Notice that the implementation allows different output ports to be selected from the same region (adaptivity). Once all regions have been evaluated, all the selected output ports from all the regions that matched the packet are ORed and passed to the arbiter. The arbiter may choose one output port based on different criteria. Then the packet will be routed to the next switch until arriving to destination. As shown, the control unit of the region-based routing mechanism can be implemented using a very simple and fast combinational logic circuit 2 as it is shown within the dotted square in Figure 2. Note that RT and RM units have been serialized (one is performed after the other). Although such a decision will increase latency, it will reduce energy consumption. Thus, there is a trade off between the latency penalty (routing time) and energy consumed that must be evaluated when designing the NoC. 3.3 Software Description The algorithm for computing regions is divided into sequential phases. Figure 3 shows the different phases. Initially, as input parameters, the algorithm receives the topology of the network and a set of routing restrictions. The network can be a 2D mesh of any size and with or without failures. As an example, Figure 3 shows a 2D mesh with a link failure. The set of routing restrictions depends on the routing algorithm applied to the network. Notice that considering the set of routing restrictions allows to compute in a later phase the entire set of possible routing paths for every sourcedestination pair. Thus, routing is not restricted at this stage. In a ﬁrst phase, the algorithm computes the possible set of routing paths for every source-destination. This is a challenging problem since the number of possible paths can be extremely high. For this, the algorithm ﬁrst tries to compute all the minimal paths for every source-destination3 pair. If, for a particular pair of nodes, there is no valid minimal path (in case the topology is irregular), then it searches for paths 2 Latency and area analysis are performed in Section 4. 3Minimal paths are searched by assuming that the underlying topology is a mesh network with probably some failed links. TOPOLOGY & ROUTING RESTRICTIONS SET PATH COMPUTATION REGION COMPUTATION max_regions REGION MERGE REGION SET Figure 3. Stages in the computation process of regions. allowing an increasing number of misrouting hops. Once a non-minimal valid path is found, the algorithm switches to the next pair of nodes. Thus, the algorithm ﬁnds all the possible minimal paths and, when necessary, ﬁnds a unique non-minimal path. As an example, Figure 4 shows different paths computed for the 2D mesh. All the paths have in common the destination (switch d). SWITCH (N,d,S) (N,d,E) (W,d,S) (W,d,E) (I,d,S) (I,d,E) ({N,W,I},d,{S,E}) SWITCH (N,d,S) (W,d,E) (I,d,S) (W,d,S) (I,d,E) ({N},d,{S}) ({W,I},d,{S,E}) d Figure 4. Routing paths and routing options. The routing paths are computed and stored in a distributed way. In particular, the algorithm packs all the routing info into a set of routing options. Also, routing options are grouped per switch where they are applied. Each routing option (ip, dst, op) is deﬁned by an input port (ip), a destination (dst), and an output port (op), and indicates that a packet at the switch where the routing option is deﬁned, that is coming through the input port ip, and going to destination dst may take output port op. For instance, in Figure 3, routing options for switches plotted as a box and as a circle are shown. For instance, (N , d, S ) at the box switch indicates that a packet coming through N north port going to destination d can be routed through S output port. Additionally, routing options may be packed per output port and per input port. First, packing per output port is performed. Two routing options at the same switch are packet if both have the same input port and the same destination ID. Therefore, both routing options (N ,d,S ) and (N ,d,E ) can be packed into the routing option (N ,d,{N ,E }). Notice that by doing this, we are representing all the possible adaptive routing options into a single routing option. Once routing options are packed per output ports, further packing can be done, now depending on the input port. If two routing options at a given switch have the same destination IDs and the same set of output ports, then they can be further packed into one routing option. The input ports of the resulting routing option is the union of all the input ports of the routing options being packed. For instance, in the Figure, routing option ({N ,W ,I },d,{S ,E }) has been obtained by packing (N ,d,{S ,E }), (W ,d,{S ,E }), and (I ,d,{S ,E }) routing options. In the second phase, the algorithm computes the routing regions from the routing options. At every switch the algorithm groups destinations reachable through the same output ports at the switch and coming from the same set of input ports. For instance, Figure 5 shows some regions computed for the selected switch. In particular, only regions deﬁned for output ports S and E are plotted. The ﬁrst region (R1) includes switches inside the box deﬁned by switches x1 and x2. This region is deﬁned for destinations that are reachable only through the S output port and for packets coming from input port W , N , or I . In the same sense, region R2 is deﬁned for packets using output port E and coming through input ports N , W , or I . Finally, region R3 is deﬁned for packets coming either from input ports N , W or I , and can use either output port E or S . Notice that routing is not restricted, since the different regions are computed based exclusively on the output ports used. R1 ({N,W,I}, {x1,x2}, S) R2 ({N,W,I}, {x3,x4}, E) R3 ({N,W,I}, {x5,x6}, {S,E}) R1 ({N,W,I}, {x1,x2}, S) R2’ ({N,W,I}, {x3,x6}, E) x3 x5 x1 x2 x4 x6 Figure 5. Some computed regions. Finally, at the last phase, the algorithm tries to pack all the regions in order to bound the maximum number of allowed regions. For this, the algorithm takes a third parameter (max reg ions parameter) that indicates the maximum number of regions allowed at any switch. The algorithm will pack regions by restricting routing. This will be done by reducing the number of output ports that can be used at a given switch to reach a set of destinations. However, the algorithm will guarantee that always there will be an output port that can be used (i.e. connectivity is ensured by the algorithm). In order to reduce the number of routing options, the algorithm focuses at switches using a number of regions higher than the maximum allowed (max reg ions parameter). Then, at a given switch, it compares each region with the remaining ones, checking if they can be merged. Two regions can be merged if the combination of both regions deﬁnes a box of switches and the output ports of one of the regions is a subset of the output ports of the other region. For instance, from the previous example, the algorithm detects that regions R2 and R3 can be merged into one region. Effectively, the output port of region R2 (E ) is a subset of output ports of R3 ({S , E )}). However, the resulting region will restrict routing. In order to keep deadlock freedom, the output ports of the resulting region will result from the intersection of the output ports of the two regions being merged. Thus, in the example, the output port of the resulting region is E . This way, we prevent the use of the S output port for packets going to R2(cid:1) (which is not allowed by the original routing algorithm). Although routing is restricted, we expect this limitation to have a low impact on ﬁnal performance. There are two reasons to make this afﬁrmation, First, routing is restricted at some particular switches, thus, for a given particular path, alternative routing options will be eliminated only at some switches. The second reason is that the packing will be performed only at those switches with a high number of regions. As we will see, most of the switches will require a very low number of regions, thus no requiring packing. As an example, Figure 6 shows a mesh with 64 switches and seven link failures. The applied routing algorithm is SR (see Section 3.4). Figure shows the computed regions for the switch plotted in a black circle before (Figure 6.a) and after restricting routing (Figure 6.b). We can see that initially up to 14 regions (r0, r1, ..., r13 in the ﬁgure) are required for the switch to properly route packets (the example shows the case for the switch with the maximum number of regions). For instance, r0 deﬁnes the switches that are reachable from the switch by using the W output port. As can be noticed, regions r0 and r1 are reached by using a different set of output ports (output ports N and W for r1). Thus, maximum adaptivity offered by the underlying routing algorithm is preserved by the computed regions. In order to reduce the number of regions, routing restrictions are applied. In particular, Figure 6.b shows the minimum number of regions achievable. As it can be observed, former regions r0, r3, r4, r6, r8, r10, and r12 have been packed into the new region r0. For this, routing has been restricted and all of the destinations can be reached now only through the W output port. Also, former regions r1, r2, and r5 have been packed into region r1, and regions r7 and r11 into region r4. Notice also that former regions r13 and r9 have been kept (new regions r2 and r3, respectively). At the end, only ﬁve regions are required for the plotted switch at the expense of some routing restrictions. If, however, the number of allowed regions is higher than ﬁve, then, more routing options would be preserved. 3.4 Segment-based Routing (SR) Segment-based routing algorithm (SR) is a simple, effective and topology-agnostic routing algorithm that does not rely on the use of virtual channels and obtains a good performance for any topology. Its key characteristic is that, in the presence of any possible combination of faults, SR beneﬁts from the regularity inherent in the underlying topology. It always provides a valid and deadlock-free path for every source-destination pair of nodes in a fast way. SR has been proposed for networks with regular [8] and irregular [9] topologies. first subnet Starting Switch 1 A 4 E 17 I S 1 3 S 6 16 5 S 2 7 S 5 14 B 2 F 15 J T 18 T M N T 19 C 6 G 13 K 20 O 8 S 3 10 S 4 12 S 7 D 9 H 11 L 22 P 21 Figure 7. Example of computing SR. Basically, the algorithm splits the network into different network segments (groups of switches and links). Thus, a network is formed by one or more segments and each network component belongs to a unique segment. As an example, Figure 7 shows a regular topology with two induced link failures. From this topology, SR computes the segments described in Figure 7. In particular, 7 segments have been computed (from S 1 to S 7). A segment is deﬁned as a list of interconnected switches and links. Segment S 1 consists of switches {A, B , F, E } and links {1, 2, 3, 4}. Segment S 2 consists of switches {C, G} and links {5, 6, 7}, while segment S 6 is formed only by switch {I } and links {16, 17} and so on for the rest of segments. The main rule followed to compute segments is that every new computed N N E N W W W W W N W SW (ei) S (n) S (n) W (ei) S ES N r0 r3 r1 r2 r5 r7 r11 r10 r6 r4 r9 r8 r12 r13 input port link failure (a) S (n) E S (newi) N W (esi) r2 r0 r1 r4 r3 (b) Figure 6. Regions for switch (3, 7). (a) Before packing the regions and (b) after maximum packing. segment (except the ﬁrst one) must start and end on an already computed one. Notice that except for the initial segment, the remaining segments start and end on a switch that already is part of a previously computed segment (e.g. S 2 starts/ends from/on S 1). As soon as the complete topology is partitioned into segments, SR adds a bidirectional routing restriction to every segment. By partitioning the network into independent segments, SR is able to place a routing restriction in a segment independently of the remaining segments. Any possible combination will end up in a routing algorithm that is deadlock-free and keeps connectivity among all end nodes (see [8] for a demonstration). Figure 7 shows all the possible routing restrictions that can be placed on any segment. For example, in S 1 we can place a bidirectional routing restriction at switches B , E or F . From all the possible combinations, SR selects only one routing restriction in each segment. As an example, we have selected just one routing restriction (the ones in boldface) in each of the seven segments in Figure 7. Notice that the way the network is divided into segments and the way routing restrictions are placed in every segment ensures that no cycles will be formed through the entire network. Finally, source-destination paths are calculated following the path balancing algorithm described in [3]. This method minimizes the deviation of link weight. Then, routing tables are calculated and distributed to every switch within the network. However, in this paper, the adaptive version of SR is proposed. To achieve this objective, SR follows the routing methodology for meshes reported in [8] (shown in Figure 7). The main difference with the previous method is that we are not required to calculate routing tables. Instead, the region-based routing mechanism will automatically calculate the paths based on the locations of the routing restrictions provided by SR. The way segments are computed and restrictions are placed inﬂuence the ﬁnal performance of the algorithm. Figure 8 shows two methods that will be explored in this paper. The ﬁrst one (Figure 8.(a)) will be referred to as SR and will search segments from top to bottom and at each row in a different direction. In the second one (Figure 8.(b)) segments will be searched from left to right and at each column in a different direction. This algorithm will be referred to as SR2. Figure shows with arrows the direction of the search of segments. S 1 S 2 S 3 S 4 S 5 S 6 S 7 S 8 S 9 S 10 S 16 S 15 S 14 S 13 S 12 S 11 S 1 S 4 S 2 S 3 S 5 S 6 S 7 S 8 S 9 S 10 S 11 S 12 S 14 S 15 S 16 S 13 T Y A B C D E F G H I K M N O P Q R S T X J L U V W Y A B C D E F G H I K L M N O P Q R S V X U W J (a) SR (b) SR−2 Figure 8. Different SR setups. 4 Evaluation In this section we evaluate the region-based routing mechanism and the algorithm used for computing regions. First, we analyze the number of regions required for different combinations of topologies and routing algorithms. For this, we use different routing algorithms in a regular mesh network: Dimension Order Routing (DOR), OddEven (OE), SR, and up*/down* (UD; a BFS spanning tree is formed with the root at switch (0,0)). For SR, two different layouts (SR and SR2) are explored, both shown in Figure 8. Also, we analyze the SR, SR2 and UD routing algorithms in the presence of different topologies (mesh network with different link failures injected). In this way we analyze which is the minimum number of regions required for each topology-routing combination. Then, we explore the performance achieved by the different routing algorithms when using the regions. This analysis is performed in two directions. First, the performance degradation of the network when using a lower number of regions is analyzed. Second, the performance degradation as we increase the number of failures in the network is explored. The goal of this analysis is to determine which is the number of regions required to achieve acceptable levels of performance and also which is the best routing algorithm. Finally, we analyze the proposed hardware in terms of latency, area, and cost. 4.1 Simulation Tool We developed a ﬂit-accurate simulator that models arbitrary switch-based NoCs with point-to-point links. Each switch has a non-blocking crossbar connecting input and output ports, which allows multiple packets to be simultaneously transmitted. The crossbar is able to transmit one ﬂit per connection per cycle. In our simulations, every switch has an input-buffer size of 5 ﬂits. The maximum bandwidth of each link is set to 1 ﬂit per cycle. We use a constant packet size of 5 ﬂits. We assume the same constant packet rate for each end-node and simulate 5 million packets after a warm-up session of 1 million arrived packets. The routing decision is made at every switch, by checking the regions as speciﬁed by the region-based routing mechanism. If multiple output ports are available for a header ﬂit, a random selection of the output is performed. When the routing decision is complete, ﬂits are forwarded over a link only if there is available buffer space at the next input port. The routing time at each switch is set to 2 cycles. This includes the time to check the regions, perform the crossbar arbitration, and set up the crossbar connections. We consider several speciﬁc communication patterns between pairs of nodes: uniform, bit reversal, perfect shufﬂe, butterﬂy, matrix transpose, and hot spot. For hot spot trafﬁc, in addition to regular uniform trafﬁc every packet has 6% of probability to be destined to a hot spot node located at coordinates (3,3). We have evaluated 8 × 8 meshes, and we have modeled these topologies with up to seven randomly-injected link failures. For all simulations, we measure both network throughput and packet latency. 4.2 Required Number of Regions Figure 9 shows the number of regions required for implementing different routing algorithms in a 8 × 8 mesh network without failures. The ﬁgure also shows the minimum number of regions required to guarantee network connectivity (reducing adaptiveness). The ﬁgure shows that DOR can be implemented with only four regions at maximum on every switch. For other routing algorithms, UD requires six regions, SR seven and OE eight. However, these routing algorithms are partly adaptive (provide more than one routing option), contrary to DOR that is a deterministic routing algorithm. Thus, the number of regions for all these routing algorithms can be reduced to only four regions at maximum on every switch. Obviously, this reduction may come with an impact on performance. Later in this section we analyze this. Figure 9. Number of regions in a 8 × 8 mesh. Figure 10 shows the number of regions (max and min) required for a 8×8 mesh network with a different number of link failures (injected randomly; the position of the link failures are shown in Figure 6) for SR, SR2, and UD. As it can be observed, for SR the number of required regions grows logarithmically with the number of link failures. Fourteen regions are enough to tolerate seven link failures (5.4 % of failed links). Also, the maximum number of regions can be bounded by the algorithm to only eight for all the cases. However, for SR2 and UD, a higher minimum number of regions is required. Ten regions are needed to cope with the seven link failures case. Also, SR2 would require up to 18 regions without restricting routing. Figure 11 shows the percentage of switches requiring different number of regions for the 8×8 mesh without link failures and without restricting adaptivity. As expected, all the switches require four regions at most with DOR. More interesting, when using other routing algorithms, almost half of the switches require only four (for SR2 and UD) or ﬁve regions (for SR and OE). This result may indicate that the overall power consumption of the mechanism will be low, since not all the implemented regions will be activated in all the switches. Figure 10. Number of regions in a 8 × 8 mesh when increasing the number of failures. Figure 11. Percentage of switches requiring different number of regions without restricting adaptivity in a 8 × 8 mesh. Figure 12 shows for SR the distribution of regions for the 8 × 8 mesh network with an increasing number of link failures with maximum adaptivity. An interesting observation is that regardless of the number of link failures, more than 50% of switches require only seven regions. Also, as the number of link failures increase, there are some switches that require more regions. Figure 12. Percentage of switches requiring different number of regions for a 8 × 8 mesh with increasing number of link failures with maximum adaptivity. SR routing algorithm. 4.3 Performance Evaluation Figure 13 shows the network throughput achieved by the different routing algorithms in different trafﬁc scenarios for the 8 × 8 mesh network with no link failures. Also, results for the maximum and minimum number of regions are obtained. The ﬁrst observation is that DOR achieves higher throughput than the rest of algorithms for uniform trafﬁc pattern. This result is well known. In fact, DOR behaves better than adaptive routing algorithms in meshes with no link failures. However, for other trafﬁc patterns DOR obtains similar or lower network throughput than the rest of routing algorithms. SR and OE routings beneﬁt from their adaptiveness for bit-reversal, shufﬂe, and transpose patterns. Also, SR achieves higher network throughput for butterﬂy pattern. These results are achieved using 7 regions in SR and 8 regions in OE (see Figure 9). Figure 13. Network throughput in a 8×8 mesh. When reducing the number of regions down to the minimum (four), an interesting result can be observed. OE and SR2 suffer a signiﬁcant drop in network throughput. Throughput decreases signiﬁcantly for uniform, bitreversal, shufﬂe, and transpose trafﬁc patterns. In some cases (uniform, shufﬂe, and transpose) OE and SR2 perform signiﬁcantly lower than the rest of routing algorithms. On the other hand, the rest of routings achieve roughly the same throughput numbers when using four regions. For UD routing, it can be noticed that its performance is marginally related to the number of regions. Practically, this routing achieves the same performance with any number of regions. To summarize, on average (last set of columns in Figure 13), all the routing algorithms achieve similar performance, except OE and SR2 that are highly sensitive to the number of regions. Indeed, when reducing the number of regions down to four, the percentage of routing options eliminated for OE is 22%. For the rest of routing algorithms, the percentage is lower than 15%. When eight regions are allowed, on average, SR and SR2 exhibit the highest throughput taking advantage of their adaptiveness. Now, let us focus in the performance achieved when dealing with meshes with some link failures. Figure 14 shows the performance achieved in a network with one link failure for different routing algorithms, different trafﬁc patterns and different number of regions. As it can be noticed, most of the network throughput achievable by SR is obtained with only six regions (practically the maximum in hotspot and shufﬂe and more than 80% in the rest of trafﬁc patterns). Also, when using eight regions, throughput is roughly the maximum for all the trafﬁc patterns. Figure 14. Network throughput in a 8× 8 mesh with one link failure. Figure 15 shows the performance achieved in a mesh network with seven link failures (the topology is shown in Figure 6). In this case, eight regions provides again most of the network throughput achievable by SR. Also, with an increasing number of link failures (up to seven) the percentage of routing options eliminated is only 5% in the worst case (when reducing the number of regions to eight in a mesh network with seven link failures). On average, SR achieves 20% higher throughput than UD and 30% higher than SR2. Figure 15. Network throughput in a 8× 8 mesh with seven link failures. Finally, Figure 16 shows the throughput degradation with an increasing number of link failures for different trafﬁc patterns (throughput achieved in a mesh network with no link failures (0F) is included). SR with eight regions is used for all the cases. As it can be observed, performance decreases as the number of failures increases up to the point that network throughput becomes constant. As the number of failures increases, the number of routing options eliminated to keep the number of regions constant increases, thus, loosing adaptiveness. Also, a larger part of the network becomes irregular. However, SR combined with region-based mechanism with 8 regions keeps network throughput even with link failures for hot-spot and transpose trafﬁc. For the rest, throughput decreases 30% for bitreversal and uniform, 25% for butterﬂy, and 40% for shufﬂe. Figure 16. Network throughput in a 8× 8 mesh with an increasing number of link failures. SR with eight regions. 4.4 Area and Cost Evaluation As an approximation of the area and time constrains of the region-based routing mechanism, we have synthesized our implementation onto Xilinx Virtex 2 FPGA at 3.3V and 133 MHz. Results show that the circuit would require about 30 logic gates per region. Therefore, if we were using a 8×8 mesh where 8 regions are required to support adaptiveness and fault-tolerance, the total overhead required for implementing the region-based control portion would be around 240 gates, which seems not to be a signiﬁcant overhead when considered in the context of ASIC. Table 1 shows the equivalent memory size required for an implementation with routing tables. In particular, routing with forwarding tables requires a table with as many entries as the number of nodes and input ports, and every entry needs to store the ports returned by the routing function. Hence the cost of this alternative is N × log(d) × d, where N is the number of nodes and d is the number of ports. As can be seen, for memory-based solutions the overall requirements in memory grow exponentially with the network size. Contrary to this, the region-based routing requires per region four registers of size log(N )/2 (ROW1 , COL1 , ROW2 , and COL2 registers), one register with d + 1 bits (I P register) and a register with d bits (OP register). Table 1 shows the number of bits required (per switch and per chip) for an implementation with 16 regions. Also, Table 1 shows the number of logic gates required. As can be observed, the requirements for the mechanism are very modest and less than half a kilobyte is required per switch, for any network size. Even the required logic per switch slightly increases with network size, the total logic and memory requirements are signiﬁcantly low. It has to be noted that with the memory requirements and the number of logic gates shown in Table 1, all the routings under all the evaluated networks could be applied without any routing restriction applied (with no performance degradation). Also, as a reference point, in [5] a NoC FPGA implementation of the full switch implementation required approximately from 10,000 to 26,000 gates depending on the Memory-based routing Memory memory per switch in chip 512 b 32 Kb 2 Kb 512Mb 8 Kb 8 Mb 128Kb 128Mb Bits (registers) per switch 336 b 400 b 464 b 528 b Region-based routing Logic gates Bits (registers) per switch in chip 480 21 Kb 608 100 Kb 736 464 Kb 1024 2 Mb Logic gates in chip 31 K 156 K 754 K 4.2 M NoC 8 × 8 16 × 16 32 × 32 64 × 64 Table 1. Memory requirements for table-based routing and region-based routing. 16 regions. routing algorithm selected. A hardware implementation of the region-based routing would require less than 5 percent of the hardware overhead. The measured latency of the critical path for the regionbased routing mechanism is 6 ns (for the FPGA implementation). This delay includes the generation of the RegionTrigger signal plus the time required to check whether the address destination of the packet is inside the region (all the regions perform the same comparison concurrently) thus generating the region output ports assignments. If we were using a table lookup system, the system would require an access time that grows with network size. 5 Conclusion and Future Work We have proposed an scalable and versatile implementation of routing algorithms in NoCs. When combined with an appropriate routing algorithm region-based routing mechanism achieves to compress redundant routing information while providing the required ﬂexibility to support faulttolerance. The strategy is able to implement the most commonly used routing algorithms in any topology with no performance degradation and at the same time, it is efﬁcient in terms of area, speed and power consumption. The hardware required to apply the routing mechanism is not complex and does not depend on the network size thus being appropriate for large network implementations. In order to increase performance the proposed strategy can be combined with virtual channels. The use of regionbased routing also opens the door to the use of dynamic routing algorithms that takes into account real trafﬁc characteristics at the time of calculating routing restrictions. We plan to take this approach in future work implementations. "
NoC-Based FPGA - Architecture and Routing.,"We present a novel network-on-chip-based architecture for future programmable chips (FPGAs). A key challenge for FPGA design is supporting numerous highly variable design instances with good performance and low cost. Our architecture minimizes the cost of supporting a wide range of design instances with given throughput requirements by balancing the amount of efficient hard-coded NoC infrastructure and the allocation of ""soft"" networking resources at configuration time. Although traffic patterns are design-specific, the physical link infrastructure is a performance bottleneck, and hence should be hard-coded. It is therefore important to employ routing schemes that allow for high flexibility to efficiently accommodate different traffic patterns during configuration. We examine the required capacity allocation for supporting a collection of typical traffic patterns on such chips under a number of routing schemes. We propose a new routing scheme, weighted ordered toggle (WOT), and show that it allows high design flexibility with low infrastructure cost. Moreover, WOT utilizes simple, small-area, on-chip routers, and has low memory demands","NoC-Based FPGA: Architecture and Routing  Roman Gindin, Israel Cidon, Idit Keidar  Electrical Engineering Department  Technion - Israel Institute of Technology  Haifa 32000, Israel  {rgindin@tx,idish@ee,cidon@ee}.technion.ac.il  Abstract  We present a novel network-on-chip-based architecture  for future programmable chips (FPGAs).  A key challenge  for FPGA design is supporting numerous highly variable  design instances with good performance and low cost.   Our architecture minimizes the cost of supporting a wide  range of design  instances with given  throughput  requirements by balancing the amount of efficient hardcoded NoC infrastructure and the allocation of “soft”  networking resources at configuration time. Although  traffic patterns are design-specific, the physical link  infrastructure is a performance bottleneck, and hence  should be hard-coded. It is therefore important to employ  routing schemes  that allow  for high  flexibility  to  efficiently accommodate different traffic patterns during  configuration. We examine  the required capacity  allocation for supporting a collection of typical traffic  patterns on such chips under a number of routing  schemes. We propose a new routing scheme, Weighted  Ordered Toggle (WOT), and show that it allows high  design flexibility with low infrastructure cost. Moreover,  WOT utilizes simple, small-area, on-chip routers, and has  low memory demands.   1. Introduction  Networks-on-Chip (NoCs) e.g.  [1],  [4],  [5],  [8],  [9],  [12],   [15], [17] are commonly considered as a scalable solution  for on-chip communication. However,  it  is also  understood that there is no ""one size fits all"" NoC  architecture  [5], as different silicon systems have very  different requirements from their NoCs. For example, in a  System-on-Chip (SoC), the network usage patterns are  many times known a priory. Hence, the NoC can be  synthesized with  the “correct”  link capacities for  supporting the required usage  [10]. In CMP designs  [14],   [17] the traffic patterns depend on the various executed  programs and hence can vary dramatically within the  same system implementation. Therefore, CMPs’ NoC  resources should support all perceivable software  scenarios.    In contrast to both of the above, in a Field  Programmable Gate Array (FPGA), the communication  patterns are not known at fabrication time but may  become much better determined when the chip is  configured for a specific functionality.  This unique  scenario implies that there a cost and performance  advantage in splitting an FPGA’s NoC construction  among the two design stages.   In this paper, we introduce a NoC design process and  architecture for FPGAs. A distinctive feature of FPGA  systems is that they include a combination of hard and  soft  functionalities.   The hard  functionality  is  implemented in silicon; it typically includes special  purpose modules like processors, multipliers, external  network and memory  interfaces, etc. The  soft  functionality is configured using programmable elements  (gate arrays, flip-flops, etc.) and routing components.  Modern FPGAs contain hundreds of  thousands of  programmable elements, in addition to special purpose  modules  [22]. As technology scales, the sheer number of  logic units will render a flat FPGA chip design  unmanageable. FPGA engineers are already experiencing  unacceptable place-and-route times for large designs with  tight timing constraints. To remedy this problem, modern  FPGA CAD tools, like Xilinx’s PlanAhead, are already  supporting a certain degree of hierarchical design: they  allow designers to implement independent modules on the  chip, which are later connected. We thus envision a future  FPGA that is organized hierarchically, whereby the chip  is divided into high-level regions (some programmable  and some hard), interconnected by a NoC.   When architecting an FPGA NoC, one has to decide  which functionalities are implemented as hard cores and  which are left as soft. There is a tradeoff between the  flexibility offered by the soft part and the higher  performance offered by hard part. Since inter-module  communication is often a bottleneck, it is important to                design the NoC architecture for high performance. We  therefore advocate laying out the network infrastructure,  including metal wires and hard-coded routers in silicon.  At the same time, in order to allow for maximum  flexibility, the NoC infrastructure should be able to  accommodate multiple routing schemes and a large  variety of traffic patterns. To this end, we allow network  interfaces to be soft. Simplistic routing schemes, like XY,  can employ small interfaces, whereas more elaborate  source-routing schemes ( [1], [13]) may have the interfaces  store large routing tables. Our novel architecture is  detailed in Section  2.  The main challenge is exploiting network resources  efficiently, i.e., supporting a large number of program  designs while investing minimal resources (wires and  logic). In this context, there is an inter-play between the  link capacity requirements and the routing scheme used to  route packets between modules. A routing scheme that  balances the load over all links readily supports more  designs using smaller link capacities than an unbalanced  one.   Section   3 formally defines FPGA routing (on our  suggested architecture) as an optimization problem. In  order to study the inter-play between routing and capacity  requirements, we define a new concept called design  envelope, capturing the required capacity for a collection  of traffic patterns. The more traffic patterns the envelope  accommodates, the more flexibility is offered to the  designer configuring the chip. We study the design  envelopes required to accommodate a collection of  patterns with each of the routing schemes.   In Section  4, we present efficient solutions to the FPGA  routing problem. Note that traditional routing algorithms  like XY lead to unbalanced capacity allocation  [10], and  are therefore not suitable for programmable chips. It is  possible to improve the balance by splitting the flow,  toggling between sending on XY and YX routes  [18]   [20]; we call this approach toggle XY (TXY). However,  TXY is not optimal when traffic requirements are not  symmetric, (which is very common in HW architectures).  We improve it by adding weights to flow division (based  on the design pattern), and call the resulting algorithm  weighted toggle XY (WTXY).   Unfortunately, both TXY and WTXY have a major  disadvantage – they split a single flow among two routes.  This can lead to out-of-order arrivals, requiring large  re-order buffers, especially  in congested networks.  Moreover, re-ordering requires the addition of sequence  numbers to packet headers. We therefore suggest the use  of ordered algorithms, which do not split flows, and  ensure in-order arrivals. We do this by selecting one route  (XY or YX) to each source-destination flow. We present  the weighted ordered toggle (WOT) algorithm, which  assigns XY and YX routes to source-destination pairs in a  way that reduces the maximum network capacity for a  given traffic pattern. The WOT routes are calculated  when the chip is programmed, and are loaded into a  vector holding a bit per destination in the CNI.  In typical SoC and FPGA designs, the communication  load is not divided evenly among all modules. Rather, a  handful of modules are hotspot modules (or in short,  hotspots), which communicate with many other modules.  A hotspot can be an interface to external communication  or memory, a master module that communicates with a  number of slaves, a dispatcher that forwards requests  from multiple masters to multiple slaves  [17], etc.  Additional examples are given by  [1], e.g., in one of their  reference designs, an SDRAM module has 7 connections,  and an SRAM module has 4, while other modules have 2  or 3 connections each. We therefore focus on traffic  patterns involving one ore more hotspots, (including a  mapping of  the reference example above  to our  architecture), when evaluating our solutions in Section  5.    Our evaluation shows that WOT routing reduces the link  capacity cost across a wide range of designs. In some  cases, its most loaded link requires 40% less capacity than  the maximum required with XY, and up to 15% less  capacity than with TXY.  We further compare unconstrained placement of hotspots,  where the designer can locate hotspots anywhere on the  chip, with constrained placement. Our results (cf. Section   5) show that constrained placement can significantly  reduce the cost. For example, unconstrained placement of  two hotspots with WOT routing requires 20% more  capacity (in the busiest link) than constrained placement  that dictates that the distance between the hotspots is at  least three hops. With three hotspots, unconstrained  placement requires 35% more capacity than placing the  hotspots at a distance of at least three hops.   Finally, in Section  6, we present implementations of the  four routing schemes studied in this paper. Since all our  schemes can be supported by regular router  [4],  [20] we  implemented only the logic required for the routing  decision leaving out  other router design issues like  buffers and scheduling. We show that all of our studied  routing schemes require very few programmable elements  (at most tens) and hence occupy very little area on the  chip.  In summary, the main contributions of this paper are -   1. A new NoC-based architecture for FPGA, which  balances between the flexibility of soft logic and the  better performance of dedicated logic.   2.  A design methodology for such an architecture.  3. A balanced  in-order routing algorithm for  this  architecture, which minimizes the cost of supporting a  large  set of designs within given performance  requirements.  1.1 Related Work  Sethuraman et al.  [19] propose a fully soft NoC router  design using current-day FPGA technology, which does  not include any burned-in (hard) NoC infrastructure. In      contrast, we propose an architecture for future FPGA  platforms, which is partly embedded supporting in silicon  and partly defined and tuned at configuration-time.  Hard  routers offer better performance, lower power and better  silicon area usage than their soft counterparts.  On the  other hand, any excess NoC resource that is not utilized in  a specific design, cannot be converted to another usage.  DyNoC  [14] is an architecture for adaptive routing using  reconfigurable hardware.  There are several additional  works that present adaptive and dynamic routing, like  DyAd  [11], and Odd-Even routing by Chiu  [6]. Unlike  our solution, these solutions do not perform offline  optimizations at configuration  time;  instead,  they  dynamically changes routing policies on the fly. This  approach is very effective when the traffic pattern is not  known and the network should balance itself during the  runtime. However, an FPGA’s communication pattern is  many times determined at configuration time; offline  optimization can be very effective. The simpler router  design, in turn, reduces hardware cost in terms of area  and power consumption. We therefore focus on static  routing schemes in this paper.   The TXY algorithm was independently developed by Seo  et al. for interconnection networks under the name of  O1TURN  [18]. It has been shown to be worst-case nearoptimal for uniform traffic, where all nodes send and  receive at the same rate  [20]. In this paper, we focus on  typical hardware traffic patterns, where some modules  transmit and receive more than others. For such patterns,  our weighted algorithms outperform TXY. Moreover,  TXY results in out-of-order packet reception and hence  requires large re-order buffers. Towels et al.  [21] use  linear programming for solving general flow problems for  uniform traffic, but do not cover the case of asymmetric  non-uniform patterns as considered herein.   2. High Level Architecture  This section presents the proposed architecture for NoCbased FPGA. It first discusses our proposed hierarchical  chip structure and the advantages of such an organization.  We then discuss how the NoC paradigm is applied to  FPGA. Afterwards, we discuss which parts of the system  are hard-coded on the chip, and which are configurable  (soft). Finally, we discuss design methodology for NoCbased FPGA.   Hierarchical Organization  We propose a hierarchical architecture for future FPGAs,  consisting of two types of regions connected by a NoC:   (1) Configurable Regions (CR), resembling today’s  FPGA, consisting of programmable logic, which is a  collection of possibly thousands of lookup tables (LUTs)  and an internal programmable routing matrix; and (2)  Functional Regions (FR), performing a predefined task,  e.g., general purpose processor, DSP, fast external  interface, etc. FRs are implemented as hard IP cores in  order  to  improve performance and reduce power  consumption.   There are two main reasons for such hierarchical and  modular structure for large programmable chips. First,  CAD place-and-route tools cannot cope effectively with a  flat design including a large number of modules. Even  today, commercial tools like Xilinx's PlanAhead, and  Synplicity's Amplify support a hierarchical design: the  chip designer divides  the flat chip  into regions,  designating which modules reside in each region. Then,  place-and-route is run per region. In the last stage,  connections among regions are wired using remaining  resources. Second, long wires incur a high cost in terms  of delay and power. In modern FPGAs, each LUT can be  wired to any other element on the chip. Utilizing the long  wires between remote elements induces long delays. Our  design eliminates such long wires, and utilizes direct  wires only within the relatively small CRs.   NoC  The regions are interconnected using a NoC.  There are  several reasons to base the future FPGA on NoC. First,  NoC is much more scalable than all other interconnect  solutions, such as point-to-point wires, buses, etc.  [4].  Another reason is spatial reuse, which allows for scalable  power cost compared to the increased routing matrix that  is used in traditional FPGA. Moreover, FPGAs are often  used as prototypes for ASIC. If the ASIC is migrating to  NoC, the FPGA architecture should support NoC as well.  In our architecture, the NoC is a uniform mesh and each  region is connected to a router via a local router interface.  Like other NoCs, we use wormhole routing. The router  structure is very similar to ones in previous NoC designs  (e.g.,  [8], [3], [15], [1]) and includes support for multiple  QoS classes, similar to QNoC  [4]. Consequently, we do  not detail the design here.    While it is possible to also provide direct communication  wires between adjacent CRs not via the NoC, we chose to  avoid such communication in order to simplify the design  and the inter-region communication methodology.   Soft vs. Hard  The NoC consists of several components, like routers,  wires and a network interface. The routers and links are  part of the NoC’s hard-core infrastructure, so as to allow  maximum performance in terms of area and power. As  this hard-coded infrastructure is always laid out, it is  designed to support a large class of applications.    Due to the fact that FPGA has to support a lot of  unknown applications with different traffic patterns, we  suggest a configurable network interface (CNI), which is  a mix of soft and hard parts. Each region is connected to  the network via at least one CNI. A CR may have several  CNIs, which can be configured to work together (when  the CR acts as a single task module) or apart (when a CR  is divided into multiple modules).   The CNI performs the following functions: network  physical interface, buffering, reordering, fragmentation/  reassembly (from application specific blocks to packets      and from packets to wormhole flits), interface to the  region modules and routing support. Most of these  functions are not application-specific, and are therefore  hard coded in order to achieve better power, area, and  performance. However, there are several application  specific functions, like adaptation layers, playback and  routing. The routing logic needs to be flexible, as  different applications have different communication  needs. Therefore, the routing layer and the applicationspecific layers are configurable, i.e., implemented from  programmable elements.  Example Layout  Figure 1. Example of network on programmable chip.  Figure 1 depicts an example chip design using our  architecture. Some links (sets of parallel wires) between  routers are routed across the CRs. This can be done since  the network and CRs are designed by the same vendor at  the same time, and therefore, it is possible to deploy long  wire repeaters at the right places in the silicon of the CRs.  This allows for a regular mesh topology, and avoids the  complexity that can result from an irregular mesh  [18].   Design Methodology  Given the hierarchical chip organization, the design  methodology for programming such FPGA consists of the  following phases: (i) division into high-level modules,  roughly the size of CRs; (ii) placement of high-level  modules;  (iii) implementation of each module within a  region; and (iv) inter-region routing.   In the placement phase, the designer performs the  mapping of  the high-level modules  to  the chip's  functional and configurable  regions. This process  considers the gate count and functionality of the logical  modules, and chooses CRs or FRs that can accommodate  them. Clearly, modules with heavy communication  requirements between them should be placed as close as  possible, on the same CR when space allows.    Next, the routing of the inter-module communication is  performed. The  routing should comply with  the  constraints (capacity) of the pre-built network.   Placement was extensively studied in VLSI – e.g.  [16],  and known techniques are, by-and-large, applicable to our  design. Therefore, in this paper, we focus on the routing  phase of the design, which is unique to NoC-based  FPGA.   3. FPGA Routing Problem Definition  : Vf +ℜ→2 A NoC-based FPGA is provided to the user with a prebuilt NoC infrastructure. This network needs to be  flexible enough to support many user applications that are  unknown during the design of the network. A routing  scheme for this network needs to address the challenge of  providing high  resource utilization with a  low  infrastructure cost. Thus, routing algorithms should be  both efficient and flexible.   This section defines FPGA routing as an optimization  problem. It assumes the placement phase is complete, and  inter-region traffic requirements are known.   A grid is comprised a set of vertexes V and edges E.  Every node on the grid has unique (x,y) coordinates. Each  coordinate ranges between 0 and n-1.   A node represents a module attached to a router, which  can be a region or part thereof.  A traffic pattern is a function . This  function defines the flows between every pair of nodes. A  class of traffic patterns is a set F.    More accurately, these flows should be accommodated by  the link capacities. They do not necessarily represent the  peak traffic on the link, in case queuing delays are  tolerated during peaks. For example, capacity can be  allocated for accommodation 90% of the traffic demands,  implying that say 99% of the traffic has a known delay  bound.  A path from v1 to v2 is a connected sequence of links  that starts at node 1v and ends at node  2v .   A routing algorithm A defines the fractions of the flows  that are sent over all possible routes from every source to  every destination. For example, half of the flow is sent  over one possible route and the other half over another  route.   An ordered routing does not split flows, i.e. each pair is  mapped to a single path.   The link capacity for given A and f is the sum of all the  flows that are routed on this link.   The maximum capacity for given A and f is the capacity  of the most loaded link in the network for a given traffic  requirement pattern. On uniform meshes, this will dictate  the capacity of all links. The maximum capacity reflects  the cost of supporting the given traffic pattern.  A routing algorithm should minimize the maximum  capacity, i.e., the cost for supporting a given f.   The design envelope of a given algorithm for a class of  traffic requirement patterns F is the minimal assignment  of capacities to the links in the NoC graph, which  supports every traffic pattern in F. Each link's capacity in            the envelope is its maximum capacity over all traffic  patterns in the specified class with a given routing  algorithm A. Uniform design envelopes are preferred.   Another metric that is used to evaluate the routing  algorithms is the gate count. Gate count is the amount of  logic required for the implementation of the routing  decision circuit. Clearly, lower gate counts are preferred.   4. Routing Algorithms  We study and propose several routing algorithms and  evaluate them in terms of maximum capacity and design  envelope.  This section presents the routing techniques  and analyzes the efficiency of the algorithms.   4.1 Flow splitting algorithms  This section presents routing algorithms that improve the  network's balance by splitting flows among several paths.  Before presenting these algorithms, we recall the simple  and well-known XY and YX routing algorithms, upon  which they improve.   XY routing first routes packets horizontally, towards  their X coordinate, and then vertically, towards their Y  coordinate. XY is commonly used in NoCs thanks to its  simplicity and inherent deadlock-freedom. However, it  induces unbalanced link loads  [10].   YX routing uses the same technique but reverses the  order of the vertical and horizontal routing.    Toggle XY Routing (TXY)  [20] improves XY's loadbalancing by routing half the packets in the XY path and  the other half in the YX path. Each packet header  includes a bit indicating whether its route is XY or YX.  Each CNI toggles between XY and YX packets. This  scheme avoids deadlocks by using two virtual channels  (VCs) per router: one for XY paths and one for YX with  any fair scheduling scheme between VCs.  Weighted Toggle XY Routing (WTXY) improves TXY  by sending different portions of flows on the XY and YX  paths.   Splitting the traffic evenly between XY and YX routes, as  in TXY, does not always achieve optimal load balancing.  In some cases, sending a different portion of the traffic by  each route may achieve better load-balancing and thus  reduce capacity requirements. For example, Figure 2  shows how the maximum link capacity is affected by the  fraction of traffic routed XY on a 5x5 grid with two  hotspots at locations (1,1) and (2,1) on the grid, where  each node send the same amount of data to every hotspot.   Here, optimal capacity is achieved when roughly 2/3 of  the traffic is routed XY, offering an almost 20%  improvement over TXY.  25 y YX on ly 20 t i c a p a c x a M 15 10 0 X Y on ly 15 11.94 0 .2 0 .4 0 .6 XY f r ac tion 0 .8 1 Figure 2. The impact of the XY fraction on 5x5 grid  w ith two hotspots at locations (2,1) and (1,1).  WTXY therefore chooses the best XY fraction,  xyc ,  according to given traffic requirements. This fraction is  calculated offline for each application before the chip  programming phase, and is loaded to the CNI with the  rest of the configuration data. The calculation of  xyc is  beyond the scope of this paper since our main focus is on  ordered algorithms.   WTXY can be implemented (see Section  6) using a  random number generator and a comparator in the  network interface that assigns the route bit in the packet  header according to  xyc . Deadlocks are avoided the same  way as in TXY.   4.2 Ordered algorithms  3 ⋅ ) The main problem of the algorithms described above is  the flow splitting. When flows are sent over multiple  routes, the packet arrival order may differ from the  sending order. This requires the receiver CNI to maintain  large reordering buffers. Consider, e.g. a relatively small  window of  n2 outstanding packets,  ( 2n ) nodes  communicating with each hotspot and about 10% of the  nodes are hotspots.  In this case total memory requirement is ( 1.0 n ⋅ 2 2 n . If  n is 7, we have to put enough memory for 16 K packets.  The situation is even worse in case there are multiple  classes of services or more source destination (S-D) pairs.   Moreover,  the  flow  splitting  routing  requires  communication overhead to carry the numbering of the  packets to allow the reordering at the destination.   We therefore propose ordered algorithms, in which all the  packets between the same S-D pair are sent over the same  route. Hence, packets arrive in the order that they are  sent, and no re-order buffer is required.   A simple way to assign routes per flow is Source Toggle  XY - STXY. This algorithm toggles the routes of entire  flows. To create a better spreading of the flows, we use  both source and destination addresses to assign the route.  For example, a bitwise XOR of the source and destination  addresses is used to determine the route. Using this  technique gives us a close approximation to TXY with a  small quantization error since half of the flows is sent on  XY path and the other half on YX.  However, due to the previously shown advantages of  weighted routing, we suggest the Weighted Ordered  Toggle (WOT) algorithm. WOT divides the source                  destination routes in a way that produces best result in  terms of maximum capacity.   The remaining challenge is to develop a technique for  effectively choosing the route of each source-destination  pair. First, we present analytical solution for a single  hotspot and later show heuristic algorithm for the general  cases.  WOT Optimal Solution for Single Hotspot   In case of a single hotspot, we have a single destination  on the square grid and all other nodes on the grid send the  same flow f to it. Consider a hotspot at coordinates (x,y),  as shown in Figure 3 (coordinates range from 0 to n-1).    Figure 3. Location of a single hotspot.  )yx, For each node we define a binary indicator ijr that dictates  whether the flow from node i to node j is routed XY or  YX. We also divide the nodes to four groups - left-down  (LD), left-up (LU), right-down (RD) and right-up (RU).  For example, the LD group consists of the nodes with  both coordinates smaller than ( . In our case, index i  refers to the hotspot node. In order to find the maximum  we show two lemmas.  Lemma 1:  In case of the single hotspot, and if all routes  are either XY or YX, the most loaded link is one of the  links adjacent to the hotspot.  Proof: Suppose by the way of contradiction that the most  loaded link, maxL , is not the incoming link of the hotspot.   Since the routing algorithm performs only one turn  (XY/YX) we have two possible locations of maxL .   (1) maxL  shares one coordinate (X or Y) with the hotspot.  In this case, the flow reaches the hotspot with the addition  of the flow of the hotspot's closest neighbor.  (2) maxL  does not share any coordinate with the hotspot  Without loss of generality, suppose that maxL located at  RU. That means that in order to reach the destination  within one turn, the flow is routed down/left or left/down  (depending on whether maxL  is horizontal or vertical)  and contributes to link of the right of the hotspot.   In both cases the capacity of the adjacent link is larger or  equal to maxL  .                                                                  (cid:131)   Using ijr indicators and the hotspot location (x,y), we can  calculate the flows on the hotspot incoming links. For  example,  the  flow  on  the  left  link  is  . This flow consists of the  flows from x nodes located on the same coordinate, which  are not split, and from the split flows from x(n-1) nodes  residing to the left of the hotspot that send the data using  F left = x +    ∑ ( r ij ) j ∈ ( LU , LD == YX )  ⋅  f , ( ( V F H max max max = max = )up down F YX. We calculate similar formulas to all the incoming  flows.  Lemma 2:  The lowest maximum capacity achieved  when the absolute value of the difference D between  maximum horizontal and the vertical flows on the  adjacent  links  to  the hotspot ( and ) is minimal.  Proof: Suppose by the way of contradiction that it is  possible to reduce the difference. This is done by rerouting any flow (all the flows are the same) from the  most loaded link to any other link. If we reduce the  difference, we reduce the most loaded link which is  impossible.                                                                        (cid:131)  Our algorithm relies on Lemma 1 and 2. In order to find  the optimum WOT route assignment we minimize the  difference D as described by the following equation -   left F rightt F ) , min ( D ) = min H − max V max ( ) = min max ( ( F left , F right ) , max ( F left , F right ) ) while working with binary variables  ijr . The equation is  solved with known techniques like linear programming.  WOT General Heuristic  In general case, when the analytical solution is more  complex we suggest heuristic algorithm. We studied three  techniques –   Iterative Assignment – in this technique we first  calculate the  xyc ratio. The grid is initialized to STXY.  The algorithm serially scans the nodes and changes the  routes in such a way that XY ratio will be as close as  possible to the optimal ratio that was calculated before.   Random Assignment – in this technique we also  calculate the optimal ratio and scan the (S-D) pairs. For  each pair we assign XY routing with the probability equal  to the optimal ratio and YX with the complement  probability. In this way we’ll have approximately the  desired ratio.   Min-Max Assignment – seeks an optimal assignment  without prior knowledge of the optimal ratio. The  algorithm starts with STXY as its initial assignment. In  each step, the algorithm first finds the most loaded link, l,  and then goes over all the S-D pairs that contribute traffic  to l. For each such S-D pair, the algorithm calculates the  cost that would result from changing the route of this pair  (and only this pair) from XY to YX or vice versa. Among  these, the algorithm chooses the one that leads to the  lowest maximum capacity on links affected by the route  change. Subsequently, a new step starts with the new  route assignment. The algorithm converges either after a  predefined number of iterations or upon reaching a local  minimum. In the latter case, WOT can achieve even lower  cost than WTXY, since it is not limited to a single  ratio for the entire network.   The following graph shows the comparison between the  three systems. We compare here by the relative difference  between  the achieved maximum capacity and  the  maximum capacity of the WTXY algorithm. In our  xyc        simulation we randomize the number of hotspots in range  (1-3) and the also the location of the hotspots for grid size  from 5 to 10. For each grid size we perform 1000  simulations. The y-axis is  .   C C max − WOT WTXY max r o r r E e v i t a e l R 0 .3 0 .2 0 .1 0 -0 .1 5 C WTXY max M in-m ax Itera ti ve R and om 6 7 Grid Size 8 9 1 0 Figure 4. Relative overhead for WOT route assignment  schemes vs. WTXY.  We can observe that the min-max scheme produces the  best results and is very close and sometimes even better  then WTXY in terms of maximum capacity.    In the rest of this paper, we use the min-max assignment  for evaluating the WOT algorithm.    Finally, a well known technique for ordered routing is  source-routing, where the source chooses the path to each  destination and sends it in the packet header. We  evaluated the cost of source routing in our architecture  using efficient path assignments. The required capacities  were very similar to those required by WOT, without  taking into account the increase in flow induced by the  large headers used in source-routing. Moreover, source  routing requires more lookup-table hardware for storing  the routes in each source. Therefore, we found sourcerouting to be inferior to WOT in our design.   5. Evaluation  We turn to evaluate the capacity requirements of the  different routing schemes under typical on-chip traffic  patterns using simulations. First four sections show the  results for synthetic examples, and the last section shows  the result of the real-world design mapped to our  architecture.  5.1 Single Hotspot  We first examine traffic patterns in which all nodes  communicate with one hotspot with different routing  schemes.  We compare the results to a theoretical lowerbound, which is computed by dividing the hotspot's  communication requirements by the number of links  leading to it.  y t i c a p a C 20 18 16 14 12 10 8 6 4 2 0 XY TXY S TXY W TXY W OT CORNER CENTER INTERNA L HOR. EDGE V ER. EDGE Loc a tion of the ho t spot Figure 5. Maximum capacity requirements for one  hotspot on a 5x5 uniform grid at various locations  w ith different routing schemes.  We observe that XY requires the highest maximum  capacity in the most loaded link. This is expected, since  XY does not balance the load among links. By alternating  between XY and YX  routes, TXY and WTXY  considerably reduce the capacity requirements, and are  optimal for hotspots located in the center or at a corner of  the grid. When the hotspot’s X and Y coordinates are not  symmetric, that is, when its distance from the edge on one  axis is greater than the other, splitting the traffic evenly  between the XY and YX routes is not optimal, and  WTXY can improve on TXY by up to 33%. Observe that  the WOT algorithm, despite of being in-order, produces a  cost that is very close to that of WTXY.  We further observe that locating a hotspot in the center of  the grid requires the smallest capacity, whereas a corner  hotspot requires the highest capacity. This is because in  the center, the load can be spread evenly among all four  directions. This suggests that smaller link capacity can be  allocated if the user is restricted in her placement of  hotspots. Some hotspot modules provide external  interfaces and must therefore reside at the edge of a chip.  Nevertheless, it is reasonable to require locating them in  the middle of an edge rather than in a corner, and  consequently save 33% in capacity. Figure 6 shows the  histogram of link capacities of the envelope for all  possible hot spot location for all the routing schemes. It is  evident that XY is highly unbalanced, with vertical links  five times as loaded as horizontal ones. In contrast, WOT  and WTXY can satisfy the design envelope with balanced  grids.  20 15 10 5 s k n i L f o r e b m u N 0 0 XY TXY S TXY W TXY W OT 5 10 C apac i ty 15 20 Figure 6. Histogram of link capacities in design  envelopes of all possible single hotspot locations.  5.2 Two hotspots                    Next, we examine traffic patterns involving two hotspots.  We observe that in this case, the cost is highly dependent  on the distance between the hotspots. If the hotspots are  close together, there is a high concentration of traffic in  the area where they are located, whereas if they are far  apart, they barely impact each other, and the capacity  requirements are similar to those for a single hotspot. This  trend is depicted in Figure 7. We conclude that by  restricting the allowed distance between hotspots, one can  save up to 25% capacity.  We can see that the weighted  algorithms present better results then regular and that the  ordered algorithms show costs that are very similar to the  un-ordered version (TXY vs. STXY and WTXY vs.  WOT).   y t i c a p a C 30 25 20 15 1 XY TXY STXY WTXY WOT 2 3 4 M in imum Dis tanc e be tween the ho tspo ts 5 Figure 7. Maximum worst case link capacity required  for two hotspots versus the distance between them.  5.3 Three hotspots  Here, we examine the case of three hotspots. The minimal  distance between the closest pair among the three  hotspots is dominant in determining the load, as is shown  in Figure 8. Weighted algorithms present lower costs here  and the ordered version are very close to the flowsplitting algorithms.  In  total, weighted algorithm  outperforms the regular one about 20%. We can also see  that for an unconstrained placement, WOT is even better.   XY TXY STXY WTXY WOT asymmetry of the traffic pattern increases the influence of  the weight grows. One of the comparison graphs is shown  below. The graph presents average maximum capacities  for various grid sizes. Each grid size was simulated with  100 random patterns.   C . x a M 110 100 90 80 70 60 50 40 30 20 10 XY TXY W TXY S TXY W OT 5 6 7 Grid Size 8 9 1.0=hsP noP send hs , = 05.0 Figure 9. Comparison of routing schemes for  and   ,   for various grid size.  8.0= hsP send We can see that WOT incurs lower costs than all other  routing algorithms despite the constraint of in-order  routing. Further, STXY's maximum capacity is very close  to that of TXY. The following graph shows the average  improvement of the weighted algorithms for the same  case as the previous figure.   1 o i t a R C . x a M 0 . 95 0 . 9 0 . 85 0 . 8 5 Cm ax , wt x y /Cm ax , t x y Cm ax , wo t /Cm ax , s t x y 8 9 6 7 Grid Size Figure 10 – Average improvement of the weighted  algorithms.  y t i c a p a C 40 30 20 1 2 3 M in imum Dis tanc e be tween the ho tspo ts 4 5.5 A Real World Example  Figure 8. Maximum link capacity required for three  hotspots versus minimal distance between a pair of  them.   5.4 Complex traffic patterns  In this section, we compare the routing algorithm using  more realistic data patterns. We use traffic model  described in  [3] . This random model has three parameters  for each node – a probability to be a hotspot, a probability  to send data to a hotspot node and a probability to send  data to a non-hotspot.   We use this model for various values of the probability.  For the symmetric graphs the weighted algorithms  produce similar costs as the non-weighted ones, but as the  In this section we show the results of the routing  algorithms applied to the MPEG 4 decoder design of  Bertozzi et al.  [1]. The logic diagram of the design is  shown in Figure 11 .   Figure 11 – MPEG4 Decoder block diagram.  We can see that the design includes hotspots: the  SDRAM module has 7 connections; the SRAM module                                has 4, while other modules have small connectivity to  other modules.  As discussed in Section  2, the design process has two  phases  (1) Mapping of the logical graph to the NoC grid  and the placement of the modules; (2) routing the interregion flows on the NoC.  In previous examples, we worked with given mapped  designs. In order  to compare  the various routing  algorithms for the reference design, we need to first map  and place its modules in regions in our architecture. We  do so manually. The mapped graph is shown in Figure 12.   deadlock  [18] supports all routing techniques presented in  this paper. The router is a constant part of the hardware  infrastructure laid by the vendor in the programmable  chip. The data width of the router and the maximum  frequency is dictated by the required capacity. The  difference between the routing techniques is in the CNI.  Recall that in our design, the CNI is located in the CR and  is partially implemented using programmable logic to  allow flexibility.   Figure 14 shows the schematic implementation of the  circuits that produce the control bit of the packet header  that determines the routing – XY or YX for each of the  routing techniques.   Figure 12 – MPEG4 mapped design.  The grid has 3 rows of regions. The first and the last rows  contain one CPU each and one large CR with 3 CNIs.  The middle row contains 2 CRs with 2 CNIs each. The  reference design is relatively small and thus mapped to a  small grid. Note that in future designs, modules are  expected to be much more complex.   Figure 13 shows the histogram of the link capacities of  this design for several routing schemes. We can see that  WOT produces the least loaded maximum link (1053  compared to 1539) and generates a balanced capacity  distribution, matching  the results of our synthetic  experiments above.   15 XY YX S TXY W OT s k n i L f o r e b m u N 10 5 0 81 243 405 567 729 891 1053 1215 1377 1539 C apac i ty Figure 13 – MPEG4 links capacity histogram.  6. Hardware Costs  This section analyzes the incremental hardware costs  for  supporting each of the different routing schemes in the  reference FPGA NoC design of Section  2. As noted  above, a simple router with 2 virtual channels to avoid  Figure 14 – Routing circuit implementation.  TXY is implemented with a simple flip-flop that inverts  its state every sent packet.   WTXY  is a  random number generator  (RNG)  implemented using linear feedback shift register (LFSR)  and a comparator that compares the random value to the  predefined threshold (Cxy). If the random is larger than  Cxy – the routing bit is XY, and otherwise it is YX.   The STXY implementation is a bitwise XOR of the  source and destination IDs. The width of the XOR gate  depends on the number of bits required for unique node  ID representation. In our case, the ID width can be  limited to the logarithm of the number of nodes in the  grid -  ( log n ). We implement it with 4-way LUTs’ tree  configured to perform XOR.   WOT routing circuit implementation uses Look-UpTables (LUT) for logic implementation of the routing  decision. 4-way LUT is one of the basic elements of the  FPGA – c.f.  [22] and is enough to implement up to 16  possible destinations. The 4-way LUT is easily expanded  by cascading and multiplexing to the desired width.   We  implemented  the routing decision circuits and  synthesized it using Synplify 8 synthesis tool. In our  implementation we used 5-bit ID for source and  destination identification. In Table 1 we show the actual  and theoretical cost of each routing scheme.    )2 ( Table 1 – Routing circuit comparison for various  schemes.  Notes  LUT  coun t for  n = 5  1  -  LUT count for n  – worst case  Routin g  Scheme  TXY  1                32     32     3  2  ( )2 log ( 2 ⋅ 2n  16 ( 2n ) ) log 2 WOT  STXY  WTXY  We used 16-bits  RNG and 16 bit  comparison  Bitwise  xor  of  log n words.Each  LUT  implements  XOR of 4 bits  Each LUT acts as a  16 bit ROM. We  need  to cascade  several LUTs  to  perform  the  lookup.   This is the worst case. For sparse vectors the logic  implementation can be more efficient.  We can see that WOT routing is very efficient in terms of  hardware. Even though WTXY gate count does not  depend on the grid size at all, for smaller grids (up to  relatively large grids with n = 9) WOT performs better  than WTXY.    All the routing schemes presented here introduce a very  low overhead in terms of area and can be easily  implemented on the chip.   7. CONCLUSIONS  We have presented new hybrid architecture  for  programmable chips based on NoC. We recognized that  the main challenge of such programmable chips is  designing flexible routing scheme to efficiently support  variety of  application on  a pre-built network  infrastructure. We studied routing schemes that can be  used in this architecture, and their impact on the capacity  requirements. We presented a simple yet efficient routing  algorithm, WOT, which can be configured to balance link  loads according to traffic patterns defined when the chip  is configured. Since WOT is an ordered algorithm, it  eliminates the need for large reordering buffers and  reduces the cost of the network. The cost of the WOT  implementation is low.   8. "
Transaction-Based Communication-Centric Debug.,"The behaviour of systems on chip (SOC) is complex because they contain multiple processors that interact through concurrent interconnects, such as networks on chip (NOC). Debugging such SOCs is hard. Based on a classification of debug scope and granularity, we propose that debugging should be communication-centric and based on transactions. Communication-centric debug focuses on the communication and the synchronisation between the IP blocks, which are implemented by the interconnect using transactions. We define and implement a modular debug architecture, based on NOC, monitors, and a dedicated high-speed event-distribution broadcast interconnect. The manufacturing-test scan chains and IEEE1149.1 test access ports (TAP) are re-used for configuration and debug data read-out. Our debug architecture requires only small changes to the functional architecture. The additional area cost is limited to the monitors and the event distribution interconnect, which are 4.5% of the NOC area, or less than 0.2% of the SOC area. The debug architecture runs at NOC functional speed and reacts very quickly to debug events to stop the SOC close in time to the condition that raised the event. The speed at which data is retrieved from the SOC after stopping using the TAP is 10 MHz. We prove our concepts and architecture with a gate-level implementation that includes the NOC, event distribution interconnect, and clock, reset, and TAP controllers. We include gate-level signal traces illustrating debug at message and transaction levels","Transaction-Based Communication-Centric Debug ; Kees Goossens1 2 , Bart Vermeulen1 , Remco van Steeden3 , Martijn Bennebroek4 1 Research, NXP Semiconductors, The Netherlands, f(cid:2)rst.lastnameg@nxp.com 2 Computer Engineering, Technical University Delft, The Netherlands 3 Testable Design and Test of Integrated Systems, Technical University of Twente, The Netherlands 4 Research, Philips, The Netherlands, Martijn.Bennebroek@nxp.com Abstract(cid:151) The behaviour of systems on chip (SOC) is complex because they contain multiple processors that interact through concurrent interconnects, such as networks on chip (NOC). Debugging such SOCs is hard. Based on a classi(cid:2)cation of debug scope and granularity, we propose that debugging should be communication-centric and based on transactions. Communication-centric debug focusses on the communication and the synchronisation between the I P blocks, which are implemented by the interconnect using transactions. We de(cid:2)ne and implement a modular debug architecture, based on NOC, monitors, and a dedicated high-speed event-distribution broadcast interconnect. The manufacturing-test scan chains and IEEE1149.1 test access ports (TA P) are re-used for con(cid:2)guration and debug data read-out. Our debug architecture requires only small changes to the functional architecture. The additional area cost is limited to the monitors and the event distribution interconnect, which are 4.5% of the NOC area, or less than 0.2% of the SOC area. The debug architecture runs at NOC functional speed and reacts very quickly to debug events to stop the SOC close in time to the condition that raised the event. The speed at which data is retrieved from the SOC after stopping using the TA P is 10 MHz. We prove our concepts and architecture with a gate-level implementation that includes the NOC, event distribution interconnect, and clock, reset, and TA P controllers. We include gatelevel signal traces illustrating debug at message and transaction levels. I . IN TRODUCT ION Today’s high-performance systems on chip (SOC) contain many intellectual property (I P) blocks, such as memories, dedicated hardware blocks, and programmable processors. Applications are implemented by a number of concurrent computations threads running on the (programmable) I P blocks. The threads communicate through the SOC interconnect. In the past, the SOC interconnect was single-threaded. As a result, all computation threads were effectively serialised by processing one transaction at a time, offering a simple linear view on the SOC. Today, however, SOC interconnects, such as multi-layer Amba [1] and networks on chip (NOC) [2], [3], support multiple concurrent transactions. As a result, a single thread of control no longer exists in the interconnect, and transactions between different computation threads are not constrained to any particular order in time. Because SOCs comprise multiple programmable processors that interact through a concurrent programmable interconnect their behaviour is very complex, and designing right-(cid:2)rsttime SOC hardware and software has become dif(cid:2)cult [4], [5]. In this paper we address the challenge of debugging Fig. 1. Computation-centric debug (a) vs. communication-centric debug (b). SOCs. Debugging involves observing the SOC in its target environment and controlling its execution (stopping, single stepping, etc.) to ef(cid:2)ciently and effectively locate the root cause of any undesired behaviour. We propose a new debug methodology that is centred on communication and based on transactions. We discuss these aspects in turn. A. Communication-centric debug Monitoring and debugging computation, especially of a single processor, is a mature area for which tools already exist [6], [7]. However, debug of multiple (programmable) I P blocks on a SOC is an emerging research (cid:2)eld. Debugging I P blocks by themselves is not enough, because the complexity of the SOC increasingly resides in the interactions between the I P blocks [4], [8]. Therefore, debug must be conducted at a higher system level, where the computation threads and communication threads interact. Because the interconnect implements the communication, and hence the synchronisation between the I P blocks it is the natural locus for system-level debug, where all transactions can be observed concurrently, (partially) ordered, or fully sequentialised. Figure 1(a) illustrates conventional computation-centric debug, where I P blocks with their threads are monitored, triggering events. Based on these events the debug control can stop and inspect the state of the I P blocks. Instead, we propose to focus on monitoring the communication between the I P blocks (Figure 1(b)), where the debug control manages the interaction between the I P blocks by controlling the interconnect. (Of course, the I P blocks can still be monitored and controlled too, as shown by the dashed lines.) B. Transaction-based debug. Processor software is typically debugged at either the source code, or the processor instruction level. The latter is the lowest level that is still meaningful for the programmer. Processor instructions constitute a natural abstraction level between the software and the processor hardware. But (software) threads also communicate with each other via the interconnect, using transactions. Transactions are the result of processor instructions (such as load and store) that cause activity on the interconnect and other I P blocks such as memories. Transactions therefore are a natural interface between computation and communication. The instruction abstraction is important for integrated hardware/software debug, while the concept of a transaction is fundamental for systemlevel debug (i.e. multiple I P cores). This is con(cid:2)rmed by the use of transaction-level modelling (T LM) for interconnects and SOCs. Transactions allow us to recuperate a globally consistent view on the SOC, as described next. Consistent SOC view. A globally consistent view on both SOC hardware and software is desirable, but has been hard to achieve for the following reasons. First, software debug takes place at instruction level or above, while hardware debug is typically performed at the level of clock cycles. Experience shows that it is hard to correlate the information at these levels. Second, as Figure 2(a) illustrates, there may be no globally consistent state at any point in time (clock cycle). Due to e.g GA L S , SOCs are no longer fully synchronous, which gives rise to non-determinism at the level of clock cycles. I P 2 and I P 3 are in different clock domains that may never have simultaneous clock edges. Moreover, cross-clock-domain synchronisation may be non-deterministic, such that there may not be a single point in time at which the entire SOC (chip) is in a global state that can be correlated with pre-silicon simulation models and test benches [9]. Finally, there may be no point in time where all processors have (cid:2)nished their instructions, as illustrated in Figure 2(a). Transaction-based debug. By abstracting the view on the SOC from clock cycles to transactions, the dark lines in Figure 2(b) may be interpreted as (cid:147)transaction cycles(cid:148) instead of clock cycles. Alternatively, the transaction cycles can be enforced as shown in Figure 2(c), by inserting idle cycles or stretching the clocks. In the example, the entire SOC advances in lock step at the level of transactions. This transaction trace can be correlated to processor instructions, is deterministic, is globally consistent, and can be faster than simulation at the clock or instruction level. The notion of consistency will be further elaborated in Sections II and III. C. Main contributions This paper addresses the problem of debugging of complex multi-processor SOCs with concurrent interconnects. We introduce the new concept of communication-centric transaction-based debugging. Based on a model of transactions and of multi-hop interconnects, we introduce a detailed classi(cid:2)cation of debug scope and granularity. Processor and interconnect monitoring and debug coincide at a few points: the instruction/(cid:3)it level, which is the smallest grain of control; the message level where interactions between master and slave I P blocks are visible, and the transaction level, where master behaviour alone is traced. We further de(cid:2)ne a general debug architecture, based on monitors, I E E E1149.1 test access port (TA P), and several debug interconnects. We apply the debug architecture to the ˘thereal NOC [10], monitors [11], [12], and the debug infrastructure of [13]. In particular, we distribute events raised by monitors using a fast dedicated broadcast interconnect. It stops transaction valid/accept handshakes between the network interfaces (N I) and I P blocks, after which the manufacturing-test scan chains are accessed using the TA P controller to read out the NOC and I P state. Our debug architecture requires no changes to I P , routers, or N I kernels, and very few changes to N I shells. Given that the scan chains and TA P controller are already present in the SOC the additional area cost is limited to the monitors and the event distribution interconnect, which are only 4.5% of the NOC area. The debug architecture reacts very quickly to debug events (it runs at NOC functional speed) to stop the SOC close in time to the condition that raised the event, and to not lose data. The speed at which data is retrieved from the SOC after stopping is acceptable (10 MHz test clock). To scan out around 43000 registers in our example NOC takes approximately 4 milliseconds. Finally, three example gate-level signal traces are included of a debug example at message and transaction levels, to illustrate that the debug concepts and architecture function. In the remainder of this paper we (cid:2)rst introduce models for transactions and interconnects (Section II). In Section III we identify and classify scopes and levels of abstraction in the architecture at which debug can take place. We de(cid:2)ne a general debug architecture in Section IV, which is re(cid:2)ned in Section V. We present our results in Section VI and related work in Section VII, and conclude in Section VIII. I I . TRAN SACT ION AND IN T ERCONN ECT MOD E L S In this section we de(cid:2)ne the general model for transactions, and an interconnect model tuned for NOCs. The models serve as input for the classi(cid:2)cation of debug scopes and abstractions of Section III. A. Transaction Model I P blocks interact on ports, either directly or using an interconnect, and use transactions. A transaction is initiated by a master port, by sending a request that is executed by the I P block attached to the receiving slave port. (For brevity, in the remainder for (cid:147)master(cid:148) read (cid:147)master port,(cid:148) and for (cid:147)slave(cid:148) read (cid:147)slave port.(cid:148)) The execution may result in a response that the slave sends to the master. A message is a request or a response. Typical requests include read and write commands; read data and write acknowledgements are typical reponses. Both communication protocols based on distributed shared memory (e.g. D T L [14], AX I [15], and OC P [16]) and message passing (no responses) (cid:2)t within this model. The master and slave are connected by an intermediate interconnect, as shown in Figure 3. Between every pair of blocks, a request is passed from the initiator to the target; the Fig. 2. Globally consistent state of a SOC , is absent at clock-cycle level (a), but is possible at the transaction level (b) and (c). response travels in the opposite direction. Hence, the master is the (cid:2)rst initiator and the slave the last target. Requests and responses can be transferred independently, using a valid-accept handshake.1 The initiator offers a request to the target by driving the valid signal high. The target in turn indicates that it has accepted the request by driving the accept signal high. For responses, the roles of initiator and target are reversed. A transaction starts when the master signals that the request is valid, and is in progress until the transaction completes, which is when the master has accepted the response. Depending on the protocol, transactions may either be split (the request and response handshakes are independent, e.g. AX I) or not (request and response handshakes coincide, e.g. A PB). In addition, split transactions can either be pipelined (i.e. allow multiple outstanding requests, e.g. AX I), or not. The transaction model describes the interactions between the I P blocks and the interconnect. Next, we turn our attention to the details of transaction transportation. B. Interconnect Model In this section we describe a general multi-hop interconnect model. It covers connection-oriented NOCs such as Mango [17], Nostrum [18], ˘thereal [10], and FAUST [19], and connection-less NOCs [20], [21], [22], but also hierarchical busses [23], [15]. A NOC con(cid:2)guration (or use case) [24] is a set of connections [25]. A con(cid:2)guration is either explicitly declared in connection-oriented NOCs or implicitly de(cid:2)ned by the masterslave communication pattern in connection-less NOCs. Transactions between a single master and one or more slaves take place on a connection. All transactions on a single connection are ordered. This means that the order of the requests offered to the NOC by the master, and the order of the requests offered to each slave by the NOC, are equal to the order of 1 To be more precise, the request and response can be subdivided in signal groups; for e.g. AXI, the request comprises the command group and the write data group, and the reponse comprises the read data group. The validaccept handshake then typically takes places per word and independently for each signal group. Depending on the protocol there may be restrictions on interleaving, ordering, and pipelining. For example, the write data may come before the write command in AX I , but not in AHB . AX I allows message interleaving, where D T L and AHB do not. In this paper, to simplify presentation, we use the handshake at the level of messages instead of signal groups. Our concepts apply unchanged also to signal groups. Fig. 3. Architecture model. the responses offered to the master by the NOC.2 (We assume that on a connection, the slave offers responses in the same order as it accepted the requests, regardless of the precise order in which it executes them.) There is no execution order speci(cid:2)ed between transactions of different slaves on the same connection, nor between transactions on different connections. In the former case, executions can be forced to be in order by disallowing multiple outstanding transactions to different slaves. In the latter case, however, no ordering can be enforced due to the concurrent nature (distributed arbitration) of NOCs. Multi-hop interconnects are composed internally of multiple stages ((cid:147)hops(cid:148)), as shown in Figure 3. NOCs are mostly packet based, which means that messages (requests or responses) are internally transported in (almost always) smaller packets. Routers transport packets, and network interfaces (N Is) convert messages to and from packets. Inside the kernel and router network packets are usually split in smaller units, called (cid:3)its, which consist of a (cid:2)xed number of words. A N I is often split into separate components, the N I kernel and the N I shell [26]. The kernel and routers perform the O S I [27] network layer functions, i.e. move data from one N I kernel to another. The shells implement the O S I transport layer, which in the present context could be called the transaction layer. The shells convert I P transactions received on master or slave N I ports into serial data for the kernel. All knowledge regarding I P protocols (and possible conversions) resides in the shells. As a result, kernels are highly re-used, even across I P-block protocols, and shells are re-usable in systems that 2A number of protocols require in-order reads and in-order writes, but allow re-ordering between them. This does not essentially change the argument. Fig. 4. Transaction ordering and interleaving. utilize the same I P-block protocols. Packet boundaries are always aligned with (cid:3)it boundaries. Most NOCs also align messages and packets, i.e. where a message header is always immediately preceded by a packet header, at the cost of packetisation ef(cid:2)ciency [26]. Sometimes message and packet headers are even merged to increase ef(cid:2)ciency. In both cases, kernel and shell functions are combined at the expense of easy re-use. ˘thereal does not align message and packet boundaries, which complicates debug [12]. To avoid buffer over(cid:3)ows and possible data loss in the N I kernels and routers, NOCs use link-level (cid:3)ow control (L L FC): an initiator (router or N I kernel) sends a (cid:3)it to the target (router or N I kernel) only if there is space in the target buffer. To avoid data loss and deadlock, end-to-end (cid:3)ow control (E 2 E FC) is an independent mechanism that ensures that connection buffers in the N I kernels do not over(cid:3)ow. That is, data only leaves a sender’s N I kernel buffer when there is space in the receiver’s N I kernel buffer. All NOCs offer a best-effort service (BE) and a number of NOCs also offer a guaranteed service (G S) [17], [18], [10], [19]. L L FC is used by all NOCs except Nostrum for BE . E 2 E FC is used by SPIN for BE , and by ˘thereal and FAUST for BE & G S . Note that ˘thereal and Nostrum do not use L L FC for G S , because contention is guaranteed to be absent. In the next section, we classify debug activities based on the transaction and interconnect models. First, we illustrate the transaction model. C. Example Figure 4 shows an example of the concepts introduced above. Master 1 uses split pipelined transactions because it sends request 2 before response 1 has arrived. Requests and responses may overlap; e.g. response 1 and request 3 in Figure 4[A]. Requests 1-3 of master 1 are executed by different slaves: 1, 2, and 1, respectively. Slave 2 executes request 2 before slave 1 executes request 1 (Figure 4[B]), even though the latter was accepted earlier by the NOC. This can occur when request 1 took longer to arrive than request 2 (e.g. due to a longer path, more congestion, less reserved bandwidth, or lower QoS), or when slave 1 is slower than slave 2. Whatever the execution order, responses are always offered to the master in the correct order (Figure 4[C]). However, note that transactions of different connections are unordered. For example, request 2 of master 1 and request 4 of master 2 are Fig. 5. Scope and granularity of debugging. executed and completed in the reverse order of acceptance by the NOC, cf. Figure 4[D]. Finally, note that master 2 does not use split transactions: transaction 4 completes before transaction 5 is started. I I I . SOC AND NOC D EBUG In this section we de(cid:2)ne various scopes of debug: SOC, I P , and NOC. Then we de(cid:2)ne the temporal granularity at which we can debug, e.g. clock cycles, (cid:3)its, messages, and transactions. We show that communication-centric debug based on transactions offers a good combination of spatial and temporal localisation of SOC problems. A. Debug scope Until now, SOCs have been debugged predominantly by concentrating on the computation, through monitoring and debugging of the programmable processors. However, SOC complexity is shifting from computation on a single processor to the interaction between the multiple processors. In addition, interconnects are increasingly complex, and no longer impose a single thread of control through transaction serialisation. The scope of debug therefore extends beyond the traditional debugging of processors, to include debugging of the interconnect, and system-level debug that concentrates on the interactions between I P blocks. Therefore, acting on events from the monitors, we control the interaction between I P blocks through the interconnect (Figure 1(b), instead the internal behaviour of the I P blocks (Figure 1(a)). B. Debug granularity In the following we restrict our discussion to NOCs and other multi-hop interconnects. We can distinguish various levels of granularity at which a NOC can be debugged. The horizontal axis in Figure 5 shows a number of levels at which a NOC can be monitored and potentially stopped. First, we discuss each of the cycle, (cid:3)it, link-level (cid:3)ow control (L L FC), packet, endto-end (cid:3)ow control (E 2 E FC), message, and transaction levels in turn. Then we discuss the vertical axis of processor debug granularity. Stopping a NOC at the clock cycle or (cid:3)it granularity halts all BE and G S data in the NOC in situ, i.e. in the N Is and routers. Both stop methods function well in synchronous NOCs, and the latter also in asynchronous NOCs, where N Is and routers handshake at least at the level of (cid:3)its. The former method can be achieved by gating the NOC clock, the latter method by masking the (cid:3)it handshake between routers (forcing the (cid:3)it valid signal to ‘0’). Stopping a NOC at the level of L L FC can be implemented by masking the L L FC credit counters to 0. As a result N Is and routers do not send data anymore because it seems as if all receiving buffers are full. The (cid:3)it granularity coincides with the L L FC granularity when all traf(cid:2)c types use L L FC. This is not the case for ˘thereal and Nostrum that do not use L L FC for G S traf(cid:2)c. Stopping L L FC stops all BE traf(cid:2)c in situ, but all G S traf(cid:2)c continues to be transported. Stopping a NOC at the granularity of packets is only useful in two degenerate cases. First, if L L FC is performed at packet level (store and forward), reducing it to L L FC granularity. Second, if packets and messages coincide, when it corresponds to the message level, described below. Stopping a NOC at the granularity of E 2 E FC by masking E 2 E FC allows all packets that are in the router network to continue to their destination N Is, but prohibits packets from leaving the N Is. This (cid:3)ushes the router network, and all useful state remains in the N Is. However, E 2 E FC is often performed at the granularity of words, and masking the E 2 E FC can therefore result in messages being split over the sender and receiver N Is. NOCs that perform the local E 2 E FC [28] at the level of messages do not suffer from this problem. By stopping a NOC at the level of messages, i.e. requests and responses, we intend that packets, L L FC, and E 2 E FC continue to operate. The NOC (cid:2)nishes message handshakes that are in progress with the I P blocks. This can happen at four places (cf. Figure 3): the request at the master, the request at the slave, the response at the slave, and the response at the master. This may in fact take a number of clock cycles (see footnote 1). For example, the NOC continues with master requests until all write data words have been accepted. This method may be implemented by masking the valid and/or accept of the request and response at the master and slave to ‘0’. We give more details in Section V-B. The coarsest granularity of stopping is at the level of transactions, i.e. when all outstanding messages have completed. In this case, essentially the NOC and all slaves continue operating. This may be implemented by masking the request accept to ‘0’ at the master N I ports: no new transactions are accepted by the NOC, and all outstanding transactions are completed. Figure 2(c) is an example of a trace where all I P blocks execute a single transaction at a time. However, this level is limited for two reasons. First, if an I P block uses pipelined transactions (e.g., master 1 in Figure 4), then there may be no time at which all transactions have completed because the I P block wants to issue a request before accepting a response. Second, the complexity of SOCs resides in the synchronisation of I P blocks, i.e. in the concurrency and interleaving of transactions. Message-level debug exposes the ordering of messages at both masters and slaves. But transaction-level debug exposes the ordering of transactions at the masters only, which could also be obtained from I P-block debug alone. Therefore, because not all NOCs are synchronous, many NOCs do not have E 2 E FC, and not all NOCs use L L FC for all traf(cid:2)c classes, the most generally useful NOC debug granularities are the (cid:3)it level and the message level. At the (cid:3)it granularity the data is stopped in situ, i.e. in the routers & N Is, whereas at the message level, the router network is (cid:3)ushed and all messages gather in the N Is. The former is required to debug the NOC, and the latter is most suited to debug the SOC, i.e. the interactions between the I P blocks and the interconnect. Processors: Although a (cid:2)ne-grained distinction can be made for processors, like we have done for NOCs, here we restrict ourselves to processor debug at the level of clock cycles, instructions, messages, and transactions (vertical axis in Figure 5). Instructions are the lowest level that a programmer can relate to, messages expose the interleaving of master and slave threads, whereas transactions show only the master view. Conclusion: The important conclusion that can be drawn from Figure 5 is that processor and interconnect monitoring and debug coincide at a few points that are marked in Figure 5: the instruction/(cid:3)it level, which is the smallest grain of control; and the message level where interactions between master and slave I P blocks are visible. The other points serve to debug either the NOC or processors internally, or debug only the master I P blocks. C. Debug actions To debug, relevant parts of the system must be monitored. Monitors generate an event whenever something interesting is observed. The debug infrastructure can react on events by sending information out of the chip, stopping (part of) the SOC, etc. When the SOC is stopped its state can be read out and/or changed, before continuing operation. Traditionally, it may be possible to continue by performing a single step on all or some of the clocks (or I P blocks), or by resuming full operation. Because we focus on the communication rather than the computation, we can enforce (cid:147)single stepping(cid:148) on any of the debug granularities described before. As an example, consider Figure 4. If the NOC stops accepting new messages from initiators and stops offering new messages to targets at (cid:147)message time(cid:148) 1, then requests 1, 2, and 4 have been accepted from the masters, but not yet been offered to the slaves. By allowing a single message step on both slaves we advance to message time 2. Another single step on the slaves executes request 4 and advances to time 3, and so on. In this paper, we de(cid:2)ne and implement the functionality that enables stopping, single-stepping, etc., but leave the use of this infrastructure for future work. IV. G EN ERA L D EBUG ARCH I T ECTURE In this section we give an overview of the general communication-centric debug architecture. The details follow in the next section. Figure 6 shows the hardware debug architecture, with the functional NOC at the centre. For clarity only the request signals between master, NOC, and slave are shown. The response architecture is similar. and (2) between slave N I port (SN I P) and slave; similarly for responses between (3) slave and SN I P , and (4) between MN I P and master. As shown in Figure 6 by the two AND gates, this is accomplished by masking the request accept signals at the MN I P (1) and the request valid signal at the SN I P (2). Omitted from the (cid:2)gure is the similar masking of the response accept signal at the SN I P (3), and the reponse valid signal at the MN I P (4). The signals are masked when an event has been received on the ED I (more on this below), and when the (cid:2)nite state machine of the N I port in the N I indicates that the message is complete. The latter information ((cid:147)end of message(cid:148) or EOM in the (cid:2)gure) is already present in N I for transactionsafe connection recon(cid:2)guration [26], [30]. It is read out nonintrusively by the stop module from the N I shell, as indicated by the dashed arrow [Y]. The event distribution interconnect determines when events arrive at the N Is, and hence when the message handshakes are masked. For transaction-level debug only the message handshake between the master and the MN I P is masked, using the same infrastructure, as soon as the current message has (cid:2)nished. C. Event distribution interconnect The event distribution interconnect (ED I) delivers events from the monitors and other event generators, such as the TA P controller, to the relevant debug components, such as the N Is and TA P controller. Ideally, when an event is generated anywhere in the SOC, all ongoing handshakes of messages must be (cid:2)nished, and no new handshakes must be initiated. Essentially, this requires global single-cycle event distribution, which is not scalable and dif(cid:2)cult to implement in lay-out. Our ED I is the best alternative: events are broadcast synchronously at the (high) NOC functional frequency by pipelined stop modules. A stop module sends incoming events to all its neighbours: to ensure the broadcast dies out it does not respond to incoming events in the next cycle. The ED I is a scalable solution, with minimal latency (1 cycle per stop module), to minimise the number of new message handshakes starting after the event occurred. Note that an asynchronous ED I implementation is quite possible, and (cid:147)cycle(cid:148) is then replaced by (cid:147)handshake.(cid:148) The ED I uses the same topology as the NOC, and can be placed and routed alongside the routers and N Is, to avoid changes to the top-level lay out, and to ensure that it runs at the NOC functional frequency. This is suggested in Figure 6 by positioning the related blocks vertically above one another. With a (cid:3)it size of three, the ED I propagates events three times faster than the data that caused the event; this is quick enough to distribute an event to all stop modules before the message on which the monitor triggered can leave the NOC ((cid:2)nish its handshake). The critical timing for this occurs when a monitor is attached to the link between a router and the destination N I . If the monitor triggers on the (cid:2)nal word (in a (cid:3)it) of a message, then that message must be the last one to complete its handshake on that N I port. This is achieved because the next (cid:3)it, part of the next message, has latency of at least a two cycles before being offered to the I P block [26]. The event arrives at the N I shell also two cycles after it has been generated, and is in time to mask the appropriate valid or accept signal. Stop modules tell the N I shells to (cid:2)nish ongoing messages and then stop, as described previously. However, if an ongoing message does not (cid:2)nish, e.g. due to a non-responsive / stopped I P block, in(cid:2)nitely long message, or message-level interdependencies, a second stop signal can be used to forcefully stops all ongoing messages. The (cid:2)rst stop signal can be generated by both monitors or the TA P controller, but the second stop signal can be given only by the TA P controller. The stop monitors contain a small F SM , which implements this stop sequencing, and ensures that the broadcast dies out and that multiple concurrent events are handled correctly. For example, two monitors can trigger simultaneously. Or a monitor can trigger after some or all of the N Is are already masking the message handshakes because it may take some time to (cid:3)ush the NOC, i.e. allow all data to arrive at the slave N I ports. D. Debug data distribution interconnect The debug data interconnect (DD I) can be a dedicated interconnect, such as a bus. Alternatively, existing interconnects can be re-used, such as the functional NOC, or manufacturingtest scan chains accessible via an I E E E1149.1 test access port (TA P) [31]. The functional NOC can be re-used as DD I , but the process of (cid:3)ushing the NOC of functional data before it can be used to transport debug data is non-trivial. If it has to be possible to resume operation after stopping and debug, the functional data that was (cid:3)ushed out of the NOC must be restored, which is also dif(cid:2)cult. It is standard industry practice to use scan chains [32] to test for manufacturing defects. The entire NOC (and all I P blocks) will therefore typically contain scan chains. For this reason, we use scan chains to implement the DD I , as proposed by [33], [29], [13]. We use NX P’s standard design (cid:3)ow with gate-level synthesis and scan-chain insertion for the NOC, with an exception for the optimised hardware FI FOs used in the routers [34] and N Is [26], which contain a dedicated test infrastructure [35]. I E E E 1 1 4 9 . 1-compliant scan-based manufacturing-test and debug infrastructure is accessed using a TA P . Using the TA P data can be sent in and out of the chip over four or (cid:2)ve dedicated chip pins. The NOC and every I P block have a test wrapper and test control block (TCB) to isolate and control the block during manufacturing test, respectively. They also have an access-control test point register (AC - T PR) to select which internal scan chains to route to the TA P using the test access mechanism. In this way, the infrastructure that is used for manufacturing test is largely re-used for debug. SOCs often have sophisticated programmable clock generation that allows modifying the clock signal per I P block at run time. The clock controller switches each I P block’s clock between off, one or more functional frequencies, one or more test frequencies, and a debug frequency. The test and debug infrastructures are independent from the functional interconnect in terms of wiring. However, the test and debug infrastructure cannot operate at the same time as the functional interconnect because they access the same state (registers and FI FOs) and because they operate at different frequencies (the functional NOC frequency is much higher). The TA P controller orchestrates the interactions between the TCBs and clock controller 1) to manage the transition from functional state and clock to the test or debug state and clock for each block, 2) to obtain test or debug access to a block, 3) to use the AC - T PRs to select the chain chains of the block, and 4) to use the TA P to transport the scan data into or out of the chip. The advantage of re-using the test and debug infrastructure as DD I is that it comes at virtually no extra cost because all I P blocks already contain the required hardware. The disadvantage is the relatively low speed at which they run (10 MHz), compared to a functional interconnect. Moreover, access to the state is sequential per scan chain, where a dedicated functional interconnect [36] could provide faster memory-mapped access, although at a higher area cost. Reading and modifying the I P and NOC state can be offered by both alternatives. E. Debug control interconnect In this section we describe how the monitors, ED I , and DD I are programmed and together offer message-based communication-centric debug. The debug infrastructure (monitors, the DD I , clock controller) is programmable at run time. To be precise, it contains a number of programmable registers, to specify information such as: the data pattern to be matched by the monitor [11], [12], AC - T PR scan chain selection, the run / stop / second forced stop state of the ED I , the run / stop state of each N I shell and I P block, and the clock control registers. The task of the DC I is to provide access to these registers using the TA P . The DC I can be implemented with a dedicated interconnect, such as a (low-speed) control bus (e.g. A PB). Alternatively, existing interconnects can be re-used, such as the scan chains (extending [29], [37]) or the functional NOC (proposed for monitors in [11]). In our experiments we use the manufacturing-test scan chains for the DC I , like we did for the DD I . All debug control registers are accessible via control scan chains that are separate and independent from the (data) scan chains containing the state of NOC and I P cores. The control registers are readable and programmable also when the SOC is in functional mode. The control scan chains are accessed via the TA P at the debug clock frequency. To program the monitors, and to read out NOC registers after a monitor event caused the NOC to stop, the following steps are performed: 1) On reset the TA P controller, TCBs, ED I and monitors are disabled. 2) Using the TA P (Fig. 6[A]), from outside the chip at the debug clock frequency, program the monitors with the desired pattern to be matched. 3) An event generated by a monitor is indicated in a monitor register, and is distributed by the ED I to all N Is, and all relevant valid and accept signals are masked (Fig. 6[B]). 4) At the same time, using the TA P and DC I poll from offchip if the event has already occurred by reading out the stop module registers (Fig. 6[C]). Note that these registers are in the (fast) functional clock domain, and that they are polled from the (slow) debug clock domain through the control scan chains. Polling takes place independently from the NOC that continues to operate at its functional frequency. This is not a problem because once stopped, the debug state is stable. 5) When an event has been detected, wait until all ongoing messages have terminated and the NOC is in a quiescent state. This can be checked by polling the (cid:147)stopped(cid:148) debug status registers in the N Is, like the stop modules debug status registers. Alternatively, a second stop can be sent via the TA P controller (Fig. 6[A&C]) to forcefully stop all ongoing message handshakes, whether they have (cid:2)nished or not. Note that because the debug clock frequency is much lower than the ED I frequency, the rising edge of this TA P stop signal is detected and used to accomplish this safely. 6) Then, the clock controller is programmed via the TA P (Fig. 6[D]) to switch the NOC and/o I P blocks from the functional to debug clock. 7) At this point, the TA P controller and the NOC and I P blocks all operate at the debug clock frequency, and the state of the NOC and I P blocks (registers and hardware FI FOs) can be read out and/or modi(cid:2)ed (Fig. 6[F]) via the TA P (Fig. 6[G]) after programming the AC - T PR registers, which select the scan chains for scan out (Fig. 6[E]). These steps are performed by off-chip debug software. All steps except 5 are required for traditional computation-centric debug. The following section illustrates some of these steps with simulation results. F. Design (cid:3)ow To apply the debug concepts and architecture we use the standard NX P design (cid:3)ow. The NOC is synthesised with gatelevel synthesis tools, resulting in a netlist. Scan chains are then inserted in the netlist. The clock controller, reset controller, and TA P controller are inserted, before the design lay-out can be generated. In our example, we used register-based FI FOs instead of the optimised hardware FI FOs, to simplify the design (cid:3)ow. V I . R E SU LT S We applied the debug architecture described in the previous section to a simple network containing two routers connecting two masters and two slaves, each with a dedicated N I . We focus on one master-slave pair, the other pair just generates traf(cid:2)c on the shared link between the two routers. In general, the number of monitors depends on the desired coverage of NOC components and I P blocks [38]. Here, each router has a monitor, which can observe all of its incoming links. A. Performance and cost 0:006mm Our proposed architecture is modular, and the monitors, event and data and control interconnects can be dedicated or re-used, and be different or the same. We have chosen for a dedicated event distribution interconnect (ED I) to ensure a quick reaction to event, to minimise missing data when the NOC is stopped. The ED I makes use of the NOC clock and lay-out, and therefore scales well in size. The data and control distribution interconnects (DD I and DC I) re-use the existing scan chains. In terms of area, the cost of the monitors depends entirely on the desired functionality. Our monitor occupies 2 in 0:13-micron CMO S , most of which is due to its programmable registers. The ED I consists of as many stop modules as there are routers, two in our example. The area of 2 ). The DD I , DC I , and a stop module is negligible (0:0002mm TA P controller do not add any area because they are present in all designs. The total area of the simple NOC was 2:26mm 2 , and the debug infrastructure added 4.5%. If the NOC area is around 4% of that of a SOC [39], the cost of SOC-level debug is less than 0.2%. Speed. The ED I operates at the NOC functional speed, or 500 MHz in 0.13 micron CMO S . It can run faster, but we chose to share the NOC clock, for ease of integration. The maximum time it takes the ED I to distribute an event is equal to the the NOC functional clock period times the longest path in the NOC, assuming monitors are attached to N Is and/or routers. The entire test and debug infrastructure, i.e. DD I , DC I , TA P , and TA P controller, runs at 10 MHz. To scan out the 43050 registers in our small NOC only takes 4.3 milliseconds. In practice, the scan-out speed is much lower, around one state dump per second, due to restrictions in memory size of debugger hardware, and the low ef(cid:2)ciency of board to PC communication. This is suf(cid:2)ciently fast because the human debugging the system usually requires more time. B. Simulations Figure 7 contains three sets of traces that show the debug architecture in action. The master communicates with a slave on D T L ports, via intermediate masterside NI shell, masterside NI kernel, two routers, slaveside NI kernel, and slaveside NI shell. The traces show the request and response signals at the masterside NI shell port (MN I P) and at the slaveside NI shell N I port (SN I P). The former is a target port, the latter an initiator port. Normal operation: The top trace in Figure 7 shows normal operation, where the MN I P accepts four commands from the master, shown by the four marked pulses on the dtl cmd accept (labelled (cid:147)write(cid:148) and (cid:147)read(cid:148)) immediately below the clock signal. The (cid:2)rst and third command are writes (dtl read cmd is low), the others are read commands. The last of the eight write data words ((cid:147)wdata(cid:148)) of the (cid:2)rst write command is signalled by a ‘1’ on the dtl write last signal. The write command and the write data are transported from the MN I P to the SN I P and offered to the slave (dtl cmd valid is high), as illustrated by the solid arrows. Note that the write data that was offered contiguously to the MN I P arrives spread over time at the slave, because the master and slave using a G S connection that happens to use non-contiguous TDMA slots. Similarly, as illustrated with the dashed arrows in Figure 7, the read command is accepted by the MN I P , transported to the SN I P and then offered to the slave (dtl cmd valid is high). The slave responds with read data ((cid:147)rdata(cid:148)), which is transported and offered to the master (dtl rd valid). The master accepts the read data before offering another write and read command. Note that the writes are posted, and that the write and read commands are pipelined. Transaction-level debug: In this scenario (Figure 7, middle set of traces), the monitor at the router connected to the slave N I triggers an event and generates a stop signal for all N Is. The event is raised immediately after the (cid:2)rst read command. For transaction-level debug, only the master N I reacts to the stop signal (stop in, in dashed circle). Thus, as during normal operation, the write and read commands are offered to the slave that reacts as before. The master also accepts the read data, and thus (cid:2)nishes all outstanding transactions. As explained in Section V-B, the N I shell tracks the completion of messages. It does not accept any new commands after the stop event, even when the master offers a new write (dtl cmd valid is high). This is illustrated by the absence of the second write and read commands. The N I shell also tracks the number of outstanding transactions ((cid:147)livetrns(cid:148)), which goes low after the (cid:2)rst read has completed. This, together with the asserted stop signal, blocks the shell ((cid:147)blocked(cid:148) goes high, see dashed circle). The TA P controller polls the blocked signal of the master N Is to observe when there is no longer any activity in the NOC. At this point it can lower the NOC clock from functional frequency to the test frequency, and scan out all NOC state. This is not shown in this and the next signal trace for lack of space. Message-level debug: Message-level debug (Figure 7, bottom set of traces) is similar to the transaction-level debug. The same event is raised but now results in a stop signal to both master and slave N Is. The stop signal originates from the router that is closer to the slave N I , and therefore reaches the slave N I earlier than the master N I . In fact, it reaches the slave N I before the write command and data do. As a result, the message handshake is not initiated, and the N I shell keeps the write command and data in its FI FOs and does not offer them to the slave. This is evident from the absence of a pulse on the dtl cmd valid. Note that unlike for transaction-level debug, the master N I shell does not enter a blocked state because the write request remains pending at the slave N I . V I I . R E LAT ED WORK In the domain of multi-processors and their programming [8], [40] describe a system to reproducibly replay parallel program executions by saving traces between different computation threads. [41], [42] aim for the same by focussing on interactions using shared variables. These works focus on post-hoc replay and analysis of parallel programs, whereas we focus on actively controlling the concurrency. Fig. 7. Gate-level signal traces. A good overview of SOC debug can be found in [5]. Our work is based on TA Ps, like [33], [29], [37], [13]. Their approach is computation-centric whereas our approach is communication-centric instead. [36] proposes TA P-based debug. In contrast to our DD I , their TA P controller uses the functional interconnect to access the state of processors. This assumes that the functional interconnect is stateless, which is not the case for NOCs. The NOC would have to be (cid:3)ushed of data before use by the TA P controller, and even more dif(cid:2)cult, the state would have to be reinstated before single stepping or resuming operation. [43] tackles the problem of debugging multiple processors using simulation with instruction-set simulators. The interconnect, or silicon debug are not considered. [44] proposes synchro-tokens that are similar to our (cid:3)it handshake for test and debug of GA L S SOCs. However, they do not use their method for a communication-centric debug as proposed here. V I I I . CONCLU S ION S In this paper we addressed the debugging of complex SOCs. This is hard because they contain multiple processors that interact through concurrent interconnects, such as NOCs. We classify the scope and temporal granularity of debug. We show that the scope of debug extends beyond the traditional debugging of processors, to include debugging of the interconnect, and system-level debug that concentrates on the interactions between I P blocks. Therefore, our debug methodology is communication-centric. Furthermore, processor and interconnect monitoring and debug coincide at a few points: the instruction/(cid:3)it level, which is the smallest grain of control; the message level where interactions between master and slave I P blocks are visible, and the transaction level, where master behaviour alone is traced. Based on these insights we de(cid:2)ne and implement a modular debug architecture, based on a NOC, monitors, and a dedicated high-speed event-distribution broadcast interconnect. The data distribution interconnect (DD I) and debug control interconnect (DC I) re-use the manufacturing-test scan chains and I E E E1149.1 test access ports (TA P). To apply our debug concepts of the functional SOC only the N I shells require changes. The additional area cost for debug is limited to the monitors and the event distribution interconnect, which are 4.5% of the NOC area, or less than 0.2% of the SOC area. The debug architecture runs at NOC functional speed and reacts very quickly to debug events to stop the SOC close in time to the condition that raised the event. The speed at which data is retrieved from the SOC after stopping is 10 MHz, which is suf(cid:2)cient. We proved our concepts and architecture with a gate-level implementation of a small SOC, consisting of behavioural I P blocks, gate-level NOC with scan chains, the broadcast event distribution interconnect, and clock, reset, and TA P controllers. Gate-level signal traces illustrated debug at message and transaction levels. "
Implementation and Evaluation of a Dynamically Routed Processor Operand Network.,"Microarchitecturally integrated on-chip networks, or micronets, are candidates to replace busses for processor component interconnect in future processor designs. For micronets, tight coupling between processor microarchitecture and network architecture is one of the keys to improving processor performance. This paper presents the design, implementation and evaluation of the TRIPS operand network (OPN). The TRIPS OPN is a 5times5, dynamically routed, 2D mesh micronet that is integrated into the TRIPS microprocessor core. The TRIPS OPN is used for operand passing, register file I/O, and primary memory system I/O. We discuss in detail the OPN design, including the unique features that arise from its integration with the processor core, such as its connection to the execution unit's wakeup pipeline and its in flight mis-speculated traffic removal. We then evaluate the performance of the network under synthetic and realistic loads. Finally, we assess the processor performance implications of OPN design decisions with respect to the end-to-end latency of OPN packets and the OPN's bandwidth","Implementation and Evaluation of a Dynamically Routed Processor Operand Network Paul Gratz ∗ ∗ , Karthikeyan Sankaralingam , Heather Hanson , Premkishore Shivakumar Robert McDonald , Stephen W. Keckler , and Doug Burger † † † Department of Electrical and Computer Engineering, The University of Texas at Austin Department of Computer Sciences, The University of Texas at Austin cartel@cs.utexas.edu † ∗ † † Abstract— Microarchitecturally integrated on-chip networks, or micronets, are candidates to replace busses for processor component interconnect in future processor designs. For micronets, tight coupling between processor microarchitecture and network architecture is one of the keys to improving processor performance. This paper presents the design, implementation and evaluation of the TRIPS operand network (OPN). The TRIPS OPN is a 5x5, dynamically routed, 2D mesh micronet that is integrated into the TRIPS microprocessor core. The TRIPS OPN is used for operand passing, register ﬁle I/O, and primary memory system I/O. We discuss in detail the OPN design, including the unique features that arise from its integration with the processor core, such as its connection to the execution unit’s wakeup pipeline and its in ﬂight mis-speculated trafﬁc removal. We then evaluate the performance of the network under synthetic and realistic loads. Finally, we assess the processor performance implications of OPN design decisions with respect to the end-toend latency of OPN packets and the OPN’s bandwidth. I . INT RODUCT ION As process technologies continue to descend into the deep sub-micron range, wire delay and design complexity become limiting factors for current microprocessor designs [1]. In current processor microarchitectures, data and control are conveyed on specialized busses and other ad-hoc interconnect. Some processor designs incorporate extra pipeline stages to accommodate the wire delay global wires require [2]. Wire routing and electrical design of these specialized busses increases in complexity with smaller process technologies. Micronets, or microarchitecturally integrated on-chip networks, provide a solution to this design challenge by offering an alternative to bus-based interconnects that are scalable and have reasonable design complexity. On-chip networks enjoy a scaling advantage relative to busses since network wire lengths between adjacent routers can be kept short and unidirectional. On-chip networks also enable better pipelining of data between nodes and greater aggregate bandwidth than busses. Finally, design complexity is bounded since a router is designed once and replicated for use wherever needed. Micronets, a type of on-chip network, are integrated with the microarchitecture of their host processors to improve system performance. For example a micronet may be tightly integrated with a processor’s pipeline to reduce packet generation latency, by taking advantage of information available before the full data payload has been computed. Micronets can also be used Fig. 1. TRIPS chip plot with operand networks highlighted. to implement, in a distributed fashion, higher level protocols that are centralized in more traditional architectures, such as instruction commit. In this paper we discuss and evaluate the design and implementation of one such micronet, the TRIPS prototype processor’s operand network (OPN). Figure 1 shows a plot of the TRIPS prototype processor chip, which was fabricated in a 130nm ASIC process. On the right side of the ﬁgure are the two processors, each with its own separate operand network, as indicated by the superimposed diagram. Each processor’s OPN is a 5x5 dynamically routed 2D mesh network with 140bit links. The OPN connects a total of 25 distributed execution, register ﬁle, and data cache tiles. Each tile is replicated and interacts only with neighboring tiles via the OPN and other control networks. The OPN subsumes the role of several traditional microprocessor interconnect busses, including the operand bypass network, register ﬁle read and write interface, and the L1 memory system bus. We describe our experience in implementing the OPN in a 130nm process technology as a part of the TRIPS prototype processor and include a discussion of timing and area costs of the network. We also explore the latency and bandwidth of the OPN under statistical loads common in the networking literature as well as realistic loads extracted from TRIPS program execution. Finally, we examine trade-offs in OPN implementation, including the sensitivity of processor performance to network latency and bandwidth. The remainder of this paper is organized as follows. Section II describes related work in micronets. Section III introduces the TRIPS processor microarchitecture. Section IV describes the design and implementation of the TRIPS OPN, highlighting where the OPN is different from typical on-chip networks. Section V presents an evaluation of the network’s performance under different loads and explores the sensitivity of processor performance to OPN latency and bandwidth. Section VI concludes and discusses future work in micronets. I I . RE LAT ED WORK The ﬁrst operand bypass network was introduced with the IBM System 360/model 91 to avoid delaying the sequential execution of dependent instructions [3]. This bypass network employed a simple broadcast bus (common data bus) that linked each ALU output to each ALU input. Thus an instruction can receive an operand directly from a preceding instruction’s output without the delay of passing it through the register ﬁle. The development of deeply pipelined superscalar architectures drove increases in the complexity and latency of bypass networks. Deeper pipelines increased the number of stages in which an instruction could produce a result or consume an operand. Wider pipelines increase the bypass bus network complexity quadratically with the number of ALUs because of the full connectivity between ALU outputs and ALU inputs. This N 2 scaling rate is not viable beyond a small number of ALUs. The Multiscalar processor architecture used partitioning of the ALUs and register ﬁle into separate components connected by a ring to reduce operand delivery complexity [4]. Similarly, the Alpha 21264 architecture divided its four integer ALUs into two clusters to reduce the complexity of its bypass network [5]. Operands produced within one cluster are available for use in the same cluster in the next cycle, but they must pay a single cycle penalty to be used in the other cluster. Other machines have sought to reduce communication latency between processors through cross-processor register-toregister communication. The M-Machine employed an on-chip cluster switch to connect the register bypass networks for three processors; an instruction writing to a remote register injects its result into the switch, which delivers the data to a waiting instruction on a remote processor [6]. The MIT RAW processor took this strategy further, by using a 4x4 mesh network to interconnect its processor tiles between execution units [7]. The integration of the RAW network into the local bypass network of each execution unit reduced the latency of operand passing between units to three cycles. One interesting feature of RAW is that network routing arbitration and ordering are statically determined. While this strategy simpliﬁes the routers, a compiler or programmer must generate a routing program that executes concurrently with the application program. In addition to the statically routed network, RAW also implemented dynamically routed networks for load/store trafﬁc. The TRIPS OPN is also integrated directly with the execution unit. However, to allow for out-of-order instruction execution and uncertain memory delays, the OPN routers are dynamic. We also employed additional routing optimizations to reduce the per-hop latency to one cycle. The Monsoon processor was a dataﬂow architecture that used a custom switched interconnection network to provide similar capabilities as the TRIPS OPN [8]. The WaveScalar processor has a similar philosophy and execution model as TRIPS, but uses a hierarchy of interconnection networks to pass operands between processing elements [9]. Operands are broadcast within the eight processing elements making up one domain. Operands pass through a crossbar switch to travel between the four domains that make up a cluster. Operands traveling to another cluster traverse a 2D mesh network similar to the TRIPS OPN. Taylor, et al. [10] and Sankaralingam, et al. [11] both present useful taxonomies of operand networks. Taylor categorizes operand networks based on whether the assignment, transport, and ordering are each either static or dynamic. In their terms, the TRIPS OPN has statically assigned operations that are dynamically transported and ordered. Sankaralingam categorizes operand networks based on network organization (point-to-point vs. broadcast), network architecture (single hop vs. multi-hop), and router control (static vs. dynamic). By this taxonomy, the TRIPS OPN is a point-to-point, multi-hop network with dynamic router control. Pinkston and Shin [12] use data from the 2003 International Technology Roadmap for Semiconductors (ITRS-2003) [13] to demonstrate how trends in semiconductor technology are leading to partitioned microsystem architectures. They provide a taxonomy that categorizes microsystem architectures based on how they are partitioned, with the insight that the trend toward partitioned architectures is driving the adoption of onchip networks. By this taxonomy TRIPS processor core is physically partitioned in a compiler-visible form. Dally and Towles [14] proposed a 2D torus network as a replacement for general on-chip interconnect, but not speciﬁcally for operand networks. They claim that on-chip network modularity would shorten the design time and reduce the wire routing complexity. Our experience bears this out, as the design, implementation, timing optimization, and veriﬁcation of the TRIPS OPN were all straightforward. On-chip routed networks have also been proposed for use in SoCs (system-ona-chip) such as in CLICHE [15], in which a 2D mesh network is proposed to interconnect a heterogeneous array of IP blocks. I I I . TR IPS PROCE S SOR OVE RV IEW TRIPS is a distributed processor consisting of multiple tiles connected via multiple micronets. Figure 2 shows a tile-level diagram of the processor with its OPN links. The register read instructions that fetch block inputs from the RTs and deliver them to waiting instructions via the OPN. Instructions within the block then execute in dataﬂow order. Load and store instructions compute their addresses in the ETs, which are then transmitted to one of the DTs to access the data cache. Addresses are interleaved across the DTs on cache-line boundaries (64 bytes). Register outputs are transmitted back to the RTs where they wait in write queues before updating the architecturally persistent register ﬁle banks. When all of the RTs and DTs have received all of the register writes and stores for the block, they communicate this to the GT via the global control network (GCN). When the GT receives completion notiﬁcation from all DTs and RTs, the block is complete. If the block has not caused any exceptions, the GT signals to the DTs and RTs that the block can commit. The DTs then update the cache with the store values from the store buffers and the RTs update the register ﬁle banks with the contents of the write queues. When all of the state of the block has committed, a new block may be mapped into its place for execution. The TRIPS processor allows up to 8 blocks in-ﬂight and executing simultaneously, with 1 being non-speculative and 7 being speculative. Complete details of the TRIPS microarchitecture and can be found in [17]. During block execution, the TRIPS operand network (OPN) has the responsibility for delivering operands among the tiles. The TRIPS instruction formats contain target ﬁelds indicating to which consumer instructions a producer sends its values. At runtime, the hardware resolves those targets into coordinates to be used for network routing. An operand passed from producer to consumer on the same ET can be bypassed directly without delay, but operands passed between instructions on different tiles must traverse a portion of the OPN. The TRIPS execution model is inherently dynamic and data driven, meaning that operand arrival drives instruction execution, even if operands are delayed by unexpected or unknown memory latencies. Because of the data driven nature of execution and because multiple blocks execute simultaneously, the OPN must dynamically route the operand across the processor. IV. OPN DE S IGN AND IM P L EMENTAT ION The operand network (OPN) is designed to deliver operands among the TRIPS processor tiles with minimum latency. While tight integration of the network into the processor core reduces the network interface latency, two primary aspects of the TRIPS processor architecture simplify the router design and reduce routing latency. First, because of the block execution model, reservation stations for all operand network packets are pre-allocated, guaranteeing that all OPN messages can be consumed at the targets. Second, all OPN messages are of ﬁxed length, one ﬂit broken into header and payload phits. A. OPN Design Details The OPN is a 5x5 2D routed mesh network as shown in Figure 2. Flow control is on/off based, meaning that the receiver tells the transmitter when there is enough buffer space available to send another ﬂit. Packets are routed through the Fig. 2. Block diagram of the TRIPS processor core with tiles and OPN network connections. processor contains ﬁve types of tiles: execution tiles (ET) which contain ALUs and reservation stations, register tiles (RT) which each contain a fraction of the processor register ﬁle, data tiles (DT) which each contain a fraction of the level-1 data cache, instruction tiles (IT) which each contain a fraction of the level-1 instruction cache, and a global control tile (GT) which orchestrates instruction fetch, execution and commit. In addition, the processor contains several control networks for implementing protocols such as instruction fetch, completion, and commit in a distributed fashion. Tiles communicate directly only with their nearest neighbors to keep wires short and mitigate the effect of wire delay. ISA and execution model: TRIPS is an explicit datagraph execution (EDGE) architecture, an instruction set architecture with two key features: (1) the hardware fetches, executes, and commits blocks of instructions, rather than individual instructions, in an atomic fashion; and (2) within a block, instructions send their results directly to other instructions waiting to execute, rather than communicating through a common register ﬁle [16]. The compiler is responsible for constructing blocks, which can contain up to 128 instructions. Since basic blocks typically contain only a handful of instructions, the TRIPS compiler uses techniques such as predication, loop unrolling, and function inlining to create large hyperblocks. After hyperblock formation, a scheduler maps the block onto the ﬁxed array of 16 execution units, with up to 8 instructions per ET. The scheduler is aware of the topology of the ETs and attempts to minimize the distance between dependent instructions along the program’s critical path. The scheduler determines where an instruction will execute and encodes this in the program binary, but the hardware executes instructions in dataﬂow order based on when an individual instruction’s operands arrive. Block execution: Processing a TRIPS block requires four phases: fetch, execute, complete, and commit. To fetch a block, the GT transmits a fetch request to each of the ITs using the TRIPS global dispatch network (GDN). Each IT then retrieves a portion of the block (32 instructions) from its instruction cache bank and delivers them to pre-allocated reservation stations in the ETs and RTs. An instruction waits in its reservation station until all of its operands have arrived before it can execute. Block execution is instigated by special Control phit Field Valid Type (LD/ST/etc.) Block ID Dest. node Dest. instruction Source node Source instruction bits 1 4 3 6 5 6 5 Data phit Field Valid Type (normal/null/exception) Data operation (access width) Data payload LD/ST Address bits 1 2 3 64 40 B R EAKDOWN O F B I T S F O R O PN CON T RO L AND DATA P H I T S . TABLE I network in Y-X dimension-order with one cycle taken per hop. A packet arriving at a router is buffered in an input FIFO prior to being launched onward towards its destination. Due to dimension-order routing and the guarantee of consumption of messages, the OPN is deadlock free without requiring virtual channels. The absence of virtual channels reduces arbitration delay and speeds routing. Each operand network message consists of a control phit and a data phit. The control phit is 30 bits and encodes OPN source and destination node coordinates, along with identiﬁers to indicate which instruction to select and wakeup in the target ET. The data phit is 110 bits, with room for a 64-bit data operand, a 40-bit address for store operations, and 6 bits for status ﬂags. Table I shows a breakdown of all of the bits in the data and control phits. The data phit always trails the control phit by one cycle in the network. The OPN supports different physical wires for the control and data phit so one can think of each OPN message consisting of one ﬂit split into a 30-bit control phit and a 110-bit data phit. Because of the distinct control and data wires, two OPN messages with the same source and destination can proceed through the network separated by a single cycle. The data phit of the ﬁrst message and the control phit of the second are on the wires between the same two routers at the same time. Upon arrival at the destination tile, the data phit may bypass the input FIFO and be used directly, depending on operation readiness. This arrangement is similar to ﬂit-reservation ﬂow control, although here the control phit contains some payload information and does not race ahead of the data phit [18]. In all, the OPN has a peak injection bandwidth of 175 GB/sec when all nodes are injecting packets every cycle at its designed frequency of 400MHz. The network’s bisection bandwidth is 70 GB/sec measured horizontally or vertically across the middle of the OPN. Figure 3 shows a high-level block diagram of the OPN router. The OPN router has ﬁve inputs and ﬁve outputs, one for each ordinal direction (N, S, E and W) and one for the local tile’s input and output. The ordinal directions inputs each have two four entry deep FIFOs, one 30 bits wide for control phits and one 110 bits wide for data phits. The local input has no FIFO buffer. The control and data phits of the OPN Fig. 3. OPN router microarchitecture. packet have separate 4x4 crossbars. All arbitration and routing is done on the control phit, in round-robin fashion among all incoming directions. The data phit follows one cycle behind the control phit in lock step, using the arbitration decision from its control phit. B. OPN/Processor Integration ET/OPN datapath: Figure 4 shows the operand network datapath between the ALUs in two adjacent ETs. The instruction selection logic and the output latch of the ALU are both connected directly to the OPN’s local input port, while the instruction wakeup logic and bypass network are both connected to the OPN’s local output. The steps below describe the use of the OPN to bypass data between the ALUs. (cid:127) Cycle 0: Instruction wakeup/select on ET 0 – ET0 selects a ready instruction and sends it to the ALU. – ET0 recognizes that the instruction target is on ET1 and creates the control phit. (cid:127) Cycle 1: Instruction execution on ET0 – ET0 executes the instruction on the ALU. – ET0 delivers the control phit to router FIFO of ET1. (cid:127) Cycle 2: Instruction wakeup/select on ET1 – ET0 delivers the data phit to ET1, bypassing the FIFO and depositing the data in a pipeline latch. – ET1 wakes up and selects the instruction depending on the data from ET0. (cid:127) Cycle 3: Instruction execution ET1 – ET1 selects the data bypassed from the network and executes the instruction. The early wakeup, implemented by delivering the control phit in advance of the data phit, overlaps instruction pipeline Fig. 4. Operand datapath between two neighboring ETs. control with operand data delivery. This optimization reduces the remote bypass time by a cycle (to one cycle) and improves performance by approximately 11% relative to a design where the wakeup occurs when the data arrives. In addition, the separation of the control and data phits onto separate networks with shared arbitration and routing eliminates arbitration for the data phit and reduces network contention relative to a network that sends the header and payload on the same wires in successive cycles. This optimization is inexpensive in an on-chip network due to the high wire density. The OPN employs round-robin arbitration among all of the inputs, including the local input. If the network is under load and chooses not to accept the control phit, the launching node captures the control phit and later the data phit in a local output buffer. The ET will stall if the instruction selected for execution needs the OPN and the ET output buffer is already full. However, an instruction that needs only to deliver its result to another instruction on the same ET does not stall due to OPN input contention. While OPN contention can delay instruction execution on the critical path of program execution, the scheduler is effective at placing instructions to mitigate the distance that operands must travel and the contention they encounter. Selective OPN message invalidation: Because the TRIPS execution model uses both instruction predication and branch prediction, some of the operand messages are actually speculative. On a branch misprediction or a block commit, the processor must ﬂush all in-ﬂight state for the block, including state in any of the OPN’s routers. The protocol must selectively ﬂush only those messages in the routers that belong to the ﬂushed block. The GT starts the ﬂush process by multicasting a ﬂush message to all of the processor tiles using the global control network (GCN). This message starts at the GT and propagates across the GCN within 10 cycles. The GCN message contains a block mask indicating which blocks are to be ﬂushed. Tiles that receive the GCN ﬂush packet instruct their routers to invalidate from their FIFOs any OPN messages with block-identiﬁers matching the ﬂushed block mask. As the invalidated packets reach the head of the associated FIFOs Component Router input FIFOs Router crossbar Router arbiter logic Total for single router % Router Area % E-Tile Area 74.6% 7.9% 20.3% 2.1% 5.1% 0.5% – 10.6% A R EA O C CU P I ED BY TH E COM P ON EN T S O F AN O PN ROU T E R . TABLE II they are removed. While we chose to implement the FIFOs using shift registers to simplify invalidation, the protocol could also be implemented for circular buffer FIFOs. A collapsing FIFO that immediately eliminates ﬂushed messages could further improve network utilization, but we found that the performance improvement did not outweigh the increased design complexity. In practice, very few messages are actually ﬂushed. C. Area and Timing The TRIPS processor is manufactured using a 130nm IBM ASIC technology and returned from the foundry in September 2006. Each OPN router occupies approximately 0.25mm2, which is similar in size to a 64-bit integer multiplier. Table II shows a breakdown of the area consumed by the components of an OPN router. The router FIFOs dominate the area in part because of the width and depth of the FIFOs. Each router includes a total of 2.2 kilobits of storage, implemented using standard cell ﬂip-ﬂops rather than generated memory or register arrays. Utilizing shift FIFOs added some area overhead due to extra multiplexors. We considered using the library generated SRAMs instead of ﬂip-ﬂops, but the area overhead turned out to be greater given the small size of each FIFO. A single OPN router takes up approximately 10% of the ET’s area and 14% of a processor core. While this area is signiﬁcant, the alternative of a broadcast bypass network across all 25 tiles would consume considerable area and is not feasible. We could have reduced router area by approximately Component Control Phit Path Read from instruction buffer Control phit generation ET0 router arbitration ET0 OPN output mux ET1 OPN FIFO muxing and setup time Latch setup + clock skew Total Data Phit Path Read from output latch Data phit generation ET0 OPN output mux ET1 router muxing/bypass ET1 operand buffer muxing/setup Latch setup + clock skew Total TABLE III Latency % Path 290ps 620ps 420ps 90ps 710ps 200ps 2.26ns 110ps 520ps 130ps 300ps 360ps 200ps 1.62ns 13% 27% 19% 4% 31% 9% – 7% 32% 8% 19% 22% 12% – C R I T I CA L PATH T IM I NG F O R O PN CON T RO L AND DATA P H I T. 1/3 by sharing the FIFO entries and wires for the control and data phits. However, the improved OPN bandwidth and overall processor performance justiﬁes the additional area. We performed static timing analysis on the TRIPS design using Synopsys Primetime to identify and evaluate critical paths. Table III shows the delay for the different elements of the OPN control and data critical paths, matching the datapath of Figure 4. We report delays using a nominal process corner, which we obtained by scaling our worst-case process corner delays by a factor of 2/3. A signiﬁcant fraction of the clock cycle time is devoted to overheads such as ﬂip-ﬂop read and setup times as well as clock uncertainty (skew and jitter). A custom design would likely be able to drive these overheads down. On the logic path, the control phit is much more constrained than the data phit due to router arbitration delay. We were a little surprised by the delay associated with creating the control phit, which involves decoding and encoding. This path could be improved by performing the decoding and encoding in a previous cycle and storing the control phit with the instruction before execution. We found that wire delay was small in our 130nm process given the relatively short transmission distances. Balancing router delay and wire delay may be more challenging in future process technologies. D. Design Optimizations We considered a number of OPN enhancements but chose not to implement them in the prototype to simplify the design. One instance where performance can be improved is when an instruction must deliver its result to multiple consumers. The TRIPS ISA allows an instruction to specify up to 4 consumers, and in the current implementation, the same value is injected in the network once for each consumer. Multicast in the network would automatically replicate a single message in the routers at optimal bifurcation points. This capability would reduce overall network contention and latency while increasing ET execution bandwidth, as ETs would spend less time blocking for message injection. Another optimization would give network priority to those OPN messages identiﬁed to be on the program’s critical path. We have also considered improving network bandwidth by replicating the operand network by replicating the routers and wires. We examine this optimization further in Section V-E. Finally, the area and delay of our design was affected by the characteristics of the underlying ASIC library. While the trade-offs may be somewhat different with a full-custom design, our results are relevant because not all on-chip networked systems will be implemented using fullcustom silicon. Our results indicate that such ASIC designs would beneﬁt from new ASIC cells, such as small but dense memory arrays and FIFOs. V. OPN EVALUAT ION In this section, we evaluate the behavior of the OPN on statistical and realistic network workloads, using our operand network simulator to model the OPN hardware. We characterize the operand network message workload and show that injection is not distributed evenly across the nodes, due to the TRIPS execution model and scheduler optimizations. Finally, we examine the sensitivity of program performance and operand network latency to OPN bandwidth and latency parameters. A. Methodology The OPN simulator is a custom network simulator conﬁgured with the operand network design parameters. It can inject messages using different trafﬁc patterns, including random and bit-reversal, with variable injection rates. It can also accept a network trace ﬁle that speciﬁes source nodes, destination nodes, and injection timestamps. We obtained realistic workload traces from an abstract TRIPS processor performance estimator (tsim-cyc), which runs compiled TRIPS programs. This simulator models TRIPS block execution at a high level, but employs a simple analytical performance model without accurate OPN contention estimation. Nonetheless, this simulator matches performance of the logic design of TRIPS to within 25%. The high simulation speed of tsim-cyc allows us to obtain traces for long running programs. However, the message injection times only approximate those that will be seen in hardware. For more detailed analysis, we also used our low-level simulator (tsim-proc) which accurately models all aspects of a TRIPS processor core, including network contention. This simulator has been validated for accuracy against the TRIPS RTL and hardware. Unfortunately, the speed of this simulator prevents analysis of large programs. Our realistic workloads include programs from the EEMBC [19] and SPEC2000 [20] benchmark suites. The 30 EEMBC benchmarks are small enough to run to completion on both tsim-cyc and tsim-proc. The 19 SPEC CPU2000 benchmarks were run with the Minne-SPEC [21] reduced input set, but were still too long-running for tsim-proc. The SPEC benchmarks were run to completion (50 million cycles for the shortest benchmark), or for 300 million cycles after program    , 2  1 4      0 9 , #  / 0 7 0 1 1          Fig. 5. Offered vs accepted rate for random and bit-reversal trafﬁc. , 2 2 5 , 5 5 :  , 5 8  , 7 9   5  . 7 , 1 9  0 6 : ,  0  , 5   5  2 . 1 2 0 8 , 2  7  / 5 , 7 8 0 7 8   9 7 , .  8   2 9  4  1 ; 5 7 ; 4 7 9 0   : 5   8 0 , ; 0 7 ,  0 Fig. 7. Average offered rates in SPEC CPU2000 benchmark traces. cycles for increasing offered rate. The average latency for bitreversal trafﬁc gradually increases from around 5 to 8 cycles for offered rates of 1% to 32%. The latency then increases exponentially as the network becomes saturated. Similarly for random trafﬁc the latency increases from about 4 to 7 cycles for offered rates from 1% to 40% before increasing dramatically. This diagram shows that 32% and 40% are the saturation offered rates for bit-reversal and random trafﬁc respectively. C. OPN Trafﬁc Trace Analysis Our earlier work examining the OCN [22] showed that real benchmark generated trafﬁc in on-chip networks would not be modeled well by traditional synthetic loads. We perform a similar analysis for the OPN using network traces generated from tsim-cyc and characterize the network workload. Variation in application offered rate: Figure 7 shows the average offered rate for various SPEC CPU2000 benchmark traces generated from tsim-cyc. For each application’s trace, we derived the offered rate by dividing the number of total messages by the product of the cycle count and the number of injecting nodes (25 for the 5x5 network). While the offered rates vary widely, from under 1% for twolf to almost 14% for mcf, the average offered rate is well below the saturation threshold range of 30-40% for bit-reversal and uniform random. The magnitude of the offered rates correlate to the degree of ILP that the TRIPS compiler has exposed to the processor. A benchmark with more exposed ILP will have more operations occurring simultaneously and will therefore generate more operands each cycle than a benchmark that has long dependency chains and lower ILP. Average packet hop distance: Figure 8 shows the average number of hops, or router traversals, from source to destination for OPN packets from various SPEC CPU2000 benchmark traces. These values are generated by averaging the Manhattan distance from source to destination for each packet in the trace of each benchmark. Because the OPN is a single cycle per hop network, this distance also represents a best case routing delay for each packet in the absence of any network contention. Fig. 6. Offered rate vs average latency for random and bit-reversal trafﬁc. warmup. The traces include 2–70 million operand messages, depending on the benchmark. B. Synthetic Statistical Loads Interconnection networks are typically evaluated by examining their performance on stochastically generated loads. Two common loads are bit-reversal and uniform random trafﬁc. In bit-reversal, each node exchanges packets with a node on the opposite side of the network. The random trafﬁc model randomly chooses source and destination pairs from among all the TRIPS core tiles. Both trafﬁc models inject packets at a uniform random distributed rate. Figure 5 shows the offered vs. accepted rate for both of these types of trafﬁc. The offered rate is the rate at which packets are generated, while the accepted rate is the throughput of the network. In these diagrams, the offered and accepted rates are shown as a percentage of the peak injection bandwidth. The accepted rate tracks the offered rate for bit-reversal trafﬁc up to 33%, from there the accepted rate continues to increase, ﬁnally leveling off at 44%. For random trafﬁc the accepted bandwidth tracks the offered bandwidth up to approximately 46% before leveling off to a maximum of 47%. These are typical curves for this type of 2D mesh network. Figure 6 shows the average measured packet latency in         , 2 2 5 , 5 5 :  , 5 8  , 7 9  5   . 7 , 1 9  0 6 : ,  0  , 5   5  2 . 1 2 0 8 , 2  7  / 5 , 7 8 0 7 8   9 7 , .  8  2  9  4  1 ; 4 7 9 0  ; 5 7  : 5   8 0 , ; 0 7 ,  0  8 5 4 Fig. 8. Average number of hops from source to destination for various SPEC CPU2000 benchmarks.               11070/ #,90                        # # # # Fig. 9. sources. Offered rates for SPEC CPU2000 benchmarks broken down by The ﬁgure shows that average hop distance for all benchmarks is 2.13. There is little variance from one benchmark to the next. In TRIPS, instructions are statically mapped on particular nodes; the TRIPS compiler tries to map instructions as close as possible to the source of their operands, be that the register ﬁle or data cache for register reads or memory loads, or other E tiles producing their operands directly. Offsetting this, the compiler must ensure that the instructions are evenly distributed across the execution resources to ensure minimal resource contention. Variation in offered rate by source: While the overall average offered rate of the OPN is low, that metric does not accurately capture hot spots in the network. Figure 9 shows the average offered rate for each individual OPN node as a percentage of the peak offered rate of one message per cycle, Fig. 10. Link utilization for mesa SPEC CPU2000 benchmark. averaged across all SPEC CPU2000 benchmarks. The X-Y plane of the graph matches the layout of the 5x5 operand network and the different shades highlight the different tile types. The per-tile offered rates vary widely, from a low of 2.6% for E15 in the lower right to a high of 16.7% for E0 at the upper left. The disposition of offered rates reﬂects the TRIPS compiler’s instruction placement optimizations that attempt to minimize operand routing distance. Thus, instructions are preferentially placed near the register ﬁle and data cache tiles to reduce block input and output latency. Our analysis shows that even though average offered rate is low, applications can easily create network injection hot spots that may approach local saturation, producing higher than expected transmission latencies. The compiler schedules instructions to more evenly distribute the network trafﬁc; however such optimizations must be balanced against the effect of increasing the average source to destination hop count. Variation in link utilization: Hot spots also form when many messages must pass through the same link. Figure 10 labels each OPN link with the link occupancy percentage for the mesa SPEC CPU2000 benchmark. We choose to show the data for one benchmark instead of averaging across all of them because of the variance across the benchmarks. The southbound link between E4 and E8 has a high utilization of 41% and many other links are in the 15%–20% range. High link utilizations will have a disproportionately large effect on latency because of congestion and limits the performance improvement available through virtual channel ﬂow control. Our experience shows that other benchmarks place a maximum load of only 5-10% on any link. Trafﬁc burstiness: In addition to load variability across applications and network nodes, offered trafﬁc can vary over     Fig. 11. Distribution of offered rates measured as a percentage of packets injected at a given offered rate. time. TRIPS naturally has trafﬁc bursts because a block begins execution through the injection of many register values from the top of the network. To measure this burstiness, we examined the trace at 1000 cycle intervals, counted the number of messages in each interval, and computed the offered rate for the interval. Figure 11 shows a histogram of the offered rates for two SPEC CPU2000 benchmarks, mcf and mgrid. The X-axis shows the histogram buckets at 1% intervals, while the Y-axis shows the fraction of all messages that fall into each bucket. The ﬁgure shows that mcf has a relatively stable offered rate centering around 29% for most packets, meaning that the network is evenly loaded over time. Conversely, mgrid shows more diversity in its offered rates with signiﬁcant numbers of packets clustered around 20%, 34% and 38%. Based on these results we conclude that the trafﬁc of mgrid has more bursts than that of mcf, and likely has spikes in latency for critical operands traversing the network. Such bursts may motivate lightweight network designs that tolerate and spread trafﬁc in response to varying loads. D. Network Simulator-based Analysis To examine how the network performs under load, we applied the traces to the OPN trace-driven simulator. The inherent weakness of trace-driven network simulation is the lack of a feedback loop between the network simulation and trace generation. In the real TRIPS processor, network congestion will throttle instruction execution, in turn throttling the offered rate. To bound this difference, for each message we tracked the instruction block to which it belongs and ensures that messages from only eight consecutive blocks are considered for injection at any one time. These eight correspond to the one non-speculative and seven speculative blocks that can execute simultaneously. This approach represents a reasonable compromise that keeps the processor and network simulators separate. The lack of intra-block throttling places some excess stress on the network, giving additional insight on the load if the network were ideal and non-contented.        , 2 2 5 , 5 5 :  , 5 8  , 7 9  5   . 7 , 1 9  0 6 : ,  0  , 5   5  2 . 1 2 0 8 , 2  7  / 5 , 7 8 0 7 8   9 7 , .  8   2 9  4  1 ; 4 7 9 0  ; 5 7  : 5   8 0 , ; 0 7 ,  0 03.2,7 1 1 0 7 # / 0  , 9 0           .   8 0  11070/#,90 ,903. Fig. 12. Offered rates and latencies for SPEC CPU2000 benchmarks from the OPN network simulator.        ,  9  2 0  ,  1 1 9 7  ,  1  7 1  ,   1 1 9  , : 9 4 . 4 7 , 8 0 1 5  0  0  7   9 2 3 5  . , .  0  . , 3 7 / 7  . 5 0   . 4 3 ; 0 3 /  9  0 7  / 5 0   1  9 ,  1 1 9  / . 9 7 3    7 1  9  2 , 9 7    4 8 5 1 5  9 1  4  5 3 9 7 .   5 :  2 4 /  7 4 9 , 9 0  7 4 : 9 0 4 4  : 5  7 8 5 0 0 /  9 4 4    9 0  9  9 9 8 5 7   ;  9 0 7  ; 0 7 ,  0 03.2,78  8 0 .   0947%7,.0$2:,947 #%$2:,947 Fig. 13. EEMBC benchmark latency from the high-level tsim-cyc traces versus the detailed processor model tsim-proc. Offered rate and latency: Figure 12 shows the average offered rate and latency for the SPEC CPU2000 on the OPN simulator. Compared to the results in Figure 7, the offered rates are signiﬁcantly lower because of the block-level throttling. The benchmarks that had the highest offered rates show offered rates that are reduced by as much as two thirds. The right bar for each benchmark shows the average message latency for each benchmark. In general, benchmarks with higher offered rates show higher average latencies, but certain benchmarks show the reverse. For example, mcf has the highest offered rate at 5.1% while it has a fairly average latency of about 10 cycles. Conversely mgrid has a fairly average offered rate of around 3% but the highest average latency at 15.5 cycles. This dichotomy can be attributed to the burstiness in the trafﬁc and high utilization of particularly hot links. The OPN simulator shows that the average latencies are high, ranging from 6 to 15 cycles. Although throttling will prevent actual OPN latencies from reaching these levels, the measured latencies highlight where OPN network performance improvement has a direct affect on processor performance. Network throttling: To examine the impact of throttling on latency, we used the cycle-level simulator tsim-proc. Because tsim-proc is approximately 300 times slower than the tsim                                  30947 309478 309478 309478 /0,           . 3 0 9 ,  Fig. 14. Comparison of baseline OPN versus an OPN without the early wakeup and an OPN that consumes two cycles per hop. , 2 2 5 , 5 5 :  , 5 8  , 7 9   5  . 7 , 1 9  0 6 : ,  0  , 5   5  2 . 1 2 0 8 , 2  7  / 5 , 7 8 0 7 8   9 7 , .  8   2 9  4  1 ; 5 7 ; 4 7 9 0   : 5   8 0 , ; 0 7 ,  0 Fig. 15. Average packet latency for SPEC CPU2000 benchmarks with 1, 2, 3 and 4 OPN networks. cyc simulator we used to generate traces, we chose the shorter EEMBC 2.0 suite of embedded system benchmarks. Figure 13 shows the average latencies of OPN packets as measured in both tsim-proc and the OPN network trace driven simulator. While the network simulator shows an average latency of 7 cycles, tsim-proc shows only 2.25 cycle, again due to throttling from instruction dependences in the program. This can be a little deceiving because throttling manifests as stalls in the execution tiles (ETs) rather than in the network. Thus for network research, trace-based simulation still provides good insight into network behavior, but one must take care when analyzing system performance based on network performance. E. Operand Network Sensitivity Studies Packet End-To-End Latency: The TRIPS prototype is designed to support one-cycle communication latency between adjacent ETs. Speculative injection of the operand message header, early wakeup of the consumer, and bypassing directly from the network input limit the latency of operand network transmission. Each additional hop in the network costs only one cycle. To examine the sensitivity of performance to latency, we simulated two alternate designs. The ﬁrst emulates an architecture that does not have early wakeup and thus requires one additional cycle for every operand transmission. The second emulates a two-cycle-per-hop network to model slower routers and wires. Figure 14 compares the IPC (instructions-per-clock) of the TRIPS processor core for the different design points. Without early wakeup, processor performance drops by about 11%; a two-cycle per hop network decreases IPC by 20%. Thus performance of TRIPS is quite sensitive to OPN latency. Bandwidth: A simple way to improve the performance of a network is to increase its bandwidth. Typically one would increase the bit-width of the network’s interfaces to decrease the number of ﬂits per message, network occupancy, and message injection and extraction latency. Because the OPN already has single-ﬂit packets, increasing the link-width will not affect network occupancy or processor performance. Another way to improve the performance of a network is to decrease the network diameter by using higher-radix routers and a more highly interconnected topology. This approach is not a good ﬁt for the OPN for two reasons. First, increasing the radix of the routers increases the logical complexity of the routers, possibly to the point of becoming the TRIPS core’s critical timing path. Second, as shown in ﬁgure 8 the average hop distance for packets on the OPN is just over 2, so increasing the network’s order would not decrease the endto-end latency of a large fraction of the injected messages. Slowing down the clock or pipelining the routers in order to achieve timing would mitigate any gains. As an alternative, we investigated replicating the network links and routers as a means to increase the effective bandwidth of the network and reduce contention. We simulate a simple scheme in which nodes inject packets into each network in a round-robin fashion. If a network is blocked due to congestion, the injecting node skips it until the congestion is alleviated. Figure 15 shows the average packet latency from the OPN trace simulator for the SPEC CPU2000 benchmarks with 1 (the current OPN conﬁguration), 2, 3 and 4 networks interconnecting the nodes of the OPN. The expected latency without contention is shown as “Ideal”. The biggest improvement in latency occurs between 1 and 2 networks, almost halving the average latency. However, replication comes at a cost of doubling the area consumed by the network. V I . CONCLU S ION S AND FUTURE WORK In this paper, we presented the design, implementation, and evaluation of the TRIPS OPN. The TRIPS OPN is a micronet that interconnects the functional units within the TRIPS processor core. The OPN replaces an operand bypass bus and primary memory system interconnect in a technology scalable manner. The tight integration between the OPN and the processor core elements enables fast operand bypassing across distributed ALUs, providing opportunity for greater instruction-level concurrency. Our implementation and fabrication shows that such a network is feasible in terms of area and delay, and that the network design provides good performance for the trafﬁc provided by real applications.  semiconducAvailable: [10] M. B. Taylor, W. Lee, S. P. Amarasinghe, and A. Agarwal, “Scalar Operand Networks: On-Chip Interconnect for ILP in Partitioned Architecture,” in 9th International Symposium on High-Performance Computer Architecture (HPCA), 2003, pp. 341–353. [11] K. Sankaralingam, V. A. Singh, S. W. Keckler, and D. Burger, “Routed Inter-ALU Networks for ILP Scalability and Performance,” in IEEE International Conference on Computer Design (ICCD), 2003, pp. 170– 177. [12] T. M. Pinkston and J. Shin, “Trends toward on-chip networked microsystems,” Int. J. High Performance Computing and Networking, vol. 3, no. 1, pp. 3–18, 2005. [13] “International technology roadmap for tors (ITRS), 2003 edition.” [Online]. http://public.itrs.net/Files/2003ITRS/Home2003.htm [14] W. J. Dally and B. Towles, “Route Packets, Not Wires: On-Chip Interconnection Networks,” in 38th Design Automation Conference (DAC), 2001, pp. 684–689. [15] S. Kumar, A. Jantsch, M. Millberg, J. ¨Oberg, J.-P. Soininen, M. Forsell, K. Tiensyrj ¨a, and A. Hemani, “A Network on Chip Architecture and Design Methodology,” in IEEE Computer Society Annual Symposium on VLSI (ISVLSI), 2002, pp. 117–124. [16] D. Burger, S. W. Keckler, K. S. McKinley, M. Dahlin, L. K. John, C. Lin, C. R. Moore, J. Burrill, R. G. McDonald, W. Yoder, and the TRIPS Team, “Scaling to the End of Silicon with EDGE Architectures,” IEEE Computer, vol. 37, no. 7, pp. 44–55, July 2004. [17] K. Sankaralingam, R. Nagarajan, P. Gratz, R. Desikan, D. Gulati, H. Hanson, C. Kim, H. Liu, N. Ranganathan, S. Sethumadhavan, S. Sharif, P. Shivakumar, W. Yoder, R. McDonald, S. Keckler, and D. Burger, “The Distributed Microarchitecture of the TRIPS Prototype Processor,” in 39th ACM/IEEE International Symposium on Microarchitecture (MICRO), 2006. [18] L.-S. Peh and W. J. Dally, “Flit-Reservation Flow Control,” in 6th International Symposium on High-Performance Computer Architecture (HPCA), 2000, pp. 73–84. [19] A. R. Weiss, “The Standardization of Embedded Benchmarking: Pitfalls and Opportunities,” in IEEE International Conference on Computer Design (ICCD), 1999, pp. 492–498. [20] J. L. Henning, “SPEC CPU2000: Measuring CPU Performance in the New Millennium,” IEEE Computer, vol. 33, no. 7, pp. 28–35, 2000. [21] A. J. KleinOsowski and D. J. Lilja, “MinneSPEC: A New SPEC Benchmark Workload for Simulation-Based Computer Architecture Research,” Computer Architecture Letters, vol. 1, 2002. [22] P. Gratz, C. Kim, R. McDonald, S. W. Keckler, and D. Burger, “Implementation and Evaluation of On-Chip Network Architectures,” in IEEE International Conference on Computer Design (ICCD), 2006. We used synthetic benchmarks along with static traces generated from SPEC CPU2000 trafﬁc to evaluate the performance of the OPN micronet under different loads. We found that the offered trafﬁc varied widely across multiple applications and across different processor tiles; stochastic workloads are not representative of such real workloads. Our experiments conﬁrm the expectation that distributed processor performance is quite sensitive to network latency, as just one additional cycle per hop results in a 20% drop in performance. Increasing the link width in bits does not help this network since the messages already consist of only one ﬂit. Replicating the network to improve bandwidth and reduce latency is promising as increasing the wire count in on-chip networks is not prohibitively expensive. However, network router area (particularly router buffers) is not insigniﬁcant and these costs must be balanced with network performance beneﬁts. We expect that ﬁne-grained networks will increase in importance, initially as memory oriented networks for chip multiprocessors and SoCs, but ultimately in support of ﬁner grained communication and synchronization. Further research is needed to re-examine standard multichip interconnection network architectures with respect to the constraints and opportunities of on-chip networks. In addition to network latency and area, we expect network power, efﬁciency, and quality of service to be critical. We also expect micronets to provide new opportunities in other aspects of distributed system and processor design. As an example, we are currently examining how micronet ﬂow control can help reduce area overheads of distributed memory ordering hardware. "
Towards Open Network-on-Chip Benchmarks.,"Measuring and comparing performance, cost, and other features of advanced communication architectures for complex multi core/multiprocessor systems on chip is a significant challenge which has hardly been addressed so far. This document outlines the top-level view on a system of benchmarks for networks on chip (NoC), which intends to cover a wide spectrum of NoC design aspects, from application modeling to performance evaluation and post-manufacturing test and reliability. For performance benchmarking, requirements and features are described for application programs, synthetic micro-benchmarks, and abstract benchmark applications. Then, it proposes ways to measure and benchmark reliability, fault tolerance and testability of the on-chip communication fabric. This paper introduces the main concepts and ideas for benchmarking NoCs in a systematic and comparable way. It will be followed up by a report that will define a benchmark framework and the syntax of interfaces for benchmark programs that will allow the community to build-up a benchmark suite","Towards Open Network-on-Chip Benchmarks  Cristian Grecu1, Andrè Ivanov1, Partha Pande2, Axel Jantsch3, Erno Salminen4, Umit Ogras5,  Radu Marculescu5  1University of British Columbia, 2Washington State University, 3Royal Institute of Technology,  4Tampere University of Technology, 5Carnegie Melon University  Abstract— Measuring and comparing performance, cost, and  other features of advanced communication architectures for  complex multi core/multiprocessor systems on chip is a significant  challenge which has hardly been addressed so far.  This document outlines the top-level view on a system of  benchmarks for Networks on Chip (NoC), which intends to cover  a wide spectrum of NoC design aspects, from application  modeling to performance evaluation and post-manufacturing test  and reliability. For performance benchmarking, requirements  and features are described for application programs, synthetic  micro-benchmarks, and abstract benchmark applications. Then,  it proposes ways to measure and benchmark reliability, fault  tolerance and testability of the on-chip communication fabric.  This paper  introduces the main concepts and  ideas for  benchmarking NoCs in a systematic and comparable way. It will  be followed up by a report that will define a benchmark  framework and the syntax of interfaces for benchmark programs  that will allow the community to build-up a benchmark suite.   Index Terms— metrics,  evaluation, benchmarks.    networks-on-chip,  performance  I. INTRODUCTION  T  HIS paper motivates the need for a Network-on-Chip  benchmarking, outlines the compelling features of a NoC  benchmarking environment, and describes an initiative toward  establishing some widely accepted and useful benchmarks.   A. Benchmarking Problem  The practical implementation and adoption of the NoC  design paradigm faces multiple unresolved issues related to  design methodology/technology and analysis of architectures,  test strategies and dedicated CAD tools[1]. To advance and  accelerate the state of the art of the NoC paradigm R&D, the  community  is  in need of widely available  reference  benchmarks [1][2].   Classic benchmarks for multiprocessor systems [3][4] are  application-oriented, and cannot be used directly  for  communication-intensive  architecture  such  as NoCs.  Moreover, the nature of the applications running on NoCbased designs is expected to be more varied and heterogeneous  to  typical  applications  for multiprocessor  compared  computers.  The current SoC benchmark circuits, e.g. ITC 2002 [5],  contain only a very limited number of blocks; they target test  development only, and do not reflect the high level of  integration specific to the NoC scenarios. We promote a  collaborative initiative to develop NoC benchmarks that will  foster improved and accelerated development in this field.  B. Benchmarks Characteristics  The problem of NoC design is extremely complex and it can  only be solved by identifying and parameterizing the elements  that potentially define a NoC, their properties, and their  interactions. As such, it is clear that a NoC benchmark has to  be applied on more than a simple topological description of a  certain circuit (such as in the case of well-known ISCAS85 or  ITC 2002 benchmarks), but rather on a combination of  hardware blocks (functional IP cores), a NoC fabric (routing  elements and wires) with a certain topology/architecture, and a  specification for the traffic of data through the entire system.  These elements can be thought to be orthogonal, and different  benchmark scenarios can be created by their superposition.  A minimal list of properties and features that a NoC  benchmarking environment should exercise  includes  the  following:  - Network size (small, medium, large)  -  IP core composition (amount of processing, memory  cores, other)  - Topology (regular, irregular)  - Traffic characteristics (spatial and temporal)   - QoS requirements (best-effort, guaranteed bandwidth,  guaranteed latency)  The different characteristics of the NoC benchmarks with  respect to the above properties will allow the evaluation of the  NoC performance parameters, among the most important ones  being the throughput, latency, power/energy dissipation, and  silicon area.   C. Proposal Outline  In view of the propriety issues involved, we propose that the  interested community work toward the development of a set of  synthetic benchmarks characterized by the following sets of  orthogonal parameters:         i)  ii)  the associated  A set of relevant metrics and  measurement methodologies  A set of parameterized reference inputs for the NoC  benchmarks, consisting of:  a. NoC  functional cores composition  (# of  Processing Elements - PEs -, number and size of  memories, number of I/Os)  b.  interconnect architectures;  c. data communication requirements.  The term synthetic refers to some level of abstraction, for  example a task graph with known computation times and  communication loads instead of actual application code. Of  major  importance  is  the  benchmark measurement  methodology, which defines parameters of interest and their  points of measurement (in time and space) relative to the  structure and representation of the NoCs. For coherent,  reproducible evaluation and comparison of research data,  adequate metrics must be available.  These sets of parameters (a, b, c), when combined, would  suffice to yield a meaningful and useful representative set of  NoC characteristics. For instance, core composition would  characterize the NoC with respect to the number of processing  elements, memory elements, I/Os. For testing purposes,  additional information would be required, for example, the test  strategy (e.g., BIST, scan, etc.) for each functional core and  test related parameters (e.g., number of scan chains, scan  chains lengths, number of test patterns, number of I/Os, etc.).  This type of information is mainly intended to allow the  development and comparison of system level test synthesis and  scheduling techniques.  The interconnect architecture is intended to characterize  NoCs with respect to the data transport capabilities of the  communication  fabrics.  The  proposed  interconnect  architectures to date can be classified into one of the  following: cube-based  topologies,  tree-based  topologies,  irregular ones, and their different combinations.  Data communication  requirements would define  the  communication needs of the synthetic NoC. This set of  parameters consists of inter-core bandwidth/latency, data  integrity requirements, and spatial/temporal traffic distribution.  D. Long-term Benefits    The NoC benchmarks would envisage benefits similar to  those ensuing from our field or related fields. Examples  include:   Improved sharing and comparison of R&D results  The NoC design is still in its infancy and, generally,  companies and institutions are not open to share specifications,  models, and other proprietary data regarding NoCs. A set of  academic, synthetic benchmarks can be shared and used  without these limitations. The existence of an open format for  benchmarks specifications makes possible for  interested  research groups to contribute with relevant models and test  cases.  Increased, healthy competitiveness between R&D  The use of different metrics  and measurement  methodologies complicates comparison. For example, some  research groups tend to emphasize just the implementation  related issues, such as area or power, whereas the others  provide only a set of (anecdotal) performance values.  Increased reproducibility of results and commonality for  comparative purposes  Comparison of results is only possible if input data and  measurement are fully reproducible. Often, researchers tend to  use proprietary test cases, not always fully characterized and  therefore not entirely reproducible. Common benchmarks  allow fair and consistent comparison of different approaches.  Reproduction also  requires  thorough documentation of  measurement settings.  Accelerated development and analysis  Usually, designers build testbenches based on the initial  specifications to verify the functionality and performance of  complex systems. The existence of standardized input data and  hardware models can speed-up  the  initial design and  performance estimation phase.  Better scalability compared to application benchmarks  Application benchmarks offer the best accuracy but are  difficult to port for different systems, and their simulation is  time intensive, compared to synthetic benchmarks. They also  scale poorly with system size, for example the number of tasks,  which defines the maximum number of processing elements, is  fixed. Synthetic benchmarks  are more  suitable  for  benchmarking purposes  since they can exhibit properties of  particular fixed size application benchmarks, but can scale  with system size while still retaining these properties.  II. PERFORMANCE BENCHMARKS  Since many different parameters and factors influence the  performance perceived for a particular application, we need a  versatile set of devices to analyze, measure and compare the  performance of different NoCs. The overall performance for a  particular application is the ultimate criteria. However, there  are many different factors influencing this figure such as  algorithm design, functional partitioning, resource allocation,  mapping, communication services, buffering, flow control,  routing algorithms, physical design, clocking strategies, and so  on. To be able to accurately analyze and assess individual  factors we need a more sophisticated set of characteristics.  In the following two types of benchmarks are defined.   Benchmark programs are programs or models resembling  real applications, which exercise the entire communication  architecture and design methodology. Hence, they can be used      to assess the suitability of a particular NoC for a given  application or application domain.   Micro-benchmarks are stochastic benchmarks, which  exercise one or a few specific aspects of a communication  architecture. They can be used to gain insight into a particular  design feature or to compare and rate a number of alternative  techniques, e.g. several routing algorithms. The most common  example is uniform random traffic.  A. Hardware Platform Model   The model of hardware platform consists of a set of  Resource Models  (RMs)  that are connected  to  the  communication network, as shown in Figure 1.   RM RM RM RM NI/sockets M R M R R M R M RM RM RM RM Figure 1: A number of resource models (RMs) connected  to the communication network.  A RM is a model written in C, C++, SystemC, VHDL or  Verilog. It may be arbitrarily simple or complex. It models the  functionality of a resource in the system which communicates  with other resources over the network. It may represent a PE  (such as processor with software, a dedicated hardware block,  a DSP block), an on- or off-chip memory, or any other  resource in the system. The main requirement is that it  complies with one of the defined interface protocols like OCP  [6], AMBA [8], or similar (see Sec. 2.6) and that functional  tasks may be mapped onto RMs. The functionality may be  presented either as benchmark programs or as microbenchmarks.    B. Benchmark Programs  Programs are modeled as programs written in C, SystemC,  VHDL or any other language but without considering the final  implementation, be it any combination of HW and SW. For  benchmark programs the figure of merit is defined by the  benchmark itself because it may be different for different  benchmarks. In many cases the end-to-end delay and the  overall throughput of the system will be the most important  figures of merit. However, some benchmarks may focus on  other aspects. For instance, for a benchmark representing a  hard real-time system, the most important figure is the number  of deadline misses. For most embedded systems the memory  size has to be minimized. Consequently, benchmarks that  represent those applications will focus also on the amount of  buffer space needed at the network interfaces.  A benchmark program provides the following information:  Functionality:  - Application model  - Mapping and scheduling of application tasks to the  resource models (RMs)  - Set of RM as C, C++, SystemC, VHDL or Verilog models  Usage:  - Directions to connect the RMs to the network  -  Instructions for configuration and compilation  - Deadlines or other non-functional requirements of the  application (optional)  Topology and Mapping (optional):  - Size and topology of the network, the structure of the  routers  - Binding of a RM to a position in the network  Task descriptions are  independent of  the underlying  hardware (RM and network) and it defines the temporal and  spatial properties of the traffic. Tasks are explicitly mapped to  resources. In simplest the case, there is exactly one task per  RM but there can also be multiple tasks per RM. A resource  translates the operation count in application model into time  units. For example, if a simple resource model can perform 2  operations/cycle, a task with 500 operations takes 500/2=250  cycles to execute on that RM. Given the cycle time, this can be  translated to, for example, nanoseconds.  The usage information gives all necessary directions to  connect the RMs to the terminals of the network, initialize  them, run the benchmark and interpret the results. For instance  a benchmark program may require an initialization phase of  1µ s followed by an evaluation phase of 5 µ s.  The topology and mapping information are optional.  However, if it is not provided the evaluation includes also the  design methodology and  the  techniques  for network  configuration, resource allocation and resource binding. If a  benchmark program targets the benchmarking of the network  architecture (topology, routing, switching, etc.) it has to  provide the topology and mapping information because  different mappings may lead to radically different performance  results for the same network architecture. If this information is  not provided, the benchmark program can be used to evaluate  the combined effect of a topology configuration, mapping, and  the communication network itself.  C. Micro-Benchmarks  Micro-benchmarks intend to exercise a NoC in a very  specific way or measure a single particular aspect. Hence, they  offer insights on a specific property and facilitate the analysis  and design of a communication infrastructure. A single micro    benchmark provides only a very limited view and does not  allow for far-reaching conclusions about the suitability for an  application domain. However, a set of well designed  micro-benchmarks can give both a broad and detailed  understanding of a given communication network. Since even  a large set of micro-benchmarks is not guaranteed to represent  a real application well, micro-benchmarks are complementary  to benchmark programs. While benchmark programs evaluate  the combined effect of many aspects of the platform as well as  of  the application, micro-benchmarks  isolate  individual  properties and allow for a deeper point analysis.   Micro-benchmarks should cover a number of different  aspects which are outlined in the following.  Packets and Transactions:   The packet is the basic unit of information in the network ;  it has limited size. By measuring delay, bandwidth, jitter,  power consumption, etc. of individual packets, the network  itself, its routing, switching, buffering and flow control  mechanisms are evaluated. Distinction should be made  between the latency in the network and the delay for network  access.  Transactions are higher level communication activities that  evaluate data packetization, end-to-end flow control, streaming  capabilities and similar services offered by the network. In  addition to the network, transactions also evaluate the interface  blocks between the network and resources. Examples of  transactions are: memory read and write, read and write bursts,  transmission over an open connection or stream, opening and  closing of connections.   Unloaded and loaded case:  In the unloaded case, individual packets or transactions are  measured without any network contention. This gives data  about minimum delay and peak performance.  The loaded case investigates the network behavior when many  independent packets and transactions compete for the same  resources. Congestion, arbitration, buffering and flow control  policies will be exercised.  Temporal and spatial distribution:   In the loaded case, different traffic scenarios have to be  considered. The obvious and most widely used is the random,  uniform traffic where nodes communicate with each other with  equal probabilities. However, micro-benchmarks need to stress  the network with various different traffic scenarios to study  how the performance depends on the traffic patterns.   We need to differentiate between temporal and spatial  distribution of traffic generation [1]. The temporal distribution  determines how an individual RM generates traffic over time.  In particular, bursty traffic scenarios have to be covered  adequately. The spatial (i.e. target) distribution governs the  spatial traffic pattern: who communicates with whom. The  benchmarks should cover traffic scenarios with localized  traffic, hot-spot patterns and other typical patterns that  represent important application characteristics.  Best effort and guaranteed services:  Micro-benchmarks measure the performance of best effort  traffic. Guaranteed throughput services are supposed to  provide guarantees on minimum throughput and maximum  latency, which should be verified by some other means.  However, it is important to study the effect of resource  allocation by guaranteed services on best effort traffic  performance.  Network size:   To study the scalability of the communication networks of  various sizes with up to several hundred nodes shall be  exercised by scaling the micro-benchmark accordingly.  The set of micro-benchmarks shall systematically exercise  all important aspects of a NoC. It will give the NoC developer  insight and guidelines for improvement. It will also give the  NoC user a detailed understanding of the NoC behavior, its  strengths and weaknesses.  D. Communication-centric Application Modeling  Synthetic benchmarks and traffic generators are useful for  exercising various aspects of the communication network and  evaluating different network configurations. However, they  must be able to capture the control and data dependencies  between the tasks. For example, in an MPEG encoder, the  variable length encoding operation can start only after the  discrete cosine  transform and quantization  tasks are  completed. Therefore, simple stochastic models for RMs with  constant transfer probabilities are likely to result in inaccurate  estimates. Capturing the control and data dependencies  becomes vital especially, when one targets a specific class of  applications. A complete implementation of a real application  will obviously capture all the dependencies; however, it may  be very complicated and time-consuming. Moreover, the  behavior of a task as a traffic source/sink, i.e., its model of  communication, as opposed to the details of the computation it  performs, is sufficient for the evaluation of the communication  architecture. For this reason, we propose to utilize Finite State  Machines (FSM) that mimic the tasks of a real application in  terms of communication.  1) Overview  In the context of embedded systems, applications are usually  modeled as communication task graphs (CTG) [1]. The tasks  in the CTG exhibit both control and data dependencies.   The control dependencies imply that one task cannot be  executed before its predecessor tasks are completed, while the  data dependencies indicate communication between two tasks.  Furthermore, CTGs are widely accepted and there are publicly  available benchmarks [4] and CTG generation tools [1].  Hence, we use CTGs as an entry point to the proposed  methodology.  Basically, each PE in the network is characterized by an  FSM generated automatically from CTG and mapping  information. The PE is connected to a router via bidirectional    links and comply with common interface protocols. Instead of  generating a packet randomly, the PEs follow the rules  specified by the underlying CTG, as explained below.   2)  Data generation/consumption  Each PE is a Finite State Machine with the following  information:  Task list (i.e. mapping):   This is an enumeration of tasks that are mapped to  corresponding PEs. It provides the information regarding the  sender list (the list of PE that can send data to the current PE)  and the destination list (the list of the PEs to which the current  PE sends data).  Control information:   This provides information regarding the data dependencies.  For example: “If enough data is received from source i, then  initiate a  transaction with destination  j”. For detailed  modeling, the control information includes also the internal  state of the task (e.g. its current execution time).  Processing  time:   This expresses the amount of time it takes for the PE to  complete a certain task. After receiving enough data, the PE  will execute a finite number of operations (modeled as waiting  for certain period), before generating a response.  Transaction data amount:   This accounts for the size of the transaction that will be  generated after processing. Based on the control and data  dependencies specified by the underlying CTG, there may be a  number of different PE types:  SISD (Single input, single destination):   This is the simplest case. The PE may receive data only  from one PE. After processing the input, the resulting  transaction is sent only to a single destination.  MISD (Multiple input, single destination):   In this case, more than one PE may send data to this PE.  This situation corresponds to a scenario where inputs from  multiple sources are needed to perform a certain type of task.  After the task is completed, the response is sent to a single  destination. One can further divide MISD into two classes:  dependent or independent inputs. For the case with dependent  inputs, all the inputs have to be received before the processing  can take off, while for the independent inputs a subset of  inputs is sufficient for processing to continue.  SIMD (Single input, multiple destination):   This type of PE receives input from a single source, but the  packet generated after receiving this input is sent to multiple  destinations.  MIMD (Multiple input, multiple destination):   This type of PE reflects the most general case and the others  are simply sub-cases of MIMD. In this case, the PE can both  receive from and send to multiple PEs. The inputs can be  either dependent or independent as in the MISD case.  3) Overall operation  The PEs and their associated FSMs describing the target  application are automatically generated from the CTG of the  application, available RMs, and given mapping. Then, the PEs  and routers are connected to each other as specified by the  network topology. The PE that implements the root task in the  CTG initiates the application by injecting the input data to the  network. The input PE repeats this process at a period  specified by the CTG.  The operation of a sample PE is depicted in Figure 2. All  PEs, except the one that implements the root task, are initially  in the waiting state (State 1 in Figure 2). As soon as all  necessary data are received, the PE enters the processing state  (State 2 in Figure 2). The PE stays in this state until the  processing is completed. This duration is determined by the  underlying task graph and the type of the processing element.  This state is followed by State 3 where the PE generates the  output data and sends it to the PEs on the destination list. After  this, the PE enters again the waiting state.  1 1 2 2 Wait for new data  Wait for new data  for processing for processing Data received Data received Process the input  Process the input  data data More data elements are  More data elements are  expected expected 3 3 Processing is  Processing is  completed completed - Generate the output  - Generate the output  - Send the packets to                - Send the packets to                ..the network ..the network Figure 2: The finite state machine describing a basic  operation of a generic processing element.  To summarize, the PEs mimic the real application as the  generated data traffic matches the actual implementation of the  application. As a result,  the application run-time and  throughput can be obtained, and different network  configurations can be evaluated accurately as described below:   Application run-time:   This is the time it takes to complete the application. We  note that this metric can be reliably computed only when the  data and control dependencies within the application are  captured as in the proposed method. This method also gives  insight to utilization level of the PEs and how well the  deadlines are met.  Application throughput:   This is the minimum period at which the input PE injects  data determines the maximum application throughput. If the  input PE injects data at a faster rate, the bottleneck PE among  the downstream nodes cannot accept it at the same pace and  the data will be blocked after the available buffering resources  are exhausted.      E. Measurement Points  In addition to defining how to exercise a network, it is also  important to unambiguously define where to measure the  parameters of interest.   RM CA RM tr ansact ion delay packet delay CA NoC C A - comm un ic at i on assi st          (mode ls the NI /socke t) Figure 3: Points of measurement.  Figure 3 shows two different delays that have to be  distinguished. The packet delay denotes the delay of a packet  between the moment its header is injected into the network and  the moment when the last flit leaves the network. In the  communication assist (CA) mode usually some buffering is  done when the packet cannot enter the network immediately.  Also, for transactions and longer messages, the CA performs  packetization, de-packetization,  reordering of  received  packets, and connection management. All packet-related  measurements shall be done between the CA and the network  fabric, while all transaction-related measurements shall be  done between CA and the RM. In addition to average values,  both minimum and maximum latencies are measured at the  same time. In certain cases, the variation in latency (i.e. jitter)  is also important.  F. Interfaces and Sockets  To make a benchmark reusable for analyzing different  networks, both the benchmark and the network have to adhere  to a standard interface definition. Thus, the benchmark  initiative will define the interfaces and protocols that networks  under test and benchmark programs have to adhere to.  Similarly, the file format for delivering the benchmarks will be  defined.  Protocols such as OCP or ARM’s AMBA [8] serve this  purpose well, and their use simplifies both the benchmarking  and the design of NoC-based systems. The OCP protocol is  core-centric and interconnect agnostic, which allows the  network interfaces (NI) to actually deliver to the NoC fabric a  set of standard communication signals (defined by the OCP  socket) organized in packets and enhanced with the set of data  that is specific to the particular network protocol.  The OCP-IP socket is openly available, popular, and has  sufficient high level and advanced concepts, such as multiple  open, pipelined and out-of-order transactions. Therefore, it  seems suitable for interfacing with NoCs. All benchmark  programs or micro-benchmarks accepted  in  the NoC  benchmark suite shall adhere to the OCP-IP, AMBA or a  similar protocol. The precise requirements will be defined.  The corresponding benchmark program will thus measure  the performance of NIs from the OCP socket to the point  where packetized data is injected into the NoC interconnect.  This allows the benchmark to be independent of the functional  cores and concentrate on assessing  the communication  performance and parameters of the NoCs.  III. BENCHMARKS FOR NOC TEST AND RELIABILITY  One of the most important requirements for the NoC design  methodology to be widely adopted is to be complemented by  efficient test mechanisms [11]. In the case of NoC-based chips,  two main aspects have to be addressed with respect to their test  procedures:  -  how to test the NoC communication fabric, and   -  how to test the functional cores (processing, memory  and other modules).    Hence  the  (post-manufacturing)  test  strategies of  NoC-based interconnect infrastructures need to address two  problems:   -  testing of the switch blocks;   -  testing of the inter-switch interconnect segments.  The NoC  interconnects are characterized by poor  controllability and observability, due to the fact that they are  deeply embedded and spread across the chip. Pin-count  limitations restrict the use of I/O pins dedicated for the test of  the different components of the data-transport medium. The  NoC fabric is a mix of active and passive components that are  exposed to a multitude of faults. The NoC switches contain  both memory elements and logic blocks, for which the fault  mechanisms and models can be significantly different.  However,  they need  to be  integrated  in a consistent,  streamlined manner. On the other hand, the inter-switch links  are sensitive to interconnect-specific faults such as opens,  shorts, delay-faults, and crosstalk faults.   A distinct direction for benchmarking NoCs is reliability  benchmarking. In a broader sense, the reliability issues  encompass aspects such as tolerance to post-manufacturing  faults and transient errors, tolerance to process, voltage, and  temperature (PVT) variations, resilience to transient errors  [13].  A. Test Access Mechanisms (TAM)  The problem of test access mechanism design for NoCs can  be formally described as follows: given a set of functional  cores and an on-chip communication fabric, design a hardware  mechanism that transports test input data from an on- or offchip source to the functional cores and to the communication  infrastructure, and delivers the test output data from the cores      and NoC to an on- or off-chip test sink. This problem is  somewhat similar to the more general TAM design for SoC  core-based  test; however,  the existence of a complex,  distributed data transport mechanism (the NoC infrastructure),  raises two issues.   First, how can this infrastructure be tested, and second, what  is the relationship between the NoC infrastructure and the  TAM infrastructure? The latter question appears when one  considers that the NoCs are particular forms of SoCs, for  which test methodologies (and cored-based test in particular)  are mature enough to provide a safe, if not fully optimized,  solution. Consequently, a NoC could be considered as a classic  SoC, and tested using the core-based approach [11], with the  communication fabric tested as a separate core. On the other  hand, since a NoC will already possess a dedicated data  transport mechanism, it makes sense to reuse that for purpose  of test data transport. While this is not the place to discuss one  solution in particular, it is obvious that a test benchmark has to  be transparent to the particular TAM used by a target NoC.   With  these considerations,  the overall benchmarking  approach requires as inputs the following data:  - test information for each functional core (test type - scan,  BIST -, number of test I/Os, number and configuration of  scan chains);  - test patterns for  infrastructure;  - optional,  information on  architecture.  the functional cores and  the NoC  the NoC  the  topology of  B. Test Scheduling  The goal of manufacturing test is to ensure that the NoC is  fabricated correctly, with respect to a specified set of faults.  To achieve this, for each core, a set of test patterns are  generated (on- and/or off-chip) and applied to the cores’ test  inputs. Subsequently, test output data is collected from the test  outputs and transported to the test sink for comparison with the  expected outputs. For large numbers of cores, this activity can  become extremely time-intensive, and this can raise the cost  associated to manufacturing test to prohibitive amounts.  Therefore, test scheduling has the objective of minimizing the  test time, and implicitly the total test cost. Additional  constraints that must be considered here are test power and test  area. The power dissipated during testing must be carefully  estimated, such that the power budget of the NoC is not  exceeded and the test area overhead is within acceptable  limits.  The data that must be provided by a benchmark contains:   - each functional core test information (test type, number of  test I/Os, nature and length of scan chains),   - individual test patterns for each core and for the NoC  communication infrastructure,   - NoC topology, and other constraints such as test power  budget, thermal characteristics, etc.  C. On-line Testing  Transistor sizes used in current fabrication technologies and  increased levels of integration expose the NoCs to a multitude  of transient faults during their life-time. Among the most  significant causes we can enumerate electromagnetic noise,  cosmic radiations and PVT variations. Moreover, many of the  possible NoC applications are in fields like communications,  avionics, defense, where reliable and safe operation of the  devices is one of the most important design parameters. In  order to monitor continuously their operation and ensure that  malfunctions can be detected and compensated for, it is a  common practice to perform some amount of testing on-line,  without disrupting the operation of the devices. Different NoC  designs may be more or less suited for on-line testing, and  specific techniques must be developed to perform on-line  testing in NoCs. Therefore, an important component of the  benchmarking must be evaluating the ability of a NoC to be  tested on-line.  D. Benchmarks for Fault Tolerance and Reliability  NoCs are particularly suited  for  implementation of  fault-tolerant techniques, due to their inherent parallelism and  potential for reconfigurability. Defect/fault tolerant techniques  can be implemented at different levels, from hardware  redundancy  to software-based error  recovery schemes.  Adaptive routing algorithms combined with error control  mechanisms show great promise in achieving fault-tolerant  on-chip communication. However, the impact on NoC power  and performance can be prohibitive, since extra-hardware or  traffic management schemes required for correcting faults and  re-ordering of the packets will generally tax the power budget.  A meaningful benchmark for NoC reliability must provide  QoS- and power-constrained application data and assess the  impact of particular fault-tolerance mechanisms on NoC  implementations.  A critical requirement for determining the efficiency of  different fault tolerant designs is the availability of relevant,  quantitative metrics. A fault tolerant NoC must be able to  recover from failures of the data transport mechanism. Fault  recovery performance refers to the time required to detect and  recover from a NoC fault (e.g. a crosstalk fault on an  interconnect link, a failed memory buffer, etc.). If the  maximum time to perform a fault recovery can be bounded  while still meeting the system performance requirements, then  the network  fault  recovery mechanism can be used  successfully. The fault tolerance metrics must be independent  of specific hardware features or NoC architectures. They  should allow NoC fault recovery performance to be assessed  from an application point of view.  A comprehensive fault tolerant approach consists of five key  elements: avoidance, detection, containment, isolation, and  recovery. They may be adopted individually or as hierarchical  combinations. The effectiveness of  the corresponding    [3] The  [2] E. Salminen, T. Kangas, T. D. Hämäläinen, J. Riihimäki, ""Requirements  for Network-on-Chip Benchmarking"", Norchip, Oulu, Finland, Nov. 21-22,  2005, pp. 82-85.  Standard  Performance Evaluation Corporation, SPEC,  http://www.spec.org/hpg/  [4] R. Dick, Embedded System Synthesis Benchmarks Suites (E3S)  http://www.ece.northwestern.edu/~dickrp/e3s/  [5] ITC'02  SOC  Test  Benchmarks,  projects.com/itc02socbenchm/  [6] Open Core Protocol Specification, Release 2.2, OCP-IP, 2006,  http://www.ocpip.org/socket/ocpspec/  [7] R. P. Dick, D. L. Rhodes and W. Wolf, “TGFF: task graphs for free,""  Proc. Intl. Workshop on Hardware/Software Codesign, March 1998.  [8] AMBA 3 AXI Specification,  http://www.hitechwww.arm.com/products/solutions/axi_spec.html.  [9] Y. Zorian, E. J. Marinissen, S. Dey, ""Testing Embedded Core-Based  System Chips,"" IEEE International Test Conference (ITC'98), pp. 130-143,  1998.  [10] A. Jantsch and H. Tenhunen, editors, Networks on Chip, Kluwer  Academic Publishers, 2003.  [11] P. P. Pande, C. Grecu, A. Ivanov, R. Saleh, G. De Micheli, ""Design,  Synthesis, and Test of Networks on Chips,"" IEEE Design and Test of  Computers, vol. 22,  no. 5,  pp. 404-413,  Sept/Oct,  2005.  [12] Y. Zorian, D. Gizopoulos, C. Vandenberg, P. Magarshack, ""Guest  Editors' Introduction: Design for Yield and Reliability,"" IEEE Design and  Test of Computers, vol. 21,  no. 3,  pp. 177-182,  May/Jun,  2004.  [13] V. lyengar, K. Chakrabarty, E.J. Marinissen, “Test Access Mechanism  Optimization, Test Scheduling, and Tester Data Volume Reduction for  System-on-chip” IEEE Transactions on Computers, vol. 52,  issue 12,  Dec  2003, pp:1619 – 1632.  [14] Vassos Soteriou, H. Wang, and Li-Shiuan Peh, ""A Statistical Traffic  Model for On-Chip Interconnection Networks"", Proc. of the IEEE Intl  Symposium on Modeling, Analysis, and Simulation of Computer and  Telecommunication Systems (MASCOTS), Sept. 2006.  implementations can be estimated using quantitative metrics  generally accepted for distributed communication systems.  We consider a set of fault tolerance metrics to be used for  assessing the reliability of a NoC subsystem:  - Detection latency (Tdet): the amount of time (in cycles)  between the moment a fault appears and the moment it is  detected.  - Recovery time (Trec): the amount of time (in cycles) that  passes between the detection and recovery of a fault.  - Availability: the ratio between the amount of time the NoC  subsystem is fully functional, and the total operating time,  including detection and recovery latencies.  The three parameters defined above have direct impact on  global performance figures of merit of the NoC, specifically in  terms of QoS: Tdet, Trec define the lower limit of the achievable  latency, and therefore the level of QoS that can be guaranteed  upon occurrence of faults. These metrics complement the  application metrics (latency, throughput) and resource metrics  (area, power consumption) measured in presence of fault  tolerant techniques.  E. Future Directions  In order to provide consistent and reliable means for results  sharing and comparison, the NoC benchmarks must be  provided in a format that is simple, flexible, non-ambiguous,  and allows for future improvements. Additionally, the IPsensitive nature of such benchmarks when  reflecting  commercial designs must be protected, so that the benchmarks  can remain open to the academic/industrial communities.  Another requirement for the benchmarks and their format is  modularity: according to their place in the NoC design/test  flow, some benchmarks may be the output of a design/test step  whose input is a different benchmark. When developing the  benchmarking methodology and formats, one should consider  how different benchmarks could possibly interact with each  other.  The input formats and detailed benchmarking methodologies  are  the object of a second document  that  the NoC  Benchmarking Workgroup will make available. Interested  parties wishing to provide feedback or contribute with  specifications and models are invited to contact the group  members.  Acknowledgements  The authors would like to thank OCP-IP for its continued  support and contribution. The authors also thank other  workgroup members and industry partners for the fruitful  discussion and their useful comments during this initiative.  "
Implications of Rent's Rule for NoC Design and Its Fault-Tolerance.,"Rent's rule is a powerful tool for exploring VLSI design and technology scaling issues. This paper applies the principles of Rent's rule to the analysis of networks-on-chip (NoC). In particular, a bandwidth-version of Rent's rule is derived, and its implications for future NoC scaling examined. Hop-length distributions for Rent's and other traffic models are then applied to analyse NoC router activity. For fault-tolerant design, a new type of router is proposed based on this analysis, and it is evaluated for mutability and its impact on congestion by further use of the hop-length distributions. It is shown that the choice of traffic model has a significant impact on scaling behaviour, design and fault-tolerant analysis","Implications of Rent’s Rule for NoC Design and Its Fault-Tolerance Daniel Greenﬁeld, Arnab Banerjee, Jeong-Gun Lee and Simon Moore Computer Laboratory, University of Cambridge, UK Daniel.Greenﬁeld@cl.cam.ac.uk Abstract Rent’s rule is a powerful tool for exploring VLSI design and technology scaling issues. This paper applies the principles of Rent’s rule to the analysis of Networks-onchip (NoC). In particular, a bandwidth-version of Rent’s Rule is derived, and its implications for future NoC scaling examined. Hop-length distributions for Rent’s and other trafﬁc models are then applied to analyse NoC router activity. For fault-tolerant design, a new type of router is proposed based on this analysis, and it is evaluated for routability and its impact on congestion by further use of the hop-length distributions. It is shown that the choice of trafﬁc model has a signiﬁcant impact on scaling behaviour, design and fault-tolerant analysis. 1. Introduction Rent’s Rule is tremendously useful in the analysis of large circuits in VLSI. As well as its original use for estimating terminal counts, it is exploited in determining wire length distributions [6], as well as in estimating critical paths in complex circuits even for future technology nodes [1]. Rent’s rule has been found to apply across vastly different scales, from only tens of gates to multi-million gate designs and beyond. More recently, Rent’s rule has been derived from ﬁrst principles using basic assumptions [4]. This means that it is not just an empirical observation, but can arise from something more fundamental and can even be more generally applicable. Rent’s Rule is widely used to determine the distribution of wires lengths and to approximate the critical paths in future architectures. Christie and Stroobandt [4] suggest Rent’s Rule arises naturally as a result of the locality of connections – that is blocks are placed/mapped with consideration as to how they need to connect to each other so as to not create undue wiring. Various tiled architectures exist that vary from Chip Multi-Processors (CMPs) to arrays of programmable logic or even to heterogeneous arrays that include custom-IP blocks. If one were to replace the wiring between these blocks with a NoC, then it is important to determine what the properties of this trafﬁc might be. This knowledge will aid us in the analysis and development of improved routers. In this paper we show the importance of trafﬁc distribution models in the analysis of NoC. We make a case for a Rent’s-based model and explore its implications. We ﬁrst demonstrate the undesirable scaling behaviour of uniform random and transpose trafﬁc models in Section 2. We then discuss why Rent’s-like behaviour might be more reasonable, and a bandwidth version of Rent’s rule is derived in Section 3. From this, a hop-length distribution, the NoC equivalent of a wire-length distribution, is constructed and explored for scaling and trafﬁc implications. In Section 4, we optimise the mapping of a semi-random task graph using simulated annealing. The result is used to generate comparable best-ﬁt Rent’s and exponential hoplength distributions. The expected distribution of trafﬁc types under Rent’s rule is analysed in Section 5, and the observations are used to design a new fault-tolerant router in Section 6. The three hop-length distributions: Rent’s, exponential and uniform random, are applied in the analysis of this fault-tolerant router in Section 7. We show how the choice of trafﬁc model makes a signiﬁcant difference in the impact of faults on routability and in the quantity of congestion around faults. Rent’s distributions are shown to fare considerably better than uniform random trafﬁc in these analyses. 2. Non-Scalable Trafﬁc Let us consider an array of tiles connected by a mesh network, where each tile generates on average one unit of bandwidth. We want to consider the effects of technology scaling on the NoC. As we move to newer process technologies, the clock frequencies of the Processing Element (PE) and NoC may also increase, but not necessarily by the same amount. For our analysis, we will optimistically assume that the NoC frequency scales by the same amount as the PE frequency. However, one must also take into account the scaling behaviour of the trafﬁc over the NoC. Uniform random and transpose trafﬁc are commonly used r t e u o r r e p h t d i w d n a b f o s t i n U  100  90  80  70  60  50  40  30  20  10  0 Relative growth in bandwidth with scaling Uniform Random dist Rent’s dist, β=0.4 Transpose dist Rent’s dist, β=0.7 16 64 256 1024 Number of tiles 4096 16384 Figure 1. Scaling behaviour of bandwidth per router for different trafﬁc distributions. The scale is such that, on average, one unit of bandwidth is generated per tile. to assess NoC designs, so it is instructive to look at the implications of such trafﬁc behaviour. Uniform random trafﬁc occurs when each tile has an equal probability of communicating with any other tile. Whereas transpose trafﬁc has the tile at position (x, y ) communicate with that at (y , x). Figure 1 shows the average bandwidth requirements per router with scaling. This should not be confused with the bi-section bandwidth, which grows even faster. Here, the amount of trafﬁc generated by each tile is a single unit at all scales, and thus does not include the further effects of clock frequency scaling. We can clearly see that for these trafﬁc patterns each router has exponentially growing needs, doubling per quadrupling in tiles, so even if the router link bandwidth did scale with the PE frequency, the number of links required between routers would need to grow exponentially to keep up. Each new link corresponds to more physical wires per router, which translates to an exponentially growing number of metal layers. Now, one might argue that different NoC topologies, such as a hierarchical mesh, might be able to handle this better. Unfortunately, regardless of the topology, physical wires are still needed to transport the data from one tile to another, thus different NoC topologies do not circumvent this exponentially growing need for metal layers. Indeed, another way of interpreting Figure 1 is that the metal layers for NoC routing also needs to double per quadrupling in tiles, with corresponding power and performance implications. Clearly, such scaling behaviour is unacceptable, but it is a simple consequence of assuming uniform random or transpose trafﬁc patterns. Indeed, if logic cells in a VLSI design were randomly placed, the amount of wiring and metal layers required would scale in a similar fashion. The reason this behaviour does not occur in the VLSI domain is because locality is exploited by placing connected cells close to each other, thus minimising wiring and improving timing and performance. The equivalent of VLSI placement in the CMP-domain is task-mapping. For a relatively small 5x5 tile array, the communication-sensitive task mappings by Hu et al [8] achieved an NoC energy savings of over 60% compared to random mappings. This agrees with the 65% bandwidth reduction predicted in Figure 1 for a Rent’s versus random distribution. For small arrays, this type of non-local trafﬁc might still be tolerable. However, as we move into hundreds or thousands of tiles on a chip, the exponentially increasing cost of such non-local trafﬁc patterns makes it impractical, and thus exploiting locality in communication becomes essential. 3. Rent’s Rule Future massively parallel CMP will execute applications with enormous amounts of parallelism. CMPs are very different to multichip multiprocessors due to the much higher-bandwidth and lower-latency communication channels available between cores on chip, enabling ﬁne-grain parallelism [12]. Software in CMPs is mapped to physical locations, with data ﬂowing along paths from one core to another. This combination of ﬁne-grain parallelism and dataﬂows is analogous to circuits in VLSI. Indeed, the MIT RAW project uses the term software circuits to describe such mappings [13], explicitly noting the similarity to VLSI-like place-and-route. The difference with VLSI is that software circuits use processors instead of dedicated logic blocks, and use a NoC instead of inter-block wires. Indeed, one could transform dataﬂows in one domain to the other, interchanging dedicated logic blocks with processors, and wires with a NoC. If software circuits are anything like hardware circuits, then it is interesting to explore whether they follow similar laws, such as Rent’s rule. In VLSI designs, Rent’s rule relates the number of terminals in a boundary, to the number of blocks within that boundary by a power-law relation: T = kGβ Where T is the number of terminals, G is the number of blocks (gates), k is the average number of terminals for each block, and β is the Rent’s exponent. We shall refer to this as the classical-version of Rent’s rule, as it applies to its traditional domain of logic design. In the following sections, we wish to show that a similar relationship can be applicable to NoC bandwidth, and examine what that would mean for scaling.         3.1. Reasons for a NoC Rent’s Rule As shall be shown in Section 3.3, for Rent’s rule to emerge, a certain type of bandwidth locality is required to be present at multiple scales. The reasons why locality of tasks is desirable for NoC is directly analogous to those for classical logic placement as can be seen in Table I. Just as it is typically undesirable to place two blocks on opposite ends of the chip and wire them up, it is also undesirable to map two communicating tasks to tiles at opposite ends. Indeed, for critical paths, it is desirable to place tasks as closely together as possible. Table I. Factors in communication locality Domain to minimise Delay Wires Wire delay Congestion Wire-density Power Wire buffering & length NoC NoC latency (& congestion) Cross-sectional bandwidth Hop-length & router utilisation The emergence of Rent’s rule, however, is dependent on the properties of the underlying graph being mapped, the mapping algorithm’s objective, and the underlying physical topology. The primary requirement for Rent’s rule is a certain type of locality at multiple scales. This requires that the graph being mapped contains sufﬁcient locality. For example, complete graphs, or uniformly random graphs have little or no locality to exploit. The mapping algorithm must also favour this type of locality, even if it is not an explicit objective, but rather an implicit one such as minimising critical paths and congestion. Finally, the underlying physical topology needs to be structured so that some form of multiscale locality can exist. For example, a single central star network does not lend itself to such locality, whereas a multiscale star network does. 3.2. Derivation: Christie-Stroobandt based To arrive at a Rent’s rule for bandwidth, we can follow a directly analogous derivation to that of Christie and Stroobandt’s [4], but in place of Gates and Terminals we can use Blocks and Bandwidth. For completeness, this is described below: Let us consider a boundary with G blocks and external bandwidth B . Suppose a small perturbation is made to the boundary such that ∆G additional blocks are included. Without additional information we can only estimate that the additional blocks are likely to require the same amount of communication per block as the original G blocks. Thus we would expect the bandwidth to increase by: (cid:1) (cid:2) B G ∆B = ∆G Now let us suppose that some of this added bandwidth is actually internal, between the original G blocks and the additional ∆G blocks. This internal bandwidth reduces the external bandwidth seen on the new boundary, which we characterise by a parameter β , where 0 ≤ β ≤ 1: (cid:1) (cid:2) B G ∆B = β ∆G If ∆B and ∆G are small in comparison to B and G, we can approximate this to a differential equation: dB B = β dG G which yields the bandwidth version of Rent’s rule: B = kGβ where k is a constant of integration corresponding to the average bandwidth per tile. We should note that the exponent here is different in value and meaning to the classical Rent’s exponent, thus it is important to make a clear distinction between them. Let us refer to the classical Rent’s exponent relating terminals to gates as the Rent’s terminal-exponent, and for this new derivation as Rent’s bandwidth-exponent. Now, consider a 2-D topology with four regions of equal block count. If these are combined together, then let us deﬁne α to be the proportion of external bandwidth to the four combined individual bandwidths: α = k (4G)β = 4β−1 4kGβ So α is independent of G, and thus independent of scale. Thus another way of interpreting Rent’s rule is that at each scale, we would expect the amount of bandwidth locality (characterised by α) to be the same. Indeed, we can provide an alternate derivation of Rent’s rule with only the assumption of multiscale locality. 3.3. Derivation: Multiscale locality based Let B (G) be a function describing the average external bandwidth for boundaries containing G blocks. Suppose that for all G up to the size of the design, the following condition holds: B (4G) B (G) = 4α We shall call this the multiscale locality constraint. Applying repeatedly with n levels of hierarchy we have: B (4nG) B (G) = (4α)n In particular let us set G to one, and deﬁne x = 4n . Since G and the particular boundaries are arbitrary, let us assume this relation holds continuously, extending n to (cid:4)log4 x B (x) B (1) = R+ : Then, (cid:3) 4β B (x) = B (1) xβ which we recognise as: B = kGβ 3.4. Comparison to classical Rent’s Rule There is an important distinction to be made between the classical and bandwidth versions of Rent’s rule. In the classical version, the terminals and blocks in a design are ﬁxed, regardless of how they are used, and so the Rent’s terminal-exponent is also ﬁxed. In the bandwidth version, especially over a CMP, the bandwidths are not ﬁxed and the usage of the NoC can vary dramatically from one application to the next. Thus the Rent’s bandwidthexponent is actually dependent on the software/application currently running on top of it and may change accordingly. The bandwidth-version of Rent’s Rule does not just apply to CMPs. For a VLSI design consisting of many blocks wired together, if one replaced these wires with a NoC, then the Rent’s terminal-exponent of the former may match the bandwidth-exponent of the latter under certain conditions. This happens if the wire/terminal switching activity for the original version is highly uniform, and independent of scale, then the number of wires is directly proportional to the expected bandwidth, and the Rent’s rule for wires automatically translates into the bandwidth version with identical exponent. Among other things, this constraint requires that long wires should have the same expected switching activity as short wires, otherwise the rule may not follow, or a different exponent may result. Additionally, this assumes a seamless transition from wires to NoC and neglects the effects that NoC congestion, saturation and latency may have on bandwidth. It should be noted that the classical version of Rent’s rule can be used to estimate bandwidths in VLSI by assuming a particular wire activity. However, this is quite distinct from a bandwidth version of Rent’s rule itself. The emergence of both versions of Rent’s rule arises from the type of placement optimisation used. In the classical domain, logic is placed so as to reduce wiring, and the number of wires between blocks may correspond reasonably well with the inter-block bandwidth requirements. In the bandwidth domain, tasks are mapped so as to reduce communication. So they are optimising different attributes, resulting in different Rent’s rule domains. For example, Relative growth in bandwidth with scaling Rent’s dist, β=0.4 Rent’s dist, β=0.5 Rent’s dist, β=0.6 Rent’s dist, β=0.7  4  3.5  3  2.5  2  1.5 r e t u o r r e p h t d i w d n a b f o s t i n U  1 16 64 256 1024 Number of tiles 4096 16384 Figure 2. Relative Bandwidth per Router for different Rent’s Coefﬁcients as the number of tiles grows. The scale is such that, on average, one unit of bandwidth is generated per tile. when using a NoC, the amount of communication between two blocks does not correspond to the amount of wiring between those two blocks. Indeed, there might not be any direct wiring between communicating blocks, and even when there is, the use of wires is shared and wire activity is application dependent. Thus one cannot merely use the classical Rent’s rule to estimate such communication. 3.5. Implications for scaling We can utilise the model to estimate the scaling behaviour of trafﬁc under Rent’s rule. Figure 2 shows how the average trafﬁc per router scales. We note that a small change in Rent’s bandwidth-exponent can increase average trafﬁc per router at large scales. For an exponent of 0.4, there is little change with scaling, however an exponent of 0.7 leads to a many-fold increase. Higher exponents lead to even greater increases with scaling, however, as seen in Figure 1, this behaviour is insigniﬁcant compared to the scaling of random uniform and transpose trafﬁc patterns. 4. Hop-length distributions Just as the distribution of wire-lengths is of importance in the classical VLSI domain, so is the distribution of hoplengths in NoC. The effect that hop-length distribution has on various analyses shall be demonstrated in later sections. Three hop-length distribution models are examined here. The ﬁrst model is based on Rent’s rule using Christie and Stroobandt’s equally optimised partitioning and placement model. The second assumes a uniform random model. The third examines a semi-random generated task graph that was mapped by simulated annealing, found to be close to exponential in distribution.         Hop distribution of models based on parameters of example task-graph  0.7 Rent’s dist, β=0.89 Exponential dist, α=-0.47 Random dist h d t i w d n a b f o n o i t c a r F  0.6  0.5  0.4  0.3  0.2  0.1  0  2  4  6  8 Number of hops  10  12 Figure 3. Three distributions ﬁtted to a semi-random task-graph. 4.1. The Rent’s rule-based distribution For the purposes of our NoC analysis, we shall restrict ourselves to a packet-based non-hierarchical mesh of homogeneous tiles. We use Christie and Stroobandt’s equally optimised partitioning and placement model [4] which is described in Figure 4. This assumes a more optimal placement at the lowest scales than what may commonly be found for gate-level place-and-route of large designs. In such designs, the impact of moving a gate several gatepitches away is small enough to be almost negligible, thus potentially resulting in less sensitive placement at the lowest scale. For the NoC, the impact of moving a task several tiles away can be quite signiﬁcant in latency. Thus we will assume here that there is no NoC equivalent of a Region III in the Rent’s Rule [4] (where gate-placement is less optimal). For this model of hop-distribution, we ﬁnd that the ﬁrst hop accounts for the bulk of NoC trafﬁc (75-90% for Rent’s exponents between 0.4 and 0.7). This means that communication between neighbouring tiles is the primary contributor to NoC congestion and power consumption when governed by Rent’s rule. If true, given the large cross-sectional lengths and wiring density available for communication between neighbouring tiles, this suggests that a separate low-latency communication path should perhaps be created for this, without the latency and power overheads of packetisation and NoC routing. We should note that when there is less optimal placement at the very lowest scale, the ﬁrst two hops account for the bulk of trafﬁc instead. While this analysis assumes homogeneous tiles, we can also estimate the behaviour of heterogeneous tiles. We note that a heterogeneous tile array may not be heterogeneous at all scales. For example, we can consider a 2x2 array Figure 4. Equations describing Christie and Stroobandt’s equally optimised par titioning and placement model of heterogeneous tiles that is then homogeneously tiled to a 16x16 area. In such cases, the placement of tasks is only restricted at the lowest levels, likely resulting in a signiﬁcant deviation from Rent’s Rule at these lowscales. In these cases, Christie and Stroobandt’s nonequally optimised partitioning and placement model can be used to assign a separate Rent’s exponent for that particular scale of heterogeneity. 4.2. The uniform random distribution When each source is equally likely to send trafﬁc to each destination, we have a uniform random trafﬁc distribution. This is a simple trafﬁc model commonly used to evaluate NoC. The resultant hop-distribution corresponds to the site function in Figure 4. Random mappings of task graphs also obey this hop-distribution. We see in Figure 3 that low hop-count interconnects are poorly weighted, whereas medium-sized interconnects, of around half the side length, dominate. 4.3. The exponential distribution The hop-distribution is a function of the mapping algorithm used and what it is trying to optimise. If the mapping algorithm’s objective is insensitive to the bandwidth or latency of communication, then it may very well produce a ‘random’ mapping from a communication perspective. For real applications, however, whether in the VLSI, SoC or CMP domains, communication is already a pressing system-level issue, and with further VLSI scaling will only become more so. Whether a mapping algorithm is trying to optimise for performance, or for power, locality     in communication will become an implicit or even explicit objective. However, depending on the application and mapping objective, a Rent’s distribution does not necessarily result. A semi-random task graph was generated by Task Graphs For Free [5]. The graph is not completely random in that it is split into multiple stages, with random links from each stage to the next or later stages. As these links between stages are random, there is only limited opportunity for extracting some locality. It should be noted that although this tool is commonly used, it is questionable whether this approach generates realistic task-graphs. A graph with 250 tasks was generated and mapped to an 8x8 array of tiles. Each task was randomly allocated a processor load, and each task edge randomly allocated a bandwidth load. Simulated annealing was chosen for task graph mapping. The analysis by Orsila et al [7] on simulated annealing parameter-selection was utilised. The objective function chosen for the task combines throughput and aggregate bandwidth, with the primary objective being to maximise throughput by minimising worst case load. ObjF n = NT asks × NT iles × max i∈tiles {loadi} + AggBW For each tile, the load is calculated to be the worst case of either the router load in each routing direction, or the processor load. This is deemed to be more representative of throughput measurement since a tile’s throughput can be processor-limited or router-limited. The worst-case load out of all tiles is then deemed to be representative of the inverse throughput of the entire mapped task-graph. The aggregate bandwidth measures the sum bandwidth of all tiles, and minimising this is a secondary objective. A decrease in worst-case-load can easily come at the expense of an increase in aggregate bandwidth. However, it is still in the interests of throughput maximisation for the annealer to reduce aggregate bandwidth since total throughput can become bandwidth-limited due to congestion. This is similar to how a VLSI place-and-route tool optimises critical timing paths, even at the expense of increasing wire density. The results from the annealing were then analysed to extract a Rent’s exponent and a hop-distribution. A recursive partitioning was used to determine the Rent’s exponent, and as can be seen by the ﬁt in Figure 5, there is some evidence of Rent’s-like behaviour. However the exponent is very high at 0.89, which usually suggests that it is close to a random graph. Random graphs generally exhibit poor locality and do not obey Rent’s rule behaviour. Nevertheless, looking at the hop-length distributions in Figure 6 we see evidence for some locality, though not of the Rent’s variety. For a Rent’s distribution we may expect the hop-length distribution to approximate a power-law at low values, leading to a linear relationship on a Log-Log  1000  100  10  1 y r a d n u o b t a h t d i w d n a B  0.1 1 Recursive bi-partitioning of 8x8 array β= 0.89 k = 3.8 4 16 Number of tiles in boundary 64 Figure 5. Recursive bi-par titioning of a mapping for the semi-random task-graph. h t d i w d n a b f o n o i t c a r F  1  0.1  0.01  0.001  1e-04  1e-05 Example bandwidth distribution (log-linear) average fraction of bandwidth  2  4  6  8 Number of hops  10  12 Figure 6. Bandwidth Dist. for the semi-random taskgraph taken over 100 simulated annealing mappings. scale. In Figure 6 we instead see an approximately linear relationship on a Log-Linear scale, thus indicating that it may actually be closer to an exponential distribution. An exponential distribution has a slower initial fall-off in frequency compared to a Rent’s distribution and so there is more of an emphasis for lengths of several hops, but a much smaller frequency for the single-hop, as can be seen in Figure 3. This distribution’s exponent is approximately -0.47. 4.4. Comparison of distributions The extracted Rent’s and exponential model’s exponents now allow for some meaningful comparison between these distribution models, and with the random distribution. In fact, in Figure 3 we can already observe the effect of trafﬁc models on a design decision. In both the exponential and random models, the ﬁrst hop accounts for a signiﬁcant portion of trafﬁc, whereas this is not so for the random model. Thus having a separate neighbour-to-neighbour communication path is most valuable for the Rent’s model,         a little less valuable for the exponential model, but not valuable for the random model. 5. Router-activity distributions Let us examine a simple dimension-ordered router. We can identify four main types of trafﬁc that result in different router activity: • Originating – A tile may inject a new packet • Terminating – A tile may be the ﬁnal destination for the packet and thus remove it • Turning – A tile may switch from X-routing to Yrouting or visa-versa • Through – A tile may let trafﬁc pass through without changing its direction Let us look at the distributions of bandwidths from router to router. Here, Turning and Through packets take up bandwidth on both the incoming and outgoing edges of the router, whereas the Originating and Terminating packets take up bandwidth on only one edge. So we will assign a weight of a half on Originating and Terminating steps. For a route of Manhattan hop length L, it is easy to show that the average number of Through hops is: L − 2 + 1 L The fractional term accounts for the additional Through hops for the routes without any turns. Similarly, the average number of Turn hops is: 1 − 1 L The distributions for the expected router bandwidth with Rent’s exponents ranging from 0.4 to 0.7 are illustrated in Figure 7 for 64 to 16K tiles. The relative proportion of Turning trafﬁc stays fairly constant for each value of β . This is fairly unsurprising since we have deliberately assumed a minimal number of turns. It is interesting to note that for small β there is very little Through or Turning trafﬁc. This is because, as noted in Section 4.1, the trafﬁc mainly consists of neighbour-to-neighbour communication, thus accounting for the dominance of Originating and Terminating packets. For large β , however, Through trafﬁc rapidly dominates with exponential increases in tile-size. The proportion of Turn trafﬁc, however, does not grow. This means that at large scales, a dimension-ordered router is predominantly letting trafﬁc through from one side to the other. We should note that for the uniform random model, we would expect that Through trafﬁc would dominate at all these scales, whereas in this model it only dominates at larger scales with higher Rent’s exponents. Under this Rent’s model, each router is relatively busy terminating and injecting packets, especially at lower Rent’s exponents, instead of forwarding packets. Figure 7. Expected distribution of router trafﬁc for a working tile, for systems with Rent’s exponent ranging from 0.4 to 0.7 6. A fault-tolerant router design In this section we propose a fault-tolerant dimensionordered router based on what we’ve learned about the distribution of trafﬁc types in Section 5. This design serves as one of the testcases in later sections, demonstrating how different trafﬁc distribution models affect the analysis of designs. 6.1. Implications of traﬃc distributions There are a number of important components within each tile. Existing tile-based architectures typically have a Processing Element (PE) of some sort (whether a processor, programmable-logic or custom IP block) that takes up the bulk of the tile area. Then there is the router logic which we can break down into various elements such as storage, injection, termination, arbitration, switching and signalling (including error-correction). Packet-switched routers are typically dominated by storage, arbitration and switching needs. If the probability of faults is approximately proportional to the area taken up, then the most likely faulty component is the PE, followed by these other components. Let us consider what happens if a tile has a faulty PE but a working router. The task that would have been allocated to the PE must be remapped elsewhere. Since no tasks are mapped to the faulty-PE tile, there are no Originating or Terminating packets for the tile and any logic that handles these types of packets is superﬂuous. Indeed, the faulttolerant router should be designed to ignore these requests since such PE faults may be Byzantine in nature. The dominance of Through trafﬁc in faulty-PE tiles has interesting implications for the design of routers. If the only trafﬁc that the router had to handle were Through trafﬁc, then it doesn’t need to redirect trafﬁc, so a Switch isn’t needed, nor an Arbiter for managing the Switch, and with simple ﬂow-control, the Storage FIFO could be eliminated as well. Indeed it no longer looks like a router, but instead becomes a pipelined interconnect. Surprisingly, perhaps, such an interconnect is sufﬁcient to handle the bulk of trafﬁc that the router would need to handle. Of course, actually routing trafﬁc is important too, but this requires all the switching, storage and arbitration logic in the router. Altogether this suggests that a layered approach should be taken to router fault-tolerant design which is summarised in Table II. Table II. Suggested modes of operation for a faulttolerant router Operation Mode Normal Through & Turn trafﬁc only Through trafﬁc only Turn-off/Ignore Fault-type Handled none Storage, switching and arbitration logic for Injection and Termination, and PE All storage, switching and arbitration router logic, and PE fault-free faulty PE or Injection / Termination logic faulty PE or bulk of router logic The three modes of operation vary from supporting only basic Through trafﬁc, to supporting both Through and Turning trafﬁc, and to fault-free operation supporting all types of trafﬁc. It should be noted that one may still need to route around faulty tiles if the links between tiles are also faulty. However, error-correction, whether applied at every tile, or every N hops, or end-to-end, may be used in conjunction with this strategy to further reduce the probability of failure. Previous work has also shown how a gracefully degrading NoC router [9] can maintain some functionality with some faults in the arbiters, switches and allocation, but it does not consider an even deeper faulttolerance mode that lets trafﬁc go through by effectively bypassing the router altogether. Figure 8 illustrates how a through-mode can improve fault tolerance compared to regular dimension-ordered routing. Here four different routing strategies are shown and how they affect a node’s reachability. Grey nodes indicate an unreachable node from the source. We can see that XY has worse reachability than an adaptable XY-YX routing algorithm, with the whole left side of Figure 8. Plot of tiles unreachable from a source tile due to faults the array unreachable. The redundancy in paths offered to the XY-YX routing algorithms allow the system-level mapping to statically choose a fault-free path. Note that we do not use any adaptive routing here, and so packets are still guaranteed to arrive in-order. Recall that we have assumed that the type of fault is statically covered, with the system-level providing for some adaptability to detect and cover the fault. The through-mode routers fare much better, especially compared to the regular XY routing algorithm. The one node that is unreachable by the XY-YX-thru router is because both possible turn points have faults there. 6.2. Area estimation Let us now assume we add a through-only mode for the XY and XY-YX routers. In order to provide throughmode support, additional wrapper logic needs to be placed around the existing router. This results in additional area, and consequently more places for faults to occur. Thus an assessment of such a router needs estimates of the area penalty imposed by through-mode support. To gain a ﬁrst-order estimate of areas, a Virtual-Channel (VC) wormhole router [10] was modiﬁed to include the bypass muxing and logic, as well as the additional buffering needed for a single ﬂit per direction. For ﬂow-control, the input ﬂit for each port is stored and immediately sent to the output of the adjacent port on the following cycle. A credit-based ﬂow control is used based on the free FIFO entries available at the destination router’s input port per virtual channel. The sent packet’s VC is used to decrement the credit before forwarding this information back to the previous router. This implicit credit reservation ensures that the faulty router is always guaranteed to be able to send a ﬂit, if it has one waiting. At this point the router acts as a pipelined interconnect, with some ﬂow control. In this way, only a single ﬂit per port needs to be stored for this path, rather than one ﬂit per VC per port. Also, full throughput can still be obtained for each VC without interruption as long as the FIFO size is larger than the round-trip latency of credit and ﬂit propagation. This means that a router with FIFO depth of four can still theoretically source trafﬁc through a single faulty router at full throughput. However, this round-trip latency means that the source router may be stalled for longer, and thus may reach saturation with less trafﬁc than if without the faulty router(s). If two or more neighbouring routers are acting in through-mode, and the FIFOs are not large enough to hide the credit-latency, then this can also cause a latency impact for congested VC channels. Table III. Original & through-mode areas Design @90nm Original router With through-mode area increase Dimensions(µm) 497x495.6 510.2x509.6 5.6% For simplicity, the conﬁguration of router mode was assumed to be static and setup by a scan-chain of two conﬁguration registers per router. The full control logic, however, was not implemented, as the goal was merely to obtain reasonable area estimates. The results after synthesis, and placement are summarised in Table III. We should note a potential future optimisation in area by removing the dedicated through-mode ﬂit storage and using part of the regular router FIFO storage instead. 7. Fault tolerance analyses It is desirable to minimise the impact of faults on the system level performance, power and yield/cost. While there are a lot of different types of fault, whether static manufacturing faults, electromigration, signal integrity and timing faults due to parametric variation, for our purposes we shall assume that there is a predetermined static cover of these faults while the device is turned on. We are interested in comparing, to a ﬁrst-order approximation, the impact of faults for some NoC fault-tolerance approaches. We are not as interested in whether or not a task graph can feasibly be mapped to an array of faulty tiles and routers. This is because even with simple XY routing, there is a trivially ﬁndable subset of tiles that are capable of talking to every other tile in this subset. This subset may be relatively small, but it means that a task graph can still feasibly be mapped. Importantly, however, its performance may be severely crippled as a result, thus we are more interested in the system-level impact than mappability. 7.1. Unreachability analysis When a task graph is mapped to an array of tiles, each task is assigned to a particular tile. The logical links between tasks then need to be routed from a physical source tile to a physical destination tile. Obviously if the logical link resides entirely in the same physical tile, then no such routing is needed. We shall ignore these and focus on the actually routed logical links, which we shall refer to as task-links. In the presence of router faults, some of these task-links may not have a reachable destination from the source. We shall refer to such links here as unreachable task-links. One way of estimating the impact of faults at the system-level is to determine the fraction of task-links that are unreachable. Each unreachable task-link would require the remapping of either the source or destination task to another tile. If there are a large number of these, we would expect the incremental remapping process to be computationally expensive, and that the resultant mapping would incur penalties in the system’s objective function, whether it be in power or performance. As was shown earlier in Figure 8, faults in the various routers can result in signiﬁcantly many unreachable nodes. The through-mode routers can also result in unreachability, and of course when the through-mode path fails, then the fault behaviour resembles the non-through-mode counterpart. The through-mode wrapper adds additional logic which we must add to the probability of failure. Although it is possible for the bypass path to be faulty whilst the core logic is not, for now we will use a pessimistic bound and assume that if any of the wrapper is faulty, then the whole router is considered to be faulty. Given the small area overhead, this assumption should only have a negligible effect on the ﬁnal results. Let us now analyse the fraction of task-links that are likely to be unreachable for these dimension-ordered routers. We shall assume that all the task-links that have been mapped are between pairs of functioning tiles. Tasks that are mapped to non-functioning tiles or tiles with nonfunctioning routers will need to be remapped regardless of the behaviour of the router. As this is independent of the choice of router, this inﬂuence on task-links and remapping is not considered in this analysis. Let fnorm be the fault rate for the normal router. We shall not estimate this value but instead use it as a parameter with which we can explore fault-rate sensitivity. For an XY router, consider a path of m hops. Then the probability of the task-link being unreachable is: fX Y (m) = 1 − (1 − fnorm )m−1 For the XY-YX router, we need to separately consider turning and non-turning paths, yielding: fX Y 2 (m) = 1 m fX Y (m) + m − 1 m fX Y (m)2 For Through-Mode routers, let θ be the fractional area overhead of the wrapper logic. Then for a simple Poisson fault distribution, the probability of a through-mode failure is estimated by: fthru = 1 − (1 − fnorm)θ And the probability of a turn failure is given by: fturn = 1 − (1 − fnorm)θ+1 Then the probability that a path of m hops fails for an XY-router with through-mode is: fX Y T (m) = 1 m − 1 m (cid:5) (cid:5) 1 − (1 − fthru)m−1 (cid:6) + m 1 − (1 − fturn ) (1 − fthru)m−2 (cid:6) Finally, for an XY-YX router with through-mode: fX Y 2T (m) = m 1 − (1 − fturn ) (1 − fthru )m−2 1 (cid:5) 1 − (1 − fthru )m−1 (cid:6) + m − 1 m (cid:5) (cid:6)2 Now we are in a position to use hop-distributions to determine the expected fraction of task-links that are unreachable and that thus need task-remapping. We do so by merely taking the weighted sum over the hopdistribution. Figure 9 shows the results for the three distributions: a Rent’s rule-based, an exponential, and a uniform random distribution, as well as a comparison between the three on just the XY-router. As can be seen, the distribution makes a signiﬁcant impact on the expected fraction of unreachable task-links. Taking the example of the XYrouter, which consistently performs the worst, the uniform random distribution results in about about a 4x increase compared to the Rent’s distribution. We can also see that relative comparisons can also be affected. For example, the Random distribution suggests that XY-with-through is approximately twice as good as the XY-YX routing for large fault-rates, whereas in the other distributions this relative gap is certainly not as large. Looking at the more realistic Rent’s and exponential distributions, we see that the absolute frequency of unreachable task-links induced by faults is considerably small, even with large fault-rates in the regular router. Looking at Table IV, we see that at a 1% fault rate in the  0 0%  0.01  0.02  0.03  0.04  0.05  0.06  0.07  0.08 10% 2% 4% 6% 8% Ordinary router fault rate F r c a i t o n o f d n a b w i h d t i l / s k n Unroutable links w Rent’s dist XY XY-YX XY-Thru XY-YX-Thru  0 0%  0.02  0.04  0.06  0.08  0.1  0.12  0.14 Unroutable links w Exponential dist  0.16 10% 2% 4% 6% 8% Ordinary router fault rate F r c a i t o n o f d n a b w i h d t i l / s k n XY XY-YX XY-Thru XY-YX-Thru  0 0%  0.05  0.1  0.15  0.2  0.25  0.3  0.35 10% 2% 4% 6% 8% Ordinary router fault rate F r c a i t o n o f d n a b w i h d t i l / s k n Unroutable links w Random dist XY XY-YX XY-Thru XY-YX-Thru  0 0%  0.05  0.1  0.15  0.2  0.25  0.3  0.35 10% 2% 4% 6% 8% Ordinary router fault rate F r c a i t o n o f d n a b w i h d t i l / s k n Unroutable XY links w each dist Random Exponential Rent Figure 9. Plots of task-link unreachability for three hoplength distributions Table IV. Unreachable task-links due to router faults, at a 1% fault rate for non-through-mode routers. Router XY XY2 XYT XY2T Rent Exp Rand 0.88% 1.6% 4.2% 0.27% 0.46% 0.91% 0.29% 0.50% 0.96% 0.017% 0.029% 0.052% non-through router, the XY-YX-through-mode router has almost two orders of magnitude less impact than that of the plain XY router. However, it is interesting to note that even the XY router is expected to impact less than 1% of mapped links for the Rent’s Distribution. This suggests that even a 1% fault rate in the XY routers may potentially be accommodated by system-level task remapping, although the same may not hold for the Random distribution at 4.2%. 7.2. Induced congestion analysis We can also use the hop-distribution to calculate the impact of a single fault on the congestion of surrounding tiles. Let us compare this for our Through-Mode fault                 model to another approach which involves turning upon encountering faults [3], [11]. Lets refer to this strategy as Turn-Mode fault-tolerance. For paths along a single dimension, one turn at a fault is insufﬁcient, and so a second turn is forced immediately after the ﬁrst one, to route around the fault. Alternate models use fault-rings or convex fault-regions which direct trafﬁc around it [2], which can potentially result in even more congestion, since all the trafﬁc encountering faults, are forced to route around a ring or ﬁxed path surrounding the faults. For the Through-Mode model, it can be shown that the expected increase in trafﬁc at a node is given by:   R (x, y ) = R1 = − 2L(cid:10) R1 x = 0 ∧ y = 0, R2 (|x| + |y |) x (cid:3)= 0 ∧ y (cid:3)= 0, R3 (|x| + |y |) 1 − 1 F (k) otherwise (cid:2) (cid:1) k=1 R2 (d) = F (d) 4d + R3 (d) = − 2L(cid:10) k=d+1 k 2L(cid:10) (cid:1) k=d+1 F (k) 2k k − d − 1 2k (cid:2) F (k) Where a value of one corresponds to all the trafﬁc that would have been handled by the faulty router (including terminating and originating trafﬁc). For the dimensionordered approach, a numerical solution was run by computing the difference between routes with the fault and without the fault, weighted by the hop-distribution. These two are compared in Figure 10. Here, the shades correspond to the percentage change in total router activity compared to the average router activity. It does not differentiate between the different router directions. The lighter regions correspond to a reduction in trafﬁc, and naturally the faulty through-mode router handles less trafﬁc. However, this plot does not count the trafﬁc that should have originated and terminated from the router, as this can no longer be handled by either and must be addressed by a system-level remapping. The intensity of the faulty turn-mode router corresponds to this trafﬁc, and so is different between the three distributions. We note that for the turn-mode routers, all three distributions result in more router activity immediately surrounding the fault, but the impact is quite localised. The through-mode router however has less marked router activity around the fault but results in slightly increased router activity over a larger area. It also results in reduced router activity along the axes of the faulty router. This is because when switching a task-link’s routing from XY to YX, all the intermediate routers along the X axis also experience less trafﬁc. We can also see signiﬁcant differences between the three distributions. The random distribution results in marked congestion around the Turn-mode router and under-utilisation in routers along the axis of the Throughmode router. The exponential distribution appears to be somewhere in the middle between the random and Rent’s type distributions. Both the spatial distribution and quantity of congestion impact the system’s ability to compensate by remapping. The impact of the faulty Through-mode router on the surrounding tiles extends considerably far in the Random model (9% even at the corners of the array), and this may make such a router less desirable than the localised congestion of the Turn-mode router. In the Rent’s distributions, however, the inﬂuence is far less (at most 4.5% around the fault, dropping rapidly to just 0.05% at the corners of the array), making it more congestion-friendly than the Turn-mode router (which is 15% around the fault). This means that under the Rent’s distribution, the Through-mode router is clearly superior, whereas this is not so under the Random distribution. Thus the choice of distribution makes a signiﬁcant impact on the selection of router designs. 8. Conclusions In this paper we have shown that the commonly used uniform random and transpose trafﬁc models for NoC do not scale with process technology. Noting the similarities between task mapping on CMP with place-and-route on VLSI, we explored a Rent’s Rule connectivity model. We have given two derivations for a bandwidth version of Rent’s Rule that can be applied to NoC. We have shown the conditions under which Rent’s Rule is applicable, particularly for the System-level mapping of tasks in CMP architectures. It was noted that the Rent’s bandwidthexponent can vary from one application to next, especially for CMP architectures. Assuming the law to hold, one can estimate the future requirements of NoC with scaling, and this was shown to be sensitive to the Rent’s bandwidthexponent of the application. It was then shown how Rent’s Rule results in a distribution of hop-lengths. Two other hop-lengths were also examined, one representative of uniformly random trafﬁc, and an exponential distribution that appears more representative for a semi-random graph mapped using simulated annealing. Under Rent’s rule, we analysed the distribution of trafﬁc types that a dimension-ordered router has to handle. This showed that the dominant router activity was in routing packets from one side and forwarding it to the opposite side. A through-mode that bypasses the router logic was thus explored as a fault-tolerance mechanism. For a modest Through-mode w Rent Through-mode w Exponential Through-mode w Random n o i t i s o p Y e l i T 4 3 2 1 0 -1 -2 -3 -4 n o i t i s o p Y e l i T 4 3 2 1 0 -1 -2 -3 -4 +40% +20% 0% -20% -40%+ 4 3 2 1 0 -1 -2 -3 -4 43210-1-2-3-4 43210-1-2-3-4 Turn-mode w Rent Turn-mode w Exponential +40% +20% 0% -20% -40%+ 4 3 2 1 0 -1 -2 -3 -4 43210-1-2-3-4 Turn-mode w Random +40% +20% 0% -20% -40%+ +40% +20% 0% -20% -40%+ +40% +20% 0% -20% -40%+ +40% +20% 0% -20% -40%+ 4 3 2 1 0 -1 -2 -3 -4 4 3 2 1 0 -1 -2 -3 -4 43210-1-2-3-4 Tile X position 43210-1-2-3-4 Tile X position 43210-1-2-3-4 Tile X position Figure 10. Collateral effects of a fault on router activity. The fault is located at the middle, and the shade indicates the change in activity of surrounding routers due to re-routing of paths that should have gone through the faulty router. area increase, it was qualitatively shown that this would allow improved reachability, however a quantitative analysis was desirable to assess its system-level impact. Two analyses were introduced that make use of the hopdistribution, one that calculates the number of unreachable task-links caused by faults, and another that calculates the expected congestion impact due to a fault. The different hop-length distributions were shown to produce markedly different results in these analyses, with the Rent’s distribution performing much better than the uniform random distribution. When comparing the congestion impact of faults in the through-mode router versus a turn-based faulttolerant router, it was shown that the choice of distribution made a signiﬁcant impact. Under the Rent’s distribution, the through-mode router was clearly better, but this was not so under the random distribution. "
Trade-offs in the Configuration of a Network on Chip for Multiple Use-Cases.,"Systems on chip (SoC) are becoming increasingly complex, with a large number of applications integrated on the same chip. Such a system often supports a large number of use-cases and is dynamically reconfigured when platform conditions or user requirements change. Networks on chip (NoC) offer the designer unsurpassed runtime flexibility. This flexibility stems from the programmability of the individual routers and network interfaces. When a change in use-case occurs, the application task graph and the network connections change. To mitigate the complexity in programming the many registers controlling the NoC, an abstraction in the form of a configuration library is needed. In addition, such a library must leave the modified system in a consistent state, from which normal operation can continue. In this paper we present the facilities for controlling change in a reconfigurable NoC. We show the architectural additions and the many trade-offs in the design of a run-time library for NoC reconfiguration. We qualitatively and quantitatively evaluate the performance, memory requirements, predictability and reusability of the different implementations","Trade-Offs in the Conﬁguration of a Network on Chip for Multiple Use-Cases Andreas Hansson1 and Kees Goossens2 1Electrical Engineering, Eindhoven University of Technology, Eindhoven, The Netherlands 2Computer Engineering, Delft University of Technology, Delft, The Netherlands 3Research, NXP Semiconductors, Eindhoven, The Netherlands 3 , m.a.hansson@tue.nl, kees.goossens@nxp.com Abstract— Systems on chip (SoC) are becoming increasingly complex, with a large number of applications integrated on the same chip. Such a system often supports a large number of usecases and is dynamically reconﬁgured when platform conditions or user requirements change. Networks on Chip (NoC) offer the designer unsurpassed runtime ﬂexibility. This ﬂexibility stems from the programmability of the individual routers and network interfaces. When a change in use-case occurs, the application task graph and the network connections change. To mitigate the complexity in programming the many registers controlling the NoC, an abstraction in the form of a conﬁguration library is needed. In addition, such a library must leave the modiﬁed system in a consistent state, from which normal operation can continue. In this paper we present the facilities for controlling change in a reconﬁgurable NoC. We show the architectural additions and the many trade-offs in the design of a run-time library for NoC reconﬁguration. We qualitatively and quantitatively evaluate the performance, memory requirements, predictability and reusability of the different implementations. I . IN TRODUCT ION Systems on Chip (SoC) grow in complexity with an increasing number of processors, memories and accelerators integrated on a single chip. These heterogeneous high-complexity chips are programmable and integrate a rich set of applications [1]–[4], e.g. PDA phones with mp3 players, cameras, radios and gaming. The application dynamism is increasing, both between applications, through run-time addition or removal [4], and within applications, for example the variation on bandwidth and computation demand in MPEG-4 [2]. Networks on Chip (NoC) have emerged as the design paradigm for scalable on-chip communication architectures, providing better structure and modularity while allowing good wire utilisation through sharing [5]–[8]. The programmability of the NoC enhances ﬂexibility and reuse in both the long term, over platform generation, and short term, when switching between use-cases on a milli-second granularity [1], [4], [7]. Internally, programming the NoC involves the individual routers and network interfaces (NI). To mitigate the complexity of reconﬁguration, the abstraction level must be raised and provide an interface between NoC implementations and applications [2], [9]. The system typically moves from one conﬁguration to the next as a result of user interaction [4], change in environment, e.g. signal reception, or resource availability, such as the battery level. A switch in use-cases upon such an event causes changes in the application task graph and subsequently the network connections [2], [10]. NoC reconﬁguration requires: 1) the means for performing changes, i.e. a programmable NoC architecture [11], [12], 2) the means to specify and compute conﬁgurations, either at run time [1], [2] or at design time [10], and 3) the facilities for controlling change, that is, orchestration of the actual reconﬁguration. When performing the latter task, it is crucial that the changes are applied in such a way as to leave the modiﬁed SoC in a consistent state, that is, a state from which the system can continue processing normally rather than progressing towards an error state [13]. The Intellectual Property (IP) modules in the SoC move from one consistent state to the next by issuing and serving transactions. These transactions modify state and while in progress, have transient state distributed in the system, including also the NoC. Simply updating the NI registers could cause out-of-order delivery or even no delivery, with an erroneous behaviour, e.g. deadlock, as the outcome. Should a transaction be corrupted or not even allowed to ﬁnish due to inconsistent NoC reconﬁguration, it is unlikely that the application will recover. Consider for example a situation where the response to a read transaction never arrives, causing the IP module that initiated the transaction to stall indeﬁnitely. Similarly, a write transaction that is only partially delivered might cause the target IP to stall, waiting for the outstanding data elements. These events are catastrophic to the SoC and must be avoided or recovered from. In this paper we address the actual conﬁguration process. We introduce the Æthereal Run-Time (ART) library for NoC reconﬁguration with a basic set of functions, through which we can connect the IP ports in a graph structure and dynamically add, remove or modify the interconnections, thus changing the task-graph topology in a consistent way. We show: 1) the hardware support required by the library to ascertain transaction consistency, and 2) the trade-offs (performance, memory requirements, predictability, ﬂexibility, reusability) in implementing the library functions. The remainder of this paper is organised as follows. Section II introduces related work and is followed by a problem description and delimitation in Section III. Then, Section IV discusses the architectural support and the different operations that are used to reconﬁgure the NoC. The details of the ART library implementation are given in Section V before we present the results in Section VI. Finally, we conclude in Section VII. I I . R E LAT ED WORK A comprehensive model of dynamic change management is given in [13]. The work discusses the requirements of the conﬁguration management and the implications of evolutionary change of software components. A more practical approach to dynamic application reconﬁguration in multi-processor SoCs is presented in [4], [14]. Reconﬁguration of the interconnect is, however, not addressed. A methodology for deriving procedures for deadlockfree reconﬁguration between routing functions is introduced in [15]. The work shows how reconﬁguration-induced deadlock, caused by additional dependencies in the transition between the old and new routing function, can be avoided. In this work we assume a ﬁxed network topology and employ turn-prohibited routing [16] with a ﬁxed set of prohibited turns. Hence, reconﬁguration-induced deadlock cannot occur. Much work is focused on complete NoC design and compilation ﬂows [9], [17], [18]. While the works suggest automated generation of conﬁguration code [17] and standardised NoC application programming interfaces [9], no details are given as how to facilitate the dynamic changes. NI architectures are presented in [3], [7], [11], [19], [20]. The interfaces in [11], [19], [20] are OCP compliant, whereas [7] presents an NI with support for DTL and AXI. A specialised NI, tightly coupled to an accelerator, is implemented and evaluated in [3]. All these works focus on providing programmability and only brieﬂy discuss how to employ it in practice. Methodologies for derivation of network conﬁgurations are presented in [1], [10], [21]–[24]. Design-time algorithms limited to a single use-case [22]–[24] are plentiful. In [10], multiple use-cases are supported by computing a number of hierarchical conﬁgurations at design-time. The works in [1], [21] add even more ﬂexibility by deciding the resource binding at run-time and allowing task migration. While showing how to determine NoC conﬁgurations, none of the works detail how to deploy it. Concluding, existing work addresses the problem of providing hardware run-time programmability and deriving a conﬁguration to be programmed. This paper investigates the actual detailed conﬁguration process and presents the necessary hardware additions and the trade-offs in designing a library for consistent NoC reconﬁguration. I I I . PROBL EM D E SCR I P T ION NoCs comprise two components: routers (R) and network interfaces (NI). The routers can be randomly connected amongst themselves and to the NIs (i.e., there are no topology constraints). The routers transport packets from one NI to another. The NIs enables end-to-end services [7] to the IP modules and are key in decoupling computation from communication [11], [25]. The NI is responsible for (de-)packetisation, for implementing the connections and services, and allows the designer to simplify communication issues to local point-topoint transactions at IP module boundaries, using protocols natural to the IP (e.g., AXI or OCP) [25]. The IP modules, or rather their ports, acts as either masters or slaves [7]. Masters initiate transactions by issuing requests. One or more slaves receive and execute each transaction. Optionally, a transaction also includes a response, returning data or an acknowledgement from the slave to the master. The term connection is used throughout this paper to denote a bidirectional peer-to-peer inter-connection between a master and a slave IP. As shown in Figure 1, a connection comprises a request channel, from master to slave, and a response channel in the reverse direction. Every connection is bound to four unique queues: one in the sending NI and one in the receiving NI, for both the request and the response channel. The Quality of Service, best-effort (BE) or guaranteed service (GS), is determined on a per-connection basic. Guarantees are provided by time-division multiplexed (TDM) virtual circuits [7]. Path = {East, South, East} Time slots = {0, 1} Dest. queue = 0 Dest. space = 4 Master D T L L T D Slave IP NI Router network NI IP Fig. 1. A connection with request and response channel on a 2×2 mesh. To set up a channel, only the source NI requires programming [7], [11]. Hence, the target NI is always ready to receive data and does not require any programming to do so. As indicated in Figure 1, the information required is: • a path through the router network, • a queue identiﬁer, selecting a queue in the destination NI, • the end-to-end ﬂow control credits, reﬂecting the size of the destination queue, and, in the case of a GS connection, • a set of TDM time-slots determining when the channel may use the network links. The NI also offers the possibility to set a lower limit for when data and credits may be sent. These threshold values are initialised to zero and therefore require no modiﬁcation if the functionality is not used. We assume that the conﬁguration is already computed, either off-line [10] or on-line [1], [21]. Similar to [1], [4], [7], [21], the conﬁguration is done by a general-purpose CPU (in our case an ARM) and is initiated by events in the system, caused by e.g. a user command to start or stop an application, an embedded resource manager [26] or mode switches in the incoming streams [4]. When modifying or closing channels, both the NoC and the IPs must be left in a consistent state, that is, a state from which the system can continue processing normally rather than progressing towards an error state [13]. Simply updating the NI registers could cause out-of-order delivery or even no delivery, with an erroneous behaviour, e.g. deadlock, as the outcome. Reconﬁguration of the tasks running on IP modules [4], [14], [27] is an important task that lies outside the scope of this paper. However, the techniques shown here are also applicable at the higher IP level. While parts of the presented algorithms are speciﬁc to Æthereal, the concepts described apply to NoCs in general and we give suggestions to alternative implementations throughout the text. IV. N E TWORK CON FIGURAT ION The conﬁguration of the network is hidden from the application programmer by an application-level conﬁguration API, such as C-HEAP [27], that switches between use-cases and leaves the actual NoC reconﬁguration to the ART library software. This approach hides the underlying implementation and eases integration as well as modiﬁcations [9]. Thereby, it is the responsibility of the conﬁguration management system, not the user, to determine the speciﬁc ordering of actual change operations applied [13]. A. Conﬁguration registers The network connections are conﬁgured at run time via a memory-mapped conﬁguration port on the NI, denoted Conﬁg in Figure 2. The conﬁguration port offers access to the NI registers through normal read and write transactions [12]. Shells Kernel Master mask equal block D T L Slave A X I slot0 slot1 slot2 path queue BE/GS limit space Conﬁg Fig. 2. Network interface registers. As seen in Figure 2, the registers are divided into three different blocks with three different address ranges. First, the path, queue, BE/GS, limit and space registers, with one instance per outgoing channel. This block controls, in order, the path through the router network (source routing), the queue to which data is delivered in the destination NI, the assigned service level (BE/GS), lower limits for when data and credits may be sent, and the ﬂow-control space counter that is used to track the occupancy of the destination queue. Second, the slot table, in this example with only three slots. These registers determine when the GS channels are scheduled and may inject ﬂits into the router network. Third, the registers in the protocol shells [7]. These registers are optional and depend on the functionality of the speciﬁc shell instantiations. The DTL shell connected to the master in Figure 2 has mask, equal and block registers. The ﬁrst two are used to implement narrowcast connections [7], whereas the latter is used when inactivating and closing connections, further discussed in Section IV-E. B. Conﬁguration infrastructure We assume that there exists one or more conﬁguration master modules that execute the conﬁguration operations. Such a module, typically a general-purpose CPU, only has one data bus and needs an infrastructure to facilitate access to the NIs in the platform. By connecting the conﬁguration port back to a (DTL) initiator port on the NI, as done on NIm and NIs in Figure 3, the conﬁguration data is carried by the NoC itself [7] instead of an additional control network, as advocated in [2]. By reusing the existing infrastructure, low-bandwidth control trafﬁc is allocated resources just like any user-generated trafﬁc. In Section IV-F we show that it is still possible to separate user trafﬁc from control trafﬁc by allocating TDM slots also for the latter. To reach NIs other than NIc from the conﬁguration master we set up conﬁguration connections over which the conﬁguration data is read and written. As illustrated in Figure 3(a), the ﬁrst step is to open a conﬁguration request channel from NIc to NIs . The opening of this channel only involves manipulation of the Rc registers in NIc , as indicated by the arrow marked 1. The conﬁguration request channel makes the Rs register in NIs accessible via the DTL port on NIc . To enable communication of responses back to NIc , we also establish a conﬁguration response channel, from NIs back to NIc . This is done by writing to the Rs registers of NIs , indicated by the arrow marked 2 in Figure 3(a). The response channel is left unused but in place when moving on to the initialisation of NIm (arrows 3 and 4). The conﬁguration connections, allocated resources just like any other connection [10], are all set up in the conﬁguration initialisation phase, further covered in Section V. Hence, the conﬁguration response channels are set up once and are already in place when the ﬁrst user-speciﬁed connection is opened. The conﬁguration request channel, however, is inactivated and re-opened for every NIt . There are two opportunities for adding parallelism in the conﬁguration infrastructure. First, having more than one conﬁguration master, e.g. hierarchically distribute the control to different subsystems. Second, by using more ports on NIc for accessing remote NIs and thereby enabling one master to conﬁgure multiple NIs in parallel. In our experiments we use a single conﬁguration master, and only one port for remote conﬁguration. C. Conﬁguration operations Conﬁguration of the network relies on three toplevel operations, closely related to the channel create, channel reconﬁgure and channel destroy primitives of CHEAP [27]. • Open a new connection. • Modify an existing connection. • Close an existing connection. Master bm,req bm,resp NIm D T L D T L c Rm Conﬁg Router network 4 1, 3 Conﬁg master Rc m C o n ﬁ g NIc DTL 2 Slave Master bs,req bs,resp NIs L T D L T D Rs c Conﬁg 1: 2: 3: 4: NIc NIs NIc NIm → → → → NIs NIc NIm NIc creq cresp Router network bm,req bm,resp NIm D T L D T L s c Rm Conﬁg 4 1, 3 Conﬁg master Rc m C o n ﬁ g NIc DTL 2 Slave bs,req bs,resp NIs L T D L T D Rs m c Conﬁg 1: 2: 3: 4: NIc NIs NIc NIm → → → → NIs NIm NIm NIs (a) Opening the conﬁguration connections. (b) Opening a user-speciﬁed connection. Fig. 3. Using the NoC itself as conﬁguration infrastructure. 1) Opening a connection: Algorithm IV.1 describes how a channel is opened on a target NI, hereafter denoted NIt . Algorithm IV.1 Open channel emanating from NIt 1) Open a conﬁguration request channel to NIt . a) Set the path, the destination queue in NIt (the queue of the looped-back initiator port), the initial space, and, if using GS for conﬁguration, also the time slots by writing to NIc via the Conﬁg port. b) Enable the queue in NIc for scheduling by the NI kernel scheduler. 2) Open the channel emanating from NIt . a) Set the path, the destination queue, the initial space, the thresholds, and time-slots in NIt by writing to the DTL port on NIc . b) Enable the queue in NIt for scheduling. 3) Close the conﬁguration request channel to NIt . a) Await the completion of the writes to NIt by reading the Conﬁg port of NIc until no data is left in NIc and all credits have returned from NIt (i.e. the transactions are consumed and executed by NIt ). In Step 2, with the conﬁguration channel in place, the registers of NIt are written. Thereafter, in Step 3a, the conﬁguration master waits until all transactions are delivered to the Conﬁg port on NIt . As seen in Step 3a this is done by busy-waiting on the local status registers in NIc until all data has been sent and all credits have returned. In a NoC that does not employ end-to-end ﬂow control this can be implemented with acknowledged writes or by reading back the value of the last register written. Using the primitive for channel opening, Algorithm IV.2 lists the steps necessary to open a connection between a master and a slave port. Recall that the conﬁguration response channels are already in place, as shown in Figure 3(b). The three writes required to set the registers in a target NI are posted and delivered in order as they use the same conﬁguration channel. When switching between different target NIs, the conﬁguration request channel must be closed. Therefore, Step 3 waits until NIs has consumed and hence executed the transactions (Step 3a in Algorithm IV.1). Algorithm IV.2 Open connection between NIm and NIs 1) Open a conﬁguration request channel from NIc to NIs . 2) Open the response channel from NIs to NIm . 3) Close the conﬁguration request channel from NIc to NIs . 4) Open a conﬁguration request channel from NIc to NIm . 5) Open the request channel from NIm to NIs . 6) Close the conﬁguration request channel from NIc to NIm . As we will see in Section VI, the time required to open a connection varies, although the operation only involves writes to the NI registers. D. Modify a connection When the source and destination remain the same (NI as well as queue), but the properties of the connection change, then we say the connection is modiﬁed. This is for example done to adapt to quality changes in a scalable algorithms [26], redistribute resources (links and time slots) between the connections, or to move connections from one path to another so that routers can be powered down [9]. For the individual channel, the modiﬁcation can reﬂect a change in: • the data or credit threshold, • the allocated bandwidth, • the guaranteed latency, or • the path through the network. The ﬁrst three modiﬁcations require only minimal updates. Time slots can be added and removed and threshold values changed even while the channel is being used. Modifying the path, however, is more complicated as we must preserve (loss-less) in-order delivery. A shorter path means ﬂits can arrive earlier than those already sent on the longer, or more congested, path and thereby invalidate the in-order delivery. For a GS channel, the transmission delay is known and directly proportional to the path length, as contention is avoided by means of pipelined virtual circuits [7]. Hence, if the new path is longer, then the update can be done without disabling and later re-enabling the queue. Should the path be shorter, then it is possible to only disable scheduling for δ ﬂit cycles where δ is the reduction in path length. Note, however, that this additionally requires the conﬁguration to be done over a GS connection as the inter-arrival time of the disable and re-enable operations at NIt must not be shorter than δ ﬂit cycles. If conﬁguration is done with BE connections this is not possible to guarantee. Modifying the path of a BE channel, with a nondeterministic delay, is comparable to adaptive routing, where a ﬂit i + 1 may overtake ﬂit i, due to a shorter or less congested route [28]. In Algorithm IV.3, we show the general (and safe) procedure for modiﬁcation of any (BE or GS) channel. Algorithm IV.3 Modify channel emanating from NIt 1) Open a conﬁguration request channel from NIc to NIt . 2) Reconﬁgure NIt . a) Disable scheduling of the queue. b) Await the return of all outstanding credits. c) Update the path and slots. d) Enable scheduling of the queue. 3) Close the conﬁguration request channel from NIc to NIt . The channel modiﬁcation can safely be done as soon as the router network has delivered all the ﬂits to the destination NI. Consider for example modiﬁcation of the request channel, creq in Figure 3(b). When the router network is empty there is no longer any risk for out-of-order delivery. However, NIm has no notion of what data is received, only what data is consumed, and this in the form of end-to-end ﬂow control. Hence, to ascertain that no ﬂits are in ﬂight, Step 2b not only waits until the router network has delivered all the ﬂits to the destination NI, but also until the data is consumed and the credits returned. This corresponds to emptying both the router network and bs,req in Figure 3(b). As a consequence of the above, channel modiﬁcation requires the destination of the channel, exempliﬁed by the slave in Figure 3(b), to sink all outstanding transactions. This might require progress on the reverse channel and due to this, care must be taken when ordering the conﬁguration operations. E. Close a connection The crux in closing a connection is to ensure that the master, slave and the network are left in a consistent state after the change, with no transient state distributed anywhere along the request-response path. Transactions, e.g. a DTL or AXI read, are initiated by the master and accepted by NIm . NIm subsequently engages in a number of transactions with NIs through the exchange of ﬂits and credits. Then, NIs acts as a master to the slave and delivers the request and accepts the response. Thereafter, the response goes through the network and is presented to the master IP module. The key observation is that transactions take place both on the IP (reads/writes) and on the network level (ﬂits/credits). When quiescence is reached on both levels, a change can take place with a consistent state as the outcome. Using the deﬁnition of [13], a module is quiescent if: 1) it will not initiate new transactions, 2) it is not currently engaged in a transaction that it initiated, 3) it is not currently engaged in servicing a transaction, and 4) no transactions have been or will be initiated which require service from this module. 1) Requirements: To achieve quiescence, a number of actions must be taken. Starting at the master in Figure 3(b), it is necessary to force the module into a state where it is not initiating any new transactions, but still continues to accept and service transactions that are outstanding. In terms of requests and responses, this translates to: 1) No new requests must enter the request buffer bm,req , but an ongoing request (e.g. a write burst) must be allowed to ﬁnish. 2) Requests already accepted into bm,req must be delivered, such that bm,req , creq and bs,req are empty. 3) All initiated transactions that require a response must be allowed to ﬁnish. Thereby, also bs,resp , cresp and bm,resp are empty. Step 1 requires knowledge of how command/address/data is accepted from the master and what timing relations are allowed between these groups of signals. Typically, a validaccept handshake ﬁrst takes place on the command and address signals with the master offering a request to the NI by driving the valid signal high. The NI in turn indicates that it has accepted the request by driving the accept signal high. Thereafter, the two parties engage in a similar handshake on the potential data words involved in the transaction. Step 2 arises due to the fact that the NoC acts as a slave, thus giving the master the impression that a transaction involving only a request is ﬁnished already when delivered to the NI shell (if posted). Step 3 requires knowledge of the various types of transactions allowed by the protocol to determine whether a request also gives rise to a response. This functionality is implemented in protocol shells, separating protocol speciﬁc functionality from the NI kernel. 2) Hardware implementation: The master protocol shell requires one input signal to initiate the close operation, passivate, and one output signal to assert that a quiescent state is reached, is quiescent. These two signals are read and written from the NI kernel through the block control register, shown in Figure 2. When a request channel is passivated, any ongoing request is allowed to ﬁnish, whereafter the accept signal to the master is gated and kept low, thus preventing it from initiating any new transaction1 . Before the is quiescent is asserted, all ongoing transactions must also ﬁnish. We implement this check with a counter inside the shell. The value is incremented for every accepted request that gives rise to a response, such as a read, 1 This infrastructure can also be used for debugging purposes [29]. and decremented for every response delivered back to the master. A signal activate is used to bring the shell back to an active state where new transactions can be initiated. 3) Algorithm: With the above functionality implemented in the protocol shells, a connection is closed according to Algorithm IV.4. Steps 7, 8 and 12 are carried out by repeatedly polling a remote register. These operations can be transformed to local busy-waits by using the concept introduced in [30]. Algorithm IV.4 Close connection between NIm and NIs 1) Open a conﬁguration request channel to NIs . 2) Set the thresholds for both data and credits to zero. 3) Close the conﬁguration request channel to NIs . 4) Open a conﬁguration request channel to NIm . 5) Set the thresholds for both data and credits to zero. 6) Passivate the connection by writing to the block registers in the shell. 7) Await the quiescent state from the shell. 8) Await the outgoing request queue to be emptied and the return of all outstanding credits (plus the sending of any potential credits back to NIs ). 9) Clear the slot table reservation. 10) Close the conﬁguration request channel to NIm . 11) Open a conﬁguration request channel to NIs . 12) Await all outstanding credits. 13) Clear the slot table reservation. 14) Close the conﬁguration request channel to NIs . Algorithm IV.4 assumes independent transactions, where completion of a transaction does not depend on any other transactions with other IPs [13]. If such dependencies do exist, they must be addressed by the user of the ART library, as the change then is on a granularity larger than a single connection. The library provides functions for manipulation of individual channels or even individual registers to facilitate such use. Note that in NoCs that do not employ end-to-end ﬂow control, quiescence in the router network has to be implemented by e.g. inserting a special tagged message as an end-of-stream marker [21]. F. Guaranteed-service conﬁguration When NoC conﬁguration is done via BE channels, no timing guarantees can be given on the operations performed. The beneﬁt is that no resource reservation has to be made for the conﬁguration connections, that are both seldom used, and have very low bandwidth requirements. The drawback is the uncertainty in when the conﬁguration operations are executed, and potentially long delays in case of congestion. Although resource reservations for conﬁguration connections might initially seem wasteful, it is, however, possible to reuse the same resources (TDM slots) for all the conﬁguration connections, wherever two or more conﬁguration channels share a common link. This is due to the fact that we use the conﬁguration request and response channels in a mutually exclusive manner, where one is always passivated before another one is used. Hence, it sufﬁces to reserve a single time slot per link for all the conﬁguration connections. In a network with n TDM slots this bounds the reserved resources to 1 n of the total ingress/egress bandwidth. The actual use of these time slots is multiplexed by the conﬁguration master at run time. V. L IBRARY IM P L EM EN TAT ION The API shown in Listing V.1 constitutes the foundation of the ART library for manipulation of the NoC conﬁguration. The art conﬁg init is performed as a part of the bootstrap procedure, before any calls to the other functions. It instantiates the conﬁguration response channels, from every NI back to NIc . Listing V.1 Top-level conﬁguration operations. /* Initialise the config response channels. */ void art_config_init(); /* Open, modify and close a connection. */ void art_open_conn(const conn_t* const conn); void art_modify_conn(const conn_t* const old_conn, const conn_t* const new_conn); void art_close_conn(const conn_t* const conn); The last three operations all work on the granularity of a connection, represented by a data structure carrying all the information required to conﬁgure the NIs (and optionally also the routers). The C implementation of this structure is shown in Listing V.2. Listing V.2 Connection structure. typedef struct { /* Location of master and slave */ ni_t master_ni; unsigned char master_queue_id; ni_t slave_ni; unsigned char slave_queue_id; /* Quality-of-Service, GS/BE */ service_t qos; /* Settings for the master NI */ path_t master_slave_path; unsigned char master_space; unsigned char master_data_limit; unsigned char master_credit_limit; slot_t master_slot_list; unsigned char master_channel_id; unsigned int narrowcast_equal; unsigned int narrowcast_mask; /* Settings for the slave NI */ path_t slave_master_path; unsigned char slave_space; unsigned char slave_data_limit; unsigned char slave_credit_limit; slot_t slave_slot_list; } conn_t; The top-level operations in turn call a number of internal functions for encoding of the struct ﬁelds into 32-bit words, sc module initConﬁgConnections openConnection modifyConnection closeConnection l a c o L Remote SRAM conﬁg init open conn modify conn close conn main ARM Remote l a c o L (a) Ideal processor. (b) ARM7 SystemC model. Fig. 4. Conﬁguration master instantiations. e.g. art encode path, opening and closing of conﬁguration connections, art open conﬁg conn and art close conﬁg conn, or reading/writing speciﬁc registers, such as art set slots and art get transaction. The ﬁnal outcome is calls to DTL read and write operations, which is the only processor-speciﬁc part of the library. This allows for easy porting to other platforms [31]. The library functions can also be used together with a power management library as it implements the functionality to close and modify connections. Together with hardware support, e.g. clamping of the network link signals, this affords safe power down of parts of the NoC. Note though that modiﬁcations of the network topology may necessitate modiﬁcations of the BE routing function to preserve full connectivity. In such a situation it is imperative to address also reconﬁgurationinduced deadlock [15]. V I . R E SU LT S In our experiments, we use two different conﬁgurationmaster implementations. First, a SystemC module where computation is done without progressing the simulation time, i.e. inﬁnitely fast. Only the read and write operations contribute to the conﬁguration time. This option, depicted in Figure 4(a) is referred to as the ideal processor. Second, the implementation shown in Figure 4(b), comprising a SWARM SystemC model of an ARM7 [32], clocked at 100 MHz. The core has a von Neumann architecture, and a bus within the tile determines whether the read and write transactions go to the local memory, through the Local port to the corresponding Conﬁg port of NIc , or via the Remote port to a remote NI. To assess the impact of the processor execution time, we compile two different binaries for the ARM: one with the connection structures and the ART library, as described in Section V, and one where the conﬁgurations are pre-compiled into plain read and write calls, thus removing all computation and minimising the amount of function calls. Both binaries are compiled using arm-elf-gcc version 3.4.3 with the compiler options -mcpu=arm7 -Os. Unless indicated otherwise, conﬁguration is done using BE connections. The conﬁguration-related trafﬁc thus contends with connections that are already open. The ﬁrst measure we study is the time required to initialise the conﬁguration connections and how this grows with the number of NIs. Figure 5(a) shows the effect of increasing the mesh size from 1×4 to 5×4, constantly having two NIs per router. It is clear that the NoC is not the limiting factor, as the ARM is consistently more than 8 times slower than the ideal processor for the read/write implementation, and 70 times slower using the library. All three implementations show a linear growth, as expected with a constant work per NI. No user-speciﬁed trafﬁc is yet occupying the NoC. Figure 5(b) shows the effect on the cumulative setup time when varying the number of connections on a ﬁxed 4×4 mesh with two NIs per router. The setup time is measured from the point when initialisation of all the conﬁguration response channels is completed. The impact of the latter is shown in Figure 5(a) (the scenario with 32 NIs). The user-speciﬁed connections have a total bandwidth of 6.2 Gbyte/s, uniformly spread across the NoC. With an average burst size of 8 words and a ﬂit size of 3 words this roughly amounts to 20% of the total ingress/egress bandwidth. All connections belong to the GS class of trafﬁc and are thus subjected to non-workconserving arbitration. Also in this experiment the ARM is roughly 10 times slower than the ideal processor when using read/write calls, and 60 times slower with the library implementation. The time required grows linearly with the number of connections, only showing some minor ﬂuctuations. The worst case setup times for a single connection are 0.46, 3.04 and 16.72 µs for the three implementations. Again, we conclude that the computation time is far greater than the time spent on communication and we can see that the contention from already opened connections is hardly noticeable. We also evaluate the possibility to exploit locality when multiple channels share the same source NI and there are no ordering constraints between the channels, e.g. when opening connections. In contrast to Algorithm IV.2, we iterate over the NIs rather than over the connections when opening a set of channels. Then, for every NI, the conﬁguration request channel only has to be opened and closed once. Applying this strategy, the cumulative time still grows linearly with the number of connections. However, for the ideal processor, the time required to open all connections is consistently less than half or what is achieved in Figure 5(b). Similarly for the ARM implementations, the total setup time is roughly 40% less using this technique. Next, we assess the memory requirements of the ARM binary. To reduce the memory footprint, we also specify the bit widths of the different ﬁelds in the connection structure. Note though, that bit members generally worsen the execution time as many compilers generate inefﬁcient code for reading and writing them. The binary size for a varying number of NIs is shown in Figure 6(a). The various NoC instantiations correspond to the same mesh networks as in Figure 5(a), here together with a 40 connection use-case. Expanding the library functions to read and write calls roughly doubles the size. This is to be compared with the ten-fold speedup observed in Figure 5(a). Figure 6(b) shows the corresponding scaling with the number of connections. Here, the difference between the library and the read/write implementation becomes more obvious as the number of connections grows. As an example, the expansion to read/write calls increases the size with 170% Ideal processor ARM read/write ARM library Number of NIs T i m e ( µ ) s 40 32 24 16 8 350 300 250 200 150 100 50 0 (a) Initialisation time for different number of NIs. Ideal processor ARM read/write ARM library Number of connections T i m e ( µ ) s 150 140 130 120 110 100 90 80 70 60 50 40 30 20 10 2500 2000 1500 1000 500 0 (b) Setup time for different number of connections. Fig. 5. Execution time. ARM read/write ARM library ARM library, bit ﬁelds Number of NIs B i a n r y s i e z ( y b t ) s e 40 32 24 16 8 20000 15000 10000 5000 0 (a) Binary size for different number of NIs. ARM read/write ARM library ARM library, bit ﬁelds Number of connections B i a n r y s i e z ( y b t ) s e 150 140 130 120 110 100 90 80 70 60 50 40 30 20 10 50000 40000 30000 20000 10000 0 (b) Binary size for different number of connections. Fig. 6. Binary size. for the 150 connection case. We conclude that the effect of bit members is only a few percent reduction of the binary size. Moreover, as expected, we observe an execution time that is slightly worsened, although the measured increase is a mere 1.2%. Taking both these facts into account it is hardly justiﬁable to employ bit members unless memory footprint is absolutely critical. Table I summarises the analytical approximations of the graphs in Figure 5 and Figure 6. Note that the table does not show the constant term. It is clear from our experiments that the quantitative differences between the ARM library and read/write implementation are both in performance, where the library is roughly a factor six slower, and size with only half the binary size. Qualitatively, the library offers a ﬂexibility and reusability that is impossible to achieve without it. Taking these facts into account, we propose to pre-compute the read/write calls TABLE I ANA LY T ICA L A P PROX IMAT ION S O F TH E L IN EAR T ERM IN F IGUR E 5 AND 6 Ideal processor ARM read/write ARM library 95 784 6385 246 2506 13594 647 2305 9575 149 67 262 104 Initialisation time / NI (ns) Setup time / conn. (ns) Tear-down time / conn. (ns) Binary size / NI (bytes) Binary size / conn. (bytes) for any use-case that is ﬁxed and do not require any run-time ﬂexibility. The conﬁguration channels themselves constitute a good example of such a use-case. Here, we can enjoy a six-fold speed up without sacriﬁcing any ﬂexibility. Similarly, for applications that frequently change between a set of predeﬁned operation points, e.g. due to QoS levels, it is possible to pre-compute the instruction sequences and thereby speedup the transitions. While the effect of contention on connection instantiation time is hardly noticeable in Figure 5(b), Figure 7 shows a far more adverse scenario. A network similar to the one shown in Figure 3, with only one router between the NIs is conﬁgured with varying load on the links between NIm , and NIs . The raw link bandwidth is 2000 MB/s. A total of 40 IPs are active in the setup, communicating over 20 connections, of which 10 are BE. The TDM table has a size of 11, leaving one slot unreserved. Note though that the work-conserving BE scheduling also may use unoccupied GS slots. ) s µ ( e m i T Ideal processor ARM read/write ARM library 25 20 15 10 5 0 6.95 1.84 0.39 600 800 1000 1200 1400 1600 1800 2000 Offered load (MB/s) Fig. 7. Connection setup time with varying link contention. When conﬁguration is done with GS connections, the end result is a conﬁguration time of 0.39, 1.84 and 6.95 µs for the three different implementations. As seen in Figure 7 this corresponds to the BE conﬁguration time when link contention is low. We can thus conclude that the non-work-conserving nature of the TDM scheduling only marginally increases the time required. Furthermore, it is clear from the experiment that the offered load must be high (more than 60%) before there is any noticeable effect on the conﬁguration time. Moreover, the impact varies greatly for the three implementations. The ideal processor is slowed down 8 times by the contention whereas the ARM implementations are in the order of 2.5 times slower. The latter are less affected by the increased communication time as it constitutes a smaller part of the total execution time. Note that using GS connections for conﬁguration enables us to give timing bounds on use-case transitions, if only the IP modules are guaranteed to react in a bounded time. When opening connections this is not an issue, as the IPs per construction are in an idle state. However, as we shall see, closing time is largely dependant on the IP reaction time. Similar to Figure 5(b), Figure 8 shows the cumulative execution time required to tear down a use-case with a varying number of active connections. In contrast to an open operation, a tear down also involves waiting for in-ﬂight transactions to ﬁnish. Here, we use an ideal model of the IP that is able to stop after every single transaction. Typically, reconﬁguration points occur much more infrequently [21]. Even with the ideal model, the IP modules play an important part in determining the time required to carry out a tear down operation. This also causes a large variation across the different connections. For the ARM the fastest tear-down operation is completed in a little more than 1 µs while the library implementation requires 4.8 µs. The ideal processor does the corresponding action in 12 ns. As the IPs are involved it is not trivial (or even possible) to bound the time required to tear down a connection. The measured worst case, however, is 12 µs for all three methods. Ideal processor ARM read/write ARM library ) s µ ( e m i T 2000 1500 1000 500 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 Number of connections Fig. 8. Tear-down time. Figure 9 shows the distribution of setup and tear-down times for the different connections, using the ideal processor and BE connections for conﬁguration. As expected, the setup times are less varying, with a narrow distribution around 250 ns. The only source of variation is due to scheduling and contention in the network. The tear-down times are far more varied with a long tail to the right. To study the impact of the remote polling used in the close operations we also implement a polling engine, similar to the Synchronisation-Operation Buffer in [30]. This minuscule hardware module is added just before the Conﬁg port of the NIs and transforms the remote busy-wait to a local busywait. Instead of reading the remote register and then check for a certain value, the desired value is ﬁrst programmed in the polling engine, whereafter a subsequent read only returns once this value is read. This addition reduces the average teardown time with 8%. More important, it reduces the amount of remote reads with 65%, but introduces one extra write for each connection as the desired value must be set. The total amount of remote transactions and data trafﬁc is reduced with 30%. V I I . CONCLU S ION AND FU TURE WORK In this paper we present the facilities for controlling change in a reconﬁgurable NoC. To mitigate the complexity in programming the NoC, we advocate an abstraction in the form of a run-time conﬁguration library. Such a library must leave the modiﬁed system in a consistent state, from which normal operation can continue. Setup Tear down Tear down with polling engine y c n e u q e r F 100 80 60 40 20 0 0 1 0 0 1 0 1 2 0 1 3 0 1 4 0 1 5 0 1 6 0 1 7 0 1 8 0 1 9 0 1 2 0 0 3 0 0 4 0 0 5 0 0 6 0 0 7 0 0 8 0 0 9 0 0 > 1 0 0 0 1 0 0 0 Time (µs) Fig. 9. Setup and tear-down time distribution We introduce a library for NoC reconﬁguration with a basic set of functions, through which we can connect the IP ports in a graph structure and dynamically add, remove or modify the interconnections. We show the architectural additions and the many trade-offs in the design of the run-time library and evaluate the performance, memory requirements, predictability and reusability for the various options. From our experiments we can conclude that the NoC conﬁguration has implications on the practical management policies of multiple use-cases. Even with hardware polling engines, the tail in tear-down times might cause severe predictability problems when applications are deactivated and freed resources have to be reallocated to new applications. In some cases the problem can be avoided by allocating mutually exclusive resources to the various applications, but in general this problem must be addressed on the application level. Furthermore, as the network is not the performance bottleneck, adding more ports and hence more parallel conﬁguration connections is of little or no use. However, a centralised approach might soon reach serious scalability limitations. The time required to reconﬁgure the network is already substantial and this does not include the process of computing a new conﬁguration and the reconﬁguration of tasks running on the IP modules. A ﬁrst step is to distribute the work to multiple conﬁguration masters. Further decentralisation is possible by employing the distributed programming model. In our future work, we plan to explore the coupling with a run-time mapper. "
Architecture of the Scalable Communications Core.,"The scalable communications core (SCC) is a power- and area-efficient solution for physical layer (PHY) and lower MAC processing of concurrent multiple wireless protocols. Our architecture consists of coarse-grained, heterogeneous, programmable accelerators connected via a packet-based 3-ary 2-cube network on chip (NoC). The combination of the accelerators, which were developed for key communications operations, and the NoC results in an architecture that is flexible for multiple protocols, extensible for future standards and scalable to support multiple simultaneous streams","Architecture of the Scalable Communications Core  Jeff Hoffman  jeffrey.d.hoffman@intel.com  David Arditti Ilitzky  david.arditti@intel.com  Anthony Chun  Aliaksei Chapyzhenka  anthony.l.chun@intel.com aliaksei.chapyzhenka@intel.com Intel Corporation; Santa Clara, California  Abstract  The Scalable Communications Core (SCC) is a  power- and area-efficient solution for physical layer  (PHY) and lower MAC processing of concurrent  multiple wireless protocols.  Our architecture consists  of coarse-grained, heterogeneous, programmable  accelerators connected via a packet-based 3-ary 2cube Network on Chip (NoC).  The combination of the  accelerators, which were developed  for  key  communications operations, and the NoC results in an  architecture that is flexible for multiple protocols,  extensible for future standards and scalable to support  multiple simultaneous streams.   1.  Introduction  This paper provides an overview of  the  architecture of the Scalable Communications Core  (SCC), which is a research effort into designing a  reconfigurable radio baseband that is capable of  processing several wireless standards with a common  set of hardware.  Our goal is to develop and demonstrate a  multiprotocol wireless baseband that is energy and  area efficient.    Applications range from laptop  computers, handheld communicators and Ultra Mobile  Personal Computers  that connect seamlessly  to  available wireless networks such as WiFi, WiMax or  cellular infrastructure, while also supporting digital  television or GPS.  The SCC architecture consists of  flexible  processing elements or accelerators that are based  upon computational kernels that are common in the  receiver or transmitter baseband pipelines of many  protocols.   While there are many functions that are common  across protocols, there may also be differences in the  sequence of functions.  In addition, computational  resources may  be  shared  across  protocols  simultaneously to reduce hardware area.  In order to  meet both of these requirements, a flexible data  routing scheme is necessary.  We defined a Network  on Chip (NoC) [15] that enables flexible routing of  data between the computational resources.  This paper is organized as follows: Section 2  summarizes  the  protocol  and  architecture  requirements, respectively.  Section 3 describes the  overall architecture and Section 4 summarizes the  processing elements.  Sections 5, 6 and 7 describe the  control plane, data plane and the routing algorithms.   Section 8 discusses an example of a protocol mapped  to the architecture and Section 9 shows the area  efficiency that is obtained by the SCC architecture  when compared to fixed ASIC solutions.  2. Requirements  2.1 Protocol Requirements  With wireless protocols being continuously  developed and one to two new protocols introduced  every year, it is estimated that by 2007 a radio in a  laptop computer will need to support 15 different  protocols [9]. Two problems arise: 1) time to market  of compatible products is very short, and 2) increased  processing power is required to support new, higher  speed protocols.  In a wireless radio, the baseband component of a  PHY (physical layer) is defined as processing between  the Radio Frequency Integrated Circuits (RFIC) and  the MAC (Media Access Control) and encompasses  demodulation and modulation, error correction  encoding and decoding and  interleaving and  deinterleaving.  In different wireless standards the  algorithms that are required for these functions may  differ; if fixed function hardware is used, a separate  hardware implementation may be required for each  standard.  Another problem with fixed hardware  solutions is that the introduction of a new standard                may require the development of a new hardware  implementation, which increases cost and time-tomarket.  SCC was developed to address this problem of  efficiently supporting numerous current and future  wireless protocols with  a  common  set of  reconfigurable baseband hardware.  SCC can be  configured to support different protocols  with area  and power approaching that of comparable dedicated  hardware components [9].   An integral part of the development of the SCC  architecture was an analysis of the computational  requirements of a number of communications  protocols [2].  The wireless protocols that are currently  targeted for SCC are WiFi (IEEE 802.11a, g, n)  [5][6][7], WiMax (IEEE 802.16e) [4] and Digital TV  (DVB-H) [3].  In addition, we have a simultaneous operation  (“SimOp”) requirement that is defined as follows: a)  process Digital TV, b) simultaneously process either  WiFi or WiMax and c) support handoffs between  WiFi and WiMax. This supports a scenario where the  user is connected to the Internet via WiFi or WiMax  networks and watching Digital TV simultaneously.  2.2 System Design Constraints   Based upon the protocol requirements, we can  derive a set of NoC architecture constraints as follows:   2)  1) While many network configurations are  possible (stored configurations) only a few  are in use at any given time (active set).   In contrast to the recently announced NoC in  [14] , data flow in SCC is very specific to  communications applications:  because the  MACs and RFICs are on opposite ends of the  NoC, data flows vertically (Tx down; Rx up).   3) The NoC configuration  is mainly static  during the course of a protocol frame, but  changes as the protocol changes states from  header to payload, Tx to Rx, and from  protocol to protocol.   4) Because multiple simultaneous protocols are  required, multiple NoC configurations are  active concurrently.  5) The NoC packet size  is variable and  dependent on protocol, rate, and stage of the  PHY pipeline.  3. Overall Architecture  The SCC system architecture, shown in Figure 1,  consists of a 3-ary 2-cube (3x3 2D mesh) interconnect  and a heterogeneous set of coarse-grained, highlyoptimized baseband Processing Elements (PEs). This  NoC architecture was selected to fulfill the system  flexibility and scalability constraints mentioned  earlier.   SCC interfaces to separate RFIC via a Digital  Front End (DFE).  Currently, we are assuming that a  different RFIC is required for each standard.  SCC also has network interfaces to protocolspecific MACs.  We are assuming that a separate  MAC is required for each standard.  While much of  recent SDR  research has  employed reconfigurable logic [16] , the resulting cost  in area and power far exceeds the constraints of  mobile wireless client platforms.  Instead, we defined  the PEs after analyzing PHYs of several wireless  protocols and determining a common set of kernels  [2].  These processing elements or accelerators can be  configured to achieve the required flexibility using  corresponding programming technology that we are  developing to enable developers to program, debug  and validate new protocols.  The PEs are connected in a low latency 3-ary 2cube that provides flexible data routing and enables  the architecture to scale for future requirements.  Each  PE injects packets into the network through a router’s  local/client port via an asynchronous data interface  that allows the router and PE to operate at different  clocks.   This architecture retains a high degree of  parallelism and  low energy characteristics of a  traditional ASIC PHY pipeline while providing  complete flexibility in routing data between highly  configurable processing elements.  In addition, the  computational resources are time multiplexed to  support multiple streams and  resource context  switching  is data driven. Finally, memory  is  distributed and local to the PEs.  Scalability is a key attribute of the architecture.   The number and types of PEs that are instantiated in a  particular SCC implementation is dependent upon the  expected workload.  For example, the designer can  instantiate the appropriate mix of PEs that is necessary  to meet the computational requirements of the worst  case protocol scenario and attach them via the NoC.  For non-worst case scenarios, the unused PEs are  clock gated to reduce power consumption.        Table 1 summarizes the processing elements.   Unless otherwise  indicated,  latency and peak  throughput are from simulation of 130Mbps 2x2  802.11n RX with a 233MHz clock.  Area and power  are from preliminary synthesis on 90nm CMOS at  0.9V not based on layout.  4.1 Digital Front End (DFE)  The DFE is the interface between baseband and  AFE.  On the Rx side, the DFE resamples and filters  input samples from the ADC and mitigates receiver  distortions such as DC offset and IQ imbalance.  On  the Tx side, the DFE interpolates and filters samples  that are sent to the DAC.  4.2 Data Stream Processing Element (DPE)   The DPE performs computationally intensive  operations such as the FFT that is used in OFDM  protocols.  The core structure consists of control and  computational units and several memory blocks.   The DPE data path is a set of computational  elements connected via flexible interconnect matrices.  Asynchronous data path  swap units  support  commutations from any of four inputs to any of four  outputs.  Reconfiguration of the data-path can be done  on-the-fly with  interconnection  information and  operation parameters stored in the configuration cache.  The DPE  is  a  flexible, extensible and  synthesizable core in which the system designer can  vary the set of elements, interconnection options and  control  features  to optimize cost, power and  performance.   4.3 CRC, Scrambling, Convolutional  Coding (CCPE)  The CCPE provides the CRC, scrambling, and  convolutional encoding functions required by WiFi,  WiMAX, and DTV standards and future protocols  such as 3GPP LTE.  Features of this architecture  include radix-16 CRC with configurable polynomial  up  to 32nd degree;  radix-16 scrambling with  configurable polynomial and seed up to 15th degree;  radix-256 convolutional coding with configurable  polynomials and constraint length and support for  multiple simultaneous streams.  4.4  Interleaving (ILVPE)  The ILVPE performs permutation algorithms such  as  interleaving, puncturing,  and multiplexing.   Programmable address generators and a switch matrix  provide arbitrary permutation at a rate of eight  symbols per  lock.  Interleaving buffers are  Figure 1. SCC architecture consists of  heterogeneous PEs connected via NoC.  4. Processing Elements  This section describes the current set of SCC PEs.   Each PE holds multiple function configurations and  provides context switching for multiple streams.   Function and context selection are based on control  information embedded in the packet headers.  PEs can  be mixed and matched by the developer at compile  time in order to meet the application requirements.  Table 1. Classification of processing elements  Lat  µs  Thpt  Msps  0.62 40  43.0 111 10.77 TBD 3.12 160 2.34 TBD Area  Pwr  mm2  mW  3.60 TBD Label Algorithm  Classification  DFE AGC, resample,  filter, detect  DPE1 Small FFT, chnl.phase-freq. est.,  combine, QAM  map  DPE2 Large FFT, etc.  (WiMax)  Puncture,  interleave  HSV High speed Viterbi  0.55 466 TBD TBD LPV  Low power Viterbi  4.21 58.2 0.75 9.7  (DVB)  Turbo decode  (WiMax)  RSD RS decode (DVB) 35.09 76.1 1.12 6.0  RSE RS encode  6.18 123 0.31 3.7  (WiMax)  CCPE CRC, scramble,  conv. code  0.71 932 5.60 TBD 8.24 117 1.32 TBD ILV  TD  0.30 932 0.92 5.4      dynamically allocated  from a pool of shared  memories.  Micro-code uses run length compression  and looping to reduce memory size.  4.5 Viterbi Decoding (VDPE)  The VDPE provides Viterbi decoding  for  convolutional codes with support for any polynomial  up to 64 states and code rates from 1/2 to 7/8.  There  are currently two flavors of this PE: a low power 62.5  Mbps radix-2 implementation and a high throughput  600 Mbps radix-4 implementation that will support  802.11n requirements.  4.6 Turbo Decoder (TDPE)  The TDPE performs decoding of Convolutional  Turbo Codes (CTC) that are required for 802.16e [4].   This decoder is flexible and will also support the CTC  requirements for 3GPP LTE.  4.7 Reed-Solomon Encoding (RSEPE) and  Decoding (RSDPE)  The RSEPE and RSDPE perform Reed-Solomon  encoding and decoding for a multitude of wireless and  broadband protocols, including support for extended,  shortened, and punctured codewords.  Configurable  parameters include correction capacity (T), codeword  size (N),  information bytes (K), Galois field GF(2m),  primitive polynomial p(x) and starting and ending  exponents of alpha (α) used  in  the generator  polynomial g(x).  4.8 ARC Processor  SCC incorporates one ARC1 605 RISC processor  [1]  that is used for overall management of the device.    5. Control Plane  The control plane that configures and controls the  PEs consists of two components: 1) a bus based on the  Open Core Protocol (OCP) [8] enables the ARC to  manage the PEs and 2) mailboxes enable the exchange  of primitives between the ARC and the MACs.  The  control plane is used to perform very light control and  status functions.  Data driven control within the data  packets is used to specify the operations that each PE  performs.  6. Data Plane  The SCC solution for an efficient and flexible  data plane is a regular 3-ary 2-cube directed network  that combines a high efficiency and low overhead  ASIC streaming data flow with packetization and  routing. This interconnect was selected because it is  more power efficient than bus or crossbar structures  and scales easily  to an  increasing number of  processing elements with lower latency than crossbar  structures.    The 3-ary 2-cube network topology resembles that  of a cylinder.  Every PE client is connected into the  network through a node’s client port.  Generally, data  flows from one end of the cylinder to the opposite  end.  Resource sharing occurs in the horizontal  direction.  Intra-cylinder data flow may side step  around PEs depending on the protocol.  Other k-ary ncube (KNC) topologies such as rectangular or toroidal  are also possible.  While data may be routed from any source PE to  any sink PE, our layout keeps routing length to a  minimum: PE pairs with a high traffic volume were  placed adjacent to each other in the mesh.  6.1 Packet Hierarchy  We define two packet sizes: (1) logical packets  are sized to accommodate natural data blocks such as  an OFDM symbol, interleaver block, or RS codeword  and (2) physical packets are sized for low latency and  are the atomic routing unit.  Physical packets are  typically small fragments of a logical packet.   In communications applications, logical packets  could be very large.  For example, the DVB-H  standard requires an 8k point FFT.  At 16-bit complex  precision, this logical packet is 262144 bits.  At a  transfer rate of 6.4 Gbits/s, a route is blocked for 40.96  µs.  This is an unacceptable latency for an 802.11a  stream that is sharing the same interconnect segment  and/or PE.  Therefore, we divide the 262144 bit  logical packet into 512x512 bit physical packets.  The  worst case latency due to contention for interconnect  or PE resources is now reduced to an acceptable 80ns.  Due to resource sharing, physical packets may  arrive at a PE interleaved by stream.  Each PE includes  a buffer to reassemble the physical packets into logical  packets, where the buffer size is PE-dependent.  Once  a complete logical packet has been reassembled, we  issue it to the processing function within the PE.  Different functions may have different natural  data size requirements.  For instance, since WiMax  has multiple OFDM symbol sizes and RS codeword  sizes, we provide configurable parameters for each  1 ARC is a trademark of ARC International.                                                                                   output stream supported by a PE that control the  packet fragmentation process.  In addition, the logical packet size may change  during the course of frame reception.  For example,  the last RS codeword in a stream may be shortened.   Therefore, we include a logical packet size field in the  logical packet header that controls the reassembly  process.  Many protocols, such as WiFi and WiMax, have  precise transmission timing requirements.  Due to  resource sharing, some amount of jitter through the  PHY pipeline is expected.  We have included a time  stamp field in the logical packet header in order to  schedule the transmission of logical packets and on  receive, to synchronize our reference clock frequency  with that of the transmitter.  6.2 Data Driven Control  The packet header contains the address of the  destination PE, Function ID that designates the  operations  to be performed on  the packet and  additional control information.   The Function ID enables flexibility and context  switching to support simultaneous operation so that  centralized control is not required for most of the data  processing.  7. Router Description  7.1 Router Requirements  The router requirements, which are summarized  in Table 2, were dictated by the throughput necessary  for supporting simultaneous operation as well as  meeting the latency requirements for each of the target  protocols.   7.2 Router Design  The router is the fundamental building block of  the NoC interconnect and was designed to meet two  basic goals: 1) provide a reliable communication  media by presenting a  low Deadline Missing  Probability (fulfill the high throughput and latency  system constraints) and 2) reduce power consumption  in order to fulfill power system constraints.  Table 2. Summary of router requirements.  Requirement  Minimum channel bandwidth to  support worst case (802.11n DPE to  ILVPE stream)  Minimum aggregate NoC Bandwidth  to support worst case (802.11n and  DVB concurrently)  Latency 802.11n  SIFS  PHY budget  NoC budget  Latency 802.16e  frame length  PHY budget  NoC budget   Value  1152 Mbits/s   4.612 Gbits/s   16µs  < 6µs  < 0.6µs  5ms  < 500µs  < 50µs  The router is shown in Figure 2. It implements  completely flexible data routing between processing.     This module provides the following features:  • Four I/O ports including data and credit lines.  • One local port for PE packet injection and  ejection with data and credit lines.  • Arbitrated 5x5 crossbar packet routing.  • Up to five simultaneous crossbar connections.  • Full duplex pair-wise data flow per port.  • Two-way point-to-point flow-control per port.  l e b u o e s r D f f t B u u p n I t t t s u e p u g e O i s r R Figure 2. Block diagram of the router.  7.3 Router Area and Power  The nine routers were synthesized in a 90 nm  CMOS process.  The area is estimated to be 0.8 mm2  and the power is 18 mW when clocked at 250 MHz.  7.4 Flow Control (FC)  The FC protocol determines how router resources  (buffers, state and channel bandwidth) are allocated  and how packet collisions over resources are resolved                                                      [10]. In the SCC, the physical transfer digit (phit) is  the same size as the flow control digit (flit) or 32 bits  and so FC is performed on every phit.    Currently the flow control is performed in a pointto-point (P2P) basis on every router port separately.  We have implemented a wormhole FC with a Stall/Go  backpressure mechanism: one request line is used for  Forward Flow Control (FFC) and one stall/go line is  used for Reverse Flow Control (RFC). No End-to-End   flow control was implemented; the backpressure  mechanism which propagates  the sink’s buffer  availability upstream through the routers up to the  traffic source stops flit injection into the network. The  flits injected in the meantime generate compressions  along  the worm (packet  trajectory  through  the  network) thus requiring extra buffers at each router  port to avoid flit loss.   Wormhole flow control was selected to fulfill the  NoC power constraints because it reduces buffer size  in contrast to cut-through or packet-buffer flow  control, while it allocates channel bandwidth more  efficiently than bufferless flow control which is  susceptible to packet drop or packet deflection.   7.5 Routing Algorithm (RA)  Routing algorithms can be divided in three  general groups according to how they select the path  from source to destination [11]: 1) Deterministic, 2)  Partially adaptive and 3) Fully adaptive algorithms.  Fully adaptive algorithms are usually paired with  Virtual Channels (VC), which result in an increase in  the total buffer count and power consumption; because  of our power constraints, no fully adaptive algorithms  or VC where considered for the SCC NoC.  All the algorithms that were simulated for the  SCC router are minimal [10] in order to maximize  bandwidth utilization. The fault detection and recovery  problem is not solved at the NoC level (as in  nonminimal routing) and is delegated to upper layers.  While all the algorithms that were implemented are  deadlock-free in a mesh topology, none is guaranteed  to be free of one dimensional deadlock (wraparound  ring deadlock) for an arbitrary KNC. However, the  SCC architecture is guaranteed to be free of deadlock  because of its  topology (3 nodes per dimension) and  that all routing algorithms are minimal.  Several RAs were simulated in order to evaluate  how much adaptive algorithms improve the network  performance for a small scale network such as the  SCC NoC. The following are the algorithms for the  SCC router that we examined:   • Dimension Order XY minimal routing -  Deterministic  • Dimension Order YX minimal routing -  Deterministic  • Odd Even minimal routing [12] - Partially  Adaptive  7.6  Input Selection Policy  The input selection policy arbitrates between  contending inputs ports requesting the same output  [13]. Our primary concern is to reduce packet blocking  in order to manage a low average latency. Given a  dimensional order nonadaptive routing algorithm,  turns in packet trajectory tend to create more packet  blocking. In order to avoid turns in packet trajectory  and due to its low implementation complexity the  implemented  input  selection  policy  for  the  nonadaptive routing model is a No-Turn policy (select  input channel directly across from output channel)  with prioritized clockwise turns to resolve ties.   For the partially adaptive routing algorithms that  we simulated, different input selection policies were  analyzed in order to guarantee resource allocation  fairness and reduce starvation in order to meet the  constrained latency requirements. These selection  policies were: 1) Local First Come First Served  (LFCFS) which gives preference to incoming packets  that have waited (while being blocked) for more  packets to complete transmission at the current node  and 2) Global First Come First Served (GFCFS) which  gives preference to incoming packets that have been  more time inside the network since their injection  time. These selection policies guarantee a degree of  allocation fairness and reduce starvation at a cost of  higher implementation complexity.  7.7 Output Selection Policy   The output selection policy arbitrates between  possible output ports for an input port only when  routing adaptively. Once again, in order to minimize  turns in the packet trajectory and due to its simplicity  the implemented output selection policy is a No Turn  policy with prioritized clockwise turns to resolve ties.  7.8 Router Performance  The channel capacity in the SCC NoC for a clock  frequency of 250 MHz is 8000 Mbps. The network  capacity for the current configuration of the SCC NoC  (with 15 client ports) is 120 Gbps. The network  capacity is the baseline for all the benchmarking  comparisons.      All benchmarking data points were obtained from  RTL simulation. The performance evaluations where  done for different offered traffic before saturation  point (~30% of network capacity). The network is  defined to be under saturation whenever the offered  traffic is not achieved by all the clients given that all  clients are transmitting simultaneously.   The simulation test bench generates synthetic  traffic at every client port in the NoC concurrently.  Every node selects a destination randomly (with a  uniform distribution) on a logical packet basis and  sends a random size (with a uniform distribution)  logical packet to the selected destination. As the  simulation advances the offered traffic (per client) is  increased periodically after a predefined amount of  logical packets has been completely transmitted. For  every logical packet, the offered traffic is kept as an  upper bound, so that no client port will exceed the  offered traffic in the duration of a logical packet, i.e.  individual physical packets may exceed the offered  traffic if and only if the average traffic for the logical  packet remains beneath the upper bound.   Next, the throughput and latency benchmarking  results are shown in Figure 3 for different physical  packet sizes. These figures show the benchmarking  results obtained by simulating with a fixed maximum  allowable logical packet size, which is kept as an  upper bound for the randomly generated logical packet  sizes. This upper bound size corresponds to the  maximum size of an 802.16e symbol as represented in  the SCC: 40960 bits.   The throughput graph shows the overall mesh  average throughput performance, i.e. the percentage of  achieved offered traffic by all the client ports. The  latency graph shows the absolute maximum latency (in  µs) experienced by a physical packet.   The different router performance metrics taken at  saturation point in the worst maximum packet size  scenario (64 flits or 2048 bits per physical packet) are  summarized in Table 3. Table 3 shows that all  throughput and latency requirements listed at Table 2  were satisfied by the three router implementations.  7.9 Analysis  Based on  these  results, we have selected  Dimension Ordered YX minimal routing because of its  performance before saturation condition.  The Odd Even adaptive routing does not provide  much improvement over the deterministic algorithms’  performance. The reasons for this are 1) the small  dimensions of the current SCC NoC and 2) the  minimal routing constraint because any packet needs  at most one turn to reach its destination, so that  adaptive routing is not fully utilized.    Even though the SCC network dimensions may  increase in the future (which would improve the  performance of adaptive routing for arbitrary uniform  traffic) Odd Even routing algorithms were not selected  because, as stated in Section 6, PE groups with high  communication  requirements between  them are  mapped nearby, in a clustered fashion, thus creating  bounded traffic patterns that will limit the Odd Even  algorithm performance.  8. Protocol Mapping Example  To illustrate how the NoC architecture enables the  flexibility of the SCC architecture, we will describe  the mapping of 802.11n WiFi protocol [7] to the  architecture.  8.1 Layers and Service Access Points (SAP)  The IEEE 802.11 standard [5] [6] follows the  ISO/IEC basic reference model shown in Figure 4.   Control and data primitives are exchanged between the  layers using service access points (SAP).  Control primitives  include PLME-GET and  PLME-SET  for  accessing  the management  information base (MIB), PHY-TXSTART and PHYTXEND for transmit control, PHY-RXSTART and  PHY-RXEND for receive control, and PHY-CCA and  PHY-CCARESET for channel idle-busy monitoring.   These are exchanged between MAC and ARC using  the mailbox.  Table 3. Summary of router performance.  Requirement  Avg channel  bndwdth (Mbps)  Tot aggregate  NoC bndwdth  (Gbps)  Max pckt  latency  (µs)  Odd  Even  2393.5  YX  XY  2392.1  2392.6  35.903  35.881  35.888  0.5106  0.4906  0.4910        100 99 98 97 96 95 94 93 92 91 ) % e g a t n e c r e p ( e c n a m r o f r e P l l a r e v O Ma x i m um L og i c a l Si z e A l low e d : 4 09 6 0 b i t s OddEven, phy s ize 512 bits OddEven, phy s ize 1024 bits OddEven, phy s ize 1536 bits OddEven, phy s ize 2048 bits YX, phy s ize 512 bits YX, phy s ize 1024 bits YX, phy s ize 1536 bits YX, phy s ize 2048 bits XY, phy s ize 512 bits XY, phy s ize 1024 bits XY, phy s ize 1536 bits XY, phy s ize 2048 bits 90 0 500 1000 a)  Overall Throughput -Uniform Traffic Saturati on Poi nt Minimum Throughput Requirement 1500 2000 2500 3000 3500 Of f e r ed Tr af f i c (Mbps /c l i ent channe l ) OddEven, phy s ize 512 bits OddEven, phy s ize 1024 bits OddEven, phy s ize 1536 bits OddEven, phy s ize 2048 bits YX, p hy s ize 512 bits YX, p hy s ize 102 4 bits YX, p hy s ize 153 6 bits YX, p hy s ize 204 8 bits XY, p hy s ize 512 bits XY, p hy s ize 102 4 bits XY, p hy s ize 153 6 bits XY, p hy s ize 204 8 bits Maximum Latency Requ irement 2.0906 2.0906 1.9266 1.7266 1.5266 1.3266 1.1266 0.9266 0.7266 0.5266 0.3266 ) s u ( y c n e t a L 0.1266 0 5 10 b)  Ov erall packet latenc y lim its -Uniform Traffic M ax i m um Log i c a l Si z e A l lowed : 4 0960 b i t s Sat urati on P oi nt 15 20 25 30 35 40 45 Of fe red Tra f f i c (pe rcent ag e o f cap aci t y % ) Figure 3. a) Measured throughput and b) maximum latency for different routing algorithms,  various physical packet lengths, for a logical packet length of 40960 bits (corresponding to the  802.16e protocol). 8.2 Data Driven Processing  An example of the control and data flow between  the PEs for a receiver pipeline is shown in the railroad  diagram in Figure 5.  PHY Service Data Units (PSDU) octets are  transferred over the NoC in packets.  Prefixing each  packet is the header that contains all of the control  information needed to route and process the packet.   This relieves the ARC of most of the low level control.   The ARC is responsible for only high level inter-frame  control as illustrated in Figure 5.   The high  level  inter-frame control functions  include exchange of PLME and PHY-SAP primitives  with  the MAC, management of  the RFIC, PE  configuration adjustments for PSDU length, and power  management periods of inactivity.  Low level intra-frame control is accomplished  using a collection of tags embedded in the header of  each packet.  Primary among these tags is Function ID  (FID) and Stream ID (SID) where FID refers to a  function and SID refers to a context.    These two  references are combined to reference the next packet  header.  In order to constrain latency, the streams are  processed in a time division multiplexed manner.  This  requires saving and restoring context.  The SID  uniquely identifies a signal stream and therefore can be  used as a context reference.  Figure 4. IEEE 802.11 mapping to ISO/IEC  Basic Model (from [5] ).                          shown in Figure 6a, packets enter the NoC from the  WiFi MAC and travel to the CCPE, ILVPE, DPE, DFE  and then exit the NoC to the WiFi RFIC. PEs that are  not used for this protocol are bypassed and clock gated  to reduce power consumption.  For the 802.11n Rx Pipeline, samples from the  RFIC are FFTed, deinterleaved, Viterbi decoded,  descrambled and then passed to the WiFi MAC. As  shown in Figure 6b, samples enter the NoC from the  WiFi RFIC and are packetized in the DFE; packets  pass to the DPE, ILVPE, HSVPE, CCPE and then exit  the NoC to the WiFi MAC.  The NoC is the key to efficiently reusing the PEs  to reduce the overall hardware area. The DPE, ILVPE  and CCPE blocks are used for both the Rx and Tx  pipelines and the NoC enables data to be routed in  different directions for Rx and Tx paths.  In addition,  the CCPE and ILVPE blocks are reused across  protocols, thus leading to hardware savings when  compared to discrete fixed-function solutions.  Figure 5. Example Rx Data and Control  Railroad Diagram.  In order to enable resource sharing between  streams, each PE supports multiple profiles.  The FID  uniquely identifies a function profile.  Each function  profile in turn may reference a micro-code section.    In order to minimize configuration storage, we  allow more than one stream to reference the same  function profile.  Furthermore, we allow more than one  function profile to reference the same micro-code  section.  Finally, we give SID and FID local context in  order to reuse of the number space from one PE to the  next.  8.3 Data Flow  To illustrate how the NoC supports the flexibility  and area efficiency of this architecture, we will show  examples of data flow for WiFi (802.11n). The  802.11n protocol is half-duplex so that the Tx and Rx  times do not overlap. For the 802.11n Tx pipeline,  information bits from the WiFi MAC are scrambled,  convolutionally encoded,  interleaved, mapped  to  symbols, inverse FFTed and then transmitted. As  Figure 6. Data flow for the 802.11n pipeline.  9. Area comparison  As discussed in the example in Section 8, our  NoC-based architecture enables the PEs to be timeshared across multiple protocols.  When compared to  the corresponding combination of multiple fixedfunction discrete ASIC solutions, the SCC architecture  has smaller area and cost.   Synthesis results for a 90 nm CMOS process are  summarized in Figure 7 and show that the area  consumed by the SCC architecture is slightly larger              [3]  ETSI, “Transmission System for Handheld Terminals  (DVB-H),” EN 302 304; June 2004.  [4]  IEEE, “Draft IEEE Standard for Local and  Metropolitan Area Networks; Part 16: Air Interface for  Fixed and Mobile Broadband Wireless Access  Systems,” P802.16e/D12, October 2005.  [5]  IEEE, “Part 11: Wireless LAN Medium Access Control  (MAC) and Physical Layer (PHY) Specifications,”  802.11a-1999.  [6]  IEEE, “Part 11: Wireless LAN Medium Access Control  (MAC) and Physical Layer (PHY) Specifications; HighSpeed Physical Layer in the 2.4 GHz Band,” 802.11b1999.  [7]  IEEE, “TGn Sync Proposal Technical Specification,”  802.11-04/0889r6, May 2005.  [8]  OCP-IP Association, Open Core Protocol Specification  2.1, rev 1.0  [9]  E. Tsui, et al., “Reconfigurable Communications  Architecture for Adaptive Radios,” SoC 2003 Tutorial  on Reconfigurable Computing, 2003.  [10]  W. J. Dally and B. Towles, “Principles and Practices of  Interconnection Networks”, Morgan Kaufman, 2004.  [11]  P. Mohapatra, “Wormhole routing techniques for  directly connected multicomputer systems,” ACM  Computing Surveys, Vol. 30, no. 3, pp. 374–410, Sept.  1998.  [12]  G.M. Chiu, ""The Odd-Even Turn Model for Adaptive  Routing,"" IEEE Transactions on Parallel and  Distributed Systems, Vol. 11,  no. 7,  pp. 729-738,  July   2000.  [13]  C.J. Glass and L.M. Ni, ""The Turn Model for Adaptive  Routing,"" Journal of the Association for Computing  Machinery, Vol. 41, No. 5, pp. 874-902, Sep. 1994.  [14]  S. Vangal, et al, “An 80-Tile 1.28 TLOPS Network-onChip in 65nm CMOS”, ISSCC 2007, Session 5.2.  [15]  T. Bjerregaard and S. Mahadevan, “A Survey of  Research and Practices of Network-on-Chip,” ACM  Computing Surveys, Vol. 38, March 2006.  [16]  H. Fujisawa, et al., “Flexible Signal Processing Platform  Chip for Software Defined Radio with 103 GOPS  Dynamic Reconfigurable Logic Cores,” Asian SolidState Circuits Conference, IEEE, 2006.  than that of a typical ASIC in single protocol  applications.  This extra area can be attributed to the  NoC and the configurability of the PEs.  However,  when a combo WiFi+WiMax+DVB-H baseband is  implemented in SCC architecture, the area savings due  to resource sharing is clear.  The SCC-based multiradio baseband is estimated to be 34% smaller than the  same radio built from three ASICs.  10. Summary  In this paper, we have presented an overview of  the SCC architecture, which is a solution to supporting  multiple wireless protocol PHYs in a reconfigurable  baseband. A key aspect of this architecture is the NoC  interconnect between the processing elements that  supports the goals of flexibility and scalability as well  enabling  the  resource sharing  that makes  the  architecture area-efficient.  We are currently instantiating a silicon prototype  of the architecture and we will report on these results in  the future.  ASIC vs. SCC area  (normalized ) 4 .00 3 .50 3 .00 2 .50 2 .00 1 .50 1 .00 0 .50 0 .00 SCC ASIC 802.11n 802.16e DVB-H 11n+16e+DVB Figure 7. Area comparison between SCC and  corresponding discrete ASICs for single and  multiple protocols.  11. Acknowledgements  We would like to acknowledge numerous past and  present team members who have contributed to the  SCC project.  12. "
Solutions for Real Chip Implementation Issues of NoC and Their Application to Memory-Centric NoC.,"This paper describes real chip implementation issues of network-on-chip (NoC) and their solutions along with series of chip design examples. The solutions described in this paper cover both architectural aspects and circuit level techniques for practical chip implementation of NoC. As for architecture level solutions, topology selection, chip-aware protocol design, and on-chip serialization (OCS) for link area reduction are explained. For circuit level techniques, SERDES and synchronizer design, crossbar switch partial activation, and low-voltage link are presented as the foundations for power and area efficient NoC implementation. Regarding presented solutions for NoC implementation, this paper proposes memory centric NoC (MC-NoC) for homogeneous multi processor SoC (MPSoC). Flexibility and feasibility of task mapping on homogeneous SoC is the key feature of the MC-NoC. 8 dual port SRAMs connected to crossbar switches in hierarchical star topology network facilitate data communication between processors, regardless of task mapping into the MC-NoC. Experimental result obtained by mapping edge detection tasks on the MC-NoC in various configurations shows almost constant performance. This result proves the effectiveness of the proposed architecture. The MC-NoC based SoC is also implemented on TSMC 0.18 um process technology","Solutions for Real Chip Implementation Issues of NoC  and Their Application to Memory-Centric NoC  Donghyun Kim, Kwanho Kim, Joo-Young Kim, Seung-Jin Lee and Hoi-Jun Yoo   Semiconductor System Laboratory, Department of Electronic Engineering and Computer Science  Korea Advanced Institute of Science and Technology (KAIST)  Daejeon, Republic of Korea  donghyun53@eeinfo.kaist.ac.kr  Abstract – This paper describes real chip implementation  issues of Network-on-Chip (NoC) and their solutions along  with series of chip design examples. The solutions described in  this paper cover both architectural aspects and circuit level  techniques for practical chip implementation of NoC. As for  architecture level solutions, topology selection, chip-aware  protocol design, and On-Chip Serialization (OCS) for link area  reduction are explained. For circuit level techniques, SERDES  and synchronizer design, crossbar switch partial activation,  and low-voltage link are presented as the foundations for  power and area efficient NoC implementation.   Regarding presented solutions for NoC implementation, this  paper proposes memory centric NoC  (MC-NoC)  for  homogeneous multi processor SoC (MPSoC). Flexibility and  feasibility of task mapping on homogeneous SoC are the key  feature of the MC-NoC. 8 dual port SRAMs connected to  crossbar switches  in hierarchical star topology network  facilitate data communication between processors, regardless  of task mapping into the MC-NoC.  Experimental result  obtained by mapping edge detection tasks on the MC- NoC in  various configurations shows almost constant performance.  This result proves  the effectiveness of  the proposed  architecture. The MC-NoC based SoC is also implemented on  TSMC 0.18 um process technology.  INTRODUCTION  I.  Networks-on-Chip (NoC) is emerging as promising  technique for future complex SoC which consists of more  than few tens of Intellectual Properties (IPs). Modular  structure of NoC makes chip architecture highly scalable,  and well-controlled electric parameters of the modular block  improve reliability and operation frequency of on-chip  interconnection network. After the NoC design paradigm has  been proposed [1, 2], research area concerning NoC is fairly  matured. There have been many researches on theoretical  aspects of NoC such as topology selection, quality of service  (QoS) guarantee, automated design tool development and  routing scheme design [3-6]. Although these researches  improve knowledge and facilitate high-level design of NoC,  noble  circuit  techniques  essential  for  real  chip  This research was supported by the MIC(Ministry of Information  and Communication), Korea, under  the  ITRC(Information  Technology Research Center) support program supervised by the  IITA(Institute of Information Technology Advancement)"" (IITA2006-(C1090-0603-0012)  implementation haven’t been covered in detail. In addition to  that, importance of the study focusing on the real chip  implementation is emphasized along with multi-processor  SoCs (MP-SoCs) are emerging on market, such as Intel  Quad Processor [7].  In this paper we describe design issues come up with real  chip implementation of NoC and their solutions in the  perspective of architecture and circuit level techniques, based  on the chip design examples of authors’ research group [812]. Architecture level issues of NoC include selection of  optimal NoC  topology and designing communication  protocol regarding hardware complexity. For practical chip  implementation,  these  issues  are discussed mainly  concerning power consumption and silicon area required.  Evaluation on various topologies of NoC shows that  hierarchical star topology is the most efficient in general.  Then, effect of serialization ratio on the interconnection link  and switch fabric area is also discussed. Finally, NoC  protocol which adopts aligned packet format for hardware  simplicity is explained. As for the circuit level techniques of  practical NoC  implementation, high speed SERDES,  synchronizer design, low-voltage swing link, and crossbar  switch partial activation technique are described in later  section.  Based on the experiments and analysis presented in this  paper, we propose memory centric NoC (MC-NoC) for  homogeneous multi processor SoC. The MC-NoC  is  comprised of 8 dual port memories and crossbar switches  configured in hierarchical star topology, providing dynamic  communication channels between processors. In contrast to  conventional 2D mesh array NoC such as RAW  microprocessor [14], the MC-NoC supports flexible mapping  of task regardless of data communication characteristic  between processors. Experimental results show that different  task mapping configurations on the MC-NoC do not present  much difference in overall performance. This result proves  flexibility and feasibility of task mapping on the proposed  MC-NoC.              The rest of the paper is organized as follows. In section 2,  previous works in authors’ research group and other related  works are briefly explained. After that, chip implementation  issues of NoC and their solutions in the perspective of  architecture and circuit level technique are presented in  section 3. Section 4 proposes the MC-NoC regarding the  solutions discussed in section 3. After the architecture and  operation of the proposed MC-NoC is explained in detail,  benefits of applying MC-NoC to the homogeneous multiprocessor SoC are also described. Experimental results  obtained by mapping edge detection task on MC-NoC are  given in section 5. Finally, summary of implementation  results and conclusion are made in section 6 and 7  respectively.  II. RELATED WORKS  As mentioned in the previous section, many researches  on theoretical aspects of NoC have been reported. For highlevel design feasibility of NoC, SUNMAP & Xpipes tools  are recognized as automated tools for topology selection and  synthesis, respectively [3][5]. In the SUNMAP tool, best  topology of NoC is automatically selected for a given  bandwidth constraint. Then, Xpipes generates synthesizable  RTL for the selected NoC topology. Other researches on  high-level design of NoC include AEthereal and DyAD NoC.  In the AEthereal NoC, traffics are divided into two classes –  guaranteed service (GS) and best effort (BE) – and combined  router for both classes of traffic was designed to resolve  quality-of-service (QoS) problem in NoC [4]. The DyAD  NoC was proposed combining  the advantages of  deterministic and adaptive routing to improve performance  and capacity of NoC [6]. Although, only few of the previous  works are introduced here, vast amount of high-level design  and theoretical studies of NoC are being actively performed  by numerous research groups.  On the other hand, researches performed by authors’  group have been focused on circuit level design issues of  NoC regarding real chip implementation. The world first  chip implementation of packet switched NoC was featured  with on-chip serialization (OCS) and strobe signal based  synchronization [8, 12]. The OCS technique which is  enabled by the proposed synchronization scheme reduces  silicon area required for interconnection link and switch  fabric. Circuit techniques for low power NoC were also  proposed along with heterogeneous SoC implementation [9,  11]. In this work, low-voltage signaling, crossbar switch  partial activation, and SILENT coding schemes for serial  link are presented as the circuit techniques for low power  NoC. The most recent chip was designed aiming at high  speed NoC. In this work, wave front train (WAFT) SERDES  and aligned packet format are proposed [10]. The aligned  packet format reduces hardware complexity of network  interface modules while WAFT SERDES implements high  speed serial link for high performance NoC. In addition to  that, crossbar switch with adaptive bandwidth control was  also proposed to lower the operation frequency of NoC  without  loss  in bandwidth  [13]. Beside  the chip  implementations of NoC, analytical studies also have been  performed in authors’ research group. In the analytical work  for low power NoC design, various NoC topologies were  compared in the point of power consumption and silicon area  required [11]. And, impact of serialization ratio and protocol  design on power consumption and chip area was also  discussed to answer the practical chip implementation issues  [12].  In  the next  section, we will discuss chip  implementation issues of NoC and their solutions based on  the experiences obtained from the previous researches.  III. NOC IMPLEMENTATION ISSUES & SOLUTIONS  This section discusses chip implementation issues of  NoC and corresponding solutions in architectural aspect and  circuit level techniques. Architecture level issues of NoC  discussed in this section are topology selection, protocol  design and serialization scheme design. The circuit level  issues include high speed SERDES, synchronizer design,  crossbar partial activation technique and low-voltage swing  link.  A. Topology Selection  One of the challenging issues in the NoC design is  choosing the best topology to meet the bandwidth and  latency requirements for the target application with the  lowest power and area cost. In these prior works, the  candidate pool of topologies was limited to the regular and  homogeneous topologies like a mesh, torus, cube, tree or  multistage network. However, in the heterogeneous SoCs  like embedded systems, the communication flows are  certainly localized, not uniformly distributed. In this case, it  is highly possible that the optimal topology can be a  heterogeneous and hierarchical topology rather than a  homogeneous and flat topology. In this section, such  hierarchical and heterogeneous network topologies are  investigated briefly in practical terms of energy consumption  and area costs.  Topologies are categorized into two groups in this  analysis: flat topologies such as a bus, star, mesh, and pointto-point and hierarchical topologies, for example local-bus  global-star, local-star global-mesh and local-star global-star.  The hierarchical topologies consist of a local and global  network topology where the local and global network can  have any type among the basic topologies. We comparatively  analyze the four flat topologies and the three hierarchical  topologies.  We assume that the size of each processing element (PE)  is uniform as 1mm x 1mm and the PEs are placed as a square  matrix regardless of the topology. The number of PEs, N,  scales from 16 to 100. The hierarchical topology is assumed  to be divided  into N of clusters and each cluster  contains N of PEs. There are two kinds of traffic patterns;  one is uniform random traffic and the other is localized  traffic with a locality factor. The locality factor means a ratio  of the intra-cluster traffic to the overall traffic. In a hetero-        Figure 1.  (a) Energy consumption according to the number of PEs and (b) traffic locality factor and (c) network area according to the number of PEs  geneous system, the locality factor can represent the  localized traffic pattern quantitatively.  We use an average packet traversal energy Epkt as a  network energy efficiency metric which can be estimated by  the equation shown in Fig. 1, summing up the energies on  switching hops, links and a final destination buffer [1, 15].  The area cost of a network can be derived by the equation in  Fig. 1, summing up the area of switches and links. Those  energy terms and physical area of a queuing buffer, an  arbiter, and a switch fabric are measured from the circuit  implementation in 0.18 μm technology [9].  Fig. 1(a), (b) shows the comparison of the energy  consumption under various traffic conditions. In any traffic  condition, the point-to-point topologies show the best energy  efficiency. If the point-to-point topologies cannot be adopted  due to its infeasibility, the performance of star topologies is  the best among the others. If N is fixed to 36 (See Fig. 1(b)),  for instance, the flat star is the best for less localized traffic  while the hierarchical star (L-star G-star) is the best for more  localized traffic. The mesh always consumes 30~80% more  energy than the hierarchical star does. Fig. 1(c) shows the  area comparison of all of the topologies. The hierarchical bus  topology shows the lowest area cost. However, local-bus  global-star/mesh and local-star global-star/mesh topologies  also occupies as little area as the hierarchical bus does. This  is because the area of total network strongly depends on local  networks rather than a global network. If we compare the  area of a network with that of PEs, the H-star network  consumes 20% of the PE-area but mesh consumes 50%.  According to our analysis, as the traffic gets localized,  the energy cost of the mesh does not scale down as much as  other hierarchical topologies do. Moreover, the area cost of  the mesh is usually three times larger than that of other  hierarchical topologies. As a result, the hierarchical star  (local-star global-star) topology is the most cost-efficient and  scalable topology for the heterogeneous systems where the  traffic is localized. The energy cost is the lowest among  hierarchical topologies and the area cost is also comparable  with the hierarchical bus.  B. Packet Format and Protocol  In this sub-section, we discuss packet format and  protocol issues for a real implementation of NoC. Because a  NoC’s data link, network layers should be implemented with  hardware, a definition of packet format has to consider the  physical channel structure. Typically, independently defined  packet, flit and phit (physical digit) formats shown in Fig.  2(a) are used to support the concept of a layered architecture.  However, such variable length packet formats cause a packet  processing burden for a NoC. Misalignment among packet,  flit, and phit formats makes packet parsing operations  difficult.   Figure 2.  Packet format definitions: (a) typical and (b) aligned        For an efficient hardware implementation, we propose an  aligned packet format which is defined in relation to the  physical layer structure, shown in Fig. 2(b). This packet  format defines a fixed-length packet and dedicated link wires  are assigned to each packet field. Advantages of this scheme  over the typical non-aligned packet format are as follows:  The packet parsing process is very simple, and bit-width of a  field can be easily increased with additional link wires. A  disadvantage of this scheme is that its link utilization is  inefficient if some fields are disabled. Although the nonaligned packet format can utilize the channel resource  efficiently, the packet format has a complex packet parsing  procedure and inflexible bit-width adjustment. As a result,  the aligned packet format provides helps for efficient  hardware implementation for NoCs.  C. On-chip Serialization  On-chip serialization (OCS) technique reduces the link  width. Therefore the area and energy-consumption of the  switch as well as the energy consumption of the link are also  reduced. Fig. 3 illustrates the concept of the OCS. A  serialization and de-serialization (SERDES) circuit are  inserted at I/O of a processing unit (PU) reducing the bitwidth of the I/O thus the switch size. The reduced crossbar  switch size results in decrease of coupling capacitance of  wires in the switch fabric, also contributes to the reduction of  switch energy consumption. In the case of link, as the  number of wires is reduced, wire space can be widen, which  results in reduction of wire capacitance load or energy  consumption. Because of the decreased bit-width, however,  driver size must be increased to come up with increased  operation frequency. In addition to that, the OCS increases  switching activity factor because it breaks the correlation  between consecutive transfer units which are often observed  in the address field of memory access.   Fig. 4 shows the two conventional serializer circuits. The  shift-register type serializer loads the parallel data through  the 2:1 MUXs. After the load operation, the shift mechanism  of the series flip-flops (F/Fs) realizes high-speed serialization.  The maximum clock frequency is given by   f MAX = ,  1 T T T MUX HOLD SETUP + + where the TMUX is the 2:1 MUX delay time, and the  TSETUP and THOLD are the setup and hold time of the F/F. The  conventional architecture has two problems. First, maximum  clock frequency is limited by the delay time of the D-FF.  Second, the high-speed clock for the serialization becomes  system overhead.   Figure 4.  (a) Shift-register type and (b) MUX-tree type serializers  A new SERDES architecture, wave-front train (WAFT),  is proposed to overcome these limitations [10]. The WAFT  uses physical delay constant of delay elements (DEs) as a  timing reference instead of clock, and utilizes signal  propagation phenomenon instead of the shifting mechanism.  Fig. 5 shows a 4:1 WATF serializer circuit. When EN is  low, D<3:0> is waiting at QS<3:0>. The VDD input of  MUXP, which is called a pilot signal, is also loaded to QP.  The GND input of MUXO discharges the serial output  (SOUT) while the serializer is disabled. If EN is asserted,  QS<3:0> and the pilot signal start to propagate through the  serial link wire. Each signal forms a wave-front of the SOUT  signal, and the timing distance between the wave-fronts is  the DE and MUX delay which we call a unit delay. The  series of wave-fronts propagates to the de-serializer like a  train. When the SOUT signal arrives at the de-serializer, it  propagates through the de-serializer until the pilot signal  arrives at the end of the de-serializer, or STOP node. As long  as the unit delay times of the sender and the receiver are the  same, D<3:0> arrives at its exact position when the pilot  signal arrives at the STOP node. When the STOP signal is  asserted, the MUXs feed back its output to its input, so that  the output value is latched.  Figure 3.  The concept of the on-chip serialization.  Figure 5.  Wave-Front-Train Serdes           D. Synchronization  In multiple-clock-domain systems with heterogeneous  PEs, the global synchronization among clock domains is  getting challenging. One NoC contribution to such a SoC  design is to remove the burden of global synchronization by  using mesochronous communication, which implies that the  network blocks share the same clock but the clock phase  may different from each other due to asymmetric clock tree  design.  One of simple and feasible ways  to  implement  mesochronous communication  is  source  synchronous  technique in which strobe signal is transmitted together with  data through a sideband link. Without global synchronization,  the strobe signal is transmitted along with phits as a timing  reference at a receiver. In order to suppress unnecessary  power consumption, the strobe is activated only when a phit  is valid. A receiving part latches the phit using the strobe  signal, and synchronizes the phit with the local clock using a  first-in-first-out (FIFO) synchronizer.   A design challenge in the source synchronous scheme is  that the delay time for distributing the strobe signal over  latches of the FIFO is significant, so that the strobe skew can  possibly result in failure of latching the phit data. In order to  solve this problem, matched-delay FIFO architecture is  proposed [16]. Fig. 6 shows the matched-delay scheme in a  receiver FIFO. The receiver FIFO has 12 20b flip-flops  (F/Fs) and each 20b F/F is enabled by a clock signal  generated by the ring counter. In the matched-delay  architecture, the strobe is generated by the CLKNET directly  so that the strobe signal leads the phit signals by the amount  of the BUFB delay time. As a result, both the strobe and phit  have the same total delay time as  Strobe delay ≈ Phit delay ≈ tPD + tBUFA + tCQ + tBUFB  where the tPD and the tCQ mean propagation time on the  long wire and clock-to-Q delay time of a F/F, respectively.  This architecture can be effectively applied to source  synchronous scheme because it provides the matched delay  regardless of the number of F/Fs, and operation frequency.  Figure 6.  Matched-delay architecture in a receiver  E. Low-power techniques  1) Low-voltage Signaling  Global interconnect lengths can extend up to several  millimeters in the case of a large SoC. Global links consume  considerably more power compared to local links due to their  large parasitic capacitance. Low-swing signaling can  improve  the power efficiency of such  long wires  significantly [11]. A low-swing interconnection is composed  of a low-swing driver at the transmitting end and a sense  amp at the receiving end. The driver uses VSWING instead of  VDD to drive the differential pair. Generally, lowering  VSWING decreases the power required to drive a long wire.  However, VSWING cannot be lowered indefinitely due to  super-linearly increasing power consumption at the receiver  side to restore the low-swing signal to its full-swing value.  Instead, an optimum voltage swing level exists at which the   energy consumed at the transmitting and receiving sides is   minimized [17]. Careful simulation incorporating parasitic  values extracted from layout can be conducted to determine  this optimum voltage swing.  Fig. 7 shows a low-swing signaling scheme that has been  implemented and tested on actual silicon [9]. The driving  circuit is made using n-MOSFETs for both pull-up and pulldown transistors to take advantage of the lower linear  resistance n-MOSFETs provide at small drain-source voltage  compared to p-MOSFETs. A simple clocked sense amp is  used with a three-stage inverter chain in the receiving circuit.  The sense amp is designed to amplify differential input  signals having as low as 200-mV swing to 1.6-V CMOS  logic levels with small delay. A clock restoration circuit is  used in conjunction with differential strobe signals to provide  clocking for the sense amp. The circuit was designed with  the transmitter and receiver connected by a 5.2mm wire. The  wire was intentionally made in a winding pattern to simulate  global interconnects in real SoCs. The delay was measured  to be 0.9ns with a variation of less than 5%. The optimal  voltage swing at a signaling rate of 1.6 Gb/s was found to be  0.3V. Thanks to the low-swing signaling, the power  dissipation on the global link was reduced to just 1/3 of that  on a conventional repeated link without the area overhead of  using repeaters.  2) Crossbar Partial Activation Technique  A conventional n x n crossbar fabric is composed of n2  crossing junctions that are implemented by NMOS passtransistors. During each transaction, an input driver must  charge or discharge 2n  transistor-junction capacitors  connected to the row-bar (RB) and column-bar (CB) that  have been selected. As a result, the power consumption of  the crossbar fabric becomes significant with a large number  of ports. Unneeded power dissipation can be reduced using  crossbar partial activation technique (CPAT) as illustrated in  Fig. 8 [9]. By partitioning an n x n fabric into 4x4 tiles, the  total activated capacitive loading is reduced by a factor of  n/4. A gated input driver at each tile activates its sub-RB  only when a column in that tile is selected by the scheduler.  Only 4 additional four-input OR-gates are needed at each tile        Figure 7.  Low-signaling scheme and its transceiver circuits  for the implementation of the CPAT. The output path, CB, is  also divided into two sub-CBs to reduce capacitive loading.  The sub-CBs are joined at the output using a 2:1 MUX. For  the 8x8 fabric shown, power savings of 22% is obtained at  90% offered load using the CPAT. The OR-gates and MUXs  added account for only 2% of the overall power consumption.  Applying the CPAT to a 16x16 fabric results in a 43% power  saving.  Figure 8. Proposed crossbar with partial activation  IV. MEMORY CENTRIC NOC  In this section, we propose memory centric NoC (MCNoC) which facilitates flexible and traffic-independent  mapping of task on homogeneous MP-SoC. From the  discussions in section 3, hierarchical star topology network,  and synchronization scheme is adopted for the MC-NoC.  Although, hierarchical star topology is proven to be  advantageous for localized traffics of heterogeneous SoC,  effectiveness of the hierarchical star topology still holds for  MC-NoC which is applied for homogeneous SoC. This is  because most of the traffics in homogeneous SoC are  localized by applying the MC-NoC. The detailed description  will be given later in this section.  1) Architecture & Operation  Fig. 9 (a) shows architecture and operation of the MCNoC. In this figure, MC-NoC is applied for homogeneous  multi-processor SoC which incorporates 10 RISC processors.  Building blocks of the MC-NoC are dual port SRAM,  crossbar switch, network  interface (NI), and channel  controller.  In  the MC-NoC, dual port SRAMs are  dynamically assigned to the subset of the RISC processors  involved in data communication. Then, shared data is  exchanged by accessing assigned dual port SRAM. Crossbar  switches of the MC-NoC provide non-blocking concurrent  interconnections between dual port SRAMs and RISC  processors. Operation frequency of the crossbar switches is  decided to be twice of the other part of the MC-NoC to  reduce overhead of packet switching latency. The NI  performs packet processing and clock synchronization  between crossbar switch and other building blocks of the  MC-NoC. The key building block of the MC-NoC is the  channel controller. The channel controller automatically  manages communication channels between RISC processors  to facilitate mapping of task on homogeneous SoC. Role of  channel controller is described in more detail with the  operation of the MC-NoC.   Fig. 9 (b) briefly represents important steps of MC-NoC  operation. In this figure, crossbar switches are not drawn for  simplicity of the description. While the operation is  explained, we will assume that RISC processor 0 wants to  pass the processed results into RISC processor 2 and 3. The  MC-NoC operation is initiated by RISC processor 0 sending  Open Channel request to the channel controller. The  information about source and destination RISC processors is         Figure 9. (a) Architecture of the MC-NoC, (b) Overview of the MC-NoC operation   also included in the Open Channel request. After that,  channel controller assigns one dual port SRAM as a data  communication channel if any of the SRAMs is available.  By updating routing look up tables (LUT) in NIs of  corresponding processors, SRAM assignment is completed.  In this way, assigned SRAM is made to be accessible only  for the RISC processors involved in data communication. At  the end of data transfer through the dual port SRAM, source  RISC processors send Close Channel request to the channel  controller. Then, channel controller invalidates updated  LUTs after checking completion of data transfer. In the  proposed MC-NoC, each processor is able to send multiple  Open Channel request as required. If all the SRAMs are used  by other processors the data transfer should be stalled until  one of the SRAM becomes available. In the MC-NoC  Open/Close Channel request and LUT update are performed  by sending special packets which is not visible to any  processors or memories. By controlling operations of the  MC-NoC using special packets, it has advantage of removing  addition control signal wires.    While data communication is performed through the dual  – port SRAM which is assigned by the channel controller,  progress of data access from destination processors may  differ from each other. To improve programming feasibility  of the multiple RISC processors, the MC-NoC provides data  synchronization scheme to resolve consistency problem  occurred from the different data access order of destination  processors. Fig. 10 (a) shows the case. Processor 2 reads data  from address 0x0, while processor 3 accesses address 0xC.  Until this moment, processor 0 has written valid data only at  the address 0x0. The next step is shown in Fig. 10 (b). In this  case, only processor 2 gets valid data from the dual port  SRAM and processor 3 receives invalidate signal from valid  check logic inside the dual port SRAM. After that, the NI of  processor 3 holds the processor and retry read after specified  wait cycles as shown in Fig. 10 (c). Once processor 0 writes  valid data at address 0xC, processor 3 also gets valid data  and continues processing. (Fig. 10 (d)) In the MC-NoC  operation, the retry procedure described in Fig. 10 (c) is  transparent to the RISC processors because the NI module of  the MC-NoC automatically manages the procedure. As like  Open/Close Channel request, invalid signal from valid check  logic of the dual port SRAM is also transferred as a special  packet. In our implementation, the valid check logic takes  5% of the dual port SRAM area. This is shown in Fig. 11.   2) Benefits of the MC-NoC  Main advantage of the MC-NoC is its flexibility of task  mapping on homogeneous SoC. In this section the benefits  of the MC-NoC is discussed through the comparison with  conventional 2D mesh topology NoC.   As a task mapping example, edge detection operation is  shown in Fig. 12 (a) [18]. In the figure, rectangular boxes  represent processors performing tasks and solid/dotted lines         A t r o P B t r o P A t r o P B t r o P A t r o P B t r o P A t r o P B t r o P NoC. Because, there is no contention in data flow for the  task mapping of Fig. 12 (b), it will outperform task mapping  of Fig. 12 (c), even though all other conditions are equally  given. The contention of data flow in Fig. 12 (c) is visualized  by the number of arrows in the same locations. The  drawback of the conventional 2D mesh NoC is dependency  of overall SoC performance on mapping of the task. Even  more, finding optimal task mapping may be very difficult for  applications with complex data dependencies. Longer  average hop counts and possibilities of deadlock on the way  of finding badwidth-optimzed task mapping are additional  drawbacks of the 2D mesh NoC.   Converted  Data RGB to HSI Conversion Filtered Data Difference of Gaussian 0 - 1 Difference  of Gaussian 1 - 2 Difference of Gaussian 2 - 3 Gaussian Filtering (sigma 0) Gaussian Filtering (sigma 1) Gaussian Filtering (sigma 2) Gaussian Filtering (sigma 3) (a) Edge Detection Operation Flow Contended Node Figure 10.  Data synchronization scheme of the MC-NoC  DoG 0 - 1 Gaussian Filtering (sigma 0) Gaussian Filtering (sigma 0) RGB to  HSI Conv. 836 um 172 um Figure 11. Layout of the dual port SRAM in the MC-NoC  depict data flow between processors. In this operation, input  image is converted from RGB color space to HSI color space  first. The converted image is processed by Gaussian filters  with varying coefficients (sigma) and subtractions between  filtered results are calculated to detect edges in different  scale. Fig. 12 (b) and (c) shows mapping of edge detection  operation on homogeneous SoC with conventional 2D mesh  Gaussian Filtering (sigma 1) RGB to  HSI Conv. Gaussian Filtering (sigma 3) Gaussian Filtering (sigma 1) Gaussian Filtering (sigma 2) Gaussian Filtering (sigma 3) DoG 1 - 2 Gaussian Filtering (sigma 2) DoG 2 -3 DoG 0 - 1 DoG 1 - 2 DoG 2 -3 (b) Task Mapping on 2D Mesh NoC        w/o Contention  (C) Contended Task Mapping on 2D        Mesh NoC Left Side Gaussian Filtering (sigma 0) RGB to  HSI Conv. Gaussian Filtering (sigma 1) DoG 0 - 1 Dual Port SRAM 0 Dual Port SRAM 1 Dual Port SRAM 2 Dual Port SRAM 3 Dual Port SRAM 0 Gaussian Filtering (sigma 2) Gaussian Filtering (sigma 3) DoG 1-2 DoG 2-3 Right Side (d) Task Mapping on the MC-NoC Figure 12. Mapping of Edge Detction task on conventional 2D mesh NoC  and the MC-NoC                                 Feasibility of task mapping on the MC-NoC can be  shown in Fig. 12 (d). For simplicity, crossbar switches are  not drawn and only portion of the MC-NoC is depicted in  this figure. In the MC-NoC, processors and dual port  SRAMs are interconnected through the crossbar switches  which provide full non-blocking connections. Therefore,  interchanging task mappings just inside the left side or right  side of the MC-NoC does not affect the data flow  characteristic and  resulting overall performance. For  example, interchanging task mapping of difference of  Gaussian (DoG) 0-1 and RGB to HSI conversion in Fig. 12  (d) has no impact on contentions in data flow. This attribute  of the MC-NoC improves flexibility of task mapping on  homogeneous SoC, because the key decision of the task  mapping reduces to whether given task is mapped on the left  side or right side. Dual port SRAM is adopted to remove  performance loss when SRAM accesses are come from both  left and right side of the MC-NoC.   In addition to that, variations on required bandwidth  between co-working tasks are also successfully supported by  the MC-NoC. If large bandwidth is required for some of  tasks, multiple numbers of SRAMs can be dynamically  assigned for the demanding task. For small amount of data  transfer, only one dual port SRAM is assigned. In the point  of traffic characteristic, the MC-NoC improves locality  because most of packet transaction between processors and  memories are confined into single crossbar switch to which  the involved processors and SRAMs are connected.  V. EXPERIMENTAL RESULT  To demonstrate feasibility and flexibility of task mapping  on the MC-NoC, this section briefly reports experimental  results, showing how the overall performance is affected by  different task mappings on the MC-NoC. For comparison,  tasks of edge detection operation are mapped into to the  homogeneous SoC shown in Fig. 9 (a).   The different mapping configurations are depicted in Fig.  13. Although the MC-NoC is drawn as single rectangular  black box for simplicity, the architecture shown in Fig. 9 (a)  is exactly applied for the performance comparison. At first,  RGB to HSI conversion, Gaussian filter operations, and  differences of Gaussian (DoG) calculation tasks are mapped  randomly on the given architecture (Fig. 13 (a)). In the  second, Gaussian filter operations are mapped on the upper  half of the SoC, while DoG calculations are mapped on the  lower half (Fig. 13 (b)).  Similarly, the DoG and Gaussian  filtering tasks are separated into left and right respectively, in  the third task mapping configuration (Fig .13 (b)). The  results of performance comparison are given in table 1. In  performance comparison, verilog HDL description is used  for the MC-NoC and other part of the SoC. Therefore,  performance comparison result derived from the simulation  is cycle-accurate. In table 1, numbers in “Cycle Count”  column means required clock cycles to perform edge  detection for 320 x 240 pixels of image. The number in rightmost column shows cycle count ratio compared to the task  mapping configuration in Fig. 13 (a).  Figure 13. Different Task Mappings of Edge Detection Operation  TABLE I.   PERFORMANCE COMPARIONS OF DIFFERENT TASK  MAPPINGS ON THE MC-NOC  Task  Mapping  (a)  (b)  (c)  Cycle Count  % ratio to mapping (a) 8,202,900  8,086,580  8,021,820  1  0.986  0.978  Figure 14. Chip micrograph of the MC-NoC based SoC            [6]  [5] Davide Bertozzi and Luca Benini, “Xpipes : A Network-on-chip  Architecture for Gigascale Systems-on-Chip” IEEE Circuits and  Systems Magizine, Vol. 4, Issue 5, pp 18-31, 2004.  Jingcao Hu and Radu Marculescu, “DyAD – smart routing for  networks-on-chip”,  IEEE Proceeding of Design Automation  conference, pp.260-263, June 2004.  [7]  Intel web site - http://www.intel.com/quadcoreserver/  [8] Se-Joong Lee, et. al. “An 800MHz Star-Connected On-Chip Network  for Application to Systems on a chip”, IEEE Digest of Internaltional  Solid State Circuits Conference, vol. 1, pp. 468-489, February, 2003.  [9] Kangmin Lee, et. al. “A 51mW 1.6GHz On-Chip Network for Lowpower Heterogeneous SoC Platform”, IEEE Digest of International  Solid State Circuits Conference, vol. 1, pp.152-518, February, 2004  [10] Se-Joong Lee, et. al. “Adaptive Network-on-Chip with Wave-Front  Train Serialization Scheme”, IEEE Digest of Symposium on VLSI  Circuits, pp. 104-107, June, 2005  [11] Kangmin Lee, Se-Joong Lee, and Hoi-Jun Yoo, “Low-Power  Networks-on-Chip  for High-Performance SoC Design”,  IEEE  Transactions on Very Large Scale Integration (VLSI) Systems, vol.  14, no.2, pp.148-160, February 2006.  [12] Se-Joong Lee, Kangmin Lee and Hoi-Jun Yoo, “Analysis and  Implementation of Practical Cost-Effective Network-onChips, IEEE  Design & Test of Computers Magazine (Special Issue for NoC),  Septempber 2005.  [13] Donghyun Kim, Kangmin Lee, Se-joong Lee and Hoi-Jun Yoo, “A  Reconfigurable Crossbar Switch with Adaptvie Bandwidth Control  for Netwokr-on-Chip”, IEEE International Symposium on Circuits  and Systems, Vol.3 pp.2369-2372, May 2005.  [14] Taylor, M. B. et. al., “The Raw Microprocessor : A Computational  Fabric For Software Circuits and General-Purpose Programs”, IEEE  Micro, vol. 22, Issue 2, pp. 25-35, March-April 2002  [15] V. Nolle, et. al., “Operating-system controlled network on chip”,  IEEE Proceedings of Design Automation Conference, pp.256-259,  June 2004.  [16] Se-Joong Lee, et. al. “Packet-Switched On-Chip Interconnection  Network for Systems-on-Chip Applications”, IEEE Transactiosn on  Circuits and Systems II, vol. 52, no. 6, pp. 308-312, June 2005.  [17] C. Svensson. “Optimum voltage swing on on-chip and off-chip  interconnect”, IEEE Jorunal of Solid-State Circuits, vol. 36, no. 7, pp.  1108-1112, July 2001.  [18] David G. Lowe, “Distinctive Image Features from Scale-Invariant  Keypoints”, ACM International Journal of Computer Vision, Vol. 60,  Issue 2, pp. 91-110, 2004  The result in table 1 proves the flexibility of the task  mapping on the proposed MC-NoC. In the MC-NoC based  homogeneous SoC, difference  in overall performance  according to the various task mapping is less than 3%. This  feature of the MC-NoC also facilitates software level  optimization of NoC.  IMPLEMENTATION RESULT  VI.  As mentioned in previous section, the MC-NoC and  other peripheral parts of the SoC shown in Fig. 9 (a) were  designed using verilog HDL. After that verified HDL is  synthesized and placed & routed using TSMC 0.18um  process library. However, the dual port SRAMs of the MCNoC is designed using full custom design methodology to  reduce area overhead of valid check logic. The chip  micrograph of the MC-NoC is shown in Fig. 14. The size of  chip is 7.7mm x 5mm and the operation frequency of the  MC-NoC is designed to be 400MHz while other part of the  SoC operates at 200MHz clock frequency. In the MC-NoC  implementation, strobe signal based clock synchronizers,  such as introduced in section 3, are also integrated into the  chip for clock synchronization between crossbar switch and  other part of the SoC.  VII. CONCLUSION  In this paper, we introduced real chip implementation  issues of NoC and provided detailed architectural and circuit  level solutions. The presented solutions are based on the  previous works performed by authors’ researches group.  After that, memory centric NoC (MC-NoC) is proposed  regarding architectural and circuit level solutions presented  in section 3. As a result, hierarchical star topology network  and synchronizer design is adopted from the previous design  experiences. The proposed MC-NoC  is feature with  flexibility and feasibility of task mapping on homogeneous  SoC. Throughout researches including the proposed MCNoC, we presented the practical chip implementation of NoC  for both heterogeneous and homogeneous SoCs.  "
"ASC, a SystemC Extension for Modeling Asynchronous Systems, and Its Application to an Asynchronous NoC.","This paper presents ASC, an Asynchronous SystemC library, as an extension of SystemC for modeling asynchronous circuits. ASC includes a set of port and channel primitives offering the same communication primitives as the common languages used for asynchronous circuits modeling (CHP, Tangram or Balsa). ASC also offers operators and statements in order to accurately model arbiters, which are the basic components of asynchronous network on chips. The aim of this work is to provide to the designers the means of modeling and verifying asynchronous circuits as well as GALS and NoC systems. Synthesis of ASC models with the help of the TAST framework is under development. As an illustrative example, the modeling of an asynchronous network-on-chip architecture using the ASC library is described. This NoC has been successfully integrated into a complex GALS NoC architecture taking advantage of a multi-level SystemC based verification environment","ASC, a SystemC extension for Modeling  Asynchronous Systems, and its application to an  Asynchronous NoC Cedric Koch-Hofer(1), Marc Renaudin(1), Yvain Thonnart(2), Pascal Vivet(2)  (1) TIMA laboratory, 46 Av. Félix Vialet, 38031 Grenoble – FRANCE  (2) CEA-LETI, 17 rue des Martyrs, 38054 Grenoble – FRANCE  {cedric.koch-hofer, marc.renaudin}@imag.fr; {yvain.thonnart, pascal.vivet}@cea.fr Abstract—This paper presents ASC, an Asynchronous  SystemC library, as an extension of SystemC for modeling  asynchronous circuits. ASC includes a set of port and channel  primitives offering the same communication primitives as the  common languages used for asynchronous circuits modeling  (CHP, Tangram or Balsa). ASC also offers operators and  statements in order to accurately model arbiters, which are the  basic components of Asynchronous Network on Chips. The aim  of this work is to provide to the designers the means of modeling  and verifying asynchronous circuits as well as GALS and NoC  systems. Synthesis of ASC models with the help of the TAST  framework is under development. As an illustrative example, the  modeling of an asynchronous Network-on-Chip architecture  using the ASC library is described. This NoC has been  successfully integrated into a complex GALS NoC architecture  taking advantage of a multi-level SystemC based verification  environment.  I. INTRODUCTION  With circuit size reaching billion transistors, traditional busbased single-clock architectures become unusable. In order to  leverage this problem new communication structures called  “Network on Chip” (NOC) has appeared [1]. However, the  clock management of the NoC and the integration of cores  with different clock frequencies are pretty challenging. Using  fully asynchronous circuits for implementing the NoCs allow  handling these problems properly [2]. Moreover asynchronous  NoCs take advantage of the benefits of asynchronous circuits  such as low power consumption, communication robustness…  An asynchronous circuit [3] does not use a global periodic  signal to synchronize the transfer of data between its different  blocks. The synchronization of a data transfer between two  blocks of an asynchronous circuit is done via a local handshaking protocol. To hide the details of these hand-shaking  protocols, most of the modeling languages of asynchronous  circuits are based on CSP  [4]. Examples of such modeling  languages are: CHP [5], Balsa [6] and Tangram [7]. The main  problems of these languages are that they are not supported by  standard CAD tools and not adequate to model syn chronous  circuits. It is therefore a big issue for the design of  Asynchronous Network on Chip  [8] (NoC) interconnecting the  synchronous components of a Globally Asyn chronous Locally  Synchronous [9] (GALS) system. Previous works using  standard CAD tools for the design of synchronous and  asynchronous circuits are presented in sub-section I.A.  SystemC [10] is an open source C++ library supported by  most of the standard CAD tools. The aim of SystemC is to  model complex systems, like System on Chips (SoCs), at  different levels of abstraction. A SystemC model is made of  several processes,  running  concurrently, which  are  communicating via abstract interfaces. As presented in subsection I.B, the modeling facilities provided by SystemC do  not allow to properly modeling asynchronous logic, and hence  Asynchronous Network on Chips. However, one of the  strength of SystemC is to be easily extended. In subsection I.C we present the previous works carried out on  SystemC  for modeling Asynchronous Logic.  In subsection I.D we introduce how we leverage the problem of  modeling Asynchronous NoC with SystemC.  A. Related Works  A design flow using standard commercial HDL synthesis  tools is presented in [11]. This design flow is based on the  subclass of asynchronous circuits called Null Convention  Logic (NCL) [12]. This design flow takes as input a VHDL  description coded with an asynchronous functional level using  the 3NCL type (a single-rail multi-valued representation of  NCL). The proposed NCL approach allows mixing  synchronous and asynchronous design, which is adapted to  model GALS architecture. However, NCL VHDL guidelines  can not properly model channel handshaking, neither  asynchronous arbiters.  Another approach using standard HDL tools is proposed  in [13]. This framework relies on a verilog package modeling  asynchronous communication channels. With the help of this  package, the models can be simulated using standard verilog  simulators, and synthesized using the ACK framework. This  synthesis flow takes as input a Verilog description and  separately generates the datapath and the control part. By this  way, it is also possible to mix synchronous and asynchronous  design. But  this method only  targets bundled-data  asynchronous circuits which require a careful and difficult  timing analysis. We aim to be able to model more robust  design style, namely Quasi-Delay-Insensitive asynchronous  logic  [5] (QDI).  A design framework allowing mixing asynchronous and  synchronous circuits  is presented  in [14]. This design  framework is based on a tool which generates functional    VHDL from an extension of the CHP language. The  asynchronous part of the circuit has thus to be described using  the CHP language, which is not familiar to synchronous  designers.  B. SystemC Limitations  Every block of an asynchronous circuit  is running  concurrently. The communications between asynchronous  blocks are synchronized via a local hand-shake protocol. Most  of the standard SystemC channels use the request-update  scheme [10] to synchronize their output values. By this way,  the communications are implicitly synchronized on a unique  delta-cycle of the SystemC simulator. The concurrency  between the processes using these channels is, therefore, more  restricted than in a real asynchronous circuit. Thus, the  functional behavior of the model of an asynchronous circuit  using this kind of channels can not be properly checked.  To illustrate this, a fork of an asynchronous circuit can not  be properly modeled with a channel using request-update.  Figure 1 illustrates how a fork can be modeled with the help of  the channel sc_signal and the ports sc_in and sc_out. In this  example the process prod::process sends in parallel a data to  the two processes cons1::process and cons2::process. By this  way, the data sent on the sc_signal are available on its two  connected input ports at the next delta cycle. However, in an  asynchronous circuit no assumption can be made on the  availability date of the input ports, and certainly not such a  synchrony. Thus, a correct model of an asynchronous fork  corresponds to two independent parallel communicating  actions. The proposed solution to this problem is presented in  sub-section II.D.  Figure 1: Fork with sc_signal  For checking the Delay Insensitivity of the model of an  asynchronous circuit, all the valid scheduling of the processes  should be tested. For example if we model a fork with two  parallel communications c1 and c2, we should test for any  interleaving of communications c1 and c2. Hopefully, the  specification of the SystemC scheduler is non-deterministic.  However, the system has to be simulated with a particular  implementation of the scheduler. A promising solution for this  problem is presented in [15]. It presents a method and tools  enabling to efficiently generating the different scheduling  allowed by the scheduler specification.  An asynchronous circuit synchronizes its blocks only by  using its communicating channels. Thus, any protocol used by  the communicating channels of an asynchronous circuit has to,  at least, respect the following two properties: to be blocking  and to be memory-less. For example, if a block has to first  write into a block A and then access to a block B. The end of  the communication with A is a precondition for accessing B.  However, if the communicating channels do not respect one or  the two previous properties, some extra synchronization  mechanisms have to be added to guarantee the sequencing of  A and B. Thus, channels like the sc_signal which are nonblocking or sc_fifo which are not memory-less (they have at  least one place) are not suited to model asynchronous  channels. A solution to this problem is presented in subsection II.C.  C. Related Works on SystemC  A SystemC channel library for modeling asynchronous  circuits is defined in [16], but it does not take into account  some key  issues of asynchronous circuits  like nondeterministic choice and delay insensitivity. Moreover, this  library was only tailored for modeling and is not adapted for  synthesis.  A multi-level modeling approach using SystemC and  Transaction Level Modeling is presented in [17]. The standard  SystemC sc_fifo primitives are used to model asynchronous  channels, but with the limitations mentioned above: the sc_fifo  are not memory-less and it is not possible to model active  input/passive output channels, also called “pull” channels.  D. Asynchronous SystemC (ASC)  In this paper, the proposed ASC library is a set of C++  classes on top of the SystemC library. This library is intended  to be used for properly modeling asynchronous circuits and  particularly asynchronous NoCs of GALS systems. This  library enables us to model any class of asynchronous circuits  (QDI, micro-pipelines [3]…) and it is also intended to be used  for synthesis as an input of the TAST framework [18]. Some  modeling guidelines are required in order to enable the  synthesis process. ASC models can be validated using the  tracing facilities of the ASC library. The format of the trace  files generated by these tracing facilities respect a model of  time adapted to asynchronous circuits. Details about these  tracing facilities and model of time are not given in this paper.  The ASC library is fully compatible with the SystemC  standard defined in [10] and do not need external tools, except  a C++ compiler.  To demonstrate the use of the ASC library, this paper  presents how the library is exploited for modeling the  switching elements of a real Asynchronous NOC [17]. The  ASC library enabled us to model and verifies a complete  GALS NoC system  in a multi-level SystemC based  verification environment.  The organization of the paper is as follows. Section II  presents the basic elements of modeling provided by the ASC      library. As a relevant illustrative example, section III describes  how the library is used to model the components of an  Asynchronous NoC. Finally, conclusions and future works on  the ASC library are presented in section IV.  II. BASIC ELEMENTS OF MODELING  As for SystemC, the description of an asynchronous design  is a collection of modules interconnected via ports and  channels. The communication between modules can only be  performed using the predefined ports and channels of the  ASC library.  The example of sub-section  I.B illustrates that a modeling  language of asynchronous circuits has to be able to model  parallel communication. The ASC library defines new  operators and methods supporting parallel communication in  SystemC.  Arbiters [19][20] are one of the most important kind of  components of a NoC. Their functions are to choose between  asynchronous concurrent requests. Sometimes, this choice can  be non-deterministic. For correctly managing this special  case, the set of choice of C++ statements is extended by the  ASC library.  When a choice has to be deterministic, it is useful to check  that the guards of this choice are always mutually exclusive. In  order to check this property during a simulation, a new  statement is added to C++.  A. The Modules  In SystemC a sc_module can contain an arbitrary number of  processes, modules, channels… Thus, this single element is  used for describing the behavior and the structural part of a  system. The ASC library defines two modules allowing to  clearly separate the behavior from the structure. These  modules also define some new methods used for describing  “asynchronous” processes.  The container modules allow defining the system in a  hierarchical way. This kind of modules can contain the  following elements:  • Other modules: container modules or process  modules.  • Channels of the ASC library.  • Ports of the ASC library.  These modules are not allowed to contain any process. A new  container is created by deriving from class as_container.  The process modules are used to define the behavior of the  system and its concurrent aspects. These kinds of modules  contain one process thread and an arbitrary number of ports.  As the container modules, the process modules can also  contain ASC modules and ASC channels. A new process  module is created by deriving from class as_process. This  class is composed of the following public methods:  •  process: this pure virtual method defines the behavior  of the module. It is up to the derived module to  implement this method. The synchronization between  processes can only be done with the communication  primitives provided by the ports of sub-section II.B   and the idle method.  •  idle: this method is overloaded and can take as  parameter, a list of passive ports or a list of dynamic  processes. The behavior of this method is detailed in  sub-section II.B and sub-section II.D.  Moreover, the process module inherits the public methods of  the standard sc_module. For example, the wait method can be  used in the process method of a process module for simulating  its latency.  B. The Ports  The ports are  the communication  interfaces of  the  processes. Most of  their communication primitives are  blocking. They are two different kinds of ports: passive ones  and active ones. The ports are unidirectional (input or output)  and can be connected to at most one other port via one of the  channel of sub-section II.C. It is possible to directly connect a  port to another port if and only if the two ports implement the  same interface and one port belongs to the parent module of  the module having the other port. The port of the parent  module will play the role of a proxy by forwarding the method  call of its connected port.  The communication between two ports of an asynchronous  circuit is done by handshake. The active port initiates the  transaction and the passive port acknowledges it. Thus, a  passive port is able to identify if the connected active port has  triggered a transaction. A passive port defines a method called  probe which checks the status of the connected active ports.  The ASC library defines two passive ports: as_passive_in and  as_passive_out. They can be connected to the active ports:  as_active_out and as_active_in respectively.  In sub-section II.A we have seen that the idle method of an  as_process can take a list of passive ports as parameter. The  process executing this method is suspended if and only if no  transactions have been triggered by one of the active ports  connected to the parameter’s passive ports. A suspended  process is woken up when a new transaction is triggered by  one of the connected active ports. A list of passive ports is  constructed with the overloaded operator “|”.This operator  plays the same role with the idle method and the passive ports  as the one of SystemC with the wait statement and the  sc_event.  The idle method also supports a list of passive ports  constructed with the standard list of the C++ STL library. By  this way it is possible to dynamically manage the input list of  the idle method.  C. The Channels  The channels define the communicating means of a system.  A channel  implements  the communicating protocol of  different  interfaces. The ASC  library defines  the  two  following channels:  •  as_pull: interconnects an as_active_in port with an  as_passive_out port.  as_push: interconnects an as_active_out port with an  as_passive_in port.  The send method of these channels is used by the output ports  for sending data to the input ports. The data sent with this  •    method must define a copy operator because the channel  transfers a copy of them. The receive method is used by the  input ports for receiving the data sent by the output ports with  the previous method. This method returns a reference on the  data transferred by the channel. The nb_probe method is used  by the passive ports for checking the status of the connected  active ports. The as_push and as_pull channel define a first  version of this method which return true if and only if a  transaction is pending. The as_pull channel also defines  another version of the nb_probe which returns the data  transferred by the channel when a transaction is pending.  These two channels use two different handshaking protocols  illustrated in Figure 2.  As we have seen in sub-section I.B, the synchronization  mechanism of an asynchronous channel has not to rely on a  global mechanism like request-update. SystemC  was created  for modeling hardware components of a system as well as  software parts. Like asynchronous circuits the software does  not make any assumption about delays and thus requires the  same kind of facilities. Indeed, the immediate notification of  SystemC allows synchronizing the execution of processes  without using any global mechanism. The handshaking  protocols defined by the send and receive blocking methods of  these channels use only immediate notification. Moreover,  these channels are memory-less. The data are directly  transferred from the input port to the output port. Thus, these  channels  respect  the  fundamental properties of  the  communicating protocol used by the asynchronous circuits.  Figure 2: Handshaking Protocols  By default, a communicating action with ASC takes place  without latency, but at some stages of the design process it is  required to have channels with latency, for example when the  designer has to interface asynchronous components with  synchronous ones, or has to analyze the temporal behavior of  the system. For this purpose, the constructors of the ASC  channels are overloaded. By this way it is possible to define  the latency of the request transition and/or of the acknowledge  transition.  D. Parallel Communication  When modeling a system, it is often required to perform  several communicating actions in the same process in parallel.  For this purpose, the ASC channels define the two following  new communicating primitives: par_send and par_receive.  Each of these methods creates a new thread performing the  corresponding communicating actions. The synchronization of  these sub-processes is done with the “||” operator and the idle  method. The overloaded operator “||” creates a list of dynamic  processes. The idle method suspends the execution of the main  process until the end of its parameter’s dynamic processes.  As for the passive ports, the list of dynamic processes given  as parameter to the idle method can be built with the list of the  STL library.  For modeling a coherent system, the following guidelines  must be respected:  • Always use the par_send/par_receive methods with  the idle method.  • Never use the same port for two or more parallel  communications.  • Be careful when using the same variable as parameter  for two or more parallel receive.  With these new facilities, the fork of the example of subsection I.B can be accurately modeled with the ASC library  with the code of Figure 3 (where out1 and out2 are two  distinct ASC output ports).  void producer::process(void)  {    while(1)      idle(out1.par_send(1) \           || out2.par_send(1));  }  Figure 3: Fork with ASC  E. Non Deterministic Choice  For an asynchronous system, arbitration means arbitration  between truly concurrent events, not only a value-based  arbitration. When two or more concurrent requests occur, the  choice for one of the requests made by these arbiters is not  deterministic, but has to be reliable. For modeling this  behavior, the ASC library defines two new statements:  as_choice_nd and as_guard.  An as_choice_nd defines a non-deterministic choice over  different as_guard. The syntax of these new statements is  defined by the pseudo Extended Backus Normal Form  (EBNF) rules given in Figure 4. As for the traditional C++  switch-case statement, nested as_choice_nd is supported.  _ seq opt def _ st }  opt select _ st :: = as_choice_  (nd guard _ seq  { )  case guard _ seq :: = guard  |  guard _ seq  ,  guard guard :: = as_guard  (  exp  ,  const_exp ) case _ seq :: = case _ st  |  case _ seq case _ st = case _ st :: case const_exp  :  statement _ seq def _ st :: = default   : statement _ seq opt opt Figure 4: EBNF Syntax of Non Deterministic Choice  When more than one guard is true, the non-deterministic  choice uses a random number generator to choose a unique  guarded statement and executes it. Each non-deterministic  choice has  its own random generator. These random                         generators are generated by a factory of random generators.  The random generators produced by this factory are noncorrelated. The seed used by the factory for producing the  random generators can be defined by the designer. By this  way, it is possible to replay a given simulation many times  (required for debug purposes).  Figure 5 describes a model of an asynchronous arbiter. Two  producers are concurrently accessing a shared consumer. An  asynchronous arbiter manages the incoming requests. The idle  statement suspends the arbiter process until one of the  producer triggers a new transaction. This method takes a list of  passive ports built with the “|” operator. The as_choice_nd and  as_guard statements allow the arbiter to perform a nondeterministic choice when the two producers want to access  the shared consumer.  Figure 5: ASC Model of an Asynchronous Arbiter  F. Deterministic Choice  When the guards of a choice are mutually exclusive, the  traditional C++ statements if-then-else if-…-else could be  used. However, this kind of statement does not check that the  guards are mutually exclusive. Indeed, this kind of statement  stops the evaluation of its guards as soon as a true guard is  found. When the designer is manipulating complex guards, it  is useful to check that these guards are always mutually  exclusive. To handle this case, ASC defines a new statement  as_choice_d.  This statement has the same syntax as the as_choice_nd, and  can be nested. However, only one guard of an as_choice_d  must be true. If this condition is not fulfilled, an exception is  raised at runtime.  III. ASYNCHRONOUS NOC MODELING  The proposed ASC library has been fully implemented as  C++ template classes on top of the SystemC 2.1 library. The  proposed modeling strategy has then been successfully applied  in order to model ANOC, an Asynchronous Network-on-Chip  architecture  [17][21].  A. ANOC Architecture  A globally shared bus cannot meet the increasing demands  of System-on-Chip interconnects, mainly because the longwires result in slow signal propagations. Alternatively,  Network-on-Chip  (NoC) architectures  [1] provide high  scalability, high versatility, and high throughput with good  power efficiency. For very large scale integration, NoCs are  fully scalable because the number of nodes can be increased to  provide higher global throughput, without degrading local  network link performances, while the dissipated power is  reduced to active NoC links only.  The ANOC communication architecture is composed of  nodes, links between nodes, and computation resources  (Figure 6). The NoC nodes are the basic switching elements of  the network: they are responsible for handling the wormhole  protocol and arbitrating between any conflicting packets. The  computational resources built the complete system; they can  be either configurable hardware IP blocks (FFT, MPEG…) or  generic blocks (CPU, DSP, memories…).  HW Resource Memory HW Resource node node node HW Resource CPU Memory node node node Memory HW Resource Memory node node node Figure 6: ANOC architecture  The NoC distributed communication architecture  is  perfectly adapted to the Globally Asynchronous Locally  Synchronous (GALS) paradigm. The NoC nodes and links are  implemented using QDI asynchronous logic while the NoC  functional units are implemented with standard synchronous        design methodologies. A dedicated Network  Interface  performs the synchronization between the synchronous and  asynchronous domains [22] and also provides all facilities to  access the communication medium : network routing path  programming, network data packet generation, decoupling  FIFO [23].  B. ANOC Protocol  The elementary piece of information transiting on the links  of ANOC is called a flit (flow control unit), which contains a  32-bit data. In order to minimize the overhead of routing  information in each communication, the ANOC protocol is  based on packet switching: several consecutive flits are  gathered into a single packet, which is transmitted without  interruption from node to node in a wormhole data flow. The  packet protocol requires a specific signaling in order to  identify the first flit and the last flit of a given packet. Two  additional bits are added to the 32-bit data path, coding  respectively the begin of packet (BOP) and the end of packet  (EOP). Using this signaling, a packet can have any length,  from a single flit to any number of flits.  For an efficient handling of the routing strategy within  ANOC, we use static routing of the packets. This avoids  having complex routing tables within the NOC nodes. The  routing information (path to target) is provided in the first flit,  which is called the header flit of the packet. The path to target  is a vector of “dibits” containing the successive directions to  follow within the NOC topology. Each “dibit” encodes one of  the 4 directions: NORTH, EAST, SOUTH, WEST, while the  input direction encodes the final destination (DEST). Each  node actually uses the two lower bits of the path to target  vector to route the whole packet in the given direction, and  shifts the path to target so that it can be used by the successive  nodes. Figure 7 provides a synoptic of ANOC packet  encoding.  BOP BOP 33 33 BOP BOP 33 33 EOP EOP 32 32 EOP EOP 32 32 Header flit payload Header flit payload Path to target Path to target 31                    18 31                    18 17                      0 17                      0 Header flit Body f lit payload Body f lit payload 31                                                              0 31                                                              0 Body flit Figure 7: ANOC packet protocol  C. ANOC Node Architecture  The NoC node is the basic switching element of ANOC. It  handles the packet wormhole dataflow onto its 5 bi-directional  ports (North, East, South, West, and Resource) and is  composed of 5 input and 5 output asynchronous QDI  controllers. The node interconnection scheme (Figure 8)  corresponds to a 4 out of 5 interconnection matrix between the  various input and output controllers.  Packet routing, on the one hand, is handled by the input  controllers, using the two lower bits of the path to target in the  packet header. The asynchronous input controller waits for an  incoming flit and stores it into its internal buffers until the  corresponding output controller is ready to get a new flit.  According to the ANOC protocol, the path-to-target field is  shifted before transmission to the corresponding output  controller.  IN North IN North OUT North OUT North OUT Res OUT Res IN Res IN Res OUT OUT West West IN IN West West IN IN East East OUT OUT East East OUT South IN South OUT South IN South Figure 8: ANOC node architecture  Arbitration, on the other hand, is done in the output  controllers, which may receive new flits from the 4 input  controllers of the other directions. The output controller  arbitrates new packets between the 4 channels, and then routes  all the flits of the selected packet before granting a new  arbitration decision.  D. ANOC Modeling with ASC  The ANOC node architecture has been modeled using the  primitives of the proposed ASC library. The corresponding  ASC model of ANOC node represents 1500 SystemC lines.  The data-flow behavior of the asynchronous input controller  (Figure 9) was described using the ASC deterministic choice  on the begin-of-packet tag included in every flit. When a new  flit is received from the input IN channel, a detection of the  BOP bit is performed. A deterministic choice is made  according to the BOP bit value. If the received flit is a header  flit (BOP=1), the direction of the new packet is obtained from  the path-to-target lower bits, and the path-to-target is shifted  right. Then the flit is sent towards the proper output channel  and a begin-of-packet information is sent to the proper output  controller. Respectively, if the received flit is not a header flit,  the corresponding flit is sent towards the proper output  channel using the current packet direction.  ASC channels enable precise modeling of the asynchronous  behavior of the processes using the send and receive channel  communication primitives. Conventional C++ operators allow  writing simple and compact code, handling the routing using  channel arrays, and shifting of the path-to-target using the  binary right shift operator.  Respectively, modeling of the output controller benefits  from the non-deterministic choice of ASC: one of the main  modeling issue is the proper modeling of asynchronous  arbiters as described in section II.E. The channel probe        operator has been used to model (Figure 10) and validate the  arbitration policy within the NoC node. Such an asynchronous  arbiter can then be implemented in asynchronous QDI  logic [19][20].  class Process_Shift_Route : public as_process  {    // ports    as_passive_in< sc_lv<34> > IN;    as_active_out< sc_lv<34> > OUT[4];    as_active_out< sc_logic >  NEW_PACKET[4];    // variables    sc_lv<34> data;    sc_logic  bop;    sc_lv<2>  current_dir;    // main process    virtual void process(void) {      while(true) {        IN.receive(data);        bop = data[33];        as_choice_d(as_guard(bop==0,no_BOP),                    as_guard(bop==1,BOP)) {          case BOP:            current_dir = data.range(1,0);            data.range(17,0) = data.range(17,0)>>2;            idle( \              OUT[current_dir].par_send(data) || \              NEW_PACKET[current_dir].par_send(1));            break;          case no_BOP:            OUT[current_dir].send(data); break;  }}}};  Figure 9: Routing in the Input Controller  This asynchronous arbiter waits for a new packet to be  received from any of the input controllers. The arbiter  determines the direction from which a new packet will be  routed, and issues a command GET_PACKET containing the  arbitration choice. Finally, according to the GET_PACKET  channel, another asynchronous process finally multiplexes the  flits from the input controllers towards the node output link.  The proposed ANOC node ASC model, even if quite  straightforward, has proven to be efficient and precise with  respect  to  the  implemented Quasi Delay  Insensitive  asynchronous logic of the node.  Compared to a preliminary ANOC model using SystemC  sc_fifo primitives [17], the asynchronous logic modeling using  the ASC library allowed us to have a more precise model:  modeling of both  active/passive  and passive/active  handshaking protocols, modeling of memory-less channels  (the ASC model is more precise regarding its pipelining),  modeling of parallel operator and running a real random  simulation of non-deterministic behavior of such arbiters.  In terms of timing verification, the NOC node ASC model  can also be annotated using wait(delay) statements from lowlevel numbers. Lastly, some new traces facilities and a new  model of time within ASC library allows to generate more  accurate traces (not presented in this paper).  E. ANOC Mixed-Level Validations  In order to address the validation challenges of NoC based  architectures a complete verification methodology based on  SystemC using ASC library was setup in order to perform  mixed level simulations of complete NoC platforms.  class Process_Arbiter : public as_process  {    // ports    as_passive_in< sc_logic >  NEW_PACKET[4];    as_active_out< sc_lv<2> >  GET_PACKET;    // main process    virtual void process(void) {      while(true) {        idle(NEW_PACKET[0] | NEW_PACKET[1]           | NEW_PACKET[2] | NEW_PACKET[3]);        as_choice_nd(          as_guard(NEW_PACKET[0].nb_probe(),FROM_0),          as_guard(NEW_PACKET[1].nb_probe(),FROM_1),          as_guard(NEW_PACKET[2].nb_probe(),FROM_2),          as_guard(NEW_PACKET[3].nb_probe(),FROM_3))        {            case FROM_0:              NEW_PACKET[0].receive();               GET_PACKET.send(0); break;            case FROM_1:              NEW_PACKET[1].receive();               GET_PACKET.send(1); break;            case FROM_2:              NEW_PACKET[2].receive();               GET_PACKET.send(2); break;            case FROM_3:              NEW_PACKET[3].receive();               GET_PACKET.send(3); break;  }}}}; Figure 10: Arbitration in the Output Controller  In Figure 11, the ANOC node can be modeled in SystemC  at two levels: whether at TLM level [24] or at handshake level  using the proposed ASC library, as described in Section III.D.  The TLM level provides an efficient bit-true functional  simulation model used for application validation onto the NoC  architecture. The handshake level model developed with the  ASC library allows validating and debugging the internal  architecture of the asynchronous NoC node, and provides a  more timing accurate SystemC model for system simulations.  The NOC node models have been validated using extensive  random data traffic simulations.  For the NoC synchronous units, they can be modeled either in  SystemC at TLM level for application validation or in VHDL  at RTL level for final synthesis.  NOC  Synchron ous Units SystemC TLM l ev el NOC  Asynchron ous Nodes SystemC TLM level TLM-to-RTL Wrapper TLM-to-ASC Wrapper VHDL RTL l evel SystemC ASC level ANOC  GALS ar chite cture Figure 11: ANOC Mixed-Level Simulations                    In the proposed verification environment, we aim at jointly  validating low level and high level models. For this purpose,  we developed some dedicated simulation wrappers: A TLMto-RTL wrapper to connect a VHDL RTL unit within its TLM  equivalent model, and a TLM-to-ASC wrapper to connect the  ASC NOC node within its TLM equivalent model. The mixed  SystemC/VHDL simulation is then easily handled using  standard CAD tools (Cadence SimvisionTM).  The proposed ASC library and the SystemC language  allowed us to model and verify mixed-level, mixed-language  designs in a unique SystemC framework: test-benches at high  level using SystemC at TLM level, asynchronous NoC nodes  with the ASC library, as well as synchronous logic blocks in  VHDL at RTL level.  IV. CONCLUSION  This paper presented ASC, an Asynchronous SystemC  library for modeling asynchronous circuits and systems. This  library defines some special constructs for accurately  modeling the basic component of asynchronous circuits and  NoCs. The proposed ASC library has been successfully used  to model and verify an asynchronous Network-on-Chip  architecture that has been designed and fabricated. Being able  to model QDI asynchronous logic with the ASC library and  within a SystemC mixed-level environment was a key  advantage to demonstrate the concept of the proposed NOC  GALS architecture [21].  Finally, modeling asynchronous logic with the ASC library  is the first step towards the synthesis of GALS systems. Our  final goal is to be able to synthesize these models with the  TAST framework. We are currently formally defining the  synthesis process of ASC based models to efficiently generate  gate level asynchronous circuits.  V. ACKNOWLEDGEMENT  This work is partially supported by the French government  in the framework of MEDEA+, through the 2A703 NEVA  project (Networks on Chips Design Driven by Video and  Distributed Applications).    [1] A. Jantsch, H. Tenhunen, “Networks on Chip”, Kluwer  Academic Publishers, February 2003.  [2] S.F. Nielsen and J. Sparsø “Analysis of low-power SoC  interconnection networks”, in 19 th Norchip, 2001, pp 77-86.  [3] J. Sparsø and S. Furber, “Principles of Asynchronous Circuit  Design”, Kluwer Academic Publishers, Boston, 2001.  [4] C.A.R. Hoare;  “Communicating Sequential Processes”,  Communications of the ACM 21, 1978, pp 666-677.  [5] A.J. Martin, “Programming in VLSI: From Communicating  Processes to Delay-Insensitive Circuits”, in Developments in  Concurrency and Communication, C. A. R. Hoare, Ed.: UT   Year Programming Series, 1990, pp 1-64.  [6] D. Edwards, A. Bardsley, “Balsa: An Asynchronous Hardware  Synthesis Language”, The Computer Journal vol.45(1), pp12-18.  [7] K. Van Berkel, “Handshake Circuits – An Asynchronous  Architecture for VLSI Programming”, Cambridge University  Press, 1993.  [8] L. Benini and G. de Micheli, “Networks on Chips: A New SoC  paradigm”, in Computer, vol. 35, 2002, pp 70-78.  [9] J. Quartana, L. Fesquet, and M. Renaudin, “Modular  Asynchronous Network-on-Chip: Application to GALS System  Rapid Prototyping”, presented at Very Large Scale Integration  Systems (VLSI-SoC'05), Perth, Australia, 2005  [10] IEEE Std 1666-2005 SystemC Language "
A Hybrid Analog-Digital Routing Network for NoC Dynamic Routing.,"Dynamic routing can substantially enhance the quality of service for multiprocessor communication, and can provide intelligent adaptation of faulty links during run time. Implementing dynamic routing on a network-on-chip (NoC) platform requires a design that provides highly efficient optimal path computation coupled with reduced area and power consumption. In this paper, we present a hybrid analog-digital routing network design that enables efficient dynamic routing on an NoC architecture. The digital part provides accurate real-time traffic estimation using a temporal cost evaluation and adaptation scheme. The analog network, which is distributed within the digital communication network, provides an efficient implementation for the optimal routing algorithm with extremely low power consumption. Our results demonstrate the effectiveness of the hybrid analog-digital design, with a significant improvement in latency over the static routing for random hot spot traffics","A Hybrid Analog-Digital Routing Network for NoC Dynamic Routing Terrence S.T. Mak1 ∗ , Pete Sedcole1 , Peter Y.K. Cheung1 , Wayne Luk2 and K.P. Lam3 1Department of Electrical and Electronic Engineering Imperial College London, London, UK 2Department of Computing, Imperial College London, London, UK 3Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong Abstract Dynamic routing can substantially enhance the quality of service for multiprocessor communication, and can provide intelligent adaptation of faulty links during run time. Implementing dynamic routing on a network-on-chip (NoC) platform requires a design that provides highly efﬁcient optimal path computation coupled with reduced area and power consumption. In this paper, we present a hybrid analog-digital routing network design that enables efﬁcient dynamic routing on an NoC architecture. The digital part provides accurate real-time trafﬁc estimation using a temporal cost evaluation and adaptation scheme. The analog network, which is distributed within the digital communication network, provides an efﬁcient implementation for the optimal routing algorithm with extremely low power consumption. Our results demonstrate the effectiveness of the hybrid analog-digital design, with a signiﬁcant improvement in latency over the static routing for random hot spot trafﬁcs. 1 Introduction Network-on-Chip (NoC) architectures have recently been proposed as a promising solution to the increasingly complicated on-chip communication demands and challenges [2]. Such architectures consist of a network of regular tiles where each tile can be an implementation of general-purpose processors, DSP blocks, memory blocks and embedded reconﬁguration modules etc. Communication among these tile-based modules is following a packetswitch or circuit-switch scheme where messages are transmitted among the processing elements. Typically, there is a router at each tile to provide next-hub forwarding and ∗ Email: t.mak@imperial.ac.uk; T. Mak is gratefully acknowledge supports from the Croucher Foundation. switching, which is similar to that found on computer and data networks. In such a NoC environment, routing of messages (or packets) becomes a critical issue, which determines the inter-processor communication performance. Routing provides a protocol for moving data through the NoC infrastructure and also determine the path of data transport. The selection of communication pathway would greatly affect the latency of packets transmitted from the source to the destination and therefore can have signiﬁcant impact on the overall trafﬁc ﬂow in the network. An intelligent routing mechanism is required to effectively control the trafﬁc ﬂow and minimizing the transportation latency. ) s le c y c  ( y c n e t a L Figure 1. Latency versus network load for different routing schemes. The ﬁgure shows how the employment of dynamic adaptive routing scheme would signiﬁcantly delay the network critical point. (Data adapted from [5] Fig. 5) Dynamic routing (or adaptive routing) has been widely adopted in computer and data network design. Using the on-line communication patterns and real-time information. Dynamic routing can effectively avoid hot spots trafﬁcs or faulty components, and reduce the chances that packets being continuously blocked. It has been shown in [5] that by applying the dynamic adaptive routing strategy in the random trafﬁc network, the network saturation point (or critical point) can be signiﬁcantly delayed. Several adaptive routing algorithms within the context of NoC have been proposed and their performance was reported. An implementation of wormhole adaptive Odd-Even routing was described [9]. In [15], a minimal routing with partially adaptive protocols was investigated. Consensus results were found from these investigation that there is noticeable improvement on the packets latency and trafﬁc load balancing. However, when comparing to its static routing counterpart, implementation of dynamic routers requires signiﬁcantly more resources, dissipates more power and may introduce extra delay to packets in the network. This is due to extra logics are required for implementing the optimal path computation and which is usually computationally expensive. Although it has been shown that dynamic routing can provide a better quality of service in packet switching, it is still calling for a more hardware-economical design methodology for NoC applications. In the last two decades, analog current-mode CMOS circuits have been widely studied and successfully implemented for a wide spectrum of applications from analog signal processing and communication [17] to low-power bio-implantable electronics (or neuromorphic circuits) [14]. The ultra-low power consumptions and small circuit area enable analog circuit design to realize computational intensive algorithms and operating control systems in many time-critical applications. In this paper, we propose a hybrid analog-digital on-chip network capable of implementing dynamic routing decisions. In the network, data transport is in the digital domain. The transport layer includes an evaluation of the latency which is used by a low-power analog control network to make routing decisions. The novel contributions of this paper are as follows: 1) A routing scheme for on-chip networks, which provides efﬁcient evaluation of network congestion and minimal latency packetforwarding at run-time. 2) A high-performance and lowpower hybrid analogue-digital network for on-chip dynamic routing. This paper is organized as follows: In section 2, the background theories are presented. Implementation and the architecture of the hybrid analog-digital routing network are described in section 3. Results are presented in section 4. Section 5 evaluates the results and concludes the paper. 2 Methods Minimal cost (or shortest path) computation is fundamental among different dynamic routing implementation strategies, such as wormhole, store-and-forward and virtual cut-through [4]. The basic idea is that the routing algorithm always chooses the least congested route toward the destination. The least congested route can be found based on the shortest path computation where the path cost is the runtime congestion conditions. Since the trafﬁc intensity and conditions are changing at run-time, the dynamic routing algorithm should be able to discover the congestions and perform shortest path computation at the same time. We propose a temporal cost evaluation and adaptation scheme, which provides an evaluation of congestions at run-time and that the information will be used for shortest path computation and minimal latency packet switching adaptively. The background of shortest path computation and the temporal cost evaluation and adaptation scheme are described below. 2.1 Background (cid:1)k The shortest path problem can be described as follows: Given a directed graph G = (V , A) with N = {V } nodes, M = {A} arcs, and a cost associated with each arc (i, j ) ∈ A, which is denoted as Ci,j . The arc cost can be deﬁned subjected to different applications, and in our case the cost is the delay for a packet traveling from node i to node j . The total cost of path p = (cid:2)vo , v1 , ..., vk (cid:3) is the sum of the costs of its constituent edges: w(p) = shortest path of G from i to j is then deﬁned as any path p with cost w(p) = min{w(p) : i (cid:1) j }. Consider an N -node single-destination shortest path problem, where the network is deﬁned as a set V of nodes {vi , i = 1, ..., N − 1} have arcs {ai } in the set A(vi ) to nodes {vj , j (cid:4)= i} with costs Ci,j and vN = K is the destination node. The Bellman-Ford algorithm [1, 7] deﬁnes a recursive procedure in step k , to ﬁnd the new estimate ˆV (k) (vi ) for the expected shortest path cost from vi to the destination K using the previous estimates in step k − 1, known as dynamic programming. i=1 Ci−1,i . The ˆV (k) (vi ) = min a∈A(vi ) [Ci,j + γ · ˆV (k−1) (vj )] (1) where γ is the discount factor that determines the importance of estimates that is far away from the current node vi . Thus, the estimate of the path cost from the current node vi to the designation node K by taking k steps can be expressed as ˆV (k) (vi ) = Ci,i+1 + γ · Ci+1,i+2 + γ 2 · Ci+2,i+3 + ... + γ k · CK−1,K (2) Thus, Eq. 1 is the recursive form of expected delay evaluation from Eq. 2. In addition, the optimal decisions at node i, ˆa(vi ), that leads to the shortest path can be readily obtained from the argument of the Bellman equation as follows ˆa(vi ) = arg min a∈A(vi ) [Ci,j + γ · ˆV (k−1) (vj )] (3) where A(vi ) represents the set of arcs that are emerging from node vi . 2.2 Temporal Cost Evaluation From the above shortest path formulation, the path cost has been assumed to be a constant value during the computation. This is applicable for the case of static routing, in which the path costs are estimated at pre-fabricated time and optimal routing and scheduling are determined based on these estimated costs. In the dynamic routing case, we allow the cost to be evolved according to the congestion of the network. Therefore, the cost will not be a static value, and it will capture the congestion situation of paths at runtime. A method to obtain an approximate of this cost at run-time is necessary. Intuitively, the cost function explicitly models the latency (waiting time) that packets will spend on a particular node. If the latency is large, it implies that the congestion is likely to happen at this node and vice versa. Therefore, by measuring the average waiting time that packets spend on the node can provide a good approximation for the path congestion conditions. Thus, we denote the temporal cost function Ci,j (k) for the path cost from node i to j and which is the average waiting time of k previous packets. This cost function can then be used for computing the shortest path. Speciﬁcally, the waiting time for the k-th packet is denoted as Ni,j (k). Based on the exponential smoothing method [8], the average of k previous packets’ waiting time is as follows: Ci,j (k) = (1 − α)Ni,j (k) + (1− α)Ni,j (k − 1)α + (1 − α)Ni,j (k − 2)α2 + ... + (1 − α)Ni,j (1)αk−1 = (1 − α) · k−1(cid:2) Ni,j (k − i) · αi (4) (5) i=0 where α (0 < α <1) is called the smoothing constant that determines the number of samples to be considered. This function can be readily converted into a recursive form (in Eq. 6), which can be easily implemented in hardware. Ci,j (k + 1) = (1− α) · Ni,j (k) +α · Ci,j (k) (6) An alternative way to obtain this estimate is by using Little’s formula [12], in which the expected waiting time W = L/λ where L is the expected number of customers in the queue and λ is the mean arrival rate of packets into the queue. In this case, λ can be obtained by measuring the inter-arrival time. Also L can be obtained by measuring the number packets in the FIFO waiting to be sent. However, the division and several multiplications can be expensive in hardware. But this approach can reduce the size of the FIFO required. It is of the designer’s disposal for choosing either approach for computing the expected waiting time. 2.3 Distributed Shortest Path Computation The shortest path algorithm can be implemented using a distributed parallel approach [3], which is originally proposed with the motivation of computational speed-up. The on-chip network provides distributed processing elements and communication channels, which are similar to the computer networks. Distributed shortest path computation can be realized by taking advantage of the distributed environment to provide efﬁcient shortest path computation. In the following, we will present a distributed shortest path computation network and mapping of NoC communication tasks to this network for dynamic routing. A mesh network topology will be used throughout the paper for illustrating the idea. However, the methodology is not limited to the mesh topology and simple modiﬁcation can be made for tackling other topologies, such as torus and Butterﬂy FatTree (BFT). Suppose a communication task graph and their mappings to a tiled-based architecture are given as in Fig. 2(a). Nodes represent the tiles or processing elements, dotted lines represent the communication tasks and arrows represent the possible routes for transporting packets to the destination. A distributed shortest path computation network can be readily constructed based on the network from ﬁgure (a). The corresponding shortest path computation network is shown in Fig. 2(b). The nodes and the topology of the network is the same as the original architecture. As the Bellman recursive equation (Eq. (1))is implemented at each node, the arrows here represent the direction of the information ﬂow. The output from node i is ˆV (vi ), which is the expected cost to the destination from node i. The inputs for node i are ˆV (vj ), for all node j are the neighbors of node i. When each node is performing the computation as stated in Eq. (1), the expected cost ˆV (vi ), for all i, will converge to the optimal value with a speed of O(log2 (N − 1)) where N is the total number of nodes [11]. Note that this methodology enables shortest path computation for multiple-sources single-destination communications tasks. In other words, for each shortest path compu(a) (b) CRight Communication  Tile/PE channel Communication task min t1 t2 t3 t4 Possible  routes CDown t5 t6 t7 t8 t1 t2 t3 t4 t9 t10 t11 t12 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t13 t14 t15 t16 Tile-based architecture Shortest path computation network Figure 2. (a) A 4 × 4 tile-based mesh architecture where nodes are the tile/PE and edges are the communication channels. The dotted lines are examples of the communication tasks. (b) The corresponding shortest path computation network for the network in (a) and node t16 is the destination node for the communication tasks. tation network, it can only handle communication tasks that are having the same destination. For communication tasks with different destinations, simply using another independent shortest path computation network for the necessary computation. Since adding the shortest path computation network may introduce extra hardware cost, such as power and area, there is not necessarily to implement dynamic routing for all communication tasks. A hybrid implementation that involves both dynamic routing and static routing can be an effective solution. One of this example can be found in [10] for using partial dynamic routing scheme. Judicious decisions and analysis are required to determine the communication tasks partitioning for dynamic and static routings which is beyond the scope of this paper and considered for future work. 3 Implementation 3.1 Hybrid Analog-Digital Routing Network Fig. 3 shows the design of a router, which enables dynamic routing. Basically, the router design is similar to conventional routers, which were depicted in the context of NoC [9]. There are two additional blocks that implement the dynamic routing algorithm. They are the congestion monitor and the controller for the switch. The congestion monitor implements the temporal cost evaluation algorithm that compute the estimate of average latency that packets will spend in this router for each of the queues. Speciﬁcally, a time tag, which used for measuring the waiting of a packet, will be added to each of the packet when they enter the queue. At the switch, the time tag will be examined and compared to the current time, so that the time spend that the packet has spent in the router can be obtained. On the other hand, optimal path computation is implemented using analog circuit. Since for a standard mesh or torus on-chip network, there are usually no more than four directions that a packet can choose as the next forwarding hub. In other words, if the optimal path computation is realized using analog circuit, there is no need to implement a high resolution Analog-to-Digital (ADC), as the directions are only one or two bits information. By taking the advantage of this fact, the analog circuit becomes a suitable candidate for implementing the minimum value network. As can be seem from Fig. 3 label (2), the analog optimal path computation outputs the directions to the control through ADC. Label (5) and (6) are the inputs of the cost estimates from the digital congestion monitors of neighbor nodes. These cost estimates are used for the shortest path computation. To neighbor  units Congestion  monitor Arbiter Router North South East West Local Switch North South East West Local 5 6 From neighbor  units Analog optimal path  computation 2 Controller Figure 3. Design of a router that comprises of digital congestion monitor and analog optimal path computation. The digital part evaluates the temporal latency costs at the queues and that the costs are inputting to the analog computational network. 3.2 Analog Implementation of the Shortest Path Computation Network A continuous-time extension of the recursive BellmanFord algorithm can then be formulated from Eq.(1), to give [11] dVvi (t) dt = −τiVvi (t) + min a∈A(vi ) [Ci,j + γ · Vvj (t)] (7) where Vvi (t) represents the approximated ˆVvi in the continuous time, and τi is a ﬁrst-order lag constant which is implementation-dependent. Notice that when dVvi (t)/dt is approximated by the discrete-time ﬁnite difference Vvi (t + 1) − Vvi (t) and τi = 1, Eq.(2) reduces back to Eq.(1). Eq.(2) provides a natural setting for parallel computational circuits, without the need of imposing sequential constraints as in the traditional digital Bellman-Ford equation (Eq.(1)). 3.2.1 Current-Mode Shortest Path Computation An analog voltage-mode implementation using the binary relation inference network (BRIN) [11] for solving the dynamic programming was previously reported. In [13], a current-mode implementation for implementing Eq. 2 was presented. The current mode design is with a simpler and more effective current-mode design using only losertake-all (LTA or min) circuits [6, 16] previously developed for neural-fuzzy systems to solve the shortest path problem. By extending the previous design, we introduce the digital-analog conversion (DAC) and analog-digital conversion (ADC) to provide an interface for communication between the analog network and the digital computational units. Fig. 4(a) shows a typical mesh network architecture with N 2 nodes. Node SN ,N is the destination node and the arrows represent the possible route to SN ,N . A analog distributed shortest path computation network can be constructed following the topological conﬁguration of network in Fig. 4(a). Fig. 4(b) is the analog current-mode realization of the the shortest path computation network. The arrow represents the analog current direction and the topology of the analog circuit is constructed followed the original mesh network. Each computational unit implements the Eq. (2). Labels 2, 5, and 6 represent the interfacing with the digital router and labels 1, 3 represent the interconnections between different analog computational units. This ﬁgure signiﬁes that the analog computational network is continuously computing the shortest path while digital keeps feeding the new cost estimates into the network. 3.2.2 The Analog Computational Unit (cid:1)4 The right hand side of Eq.(1), in which a minimum operation is performed over all possible actions from node vi , provides the necessary framework for using the LTA in a dynamic programming unit for solving an N -node directed graph problem. The circuit schematic is shown in Fig. 5. It comprises of three blocks, which are the digital-analog conversion (DAC), analog-digital conversion (ADC) and a Loser-take-all (LTA) circuit. Inputs from labels 5 and 6 are digital inputs, which are voltages of either zero or Vdd. Current mirror Q1, Q2, Q3 and Q4 are for amplifying the reference current to convert the digital input into analog current inputs. Speciﬁcally, the output current from this DAC i=0 qi 2i for inputs (5) and i=0 pi2i for inputs would be (6). Current mirrors Q1-Q4 amplifying the current accordingly based on the geometric parameters W and L modiﬁcation during circuit layout. Inputs 3 and 4 are current inputs, which are coming from other computational units (see Fig. 4(b)). There are two outputs from the minimum circuits. The Vout outputs the minimum current, which represents the expected latency. This will become the input of (3) and (4) for other computational units. The transistor (F0 ) gives the output max(A−B , 0). In this case, if A > B , the output will be a non-zero value, otherwise, the output will be zero. Based on output from (F0 ), we can readily convert it into digital signal by using a typical algorithmic ADC. The output (D0 ) is a digital signal for the router controller, which (cid:1)4 (a) (b) 5 6 2 S1 ,1 S1 ,2 S1 ,N U nit(1,1 ) 4 5 6 2 1 5 6 3 5 6 2 5 1 U nit(1,2 3 U nit(1,N) 1 ) 4 4 2 1 S2 ,1 S2 ,2 U nit(2,1 3 1 U nit(2,2 3 ) 4 ) 4 SN,1 5 SN,N 1 U nit(N,1) 3 1 U nit(N,N) Figure 4. (a) A typical mesh network with N 2 nodes. Node SN ,N is the destination node and the arrows represent the possible route to SN ,N . (b) A computational network that corresponds to the network from (a). The arrow represents the analog current direction and the topology of the analog circuit is constructed following the original mesh network. Each computational unit implements the Eq. (2). Labels 2, 5, and 6 represent the interfacing with the digital router and labels 1, 3 represent the interconnections between different analog computational units. will control the directions of next hub forwarding. 4 Results and Discussion 4.1 Hybrid Network Evaluation To evaluate the performance and network convergence of the hybrid analog-digital routing network, we have imple0.50 micron model. Networks of size 3 × 3 and 4 × 4 were mented the design using CMOS transistors using the AMIS studied. By using the SPICE simulation, we can obtain the network response curve by measuring the current outputs from each of the analog computational units and the digital vergence for a 3 × 3 network. The output, which is labeled outputs from the ADCs. Fig. 6(a) shows the network con(1) from Fig. 5, was measured and the output from four different units that were changing with time are shown in the ﬁgure. The curves shows the evolution of currents from the network responding to the changes of the digital input (the latency estimate) at t = 500 ns. It can be observed that the analog responses to the digital inputs and converges to a steady state in 100 ns. The lower panel shows the relative error of the output signals relative to the numerical solutions, which was obtained from the algorithmic computation using Matlab. This result show that the error for the than 100 ns. Fig. 6(b) shows the results for a 4 × 4 netshortest path computation is reduced to less than 5% in less work simulation. The output from nine different units that were changing with time are shown in the ﬁgure. A similar observation can be made that the network converges to the optimal shortest path in around 150 ns. In order to compare our mixed-signal and a purely digital implementation for power consumption and hardware area, a digital counterpart of the analog circuit was implemented. Ripple carried adders was used for the addition circuit and the minimize circuit was implementing by using a subtractor and a multiplexer. The same number of bits as the DAC were used in the digital circuits. As the digital circuit was also using the same technology model and implemented using CMOS transistors, the fair comparison for the power consumption and area can be obtained. We compared the area by counting the number of transistors used between the two different circuits. The area comparison result is shown in Fig. 7(a). We compared the area of the analog circuit and its digital counterpart for different network size varying from 4 to 16. The analog circuit area is generally much smaller than its digital counter part. For smaller network, such as 4-node network, analog area is around half of the digital design (64 versus 124 for analog and digital respectively). For a large network such as 16-node network, analog area is only 20.6% of the digital area (453 versus 2192 for analog and digital respectively). Comparing circuit area based on transistor count may not be accurate. A transistor in an analog circuit is genDAC 5 q0 q1 q2 q3 Q1 Q2 Q3 Q4 p0 6 p1 p2 p3 Q1 Q2 Q3 Q4 3 V1 V2 4 ADC D0 2 IRef Vout 1 F0 Figure 5. The circuit schematic of a analog computational unit. It comprises of three blocks, which are the digital-analog conversion (DAC), analog-digital conversion (ADC) and a Loser-take-all (LTA) minimizer circuit. eral much larger than that in a digital circuit. However, this is after partially compensated the reduced routing found in analog circuits. Therefore, the area results presented above is probably optimistic in favour of the analog implementation. More accurate comparison will be presented after the circuits are implemented as physical layout. We also compared the power consumption between the analog and its digital counterpart. The power analysis was performed using SPICE1 , in which the average power dissipation in conjunction with the transient analysis. The circuit was injected with a set of random test vectors and was allowed to run for one section for the transient analysis with V dd = 3V . The analog design generally consumes signiﬁcantly less power than its digital counterpart. Even for a small network, 4-node network, the analog circuit consumes 19% of the power that the equivalent digital circuit consumes (0.95 mW versus 5 mW for analog and digital circuits respectively). For a larger network, 16-node network, the analog circuits consumes 23% of the power that the digital circuit consumes (12.7 mW versus 54.8 mW for analog and digital circuits respectively). In general, the analog design consumes less area and power when comparing to its digital counterpart. This advantage is more signiﬁcant for a large network, such as 16node network. From our experiments, an analog design 1 The average power P for a time interval (t1, t2) is computed by using the trapezoidal rule approximation to evaluate the integral ˆP (t1 , t2 ) = (cid:3) t2 t1 t2−t1 1 P (τ )dτ with a 4-bit DAC is well robust for handling the minimal network computation. Further analysis on the affects of interconnect parasitic and substrate noise, which are the major sources of noise in the digital circuit, on the analog circuit will be investigated in the future. 4.2 Dynamic Routing under Random Traﬃc To evaluate the performance gains that can be achieved with the temporal cost adaptation and the hybrid routing network, a higher level simulation platform is required. Simulink has become a popular platform for rapid prototyping and several Simulink tool boxes were developed that can synthesis the cycle-accurate model into VHDL and EDIF netlist for hardware realization, such as Xilinx System Generator [18]. We have developed a cycle-accurate interconnection packet-switching network model based on Simulink. The router is implemented based on ﬁxed-point cycle accurate model and can be readily translated into synthesizable VHDL for further implementations. The analog part of the routing network was implemented by using the Euler integration method by converting the differential equations into differences equations, which can be readily implemented using a cycle-accurate simulator. We simulate several mesh networks with different routing schemes and design parameters under random hot spot trafﬁc patterns. Under each load and conﬁguration, net70.0µ 60.0µ 50.0µ 40.0µ 30.0µ 20.0µ 400.0n ) A ( t n e r r u C r o r r e e v i t l a e R 0.3 0.2 0.1 0.0 400.0n 500.0n 600.0n Time (s) 700.0n 800.0n 500.0n 600.0n Time (s) 700.0n 800.0n ) A ( t n e r r u C 90.0µ 80.0µ 70.0µ 60.0µ 50.0µ 40.0µ 30.0µ 20.0µ 400.0n 500.0n 600.0n Time (s) 700.0n 800.0n r o r r e e v i t l a e R 0.3 0.2 0.1 0.0 400.0n 500.0n 600.0n Time (s) 700.0n 800.0n Figure 6. Current-mode shortest path computation results and relative error for a 3 × 3 network (a) and a 4 × 4 network (b). (a) The upper panel shows the outputs, which is labeled (1) in Fig. 5, for four different computational units. This value is the expected latency to the destination node. The lower panels shows the relative error of the expected latency from all units. (b) The outputs, which is labeled (1) in Fig. 5, for nine different computational units and the lower panel shows the corresponding relative error. work of sizes 4 × 4 are simulated, which uses X-Y static routing and temporal cost adaptation dynamic routing, respectively. In X-Y routing, the packet follows the row ﬁrst, then moves along the columns towards the destination [4]. The efﬁciency of the two different types of routing is evaluated through latency curve. Similar to other work in the literature, we assume that the packet latency spans the instant when the ﬁrst packet is created, to the time when the last packet is ejected to the destination node, including the queuing time at the source. In this case, the processors generate messages at the time intervals chosen with exponential distribution. Similar to the work presented in [9], hot spot trafﬁc pattern is used in the simulation. Fig. 9(a) shows the frequency versus packet latency for the static and dynamic routing from the same simulation model. The trafﬁc load injected into the network is smaller than the critical trafﬁc load. Both distributions are having the peak on latencies between zero and 500 cycles. However the distribution for the static routing appears to have a much longer tail. The variance of the packet delay is much larger than the one with dynamic routing, which has all packets arrived in 2000 cycles or less. The dynamic routing scheme provides a better quality of service than the static routing in random trafﬁc. In ﬁg. 9(b), the latencythroughput ﬁgures under the hot spot random trafﬁc pattern is shown. The average latency is measured for different trafﬁc injection loads, which has been normalized. The robustness of different network designs that tolerates trafﬁc loading can be found in this ﬁgure. For trafﬁc load less than 0.2, both static and dynamic routing networks provide a stable service for packets and the latency is roughly 50 cycles. When the trafﬁc load is larger than 0.2, the latency from a static routing network begins to increase and when the trafﬁc load equals to 0.4, the latency increases sharply and that the network is saturated. For the dynamic routing network, the latency increases slowly even for trafﬁc load larger than 0.2. The latency increases rapidly after trafﬁc load is larger than 0.5. The network with dynamic routing has a much larger tolerance to the random trafﬁc loadings. It has been shown that dynamic routing can provide signiﬁcant improvement on latency for random trafﬁcs. In addition, our results show that a network with dynamic routing can provide an overall improvement on trafﬁc ﬂow and the average waiting time for the entire network can be reduced. Fig. 8 shows the evolution of average waiting time for all queues in the network over 20000 cycles. The average waiting time is an important estimate indicating that the congestion status and trafﬁc ﬂow over the network. The larger of the average waiting time implies that the trafﬁc congestion is more serious in the network. In the ﬁgure, the static routing result shows a much higher average waiting time implying that there are more congested nodes in the network.          Analog  Digital 2.0k 1.5k 1.0k t n u o c r o t s s i n a r T 500.0 0.0 4 6 8 14 10 12 Network size (node) ) W ( r e w o P 60.0m 50.0m 40.0m 30.0m 20.0m 10.0m 0.0  Analog  Digital 4 6 8 10 12 14 16 Network size (node) 16 Figure 7. The comparison between the analog and its digital counterpart on hardware area and power consumption for the optimal path computation. (a) The transistor count comparison. (b) Power consumption comparison. For the network using dynamic routing, the average waiting time is reduced signiﬁcantly. The results suggest that introduction of dynamic routing may provide an effective ﬂow control for random trafﬁcs. 5 Discussion and Conclusion A hybrid analog-digital routing network for network-onchip dynamic routing has been presented. The hybrid design methodology, by taking the advantages of low-power analog computation and accuracy of digital computation, can signiﬁcantly reduce the hardware area and power dissipation for on-chip dynamic routing while provide effective run-time dynamic routing. Our results show that the hybrid design takes only 20% of the area and consumes 23% of the power of that the digital equivalent requires. The hybrid approach provides a hardware economical solution for implementing on-chip dynamic routing. The implementation of hybrid routing network is based on a temporal cost evaluation and adaptation scheme, in which packets are forwarding to a path with the minimum expected waiting time. Our results show that with the adaptation scheme, average latency and overall average packets waiting time can be reduced signiﬁcantly. The proposed design methodology enables hardware cost-effective dynamic routing on chip and would substantially enhance the quality of service for on-chip communication tasks. The proposed dynamic routing scheme can be readily adapted into other hybrid routing schemes including hybrid adaptive/deterministic routing to achieve the best performance and design tradeoff and judiciously customized to match the given application trafﬁc patterns. Figure 8. The evolution of average waiting time (temporal cost) for the entire network using two different routing schemes.     180 150 120 90 60 30 0 y c n e u q e r F 0 1000  Static routing  Dynam ic routing  static routing  dynam ic routing 600 500 400 300 200 100 ) s e l c y c ( y c n e t a L 2000 3000 Latency (cycle) 4000 0 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Normalized traffic Figure 9. The comparison between dynamic routing using temporal cost adaptation and static X-Y routing. (a) The frequency distribution of packet delays. (b) The average latency versus normalized trafﬁc injection load. [13] T. Mak, K. Lam, H. Ng, G. Rachmuth, and C.-S. Poon. A current-mode analog circuit for reinforcement learning problems. In Proc. of the IEEE Symposium on Circuits and Systems, 2007. [14] C. Mead and M. Ismail. Analog VLSI Implementation of Neural Systems. Springer, 1989. [15] F. Moraes, N. Calazans, L. Moller, and L. Ost. Hermes: An infrastructure for low area overhead packet-switching networks on chip. The VLSI Integration, 38:69–93, 2004. [16] T. Serrano-Gotarredona and B. Linares-Barranco. A highprecision current-mode WTA-MAX circuit with multichip capability. IEEE J. Solid-state Circuits, 33:280–286, 1998. [17] C. Toumazou, F. J. Lidgey, and D. G. Haigh. Analogue IC Design: The Current-Mode Approach. Institution of Electrical Engineers, 1993. [18] Xilinx. Xilinx System Generator for DSP version 8.2.02: User Guide. 2006. "
The Impact of Higher Communication Layers on NoC Supported MP-SoCs.,"Multi-processor systems-on-chip use networks-on-chip (NoC) as a communication backbone to tackle the communication between processors and multi-level memory hierarchies. Inter-processor communication has a high impact on the NoC traffic but, to this day, there have been few detailed studies. Based on a realistic case study, we present a contrastive comparison of cache-based versus scratch-pad managed inter-processor communication for (distributed shared-memory) multiprocessor systems-on-chip. The platforms we target use six DSP nodes and a shared L2 memory, interconnected by a packet-switched network-on-chip with differentiated services. The first version of the platform uses caches to perform inter-processor communication whereas the second one uses a novel type of distributed DMA to help performing scratch-pad management. With detailed simulation results we show that the scratchpad application mapping has the best overall performance, that it helps smoothing NoC traffic and that it is not sensitive to the quality-of-service (QoS) used. We furthermore demonstrate that, on the contrary, cache-based MP-SoCs are very sensitive to the QoS level and that they generate significantly more NoC traffic than their scratch-pad counterpart. We recommend, where possible, to use scratch-pad management for NoC supported MP-SoCs as it yields performant, predictable results and can benefit from platform virtualization to achieve composability of applications","The Impact of Higher Communication Layers on NoC Supported MP-SoCs T.Marescaux (cid:3) , E. Brockmeyer (cid:3) , H.Corporaal y (cid:3) IMEC, Kapeldreef 75, 3001 Leuven, Belgium fmarescau,brockmeyg@imec.be yTechnical University Eindhoven, 5612 AZ Eindhoven, The Netherlands Abstract(cid:151) Multi-processor systems-on-chip use networks-onchip (NoC) as a communication backbone to tackle the communication between processors and multi-level memory hierarchies. Inter-processor communication has a high impact on the NoC traf(cid:2)c but, to this day, there have been few detailed studies. Based on a realistic case study, we present a contrastive comparison of cache-based versus scratch-pad managed interprocessor communication for (distributed shared-memory) multiprocessor systems-on-chip. The platforms we target use six DSP nodes and a shared L2 memory, interconnected by a packetswitched network-on-chip with differentiated services. The (cid:2)rst version of the platform uses caches to perform inter-processor communication whereas the second one uses a novel type of distributed DMA to help performing scratch-pad management. With detailed simulation results we show that the scratchpad application mapping has the best overall performance, that it helps smoothing NoC traf(cid:2)c and that it is not sensitive to the quality-of-service (QoS) used. We furthermore demonstrate that, on the contrary, cache-based MP-SoCs are very sensitive to the QoS level and that they generate signi(cid:2)cantly more NoC traf(cid:2)c than their scratch-pad counterpart. We recommend, where possible, to use scratch-pad management for NoC supported MP-SoCs as it yields performant, predictable results and can bene(cid:2)t from platform virtualization to achieve composability of applications. I . IN TRODUC T ION Multi-processor systems-on-chip (MP-SoC) display complex communication that requires a communication backbone to provide (cid:3)exibility, scalability and different levels of quality-of-service (QoS). Networks-on-chip (NoC), are natural candidates to become this communication backbone. The choice of NoC features and parameters are thus critical for the communication architecture. However, the type of interprocessor communication (message-passing or shared memory with caches or scratch-pads) has a high impact on the traf(cid:2)c generated on the NoC and has thus to be taken into account. This article addresses the impact of architectural choices such as the usage of a cache versus a scratch-pad memory on NoC supported MP-SoCs. We present the parallelization of a video encoder with its mapping to two different (cid:3)avors of an MP-SoC platform with six DSP nodes and a shared L2 memory, interconnected by a packet-switched NoC with differentiated services. The (cid:2)rst architecture provides cachebased inter-processor communication, whereas the second one relies on software-controlled DMAs to perform scratch-pad management. We introduce a novel type of distributed DMA to help perform scratch-pad management in complex memory hierarchies. The major contribution is a simulation-based detailed quantitative comparison of the differences between the two mappings. We show that the scratch-pad mapping has the best overall performance, that it helps smoothing NoC traf(cid:2)c and that it is not sensitive to the QoS. We furthermore demonstrate that, on the contrary, cache-based MP-SoCs are very sensitive to the QoS level and to the amount of bandwidth available and that they generate signi(cid:2)cantly more NoC traf(cid:2)c than their scratch-pad counterpart. To the best of our knowledge, this is the (cid:2)rst real-life case study performing a detailed comparison and analysis of scratch-pad versus cache-based NoC-supported MP-SoC systems. The rest of this article is structured as follows. Section II details the MP-SoC platform architectures we are addressing. Section III discusses related work. In section IV we present the parallelization and mapping of the application. Section V details the simulation and extensively discusses the results. Finally, section VI concludes. I I . P LAT FORM ARCH I T EC TUR E The MP-SoC hardware architectures we target are composed of processor and memory nodes interconnected by a switched communication backbone (Figure 1). Processor nodes typically contain one or several processing units (CPUs, DSPs, ...) and a local hierarchy of buses and memories. The simplest form of local memory hierarchy, includes a scratch-pad memory (L1) for data and a local instruction memory. Memory (and I/O) nodes only contain a memory hierarchy. All types of nodes interface to the NoC through a bridge. In its simplest form, the bridge is a direct interface to the Network Interface (NI) and allows processor nodes to perform message passing. In this article we address more complex versions of bridges that interface bus-based communication to the NoC backbone. The role of these bridges is to encapsulate (and decapsulate) memory transactions into (from) NoC transactions. In this article we compare two variations of the same platform architecture that only differ by the type of bridges used. The (cid:2)rst platform type uses a communication assist (Section II-A) bridge to help performing scratch-pad management, whereas the second uses a cache-bridge (Section II-B) to perform communication across the various layers of the Processor Node Memory or I/O Node L3 CPU L1 Data /Cache L1 Inst. L2 External Memory Interface Bridge (DMA or $ ctrl.) Bridge (DMA or $ ctrl.) NI NI NI Network-on-Chip Fig. 1. MP-SoC platform using an NoC as a communication backbone. Processor, Memory and I/O nodes interface to the NoC by means of bridges (cache-controllers or DMAs) that encapsulate bus transactions into NoC transactions. memory hierarchy. Note that the platform version using cachebridges can lock a portion of the cache to use as a scratchpad for local data such as stack and heap. In this sense this architecture is already an optimization with respect to classic pure cache-based architectures in which any local data may generate cache misses and replace useful (local) data. A. Scratch-Pad Management with Communication Assist The communication assist (CA) is a hardware component (an evolution of a DMA) that manages the transfers of blocks of data between different memories in a system. It performs transfers independently of the processor, so that communication and computation happen in parallel. The CA is controlled by code included in the application running on the (local) processor. Controlled prefetching of data greatly helps hiding the long communication latency introduced by the NoC, the bridges and the remote memory layers. The prefetch code can of course be manually inserted in the application code, but it can also in great part be automatically annotated by an application analysis tool (Section IV-D). Note that computing how long ahead the data prefetch should be starting to avoid the processor to stall (waiting for data) requires a good estimate of the bandwidth actually offered by the communication infrastructure for the given transfer. To this end, we prefer to use NoCs that allow reservation of resources to provide bandwidth guarantees to individual connections. The CA is an advanced distributed DMA controller. Distributed means in this context that the CAs at both ends of the connection are working together to execute a blocktransfer, using a communication protocol on top of the network protocol. Block-transfers (BTs) are transfers of a datastructure, or a part of data-structure, e.g. a macro-block of a larger image. Source and destination data-structures of a blocktransfer may have different sizes, layout and random offsets. The CA supports two-dimensional BTs to transfer smaller parts of a larger multi-dimensional data-structure, such as a video frame, without losing the spatial relation between the different parts. For example, two neighboring macro-blocks of an image can be transferred in two separate block-transfers, while in the destination memory they can form a single data-structure as if the two were transferred at once. Distributing DMA functionality enables not only scalability of the platform, but also memory to memory transfers. For instance a processor can request a transfer from off-chip SDRAM (L3) to on-chip L2. The CA on the processor tile forwards the request to the CA of the L2 memory tile. B. Cache Bridge The cache bridge is in the (cid:2)rst place a cache controller. In our system the cache is con(cid:2)gured to have a write-back policy and the replacement policy is least recently used (LRU). The cache line-size is set to 16 bytes. Associativity and set-size are con(cid:2)gurable. Section V explores the effect of a sweeping of these parameters on application performance and traf(cid:2)c generated. Cache misses and line evictions are encapsulated by the cache bridge into NoC transactions. Based on a simple (and small) look-up table, the destination NoC address is resolved within the cache bridge depending on the physical address of the cache line affected by the miss or the eviction. Within address ranges mapped on the same remote memory node, memory consistency (of private data) is guaranteed by the in-order-packet delivery property of the NoC (Section IIC). For shared data, it is the responsibility of the designer (or of the design-tool) to guarantee release consistency at a higher level. No support for hardware cache coherency is provided. Indeed, as NoCs are not broadcast communication architectures (not in the sense a shared bus is) it is not possible to support simple snooping cache coherency protocols. In this case, one has thus to implement directory-based cache coherency protocols, that are very expensive for NoCs both in terms of latency as in terms of area. As coherency is nevertheless required for (cached) shared data, the cache bridge provides support to perform softwarecontrolled cache coherency. In particular, the cache bridge has control (and status) registers memory-mapped on the processor bus to allow explicit invalidates and (cid:3)ushes of lines containing an address the processor explicitly requests to be invalidated or (cid:3)ushed. Particular care has to be paid to align data of cached shared data to the cache lines (Section IV-E). C. Network on Chip The communication backbone of our platform architecture is a packet-switched network-on-chip with prioritized virtual channels to support differentiated quality-of-service levels. Though the current case study would be relevant to other types of NoCs, we have used a model of a network similar to the (cid:148)GS-BE central programming architecture(cid:148) of the ˘thereal NoC [1]. GS-BE is a pure packet-switched NoC that provides Guaranteed Services (throughput and latency) and Best Effort. The guaranteed services are provided by dividing the bandwidth in time-slots. Such an NoC is relevant to our purpose because it offers the types of bandwidth guarantees the scratch-pad memory management techniques require and it allows to contrast the effect of the QoS offered by the NoC on the traf(cid:2)c offered by the various bridge types. The NoC uses wormhole switching with source routing for (cid:3)exibility and deadlock avoidance. Routing is deterministic to guarantee in-order packet delivery and no packet-dropping is allowed. Two classes of QoS are supported a low-priority best effort (BE) class and a high-priority, connection-oriented, guaranteed throughput (GT) class. All GT connections use a (centrally computed [2], [3]) contention-free time-slot allocation algorithm to guarantee that contention between GT packets can never occur in the system. The priority scheme ensures that GT packets always win contention over the BE ones, hence guaranteeing bandwidth and (cid:2)xed packet latency. The amount of bandwidth reserved on a GT connection is proportional to the number of time-slots allocated to it. I I I . R ELAT ED WORK Poletti and al. discuss in [4] the advantages of DMAenabled scratch-pad memories with respect to caches in a multi-processor environment. The MP-SoC considered uses an AMBA bus as communication backbone. The paper focuses on dynamic applications and dynamic scratch-pad management, requiring a dynamic memory manager for the scratch-pad. The authors use a classic DMA that can only perform blocktransfers between a processor node and the memory node on the next layer in the memory hierarchy. In contrast, we focus on NoC supported MP-SoCs and our communication assist has the ability to perform transfers between any layers of the memory hierarchy (memory node to memory node transfers are possible). Another article of Poletti and al. [5] presents a technique to implement message passing on top of (distributed) shared memory MP-SoCs. Two (cid:3)avors of MP-SoC communication backbones are used: a shared bus and a cross-bar. One of the applications used by the authors to test their messagepassing API is QSDPCM, the same application we use as a test case. The authors do not (extensively) discuss the parallelization of the application, nor specify the resolution of the images encoded, nor the processor architecture used to execute it. Results are thus impossible to compare and this is particularly unfortunate as the authors seem to claim that presenting a distributed shared-memory view of the platform to the application leads to inef(cid:2)ciencies. This is quite surprising, since our application uses a distributed shared-memory view of the platform and achieves very high processor utilization loads with a low number of stalls. The approach presented by Bekooij and al. in [6] also relies on providing a message-passing API on top of a distributed shared-memory architecture. In contrast to the previous approach, the authors spend much effort in hiding the communication latency. They indeed rely on having specialized hardware modules to manage token FIFOs. These FIFOs are properly dimensioned to hide the latency of the NoC and of Current frame Input SS2 SS4 VLC QC ME1 ME2 ME4 Output MC SS2 SS4 Reconstructed previous frame Fig. 2. Overview of the QSDPCM algorithm. the memory hierarchy to keep the processing pipeline (cid:2)lled. The MP-SoC considered in [6] uses the ˘thereal NoC as communication backbone. Benini and al. present MPARM in [7]. MPARM is a SystemC MP-SoC platform simulator that allows simulating real applications and operating-systems using instruction set simulators (ISSs). MPARM has a strong focus on state-ofthe art communication backbones: AMBA, ST-Bus (though a bit-accurate model of the (cid:2)-pipes NoC is being integrated). Conceptually, MPARM is similar to the NoCTurn simulator used in this article. IV. A P P L ICAT ION MA P P ING A. Application Overview The Quad-Tree Structured Differential Pulse Code Modulation (QSDPCM) algorithm is an inter-frame compression technique for video images [8]. It consists of a hierarchical motion estimation, quad-tree quantization, variable length encoding and image reconstruction (Figure 2). The encoder iterates over all macro-blocks (MBs) in the image, (cid:2)rst horizontally (Yloop) then vertically (X-loop), and processes them one at a time. The hierarchical motion estimation consists of 3 steps: (cid:2)rst at quarter resolution (ME4), then half resolution (ME2) and (cid:2)nally at full resolution (ME1) for every macro-block (16x16 pixels) in the image. It determines the motion vector with which the difference between the current and reconstructed previous image can be encoded using the shortest code word. For this purpose the input image is sub-sampled by a factor four (SS4) and two (SS2). Quad-tree quantization (QC) works by recursively splitting the motion compensated predicted error frame from the ME1 step into four quadrants until, within a certain threshold, such a quadrant can be approximated by its quantized mean value. The image reconstruction decodes the frame by dequantization and motion compensation (MC). The output stream is compressed using variable-length coding (VLC). B. Pro(cid:2)ling & load balancing We aim to achieving a target performance of 30 frames per second at VGA resolution (640x480) with a minimum 60000 50000 40000 30000 20000 10000 ] s e l c y c [ e m i t n o i t u c e x E 0        SS previous     SS current                    ME4 ME2 ME1    QC  MC Fig. 3. QSDPCM kernel execution times for QCIF resolution. required L1 memory and a maximum of useful processing load, well balanced over all processors. We start by pro(cid:2)ling the sequential, kernel optimized QSDPCM code on a single TI C62 DSP. The computation time of the different functions can be measured by compiling and running the performance optimized application on an instruction set simulator (ISS). The results are displayed in (cid:2)gure 3. To achieve the performance target, 6 DSPs are required when they are operating at their most energy ef(cid:2)cient frequency (410MHz in this case [9]). From the measurements it is clear that the ME1 task is by far the most demanding. It has about 4 times more cycles than the other motion estimation tasks or the complete quantization and coding together. C. Parallelization Analysis of the code shows that all kernels can be executed in parallel by constructing a functional pipeline with each kernel in a different pipeline stage. Balanced requirements for the processor nodes can be achieved by merging multiple kernels into one thread and assigning them to a single DSP, and by exploiting data-level parallelism by duplicating threads and assigning each of them to a different processor. For the QSDPCM application a 3-stage functional pipeline and a 4-way data split (of the ME1 thread) is chosen. The SS2, SS4, ME2 and ME4 kernels are merged into the ME42 thread and the QC and VLC kernels are merged into the QC thread, as shown in (cid:2)gure 4. If all stages of the pipeline are synchronized at the slice level, the data-split is also (naturally) a split of a slice. We have chosen to make the data-level split of the ME1 threads at the granularity of a slice, rather than a macro-block, giving each of the four ME1 processors (ME[1..4]) one quarter of a slice (10 macro-blocks) to process (Figure 5). Such a coarsegrain data split has the advantage of averaging out the variation in processing time over all macro-blocks inside a slice chunk. Moreover with such a split, data transfers to each processor can be larger, which decreases the relative overhead in the case of the DMA supported scratch-pad mapping (though we chose to keep them small for reduced L1 size). The disadvantage of this data-split choice is that it takes longer before the functional pipeline is (cid:2)lled ((cid:2)lling this functional pipeline is denoted as pre-amble later on). Special attention needs to be paid to the motion vector arrays MV2 and MV1 as they are shared data between the 1 e g a t S 2 e g a t S 3 e g a t S ME42 ME1 ME1 ME1 ME1 QC Fig. 4. QSDPCM parallelization: three stage functional pipeline and a 4-way data split of ME1. 16 Slice QC ME1[1] ME1[2] ME1[3] ME1[4] M 42E Fig. 5. QSDPCM synchronization is performed at the slice level. ME42 processes an entire slice and is one slice ahead of the 4-way data split of the ME1 threads. That data-split is made at a coarse granularity of 10 macro-blocks per processor. QC is the last stage of the functional pipeline; it processes an entire slice. pipeline stages M E 42 ! M E 1 and M E 1 ! QC , respectively. A motion vector is composed of a X and a Y coordinate (unsigned characters) encoding the displacement of a given macroblock between the previous and the current frame. In a VGA (640x480) image there are 40x30 macroblocks of 16x16 pixels. Note that, given the parallelization, the MV2 array is written by a single processor (ME42) and (non-overlapping portions of it are) read by the four ME1 processors, whereas parts of the MV1 array are written by the four ME1 processors and read by a single processor (QC). The very same data and functional split of the application is used in both mapping cases: scratch-pad and cache. Application threads are statically scheduled using barrier synchronization at the Y-loop (slice) level, which iterates over the macro-block stripes of the image (Figure 5). D. Scratch-Pad Mapping In the case of the scratch-pad mapping, inter-processor communication is performed by transferring blocks with a communication assist. To automatically schedule the prefetching of the blocktransfers (and optimally lay-out arrays in the memory hierar          chy), the application mapped has been analyzed with a multiprocessor version of the single-processor tool (MHLA) described in [10]. The memory access analysis is an input to the MHLA tool to decide on the assignment of data to the different memory layers and the scheduling of the block-transfers. A trade-off between bandwidth and L1 memory usage must be made to choose the right size of the data copy. The overhead of the block-transfer header is lower when choosing large copies, but they require more space in L1. The MHLA tool generates C-code with scheduled block-transfers for a family of (Paretooptimal) solutions with different memory requirements (the early version used generated the block-transfer schedules but required manual intervention to (cid:2)nalize the C-code). It is the responsibility of the designer to choose one particular tradeoff point. For this application, we have chosen to use 4KB of scratch-pad memory. We thus assume that the platform running this application provides processor nodes with L1 memories of at least 4KB. In the scratch-pad mapping the coherency issue is solved for the shared data of the motion vector arrays MV2 and MV1. Indeed, coherency problems can only occur when several processors write shared data. The only array written by multiple processors is the motion vector output of the ME1 stage: the MV1 array1 . Given the chosen data split (slice chunks with 10 macro-blocks per chunk) there are no overlapping macroblocks between ME1 processors. Consequently all motion vectors written in the MV1 array as an output of ME1 are written by a single processor (different processors write different parts of the array). Moreover, as data-transfers performed with the CA allow to copy only the required data (contrary to a cache where a cache line could contain data belonging to another processor) no two entries of MV1 in L2 are written by the same processor, this avoiding all coherency issues. E. Cache Mapping The cache mapping version of the QSDPCM is adapted to the platform architecture using cache bridges. There are three main differences between this code and the scratchpad version. The (cid:2)rst difference is that there is no explicit prefetching of data. Data prefetching with caches is a known technique used in literature [11], but is out of the scope of this article. A direct consequence is that every read or write miss will cause the processor to stall, waiting for data to be fetched over the NoC. This stall time can be furthermore increased by also waiting for the (cid:3)ush to memory of the line being currently replaced by the new line fetch (assuming a write-back cache). The second difference is that ensuring coherency of shared data for the MV1 array becomes a problem. Indeed, all ME1 vectors still write at the same (non-overlapping) positions in the MV1 array. However, portions of the MV1 array, private to two different processors will share the same cache line. Indeed, every ME1 processor processes 10x30 macroblocks (40 macro-blocks per slice, 30 slices per frame and a data-split between 4 ME1 processors) and hence requires MV1 arrays of size 300 bytes, which is not a multiple of 16, the cache-line size. In case of private portions of the MV1 array overlapping on the same cache line, a processor will overwrite portions private to the other, resulting in data corruption. The need to ensuring coherency has an important impact on the application code. Indeed, MV1 data needs to be properly laid out in memory. To this end, we have chosen to use a padding of 4 bytes for the MV1 arrays: 304 is a multiple of 16, so that all MV1 data private to a given ME1 processor never has to share a cache line with MV1 data from another processor. The third difference in application code comes from the need to explicitly add cache-related commands to the application code to properly transfer all shared data. Speci(cid:2)cally, all shared data that is being read needs explicit line invalidations by the reading processor to guarantee that the latest value stored in L2 is retrieved (the value cached locally could be out-of-date as no hardware cache coherency is provided). Conversely, to guarantee that all shared data, temporarily stored in the local caches, is effectively committed to the L2 memory requires explicit (cid:3)ushes in the application (we use a writeback cache to avoid the systematic line (cid:3)ushes a write-through cache would load the NoC with). Explicit (cid:3)ushes and invalidates are performed using an API (cache line (cid:3)ush(unsigned char* addr) and cache line invalidate(unsigned char* addr)) that accesses memory-mapped control and status registers of the cache. Both functions simply take the physical address of a byte (short and int require explicit casting in the application code). A third API function, cache sync() needs to be called to ensure that the previous explicit (cid:3)ush or invalidate has succeeded. Note that the linker script speci(cid:2)es that only the frame (and a few smaller frame-related) arrays to reside in L2 and be actually cached. The stack is situated in a local scratch-pad memory (locked cache lines) to optimize away useless traf(cid:2)c on the NoC, improving application performance. Mapping this application to a pure cache-based system simply requires an adaptation of the linker script and a recompilation. V. R E SU LT S This section describes the instantiation of two versions of the MP-SoC platform used to map the cache and scratchpad versions of the parallelized QSDPCM application. These platforms only differ by the type of NoC bridge used. In addition to varying the type of high-level communication bridge we also vary the type of QoS used on the NoC (BE and GT). We present and analyze system-level results contrasting the effects of the application mapping type (cid:3)avor (and of the related NoC bridge). We furthermore compare the effect of the QoS on the same type of application mapping. 1 There are two MV1 arrays, one for every dimension of the motion vector, but as the discussion is the same for either of them, we refer to the MV1 array without specifying the dimension. The parallelized version of QSDPCM presented in this article requires six processor nodes (ME42, ME1[1], A. Simulation Setup ME1[2],ME1[3],ME1[4] and QC) to run the various threads and an L2 memory node to store the current and reconstructed VGA frames plus a few other smaller temporary arrays required for the kernel processing. Figure 6 shows the instantiation of the MP-SoC platform used for the experiments as well as the mapping of the various threads to the processor nodes. Every processor node is composed of a TI-C62 DSPs (an ISS), with a hierarchy of buses, an instruction memory and a scratch-pad data memory (L1). The DSP can issue two load/stores per cycle and the L1 is a banked memory with four banks. The memory node contains an L2 memory. The NoC interconnects all processor and memory nodes. It is composed of routers and network interfaces modeled in SystemC at the transaction-level. It provides both BE and GT QoS classes and uses a mesh topology. An atomic transaction of the NoC is the transmission of a (cid:3)it between any two routers or between a network interface and a router. A (cid:3)it, or (cid:3)ow-control unit, is the sub-unit of a packet for which the processing of the (cid:3)ow control is atomic. On the data-path, the (cid:3)it can be furthermore decomposed into pipelined units of data (called phits). The NoC modeled here uses a 32-bit data-path (plus 2 bits for out-of-band (cid:3)ow control information) and has three phits per (cid:3)it. In one transaction (one (cid:3)it-clock) there are thus 96 bits of data transmitted. The purpose of the experiment is to show the effect of application mapping and bridge types over NoC traf(cid:2)c. We did thus not attempt to optimize NoC topology nor thread to DSP node mapping. The underlying assumption is that we use an MPSoC platform designed for a certain application domain and thus not optimized to a particular application. In other words, the platform architecture is an input to the application mapping process. Moreover, an application dynamically started by an operating system may have a sub-optimal allocation of threads to processing resources. that uses an XML-based con(cid:2)guration (cid:2)le to describe the MP-SoC platform and the NoC in a very (cid:3)exible way (this instance of the NoC is con(cid:2)gured to mimic ˘thereal; the main differences are likely to come from the packetization policy in the network interfaces as they are less detailed in literature). At the NoC level, the atomic transaction is a (cid:3)it (96 data bits per (cid:3)it, ie. three 32-bit phits) whereas for a processor the atomic transaction consists of the simulation of one ISS cycle. As the QSDPCM application has a relatively high data locality, at VGA resolution it requires relatively modest amounts of bandwidth from the NoC. To have a realistic NoC load we have chosen to clock it at a modest 75MHz (40ns per (cid:3)it), providing a total bandwidth of about 4GB/s (286 MB/s per point-to-point link). The TI-C62 DSP instruction set simulators are clocked at 500MHz and instructions (cid:2)t in L1. We use time-annotated functional models of communication-assist and cache bridges. For (cid:3)exibility, the NoC uses source routing and has been con(cid:2)gured to provide both BE and GT services. We use wormhole-switched, input-buffered, routers with two prioritized virtual channels. The low-priority virtual channel carries only BE packets and can buffer up to 8 (cid:3)its. The high-priority virtual channel is assigned to transfer GT packets. A central contention-free allocation of time-slots guarantees that no two GT packets collide in the system at any time, thus effectively guaranteeing the throughput. A direct consequence is that the GT virtual channel only needs buffering space for phits of the (cid:3)it it is currently processing (the (cid:3)it will be out at the next (cid:3)it cycle). We thus use a 1-(cid:3)it deep buffer. The GT network interfaces are con(cid:2)gured with time-slot wheels of 16 time-slots (18MB/s per time-slot in the current con(cid:2)guration). The GT network interfaces furthermore use end-to-end credit-based (cid:3)ow control to guarantee that the target GT network interface has suf(cid:2)cient buffer space to consume all GT data it receives from the NoC. ME42 ME1[1] ME1[2] B. Experiments TI-C62 L1 Data L1 Inst. TI-C62 L1 Data L1 Inst. TI-C62 L1 Data L1 Inst. Bridge (CA or $) NI (0,0) (1,0) NI Bridge (CA or $) Bridge (CA or $) NI NI Bridge (CA or $) TI-C62 L1 Data L1 Inst. TI-C62 L1 Data L1 Inst. ME1[3] ME1[4] (0,1) (1,1) NI Bridge (CA or $) L2 Bridge (CA or $) NI (0,2) (1,2) NI Bridge (CA or $) TI-C62 L1 Data L1 Inst. QC Fig. 6. Mapping of parallelized QSDPCM to MP-SoC platform. We use NoCTurn, a custom MP-SoC platform simulator to build the cache-bridge and communication assist versions of this MP-SoC platform (Figure 6) and simulate the QSDPCM application. NoCTurn is a transaction-level SystemC simulator We have two (cid:3)avors of the same application (and MP-SoC platform) and two possible QoS con(cid:2)gurations for the NoC. In the case of the scratch-pad mapped application, we only present the results using the GT QoS. Indeed, data prefetching code has been inserted in the application code based on the assumption the NoC can guarantee pre-computed bandwidth requirements, making it a natural candidate to use only the GT QoS. Using GT connections has here not only the advantage of guaranteeing the precomputed application performance, but also of guaranteeing the same performance regardless of other applications potentially concurrently sharing communication resources. This technique combining pre-computed data prefetches over a guaranteed NoC allows to optimize applications independently of each other and guarantees run-time composability. This experiment is referred to as MHLA-GT. For the cache-based version of the application we have the choice of using a single QoS class (BE or GT) or have certain connections use BE and the others GT. For simplicity, we have chosen to study the two cases at the extremes of the spectrum. In one class of experiments (Cache-BE) we use only BE connections and in the other (Cache-GT) only GT connections are used. For all three classes of experiments we use the same precomputed paths through the NoC. Furthermore the MHLA-GT and Cache-GT experiments use the exact same (pre-computed) time-slot allocations. Table I shows, for every source network interface, the source routing con(cid:2)guration and the time-slot allocation (GT only). The source routing is indicated as a list of coordinates of the routers to follow from source to destination (Figure 6). The source slots are only relevant to GT connections (MHLA-GT and Cache-GT experiments). They indicate the time-slots at which the source network interface may inject (cid:3)its in the network. As GT (cid:3)its are guaranteed to cross one hop per (cid:3)it-clock (there is one (cid:3)it per time-slot), it is suf(cid:2)cient to specify the time-slot table at the source network interface (no time-slot tables are required in the routers). Path M E 42 ! L2 L2 ! M E 42 M E 1[1] ! L2 L2 ! M E 1[1] M E 1[2] ! L2 L2 ! M E 1[2] M E 1[3] ! L2 L2 ! M E 1[3] M E 1[4] ! L2 L2 ! M E 1[4] QC ! L2 L2 ! QC Source Routing (0; 0); (1; 0); (1; 1) (1; 1); (1; 0); (0; 0) (0; 1); (1; 1) (1; 1); (0; 1) (0; 2); (0; 1); (1; 1) (1; 1); (0; 1); (0; 2) (1; 0); (1; 1) (1; 1); (1; 0) (1; 1) (1; 1) (1; 2); (1; 1) (1; 1); (1; 2) Source Slots (GT) 0 1 0 1 8 3 10 5 3 7 12 9 5 6 11 12 TABLE I. Path and time-slot allocations in the experiments. The careful reader will notice that we have allocated two time-slots for the (ME42,L2) and (QC,L2) connections and one time-slot for each of the (ME1[1..4],L2) connections. This choice is derived from the need to satisfy the bandwidth requirements computed for the scratch-pad (MHLA) mapping given the constraint of an L1 data memory of 4KB (Section IVD). Note that all connections have for target the L2 memory. The sum of the number of allocated time-slots at the memory side is 8 (8 slots for incoming data and 8 for outgoing data) amounting to 50% of the bandwidth of the NoC and thus sparing some bandwidth for other applications to execute concurrently on the same MP-SoC. NoCTurn achieves very good simulation speeds. With all statistics enabled, the MHLA-GT experiment encodes a VGA frame in 9.1 minutes, corresponding for the DSPs to a total of 317K cycles/s (53K cycles/s per DSP) and for the NoC to a total of 3K (cid:3)its/s (respectively 374K cycles/s and 3.5K (cid:3)its/s without extensive NoC statistics). C. Exploration of Cache Parameters By construction, we have speci(cid:2)ed that the scratch-pad mapping of the application needs an L1 memory of at least 4KB. But, what cache size and con(cid:2)guration should be used for the cache-based mapping? Thanks to the high (cid:3)exibility of our platform simulator, we have run a batch of experiments varying the cache size and its associativity. We have explored a direct-mapped cache, a 2-way and a 4-way set-associative cache. For all associativity settings we ran experiments with cache sizes of 512 bytes, 1KB, 2KB, 4KB, 8KB and 16KB. All caches are con(cid:2)gured to (cid:3)ush dirty lines to L2 using a write-back policy and use a least recently used (LRU) line replacement policy. Figure 7 is a plot of the total number of read and write misses that occurred during the processing of one complete VGA frame. For every of the ME42, ME1[2] and QC processors we show the cache misses for a selection of cache sizes and associativity (2k 4w indicates a 4-way set-associative cache of 2KB and 4k 1w a direct-mapped cache of 4KB). A high number of cache misses indicates a large stall penalty for the DSP (waiting for data to be fetched over the NoC) and is thus inversely proportional to the application performance. We are to select a cache con(cid:2)guration where, for all thread types, the number of misses is low. For this application, the direct-mapped cache performs poorly for sizes under 8KB, whereas there is little performance difference when going from a 2-way set-associative cache to a 4-way one. WR Misses RD Misses ME42 ME1[2] QC 4K, 2-way 4K, 2-way 4K, 2-way s d n a s u o h T 650 600 550 500 450 400 350 300 250 200 150 100 50 0 2k_1w 2k_2w 2k_4w 4k_1w 4k_2w 4k_4w 8k_1w 8k_2w 8k_4w 2k_1w 2k_2w 2k_4w 4k_1w 4k_2w 4k_4w 8k_1w 8k_2w 8k_4w 2k_1w 2k_2w 2k_4w 4k_1w 4k_2w 4k_4w 8k_1w 8k_2w 8k_4w Fig. 7. Cache misses for various cache sizes and cache associativity. Given the results of Figure 7, a 4KB, 2-way set-associative cache seems to be a good performance/hardware-complexity trade-off. The rest of the experiments is thus performed using these parameters for the cache con(cid:2)guration (optimal for this application). D. Compared Performance This section compares the three experiments MHLA-GT, Cache-GT and Cache-BE in terms of execution time. The GT and BE experiments use the optimal con(cid:2)guration: a 2-way set-associative cache of 4KB with write-back and LRU commit and eviction policies. It is important to note that in both experiments not all data is cached. For instance the DSP stack (contains scalars and many intermediate arrays) is mapped to a local scratch-pad memory (or to locked lines of a cache that needs then to be bigger than 4KB). This hybrid solution is already an optimization with respect to a purely cache-based system where a temporary scalar put on the stack could replace a cache-line needed in the next processing cycle. For each of the three experiments and for every processor type (ME42,ME1 and QC) (cid:2)gure 8 presents a stack-plot of the necessary DSP cycles required to complete the processing of one frame. We only present the results for the processor ME1[2] as the other ME1 processors have very similar results (an indication that the difference in latency due to the difference in number of NoC hops is negligible). The more DSP cycles required, the lower the application performance. The total DSP cycles are the sum of the cycles spent in L2 stalls, L1 bank con(cid:3)icts, thread synchronization, pipeline pre-amble and processing cycles. The L2 stalls correspond to the accumulated cache-miss penalties for the cache-based experiments and to the accumulated cycles waiting for blocktransfers for the MHLA experiment (not all data prefetches can be perfectly overlapped with computing time). L1 bank con(cid:3)icts occur when both load/store units of the TI-C62 DSP attempt to access the same bank in the 4-banked L1 data memory. They prove to have a minimal impact on the overall processing cycles so we omit them in discussing the results thereafter. Thread synchronization corresponds to the number of cycles a DSP stalls waiting for the other pipeline stages to complete before processing the next image slice. The preamble is the time the ME1[1..4] and QC processors spend in the (cid:2)rst thread synchronization waiting for the pipeline to (cid:2)llup. Finally the processing cycles (roughly) correspond to the time spent in effective data processing. First notice that, for every experiment (MHLA-GT, CacheBE and Cache-GT) taken in part, the total number of required DSP cycles is almost the same for all processor types2 . This result is very simply explained by the fact that all pipeline stages are synchronized with one another. While obvious, this effect is typically not captured when using traf(cid:2)c generators, regardless of whether using randomly generated data or application traces from single-processor simulations. Overall, the MHLA-GT experiment has the best performance and the best ratio of processing cycles to total number of cycles (Table II). MHLA-GT also displays the lowest ratio of cycles spent in thread synchronization and L2 stall cycles (Table II). The percentage of total cycles spent in preamble is marginally higher for the MHLA-GT experiment but its absolute value is lower. Experiment MHLA-GT Cache-BE Cache-GT L1 1.5 0.9 0.3 L2 14.6 26.1 46.6 Processing 66.8 45.2 17.7 Preamble 5.9 4.9 5.1 Thread Sync 11.2 22.9 30.3 TABLE II. Average percentages of L1 and L2 stalls, processing, preamble and thread synchronization cycles with respect to the total number of cycles. On the contrary, despite the very same time-slot allocation, the Cache-GT displays the worst performance, a factor 3.7 slower than its scratch-pad counterpart. Most of the DSP cycles are spent in L2 misses (46.6 %) and in thread synchronization (30.3 %). Whereas in the MHLA-GT case the bandwidth 2 The processor running the QC thread has a slightly higher number of (total and L2 stall) cycles, but this is an artifact of the application testbench that after processing one frame, fetches the complete reconstructed frame from L2 to verify its correctness by computing a CRC. s n o i l l i M L2 stalls L1 bank conflicts Processing THREAD SYNC PREAMBLE ME42 ME1[2] QC 120 100 80 60 40 20 0 MHLA, GT 4k_2w, BE 4k_2w, GT MHLA, GT 4k_2w, BE 4k_2w, GT MHLA, GT 4k_2w, BE 4k_2w, GT Fig. 8. Processing cycles. Cache (BE and GT) versus MHLA (GT). reserved is suf(cid:2)cient to avoid stalls thanks to the timely data pre-fetches, the same amount of reservation is insuf(cid:2)cient for the Cache-GT experiment. Despite the guarantees provided by the GT connections, the communication becomes a bottleneck causing very high cache-miss penalties. The higher amount of thread synchronization (Figure 8) is a direct consequence of the cache miss penalties on the ME42 processor. For every slice the ME1[1..4] and QC processors stall, meaning that the cache miss penalty on ME42 cannot be hidden in the processing pipeline. Compare now the Cache-BE to the Cache-GT experiment (Figure 8). In terms of overall performance the BE version is a factor 3.1 better than its GT counterpart. The main difference in performance comes from the higher amount of stall cycles spent in waiting for cache misses to L2. Naturally, both applications have strictly the same number of cache misses as they are running the same binary code and the cachecon(cid:2)guration is the same. The difference in the cache miss penalty only comes from the amount of bandwidth available on the NoC in both experiments. In the Cache-BE experiment as much bandwidth as available can be used, so in the current setup, where the QSDPCM is the only application running on the platform, we obtain the best possible performance of the cache mapping. Of course, when sharing the communication resources with another application, the performance of the Cache-BE experience will not only degrade, but it will degrade in a completely non-predictable manner (no composability of applications). Conversely, in the Cache-GT experiment, whereas bandwidth is reserved so that performance is predictable, the maximum bandwidth available is also constrained by the reservations. In this case, the amount of GT bandwidth reserved is insuf(cid:2)cient to absorb the traf(cid:2)c peaks generated by the cache misses. The direct consequence is a very high cache miss penalty explaining the 46.6% of cycles spent in L2 stalls compared to the 26.1% of the Cache-BE experiment (Table II). In the Cache-GT experiment, the ME1[2] processor spends 45.1% of its total cycles in thread synchronization, whereas for the ME42 and QC the amount of thread synchronization is almost negligible. This is an indication that the ME1[1..4] processors have suf(cid:2)cient bandwidth allocated with the one time-slot, but have to wait on ME42 and QC. The communication bottleneck is thus in the bandwidth allocated to these two processors. Naturally, more time-slots can be allocated for ME42 and QC. However, with all slots allocated, the overall performance will be at best as good as the BE one. The more time-slots allocated, the less resources are available to other applications. The careful reader may notice that, especially for the ME42 processor, there are more (cid:148)processing(cid:148) cycles for the CacheGT experiment than for the Cache-BE (Figure 8). This is an artifact of way processing cycles are counted on the ISS for the cache experiment. Indeed, we count the explicit cache (cid:3)ushes (and invalidates) as application code and hence they appear as processing cycles. In ME42, explicit cache (cid:3)ushes occur in a loop when committing the MV2 (motion vector) array to L2. At every cache (cid:3)ush the DSP stalls waiting for the line to be committed With less bandwidth available in the GT experiment, the cache (cid:3)ushes cause the DSP to stall longer, arti(cid:2)cially increasing the (cid:148)processing(cid:148) cycles. Finally, note that the best possible case of cache mapping, Cache-BE, is still a factor 1.23 slower than the scratch-pad MHLA-GT mapping. The difference is explained by the lower number of stall cycles in L2 memory accesses of 14.6% for MHLA-GT versus 26.1% for Cache-BE (Table II). The scratch-pad version is able to hide an important part of the latency of the NoC and the entire memory hierarchy by prefetching data (overlapping communication with computation). A cache also attempts to overlap communication and computation by exploiting spatial locality, but cannot be as effective as a (good) scratch-pad manager. Indeed, application control (cid:3)ow can be analyzed of(cid:3)ine to perform scratch-pad management. E. Compared NoC Traf(cid:2)c We have shown that the cache miss penalty is related to the amount of bandwidth available on the NoC. The less bandwidth available, the higher the miss penalty. Moreover, the cache generates more traf(cid:2)c than the scratch-pad experiment. Indeed, after the (cid:2)rst compulsory misses, all cache misses (reads or writes) that replace (valid) dirty lines require not only the missing line to be fetched from L2, but also the dirty line to be committed to L2. Scratch-pad management can avoid this issue by performing a life-time analysis of the arrays and avoiding to replace the useful ones. An additional reason for a higher traf(cid:2)c on a cache-bridge architecture is the ratio between protocol overhead and amount of useful data sent. The cache bridge protocol needs minimally an overhead of two words per missing line. One word is used for the address of the line to fetch/commit and the other for other (cid:3)ags and a command: read, write, read-answer, write-acknowledge. In a cache the ratio bridge protocol to application data sent over the NoC is (cid:2)xed by the cache-line size. The communication assist bridge has also a protocol overhead (8 words for the most complex 2-dimensional block transfers). However, the amount of data transferred can be proportionally much larger than the protocol overhead, hence reducing the gross amount of traf(cid:2)c NoC load with respect to a cache bridge. ) s / B M ( t u p h g u o r h T t n a t s n I ) s / B M ( t u p h g u o r h T t n a t s n I ) s / B M ( t u p h g u o r h T e g a r e v A 20 10 0 0 120 100 80 60 40 20 0 0 120 100 80 60 40 20 0 0 Cache BE ME42 (a) MHLA GT Cache GT 5e+07 1e+08 1.5e+08 2e+08 MHLA GT Cache BE L2 (b) Cache GT 5e+07 1e+08 1.5e+08 2e+08 Cache BE L2 (c) MHLA GT Cache GT 5e+07 1e+08 1.5e+08 Simulation Time (ns) 2e+08 Fig. 9. Throughput of cache (BE and GT) versus MHLA (GT) on the output network interfaces. Instant-throughput of ME42 (a) and L2 (b), average throughput of L2 (c). Whereas for a cache miss the processor stalls for a duration equivalent to the latency of the miss, in the standpoint of NoC traf(cid:2)c it is dif(cid:2)cult to (cid:2)nd a relevant de(cid:2)nition for the latency of a block-transfer. Indeed, different code portions request different BT sizes and the pre-fetching hides a part of their transport latency. A relevant measure is then the L2 stall cycles and these are directly measureable on the processor (Figure 8). We therefore analyze NoC traf(cid:2)c from a bandwidth point of view. Figures 9(a,b) show the instant throughput at the output of the network interfaces in the ME42, and L2 nodes. The Cache-GT experiment generates traf(cid:2)c for the longest time as the execution of the application takes longer. This shows, now in terms of NoC traf(cid:2)c, that the execution of the CacheGT mapping is slower due to the limited bandwidth (effect already noticed on the L2 stall cycles of Figure 8). The effect of saturation due to the limits imposed by the reservations is best observed in Figure 9(a) (Cache-GT experiment). Note however that the traf(cid:2)c saturates at a level lower than the actual amount of bandwidth reserved, simply because of processor stalls induced by the complex synchronization between the             various pipeline stages (themselves limited by the bandwidth reservations). Note in (cid:2)gure 9(b) that Cache-GT displays numerous peaks of traf(cid:2)c, thereby making a very poor usage of the reserved resources. The two other experiments make a much better usage of the bandwidth, either because the scratchpad mapping smoothes the traf(cid:2)c (MHLA-GT) or because the peaks of cache traf(cid:2)c can be absorbed with all bandwidth available (Cache-BE) (Figure 9(b)). This shows, in terms of NoC traf(cid:2)c, that data-prefetching allows (cid:2)lling much better the guaranteed bandwidth, making a much better usage of the reserved resources. Figures 9(b,c) contrast the instant and average throughput of the output of the L2 memory node. The main contribution to this traf(cid:2)c is thus due to read-responses to requests from the DSPs. It is very interesting to notice that, whereas the MHLAGT has a lower average traf(cid:2)c than the Cache-BE experiment, in terms of instant throughput it generates much higher peaks. Though this may seem at (cid:2)rst counter-intuitive, it is actually explained by the fact that the scratch-pad based mapping performs read requests in much larger blocks than a cacheline. Therefore, every read request on the scratch-pad mapping generates a large burst of data (albeit limited by the GT reservations). On the contrary, a cache requires a larger number of request/response transactions to transfer the same amount of data. This has two consequences: on the one hand, overall more traf(cid:2)c needs to be sent as the overhead of the cachebridge protocol is multiplied by the number of transactions (this explains the high average throughput). On the other hand, the peaks of data generated are limited by the cache-line size thereby explaining the low peaks of instant traf(cid:2)c for the cache BE experiment (the high density of peaks adds up to a higher average throughput though). Note furthermore that an increase in the number of request/response transactions for the same amount of application data transferred has a nonnegligible impact on latency (which explains the particularly poor behavior of the Cache-GT experiment). V I . CONC LU S ION This paper presents the parallelization of a video encoder application and its mapping to two (cid:3)avors of a NoC supported MP-SoC platform. The (cid:2)rst version of the platform uses scratch-pad management to perform data transfers in the memory hierarchy, whereas the second one is cache based. We introduce a novel type of distributed DMA to support the data transfers of the scratch-pad version of the platform. By running the two versions of the application on a custom MP-SoC platform simulator, we quantitatively show that the scratch-pad mapping obtains the best performance results overall. We show that this mapping bene(cid:2)ts from using a guaranteed throughput QoS as its performance can be guaranteed regardless of other application load on the platform. Furthermore, this mapping helps in smoothing the NoC traf(cid:2)c and can be used to ef(cid:2)ciently (cid:2)ll-up the NoC resources reserved. Our results show that, on the contrary, the GT QoS greatly hinders the performance of the cache-based mapping (a factor 3.7 slower than its scratch-pad counterpart). Using a best-effort QoS (on an unloaded NoC), the cache-mapping version is only 20% slower than the scratch-pad mapping. However, the NoC load will impact the BE cache-mapping in an unpredictable way, possibly requiring an application re-write taking into account the other traf(cid:2)c loads, introducing high NRE re-design costs. We demonstrate that, for the transfer of the same amount of application data, a cache-based MP-SoC generates signi(cid:2)cantly more traf(cid:2)c on the NoC than a (correctly used) scratchpad supported MP-SoC. We recommend, where possible, to use scratch-pad management for NoC supported MP-SoCs as they show excellent performance. In addition, they are not sensitive to QoS levels and can thus bene(cid:2)t from guaranteed bandwidth to achieve predictability. Finally, their ef(cid:2)cient usage of the bandwidth reservations leaves headroom for other applications, ensuring composability. ACKNOW L EDGM ENT We would like to thank our colleagues at IMEC for their valuable help. Special attention goes to the contributions of G. Vanmeerbeeck and P. Avasare to the NoCTurn MP-SoC platform simulator and of R. Baert to the application mapping. "
Mesh of Tree - Unifying Mesh and MFPGA for Better Device Performances.,"In this paper we present a new clustered mesh FPGA architecture where each cluster local interconnect is implemented as an MFPGA tree network. Unlike previous clustered mesh architectures, the mesh of tree allows us to consider large clusters sizes (thanks to MFPGA depopulated local interconnect). Experimentation shows that we obtain a reduction of 14% in switches number and 2 times in the placement and routing run time. Furthermore, compared to MFPGA, the mesh of tree achieves full mutability of all MCNC benchmarks since we can easily control both clusters LUTs occupation and mesh channel width","Mesh of Tree: Unifying Mesh and MFPGA for Better Device Performances Zied Marrakchi, Hayder Mrabet, Christian Masson and Habib Mehrez LIP6, Universit ´e Pierre et Marie Curie 4, Place Jussieu, 75252 Paris, France zied.marrakchi@lip6.fr Abstract In this paper we present a new clustered mesh FPGA architecture where each cluster local interconnect is implemented as an MFPGA tree network [6]. Unlike previous clustered mesh architectures, the mesh of tree allows us to consider large clusters sizes (thanks to MFPGA depopulated local interconnect). Experimentation shows that we obtain a reduction of 14% in switches number and 2 times in the placement and routing run time. Furthermore, compared to MFPGA, the mesh of tree achieves full routability of all MCNC benchmarks since we can easily control both clusters LUTs occupation and mesh channel width. 1. Introduction Modern Mesh FPGA architectures are based on a clustered architecture, where a number of lookup tables (LUTs) are grouped together to act as the conﬁgurable logic block. The motivation of using clusters is manifold: to reduce area, to reduce critical-path delay, and to reduce CAD tool runtime [2] [3]. This trend is followed by some FPGAs from Xilinx (the Virtex and Spartan families) and Altera (the Stratix and Cyclone families). All of these FPGAs are based on clusters of 4-input lookup tables. In some FPGAs, such as Altera’s APEX family, these internal cluster connections are fully populated or fully connected. This is equivalent to employing a full crossbar: a crosspoint switch exists at intersection point of every LUT input and every cluster input or feedback connection. Such a high degree of connectivity makes routing easier, but it has signiﬁcant area overhead. This penality is increasing especially in the case of architectures with high clusters sizes. Our previous studies of MFPGA architecture [6] showed that it leads to better logic density than mesh especially for small benchmark circuits. We have improved the placement strategy to enhance routability and to target benchmark circuits with higher occupation than those considered in [8]. Despite our efforts we found that the proposed architecture Disjoint S Block M S B M S B M S B M S B LB LB LB LB Figure 1. Mesh of Tree cannot deal with circuits exceeding 80% of logic blocks occupation. To deal with both architectures drawbacks we propose in this work to use the MFPGA sparse interconnect as an alternative switch matrix inside the Mesh clusters. Since clusters size is limited, the MFPGA interconnect topology seems to be an interesting local interconnect, which allows us to control the logic occupation inside each cluster. In this paper section 2 presents a description of the Mesh of Tree architecture. First it presents the mesh interconnect level and then the local cluster interconnect which is similar to the MFPGA connecting networks. Section 3 describes the conﬁguration ﬂow to implement a netlist on the presented architecture. It insists on challenges for performing place-and-route inside clusters (MFPGA). In the experimentation section, based on MCNC benchmark, we compare the Mesh of Tree architecture to the common clustered mesh in term of switches number requirement. 2 Mesh of MFPGA Architecture The architecture that we propose has a mesh of tree interconnect topology. It starts with a mesh of nodes and builds CLB CLB CLB CLB C C C C S C S C S C C S C S C S C CLB CLB CLB CLB C C C CLB CLB CLB CLB C C C L : Logic Block C : Connection Box S : Switch Box CLB CLB CLB CLB C C C C S C S C S C W Figure 2. Mesh architecture a separate hierarchical network along each row and column cluster. The resulting network corresponds to a mesh of clusters where each local interconnect is equivalent to a MFPGA interconnect. Figure 1 shows a cluster local interconnect (here with a simple one level MFPGA topology) and how it is connected to other clusters. We can consider that netlist implementation on this architecture can be run in two stages: - Mesh stage: In this stage clusters are considered as black boxes with i inputs and j outputs. The initial netlist is partitioned into N independent sub-netlists where N corresponds to the number of clusters of the mesh architecture. - MFPGA stage: Each one of the N sub-netlists is mapped seperately in a cluster. Since cluster local interconnect is similar to MFPGA hierarchical interconnect, clusters will be referred as MFPGA. Then the clusters netlist is placed and routed on the mesh using VPR. In the following, both architecture stages and their corresponding tools will be described. 2.1 Mesh routing interconnect As shown in ﬁgure 2 mesh-based architecture is composed of clustered logic blocks (CLB), switch blocks (S), connection blocks (C), and I/O blocks. Interconnection between clusters is formed by the C and S blocks, comprising the horizontal and vertical routing channels. The C block is the region where the CLB input and output pins connect to the routing channels. The S block is where connections are made between the horizontal and vertical routing channels, allowing nets to turn corners or extend farther along the channel. Each routing channel contains W parallel tracks of wires, where W is called the channel width. The same width is used for all channels. The cluster inputs are connections from the external routing, carrying signals from other clusters into this one. MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB LB LB LB LB LB LB LB LB LB LB LB LB LB LB LB LB e v e L l 1 e v e L l 2 Figure 3. Downward Network 2.2 MFPGA routing interconnect The MFPGA hierarchical interconnect were described previously in [6]. In the following we present a more detailed description which will be very useful to extract architecture routing constraints and to present challenges on performing place-and-route. MFPGA contains LUT-based logic blocks and two unidirectional connecting networks. Logic Blocks (LBs) and interconnect are organised into levels. Let nb(cid:1) denote the number of levels of a given architecture. In each level we have a set of clusters, and C denotes the set of clusters in all levels. A cluster with index c belonging to level (cid:1) is noted by cluster((cid:1), c). Each cluster((cid:1), c) where (cid:1) ≥ 1 contains a set of MSBs (Mini Switch Boxes) and 4 sub-clusters. Sub-clusters of cluster((cid:1), c) are cluster((cid:1) − 1, 4c + i) where i ∈ {0, 1, 2, 3}. The MSB with index m belonging to cluster((cid:1), c) is denoted M SB ((cid:1), c, m) where m ∈ {0, . . . , 4(cid:1) − 1}. Each MSB contains 4 inputs driven by the upper level and 1 feedback coming from a leaf output pin. Each cluster in level (cid:1) contains nbM SB ((cid:1)) = 4(cid:1) . Each cluster in level 0 is denoted cluster(0, c) or leaf cluster(c) and corresponds to the Logic Block (LB) and contains 4 inputs, 1 output, no MSBs and no sub-cluster. Each cluster((cid:1), c) where (cid:1) < nb(cid:1) − 1 has an owner in level (cid:1)(cid:1) where (cid:1)(cid:1) > (cid:1) denoted cluster((cid:1)(cid:1) , c ÷ 4((cid:1)(cid:1)−(cid:1)) ). We deﬁne for each cluster((cid:1), c) a position inside its owner in level (cid:1) + 1 (direct owner) by the following function: pos : C −→ {0, 1, 2, 3} cluster((cid:1), c) (cid:4)−→ c mod 4 Two clusters belonging to level (cid:1) and having the same owner at level (cid:1) + 1 have two different positions. To get the cluster owner in level (cid:1)(cid:1) deﬁne the function: of cluster((cid:1), c) ((cid:1) < (cid:1)(cid:1) ≤ nb(cid:1) − 1) we owner : C × IN −→ C (cluster((cid:1), c), (cid:1)(cid:1)) (cid:4)−→ cluster((cid:1)(cid:1) , c ÷ 4(cid:1)(cid:1)−(cid:1))     C l u r e t s l e v e l 0 MSB MSB MSB MSB C l u r e t s l e v e l 2 (a) C l u r e t s l e v e l 1 MSB MSB MSB MSB MSB MSB MSB MSB ~ ~ ~ ~ ~ ~ ~ ~ C l u r e t s l e v e l 0 MSB MSB MSB MSB C l u r e t s l e v e l 2 (b) C l u r e t s l e v e l 1 MSB MSB MSB MSB MSB MSB MSB MSB ~ ~ ~ ~ ~ ~ ~ ~ LB LB LB LB LB LB A B Figure 4. Upward Network 2.2.1 Downward Network Figure 3 shows a sparse downward network based on unidirectional MSBs. The downward interconnect topology is similar to the butterﬂy fat tree. Each MSB of a cluster((cid:1), c) where (cid:1) > 1 is connected to each sub-cluster in one and only one input pin. We say an M SB ((cid:1)(cid:1) , c(cid:1) , m(cid:1) ) is the successor of an M SB ((cid:1), c, m) where 0 < (cid:1)(cid:1) < (cid:1) if there is a downward directed path from M SB ((cid:1), c, m) to M SB ((cid:1)(cid:1) , c(cid:1) , m(cid:1) ). The path between an MSB and its successor is unique. We deﬁne the function: M odi : IN −→ IN Thus each M SB ((cid:1), c, m) has a successor in each subcluster belonging to level (cid:1)(cid:1) M SB ((cid:1)(cid:1) , c(cid:1) , m(cid:1) ) where 0 < (cid:1)(cid:1) < (cid:1), with: m (cid:4)−→ m mod nbM SB (i) m(cid:1) = M od(cid:1)(cid:1) ◦ · · · ◦ M od(cid:1)−1 (m) (1) 2.2.2 Upward Network We propose to connect the output signals of leaf clusters to speciﬁc MSBs of upper levels. Thus for each logic block output, we deﬁne a list of feedbacks. Each one enables the output to reach an MSB in a particular level. The way feedbacks are distributed has an important impact on the structure routability. Connecting an output of a leaf cluster to MSBs with different indexes increases the number of paths from a source to a destination. This speciﬁc distribution is described in ﬁgure 4-(a). Figure 4-(b) shows how the cluster leaf ’A’ output can reach the cluster leaf ’B’ inputs using different paths. Each leaf cluster cluster(0, c) is connected to one and only one M SB ((cid:1), c(cid:1) , m) in level (cid:1) > 0. c(cid:1) is the index of the owner of cluster(0,c) in level (cid:1): c(cid:1) = c ÷ 4(cid:1) ; m is given by: (cid:1) m = (pos(cluster(0, c)) + (cid:1) − 1) mod 4+ (cid:2)(cid:1)−1 =1 pos(owner(cluster(0, c), )) × nbM SB () (2) MSB MSB MSB MSB MSB MSB MSB MSB Local Interconnect Local Interconnect In In LB LB LB LB Out Out LB LB LB LB Out Out Figure 5. Connection with outside MFPGA  Partitioning MFPGA  Partitioning Mesh Partitioning Main Netlist MFPGA  Partitioning MFPGA  Partitioning Detailed Placement Detailed Placement Detailed Placement Detailed Placement Routing Routing Routing Routing Pins Reordering VPR:Place & Route MFPGA flow Figure 6. Mesh of Tree conﬁguration ﬂow 2.2.3 Connection with outside Output pads are clustered with the logic blocks at level0 . The number of output pads per cluster can be varied to obtain the best design ﬁt. We use a local interconnect between the logic block outputs and the output pads. Input pads are connected directly to MSBs of the highest level. In this way each Input pad can reach all logic blocks. To add ﬂexibility, as shown in ﬁgure 5, an input pad can be connected to more than one MSB. This enables pads to reach logic blocks from different paths and in different pins. 3 Conﬁguration ﬂow In the following we present the different steps to implement a netlist on the Mesh of Tree architecture. 3.1 Mesh partitioning The purpose of the partitioning step is to distribute netlist instances between mesh architecture clusters (subdomains) in order to reduce external communication (cut) and congestion. Since we have a balanced mesh interconnect (the same width is used for all channels), it is both mandatory to match cluster I/O resources and worthwhile to spread the congestion over all the interconnect. Despite the success of multilevel algorithms [5] in producing partitionings in                         which the cut is minimized, this cut is not uniformly distributed across the different subdomains. That is, the number of hyperedges that are being cut by a particular subdomain (referred to as the Subdomain Degree) may be significantly higher than that of other subdomains. Therefore it is of great importance to produce partitioning solutions that minimize the cut but also minimize the Maximum Subdomain Degree (MSD). To address this problem we developed a multi-objective partitioning tool, in which the MSD is the highest priority objective and the cut is the second highest. We implemented a solution similar to the direct multi-phase reﬁnement presented in [10]. Here, using hMetis tool, we ﬁrst generate a partitioning solution with the cut as an objective, next we apply a multi-phase multi-objective reﬁnement with MSD as the highest priority objective. After main netlist partitioning, we obtain a clusters netlist and N subnetlists each one describing how LBs must be connected inside each cluster. In ﬁgure 6 the main netlist is partitioned into 4 sub-netlists targeting a clustered mesh architecture with 4 clusters. Each sub-netlist is mapped separately using the MFPGA conﬁguration ﬂow. Then, after a pin reordering of the inter-clusters netlist (in order to match pin assignemnts done at each MFPGA level), this netlist is mapped using VPR place and routing tool. 3.2 MFPGA placement The MFPGA placement problem can be stated as assigning to each netlist cell a logic block (leaf) in the MFPGA architecture. The way how cells are distributed has an important impact on routability. In fact once cells are placed, the router tries to ﬁnd a path to connect a source LB (cluster leaf) to its destinations LBs (cluster leaf) using architecture resources. Thanks to the interconnect predictibility provided by this MFPGA architecture we can introduce, in the placement phase, some conditions to limit later conﬂicts in the routing phase. 3.2.1 Conﬂict conditions Deﬁnition 1 There is a resource conﬂict problem in level (cid:1) if 2 leaf clusters (or more), such as cluster(0, c) and cluster(0, c(cid:1)) reach a cluster((cid:1), c(cid:1)(cid:1) ) on the same pin p. Property 1 The owner in level (cid:1) + 1 of cluster((cid:1), c(cid:1)(cid:1)) has one and only one M SB ((cid:1) + 1, c(cid:1)(cid:1) ÷ 4, m) which can reach this cluster((cid:1), c(cid:1)(cid:1) ) on pin p. Deﬁnition 2 Referring to the previous property, the deﬁnition 1 can be stated as: There is a resource conﬂict problem in level (cid:1) if 2 leaf clusters (or more) such as cluster(0, c) and cluster(0, c(cid:1)) try to reach a cluster((cid:1), c(cid:1)(cid:1) ) and have both already reached its owner cluster((cid:1) + 1, c(cid:1)(cid:1) ÷ 4) at the same M SB ((cid:1) + 1, c(cid:1)(cid:1) ÷ 4, m).      Cell4 Cell0 Cluster(0,3) Cluster(0,9) Cell1 Cluster(0,5) Cell3 Cell2 Netlist Cluster(0,18) Cluster(0,10) Placed Netlist Figure 8. Netlist to route From deﬁnition 2, we can detect a resource conﬂict by ﬁnding 2 leaf clusters reaching the owner cluster of a destination in the same MSB. We consider that cluster(0, c) reaches cluster((cid:1), c(cid:1)(cid:1) ) in M SB ((cid:1), c(cid:1)(cid:1) , m) using the level (cid:1)up , and that cluster(0, c(cid:1)) reaches the same cluster destination in M SB ((cid:1), c(cid:1)(cid:1) , m(cid:1) ) using level (cid:1)(cid:1) ¿From equation (2) and (1) in this order we get: up . (cid:2)(cid:1)−1 (cid:2)(cid:1)−1 m = (pos(cluster(0, c)) + (cid:1)up − 1) mod 4 + =1 pos(owner(cluster(0, c), )) × nbM SB () m(cid:1) = (pos(cluster(0, c(cid:1))) + (cid:1)(cid:1) =1 pos(owner(cluster(0, c(cid:1) ), )) × nbM SB () up − 1) mod 4 + thus, (3) m = m(cid:1) (cid:7) (pos(cluster(0, c)) + (cid:1)up )[4] = (pos(cluster(0, c(cid:1))) + (cid:1)(cid:1) pos(owner(cluster(0, c), )) = pos(owner(cluster(0, c(cid:1) ), )) up )[4] ∀ ∈ {1, . . . , (cid:1) − 1} (4) P roof : Equations (3) correspond to the decomposition of m and m(cid:1) in the base 4 because: - 0 < (pos(cluster(0, c)) + (cid:1)up )[4] < 4 - 0 < pos(owner(cluster(0, c), )) < 4 ∀j, c - nbM SB () = 4 Therefore we obtain results presented in equation (4). Lemma 1 We say 2 leaf clusters cluster(0, c) and cluster(0, c(cid:1)) are in conﬂict to drive a common destination cluster((cid:1), c(cid:1)(cid:1)) if and only if: pos(cluster(0, c)) − pos(cluster(0, c(cid:1))) = ((cid:1)(cid:1) pos(owner(cluster(0, c), )) = pos(owner(cluster(0, c(cid:1) ), )) ∀ ∈ {1, . . . , (cid:1)} up − (cid:1)up )[4] 3.2.2 Placement example We refer to the netlist presented in ﬁgure 8. We propose to place cells as shown in ﬁgure 7. In this example cell0, cell1, cell2 and cell3 are placed respectively in cluster(0, 9), cluster(0, 5), cluster(0, 10), A B C D A B C D B C D A 3 3 3 3 2 2 2 0 2 1 1 0 1 0 1 B LB D 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 C A A B C D 0 0 0 0 00 0 0 MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB MSB Cluster(1,0) Cluster(2,0) Cluster(3,0) Cluster(1,1) Cluster(1,2) Cluster(1,3) Cluster(1,4) Cluster(2,1) (1,2,0) MSB MSB MSB (2,0,0) MSB MSB MSB (1,1,0) MSB MSB MSB (3,0,16) MSB ~ ~ ~ ~ LB LB LB LB LB LB LB LB LB Cell0 LB LB LB Cell1 Cell2 LB LB Cell4 Cell3 Figure 7. Detailed placement example cluster(0, 18), and cluster(0, 3). Referring to the clustered netlist, cluster(0, 18) and cluster(0, 3) have a common destination cluster(0, 5) at level 0. cluster(1, 2) and cluster(1, 1) are also two common destinations at level 1. We check now referring to Lemma 1 whether there is a resource conﬂict to connect both sources to the three destinations. We propose to use the lowest possible level to connect a source to its destinations. To reach cluster(0, 5), cluster(0, 3) must go up to level 2 ((cid:1)up = 2) and cluster(0, 18) to level 3 ((cid:1)(cid:1) up = 3). Since pos(cluster(0, 3)) = 3 and pos(cluster(0, 18)) = 2, we get pos(cluster(0, 3))−pos(cluster(0, 18)) = (cid:1)(cid:1) (cid:1)up . Thus the condition of lemma 1 is satisﬁed and there is a resource conﬂict in level 0 to reach cluster(0, 5). The second common destination is cluster(1, 2). To reach this destination, cluster(0, 3) must be connected up to level 2 ((cid:1)up = 2) and cluster(0, 18) to level 3 ((cid:1)(cid:1) Since pos(cluster(0, 3)) = 3 and pos(cluster(0, 18)) = 2, we get pos(cluster(0, 3)) − pos(cluster(0, 18)) = up − (cid:1)up . Thus the ﬁrst condition in lemma 1 is satisﬁed. We check now the second condition of lemma 1 since destination cluster(1, 2) belongs to level 1 ((cid:1) > 0). We have owner(cluster(0, 3), 1) = cluster(1, 0) and up − up = 3). (cid:1)(cid:1) owner(cluster(0, 18), 1) = cluster(1, 4) . Since pos(cluster(1, 0)) = pos(cluster(1, 4)) = 0 the second condition of lemma 1 is veriﬁed too. Thus there is a resource conﬂict at level 1 to connect cluster(0, 3) and cluster(0, 18) to cluster(1, 2). We have the same problem with the third common destination cluster(1, 1). The routing solution of the placed netlist using the lowest levels is presented in ﬁgure 7. The dashed arrows present the resource conﬂicts. To prevent resource conﬂicts we propose: - To change positions of the leaf cluster sources. - To change positions of the sources owners in level 1. When we try to resolve a congestion problem to reach a destination we can introduce unexpected problems to reach other destinations. The aim of the following sections is to present a method to model all placement constraints and to perform the optimal positions assignement. 3.2.3 Partitioning The way in which we distribute logic blocks between MFPGA clusters has an important impact on routing congestion reduction. Based on the upward interconnect speciﬁcity we notice that the number of different paths to connect a source to a destination depends on their enclosing clusters. If they are packed in the same cluster, the source can use more levels to reach its destination and therefore more paths. From this remark we can consider that the netlist cut reduction is an important factor for routability improvement. A second partitioning objective is deduced from routability conditions presented in Lemma 1. Let us consider 2 leaf sources cluster(0, c) and cluster(0, c(cid:1)) driving a common destination cluster(0, c(cid:1)(cid:1)). If we pack both sources in the same cluster we obtain on the one hand (cid:1)up − (cid:1)(cid:1) is the lowest used level to reach the destination). On the other hand we get pos(cluster(0, c)) − pos(cluster(0, c(cid:1))) (cid:9)= 0. In this case referring to Lemma 1, the conﬂict condition is not veriﬁed and no resource conﬂict occurs. To include this objective in the clustering technique, we propose to construct a Cells Constraints Graph (CCG). The CCG denoted as Gn = (V , En ) consists of a set of vertices and weighted edges derived from the netlist. An edge is established between two vertices when they drive the same destination cluster, which are called adjacent. Each edge contains a weight equal to the number of common destinations between two adjacent vertices. Using only this graph up = 0 ((cid:1)up C0 C1 C2 C3 C4 C5 Netlist hypergraph 1 C0 C1 2 1 C4 1 C2 Cell Constraints Graph 1 2 C0 C1 1 C2 1 1 1 1 C3 1 C4 C5 Constrained Hypergraph Figure 9. CCH: Cell Constraints Hypergraph shift = 1 Level = 0 shift = 1 Level = 1 Cell 4 Cell 3 shift = 3 Level = 0 shift = 3 Level = 1 Figure 10. ACCG: Advanced Cell Constraints Graph in the partitioning weakens the obtained clusters netlist results in term of external communication. To take this in consideration, we propose, as presented in ﬁgure 9, to generate from the initial netlist hypergraph and the CCG a new Constrained Cells Hypergraph CCH. In this hypergraph, vertices are cells (as in the netlist) and it contains all hyperedges of the netlist and all edges of the CCG. This constrained hypergraph is partitioned by hMetis and objectives priority will be deﬁned by hyperedges weights. We propose to use a top-down patitioning approach (global connectivity informations). We ﬁrst construct clusters of the top level and then each cluster is partitioned into subclusters. This is done until the bottom of the hierarchy is reached. To run partitioning we used hMetis [5] since it generates a good solution in a short time due to its multiphase reﬁnement approach. 3.2.4 Detailed placement If during detailed placement we take lemma 1 in account, signiﬁcant gain can be obtained in term of routability and congestion reduction. For this purpose we introduce the Advanced Cells Constraints Graph (ACCG) which is associated to a given a multilevel clustered netlist (previous section) and a placement problem. Advanced Cell Constraints Graph: We can say that an ACCG is a CCG that contains extra cells partitioning informations required to check conditions of Lemma 1. An ACCG denoted as Gn = (V , En ) consists of a set of vertices and directed edges derived from the netlist and the way its cells are partitioned between clusters in each level, where each vertex corresponds to a cell of the netlist. A pair of opposite directed edges is established between two vertices when they drive the same destination cluster (located at any level), which are then called adjacent. To be able to verify conditions proposed in lemma 1, we need to add some informations to the constraints graph. Those informations are stored in each directed edge connecting two adjacent vertices as a list of pairs (shift, level), featuring: - The forbidden shift between adjacent vertices positions. shif t = ((cid:1)up − (cid:1)(cid:1) up ) mod 4. - The level where is located the common destination cluster. It is worthwhile to use the lowest level feedback link to connect a source to its destination, since it has an important impact on delay reduction. That’s why when we construct the ACCG as described in ﬁgure 11, (cid:1)up corresponds to the lowest level where the source has to go up to reach its destination. Reducing the conﬂict between sources using the lowest level is beneﬁcial for the ﬁrst routing iteration. In fact, as explained in [8] we use an iterative rip-up routing algorithm based on the congestion negotiation. We assign an adjustable cost to each feedback. A lower level induces lower cost; consequently in the ﬁrst routing iteration, signals will be routed using the lowest levels. Using the lowest levels to construct the ACCG has two advatages: - Fewer switches will be crossed to route signals. - A good initial solution for the iterative router exists: ﬁrst iteration is run with the least number of resource conﬂicts. Figure 10 presents the Advanced Cell Constraints Graph (*) for each leaf cluster cl for each level l for each receiver rc of cl in level l for each leaf driver dr of rc //cl and dr both drive rc if rc has no common subreceiver of dr and cl if No edge between cl and dr create edge e between cl and dr end if level = GetLevel(rc) shift = ShiftCompute(cl,dr,rc) append pair(shift,level) to e end if end for for for for Figure 11. ACCG construction constructed from the placed netlist in ﬁgure 8. In line (*) incrementally in the sequel. To check whether there is a resource conﬂict (*), we must check conditions of lemma 1. To do this we need informations about the ﬁrst source position, the second source position (adjacent), the forbidden shift and the destination level. All those informations are provided by the ACCG. To ﬁnd the best detailed placement combination we propose to use an adaptive simulated annealing algorithm [7] [1]. In this algorithm the operating parameters are controled using statistical techniques. Moves are randomly applied to the conﬁguration and consist in assigning new positions. First we choose randomly an element to be moved, it can be a basic cell or a cluster of cells (located at any level). Second we choose randomly the new position inside the direct cluster owner, if it is occupied we swap both elements positions. The cost function is updated incrementally by evaluating the incremental cost of the moved vertices and their adjacents. Moving a cells cluster is important referring to lemma 1 and can lead to cost reduction. In this case, since the ACCG vertices correspond only to basic elements, we update the cost by visiting all basic elements of the moved cluster and their adjacents. We adopt a hard windowing move restriction approach. As presented in ﬁgure 12, a cell or a cluster can only move inside its direct owner. This restriction is important to keep constant the partitioning result obtained by the tool described in section 5. In addition, by respecting this restriction, we do not have to update the ACCG since the common receivers and the levels to use to reach them always remain the same. This yields important run time reduction for the cost updating phase. 3.2.5 Logic replication The idea behind logic replication is that by making copies of one or more logic cells, one can maintain the logical behavior of a netlist while, hopefully, enabling additional optimization. Consider a logic cell a and suppose that we have created a duplicate cell a(cid:1) . The cell a(cid:1) takes precisely the same inputs as a and produces exactly the same boolean function of those inputs as its output. In this situation, the pins in the circuit that need to receive this signal may now obtain it from either the output of a or a(cid:1) . This adds freedom and enhances routability since it enables to reach destination cells using additional routing resources. In addition if we place the duplicate logic block a(cid:1) inside the super cluster (owner) containing the original logic block a, we will not add routing congestion to connect a(cid:1) inputs. This means that logic replication must be done after original logic blocks partitioning. Thus we have to estimate which are logic blocks that need to be duplicated before Figure 12. Moves range limiters of the algorithm, we test whether the common receiver rc has already a sub-cluster (slave) which is also a common receiver of cl and dr. This veriﬁcation is important to avoid computing many times the same conﬂict to reach a destination. In fact the conﬂict can occur when reaching the destination or its owners. In the netlist described in section 3.2, we have a conﬂict driving cluster(0, 5) and its owner cluster(1, 1). In the routing phase this conﬂict will be considered only once. That is why in the generated ACCG we append in the edge only the couple (1, 0) corresponding to destination cluster(0, 5) and the couple (1, 1) corresponding to destination cluster(1, 2), but we do not append the couple (1, 1) corresponding to destination cluster(1, 1). In addition referring to lemma 1, If there is no conﬂict to reach cluster(l, c), there will be no conﬂict to reach any one of its owners. Simulated Annealing technique: A detailed placement consists in assigning a position for each cell and each cluster of cells inside its owner. The objective is to reduce the number of resource conﬂicts. To compute this number we take each vertice in the ACCG and we check whether conditions of lemma 1 are veriﬁed, if such the global cost function is incremented by 1. Computing this cost for a speciﬁc detailed placement is given by the pseudo-code of ﬁgure 13. The cost is updated cost = 0 for each vertice v mark vertice v visited for each adjacent vertice adj of v if adj was not visited for each couple (level,shift) if conﬂict(v,adj,shift,level) cost++ end if end for end if end for end for (*) Figure 13. Incremental cost computing partitioning to reserve vacant positions and depopulate the containing clusters. To consider this we use the CCG graph to attribute weights to logic blocks. Logic block weight is equal to the number of its adjacent vertices in the CCG. Vertices weights are added to the CCH hypergraph and the partitioner will distribute vertices and controle clusters population based on these informations. Once logic blocks are partitioned between clusters, we run the detailed placement. After the last placement iteration we deﬁne logic blocks arrangement inside clusters and we evaluate the number of conﬂicts that will occur in the ﬁrst routing iteration. Using the ACCG we can easily identify logic blocks (drivers) leading to these conﬂicts and duplicate them inside their clusters owners. When the routing iterations are progressing, the router can use the duplicate logic blocks to reach some destinations. 3.3 MFPGA routing Once netlist logic blocks are placed, the router tries to connect signals using MFPGA interconnect resources. As explained in [8], the router is an adaptation of pathﬁnder [9]. Since the placement objective is to minimize resources conﬂict number in the ﬁrst routing iteration, the router will start with a good initial solution (the ﬁrst iteration consists in conecting a source to a destination using the lowest level). In the next iterations, the router will try to resolve conﬂicts using higher levels (congestion negotiation) 3.4 Pins reordering As we do not have a full crossbar inside clusters (MFPGA topology), inputs and outputs cannot be considered as logically equivalent. As presented in ﬁgure 6, each subnetlist is placed and routed seprately. In the detailed placement phase, sub-netlist inputs/outputs are assigned to speciﬁc cluster inputs and outputs pins (MFPGA input pads placement). This new ordering must be back annotated in the clusters interfaces of the inter-clusters netlist. The pins ordering constraint is very penalizing in the clusters netlist routing (see next section). To alleviate the effect of this penality we provide groups of equivalent cluster input pins. Input pins (MFPGA Input pads) are equivalent if they drive the same MSBs. For example if we consider the cluster presented in ﬁgure 1 we notice that it contains four groups of logically equivalent input pins. Each group is composed of 3 inputs connected to the same MSB. To enhance interclusters routability we distribute the equivalent pins over the four cluster sides. 3.5 Mesh placement and routing To place and route the clusters netlist we use VPR tool [4]. After clusters placement, VPR achieves routing with the lowest channel width. Having some equivalent cluster pins gives more ﬂexibility to the router and has an important impact to reduce channel routing width. 4 Experimental results To evaluate architectures and tools performances, we placed and routed some of the largest MCNC benchmark circuits. 4.1 MFPGA routability evaluation First we evaluated the efﬁciency of MFPGA ﬂow and especially placement phase. It is a real challenge to perform place-and-route on MFPGA especially with high LUTs occupation. We want to check whether the new iterative detailed placement technique (presented in section 3.2.4) can improve routability compared to the constructive technique presented in [8]. In table 1 we present the effect of both placement techniques (iterative and constructive) on circuits routability. We notice that constructive technique is inefﬁcient with circuits with high occupation. It fails with circuits with occupation more than 50%. The iterative technique is more efﬁcient and can deal with circuits having until 80% of occupation. To deal with circuits having high occupation is also important in the case of Mesh of Tree architecture since it allows to pack a higher number of LUTs into each cluster and consequently to reduce external communication and the number of clusters in the mesh level. This has an important impact on area reduction. 4.2 Mesh of Tree vs Mesh We use the same benchmark circuits to compare switches requirement between Mesh of Tree and clustered Mesh architectures. The clustered mesh architecture uses a uniform routing with single-length segments and a disjoint switch box. Each cluster logic block contains four 4-LUTs. 10 inputs and 4 outputs are distributed over the cluster sides. LUTs pins are connected to cluster pins using a full local crossbar. For connection blocks (C), Fc = O.5 and Fcout = 0.25 are chosen. These switch density choices are made to be consistent with previous work [2]. We use t-vpack to construct clusters and the channel minimizing router VPR 4.3 to route the mesh. VPR chooses the optimal size as well as the optimal channel width to place and route benchmark circuits. Concerning the mesh of Tree architecture, mesh interconnect level is uniform with singlelength segments and disjoint switch box. Each cluster contains 256 4-LUTs. The choice of the optimal number of clusters inputs/outputs is not obvious and depends on the complexity of netlist to implement. When we examine the largest MCNC benchmark we notice that they can be divided into two categories: those with low interconnect utilization (such as s38417, s38584.1 and bigkey) and those with high interconnect utilization (such as spla and pdc). Based on this remark we propose to target two different Mesh of Tree architectures: - Architecture with clusters having 64 inputs and 64 outputs to implement circuits with low interconnect utilization. - Architecture with clusters having 128 inputs and 64 outputs to implement circuits with high interconnect utilization. In both cases input pins are divided into groups of 4 pins. In each group those four input pins are logically equivalent (connected to the same MSBs). To give more ﬂexibility to the external router, the four equivalent pins are distributed over the 4 cluster sides. Since we do not have a full equivalence between input pins we use an external interconnect with full ﬂexibility: Fc = 1. As shown in table 2 we placed and routed the same benchmark circuits on both architectures. All circuits were totaly routed. In fact in the case of the Mesh of Tree architecture, we can control clusters occupation with the mesh partitioner (section 3.1). Concerning the external mesh interconnect, VPR chooses the minimum channel width. In both cases we have computed the required switches number by each architecture. In the case of the Mesh of Tree we obtain better density and the number of the switches is reduced by 14% compared to the mesh. As cluster size is limited (< 256), the MFPGA ﬂow run time is very short (about 10 s). We can consider that this is the required time to place and route logic blocks inside clusters, since this can be done separately (in parallel). In addition, since the clusters size of the Mesh of Tree is 64 times bigger than in the case of clustered Mesh, the netlist that must be placed and routed by VPR is smaller in term of instances and nets number. This explains why the run time in the case of the Mesh of Tree was two times reduced. 4.3 Partitioning ob jectives comparison As explained in section 3.1, the mesh partitioning strategy has an important impact on inter-clusters routing phase. In table 3 we present a comparison between a partitioning minimizing the cut and a partitioning which considers both the cut and the Max Subdomain Degree (MSD). In the last case we notice that the MSD is reduced by 9% and the CUT is increased by 1.5%. It means that the external communication is slightly increased and the congestion is better distributed. Consequently we obtain a 4% reduction in the mesh routing channel width. 5 Conclusion In this work we have shown that, for MFPGA, by focusing most of the effort on the placement phase, the predictability of the hierarchical interconnect allows us to obtain good routability results despite depopulated routing resources. In fact we succeeded to route netlists up to 80% LUTs occupation (and less than 2K LUTs). This has led us to propose an architecture that uniﬁes the merits of MFPGAs and Mesh architectures. This architecture is called Mesh of Tree and ensures routability of all MCNCs benchmarks (thanks to the mesh channel width ﬂexibility) while being 14% smaller in term of switches number. Notice that solving independently intra-clusters (MFPGA level) and inter-clusters (Mesh level) is penalizing. Indeed, performances can be signiﬁcantly improved by a better interaction between these two designs ﬂows (at the pins assignement level). Therefore, we can consider our results as a lower bound of the the quality of the proposed architecture. The Mesh of tree can be very promising architecture especially when we target very large netlists implementation (> 20K LUTs). "
Design Technologies for Networks on Chips.,"Summary form only given. Networks on chips provide structured solutions for fast and low-power interconnect, but need to be adapted to the performance and physical design requirements of the host chip. Efficient and optimal design of such networks is an error-prone, tedious and time-consuming task. Thus, NoCs require design environments in which the network can be instantiated and tuned automatically, and where the designer steers the design by providing high level models of requirements and constraints. This talk would survey the state of the art in design automation for NoCs","NOCS 2007  Keynote 2  Design Technologies for Networks on Chips  Giovanni De Micheli  EPFL  Abstract:      Networks on Chips provide structured solutions for fast and low-power interconnect, but need to be  adapted to the performance and physical design requirements of the host chip. Efficient and optimal  design of such networks is an error-prone, tedious and time-consuming task. Thus, NoCs require design  environments in which the network can be instantiated and tuned automatically, and where the  designer steers the design by providing high level models of requirements and constraints. This talk will  survey the state of the art in design automation for NoCs.  Bio:     Giovanni De Micheli is Professor and Director of the Integrated Systems Centre at EPF Lausanne,  Switzerland, and President of the Scientific Committee of CSEM, Neuchatel, Switzerland. Previously, he  was Professor of Electrical Engineering at Stanford University. He holds a Nuclear Engineer degree  (Politecnico di Milano, 1979), a M.S. and a Ph.D. degree in Electrical Engineering and Computer Science  (University of California at Berkeley, 1980 and 1983).      His research interests include several aspects of design technologies for integrated circuits and  systems, such as synthesis, hw/sw codesign and  low-power design, as well as systems on  heterogeneous platforms including electrical, optical, micromechanical and biological components. He  is author of: Synthesis and Optimization of Digital Circuits, McGraw-Hill, 1994, co-author and/or coeditor of six other books and of over 300 technical articles. He is, or has been, member of the technical  advisory board of several companies, including Magma Design Automation, Coware, Aplus Design  Technologies, Ambit Design Systems and STMicroelectronics.      Prof. De Micheli is the recipient of the 2003 IEEE Emanuel Piore Award for contributions to computeraided synthesis of digital systems. He is a Fellow of ACM and IEEE. He received the Golden Jubilee Medal  for outstanding contributions to the IEEE CAS Society in 2000. He received the 1987 D. Pederson Award  for the best paper on the IEEE Transactions on CAD/ICAS, two Best Paper Awards at the Design  Automation Conference, in 1983 and in 1993, and a Best Paper Award at the DATE Conference in 2005.     He has been serving IEEE in several capacities, namely: Division 1 Director (2008-9), co-founder and  President Elect of the IEEE Council on EDA (2005-7), President of the IEEE CAS Society (2003), Editor in  Chief of the IEEE Transactions on CAD/ICAS (1987-2001). He was Program Chair of the pHealth and VLSI  SOC conferences in 2006. He was the Program Chair and General Chair of the Design Automation  Conference (DAC) in 1996-1997 and 2000 respectively. He was the Program and General Chair of the  International Conference on Computer Design (ICCD) in 1988 and 1989 respectively. He is a founding  member of the ALaRI institute at Universita' della Svizzera Italiana (USI), in Lugano, Switzerland, where  he is currently scientific counselor.                    "
Approaching Ideal NoC Latency with Pre-Configured Routes.,"In multi-core ASICs, processors and other compute engines need to communicate with memory blocks and other cores with latency as close as possible to the ideal of a direct buffered wire. However, current state of the art networks-on-chip (NoCs) suffer, at best, latency of one clock cycle per hop. We investigate the design of a NoC that offers close to the ideal latency in some preferred, run-time configurable paths. Processors and other compute engines may perform network reconfiguration to guarantee low latency over different sets of paths as needed. Flits in non-preferred paths are given lower priority than flits in preferred ones, and suffer a delay of one clock cycle per hop when there is no contention. To achieve our goal, we use the ""mad-postman"" technique: every incoming flit is eagerly (i.e. speculatively) forwarded to the input's preferred output, if any. This is accomplished with the mere delay of a single pre-enabled tri-state driver. We later check if that decision was correct, and if not, we forward the flit to the proper output. Incorrectly forwarded flits are classified as dead and eliminated in later hops. We use a 2D mesh topology tailored for processor-memory communication, and a modified version of XY routing that remains deadlock-free. Performance gains are significant and can be proven greatly useful in other application domains as well","Approaching Ideal NoC Latency with Pre-Conﬁgured Routes George Michelogiannakis1, Dionisios Pnevmatikatos2 and Manolis Katevenis1 Institute of Computer Science (ICS) Foundation for Research & Technology - Hellas (FORTH) – member of HiPEAC P.O.Box 1385, Heraklion, Crete, GR-711-10 GREECE Email: {mihelog,pnevmati,kateveni}@ics.forth.gr Abstract 1 Introduction In multi-core ASICs, processors and other compute engines need to communicate with memory blocks and other cores with latency as close as possible to the ideal of a direct buffered wire. However, current state of the art networkson-chip (NoCs) suffer, at best, latency of one clock cycle per hop. We investigate the design of a NoC that offers close to the ideal latency in some preferred, run-time conﬁgurable paths. Processors and other compute engines may perform network reconﬁguration to guarantee low latency over different sets of paths as needed. Flits in non-preferred paths are given lower priority than ﬂits in preferred ones, and suffer a delay of one clock cycle per hop when there is no contention. To achieve our goal, we use the “madpostman” [5] technique: every incoming ﬂit is eagerly (i.e. speculatively) forwarded to the input’s preferred output, if any. This is accomplished with the mere delay of a single pre-enabled tri-state driver. We later check if that decision was correct, and if not, we forward the ﬂit to the proper output. Incorrectly forwarded ﬂits are classiﬁed as dead and eliminated in later hops. We use a 2D mesh topology tailored for processor-memory communication, and a modiﬁed version of XY routing that remains deadlock-free. Our evaluation shows that, for the preferred paths, our approach offers typical latency around 500 ps versus 1500 ps for a full clock cycle or 135 ps for an ideal direct connect, in a 130 nm technology; non-preferred paths suffer a one clock cycle delay per hop, similar to that of other approaches. Performance gains are signiﬁcant and can be proven greatly useful in other application domains as well. 1 also with the University of Crete, Dept. of Computer Science, Heraklion, Crete, Greece 2 also with the Technical University of Crete, Dept. of Electronic and Computer Engineering, Chania, Crete, Greece Networks-on-Chip (NoCs) are key components of the emerging Systems-on-Chip (SoCs). As SoCs grow in area, complexity and functionality, so do their communication requirements in terms of performance (latency and throughput) and number of interconnected components. Reducing NoC latency is crucial for SoC performance, since it is introduced to every communication pair within the SoC. Latency may become vital in the case of real-time SoCs. It may also play an especially important role in the case of processor units communicating with other processor units, local memory, shared memory or cache blocks. In this paper, we propose a NoC with latency close to the ideal, i.e. that of long buffered wires. We examine our proposal in a chip consisting of many processor units and RAM blocks. However, our ideas are general and can be easily adapted to other NoC styles. We achieve low latency communication by deﬁning and differentiating pre-conﬁgured preferred low-latency paths adapting the “mad-postman” [2, 5] technique proposed two decades ago for inter-chip communication networks. Preferred paths are formed by pre-driving tri-state select signals within a switch. Therefore, ﬂits will be eagerly (i.e. speculatively) forwarded to their input’s preferred outputs. Preferred path delay per hop is solely that of a pre-enabled tri-state driver. Pre-enabling is crucial because these control signals fan out to many bits, thus driving them incurs considerable delay. Packets may consist of a single or multiple ﬂits [11]. Flits that are eagerly forwarded to a wrong switch output are terminated later in the network as “dead” ﬂits. They are forwarded to their correct output at a lower priority than ﬂits which originate from the input having this output as preferred (if any), and suffer a latency of one clock cycle when there is no contention. In order to provide preferred paths with ﬂexibility and to be able to distinguish incorrect eager forwarding, we utilize a modiﬁed version of XY routing which remains deadlockfree. According to it, a ﬂit is considered to have been correctly eagerly forwarded if it moves closer to its destination in any of the two axes. A ﬂit is considered dead if the distance between it and the destination increased in any of the two axes with its last hop. This way, we can easily distinguish an incorrect eager ﬂit forwarding, as well as a dead ﬂit in the network. Network reconﬁguration is possible at any time by any processing element (PE) or other user block in the network. It is accomplished by sending specially formatted singleﬂit packets to the switching nodes that need to be reconﬁgured. Reconﬁguration can be requested at any time, but is carefully applied to the switching node to prevent out-oforder delivery of ﬂits belonging to the same packet. Dealing with out-of-order ﬂit delivery complicates the NoC interfaces and is rarely allowed in NoCs. To fully exploit the mad-postman technique and ensure its proper operation, we take a slightly different approach for switching node architecture than most past research. Our switch resembles a buffered crossbar [8], having one FIFO at each crosspoint and schedulers at each output. The scheduler monitors the FIFOs and the preferred path, and determines which FIFO it can serve next, if any. At each input a combinational routing logic determines if the incoming ﬂit needs to be forwarded to a non-preferred output. If so, it enqueues the ﬂit in the appropriate crosspoint FIFO. We evaluate our proposed approach on a 2D mesh topology [3] tailored for our target application, i.e. processors communicating with RAM blocks. We attempt to minimize the number of switching nodes as well as the NoC overhead by placing one switching node per 4 RAM blocks. The RAM blocks are placed without any free space between them, essentially forming a bigger block. We also investigate ﬂoorplan options for our switching nodes by evaluating two different shapes (rectangular and cross-shaped), and outline some modiﬁcations to our switch to further reduce occupied area. Topology and ﬂoorplan choices, however, do not affect our low-latency contribution and are made according to application and optimization needs. Simulation results show that, in a 130 nm technology, our design functions at 667 MHz under typical case conditions. It offers preferred path latency of approximately 360 ps per hop that increases to approximately 500 ps per hop when taking into account an 1 mm long wire at each output. This is compared to 135 ps latency for straight buffered wires of a similar length that offer no conﬁguration or routing capability. Non preferred path latency is one clock cycle when there is no contention. Our base switching node design, for 39-bit wide datapaths, occupies an area of 637 µm × 310 µm when its ﬂoorplan is rectangular. We believe that our proposed NoC concept is the means to approach the ideal latency as closely as possible. It may also be combined with orthogonal past NoC research to further improve performance as well as other aspects. The rest of the paper is organized as follows: Section 2 provides a summary of past NoC research. Section 3 explains the mechanism for pre-conﬁgured low latency paths. Sections 4 and 5 present our proposed switch architecture and describe our NoC’s topology. Section 6 presents our placement and routing results, and section 7 identiﬁes room for future work. Finally, section 8 provides our conclusions. 2 Related Work Research has examined performance-enhancement techniques [6, 7, 9]. These approaches are based on precomputing routing, virtual channel (VC) allocation, and arbitration decisions, as well as speculative pipelines to minimize deterministic routing latency. Implementations of these approaches with VCs and various datapath widths are able to function with a clock frequency of around 500 MHz in technologies ranging from 70 nm to 130 nm. While these approaches can yield per hop latency of one clock cycle, this latency is not guaranteed. These designs suffer higher penalties from contention and blocking delays, that signiﬁcantly increase latency. Moreover, one clock cycle per hop is their minimum possible latency, while our proposed NoC provides constant minimum per-hop latency, independent of the clock period. Asynchronous approaches achieve 2 ns per hop [1] for highest-priority ﬂits. Finally, routing algorithms have also been proposed. Many recent NoCs utilize adaptive routing algorithms [4, 12], to route around congested or other problematic areas according to some criteria. As explained in subsection 3.4, our NoC implements a deterministic routing algorithm. Flexibility in preferred paths is already provided. The implementation of adaptive routing algorithms for nonpreferred paths is left as future work. 3 Preferred Paths 3.1 Mad-Postman Mad-postman [2, 5] was introduced in inter-chip packetswitched communication networks. It offered minimal perhop latency by eagerly forwarding an incoming ﬂit to the same direction in the same axis that it entered the switch from. There was no logic or delay during this forwarding more than that of simple multiplexor or tri-state cell. Incoming ﬂits were also stored in the switch for checking that they were correctly eagerly forwarded. The network strictly followed XY routing algorithm. Thus, a ﬂit was regarded as correctly eagerly forwarded if it followed XY routing. Flits which were incorrectly forwarded remained in storage in the switch and were later sent to the appropriate output. We ﬁnd that this concept can be applied to NoCs. 3.2 Preferred Paths The original mad-postman strictly followed XY routing. Therefore, a ﬂit would suffer a routing logic and buffering penalty once at its ﬁnal hop (in order to be ejected to the local PE output), and possibly once more when it changed axes when traversing the network. We would like our NoC to be able to provide complete paths with the minimum perhop latency. Moreover, we would like to provide the ﬂexibility to change those paths at run-time to meet various application demands, such as a processor allocating more RAM blocks for itself. To meet these goals, we introduce preferred paths. Each input is directly connected to a tri-state buffer at each other port’s output. Each output has at most one preferred input. That input’s tri-state driver is pre-enabled. Therefore, an incoming ﬂit to that input would be eagerly forwarded to each output having this input as preferred. This is achieved solely with the delay of a pre-enabled tristate driver. Note that an input may have multiple preferred outputs. Thus, preferred paths can fork and simulate a broadcast network if so desired at run-time. However, preferred paths may not converge as only one tri-state may safely drive a wire at any time. Each input also features a combinational routing logic that examines each incoming ﬂit and determines whether it must be forwarded to an output other than the preferred. If so, it enqueues it in the appropriate crosspoint FIFO to be later forwarded by that output’s arbitration logic. A ﬂit needs to be forwarded to an output if it was mistakenly eagerly forwarded. Later hops regard that ﬂit as dead. Dead ﬂits are not forwarded to any output by the routing logic. They propagate through the network in preferred paths until they reach an input with no preferred outputs. Then, they are either terminated or forwarded in XY manner and possibly enter a circle, as discussed in subsection 3.6. Dead ﬂits occupy resources and therefore may be a nuisance. However, previous research indicates that this effect does not reduce the performance of the network beyond that of virtual cut-through or wormhole networks [5]. If fair arbitration is desired without demands for very low latency at some part of the network, that part can be reconﬁgured to remove any preferred outputs from switch inputs. 3.3 Packet Format Packets may consist of a single or multiple ﬂits, in the manner described in [11]. Single-ﬂit packets are used for reconﬁguration and read requests. Multi-ﬂit packets are used for transferring multiple words of data to write to a RAM, or from a RAM as a reply to a read request. Flits feature 6 packet ID and 1 ﬂit type control bits. The ﬂit type bit marks the initial ﬂit of a packet as request (single-ﬂit packet) or address, and thereafter data ﬂits with the same packet ID as body or tail. The 32 payload bits contain data in the case of data ﬂits and destination address, byte enables and packet type in the case of address or request ﬂits. Each switch is identiﬁed by unique X,Y coordinates. The ﬂit’s ﬁnal destination is determined by two extra bits specifying the user block among the 4 the switch is connected to. The initial ﬂit of a packet is an address or request ﬂit. Data ﬂits in the same packet have the same ID and will be treated by each switch as the corresponding address ﬂit was. Since ﬂits are eagerly forwarded without being able to process their headers, all ﬂits in a packet will be incorrectly eagerly forwarded in the same way throughout the network. The same applies to the duplicate ﬂit complication, explained in subsection 3.5. Attempting to do otherwise would require combinational logic in preferred path hops, and thus would dramatically increase per-hop latency. 3.4 Routing Based on our need to accurately classify ﬂits as dead, we choose to implement a deterministic routing algorithm. Non-deterministic (adaptive) routing algorithms introduce the uncertainty in dead ﬂit classiﬁcation. This is due to the fact that conditions, and therefore adaptive routing decisions, are subject to change at any time. Therefore, the switch currently examining a ﬂit is unsure if the ﬂit’s previous hop regarded this switch as the best next hop at the time, or if the ﬂit was incorrectly eagerly forwarded. Since making switches aware of neighbouring network conﬁguration is too costly, we adopt a deterministic routing algorithm. As a result, we chose a slightly modiﬁed version of XY routing. XY routing instructs a ﬂit to ﬁrst complete its movement in the X axis, and then switch to the Y axis to reach its destination. Our NoC follows this routing algorithm, but is more ﬂexible in allowing eager forwardings that do not adhere to strict XY routing. Speciﬁcally, a ﬂit is considered to have been correctly eagerly forwarded, and therefore is not forwarded to another output by the switch, simply if it is approaching its destination in any of the two axes. This may result in a ﬂit reaching its destination via a route that does not comply with strict XY routing. In the example Figure 1 illustrates, the ﬂit arrives from source S to destination D solely through preferred paths (solid lines). Switch A sees that the ﬂit approached destination D in the Y axis, and therefore regards this eager forwarding as correct. XY routing would have the ﬂit pass through non-preferred paths (dashed lines) and switch dimensions at node B, after having fully completed its traversal in the X axis. Because we would like to provide preferred paths with full ﬂexibility, and also because disallowing these paths by forwarding ﬂits again in non-preferred paths introduces an unnecessary overhead, we choose to modify our XY routing algorithm accordingly. Similarly, a ﬂit is considered dead simply if it moves away from its destination in any of the two axes. 3 b D B c 2 A S 1 a Figure 1. Correct eager forwarding scenario that does not comply with strict XY routing. 3.5 Duplicate Flits Due to the above mechanisms, our NoC faces the complication of multiple copies of the same ﬂit reaching their destination via different routes. An example of such an occurrence is illustrated in Figure 2. In that example, the ﬂit leaving source S will be eagerly forwarded via the preferred path (solid lines) until it reaches destination D. However, switch A will regard this eager forwarding as mistaken, since it has no preferred path knowledge for its neighbours and the ﬂit’s distance from destination D increases in the Y axis. Therefore, it will forward another copy of the ﬂit to destination D via non-preferred paths (dashed lines). Duplicate ﬂits must be handled at the network interface logic of the blocks. Network interface issues are addressed in subsection 5.1.  4   c  5 D   3 b 2 1 A S Figure 2. Duplicate ﬂit scenario. 3.6 Deadlock-Freeness XY routing is deadlock-free [10]. Therefore, deadlock hazards in our NoC are introduced by preferred paths since they do not necessarily follow XY routing and ﬂits propagate in them without any control. Since preferred paths do not content for resources and our NoC follows XY routing otherwise, no ﬂits will indeﬁnitely wait to be served. To guarantee this, a switch needs to be able to serve FIFOs, and therefore resolve contention, if the preferred path has been continuously active for an unreasonably long period of time with a FIFO non-empty. FIFOs are then served until all are empty. During this time, ﬂits arriving in the broken preferred path will be enqueued in the appropriate FIFO behind previous ﬂits of the same packet. However, we need to investigate the possibility of a ﬂit traversing the network indeﬁnitely. We combat this issue in two ways. First, we provide constraints which, if followed, guarantee that no ﬂit will indeﬁnitely travel through the network. If all preferred paths in the NoC are straight lines, ﬂit propagation follows strict XY routing. Therefore, every turn is handled by routing logic and ﬂits cannot enter a circle. This is the case with original mad-postman networks [2, 5]. Flits cannot enter a circle also in the case of preferred paths having exactly one turn. In this case, circles are formed by four different preferred paths. Therefore, a ﬂit would be examined by routing logic four times before completing a single loop. At these times, the ﬂit will either be considered dead, or it will be propagated according to XY routing. Therefore, in two out of the four checkpoints the ﬂit will be forwarded according to the circle. However, in at least one of the other two it will be forwarded in the other axis and leave the circle. Preferred paths with two turns may form a circle if the two routing logic checkpoints forward the ﬂit according to the circle. Therefore, if preferred paths in our network contain up to one turn, no ﬂits will indeﬁnitely propagate in circle. This restriction does not take into account turns with switch data ports, as they cannot be part of a circle. Second, we investigate the consequences of a formed circle. As already described in subsection 3.4, each switch in the circle will examine if the preferred path forwarding was correct and forward a copy as necessary. This guarantees that a copy of the ﬂit in the circle will be delivered to its destination. The ﬂit will continue to propagate inside the circle. Other ﬂits contenting for occupied resources will be forced to wait. If the ﬂit in the circle propagates such as the preferred path is not idle at any clock cycle, contenting ﬂits will face an increased queueing delay. However, as already described in the beginning of this subsection, contenting ﬂits will eventually be served. This poses a performance issue, but no deadlock will occur. When FIFOs are served, the ﬂit in the circle will be examined by routing logic and therefore may be terminated as dead. 3.7 Reconﬁguration Reconﬁguration of our network’s preferred paths consists of changing outputs’ preferred inputs in the appropriate switches. Any PE or other user logic block can request reconﬁguration by sending properly formatted single-ﬂit conﬁguration packets. These packets contain the destination node, the output to be reconﬁgured, and the new preferred input. Conﬁguration ﬂits are enqueued in the appropriate crosspoint FIFO of their destination, even if that ﬂit follows a preferred path. When the conﬁguration ﬂit is selected by the output’s arbiter, it is stored in the output’s conﬁguration register which stores the active conﬁguration, instead of being forwarded to the next hop. If we were to immediately alter the tri-state enable signals, we would risk out-of-order delivery of ﬂits belonging to the same packet. Consider the example of Figure 3. Switch S transmits a packet to destination D. The initial ﬂit (ﬂit I) is constantly in non-preferred paths (dashed lines), and therefore is forwarded at every hop by XY routing logic. If a user block in the network was to reconﬁgure switch A to select input 1 as preferred for output port 2, later ﬂits (ﬂit II) would now reach destination node D via a preferred path (solid lines). Therefore, if ﬂit I is in transit and switch A is reconﬁgured before the last ﬂits of that packet reach it, those last ﬂits could reach destination D before the ﬁrst ﬂits. D II 1 2 A I S Figure 3. Out-of-order delivery scenario. Because out-of-order delivery of ﬂits belonging to the same packet can be a nuisance for destinations and also because address ﬂits must always precede the corresponding data ﬂits, it must be prevented by our NoC. This can be accomplished by delaying the application of the new conﬁguration for each output until it is safe. Speciﬁcally, once a new conﬁguration is received at an output, it is only applied when the old preferred path has been idle for 1 clock cycle, the new preferred path’s FIFO is empty and no more ﬂits from a packet are expected in those paths. As explained in subsection 3.3, ﬂits forming a packet are labelled. Therefore, after receiving the packet’s address ﬂit and until receiving the tail ﬂit, the arbitration logic knows that more ﬂits are expected in this speciﬁc path and therefore delays the application of the new conﬁguration. Blocks requesting reconﬁguration are unsure exactly when it is applied, unless application demands dictate the implementation of a reconﬁguration acknowledgment or polling mechanism. Due to this technique, no preferred path will change at each used switch from the time it receives the ﬁrst ﬂit of a packet until it forwards the last. This ensures that all ﬂits of the same packet follow the same path, and therefore a switch will not wait indeﬁnitely for tail ﬂits. Thus, all ﬂits belonging to the same packet will be delivered in-order. Different packets from the same source to the same destination may be delivered out-of-order. Since switches have no information regarding more expected packets in the same path, they may apply their new conﬁguration if the preferred path becomes idle for one clock cycle. Even if the source transmits back-to-back, the preferred path may become idle for one clock cycle due to broken preferred paths to avoid starvation effects caused by contention, as explained in subsection 3.6. Therefore, this issue must be handled in the network interface logic as explained in subsection 5.1. Arbitration logic serves ﬂits stored in FIFOs until all are empty, as described in section 4. If any ﬂits arrive through the preferred path during this time, they are enqueued in their preferred output’s FIFO. Arbitration logic can then serve them in priority according to the implemented algorithm. This imposes a single clock cycle delay regardless of contention from other inputs, in this infrequent scenario. However, attempting to do otherwise would introduce extra combinational logic and timing hazards. These ﬂits will still be forwarded according to the preferred path. This serves the purpose of avoiding out-of-order delivery scenarios and taking advantage of preset multi-hop preferred paths. 3.8 Backpressure Our NoC needs to provide a mechanism for not dropping ﬂits due to full FIFOs. This mechanism must inform each output in a switch whether it can safely transmit a ﬂit to the next hop. Likewise, the previous switch would also be informed if it can safely transmit to the current. Therefore, long packets reaching a congestion point will be stored in many, possibly consecutive, switches. Since ﬂits of the same packet are guaranteed to follow the same path and arrive in-order, no recombination care must be taken. Depending on area constraints and trafﬁc patterns, we can adopt two different approaches. According to the ﬁrst, if any of the next hop’s FIFOs that have the output port in question as their input is almost full (to cover for backpressure signal propagation delay), the output’s arbiter is alerted to not transmit any more ﬂits until the signal is de-asserted and it is safe again. This approach requires only one wire from each output’s next hop and the simplest logic. According to the second approach, one wire from each of the next hop’s FIFOs that have the output in question as their input alerts the switch exactly which crosspoint FIFO is almost full. This way, the arbiter needs to process packet IDs of ﬂits in FIFO heads to determine if it can safely transmit any of them. With this approach, FIFOs that are not full are able to receive ﬂits, and thus communication and FIFO utilization is more efﬁcient. However, 6 wires are required at each output and also the arbiter must be able to process ﬂit packet IDs in each FIFO head. Extra care must be taken for ﬂits forwarded in preferred paths. In our NoC we cannot know the ﬁnal destination of ﬂits travelling in preferred paths before they have been eagerly forwarded, nor can we control their transmission. Therefore, if a backpressure signal to an output port is asserted, the preferred path leading to this output is broken. Thus, all subsequent ﬂits in that path are enqueued in the appropriate crosspoint FIFO, and later forwarded according to the preferred path. Alternatively, the preferred path could remain intact, but ﬂits in that path are still enqueued as above. Therefore, as long as a ﬂit travels in a preferred path, it is not affected by contention or congestion. However, since preferred path ﬂits take precedence over ﬂits in FIFOs, congestion is slower to resolve. 4 Switch Architecture Switch input port components and connections are shown in Figure 4(b). Output port components are illustrated in Figure 4(a). Data wires are illustrated as solid lines and control wires as dashed lines. Our switch is a composition of the above ﬁgures, featuring 6 input/output ports. Each input is connected to each other port’s output. Our switch resembles a buffered crossbar [8] in that it features one FIFO at each crosspoint and independent conﬁguration and arbitration logic at each output. A combinational routing logic block at each input decides at which FIFO, if any, should the incoming ﬂit be enqueued. This choice of switch architecture takes into account mad-postman’s operation, since incoming ﬂits are examined by the routing combinational logic before being able to be enqueued into FIFOs. Therefore, dead ﬂits do not occupy FIFO lines. Moreover, since our current NoC does not include virtual channels, as addressed in subsection 4.1, crosspoint queueing removes the nuisance of head-of-lineblocking. Finally, the use of one arbitration and conﬁguration logic block per output results in simpler logic and therefore shorter critical paths. Output conﬁguration logic is responsible for storing and updating preferred path conﬁguration. Arbitration logic is responsible for serving the FIFOs. Non-empty FIFOs as well as FIFOs to which a ﬂit is being enqueued are selectable. Arbitration logic starts serving FIFOs once there is a selectable FIFO and the preferred path has been idle for one clock cycle. This serves the purpose of prioritizing preferred path ﬂits without unreasonably preventing FIFOs from being served. It stops serving them when they are all empty. Arbitration takes place during the preferred path idle cycle, for the next cycle. Therefore, our NoC achieves one clock cycle per-hop latency for non-preferred paths when there is no contention. Arbitration algorithm details may depend on exact NoC demands. Each output is driven by tri-states directly connected via dedicated wires to each other port’s input. Each output is also driven by a tri-state which connects the output wire with a multiplexer which forwards the FIFO ﬂit bePort 1 Port 2 Port 3 Port 4 Port 5 Port 6 Figure 5. Preferred path bus. ing served by arbitration logic, if any. Tri-state enable signals are driven by the output’s conﬁguration and arbitration logic. Preferred paths are thus formed by pre-enabling tristates, therefore connecting an input with any number of preferred outputs. Depending on preferred path ﬂexibility and area needs, an extra optimization may be necessary to further reduce switch area. Instead of directly connecting each input to each other output, a preferred path bus could be deployed, as in Figure 5. This vastly limits the number of preferred input-output pairs that can be conﬁgured to only one input with any number of outputs. However, intermediate designs can also be implemented. For instance, one such preferred bus in the X axis and one in the Y could be deployed, perhaps even connected to each other with tri-states. Therefore, depending on exact preferred path communication needs, NoC area overhead can be reduced. 4.1 Virtual Channels Virtual channels (VCs) are useful for deﬁning multiple logical topologies within the network, adaptively routing around congested or faulty nodes and providing packet priority and thus guaranteed QoS classes [10]. However, in our NoC preferred paths already provide a means to prioritize packets compared to others as well as form different low-latency topologies. Moreover, our NoC’s topology is already tailored to our speciﬁc application environment. Finally, our NoC faces challenges in implementing adaptive routing algorithms, as explained in subsection 3.4. For these reasons, our current NoC does not include VCs. Introducing them would multiply FIFOs, which translates into a signiﬁcant area overhead since our switch features one FIFO at every crosspoint. However, other NoC applications may have different design priorities and requirements which make VCs more attractive. 5 Network Topology We tailor a 2D mesh topology to our target application, which is an array of processors and RAM blocks, aiming to minimize area overhead in addition to latency. This topology is illustrated in Figure 6. We assume a ﬂexible system that assigns memory blocks to processors according to Output Input1 Input2 Input3 Input4 Input5 Config & Arbitration Preferred Path Tri-states Input1 Input2 Input3 Input4 Input5 (a) Output port components Output Port Components Output Port Components Input Output Port Components Output Port Components Output Port Components Other port outputs Routing Logic (b) Input port components Figure 4. Switch architecture. A d d r A d d r A d d r A d d r Data Data Data Data A d d r A d d r A d d r A d d r Data Data Data Data A d d r A d d r A d d r A d d r Data Data Data Data A d d r A d d r A d d r A d d r Data Data Data Data A d d r A d d r A d d r A d d r Data Data Data Data A d d r A d d r A d d r A d d r Data Data Data Data a Figure 6. Rectangular-shaped ﬂoorplan. application needs, and therefore proﬁts from the reconﬁguration capabilities of our NoC. We used single-port RAM blocks. In our 130 nm implementation library, RAM blocks feature data pins on one side of the X axis and address pins on one side of the Y axis. We therefore place four RAM blocks to form one larger network block. We rotate and mirror RAM blocks to place all data pins on the X axis and all address pins on the Y axis. CPUs and other user blocks may be placed as part of such a block or as a whole network block themselves, depending on their size. In our current NoC each switch has 6 input/output ports. Each input is connected to each other port’s output. Two of these input/output ports are used for inter-switch communication in the X axis, and other two in the Y axis. The rest two ports are used for communication with the data inputs of the 4 adjacent RAM or other user logic blocks. Given the data pin placement, one port is used for each two RAM blocks facing each other in the X axis. One data output is Data input 1 - non-preferred Data input 2 - preferred Figure 7. 2x1 switching logic. wired to both RAM block data input interfaces. The two RAM blocks’ data outputs are connected to a simple logic illustrated in Figure 7, resembling a 2x1 switch. It has no routing logic and only features one FIFO which is used by the non-preferred input. From the moment a RAM block receives a read request from the address interface until it is able to output data, it notiﬁes this 2x1 switch to choose that RAM block’s data output as preferred. This switch therefore imposes minimal latency impact. It may also be reconﬁgured as other switches. If area permits, a request FIFO may be implemented to store the order of requests received by the RAM blocks. This will enable it to always anticipate the next generated ﬂit, and thus avoid non-preferred path delays in case of multiple requests to both RAM blocks. RAM block address interfaces are wired to the nearest Y axis output. These connections are illustrated with dashed lines. Thus, the RAM blocks immediately above a switch have their address interfaces wired to the Y output leading upwards. This way, we avoid implementing extra output ports for address inputs. As data input interfaces, address interfaces monitor each incoming ﬂit to determine if it is destined for that RAM block. The potential increased contention for outputs wired to address interfaces is outweighed A d d r A d d r A d d r A d d r Data Data Data Data A d d r A d d r A d d r A d d r Data Data Data Data A d d r A d d r A d d r A d d r Data Data Data Data A d d r A d d r A d d r A d d r Data Data Data Data A d d r A d d r A d d r A d d r Data Data Data Data A d d r A d d r A d d r A d d r Data Data Data Data a b Figure 8. Cross-shaped ﬂoorplan. by the signiﬁcantly less area required by our switch. Finally, for switches that serve exclusively RAM blocks, we can further reduce the required switch area since RAM blocks will never need to communicate to each other directly. Therefore, each switch data input should not be connected to the other switch data output, therefore saving two internal switch connections and all the accompanying logic. For switch placement, we examined two ﬂoorplan alternatives evaluated in section 6. In the ﬁrst, switches are placed in the corners of larger network blocks as a cross, shown in Figure 8. This requires only a small distance between user blocks in each axis. Moreover, wire length, and therefore propagation delay, between each switch is minimal, even in the Y axis. The second placement, shown in Figure 6, has the switch solely in the X axis between two large blocks in a rectangular shape. User block distance in the Y axis is truly minimal, and is only used for memory address interface logic. Communication with switches in the Y axis is achieved by wires in higher metal layers routed above RAM blocks, or possibly in any metal layer routed above address interface logic. Y axis communication wire length is equal to twice the RAM block’s height, approximating to 1 mm in our placement and routing. 5.1 Network Interfaces PEs, RAM blocks or other user logic blocks need NoC interface logic. This logic is responsible for enabling communication between the NoC and the block. It is responsible for submitting properly formated packets divided in ﬂits, as explained in subsection 3.2, as well as receiving ﬂits destined to the user block. For data network interfaces, incoming ﬂits must be brieﬂy stored until the whole packet is complete and thus able to be submitted to the user block for processing. Address network interfaces only receive one ﬂit per packet containing the address. In addition, network interface logic must be able to arbitrate between complete packets in a desired manner, e.g. submit read and write requests in the order they were transmitted by the source to satisfy sequential consistency. Network interface logic must also identify which ﬂits of a packet it has already received and discard duplicate copies. There are various implementations of this functionality according to design optimization priorities. To simplify this task, the ﬂit control bits could be expanded, or the packet ID bits lessened, to include sequence number bits. Out-of-order delivery of ﬂits belonging to the same packet is impossible, as explained in subsection 3.7. However, ﬂits belonging to different packets from the same source may be delivered in any order. Therefore, interface logic must submit packets for processing once complete, regardless of other incomplete packets. To implement this functionality, we deploy multiple small FIFOs in data interfaces and registers in address interfaces, and enqueue ﬂits accordingly. In case we would like packets to be submitted in the order they were sent by the source, the relative packet order can be retrieved from the packet header. Network interface logic must also handle multiple incoming packets from various sources. Flits from these packets may arrive in any order. If all buffer is used up, extra incoming ﬂits are stored in previous hops through backpressure. Therefore, no excessive buffer space is required. Since one data interface FIFO is reserved per packet until it is complete and submitted for processing, and packets may arrive out-of-order, deadlocks may occur. Lets assume that packet A has partially arrived at the target RAM. Packet B from the same source arrives at the ﬁnal hop before A’s tail ﬂit. However, all of the RAM’s data interface FIFOs are reserved. Therefore, packet B waits in the switches due to backpressure, not allowing packet A’s tail ﬂit to arrive. This scenario requires several packets arriving out-oforder through the same path with partially complete packets, since the data interface logic deploys several FIFOs. Assuming sources do not submit packets interleaved, outof-order delivery is only caused by reconﬁguration. Therefore, limiting the number of active reconﬁgurations that can occur at any one time to less than the number of data interface FIFOs guarantees that at least one FIFO will be eventually freed and no deadlock will occur. Implementation of this restriction may require software synchronization primitives, deﬁning areas each CPU can reconﬁgure paths in, or a reconﬁguration acknowledgment mechanism. 6 Layout Results We performed placement and routing using a 130 nm library available to European universities. Synthesis was conducted with Synopsys Design Compiler version 2004.06-SP2, placement and routing with Cadence SOCEncounter version 3.3 and simulation with Verilog-XL version 05.10.002-p. We chose single-port RAM blocks of 4096 lines of 32 bits each (128kbits), with a column mux of 16. Without power rings, they are 715.07 µm long on the X axis, and 551.64 µm long on the Y axis. Larger RAM blocks had a disproportionally larger cycle time, while multi-port Table 1. Switch p&r results (typical). Impl. lib. P. supply Clock freq. I/O ports FIFOs FIFO lines Flit width Gates Cells Cell area Int. nets Comb. area Non-comb. Leakage p. Dynamic p. 130nm 1.2V 667 typ. - 400 w.c. (MHz) 6 30 2 39 bits Pref. bus 38865 13369 183056µm2 12703 72420µm2 110632µm2 85µW 77mW Full switch 44874 15001 195228µm2 13595 84424µm2 110798µm2 91µW 80mW Change -13% -11% -6% -6.5% -14% -0.1% -7% -3% RAMs were larger and more power consuming. Switch p&r details are shown in Table 1. Results presented are under typical case conditions. Power consumption results are under heavy switching activity. Preferred path latency per switch ranged from 300 to 420 ps. If we also include a 1 mm long wire at the output, approximately twice a RAM block’s height, latency increases to 450-550 ps, as compared to 135 ps for straight buffered wires of a similar length without any conﬁguration or routing capability. When there is no contention, non-preferred path latency is one clock cycle. Contention without starvation effects increases non-preferred path latency depending on various factors, but does not affect preferred path latency. Our design functions at 667 MHz under our library’s typical case conditions, and at 400 MHz under worst case conditions. At 667 MHz, RAM blocks require a 25 µm wide power ring. Therefore, RAM block effective size is 740.07 µm × 576.64 µm. In an orthogonal shape, one switch occupies a minimum area of 637 µm × 310 µm. In the rectangular placement option as explained in section 5 and illustrated in Figure 6, switch height (a) is 170 µm at minimum. Since we require one switch every 4 RAM blocks (or user blocks of roughly the same size), NoC area overhead is 13%. In the cross placement option as depicted in Figure 8, switch height in the X axis (a) is 130 µm, while switch length in the Y axis (b) is 140 µm. In this case, NoC area overhead is 18%. This shows that area efﬁciency drops in the second case. However, cross-shaped switches have the least possible distance between each other even in the Y axis, therefore minimizing propagation delay between them. P&r details of the area-efﬁcient single-preferred path switch explained in section 4 are shown in Table 1. In the rectangular placement option, switch height (a) is 133 µm (22% decrease). In the cross placement option, switch height in the X axis (a) is 118 µm (9% decrease), while switch length in the Y axis (b) is 114 µm (18.5% decrease). This imposes a NoC area overhead of 10% in the ﬁrst case and 16% in the second. These results show that the area gain is small, but in some applications it could outweigh the loss in preferred path ﬂexibility. 7 Future Work A number of issues should be addressed in the future. Firstly, while our current NoC utilizes a deterministic routing algorithm as explained in subsection 3.4, adaptive routing has signiﬁcant beneﬁts to offer. For instance, congestion can be avoided by later ﬂits. Therefore, a customized version of an adaptive routing algorithm should be investigated to provide our NoC with more ﬂexibility. Secondly, our NoC needs to be made fault-tolerant since faults may appear in a chip’s lifetime, especially in technologies narrower than 130 nm. This will impose an unavoidably increased area overhead. However, technology trends dictate that designs must be fault-tolerant in some way in order to be trusted for future designs. Thirdly, our NoC needs to be evaluated in a complete system under various workloads and demands. This will enable us to accurately analyze our contribution’s impact, as well as the effect of dead ﬂits in our NoC’s performance. Moreover, we can investigate the optimal method for choosing preferred paths in a given application environment, as well as the impact of this choice. Finally, our NoC can face synchronization issues which may result in design limitations. Since preferred paths are purely combinational, ﬂits traversing them can arrive at their destinations and other switches at any point during the clock cycle. Thus, ﬂits may violate ﬂip-ﬂop setup or hold time upon arrival. There are several approaches to combat this issue. Firstly, we could impose a constraint on our preferred paths so that this problem will never occur. For example, we could limit the number of continuous preferred path hops such that ﬂits will enter a non-preferred path before the end of the clock cycle that they entered their current preferred path in. For example, assuming that ﬂits are submitted from non-preferred paths in the very beginning of the clock cycle, Furthermore, we could deploy synchronizers at every switch and PE interface logic. While they will not affect preferred paths until ﬂits exit them, their imposed latency in non-preferred paths is excessive. Finally, our switch components can easily be implemented asynchronously with known design methodologies. The problem in this approach lies in the necessary handshake between switches and PEs to guarantee that no ﬂit fragments will be routed through the network. This handshake’s imposed delay will prevent that number is [ Pre f Pat hH o p+W ireLat ency ClockPeriod ]. us from offering our current low per-hop latency, both in preferred and non-preferred paths. "
A Study of NoC Exit Strategies.,The throughput of a network is limited due to several interacting components. Analysing simulation results made it clear that the component that was worth attacking was the exit bandwidth between the network and the connected resources. The obvious approach is to increase this bandwidth; the benefit is a higher throughput of the network and a significant lowering of the buffer requirements at the entry points of the network; this because worst case scenarios now happens at a higher injection rate. The result we present shows significant differences in throughput as well as in average and worst case latency,"A study of NoC Exit Strategies  Mikael Millberg & Axel Jantsch  KTH - Royal Institute of Technology, Sweden  {micke, axel}@imit.kth.se  The throughput of a network is limited due to several   interacting components. Analysing simulation results  made it clear that the component that was worth  attacking was the exit bandwidth between the network  and the connected resources. The obvious approach is  to increase this bandwidth; the benefit is a higher  throughput of the network and a significant lowering of  the buffer requirements at the entry points of the  network; this because worst case scenarios now  happens at a higher injection rate. The result we  present shows significant differences in throughput as  well as in average and worst case latency.  Offering services with best effort performance,  naturally, gives no hard guarantees due to the dynamic  behaviour of any general purpose system. In order to  make use of such services statistical performance  measures are instead utilised. This leads to that the  traffic often has to be below a certain threshold for  which the desired statistical properties can be given.  These properties can be derived, either, from a rigid  reasoning based on the implementation or from an   analysis of simulations. From this analysis the desired  properties can be given with a safety margin.  Given that we are bound to offer services with  statistical characteristics on performance, how do we  do this to a low cost? The cost in this context is the  required buffers needed to guarantee no packet losses  together with a safety margin in terms of injection rate  to ""guarantee"" a worst, and average, case latency.  The approach that we have chosen within, Nostrum  [1], for giving the service of Best effort, is by utilising  deflective routing, with no explicit buffering, to keep  the size of the switches small[2]. Through simulations  with uniform random traffic patterns, we observed that  there seems to exist a ""hard"" limitation on the network.  On a 4x4 mesh this limit is reached for an injection rate  of 0.63 packets/node/cycle for the best performing  routing strategy tried out. Once this limit is reached  packets start queuing up at the entry points of the  network and  the worst case  latencies grows  exponentially.  Analysing simulation results made it obvious that  an increased bandwidth between the network and the  connected resources would make the network perform  better. The price for this solution is that packets now  need to be buffered at the exits of the network, but  from an overall perspective the total buffers required is  lowered for a moderately to heavily loaded network. At  the limit injection rate the average latency was reduced  from 25 to 15 clock cycles and the observed worst case  latency from 280 to 180 clock cycles. This will give  better margins before the network saturates or a higher  throughput with the previous margin kept. All this  assumes that there no single node or bisection cut of  the network are exposed to a static over-utilisation.  The validity of the chosen approach is not restricted  to uniformly random traffic patterns on meshes but  also applicable to ""any"" topology where the traffic  pattern involves potential network exit congestions due  to multiple sources having the same destination and/or  multiple routing paths are possible.  The network exit strategy has not received as much  attention as other parts of network design. Most work  that in detail analyse cost and performance of a router  and the network as a whole, e.g. [3, 4, 5] assume an  ideal packet ejection model, which means that packets  are absorbed by the receiving node as soon as they are  delivered by the network. In [6] an ejection policy is  studied that reduces the cost and complexity of the  router while minimizing the impact on performance.  However, to our knowledge no study about the tradeoffs involved in increasing the network exit bandwidth  has been reported.  [1] M Millberg et al. The Nostrum backbone - a  communication protocol stack for networks on chip. VLSI  Design Conference, January 2004.  [2] E. Nilsson et al. Load distribution with the proximity  congestion awareness in a network on chip.  DATE  2003.  [3] A. A. Chien. A cost and speed model for k-ary n-cube  wormhole routers. IEEE Transactions on Parallel and  Distributed Systems, 9(2):150-162, Feb. 1998.  [4] L. S. Peh and W. J. Dally. A delay model for router  micro-architectures. IEEE Micro, 2001.  [5] E. Rijpkema, et al. Trade offs in the design of a router  with both guaranteed and best-effort services for networks on  chip. DATE, 2003.  [6] Z. Lu and A. Jantsch. Flit ejection in on-chip wormholeswitched networks with virtual channels. NorChip, 2004.            "
Reducing Interconnect Cost in NoC through Serialized Asynchronous Links.,This work investigates the application of serialization as a means of reducing the number of wires in NoC combined with asynchronous links in order to simplify the clocking of the link. Throughput is reduced but savings in routing area and reduction in power could make this attractive,"Reducing Interconnect Cost in NoC through Serialized Asynchronous Links∗  Simon Ogg1, Enrico Valli2, Crescenzo D'Alessandro3, Alex Yakovlev3, Bashir Al-Hashimi1, Luca Benini2  1University of Southampton, 2University of Bologna, 3Newcastle University  {so04r, bmah}@ecs.soton.ac.uk  ∗ This work is funded by EPSRC (UK) grant EP/C512804/1 and is greatly acknowledged.  Abstract  This work  investigates  the application of  serialization as a means of reducing the number of  wires in NoC combined with asynchronous links in  order to simplify the clocking of the link. Throughput is  reduced but savings in routing area and reduction in  power could make this attractive.  1. Introduction and Motivation  As multiprocessor SoC solutions increase there are   benefits to provide a scalable on chip communication  structure such as NoC. Interconnect cost, in terms of  the number of wires required between switches, could  also be considerable in NoC structures since each  switch is connected by a point-to-point link to a  neighboring switch. The high cost of parallel links has  been shown in [1] when inter-wiring spacing, shielding  and repeaters are considered. The number of links in  NoC will grow as more cores are integrated into a  system. This work proposes  the application of  serialization as a means of reducing the interconnect  cost in NoC.   2. Proposed Link & Results  The link, shown in Figure 1, has several blocks;  synchronous/asynchronous converter interfaces (1&5),  asynchronous serializer (2) and de-serializer (4), and  wire buffers (3). The implementation uses a bundled  request with the data. The synchronous/asynchronous  interfaces uses a FIFO mechanism to pass data  between the synchronous and asynchronous domains.  The serializer and de-serializer split each 32 bit flit into  smaller bit slices which are then sent serially along the  buffered wires. The buffered wires allow pipelining of  the bit slices to improve throughput.  Switch  u B f u B f Switch  m CLK A  A S / I n . t  m  m S A / I n t n  CLK A  e S i r n  D e S e r m  1  2  3  4  5  ASYNCH.  SYNCH.  SYNCH. 3  Figure 1 Proposed Link  The synchronous 32 bit wide link was compared to  our proposed asynchronous 4 and 8 bit wide link. All  circuits were simulated with Cadence Spectre using ST  0.12µm technology. The logic area cost  for the  synchronous implementation is 15850 µm2. For the  asynchronous 8 and 4 bit wide the logic area cost is  18900 µm2 and 18950 µm2 respectively. Figure 2 shows  for a wire length of 1000 µm wiring area of the  synchronous link is reduced from 30000 µm2 down to  7500 µm2 and 3750 µm2  for the asynchronous 8 and 4  bit wide  links respectively. Throughput  in  the  asynchronous  links  is  limited due  to need  to  acknowledge each transfer, the link saturates 207  MFlits/s for the 8 bit wide link. Figure 3 shows power  in the 8 bit wide asynchronous link (I3) is up to 25%  lower compared to the synchronous link (I1) depending  on link usage. Power in the link is dominated by the  synchronous/asynchronous converters.  Switch Speed V Throughput 0 50 100 150 200 250 300 350 50 100 150 200 250 300 Sw itch Clock (MHz ) h T r u p h g u o t ( M F t i l c e s s / ) I1-Synch I2-Asynch(4) I3-Asynch(8) Wire Length v Wire Area 100000 90000 80000 70000 60000 50000 40000 30000 20000 10000 0 0 500 1000 1500 2000 2500 3000 Wire Leng th (µm) W i r i A g n r a e ( µ m 2 ) I1-Synch (32) I2-Asynch(4) I3-Asynch(8) Figure 2 Throughput and Wire Area  A feasible serialized asynchronous link has been  demonstrated which lowers the point to point wiring  interconnect cost. Throughput is limited due to the pertransfer acknowledgement scheme and the authors are  currently investigating ways to improve this.  Average Power for 50% Usage  0 100 200 300 400 500 600 700 800 I1-Synch 50% I2-Asynch4 50% I3-Asynch8 50% Implementation (link usage) A e g a e v r o P w e ( r µ W ) Ser/Des Buffers Asynch Synch Conv.  Average Power for 25% Usage  0 100 200 300 400 500 600 700 800 I1-Synch 25% I2-Asynch4 25% I3-Asynch8 25% Implementation (link usage) Figure 3 Average Power  3. "
Bi-Synchronous FIFO for Synchronous Circuit Communication Well Suited for Network-on-Chip in GALS Architectures.,"The distribution of a synchronous clock in system-on-chip (SoC) has become a problem, because of wire length and process variation. Novel approaches such as the globally asynchronous, locally synchronous try to solve this issue by partitioning the SoC into isolated synchronous islands. This paper describes the bisynchronous FIFO used on the DSPIN network-on-chip capable to interface systems working with different clock signals (frequency and/or phase). Its interfaces are synchronous and its architecture is scalable and synthesizable in synchronous standard cells. The metastability situations and its latency are analyzed. Its throughput, maximum frequency, and area are evaluated in function of the FIFO depth.","Bi-Synchronous FIFO for Synchronous Circuit Communication Well Suited  for Network-on-Chip in GALS Architectures  Ivan MIRO PANADES  STMicroelectronics  17, avenue des Martyrs  38054 Grenoble, France  ivan.miro-panades@st.com  Alain GREINER  The University of Pierre and Marie Curie  4, Place Jussieu   75252 Paris, France  alain.greiner@lip6.fr  Abstract  The distribution of a synchronous clock in Systemon-Chip (SoC) has become a problem, because of wire  length and process variation. Novel approaches such  as the Globally Asynchronous, Locally Synchronous try  to solve this issue by partitioning the SoC into isolated  synchronous islands. This paper describes the bisynchronous FIFO used on the DSPIN Network-onChip capable  to  interface systems working with  different clock signals (frequency and/or phase). Its  interfaces are synchronous and its architecture is  scalable and synthesizable in synchronous standard  cells. The metastability situations and its latency are  analyzed. Its throughput, maximum frequency, and  area are evaluated in function of the FIFO depth.  1. Introduction  In deep sub-micron processes, the largest parts of  the delays are related to the wires. In multi-million  gates System-on-chip (SoC), achieving timing closure  is difficult, as place & route tools have difficulty  coping with long wires and balancing the clock tree.   The Globally Asynchronous, Locally Synchronous  (GALS) [18,19] approach attempts to solve  this  problem by partitioning  the SoC  into  isolated  synchronous islands that have frequency and phase  clock independency. With this approach, the timing  constraints of  the SoC can be bounded  to  the  isochronous limit of each island. In this case, the  communications between islands should be carried out  by mixed-timing  interfaces  that adapt  the clock  frequency and phase discrepancy. Such interfaces are  not  trivial [7] since  the synchronization failure  (metastability) of  the  registers can corrupt  the  transferred data. Many architectures have been  proposed to solve this issue, some of them restricted to  study only skew [12, 13, 17], and jitter [17] correction.  Others use pausible or stretchable clocks which pause  [1, 6], or stretch [2, 3] the receiver clock to synchronize  the data in a safe rising edge of the receiver clock.  A number of approaches interface systems with a  rational clock relation [11, 14]. A recent published  paper from The University of Pierre and Marie Curie  synchronous↔asynchronous  proposes  a  novel  converter well suited for Network-on-Chip (NoC) in  GALS architectures [20]. Also in [22] asynchronous to  synchronous  and  synchronous  to  asynchronous  interfaces for GALS NoC are implemented using Gray  FIFOs (first-in, first-out). Finally, the most generic  architecture where frequency and phase are unknown is  resolved by robust designs where metastability is well  analyzed. The basic architecture is the “two-flop  synchronizer”  [7] where only  the  request and  acknowledge  signals  are  synchronized. Fastest  architectures use FIFOs [4] or a RAM bank [5], where  write and read pointers are transferred between clock  domains using the Gray code. J. Jex et al. [9] and  Chelcea-Nowick [8] propose optimized architectures  for mixed-timing systems. The J. Jex et al. solution was  originally designed to be used on supercomputer  interfaces. The solution can modify the number of  registers in the synchronizer to increase the robustness  against metastability, and is suited to high bandwidth  communications since the control signals are optimized.  The solution of Chelcea et al. [8] proposes a mixedtiming interface for mixed synchronous to synchronous,  asynchronous to synchronous, and synchronous to  asynchronous interfaces. Its architecture is modular and  requires low area overhead. However, it requires full  custom cells to implement the Full and Empty detectors,  which becomes less attractive for synchronous standard  cell flows.  The FIFO presented in this document is a bisynchronous FIFO  [10] able  to  interface  two  synchronous  systems with  independent  clock  frequencies and phases. A latency optimization of the  architecture in which both interfaces have the same  clock  frequency  but  different  clock  phase  (mesochronous) is also proposed. The main features of          the bi-synchronous FIFO are low latency, robustness to  metastability, small area, scalability, maximized  throughput, and synthesizability on synchronous  standard cell flow. This bi-synchronous FIFO is been  used on the DSPIN [21, 23] Network-on-Chip (NoC),  where ten bi-synchronous FIFOs of 34 bits are used per  router:  Five have 8 words depth, and five have 4 words  depth. Since a SoC can contain more than 30 routers,  the area of this component have to be careful designed  to minimize the total SoC area. Furthermore, the depth  of the bi-synchronous FIFOs in this kind of NoC is no  deeper than 10 words, therefore a register-base FIFO is  more suited than a RAM-base FIFO due to its lower  area and its simplicity of test. Thus, its test method can  be  the  scan-chain method. Moreover,  the bisynchronous FIFO makes the DSPIN NoC well suited  to  the GALS approach as  the DSPIN  router  communicates  asynchronously with  the  local  subsystem, and mesochronously with the neighbor  routers.  The paper is organized as follows. Section 2  presents a novel encoding algorithm well suited to  interface two clock systems. In Section 3, all the bisynchronous FIFO modules are detailed. In Section 4,  the analysis of latency, throughput, area, and maximum  frequency are analyzed and compared with existing  solutions.  2. Bubble encoding  In this section, a novel-encoding algorithm based on  a token ring is demonstrated to be useful on the  synchronization of pointers between two independent  clock domains.   2.1. Token ring  A  token  ring  is a  succession of nodes  interconnected in a circular manner that contain tokens.  It can be described with N registers (with enable signal)  interconnected in serial, like a cyclic shift-register.  Figure 1 shows an example of a token ring with 5  registers.   N E N E N E N E N E Enable  Clk  Figure 1. Token ring  If the enable signal is true, the content of the  register is shifted (register i is shifted to register i+1, an  register N-1 to register 0) at the rising edge of the clock,  otherwise the register maintains the data. A token is  represented by the logic state 1 of the register. The  number of tokens in a token ring can be from one to N,  where N is the number of registers.  A token ring with one token can be used as a statemachine. The position of the token defines the state of  the state-machine. The enable signal of the token ring is  equivalent to the next-state of the state-machine. If this  condition is true, the token ring shifts and the statemachine changes. It is also possible to define a statemachine when the token ring contains two consecutives  tokens, the state of the state-machine can be defined,  for example, as the position of the first one.   2.2. Synchronizing the Token  Since the position of the tokens defines the state of  the state-machine, the synchronization of the position  can be exploited to interface the two clock domains. To  synchronize the state of the state-machine, a parallel  synchronizer (two registers per bit) can be used, as  shown in Figure 2.  Token ring Enable  Clk_write  EN  N E N E N E N E N E Clk_read  Figure 2. Synchronization of a token ring   Parallel synchronizer However, as described by R. Ginosar, the parallel  synchronizer [7] does not guarantee the correctness of  the  result. Figure 3  shows  an  example of  synchronization. Solution A, B, C and D are all the  possible solutions when the metastability of the register  changes its content. Solution A and B are correct since  the token is well determined. Solution C is exploitable  using some logic but Solution D is useless due to  absence of information. Because the token was moving,  the metastability of the register can be resolved to a  useless result. In that case, one clock cycle should be  waited to attempt to obtain a useful data.  To solve this issue, we propose to use two  consecutive tokens (bubble encoding) in the token ring.  As the metastability affects the changing registers, the  use of two consecutive tokens prevents some registers                        0  N E 1  N E 0  N E 0  N E 0  N E 1  EN  Metastability 1 EN 0 N E 1 N E 1 N E 0  N E 0  N E Metastability Possible solutions:  Solution A  Solution B  Solution C  Solution D  0  0  0  0  1  0  1  0  0  1  1  0  0  0  0  0  0 0 0 0 (useless)   Possible solutions:  Solution A  Solution B  Solution C  Solution D  0 0 0 0 1 0 1 0 1  1  1  1  0  1  1  0  0  0  0  0  Figure 3. Possible solution in the synchronization  of a single token ring containing one token  Figure 4. Possible solution in the synchronization  of a token ring containing two successive tokens  from changing. Assuming that registers i and i+1 have  the tokens, if the token ring shifts, register i+2 gets a  token, register i loses its token, and register i+1 does  not change (it shifts its token and gets a token). In  terms of logic value, register i and i+2 change state but  register i+1 remains unchanged. Because there always  exists a register that does not change state, it is always  possible to detect a token. Figure 4 shows an example  of synchronization. For example, we can define the  position of the detected token by the position of the  first logic 1 after a logic 0 (starting from the left). In  this case, all solutions A, B, C, and D are correct  because the token can be well defined; it is always  possible to detect a transition between 0 and 1. This  encoding algorithm does not avoid the metastability on  the synchronizer. It just guarantees that the position of  the token will be detected and a useless situation will  never occur.  The token ring and the bubble encoding presented  in this section are used on the definition of the statemachines of the bi-synchronous FIFO and will be  detailed in next section.  3.  Bi-synchronous FIFO  architecture  is scalable and synthesizable  in a  synchronous standard-flow without using custom cells.  As shown in Figure 5, five modules compose the bisynchronous FIFO architecture: Write pointer, Read  pointer, Data buffer, Full detector, and Empty detector.  The Write and Read pointers indicate the position to be  written and to be read in the Data buffer, the Data  buffer contains the buffered data of the FIFO, and the  Full and Empty detectors signal the fullness and the  emptiness of the FIFO.  To better understand the bi-synchronous FIFO, its  interfaces and protocol are detailed.  Write Write pointer  Empty  detector  Empty  Data_write Data buffe r  Data_read  Full Full  detector Read pointer  Read  Clk_write Clk_read  This section presents the architecture of the bisynchronous FIFO and its application in multi-clock  systems. The goal of this FIFO is to interface two  synchronous systems having different clock signals.  Each system is synchronous with its clock signal but  can be asynchronous (frequency and/or phase) to the  others. The challenge of this architecture is to hide all  synchronization issues while respecting the FIFO  protocol on  each  interface. Furthermore,  this  Figure 5. Bi-Synchronous FIFO architecture  3.1. Bi-synchronous FIFO interface and  protocol  The bi-synchronous FIFO has a sender and a  receiver interface. As shown in Table 1, each interface  has its own clock signal, Clk_write for the sender and  Clk_read for the receiver.                       Position of Write pointer  Write pointer bits  Data elements  Read pointer bits  …  …  0 0 0 1 1 …  0  0 1 1 0 0 …  0  …  …  0 0 0 0 1 …  1 0 1 1 0 0 …  0 …  …  1 1 0 0 0 …  0 0 1 1 0 0 …  0 …  …  0 1 1 0 0 …  0  0 1 1 0 0 …  0  …  …  0 0 1 1 0 …  0 0 1 1 0 0 …  0 Position of Read pointer  a) FIFO empty  b) FIFO contains 1 element  c) FIFO quasi-full 3  contains N-3 elements  d) FIFO quasi-full 2  contains N-2 elements  e) FIFO full  contains N-1 elements  Figure 6. Write and Read pointer position definition and Full and Empty conditions in  terms of tokens position   The FIFO protocol is synchronous; all input and  output signals in the sender and receiver interfaces are  synchronous to  their clock signal Clk_write and  Clk_read, respectively.  f a e c r e r e e d n n S t i Full  Table 1. Sender and receiver interface signals  Signal  Description  Data_write Data to be written into the FIFO  Write  Input signal requesting a write  into the FIFO  Output signal indicating the  fullness of the FIFO  Sender clock signal  Output data from the FIFO  Input signal requesting a read in  the FIFO  Output signal indicating the  emptiness of the FIFO  Receiver clock signal  Clk_write  Data_read  Read  Clk_read  Empty  n f i r e e c v a i e r c e e t R The queuing and dequeuing of data elements in the  FIFO follows the next synchronous protocol. The  Data_write is queued into the FIFO, if and only if, the  Write signal is true and the Full signal is false at the  rising edge of Clk_write. Symmetrically, data is  dequeued to Data_read, if and only if, the Read signal  is true and the Empty signal is false at the rising edge of  Clk_read.  The clear partitioning of the sender and receiver  interfaces into synchronous and independent interfaces  simplifies the timing constrains analysis for all the  modules connected to the FIFO ports.  3.2. Write and read pointers  The Write and Read pointers are implemented using  the described token rings with the bubble-encoding  algorithm. The position of the tokens determines the   position of  the pointer. The position of  the  Write_pointer is defined by the position of the register  containing the first token (starting from the left) as  shown in Figure 6a. Likewise, the position of the  Read_pointer is defined by the position of the register  after the second token (starting from the left). The Full  and Empty detectors exploit this particular definition of  the pointers and will be explained hereinafter.  The Write_pointer shifts right when the FIFO is not  full and the Write signal is true. Likewise, the  Read_pointer shifts right when the FIFO is not empty  and the Read signal is true.  As the write and read interfaces belong to different  clock domains, the token rings are clocked by their  clock signal, Clk_write and Clk_read respectively.  3.3. Data buffer  The Data buffer module is the storage unit of the  FIFO. Its  interfaces are: Data_write, Data_read,  Write_pointer, Read_pointer, and Clk_write. It is  composed by a collection of data-registers, AND gates,  and tri-state buffers as shown in Figure 7.  The input data, Data_write, is stored into the dataregister pointed by the Write_pointer at the rising edge  of Clk_write. AND gates recode the Write_pointer into  a one-hot encoding which controls the enable signals of  the data-registers. Likewise,  the Read_pointer  is  recoded into one-hot encoding which controls the tristate buffer on each data-register. Finally,  the  Data_read signal collects the outputs of the tri-state  buffers. It is also possible to replace the tri-state buffers  with multiplexers to simplify the Design for Test (DfT)  of the FIFO.  The width and number of data-registers determine  the width and the depth of the FIFO. The depth also  determines the range of the Write and Read pointers.              Write pointer module N E N E N E N E W0  W1 W2 W3  W4 Write  Full  Clk_write  EN  N E Data write  D  Q  EN  D  Q  EN  D  Q  EN  D  Q  EN  D  Q  EN  Data_read  R0  R1  R2  R3  R4  EN  N E N E N E N E N E Read  Empty  Clk_read  Figure 7. Write pointer, Read pointer, and Data buffer detail  Read pointer module 3.4. Full detector  The Full detector computes the Full signal using the  Write_pointer and Read_pointer contents. No status  register is used as in the J. Jex et al. [9] or ChelceaNowick [8] solutions. The Full detector requires N twoinput AND gates, one N-input OR gate, and one  synchronizer, where N is the FIFO depth (Figure 8).  The detector computes the logic AND operation  between the Write and Read pointer and then collects it  with an OR gate, obtaining logic value 1 if the FIFO is  Full (Figure 6e) or quasi-Full (Figure 6c and 6d)  otherwise is 0. This value is finally synchronized to the  Clk_write clock domain  into Full_s. Since  the  W0  R0  W1  R1  W2  R2  W3  R3  W(N -1)  R(N-1)  Wi:  Ri:  Write pointer i Read pointer i  Full_s  Clk_wr ite  Figure 8. Full detector detail  synchronization has a latency of one clock cycle and  the situation on Figure 6c can potentially be metastable,  the detector has to anticipate the detection of the Full  condition. Is for this reason that the output of the OR  gate detects the Full and the quasi-Full conditions.   The Full detector in Figure 8 can be optimized since  the synchronization latency inhibits, in some cases, the  FIFO from being completely filled. For example, if the  FIFO is in the situation of Figure 6c, and the sender  does not write any other data, the Full_s will be  asserted even if the FIFO is not filled completely. As  the fullness has to be anticipated, the detector is  dimensioned to stop the sender before overflowing the  FIFO, if the sender was writing continuously.  An improved Full detector implementation would  be more complex (as the Empty detector), and would  therefore require greater die area. However, a nonoptimal Full detector does not penalize the throughput  of the FIFO as much as a non-optimal Empty detector.  For example, assuming an optimal Empty detector and  a non-optimal Full detector, the Full condition occurs  when the receiver is not able to consume all the data. In  this case, even with a non-optimal Full detector, the  receiver limits the throughput of the FIFO.  Therefore,  design effort and chip area should be devoted to  improving the performance of the Empty detector.                      Full  Full_s  Write  Clk_wr ite  Figure 9. Full detector optimization Even when using a non-optimized Full detector, a  low cost optimization can improve its performance.  Figure 9 shows an additional module connected to the  Full_s signal, which improves the Full detector. The  module's operation is as follows: if the writer was not  writing before asserting the Full_s signal, the Full  signal is delayed one clock cycle, giving a second  chance to the writer to fill completely the FIFO.  3.5. Empty detector  The implementation of the Empty detector is similar  to the Full detector because both employ the Write and  Read pointer contents. As seen in the previous  paragraph, the Full detector has to anticipate the  detection of the Full condition to avoid FIFO overflow.  As the Empty detector is correlated to the FIFO  throughput, its detection has to be optimized, and no  anticipation detector should be used.   Figure 10 shows the Empty detector for a five word  FIFO. First, the Write_pointer is synchronized with the  read clock into the Synchronized_Write_pointer (SW)  using a parallel synchronizer. Next, the Read_pointer is  recoded into the AND_Read_pointer (AR) using twoinput AND gates, operation also done  in  the  Data_buffer module. The output of AR is a one-hot  encoded version of the Read_pointer. Finally, the  Empty condition is detected comparing the SW and AR  values using  three-input AND gates. As  the  metastability can perturb some bits of the SW (as seen  on Figure 4), each pair of consecutive bits is compared  to find a transition between 0 and 1. Their analysis is as  Write pointer module Wi:  SWi:  ARi:  Write pointer i  Synchronized Write pointer i  AND Read pointer i  Enable_write  Clk_write  N E EN  N E N E N E N E W0  W1  W2 W3 W4 Clk_read  0  0  0  1  1  SW0 = 0  SW1 = 0  SW2 = 0 SW3 = 1 SW4 = 1 AR0 = 0  AR1 = 0  AR2 = 1  AR3 = 0  AR4 = 0  From Data Buffer Synchronizer 0  N E 1  N E 1  N E 0  N E 0  N E Enable_read  Clk_read  EN  Read pointer module Figure 10. Empty detector detail SW0 = 0  SW1 = 0 AR0 = 0 SW1 = 0 SW2 = 0 AR1 = 0 SW2 = 0 SW3 = 1  AR2 = 1 SW3 = 1 SW4 = 1  AR3 = 0 SW4 = 1 SW0 = 0  AR4 = 0  0  0  1  0  0  Empty = 1                     follows, if the values of SWi = 0 and SWi+1 = 1 that  means that the SW pointer is on position i+1.  Furthermore, when ARi = 1 that means that the AR  pointer is on position i+1 (Figure 10).  The FIFO is considered empty (see Figure 6a) when  the Write_pointer points the same position of the  Read_pointer. This can be detected when SWi=0,  SWi+1=1 and ARi=1 for any i. These comparisons are  computed by means of the three-input AND gates.  Finally, a N-input OR gate collects all the values of the  three-input AND gates to generate the Empty signal.  This N-input OR gate and the one on the Full detector  can be decomposed with log2N levels of two-input OR  gates.  The latency introduced by the synchronization of  the Write_pointer cannot corrupt the FIFO, because a  change in this pointer cannot underflow/overflow the  FIFO, it just introduces latency into the detector.  The advantage of the bubble-encoding algorithm in  this detector relies on the continuous detection of the  Write_pointer position. Otherwise, as seen on Figure 3  Solution D, its position cannot be detected and the  Empty condition should be asserted to avoid a possible  underflow of the FIFO, thus, introducing one additional  clock cycle latency to the FIFO.  3.6. Mesochronous adaptation  The FIFO architecture was originally designed to  interface two independent clock domains, but can be  adapted to interface mesochronous clock domains  where the sender and the receiver have the same clock  frequency but different phase. The difference of phase  can be constant or slowly varying. We find examples  interfacing mesochronous clock domains that employ  the predictability of the rising edges to avoid the  metastability situations  [15, 16]. The proposed  adaptation lowers the FIFO latency by reducing the  number of registers on the synchronizer module. Since  metastability can be avoided when the rising edges of  the clock signals are predictable, the two rows of  registers on the synchronizer can be reduced to a single  row of registers as shown in Figure 11b. The remaining  row of registers is clocked using a delayed version of  the read clock. This delay must be chosen to exchange  the data without metastable situations (Figure 11a). The  delay can be a programmable delay, or any other  metastability-free  solution, as  for example  the  Chakraborty-Greenstreet [11] architecture allowing the  FIFO to work also on plesiochronous (small difference  of frequency) clocks. Likewise, if the write and read  clock are out of phase by 180º (clock-inverter), no  a)  IN  Clk_write  Clk_write  delay  Clk_ read  b)  OUT Clk_ read  delay  Write pointer  Read pointer  Figure 11. Mesochronous adaptation programmable delay  is needed because, byconstruction, the communication is free of metastability.  The interface is free of metastability also if the  difference of phase varies between 90° and 270°. This  type of implementation is done on the DSPIN NoC  routers. The clock-tree of each DSPIN router is  balanced with a 5% skew. Then, in the routers placed  on the Y row and X colon position, where (Y+X) is an  odd number, the first clock-buffer of the clock-tree  have been replaced by a clock-inverter. Finally, the top  pins of the clock-trees routers are balanced with a 50%  skew, which is easier to design and less power  consuming than a fully synchronous clock tree balance.  With this procedure, neighbor routers are 180° out of  phase with a tolerance of 50% skew.  This mesochronous  adaptation of  the bisynchronous FIFO is simple and allows switching  between mesochronous and asynchronous modes. This  adaptation is interesting in the design of a multi-million  gate SoC in deep sub-micron technology, where the  delay of  long wires can drastically vary with  temperature, voltage, and process. In such a system, the  mesochronous clock distribution could fluctuate to an  undesirable metastable situation, making the FIFO data  useless. By switching the bi-synchronous FIFO into the  asynchronous mode, robustness against metastability is  improved, preventing the SoC from requiring redesign.  4.  Simulation and analysis  Both synthesizable VHDL models and cycle  accurate SystemC models of the bi-synchronous have  been designed. We have simulated the bi-synchronous  FIFO to characterize its latency, throughput, frequency,  and area.  Clk_write  Data_write  Write  Clk_ read  Sync_1  Sync_2  Full  Data_read  Clk_write  Data_write  Write  Clk_ read  Delayed  Clk_ read  Sync_1  Full  Data_read  ∆T  TClk_read  TClk_read  Figure 12. Latency analysis   4.1. Latency analysis  As the sender and the receiver have different clock  signals, the latency of the FIFO depend on the relation  between these two signals.  The latency of the FIFO can be decomposed in two  parts: the state machine latency and the synchronization  latency. As the state-machines are designed using  Moore automates, its latency is one clock cycle. Two  registers compose the synchronizers and its latency is  ∆T plus one clock cycle. Where ∆T is the difference, in  time, between the rising edges of sender and receiver  clock. As this difference is between zero and one  Clk_read clock cycle, the latency of the bi-synchronous  FIFO is between two and three Clk_read clock cycles.  Figure 12 shows the detail of the latency. Sync_1 and  Sync_2 are the synchronization registers. The latency  of the bi-synchronous FIFO is equivalent to the latency  of the J. Jex et al. [9] solution. This latency can be  lower, but the robustness to the metastability would be  penalized [24, 7].  When the bi-synchronous FIFO is adapted to a  mesochronous clock distribution, the latency of the  FIFO is reduced, because a single register replaces the  two-register synchronizer. In addition, the ∆T is  constant as the difference of phase is constant. In that  case, the latency of the FIFO is one clock cycle plus ∆T,  as shown in Figure 13.  4.2. Throughput Analysis  The throughput of the bi-synchronous FIFO was  analyzed in function of the FIFO depth. As the  synchronizers add latency, the flow control of the FIFO  is penalized and its performances are influenced. In  case of deep FIFO, those latencies do not decrease the  FIFO throughput since the buffered data compensate  the latency of the flow control. Table 2 shows the  ∆T  TClk  Figure 13. Latency analysis with mesochronous  adaptation  minimum FIFO depth for 50% and 100% throughput in  function of the clock relation. For FIFO depth of 6 or  above, the synchronization latency has no influence on  the flow control and the FIFO is able to deliver one  word per cycle  (100%  throughput) even on  asynchronous clock relation. For the asynchronous  analysis, the write and read clock signals frequencies  are similar, otherwise it is not possible to obtain 100%  throughput.   Table 2. Minimum FIFO depth in function of the clock  relation and required throughput   Minimum depth  Minimum depth  for 50 %  for 100 %  throughput  throughput  5  6  4  5  Asynchronous  Mesochronous  4.3. Area and frequency estimation  The area and frequency estimation of the FIFO was  computed once synthesized on CMOS 90nm GPLVT  STMicroelectronics standard cells. Different FIFO  depths are used to illustrate the scalability of the  architecture and its performances in terms of maximum  frequency. To minimize the power consumption, a  clock gating technique is used. Two architectures were  synthesized, one with the tri-state buffers and another  with multiplexers.   Table 3 shows the area and frequency estimation of  a 32-bit bi-synchronous FIFO in function of the FIFO  depth and type of output port. Note that the maximum  frequency of the write clock is greater than the one of  the read clock. The limitation of the read clock is due  to the Empty detector.  The architecture with tri-state buffers has greater  area than the one with multiplexers. This phenomenon      is due to the large area of the tri-state buffers.  Moreover, the maximum clock frequency of the read  part with tri-states is greater than the one with  multiplexers, since the multiplexers are decoded in a  log2N manner rather than in parallel.  Type  Table 3. Area and frequency in function of FIFO depth   and type of output port  Area  Max. Write  (µm²)  Freq. (MHz)   3304  2000   6581  2000   13384  2000   4082  2000   8032  2000   16101  2000  Max. Read  Freq. (MHz)  1110  1000  769  1428  1250  1110  FIFO  Depth  4  8   16  4  8   16  x u M i t r T e a t s 4.4. Comparison with other existing designs  This architecture has been compared with similar  architectures to analyze its area and latency. As its  architecture is synthesizable with standard cells, the  comparison with the others is done after Synopsys  synthesis with the same timing constraints.  The selected architectures are a register-base Gray  FIFO and the J. Jex et al. [9]. Their VHDL description  has been written for different FIFO depths: 4, 8 and 16  words of 32bits. A clock-gating technique is applied  but no tri-state buffer is used. Table 4 shows the  estimated area and the area overhead percentage of  these architectures compared to the presented solution.  FIFO  Depth  Table 4. Area and overhead comparison between other   existing designs  Register-base  Gray FIFO  µm2 (%)   5113 (+54%)   9702 (+47%)   20364 (+52%)  µm2 (%)   3364 (+1.8%)   6858 (+4.2%)   14362 (+7.3%)  This  Design  µm2   3304   6581   13384  J. Jex et al. [9]   4  8  16  The register-base Gray FIFO has a 50% bigger area  than the presented architecture. Even if the number of  registers and synchronizers  is  lower  than our  architecture, the Gray code algorithm adds complexity  to the read and write state machines. Nevertheless, the  Full and Empty detectors are fully optimized, as the  write and read sides know the exact position of the read  and write pointer (after synchronization).   The J. Jex et al. [9] architecture has similar  complexity as ours, but its area increases more than  ours when the FIFO depth increases. Moreover, its Full  detector is not optimized and suffers the same problem  of the non-optimized Full detector presented in Figure  9. To correct this issue, the optimization of Section 3.4  could be used, regardless the increase of the total area.  In terms of FIFO latency, all three have the same  latency, 2-3 clock cycles, since all of them use Moore  state-machines and two flip-flops synchronizers.  5. Conclusions  A new bi-synchronous FIFO has been presented and  analyzed. It is well suited to interface different systems  working with independent frequency and/or phase  clock signals.  It uses a novel encoding algorithm combined with  an astute definition of the FIFO pointers that avoids the  utilization of status registers. Its write and read pointers  are directly combined to obtain the Full and Empty  detectors.   Both of its interfaces are synchronous to its relative  clock signals. Moreover, its architecture is designed to  be synthesized using a synchronous standard cell  design flow. None of its modules requires custom cells.  A simple mesochronous adaptation is proposed  which reduces the latency of the FIFO. Its latency is 23 clock cycles in asynchronous mode, and 1-2 clock  cycles in mesochronous mode.  Both SystemC cycle accurate and VHDL models of  the bi-synchronous FIFO has been designed. The FIFO  throughput depends on the FIFO depth. Throughput is  100% when the FIFO depth is six or above.  Using CMOS 90nm GPLVT STMicroelectronics  standard cells, we have synthesized and analyzed the  FIFO area and maximum frequency for different FIFO  depths. Two architectures are analyzed, one with tristate buffers and another with multiplexers. A 32-bit bisynchronous FIFO with eight words depth requires  6581µm2 and its maximum clock frequency is 1GHz.  The comparison with existent  synthesizable  asynchronous FIFOs shows a better integration density  at the same data latency.    "
NoC Design and Implementation in 65nm Technology.,"As embedded computing evolves towards ever more powerful architectures, the challenge of properly interconnecting large numbers of on-chip computation blocks is becoming prominent. Networks-on-chip (NoCs) have been proposed as a scalable solution to both physical design issues and increasing bandwidth demands. However, this claim has not been fully validated yet, since the design properties and tradeoffs of NoCs have not been studied in detail below the 100 nm threshold. This work is aimed at shedding light on the opportunities and challenges, both expected and unexpected, of NoC design in nanometer CMOS. We present fully working 65 nm NoC designs, a complete NoC synthesis flow and detailed scalability analysis","NoC Design and Implementation in 65nm Technology Antonio Pullini1 , Federico Angiolini2 , Paolo Meloni3 , David Atienza4,5 , Srinivasan Murali6 , Luigi Raffo3 , Giovanni De Micheli4 , and Luca Benini2 1Politecnico di Torino, Torino, Italy 2DEIS, University of Bologna, Bologna, Italy 3DIEE, University of Cagliari, Cagliari, Italy 4LSI, EPFL, Lausanne, Switzerland 5DACYA, Complutense University, Madrid, Spain 6CSL, Stanford University, California, USA Abstract As embedded computing evolves towards ever more powerful architectures, the challenge of properly interconnecting large numbers of on-chip computation blocks is becoming prominent. Networks-on-Chip (NoCs) have been proposed as a scalable solution to both physical design issues and increasing bandwidth demands. However, this claim has not been fully validated yet, since the design properties and tradeoffs of NoCs have not been studied in detail below the 100 nm threshold. This work is aimed at shedding light on the opportunities and challenges, both expected and unexpected, of NoC design in nanometer CMOS. We present fully working 65 nm NoC designs, a complete NoC synthesis ﬂow and detailed scalability analysis. 1 Introduction As steady progress is being made in the miniaturization of chip features, embedded systems are quickly evolving towards complex devices, including a large set of computation engines, dedicated accelerators, input/output controllers and multiple memory buffers. MultiProcessor System-onChip (MPSoC) is a commonly used term to describe the resulting outcome. However, this feature- and performanceoriented evolution is not devoid of signiﬁcant challenges, including mastering the increasing design complexity and minimizing power consumption. Moreover, miniaturization itself is bringing its own set of design issues at the physical level, originated primarily by an increasing ratio of wire vs. logic propagation delay. One of the most critical areas of MPSoC design is the interconnect subsystem, due to architectural and physical scalability concerns. Traditional shared bus interconnects are relatively easy to design, but do not scale well. Thus, evolutions have been conceived both from the protocol (e.g. outstanding transactions with out-of-order delivery) and the topology (e.g. bridges, crossbars) points of view. Nevertheless, scalability is still suboptimal, as protocol improvements still hit a bandwidth limit due to the available physical resources, and topological extensions require the use of bridges (i.e. multiple buses or “spaghetti-like” design)) or large area overheads in routing structures (i.e. using crossbars). Networks-on-Chip (NoCs) have been suggested as a promising solution to the scalability problem [5]. By bringing packet-based communication paradigms to the on-chip domain, NoCs address many of the issues of interconnect fabric design. Wire lengths can be controlled by matching network topology with physical constraints and bandwidth can be boosted by increasing the number of links and switches. Furthermore, compared to irregular, bridge-based assemblies of clusters of processing elements, NoCs also help in tackling design complexity issues [21, 6]. While these key advantages of NoCs have been largely accepted nowadays, the practical implementation of NoCs in very deep submicron technology, below the 100 nm threshold, is a very open challenge. The crucial issue is again related to wiring. Even if capacitive loads and propagation delays can be controlled much better than in shared buses, issues such as wiring congestion, link power consumption, and the need for placement-aware logic synthesis still have to be explored to assess the feasibility of NoCs in forthcoming technology nodes. This paper presents a detailed description of a 65 nm NoC design ﬂow and outlines some of the tradeoffs that a next-generation back-end implies. In this work we explore link performance, placement issues, scaling results in shifting from 90 nm to 65 nm technologies, and the degrees of freedom allowed by the availability of multiple libraries (with different power/performance tradeoffs) at the same technology node. The remainder of the paper is organized as follows. In Section 2 we overview previous work in the ﬁeld on onchip interconnects in general and, more particularly, focus on NoC synthesis and topology design and layout. Then, in Section 3 we introduce our NoC design ﬂow, spanning from the target application task graph to the placement&routing steps. Next, in Section 4 we analyze and discuss the major properties of the 65 nm NoC design ﬂow, while in Section 5 we show performance, area and power comparisons. Finally, in Section 6 we draw our conclusions and propose future extensions. 2 Related Work The problem of high-performance or low-power synthesis of on-chip interconnects based on the bus paradigm has been studied extensively in the literature [30, 12, 31]. The use of point-to-point links and bus design using ﬂoorplan feedback has been also explored [17]. Recent research has focused on efﬁcient synthesis methods for NoC-based interconnects and comparisons with bus-based SoCs [18, 24, 2]. Relevant research in application-speciﬁc custom topology design has been proposed in earlier works on NoC design for well-behaved or regular trafﬁcs [32, 16, 9]. Floorplanning estimations and analytical models have been employed during the topology design process to obtain area and wirelength estimates [34, 24], but these works are limited to libraries of standard topologies. A physical planner has been used to focus on minimizing power consumption on the links of NoC topologies [1]. However, this method does not consider the area and power consumption of switches in the design. In [27], a ﬂow that addresses the problem of full custom NoC topology design with early ﬂoorplan estimation is proposed. However, even though these works have considered the various problems of NoCs synthesis at the physical level, none of them has studied and covered extensively the possible consequences of different process technology nodes on complete NoC-based interconnects, as we present in this paper. In addition, methods to build area and power models for various NoC components have been developed to enable system-level exploration of SoCs using NoC interconnects [4, 14, 29, 39]. However, the existing area-power models of the NoC components, such as switches or network interfaces, were not targeting the 65 nm manufacturing technology and may need to be revised with the latest back-end tools to properly capture model requirements, as we illustrate in our results. At a higher level of abstraction, different methods have been proposed to analyze trafﬁc information and obtain models that can be utilized as inputs to bus and NoC design methodologies [22, 26]. These approaches are complementary to this work. In addition, the problem of supporting multiple applications has been studied [3, 25]. Also, methodologies that unify resource reservation, mapping and routing in NoC designs have presented [15, 23]. However, these works do not fully explore the topology design process. Important research contributions have been presented on automatic code generation of NoC topologies for simulation [40, 33, 10] and synthesis [20, 19]. These works complement the presented one, as their inputs are typically NoC designed topologies and even enable the use of postsynthesis timing libraries in their simulation models. A very interesting study on the impact of technology scaling on the energy efﬁciency of standard topologies (such as meshes, tori and their variants) has been presented in [13]. Our work differs from this research in two ways: ﬁrst, we consider the design of platform-speciﬁc NoC topologies and architectures. Second, we use a complete design ﬂow that is integrated with standard industrial toolchains to perform accurate physical implementations of the NoCs. 3 NoC Design Flow An overview of our proposed complete ﬂow for designing NoCs for MPSoCs is presented in Figure 1. This ﬂow comprises several tools that are integrated together. First, SunFloor, which automates the front-end process of NoC design. Second, ×pipesCompiler, which automates the architecture generation phase (leveraging the ×pipes library of NoC components). Finally, several industrial tools that automate the back-end processes, i.e. the logic synthesis and physical design. 3.1 SunFloor The SunFloor tool, presented in [27], is used to synthesize the best custom NoC topology for a given MPSoC platform, which satisﬁes the communication constraints of the design. The tool uses as inputs the communication characteristics of the applications, as well as the design objectives and constraints. Then, it generates the optimal network topology, i.e. the number and size of needed switches, the connectivity between them and the paths used by the different trafﬁc ﬂows to route data across the switches. To this end, the tool includes several important features: • It supports multiple optimization objectives, such as minimizing power consumption and maximizing performance. • It synthesizes topologies that are free from messageand routing-dependent deadlocks. • It closes the gap between the architectural and physical design phases by performing a ﬂoorplan-aware synthesis process. • It automatically sets the NoC architectural parameters, like the frequency of operation. 3.2 SunFloor Area and Power Libraries In order to synthesize efﬁcient NoC topologies, the SunFloor tool needs accurate area and power models describing the NoC switches and links [27]. The models used need to be parametric in various aspects, such as the switch cardinality. As it can be seen from Figure 1, our toolchain not only supports a front-end to back-end ﬂow, but also has feedback from the back-end physical design process to the front-end phase. Figure 1. Our proposed complete NoC design ﬂow for MPSoCs At the beginning of the NoC design process, preliminary syntheses (including placement&routing) of the switches are carried out, varying several architectural parameters, namely, the number of input and output ports, the depth of buffers and the ﬂit width. The same process is performed for NoC links, varying their length and ﬂit width. Then, from the layout implementations, the area, power and delay values are obtained for the different conﬁgurations. The results are stored as tables and utilized by the SunFloor tool. The area, power and frequency values for some example switch conﬁgurations are shown later in Figure 4. The power consumption of different NoC links is presented in Figure 3. 3.3 The ×pipes NoC Library In the next phase of the design ﬂow, the RTL-level SystemC code of the switches, network interfaces and links for the designed topology is automatically generated. To this end, we use ×pipes [7], a library of soft macros for the network components, and the associated tool ×pipesCompiler [20], which conﬁgures and interconnects the network elements and the cores. The RTL-level SystemC representation of ×pipes can be fully simulated within a cycle-accurate virtual platform, to assess and optimize its performance and for validation purposes. ×pipes includes three elementary components: switches, Network Interfaces (NIs) and links, which are highly conﬁgurable to be able to build any NoC topology. Switches can be instantiated with any number of input and output ports, and include FIFOs at each port to implement output buffering. The switches include logic to implement an ACK/NACK ﬂow control policy [11], and handle priorities among incoming packets that demand the same physical link. NIs are instantiated to enable the communication of any external component to the NoC (e.g. processing cores or memories) using the Open Core Protocol (OCP) v2.0 [28]. The NIs are in charge of converting the OCP transactions into packets and then into sequences of bits or FLow control unITS (ﬂits), which are the basic transmission element, thus limiting the number of physical wires required for each link. Two different types of NIs can be instantiated in ×pipes, according to the role of the connected component, namely initiator and target. For master/slave cores, two NIs (one of each type) need to be instantiated. ×pipes uses source-based routing. Hence, each NI includes a Look-Up Table (LUT) that holds the paths to reach the other cores with which communication is expected to happen. The connectivity of each core to other cores, and therefore its associated LUT, is deﬁned in the previous phase of our NoC synthesis process (see Section 3.1). ×pipes supports link pipelining, where logical buffers are interleaved along links. This feature reduces signal propagation delays, and as we illustrate in our analysis (Section 4.2) and results (Section 5), it is a very relevant element in latest technology nodes. ×pipes is a fully synchronous NoC. The main reason for this choice is to avoid the design ﬂow complexity, the hardware cost and the performance overhead associated with clock domain crossing. However, the NIs feature two different clock inputs on the NoC side and on the OCP side; the only constraint is that the ×pipes frequency should be an integer multiple of the frequency of the OCP core. This arrangement has minimal hardware and performance penalties, while still providing ﬂexible support for attaching to the NoC cores running at many different possible speeds. 3.4 Flow Back-End In the proposed NoC design and synthesis framework for ×pipes we provide a complete back-end ﬂow based on standard cell synthesis. First, we perform the logic synthesis step, by utilizing standard Synopsys tools. The details of this part of the ﬂow will be given in Section 4.1. We follow this procedure by using 90 and 65 nm technology libraries by a partner foundry, tuned for different performance/power tradeoffs, with different threshold and supply voltages. While full custom design would certainly improve results, it would also greatly decrease ﬂexibility and increase design time. Figure 2. The synthesis ﬂow for ×pipes During synthesis, we can optionally instruct the logic synthesis tools to save power when buffers are inactive by applying clock gating to NoC blocks. The gating logic can be instantiated only for sequential cells which feature an input enable pin, which are a large majority of the datapath ﬂip-ﬂops of ×pipes. We subsequently perform the detailed placement&routing step within Synopsys Astro [35]. Two of the main placement strategies commonly available within industrial tools are virtual ﬂat and soft macros. In the former option, the tool is fed with the complete design, and albeit placement guidelines can be given, the tool is allowed to modify the global ﬂoorplan. This theoretically allows for maximum optimization and better handling of design violations; unfortunately, for a design as large as a whole NoC-based chip, we found it to be extremely demanding on system resources (more than 5 GB of RAM were needed by the placement process, and runtimes were unacceptable). The soft macro alternative is based on rigid fences which separate ﬂoorplan areas. Each module of the design is assigned to one such area; the tool is able to freely perform placement operations within such modules and areas, but it is not allowed to trespass fences. We resort to a mix of the two strategies for optimal results. First, we feed Astro with a rough ﬂoorplan, generated either manually or by SunFloor. This ﬂoorplan contains hard macros and soft macros, separated by fences. The hard macros represent IP cores and memories, and are modeled as black boxes. Hard macros are deﬁned with a Library Exchange Format (LEF) ﬁle and a Verilog Interface Logical Model, and obstruct an area of our choice. These boxes also obstruct some of the metal layers laying directly above; the exact number of obstructed levels is conﬁgurable, depending on how many metal layers the IP cores are supposed to require and on whether we want to allow over-the-cell routing for the NoC wires vs. between-the-cell. Soft macros enclose the modules of ×pipes; by constraining the placement tool to operate on one tile at a time, faster runtimes can be achieved. For proper results, however, it becomes necessary to specify rough timing constraints at the soft macro boundaries; we achieve this by pre-characterization of the links (please see Section 4.2 below). The next step in the ﬂow is clock tree insertion. While a separate clock tree could be added to each soft macro, it would be difﬁcult to control the skew when joining the trees together and attaching them to a single clock source. Therefore, for this step, we shift up again in the design hierarchy, and operate at a global level. The clock tree is added by leveraging clock borrowing algorithms in the tools; in other words, clock skews are exploited to accommodate the delay properties of the circuits, by supplying wider clock periods where the logic paths are most critical. Once the clock tree has been generated, its wires are kept untouched within the tool, to prevent further skews from appearing. At this point, the power supply nets are added. To improve supply stability, we choose the power grid scheme instead of the traditional power ring; power nets are distributed from the topmost metal layers of the chip, instead of from a ring around the die. This minimizes IR drops (voltage drops and ﬂuctuations due to resistive effects in the supply networks and to the current draw). After the power nets have been routed, the tool begins to route the logic wires. After an initial mapping, search&repair loops are executed to ﬁx any violations. As a ﬁnal step, post-routing optimizations are performed. This stage includes crosstalk minimization, antenna effect minimization, and insertion of ﬁller cells. Finally, a signoff procedure can be run by using Synopsys PrimeTime [38] to accurately validate the timing properties of the resulting design. 3.5 Post-Layout Analysis Post-layout veriﬁcation and power estimation is achieved as follows. First, the HDL netlist representing the ﬁnal placed&routed topology, including accurate delay models, is simulated by injecting functional trafﬁc through the OCP ports of the NIs. This simulation is aimed both at verifying functionality of the placed fabric and at collecting a switching activity report. At this point, accurate wire capacitance and resistance information, as back-annotated from the placed&routed layout, is combined with the switching activity report using Synopsys PrimePower [37]. The output is a layout-aware power/energy estimation of the simulation. 4 Wire Design in 65 nm Technologies As mentioned above, wires are a very important element in sub-100 nm technologies. Our experience with a 65 nm design ﬂow has shown that wires are critical both within NoC modules and for inter-module links. The following subsections will brieﬂy describe our ﬁndings at both levels. 4.1 Placement-Aware Logic Synthesis The traditional ﬂow for standard cell design features logic synthesis and placement as two clearly decoupled stages. While our in-house experience [2] shows that this ﬂow achieved reasonable enough results for 130 nm NoC design, we have found this assumption to be substantially inadequate at the 65 nm node. The origin of the problem lies in the same concept that enables the splitting of the two steps, namely, wireload models. Wireload models are pre-characterized equations, supplied within technology libraries, that attempt to predict the capacitive load that a gate will have to drive based on its fan-out alone. A gate driving a fan-out of two other gates is very likely to be part of a local circuit. Thus, its capacitive load is little more than the input capacitance of the two downstream gates. A gate with a fan-out of one thousand is likely to be the driver of a global network. Therefore, some extra capacitance is expected due to the long wires needed to carry the signal around. This assumption works very well as long as wire loads do not become too large. Otherwise, the characterization of wireload models becomes very complex, and the prediction inaccuracies become critical. In our 65 nm test explorations, we have found unacceptable performance degradation due to inaccuracies in wireload estimation. Even when synthesizing single NoC modules (i.e., even without considering long links), after the logic synthesis step, tools were expecting some target frequency to be reachable. However, after the placement phase, the results were up to 30% worse. Unfortunately, traditional placement tools are not able to deeply modify the netlists they are given as an input. In general, they can only insert additional buffering to account for unexpected loads on few selected wires. Therefore, if the input netlist is fundamentally off the mark due to erroneous wireload expectations, not only a performance loss is certain, but the placement runtime skyrockets. To address this issue we leverage placement-aware logic synthesis tools, such as Synopsys Physical Compiler [36]. In this type of ﬂow, after a very quick initial logic synthesis based on wireload models, the tool internally attempts a coarse placement of the current netlist, and also keeps optimizing the netlist based on the expected placement and the wire loads it implies. The ﬁnal resulting netlist already considers placement-related effects. Therefore, after this netlist is fed to the actual placement tool, performance results do not incur major penalties. In addition, we have found other wiring- and placementrelated problems within soft macros due to congestion. In our test designs, placements tools performed poorly both when modules had to be placed within too small and too wide fences. While the former case is clearly understandable, we attribute the unexpected latter effect to the placement tool heuristics, which are probably performing worse when the solution space becomes very large. Thus, the problem must be solved by proper tuning of the spacing among the soft macro fences and, consequently, accurate area models of the NoC modules are required to avoid very time-consuming manual interventions in the synthesis process. sumption. The reason is that when links are pushed for high performance, back-end tools automatically insert large amounts of buffering gates, increasing the energy cost of the links. In our validation experiments, the feasibility threshold of high-frequency or very long links was in some cases set by the inability to decrease delay further and in some cases by crosstalk concerns. In other words, the added buffers would sometimes be too large to be safely deployed. Another extremely important dependency we noticed was on the speciﬁc technology library in use. As Section 5 shows, especially at the 65 nm node, a single “technology library” no longer exists for standard cell design. In fact, manufacturing technologies are spreading across a variety of libraries optimized for speciﬁc uses, such as low power or high performance, with several intermediate levels featuring for example different threshold voltage values. In this case, if very low power libraries are used, the size and speed of the buffers that can be interleaved along wires becomes dramatically inferior, which results in much tighter constraints on frequency of operation or length. Figure 3(a) reports power consumption for a 65 nm low power library tuned for a low threshold voltage (called LP-LVT in the following), and therefore for a power/performance tradeoff. Figure 3(b) is based on a 65 nm low power library tuned for a high threshold voltage (LP-HVT), and therefore for minimum power consumption. As can be seen, the LP-HVT library is substantially more power effective than the LP-LVT library, but puts much tighter constraints on link feasibility. Link repeaters can be used to tackle this issue. We deﬁne repeaters as clocked registers along links. By providing one or more extra clock periods to traverse long distances, they solve the link infeasibility problem at a much lower cost than that of deploying whole NoC switches in the middle of the links. In some cases, repeaters may even produce more power-effective solutions than regular wire buffering along particularly critical links, but at a performance cost (i.e., one extra cycle of latency). In all cases, the NoC ﬂow control protocol must be designed in such a way as to enable a transparent insertion of the repeaters. Alternatively, repeaters must contain extra logic to properly handle the ﬂow control handshake signals. In our design ﬂow we include support for pipelined links at all levels of abstraction, starting from the high-level SunFloor tool down to ﬁnal layout tools. In fact, in our earlier work [27], the topologies synthesized by SunFloor required that the links could be traversed in a single clock cycle. In this work, we have removed this assumption by including in SunFloor the pre-characterization of link delay information. Therefore, Sunﬂoor automatically pipelines long links in the design, based on the targeted frequency of operation. When a link is pipelined and its latency increases, SunFloor considers this information to determine the average latency of the NoC and, therefore, takes it into account in its cost metrics. 4.2 Link Delay and Link Power 5 Experimental Results In order to assess the impact of global wires, we have studied 65 nm NoC links in isolation from the NoC modules. An overview of some of our analyses can be found in Figure 3. Our results show that several factors have to be considered in link design. Two obvious factors are link length and desired clock frequency. Short links or links clocked at a very slow frequency do not pose problems. However, as either length or target frequency are increased, an undesired effect appears in the form of high power con5.1 Technology Scaling from 90 to 65 nm In our ﬁrst set of results (see Figure 4) we have studied the effect of scaling when the ×pipes switches are synthesized in four different libraries, namely, two 65 nm and two 90 nm ones, tuned for different power/performance tradeoffs (LP-LVT and LP-HVT). In these experiments, switches were fully placed&routed, including the addition of a clock Normalized power 45 40 35 30 25 20 15 10 5 0 9,0 5,0 Link length (mm) 2,5 1,5 0,5 250 500 750 1000 1250 1500 1750 2000 Clock frequency (MHz) (a) performance/power oriented 65 nm library (LP-LVT) (a) power 6,0 5,0 4,0 Normalized power 3,0 2,0 1,0 0,0 9,0 5,0 Link length (mm) 2,5 1,5 0,5 250 500 750 1000 1250 1500 1750 2000 Clock frequency (MHz) (b) very low-power 65 nm library (LP-HVT) (b) operating frequency Figure 3. Power consumption of 38-bit links of varying lengths at different operating frequencies. Values normalized to shortest link at slowest frequency for conﬁdentiality reasons. Missing columns represent infeasible length/frequency combinations. tree. Then, syntheses were tuned for the maximum operating frequency. To this end, we disabled the clock gating option. As can be seen in the results, 65 nm libraries provide large opportunities for improvement over their 90 nm predecessors. In fact, we have observed power consumptions which are about 50% lower (up to 75% lower when comparing the LP-HVT versions), and area savings of 4050%. It is also important to observe the large difference in synthesis results among two different libraries at the same technology node. For the 65 nm case, the LP-HVT library is consuming one order of magnitude less power than the LP-LVT variant. In addition, our results indicate that this performance spread is increased compared to the 90 nm libraries. For example, by observing the achievable clock frequency, LP-HVT 65 nm libraries reach 50% lower frequencies than their 90 nm equivalents, but LP-LVT 65 nm libraries are actually 25% faster than their 90 nm equivalents. This trend suggests that new degrees of freedom are available to designers in new technology nodes. In our second set of experiments we have analyzed complete NoC topologies, namely 4x4 meshes (see Figure 5). We have synthesized them with the higher-performance version of the 90 nm and 65 nm libraries presented above. For the 90 nm case, we modeled IP cores as 1 mm2 obstructions, (c) area Figure 4. Analysis of two representative ×pipes switches in different technology libraries. Figures normalized to the 4x4 switch in the LP-HVT library. while, for the 65 nm topologies, we assumed the same hypotheses and a scaled one, where IP cores require 0.63x0.63 mm2 . The area scaling factor is derived from datasheet analyses and experiments on adder designs. As the results in Table 1 show, the jump to the 65 nm node presents large advantages in area, power consumption and maximum achievable frequency. The most impressive result is the power over bandwidth metric, which improves by a factor of 2. The gains are similar to those for single switches reported above, except for the power consumption ﬁgure, which features smaller savings. The main reason is that, in regular meshes, links are generally (a) 90 nm, 1 mm2 obstructions (b) 65 nm, 1 mm2 obstructions (c) 65 nm, 0.63x0.63 mm2 obstructions Figure 5. Three 4x4 ×pipes meshes. Relative frequency Relative cell area Relative power Relative bandwidth Relative power/bandwidth Relative link power 90 nm, 1 mm2 1.00 1.00 1.00 1.00 1.00 1.00 65 nm, 1 mm2 1.25 0.49 0.66 1.25 0.53 1.16 65 nm, 0.63x0.63 mm2 1.25 0.48 0.63 1.25 0.50 0.71 Table 1. Synthesis results on three 4x4 NoC meshes. Figures normalized to the 90 nm results. short (at most 1.2 mm in the meshes with 1 mm2 cores), enough so to not represent a performance bottleneck even at the 65 nm node. However, in 65 nm technology, there is still a power consumption penalty to be paid due to the extra required buffering along the wires. For this reason, the links in the 65 nm mesh with 1 mm2 cores, which are the most constrained of this experiment due to a mix of technology properties, length and operating frequency, are the most power-expensive and have an impact on overall ﬁgures. The scaled 65 nm mesh is less link-constrained, leading to slightly smaller area and power consumption. 5.2 Topology design Next, we have applied the SunFloor tool to a high bandwidth application, typical of today’s video applications, and to a low bandwidth application, typical of mobile applications. 5.3 High Bandwidth Application The objective of this experiment, whose results are outlined in Table 2, was twofold. First, we aimed at ﬁnding the impact of technology scaling on the sizes of the communication architectures and on the topologies required to match the application characteristics. Second, we wanted to analyze the impact of the choice of libraries (i.e. LP-LVT or LP-HVT) used for the technology process. The comparisons we performed are: • Same Platform for both 90 nm and 65 nm In this experiment, we assumed that the same platform would be used in 90 nm and 65 nm nodes, and we tried to ﬁnd the impact of technology scaling on the designed Figure 6. Enhanced VOPD application, called DVOPD, with the capability to decode two streams in parallel. NoCs. This is often done by system designers, who reuse the same platform (possibly as a part of a bigger system) to reduce the design and veriﬁcation efforts. This analysis is based on a Dual Video Object Plane Decoder (DVOPD) application, where two video streams are decoded in parallel by utilizing 26 processing/hardware cores. This application is a scaled version of the VOPD benchmark presented in [8]. The communication characteristics of the DVOPD benchmark are shown in Figure 6. We have assumed that the cores of the application (each core is represented by a vertex in Figure 6) were of size 1 mm2 in 90 nm technology and would shrink to 0.63x0.63 mm2 when migrating to 65 nm. Library, Application 90 nm LP-LVT, DVOPD 90 nm LP-HVT, DVOPD 65 nm LP-LVT, DVOPD 65 nm LP-HVT, DVOPD 65 nm LP-LVT, DVOPDX2 65 nm LP-LVT, TVOPD Max Freq. 400 MHz 400 MHz 800 MHz 800 MHz Switch Count 4 4 6 10 Largest Switch 10x9 10x9 7x6 7x7 Switch Power 140.83 mW 59.13 mW 131.99 mW 189.35 mW Link Power 57.58 mW 24.46 mW 47.98 mW 79.93 mW Total NoC Power 198.3mW 83.59 mW 179.97 mW 269.29 mW Avg. latency 3.42 cycles 3.91 cycles 4.24 cycles 4.35 cycles Table 2. High Bandwidth Application Results • Higher Bandwidth Platform in 65 nm To evaluate the scalability of the interconnect in 65nm technology, we have additionally considered a second benchmark, where the bandwidth requirements of the DVOPD were doubled, referred to as DVOPDX2. • Larger and Higher Bandwidth Platform in 65nm As the core sizes are smaller in 65 nm technology, we could ﬁt more cores on the chip in comparison to 90 nm. Therefore, to take this effect into account, we considered a third benchmark, called TVOPD, where 3 video streams were decoded in parallel following the same graph as in the DVOPD application (shown in Figure 6), instead of 2 video streams as in DVOPD. This new design consisted of 38 cores. We also assumed that the base application bandwidth requirements would be doubled, as in DVOPDX2. The characteristics of the NoCs synthesized by our tool chain for the benchmarks are shown in Table 2. The average latency presented in the table is deﬁned as the latency for a head ﬂit of a packet to move from the output of the initiator NI to the input of the target NI, when there is no congestion in the network. In this study, we ﬁxed the network ﬂit width to match the data width of the cores (equal to 32 bits). The DVOPD application bandwidth requirements demanded a 400 MHz operation for the NoC, which was automatically determined by the SunFloor tool. We could observe several interesting facts: • The switches designed using the LP-HVT libraries were not able to meet the required frequency and bandwidth requirements, due to their focus on very low power operation. Thus, only the LP-LVT libraries resulted in valid designs for the benchmark. • For the DVOPD application (represented by the rows 1-4 in the table), the best topology synthesized by our tool ﬂow remains the same (i.e., same switch count and sizes), with both 90 nm and 65 nm libraries. The ratio of link power to switch power consumption, however, increased when moving to the 65 nm technology. This is despite the fact that, for this benchmark, the core sizes were smaller in 65 nm technology, which led to an overall reduction in the total length of wires. The reason for this reduction was that the switch power consumption reduces by 55% when we moved from 90 nm to 65 nm, whereas the wire power consumption was reduced only by 31%. This result is in agreement with the ﬁndings in Table 1. • The number of switches needed increased to 6 and 10 for the DVOPDX2 and TVOPD scenarios, respectively. This is because these benchmarks have doubled bandwidth requirements with respect to the DVOPD application; thereby, they require double the operating frequency for the NoC (800 MHz). In fact, as s k n i L f o r e b m u N 80 70 60 50 40 30 20 10 0 unpipelined 1−stage pipeline DVOPDX2 TVOPD Figure 7. Amount of pipelined links in two sample benchmarks. big switches cannot satisfy such a high operating frequency, the SunFloor tool synthesized a design with many smaller switches. As the topology size increases, as expected, the average head ﬂit latency also increases. • The 65 nm technology is very power efﬁcient. In fact, this technology supported twice the application bandwidth requirements (the DVOPDX2 benchmark) at a lower power consumption than the 90 nm technology library. 5.4 Eﬀect of Link Pipelining The SunFloor tool automatically pipelines long links, based on the required NoC operating frequency and the link lengths obtained from the ﬂoorplan of the design. Such link pipelining is needed for NoCs that require a high operating frequency. As an example, without link pipelining support, the NoC for the DVOPDX2 and TVOPD designs could only operate at 500 MHz, while the application bandwidth requirements necessitate 800 MHz operation. In Figure 7, we plot the number of pipeline stages required for the different links in the DVOPDX2 and TVOPD designs. A nonpipelined link requires one clock cycle for traversal, while a link with a single pipeline stage requires two clock cycles. For all these benchmarks, we found that all the links could be traversed within 2 clock cycles. As the design complexity increases (when we move to the TVOPD design), the portion of links that require pipelining also increases. The SunFloor tool automatically considers the increase in latency due to link pipelining when determining the average latency of the NoC, and is therefore able to account for the overhead in its performance metrics.     Library 90 nm LP-LVT 90 nm LP-HVT 65 nm LP-LVT 65 nm LP-HVT Max Freq. 50 MHz 50 MHz 50 MHz 50 MHz Switch Count 2 2 2 5 Largest Switch 11x11 11x11 11x11 9x9 Switch Power 10.46 mW 4.27 mW 4.72 mW 2.61 mW Link Power 5.47 mW 2.1 mW 2.31 mW 1.65 mW Total NoC Power 15.93 mW 6.36 mW 7.03 mW 3.86 mW Avg. latency 3.94 cycles 3.94 cycles 3.94 cycles 3.94 cycles Table 3. Low Bandwidth Application Results. Application DVOPD DVOPD DES DES DES DES Library 90 nm LP-LVT 65 nm LP-LVT 90 nm LP-LVT 90 nm LP-HVT 65 nm LP-LVT 65 nm LP-HVT Bandwidth per mW 67.27 MB/s/mW 159.64 MB/s/mW 94.62 MB/s/mW 229.56 MB/s/mW 207.68 MB/s/mW 378.23 MB/s/mW Table 4. Bandwidth supported per milliwatt of power consumption Figure 8. DES benchmark. 5.5 Low Bandwidth Application NoCs can be used effectively not just for high bandwidth applications, but also for low bandwidth applications that have tight power budget constraints. Therefore, in our ﬁnal set of experiments, we have assessed the performance of NoCs to forthcoming requirements of low-power applications and mobile systems. In order to represent mobile applications with these low power requirements, we have considered the DES encryption benchmark, a low bandwidth application that is implemented on 19 cores. The communication characteristics for the benchmark are shown in Figure 8. The designed NoCs for the 2 different technologies for the LP-LVT and LP-HVT libraries are shown in Table 3. As seen from the table, for low power requirements, the LP-HVT libraries are far superior to the LP-LVT libraries. As an example, for the DES mapping in the 65 nm LP-HVT technology, we also present the resulting chip layout in Figure 9. In addition, we investigated the energy efﬁciency of the NoCs for the different applications across the different technology generations. The total bandwidth required by the DVOPD application is 13.34 GB/s, while for the DES application, it is 1.46 GB/s. In Table 4, we present the bandwidth supported per milliwatt of power consumption by the different NoC designs for the DVOPD and DES applications. This metric captures the energy efﬁciency of the different technology libraries. The 65 nm technology libraries have much higher energy efﬁciency. For example, for the DES application, using the LP-LVT libraries, a 2.19X improvement is obtained when compared to the 90 nm technology. Another interesting fact to note is that, for the DES application, the NoC supports a higher bandwidth per mW power consumption than for the DVOPD application. This is because of two reasons: ﬁrstly, the DVOPD application needs a higher operating frequency, which requires the synFigure 9. Layout of the DES mapping on 65 nm LP-HVT technology. Over-the-cell routing was allowed in this example. thesis tools to utilize more power intensive components for the switches. Secondly, the communication trafﬁc is more evenly spread in the DVOPD application, thereby requiring more inter-switch trafﬁc ﬂows than the DES application. Finally, we have compared the quality of the custom topology generated for the DES benchmark with that of a mesh topology (19 switches, with each core connected to a switch) and a quasi-mesh topology (10 switches, with 2 cores connected to a single switch). In this case we have performed cycle-accurate simulations of the DES benchmark with the designed NoCs using the ×pipes platform [20]. The total application runtimes for the 3 designs are shown in Figure 10. As this ﬁgure indicates, the entire application performance (which also includes the time for computation) improves by 7% when the custom topology is used. 6 Conclusions And Future Work NoCs have emerged as a promising structured way of realizing interconnections on silicon, and obviate the limitations of bus-based solution. NoCs can have regular or ad-hoc topologies, and it is essential to assess their performance and power features in forthcoming technology nodes. In this paper, we have performed a complete and thorough study of the trends imposed by deep submicron manufacturing processes in fully working 65 nm NoC designs. Moreover, we have presented a complete platform In [6] L. Benini, et al. Networks on chips: Technology and Tools. Morgan Kaufmann Publishers, 2006. [7] D. Bertozzi, et al. xpipes: A network-on-chip architecture for gigascale systems-on-chip. IEEE Circuits and Systems Magazine, 2004. [8] D. Bertozzi, et al. NoC synthesis ﬂow for customized domain speciﬁc multiprocessor systems-on-chip. IEEE Trans. PDS, 2005. [9] J. Chan, et al. Nocgen: a template based reuse methodology for NoC architecture. In Proc. ISVLSI, 2004. [10] M. Coppola, et al. OCCN: a network-on-chip modeling and simulation framework. In Proc. DATE’04, 2004. [11] W. Dally, et al. Principles and Practices of Interconnection Networks. Morgan Kaufmann Publishers, 2003. [12] M. Gasteier, et al. Bus-based communication synthesis on system level. ACM TODAES, 1999. [13] W. Hang-Sheng, et al. A technology-aware and energy-oriented topology exploration for on-chip networks. In Proc. DATE, 2005. [14] W. Hang-Sheng, et al. Orion: a power-performance simulator for interconnection networks. In Proc. MICRO, 2002. [15] A. Hansson, et al. A uniﬁed approach to constrained mapping and routing on NoC architectures. In Proc. CODES+ISSS, 2005. [16] W. H. Ho, et al. A methodology for designing efﬁcient on-chip interconnects on well-behaved communication patterns. In Proc. HPCA, 2003. [17] J. Hu, et al. System-level point-to-point communication synthesis using ﬂoorplanning information. In Proc. ASP-DAC, 2002. [18] J. Hu, et al. Exploiting the routing ﬂexibility for energy/performance aware mapping of regular NoC architectures. In Proc. DATE, 2003. [19] Y. Hu, et al. Communication latency aware low power NoC synthesis. Proc. DAC ’06, 2006. [20] A. Jalabert, et al. xpipescompiler: A tool for instantiating application speciﬁc NoC. In Proc. DATE, 2004. [21] A. Jantsch, et al. Networks on chip. Kluwer Academic Publishers, 2003. [22] K. Lahiri, et al. Design space exploration for optimizing on-chip communication architecture. IEEE T-CAD), 2004. [23] S. Manolache, et al. Fault and energy-aware communication mapping with guaranteed latency for applications implemented on NoC. In Proc. DAC, 2005. [24] S. Murali, et al. Mapping and physical planning of NoC architectures with quality-of-service guarantees. In Proc. ASP-DAC, 2005. [25] S. Murali, et al. A methodology for mapping multiple use-cases onto NoCs. In Proc. DATE, 2006. [26] S. Murali, et al. An application-speciﬁc design methodology for stbus crossbar generation. In Proc. DATE, 2005. [27] S. Murali, et al. Designing application-speciﬁc networks on chips with ﬂoorplan information. In Proc. ICCAD, 2006. [28] OCP-IP. Open core protocol standard, 2003. http://www.ocpip.org/ home. [29] G. Palermo, et al. Pirate: A framework for power/performance exploration of network-on-chip architectures. In Proc. PATMOS), 2004. [30] S. Pasricha, et al. Fast exploration of bus-based on-chip communication architectures. In Proc. CODES+ISSS, 2004. [31] C. S. Patel. Power constrained design of multiprocessor interconnection networks. In Proc. ICCD, 1997. [32] A. Pinto, et al. Efﬁcient synthesis of NoCs. In Proc. ICCD, 2003. [33] D. Siguenza-Tortosa, et al. Vhdl-based simulation environment for proteo noc. In Proc. HLDVT Workshop, 2002. [34] K. Srinivasan, et al. An automated technique for topology and route generation of application speciﬁc on-chip interconnection networks. In Proc. ICCAD, 2005. [35] Synopsys. Astro. http://www.synopsys.com. [36] Synopsys. Physical Compiler. http://www.synopsys.com. [37] Synopsys. PrimePower. http://www.synopsys.com. [38] Synopsys. PrimeTime. http://www.synopsys.com. [39] T. T. Ye, et al. Analysis of power consumption on switch fabrics in network routers. In Proc. DAC, 2002. [40] X. Zhu, et al. A hierarchical modeling framework for on-chip communication architectures. In Proc. ICCAD, 2002. Figure 10. Runtime comparison of best topology synthesized by SunFloor vs. quasi-mesh and mesh topologies generation ﬂow using NoC interconnects that considers the design constraints imposed by the 65nm technology node to generate fully functional chip layouts from initial high-level application models. Our experimental results show that, while new technology nodes allow for large beneﬁts in terms of power consumption, device area and operating frequency, they also pose non-trivial challenges, which must be properly tackled by NoC design ﬂows. Our experience with a 65 nm NoC ﬂow led us to the conclusion that an investment was needed in design tools, especially in the back-end phase, and that architectural support (pipelined links) was also required for optimal results. A very positive outcome, however, is that the scalability of NoCs does not deteriorate even for large 65 nm designs, and that NoCs prove capable of tackling the challenges of 65 nm processes. In the future, we plan to perform a more careful analysis of the parasitic and leakage effects in the design of ultra-low power NoCs. 7 Acknowledgments This work is partially supported by the Swiss National Science Foundation (FNS Research Grant 20021109450/1), the US National Science Foundation (NSF, contract CCR-0305718) for Stanford University, the Spanish Government Research Grant TIN2005-5619, and a grant by STMicroelectronics for University of Bologna. "
On the Design of a Photonic Network-on-Chip.,"Recent remarkable advances in nanoscale silicon-photonic integrated circuitry specifically compatible with CMOS fabrication have generated new opportunities for leveraging the unique capabilities of optical technologies in the on-chip communications infrastructure. Based on these nano-photonic building blocks, we consider a photonic network-on-chip architecture designed to exploit the enormous transmission bandwidths, low latencies, and low power dissipation enabled by data exchange in the optical domain. The novel architectural approach employs a broadband photonic circuit-switched network driven in a distributed fashion by an electronic overlay control network which is also used for independent exchange of short messages. We address the critical network design issues for insertion in chip multiprocessors (CMP) applications, including topology, routing algorithms, path-setup and tear-down procedures, and deadlock avoidance. Simulations show that this class of photonic networks-on-chip offers a significant leap in the performance for CMP intrachip communication systems delivering low-latencies and ultra-high throughputs per core while consuming minimal power","On the Design of a Photonic Network-on-Chip Assaf Shacham Columbia University Dept. of Electrical Engineering 500 W 120th St., New York, NY 10027 assaf@ee.columbia.edu Keren Bergman Columbia University Dept. of Electrical Engineering 500 W 120th St., New York, NY 10027 bergman@ee.columbia.edu Luca P. Carloni Columbia University Dept. of Computer Science 1214 Amsterdam Ave., New York, NY 10027 luca@cs.columbia.edu Abstract Recent remarkable advances in nanoscale siliconphotonic integrated circuitry speciﬁcally compatible with CMOS fabrication have generated new opportunities for leveraging the unique capabilities of optical technologies in the on-chip communications infrastructure. Based on these nano-photonic building blocks, we consider a photonic network-on-chip architecture designed to exploit the enormous transmission bandwidths, low latencies, and low power dissipation enabled by data exchange in the optical domain. The novel architectural approach employs a broadband photonic circuit-switched network driven in a distributed fashion by an electronic overlay control network which is also used for independent exchange of short messages. We address the critical network design issues for insertion in chip multiprocessors (CMP) applications, including topology, routing algorithms, path-setup and teardown procedures, and deadlock avoidance. Simulations show that this class of photonic networks-on-chip offers a signiﬁcant leap in the performance for CMP intrachip communication systems delivering low-latencies and ultra-high throughputs per core while consuming minimal power. 1. Introduction A major design paradigm shift is currently impacting high-performance microprocessors as critical technologies are simultaneously converging on fundamental performance limits. Essentially, the scaling in transistor speeds and integration densities can no longer drive the expected congruent multiples in computation performance [14]. Accelerated local processing frequencies have clearly reached a point of diminishing returns: further increases in speed lead to tighter bounds on the logic coherently accessed onchip [3, 13] and the associated power dissipation is exacerbated in an exponential fashion [2, 22]. Evidence of this trend is unmistakable as practically every commercial manufacturer of high-performance processors is currently introducing products based on multi-core architectures: AMD Opteron, Intel Montecito, Sun Niagara, and IBM Cell and Power5. These systems aim to optimize performance per watt by operating multiple parallel processors at lower clock frequencies. Clearly, within the next few years, performance gains will come from increases in the number of processor cores per chip [14, 22], leading to the emergence of a key bottleneck: the global intrachip communications infrastructure. Perhaps the most daunting challenge to future systems is to realize the enormous bandwidths capacities and stringent latency requirements when interconnecting a large number of processing cores in a power efﬁcient fashion. Low latency, high data-rate, on-chip interconnection networks have therefore become a key to relieving one of the main bottlenecks to CMP system performance. Signiﬁcant research activity has recently focused on intrachip global communication using packet-switched micronetworks [1,6,10,11,19]. These so-called networks-on-chip (NoC) are made of carefully-engineered links and represent a shared medium that is highly scalable and can provide enough bandwidth to replace many traditional bus-based and/or point-to-point links. However, with a ﬁxed upper limit to the total chip power dissipation, and the communications infrastructure emerging as a major power consumer, performance-per-watt is becoming the most critical design metric for the scaling of NoCs and CMPs. It is not clear how electronic NoCs will continue to satisfy future communication bandwidths and latency requirements within the power dissipation budget. Photonic interconnection networks offer a potentially disruptive technology solution with fundamentally low power dissipation that remains independent of capacity while providing ultra-high throughputs and minimal access latencies. One of the main drivers for considering photonic NoCs is the expected reduction in power expended on intrachip communications. The key power saving rises from the fact that once a photonic path is established, the data are transmitted end to end without the need for repeating, regeneration or buffering. In electronic NoCs, on the other hand, messages are buffered, regenerated and then transmitted on the inter-router links several times en route to their destination. In previous work [27] we provided a detailed power analysis of a photonic NoC, and compared it to an electronic NoC designed to provide the same bandwidth to the same number of cores. The compelling conclusion of the study was that the power expended on intrachip communications can be reduced by two orders of magnitude when high-bandwidth communications is required among the cores. In this paper we explore the design and performance of an optical NoC that can capitalize on the enormous capacity, transparency, low latency, and fundamentally low power consumption of photonics. The construction of this optical NoC is based on remarkable advances made over the past several years in silicon photonics that have yielded unprecedented control over device optical properties. Fabrication capabilities and integration with commercial CMOS chip manufacturing that are now available open new exciting opportunities [8]. The optical NoC building blocks are nanoscale photonic integrated circuits (PICs) that employ optical microcavities, particularly those based on ring resonator structures shaped from photonic waveguides which can easily be fabricated on conventional silicon and silicon-on-insulator (SOI) substrates. This new class of small footprint PICs can realize extremely high interconnection bandwidths which consume less power and introduce less latency than their contemporary bulk counterparts. Compatibility with existing CMOS fabrication systems and the juxtaposition with silicon electronics enable direct driving, controllability, and the integration of these optical networks with processor cores and other silicon-based systems. By using photonic NoCs we exploit the unique advantages in terms of bandwidth, latency, and energy that have made photonics ubiquitous in long-haul transmission systems for on-chip interconnection networks. High speed optical modulators, capable of performing switching operations, have been realized using these ring resonator structures [29, 30] (see Fig. 1) or the free carrier plasma dispersion effect in Mach-Zhender geometries [21]. The integration of modulators, waveguides and photodetectors with CMOS integrated circuits for off-chip communication has been reported and recently became commercially available [8]. On the receiver side, SiGe-based photodetectors and optical receivers were fabricated with reported high efﬁciencies [9]. Finally, low-loss waveguide technology with crossovers and a fairly aggressive turn radii has made some remarkable recent advances and the enabling Figure 1. A silicon ring resonator [30] (left) and a 10 Gb/s eye-diagram from a silicon modulator [8] (right) technologies are currently available [4, 15]. For the ﬁrst time, the integration of a fully functional photonic system on a VLSI electronic die can be realistically envisioned, and, in particular, the photonic elements necessary to build a complete photonic NoC (dense waveguides, switches, modulators, and detectors) are now viable for integration on a single silicon chip. Fig. 1 shows published examples of some of the silicon electro-optic devices mentioned above. In this paper we present a novel architecture for a photonic network-on-chip, and discuss its advantages while addressing critical design challenges. Network topology, routing algorithms, and ﬂow control are discussed in detail. We developed an event-driven network simulator to quantitatively evaluate design aspects such as deadlock avoidance and recovery, path diversity and ﬂow control. The conclusion of the quantitative study demonstrates the potential performance leap offered by the integration of a photonic micro-network within high-performance multi-core systems. 2 Architecture Overview The photonic NoC architecture employs a hybrid design, synergistically combining an optical network for bulk message transmission and an electronic network, with the same topology, for distributed control and short message exchange. While photonic technology offers unique advantages in terms of energy and bandwidth, two necessary functions for packet switching, namely buffering and processing, are very difﬁcult to implement. Electronic NoCs which have many advantages in ﬂexibility and abundant functionality tend to consume high power which scales up with the transmitted bandwidth [22]. The hybrid approach deals with this problem by employing two layers: 1. A photonic interconnection network, comprised of silicon broadband photonic switches interconnected by waveguides, is used to transmit high bandwidth messages. 2. An electronic control network, topologically identical to the photonic network, is used to control the photonic network and for the exchange of short control messages. Every photonic message transmitted is preceded by an electronic control packet (a path-setup packet) which is routed in the electronic network, acquiring and setting-up a photonic path for the message. Buffering of messages is impossible in the photonic network, as there are no photonic equivalents for storage elements (e.g. ﬂip-ﬂops, registers, RAM). Hence, buffering, if necessary, only takes place for the electronic packets during the path-setup phase. The photonic messages are transmitted without buffering once the path has been acquired. This approach has many similarities with optical circuit switching, a technique used to establish long lasting connections between nodes in the optical internet core. The main advantage of using photonic paths relies on a property of the photonic medium, known as bit-rate transparency [25]. Unlike routers based on CMOS technology that must switch with every bit of the transmitted data, leading to a dynamic power dissipation that scales with the bit rate [22], photonic switches switch on and off once per message, and their energy dissipation does not depend on the bit rate. This property facilitates the transmission of very high bandwidth messages while avoiding the power cost that is typically associated with them in traditional electronic networks. Another attractive feature of optical communications results from the low loss in optical waveguides: at the chip scale, the power dissipated on a photonic link is completely independent of the transmission distance. Energy dissipation remains essentially the same whether a message travels between two cores that are 2 mm or 2 cm apart. Furthermore, low loss off-chip interconnects such as optical ﬁbers enable the seamless scaling of the optical communications infrastructure to multi-chip systems. The photonic network is comprised of broadband 2×2 photonic switching elements which are capable of switching wavelength parallel messages (i.e. each message is simultaneously encoded on several wavelengths) as a single unit, with a sub-ns switching time. The switches are arranged as a two dimensional matrix and organized in groups of four. Each group is controlled by an electronic circuit termed electronic router to construct a 4×4 switch. This structure lends itself conveniently to the construction of planar 2-D topologies such as a mesh or a torus. A detailed explanation on the design of the photonic switching elements and the 4×4 switches is given in Section 3. Two-dimensional topologies are the most suitable for the construction of the proposed hybrid network. The same reasons that made them popular in electronic NoCs, namely their appropriateness to handle a large variety of workloads and their good layout compatibility with a tiled CMP chip [6], still apply in the photonic case. Further, high-radix switches are very difﬁcult to build with photonic switching elements so the low-radix switches, the building blocks of mesh/torus networks, are a better ﬁt. Torus networks, which offer a lower network diameter, compared to meshes at the expense of having longer links [7], are the obvious choice since the transmission power on photonic links is independent of the length, unlike in copper lines. Topological means can also be employed to overcome the lack of buffering in photonics. Since the photonic switching elements have small area and power consumption, many of them can be used to provision the network with additional paths on which circuits can be created, thus reducing the contention manifested as path-setup latency. Electronic/Optical and Optical/Electronic (E/O and O/E) conversions are necessary for the exchange of photonic messages on the network. Each node therefore includes a network gateway serving as a photonic network interface. Small footprint microring-resonator-based silicon optical modulators with data rates up to 12.5 Gb/s [29] as well as 10 Gb/s Mach-Zehnder silicon modulators [8] and SiGe photodetectors [9] have been reported and have recently become commercially available [8], to be used in photonic chip-to-chip interconnect systems. The laser sources, as in many off-chip optical communication systems [8, 16] can be located off chip and coupled into the chip using optical ﬁbers. The network gateways should also include some circuitry for clock synchronization and recovery and serialization/deserialization. When traditional approaches are used, this circuitry can be expensive both in terms of power and latency. New technological opportunities enabled by the integration of photonics onto the silicon die may reduce these costs. An example of such an opportunity is an optical clock distribution network which can be used to provide a highquality low-power clock to the entire chip thus alleviating the need for clock recovery in the gateways. In any case, the gateway design should account for these issues. Since electronic signals are fundamentally limited in their bandwidth to a few GHz, larger data capacity is typically provided by increasing the number of parallel wires. The optical equivalent of this wire parallelism can be provided by a large number of simultaneously modulated wavelengths using wavelength division multiplexing (WDM) [4] at the network interfaces. The translating device, which can be implemented using microring resonator modulators, converts directly between space-parallel electronics and wavelength-parallel photonics in a manner that conserves chip space as the translator scales to very large data capacities [20, 31]. Optical time division multiplexing (OTDM) can additionally be used to multiplex the modulated data stream at each wavelength to achieve even higher transmission capacity [17]. The energy dissipated in these large parallel structures is not small, but it is still smaller then the energy consumed by the wide busses and buffers currently used in NoCs: the network gateway interface and corresponding E/O and O/E conversions occur once per node in the proposed system, compared to multiple ports at each router in electronic equivalent NoCs [27]. The employment of 4×4 switches places a unique constraint on the placement of the gateways. Since the photonic switches do not have a ﬁfth port for injection/ejection the gateways require specially designed access point to permit injection and ejection of messages without interfering with pass-through trafﬁc. The gateway placement policy and the design of access points will be described in Subsection 3.2. 2.1 Life of a Packet on the Photonic NoC Finally, to illustrate the operation of the proposed NoC we describe the typical chain of events in the transmission of a message between two terminals. In this example, a write operation takes place from a processor in node A to a memory address located at node B. Both are arbitrary nodes connected through the photonic NoC. As soon as the write address is known, possibly even before the contents of the message are ready, a path-setup packet is sent on the electronic control network. The packet includes information on the destination address of node B, and perhaps additional control information such as priority, ﬂow id, or other. The control packet is routed in the electronic network, reserving the photonic switches along the path for the photonic message which will follow it. At every router in the path, a next-hop decision is made according to the routing algorithm used in the network. When the path-setup packet reaches the destination node B, the photonic path is reserved and is ready to route the message. Since the photonic path is completely bidirectional a fast light pulse can then be transmitted onto the waveguide, in the opposite direction (from the destination node B to the source node A), signaling to the source that the path is open (using a technique presented similar to Ref. [26]). The photonic message transmission then begins and the message follows the path from switch to switch until it reaches its destination. Since special hardware and additional complexity are required to transmit and extract the counter-directional light pulses, an alternative approach can be used: This approach is based on transmitting the message when the path is assumed to be ready according to the maximum expected path reservation latency. While this scheme does not utilize the network resources as well as the ﬁrst one, it requires less hardware resources. After the message transmission is completed a pathteardown packet is ﬁnally sent to release the path for usage by other messages. Once the photonic message has been Figure 2. Photonic switching element: (a) OFF state: a passive waveguide crossover. (b) ON state: light is coupled into rings and forced to turn received and checked for errors, a small acknowledgement packet may be sent on the electronic control network, to support guaranteed-delivery protocols. In the case where a path-setup packet is dropped in the router due to congestion, a path-blocked packet is sent in the reverse direction, backtracking the path traveled by the path-setup packet. The path-blocked packet releases the reserved switches and notiﬁes the node attempting transmission that its request was not served. 3 Network Design In this section we describe in detail the proposed implementation of the photonic network and its electronic control layer, touching some key implementation issues. 3.1 Building Blocks The fundamental building block of the photonic network is a broadband photonic switching element (PSE), based on a ring-resonator structure. The switch is, in essence, a waveguide intersection, positioned between two ring resonators (Fig. 2). The rings have a certain resonance frequency, derived from material and structural properties. In the OFF state, when the resonant frequency of the rings is different from the wavelength (or wavelengths) on which the optical data stream is modulated, the light passes through the waveguide intersection uninterrupted, as if it is a passive waveguide crossover (Fig. 2a). When the switch is turned ON, by the injection of electrical current into p-n contacts surrounding the rings, the resonance of the rings shifts such that the transmitted light, now in resonance, is coupled into the rings making a right angle turn (Fig. 2b), thus creating a switching action. Photonic switching elements and modulators based on the forementioned effect have been realized in silicon and a switching time of 30 ps has been experimentally demonstrated [29]. Their merit lies mainly in their extremely small footprint, approximately 12µm ring diameter and their low and messages that are routed straight through do not block other messages and cannot be blocked. To limit the blocking problem U-turns within the switches are forbidden. The blocking relationships between messages are summarized in Table 1. Table 1. Inter-message blocking relationships in the 4×4 photonic switch Blocked message I Blocked message II Current message North→West East→North West→South West→South North→West South→East East→North South→East North→West South→East West→South East→North Being nonblocking is an important requirement from an atomic switch in an interconnection network. Nonblocking switches offer improved performance and simplify network management and routing. However, constructing a nonblocking 4 × 4 switch with the given photonic building blocks requires an exceedingly complex structure. This will have a negative impact on the area and, more importantly, the optical signal integrity, as each PSE hop can introduce additional loss and crosstalk. The design choice is, therefore, to use the blocking switch, because of its compactness, and bear its blocking properties in mind when designing the topology and a routing algorithm. It is worth mentioning that different PSE grouping schemes can be used, where the directions of the waveguides are ﬂipped, causing the blocking properties to slightly change. One possible scheme is to group the PSEs as a mirror-image of the current grouping scheme, where the directions of all waveguides are ﬂipped. The analysis of this case is identical to the original grouping scheme. In yet another scheme, the direction of only one pair of waveguides is ﬂipped (either the vertical or the horizontal). In this case each turning message may block one other message. A related constraint, resulting from the switch structure, concerns the local injection/ejection port. Typically, 2-D mesh/torus NoCs use 5×5 switches, where one port is dedicated for local injection and ejection of packets. A 5×5 switch, while very simple to implement as an electronic transistor-based crossbar, is quite difﬁcult to construct using 2×2 photonic switching elements. The injection and ejection of packets is, therefore, done through one of the 4 existing ports, thus blocking it for through trafﬁc. This design decision places constraints on the topology, as described in the next subsection. Figure 3. 4×4 switch. Four photonic switching elements (PSE) controlled by an electronic router (ER). power consumption: less than 0.5 mW, when ON. When the switches are OFF, they act as passive devices and consume nearly no power. Ring-resonator based switches exhibit good crosstalk properties (> 20 dB), and a low insertion loss, approximately 1.5 dB [28]. These switches are typically narrow-band, but advanced research efforts are now undergoing to fabricate wideband structures capable of switching several wavelengths simultaneously, each modulated at tens of Gb/s. It is also reasonable to assume that the loss ﬁgures can be improved with advances in fabrication techniques. The PSEs are interconnected by silicon waveguides, carrying the photonic signals, and are organized in groups of four. Each quadruplet, controlled by an electronic circuit termed an electronic router, forms a 4×4 switch (Fig. 3). The 4×4 switches are, therefore, interconnected by the inter-PSE waveguides and by metal lines connecting the electronic routers. Control packets (e.g. path-setup) are received in the electronic router, processed and sent to their next hop, while the PSEs are switched ON and OFF accordingly. Once a packet completes its journey through a sequence of electronic routers, a chain of PSEs is ready to route the optical message. Owing to the small footprint of the PSEs and the simplicity of the electronic router, which only handles small control packets, the 4×4 switch can have a very small area. Based on the size of the microring resonator devices [29], and the minimal logic required to implement the electronic router, we estimate this area at 70 µm × 70 µm. A keen observer will notice that the 4 × 4 switch in Fig. 3 is blocking. For example, a message routed from South to East will block message requests from West to South and from East to North. In general, every message which makes a wide turn (i.e. a turn involving 3 PSEs) may block two other message requests that attempt to make wide turns. Messages that make narrow turns (e.g. South to West) Figure 4. A 4-ary 2-D folded torus network (thick lines and dark ovals), access points (thin lines and light ovals), and 16 gateways (rectangles). One access point is shaded and enlarged. 3.2 Topology The topology of choice in our design reﬂects the characteristics of the entire system - a chip multiprocessor (CMP), where a number of homogeneous processing cores are integrated as tiles on a single die. The communication requirements of a CMP are best served by a 2-D regular topology such as a mesh or a torus [24]. These topologies match well the planar, regular layout of the CMP and the application-based nature of the trafﬁc - any program running on the CMP may generate a different trafﬁc pattern [7]. As mentioned above, a regular 2-D topology requires 5×5 switches which are overly complex to implement using photonic technology. We therefore use a folded torus topology as a base and augment it with access points for the gateways. An example of a 4×4 folded torus network, with the augmenting access points appears in Fig. 4. The access points for the gateways are designed with two goals in mind: (1) to facilitate injection and ejection without interference with the through trafﬁc on the torus, and (2) to avoid blocking between injected and ejected trafﬁc which may be caused by the switches internal blocking. Injection-ejection blocking can be detrimental to the performance and may also cause deadlocks. The access points are designed such that gateways (i.e. the optical transmitters and receivers) are directly connected to a 4×4 switch (the gateway switch), through its West port (see Fig. 4). We assume, without loss of generality, that all the gateways are connected to the same port in their respective switches. To avoid internal blocking a set of injection-ejection rules must be followed: injected messages make a turn at the gateway switch, according to their destination, and then enter the torus network through an injection switch. Messages are ejected from the torus network when they arrive Figure 5. Example of a deadlock-avoiding path on the augmented folded torus network. to the ejection switch associated with their ﬁnal destination. The ejection switches are located on the network, at the same row as the gateway switch, and this is the place where the ejecting messages turn. Finally, ejected messages pass through the gateway switch without making turns. An example of a path with the different kinds of switches is illustrated in Fig. 5. Since torus networks are edge-symmetric [7], injection can be done at any port of the gateway switch, as long as the structure of the access point is rotated accordingly. An explanation on how this structure reduces contentions and avoids deadlocks is provided in Section 5. The design of the access points contributes to a larger switch count in the network, as every access point requires 3 additional switches. However, each switch has rather small footprint and power dissipation, thus making the overall penalty minimal compared to the global power savings enabled by the photonic design [27]. A network designer may take advantage of the small footprint to improve the performance by increasing path diversity. Whenever the path-setup packet faces contention it is buffered in the electronic router until the blocking is cleared. The torus network can be augmented with additional paths, without changing the number of access points, so that the probability of blocking is lowered and the pathsetup latency is, accordingly, reduced. Owing to the small footprint of the switches, the simplicity of the routers, and the fact that the PSEs only consume power when they cause messages to turn, the power and area cost of adding parallel paths is not large. The latency penalty that results from the increased hop-count should be balanced against the latency reduction achieved by mitigating contention. This study is performed in Section 7. 3.3 Routing Dimension order routing is a simple routing algorithm for mesh and torus networks. It requires minimal logic in the routers and, being an oblivious algorithm, it does not require the routers to maintain a state or exchange additional information between them. We use XY dimension order routing on the torus network, with a slight modiﬁcation required to accommodate the injection/ejection rules described in Subsection 3.1 above. Each message is encoded with 3 addresses: 2 intermediate addresses and a ﬁnal address, encapsulated within one another. The ﬁrst intermediate address directs the message to the injection switch on torus network, thus causing the message to make the turn at the gateway switch, as required by the injection rules (see Fig. 5). The message is then routed on the torus, using plain XY dimension order routing, to the second intermediate address, the ejection switch (in the ﬁnal destinations row, but one column away from it). Only then the ﬁnal address is decapsulated and the message is forwarded to the destination gateway, where it arrives without having to turn, according to the ejection rules. The address encapsulation mechanism relieves the routers from processing system-scale considerations when setting up a path and preserves the simplicity of dimension order routing in the torus network. When the torus network is path-diversiﬁed [7], (or path-multiplied, where several parallel lanes exist in each row/column), the address encapsulation mechanism can be used to take advantage of the path diversity while preserving the simplicity and obliviousness of dimension order routing. The encoding of the intermediate addresses can be done with the goal of balancing the load between parallel lanes, thus reducing the contention. According to this method the ﬁrst intermediate address will be an injection switch on one of the lanes, as chosen by the gateway. The ejection, among the several parallel lanes, is also chosen by the gateway and encoded on the second intermediate address. The ﬁnal address, of course, does not change. The selection of intermediate addresses is equivalent to choosing, at random, one among several torus sub-networks thus balancing the load among them. In Section 7 we use the load-balancing approach when evaluating the effect of path diversity. Alternative intermediate address selection methods can be used such as restricting one lane to high priority trafﬁc or allocating lanes to sources or designated ﬂows. 3.4 Flow Control The ﬂow control technique in the network greatly differs from common NoC ﬂow control methods. The dissimilarity stems from the fundamental differences between electronic and photonic technologies and mainly from the fact that memory elements (such as ﬂip-ﬂops and SRAM) cannot be used to buffer messages or even to delay them while processing is done. Electronic control packets are, thereFigure 6. Qualitative timing diagram of a successful message setup (left) and a blocked setup request (right) fore, exchanged to acquire photonic paths, and the data are only transmitted, with a very high bandwidth, once the path has been acquired. The path acquisition procedure requires the path-setup packet to travel a number of electronic routers and undergo some processing in each hop. Additionally, the packet may experience blocking at certain points in its path further contributing to the setup latency. Once the path is acquired, the transmission latency of the optical data is very short and depends only on the group velocity of light in a silicon waveguide: approximately 6.6 × 107 m/s, or 300 ps for a 2-cm path crossing a chip [15]. The network can, therefore, be perceived as a fast circuit-switched network, where the path-setup latency is much longer than the transmission latency. On the other hand, path-setup latency is still on the order of nanoseconds, a very short time compared to typical millisecond-setup-time circuit-switched networks. Hence, it can still be considered fast and can emulate packet switching trafﬁc when packets are fairly large. The timing diagram in Fig. 6 illustrates the timing discrepancy. The decision regarding the minimal size of the data unit exchanged on the photonic network must, therefore, take into account the path-setup latency. Exchange of small packets, such as memory read requests or cache-coherency snoop messages, for example, is clearly inefﬁcient. The exchange of memory pages or long cache lines, instead, can utilize the photonic network much better. A good example is represented by the IBM/Toshiba/Sony Cell Broadband Engine processor where the bulk of the interconnection network trafﬁc is made of DMA transactions [18]. For other applications, long lasting connections can be set up between processors that are expected to communicate frequently, providing a high-bandwidth link with minimal latency and low power consumption on which packets of any size can be transmitted. The hybrid network architecture addresses the small packets exchange problem elegantly. Control messages that carry no data and are of a very small size, such as read requests, write acknowledgments, or cache snoops, can be exchanged on the control network which is, in essence, a lowbandwidth electronic NoC. These control messages are not expected to present a challenge for the control network because of their small size and will not require large resources in terms of additional circuitry or power. Further, some applications, such as cryptanalysis for example, are characterized with exchange of very small data messages without any locality that can be exploited for grouping or speculative fetching. A CMP featuring the proposed hybrid architecture can utilize the electronic control network to exchange these massages at a reasonable performance. In any case, it is of interest to study the optimal photonic message size in a given implementation of the network. This depends on the network size, on the latency of the individual components (routers, photonic links, electronic links, etc.), and on the bandwidth of the gateways. While one would want to minimize the setup overhead by using large messages, their size should be kept small enough to allow for good ﬂexibility and link utilization. In Section 6 we analyze the optimal message size for the proposed network using an event-driven simulation model. 4 Simulation Setup A key stage in the development of the ideas presented above is their functional validation using simulation. The correctness of the distributed path-setup procedure and the routing algorithm, for example, must be veriﬁed in a software environment that models accurately the network architecture. A quantitative performance study, using a variety of trafﬁc loads, should also be carried out to evaluate algorithms, topologies and ﬂow control techniques. This performance study also requires an accurate simulation model. We developed POINTS (Photonic On-chip Interconnection Network Trafﬁc Simulator), an event-driven simulator based on OMNeT++ [23]. OMNeT++ is an open source simulation environment that provides good support for modular structures, message-based communications between modules, and accurate modeling of physical layer factors such as delay, bandwidth and error rate. The implemented model is highly parameterized to allow for a broad exploration of the design space. For the study in this paper the following design point is chosen: The system is a 36-core chip multiprocessor (CMP), organized in a 6×6 planar layout, built in a future 22 nm CMOS process technology. The chip size is assumed to be 20 mm along its edge, so each core is 3.3 mm × 3.3 mm in size. The network is a 6×6 folded-torus network augmented with 36 gateway access points (Fig. 4 presents a similar, albeit smaller, network), so it uses 144 switches, organized as 12×12. The electronic routers, each located at a center of a switch, are spaced by 1.67 mm and the PSEs (576 are used) are spaced by 0.83 mm. The area and spacing considerations dictate the timing parameters of the network, as modeled in simulation. We assume a propagation velocity of 15.4 ps/mm in a silicon waveguide for the photonic signals [26] and 131 ps/mm in an optimally repeatered wire at 22 nm for the electronic signals traveling between electronic routers [12]. The interPSE delay and inter-router delay are, therefore, 13 ps and 220 ps respectively. The PSE setup time is assumed to be 1 ns and the router processing latency is 600 ps, or 3 cycles of a 5GHz clock. Message injection processes in NoC simulation models are typically Bernoulli or modulated-Bernoulli processes, which work well with packet-switched slotted network. Since our architecture resembles circuit-switching more than packet- switching, we model the inter-message gap as an exponential random variable with a parameter µIM G . In the simulation reported in this paper we use uniform trafﬁc. While this trafﬁc pattern does not necessarily model the actual loads presented to the network in a CMP, it serves well as an initial measurement technique to demonstrate the capacity of the network and as a reference to use in future measurements. In the following sections we describe three simulationbased studies performed using POINTS. 5 Dealing With Deadlock Deadlock in torus networks has been studied extensively. When dimension order routing is used, no channeldependency cycles are formed between dimensions, so deadlock involving messages traveling in different dimensions cannot occur [7]. Virtual channel ﬂow control has been shown to be successful in eliminate intra-dimension deadlocks [5] and make dimension order routing deadlock free. Both these proofs assume that each router in the torus network is internally nonblocking. As mentioned in Section 3, this is not the case in our network. Area and technology constraints lead us to use a 4×4 switch which has some internal blocking between messages. We recall that every wide turn in the switch may block two other wide turns. Messages that make narrow turns and messages that pass straight through do not block other messages and cannot be blocked, and U-turns are forbidden. Therefore, we must evaluate the topology, ﬁnd when deadlocks may occur, and develop solutions to avoid them. The injection-ejection are explained in Section 3 and illustrated in Fig. 5. They include the separation of injection and ejection to different switches so that turns that may block other messages cannot occur in the same switch. To prove this we inspect each of the 3 switches comprising the access point: Figure 7. Gateway (a), injection (b), and ejection (c) switches. All possible message-paths are marked to demonstrate that no blocking interactions occur • Gateway switch: Injected messages are required to make a turn towards the injection switches. Ejected messages arrive from the ejection message and pass straight through. Therefore, blocking cannot happen. • Injection switch: messages already traveling on the torus network do not turn to the injection paths, so no blocking interactions exist between them and the injected messages. • Ejection Switch: messages may arrive only from the torus network and they either turn for ejection or continue straight through. Since no messages arrive from the gateway switch, none of the blocking interactions may happen. In Fig. 7 the three switches are shown with all the possible paths marked on them. The reader is invited to verify that none of the internal blocking scenarios, listed in Table 1, occur. Even though injection-ejection blockings are completely avoided, and so are the associated performance penalty and possible deadlocks, the problem of intra-dimensional blocking of dimension order routing still remains. The accepted solution for this problem is virtual channel ﬂow control [5] where the channel dependencies are removed by splitting the physical channel to several virtual channels that compete with each other for router bandwidth. This approach is difﬁcult to implement in a circuit-switched network where the channel bandwidth cannot be divided between several circuits. Hence, in our network we solve the intra-dimensional deadlock problem using path-setup timeouts. When a pathsetup packet is sent, the gateway sets a timer to a predeﬁned time. When the timer expires, a terminate-ontimeout packet is sent following the path-setup packet. The timeout packet follows the path acquired by the path-setup packet until it reaches the router where it is blocked. At that router, the path-setup packet is removed from the queue and a path-blocked packet is sent on the reverse path, notifying the routers that the packet was terminated and the path should be freed. If a deadlock has occurred, the system recovers from it at that point. While this method suffers from some inefﬁciency because paths and gateway injection ports are blocked for some time until they are terminated without transmitting, it guarantees deadlock-recovery. In another possible scenario, the path-setup packet is not deadlocked but merely delayed and it reaches its destination while the timeout packet is en-route. In these cases the timeout packet reaches the destination gateway where it is ignored and discarded, and the path is acquired as if the timeout had not expired. This procedure has been tested in extensive simulations and has shown to be effective in resolving deadlocks. 6 Optimizing Message Size In order to maintain the network efﬁciency as well as its ﬂexibility and link utilization the message duration should be carefully picked. If too large messages are used, then link utilization is compromised as well as latency when messages are queued in the gateway for a long time while other long messages are transmitted. On the other hand, if messages are too small, then the relative overhead of the pathsetup latency becomes too large and efﬁciency is degraded. Of course, there is no technical reason preventing us from granting full freedom in message-sizing to each node, but this may cause starvation and unfairness. In this section we study the optimal size with respect to the overhead incurred in the path-setup process under the assumption that it is constant across all messages. We deﬁne the overhead ratio as: ρ = Tpath−reservation Tmessage−duration where Tpath−reservation is the time between the transmission of the path-setup packet and the transmission of the path-teardown packet, and Tmessage−duration is the time during which actual transmission takes place, corresponding to the size of the message (see Fig. 6). The smaller the value of ρ, the higher the network efﬁciency. In Fig. 8 we plot ρ as a function of the path length and Tmessage−duration , for a completely unloaded network. In this simulation messages of different sizes, addressed to all destination in the network, are injected at node (0,0), while all the other nodes do not transmit. No generality is lost when we inject from only a single node because of the edgesymmetry of the torus network. The optimal message size is the smallest size which does not exceed a certain overhead ratio. As an arbitrary limit for an unloaded network we set the maximum allowed overhead to 20%. The maximum allowed overhead ratio is, therefore, ρ=1.25. In Fig. 8, where the 20% overhead line appears as a dashed line, we can see that the limit is met by Figure 8. Overhead ratio as a function of path-length and message duration in a unloaded 12×12 torus network messages with duration larger than 50 ns, for the longest path (13 hops). We therefore pick 50 ns to be the message duration in the network and use this duration in the rest of the simulations. It is worth mentioning that thanks to the huge bandwidth that can be transmitted in the photonic waveguides and the broadband switches (see Section 2), the amount of data that can be transmitted in 50 ns can be more than 2 KBytes, supporting the exchange of full memory pages or large DMA transactions. Naturally the overhead will be larger when the network becomes loaded with trafﬁc from other nodes, as path acquisition is expected to take longer due to blocking. To evaluate the effect of congestion on the message setup overhead we transmit 50-ns messages, from all nodes, with uniformly distributed addresses. The load on the network is managed by controlling the distribution parameter of the exponentially distributed inter-message gap (µIM G ). The load offered (α) to the network is then given as: α = Tmessage−duration Tmessage−duration + 1 µIM G 1 µIM G 1 µIM G At the limit of constant transmission by all sources → 0) the offered load approaches 1, and when the ( → ∞) the offered inter-message gap is very large ( load approaches zero. The results of the congestion experiment are shown in Fig. 9. Fig. 9 reveals that the overhead in a loaded network, even lightly loaded, is larger, as was expected. The overhead ratio rises quickly to a value of 3 (or a path-setup latency of 100 ns) for loads exceeding a 0.6 value. Clearly the increased congestion and its detrimental effects on the latency must be dealt with. Adaptive routing algorithms, which use information about the availability of adjacent paths when making a routing decision, can be used to locate alternaFigure 9. Overhead ratio for different pathlengths in an unloaded network tive paths for messages and reduce the blocking probability. Another technique is to increase the path diversity by augmenting the network with parallel lines. This approach is considered in the next section. 7 Increasing Path Diversity One of the advantages of packet-switching networks lies in the statistical multiplexing of packets across channels and its extensive usage of buffers. These allow for distribution of loads across space and time. In a photonic circuitswitched network, there is no statistical multiplexing and buffering is impractical. Additional paths, however, can be provisioned, over which the load can be distributed using either random load-balancing techniques, or adaptive algorithms that use current information on the network load. The topology chosen for the proposed network, a torus, can be easily augmented with additional parallel paths that provide path-diversity and facilitate this distribution of the load. The performance metric used to evaluate the improvement gained by adding the paths is again the path-setup overhead ratio, which is derived directly from the pathsetup latency. Similarly to the previous experiment, we set Tmessage−duration at 50 ns. TIM G is exponentially distributed with a parameter µIM G which is, again, varied to control the offered load. Network with path diversity values of 1-4 are simulated, where a value of 1 represents the baseline 6×6 torus with 36 access points and a value of 4 represents a 24×24 torus, also with 36 access points. Naturally, path diversity has overheads in terms of hardware and increased zero-load latency as a result of the larger network diameter. Table 2 lists the numbers of switches required to implement each of the networks simulated. If we assume that the area of the 4×4 switch is about 5000 µm2 then, theoretically, more than 80000 such switches can be integrated in the photonic layer of a 400 mm2 die. The power dissiFigure 10. Overhead ratio vs. offered load for a 12×12 torus network with 36 gateway access points (324 switches). Figure 11. Latency and average bandwidth offered load for a 12×12 torus netvs. work with 36 gateway access points (324 switches). pated by the diversiﬁed network scales sub-linearly with the number of switches as switches only consume power when they cause a message-turn. The number of turns is ﬁxed and independent of the number of switches, thereby setting a strict upper bound on the power expended in forwarding the photonic message regardless of the actual physical distance traveled [27]. Table 2. Switch counts for networks with different path-diversity values PD Net- Gatevalue work way Injec- Ejection tion TOTAL 1 2 3 4 36 144 324 576 36 36 36 36 36 72 108 144 36 72 108 144 144 324 576 900 The simulation results are given in Fig.10. First, as expected, the increased network diameter caused by the provisioning of paths actually increases the latency when the network is lightly loaded and blocking is not frequent. As the network becomes congested, message blocking starts to dominate the path-setup latency and the additional paths, which reduce the blocking, contribute to a lower latency and a more efﬁcient network. Second, a path-diversity degree of 4 is certainly unnecessary in the test case and in fact it performs poorly compared to a network with a path diversity of 3 because it does not reduce the blocking probability signiﬁcantly, while, on the other hand, it increases the network diameter. As a ﬁnal measurement, Fig. 11 illustrates the motivation for the integration of a photonic network on chip, i.e. the immense bandwidth that can be routed using wavelength division multiplexing and broadband switches. Fig 11 shows the path-setup latency and the average bandwidth available per port for a network with a path diversity value of 2 as a function of the offered load, when 50-ns messages are injected, assuming a peak bandwidth of 960 Gb/s using OTDM and WDM. It can be observed that even under low loads and relatively low average latencies of 30 ns an average bandwidth of 230 Gb/s can be provided to each core, while using simple routing algorithms and circuitsetup methods. The bisection bandwidth corresponding to this operating point is 2.07 Tb/s. 8 Conclusions Recent extraordinary advances in the fabrication of silicon photonic devices and their integration with CMOS electronics on the same die open a new ﬁeld of opportunities for on-chip and off-chip interconnection networks designers. In this paper we gave a detailed presentation of a new architectural approach based on these opportunities: a networkon-chip combining a photonic circuit-switched network for high-bandwidth bulk data transmission and an electronic network which controls the photonic network while providing a medium for the exchange of short messages. The presentation covered critical design issues such as topology, routing algorithms, path-setup/teardown procedures, and deadlock avoidance rules and recovery procedures. We developed POINTS, an event-driven simulator, and used it to validate the architecture and to complete a set of studies showing that low-latency low-power photonic links can be set up within tens of nanoseconds. By limiting the message duration in the circuit-switched photonic network to a short time, one can emulate a high-bandwidth packetswitched network, with an average bandwidth on the order of hundreds of Gb/s per core. While the exchange of data at such bandwidths leads to exceedingly high power dissipation in electronic NoCs, in a photonic NoC, end-to-end paths are formed across a chain of low-power transparent switching elements. Power consumed in routing the high-bandwidth messages can therefore be dramatically reduced. Photonic NoCs can present a true leap in the sheer performance of intrachip interconnection networks and, more importantly, in their performance per watt. "
A New Binomial Mapping and Optimization Algorithm for Reduced-Complexity Mesh-Based On-Chip Network.,"This paper presents an efficient binomial IP mapping and optimization algorithm (BMAP) to reduce the hardware cost of on-chip network (OCN) infrastructure. The complexity of BMAP is O(N2log(N)). Based on our OCN system synthesis flow, the proposed algorithm provides more economic network component mapping in comparison with traditional OCN mapping algorithm. The experimental result shows total traffic on network is reduced by 37% and average network hop count is reduced by 46%. With further optimization, the hardware efficiency is enhanced therefore the total hardware cost of network infrastructure is reduced to 51%~85%","A NEW BINOMIAL MAPPING AND OPTIMIZATION ALGORITHM   FOR REDUCED-COMPLEXITY MESH-BASED ON-CHIP NETWORK  Wein-Tsung Shen, Chih-Hao Chao, Yu-Kuang Lien, and An-Yeu (Andy) Wu  Graduate Institute of Electronics Engineering, and Department of Electrical Engineering, National Taiwan University,   Taipei 106, Taiwan, R.O.C.  {wtshen,chihhao}@access.ee.ntu.edu.tw, andywu@cc.ee.ntu.edu.tw  Abstract – This paper presents an efficient binomial IP  mapping and optimization algorithm (BMAP) to reduce  the hardware  cost  of  on-chip network  (OCN)  infrastructure. The complexity of BMAP is O(N2log(N)).  Based on our OCN system synthesis flow, the proposed  algorithm provides more economic network component  mapping in comparison with traditional OCN mapping  algorithm. The experimental result shows total traffic on  network is reduced by 37% and average network hop  count is reduced by 46%. With further optimization, the  hardware efficiency  is enhanced therefore the total  hardware cost of network infrastructure is reduced to  51%~85%.  I.  INTRODUCTION  With the increasing complexity of System-on-chip (SoC)  design, data exchange within chip is becoming more difficult.  Traditional  interconnection  approaches  cannot provide  sufficient support for future giga-scale SoC design. On-chip  network (OCN) [3] is an approach to solve the incoming  physical  routing,  flexibility, scalability, and  reliability  problems.  OCN provides a possible and economical method to  integrate complex systems on a single chip with the advanced  VLSI technology [1][2]. Many researches indicate that the  synthesis of OCN dominates the infrastructure hardware cost  and network performance [5]. The target of OCN synthesis is  to find the suitable network infrastructure with minimum cost.  Among many network topologies, we choose two dimensional  mesh based topology as our OCN topology. The mesh-based  architecture has good scalability and regularity, and hence it’s  comprehensively adopted as OCN basic topology in many  related works Radu’s tile[4], Xpipes[6]. SUNMAP[8] propose  an OCN system design flow, NMAP, to map IPs on given  topology and generate the synthesized OCN target in SystemC.   The NMAP [6]-[8] model the OCN synthesis problem as  a shortest-path optimization problem and runs a O(N4log(N))  complexity algorithm. We discover that the synthesis flow can  be partitioned into a two-stage task. With the proposed greedy  binomial mapping and optimization algorithm (BMAP), the  complexity of synthesis is reduced to O(N2log(N)). In our OCN  design flow, the synthesis is consisted of a mapping stage and  an optimizing stage. For comparison, we take two real SoC  applications: a video object plane decoder (VOPD) and an  MPEG-4 decoder as our applications. The traffic models of the  given applications are extracted and  input  to our  implementation program. The experimental results show that  the proposed algorithm reduces 37% total traffic load on the  network and 46% network hop count.   SW Tool SoC Application NI Traffic Generator Mapping Core Topology Generator Cycle Accurate  Simulator HW Library Various  Router Models Network  Interface  Models Traffic Modeling Mapping Optimization (HW Cost) Proposed  BMAP Simulation &  Performance Evaluation Network Architecture Figure 1. OCN design flow and synthesis (marked by the dotted-line  square frame)  II. OCN SYNTHESIS REVIEW  Figure 1 shows the modified OCN design flow. The SoC  application  is  the  target SoC system running specific  application. We assume the network interface (NI) used for the  given SoC application is chosen by system designer before  OCN synthesis. NIs packetize the data transmitted between IPs  in SoC. In this paper, our NIs adopts open core protocol (OCP)  and use synchronous request-ack full handshake. We assume  that the target OCN adopts wormhole-based architecture.  Therefore, the NIs should be able to recognize three types of  flits which compose a packet: header, body, and tail. For a  given SoC application, the traffic model is extracted through  modeling the original system components and dataflow by  vertices and edges of a directed graph. The traffic matrix of  given SoC application is generated from the modeling graph.  The proposed BMAP is marked by the dotted-line square  frame in Figure 1. BMAP partitions the OCN synthesis into          mapping and optimization. The final synthesized network  architecture is improved and evaluated through three steps:   l Mapping: The mapped network architecture is the first  stage output of the OCN design flow. The basic one  comes from our binomial mapping algorithm.   l Optimization: The optimized network architecture is  the second stage output of the OCN design flow. This  one is evolved from the mapped with proposed  optimization algorithm.  l Simulation: The simulated network architecture is the  third stage output of the OCN design flow. This one is  simulated with our cycle-accurate SystemC simulator  for performance evaluation.  Each of the output architecture in the three steps can be  translated  to a real composition of existing hardware  components to form the synthesizable OCN for further  verification. In comparison with Xpipes[6] and SUNMAP[8],  the proposed BMAP tries to minimize the total traffic on  network, hop number, and the OCN hardware cost. The  proposed binomial mapping algorithm and optimization  algorithm raise the hardware efficiency of OCN. Therefore, the  total network traffic loading and transmission latency is  reduced. In the following sections we describe the proposed  mapping algorithm and optimization approach in detail.  III. PROPOSED BINOMIAL MAPPING AND  OPTIMIZATION ALGORITHM (BMAP)  Mapping a SoC system to OCN is the first and the most  important step in our design flow because it will dominate the  overall performance and cost. The computation complexity of  binomial mapping algorithm is O(N2log(N)). Compared with  the Xpipes’ work [9][10], whose computation complexity  exceeds O(N4log(N)), our work saves O(N2)  time  to  accomplish the mapping. Instead of modeling the mapping  problem as the shortest-path optimization problem, we adopt a  greedy style on the extracted traffic model. The proposed style  generates a component mapping with better total traffic load,  which is advantageous for the optimization of hardware cost.  Furthermore, the binomial mapping algorithm can be easily  combined with other existing optimization methods, such as  shortest path optimization. Figure 2 shows the proposed  binomial mapping and optimization algorithm is mainly  composed by three major operations: binomial merging  iteration, topology mapping and traffic surface creating, and  hardware cost optimization.   A. Binomial Merging Iteration  The binomial mapping iteration, shown in Figure 2,  contains three steps: calculating IP ranking, merging IP-set,  and refreshing IP-set. The iteration runs until only one IP-set is  left. The proposed fast mapping method uses binomial merging  iteration to decide the location of each IP on mesh topology.  To satisfy the requirement surface from the extracted traffic  model, this method greedily finds the best merging result  through the binomial merge. Binomial merge is based on  concept of tree structure representation for IPs. The unmapped  IPs are viewed as unconnected tree roots. Each set of IPs is  viewed as a merged sub-tree. The binomial merging iteration  transforms the IP mapping problem as a simple tree merging  problem. The tree merging takes IP ranking as cost function.  According to the ranking of each set of IPs, we merge sub-tree  and view the new sub-tree as an IP-set. After several iterations,  we can get the final tree. Considering the square feature of  mesh-based topologies, we choose binomial merge method for  efficiency. It takes log2(N) iterations in O(N2) time, where N  denotes the total IP number. Due to the low computation  complexity, binomial mapping performs well for large scale  SoC. Furthermore, binomial mapping have the flexibility of  tree structure, and hence it is easy to apply other optimization  methods, such as  localization, shortest path  [9], etc.  [5][10][11][12]   Figure 2. Proposed BMAP flow chart  1) Calculate IP Ranking:  First we define ip-sets as basic  element which descripts a set of merged ip. Based upon the  extracted traffic load of original SoC system, we can calculate  the ranking of each IP. After completing a merging iteration,  the ranking of each IP-set is updated. The IP ranking is  calculated by summing the traffic from each IP to the other IPs  and from the other IPs to each IP. The IP ranking is calculation  is expressed by Eq. (1). The ranking(i) denotes the ranking of  IP-set  i. The requirement(i,j) denotes  the bandwidth  requirement from IP-set i to IP-set j.  2) Merging IP Set:  Merging is the main step of binomial  mapping. Based upon IP ranking, the IP sets are merged twoby-two on every iteration. Therefore, it takes total log2(N)    iterations to complete the binomial merging iteration. Figure 3  shows an example with N = 16 and how the IP-sets are merged.  By merging two IP-sets, it finds the best contact between  boundaries (i.e. minimal traffic load). In binomial mapping,  there are only 4 or 16 cases. These cases are caused by  rotations of IP-sets and shown in Figure 4.  3) Refreshing IP Set:  In this step, the new requirements of  merged IP-sets is recalculated they are IP-sets as an individual  IP. The new ranking of two merged IP-sets is calculated by  summation their origin requirements and subtract requirements  between them, which can be expressed as follows:  ranking i ( ) = N (cid:229) j = 1 ( requirment i j ( , ) + requirment ( , )      j i i ) = 1 ~ , N ranking k ( ) = ranking i ( ) + ranking j ( ) requirment i ( , ) j requirement ( , ). j i (1)  (2)  Figure 3. An example of binomial merging iteration (N = 16).  B. Topology Mapping and Traffic Surface Creating  After binomial mapping, our tool produces a traffic  surface. The traffic surface shows the traffic load of each  router and the centralized traffic after binomial mapping. We  use minimal path routing, such as X-Y routing. Then the traffic  load of each router on OCN can be accumulated as centralized  traffic. Based on this surface, we can easily optimize hardware  cost by selecting proper routers from the given library of  hardware models. Figure 5 shows an example of the traffic  surface. Based upon this surface, we can find that the result of  binomial mapping is very suitable to centralize traffic.  C. Hardware Cost Optimization  Hardware cost of OCN is an important issue. Routers of  OCN system dominate the hardware cost, especially for their  buffers. According to the traffic surface, we use several  approaches: (1) eliminate dummy router, (2) router selection,  and (3) unfolding, in BMAP to reduce the hardware cost of  router buffers to mostly save the cost.   1) Dummy Router Elimination:  Some dummy routers are  added at the start point of binomial map for 4n routers. After  binomial merging iteration, the dummy routers are put to the  boundary of mesh. Therefore removing the dummy routers  can’t affect network performance.  2) Router Selection:  The cost of router buffers dominates the  OCN hardware cost. We share single buffer among low  bandwidth, simplified as BW, required  input channels.  Therefore a hardware library, which contains a variety of  routers with different buffer banks, is build. Table 1 shows the  relation of BW and n-bank router.  3) Unfolding:  After binomial merging iteration, the router  with heavy traffic would be gathered. Some router will have  traffic load over router BW. Unfolding technique is used to  double the link BW and the router BW by adding additional  router for critical node that needs larger bandwidth without  increasing latency, which is shown in Figure 6.  Figure 4. Merging cases of two IP-sets.  Figure 5. Traffic Surface of OCN system after Binomial Mapping.  (a)Traffic Surface.  (b)Unfolding Approach  Figure 6. Optimization Approaches of BMAP.  Using these approaches, we can observe the traffic  surface. For traffic under router BW, we adopt n-bank router in                          Figure 7. For traffic over router BW, we unfold the critical nod  with more routers. Finally, we can get a very low-cost  optimized OCN architecture under bandwidth constraints of  SoC applications.   IV. EXPERIMENT RESULTS  We compare BMAP with NMAP. In algorithm domain,  NMAP maps the cores onto a mesh topology under bandwidth  constraints, and minimizes the average communication delay  by iterations. It costs more than O(N4log(N)) (i.e. N is IP  number) computation time to do its shortest path optimization.  On the other hand, BMAP uses binomial merge method to get  a fast and semi-greedy optimized result, and prepares for the  next optimization phase for hardware cost-down. It costs  O(N2log(N)) complexity and takes only a few seconds for the  VOPD and MPEG-4 cases. The comparison in algorithm  domain is shown in Table 2. In order to compare performance  in a reliable way, we choose 2 different applications: VOPD  and MPEG4 (shown in Figure 8). These two applications are  also used in [9].  Figure 9 is the simulation results of binomial mapping of  BMAP comparing with NMAP, PMAP, GMAP and PBB in  [9]. Because NMAP has the best performance in [9], NMAP is  adopted as a reference point to judge our approach. In  application 1, the bandwidth is improved to 0.98 but the hop  number is increased to 1.02. It means that the binomial  merging algorithm of BMAP efficiently get a better semigreedy bandwidth-optimized result and can substitute the  initial phase of NMAP if necessary. Moreover, the hop number  can be reduced in optimization phase.  Base on the simulation results of application 1, we can say  that the binomial algorithm of BMAP is the best solution in the  initial state for regular 2-D mesh topology (i.e. IP number = 4N,  where N is integer). It seems that the results of application 2  (12 IPs) are not similar to application 1 (16 IPs). It’s because  that we add 4 dummy IPs to keep the regularity of 2-D mesh  topology and cause lower average router traffic (0.8) but  increase the Hop number (1.71). The performance degradation  of Hop will be solved in the optimization phase.  Figure 10 shows that the HW cost of application 1 and 2  reduced 50% after optimization. Therefore TABLE 3 shows  that BMAP, compared with NMAP, reduces to 89% bandwidth  and reduces to 68% HW cost. The traffic ratio and HW cost  ratio can be defined as follows.  Traffic ratio = HW ratio = ,  Traffic load of BMAP Traffic load of NMAP HW cos t of BMAP .  HW t of NMAP cos  (3)  (4)  Figure 7. Block diagram of {4, 3, 2, 1}-bank routers.   TABLE 1. HW COST AND BANDWIDTH OF N -BANK ROUTERS  n n 4 4 3 3 2 2 1 1 0 0 Area Area um2 um2 100000 100000 83400 83400 66800 66800 50200 50200 0 0 normalized normalized 100 % 100 % 83 % 83 % 67 % 67 % 50 % 50 % 0 % 0 % Max. BW (MB/s)  Max. BW (MB/s)  @ 200MHz @ 200MHz @ 66.7 MHz @ 66.7 MHz 3200 3200 1068 1068 2400 2400 801 801 1600 1600 534 534 800 800 267 267 0 0 0 0 TABLE 2. CHARACTERISTIC COMPARISON BETWEEN NMAP AND  THE PROPOSED MAPPING AND OPTIMIZATION ALGORITHM.  Algorithm  Optimization  Complexity  NMAP[8]  Initialization & Iteration  Shortest Path  O(N4logN)  BMAP  Greedy Binomial Merge  Low Cost  O(N2logN)  (a). Core Graph of VOPD .[9]  (b).  Core Graph of MPEG -4.[9]  Figure 8. Core graphic extracted for traffic load modeling.                                            TABLE 3. TRAFFIC RATIO AND HOP RATIO BETWEEN AMAP AND  NMAP.  Application  VOPD  MPEG -4  Average  NMAP vs. BMAP  Traffic Ratio  98%  80%  89%  HW Cost Ratio  51%  85%  68%  V. CONCLUSIONS  We propose a binomial mapping and optimization  algorithm, BMAP, based on our modified OCN synthesis flow  in this paper. By using the proposed BMAP, we can save 37%  total traffic load and 46% average HW costs. The binomial  mapping costs O(N2log(N)) computation complexity and is a  fast and efficient algorithm compared to NMAP [9]. The  binomial merging iteration results of BMAP is better than  NMAP in traffic load. After mapping, the proposed BMAP  adopts several approaches to optimize the hardware cost. From  simulation results of real SoC applications, BMAP saves 50%  hardware cost of the synthesized OCN by the router selection  approach. We also use unfolding approach  to  increase  bandwidth of OCN critical nodes to meet the throughput  requirement Therefore the total hardware cost of network  infrastructure is reduced to 51%~85%. The OCN architecture  is verified through hardware-software co-simulation on the  established infrastructure with CoWare ConvergenSC ESL  design tool.  "
Implementation of a Design-for-Test Architecture for Asynchronous Networks-on-Chip.,"In order to improve the testability of asynchronous NoCs, we have developed a design-for-test (DfT) architecture. In this architecture, each asynchronous network node is surrounded by an asynchronous test wrapper and the network communication channels are reused to establish high throughput TAMs. A special block, the generator-analyzer-controller (GAC) unit, has also been developed to generate test vectors, to control test flows, and to analyze the test results. This unit can be implemented on-chip or off-chip (in our experiments, it has been implemented off-chip). The operation of the test wrappers is controlled by a dedicated 2-bit configuration channel. Thanks to its scalability and versatility, the proposed architecture can be configured to adapt to any NoC topology and to any specific application.","Implementation of a Design-for-Test Architecture for Asynchronous Networks-on-Chip Xuan-Tu Tran1 , Jean Durupt1 , Yvain Thonnart1 , Franc¸ ois Bertrand1 , Vincent Beroulle2 , Chantal Robach2 1CEA-LETI, Minatec — 17 rue des Martyrs, 38054 Grenoble, France. Email: xuan-tu.tran@cea.fr 2 INPG-LCIS — 50 rue Barth ´el ´emy de Laff ´emas, 26902 Valence, France The Network-on-Chip (NoC) paradigm has recently emerged as an alternative solution for on-chip communications of the next System-on-Chip (SoC) generation. The advantages of NoC-based systems are numerous: high scalability and versatility, high throughput with good power efﬁciency,. . . The NoC distributed communication architecture is perfectly adapted to the Globally Asynchronous Locally Synchronous (GALS) platforms where the NoC nodes and links are implemented using asynchronous logic while the computational resources (i.e., IPs) are implemented with standard synchronous design methodologies. As the NoC paradigm is being brought to market, the challenge is how to test the NoC-based SoCs for manufacturing defects. To test a NoC-based system, we must address: (1) the test of the IPs and their corresponding network interfaces (NI), and (2) the test of the interconnection infrastructure consisting of network nodes and inter-node communication channels. To test the IPs in SoCs, classical approaches such as the IEEE 1500 standard or Boundary-Scan can be adopted. The difﬁculty is to get access to the embedded IPs during test process. With NoC-based SoCs, we can use the NoC architecture itself as a Test Access Mechanism (TAM). Test patterns are encapsulated into data packets that are transported on the NoC using the network protocol. The principal advantages of this approach are the absence of additional TAM hardware and the availability of multiple paths for IP testing. However, the NoC architecture should be searched for defects ﬁrst. To test NoC architectures, we have to address two issues: the test of network nodes and the test of communication channels between nodes. The difﬁculties are that network nodes have many I/O pins and that they are hierarchically embedded in the system. Additionally, the controllability and the observability of the network elements are still unresolved although the nodes can be conﬁgured to transport data to anywhere on the network. Moreover, the test of asynchronous NoCs is more difﬁcult than the test of synchronous NoCs because of the many feedback loops in asynchronous circuits, and the lack of EDA supports for testing. In order to improve the testability of asynchronous NoCs, we have developed a Design-for-Test (DfT) architecture as illustrated in Fig. 1. In this architecture, each asynchronous network node is surrounded by an asynchronous test wrapper and the network communication channels are reused to establish high throughput TAMs. A special block, the GeneratorAnalyzer-Controller (GAC) unit, has also been developed to generate test vectors, to control test ﬂows, and to analyze the test results. This unit can be implemented on-chip or off-chip (in our experiments, it has been implemented off-chip). The operation of the test wrappers is controlled by a dedicated 2-bit conﬁguration channel. Thanks to its scalability and versatility, the proposed architecture can be conﬁgured to adapt to any NoC topology and to any speciﬁc application. Figure 1. The proposed DfT architecture. Once the network architecture is tested successfully, the embedded IPs can be tested as well using the network as a high-speed TAM. If the network is faulty, the proposed architecture can be used as an other high-speed TAM to test the embedded IPs without the network protocol. In both cases, only simple test wrappers have been developed for the IPs. The proposed DfT architecture has been modeled and simulated at many design levels using SystemC, CHP, VHDL languages. Then, it has been implemented into a prototype chip in 65nm STMicroelectronics CMOS technology. The area of a node test wrapper is 96, 000µm2 , i.e. 32.7% of the area of a testable asynchronous network node. However, the area overhead of the whole proposed DfT architecture in the prototype chip (6mm2 ) is only 576, 000µm2 , less than 9% of the total chip area. The maximum test bandwidth is the same as communication bandwidth, up to 2GB yte/s. The supplementary latency added by test wrapper is 0.34ns (post-layout measurement). Test patterns and test strategies have also been deﬁned and evaluated. By using high bandwidth TAMs and parallel testing strategies, application test time is drastically reduced. "
Access Regulation to Hot-Modules in Wormhole NoCs.,"Network on chip (NoC) may be the primary interconnect mechanism for future systems-on-chip (SoC). Real-life SoCs typically include hot-modules such as DRAM controller or floating point unit, which are bandwidth limited and in high demand by other units. In this paper we demonstrate that the mere existence of one or more hot-modules in a wormhole-based NoC dramatically reduces network efficiency and causes an unfair allocation of system resources. We demonstrate that a single hot-module destroys the performance of the entire SoC, even if network resources are over-provisioned. In order to resolve the hot-module effect, we introduce a novel low-cost credit based distributed access regulation technique that fairly allocates access rights to the hot-module. Unlike other methods, this technique directly addresses the root cause of network buffer congestion phenomena. Using simulation, we show the effectiveness of the suggested mechanism in various NoC scenarios","Access Regulation to Hot-Modules in Wormhole NoCs   Isask'har Walter1, Israel Cidon2, Ran Ginosar2, Avinoam Kolodny2  Electrical Engineering Department, Technion – Israel Institute of Technology, Haifa 32000, Israel    1zigi@tx.technion.ac.il , 2{cidon, ran, kolodny}@ee.technion.ac.il  Abstract  Network on Chip (NoC) may be the primary interconnect  mechanism for future Systems-on-Chip (SoC). Real-life SoCs  typically include hot-modules such as DRAM controller or  floating point unit, which are bandwidth limited and in high  demand by other units. In this paper we demonstrate that the  mere existence of one or more hot-modules in a wormholebased NoC dramatically reduces network efficiency and  causes an unfair allocation of system resources. We  demonstrate  that a  single hot-module destroys  the  performance of the entire SoC, even if network resources are  over-provisioned. In order to resolve the hot-module effect, we  introduce a novel low-cost credit based distributed access  regulation technique that fairly allocates access rights to the  hot-module. Unlike other methods, this technique directly  addresses the root cause of network buffer congestion  phenomena. Using simulation, we show the effectiveness of the  suggested mechanism in various NoC scenarios.  Categories and Subject Descriptors  System-Level Design and Co-Design: Network-on-Chip  (NoC)  General Terms  Algorithms, Performance, Design  Keywords  Network  on-Chip, wormhole,  management, SoC  hotspot,  resource  1. Introduction  Wormhole switching  [1] is commonly employed in NoC  (e.g.  [2]- [5]), due to its small buffer requirements and low  latencies at light load. Each packet is divided into small  fixed size parts called flits, which are transmitted to the  next hop without waiting for the entire packet to be  received. This causes transmitted packets to be segmented  and “spread” along the path between the source and  destination in a pipeline fashion. The main drawback of  wormhole switching is its sensitivity to packet blocking  that may quickly consume buffers along the entire path.  Therefore,  the common design practice of high  performance wormhole networks is to allocate ample link  capacities to achieve low utilization and to employ multiple  virtual channels  [6]. In addition, in order to support a mix  of data flows with different timing criticality, mechanisms  to support Quality-of-Service requirements should be  included. For example, In QNoC  [3] packet priorities are  supported by assigning priorities to virtual channels and  defining service levels for messages according to their  relative urgency (e.g. interrupts, real time streaming or  cache line fills, cache prefetch and large data blocks). The  network is equipped with enough resources (capacities,  buffers) to deliver adequate throughput at the required  latency for each service level.  The above QNoC design methodology  [7] works properly  as long as all system modules consume messages within  their specified capacity. However, at certain times the  aggregated traffic demand might exceed a destination  module’s bandwidth. Similarly, such a module may  occasionally operate at a slower than average speed (e.g., a  variable speed coder, encoder or storage device) and  become congested coincidently or not with an incidental  usage peak. We term such a bandwidth-limited highdemanded SoC module a hot-module (HM). In such  situations, the hot-module is unable to consume incoming  packets fast enough. Hot-modules are common in real-life  SoCs, as some modules are bandwidth limited and in high  demand by other units. Hot-modules may be external (e.g.,  DRAM  [8]- [10]) or internal components (caches, CAMs,  specialized arithmetic units, special purpose processors,  SRAMs  [9]). The identities of the hot-modules are usually  known in advance as the critical resources affecting system  performance. Moreover, it is likely that such modules  remain HMs even in SoCs with multiple use-cases (e.g.,  external memory bottleneck in  [10]).  Congested modules  exist  in  systems with  any  communication  infrastructure  (including  bus-based  communication), but wormhole-based systems are much  more sensitive to hot-modules, as the entire network may  be affected: The hop-by-hop backpressure, associated with  wormhole routing, causes buffers at the router adjacent to  the hot-module to be filled up and become stalled, blocking  new arrivals to this router. This creates a domino effect, by  which the delivery of packets to ports of more distant  routers is slowed down, forming a saturation tree  [11] with  the hot-module as its root (Figure 1). Moreover, the  domino effect stretches beyond the traffic that is destined  to that destination (the saturation tree) as packets that are              destined at other modules find no free buffers at certain  routers on their route (extending the saturation tree to a  larger saturated acyclic graph). The overall NoC system  suffers increased delays in packet delivery as well as unfair  network utilization (modules near the HM get a larger  portion of its resources). This threat is particularly  troublesome in wormhole based architectures due to packet  “stretching” across multiple hops causing the hot-module  effects to extend network-wide instantly. It is very  important to note that this phenomenon is independent of  links and router bandwidth. Such a network freeze may  build up even in a system with infinite capacity links  because of a single heavily loaded module. Consequently,  even largely over-provisioned NoCs suffer from poor  performance if potential hot-modules are left unhandled.  We propose a novel one-to-many credit-based access  regulation mechanism for solving the NoC buffer overflow  problems in QNoC and other wormhole-based systems  with predefined HMs. An HM allocation controller is  introduced to arbitrate short, high priority credit requests.  The controller allows the system architect to regulate hotmodule access according  to  the quality of service  requirements of the specific system application. The  allocation algorithm employed by the controller is systemspecific, since the HM is independent of the network.  Credit requests and grants are transmitted as small highpriority signaling packets (grants and requests may be also  piggybacked on other messages). In order to eliminate a  potential round-trip latency in selected modules, autorefresh or pre-allocation is used. The access regulation  mechanism is implemented in module interfaces and in an  appropriate location (e.g., as part of the HM network  interface), while NoC routers remain unchanged. The  mechanism prevents the accumulation of packets destined  at a hot-module within the network buffers. Consequently,  other traffic remains unaffected even when the HM load  increases significantly.  The rest of this paper is organized as follows: In Section 2,  the negative effects of HMs in wormhole-based NoCs are  discussed. Related work is surveyed in Section 3. In  Section 4, a specific credit allocation technique is proposed  to allow fair sharing of the hot resource and to mitigate  effects on non-HM traffic (traffic not destined at the HM),  and Section 5 presents simulations of the suggested  mechanism.  2. Hot-module effects   The NoC buffer congestion due to HMs has several  negative effects on system performance. The hot-module  access latency is increased, as packets destined at it  contend for the limited HM bandwidth. Unfortunately,  additional significant fairness problem arises. Typically,  different source modules are at different NoC distances  from the HM (as illustrated in Figure 2a). Since a packet  has to win local output port arbitration in each router along  its path, the HM bandwidth is not fairly shared. Namely,  the sharing of the HM capacity is dictated by multiple local  decisions made by the network components, and not by  system requirements.   More precisely, NoC modules close to the HM enjoy a  much larger share of the HM bandwidth than distant ones.  This is caused by the fact that NoC routers typically  employ a locally fair, round-robin arbitration between  packets (or flits) of similar priority waiting at different  input ports and contending for the same output port.  Therefore, when its inputs are saturated, each router that is  part of the HM saturation tree equally divides the  bandwidth available at its upstream port among its input  ports. Consequently, HM throughput at a source drops  exponentially as a function of the number of hops between  the source and the HM. When the HM demand is close to  its capacity, location and distance diversity also lead to  significant differences in access latency. Packets sent by  distant sources are more likely to be blocked by other HMtraffic (i.e., traffic destined at a HM) in comparison to  packets that travel only short distances. Therefore, modules  that are  located relatively far from  the hot-module  experience extremely long access times when HM load  mounts. These issues (HM saturation throughput and HM  Figure 1: External DRAM as a SOC's hot-module, which  causes a saturation tree in the NoC (highlighted links)  IP1 (HM) IP5 IP9 R R R R IP2 IP6 R R R IP3 IP7 R R R IP4 IP8 IP10 IP11 IP12 R R R R R R IP1 (HM) IP5 R R R IP2 IP6 R R R IP3 IP7 R R R IP4 IP8 IP9 IP10 IP11 IP12 R R R R R R R IP13 IP14 IP15 IP16 IP13 IP14 IP15 IP16 (a)  (b)  Figure 2: Hot-module effects in a 4×4 YX routed NoC  (a) Source Unfairness: on its way to the hot-module (IP1), packets  generated by module 12 have to win 6 arbitrations, while module 5  packets have to win only 2.   (b) HM-traffic obstructing non-HM traffic: flow 41 slows-down (or  blocks) flow 123 (which shares a link), and in turn may affect flow  166, which is destined at an idle module and does not share any link or  router output port with HM-traffic.                access latency) will be referred to as the source fairness  problem.  Furthermore, performance degradation due to HM load is  not restricted to the HM-traffic itself. In typical NoCs, HM  and non HM-traffic compete for the same network buffer  space and router ports. Therefore, HMs that slowly  consume incoming data hinder the delivery of non-HM  packets (Figure 2b), as blocked HM packets wait inside the  network occupying expensive buffers. As a result, packets  destined to lightly loaded modules are also being stalled by  the network, suffering delays and fairness problems similar  to HM packets (Section  5).  The above discussion applies to any network in the  presence of  congested  end-points. However,  left  unhandled, hot-module effects in a wormhole network are  more severe than in a store-and-forward network, as  packets are blocked across multiple routers and buffering  space is limited.  It is important to note that these delay and fairness effects  are symptoms of HMs presence and not of an inadequately  provisioned NoC. In fact, a wormhole NoC would suffer  from the presence of a HM even with links and routers of  infinite capacity.  3. Related work  The negative effects of hot-modules were partially  explored in off-chip interconnection networks (e.g.  [11] [17]). In that literature there is no clear distinction between  the issue of HM and the congestion of a network port.  Typically, suggested solutions attempt to prevent regular  traffic from being affected by the traffic of a saturation  tree, either by not allowing one to form or by allocating hot  traffic exclusive network resources. Unfortunately, such  solutions do not bring a fair allocation of the hot resource.  In addition,  these solutions address multi-computer  networks,  in which  the design considerations are  significantly different from those of NoCs. For example,  some works modify the network routers in order to throttle  packet injection at high loads (e.g.,  [13],  [14]), discard  packets (e.g.,  [15]), deflect packets away from loaded  locations (e.g.  [16],  [17]), use separate buffers for traffic  destined at a hot-module (e.g.  [12]), or simply use a large  number of virtual channels. However, when directly  applied to NoCs, such modifications considerably increase  NoC router gate count and memory size, resulting in  excessive area and power consumption and reduced speed.  For example, NoCs typically employ static shortest path  routing based on a simple routing function, because of onchip cost and performance considerations. Note that most  of the previous techniques can either slightly postpone the  effect of HM as they only increase the number of buffers  used by non-HM traffic (by adding buffers or routing nonHM traffic away) or even increase the effect by throttling  non-HM packets.  Recently, two papers have studied related but different  congestion problems in NoCs. Ref.  [18] addresses the  classic flow control problem, regulating the communication  between a source-destination pair.  The authors combine  software and hardware mechanisms to adjust the length of  a period (""send window"") in which the source is allowed to  inject packets towards a destination. Consequently, no  sense of fair sharing of the hot-module is provided. In  addition, this scheme only responds after a saturation tree  begins to form. In  [19], an input regulation scheme is  described where each router predicts the availability of  buffers in its input ports, according to data collected from  its neighbors. When a source observes that its adjacent  router is expected to run out of buffers, it delays generation  of new packets. However, that technique does not prevent  hot-traffic from monopolizing multiple virtual channels and  thus might prevent injection of other packets towards idling  destinations. Moreover, as other classic end-to-end flow  and admission control methods  [20], that method does not  address the hot-module allocation fairness problem, since  routers and sources only have local knowledge regarding  the hot-module demand.   The proposed HM access regulation mechanism  is  considerably different from traditional end-to-end flow  control mechanisms. Flow control is conducted on a per  source-destination pair basis (e.g. TCP, send window in   [18], static window in  [21]), and prevents overflow in the  destination buffers pre-allocated for this source (e.g.  [22],   [23]). Flow control does not directly address the hogging of  network resources and does not address the problem of fair  allocation of scarce resources. In addition, all existing  schemes require at least one destination buffer per potential  source, which is inappropriate in on-chip NoCs.  4. HM access regulation  In order to reduce the dramatic effects hot-modules have on  a wormhole-based NoC (Section  5), a credit-based access  regulation mechanism is suggested: each source owns a  quota that limits the number of flits it can send towards a  HM. When a source quota is exhausted, it can resume  transmission only after being granted an additional credit.  Consequently, packets that cannot be consumed by the HM  do not wait inside the network, a saturation tree can not  form and traffic not destined at the HM remains unaffected  during congested periods.  Two types of control messages regulate the access to a  HM: if a source has insufficient credit to start delivery of a  data packet to a HM, it sends a credit request packet to a  HM allocation controller, describing  the  requested  transaction. When appropriate, the controller sends back  credit using a credit reply packet. Due to their significance  and short length, credit request and reply messages are  given a high priority level and therefore cannot be held  back in the NoC by data packets. In this work, we assume          that the NoC is equipped with a prioritized virtual channel  mechanism, such as the one described for QNoC in  [3],  guaranteeing fast access of control messages regardless of  data traffic loads.  As control packets are a few flits long and a single control  packet credits a sizable chunk of data, control traffic is a  small percentage of the HM-traffic. Therefore, the buffers  of the prioritized virtual channel are kept at low utilization,  resulting in minimal network queuing time. In order to  overcome credit request and reply latency during light load  periods, source quota is slowly self refreshing. As a result,  credit request and reply are not time critical events.  4.1 Control Messages  Using a credit request message, a source describes the data  packet(s) it wishes to send to the hot-module and asks for  credit to do so. In addition to the Destination ID field of a  regular packet, a request packet contains two mandatory  fields: Source ID and Length. The former states the  requesting module identity and the latter describes the size  (in flits) of the data packet to be delivered. The system  designer may choose to include additional information  which would enable the HM allocation controller to decide  upon the best service order. This information can be  embedded in optional fields of the request packet. An  example of such a field is a priority value, which indicates  the ""urgency"" of the data packet, relative to requests that  are sent by other sources of the same kind. A deadline field  that indicates the requested completion time can help the  allocation controller sort the requests in the best servicing  order, postponing less urgent requests to be serviced last. If  requests can be ignored unless they are served by a certain  time (e.g., speculative cache fetches), an expiration field  may be used.  Figure 3a illustrates an example of a credit request packet  in which each field fits a flit (more fields per flit are ofcourse possible). Figure 3b illustrates a credit reply packet.  The destination ID field is used to route the packet back to  the requester. The source ID enables the requester to  identify the controller sending the reply and is necessary in  a system with multiple hot-modules. The Credit field states  the number of credits granted in the reply packets.  Generally, this number is equal to the length field in the  matching request packet. However, an allocation controller  D e s t . I D S r c . I D L e n g t h P r i o r i t y E x p i r a t i o n D e a d l i n e … (a)  D e s t . I D S r c . I D C r e d i t (b)  Figure 3: Credit request (a) and reply (b) messages.  The request message may include optional fields that describe  the matching data packet.   may reply with a larger number in order to credit modules  ahead of time during light load periods. The allocation  controller may also reply with less credit than requested. In  this case, a source may choose to send part of the data  packet, thus freeing up local buffer space.  4.2 Implementation  The source control logic is embedded in the network  interfaces that connect cores to NoC infrastructure: sources  capable of communicating with potential HMs are  equipped with logic that stores current quota, generates  quota requests and handles incoming quota replies. In order  to keep track of the available credit, the source interface  includes a credit status table (CST), with an entry for each  potential HM. If all potential HMs are known during  design time, the entries can be pre-coded in hardware.  Otherwise, these entries can be programmed as part of the  configuration process.  The CST is updated by the interface control logic upon  receiving credit reply packets and upon injecting a packet:  source module interfaces are modified so that data packets  are no longer injected towards potential HMs as soon as  link-level protocol allows it. Instead, the source control  logic looks up the CST using the destination ID. If an entry  with a matching module ID does not exist, the destination  is not a potential hot-module and the data packet can be  injected into the network immediately. Otherwise, the  current credit status is retrieved and compared with the size  of the data packet. If sufficient quota exists, the packet is  injected into the network and its size is subtracted from the  corresponding CST entry, reflecting the consumed credit.  Otherwise, a request packet is generated, applying for the  missing credit.   The access to potential HMs is regulated using an  allocation controller that receives credit request messages,  decides upon service order and sends credit reply packets.  This scheduling logic can be implemented as part of the  hot-module's network interface, as an independent module,  or as a separate central unit serving multiple hot-modules.  In this work, we assume that the scheduler is embedded  within potential hot-module interfaces.  The implementation of the allocation controller unit  (Figure 4) includes a pending requests table (PRT), with an  entry for each source module. The entry fields are selected  during design time, according to the fields of request  packets and the specific system needs. For example, a  simple system may only need the source ID and length  fields, while other designs may also describe request type,  priority, deadline and expiration values. When receiving a  credit request packet, the scheduler control logic decodes  the request and logs it in its PRT (Figure 5). In addition the  allocation controller may be provided with the status of the  HM, its current speed and its current queued tasks. The                                              Credit  Requests  Credit  Replies  PRT  Requests  Decoder  Reply  Encoder Local  Arbiter  Figure 4: Hot-Module Allocation Controller.  Upon receiving request for K credits from module i     If HM idle        Send k credits to module i      Else        Log request in PRT  Upon finishing servicing a packet      /*use local arbiter and PRT to choose       next module to be served*/       ilocal_arbiter(PRT)      /*extract requested credit from PRT     kPRT(i)  Send k credits to module i  Remove request from PRT  Figure 5: Operation of a simple HM allocation controller.  local arbiter examines the PRT as well as the HM status  and chooses a module, subject to QoS, fairness definitions  and the HM status and encodes a credit reply packet  carrying a calculated amount of credit and sends it to a  selected source.  The scheduling algorithm, which is crucial to the success of  the suggested technique, allows the system architect to  adequately share the hot resource among requesting  modules during high load periods according to the system's  needs. In order to keep the cost of the HM allocation  controller hardware minimal, in this work we assume that a  simple, round robin local arbiter is implemented and  demonstrate its effectiveness. The design of more complex  controllers and schedulers is left for future work.   It should be noted that the access regulation mechanism is  optional and transparent to the HM and source modules. In  particular, the system architect may allow some modules to  access potential HMs without requesting credit at all, if  their traffic should not be delayed by the controller under  no circumstances.  5. Performance evaluation  MPEG decoder SoC. The presented results exemplify the  severity of  the HM effects  (system performance  degradation and the source fairness problem) and quantify  the extent to which the allocation scheme solves them.  The term ""end-to-end latency"" in this paper refers to the  time elapsed since the packet is created at the source until  its last flit is consumed by the destination. Therefore, the  measured latency accounts for source queuing, network  blocking, virtual channel multiplexing, link bandwidth  limitations, and overhead of the access regulation protocol.  The results are generated using the OPNET based simulator   [24], modeling a wormhole network at the flit level. The  model includes all network layer components, including  wormhole flow control, virtual channels, routing, finite  router buffers and link capacities.  5.1 Hotspot traffic  Traditionally, congestion alleviation techniques in off-chip  networks are evaluated using an ""all-to-one"" traffic pattern.  Although not typical for SoCs, this synthetic scenario is  analyzed here in order to clearly demonstrate the effects of  hot-module congestion and resource arbitration.  Each set of results has been obtained for fixed non HMtraffic, which serves as background communication, and  for varying HM load in a system similar to the one  illustrated in Figure 2.  The following evaluation model is used:   1. A QNoC system consists of 16 modules, arranged in a  4×4 grid with a single HM, placed at the upperleftmost corner. Fixed, symmetric XY routing  [3] is  employed.  2. All network links and modules (except HM) have  identical capacities (10 Gbit/sec).  3. The HM has 1Gbit/sec capacity.  4. Data packets are 200 flits long and are generated by a  Poisson process; Flits are 16 bits long.  5. Routers have a 10-flit input queue per port.  6. All possible non-HM flows exist in the system and  have identical characteristics. Similarly, all possible  HM flows exist and have identical characteristics.  7. A prioritized virtual channel is used to deliver control  packets, which are two flits long each.  8. Routers resolve contention for output ports in a  round-robin manner.  9. The allocation controller is implemented as part of the  network interface of the HM and employs roundrobin arbitration among pending requests.  In this section, the performance of the suggested HM  access regulation mechanism is examined by means of  simulation. Results are compared to a ""standard"" wormhole  based NoC with no such mechanism. Two scenarios are  used: a ""Classic"" hotspot traffic pattern and a real-life  5.1.1 System Performance  Figure 6a shows mean end-to-end delay in the system, with  and without the allocation protocol. It is clear that the  access regulation mechanism considerably reduces the  average access latency. Figure 6b breaks the results down,                                   separating HM-traffic from non HM-traffic. Due to the  bandwidth consumed by the control packets, the mean  delay of the HM-traffic is slightly increased when using the  proposed mechanism. However,  there  is a dramatic  improvement in the delay of the background traffic, which  is now almost unaffected by the mounting HM load. This  improvement is caused by the fact that HM-traffic no  longer occupies expensive network buffers, and non-HM  packets can use them effectively to reach their destinations.  The small increase in HM-traffic delay due to the load of  control messages can be further circumvented. The  designer can prevent control messages from consuming the  limited HM bandwidth by placing the controller in a  location such that the control path does not conflict with  the HM data path.  (a)  (b)  Figure 6: Mean end-to-end delay vs. HM load.  5.1.2 Source fairness  Figure 7 demonstrates the source fairness problem in an  uncontrolled wormhole network in steady-state. When the  HM maximal utilization is approached, the delays vary  largely among sources. While modules close to the HM  experience only a slight increase in their end-to-end delay  (e.g. module 5), the delay seen by distant modules (e.g.  module 16) is considerably larger. This unfairness, caused  by the different number of arbitration points along the path  (Section  2), increases as the number of SoC modules  grows. Figure 7 also shows the results of activating the HM  allocation mechanism in a system with the same loads. The  fair arbitration scheme manages to distribute the limited  resource almost equally among  the system modules  (including ones not shown) as the system approaches its  maximal steady state load. As described above, other  fairness criteria can be implemented using different HM  allocation controller policies.  An additional important performance metric under heavy  load is the saturation throughput: assuming that all sources  have data to send to the hot-module, saturation throughput  is the bandwidth each source achieves. This predicts the  system behavior at periods of extreme congestion in which  the total load exceeds the HM capacity and the system  operates at saturation point. As NoC routers employ a  round-robin based arbitration among ports, each router  effectively divides  its upstream saturation bandwidth  equally among requesting ports. Therefore, in a basic grid  based network, the further a module is from the HM, the  less bandwidth it will get. This unfairness also increases  with the number of SoC modules.   Figure 8 shows the saturation throughput with and without  access regulation mechanism. When no control is applied,  module 5 enjoys 25% of the HM limited bandwidth, while  module 12 gets less than 1% due to the large number of  hops on the path to the HM. This is explained by the  network topology (Figure 2a): router R1 (i.e., the router  directly connected to IP1) equally divides the HM capacity  between its east and south ports. Similarly, router R5  distributes it upstream HM capacity (available at its north  port) among its local and south ports, meaning that IP5  enjoys exactly a quarter of the HM bandwidth when all  sources are saturated. Following similar analysis, it can be  easily shown that IP12 only gets 1/144 of the HM capacity,  as its packets contend four times with packets from another  input port (routers R12, R8, R4, R1) and two times with  packets from two other input ports (routers R3, R2).  The HM access regulation distributes  the saturation  bandwidth fairly among all source modules, even when the  network is extremely loaded. This is attributed to the fact  that control messages bypass the slowly-moving data  packets and do not suffer from the source fairness  problems.              5.2 MPEG-4 decoder  In this test case, we use an MPEG-4 video decoder of  [9] to  evaluate the performance of the access regulation. The SoC  is composed of 12 processing elements placed on a 3x4  grid. By analyzing the communication demands (Figure 9),  it is clear that two modules are at high demand by multiple  sources: The DRAM controller has 7 incoming flows  (accounting for 25% of the total traffic), and the SRAM2  module with 4 incoming flows (22% of the total traffic).  In order to evaluate the system's performance, we use a  model similar to the one described in Section  5.1, with the  MPEG-4 video decoder communication demands  [9]. The  DRAM controller and SRAM2 modules are equipped with  allocation controllers. In order to create varying HM load,  the HM bandwidth is adjusted.   Two module mappings are used: In the first mapping (  Figure 10a), the two hot-modules are placed in relative  proximity to each other, in a way that causes some of the  packets destined at those modules to contend for the same  inter-router links. In the second mapping (  Figure 10b), the placement is optimized so that no such  sharing takes place but routing paths are kept short. This  placement minimizes the effect each HM has on the other.  5.2.1 System Performance  As in the previous example, we first examine the system  performance in steady state while increasing the HMs  utilization. Figure 11 shows the overall delays in the  system using the basic placement, with and without  allocation control. When the uncontrolled HMs utilization  increases, traffic destined to other modules suffers delays  which are more than a hundred times larger than their zero  load  latency. Activating  the  allocation  controller  mechanism frees expensive network buffers, thus allowing  the non-HM packets to arrive at their destination with  considerably smaller delays. Note that the small increase in  the non HM-traffic delay is imminent as it reflects the  growing usage of the network resources by HM packets.  Figure 12 shows similar effects when the optimized module  placement is used. Without control, non HM-traffic suffers  of extremely high delays. The latency is reduced by an  order of magnitude when the allocation controller is  introduced, without any noticeable increase in the latency  of the HM-traffic (the two HM-traffic curves in Figure 12b  overlap).  5.2.2 Source Fairness  Figure 13 shows the saturation throughput in the MPEG-4  system. As explained above (Section  2), when no control is  used, distant modules only get a small fraction of the scarce  resource. For example, the share of SDRAM bandwidth  that module 00 (VU) gets in the optimized system is more  than six times larger than the one of module 20 (ADSP)  and of module 21 (UP SMAP).  Figure 7: Mean end-to-end delays vs. HM load.  Curves represent mean delay normalized by the zero-load delay.  Figure 8: Saturation throughput.  Figure 9: MPEG-4 communication demands.  The amounts specify the average traffic [MB/s]  (a)  (b)  Figure 10: MPEG-4 SoC placement.  Basic (a) vs. optimized (b) placement                        (a)  (a)  (b)  (b)  Figure 11: Mean end-to-end delay vs. hot-modules load.   (Basic placement)  Figure 13a reveals an additional peril in uncontrolled  systems: Although HM demand is maximal, the expensive  hot-modules are idling 40% of the time. This happens since  packets destined at the different hot-modules compete for  the same network resources and block each other from  making progress in the network. As the hot-modules in the  optimized system are placed so that no such sharing  happens, the hot-modules are fully utilized. Unfortunately,  optimal placement is not always feasible due to layout and  timing constraints. HM access is successfully regulated  using the control mechanism, and each source gets a fair  share of each congested module (Figure 13b). The control  packets consume less than 2% of the HM bandwidth.  6. Summary  The unique characteristics of wormhole routing make it  particularly suitable for high-performance NoCs. However,  it is also highly vulnerable to loaded hot modules. Due to   wormhole's backpressure mechanism, NoC buffer depletion  effects extend system wide instantaneously.  Figure 12: Mean end-to-end delay vs. hot-modules load.  (Optimized placement)  Two main problems were identified: the source fairness  problem, and  the degradation of  the entire system  performance, as non HM-traffic is also blocked during HM  congestion. If HMs are left unhandled, system performance  is determined by network topology and routers' local  arbitration policy, instead of following system optimization  goals.  The main thrust of this paper is that system behavior  should be controlled explicitly by the architect rather than  by network side-effects. The network should include  mechanisms to facilitate such explicit control. In order to  solve both the fairness and the system performance  problems, we have presented a low-cost end-to-end access  regulation mechanism. Short control messages are used to  arbitrate access to the HM, thus significantly reducing  packet blocking probability and achieving fairness. The  protocol, which is transparent to the system functional units  and to the NoC, is implemented without modifying the  network routers, allowing them to be simple, and thus fast,  small and efficient. The protocol exploits a high-priority  service level which is readily available in the NoC for fast  signaling. Therefore, there is no overhead at the network                  (a)  (b)  Figure 13: Saturation throughput.  (a) Basic placement (b) Optimized placement  layer. Simple logic is added to the network interfaces of  sources, and potential HMs are instrumented with an  allocation controller, customized to system needs. We  suggest HM access regulation as an essential supplement  for any wormhole-based NoC.  7. "
Reflections on 10 Years as a Commercial On-Chip Interconnect Provider.,"Summary form only given. Sonics was founded in 1996, just as the term ""system on a chip"" began to enter the common semiconductor vernacular. The author identified the wide variety of heterogeneous components that would need to cooperate to satisfy embedded consumer and communications applications as a key challenge in completing SoC designs. Networking and telecommunications technologies seemed to offer the required abstraction, decoupling, and hard real-time performance guarantees that conventional computing approaches lacked. By early 1997, Sonics had become a licensor of active interconnect technology focused on SoC applications. This paper examines the changes the author has seen in SoC's, and how well those changes matched the predictions embodied in Sonics' products. It explores the key challenges in current embedded system design, and makes predictions about how such designs are likely to evolve","NOCS 2007  Dinner Speaker  Reflections on 10 Years as a Commercial On-Chip   Interconnect Provider  Drew Wingard  CTO, Sonics, Inc.  Abstract:     Sonics was founded in 1996, just as the term ""System on a Chip"" began to enter the common  semiconductor vernacular. The author identified the wide variety of heterogeneous components that  would need to cooperate to satisfy embedded consumer and communications applications as a key  challenge in completing SoC designs. Networking and telecommunications technologies seemed to  offer the required abstraction, decoupling, and hard real-time performance guarantees that  conventional computing approaches lacked. By early 1997, Sonics had become a licensor of active  interconnect technology focused on SoC applications.     While much has changed over the past 10 years, the fundamental principles behind Sonics' approach  have not. This talk will examine the changes the author has seen in SoC's, and how well those changes  matched the predictions embodied in Sonics' products. The talk also explores key challenges in current  embedded system design, and makes predictions about how such designs are likely to evolve.  Bio:     Drew Wingard is a founder and the Chief Technical Officer of Sonics, Inc., which has been providing  SMART interconnects since its founding in 1996. He has been the original architect of Sonics' products  and the original creator of the Open Core Protocol interface specification. He currently represents  Sonics on the Governing Steering Committee of OCP-IP, where he chairs the Specification working  group.     Prior to founding Sonics, Wingard led the development of advanced circuit and CAD methodology for  MicroUnity Systems Engineering, Inc. Previously he had co-founded Pomegranate Technology, where  he designed an advanced SIMD multimedia processor. He received a B.S. from the University of Texas,  Austin and an M.S. and Ph.D. from Stanford University, all in electrical engineering.                  "
"Fast, Accurate and Detailed NoC Simulations.","Network-on-chip (NoC) architectures have a wide variety of parameters that can be adapted to the designer's requirements. Fast exploration of this parameter space is only possible at a high-level and several methods have been proposed. Cycle and bit accurate simulation is necessary when the actual router's RTL description needs to be evaluated and verified. However, extensive simulation of the NoC architecture with cycle and bit accuracy is prohibitively time consuming. In this paper we describe a simulation method to simulate large parallel homogeneous and heterogeneous network-on-chips on a single FPGA. The method is especially suitable for parallel systems where lengthy cycle and bit accurate simulations are required. As a case study, we use a NoC that was modelled and simulated in SystemC. We simulate the same NoC on the described FPGA simulator. This enables us to observe the NoC behavior under a large variety of traffic patterns. Compared with the SystemC simulation we achieved a speed-up of 80-300, without compromising the cycle and bit level accuracy","Fast, Accurate and Detailed NoC Simulations Pascal T. Wolkotte and Philip K.F. H ¨olzenspies and Gerard J.M. Smit University of Twente, Department of EEMCS P.O. Box 217, 7500 AE Enschede, The Netherlands ∗ P.T.Wolkotte@utwente.nl Abstract Network-on-Chip (NoC) architectures have a wide variety of parameters that can be adapted to the designer’s requirements. Fast exploration of this parameter space is only possible at a high-level and several methods have been proposed. Cycle and bit accurate simulation is necessary when the actual router’s RTL description needs to be evaluated and veriﬁed. However, extensive simulation of the NoC architecture with cycle and bit accuracy is prohibitively time consuming. In this paper we describe a simulation method to simulate large parallel homogeneous and heterogeneous network-on-chips on a single FPGA. The method is especially suitable for parallel systems where lengthy cycle and bit accurate simulations are required. As a case study, we use a NoC that was modelled and simulated in SystemC. We simulate the same NoC on the described FPGA simulator. This enables us to observe the NoC behavior under a large variety of trafﬁc patterns. Compared with the SystemC simulation we achieved a speed-up of 80-300, without compromising the cycle and bit level accuracy. 1 Introduction In the Smart chipS for Smart Surroundings (4S) project [1] we propose a heterogeneous Multi-Processor Systemon-Chip (MPSoC) architecture. The SoC architecture contains a heterogeneous set of processing tiles interconnected by a Network-on-Chip (NoC). Future systems will consist of several tens or hundreds of tiles [3]. The development of such a heterogenous multi-tiled platform introduces problems related to hardware/software codesign. The MPSoC architect wants to study all kinds of trade-offs, e.g. operation bit-widths, memory sizes, and performance parameters/bottlenecks, e.g. latency and throughput. Common practice is to do extensive simulations of the MPSoC architecture before the system can be realized in silicon. In general the approach of simulating such large MPSoC designs is to either use (non-cycle accurate) high ∗ This research is conducted within the Smart Chips for Smart Surroundings project (IST-001908) supported by the Sixth Framework Programme of the European Community. level modelling or accept long simulation times with cycle accurate simulations. For systems consisting of several tens or hundreds of tiles, cycle-true simulation leads to excessive simulation times. Despite these excessive simulation times, cycle and bit accurate tests are required. These tests enable the designer to verify the design before manufacturing. Furthermore, it is possible to make accurate performance trade-offs and determine the consequences in area and power consumption of the actual design. Using the same concrete implementation (i.e. the same HDL source code) for simulation as well as synthesis we minimize the risk of errors in the design ﬂow. Future on-chip communication networks, the routers and their interfaces to the tiles, will have to support an increasing amount of services [16] compared to traditional busses. The communication infrastructure has to be able to provide guarantees, for example, guaranteed throughput, bounded latency and/or low jitter. These network services are required, because of the increasing number of applications that require real-time support. To understand those requirements and their inﬂuences on the NoC design we need to study the application-generated trafﬁc on the on-chip network in detail. For the performance of the NoC, we cannot just analyze a single router and determine a local optimal schedule, because this might cause buffering problems in the neighboring routers. Testing the network under random trafﬁc load and evaluating its routing or arbitration mechanism will not be sufﬁcient to understand the extra levels of services required by real-time applications. In this paper, we present a method to study the design choices of the network at the highest level of detail, without sacriﬁcing simulation speed. We will demonstrate this by analyzing the latency and throughput behavior of our NoC [13] under different scenarios and conﬁgurations. 1.1 Related Work There are several methods to analyze large heterogeneous systems. High level formal analysis methods can be applied [17], where an application of the system is characterized by high level parameters (e.g. latency of a task). Data dependencies and interactions of processes can only be analyzed if their characteristics can be described with the high level model. However, this is only applicable for a restricted number of cases. For example, at design time the latency and execution time of a task have to be known (manifest). Another method is system level simulation, such as SystemC [2] at different levels of abstraction. It can be used to describe systems from functional level to RTL level. The level of abstraction determines the speed of simulations. An example of SystemC simulation for NoC is the On-Chip Communication Network (OCCN) project [5], which deﬁned a universal Application Programming Interface (API) for speciﬁcation, modelling, simulation, and design exploration of NoCs. Another framework is presented by Kogel [14]. In the design ﬂows of Æthereal [11] and xpipesCompiler [12], SystemC simulation is used for performance validation. The level of detail in the SystemC simulation tremendously inﬂuences the speed of simulation. Transaction Level Modelling (TLM), abstract data types and timed simulations showed almost a 3 orders of magnitude speedup compared to Register Transfer Level (RTL) modelling [14]. However, optimizations to increase the simulation performance sacriﬁce the level of detail in the simulated results. For our packet switched NoC, we would like to monitor the cycle and bit accurate behavior under different trafﬁc loads. A bit and cycle accurate simulation is required, because adaptations to the logic behavior of the router and the network as a whole are foreseen. Furthermore, we need to do extensive cycle true simulations, with the synthesized sources, before a chip can be realized. We developed a SystemC description of the router [13], but the simulation frequency was disappointing. Seriously testing a single scenario on one speciﬁc network conﬁguration already took a full day. Therefore, an FPGA based simulator was considered. For very large multiprocessor systems, an FPGA based emulation platform makes accurate and fast system simulation possible, as proposed in the RAMP project [4]. This approach requires multiple FPGA platforms as for example provided by the Zebu-XL system emulator [8]. We would like to adopt the RAMP method and develop a simulator that requires a single FPGA. Several FPGA based implementation to validate NoCs are described. Marescaux [15] describes their implementation of interconnection networks on an FPGA. Genko [10] proposed a NoC emulation framework implemented on a Virtex-II FPGAs. The emulation platform combines trafﬁc generators, network interfaces, routers and trafﬁc receptors. The platform is controlled by the FPGA’s PowerPC and can work at 50 MHz. Genko’s approach is bounded by the maximum available slices. For example, a 6 router network required 79% of a Virtex-II Pro VP20. The approach in this paper can simulate far more routers in a single FPGA. By sequential simulation we relaxed the hardware requirements and increase the size of the simulated NoC. We demonstrate the power of the simulator with a series of test scenarios using our Network-on-Chip. The rest of the paper is organized as follows. In section 2, we describe the NoC that we would like to analyze. In section 3, we describe three methods we evaluated to simulate this network. In section 4, we describe the method that is used to simulate a parallel system sequentially. In section 5, we describe the implementation of this method onto the FPGA platform. In section 7, we show the speed improvements that are achieved by the FPGA method. In section 8, we discuss the simulator and conclude the paper in section 9. 2 Network-on-Chip For the NoC, we have deﬁned two networks (packetswitched [13] and circuit-switched [18]) that can both handle guaranteed throughput (GT) trafﬁc and best-effort (BE) trafﬁc simultaneously. The guaranteed throughput trafﬁc is deﬁned as data streams that have a guaranteed throughput and a bounded latency. The best-effort trafﬁc is deﬁned as trafﬁc where neither throughput nor latency is guaranteed. In this paper we focus on the packet-switched network. However, the approach can also be used for the circuit-switched network and other designs. 2.1 Packet-Switched Network-on-Chip The packet-switched router described by Kavaldjiev [13] implements wormhole routing with virtual channel (VC) ﬂow control. The advantage of wormhole routing is the packet-size independent buffer-size. The virtual channels are used to decrease the chance of blocking and enable the routing of GT trafﬁc. Each packet in the network consists of H header ﬂits to setup a route, an arbitrary number of data ﬂits that contain the packet’s information and one tail ﬂit that will free the router’s resources for other packets. The header describes the route, which is predetermined via source-routing, and one header ﬂit is consumed at each hop of the route. The router has ﬁve input and ﬁve output ports and four virtual channels per port. The ﬂits (atomic unit) of a packet are labelled with their virtual channel number and they are buffered in four ﬂit deep queues at the input ports. Per port, four queues are available — one queue per virtual channel. The outputs of the queues are not multiplexed per port, but directly connected to the crossbar. This is used to ease the arbitration compared to a standard wormhole router with virtual channels [6]. The crossbar is asymmetric and has 20 inputs, one input for every queue, and ﬁve outputs that are directly connected to the router’s output ports. The access to the crossbar is arbitrated by 5 round-robin arbiters - one arbiter per crossbar output. This arbitration is sufﬁcient, since a conﬂict can only arise when more than one queue contains ﬂits destined for the same output port. Due to the predictable round-robin arbitration the router is able to handle GT trafﬁc, if one single data stream is assigned per VC. Multiple BE packets can be assigned to the same output VC. ] l s e c y c [ y c n e a L t 600 500 400 300 200 100 0 0 Guarantee GT mean GT max BE mean 0.02 0.04 0.06 0.08 0.1 0.12 BE load per PE [fraction of channel capacity] 0.14 Figure 1. Message delay of the GT and BE trafﬁc vs. BE load for 6-by-6 network (queue size 2 ﬂits) For BE packets, the packets competing for the same output VC are tagged by the sender with an unique header identiﬁer (ID) per header ﬂit. The routers are connected to a free-running global counter whose value is distributed to all inputs. When an output VC is freed the next packet that claims it is the one whose header ID equals the current counter value. The uniqueness of the BE ID guarantees conﬂict-free arbitration and crossbar access, but does not guarantee bandwidth or latency. Since, at any time, the counter contains a arbitrary value, fairness is provided. Although the output round-robin arbitration is deterministic and maximum latency per hop can be determined, the speciﬁc latency and timing of packets largely depends on the global behavior of the network. Figure 1 shows the result of the latency simulation for a 6-by-6 network that has been performed in SystemC. A similar simulation is also performed on the new FPGA based simulator, where a faster and more extensive exploration of the results is possible. The details of this detailed simulation are described in section 6.2. The graph shows how the latencies of the GT and BE messages depend on the offered BE load. For the GT trafﬁc, the mean and the maximal latency of packets are given. When the offered BE load is low, the latency of the GT packets is smaller than the guaranteed (or allowed) latency. The reason is that the GT trafﬁc utilizes the bandwidth unused by the BE trafﬁc. Note: the latency of the GT packets is higher than the latency of the BE trafﬁc because in this experiment the GT packets are larger (256 bytes against 10 bytes for BE packets). With the increase of the BE load, the average and maximum latency of the GT trafﬁc increases, but the maximum GT latency never exceeds the guaranteed latency. 1. VHDL 2. SystemC 3. An FPGA For all options we modelled the NoC cycle and bit accurate. We started with a VHDL description, that could also be used to obtain synthesis results. A VHDL description was necessary, because, besides latency analysis, we are also interested in the area and power consumption of the NoC design. Router implementation with with very good throughput and latency, might result in a very high power consumption. For example, we found that buffers require a relatively large amount of area and energy. So we would like to redo the simulation of Figure 1 with different buffer sizes and investigate what the effect of buffer size on performance and energy consumption is. For the latency and throughput analysis, we were hampered by the 10 cycles per second simulation speed of the VHDL simulation. Therefore, a SystemC model seemed a good approach with, as literature suggests, orders of magnitude speed improvements [14]. The SystemC simulations gave the ﬁrst insights into the behavior of the network as is shown by Figure 1. However, to generate all the information that was required for this single graph we needed 29 hours of simulation time on a single Pentium 4. The attempt to simulate a NoC in an FPGA was inspired by the fact that an FPGA has a lot of internal storage, which enables updating a large number of registers in a single cycle. In our lab we have a platform available with a multiprocessor SoC as is described in section 5.1. This SoC has two ARM processors that can communicate via a memory interface with a single Virtex-II FPGA. This platform was used in our experiments. The method to simulate the network in an FPGA is described in the next section. In section 5 we describe the details of the FPGA simulator. 4 NoC Simulation in an FPGA There are several ways to simulate a Network-on-Chip in an FPGA. The ﬁrst idea was to instantiate the whole network in the FPGA including simple trafﬁc generators, but initial synthesis tests showed a limitation of approximately 24 routers in a Virtex-II 8000. These results were obtained with a reduced data-path of 6-bit and without the network interfaces, trafﬁc generators and simulation controllers. The two major bottlenecks were the number of CLBs and available number of tri-states in the FPGA. Therefore, a sequential simulator of the network was considered. The sequential simulator has another tradeoff between hardware resource requirements and simulation speed. The details of this sequential method are described in [19], but a coarse description is given in this section. 3 Simulation of Network-on-Chip 4.1 Sequential simulation To simulate a NoC (a network of routers) we examined three options using: The basic idea of the sequential simulator is as follows. In a synchronous parallel system, each block has some func      tionality (its combinatorial circuitry) and a number of registers that store the block’s internal state. In a normal single system cycle, all the blocks will update their state concurrently, based on the current state and its inputs. In our sequential simulator we will sequentially evaluate each block in one function cycle, which requires more overall clock cycles but less hardware. A system cycle is partitioned in N function cycles, where N is the number of parallel functions (blocks) that are sequentially simulated. All blocks that have identical functionality can reuse the combinatorial circuits. The block’s state is stored in a large memory to maintain the state of the complete parallel system. When the state of all blocks is modiﬁed we switch to a new system cycle. The method described above can be used to simulate any parallel system on any sequential processor. The frequency of the processor, the simulated functionality and the amount of registers that have to be updated determine the speed of the simulation. If we use a 32-bit processor like the Pentium 4 we can update at most 32 bits per function cycle in a bit accurate simulation. Furthermore, the evaluation of a function cycle will require multiple clock cycles. Although a Pentium 4 is a very fast processor, the simulation will not be very fast. Speed improvements can be achieved by only scheduling the registers that require evaluation in the system cycle because their inputs have changed (event driven). The number of bits that can be updated in a function cycle is much larger in an FPGA. It can read and write a large number of bits in the available internal RAM. For example the Xilinx Virtex-II 8000 FPGA has 192 dual-port block-rams of 512 positions each 36 bits wide per RAM. This makes it theoretically possible to access 6912 registers in parallel. Furthermore, the FPGA has a large amount of logic, which can evaluate the functionality of multiple blocks in parallel or a single large block. This also makes it possible to do both the evaluation and update of the registers in parallel. A function cycle can be evaluated in one FPGA clock cycle. 4.2 Sequential simulation of a NoC In a synchronous, homogeneous NoC, each router has an identical functionality. We partition each router into a single large block and sequentially evaluate each router in one function cycle. The partitioning of a whole router per block is possible due to the amount of registers and functionality per router. The amount of function cycles per system cycle equals the amount of routers in the network. Our NoC routers are described in VHDL, which can immediately be used to synthesize to FPGA or silicon technology. It is very important that we can use (almost) unmodiﬁed VHDL sources, as this will minimize the risk of errors between the synthesized hardware and the simulator. For the sequential simulator it is required to apply a small modiﬁcation of the sources, i.e. we have to re-map all the registers of the router to a large memory. The current values of the links are stored in a memory as SoC-board FPGA-board SRAM CTRL ARM 9 ARM 9 U S B U A R T AHB BUS Bridge RAM DMA M e m o r y Control address M e m o r y i n e t i n e t r f a c e data r f a c e NoC Virtex-II FPGA design APB BUS I/O CTRL Clock Interrupts I/O CTRL Figure 2. Schematic view of the hardware well. Any network consisting of a homogeneous set of N routers can be simulated independent of its interconnection topology. The topology is determined by the generated read and write address pattern for the link’s memory. 5 Implementation The implementation of the simulator requires a hardware platform, an FPGA design and software. First, we need a hardware platform with a (large) FPGA that is able to simulate the network. In section 5.1 we describe the platform we had available. Second, we need an FPGA design that is able to simulate the router according to the described method of section 4. In section 5.2 we describe the architecture and functionality of our FPGA design. Third, the NoC design needs stimuli and analysis software to evaluate the router’s performance. This is described in section 5.3. 5.1 Hardware Platform Figure 2 depicts the general block diagram of the available platform with the most important components. It consists of a SoC board and an FPGA board. The SoC board contains a dual-core ARM general purpose processor. The two ARM processors have a frequency of 86 MHz and 192 MHz. The SoC is connected with 1 MB of on-board SRAM memory and lots of peripherals and connectors. One of the connectors connects the FPGA board with the SoC board. This connector contains a memory interface with a 32-bit wide data-bus and a 17-bit wide address-bus. Via this memory interface we can control the logic of the FPGA and exchange blocks of data between the RAM of the SoC and the memory instantiated in the FPGA. The FPGA board itself does not have a separate off-FPGA memory. The FPGA board contains a Virtex-II 8000 FPGA. Any other platform that has a large FPGA and on-board SRAM memory available will be suitable for the simulations. The on-board SRAM is required by the general purpose processor that controls the simulation. This control is both generation of stimuli vectors as well as analysis of the results. This general purpose processor can either be implemented on the FPGA or, as in our case, a separate processor. With the dual-core ARM chip we can partition the control software among the two processors.     Global NoC control External interface control Simulation control R i l l i t F e c s e v o e g d Router Stimuli b u ff e r b u ff e r b u ff e r b u ff e r Input stimuli Access control Access log Received flits Link state Link state Link state Link state Router Local router state Link state Link state Input link log Link state Link state Figure 3. Schematic view of the FPGA design State Input queues Router control and arbitration Links Stimuli interfaces Total Size 1440 bits 292 bits 200 bits 180 bits 2112 bits Table 1. Required number of bits per router 5.2 FPGA implementation Figure 3 depicts the major blocks of the FPGA design. The design can be partitioned in two major parts. 1. The router part, that describes the logic of a single router and its stimuli interface. 2. The global part, which controls the FPGA and the Network-on-Chip that is simulated. For the router we separated the combinatorial logic from the registers in the original router design. The inputs and output signals of all registers are concatenated into two memory words: old and new. The old word is the current state of the router and is read from the memory at the start of a function cycle. The new word is the result of the evaluation and has to be written into the memory at the end of a function cycle. The address of the memory corresponds to the router that is evaluated. Table 1 summarizes the width of the memory word. The number of routers in the network determine the depth of the memory. In the current implementation reading the values from memory takes 1 cycle. Evaluation of the combinatorial logic and writing the result in memory takes another cycle. In total a function cycle equals 2 FPGA cycles. For a design with N routers one system cycle takes 2N FPGA cycles. The stimuli for the design are generated by software in the ARM9. We have chosen to generate the stimuli in software, because it is easier to deﬁne new tests and analyze the results in software. The disadvantage is the large amount of data that has to be copied from the ARM9 to the FPGA and vice versa. The stimuli are buffered per virtual channel (VC) in cyclic buffers in the FPGA. The output values of the network are stored per router, and not per VC, because a tile will receive at most 1 ﬂit per system cycle. The data in the buffers have a timestamp and can be read or written by the ARM9. The timestamps make it possible to store only valid data, which requires less storage space and less time to copy data. Two extra cyclic buffers make it possible 1) to log the trafﬁc of a speciﬁc link and 2) to log the access delay a ﬂit notices before it enters the network. These two buffers cannot inﬂuence the trafﬁc in the NoC. The cyclic buffers make it possible to run the simulation independently from the copying of data. Of course, we have to prevent buffer under- and overrun, because it will inﬂuence the trafﬁc in the NoC. The ARM will request the FPGA to simulate a speciﬁc number of system cycles. The FPGA will signal the ARM when it is ready with the requested number of system cycles. During the simulation in the FPGA, the ARM performs other tasks as described in the next section. 5.3 Software The simulation is completely controlled in software by one or two ARM processors. The choice for one or two processors depends on the estimated simulation length and desired simulation speed. The software is partitioned in processes that communicate via cyclic buffers. All the processes can run in parallel and do not have dependencies. Figure 4 depicts the organization of these processes and what part of the hardware is involved. The top processes require only the ARM processor and the NoC simulation itself only requires the FPGA. The two processes that interchange data between the boards require both ARM and FPGA. Each process that requires an ARM can be mapped on either of the two ARM processors. Because each process uses its own cyclic buffers, it only needs to be ﬁred when data and free memory are available. The processes that only require the FPGA or ARM run in parallel, which tremendously reduces the simulation time. We also have two ARMs available on the SoC-board that make it possible to run the generate and analysis process in parallel. Running the two processes in parallel requires more effort and time from the simulator user. Therefore, the parallelism with two ARMs is only beneﬁcial if long simulations are performed. The simplest partitioning is running the generating and loading of data on one ARM, and retrieving and analyzing on the other ARM. This requires the least amount of inter processor communication and gives a good speed-up. The simulation is performed in steps. We start with generating a routing information table. After all routes are de   Legend 6.1 Best eﬀort traﬃc Analyze data FPGA NoC FPGA Other FPGA NoC & ARM ARM Run N cycles Retrieve data Print statistics yes Ready? no Random generator Gen. routes Gen. stimuli Load stimuli Figure 4. Software processes of the simulation termined, a loop is started that has ﬁve phases. 1) We start by generating the trafﬁc for each node in a stimuli table. Any data pattern can be generated as the generation is done in software. The generation process uses a random number generator on the FPGA. Reading a 32-bit random number from the FPGA is noticeably faster compared to the standard rand() function in C. The generated stimuli table contains stimuli for at least x system cycles. 2) The generated stimuli have to be written into the input buffers of the FPGA. All input buffers are maximally ﬁlled unless no data is available. 3) After ﬁlling the buffers we start the simulation in the FPGA and evaluate x system cycles. This sequence of x simulated system cycles is called a simulation period. To prevent buffer underrun, the simulation period is ﬁxed to the size of the VC stimuli buffers in the FPGA. The simulation in the FPGA needs to be started by software, but can run autonomously. 4) After a single simulation period, we have to empty the output buffers. We retrieve the data from the output buffers that we think are interesting for the analysis. For the buffers that are not interesting we can update the read-pointer, which empties the buffer. 5) After the data is retrieved from the FPGA it is analyzed and the desired statistics are stored. When the simulation is not ﬁnished we go to step 1 and generate extra trafﬁc in the stimuli table. This makes it possible to simulate an arbitrary number of simulation periods which is not limited by the software or hardware. However, due to back-pressure in the network, not all generated data might have been written into the FPGA. To prevent the loss of this data and the potentially resulting undeﬁned state of the stimuli, all unconsumed data will eventually be written into the FPGA. If the network is overloaded with trafﬁc and it does not accept data on virtual channels for a longer time, this is reported to the user and simulation is stopped. Most of the NoC architectures are simulated with random trafﬁc uniformly distributed over time and tiles. The average latency per packet is calculated and plotted versus the packet injection rate. In this example we apply different trafﬁc patterns to a various set of NoC conﬁgurations. As a reference we use the virtual channel wormhole routing router as described in section 2 and a two dimensional 6x6 mesh topology. Before the start of the simulation we randomly select pairs of communicating tiles. Per communicating pair of tiles, we determine the route through the mesh-topology via XY-routing. XY-routing is applied, because it is deadlockfree. For a 6x6 network we could map 493 randomly selected pairs onto the network. On average, a tile communicates with 14 other tiles, which is close to the maximum of 16 (4 VCs and 4 IDs per VC). All the routes that use a speciﬁc link are uniformly distributed over the four virtual channels and four IDs per virtual channel. Each communicating pair will transport a packet of D data ﬂits at random moments in time. The data ﬂits are preceded by H header ﬂits that contain the routing information and followed by single tail ﬂit that will free the router’s resources for other packets. The number of header ﬂits is equal to the number of hops of the packet’s route. Per hop, a header ﬂit is consumed, which reduces the average number of ﬂits/cycle that arrive at a speciﬁc tile. Therefore, the latency ﬁgures in this section are plotted versus the injection and extraction rate of the tiles. The injection rate is deﬁned as the average number of ﬂits per system cycle a tile injects into the network. The extraction rate is deﬁned as the average number of ﬂits per system cycle a tile receives from the network. The latter represents the amount of useful data transported by the network. The latency per packet is measured and grouped depending on the length of its route. The minimum latency per packet is equal to 2H + D + 1 (the number of hops + the length of the packet). Figure 5 depicts the average and maximum latency of a packet versus the injection and extraction rate when all packets contain 5 data ﬂits. The difference of the average latency depending on the injection or extraction rate is caused by the header consumption of the network. Furthermore, it is visible that the average latency is inﬂuenced by the length of the route. The average latency is given for a selection of route lengths. However, the maximum latency of a single packet is not directly related to the length of the route, but mainly dependent on the packet injection rate. 6 Simulation examples 6.1.1 Improvements In this section we describe simulation examples that are performed on the NoC simulator. These examples are used both to analyze the behavior of the network and to proﬁle the simulator. The analysis of the NoC is included in this section and the proﬁle results are described in section 7. The majority of the latency seems to be caused by the access mechanism to the crossbar for BE trafﬁc at the setup of the route through the network. The access mechanism prevents two or more packets to be granted to access the crossbar and select the same output virtual channel. The packets from different input VCs have an unique ID if they ] l s e c y c [ y c n e t a L 140 120 100 80 60 40 20 0 0 Average, 2 hops Average, 5 hops Average, 9 hops Maximum latency Extraction rate 6x6, 5 data flits 8x8, 5 data flits 6x6, 11 data flits 80 70 60 50 40 30 20 10 ] l s e c y c [ y c n e t a L 0.1 0.2 0.3 0.4 0.5 Injection / Extraction rate [flit/cycle/tile] 0.6 0 0 0.05 0.1 0.15 0.2 0.25 0.3 Extraction rate [flit/cycle/tile] 0.35 0.4 Figure 5. Average and maximum latency for 5 ﬂits BE packets Figure 7. Average and maximum latency for 5 ﬂits BE packets for different topologies 80 70 60 50 40 30 20 ] l s e c y c [ y c n e a L t Optimal header ID Header compr. and ID sel. Reduced buffer size 9 hops 5 hops and have a lower latency, which is depicted in Figure 6. In this case we combined optimized header ID selection with header compression. Solely header compression did not show a major improvement. Header compression is especially beneﬁcial for higher loads and small packets. The latency at low loads is mainly effected by the ID selection. A third test was to reduce the queue size (2 ﬂits) of the NoC and see its effect on the performance. Both header compression and ID optimization were included as well. At low trafﬁc loads the buffer size marginally inﬂuences the latency. Due to the smaller buffers, the network saturates at lower loads. 0 0.05 0.1 0.15 0.2 0.25 0.3 Extraction rate [flit/cycle/tile] 0.35 0.4 6.1.2 Other scenarios Figure 6. Improved latency for 5 and 9 hops compete for the same output virtual channel. At most one packet per VC is granted if its header ID matches the global counter. In the reference simulation we chose a random ID for each header. Although this is fair, it is not fast. As the ID counter is global for the whole NoC, we can predict what the counter value is when the next header of the packet requests crossbar access at the next router, based on the minimum header delay. In the current design this delay is 3 cycles. Figure 6 depicts the latency if the header ID is optimized to this 3 cycle delay. Especially for low trafﬁc loads the latency is reduced by approximately 20%. The network almost saturates at the same extraction rate. Furthermore, for these small packets of 5 data ﬂits we noticed a relatively large overhead of the header ﬂits as the average route in a 6x6 network is 4.7 hops. The header/data ratio is almost one. Each header uses only 6 of the available 16 bits, which makes it possible to combine two header ﬂits into one and reduce the average number of header ﬂits to 2.4. The router requires only one extra multiplexer per VC to select the correct bits when it receives its ﬁrst header of a packet. We expected the network to saturate at higher loads For a second scenario we varied the network size and number of data ﬂits in the packet. We simulated various mesh topology sizes and packet sizes. In Figure 7, the latency depending on the extraction rate is depicted for both a 6x6 network and 8x8 network. For the 6x6 network we display the average latency for the scenarios with 5 or 11 data ﬂits per BE packet. From this ﬁgure, it is clear that a larger network will saturate at a lower extraction rate (lower throughput per tile). Similar behavior is observed by Duato [7, chapter 9]. Larger packets have a higher average latency, but no noticeable change of the saturation rate. 6.2 Jitter analysis As described in section 2 our virtual channel wormhole router can support both best-effort and guaranteed throughput trafﬁc. Kavaldjiev [13] performed latency measurements to show that a guaranteed throughput stream will never violate its given deadline, independent of other trafﬁc in the NoC. Figure 1 shows an example where an increasing best-effort load will increase the average and maximum latency of the guaranteed streams, but the maximum latency never exceeds the guaranteed latency.                   In the following test we performed a similar test, but we want to study the inter packet jitter of the various guaranteed throughput streams. We assume the simulated network has a clock frequency of 333 MHz and uses applications that are similar to HiperLAN/2 [9]. In this application all communication occurs in burst with periods of 4 µs. We mapped the following three types of streams onto the network: 1. Guaranteed throughput (GT) packets of 128 ﬂits with an average inter-packet time of 1333 cycles. At 333 MHz this is equal to the 4 µs period. 2. GT packets of 256 ﬂits with an average inter-packet time of 4000 cycles. 3. Best effort packets with 5 data ﬂits. The inter-packet time is varied to increase the load in the network. All BE streams are mapped on one single virtual channel. The unique ID will prevent conﬂicts at the crossbar. Both GT streams have minimum guarantee of 1/3 of packet equals: 3 · (H + D) cycles, where H is the length of the link bandwidth. The maximum latency of a single GT the route and D the length of a single packet. The test has 29 streams of type 1, 18 streams of type 2 and 103 streams of type 3, all uniformly distributed over the NoC. In this test, we are interested in the inter-packet jitter of the GT packets. Of course, it is important that the packets will arrive at the destination in less than the guaranteed latency. An extra level of service is that the packets will also arrive at a very regular interval (low jitter). In the test, we examine the latency of the two GT packet types and we make a histogram of the latencies that are grouped in the length of the route. We vary the best effort load over time and see how this inﬂuences the latency of the other streams. With this test we are able to show one of the major beneﬁts of the fast, but accurate, simulator. We need a very large number of packets to get enough accuracy of the variation in the packet latency. Suppose we simulate for 1.5 million cycles. This transports 1125 packets per stream of type 1 and only 375 packets of type 2. If we want to calculate an average latency, this is possible as we can group multiple streams of a type that have the same length, but for a histogram a lot of detail is still missing, as the latencies range from 130 to almost 400 clock cycles. Simulation of 1.5 million cycles in the SystemC simulator required roughly 1 hour and 45 minutes. In the FPGA simulator we do the same test in less then a minute. Therefore, we simulated for 15 million cycles in approximately 10 minutes and obtained detailed histogram results. Figure 8 depicts the distribution function (the percentage of packets that notice a speciﬁc maximum latency) of the latency of all streams of type 1 under different best effort loads. The y-axis gives the percentage of the packets notice the latency on the x-axis or less. For different BE loads, the distribution function changes. From this ﬁgure, it is clear that under almost no BE load, 50% of the packets arrive with the minimum latency (length route + length packet) and use the maximum bandwidth of the links. Because there are multiple GT streams in the NoC, part of the ] % [ e g a n e c t r e P 100 90 80 70 60 50 40 30 20 10 0 Minimum Latency 1/2 Through− put of link Maximum Latency BE: < 0.002 flit/tile/cycle BE: < 0.05 flit/tile/cycle BE: < 0.10 flit/tile/cycle BE: < 0.15 flit/tile/cycle BE: < 0.20 flit/tile/cycle BE: < 0.28 flit/tile/cycle 150 200 250 300 Latency [cycles] 350 400 Figure 8. Latency distribution under different BE loads ] % [ e g a t n e c r e P 100 90 80 70 60 50 40 30 20 10 0 Minimum Latency 1/2 Through− put of link Maximum Maximum Maximum Maximum Latency Latency Latency Latency BE: < 0.002 flit/tile/cycle BE: < 0.05 flit/tile/cycle BE: < 0.10 flit/tile/cycle BE: < 0.15 flit/tile/cycle BE: < 0.20 flit/tile/cycle BE: < 0.28 flit/tile/cycle 150 200 250 300 Latency [cycles] 350 400 Figure 9. Latency distribution under different BE loads, where the GT streams are trafﬁc shaped at the input packets have to share bandwidth of the link. Depending on the phase alignment of the packets they inﬂuence each other. However, this can be at most a reduction to half the bandwidth under low BE load. A little more than 10% of the packets observe this reduced throughput. Under higher best effort loads the GT packets gets inﬂuenced more and more. This has a major impact on the latency distribution function. This change in the distribution function will be noticed by the receiver as jitter in the arrival time of packets. If we now apply trafﬁc shaping and release the packets with their given bandwidth of 1/3, we expect them to experience less jitter in the network. This is depicted in Figure 9. It is clear that the streams do not get inﬂuenced by the BE trafﬁc, but experience a large latency as the packets enter the network at their guaranteed bandwidth. Although it is not directly visible in the graphs, this level of detail is only possible with long simulation times. In the previous SystemC simulations, this would have taken weeks and with our simulator the results were obtained in less than             Block CLB RAM Router Stimuli interface Network Random number generator Global control Total 1762 540 2103 2021 627 61 62 16 0 0 7053 (15%) 139 (82%) Table 2. FPGA resource usage 10 0 0.05 0.1 0.15 0.2 0.25 Injection rate [flit/cycle/tile] 0.3 0.35 0.4 20 30 40 50 60 70 80 90 100 110 S i m a u l i t n o r f y c n e u q e [ s e c y c l / m s ] 4x4 BE (5 flits) 6x6 BE (5 flits) 8x8 BE (5 flits) 6x6 BE (11 flits) 6x6 GT Figure 10. Simulation frequency versus injection rate 5 hours for 30 different levels of BE loads. Both simulation approaches can be speeded up by running multiple tests in parallel on multiple platforms. 7 Proﬁle Results In this section we describe the performance of the simulator. The simulator is realized for a Xilinx Virtex-II 8000 FPGA. Table 2 shows the resource usage of the simulator in the FPGA. From these results, it is clear that the limiting factor of the design is the number of RAM-blocks that are used. It would be possible to simulate the design in smaller FPGAs, but it would limit the maximum number of routers and/or the amount of state registers of the design. The router design is synthesized for a frequency of 6.6 MHz, which gives a function cycle frequency of 3.3 MHz. This limits the maximum simulation frequency of the simulator to (3.3 · 106 )/36 = 91.6 kHz for a 6-by-6 network. No effort was made to increase this frequency, because it was sufﬁcient for the current tests. The interface frequency is equal to the ARM frequency of 86 MHz, which makes it possible to copy data at a higher frequency. The number of system cycles that can be simulated depends on the simulation settings. As a reference we use the simulation frequency of our SystemC simulator that was used to derive Figure 1. These and other SystemC simulations on a 6x6 mesh network had an average frequency of 0.215 cycles/msec independent of the applied network load. 0.1 0.2 0.3 Injection rate [flit/cycle/tile] 0 5 10 15 20 25 30 35 40 45 T i m e [ c e s ] 6x6 mesh Generate & Simulate Generate Simulate Load Retrieve Analyze 0.1 0.2 0.3 Injection rate [flit/cycle/tile] 0 5 10 15 20 25 30 35 40 45 T i m e [ c e s ] 8x8 mesh Generate & Simulate Generate Load Analyze Retrieve Simulate Generate & Simulate Generate Load Retrieve Analyze Simulate Figure 11. Simulation times for 0.5M cycles Figure 10 depicts the simulation frequency (in cycles/msec) under different network loads for the series of performed tests as described in section 6. In all tests, we use the slowest of the two ARM processors at 86 MHz. From this ﬁgure we can conclude that the FPGA simulator is a factor of 80300 faster compared to our SystemC simulation. Under low trafﬁc loads, the frequency is close to the theoretical maximum that is limited by the FPGA. Under higher loads the frequency decreases due to extra time required by the software processes of Figure 4. Furthermore, it is visible that the frequency is approximately inversely proportional to the size of the network. For different tests on the 6x6 topology with identical load, the minor differences are caused by the relation between injection rate and packet rate. For example, the analysis and generation time for the BE tests is almost independent of the packet size. Larger packets (11 ﬂits) will cause a higher injection rate, which results in shorter simulation times at identical injection rates. The time required per process of Figure 4, to simulate 1/2 million system cycles, is depicted in Figure 11 for the besteffort tests with 5 data ﬂits in each packet for two network topologies. In these tests we run the generation and simulate process in parallel on ARM and FPGA. For most loads, the simulation process requires less time than the generation process, which completely hides the simulation time spent in the FPGA. Only in the 8x8 topology and under low loads can we achieve lower simulation times if we run more processes in parallel to the simulation process on the FPGA. Although the exact time varies between tests, in general we see that approximately 55% of the time is spend in generation of the stimuli. Loading, retrieving and analyzing data all require 15% of the time. The time for analysis increases if complex analysis techniques are included (e.g. determine variance of inter-ﬂit arrival times). As most of the time is spent in software and not by the FPGA, there is no reason to increase the FPGAs function cycle frequency.                 8 Discussion "
Implementing DSP Algorithms with On-Chip Networks.,"Many DSP algorithms are very computationally intensive. They are typically implemented using an ensemble of processing elements (PEs) operating in parallel. The results from PEs need to be communicated with other PEs, and for many applications the cost of implementing the communication between PEs is very high. Given a DSP algorithm with high communication complexity, it is natural to use a network-on-chip (NoC) to implement the communication. We address two key optimization problems that arise in this context - placement, i.e., assigning computations to PEs on the NoC, and scheduling, i.e., constructing a detailed cycle-by-cycle scheme for implementing the communication between PEs on the NoC","Implementing DSP Algorithms with On-Chip Networks Xiang Wu AMD Tamer Ragheb Rice University Adnan Aziz UT Austin Yehia Massoud Rice University Abstract Many DSP algorithms are very computationally intensive. They are typically implemented using an ensemble of processing elements (PEs) operating in parallel. The results from PEs need to be communicated with other PEs, and for many applications the cost of implementing the communication between PEs is very high. Given a DSP algorithm with high communication complexity, it is natural to use a Network-on-Chip (NoC) to implement the communication. We address two key optimization problems that arise in this context—placement, i.e., assigning computations to PEs on the NoC, and scheduling, i.e., constructing a detailed cycleby-cycle scheme for implementing the communication between PEs on the NoC. 1 Introduction DSP algorithms are often very computationally intensive. For example, the satellite TV standard DVB-S2 uses a Low-Density Parity Check (LDPC) code [15] with 64800 bit codewords; the terrestrial TV standard DVB-T uses an 8192-point FFT [11]. Performance is typically achieved by using an ensemble of processing elements (PEs) operating in parallel. CMOS scaling has resulted in huge improvements in the performance of logic gates. However, wires do not beneﬁt as much from scaling [20, 41, 46], and consequently for many applications, including LDPC decoding and FFT, the cost of implementing the communication between PEs is as high, and sometimes higher, than the cost of PEs themselves [14, 7, 30]. Therefore an emerging trend for connecting computational elements on a chip is to use a Networkon-Chip (NoC) [13]. An NoC replaces point-to-point connections (dedicated wires) with a switch fabric (wires connected to programmable crosspoints) [44]. A key advantage is that wiring resources can be shared via time-multiplexing, and so the same communication can be implemented with less interconnect. The NoC interconnect can also be made more regular, thus accelerating the physical design ﬂow. A key observation is that for DSP systems the trafﬁc between PEs can be determined statically, i.e., it is known at compile-time rather than at run-time [28]. Consequently, an optimized mapping from DSP algorithm to NoC can be computed ofﬂine, exploiting a global view of the solution space, of which dynamic (online) schedulers can not take advantage. We present a synthesis ﬂow that takes a DSP algorithm and a selected topology as inputs; it optimally implements the algorithm’s communication for that topology. Our ﬂow proceeds in two steps: placement, i.e., assigning DSP computations to PEs on the NoC (Section 4) and scheduling, i.e., constructing a cycle-by-cycle scheme for implementing the communication between PEs on the NoC (Section 5). The whole process is fully automated, and the output schedule can be directly deployed to silicon. We present experimental results for LDPC decoding and FFT in Section 6. We summarize our results in Section 7, and compare with prior work in Section 8. 2 Background 2.1 Formalizing DSP algorithms DSP computations are formalized using the concept of synchronous dataﬂow graphs (DFGs) [37, Chapter 2]. In a DFG, vertices correspond to computations such as addition or multiplication, and directed edges denote data dependencies and delays, as illustrated in Figure 1. A DSP algorithm may have tens of thousands of vertices in its DFG representation. Such algorithms are implemented using folding [37, Chapter 6], wherein a much smaller number of hardware units are time-multiplexed to implement the desired computation. 2.2 Formalizing NoC An NoC is built using a switch fabric, i.e., a collection of links and programmable crosspoints. The NoC connects a set of source nodes S to a set of sink nodes T [44]. We represent the switch fabric as an undirected graph G = (V , E ). X 1 1  a a  b  +  a c  +  a Y 1 6 5 2 3 4 Figure 1. An example of DFG: Y (n) = a·X (n)+ b · X (n − 1) + c · X (n − 2). Vertex X is the input and Y is the output. Labeled circles represent multiplications and additions; there are two edges with one sample time delay. Figure 2. A mesh-structured switch fabric G. Each vertex can be either a source or a sink, but not both, in a cycle. Sources, sinks and intermediate crosspoints all correspond to vertices, and links are modeled as edges. Intuitively, sources and sinks correspond to processing elements, while edges and intermediate nodes constitute the NoC connecting PEs. We will refer to a switch fabric and its graph interchangeably. Our approach can be applied to arbitrary network topologies. Our experiments focus on NoCs organized as meshes, i.e., with the exception of vertices on the boundary, each PE has links to 4 other adjacent PEs. We consider such NoCs because they offer an appropriate compromise between connectivity and cost [30]. 2.3 Formalizing the synthesis problem 2.3.1 Trafﬁc matrix A trafﬁc matrix is an |S | × |T | matrix M , where Mij is a non-negative integer encoding the number of packets to be transferred from source i to sink j . Given a fabric and matrix M , a schedule is a collection of conﬁgurations, where each conﬁguration consists of choices for all programmable crosspoints. These choices result in a set of channels that connect a subset of S to a subset of T . We assume the fabric does not buffer packets internally; hence for a conﬁguration to be valid, no two channels can intersect each other. For each conﬁguration, a ﬁxed-duration cycle is allocated to program the fabric and transfer packets. A schedule Σ is said to complete the matrix M , if by following the procedure above for each conﬁguration in Σ, we can transfer all packets encoded in M from S to T . A workload for a switch fabric is deﬁned to be an ordered set of trafﬁc matrices that the fabric needs to implement during the computation. A schedule Σ is said to complete a workload if by carrying out Σ cycle by cycle, all matrices in the workload will be completed in order. We employ a straightforward transformation from a DFG to a workload. Note that each edge in the DFG will result in a packet to be delivered by the NoC if its both ends are not mapped to the same PE. When an edge’s packet has not been transferred, we mark this edge as unﬁnished; and after transferring the packet, we mark it as ﬁnished. We keep track of the set R of ready edges that are marked as unﬁnished and have no precedent unﬁnished edges. At any time, set R includes all packets available for scheduling. Therefore, at the beginning of each cycle, we formulate a trafﬁc matrix encoding exactly those packets in R and submit it to our algorithm. At the end of each cycle, since some edges’ packets are delivered, some new edges may become ready and should be added into R, and ﬁnished ones will be removed from it. A new matrix will then be formulated based on the updated R in the next cycle and we repeat these steps till we ﬁnish communication. For ease of exposition, we will focus on trafﬁc matrices from now on. 2.3.2 Scheduling Recall that a valid conﬁguration is a set of non-intersecting channels, which corresponds to a collection of paths in G, and no two paths share a common vertex; we refer to such paths as being vertex disjoint. Given a switch fabric G and an assignment of DFG vertices to vertices in G, a matrix m is deﬁned to be G-feasible if there exists a single conﬁguration that completes m. It follows that a matrix m is feasible iff all entries in m are either 0 or 1, and all source-sink pairs corresponding to 1s in m can be connected by a collection of vertex disjoint paths in G. Note that each conﬁguration in the schedule can be mapped to a feasible matrix, or equivalently a vertex-disjoint-path-set (VDPS). Consequently, we will interchangeably refer to a schedule as a collection of feasible matrices or a collection of VDPSs. 2.3.3 Example We present a small but surprisingly interesting instance of the general scheduling problem in Figures 2 and 3. Speciﬁcally, it illustrates that building the schedule greedily—that is by always picking the largest possible VDPS—is suboptimum.   0 0 0 0 0 0 1a 0 0 0 1e 0 0 0 0 0 0 0 1b 0 1c 0 0 0 0 0 0 0 0 0 0 0 1d 0 1f 0   Figure 3. Trafﬁc matrix M for the fabric in Figure 2. The superscripts are packet identiﬁers, e.g., we will refer to the packet from Source 1 to Sink 2 as a. a c f a d b e d b f e c Greedy Decomp Optimum Decomp Figure 4. Greedily constructed and optimum schedules for G and M as presented in Figure 2 and 3, respectively. The largest VDPS corresponds to the packets {a, c, f }, but selecting it leads to a schedule that takes 4 cycles. 3 Physical Implementation Our development is at a relatively high level of abstraction, compared to the ﬁnal gate-level implementation of the network. For example, the cycle we refer in preceding sections to is not the chip’s clock period Tclk ; it is the time to transfer the packet. This time is signiﬁcantly larger than Tclk , and for this reason we model all channels as having the same delay, even though in practice longer interconnects may be pipelined, thereby inducing a latency of a few chip clock periods. Our primary focus in this paper is to compute an optimized placement and schedule for a given network topology and application. The broader design problem needs to consider the VLSI implementation cost of the network. The implementation cost of a network can be estimated using predictive modeling theory for VLSI. The network is physically realized using buffered wires, and programmable crosspoints. The area, delay, and power of an optimized interconnect of a given length in a given manufacturing process can be estimated using existing techniques, e.g., [10, 1, 31]. Similar values for crosspoints can be derived using the estimation approach in [46, Chapter 4]. 4 Placement Optimum placement consists of mapping rows and columns of M to fabric nodes such that the ﬁnal schedule produced has the minimum number of cycles. We show in Appendix A that calculating the optimum schedule for any given placement is NP-hard. Consequently, it is very difﬁcult to evaluate how good a placement is. Hence we design a different objective function that is much easier to compute and very helpful to the later scheduling step. Speciﬁcally, given a placement Π, deﬁne dΠ (s, t) to be the shortest distance from s to t when all edges are of unit length. The objective function Z (Π) we use is: Z (Π) = X s∈S X t∈T Mst · dΠ (s, t) For a mesh fabric, dΠ (s, t) is the Manhattan distance and can be calculated in constant time. Furthermore, if we incrementally update the placement, we just need to calculate the reduction in Z caused by the incremental update. The weight function w(x, y ) between two elements x and y in the set U = S ∪ T is deﬁned to be Mxy + Myx . Starting from an arbitrary placement, we make improvement by continuously applying exchange operations till we can not improve Z anymore. An exchange is an operation on two elements in U , where we swap the nodes to which those two are mapped under current placement. Clearly, after the exchange of x and y under Π, we obtain a new placement Π′ satisfying (1.) Π′ (x) = Π(y ) and Π′ (y ) = Π(x) and (2.) Π′ (u) = Π(u) if u is not x or y . To ensure ﬁnite termination, we perform the exchange only when the operation results in a strictly smaller objective Z (Π′ ) < Z (Π). Speciﬁcally for x and y from H , we need to calculate the following quantity: Z (Π) − Z (Π′ ) = [w(u, x) − w(u, y )][dΠ (u, x) − dΠ (u, y )] X u∈U We perform the exchange iff Z (Π) − Z (Π′ ) > 0. The placement heuristic is presented in detail in Algorithm 1. We point out that in Step 7, only when w(u, x) > 0 or w(u, y ) > 0 do we have an item to accumulate. In most situations the matrix is sparse, i.e., the number of non-zero entries in a row or column is O(1). The time taken in Step 7 is therefore O(1) as well, very fast in practice. For NoCs up to a few hundred nodes, our algorithm runs for less than 30 minutes. For networks of even larger sizes, we may either terminate the run because of timeout or divide the large network into small blocks and perform the algorithm within each block respectively. We illustrate the effectiveness of our heuristic by placing the matrix in Figure 5 on a 4 × 4 mesh. Our heuristic   0 1 2 1 0 0 2 0 1 2 1 0 0 2 2 2 1 2 1 2 2 0 1 2 2 1 0 0 2 2 2 0 2 0 0 1 2 2 2 2 0 2 2 1 0 0 1 0 2 2 2 0 2 0 1 0 0 2 0 2 2 1 1 2   Figure 5. Trafﬁc matrix as the input of Algorithm 1. transforms a starting placement that is very inefﬁcient to an optimal one as shown in Figure 6. Input Output Figure 6. Input and output placements of Algorithm 1 based on matrix in Figure 5; solid circles represent sources, empty circles represent sinks, they are both placed on a 4 × 4 mesh. 4: Algorithm 1 Placement Heuristic Input: graph G, set U = S ∪ T and matrix M Output: Π—a mapping from U to vertices in G 1: Initialize Π arbitrarily; 2: Calculate the weight w(x, y ) for all elements in U ; 3: repeat Improved ← F alse; for all x ∈ U do for all y ∈ U do Calculate ∆ = Z (Π) − Z (Π′ ) for x and y ; if ∆ > 0 then Map x to Π(y ) and y to Π(x); Improved ← T rue; 11: until Improved = F alse 10: 6: 7: 5: 8: 9: 5 Scheduling The scheduling problem for a general network is NPhard [16]. Although there are fast algorithms for crossbars and tree topologies; a crossbar, being dense, has a high implementation cost, and a tree fabric is inadequate because of its limited connectivity [44]. We will see an NoC organized as a mesh, which has low implementation cost, performs almost as well as a full crossbar for FFT and LDPC decoding. Our heuristic is built upon the metric of congestion on edges. Consider an instance of the scheduling problem, with G the fabric, and M the trafﬁc matrix, with sources S and sinks T . Deﬁne the distance de,w between an edge e = {u, v} and a vertex w by max{d(u, w), d(v , w)}, where d(x, y ) is length of the shortest path in G between x and y , when edges are of unit length. The formula of congestion can be as simple as setting Ce = 1, for all edges e. We name this basic version uniform congestion, as it does not vary over the edge set. However, the ﬁnal version we adopt is deﬁned by the following equation: Ce = X s∈S Ws de,s + X t∈T Wt de,t where Ws and Wt are the corresponding row and column sums for M . We refer to this one as distance-inverted congestion. The congestion metric is designed to reﬂect the attenuating trend when the distances to sources or sinks increase. Also importantly, this metric is fast to calculate using breadth-ﬁrst search from each source and sink: simply accumulate all source or sink quotients without caring about the order in which vertices are visited. Furthermore, the distance part can be cached since when computing the congestion, the topology is always the original graph G. Several key points in the heuristic are: 1. Loop from Line 13 to 15 chooses the path with the least blockage, thereby avoiding congested regions. 2. The computation of shortest paths in Line 12 with Dijkstra’s algorithm is very fast in theory and practice. 3. All vertices in path p∗ are isolated as in Line 19. This guarantees the vertex disjoint property of all paths added. 4. At Line 21, we always zero out a row or column with the largest sum in M and repetition of this operation will eventually turn M into an all-zero matrix, guaranteeing ﬁnite termination. 5. This procedure is constructive and always produces a feasible schedule regardless of the given placement and topology. The objective here is to minimize the number of transfer cycles, which is not necessarily proportional to the actual Algorithm 2 Heuristic to Generate a Schedule Input: graph G, placement Π and matrix M Output: Σ—a schedule completing M 1: Σ ← ∅; 2: while M has positive entries do Backup M in M ′ ; 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: V DP S ← ∅; Calculate Ce for all edges by doing breadth-ﬁrst search from each source and sink (rows and columns in M ); repeat Pick the row or column in M with the largest sum, record the corresponding vertex as v∗; if v∗ is a source (row chosen) then Put all sinks into the target set Target; else Put all sources into Target; Compute shortest paths P = {pv∗w } from v∗ to vertices w in Target; for all pv∗w ∈ P do Determine the blockage of the path, the largest Ce among edges in pv∗w ; Keep track of the path with the least blockage in p∗ connecting v∗ to w∗; if the entry in M for v∗ and w∗ > 0 then Add the path p∗ to V DP S for all v in p∗ do Remove all edges incident at v ; Set the row or column for w∗ to zeros; Set the row or column for v∗ to zeros; until all entries in M are zeros Add V DP S to Σ; Restore M from M ′ ; Determine the number of packets to transfer through each path in V DP S ; Decrease entries in M according to Step 25; packet latency as stated in Section 3. When operating in high frequency, the network must be carefully pipelined to match fast PEs, which makes the length of each transfer cycle variable. To account for the pipeline effect, a meaningful extension to Algorithm 2 would be to pack as many short paths as possible into one cycle when we have chosen a long path. 6 Experiments on LDPC Decoding and FFT As mentioned Section 2.2.1, there are an enormous number of vertices in the DFGs for the examples that motivated our work—far in excess of the number of physical PEs that can be implemented on a chip. A large size FFT is thus implemented by computing a series of of smaller size FFT, Code Num. Cycles Lower Bound C1 C3 C3 C4 C5 C6 C7 C8 18 16 16 14 14 15 18 17 13 15 12 12 13 11 12 13 Table 1. Size of schedule generated by Algorithm 2 for LDPC codes C1–C8. e.g., Maharatna et al. [32] implement a 64-point FFT using two 8-point FFT units. For LDPC decoding, folding (cf. Section 2.2.1) results in multiple code and check nodes getting mapped to the same PE, which essentially forms a new random trafﬁc matrix of a smaller size. We applied our synthesis ﬂow to implementing LDPC decoding and FFT on an NoC organized as a mesh. In view of the comments above, we chose to report results on an LDPC block of size 96, and a 512-point FFT. 6.1 LDPC Decoding An LDPC code is a block code, where there are C bits per block, which include D parity checks. It is most naturally represented as a bipartite graph on a set of C code nodes and D check nodes. The decoding algorithm [15] involves iterations of message passing back and forth between connected code and check nodes, and it is this communication that deﬁnes the trafﬁc matrix. We created 8 LDPCs C1–C8 with 96 code and 48 check nodes using randomized code construction techniques [7]. Each entry in the connection matrix corresponds to exact one transfer of a result from a code node to a check node or vice versa. The 144 code and check nodes are embedded on a 23 × 23 mesh. They are ﬁrst placed on a 12 × 12 smaller mesh with our placement heuristic and then we add one extra track between adjacent rows or columns to help routing, which makes the mesh a square of size 23 = 12 × 2 − 1. Results for the these 8 different LDPC codes are presented in Table 1, Row 2. 6.2 The FFT An N -point FFT can be implemented with parallel hardware using 1 + log2 N stages, where each stage consists of N/2 PEs that implement “butterﬂy” operations in parallel; the results of these operations are passed on to speciﬁc processing elements in the next stage [11]. Speciﬁcally, between stage l and l + 1, PEi (l) passes its results to two PEs: (1.) PEi (l + 1), and (2.) depending on i, either PEi+2l−1 (l + 1) or PEi−2l−1 (l + 1). It is straightforward to encode the results that need to be communicated from one stage to the next as a trafﬁc matrix. Since each PE has two inputs and produces two outputs, there are N/2 PEs per stage. Stage Num. Cycles F1 3 F2 4 F3 6 F4 9 F5 3 F6 4 F7 6 F8 9 DFG DIC UC F4 9 11 F8 C1 C2 9 18 16 12 20 18 L1 32 37 L2 29 34 Table 2. Size of schedule generated by Algorithm 2 for each of the 8 stages in a 512-point FFT. We placed 512 PEs on a 63 × 63 mesh for a 512 point FFT. Half of them implement stage l , and the other half implement stage l + 1. The remaining 632 − 512 = 3457 crosspoints on the mesh are used as routing resources. We give the results of our heuristic on the 512 point FFT in Table 2. 6.3 Quality of Results 6.3.1 Runtimes For both FFT and LDPC, our heuristic computed the schedule in seconds. Our implementation of the heuristic is very straightforward, and it could likely be sped-up greatly, but there is little incentive to do so since the computation is offline. 6.3.2 The BvN bound An NoC is said to be rearrangeable if it allows any source to be connected to any sink, regardless of other source-sink connections. For such NoCs, the minimum number of cycles needed to complete a trafﬁc matrix is the maximum of the row and column sums of the matrix [9]. This value is a lower bound on the number of cycles needed to complete the matrix for any NoC. We calculated this bound for the LDPC trafﬁc matrices. The comparison of our solutions and the optimum bounds is presented in Table 1, Row 3. Our schedules are fairly close to the bound, reinforcing our conﬁdence in the heuristic. (Note that a rearrangeable NoC is quite expensive to implement.) 6.3.3 Distance Inverted Congestion Table 3 demonstrates the extra complexity in distance inverted congestion does result in consistent improvement in benchmarks. 6.4 Placement Rather than measuring the beneﬁts of our placement heuristic against random placements, we illustrate the effectiveness of our placement heuristic by manually generating what we believed to be a reasonably good placement for LDPC. Speciﬁcally, we thought an interleavTable 3. Examples F4, F8, C1, and C2 are as before; L1 and L2 are larger LDPC codes. The row labeled DIC shows the number of cycles produced by the heuristic based on distanceinverted congestion. The row labeled UC shows the number of cycles produced by the heuristic based on uniform congestion. Code Heuristic Interleaving C1 C3 C3 C4 C5 C6 C7 C8 18 16 16 14 14 15 18 17 30 28 28 18 31 27 29 30 Table 4. Schedule sizes for LDPC codes C1– C8 for interleaving placements and those generated by with Algorithm 1. ing placement, in which we place all code nodes at locations {(6i, 2j ), (6i + 4, 2j )}, all checker nodes at locations {(6i+2, 2j )}, where 0 ≤ i ≤ 3 and 0 ≤ j ≤ 11, would be a good placement. In this placement, each checker row comes in between two coder rows and again we reserved one extra track between adjacent rows and columns. This placement seemed to offer reasonable connectivity given the construction of the LDPC matrix. We ran our placement heuristic against this starting point, and were surprised to ﬁnd that our placement heuristic generated a placement that when input to our scheduling heuristic resulted in a schedule that reduced the number of cycles by almost 50% compared to the schedule that our scheduling heuristic produced on the original interleaving placement (Table 4). 7 Conclusion We have developed general models and algorithms for implementing DSP algorithms with an on-chip network. Our work can be extended in several ways. First, we would like to apply these scheduling ideas to networks which do use internal buffering—for example, the switchmemory-switch architecture [3] in which packets are transferred to a large ensemble of parallel memories. Secondly, we would like to consider the problem of efﬁciently implementing communication on an NoC when trafﬁc is stochastic, but changes relatively slowly. Finally, we would like to develop algorithms which use predictive models for estimating VLSI cost of a network topology in conjunction with the techniques developed in this paper to automate the design of the entire network. 8 Relationship to Prior Work 8.1 Summary Our treatment of the NoC scheduling problem differs from multiprocessor routing [30] because our algorithm solves general trafﬁc matrices taken from DSP, whereas classic work almost entirely focuses on routing individual permutation matrices. Our work differs from the routing problem considered in physical design [42] in the following way: the main objective in physical design is to provide connectivity, while minimizing combinations of area/power/delay; we to increase the utilization of interconnects by time-multiplexing them. For majority of the NoC research that will be described in detail in the next subsection, researchers consider design constraints and objective of their networks in terms of statistical metrics. These researchers perform scheduling dynamically—routers are responsible for forwarding packets or setting up circuits based on incoming trafﬁc at the instance of time. The assumption is that the network’s behavior hinges on the input data, which is the best we can obtain for general purpose computation and networking applications. But for DSP applications, on-chip networks quite often operate independently of the actual incoming data. As a result, the amount of data and time to transfer them are given to designers exactly. Inspired by BvN decomposition [9], we solve the problem in the context of a determined future. Lastly, considering the implementation circuitry, statically scheduled routers are much simpler than dynamic counterparts. Dynamic routing requires the ability to buffer data in the presence of conﬂicts; these buffers are quite expensive. In our approach, we need to store the schedule in a ROM, but this is relatively much smaller than SRAM/ﬂop based buffers needed for resolving conﬂicts in a dynamic router. 8.2 Detailed Literature Review 8.2.1 Bus-based Networks Commercial bus-based networks such as AMBA [2] and CoreConnect [22] have been widely recognized. Sonics MicroNetwork [50] is an early attempt that automatically generates the communication subsystem in a highly customizable SoC design ﬂow. The network can be abstracted as a TDMA bus, and handles physical issues inside its root and agent transceivers. STbus [36] provides similar features, but it also supports more advanced topologies such as partial or full crossbars. Lotterybus [27] addresses the problem from a statistical point of view, offering low latency for high priority burst trafﬁc and effective bandwidth guarantee. Although shared buses are easy to design and interface with, they are criticized in [4, 13, 44] for their poor scalability. A point-to-point network with switchers or routers controlling the packet ﬂow is envisioned by Dally et al. [4, 13], along with a light weight communication protocol that enhances the performance and reliability. Bjerregaard et al. [6] summarizes the cutting edge research in a detailed survey. 8.2.2 NoC Architectures Historically, high speed interconnects were extensively studied for building multiprocessor machines. Classics such as [30] provide comprehensive coverage of network architectures and routing algorithms. The Maia system [52] leverages heterogeneous function blocks to achieve high performance and power efﬁciency for DSP applications. The communication is serviced by a hierarchical mesh network based on circuit switching. And the network itself is designed ad hoc according to the speciﬁc implementation of the algorithm. Maia exploits the trafﬁc patterns in DSP algorithms for partition of function blocks and choosing the network architecture, which is well resonated in our work. However our approach differs in that: (1.) we focus on a general topology packet switching network connecting a sea of homogeneous processing elements, (2.) we provide theoretical analysis and practical heuristics for near optimum schedule given the static trafﬁc and (3.) last but not the least, our ﬂow is completely independent of the speciﬁc algorithm to implement, i.e., orthogonal to the design of computation blocks [25]. To facilitate packet routing and physical design process, most NoC designers adopt regular structures, such as meshes or trees. The RAW processor ﬁrst appeared in [45] is composed of tiled cores. In its latest incarnation, the “scalar operand network” [43] includes two statically scheduled meshes and one dynamically scheduled mesh. Instructions to fetch data from adjacent tiles are directly exposed into ISA. The compiler is responsible for inserting communication instructions and optimizing the binary program for mesh networks. One key observation is that the latency of the network can be amortized because of dominating locality in general purpose computing. Compared with our ﬂow, developers of RAW heavily focus on data and computation partition to minimize trafﬁc, which can be considered as a much larger superset of our placement technique. Our effort is to shorten the time of communication given a batch of transfers whereas in [45], an average time cost is assumed for each communication instruction. NOSTRUM [26, 33] is an NoC system built upon a medium sized mesh and a layered protocol stack. The combined backbone based application speciﬁc platform is then delivered to SoC designers for further mapping. An interesting point made by NOSTRUM creators is that for multimedia devices and consumer electronics, trafﬁc exhibits high locality, therefore higher order topologies are rarely necessary and meshes provide suitable level of redundancy for robust operation. The Scalable Programmable Integrated Network (SPIN) described in [17] selects fat-tree as its topology and features a carefully designed router optimized for small packets with efﬁcient buffer management. Wiklund et al. in [48] employs a two dimensional mesh network SoCBUS and packet connected circuit (PCC), i.e., the packet locks the path that it traverses in the network. This path setup technique helps achieve appealing bandwidth with low latency in the dynamically scheduled network. An application to an Internet core router design is also presented in [49]. Other structures such as octagon [24] and star connect [29] have also been proposed. Rijpkema et al. [39] explore the trade off between guaranteed and best effort services in on-chip router design. An on-line matrix scheduling scheme is applied in the inputqueued architecture. Our work also enjoys the concise formulation of trafﬁc matrices, but embraces an off-line scheduling scheme without intermediate buffering. 8.2.3 Synthesis and Design Support Project PROTEO [40] provides a library of parameterized components aiming at decoupling logic design from underneath technology. Complex NoC designs can be synthesized from components in the library, whose parameters can be further tuned to support target process. Pinto et al. considers the general communication system synthesis problem in [38]. A latency insensitive network design is further proposed in [8], which focuses on a high performance onchip interconnect for IP blocks with minimal impact to the back-end design ﬂow. Zhu et al. [53] presents a hierarchical approach for modeling NoCs based on a class library. An irregular network generation procedure is described in [18]; both temporal and spacial information are exploited in the optimization process. Memory optimization issues in networking chips are explored in [47]. The deep combinatorial problem of assigning cores to network nodes is discussed in [21, 34]. A complete synthesis ﬂow of NoCs for multiprocessor SoC appears in [5], which includes xpipes [12] a library of soft macros, xpipesCompiler [23] that generates network components based on xpipes, and SUNMAP [35] that maps cores onto the selected network architecture. "
Thermal Impacts on NoC Interconnects.,"Thermal issues are an increasing concern in microelectronics due to increased power density as well as the increasing vulnerability of the system to temperature effects (delay, leakage, reliability). NoCs promise to relieve many of the scaling problems that arise with increasing levels of on-chip system integration. This paper addressed the impacts on NoC interconnect circuits under harsh uniform temperature changes and non-uniform spatial temperature distribution profiles. Temporal and spatial thermal variations were addressed in 65 nm, 45 nm and 32 nm interconnect circuits. Standard repeater insertion and differential current sensing techniques have been implemented. The circuits were analyzed in temperatures as high as 150degC for the temporal variations, with a maximum temperature difference through wire of up to 50degC. High temperature caused more delay and power overhead in smaller technologies, i.e. 45 nm and 32 nm, by as much as 71% at 150degC for a given wirelength of 3 mm in 32 nm. Spatial temperature distribution profile influenced the propagation delay by 14.7% for a maximum thermal gradient of 50degC in the worst case for a 32 nm, 3 mm repeated wire. However, the delay degradation of an alternative differential current sensing (DCS) technique is largely determined by the amplifier temperature. Future work may consider the modeling of self-heating of the interconnect circuits","Thermal Impacts on NoC Interconnects  Sheng Xu, Ibis Benito and Wayne Burleson  Department of Electrical and Computer Engineering  University of Massachusetts Amherst  Amherst, MA 01002  {shxu,ibenito,burlesong@ecs.umass.edu}  Abstract  Thermal issues are an increasing concern in  microelectronics due to increased power density as  well as the increasing vulnerability of the system to  temperature effects (delay, leakage, reliability). NoCs  promise to relieve many of the scaling problems that  arise with  increasing  levels of on-chip system  integration. However, as  the  technology scales,  temperature effects become more significant and  designing for performance becomes more difficult. It is  crucial for designers to understand the impact of  thermal variations on these systems to reduce hotspots  and maintain performance.  This paper addressed  the  impacts on NoC  interconnect circuits under harsh uniform temperature  changes and non-uniform  spatial  temperature  distribution profiles. An analytical discussion has been  provided, to consider temperature variation impact on  both gate and wire delay. Temporal and spatial  thermal variations were addressed in 65nm, 45nm and  32nm  interconnect  circuits. Standard  repeater  insertion and differential current sensing techniques  have been implemented and their performance was  compared under different thermal scenarios. The  circuits were analyzed in temperatures as high as  150ºC for the temporal variations, with a maximum  temperature difference through wire of up to 50ºC.  High temperature caused more delay and power  overhead in smaller technologies, i.e. 45nm and 32nm,  by as much as 71% at 150ºC for a given wirelength of  3mm in 32nm. Spatial temperature distribution profile  influenced the propagation delay by 14.7% for a  maximum thermal gradient of 50ºC in the worst case  for a 32nm, 3mm repeated wire. The repeated line is  affected more by a decreasing spatial temperature  profile than by an increasing profile. However, the  delay degradation of an alternative differential current  sensing (DCS) technique will be largely determined by  the amplifier temperature.   From these observations, we can conclude that as  designs scale down into future technologies, shorter  wires in NoCs will be preferable from a thermal  standpoint. Design for balanced core temperatures  becomes extremely important to avoid hotspots that  may cause performance degradation in NoCs. As an  alternative to the traditionally used repeater insertion  techniques, designers may consider  the use of  advanced circuit techniques such as DCS. Future work  may consider the modeling of self-heating of the  interconnect circuits  to provide more accurate  predictions. Wire modeling may also be modified to  account for thermal behavior of vias and contacts.  Table 1 Temperature Variat ion Effects on Delay  Technology node [nm]  65  45  Inver ter at 25ºC [ps]  16.75  19.3  PMOS device at 125ºC [ps]  16.78  19.49  NMOS device at 125ºC [ps]  20.51  24.43  Device and wire at 25ºC [ps]  51.7  51.6  Device at 125º, wire at 25ºC [ps]  52.6  71  Device and wire at 125ºC [ps]  55.6  107.3  32  22.3  22.43  27.64  50.7  66.7  100.8  Rep Delay DCS Delay Rep Energy DCS Energy  220  200  180  160  140  120  100  80  60 ] j f [ y g r e n E  100 Temperature[C]  125  150 ] s p [ y a e l D  240  220  200  180  160  140  50  75 Figure 1 Impact on delay and energy due to temporal thermal  variation on a repeated interconnect compared to DCS for a  45nm, 3mm wire.  This project is funded by SRC grant ID 1415.01.                  "
